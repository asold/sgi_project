{
  "title": "AstroCLIP: A Cross-Modal Foundation Model for Galaxies",
  "url": "https://openalex.org/W4387390369",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4226638207",
      "name": "Parker, Liam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3157856895",
      "name": "Lanusse, Francois",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227293571",
      "name": "Golkar, Siavash",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4369888190",
      "name": "Sarra, Leopoldo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221489772",
      "name": "Cranmer, Miles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214061798",
      "name": "Bietti, Alberto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214143078",
      "name": "Eickenberg, Michael",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Krawezik, Geraud",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286910602",
      "name": "McCabe, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222774078",
      "name": "Ohana, Ruben",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222592376",
      "name": "Pettee, Mariel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224772720",
      "name": "Blancard, Bruno Regaldo-Saint",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289405672",
      "name": "Tesileanu, Tiberiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202042030",
      "name": "Ho, Shirley",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4286895361",
    "https://openalex.org/W4387390336",
    "https://openalex.org/W4300089971",
    "https://openalex.org/W4318225439",
    "https://openalex.org/W2559655401",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W656301489",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4384652395",
    "https://openalex.org/W4297798428",
    "https://openalex.org/W4322734649",
    "https://openalex.org/W2994658979",
    "https://openalex.org/W4319453073",
    "https://openalex.org/W4319453761",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4367000428",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3213836217",
    "https://openalex.org/W4366208220",
    "https://openalex.org/W3120971630",
    "https://openalex.org/W4380551324",
    "https://openalex.org/W4283644577",
    "https://openalex.org/W3104062568",
    "https://openalex.org/W4306968094",
    "https://openalex.org/W3121052760",
    "https://openalex.org/W4236965008",
    "https://openalex.org/W4361194507",
    "https://openalex.org/W4249138131"
  ],
  "abstract": "We present AstroCLIP, a single, versatile model that can embed both galaxy images and spectra into a shared, physically meaningful latent space. These embeddings can then be used - without any model fine-tuning - for a variety of downstream tasks including (1) accurate in-modality and cross-modality semantic similarity search, (2) photometric redshift estimation, (3) galaxy property estimation from both images and spectra, and (4) morphology classification. Our approach to implementing AstroCLIP consists of two parts. First, we embed galaxy images and spectra separately by pretraining separate transformer-based image and spectrum encoders in self-supervised settings. We then align the encoders using a contrastive loss. We apply our method to spectra from the Dark Energy Spectroscopic Instrument and images from its corresponding Legacy Imaging Survey. Overall, we find remarkable performance on all downstream tasks, even relative to supervised baselines. For example, for a task like photometric redshift prediction, we find similar performance to a specifically-trained ResNet18, and for additional tasks like physical property estimation (stellar mass, age, metallicity, and sSFR), we beat this supervised baseline by 19\\% in terms of $R^2$. We also compare our results to a state-of-the-art self-supervised single-modal model for galaxy images, and find that our approach outperforms this benchmark by roughly a factor of two on photometric redshift estimation and physical property prediction in terms of $R^2$, while remaining roughly in-line in terms of morphology classification. Ultimately, our approach represents the first cross-modal self-supervised model for galaxies, and the first self-supervised transformer-based architectures for galaxy images and spectra.",
  "full_text": "MNRAS000, 1â€“18 (2024) Preprint 17 June 2024 Compiled using MNRAS L ATEX style file v3.0\nAstroCLIP: A Cross-Modal Foundation Model for Galaxies\nLiam Parker,1 â˜… â€ Francois Lanusse,1,3 Siavash Golkar,1 Leopoldo Sarra,1 Miles Cranmer,4\nAlberto Bietti,1 Michael Eickenberg,1 Geraud Krawezik,1 Michael McCabe,1,5 Rudy Morel,1 Ruben Ohana,1\nMariel Pettee,1,6 Bruno RÃ©galdo-Saint Blancard,1 Kyunghyun Cho,7,8,9 Shirley Ho1,7,10 and\nThe Polymathic AI Collaboration\n1The Flatiron Institute, 162 5th Ave, New York, NY, 10010, USA\n2UniversitÃ© Paris-Saclay, UniversitÃ© Paris CitÃ©, CEA, CNRS, AIM, Paris, 91190, France\n3Department of Astronomy, University of Cambridge, Madingley Rd, Cambridge, CB3 0HA, UK\n4Department of Computer Science, University of Colorado, Boulder, 430 UCB, 1111 Engineering Dr, Boulder, CO, 80309, USA\n5Lawrence Berkeley National Laboratory, Berkeley, 1 Cyclotron Rd, CA, 94720, USA\n6Center for Data Science, New York University, 60 5th Ave, New York, NY, 10011, USA\n7Prescient Design, Genentech, 149 5th Ave, New York, NY, 10010, USA\n8CIFAR Learning in Machines and Brains Fellow, Toronto, ON M5G 1M1, Canada\n9Department of Astrophysics, Princeton University, 4 Ivy Lane, Princeton, NJ, 08544, USA\nAccepted XXX. Received YYY; in original form ZZZ\nABSTRACT\nWe present AstroCLIP, a single, versatile model that can embed both galaxy images and spectra into a shared, physically\nmeaningful latent space. These embeddings can then be used - without any model fine-tuning - for a variety of downstream\ntasks including (1) accurate in-modality and cross-modality semantic similarity search, (2) photometric redshift estimation, (3)\ngalaxy property estimation from both images and spectra, and (4) morphology classification. Our approach to implementing\nAstroCLIPconsistsoftwoparts.First,weembedgalaxyimagesandspectraseparatelybypretrainingseparatetransformer-based\nimage and spectrum encoders in self-supervised settings. We then align the encoders using a contrastive loss. We apply our\nmethod to spectra from the Dark Energy Spectroscopic Instrument and images from its corresponding Legacy Imaging Survey.\nOverall, we find remarkable performance on all downstream tasks, even relative to supervised baselines. For example, for a task\nlike photometric redshift prediction, we find similar performance to a specifically-trained ResNet18, and for additional tasks\nlike physical property estimation (stellar mass, age, metallicity, and sSFR), we beat this supervised baseline by 19% in terms\nof ğ‘…2. We also compare our results to a state-of-the-art self-supervised single-modal model for galaxy images, and find that\nour approach outperforms this benchmark by roughly a factor of two on photometric redshift estimation and physical property\nprediction in terms ofğ‘…2, while remaining roughly in-line in terms of morphology classification. Ultimately, our approach\nrepresents the first cross-modal self-supervised model for galaxies, and the first self-supervised transformer-based architectures\nfor galaxy images and spectra.\nKey words:methods: data analysis â€“ galaxies: general\n1 Introduction\nAstronomical datasets continue to expand rapidly in size and com-\nplexity.OngoingsurveysliketheDarkEnergySpectroscopicInstru-\nment(DESI;Deyetal.2019)alreadyencompassmillionsofobjects\nand future surveys, like the Vera C. Rubin Legacy Surveys of Space\nandTime(LSST;IveziÄ‡et.al.2019)andEuclid(LaureÄ³setal.2011),\nare expected to broaden this scope to include billions of objects.\nA variety of computational approaches have been developed to\nprocess the data from these surveys (IveziÄ‡ et al. 2020). In recent\nyears, a growing subset of these approaches have employed data-\nâ˜… E-mail:lparker@flatironinstitute.org\nâ€ Present Address: 162 5th Ave, New York, NY 10010, USA\ndriven methodologies from machine learning (ML). To date, these\napproaches have largely been separated into two different classes:\nâ€¢Supervised methods leverage labeled subsets of observational\ndata to perform discriminative tasks like galaxy morphology clas-\nsification, photometric redshift estimation, weak lensing, etc. (for\na recent review, see Huertas-Company & Lanusse 2023), and have\nachieved significant progress in data-rich settings. However, these\nmethods are ultimately constrained by the quantity and quality of\nlabelled training samples available and are often exposed to only a\nsmallfractionofthepotentiallyavailabledataduringtraining.Addi-\ntionally, bespoke supervised models need to be retrained/redesigned\nfrom scratch for each new task, creating significant computational\ninefficiencies in the data analysis pipeline.\nâ€¢Unsupervised methods use clustering, principal component\nanalysis, and other techniques to bypass the need for labelled data.\nÂ© 2024 The Authors\narXiv:2310.03024v2  [astro-ph.IM]  14 Jun 2024\n2 Liam Parker\nThesehavebeenemployedfortaskslikestronglensdetection(Cheng\net al. 2020), anomaly detection (Margalef-Bentabol et al. 2020), etc.\nHowever, while they do not rely on labelled subsets of the data, they\nare still typically task-specific, and they have lagged behind their\nsupervised counterparts in performance (Caron et al. 2018).\nRecently,anewlineofinquiryhasexplored self-supervisedlearn-\ning (SSL) as an alternative. These approaches learn high-quality\nembeddings i.e. low-dimensional representations of the objects that\npreserve their important physical information - in the absence of\nlabeled training data. These embeddings can then be used for a va-\nriety of downstream tasks, eliminating the need to retrain bespoke\nsupervised models from scratch for each new dataset or new task.\nThis is achieved by training models to perform some surrogate\ntask, such as identifying corrupted pairs or filling in masked sub-\nsections of the input data. This in turn produces a high-quality, low-\ndimensional representation which can be used as a â€œfoundationâ€ for\ndownstream tasks; these types of models are therefore often dubbed\nfoundation models(Bommasanietal.2021).Incomputervision(CV;\nHe et al. 2021; Tong et al. 2022) and natural language processing\n(NLP;Radfordetal.2019),theseapproacheshavealreadyclosedthe\ngap with their supervised counterparts; indeed, zero- and few-shot1\ntraining on the learned representations can even exceed supervised\nperformance, especially in domains in which training large super-\nvisedmodelsfromscratchisinfeasibleduetoconstraintsonlabelled\ndata(Bommasanietal.2021).Moreover,recentworkshavenowex-\ntendedtheseresultsintothephysicalsciencesmorebroadly(Nguyen\net al. 2023; McCabe et al. 2023; Subramanian et al. 2024).\nA variety of SSL strategies have already been deployed in ob-\nservational astronomy. For example, one of the earliest explorations\nof SSL in the context of astronomical images is the application of\ntheMomentumConstrativepretrainingstrategy(MoCov2;Heetal.\n2020) on galaxy images (Hayat et al. 2020; Stein et al. 2021b). This\nframeworklearnsembeddingsofimagesbymaximizingthesimilar-\nity of embeddings between different augmented views of the same\nimagewhileminimizingsimilaritywithembeddingsofotherimages.\nTheseembeddingscanthenbeusedtopredictgalaxyredshift(Hayat\netal.2021),performsimilaritysearches,andsearchforrare,scientif-\nically interesting objects like strong gravitational lenses (Stein et al.\n2021a).Anotherprominentexampleinthisfieldistheapplicationof\na Bootstrap Your Own Latent (BYOL; Grill et al. 2020) strategy for\ngalaxymorphologyclassification(Walmsleyetal.2022a)toachieve\nstate-of-the-artperformanceafterfine-tuninginthelowdataregime.\nSSLhasalsobeenemployedongalaxyspectra.Forexample,Por-\ntillo et al. (2020) use a variational auto-encoder (VAE) to reduce\nthe dimensionality of galaxy spectra to a small latent space before\nusing a decoder to generate the rest-frame spectrum; the learned la-\ntentspacethenpossessessignificantintrinsic,rest-frameinformation\nabout the galaxy spectra, which can be used for downstream tasks\nlike outlier detection, interpolation, and galaxy class classification.\nFurther work Teimoorinia et al. (2022); Melchior et al. (2023) add\nsuccessive improvements to the existing VAE; their embeddings are\nthen similarly useful for downstream tasks like anomaly detection\n(Liang et al. 2023a,b).\nHowever, despite this recent progress, all of the current SSL ap-\nproachesinobservationalastronomyhavebeenlimitedtoembedding\n1 Inzero-shotlearning,themodelappliesitslearnedrepresentationstoiden-\ntify or categorize new, unseen data instances, without the need for additional\ntraining specifically on these new categories or instances. In few-shot learn-\ning,thepretrainedmodelisfine-tunedwithaverysmalldatasetrelatedtothe\nnew task.\nobjects from a single modality at a time. In an astrophysical context\nthough, there exist a number of complementary observations of the\nsame underlying physical processes; for example, galaxies are of-\nten measured using a variety of realizations, including photometry,\nmulti-bandimages,andopticalspectra.Assuch,auniversalfounda-\ntionmodelforobservationalastronomyshouldbeabletosimultane-\nously embed cross-modal realizations of the same physical process\ninto a shared latent space. Then, the learned representations of any\ngiven object can be easily searched across different modalities and\nused seamlessly for a variety of downstream tasks.\nIn this work, we introduceAstroCLIP, a cross-modal foundation\nmodelforgalaxies.Ourapproachconsistsoftwodistinctsteps.First,\nwe pre-train state-of-the-art image and spectrum encoders to ex-\ntract high-quality embeddings of galaxies in a single-modal, self-\nsupervised setting. Then, we align the image and spectrum embed-\ndingsbymaximizingthesimilaritybetweencross-modalembeddings\nthatcorrespondtothesamegalaxywhilesimultaneouslyminimizing\nthe similarity between cross-modal embeddings that correspond to\ndifferent galaxies.\nWe apply our methodology to optical spectra from the Dark En-\nergySpectroscopicInstrument(DESI) 2 andmulti-bandimagesfrom\nits corresponding Legacy Imaging Survey3, and demonstrate that\nour learned embeddings are organized around meaningful physical\nsemantics. This allows them to be used as powerful foundations for\nboth similarity searches and discriminative tasks. This approach is\nillustrated in Figure 1. Ultimately, we hope that in providing a pow-\nerful cross-modal foundation model for galaxy spectra and images,\nalong with a set of physically organized, low-dimensional galaxy\nembeddings, we will empower a wide variety of downstream data\nanalysis applications in the field.\nThe main contributions of our work are:\nâ€¢We develop the first self-supervised transformer-based models\nfor galaxy spectra and images.\nâ€¢Weapplyacross-modaltrainingregimetoalignthepre-trained\nimage and spectrum encoders around shared physical semantics,\ncreating a unified latent space for spectra and images.\nâ€¢We empirically demonstrate that our cross-modal embeddings\ncapture core physical properties of the underlying galaxies. This\nenables, with only minimal downstream processing, AstroCLIP to\nbe used for:\nâ€“ In-modal and cross-modal galaxy similarity searches.\nâ€“ Photometric redshift estimation\nâ€“ Galaxy property estimation from images\nâ€“ Galaxy property estimation from spectra\nâ€“ Galaxy morphology classification from images.\nCodeforourmodels,trainingandtestingkitisavailableonlinehere.\nOur paper is organized as follows. In section 2, we provide back-\nground on self-supervised learning, as well as on the particular SSL\nobjectives used in the present paper. In section 3, we describe the\nspecificsofourAstroCLIPimplementation.Insection4,weprovide\nthedatasetsthatweusetotrainourmodelsandinsection5,weout-\nline the training process of our models. In section 6, we present our\nresults on in-modal and cross-modal similarity searches, photomet-\nric redshift estimation, galaxy property prediction, and morphology\nclassification. Finally, we discuss our results and further extensions\nof our paper in section 7.\n2 https://data.desi.lbl.gov/doc/\n3 https://www.legacysurvey.org/\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 3\nShared embedding \nspace\nImage\nencoder\nSpectra\nencoder\nZero-shot prediction\nCandidate \nspectra\nNearest\nneighbor\nFigure 1.Illustration of theAstroCLIP cross-modal training strategy. This approach consists of two steps. First, galaxy images and spectra are embedded\nseparately by pretraining both an image and a spectrum encoder in a SSL setting. Then, these encoders are aligned using a cross-modal contrastive loss. Once\naligned, these embeddings allow us to connect and compare cross-modal representations. Moreover, they possess physically meaningful high-level information\nwhich can be used for a variety of downstream tasks on which the model was neither trained nor fine-tuned.\n2 Self-Supervised Learning\nIn self-supervised learning (SSL), the objective is to train a model\nto learn to extract rich, low-dimensional representations from data\nwithouttheneedforanylabels.Thisistypicallyachievedbytraining\nthemodeltoperformsomecontrivedsurrogatetaskontheinputdata.\nInrecentyears,avarietyofsuchsurrogatetaskshavebeendeveloped.\nOne common example of such a task in NLP is to predict the next\nword in a sentence given the previous words; this is typically called\nautoregressive prediction (Radford et al. 2019). Many other such\nobjectives have been developed, including masked reconstruction\n(Devlin et al. 2018; He et al. 2021), self-distillation (Fang et al.\n2021), and contrastive learning (Chen et al. 2020; Radford et al.\n2021). Ultimately, these approaches have been shown to generate\ngeneralizeable,highlyinformativerepresentationsinbothNLP(e.g.,\nGPT:Radfordetal.2019)andCV(Heetal.2021;Tongetal.2022).\nDespitetheirtask-agnostictraining,thezero-andfew-shotlearning\nperformedonthelow-dimensionalrepresentationscapturedbythese\nmodels has outperformed supervised training in a wide variety of\nsettings, especially in domains in which training large supervised\nmodels from scratch is infeasible due to constraints on labelled data\n(Bommasani et al. 2021). These successes have also highlighted\nthe importance of scale in SSL training strategies, as scaling laws\nestablishedinbothCV(Zhaietal.2022)andNLP(Fangetal.2021)\ndemonstratesignificantgainsinperformancewithlargermodelsizes,\ndataset sizes, and total compute.\nIn the following sections, we outline the relevant SSL training\nmethodologies used in the present paper. In particular, we focus on\nthe contrastive cross-modal strategy that we adopt for AstroCLIP\nin subsection 2.1. We then provide a general overview of the self-\nsupervised masked modelling strategy that we adopt for spectrum\nembeddinginsubsection2.2andtheself-supervisedself-distillation\nwith no labels strategy that we adopt for image embedding in sub-\nsection 2.3; we provide a more detailed description of both these\napproaches in subsection A1 and subsection A2 respectively. For\na comprehensive review of self-supervised methods, we direct the\nreader to (Balestriero et al. 2023).\n2.1 Cross-Modal Contrastive Techniques\nAvarietyoftechniqueshaveemergedforconnectingrepresentations\nacross modalities4. One such method, Contrastive Languageâ€“Image\nPretraining (CLIP; Radford et al. 2021), has achieved widespread\nsuccessbytrainingneuralnetworkstoalignlanguage-baseddescrip-\ntionsofobjectswiththeircorrespondingimages.CLIPworksbyus-\ninganimageembedderandatextembeddertoembedbothlanguage\nandimagerepresentationsintoasharedembeddingspace.Theseem-\nbeddersaretrainedjointlyunderacontrastiveloss,wherebypositive\npairs (i.e. image-language pairs corresponding to the same object)\narebroughtclosertogetherwhilenegativepairs(i.e.image-language\npairs corresponding to different objects) are pushed apart.\nFormally, letX âˆˆRğ‘ andY âˆˆRğ‘€ be two sets of observations of\nthesameobjectsfromtwodifferentmodalities;inCLIP,thesewould\nbeimagesandtextualdescriptionscorrespondingtothesameobjects.\nThen,thegoalistoconstructapairofencoders, ğ‘“ğœ™ : Rğ‘ â†’Rğ‘‘ and\nğ‘”ğœƒ : Rğ‘€ â†’Rğ‘‘, that compress these two modalities into a shared\nğ‘‘-dimensional space. In particular, we want this embedding space\nto maximize the mutual information between these representations,\nğ¼(ğ‘“ğœ™(x),ğ‘”ğœƒ(y)).\n4 In this context, â€œmodalityâ€ refers to the type of data input, such as im-\nages, textual descriptions, segmentation maps, etc., each requiring different\nprocessing techniques.\nMNRAS000, 1â€“18 (2024)\n4 Liam Parker\nPractically, direct computation of the mutual information is a no-\ntoriously difficult estimation problem for finite data (McAllester &\nStratos 2020; Song & Ermon 2019). Therefore, contrastive meth-\nods like CLIP typically rely on maximizing approximations of the\nmutual information. In this case, CLIP uses an Information Noise-\nContrastiveEstimation(InfoNCE;vandenOordetal.2018;Gutmann\n& HyvÃ¤rinen 2010), a variational bound on the mutual information.\nAlthough InfoNCE is biased, it represents a stable, low variance\nbound on the mutual information that has proven successful in a\nwide variety of contrastive methods (Radford et al. 2021). The In-\nfoNCE loss is given as\nLğ¼ğ‘›ğ‘“ğ‘œğ‘ğ¶ğ¸ (X,Y)=âˆ’1\nğ¾\nğ¾âˆ‘ï¸\nğ‘–=1\nlog exp(ğ‘†ğ¶(xğ‘–,yğ‘–)/ğœ)\nÃğ¾\nğ‘— exp(ğ‘†ğ¶(xğ‘–,yğ‘—)/ğœ)\n. (1)\nHere, ğœ >0 represents a smoothing parameter (sometimes referred\nto as temperature) andğ‘— represent the indices of negative examples\nnot associated with the objectğ‘–.\nAdditionally, a choice of similarity metric,ğ‘†ğ¶, must be specified\nto determine the similarity between representations in the embed-\nding space. In CLIP, the cosine similarity between two points in the\nembedding space is used, such that\nğ‘†ğ¶(xğ‘–,yğ‘—)= (xğ‘–)ğ‘‡yğ‘—\nâˆ¥xğ‘–âˆ¥2\n2 âˆ¥yğ‘—âˆ¥2\n2\n. (2)\nIntuitively,theInfoNCEobjectiveworksbybringingtogetherpoints\nintheembeddingspacethatcorrespondtothesameunderlyingphysi-\ncalobjectandpushingpointsintheembeddingspaceawayfromeach\notheriftheycorrespondtodifferentunderlyingphysicalobjects.Be-\ncause the InfoNCE loss is itself upper-bounded by the number of\nnegativesamples, log(ğ¾âˆ’1),CLIP-stylemodelsaretypicallytrained\nwith large batch sizes of negative pairs, ranging fromğ¾ = 512 to\nğ¾ = 4096, where larger batch sizes typically correlate with better\nperformance (Radford et al. 2021).\nWhile CLIP has proven successful on a variety of cross-modal\nproblems,themethodhasshowntosufferfromsomeinefficienciesin\ntrainingmodelsfromscratch,namelyduetohighcomputationalcosts\nassociatedwiththenecessarylargebatchsizeandtraininginstability\nissues when scaling up. Recently however, Sun et al. (2023) have\nshown that these issues can be partially overcome using a variety\nof techniques, including using pre-trained, single modal models as\ninitializers in the CLIP training.\n2.2 Masked Modelling\nMasked modelling is an SSL technique used to extract powerful\nrepresentations in both NLP (Masked Language Modelling, MLM;\nDevlin et al. 2018) and CV (Masked Image Modelling, MIM; Zhou\net al. 2021) settings. Given an input with random masked patches5,\ntheobjectiveinmaskedmodelingistolearntofillintheserandomly\nmasked patches using the remaining unmasked parts of the input.\nThis forces the model to learn to infer the masked patches from the\nunmasked patches, thereby encouraging robust feature representa-\ntions of the input that capture the structure and content of the input.\nThen, when an unmasked input is fed to the model, the learned pro-\njection of that input should represent a powerful, low-dimensional\nembedding. For a more formal discussion, see subsection A1.\n5 InMLM,thepatchesoftheinputaretypicallycontiguoussegmentsoftext,\nwhile in MIM, the patches of the input are typically square patches of the\nimage.\n2.3 Self-Distillation with No Labels\nSelf-distillationwithNoLabels(DINO;Caronetal.2021)isanother\nSSL technique widely used in CV and NLP. DINO was inspired\nby knowledge distillation (BuciluË‡a et al. 2006), a method which\nforces small student networks to approximate the outputs of large,\npre-trained teacher networks in order to reduce model size. Like\nknowledgedistillation,DINOstillreliesonastudentnetworkmatch-\ning the outputs of a teacher network. However, rather than using a\npre-trained teacher network, DINO instead uses a copy of the stu-\ndent network composed of an iterated average of past iterations of\nthe student networkâ€™s weights. By composing the teacher network\nthis way, the teacher network effectively undergoes an ensembling\ntechnique, enabling it to guide the student network during training\nby providing better representation outputs. Since its inception, this\ntechniquehasbeenintegratedwithmaskedimagemodelinginZhou\net al. (2021), and further improved with (Oquab et al. 2023), which\nhas demonstrated superior performance on a variety of downstream\ntasks including semantic segmentation, image classification, video\nprocessing, etc. For a more detailed treatment of DINO, iBOT, and\nDINOv2, see subsection A2.\n3 AstroCLIP Model Implementation\nThe core of our approach lies in the idea that cross-modal obser-\nvations of a given source can be thought of as filtered, noisy views\nof the same underlying physical process. Therefore, they should in-\ntrinsically possess a shared latent space in which the embeddings of\nthese cross-modal representations can be aligned. To that end, we\npresent a two-step process to train cross-modal galaxy encoders:\n(i) We pre-train two single-modal galaxy encoders separately us-\ning SSL techniques. For galaxy images, we pretrain a vision trans-\nformer(ViT;Dosovitskiyetal.2020)usingacarefullymodifiedver-\nsionoftheDINOv2self-supervisedregime(seesubsubsectionA2.4).\nFor galaxy spectra, we pretrain a 1D transformer encoder using a\nstandard mask-filling strategy (see subsection 2.2).\n(ii) We then train (or â€œfine-tuneâ€) our pre-trained models in a\ncontrastive setting (see subsection 2.1) to align the cross-modal em-\nbeddingsofthesamegalaxiesinasharedembeddingspaceusingthe\nCLIP cross-modal alignment strategy (see subsection 2.1).\nNotably, we opt to pre-train single-modal models separately before\nCLIPalignmentinsteadoftrainingtheentireAstroCLIPmodelfrom\nscratch. For one, the size of the image dataset far exceeds the size of\nthe union between image and spectrum datasets, allowing us to pre-\ntrain our image embedder on roughly two orders of magnitude more\ndata. Additionally, previous studies (Sun et al. 2023) demonstrate\nthat the training instabilities and high computational cost associated\nwithCLIP-styletrainingcanbepartiallymitigatedbyCLIP-aligning\npre-trained models.\nWe provide the details of the galaxy image and spectrum em-\nbedders below. Notably, both models implement transformer-based\narchitectures; we provide extensive background on these in subsec-\ntionA3,anddirectthereadertoVaswanietal.(2017)andDosovitskiy\netal.(2020)foradditionalinformation.Wealsoprovidedetailsonthe\nAstroCLIP model implementation. All training details are provided\nlater in section 5.\n3.1 Galaxy Image Transformer\nOur galaxy image modelâ€™s architecture is a standard vision trans-\nformer (ViT; Dosovitskiy et al. 2020). To prepare a galaxy image,\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 5\nx âˆˆRğ‘Ã—ğ‘ for the ViT architecture, we first patch the image into\nnon-overlapping, contiguous patches of sizeğ‘ƒÃ—ğ‘ƒ. These patches\nare then flattened, to create a sequencexğ‘ âˆˆRğ¾Ã—(ğ‘ƒ2 Â·ğ¶), whereğ¶\nis the number of channels andğ¾ = ğ‘2/ğ‘ƒ2 is the total number of\npatches, which becomes the effective input sequence length for the\ntransformer.\nNext,weprojectthepatchesfromdimension ğ‘ƒ2 Â·ğ¶tosomelatent\ndimension ğ·ğ¼ using a trainable, linear projectionE âˆˆR(ğ‘ƒ2 Â·ğ¶)Ã—ğ·ğ¼.\nAdditionally, we add position embeddings to each of the patch em-\nbeddings; these are standard, learnable 1D vectorsEğ‘ğ‘œğ‘  âˆˆRğ¾Ã—ğ·ğ¼\nthat allow the model to retain positional information for each image\npatch. Finally, we prepend a class tokenxclass to the sequence. This\nclasstokenisalearnableembeddingthatallowsthenetworktoaggre-\ngate global information in the image, and whose final representation\ninthenetworkservesastheglobalimagerepresentation.Altogether,\nthis results in a â€œprocessedâ€ input of\nxâˆ—=[xclass,xğ‘\n1 E,xğ‘\n2 E,..., xğ‘\nğ‘E]+Eğ‘ğ‘œğ‘ . (3)\nOnce this set of embeddings is generated, we pass them to the trans-\nformer model. The transformer consists of a series of Transformer\nblocks (Vaswani et al. 2017), each of which apply multi-head cross\nattentionfollowedbyaseriesofmulti-layerperceptron(MLP;some-\ntimes called â€œfully-connectedâ€) layers and finally a layer norm. A\nfinal layer normalization is applied to the output class and patch to-\nkens. Additionally, we attach a projection head to the class token,\nwhich consists of an additional MLP that projects the latent dimen-\nsionalityoftheViT ğ·ğ¼ tosomedesireddimensionalityoftheoutput.\nWe provide the specific implementation details of the galaxy image\nViT in subsubsection B1.1.\n3.2 Galaxy Spectrum Transformer\nOurgalaxyspectrumtransformerislooselymodeledaftertheGPT-2\nmodel, although it performs masked modeling rather than autore-\ngressiveprediction6 (Radfordetal.2019).Aswiththegalaxyimage\nViT, to prepare a galaxy spectrumy âˆˆRğ‘‡ for the transformer ar-\nchitectures, we first reshape theğ‘‡ dimensional native representation\nof the spectrum to a sequence of shape(ğ‘‡ mod A)Ã— ğµ, where each\nelement of this new sequence is a contiguousğµ-element segment\nof the original sequence, and adjacent elements have an overlap of\nsize ğ´; these new elements now form our patches,y âˆˆRğ¾Ã—ğµ. The\npatchesareonceagainprojectedtosomelatentdimension ğ·ğ‘† using\na trainable, linear project, and position encodings are added and a\nclasstokenprepended,asinEquation3.Oncethissetofembeddings\nisgenerated,wepassthemtothetransformermodel.Weprovidethe\nspecific implementation details of the galaxy spectrum transformer\nin subsubsection B2.1\n3.3 AstroCLIP Model\nThefinalAstroCLIPmodelisasortofcompositionalmodelconsist-\ningofboththeimageandspectrumtransformersoutlinedabove.The\nmodel is constructed using the following steps. First, for any given\nobservationx with a corresponding labelğ‘™ ={â€˜imageâ€™,â€˜spectrumâ€™},\nthe model patchifies the input according to the appropriate strategy\n6 We deviate from GPT-2 in that we initialize all the weights of the trans-\nformer blocks with a normal distribution with standard deviation given by\n(2 Ã—fan-inÃ—num-layers)âˆ’1/2. The dependence of thestandard deviation on\nthenumberoftransformerblocksistocounteracttheeffectofhavingaseries\nof residual connections.\nDataset Number of Galaxies\nDESI-LS after Cuts 76,446,849\nCross-Matched DESI & DESI-LS 197,632\nPROVABGS Properties 105,159\nGalaxy Zoo DECaLS Classifications 222,929\nTable 1.The number of galaxies present in each of our datasets. In par-\nticular, we pre-train our image model on the DESI-LS and our spectrum\nand AstroCLIP model on the cross-matched DESI & DESI-LS. We perform\ndownstream redshift estimation on this same dataset, property prediction on\nthe cross-matched PROVABGS dataset, and morphology classification on\nGalaxy Zoo DECaLS.\noutlined in subsection 3.1 or subsection 3.2. Next, the model pro-\ncessesthepatchifiedinputthroughtheappropriateimageorspectrum\ntransformer,resultinginaprocessedsequenceofvectorswithdimen-\nsionalityequaltotheembeddingdimensionofthetransformer,either\nğ·ğ¼ or ğ·ğ‘†.\nTotransformthese vectorsintoanembeddingspace thatisshared\nbetween the image and spectra inputs, AstroCLIP applies a multi-\nheadcross-attentionbetweenthesefinal-layertokensandalearnable\nquery vector q âˆˆR512. Specifically, the query to this multi-head\nattention isq, while the keys and values are the final-layer tokens\nof either the image or vision transformer (see Equation A9 for more\ndetails). This allows the model to use the attention scores computed\nbetweenq andthetransformer-outputvectorstoselectivelyattendto\nspecific vectors from the transformer output, effectively producing\na weighted average of some linear projection of these vectors. The\noutputofthiscross-attentionisthenasinglevector zâˆ—withthesame\nembedding dimension asq; it does not matter how many key and\nvalue vectors are received, the dimensionality will remain fixed.zâˆ—\nis itself then passed through a series of MLP blocks to producez.\nThe final outputs of the AstroCLIP model,z, are embedding vec-\ntorsofbothgalaxyimagesandspectrathatresideinashared,unified\nlatent space. We provide the specific implementation details of the\ngalaxy spectrum transformer in subsubsection B3.1. The alignment\nof the embedding vector corresponding to a galaxy image,zim, with\nthe embedding vector corresponding to a galaxy spectrum,zğ‘ ğ‘, is\nperformed during CLIP training, detailed further in section 5.\n4 Data\nWe use galaxy spectra from the Dark Energy Spectroscopic Instru-\nment(DESI)andgalaxyimagesfromitscorrespondingLegacyImag-\ningSurvey(DESI-LS).WeusebothDESIandDESI-LSforSSLpre-\ntraining, along with a variety of additional datasets for downstream\ntasks. All of these data are detailed below, and a summary of the\nnumber of galaxies in each dataset is provided in Table 1.\n4.1 Self-Supervised Training Datasets\n4.1.1 DESI-LS Images\nWe use the DESI-LS Data Release 9 from January 2021 as prepared\nbySteinetal.(2021b).Theobservationsinthenortherngalacticcap\n(NGC)werecapturedbytheBeÄ³ing-ArizonaSkySurveyfor ğ‘”andğ‘Ÿ\nbands and the Mayall Legacy Survey forğ‘§bands respectively, while\nthe observations in the southern galactic cap (SGC) were captured\nby the Dark Energy Camera Legacy Survey (DECaLS).\nWe keep every source in the sweep catalogues of the DESI-LS\nthat was not identified as a star and whose magnitude in theğ‘§-band\nMNRAS000, 1â€“18 (2024)\n6 Liam Parker\nis between 20 and 21 is kept. After imposing the magğ‘§ cut-off,\nthis results in a total of76,446,849 galaxies. Many of these galaxy\nimagesincludeoverlappingregionsoftheskyduetothesmallangular\nseparation between galaxies in galaxy clusters.\nThesegalaxiesareimagedinthreeopticalbands (ğ‘”,ğ‘Ÿ,ğ‘§ )atapixel\nscale of0.262 arcsec. The images extracted by Stein et al. (2021b)\naretakenin 256Ã—256 cut-outsandwecroptheseimagesto 144Ã—144\ncenter-cuts as the vast majority of galaxies will cover less area than\nthe total size of the cut-outs. Additionally, we normalize the images\nusing a standard Z-scoring regime, whereby we subtract the mean\nand divide by the standard deviation of the image dataset, ensuring\nthat each pixel value has a mean of 0 and a standard deviation of 1,\nthus standardizing the input data for consistent model training and\nperformance.\n4.1.2 DESI Spectra\nWeusedatafromtheDESIEarlyDataRelease(EDR)(Collaboration\net al. 2023), which consists of spectra observed during the Survey\nValidationcampaign.Thiscampaignwasdividedintothe Target Se-\nlection Validationphase,designedtofinalizetargetselection,andthe\nOne-Percent Survey, a pilot survey of the full program that covered\nroughly140deg2.Sincethedatasetincludessamplesofhighlydiffer-\nent overall amplitudes, in order to make it easier for the network to\nprocess all samples, we Z-score each individual sample. We include\nthe mean (ğœ‡) and standard deviation (ğœ) information by appending\nit to the spectrum sequence.\n4.1.3 Dark Energy Survey Image-Spectra Pairs\nWecross-matchtheDESI-LSgalaxyimagesandDESIspectrausing\nthetargetIDsassociatedwitheachgalaxy.Thisyieldsapairedgalaxy\nimage-spectrasampleof197,632galaxies.Webuildthispairedsam-\npleusingthesamepreprocessingstepsforimagesandspectradetailed\nabove. We split our sample using a 90/10 train-test split for training\nand evaluation.\n4.2 Downstream Datasets\n4.2.1 Photometric Redshift Estimation\nForphotometricredshiftestimation,weusethecatalog-reportedred-\nshifts from the DESI spectra associated with each DECaLS image\nin the cross-matched image-spectrum dataset. We remove spurious\nentries by only selecting entries for which magğ‘”, magğ‘Ÿ, magğ‘§ > 0.\nWe split the catalog using the same split as above.\n4.2.2 PROVABGS Catalog\nFor galaxy property estimation, we use a sample corresponding to\nroughly 1% of the DESI Bright Galaxy Survey. Specifically, we\ncollect estimates of the stellar mass (ğ‘€âˆ—), star formation rate (SFR),\nmass-weightedstellarmetallicity( ğ‘ğ‘€ğ‘Š),andmass-weightedstellar\nage(ğ‘¡ğ‘ğ‘”ğ‘’,ğ‘€ğ‘Š )fromthecomplementaryPRObabilisticValue-Added\nBrightGalaxySurvey(PROVABGS)Catalog(Hahnetal.2023b).In\nparticular, we match our image-spectra pairs with the PROVABGS\nreportedbest-fitoftheabovegalaxypropertiesusingtheDESItarget\nIDs associated with each galaxy. We remove spurious entries in the\nPROVABGScatalogbyonlyselectingentriesforwhich ğ‘€âˆ—>0 and\nmagğ‘”,magğ‘Ÿ,magğ‘§ >0.Thisleaves105,159samples,whichwesplit\nusing the same split as above.\n4.2.3 Galaxy Zoo DECaLS\nForgalaxymorphologyclassification,weuseGalaxyZooDECaLS 7.\nInparticular,weusetheclassificationsfromGZD-5(Walmsleyetal.\n2022b), which includes over 7.5 million volunteer response clas-\nsifications for roughly 314,000 galaxies on a variety of questions,\nincluding morphological T-types, strong bars, arm curvature, etc.\nWecross-matchtheGalaxyZooDECaLSgalaxieswiththeDESI-\nLS.Aftercross-matchingthegalaxydatabases,weremoveanygalaxy\nwith fewer than 3 volunteer classifications, resulting in a1.5% re-\nduction in dataset size. This leaves 222,929 galaxies with associated\nmorphological classifications, which we split using a randomized\n80/20 train-test split.\nFor each galaxy, we use the debiased8 volunteer votes on each\nof the ten questions. We only use a galaxy to train on a question if\n50% or more of the volunteers shown that galaxy were asked that\nquestion. Moreover, we only evaluate on galaxies on which more\nthan34volunteersgaveclassifications,asisconventioninWalmsley\net al. (2022b). To produce a discrete set of classifications for each\nof the questions, we round the highest predicted vote fraction for a\nquestion to 1, and round the remaining fractions to 0.\n5 Model Training\nAs stated above, we train our models using a two-step process; first,\nwe pretrain both image and spectrum transformers in single-modal,\nself-supervised settings on the DESI-LS galaxy images using the\nDINOv2lossandtheDESIgalaxyspectrausingamaskedmodeling\nlossrespectively.Then,wetrainthecompositionalAstroCLIPmodel\non the galaxy image-spectra pairs.\n5.1 Galaxy Image Pre-Training\nWe pretrain the galaxy image transformer on the DESI-LS galaxy\nimages using the DINO v2 self-supervised learning strategy. For\neachinputimage,wefirstcreateaset ğ‘‰oflocalandglobalcrops.We\nuse8localcropsofresolution 602 coveringarandomsquarecut-out\nwith area equal to39.4% of the input image, and 2 global crops of\nresolution 1442 covering a random square cut-out with area equal\nto 94.7% of the input image. The size of the local crops are chosen\nsuchthatsomepartofthetargetgalaxies,whicharealwayscentered,\nis always present in the local crop. The following augmentations are\nalso applied to the various crops:\nâ€¢Rotation/Orientation: We randomly flip both global and local\ncrops across both axes withğ‘=0.5 probability and randomly rotate\nthe images by a random angle sampled betweenU(0,ğœ‹).\nâ€¢Gaussian Blur: We randomly blur each channel of the images\nusing a Gaussian kernel. The Gaussian blur is selected to model\nadditional PSF smoothing, and the size of the blurring kernel is\nparameterized by lognormal fits to the PSF distribution of the data,\nas in Stein et al. (2021b). This is applied withğ‘ = 1.0 to our first\nglobal crop,ğ‘ =0.1 to our second global crop, andğ‘ =0.5 to each\nof our local crops.\n7 https://data.galaxyzoo.org/\n8 DebiasinginGalaxyZooDECaLSincludesbothredshiftdebiasing,which\nmitigates the debiasing from the fact that higher redshfit galaxies appear\nfainter and smalleer on the sky, and volunteer weighting, which discards the\nclassificationsofvolunteerswithareportedartificaterateover0.5andatleast\n150 total classifications. For more details, see Walmsley et al. (2022b).\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 7\nâ€¢Gaussian Noise:WerandomlyaddGaussiannoisetotheimage\nby sampling the noise level from lognormal distributions tuned for\neach filter channel, as in Stein et al. (2021b). As with the Gaussian\nblur,thenoiseisappliedwith ğ‘=1.0 toourfirstglobalcrop, ğ‘=0.1\nto our second global crop, andğ‘=0.5 to each of our local crops.\nNotably, we opt for far fewer augmentations than the original DINO\nv2 method - omitting solarization, color jittering, and random\ngrayscale - in order to minimize the total number of physical cor-\nruptions applied to our data.\nOnce cropped and randomly augmented, we patchify all crops\nin ğ‘‰ using Equation 3. This produces, for each global and local\ncrop, a sequence of patches of length25 and 144 respectively. For\nthe student network, we provide all sequences inğ‘‰, while for the\nteacher network, we provide only the global crops; thus, the student\nis fed 25 Ã—8 +144 Ã—2 = 488 patches, while the teacher is fed\n144 Ã—2 = 288 patches for each image. The self-distillation loss,\nLKD, is then computed as the cross-entropy loss between the class\ntoken of the student network for its given input and the centered\nand sharpened class token of the teacher network for its given input;\nthe equation for this loss is provided in Equation A2. Additionally,\nwe apply a random mask to the global crops inğ‘‰ with a masking\nratio ğ‘Ÿ âˆ¼ U(0.1,0.5). We then feed the unmasked global crops\nto the teacher network and the masked global crops to the student\nnetwork, and compute the masked-modelling iBOT loss,LMIM, as\nin Equation A5. Finally, we compute the KoLeo loss for each batch\nLkoleo, as in Equation A7.\nWe train the galaxy image ViT over the entire DECaLS using the\ncompositeDINOv2lossandtheprocedureoutlinedabove.Theexact\nimplementation details of our training are provided in subsubsec-\ntion B1.2.\n5.2 Galaxy Spectrum Pre-Training\nWe pretrain the galaxy spectrum transformer on the DESI galaxy\nspectra using the Masked-Modelling self-supervised learning strat-\negy. For each input spectrum, we patchify the spectrum into con-\ntiguous, overlapping patches as outlined in subsection 3.2. We then\nrandomly replace 6 contiguous segments of length 30 (equivalent\nto length 600 in the original spectra representation) with zeros and\ntrain the model to minimize the Mean Square Error loss between\nthe predictions and the ground truth of the replaced segments of the\nsequenceusing LMM providedinEquationA1.Theexactimplemen-\ntation details of our training are provided in subsubsection B2.2.\n5.3 AstroCLIP Training\nTo perform our contrastive training step, we remove the projection\nhead of both the pre-trained image and spectrum transformers and\nattachthemulti-headcrossattentiondescribedinsubsection3.3.We\nthenalignbothimageandspectrumtransformersusingtheInfoNCE\nloss (see Equation 1) computed between galaxy images and spectra,\nwherepositivepairsaredefinedasimage-spectrapairscorresponding\nto the same underlying galaxy, and negative pairs are defined as\nimage-spectra pairs corresponding to different underlying galaxies.\nWeusearelativelylargebatchsizeof ğ¾ =1024 image-spectrumpairs\nto increase the number of negative pairs per batch, as is convention\ninCLIP-styleexperimentsincomputerscience(Radfordetal.2021).\nThe exact implementation details of our training are provided in\nsubsubsection B3.2.\n6 Results\nTo demonstrate the capabilities of AstroCLIP, we deploy it across a\nvariety of tasks for which it was neither explicitly trained nor fine-\ntuned. To that end, we embed the galaxy images and spectra in the\nvarious held-out test sets listed above (see section 4) as follows:\nAstroCLIP : (xim,xsp)â†¦â†’( zim,zsp)âˆˆ R512. (4)\nWe normalize both image and spectrum embeddings asÂ¯zim =zim/âˆ¥\nzim âˆ¥2 and Â¯zsp = zsp/âˆ¥zsp âˆ¥2. This produces a set of normalized\ngalaxy embeddings in a shared, cross-modal latent space which can\neasily be queried, searched, and used as summary statistics for the\nensuing downstream tasks.\n6.1 Example Retrieval by Similarity Search\nWe perform example retrieval using semantic similarity search.\nSpecifically, for some query galaxy, we use its normalized vector\nembedding to search over all galaxies in the held-out test database.\nThis search is performed using the cosine-similarity (normalized\nscalar product, see Equation 2) between the embedded query galaxy\nÂ¯zğ‘ and all of the other galaxy embeddings in the test database.\nUnlikepreviousSSLmethodsinastronomy,AstroCLIPâ€™ssimilar-\nity search is not constrained to a single modality. Instead, because\ntheembeddingspaceproducedbyAstroCLIPissharedbetweenboth\nimagesandspectra,boththeimageandspectrumofanyquerygalaxy\ncan be used to search among all galaxies in the embedded dataset.\nFor example, if we wish to search for galaxy images matching a\ngiven query spectrumxsp\nğ‘– , we simply calculate the cosine similarity\nbetween the query spectrum embeddingÂ¯zsp\nğ‘– and the image embed-\ndings in the held-out test set,Â¯zim\nğ‘— , and return the target images with\nthe greatest values; no additional transformations or alterations are\nneeded.\nWepresentsomeexamplesusingthismethodforboth in-modality\nsimilarity search - where we determine the neighbors accord-\ning to the cosine similarity between same-modalitiy embeddings\n(i.e. ğ‘†ğ¶(zsp\nğ‘,zsp)or ğ‘†ğ¶(zimğ‘ ,zim)) - andcross-modality similar-\nity search - where we determine neighbors according to the co-\nsine similarity between cross-modal embeddings (i.e.ğ‘†ğ¶(zimğ‘ ,zsp)\norğ‘†ğ¶(zsp\nğ‘,zim)).Wepresenttheimagesofthefourâ€œclosestâ€galaxies\nfor a randomly selected query galaxy for all four possible pairs of\nmodalities in Figure 2 (a-e). We also present the spectra of the four\nâ€œclosestâ€ galaxies for a red quiescent galaxy and a blue star forming\ngalaxy for all four possible pairs of modalities in Figure 2 (f-i). By\nconstruction, the closest match for an in-modal similarity search is\nthe query itself. Ultimately, this sort of capability is especially im-\nportantwhensearchingforrareorinterestingobjects,asexemplified\nby Stein et al. (2021b) paper.\n6.2 Redshift Estimation\n6.2.1 Photometric Redshift Estimation\nWe evaluate AstroCLIPâ€™s performance on photometric redshift esti-\nmation. Previous studies have demonstrated that there exists signif-\nicantly more redshift information in galaxy images than that which\nwould be extractable with simple photometry alone (Pasquet et al.\n2019). As such, current machine learning methods rely on training\ndedicated, convolutional neural networks to solve this type of prob-\nlem, a task which typically involves developing an entire pipeline\nfrom scratch and training a dedicated model end-to-end. Because\nthe learned vector embeddings produced by AstroCLIP are already\nMNRAS 000, 1â€“18 (2024)\n8 Liam Parker\n(a)zğ‘\n (b) ğ‘†ğ¶(zimğ‘ ,zim)\n (c) ğ‘†ğ¶(zsp\nğ‘,zsp)\n (d) ğ‘†ğ¶(zsp\nğ‘,zim)\n (e) ğ‘†ğ¶(zimğ‘ ,zsp)\n(f) ğ‘†ğ¶(zimğ‘ ,zim)\n (g) ğ‘†ğ¶(zsp\nğ‘,zsp)\n(h) ğ‘†ğ¶(zsp\nğ‘,zim)\n (i) ğ‘†ğ¶(zimğ‘ ,zsp)\nFigure2. Exampleretrievalfrombothin-modalityandcross-modalitysearchesintheAstroCLIP-alignedembeddingspace.Inparticular,foragivenquerygalaxy\nxğ‘, we embed that galaxy using AstroCLIP aszğ‘ =AstroCLIP(xğ‘)and find the nearest neighbors of that galaxy using the cosine similarity,ğ‘†ğ¶(zğ‘,zğ‘–â‰ ğ‘),\nbetween the query embedding and the embeddings of other galaxies in the test set.Top: From left to right, we first show the images of (a) the randomly\nselected set of query galaxies, and then show the images corresponding to the closest galaxy embeddings using (b) spectrum-spectrum search, (c) image-image\nsearch, (d) spectrum-image search, and (e) image-spectrum search. Note that superscripts indicate the input modality.Bottom: We show the retrieved spectra\nof galaxies nearest to the query galaxy, pictured in each graph, using (f) image-image search, (g) spectrum-spectrum search (h) spectrum-image search, and (i)\nimage-spectrum search. We note that for in-modality searches, the closest neighbor to the query galaxy is by design the query galaxy itself.\ninformative about the input galaxies, we are instead able to use sim-\nple clustering algorithms (zero-shot) or MLP (few-shot) to extract\nphotometric redshift. Specifically, for zero-shot training, we apply\nğ‘˜-Nearest Neighbor (ğ‘˜-NN) to regress the catalog-reported redshift\nofagalaxyfromAstroCLIPâ€™sembeddingofthatgalaxyâ€™simage.For\nfew-shot training, we train a single-hidden-layer MLP with width\nğ‘¤ = 32 to perform the same regression. We include for compari-\nson the zero- and few-shot results of our unaligned galaxy image\ntransformermodel(DINO)aswellasthoseofthesingle-modalSSL\ngalaxy image model from Stein et al. (2021b). We also include two\nsupervised baselines: a ResNet18 (He et al. 2016) trained end-to-\nend on the galaxy images (see subsection B4) and an MLP trained\nend-to-end on the galaxy(ğ‘”,ğ‘Ÿ,ğ‘§ )photomtetry.\nWe report our results in Figure 3. In panel (c), we verify that our\nsupervised ResNet18 baseline is indeed able to extract more infor-\nmation than the photometry alone. Overall, AstroCLIP outperforms\nall models, including the ResNet18 in both zero- and few-shot set-\ntings. The strong zero-shot performance of the AstroCLIP model\nindicates that the galaxy image embeddings are naturally organized\nin the latent embedding space around galaxy redshift. Contrasting\nthiswiththerelativelyworsezero-shotperformanceoftheunaligned\nimage transformer model, it is clear that the CLIP alignment of the\nimages with the spectra has naturally organized the vector embed-\ndings around galaxy redshift. Given that the spectra are effectively\nperfectly informative about galaxy redshift, this is to be expected.\nEitherway,bothunalignedandCLIP-alignedmodelsoutperformthe\nStein et al. (2021b) image embedder, indicating that they are both\nbetterorganizedaround(zero-shot)andmoreinformative(few-shot)\nabout galaxy redshift.\n6.2.2 Redshift Estimation From Galaxy Spectra\nA galaxyâ€™s spectrum should contain near-perfect information on the\nredshift of that galaxy. This information is accessible with least-\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 9\n(a) Self-Supervised Zero-Shot Performance.\n(b) Self-Supervised Few-Shot Performance.\n(c) Supervised Baseline Performance.\nFigure3. Galaxyimageredshiftpredictionandresiduals.Forzero-shot,weuseasimple ğ‘˜-NNclusteringalgorithmontheAstroCLIPgalaxyimageembeddings\ntopredictgalaxyredshift.Forfew-shot,weuseasimpleMLPtoperformthesameregressiontask.Weincludeforcomparisonthefew-andzero-shotperformance\nofourunalignedgalaxyimagemodel(DINO)andastate-of-the-artself-supervisedmodelforgalaxyimages(Steinetal.2021b).Wealsoincludetwodedicated,\nsupervised, end-to-end models trained on galaxy images (ResNet18) and galaxy photometry (MLP). AstroCLIP performs better than its dedicated, supervised\ncounterpart, despite undergoing no task-specific training or finetuning.\nsquare fitting algorithms likeRedrock9, which is used to generate\ntheDESIEDR-reportedgalaxyredshifts.However,giventhatgalaxy\nimagesarenotperfectlyinformativeaboutgalaxyredshift,onewould\nexpect that the AstroCLIP spectrum embeddings should no longer\ncontain perfect redshift information after CLIP alignment. Afterall,\nunder a pessimistic interpretation of the InfoNCE loss (see Equa-\ntion 1), the AstroCLIP model should only be incentivized to keep\ngalaxyredshiftinformationthatissharedbetweengalaxyimagesand\nspectra. Therefore, there is no reason that it should not discard the\nredshift information in the galaxy spectra that is not in the galaxy\nimages. Surprisingly, however, this does not seem to be the case.\nIndeed, in evaluating the few-shot performance of the AstroCLIP\nspectrum embeddings in Figure 4, we find that there is no mate-\nrial loss of information after CLIP-alignment with the images. This\n9 https://redrock.readthedocs.io/en/latest\nis encouraging, as it means that cross-modal alignment, even with\nmodalitiesthatarenotperfectlyinformativeaboutthetheunderlying\nphysical process, can still be a good training strategy to generate a\nmodel that keeps information. We also compare our results with a\nConvolutional-Attention Network based on a state-of-the-art spec-\ntrumencoder(Melchioretal.2023)trainedend-to-endonthegalaxy\nspectra (see subsection B5), and find that it is in agreement with the\nAstroCLIP and unaligned SSL results as well.\n6.3 Galaxy Property Estimation\nWeevaluateAstroCLIPâ€™sperformanceongalaxypropertyestimation\nusingbothgalaxyimagesandspectraasinputs.Asabove,thesetasks\naretypicallyperformedbydedicated,end-to-endsupervisedmodels,\nwhereashereweareabletousesimplezero-andfew-shotlearningon\nthe AstroCLIP embeddings. In particular, we evaluate AstroCLIPâ€™s\nMNRAS000, 1â€“18 (2024)\n10 Liam Parker\nFigure 4.Galaxy spectrum redshift few-shot prediction and residuals. We\nuse a simple MLP trained end-to-end on the AstroCLIP galaxy spectrum\nembeddings to predict galaxy redshift. Surprisingly, AstroCLIP retains near-\nperfect redshift information in the spectrum embeddings even after CLIP-\nalignment with galaxy images. Operating under a pessimistic interpretation\nof the InfoNCE loss, one would expect CLIP-alignment to only retain the\nredshiftinformationthatissharedbybothspectraandimages.Wealsoinclude\nasupervisedConvolutional-AttentionNetworktrainedend-to-endforredshift\nprediction on galaxy spectra\nzero- and few-shot performance in estimating the following galaxy\nproperties from the cross-matched PROVABGS (Hahn et al. 2023a)\ncatalog detailed in subsubsection 4.2.2:\nâ€¢Mâˆ—: Stellar Mass\nâ€¢ZMW: Mass-Weighted Stellar Metallicity\nâ€¢tage: Mass-Weighted Galaxy Age (Gyr)\nâ€¢sSFR:SpecificStar-FormationRate,i.e.starformationactivity\nrelative to its stellar mass (ğ‘†ğ¹ğ‘…/ğ‘€âˆ—)\nFor zero-shot training, we use ğ‘˜-NN to regress\n[log ğ‘€âˆ—,log ğ‘ğ‘€ğ‘Š,ğ‘¡ğ‘ğ‘”ğ‘’,log ğ‘ ğ‘†ğ¹ğ‘…]; for few-shot training, we\nuse a single-hidden-layer MLP with widthğ‘¤ = 32 to perform the\nsame regression.\nAs in subsection 6.2, we include for comparison the zero- and\nfew-shot results of our unaligned galaxy image (DINOv2) model as\nwellasthoseofthegalaxyimagemodelfromSteinetal.(2021b).We\nalso include the zero- and few-shot results of our unaligned galaxy\nspectrum transformer. Finally, we include three dedicated baselines:\naResNet18(Heetal.2016)trainedend-to-endonthegalaxyimages\n(see subsection B4), a Convolutional-Attention Network based on\na state-of-the-art spectrum encoder (Melchior et al. 2023) trained\nend-to-end on the galaxy spectra (see subsection B5), and an MLP\ntrained end-to-end on the galaxy(ğ‘”,ğ‘Ÿ,ğ‘§ )photometry.\nWe report our results in Table 2. Again, AstroCLIP demonstrates\nan ability to capture in its galaxy embeddings core physical proper-\nties of the input galaxy despite undergoing no task-specific training\nor fine-tuning. For galaxy images, AstroCLIP outperforms all given\nbaselines, including previous SSL models (Stein et al. 2021b) and\nthesupervisedimage(ResNet18)andphotometry(MLP)models.For\ngalaxy spectra, AstroCLIP outperforms the supervised photometry\nbaselineonalltasks,andoutperformsthesupervisedspectrumbase-\nlineon ğ‘€âˆ—,butperformsworseon ğ‘ğ‘€ğ‘Š andğ‘ ğ‘†ğ¹ğ‘….Asabove,CLIP-\nalignment between a less informative (image) and more informative\n(spectrum) embedding has improved the zero-shot performance of\nAstroCLIP on galaxy images. However, unlike with redshift estima-\ntion, AstroCLIPâ€™s performance on spectra has deteriorated relative\nto its unaligned spectrum transformer model.\nTable 2. Galaxy property estimationğ‘…2 performance. We present As-\ntroCLIPâ€™s zero- and few-shot performance in regressing stellar mass (ğ‘€âˆ—),\nmetallicity(ğ‘ğ‘€ğ‘Š),age( ğ‘¡ğ‘ğ‘”ğ‘’),andspecific-starformationrate( ğ‘ ğ‘†ğ¹ğ‘…)from\ngalaxy images and spectra. For zero-shot, we useğ‘˜-NN and for few-shot we\nuse a single hidden layer MLP with widthğ‘¤ =32. We include for compar-\nison the zero- and few-shot performance of our unaligned galaxy image and\nspectrum SSL transforms (unaligned trans.) and of the SSL galaxy image\nmodel from (Stein et al. 2021b). We also include three dedicated, supervised\nbaselines trained on galaxy images (ResNet18), galaxy spectra (Conv+Att)\nandgalaxyphotometry(MLP).Ourmodelsareindicatedwithanasterisk( âˆ—).\nAstroCLIPoutperformsitsdedicated,supervisedcounterpartsonmosttasks,\ndespite undergoing no task-specific training or finetuning.\nSource Method ğ‘€âˆ— ğ‘ğ‘€ğ‘Š ğ‘¡ğ‘ğ‘”ğ‘’ ğ‘ ğ‘†ğ¹ğ‘…\nImages AstroCLIP\nZero-Shotâˆ— 0.74 0.44 0.27 0.44\nFew-Shotâˆ— 0.73 0.43 0.26 0.42\nUnaligned Trans.\nZero-Shotâˆ— 0.65 0.40 0.16 0.25\nFew-Shotâˆ— 0.72 0.43 0.23 0.40\nStein et al. (2021b)\nZero-Shot 0.43 0.30 0.11 0.20\nFew-Shot 0.48 0.32 0.14 0.24\nResNet18 0.72 0.44 0.23 0.32\nSpectra AstroCLIP\nZero-Shotâˆ— 0.87 0.57 0.43 0.63\nFew-Shotâˆ— 0.88 0.58 0.43 0.64\nUnaligned Trans.\nZero-Shotâˆ— 0.84 0.57 0.38 0.62\nFew-Shotâˆ— 0.88 0.64 0.47 0.69\nConv+Att 0.85 0.62 0.43 0.67\nPhotometry MLP 0.67 0.41 0.27 0.34\n6.4 Neural Posterior Estimation\nWe now perform the same set of redshift estimation/property pre-\ndiction tasks using Neural Posterior Estimation (NPE;e.g. Rezende\n&Mohamed2015;Dinhetal.2016;Papamakarios&Murray2016;\nLueckmann et al. 2017; Greenberg et al. 2019). This enables us to\nbetter understand the information content in the AstroCLIP galaxy\nembeddings.\nSpecifically, letr represent the redshift and property vector for a\ngivengalaxy.Weareinterestedinestimatingtheposteriorof r given\nthe AstroCLIP embedding of that galaxy,z. To that end, we train\nan ensemble of normalizing flows,ğ‘ğœ™, to estimate the conditional\ndistribution ğ‘ğœ™(r|z)â‰ˆ ğ‘(r|z)over the PROVABGS training set. We\nprovide the relevant background on normalizing flows in subsec-\ntion A4 and the details of our implementation in subsection B6.\nOnce trained, ifğ‘ğœ™ represents a good estimate forğ‘(r|z), we can\nuseittoefficientlycalculatetheposteriorof rğ‘–givenzğ‘–forsometarget\ngalaxy ğ‘–. If this distribution is concentrated around the true value,\nthenzğ‘–isveryinformativeof rğ‘–,whileifthisdistributionisrelatively\nflataroundthetruevalue,itislessinformative.Repeatingthisprocess\nover the held-out test dataset therefore provides a strong indication\nof the information content in the AstroCLIP galaxy embeddings.\nTypically,thisisperformedbycalculatingthenegativelog-likelihood\n(NLL) over the test set as:\nNLL = 1\nğ‘\nğ¾âˆ‘ï¸\nğ‘–=1\nlog ğ‘ğœ™(rğ‘–|]zğ‘–) (5)\nWe present the AstroCLIP and baseline NLLs in Table 3. Ul-\ntimately, these results corroborate the results presented in subsec-\ntion 6.3. Specifically, the AstroCLIP image embeddings once again\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 11\nTable 3.Average normalizing flow estimate of the negative log-likelihood\n(NLL) of the true redshift and galaxy property vector,r, given the input\nembedding z. In particular, we train an ensemble of normalizing flowsğ‘ğœ™\nto estimate the conditional distributionğ‘ğœ™(r|z)â‰ˆ ğ‘(r|z). We then use our\nnormalizing flow to calculate the log-likelihood of the truer given the input\nembedding z. AstroCLIP outperforms its dedicated, supervised counterparts\nonmosttasks,despiteundergoingnotask-specifictrainingorfinetuning.Note\nthat lower numbers are better.\nSource Method Neg. Log-Likelihood\nImages AstroCLIP âˆ— 0.76 Â±0.00\nUnaligned Transformerâˆ— 0.81 Â±0.01\nStein et al. (2021b) 1.09 Â±0.04\nResNet18 0.77 Â±0.00\nSpectra AstroCLIP âˆ— 0.14 Â±0.03\nUnaligned Transformerâˆ— 0.00 Â±0.04\nConv+Att 0.26 Â±0.00\nPhotometry MLP 0.92 Â±0.05\noutperforms both image and photometry supervised baselines, as\nwell as the Stein et al. (2021b) SSL model. Interestingly, the Astro-\nCLIPspectrumembeddingsalsooutperformthededicatedspectrum\nbaseline.\nIn addition to providing a concrete measure of the information\ncontent, ğ‘ğœ™ also enables us to efficiently sample fromğ‘(r|z). We\npresent sample distributions for a randomly chosen galaxy, along\nwith the true redshift/properties of that galaxy, in Figure D3a and\nFigure D3b respectively. We also verify thatğ‘ğœ™ is well-calibrated in\nsection E.\n6.5 Galaxy Morphology Classification\nFinally, we evaluate AstroCLIPâ€™s performance on galaxy morphol-\nogyclassification.Asabove,weevaluatethemodelâ€™sfew-shotperfor-\nmance by training a 4-layer MLP with widthğ‘¤ =128 to regress the\nGalaxyZooDECaLSmorphologyclassificationfromtheAstroCLIP\nimage embeddings over the training set. We report the performance\nof our model on the held-out test set, where we only include per-\nformance on galaxies on which more than 34 volunteers provided\nclassifications; this ensures that each answer is well-measured, and\nisconventioninthesupervisedworksfromWalmsleyetal.(2022b).\nWe report the accuracy and F1 score for each question over the\ntest set, where we weight the accuracy and F1 score by the support\nfor each class; importantly, not every class has binary classifica-\ntions, as some classes - like spiral arm count - have up to 6 classes.\nWe include for comparison the few-shot results of our unaligned\ngalaxyimage(DINOv2)model,aswellasthoseofthegalaxyimage\nmodel from Stein et al. (2021b). Additionally, we include the re-\nported accuracy/F1-score of the supervised Bayesian classifier from\nWalmsley et al. (2022b).\nWe present our results in Figure 5. We donâ€™t expect any classifier\nto be able to achieve perfect accuracy on the given tasks, as the\nvolunteerlabelsthemselvespossesssomeintrinsicuncertaintyabout\nthe underlying galaxy. Therefore, we take the supervised Bayesian\nclassifierastheupper-boundontheachievableaccuracyforF1-score\nof a data-driven model in this particular classification task. We also\nprovidetheraw,numericalresultsforthevariousmodelsinTableE1.\nOverall, AstroCLIP achieves relatively strong performance on all\nquestions. Raw accuracy score ranges from 97% on disk-edge-on to\n56% on bar. Overall, AstroCLIPâ€™s performance is at least 90% of\nthat of the supervised model on 6/10 of the questions (disk-edge-on,\nspiral-arms,bulge-size,edge-on-bulge,spiral-winding,merging).Fi-\nnally, CLIP-alignment between images and spectra - in this case the\nless informative modality - has not materially degraded model per-\nformanceonimages;theaverageaccuracyoftheunalignedDINOv2\nmodel(78%)isroughlyin-linewiththatofAstroCLIP(77%),while\nAstroCLIPâ€™s performance is even slightly better on the disk-edge-on\nand bar questions.\n7 Conclusions\nWe have presented AstroCLIP, a cross-modal foundation model for\ngalaxies. Our results demonstrate the potential for cross-modal con-\ntrastive pre-training to achieve high quality foundation models for\nastronomical data, which can be used for a variety of downstream\ntasks. These include accurate in-modal and cross-modal semantic\nsimilarity search, photometric redshift estimation, galaxy property\nprediction from both images and spectra, and galaxy morphology\nclassification.\nReinforcing our optimism for this approach, our results demon-\nstrate thateven if diverse modalitiesare not allperfectly informative\naboutdownstreamtasks,thecontrastivelearningobjectiveisstillable\ntopreservemodality-specificinformationthatexceedsthatcontained\nin other modalities. This is exemplified by the fact that our spectral\nembeddings exhibit an emergent ability to retain most of their red-\nshift information while our image embeddings exhibit an emergent\nability to retain most of their galaxy morphology information.\nUltimately, we contend that the modelâ€™s high performance on a\nwide variety of downstream tasks and its ability to retain modality-\nspecific information are key properties to allow the community to\nbuildhigher-levelmodelsthatrelyonoff-the-shelfastronomicalem-\nbeddings, just as CLIP language-image embeddings have enabled a\nwide variety of downstream applications in computer vision.\n8 Acknowledgements\nWe gratefully acknowledge the Flatiron Institute for its support. The\ncomputations in this work were run at facilities supported by the\nScientific Computing Core at the Flatiron Institute, a division of the\nSimonsFoundation.M.P.issupportedbytheDepartmentofEnergy,\nOffice of Science under contract number DE-AC02-05CH11231.\nThis work was granted access to the HPC/AI resources of IDRIS\nunder the allocation 2023-A0151014662 made by GENCI.\n9 Data Availability\nAll the code and resources for this paper will be made available\nupon acceptance. This includes the code that defines the model, the\ntraining codes, and the code used to generate the various results.\nAdditionally, we will publish the trained model which can be used\nto further evaluate performance or perform additional downstream\ntasks. The underlying data are all publicly available and all of the\nsoftware used in this work is open source.\nREFERENCES\nBahdanau D., Cho K., Bengio Y., 2014, arXiv preprint arXiv:1409.0473\nBalestriero R., et al., 2023, arXiv preprint arXiv:2304.12210\nBommasani R., et al., 2021, arXiv preprint arXiv:2108.07258\nMNRAS000, 1â€“18 (2024)\n12 Liam Parker\n(a) Accuracy.\n (b) F1 Score.\nFigure 5.Galaxy morphology classification results. We train a simple MLP on the AstroCLIP galaxy image embeddings to predict the Galaxy Zoo DECaLS\nGZD-5 morphology classification of that galaxy. We report both the class-weighted accuracy and F1-score of the various models. We also provide the reported\nclass-weightedaccuracy/F1-scoreofthesupervisedBayesianclassifier(Walmsleyetal.2022b)foreachquestion.Overall,AstroCLIPachievesrelativelystrong\nperformanceonallquestions,andclearlyoutperformsastate-of-the-artself-supervisedmodelforgalaxyimages(Steinetal.2021b).Additionally,asinFigure4,\ncross-alignment of two different modalities has not materially degraded the performance of the more informative modality, as the difference between the\nunaligned transformer image model (DINOv2) and AstroCLIP model is negligible.\nBuciluË‡aC.,CaruanaR.,Niculescu-MizilA.,2006,inProceedingsofthe12th\nACM SIGKDD international conference on Knowledge discovery and\ndata mining. pp 535â€“541\nCaron M., Bojanowski P., Joulin A., Douze M., 2018, in Proceedings of the\nEuropean conference on computer vision (ECCV). pp 132â€“149\nCaron M., Touvron H., Misra I., JÃ©gou H., Mairal J., Bojanowski P., Joulin\nA., 2021, in Proceedings of the IEEE/CVF international conference on\ncomputer vision. pp 9650â€“9660\nChenT.,KornblithS.,NorouziM.,HintonG.,2020,ASimpleFrameworkfor\nContrastive Learning of Visual Representations (arXiv:2002.05709)\nCheng T.-Y., Li N., Conselice C. J., AragÃ³n-Salamanca A., Dye S., Metcalf\nR. B., 2020, Monthly Notices of the Royal Astronomical Society, 494,\n3750\nCollaboration D., et al., 2023, arXiv preprint arXiv:2306.06308\nDevlin J., Chang M.-W., Lee K., Toutanova K., 2018, arXiv preprint\narXiv:1810.04805\nDey A., et al., 2019, arXiv preprint arXiv:1804.08657\nDinhL.,Sohl-DicksteinJ.,BengioS.,2016,arXivpreprintarXiv:1605.08803\nDosovitskiy A., et al., 2020, arXiv preprint arXiv:2010.11929\nDurkan C., Bekasov A., Murray I., Papamakarios G., 2019, Advances in\nneural information processing systems, 32\nFang Z., Wang J., Wang L., Zhang L., Yang Y., Liu Z., 2021, arXiv preprint\narXiv:2101.04731\nGirdharR.,El-NoubyA.,LiuZ.,SinghM.,AlwalaK.V.,JoulinA.,MisraI.,\n2023, in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp 15180â€“15190\nGreenberg D., Nonnenmacher M., Macke J., 2019, in International Confer-\nence on Machine Learning. pp 2404â€“2414\nGrill J.-B., et al., 2020, Advances in neural information processing systems,\n33, 21271\nGutmann M., HyvÃ¤rinen A., 2010, in Teh Y. W., Titterington M., eds,\nProceedings of Machine Learning Research Vol. 9, Proceedings of\nthe Thirteenth International Conference on Artificial Intelligence and\nStatistics. PMLR, Chia Laguna Resort, Sardinia, Italy, pp 297â€“304,\nhttps://proceedings.mlr.press/v9/gutmann10a.html\nHahn C., et al., 2023a, The Astronomical Journal, 165, 253\nHahn C., et al., 2023b, The American Astronomical Society, 945, 16\nHayatM.A.,SteinG.,HarringtonP.Z.,Lukiâ€™cZ.,MustafaM.A.,2020,The\nAstrophysical Journal Letters, 911\nHayatM.A.,HarringtonP.Z.,SteinG.,Lukiâ€™cZ.,MustafaM.,2021,ArXiv,\nabs/2101.04293\nHeK.,ZhangX.,RenS.,SunJ.,2016,inProceedingsoftheIEEEconference\non computer vision and pattern recognition. pp 770â€“778\nHe K., Fan H., Wu Y., Xie S., Girshick R., 2020, in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp\n9729â€“9738\nHe K., Chen X., Xie S., Li Y., DollÃ¡r P., Girshick R., 2021, Masked Autoen-\ncoders Are Scalable Vision Learners (arXiv:2111.06377)\nHuertas-Company M., Lanusse F., 2023, PASA, 40, e001\nIveziÄ‡ et. al. Å½., 2019, The Astrophysical Journal, 873, 111\nIveziÄ‡ Å½., Connolly A. J., VanderPlas J. T., Gray A., 2020, Statistics, data\nmining,andmachinelearninginastronomy:apracticalPythonguidefor\nthe analysis of survey data. Princeton University Press\nLaureÄ³s R., et al., 2011, arXiv preprint arXiv:1110.3193\nLemos P., Coogan A., Hezaveh Y., Perreault-Levasseur L., 2023, arXiv\npreprint arXiv:2302.03026\nLiang Y., Melchior P., Lu S., Goulding A., Ward C., 2023a, arXiv preprint\narXiv:2302.02496\nLiangY.,MelchiorP.,HahnC.,ShenJ.,GouldingA.,WardC.,2023b,arXiv\npreprint arXiv:2307.07664\nLueckmann J.-M., Goncalves P. J., Bassetto G., Ã–cal K., Nonnenmacher M.,\nMacke J. H., 2017, Advances in neural information processing systems,\n30\nMargalef-Bentabol B., Huertas-Company M., Charnock T., Margalef-\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 13\nBentabol C., Bernardi M., Dubois Y., Storey-Fisher K., Zanisi L., 2020,\nMonthly Notices of the Royal Astronomical Society, 496, 2346\nMcAllester D., Stratos K., 2020, in International Conference on Artificial\nIntelligence and Statistics. pp 875â€“884\nMcCabe M., et al., 2023, arXiv preprint arXiv:2310.02994\nMelchiorP.,LiangY.,HahnC.,GouldingA.,2023,TheAstronomicalJournal,\n166, 74\nNguyen T., Brandstetter J., Kapoor A., Gupta J. K., Grover A., 2023, arXiv\npreprint arXiv:2301.10343\nOquab M., et al., 2023, arXiv preprint arXiv:2304.07193\nPapamakariosG.,MurrayI.,2016,Advancesinneuralinformationprocessing\nsystems, 29\nPasquet J., Bertin E., Treyer M., Arnouts S., Fouchez D., 2019, Astronomy\n& Astrophysics, 621, A26\nPortillo S. K., Parejko J. K., Vergara J. R., Connolly A. J., 2020, The Astro-\nnomical Journal, 160, 45\nRadford A., Wu J., Child R., Luan D., Amodei D., Sutskever I., et al., 2019,\nOpenAI blog, 1, 9\nRadford A., et al., 2021, in International Conference on Machine Learning.\nhttps://api.semanticscholar.org/CorpusID:231591445\nRezende D., Mohamed S., 2015, in International conference on machine\nlearning. pp 1530â€“1538\nRuppert D., 1988, Technical report, Efficient estimations from a slowly con-\nvergentRobbins-Monroprocess.CornellUniversityOperationsResearch\nand Industrial Engineering\nSablayrolles A., Douze M., Schmid C., JÃ©gou H., 2018, arXiv preprint\narXiv:1806.03198\nSerra J., Pascual S., Karatzoglou A., 2018, in CCIA. pp 120â€“129\nSong J., Ermon S., 2019, arXiv preprint arXiv:1910.06222\nStein G., Blaum J., Harrington P. Z., Medan T., Lukic Z., 2021a, The Astro-\nphysical Journal, 932\nStein G., Harrington P. Z., Blaum J., Medan T., Lukic Z., 2021b, ArXiv,\nabs/2110.13151\nSubramanianS.,HarringtonP.,KeutzerK.,BhimjiW.,MorozovD.,Mahoney\nM. W., Gholami A., 2024, Advances in Neural Information Processing\nSystems, 36\nSun Q., Fang Y., Wu L., Wang X., Cao Y., 2023, arXiv preprint\narXiv:2303.15389\nTeimoorinia H., Archinuk F., Woo J., Shishehchi S., Bluck A. F., 2022, The\nAstronomical Journal, 163, 71\nTongZ.,SongY.,WangJ.,WangL.,2022,inOhA.H.,AgarwalA.,Belgrave\nD., Cho K., eds, Advances in Neural Information Processing Systems.\nhttps://openreview.net/forum?id=AhccnBXSne\nVaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A. N.,\nKaiserÅ.,PolosukhinI.,2017,Advancesinneuralinformationprocessing\nsystems, 30\nWalmsley M., SlÄ³epcevic I. V., Bowles M., Scaife A. M., 2022a, arXiv\npreprint arXiv:2206.11927\nWalmsley M., et al., 2022b, Monthly Notices of the Royal Astronomical\nSociety, 509, 3966\nZhai X., Kolesnikov A., Houlsby N., Beyer L., 2022, in Proceedings of the\nIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp\n12104â€“12113\nZhou J., Wei C., Wang H., Shen W., Xie C., Yuille A., Kong T., 2021, arXiv\npreprint arXiv:2111.07832\nvan den Oord A., Li Y., Vinyals O., 2018, CoRR, abs/1807.03748\nA Relevant Background\nA1 Masked Modelling\nAs stated above, given an input with random masked patches, the\nobjective in masked modeling is to learn to fill in these randomly\nmasked patches using the remaining unmasked parts of the input.\nFormally, let us consider an inputxcomposed of a set ofğ‘patches,\n{xğ‘–}ğ‘\nğ‘–=1.Then,werandomlymaskasubsetofthesepatchesaccording\ntoapredictionratio ğ‘Ÿtoproduce Ë†x ={Ë†xğ‘–|(1âˆ’ğ‘šğ‘–)xğ‘–+ğ‘šğ‘–e[MASK]}ğ‘\nğ‘–=1,\nwhere e[MASK] represents the value of the masked patch andm âˆˆ\n{0,1}ğ‘ represents the random mask. Letğ‘”ğœƒ be our neural network;\nthen,theprojectionsofeachunmaskedpatch ğ‘–isxğ‘– =ğ‘”ğœƒ(x)ğ‘–andthe\nprojections for each masked patchğ‘–is Ë†xğ‘– =ğ‘”ğœƒ(Ë†x)ğ‘–. The objective in\nmaskedmodelingisthentominimizethemean-squarederror(MSE)\nloss between thexğ‘– and Ë†xğ‘– for the sameğ‘–, given as\nLMM = 1\nğ‘ğ¾\nğ¾âˆ‘ï¸\nğ‘—=1\nğ‘âˆ‘ï¸\nğ‘–=1\nmğ‘– Â·(xğ‘– âˆ’Ë†xğ‘–)2, (A1)\nwhereğ‘–iteratesoverallofthepatchesinagiveninput xand ğ‘—iterates\noverallofthe ğ¾inputsinthetrainingdataset.Thisforcesthemodelto\nlearntoinferthemaskedpatchesfromtheunmaskedpatches,thereby\nencouraging the model to learn robust feature representations of the\ninput that capture the structure and content of the input. Then, when\nan unmasked input is fed to the model, the learned projection of that\ninput should represent a powerful, low-dimensional embedding.\nA2 Self-Distillation with No Labels\nAs stated above, self-distillation with no labels relies on extracting\nmeaningful embeddings by exploiting the dynamics between the\ntraining interplay of a â€œteacherâ€ and â€œstudentâ€ neural network. We\nfirst introduce knowledge distillation as relevant background, and\nthen introduce self-distillation as a modification of this framework\nthatenablesthistypeoftrainingintheabsenceofafixed,pre-trained\nteacher network.Finally, we introducethe maskeddimage modeling\nextension proposed by Zhou et al. (2021), and its culmination in a\nunified framework in DINOv2 Oquab et al. (2023).\nA2.1 Knowledge Distillation\nKnowledge distillation (BuciluË‡a et al. 2006) is a type of training\nregimethathashistoricallybeenusedtotrainasmallstudentnetwork\nto mimic the output of a large, pre-trained teacher network. The\nultimate goal of this training scheme is to compress the size of the\nteacher network.\nConcretely, let ğ‘“ğ‘¡ represent the teacher neural network, andğ‘”ğ‘ \nrepresentthestudentneuralnetwork;then,theobjectiveinknowledge\ndistillation is to minimize the cross-entropy between the outputs of\nboth networks for the same inputx, such that:\nLKD =âˆ’\nğ¾âˆ‘ï¸\nğ‘—=1\nğ‘ƒğ‘¡(xğ‘—)log ğ‘ƒğ‘ (xğ‘—). (A2)\nHere, ğ¾is the size of the training dataset, andğ‘ƒrepresents a sort of\nprobability distribution of the output ofğ‘“ or ğ‘”, which is attained by\nusing a softmax function to normalize the output of the network:\nğ‘ƒ(x)ğ‘– = exp(ğ‘“(x)ğ‘–)\nÃğ¾\nğ‘˜=1 exp(ğ‘“(x)ğ‘˜)\n. (A3)\nKnowledgedistillationisapowerfulcompressiontechnique,butitis\nnot applicable to SSL training directly, as it relies on a pre-trained\nfixed teacher network.\nA2.2 Self-Distillation\nTo enable knowledge distillation in the absence of a pre-trained\nfixedteachernetwork,knowledgeDIstillationwithNOlabels(DINO;\nCaron et al. 2021), or self-distillation, has recently been proposed.\nMNRAS000, 1â€“18 (2024)\n14 Liam Parker\nRather than distilling knowledge from a pre-trained teacher, self-\ndistillationworksbyinsteaddistillingknowledgefrompastiterations\nof the student network itself.\nConcretely, the student and teacher networks share the same ar-\nchitecture ğ‘“, albeit different weights. The weights of the student\nnetwork, ğœƒğ‘ , are updated via gradient descent, as is typical of ma-\nchinelearningtraining.However,theweightsoftheteachernetwork,\nğœƒğ‘¡,arenotgivenaccesstogradientinformation.Instead,thesearedy-\nnamicallybuiltfrompastiterationsofthestudentnetworkâ€™sweights.\nThis is done using an exponential moving average (EMA) of the\nstudent networkâ€™s weights (He et al. 2020), such that\nğœƒğ‘¡ â†âˆ’ğœ†ğœƒğ‘¡ +(1 âˆ’ğœ†)ğœƒğ‘ , (A4)\nwhere ğœ† is a tunable hyperparameter commonly referred to as the\nsmoothing or time constant.\nBy composing the teacher network as an iterated average of the\nstudent networkâ€™s past weights, the teacher network effectively un-\ndergoesaensemblingtechnique.Thistypeofmodelensemblinghas\nbeen well-explored in the literature (Ruppert 1988), and has been\nshowntoleadtobetterperformanceandgeneralizationinsupervised\nmodels. In the context of DINO, it too leads to a teacher network\nthat performs better than its student (Caron et al. 2018). Therefore,\nthe teacher network, like in vanilla knowledge distillation, is still\nabletoguidethestudentnetworkduringtrainingbyprovidingbetter\nrepresentation outputs.\nInpractice,DINOaddsadditionalelementstothisself-distillation\nscheme.Forone,topromotelocal-to-globalcorrespondence,asetof\nğ‘‰differentâ€œviewsâ€aregeneratedforeachinput,whichinthecaseof\nDINOisanimage. ğ‘‰consistsofbothâ€œglobalâ€views,whichconsistof\nlargecropsoftheimage,andâ€œlocalâ€views,whichconsistofsmaller\ncrops of the image. The entire set ofğ‘‰ is passed to the student,\nwhile only the global views are passed to the teacher. The student\nand teacher must then still generate the same output representation.\nAdditionally, to prevent a trivial collapse between the represen-\ntations learned by the student and teacher networks of the inputs,\nDINO both centers and sharpens the outputs of the teacher network\n(Caron et al. 2021).\nA2.3 image-BERT Pre-Training with Online Tokenizer\nWhile not originally introduced in a self-distillation context, MIM\n(seesubsection2.2)hasbeenextendedtothisregimeinrecentworks\nlike image-BERT pre-training with Online Tokenizer (iBOT; Zhou\net al. 2021). Specifically, given some input imagex, a masked view\nof the input,Ë†x, is fed to the student network, while the unmasked\nview x is fed to the teacher. Thus, for any given masked patchğ‘–,\nthe student network outputsË†zğ‘–ğ‘  = ğ‘ƒğ‘–ğ‘ (Ë†x), while the teacher network\noutputs zğ‘–\nğ‘¡ = ğ‘ƒğ‘–ğ‘ (xğ‘¡). These probabilities, like in subsection A2, are\nonce again computed using a softmax function. Then, Equation A2\ncan be easily rewritten as\nLMIM =âˆ’\nğ¾âˆ‘ï¸\nğ‘—=1\nğ‘âˆ‘ï¸\nğ‘–=1\nğ‘šğ‘– Â·ğ‘ƒğ‘–\nğ‘¡(xğ‘—)log ğ‘ƒğ‘–\nğ‘ (Ë†xğ‘—). (A5)\nFunctionally,iBOTincludesinitslosstermsomeadditionalcompli-\ncations. For one, iBOT performs MIM on two augmented views of\nx simultaneously. Then, Equation A5 is symmetrized by averaging\nanother cross-entropy term between patches of the other augmented\nview.Additionally,iBOTincludesinitslossanotherself-distillation-\nliketermbetweentheglobalrepresentationofthestudentandteacher\nnetwork. As in subsection A2, the teachers weights are updated as\nan EMA of the student weights.\nA2.4 DINO v2\nself-DIstillation with NO labels version 2 (DINO v2; Oquab et al.\n2023) is an extension of the DINO self-distillation framework that\nincorporatestheMIMobjectivefromimage-BERTPre-Trainingwith\nOnline Tokenizer (Zhou et al. 2021) into the DINO objective. For\nany given inputx, DINO v2 computes:\nâ€¢The LKD loss between the features extracted by the student\nnetworkfrombothglobalandlocalcropsof xandtheteachernetwork\nfrom the global crops ofx.\nâ€¢The LMIM lossbetweentherandomlymaskedpatchesgivento\nthe student and the corresponding, unmasked patches given to the\nteacher.\nFor both losses, softmax functions are applied to the outputs of the\nnetworks, and centering is applied to the teacher outputs to prevent\ncollapse. The composite DINOv2 loss is then given by\nLDINOv2 =ğ‘¤1 Â·LKD +ğ‘¤2 Â·LMIM, (A6)\nwhere ğ‘¤1 and ğ‘¤2 are scalars that weight the relative importance of\nboth the DINO and MIM losses.\nIn practice, DINOv2 also adds a regularization term to the above\ncomposite loss, called the KoLeo regularizer (Sablayrolles et al.\n2018).Thisregularizerencouragesauniformspanwithineachbatch,\nand is given by\nLğ‘˜ğ‘œğ‘™ğ‘’ğ‘œ = 1\nğ‘›\nğ‘›âˆ‘ï¸\nğ‘–=1\nlog min\nğ‘—â‰ ğ‘–\nâˆ¥xğ‘– âˆ’xğ‘— âˆ¥2, (A7)\nwhere ğ‘›represents the total size of the batch.\nUltimately, the DINOv2 loss has demonstrated superior perfor-\nmanceonavarietyofdownstreamtasksincludingsemanticsegmen-\ntation,imageclassification,depthestimation,videoprocessing,etc.,\nand a variety of ablation tests have demonstrated the importance of\nsuch a composite loss (Oquab et al. 2023).\nA3 Transformers\nTransformers(Vaswanietal.2017)areatypeofneuralnetworkarchi-\ntecture that employs an attention mechanism (Bahdanau et al. 2014)\nto attend to and contextualize various parts of its input sequence.\nHere we focus on scaled dot-product attention, a specific imple-\nmentation of attention, that requires three inputs - queries,ğ‘„, keys,\nğ¾, and values,ğ‘‰. Intuitively,ğ‘„ represents the set of elements that\nare seeking information,ğ¾ represents the elements that are being\nqueried against, andğ‘‰ represents the information that is retrieved\nbased on the similarity betweenğ‘„ and ğ¾. In this framework, each\nquery inğ‘„ is compared against all keys inğ¾ to compute a set of\nattention weights as\nğ´=softmax\n\u0012ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ¾\n\u0013\n. (A8)\nThe output is normalized by the dimensionality of the keys (ğ‘‘ğ¾) to\npreventoverlylargedotproductvalues,whichcouldleadtogradient\nvanishing or exploding problems. Ultimately, these attention scores\nencode how much each value inğ‘‰ should contribute to the output;\nindeed, the final output of the attention mechanism is computed as a\nweightedsumof ğ‘‰,wheretheweightofeachvalueisdeterminedby\nğ´as\nOutput = ğ´ğ‘‰ =softmax\n\u0012ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ¾\n\u0013\nğ‘‰. (A9)\nWhenğ‘„andğ¾arethesamesequence,thealgorithmaboveisreferred\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 15\nto as self-attention, whereas whenğ‘„and ğ¾ are different sequences,\nthealgorithmiscalledcross-attention.Ultimately,inasimpleregres-\nsion setting, this mechanism allows the network to, for a given entry\nin the query sequence, â€œpay attentionâ€ to the most relevant parts of\ntherestoftheinputsequence,calculatedin ğ´,andusetheassociated\nvalues to produce an output. Moreover, the attention mechanism is\npermutation-invariant and agnostic to sequence length.\nIn practice, trainable weights are applied to the queries, keys, and\nvaluesbeforetheyperformtheattentionoperation.Moreover,multi-\npleattentionheadsareconcatenatedateachstep,allowingthemodel\ntosimultaneouslyattendtoinformationfromdifferentrepresentation\nsubspaces at different positions. This is known as multi-head atten-\ntion, a key feature that enhances the transformerâ€™s ability to capture\na wide array of dependencies in the input data. Each attention head\ncan be thought of as an independent feature extractor, focusing on\ndifferent aspects of the input sequence. By concatenating the out-\nputs of these heads, the model integrates diverse perspectives into a\ncomprehensive representation.\nMathematically, the multi-head attention mechanism can be de-\nscribed as follows:\nMultiHead(ğ‘„,ğ¾,ğ‘‰ )=Concat(head1,head2,..., headâ„)ğ‘Šğ‘‚ (A10)\nwhere each head is computed as:\nheadğ‘– =Attention(ğ‘„ğ‘Šğ‘„\nğ‘– ,ğ¾ğ‘Š ğ¾\nğ‘– ,ğ‘‰ğ‘Šğ‘‰\nğ‘– ) (A11)\nğ‘Šğ‘„\nğ‘– ,ğ‘Šğ¾\nğ‘– ,and ğ‘Šğ‘‰\nğ‘– aretheweightmatricesforthequeries,keys,and\nvalues,respectively,forthe ğ‘–-thhead. ğ‘Šğ‘‚istheoutputweightmatrix\nthat combines the headsâ€™ outputs.\nIn practice, transformers are typically composed of several lay-\ners, each containing a multi-head attention mechanism followed by\na position-wise fully connected feed-forward network. This design\nallows the transformer to process all parts of the input sequence\nin parallel, significantly improving efficiency over architectures that\nprocess inputs sequentially. Between each layer, normalization and\nresidual connections are employed to enhance training stability and\nfacilitate the flow of gradients during backpropagation.\nA4 Normalizing Flows\nNormalizingflowsareatypeofgenerativemodelthatarecommonly\nusedinNeuralPosteriorEstimation(NPE; e.g. Rezende&Mohamed\n2015; Dinh et al. 2016; Papamakarios & Murray 2016; Lueckmann\net al. 2017; Greenberg et al. 2019) to estimate either unconditional\nor conditional probability distributions. These are useful as often\ntimesthedimensionalityandcomplexityofthedistributionofinterest\nrender it impossible to estimate by sampling techniques alone.\nA normalizing flow iteratively transforms a simple multivariate\nnoisesource,oftenthestandardmultivariateNormaldistribution ğ‘¥âˆ¼\nN(0,I5Ã—5),intothecomplexparameterdistribution ğœƒ âˆ¼Î˜througha\nseriesoflearned,vector-valuedbÄ³ective(invertible)transformations\nğ‘“ = ğ‘“1 â—¦ğ‘“2 â—¦...â—¦ğ‘“ğ‘›.Thisset-upallowsthemtosampletheprobability\ndensity of the datağœƒ by simply sampling the latent variablex, and\nthen transforming the variable toğœƒ through as ğ‘“(x). Î˜can then be\nscored using the multivariate substitution rule as\nğ‘(ğœƒ)=ğœ‹(ğ‘“âˆ’1 (ğœƒ))\nğ‘›Ã–\nğ‘™=1\n\f\f\f\f\fdet\n \nğœ•ğ‘“ âˆ’1\nğ‘™ (ğœƒ)\nğœ•ğœƒ\n!\f\f\f\f\f, (A12)\nwhere a simple inductive argument is used on ğ‘“. Note that the\nbÄ³ective transformations ğ‘“ must have easy-to-compute Jacobians\ndet ğœ•ğ‘“ğ‘™(ğœƒ)\nğœ•ğœƒ andmustbeeasytoinvertforthistasktobecomputation-\nally tractable.\nIn many cases, we are interested in the posterior distribution\nğ‘(ğœƒ|z), wherez is some summary statistic of the data. Luckily, the\ntheory of normalizing flows is easily generalized to conditional dis-\ntributions, as we simply condition the transformationsğ‘“ on z to\nproduce the complex, conditionally transformed variableğœƒ = ğ‘“(x).\nSampling and scoring is analogous to the argument presented above\nusing this conditioning.\nTypically, ğ‘“ is parameterized using a neural networkğ‘ğœ™(ğœƒ|z),\nwhereğœ™representsthenetworkparameters.Thenetworkparameters\nare generated by minimizing the Kullback-Leibler (KL) divergence\nbetweenğ‘(ğœƒ,z)andğ‘ğœ™(ğœƒ,z)ğ‘(z),whichisequivalenttomaximizing\nthe log-likelihood over the training set as\nğœ™âˆ—=argmaxğœ™\n1\nğ‘\nğ‘âˆ‘ï¸\nlog ğ‘ğœ™(ğœƒğ‘–|zğ‘–), (A13)\nwhere ğ‘ğœ™(ğœƒğ‘–|zğ‘–)is given by the scoring function above.\nB Implementation Details\nWe provide the implementations details - i.e. the specifics of model\narchitecture, training procedure, hyperparameters, etc. - for the vari-\nous models trained in the previous sections here.\nB1 Galaxy Image Transformer\nB1.1 Model Details\nWhile we experimented with various architecture sizes, we find that\nwe achieve best performance when using a ViT-L with patch size\nğ‘ƒ=12. Given that our multi-band images haveğ¶ =3 channels, this\nresults in flattened vectors of sizexğ‘ âˆˆR432. We project these to a\nğ· =1024 dimensionalembeddingusingEquation3.ForourViT,we\nuse 24 transformer layers, 16 heads in the mult-head self-attention,\nand MLP hidden layers of width4 Ã—1024 =4096. Additionally, we\nuse two separate projection heads for both our student and teacher\nViT backbones, each of which has 2048 hidden MLP dimensions\nand 3 layers. This configuration results in a model with roughly 307\nmillion trainable parameters.\nB1.2 Training Details\nPretrainingisperformedover500epochson16H100GPUsusingthe\nAdamoptimizerwithabatchsizeof96imagesperGPU,resultingin\natotalbatchsizeof1536.Welinearlyincreaseourlearningratefrom\n0 to 0.004 over the first 80 epochs of training, after which we decay\nthe learning rate with a cosine schedule. The momentum between\nstudent and teacher,ğœ†, is increased according to a cosine schedule\nfrom0.994 to1.0 duringtraining.Wesetourweightdecaytoafixed\nvalue of0.001, as we find that increasing our weight decay during\ntrainingleadstounderfitting.WeweightthelossbetweenDINOand\niBOT losses as one-to-one. Training in this regime takes roughly 48\nhours.\nB2 Galaxy Spectrum Transformer\nB2.1 Model Implementation\nAfterexperimentingwithvariouspatchsizes,weachievebestresults\nwhen using a patch size ofğµ = 20 and an overlapping segment of\nğ´ = 10. We also experimented with various model architectures,\nand find that we achieve best performance when using a transformer\nMNRAS000, 1â€“18 (2024)\n16 Liam Parker\nwith with ğ· = 768 embedding dimensions, 6 transformer blocks,\nand 6 heads in our multi-head attention module. This results in a\ntransformer with roughly 43.2 million trainable parameters.\nB2.2 Training Details\nWe pretrain our spectrum encoder on the full DESI spectra dataset\nusingamask-fillingloss.Trainingisperformedon4H100GPUsfor\na total of 500 epochs, resulting in a total training time of roughly 24\nhours.\nB3 AstroCLIP Model\nB3.1 Model Implementation\nIn our implementation, we use 4 cross-attention heads followed by\ntwolinearlayerswith512embeddingdimensions.Wealsouselayer\nnorm and GeLU activation functions. The output of this network\nis the final embedding of the galaxy,z âˆˆR512, and should reside\nin a shared, aligned embedding space after the image and spectrum\ntransformershavebeenpre-trainedandsuccessfullyalignedtocreate\nAstroCLIP.\nB3.2 Training Details\nWe train our models on the training split of our paired image-\nspectrum dataset. We use a queue length of ğ¾ = 1024 image-\nspectrumpairs.Duringtraining,weperformbasicdataaugmentation\nwithrandomverticalandhorizontalflipsandrandomrotationsonthe\nimages.WetrainourmodelsusingtheAdamWoptimizerwithabase\nlearning rate of 0.0001 with a cosine scheduler and a weight decay\nof0.01.Wetrainourmodelfor500iterationsonasingleH100GPU,\nwhich results in roughly 48 hours of training time. Finally, similar\nto the findings in Girdhar et al. (2023), we find better performance\nby fixing the value of the temperature parameterğœin Equation 1 as\nopposed to letting it free. We also set the logit scale in our loss to a\nfixed value of 15.5.\nB4 ResNet18 Image Regressor\nWe use a modified version of the ResNet18 vision model from He\net al. (2016). This model is part of the Residual Network family,\nknown for its ability to train very deep networks through the use\nof shortcut connections that skip one or more layers. We modify\nthe architecture by changing the first convolutional layer to accept\n3-channel (ğ‘Ÿ,ğ‘”,ğ‘ )images, and set the kernel size to 7 and the stride\nlength to 2. We also add a final, fully-connected layer that maps the\n512-dimensionalfeaturevectorsproducedbytheprecedingconvolu-\ntionalandpoolinglayerstothedesirednumberofoutputdimensions.\nWe train two versions of the model: one to regress galaxy\nredshift, and one to regress the galaxy property vector ğœƒ =\nlog ğ‘€âˆ—,log ğ‘ğ‘€ğ‘Š,ğ‘¡ğ‘ğ‘”ğ‘’,log ğ‘ ğ‘†ğ¹ğ‘…}. We train both over the PROV-\nABGStrainingsetfor100epochsusingtheAdamOptimizerandan\nMSE loss. During training, we prevent model overfitting by apply-\ning a number of random augmentations, namely random horizontal\nand vertical flips withğ‘ = 0.5 and random Gaussian blurring with\nkernel size 5 andğœ âˆ¼U( 0,2). We initialize the learning rate at\nğœ†=5 Ã—10âˆ’4. At each epoch, we evaluate the modelâ€™s performance\non the validation set, and take as our best model the model with the\nbest validation performance. We report our results on the held-out\ntest set. We train the model on a single A100 GPU with a batch size\nof 512, resulting in roughly 1 hour of training time.\nB5 Spectrum Property Regressor\nWe use a modified version of the time-series encoder from Serra\netal.(2018).Thisnetworkfirstappliesfourconvolutionallayerswith\n[8,16,16,32]kernelswithPReLUactivationfunctionsanddropout.\nThen, the output of the last convolutional layer split into two halves\nalongitschanneldimensions.Adot-productattentionisthenapplied,\nwhere one half of the channels serve as the keys(ğ¾)and the other\nhalfserveasthevalues (ğ‘‰)fortheattentioncalculation.Theattended\nfeatures are then compressed into a latent representation through an\nMLP with [32,32]hidden dimensions. We chose this architecture\nas it is used as the encoder in a current state-of-the-art spectrum\nautoencoder setting (Melchior et al. 2023).\nWe train two versions of the model: one to regress galaxy\nredshift, and one to regress the galaxy property vector ğœƒ =\nlog ğ‘€âˆ—,log ğ‘ğ‘€ğ‘Š,ğ‘¡ğ‘ğ‘”ğ‘’,log ğ‘ ğ‘†ğ¹ğ‘…}. We train both over the PROV-\nABGStrainingsetfor100epochsusingtheAdamOptimizerandan\nMSE loss. At each epoch, we evaluate the modelâ€™s performance on\nthevalidationset,andtakeasourbestmodelthemodelwiththebest\nvalidation performance. We report our results on the held-out test\nset. We train the model on a single A100 GPU with a batch size of\n512, resulting in roughly 10 minutes of training time.\nB6 Normalizing Flow Model\nFor our problem setting, we use a stack of quadratic rational spline\ncoupling bÄ³ections(Durkan et al.2019) as ourbÄ³ective transforma-\ntions ğ‘“.Quadraticsplinetransformationsinvolvetheuseofpiecewise\nquadratic functions to create smooth, continuous mappings between\nvariables. We condition these splines on the embeddingz with a\nfully-connected MLP. For each of our 10 random flows, we ran-\ndomly choose the number of transformations asU{3,4,5,6}and\nthe random number of MLP hidden dimensions asU[32,128]. We\ntraineachflowusingan80/20train-validationsplitonthetrainingset\nandpreventoverfittingbystoppingtrainingwhenthevalidationlog-\nlikelihood has not improved for 20 epochs. We report the negative\nlog-likelihood over the held-out test set as our results.\nC Extended Results\nWe report a variety of additional results below.\nD Attention Maps and Performance of Spectrum Encoder\nWe look at the attention maps of the cross-attention layer of the\nspectrum encoder, described in subsubsection B2.1. These plots can\nhelpinterpretwhatinformationthemodelislookingatwhenbuilding\nits representation of the spectrum.\nFigure D1 shows a number of examples of these attention maps.\nWe see that the different attention heads have specialized to look for\ndifferentfeatures.Head1seemstobelookingatthetwoextremesof\nthespectrumwhichwouldmakeitsensitivetodifferentspectraltilts.\nHead3seemstobesensitivetopeaksaroundthe9k Ëšğ´range.However,\nit is important to note that this cross-attention layer comes after the\n6 layers of self-attention of the pre-trained model. At this stage of\nthe network, information about different sections of the spectrum\nhave likely diffused throughout the entire sequence and therefore\nthe attention maps potentially access information from parts of the\nspectrum where the attention is zero.\nAdditionally, we evaluate the performance of the mask-filling\nmodel pretrained on the spectra in Figure D2. In these figures, the\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 17\n(a) Example 1\n (b) Example 2\n(c) Example 3\n (d) Example 4\nFigureD1. Randomlychosenexamplesofattentionmapsfromthecross-attentionlayeroftheself-supervisedspectrummask-fillingmodel.Thesevisualizations\nillustrate the modelâ€™s focus primarily on emission lines within the spectrum, highlighted by pronounced peaks in the attention matrices at these regions,\ndemonstrating the modelâ€™s ability to identify and emphasize significant spectral features effectively.\n(a) Example 1\n (b) Example 2\n(c) Example 3\n (d) Example 4\nFigureD2. Randomlychosenexamplesoftheperformanceoftheself-supervisedtrainedspectrummaskfillingtransformer.Thespectrumtransformerisbroadly\nable to infer the correct shape of missing regions of the spectrum from the broader spectrum context.\nMNRAS000, 1â€“18 (2024)\n18 Liam Parker\nshaded region denotes the area where the spectrum was zeroed out\nwhen passed to the model. The various inserts show close-ups of\nthe smoothed ground-truth (by taking averages of 20 bins) as well\nas the prediction of the model. We see that the model has learned\nto reproduce the prominent features of the spectrum. For example,\nin both Figure D2a and Figure D2b a number of the masked regions\nhavefallenonabsorptionandemmissionlines.Weseethatthemodel\ncan reproduce these features with high precision.\nD1 Normalizing Flow Sample Posterior Estimation\nWepresentinFigureD3aandFigureD3bthesampledposteriorfrom\nour trained normalizing flowğ‘ğœ™(ğœƒ|z)for a randomly chosen galaxy\nimage and spectrum respectively, where the flow is conditioned on\nthe AstroCLIP embedding of that image or spectrum.\nE TARP Expected Coverage Tests\nWeensurethatournormalizingflowsarewell-calibratedusingTests\nof Accuracy with Random Point (TARP) Expected Coverage Proba-\nbility (ECP) tests. These have been shown to be necessary and suf-\nficient for exploring the optimality of the posterior estimate (Lemos\net al. 2023). The TARP method is designed to evaluate the accuracy\nof generative posterior estimators by creating spherical credible re-\ngions centered on a specified random reference point,ğœƒğ‘Ÿ, and then\nassessing whether these regions capture the true parameter values.\nWe evaluate the TARP ECP over the full dimensionality of our\nproperty space, and provide the results for the ensemble of models\ntrained from images/photometry and from spectra in Figure E1 and\nFigure E2 respectively; if the ECP follows the diagonal line, i.e. it is\nequal to the confidence level for everyğ›¼âˆˆ[0,1], then the estimator\niswellcalibrated.Asshowninthefigures,allmodelsareindeedwell\ncalibratedonourheld-outtestsetonmostofthepropertyestimation\ntasks other thanlog ğ‘ ğ‘†ğ¹ğ‘…, on which some of the models are either\nslightly over- or under-confident.\nE1 Numerical Results on Galaxy Morphology Classification\nWeprovidethenumericalresultsoffew-shotlearningfromtheAstro-\nCLIP galaxy image embeddings on the Galaxy Zoo DECaLS GZD-\n5 survey detailed in subsubsection 4.2.3. We only evaluate galaxy\nclassesongalaxiesforwhichmorethan50%ofthevolunteersshown\nthat galaxy answered that question.\nThis paper has been typeset from a TEX/LATEX file prepared by the author.\nTable E1.Galaxy morphology classification results. We train a simple MLP\non the AstroCLIP galaxy image embeddings to predict the Galaxy Zoo DE-\nCaLS GZD-5 morphology classification of that galaxy. We report both the\nclass-weightedaccuracyandF1-scoreofthevariousmodelsforeachquestion.\nOverall, AstroCLIP achieves relatively strong performance on all questions,\nand clearly outperforms a state-of-the-art self-supervised model for galaxy\nimages (Stein et al. 2021b). We highlight in bold the best results on each\nquestion, excluding the reported ZooBot results.\n(a) Accuracy Scores\nQuestion CLIP DINO Stein ZooBot\nsmooth 0.83 0.84 0.78 0.94\ndisk-edge-on 0.97 0.97 0.87 0.99\nspiral-arms 0.92 0.95 0.95 0.93\nbar 0.56 0.53 0.53 0.82\nbulge-size 0.79 0.81 0.78 0.84\nhow-rounded 0.74 0.79 0.81 0.93\nedge-on-bulge 0.82 0.86 0.83 0.91\nspiral-winding 0.74 0.77 0.79 0.78\nspiral-arm-count 0.44 0.50 0.50 0.77\nmerging 0.80 0.81 0.81 0.88\n(b) F1 Scores\nQuestion CLIP DINO Stein ZooBot\nsmooth 0.83 0.83 0.68 0.94\ndisk-edge-on 0.97 0.97 0.81 0.99\nspiral-arms 0.94 0.96 0.95 0.94\nbar 0.54 0.37 0.37 0.81\nbulge-size 0.78 0.81 0.77 0.84\nhow-rounded 0.74 0.79 0.81 0.93\nedge-on-bulge 0.81 0.84 0.75 0.90\nspiral-winding 0.68 0.73 0.76 0.79\nspiral-arm-count 0.41 0.47 0.44 0.76\nmerging 0.73 0.71 0.71 0.85\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 19\n(a) Example Image Input\n (b) Example Spectrum Input\nFigure D3.Galaxy property posterior estimates for a randomly chosen galaxy image and spectrum using normalizing flows. The posterior is estimated using a\nnormalizing flow to map a multivariate Gaussianğœ‹ = N(0,I5 )into the property vectorğœƒ âˆˆR5 using learned bÄ³ective quadratic splines conditioned on the\nlatent embedding vectorzsp. The flow is then sampled by transforming samples fromğœ‹ to ğœƒ using the learned bÄ³ective transforms. The true value for each\ngalaxy property is marked with a line in blue.\nMNRAS000, 1â€“18 (2024)\n20 Liam Parker\n(a) AstroCLIP Image Embedding\n(b) ResNet18 Image Embedding\n(c) DINO Image Embedding\n(d) Stein et al. (2021b) Image Embedding\n(e) Photometry MLP\nFigure E1.Tests of Accuracy with Random Points (TARP; Lemos et al. 2023) Expected Coverage Probability (ECP) tests on the trained normalizing flow\nensembles for each image embedding/supervised method. If the ECP follows the diagonal line, i.e. it is equal to the confidence level for everyğ›¼âˆˆ[0,1], then\ntheestimatoriswellcalibrated.Overall,thevariousmethodsappeartobewell-calibrated,otherthanDINOand(Steinetal.2021b)whichareslightlybiasedon\nlog ğ‘ ğ‘†ğ¹ğ‘…, and the photometry which is underconfident onlog ğ‘ ğ‘†ğ¹ğ‘….\nMNRAS000, 1â€“18 (2024)\nAstroCLIP 21\n(a) AstroCLIP Spectrum Embedding\n(b) Spectrum Transformer Embedding\n(c) Spender\nFigure E2.Tests of Accuracy with Random Points (TARP; Lemos et al. 2023) Expected Coverage Probability (ECP) tests on the trained normalizing flow\nensembles for each spectrum embedding/supervised method. If the ECP follows the diagonal line, i.e. it is equal to the confidence level for everyğ›¼ âˆˆ[0,1],\nthen the estimator is well calibrated. Overall, the various methods appear to be well-calibrated, other than CLIP and the Spectrum Transformer onlogğ‘ ğ‘†ğ¹ğ‘…m\non which they are slightly biased.\nMNRAS000, 1â€“18 (2024)",
  "topic": "Galaxy",
  "concepts": [
    {
      "name": "Galaxy",
      "score": 0.5953165888786316
    },
    {
      "name": "Redshift",
      "score": 0.5818312764167786
    },
    {
      "name": "Computer science",
      "score": 0.5798746943473816
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5388220548629761
    },
    {
      "name": "Modal",
      "score": 0.43788713216781616
    },
    {
      "name": "Encoder",
      "score": 0.4351407289505005
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.41659706830978394
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39537355303764343
    },
    {
      "name": "Astrophysics",
      "score": 0.38272392749786377
    },
    {
      "name": "Machine learning",
      "score": 0.32444292306900024
    },
    {
      "name": "Physics",
      "score": 0.29045820236206055
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387153999",
      "name": "Flatiron Institute",
      "country": null
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210153546",
      "name": "Flatiron Health (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210157550",
      "name": "2B Technologies (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I188538660",
      "name": "University of Colorado Boulder",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I148283060",
      "name": "Lawrence Berkeley National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I109736498",
      "name": "Canadian Institute for Advanced Research",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}