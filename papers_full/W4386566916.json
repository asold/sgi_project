{
    "title": "ferret: a Framework for Benchmarking Explainers on Transformers",
    "url": "https://openalex.org/W4386566916",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2049312926",
            "name": "Giuseppe Attanasio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2608969421",
            "name": "Eliana Pastor",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2937078476",
            "name": "Chiara Di Bonaventura",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2203505503",
            "name": "Debora Nozza",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3120706522",
        "https://openalex.org/W2970014349",
        "https://openalex.org/W3173380736",
        "https://openalex.org/W3101155149",
        "https://openalex.org/W2562979205",
        "https://openalex.org/W4287324238",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3197613133",
        "https://openalex.org/W4285260356",
        "https://openalex.org/W2996507500",
        "https://openalex.org/W2890353432",
        "https://openalex.org/W3103035585",
        "https://openalex.org/W2963123635",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3212603771",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W4385570354",
        "https://openalex.org/W3085380432",
        "https://openalex.org/W3174150157",
        "https://openalex.org/W2970447476",
        "https://openalex.org/W2944854690",
        "https://openalex.org/W2963691697",
        "https://openalex.org/W3035503910",
        "https://openalex.org/W3172794097",
        "https://openalex.org/W2953073956",
        "https://openalex.org/W2962816513",
        "https://openalex.org/W4283396845",
        "https://openalex.org/W3197047205",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4285235105",
        "https://openalex.org/W3158835769",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W3117696238",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W4287887637",
        "https://openalex.org/W2943031807",
        "https://openalex.org/W3101662419",
        "https://openalex.org/W2807124908",
        "https://openalex.org/W2972376180",
        "https://openalex.org/W3187467055",
        "https://openalex.org/W2970863760",
        "https://openalex.org/W3173813266",
        "https://openalex.org/W3034917890",
        "https://openalex.org/W3173173856",
        "https://openalex.org/W4288375898"
    ],
    "abstract": "Giuseppe Attanasio, Eliana Pastor, Chiara Di Bonaventura, Debora Nozza. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. 2023.",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics\nSystem Demonstrations, pages 256–266\nMay 2-4, 2023 ©2023 Association for Computational Linguistics\nferret: a Framework for Benchmarking Explainers on Transformers\nGiuseppe Attanasio♣, Eliana Pastor♢, Chiara Di Bonaventura♠, Debora Nozza♣\n♣Bocconi University, Milan, Italy\n♢Politecnico di Torino, Turin, Italy\n♠King’s College London, London, United Kingdom\n{giuseppe.attanasio3,debora.nozza}@unibocconi.it\neliana.pastor@polito.it\nchiara.di_bonaventura@kcl.ac.uk\nAbstract\nAs Transformers are increasingly relied upon\nto solve complex NLP problems, there is an in-\ncreased need for their decisions to be humanly\ninterpretable. While several explainable AI\n(XAI) techniques for interpreting the outputs of\ntransformer-based models have been proposed,\nthere is still a lack of easy access to using and\ncomparing them. We introduce ferret, a Python\nlibrary to simplify the use and comparisons of\nXAI methods on transformer-based classifiers.\nWith ferret, users can visualize and compare\ntransformers-based models output explanations\nusing state-of-the-art XAI methods on any free-\ntext or existing XAI corpora. Moreover, users\ncan also evaluate ad-hoc XAI metrics to select\nthe most faithful and plausible explanations.\nTo align with the recently consolidated pro-\ncess of sharing and using transformers-based\nmodels from Hugging Face, ferret interfaces\ndirectly with its Python library. In this paper,\nwe showcase ferret to benchmark XAI meth-\nods used on transformers for sentiment analy-\nsis and hate speech detection. We show how\nspecific methods provide consistently better ex-\nplanations and are preferable in the context of\ntransformer models.\n1 Introduction\nTransformers have revolutionized NLP applications\nin recent years due to their strong performance on\nvarious tasks; their black-box nature remains an\nobstacle for practitioners who need explanations\nabout why specific predictions were made and what\nfeatures drove them. The development of explain-\nable AI (XAI) techniques on several NLP tasks\n(Madsen et al., 2022) has helped bridge this gap by\nproviding insight into the inner workings of trans-\nformers and helping users gain trust in their deci-\nsions. Several XAI approaches have been proposed\nin the literature (Ribeiro et al., 2016; Lundberg and\nLee, 2017; Simonyan et al., 2014a; Pastor and Bar-\nalis, 2019), also tailored to Transformer models\n(Wallace et al., 2019a; Li et al., 2016; Jin et al.,\n2019; Ross et al., 2021). Despite the importance\nof making XAI methods accessible to NLP experts\nand practitioners through practical tools, there is\nstill a lack of accessibility for transformer models.\nXAI for transformers is mainly scattered and hard\nto operationalize. Methods come with independent\nimplementations or framework-specific libraries\nthat do not allow either evaluation or cross-method\ncomparison. Further, existing implementations\nare not integrated with widespread transformers\nlibraries (e.g., Hugging Face’s transformers (Wolf\net al., 2020)). The lack of standardization and weak\ninteroperability leaves practitioners with unsolved\nquestions, such as choosing the best method given\na task and a model (Attanasio et al., 2022).\nWe introduce ferret (FramEwork foR bench-\nmaRking Explainers on Transformers), an open-\nsource Python library that drastically simplifies the\nuse and comparison of XAI methods on transform-\ners. The library stems from vertical scientific contri-\nbutions and focused engineering efforts. On the one\nhand, ferret provides the first-of-its-kind API (see\nFigure 1) to use and compare explanation methods\nalong the established criteria of faithfulness and\nplausibility (Jacovi and Goldberg, 2020). On the\nother hand, it integrates seamlessly with transform-\ners (Wolf et al., 2020), making it an easy add-on\nto existing Transformer-based pipelines and NLP\ntasks. ferret permits to run four state-of-the-art\nXAI methods, compute six ad-hoc XAI evaluation\nmetrics, and easily load four existing interpretabil-\nity datasets. Further, it offers abstract interfaces to\nfoster future integration of methods, metrics, and\ndatasets.\nWe showcase ferret on sentiment analysis and\n256\nhate speech detection case studies. Faithfulness\nand plausibility metrics highlight SHAP (Lundberg\nand Lee, 2017) as the most consistent explainer on\nsingle- and multiple-samples scenarios.\nContributions. We release ferret, the first-of-its-\nkind benchmarking framework for interpretability\ntightly integrated with Hugging Face’s transform-\ners library. We release our code and documenta-\ntion,1 an interactive demo,2 and a video tutorial.3\n2 Library Design\nferret builds on four core principles.\n1. Built-in Post-hoc Interpretability We in-\nclude four state-of-the-art post-hoc feature impor-\ntance methods and three interpretability corpora.\nReady-to-use methods allow users to explain any\ntext with an arbitrary model. Annotated datasets\nprovide valuable test cases for new interpretability\nmethods and metrics. To the best of our knowledge,\nferret is first in providing integrated access to XAI\ndatasets, methods, and a full-fledged evaluation\nsuite.\n2. Unified Explanation Benchmarking We pro-\npose a unified API to evaluate explanations. We\ncurrently support six state-of-the metrics along the\nprinciples of faithfulness and plausibility (Jacovi\nand Goldberg, 2020).\n3. Transformers-readiness ferret offers a di-\nrect interface with models from the Hugging Face\nHub. Users can load models using standard nam-\ning conventions and explain them with the built-in\nmethods effortlessly. Figure 1 shows the essen-\ntial code to classify and explain a string with a\npre-existing Hugging Face model and evaluate the\nresulting explanations.\n4. Modularity and Abstraction ferret counts\nthree core modules, implementing Explainers,\nEvaluation, and Datasets APIs. Each module\nexposes an abstract interface to foster new\ndevelopment. For example, user can sub-class\nBaseExplainer or BaseEvaluator to include\na new feature importance method or a new\nevaluation metric respectively.\n1https://github.com/g8a9/ferret\n2https://huggingface.co/spaces/g8a9/ferret\n3https://youtu.be/kX0HcSah_M4\nFeature Category\nGradient Saliency\nIntegrated Gradient Saliency\nLIME Surrogate Model\nSHAP Shapley Values\nComprehensiveness Faithfulness\nSufficiency Faithfulness\nCorrelation with\nLeave-One-Out scores Faithfulness\nIntersection-Over-Union Plausibility\nArea Under\nPrecision-Recall Curve Plausibility\nToken-level F1 score Plausibility\nHateXplain Hate Speech\nMovieReviews Sentiment\nSST Sentiment\nThermostat Generic\nTable 1: ferret at a glance: built-in methods (top), met-\nrics (middle), and datasets (bottom).\nferret builds on common choices from the inter-\npretability community and good engineering prac-\ntices. We report the most salient technical details\n(e.g., efficiency via GPU inference, visualization\ntools, etc.) in Appendix A.\n2.1 Explainer API\nWe focus on the widely adopted family of post-\nhoc feature attribution methods (Danilevsky et al.,\n2020). I.e., given a model, a target class, and a\nprediction, ferret lets you measure how much each\ntoken contributed to that prediction. We integrate\nGradient (Simonyan et al., 2014b) (also known as\nSaliency) and Integrated Gradient (Sundararajan\net al., 2017); SHAP (Lundberg and Lee, 2017) as a\nShapley value-based method, and LIME (Ribeiro\net al., 2016) as representative of local surrogate\nmethods.\nWe build on open-source libraries and streamline\ntheir interaction with Hugging Face models and\nparadigms. We report the supported configurations\nand functionalities in Appendix A.\n2.2 Dataset API\nFostering a streamlined, accessible evaluation on\nindependently released XAI datasets, we provide\na convenient Dataset API. It enables users to load\nXAI datasets, explain individual or subsets of sam-\nples, and evaluate the resulting explanations.\n257\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom ferret import Benchmark\nname = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nmodel = AutoModelForSequenceClassification.from_pretrained(name)\ntokenizer = AutoTokenizer.from_pretrained(name)\nbench = Benchmark(model, tokenizer)\nexplanations = bench.explain(\"You look stunning!\", target=1)\nevaluations = bench.evaluate_explanations(explanations, target=1)\nFigure 1: Essential code to benchmark explanations on an existing Hugging Face model using ferret.\nCurrently, ferret includes three classification-\noriented datasets annotated with human rationales,\ni.e., annotations highlighting the most relevant\nwords, phrases, or sentences a human annotator\nattributed to a given class label (DeYoung et al.,\n2020; Wiegreffe and Marasovic, 2021). Moreover,\nferret API gives access to the Thermostat collection\n(Feldhus et al., 2021), a wide set of pre-computed\nfeature attribution scores.\nHateXplain (Mathew et al., 2021). It contains\n20,148 English instances labeled along three axes:\n(i) hate (either hateful, offensive, normal or un-\ndecided), (ii) target group (either race, religion,\ngender, sexual orientation, or miscellaneous), and\n(iii) word-level human rationales (expressed only\non hateful and offensive texts).4\nMovieReviews (Zaidan and Eisner, 2008; DeY-\noung et al., 2020). The dataset contains 2,000\nmovie reviews annotated with positive and negative\nsentiment labels and phrase-level human rationales\nthat support gold labels.\nStanford Sentiment Treebank (SST) (Socher\net al., 2013). A sentiment classification dataset of\n9,620 movie reviews annotated with binary senti-\nment labels, including human annotations for word\nphrases of the parse trees. We extract human ra-\ntionales from annotations following the heuristic\napproach proposed in Carton et al. (2020).\nThermostat Datasets Thermostat (Feldhus et al.,\n2021) provides pre-computed feature attribution\nscores given a model, a dataset, and an explanation\nmethod. ferret currently provides built-in access\nto pre-computed attributions on the news topic\nclassification and sentiment analysis tasks.\n4If a model splits a relevant word into sub-words, we con-\nsider all of them relevant as well.\nThese datasets provide an initial example of\nwhat an integrated approach can offer to re-\nsearchers and practitioners.\n2.3 Evaluation API\nWe evaluate explanations on the faithfulness and\nplausibility properties (Jacovi and Goldberg, 2020;\nDeYoung et al., 2020). Specifically, ferret im-\nplements three state-of-the-art metrics to measure\nfaithfulness and three for plausibility.\nFaithfulness. Faithfulness evaluates how accu-\nrately the explanation reflects the inner working of\nthe model (Jacovi and Goldberg, 2020).\nferret offers the following measures of faithful-\nness: comprehensiveness, sufficiency, (DeYoung\net al., 2020) and correlations with ‘leave-one-out’\nscores (Jain and Wallace, 2019).\nComprehensiveness (↑) evaluates whether the\nexplanation captures the tokens the model used to\nmake the prediction. We measure it by removing\nthe tokens highlighted by the explainer and observ-\ning the change in probability as follows.\nLet xbe a sentence and let fj be the prediction\nprobability of the model f for a target class j. Let\nrj be a discrete explanation or rationale indicat-\ning the set of tokens supporting the prediction fj .\nComprehensiveness is defined asf(x)j −f(x\\rj )j\nwhere x\\rj is the sentence xwere tokens in rj\nare removed. A high value of comprehensiveness\nindicates that the tokens in rj are relevant for the\nprediction.\nWhile comprehensiveness is defined for discrete\nexplanations, feature attribution methods assign a\ncontinuous score to each token. We hence select\nidentify rj as follows. First, we filter out tokens\nwith a negative contribution (i.e., they pull the pre-\ndiction away from the chosen label). Then, we\ncompute the metric multiple times, considering the\nk% most important tokens, with k ranging from\n258\n10% to 100% (step of 10%). Finally, we aggregate\nthe comprehensiveness scores with the average,\ncalled Area Over the Perturbation Curve (AOPC)\n(DeYoung et al., 2020).\nSufficiency (↓) captures if the tokens in the ex-\nplanation are sufficient for the model to make the\nprediction (DeYoung et al., 2020). It is measured as\nf(x)j −f(rj )j . A low score indicates that tokens\nin rj are indeed the ones driving the prediction. As\nfor Comprehensiveness, we compute the AOPC by\nvarying the number of the relevant tokens rj .\nCorrelation with Leave-One-Out scores (↑). We\nfirst compute leave-one-out (LOO) scores by omit-\nting tokens and measuring the difference in the\nmodel prediction. We do that for every token, once\nat a time. LOO scores represent a simple measure\nof individual feature importance under the linearity\nassumption (Jacovi and Goldberg, 2020). We then\nmeasure the Kendall rank correlation coefficient τ\nbetween the explanation and LOO importance (Jain\nand Wallace, 2019) (taucorr_loo). taucorr_loo\ncloser to 1 means higher faithfulness to LOO.\nPlausibility. Plausibility reflects how explana-\ntions are aligned with human reasoning by compar-\ning explanations with human rationales (DeYoung\net al., 2020) .\nWe integrate into ferret three plausibility mea-\nsures of the ERASER benchmark (DeYoung et al.,\n2020): Intersection-Over-Union (IOU) at the token\nlevel, token-level F1 scores, and Area Under the\nPrecision-Recall curve (AUPRC).\nThe first two are defined for discrete explana-\ntions. Given the human and predicted rationale,\nIOU (↑) quantifies the overlap of the tokens they\ncover divided by the size of their union.Token-level\nF1 scores (↑) are derived by computing precision\nand recall at the token level. Following DeYoung\net al. (2020) and Mathew et al. (2021), we derive\ndiscrete explanations by selecting the top K to-\nkens with positive influence, where Kis the aver-\nage length of the human rationale for the dataset.\nWhile being intuitive, IOU and Token-level F1 are\nbased only on a single threshold to derive ratio-\nnales. Moreover, they do not consider tokens’ rel-\native ranking and degree of importance. We then\nalso integrate the AUPRC ( ↑), defined for expla-\nnations with continuous scores (DeYoung et al.,\n2020). It is computed by varying a threshold over\ntoken importance scores, using the human rationale\nas ground truth.\n2.4 Transformers-Ready Interface\nferret is deeply integrated with Hugging Face in-\nterfaces. Users working with their standard mod-\nels and tokenizers can easily integrate it for diag-\nnostic purposes. The contact point is the main\nBenchmark class. It receives any Hugging Face\nmodel and tokenizer and uses them to classify, run\nexplanation methods and seamlessly evaluate the\nexplanations. Similarly, our Dataset API leverages\nHugging Face’s datasets5 to retrieve data and hu-\nman rationales.\n3 Case Studies\nWe showcase ferret in two real-world tasks, fo-\ncusing on benchmarking explainers on individual\nsamples or across multiple instances. In the fol-\nlowing, we describe how ferret highlights the best\nexplainers in sentiment analysis and hate speech de-\ntection tasks. Our running examples use an XLM-\nRoBERTa model fine-tuned for sentiment analysis\n(Barbieri et al., 2021) and a BERT model fine-tuned\nfor hate speech detection (Mathew et al., 2021).\n3.1 Faithfulness Metrics for Error Analysis\nExplanations on individual instances are often used\nfor model debugging and error analysis (Vig, 2019;\nFeng et al., 2018). However, different explanations\ncan lead users to different conclusions, hindering a\nsolid understanding of the model’s flaws. We show\nhow practitioners can alleviate this issue including\nferret in their pipeline.\nFigure 2 shows explanations and faithfulness\nmetrics computed on the sentence “Great movie\nfor a great nap!” for the “Positive” class label\nmisclassified by the model as “Negative”.\nFaithfulness metrics show that SHAP adheres\nbest to the model’s inner workings since it re-\nturns the most comprehensive and relevant explana-\ntions. Indeed, SHAP retrieves the highest number\nof tokens the model used to make the prediction\n(aopc_compr(↑) = 0.41) that are relevant to drive\nthe prediction ( aopc_suff(↓) = 0.09). Further,\ntaucorr_loo(↑) = 0.43 indicates that SHAP ex-\nplanations capture the most important tokens for\nthe prediction under the linearity assumption. Al-\nthough Integrated Gradient (x Input) shows a higher\ntaucorr_loo, it does not provide comprehensive\nand sufficient explanations. Similarly, Gradient\nand Integrated Gradient show bad sufficiency and\n5https://github.com/huggingface/datasets\n259\nfrom transformers import\nAutoModelForSequenceClassification,\nAutoTokenizer\nfrom ferret import Benchmark\nname = \"cardiffnlp/twitter-xlm-roberta-base\n-sentiment\"\nmodel = AutoModelForSequenceClassification.\nfrom_pretrained(name)\ntokenizer = AutoTokenizer.\nfrom_pretrained(name)\nbench = Benchmark(model, tokenizer)\nquery = \"Great movie for a great nap!\"\nscores = bench.score(query)\nprint(scores)\n# Run built-in explainers\nexplanations = bench.explain(\nquery,\ntarget=2 # \"Positive\" label\n)\nbench.show_table(explanations)\n# Evaluate explanations\nevaluations = bench.evaluate_explanations(\nexplanations, target=2\n)\nbench.show_evaluation_table(evaluations)\n## Output\n>> {'Negative': 0.013735532760620117,\n>> 'Neutral': 0.06385018676519394,\n>> 'Positive': 0.9224143028259277}\nFigure 2: Code to explain and evaluate explanations on\na sentiment classifier (top). Token attributions (middle):\ndarker red (blue) show higher (lower) contribution to\nthe prediction. Faithfulness metrics (bottom): darker\ncolors show better performance.\ncomprehensiveness, respectively. LIME and Gradi-\nent (x Input) do not return trustworthy explanations\naccording to all faithfulness metrics.\nOnce SHAP has been identified as the best ex-\nplainer, its explanations enable researchers to inves-\ntigate possible recurring patterns or detect model\nbiases thoroughly. In this case, the explanations\nshed light on a type of lexical overfitting: the word\n“great” skews the prediction toward the positive\nlabel regardless of the context and semantics.\n3.2 Multi-Instance Assessment\nInstance-level analysis finds explainers that meet\nspecific requirements locally. However, the best lo-\ncal explainer might be unsatisfactory across multi-\nple instances. With ferret, users can easily produce\nand aggregate evaluation metrics across multiple\ndataset samples—or the entire corpus.\nWe describe how to choose the explainer that\nreturns the most plausible and faithful explanations\nfor the HateXplain dataset. For demonstration pur-\nposes, we focus only on a sample of the dataset.\nFigure 3 (Appendix C) shows the metrics av-\neraged across ten samples with the “hate speech”\nlabel. Results suggest again that SHAP yields the\nmost faithful explanations. SHAP and Gradient\nachieve the best comprehensiveness and sufficiency\nscores, but SHAP outperforms all explainers for the\nτ correlation with LOO (taucorr_loo (↑) = 0.41).\nGradient provides the most plausible explanations,\nfollowed by SHAP.\n4 Related Work\nThis section provides a review of tools and libraries\nthat offer a subset of the ferret’s functionalities,\nnamely the option to use multiple XAI methods and\ndatasets, evaluation API, transformer-readiness,\nand built-in visualization. Table 2 summarizes\nthem and compares ferret with similar frameworks.\nTools for Post-Hoc XAI. Toolkits for post-hoc\ninterpretability offer built-in methods to explain\nmodel prediction, typically through a code inter-\nface. ferret builds on and extends this idea to a\nunified framework to generate explanations, eval-\nuate and compare them, with support to several\nXAI datasets. Moreover, ferret’s explainers are\nintegrated with transformers’s (Wolf et al., 2020)\nprinciples and conventions.\nPyTorch’s Captum (Kokhlikyan et al., 2020) is\na generic Python library supporting many inter-\npretability methods. However, the library lacks in-\ntegration with the Hugging Face Hub and offers no\nevaluation procedures. AllenNLP Interpret (Wal-\nlace et al., 2019b) provides interpretability methods\nbased on gradients and adversarial attacks for Al-\nlenNLP models (Gardner et al., 2018). We borrow\n260\nMultiple\nXAI approaches\nTransformers-\nreadiness\nEvaluation\nAPIs\nXAI\ndatasets\nBuilt-in\nvisualization\nCaptum ✓ ✗ ✗ ✗ ✓\nAllenNLP Interpret ✓ ✗ ✗ ✗ ✗\nTransformers-Interpret ✗ ✓ ✗ ✗ ✓\nThermostat ✓ ✓ ✗ ✗ ✓\nContrXT ✗ ✗ ✗ ✗ ✗\nOpenXAI ✓ ✗ ✓ ✗ ✗\nNLPVis ✗ ✗ ✗ ✗ ✓\nSeq2Seq-Vis ✗ ✗ ✗ ✗ ✓\nBertViz ✗ ✓ ✗ ✗ ✗\nELI5 ✗ ✗ ✗ ✗ ✓\nLIT ✓ ✗ ✗ ✗ ✓\nERASER ✗ ✗ ✓ ✓ ✗\nInseq ✓ ✓ ✗ ✗ ✓\nferret ✓ ✓ ✓ ✓ ✓\nTable 2: Comparing off-the-shelf features across different XAI libraries. When assessing built-in visualization, we\ndisregard tools that either do not provide a unified interface or provide single data-point visualizations.\nthe modular and extensible design and extend it to\na wider set of explainers. Transformers-Interpret6\nleverages Captum to explain Transformer models,\nbut it supports only a limited number of meth-\nods. Thermostat (Feldhus et al., 2021) exposes\npre-computed feature attribution scores through\nthe Hugging-Face Hub but no features oriented to\nimplement or evaluate XAI. We support the Ther-\nmostat as a third-party add-on and let users test\nand benchmark pre-computed explanations. Un-\nlike our study, Inseq (Sarti et al., 2023) focuses on\npost-hoc interpretability for sequence generation\nmodels. Although researchers can use the library\nto add interpretability evaluations to their models,\nthe toolkit lacks built-in evaluation metrics.\nOther related approaches enable global (rather\nthan local) explainability (Malandri et al., 2022), or\nexplanation interfaces for non-transformers models\non non-NLP tasks (Agarwal et al., 2022). Other\napproaches study model behavior at the subgroup\nlevel (Wang et al., 2021; Goel et al., 2021; Pastor\net al., 2021a,b), focusing more on model evaluation\nand robustness rather than its interpretation.\nVisualization. Most studies that develop visual-\nization tools to investigate the relationships among\nthe input, the model, and the output focus either on\nspecific NLP models - NLPVis (Liu et al., 2018),\nSeq2Seq-Vis (Strobelt et al., 2018), or explainers\n6https://github.com/cdpierse/\ntransformers-interpret\n- BertViz (Vig, 2019), ELI57. LIT (Tenney et al.,\n2020) streamlines exploration and analysis in differ-\nent models. However, it acts mainly as a graphical\nbrowser interface. ferret provides a Python inter-\nface easy to integrate with pre-existing pipelines.\nEvaluation. Although prior works introduced di-\nagnostic properties for XAI techniques, evaluating\nthem in practice remains challenging. Studies ei-\nther concentrate on specific model architectures\n(Lertvittayakumjorn and Toni, 2019; Arras et al.,\n2019; DeYoung et al., 2020), individual datasets\n(Guan et al., 2019; Arras et al., 2019), or a single\ngroup of explainability methods (Robnik-Šikonja\nand Bohanec, 2018; Adebayo et al., 2018). Hence,\nproviding a generally applicable and automated\ntool for choosing the most suitable method is cru-\ncial. To this end, Atanasova et al. (2020) present a\ncomparative study of XAI techniques in three ap-\nplication tasks and model architectures. To the best\nof our knowledge, we are the first to present a user-\nfriendly Python interface to interpret, visualize and\nempirically evaluate models directly from the Hug-\nging Face Hub across several metrics. We extend\nprevious work from DeYoung et al. (2020), who\ndeveloped a benchmark for evaluating rationales on\nNLP models called ERASER by offering a unified\ninterface for evaluation and visual comparison of\nthe explanations at the instance- and dataset-level.\nCloser to ferret, the OpenXAI framework (Agar-\n7https://github.com/TeamHG-Memex/eli5\n261\nwal et al., 2022) enables a systematic evaluation of\nfeature attribution explanation, integrating multiple\nexplainers and XAI structured datasets. OpenXAI\nsupports tabular datasets while we focus on textual\ndata and NLP models.\n5 Conclusions\nWe introduced ferret, a novel Python framework\nto easily access XAI techniques on transformer\nmodels. With ferret, users can explain using state-\nof-the-art post-hoc explainability techniques,evalu-\nate explanations on several metrics for faithfulness\nand plausibility, and easily interact with datasets\nannotated with human rationales.\nWe built ferret with modularity and abstraction\nin mind to facilitate future extensions and contribu-\ntions from the community (see Appendix B for an\noverview of the ongoing development). As future\nwork, we envision off-the-shelf support for new\nNLP tasks and scenarios. Building on the classi-\nfication setup presented in this paper, we plan to\nadd support to more NLP tasks that can be framed\nas classification, such as Mask Filling Prediction,\nNatural Language Inference, Zero-Shot Text Clas-\nsification, Next Sentence Prediction, Token Clas-\nsification, and Multiple-Choice QA. One further\ndirection would be improvingferret’s interoperabil-\nity with new libraries, e.g., Inseq (Sarti et al., 2023)\nfor XAI on text generation tasks and models.\nEthics Statement\nferret’s primary goal is to facilitate the comparison\nof methods that are instead frequently tested in iso-\nlation. Nonetheless, we cannot assume the metrics\nwe currently implement provide a full, exhaustive\npicture, and we work towards enlarging this set\naccordingly.\nFurther, interpretability is much broader than\npost-hoc feature attribution. We focus on this fam-\nily of approaches for their wide adoption and intu-\nitiveness.\nSimilarly, the evaluation measures we integrate\nare based on removal-based criteria. Prior works\npointed out their limitations, specifically the prob-\nlem of erased inputs falling out of the model input\ndistribution (Hooker et al., 2019).\nAcknowledgments\nThis project has partially received funding from\nthe European Research Council (ERC) under the\nEuropean Union’s Horizon 2020 research and in-\nnovation program (grant agreement No. 949944,\nINTEGRATOR), by Fondazione Cariplo (grant No.\n2020-4288, MONICA), and by the grant “National\nCentre for HPC, Big Data and Quantum Comput-\ning”, CN000013 (approved under the M42C Call\nfor Proposals - Investment 1.4 - Notice “National\nCenters” - D.D. No. 3138, 16.12.2021, admitted for\nfunding by MUR Decree No. 1031, 17.06.2022).\nDN and GA are members of the MilaNLP group\nand the Data and Marketing Insights Unit of the\nBocconi Institute for Data Science and Analysis.\nEP did part of the work while at CENTAI and is cur-\nrently a member of the DataBase and Data Mining\nGroup (DBDMG) at Politecnico di Torino. CDB\ncontributed to the work while at Bocconi Univer-\nsity and is currently part of the UKRI Centre for\nDoctoral Training in Safe and Trusted Artificial\nIntelligence (www.safeandtrustedai.org).\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian\nGoodfellow, Moritz Hardt, and Been Kim. 2018. San-\nity checks for saliency maps. Advances in neural\ninformation processing systems, 31.\nChirag Agarwal, Satyapriya Krishna, Eshika Saxena,\nMartin Pawelczyk, Nari Johnson, Isha Puri, Marinka\nZitnik, and Himabindu Lakkaraju. 2022. OpenXAI:\nTowards a transparent evaluation of model explana-\ntions. In Thirty-sixth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks\nTrack.\nLeila Arras, Ahmed Osman, Klaus-Robert Müller, and\nWojciech Samek. 2019. Evaluating recurrent neural\nnetwork explanations. In Proceedings of the 2019\nACL Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP , pages 113–126,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nGiuseppe Attanasio, Debora Nozza, Eliana Pastor, and\nDirk Hovy. 2022. Benchmarking post-hoc inter-\npretability approaches for transformer-based misog-\nyny detection. In Proceedings of NLP Power! The\nFirst Workshop on Efficient Benchmarking in NLP,\npages 100–112, Dublin, Ireland. Association for\nComputational Linguistics.\n262\nFrancesco Barbieri, Luis Espinosa Anke, and José\nCamacho-Collados. 2021. XLM-T: A multilin-\ngual language model toolkit for twitter. CoRR,\nabs/2104.12250.\nSamuel Carton, Anirudh Rathore, and Chenhao Tan.\n2020. Evaluating and characterizing human ratio-\nnales. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 9294–9307, Online. Association for\nComputational Linguistics.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yan-\nnis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A\nsurvey of the state of explainable AI for natural lan-\nguage processing. In Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 447–459, Suzhou, China. Association\nfor Computational Linguistics.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark to\nevaluate rationalized NLP models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4443–4458, Online.\nAssociation for Computational Linguistics.\nNils Feldhus, Robert Schwarzenberg, and Sebastian\nMöller. 2021. Thermostat: A large collection of NLP\nmodel explanations and analysis tools. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, pages 87–95, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\nPathologies of neural models make interpretations\ndifficult. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3719–3728, Brussels, Belgium. Association\nfor Computational Linguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1–6,\nMelbourne, Australia. Association for Computational\nLinguistics.\nKaran Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary\nTaschdjian, Mohit Bansal, and Christopher Ré. 2021.\nRobustness gym: Unifying the NLP evaluation land-\nscape. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies: Demonstrations, pages 42–55, Online. As-\nsociation for Computational Linguistics.\nChaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin\nChen, Di He, and Xing Xie. 2019. Towards a deep\nand unified understanding of deep neural models in\nnlp. In International conference on machine learning,\npages 2454–2463. PMLR.\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,\nand Been Kim. 2019. A benchmark for interpretabil-\nity methods in deep neural networks. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable NLP systems: How should we\ndefine and evaluate faithfulness? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4198–4205, On-\nline. Association for Computational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and\nXiang Ren. 2019. Towards hierarchical importance\nattribution: Explaining compositional semantics for\nneural sequence models. In International Conference\non Learning Representations.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, et al. 2020. Captum: A unified\nand generic model interpretability library for pytorch.\narXiv preprint arXiv:2009.07896.\nPiyawat Lertvittayakumjorn and Francesca Toni. 2019.\nHuman-grounded evaluations of explanation meth-\nods for text classification. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5195–5205, Hong Kong,\nChina. Association for Computational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. CoRR, abs/1612.08220.\nShusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Va-\nlerio Pascucci, and Peer-Timo Bremer. 2018. Vi-\nsual interrogation of attention-based models for nat-\nural language inference and machine comprehen-\nsion. Technical report, Lawrence Livermore National\nLab.(LLNL), Livermore, CA (United States).\nScott M Lundberg and Su-In Lee. 2017. A unified\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\n263\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2022.\nPost-hoc interpretability for neural nlp: A survey.\nACM Comput. Surv. Just Accepted.\nLorenzo Malandri, Fabio Mercorio, Mario Mezzanzan-\nica, Navid Nobani, and Andrea Seveso. 2022. Con-\ntrastive explanations of text classifiers as a service.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nSystem Demonstrations, pages 46–53.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2021. Hatexplain: A benchmark dataset for\nexplainable hate speech detection. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(17):14867–14875.\nEliana Pastor and Elena Baralis. 2019. Explaining black\nbox models by means of local rules. In Proceedings\nof the 34th ACM/SIGAPP Symposium on Applied\nComputing, SAC ’19, page 510–517, New York, NY ,\nUSA. Association for Computing Machinery.\nEliana Pastor, Luca de Alfaro, and Elena Baralis. 2021a.\nLooking for trouble: Analyzing classifier behavior\nvia pattern divergence. In Proceedings of the 2021 In-\nternational Conference on Management of Data, SIG-\nMOD ’21, page 1400–1412, New York, NY , USA.\nAssociation for Computing Machinery.\nEliana Pastor, Andrew Gavgavian, Elena Baralis, and\nLuca de Alfaro. 2021b. How divergent is your data?\nProc. VLDB Endow., 14(12):2835–2838.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nMarco Túlio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"Why Should I Trust You?\": Ex-\nplaining the Predictions of Any Classifier. In Pro-\nceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, San Francisco, CA, USA, August 13-17, 2016 ,\npages 1135–1144. ACM.\nMarko Robnik-Šikonja and Marko Bohanec. 2018.\nPerturbation-based explanations of prediction mod-\nels. In Human and machine learning, pages 159–175.\nSpringer.\nAlexis Ross, Ana Marasovi´c, and Matthew Peters. 2021.\nExplaining NLP models via minimal contrastive edit-\ning (MiCE). In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3840–3852, Online. Association for Computational\nLinguistics.\nSoumya Sanyal and Xiang Ren. 2021. Discretized in-\ntegrated gradients for explaining language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10285–10299, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os-\nkar van der Wal. 2023. Inseq: An interpretabil-\nity toolkit for sequence generation models. ArXiv,\nabs/2302.13942.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2014a. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Workshop Track Proceedings.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2014b. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In 2nd International Conference on Learning\nRepresentations, ICLR 2014.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nHendrik Strobelt, Sebastian Gehrmann, Michael\nBehrisch, Adam Perer, Hanspeter Pfister, and Alexan-\nder M Rush. 2018. S eq 2s eq-v is: A visual debug-\nging tool for sequence-to-sequence models. IEEE\ntransactions on visualization and computer graphics,\n25(1):353–363.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 3319–3328.\nJMLR.org.\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga\nBolukbasi, Andy Coenen, Sebastian Gehrmann,\nEllen Jiang, Mahima Pushkarna, Carey Radebaugh,\nEmily Reif, and Ann Yuan. 2020. The language inter-\npretability tool: Extensible, interactive visualizations\nand analysis for NLP models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 107–118, Online. Association for Computa-\ntional Linguistics.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models. arXiv\npreprint arXiv:1904.02679.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019a. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\n264\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subra-\nmanian, Matt Gardner, and Sameer Singh. 2019b.\nAllenNLP interpret: A framework for explaining\npredictions of NLP models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations, pages\n7–12, Hong Kong, China. Association for Computa-\ntional Linguistics.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng\nZou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui\nZheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li,\nChong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai,\nJun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan,\nYuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin\nZhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong\nPeng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,\nXipeng Qiu, and Xuanjing Huang. 2021. TextFlint:\nUnified multilingual robustness evaluation toolkit for\nnatural language processing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 347–355, Online. Association\nfor Computational Linguistics.\nSarah Wiegreffe and Ana Marasovic. 2021. Teach me to\nexplain: A review of datasets for explainable natural\nlanguage processing. In Thirty-fifth Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track (Round 1).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nFan Yin, Zhouxing Shi, Cho-Jui Hsieh, and Kai-Wei\nChang. 2022. On the sensitivity and stability of\nmodel interpretations in NLP. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2631–2647, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nOmar Zaidan and Jason Eisner. 2008. Modeling an-\nnotators: A generative approach to learning from\nannotator rationales. In Proceedings of the 2008 con-\nference on Empirical methods in natural language\nprocessing, pages 31–40.\nA Technical Details\nA.1 Explainer API\nOur implementation is built on top of original im-\nplementations (as for SHAP and LIME) and open-\nsource libraries (as Captum (Kokhlikyan et al.,\n2020) for gradient-based explainers) to directly ex-\nplain Transformer-based language models.\nCurrently, we integrate Gradient (G) (Simonyan\net al., 2014b), Integrated Gradient (IG) (Sundarara-\njan et al., 2017), SHAP (Lundberg and Lee, 2017),\nand LIME (Ribeiro et al., 2016). For G and IG,\nusers can get explanations from plain gradients or\nmultiply gradients by the input token embeddings.\nFor SHAP, we use the Partition approximation to\nestimate Shapley values.8\nA.1.1 Evaluation API\nWhile human gold annotations are normally dis-\ncrete, current explainers provide continuous token\nattribution scores. Following previous work, we\nhence go from continuous scores to a discrete set of\nrelevant tokens (i.e., rj in Section 2.3) as follows.\nWe consider only tokens with a positive contri-\nbution to the chosen label (i.e., they push the pre-\ndiction towards the chosen label). For the AOPC\ncomprehensiveness and sufficiency measures, the\nrelevant tokens in the discrete rationale are the most\nk% important tokens with kranging from 10% to\n100% (step of 10%). For token-level IOU and F1\nscores plausibility measure, we follow the DeY-\noung et al. (2020) and Mathew et al. (2021) ap-\nproach, and we select the top k tokens where k\nis the average length of human rationales for the\ndataset.\nThe evaluation measures at the dataset level are\nthe average scores across explanations. Differently\nthan DeYoung et al. (2020) that use the F1 IOU\nscore, we directly compute the average token-level\nIOU.\nAll human rationales are at the token level, in-\ndicating the most relevant tokens to a given class\nlabel.\nA.2 Technical Features\nferret implements several functionalities to facili-\ntate end users in using it.\n• High-level interface. Most of ferret’s features,\nsuch as interpretability methods and evalua-\n8https://shap.readthedocs.io/en/latest/\ngenerated/shap.explainers.Partition.html\n265\nFigure 3: Faithfulness and Plausibility metrics averaged across ten samples with the “hateful” label of HateXPlain.\nDarker colors mean better performance.\ntion measures, are accessible via a single entry\npoint, the Benchmarkclass.\n• GPU-enabled batched inference. ferret re-\nquires running inference for certain execu-\ntions. It uses batching and local GPUs trans-\nparently to the user whenever that happens.\n• Visualization methods. The Benchmarkclass\nexposes several methods to visualize attribu-\ntion scores and evaluation results in tabular\nformat. These tables are plotted seamlessly\non Jupyter Notebooks (see Figure 2 (bottom)\nfor an example).\nB Ongoing Development\nferret is under active development. We are extend-\ning the core modules as follows.\nExplainers. We plan to integrate two recent in-\nterpretability methods that require training a com-\nplementary model. Sampling and Occlusion (SOC)\n(Jin et al., 2019) provides a hierarchical explana-\ntion to address compositional contributions. Mini-\nmal Contrastive Editing (MiCE) (Ross et al., 2021)\ntrains a T5 (Raffel et al., 2020) model to imple-\nment contrastive edits to the input to change the\nmodel output. Finally, we are including a third\ngradient-based algorithm. Integrated Discretized\nGradients (Sanyal and Ren, 2021) improve IG sam-\npling intermediate steps close to actual words in\nthe embedding space.\nEvaluators. We plan to include additional evalu-\nation measures such as sensitivity, stability (Yin\net al., 2022), and Area Under the Threshold-\nPerformance curve (AUC-TP) (Atanasova et al.,\n2020).\nC Additional Results\nFigure 3 shows a screenshot of dataset-level assess-\nment from our demo web app. It reports the eval-\nuation metrics averaged across ten samples with\nthe “hate speech” label for the HateXplain dataset,\ndiscussed in Section 3.\nThe user specifies a model from the Hugging\nFace Hub (HF Model field), the target class (Tar-\nget), and the set of samples of interest (List of sam-\nples). ferret web app directly computes explanation\nand their evaluation and visualizes the results.\n266"
}