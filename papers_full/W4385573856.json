{
  "title": "ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models",
  "url": "https://openalex.org/W4385573856",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2130601667",
      "name": "Jiannan Xiang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2251370987",
      "name": "Zhengzhong Liu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2112124531",
      "name": "Zhou Yucheng",
      "affiliations": [
        "UC San Diego Health System"
      ]
    },
    {
      "id": "https://openalex.org/A3168967200",
      "name": "Eric Xing",
      "affiliations": [
        "Carnegie Mellon University",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2156033562",
      "name": "Zhiting Hu",
      "affiliations": [
        "UC San Diego Health System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963912046",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2964085347",
    "https://openalex.org/W3100995786",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2141608913",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3186412754",
    "https://openalex.org/W3177423701",
    "https://openalex.org/W3115328016",
    "https://openalex.org/W3098208302",
    "https://openalex.org/W2921443639",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W2952523122",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W4287727281",
    "https://openalex.org/W2893600504",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W4285174063",
    "https://openalex.org/W2935206035",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2164079290",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2970686438",
    "https://openalex.org/W4206490919",
    "https://openalex.org/W136732505",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W2950397305",
    "https://openalex.org/W3206460194",
    "https://openalex.org/W2963592583",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W1948566616",
    "https://openalex.org/W3026997957",
    "https://openalex.org/W3035565536",
    "https://openalex.org/W1985610876",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W3034987089"
  ],
  "abstract": "Data-to-text generation is challenging due to the great variety of the input data in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse predicates). Recent end-to-end neural methods thus require substantial training examples to learn to disambiguate and describe the data. Yet, real-world data-to-text problems often suffer from various data-scarce issues: one may have access to only a handful of or no training examples, and/or have to rely on examples in a different domain or schema. To fill this gap, we propose Any-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse settings by making efficient use of any given (or no) examples. ASDOT consists of two steps, data disambiguation and sentence fusion, both of which are amenable to be solved with off-the-shelf pretrained language models (LMs) with optional finetuning. In the data disambiguation stage, we employ the prompted GPT-3 model to understand possibly ambiguous triples from the input data and convert each into a short sentence with reduced ambiguity. The sentence fusion stage then uses an LM like T5 to fuse all the resulting sentences into a coherent paragraph as the final description. We evaluate extensively on various datasets in different scenarios, including the zero-/few-/full-shot settings, and generalization to unseen predicates and out-of-domain data. Experimental results show that ASDOT consistently achieves significant improvement over baselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot setting.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1886–1899\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nASDOT : Any-Shot Data-to-Text Generation\nwith Pretrained Language Models\nJiannan Xiang1, Zhengzhong Liu1,3, Yucheng Zhou2, Eric P. Xing1,3,4, Zhiting Hu2\n1Carnegie Mellon University, 2UC San Diego,\n3Petuum Inc., 4Mohamed Bin Zayed University of Artificial Intelligence\n{jiannanx,liu,epxing}@andrew.cmu.edu, {yuz172,zhh019}@ucsd.edu\nAbstract\nData-to-text generation is challenging due to\nthe great variety of the input data in terms of do-\nmains (e.g., finance vs sports) or schemata (e.g.,\ndiverse predicates). Recent end-to-end neural\nmethods thus require substantial training ex-\namples to learn to disambiguate and describe\nthe data. Yet, real-world data-to-text problems\noften suffer from various data-scarce issues:\none may have access to only a handful of or\nno training examples, and/or have to rely on\nexamples in a different domain or schema. To\nfill this gap, we propose Any-Shot Data-to-Text\n(ASDOT ), a new approach flexibly applicable\nto diverse settings by making efficient use of\nany given (or no) examples. ASDOT consists\nof two steps, data disambiguation and sentence\nfusion, both of which are amenable to be solved\nwith off-the-shelf pretrained language models\n(LMs) with optional finetuning. In the data dis-\nambiguation stage, we employ the prompted\nGPT-3 model to understand possibly ambigu-\nous triples from the input data and convert each\ninto a short sentence with reduced ambiguity.\nThe sentence fusion stage then uses an LM like\nT5 to fuse all the resulting sentences into a co-\nherent paragraph as the final description. We\nevaluate extensively on various datasets in dif-\nferent scenarios, including the zero-/few-/full-\nshot settings, and generalization to unseen pred-\nicates and out-of-domain data. Experimental\nresults show that ASDOT consistently achieves\nsignificant improvement over baselines, e.g., a\n30.81 BLEU gain on the DART dataset under\nthe zero-shot setting.1\n1 Introduction\nData-to-text generation (Kukich, 1983a; Reiter and\nDale, 1997) aims at generating natural language\ntext conditioned on structured data content such as\ntables and graphs. The task has a broad range of ap-\nplications such as task-oriented dialog (Wen et al.,\n1Code available at https://github.com/szxiangjn/\nany-shot-data2text\n2015), weather forecasting (Goldberg et al., 1994;\nSripada et al., 2003), sports news reporting (Wise-\nman et al., 2017), and biography generation (Lebret\net al., 2016a; Wang et al., 2018).\nThe problem is challenging in practice due to\nthe vast diversity of the input data in terms of the\ndomains (e.g., finance vs sports), schemata (e.g.,\nthe set of predicates, table structures), etc. The\ninherent ambiguity makes it particularly difficult\nto learn to understand and describe the data. For\ninstance, in the tuple <Fearless, time, 2008>\nfrom a music domain, the predicate word time\nmeans the release time of an album, while in <100\nmetres, time, 9.58> from sports it expresses\nthe world record time. Recent approaches based on\nend-to-end neural models, e.g., by finetuning pre-\ntrained language models (LMs) (Puduppully et al.,\n2019a; Koncel-Kedziorski et al., 2019; Zhao et al.,\n2020), typically require massive training instances\nto resolve the ambiguity and are not applicable to\nmany data-scarce scenarios.\nIn practice, a data-to-text problem of interest\nmay have a varying number of training examples,\nranging from a (small) set to only a few shots,\nor even no examples at all, and sometimes may\nrely on available examples out of the current do-\nmain to facilitate the generation. We refer to the\ndiverse practical scenarios as the any-shot data-to-\ntext problems. Recent work has studied data-to-text\nsolutions when limited examples are available, but\nis often restricted to single specific settings. For\ninstance, Chen et al. (2020b) and Su et al. (2021) fo-\ncused on few-shot problems but fail to apply when\nno examples are accessible, while the zero-shot\nneural pipeline by Kasner and Dusek (2022) re-\nlies on human-crafted templates and thus could not\nhandle out-of-domain data.\nIn this paper, we develop Any-Shot Data-to-\nText (ASDOT ), a new flexible approach that makes\nefficient use of any given (or no) examples and\nachieves stronger generation quality compared to\n1886\nthe prior specific methods. ASDOT draws inspira-\ntion from how humans describe data, namely by\nfirst disambiguating and understanding the data\ncontent, and then fusing and organizing the infor-\nmation together into text paragraphs. As a result,\ngiven input data (e.g., a table or graph), ASDOT\nconsists of two intuitive steps, i.e., data disam-\nbiguation and sentence fusion. Importantly, each\nof the two steps is amenable to be solved with the\nappropriate off-the-shelf pretrained LMs with op-\ntional finetuning, enabling the unique flexibility\nof ASDOT in the presence of any-shot training ex-\namples. More specifically, in data disambiguation\naiming to understand each data entry (e.g., triple\n<Fearless, time, 2008>), we use the prompted\nGPT-3 model (Radford et al., 2019), which has en-\ncoded rich commonsense and world knowledge, to\nconvert the triple into a short sentence (Fearless\nwas released in 2008 ) with greatly reduced\nambiguity. The subsequent sentence fusion stage\nthen uses another LM, such as T5 (Raffel et al.,\n2020), to combine all the resulting sentences into\na coherent paragraph as the final description. The\nsentence fusion as a sub-task allows us to incor-\nporate any available in-/out-of-domain training ex-\namples as well as existing large weakly supervised\ncorpus (Kasner and Dusek, 2022) to finetune the\nLM and boost the performance.\nWe evaluate the proposed approach in a wide\nrange of practical any-shot scenarios, including\n(1) the zero-/few-/full-shot setting where we have\naccess to a varying number of training examples,\n(2) the unseen-predicates setting where we describe\nthe data of new predicates that are never seen in\nthe training examples, and (3) the out-of-domain\nsetting where we are presented only with examples\nfrom other domains. Extensive experiments show\nthat our approach consistently achieves significant\ngains over the diverse previous methods specifically\ndesigned for each of the different scenarios.\n2 Related Work\nData-to-text (D2T) generation is a long-standing\nproblem in natural language processing with broad\napplications in practice. Early research on this task\nfocused on rule-based and pipeline approaches (Ku-\nkich, 1983b; Reiter and Dale, 1997), decomposing\nthe task into text planning, sentence planning, and\nlinguistic realisation. Recent work has developed\nvarious neural approaches. Lebret et al. (2016b)\nused a neural encoder-decoder for the task, fol-\nlowed by attention (Bahdanau et al., 2015), content\nselection (Puduppully et al., 2019a), entity mod-\neling (Puduppully et al., 2019b), and style imita-\ntion (Lin et al., 2020) for further improved per-\nformance. Recent studies have also incorporated\npretrained LMs (Kale and Rastogi, 2020b; Ribeiro\net al., 2021; Clive et al., 2021). Although previous\nfully-supervised methods have achieved remark-\nable performances, most of them require a large\namount of in-domain training examples, leading\nto limited applicability to the common low-data\nscenarios in practice.\nRecent interests are aroused in zero-/few-shot\ndata-to-text generation problems. Chen et al.\n(2020b) first formulated the few-shot setting and\nincorporated a pretrained model with a pointer gen-\nerator as a solution. Chen et al. (2020a) developed a\nknowledge-grounded pretrained LM for both zero-\nand few-shot data-to-text generation. Gong et al.\n(2020) and Chen et al. (2020b) proposed to solve\nthe few-shot task with content matching and pro-\ntotype memory, respectively. There are also stud-\nies on combining templates and pretrained LM for\nzero-/few-shot generation. For example, Kale and\nRastogi (2020a) trained a neural model to rewrite\ntemplates for few-shot task-oriented dialogue. Hei-\ndari et al. (2021) applied the idea of template rewrit-\ning to build a practical few-shot data-to-text system.\nMost of the previous methods have each focused on\na specific setting (e.g., either zero- or few-shot). In\ncomparison, our work studies a wide spectrum of\nany-shot scenarios with a varying number of train-\ning examples from current or different domains. Of\nparticular relevance to our work is the approach by\nKasner and Dusek (2022), which performs zero-\nshot data-to-text generation by rephrasing given\ntemplates. However, the approach relies on human-\nwritten templates for data disambiguation and thus\nhas limited applicability to wide domains. Besides,\nthe approach involves several components (order-\ning, aggregation, compression) to fuse sentences,\nwhich restricts the use of any-shot examples for\nimprovement. The approach thus studies only in\nzero-shot settings, while our work makes a compre-\nhensive study on the diverse any-shot problems.\n3 Any-Shot Data-to-Text Generation\nWe propose ASDOT for any-shot data-to-text gen-\neration. §3.1 describes the any-shot problems. We\nthen provide an overview of our method (§3.2) and\n1887\nPrompt2Table:Michael|birthPlace|USAText:MichaelwasbornintheUSA.……Table:BuzzAldrin|birthPlace|Glen Ridge New JerseyText:\nPrompt1Table:Michael|birthPlace|USAText:MichaelwasbornintheUSA.……Table:Apollo 11 |operator|NASAText:\nbirthPlace\nSelectedByNASABuzz Aldrin\nGlenRidgeNew Jersey\nEssexCountyNew JerseyisPartOfcrewMember\noperatorApollo11\nbackupPilotWilliamAnders1963NASA\n<Apollo 11,operator,NASA><Buzz Aldrin,birthPlace,GlenRidgeNewJersey>……\nDatatriples\nInputData\nApollo is operated by NASA.BuzzAldrinwasborninGlenRidge,NewJersey.…………\nGPT-3\nDataDisambiguation\nSentenceFusion\nFinaloutputBuzz Aldrin was born in Glen Ridge, Essex County, New Jersey. He went on to become a crew member on Apollo 11 which was operated by NASA in 1963. William Anders was the backup pilot for Apollo 11.…\nShort sentences\nPLM(e.g.T5)\nWeakly-supervisedfinetuning\nAny-shotfinetuning\nFigure 1: An overview of our method. Our approach consists of two core steps, i.e., data disambiguation (§3.3) and\nsentence fusion (§3.4). The approach first leverages a prompted GPT-3 to convert each data triple into short sentences\nwith reduced ambiguity. The resulting sentences are then fused by a pretrained LM with optional finetuning using\npublic weakly-supervised corpus or available training examples.\ngive details of each of the components (§3.3, 3.4).\nFigure 1 illustrates our method.\n3.1 The Any-Shot Data-to-Text Problems\nIn the data-to-text generation task, we are given\nstructured data (e.g., a table or graph) as in-\nput, which can be represented as a set of triples\n{x1, x2, ...,xn}. Each triple xi = ⟨si, pi, oi⟩,\nsuch as <Apollo 11, operator, NASA>as in Fig-\nure 1, consists of a subject si, a predicate pi, and\nan object oi, which expresses a relation between\nthe subject and the object. The goal of the task is\nto generate a paragraph consisting of a sequence of\nwords y = {y1, y2, ..., ym}that can describe the\ninput data faithfully and fluently.\nDue to the vast diversity of the content domains,\ndata structures, and predicate sets, etc., building\na data-to-text solution often suffers from insuf-\nficient training examples for learning to under-\nstand/describe the target data. In practice, most\noften we are presented with a varying number of la-\nbeled examples, directly or remotely related to the\ntarget data. For instance, we may need to describe\na table from a financial report on a new website,\nwhere we have no access to any labeled examples\n(i.e., zero-shot) or have access to only a few de-\nscription examples (i.e., few-shot). Besides, the\navailable examples may not even be in the finan-\ncial domain (out of domain), or uses different table\nstructures (different schemata) and different table\nheaders (different predicates). We refer to the data-\nto-text training in the various practical scenarios as\nthe any-shot problem. It is highly desirable to de-\nvelop a general approach that is widely applicable\nto the different settings.\n3.2 Method Overview\nIntuitively, a data-to-text generation process con-\nsists of two core steps, namely, (1) disambiguating\nand understanding the data triples, and (2) produc-\ning the text description. Previous neural approaches\ntypically model the task in an end-to-end manner\nand require a large number of training examples to\nlearn the data-to-text mapping. In contrast, we take\nadvantage of the task structure by formulating the\ntwo stages and solving each with appropriate re-\nsources (e.g., pretrained LMs) that are readily avail-\nable. Figure 1 offers an overview of the approach.\nSpecifically, since each data triple is inherently am-\nbiguous given the compact predicate words, rich\ncommonsense and world knowledge is required to\ncorrectly understand the content. For instance, in\n<Apollo 11, operator, NASA>, a model would\nneed knowledge to determine that NASA operates\nApollo 11rather than the other way around. There-\nfore, in the data disambiguation stage, we leverage\na powerful LM—GPT-3 in our case—that contains\nmassive implicit knowledge in the parameters, to\nconvert each triple into short sentences with re-\nduced ambiguity (e.g., Apollo is operated by\nNASA). Once we collect a set of short sentences,\nin the sentence fusion stage, we use another pre-\ntrained LM with optional finetuning to compose the\nsentences into a well-formed paragraph. The stage\noffers the flexibility to make use of any available\ntraining example to boost performance.\n3.3 Data Disambiguation\nIn this stage, the goal is to generate a short sentence\nto describe each data triple precisely. As above, a\ntriple can be highly abstract and ambiguous as it\n1888\ncompresses complex relational information into the\ncompact format x = ⟨s, p, o⟩, where the predicate\np is often a concise word or phrase (e.g., the pred-\nicate time in triple <Fearless, time, 2008> ).\nTo reduce the ambiguity, we want to “recover” the\nmissing information in the triple by augmenting\nit into a complete sentence (e.g., Fearless was\nreleased in 2008 ). Another advantage of con-\nverting the structured triples into the free-form text\nis that a text sequence is more amenable to the LMs\nused in the subsequent sentence fusion stage (§3.4)\nas described shortly.\nAs the above examples show, augmenting a triple\ninto a sentence naturally requires relevant external\nknowledge (e.g., Fearless is an album). Training\na model specifically for the task could be expen-\nsive and could easily overfit to the training domain.\nInstead, we resort to the general GPT-3 model.\nSpecifically, as shown in Figure 1 (middle panel),\nwe provide GPT-3 with a few demonstrations of\nconverting triples into short sentences, and then\nfeed the target triple to elicit the desired sentence.\nAppendix A shows the complete demonstrations.\nWe found that the same set of four demonstrations\nis sufficient to be used for target data in any domain.\nWe thus use the same prompt consisting of those\ndemonstrations throughout our experiments.\nQuerying the GPT-3 API can be slow and expen-\nsive. Given a set of target data in a domain, we\nreduce the number of queries by generating tem-\nplates. More concretely, for each predicate in the\nset, we sample one triple containing the predicate,\nand generate a sentence for the triple with GPT-3.\nThen we replace the subject and object in the sen-\ntence with placeholders <subject> and <object>\nto get a template. For instance, the template for the\npredicate birthPlace in Figure 1 is \" <subject>\nwas born in <object> \". We then use the tem-\nplate to generate the sentences for all triples with\nthe same predicate.\nIt is worth noting that many existing data-to-text\napproaches, ranging from the classical pipeline\nsolutions (Reiter and Dale, 1997) to the recent\nneural methods (Kale and Rastogi, 2020a; Kas-\nner and Dusek, 2022), have also included similar\ntemplate components, while their templates are typ-\nically crafted by human annotators, making the ap-\nproaches hard to apply to the diverse new domains.\nIn contrast, our ASDOT is fully automated with the\npretrained LMs, without the need of human efforts\nnor training examples.\n3.4 Sentence Fusion\nIn the second stage, we aim to fuse the sentences\nfrom the last step and produce a final coherent\nand fluent paragraph as the output data descrip-\ntion. We naturally formulate the sentence fusion as\na sequence-to-sequence problem, and use the pre-\ntrained LMs, particularly T5 (Raffel et al., 2020), as\nthe backbone for solution. Specifically, we simply\nconcatenate the short sentences, prepended with\na prefix word \" summarize:\", and feed them into\nthe T5 model to obtain the output text. We pick\n\"summarize:\" as the prefix for T5 to mimic its pre-\ntraining configuration, since the sentence fusion\ntask is similar to the summarization task on which\nT5 was pretrained.\nA key advantage of the sentence fusion stage\nis that the component permits easy finetuning\nwith diverse available resources. On one hand,\nthere are automatically constructed weak supervi-\nsion datasets publicly available, such as WikiSplit\n(Botha et al., 2018) mined from Wikipedia’s edit\nhistory and DiscoFuse (Geva et al., 2019) con-\nstructed by rules. In our zero-/few-shot experi-\nments (§4), we finetune the sentence fusion model\nwith the public WikiFluent dataset (Kasner and\nDusek, 2022) which was constructed by applying\na sentence splitting model on the Wikipedia sen-\ntences. On the other hand, one can also use any\nlabeled data-to-text examples (by first converting\nwith the data disambiguation stage), even if the ex-\namples are from different domains. This is because\nthe general sentence fusion task tends to be domain-\nagnostic, since the operations to fuse sentences are\nusually similar across domains, e.g., by inserting\nconnective words or subsuming one sentence as\nthe clause of another. We evaluate in our experi-\nments the out-of-domain generalization ability of\nour approach.\n4 Experiments\n4.1 Datasets\nWe experiment on three widely-used data-to-text\nbenchmarks based on which we study various any-\nshot settings.\nWebNLG (Gardent et al., 2017) consists of\ndata-text pairs where each data is a set of triples\nextracted from DBpedia and the text is written\nby human to describe the data. The dataset is\nsplit into training, validation, and test set, with\n18,102/872/1,862 examples, respectively. The test\nset is further split into the test-seen and test-unseen\n1889\n0 20 40 60 80 100\n#Instances\n0\n10\n20\n30\n40\n50BLEU\nFS-KG\nKGPT\nT5-large\nNeural Pipeline\nASDOT (Ours)\n(a) WebNLG\n0 20 40 60 80 100\n#Instances\n0\n10\n20\n30\n40BLEU\nFS-KG\nKGPT\nT5-large\nASDOT (Ours) (b) DART\nFigure 2: Results of zero-/few-shot learning on WebNLG (left) and DART (right), respectively. The x-axis is the\nnumber of training examples, and the y-axis is the BLEU score. We report results of other metrics in Appendix C.\nNeural Pipeline (Kasner and Dusek, 2022) is applicable only to the zero-shot setting and the specific WebNLG data\ndue to the need of human-written templates on the dataset. Our method show superior performances under any-shot\nsettings. Our approach shows consistent improvement over the baselines, especially when the training size is small.\nWe use paired bootstrap resampling (Koehn, 2004) which confirms that our method is superior to all the baselines at\n95% statistical significance.\nsubsets. The instances in the test-unseen set are\nfrom Wikipedia categories not seen in the training\nset, which is used in our \"unseen predicates\" ex-\nperiments (§4.4). WebNLG contains 354 types of\npredicates in total.\nE2E (Novikova et al., 2017) is a data-to-text\ncorpus in the restaurant domain annotated by hu-\nman. The dataset has 42,061/547/629 examples in\nthe training/validation/test sets, respectively. The\ndataset is relatively easy since it only contains 7\ntypes of predicates and has limited patterns.\nDART (Novikova et al., 2017) is a large\nopen-domain data-to-text corpus, constructed\nfrom WikiSQL (Zhong et al., 2017), WikiTable-\nQuestions (Pasupat and Liang, 2015), as well\nas the WebNLG and E2E datasets. It con-\ntains 62,659/2,768/5,097 examples in the train-\ning/validation/test sets, respectively, and has 4,299\ndifferent predicates in total. Note that the predi-\ncates in DART include those in WebNLG and E2E.\nTo evaluate model generalization to unseen pred-\nicates, we extract a subset of 2,71 test examples\nwhose predicates are completely unseen in the train-\ning/validation sets, leading to a more difficult test-\nunseen set compared to that of WebNLG.\n4.2 Experimental Setup\nFor ASDOT , the data disambiguation stage (§3.3)\nuses the GPT-3 Davinci API provided by OpenAI,\nwith greedy decoding, maximum generation length\n256 and the stop token \" \\n\". Please refer to Ap-\npendix A for the full prompt we use. As discussed\nin Section 3.3, we require only a small number of\nGPT-3 queries by generating one template for each\npredicate. Therefore, we query GPT-3 for 4299\ntimes in total, generating for all the predicates in\nWebNLG, E2E and DART, which costs only $23\nwith the GPT-3 pricing as of 10/21/2022. For the\nsentence fusion stage (§3.4), we use T5 models of\nvarying sizes as the sentence fusion LM. In the zero-\n/few-shot settings (§4.3), we finetune the T5 with\nthe large weakly-supervised data WikiFluent (Kas-\nner and Dusek, 2022) as mentioned in §3.4. We use\nthe Adam optimizer (Kingma and Ba, 2015) with\nan initial learning rate of 3 ×10−5, and use a batch\nsize of 64, for 1 epoch. When any shot of labeled\ndata-to-text examples are available, we further fine-\ntune the sentence fusion T5 with those examples.\nFor the generation, we use beam search decoding\nwith a beam width of 5. We provide more details\nof the experimental setup in the Appendix A.\nEvaluation Metrics Following previous stud-\nies, we report the performance in terms of BLEU\n(Papineni et al., 2002) and METEOR (Banerjee\nand Lavie, 2005), as well as the recent PARENT-\nF1 metric (Dhingra et al., 2019) which measures\nthe alignment between generated text with both\nthe references and input data. We also report\ntwo embedding-based metrics BERTScore (Zhang\net al., 2019) and BLEURT (Sellam et al., 2020)\nin the Appendix C. Besides, we perform human\nevaluation in the few-shot setting as detailed later.\n4.3 Zero-, Few-, to Full-Shot Learning\nWe evaluate ASDOT in the presence of a varying\nnumber of training examples, ranging from 0, 10,\n1890\nModel BLEU METEOR P-F1\nBestPlan 47.24 39.00 -\nPipeline-Trans 51.68 32.00 -\nPlanEnc 52.78 41.00 -\nDataTuner_FC 52.40 42.40 -\nT5-small 56.90 43.05 65.20\n58.64 43.47 66.63ASDOT -small (+1.74) (+0.42) (+1.33)\nT5-base 58.53 43.89 66.82\n60.34 44.37 68.17ASDOT -base (+1.81) (+0.48) (+1.35)\nT5-large 60.38 44.49 68.49\n61.32 44.79 69.69ASDOT -large (+0.94) (+0.30) (+1.20)\nPrefix-Tuning 61.03 44.37 69.17\n61.38 44.52 69.39ASDOT -Prefix (+0.35) (+0.15) (+0.22)\nModel BLEU METEOR P-F1\nLSTM w attention 29.66 27.00 35.00\nE2E Transformer 27.24 25.00 28.00\nBART-base 47.11 38.00 55.00\nBART-large 48.56 39.00 57.00\nT5-small 47.53 39.00 59.33\n49.32 39.57 60.95ASDOT -small (+1.79) (+0.57) (+1.62)\nT5-base 49.62 39.69 61.11\n49.85 39.91 61.64ASDOT -base (+0.23) (+0.22) (+0.53)\nT5-large 50.17 40.00 61.72\n50.79 40.36 62.52ASDOT -large (+0.62) (+0.36) (+0.80)\nPrefix-tuning 50.39 40.13 61.60\n50.56 40.22 62.27ASDOT -Prefix (+0.17) (+0.09) (+0.67)\nTable 1: Full-shot learning results on WebNLG (Left) and DART (Right). ASDOT -X denotes our approach with\nT5-X as the sentence fusion model. The best scores are in bold. We also show the performance gains against\nrespective baseline models in blue.\n20, 50, 100 to the size of the full training set. We\nexperiment on the WebNLG and DART datasets,\nrespectively. In the zero-/few-shot settings, we use\nthe T5-large model for our sentence fusion LM. In\nthe full-shot setting, we test three T5 models of dif-\nferent sizes (small - 60M parameters, base - 220M,\nand large - 770M) for sentence fusion. Besides, the\nrecent Prefix-Tuning method (Li and Liang, 2021)\nshows competitive performances on the data-to-text\ngeneration task. We thus also incorporate it with\nthe T5-large architecture and report the results.\nBaselines In the zero-/few-shot settings, we com-\npare with KGPT (Chen et al., 2020a), a knowledge-\ngrounded LM pretrained on large-scale automati-\ncally constructed data-to-text corpus, as it is one\nof the few methods applicable to both zero-/few-\nshot data-to-text generation. Besides, we compare\nwith FS-KG (Li et al., 2021), a recent few-shot\ndata-to-text approach enhanced with representation\nalignment between knowledge graphs and PLMs.\nWe also compare with the end-to-end model based\non T5-large, which has shown remarkable perfor-\nmance on data-to-text tasks with sufficient training\nexamples (Ribeiro et al., 2020). Following Ribeiro\net al. (2021), for the T5 baseline, we prepend <H>,\n<R> and <T> before the subjects, predicates, and\nobjects, respectively, and add a prefix \"translate\nGraph to English:\" to the input. We finetune the\nT5 model with available shots of training examples.\nOn the WebNLG dataset, we report another base-\nline Neural Pipeline(Kasner and Dusek, 2022),\nwhich is a template-based pipeline method also\ntrained on the WikiFluent dataset and is applica-\nble only to the zero-shot setting. However, the\nmethod cannot be used on the DART dataset since\nits templates are specifically written for WebNLG\nby human.\nIn the full-shot setting, we further compare\nwith a wide range of previous full-shot state-\nof-the-art data-to-text systems, including Best-\nPlan (Moryossef et al., 2019), Pipeline-Trans (Cas-\ntro Ferreira et al., 2019), PlanEnc (Zhao et al.,\n2020), DataTuner_FC (Harkous et al., 2020) on\nWebNLG, and LSTM-with-attention, End-to-End\nTransformers, and BART-base/large (Nan et al.,\n2020) on DART.\nAutomatic Evaluation The zero-/few-shot re-\nsults are shown in Figure 2. Our method con-\nsistently outperforms baseline models on both\ndatasets, demonstrating its strong zero-/few-shot\nlearning ability. In particular, with fewer training\nexamples, our ASDOT tends to outperform other\nmethods by a larger margin. For instance, we\nachieve 16.06 higher BLEU than T5-large on 10-\nshot WebNLG, and 10.53 higher on 10-shot DART.\nThis is because the two-stage ASDOT is designed\nto excel in the low-data contexts by augmenting the\ngeneration process with rich external knowledge\nin pretrained LMs. Neural Pipeline is competi-\ntive with ours, but is restricted only to the zero-\nshot setting on WebNLG. DART contains more\ndiverse types of predicates and thus is arguably\n1891\nModel Faithfulness ↑ Contradict ↓ Fluency ↑\nKGPT 0.64 2.34 1.00\nT5-large 2.22 0.72 1.58\nASDOT 2.37 0.67 1.82\nTable 2: Human evaluation results. ↑means the higher\nthe better and ↓means the lower the better. ASDOT\noutperforms the baselines with p < 0.05 in Tukey’s\nHSD test for all the measures.\nmore challenging than WebNLG. Our approach\ntends to achieve stronger performance gains on the\ndifficult dataset.\nWe report the results of the full-shot setting in\nTable 1. The performance gain tends to be less\nsignificant compared to the zero-/few-shot settings\nas all methods are presented with a large number\nof training examples. However, our method still\nachieves consistently stronger performance over\nthe large diversity of baselines, thanks to ASDOT ’s\nproper modeling of the generation process and the\nincorporation of rich external implicit knowledge.\nHuman Evaluation We conduct a human eval-\nuation to further assess our ASDOT against other\nbaselines under the 50-shot setting on WebNLG.\nAfter training, we sample 50 test instances and ask\nthree proficient English speakers in the university\nto score the model outputs. Following Chen et al.\n(2020b), each generated result is evaluated on three\naspects: the number of the facts that are consistent\nwith the input table (Faithfulness) and contradicted\nto the table (Contradict), and the language fluency,\non a 3-Likert scale (0,1,2). The results are shown\nin Table 2. The Krippendorff alphas (Krippendorff,\n2011) for Faithfulness, Contradict, and language\nfluency are 0.49, 0.42 and 0.36, respectively, indi-\ncating a fair inner-annotator agreement. Consistent\nwith the automatic evaluation results, we observe\nthat ASDOT is substantially better than the base-\nlines on all the three aspects, suggesting that our\napproach generates more faithful and fluent descrip-\ntions.\nAblation Studies We conduct ablation studies\nto investigate the effects of both the data disam-\nbiguation and sentence fusion stages. Table 3\nshows the results. Specifically, for the sentence\nfusion stage, we study the effect of the weakly-\nsupervised finetuning on the WikiFluent corpus\n(§3.4). From the table, we can see that the perfor-\nmance drops sharply without weakly-supervised\nfinetuning, i.e., by 8.86 BLEU points for the zero-\nModel 0 10 20 50 100\nKGPT 14.19 17.50 18.40 21.68 24.72\nT5-large 10.46 29.10 41.38 46.24 48.68\nASDOT 43.33 45.16 47.46 49.36 49.39\n- w/o weak-sup 34.47 39.38 43.67 47.56 48.16\n- w/ manual templ. 42.02 43.37 46.12 48.28 48.32\nTable 3: Ablation results (BLEU) for zero-/few-shot\nlearning on WebNLG. The w/o weak-sup row shows the\nresults of ASDOT without weakly supervised finetuning,\nand w/ manual templ. shows the results of using hand-\ncrafted templates in the data disambiguation stage.\nModel BLEU METEOR P-F1\nBestPlan 34.41 37.00 -\nPipeline-Trans 38.92 21.00 -\nPlanEnc 38.23 37.00 -\nT5-small 47.34 39.95 57.99\n50.75 40.63 61.20ASDOT -small (+3.41) (+0.68) (+3.21)\nT5-base 51.11 41.42 60.94\n54.51 42.30 64.36ASDOT -base (+3.40) (+0.88) (+3.42)\nT5-large 53.97 42.37 63.81\n55.74 42.94 65.90ASDOT -large (+1.77) (+0.57) (+2.09)\nPrefix-Tuning 55.26 42.42 65.24\n55.86 42.73 65.68ASDOT -Prefix (+0.60) (+0.31) (+0.44)\nTable 4: Results on WebNLG test-unseen set.\nshot setting. However, ASDOT without weak super-\nvision still outperforms the baselines in most cases,\nvalidating the strong advantage of our approach un-\nder low-data settings. For the data disambiguation\nstage, we investigate the impact of the automatic\ntemplates produced by GPT-3. More concretely,\nwe replace the GPT-3 templates with the human-\nwritten templates from Kasner and Dusek (2022).\nThe performance is similar or decreases slightly,\ndemonstrating that the short sentences or templates\nautomatically generated in the data disambiguation\nstage are of competitive or slightly higher quality\nthan the manually created ones (perhaps due to\nhuman errors when writing the hundreds of tem-\nplates).\n4.4 Generating for Unseen Predicates\nWe now assess the model’s capability of describing\nnew predicates that are never seen during training.\nAs mentioned in §4.1, WebNLG provides such an\nofficial test-unseen set for the evaluation and we\nconstruct a similar (but more difficult) test set on\nDART where all the test predicates are not included\nin training. We train the models on WebNLG\n1892\nModel BLEU METEOR P-F1\nT5-small 37.65 33.27 43.79\n46.60 36.91 52.17ASDOT -small (+8.95) (+3.64) (+8.38)\nT5-base 46.13 36.97 49.79\n50.90 37.72 54.98ASDOT -base (+4.77) (+0.75) (+5.19)\nT5-large 46.37 36.49 50.32\n50.70 37.25 55.49ASDOT -large (+4.33) (+0.76) (+5.17)\nPrefix-tuning 47.07 36.69 49.67\n51.99 38.11 57.26ASDOT -Prefix (+4.92) (+1.42) (+7.59)\nTable 5: Results on DART test-unseen set.\nTest set Model B M P\nE2E\nT5-large 33.23 35.40 60.18\n35.51 35.98 60.06ASDOT (+2.28) (+0.58) (–0.12)\nDART\nT5-large 25.94 33.64 33.50\n30.42 35.30 36.60ASDOT (+4.48) (+1.66) (+3.10)\nTable 6: Out-of-Domain results. B, M and P represent\nBLEU, METEOR and PARENT-F1, respectively.\nand DART, and evaluate on the corresponding test-\nunseen sets, respectively. As in §4.3, we compare\nASDOT with the respective end-to-end T5 mod-\nels (small, base, large, prefix-tuning). We also\ninclude the previously reported baseline results\non the WebNLG test-unseen set, including Best-\nPlan (Moryossef et al., 2019), Pipeline-Trans (Cas-\ntro Ferreira et al., 2019) and PlanEnc (Zhao et al.,\n2020). The experimental results are shown in Ta-\nble 4 and Table 5, respectively. As can be seen,\nour method achieves consistent improvements over\nall the baseline methods, showing the robustness\nof our method to unseen predicates given the rich\ncommonsense and world knowledge introduced\nthrough the pretrained LMs in both stages. The\nsuperior performance of ASDOT over the corre-\nsponding end-to-end T5 again demonstrates the\nadvantage of our modularization that applies to and\nimproves various pretrained LMs. Similar as in\nthe zero-/few-shot experiments, here we observe\nthat on the more difficult DART test-unseen set\nwith more unseen predicates, our method achieves\nmore significant gains than on WebNLG, which\nfurther shows the advantage of our method when\ngeneralizing to unseen predicates.\n4.5 Learning with Out-of-Domain Examples\nAt last, we quantitatively measure the generaliza-\ntion ability of our approach across domains. To\nSource<Zolder, fastest Lap, Liverpool F.C.> ; <Zolder,Date, October 5>DisambigLiverpool F.C. set the fastest lap in the Zolder.Zolder was on October 5.FusionLiverpool F.C. set the fastest lap in the Zolder onOctober 5.BaselineZolder’s fastest lap is Liverpool F.C. and the dateis October 5.HumanOn October 5, 2008, Liverpool F.C. got the fastestlap at a Zolder race.Source<Aleksandra Kovac, associated Band/associatedMusical Artist, Bebi Dol> ; <Aleksandra Kovac,associated Band/associated Musical, ArtistK2 Kovacsisters duo>DisambigAleksandra Kovac is associated with Bebi Dol.Aleksandra Kovac is associated with K2 Kovac sistersduo.FusionAleksandra Kovac is associated with Bebi Dol and theK2 Kovac sisters duo.BaselineAleksandra Kovac is an associated band/associatedmusical artist with Bebi Dol and the K2 Kovac sistersduo.HumanAleksandra Kovac is associated with the musicalartist Bebi Dol and is part of the band K2 Kovacsisters duo.\nTable 7: Qualitative examples in the out-of-domain (top)\nand unseen-predicates (bottom) settings.\nsimulate the out-of-domain setting, we train our\nmodel on the WebNLG dataset and evaluate it\non the test sets of DART and E2E, respectively.\nThe DART test set includes the instances from the\nWebNLG and E2E test sets. We remove those in-\nstances to avoid any in-domain test examples (w.r.t\nthe WebNLG training examples) and any overlap\nwith E2E evaluation. We compare our method with\nthe end-to-end finetuned T5-large model. The ex-\nperimental results in Table 6 show that our method\noutperforms the baseline models on both out-of-\ndomain test sets, echoing the conclusions in pre-\nvious experiments that our approach with the two-\nstage design and integration of pretrained LMs has\na superior generalization ability to handle data-to-\ntext generation in any-shot scenarios.\n4.6 Case Study\nTable 7 shows the outputs of our ASDOT (based on\nT5-large) after the data disambiguation stage and\nthe sentence fusion stage, on two data in the out-\nof-domain and unseen-predicates settings, respec-\ntively. The generated words corresponding to dif-\nferent data triples are highlighted in different colors\n(as in Figure 1). We also provide the results of the\nT5-large baseline and the human-written references.\nAs can be seen, ASDOT develops a strong gener-\nalization ability to out-of-domain data and unseen\npredicates. In the first example, ASDOT success-\nfully disambiguates the triple <Zolder, fastest\nLap, Liverpool F.C.> into \"Liverpool F.C.\nset the fastest lap in the Zolder \" while\nthe T5 baseline fails to do so and simply generates\n1893\n\"Zolder’s faster lap in Liverpool F.C. \".\nAlso, in the second example, the baseline directly\ncopies \"associated Band/associated Musical\nArtist\" in the output while ASDOT correctly con-\nverts it into \"is associated with\".\n5 Conclusion\nWe have proposed ASDOT to deal with the diverse\nany-shot problems for data-to-text generation. AS-\nDOT is composed of two stages, data disambigua-\ntion that uses prompted GPT-3 to disambiguate\ninput data triples into short sentences, and sentence\nfusion using state-of-the-art pretrained LMs to fuse\nthese sentences into the desired paragraphs. In the\nprocess, ASDOT integrates rich external implicit\nknowledge from the large LMs, which ensures\nstrong generalization capability and broad appli-\ncability to zero-/few-/full-shot, unseen-predicates,\nand out-of-domain training scenarios. Extensive ex-\nperiments show our approach consistently achieves\nsignificant improvements over diverse baselines.\nLimitations\nOne limitation of our approach is that the data dis-\nambiguation stage is done by the GPT-3 model\nlocally, i.e., the GPT-3 model only observes one\ntriple and does not utilize the full-table informa-\ntion. In some difficult cases, the full-table context\nmay be needed for disambiguation. Besides, in this\nwork we directly use the output from GPT-3’s as the\nfinal disambiguation results, which may be prob-\nlematic since GPT-3 may not always provide the\ncorrect templates, especially when working with\nhighly-specialized domains. In addition, our cur-\nrent approach can only be applied to languages that\nhave access to large LMs.\nEthics Statement\nWe are aware of the ACL Code of Ethics and the\nACM Code of Ethics and Professional Conduct and\nstrictly adhere to the rules throughout the course of\nthis research.\nOur research does not present any new datasets\nbut introduces a new algorithm for data-to-text gen-\neration, which generates text descriptions for a\ngiven graph or table. The intended usage of the\nwork may potentially provide benefits to people\nwith difficulties in reading graphs or tables, such\nas people with visual impairment. We do not antic-\nipate direct harm with the intended usage.\nSimilar to most generation systems, if harmful\ninput, such as unethical text or input designed for\nadversarial attacks, exists, our approach is likely\nto generate unintended output. Therefore, we do\nnot recommend usages of our approach outside\ncontrolled research environment before these risks\nare mitigated. We would also like to point out\nthat a naive deployment of our method may allow\nmalicious exploitation of the backbone Large LMs,\nthus precautions such as a filtering mechanism need\nto be implemented.\nOur model makes use of the common sense rea-\nsoning ability of large LMs, which may reinforce\nexisting social stereotypes, hence care must be\ntaken when applying this approach to materials\n(e.g. tables and graphs) that are sensitive to popula-\ntions that already experience marginalization.\nComputation-wise, our finetuning procedure\ntakes around 1836 GPU/Hours on NVIDIA\nGeForce RTX 3090 Ti GPUs. Throughout the\nstudy, our prompting module makes about 4600\nAPI calls to Open-AI’s GPT-3 API.\nReferences\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR\n2015.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nJan A Botha, Manaal Faruqui, John Alex, Jason\nBaldridge, and Dipanjan Das. 2018. Learning to split\nand rephrase from wikipedia edit history. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 732–737.\nThiago Castro Ferreira, Chris van der Lee, Emiel\nvan Miltenburg, and Emiel Krahmer. 2019. Neu-\nral data-to-text generation: A comparison between\npipeline and end-to-end architectures. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 552–562, Hong\nKong, China. Association for Computational Lin-\nguistics.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang\nWang. 2020a. KGPT: Knowledge-grounded pre-\n1894\ntraining for data-to-text generation. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 8635–\n8648, Online. Association for Computational Lin-\nguistics.\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\nand William Yang Wang. 2020b. Few-shot NLG\nwith pre-trained language model. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 183–190, Online.\nAssociation for Computational Linguistics.\nJordan Clive, Kris Cao, and Marek Rei. 2021. Con-\ntrol prefixes for text generation. arXiv preprint\narXiv:2110.08329.\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-\nWei Chang, Dipanjan Das, and William Cohen. 2019.\nHandling divergent reference texts when evaluating\ntable-to-text generation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4884–4895, Florence, Italy. Asso-\nciation for Computational Linguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating training\ncorpora for NLG micro-planners. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 179–188, Vancouver, Canada. Association for\nComputational Linguistics.\nMor Geva, Eric Malmi, Idan Szpektor, and Jonathan\nBerant. 2019. DiscoFuse: A large-scale dataset for\ndiscourse-based sentence fusion. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3443–3455, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nEli Goldberg, Norbert Driedger, and Richard I Kittredge.\n1994. Using natural-language processing to produce\nweather forecasts. IEEE Expert, 9(2):45–53.\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing\nQin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020.\nTableGPT: Few-shot table-to-text generation with\ntable structure reconstruction and content matching.\nIn Proceedings of the 28th International Conference\non Computational Linguistics , pages 1978–1988,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nHamza Harkous, Isabel Groves, and Amir Saffari. 2020.\nHave your text and use it too! end-to-end neural data-\nto-text generation with semantic fidelity. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 2410–2424, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nPeyman Heidari, Arash Einolghozati, Shashank Jain,\nSoumya Batra, Lee Callender, Ankit Arun, Shawn\nMei, Sonal Gupta, Pinar Donmez, Vikas Bhardwaj,\net al. 2021. Getting to production with few-shot\nnatural language generation models. In Proceedings\nof the 22nd Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, pages 66–76.\nMihir Kale and Abhinav Rastogi. 2020a. Template\nguided text generation for task-oriented dialogue. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6505–6520.\nMihir Kale and Abhinav Rastogi. 2020b. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nZdenˇek Kasner and Ondrej Dusek. 2022. Neural\npipeline for zero-shot data-to-text generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3914–3932, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nPhilipp Koehn. 2004. Statistical significance tests for\nmachine translation evaluation. In Proceedings of the\n2004 Conference on Empirical Methods in Natural\nLanguage Processing, pages 388–395, Barcelona,\nSpain. Association for Computational Linguistics.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019. Text\ngeneration from knowledge graphs with graph trans-\nformers. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2284–2293.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability. Computing, 1:25–2011.\nKaren Kukich. 1983a. Design of a knowledge-based re-\nport generator. In 21st Annual Meeting of the Associ-\nation for Computational Linguistics, pages 145–150.\nKaren Kukich. 1983b. Design of a knowledge-based\nreport generator. In 21st Annual Meeting of the As-\nsociation for Computational Linguistics, pages 145–\n150, Cambridge, Massachusetts, USA. Association\nfor Computational Linguistics.\nRémi Lebret, David Grangier, and Michael Auli. 2016a.\nNeural text generation from structured data with ap-\nplication to the biography domain. arXiv preprint\narXiv:1603.07771.\nRémi Lebret, David Grangier, and Michael Auli. 2016b.\nNeural text generation from structured data with ap-\nplication to the biography domain. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1203–1213, Austin,\nTexas. Association for Computational Linguistics.\n1895\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei,\nNicholas Jing Yuan, and Ji-Rong Wen. 2021. Few-\nshot knowledge graph-to-text generation with pre-\ntrained language models. In Findings of the Associa-\ntion for Computational Linguistics: ACL-IJCNLP\n2021, pages 1558–1568, Online. Association for\nComputational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597.\nShuai Lin, Wentao Wang, Zichao Yang, Xiaodan Liang,\nFrank F. Xu, Eric Xing, and Zhiting Hu. 2020. Data-\nto-text generation with style imitation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 1589–1598, Online. Association\nfor Computational Linguistics.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2267–2277, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, et al.\n2020. Dart: Open-domain structured data record to\ntext generation. arXiv preprint arXiv:2007.02871.\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue ,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019a.\nData-to-text generation with content selection and\nplanning. In Proceedings of the AAAI conference on\nartificial intelligence, volume 33, pages 6908–6915.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019b.\nData-to-text generation with entity modeling. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2023–\n2035, Florence, Italy. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nEhud Reiter and Robert Dale. 1997. Building applied\nnatural language generation systems. Natural Lan-\nguage Engineering, 3(1):57–87.\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\nSchütze, and Iryna Gurevych. 2021. Investigating\npretrained language models for graph-to-text genera-\ntion. In Proceedings of the 3rd Workshop on Natural\nLanguage Processing for Conversational AI, pages\n211–227, Online. Association for Computational Lin-\nguistics.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze,\nand Iryna Gurevych. 2020. Investigating pretrained\nlanguage models for graph-to-text generation. arXiv\npreprint arXiv:2007.08426.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBleurt: Learning robust metrics for text generation.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7881–\n7892.\nSomayajulu Sripada, Ehud Reiter, and Ian Davy. 2003.\nSumtime-mousam: Configurable marine weather\nforecast generator. Expert Update, 6(3):4–10.\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel\nCollier. 2021. Few-shot table-to-text generation with\nprototype memory. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n910–917.\nQingyun Wang, Xiaoman Pan, Lifu Huang, Boliang\nZhang, Zhiying Jiang, Heng Ji, and Kevin Knight.\n2018. Describing a knowledge base. arXiv preprint\narXiv:1809.01797.\n1896\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-\nHao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned lstm-based natural lan-\nguage generation for spoken dialogue systems. arXiv\npreprint arXiv:1508.01745.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document genera-\ntion. arXiv preprint arXiv:1707.08052.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nChao Zhao, Marilyn Walker, and Snigdha Chaturvedi.\n2020. Bridging the structural gap between encoding\nand decoding for data-to-text generation. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 2481–2491.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\n1897\nA GPT-3 Prompt\nThe prefix in the prompt we use is:\nTable: Michael | birth Place | USA\nText: Michael was born in the USA.\nTable: First Clearing | location | On NYS\n52 1 Mi. Youngsville\nText: First Clearing is located at On NYS 52 1 Mi.\nYoungsville.\nTable: Abilene Regional Airport | city Served |\nAbilene Texas\nText: Abilene Regional Airport serves Abilene\nTexas.\nTable: Alfred Moore Scales | active Years\nStart Date | 1875-03-04\nText: Alfred Moore Scales started to be active on\n1875-03-04.\nB Experimental Details\nWe use a batch size of 5 and a beam search size\nof 5 for zero-shot and few-shot settings. For other\nsettings, we do model selection based on the per-\nformance on the validation set, with a batch size\nchosen from {2, 4, 8} and {1, 3, 5}, respectively.\nWe use sacreBLEU (Post, 2018) for model selec-\ntion. The URL for the metrics and corpus we use\nare shown in Table 8 and Table 9, respectively.\nC Zero-/Few-shot Experimental Results\nWe show the BLEU/METEOR/PARENT-F1 scores\nfor zero-/few-shot experiments on WebNLG\nand DART in Table 10 and Table 12, and\nBERTScore/BLEURT in Table 11 and Table 13.\nMetric URL\nBLEU https://github.com/\nmoses-smt/mosesdecoder/\nblob/master/scripts/\ngeneric/multi-bleu.perl\nMETEOR https://www.cs.cmu.edu/\n~alavie/METEOR/index.html\nPARENT https://github.com/\nKaijuML/parent\nBERTScore https://github.com/\nTiiiger/bert_score\nBLEURT https://github.com/\ngoogle-research/bleurt\nSacreBLEU https://github.com/mjpost/\nsacrebleu\nTable 8: The URLs for the metrics we use in the experi-\nments.\nDataset URL\nWebNLG https://gitlab.com/\nshimorina/webnlg-dataset/\n-/tree/master/webnlg_\nchallenge_2017\nDART https://github.com/\nYale-LILY/dart\nE2E https://github.com/\ntuetschek/e2e-dataset\nWikiFluent https://github.\ncom/kasnerz/\nzeroshot-d2t-pipeline\nTable 9: The URLs for the corpus we use in the experi-\nments.\n1898\n#Instance 0 10 20 50 100\nKGPT 14.19/20.78/20.67 17.50/23.13/ 25.77 18.40/23.44/26.49 21.68/25.30/29.22 24.72/26.71/46.50\nT5-large 10.46/25.63/23.67 24.74/32.28/42.48 41.38/36.12/52.77 45.32/39.49/59.39 48.68/39.24/60.66\nASDOT 43.99/39.32/58.23 45.16/38.95/58.24 47.46/39.35/59.85 49.36/40.08/61.25 49.39/40.09/61.08\n- w/o weak-sup 34.47/30.06/51.51 39.38/33.93/56.44 43.67/35.81/57.99 47.56/38.61/60.04 48.60/39.68/60.56\n- w/ manual templ. 42.02/38.85/58.26 43.37/38.69/58.80 46.12/38.88/60.94 48.28/39.64/62.02 48.32/39.32/61.92\nTable 10: WebNLG few-shot results. x / y / z denotes the model performance on BLEU / METEOR / PARENT-F1.\n#Instance 0 10 20 50 100\nKGPT 85.35/43.78 88.62/49.67 88.92/49.41 89.66/52.72 90.30/55.15\nT5-large 84.17/40.19 93.00/67.49 92.87/65.93 93.06/66.48 93.24/67.05\nASDOT 92.43/71.93 94.39/72.45 94.69/73.48 95.03/74.62 94.99/74.66\n- w/o weak-sup 92.01/66.43 93.05/67.84 93.10/67.32 93.54/68.10 93.93/68.05\n- w/ manual templ. 92.36/71.01 94.17/72.08 94.27/72.91 94.58/74.11 94.61/74.33\nTable 11: WebNLG few-shot results. x / y denotes the model performance on BERTScore / BLEURT.\n#Instance 0 10 20 50 100\nKGPT 11.15/19.30/18.92 14.91/19.74/23.76 16.83/21.30/26.67 20.16/23.14/31.13 20.31/23.82/31.35\nT5-large 8.43/22.67/23.81 29.97/31.44/46.82 32.96/31.76/47.36 37.08/34.43/54.10 39.92/34.90/55.05\nASDOT 38.81/36.91/54.10 40.50/36.65/56.00 41.45/36.45/57.34 42.33/36.99/57.63 42.87/36.77/58.37\n- w/o weak-sup 31.92/26.15/43.99 38.15/32.11/54.97 37.12/32.80/54.12 40.79/35.70/56.40 41.22/35.15/57.79\nTable 12: DART few-shot results. x / y / z denotes the model performance on BLEU / METEOR / PARENT-F1.\n#Instance 0 10 20 50 100\nKGPT 84.32/43.21 87.13/48.94 88.54/49.22 89.43/52.13 89.96/53.99\nT5-large 83.53/40.01 88.73/66.37 89.43/66.65 90.39/66.79 90.51/66.96\nASDOT 90.13/69.87 91.52/69.88 91.67/70.10 91.90/70.46 92.01/70.61\n- w/o weak-sup 88.94/67.96 90.21/68.13 90.44/68.37 90.56/68.46 90.84/68.66\nTable 13: DART few-shot results. x / y denotes the model performance on BERTScore / BLEURT.\n1899",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8622266054153442
    },
    {
      "name": "Sentence",
      "score": 0.6420707106590271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6285353899002075
    },
    {
      "name": "Natural language processing",
      "score": 0.5482029914855957
    },
    {
      "name": "Ambiguity",
      "score": 0.529051661491394
    },
    {
      "name": "Paragraph",
      "score": 0.483752965927124
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4353214502334595
    },
    {
      "name": "Language model",
      "score": 0.43028950691223145
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800935791",
      "name": "UC San Diego Health System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}