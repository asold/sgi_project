{
  "title": "Pre-trained Language Model Representations for Language Generation",
  "url": "https://openalex.org/W2951560313",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282315077",
      "name": "Edunov, Sergey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221724799",
      "name": "Baevski, Alexei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359259",
      "name": "Auli, Michael",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2951603207",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2920812691",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2170973209"
  ],
  "abstract": "Pre-trained language model representations have been successful in a wide range of language understanding tasks. In this paper, we examine different strategies to integrate pre-trained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization. We find that pre-trained representations are most effective when added to the encoder network which slows inference by only 14%. Our experiments in machine translation show gains of up to 5.3 BLEU in a simulated resource-poor setup. While returns diminish with more labeled data, we still observe improvements when millions of sentence-pairs are available. Finally, on abstractive summarization we achieve a new state of the art on the full text version of CNN/DailyMail.",
  "full_text": "arXiv:1903.09722v2  [cs.CL]  1 Apr 2019\nPre-trained Language Model Representations for Language Generation\nSergey Edunov∗, Alexei Baevski∗, Michael Auli\nFacebook AI Research\nMenlo Park, CA\nAbstract\nPre-trained language model representations\nhave been successful in a wide range of lan-\nguage understanding tasks. In this paper, we\nexamine different strategies to integrate pre-\ntrained representations into sequence to se-\nquence models and apply it to neural ma-\nchine translation and abstractive summariza-\ntion. W e ﬁnd that pre-trained representa-\ntions are most effective when added to the en-\ncoder network which slows inference by only\n14%. Our experiments in machine translation\nshow gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish\nwith more labeled data, we still observe im-\nprovements when millions of sentence-pairs\nare available. Finally, on abstractive summa-\nrization we achieve a new state of the art on\nthe full text version of CNN-DailyMail.\n1\n1 Introduction\nPre-training of language models has been shown\nto provide large improvements for a range\nof language understanding tasks (\nPeters et al. ,\n2018; Radford et al. , 2018; Phang et al. , 2018;\nDevlin et al. , 2018). The key idea is to train a\nlarge generative model on vast corpora and use\nthe resulting representations on tasks for which\nonly limited amounts of labeled data is available.\nPre-training of sequence to sequence models has\nbeen previously investigated for text classiﬁca-\ntion (\nDai and Le , 2015) but not for text genera-\ntion. In neural machine translation, there has been\nwork on transferring representations from high-\nresource language pairs to low-resource settings\n(\nZoph et al. , 2016).\nIn this paper, we apply pre-trained representa-\ntions from language models to language genera-\n∗ Equal contribution.\n1 Code and pre-trained models are available at\nhttps://github.com/pytorch/fairseq/tree/\nbi_trans_lm/examples/pretraining\ntion tasks that can be modeled by sequence to se-\nquence architectures. Previous work on integrat-\ning language models with sequence to sequence\nmodels focused on the decoder network and added\nlanguage model representations right before the\noutput of the decoder (\nGulcehre et al. , 2015). W e\nextend their study by investigating several other\nstrategies such as inputting ELMo-style represen-\ntations (\nPeters et al. , 2018) or ﬁne-tuning the lan-\nguage model ( §2).\nOur experiments rely on strong transformer-\nbased language models trained on up to six bil-\nlion tokens ( §\n3). W e present a detailed study of\nvarious strategies in different simulated labeled\ntraining data scenarios and observe the largest im-\nprovements in low-resource settings but gains of\nover 1 BLEU are still possible when ﬁve million\nsentence-pairs are available. The most successful\nstrategy to integrate pre-trained representations is\nas input to the encoder network ( §\n4).\n2 Strategies to add representations\nW e consider augmenting a standard sequence to\nsequence model with pre-trained representations\nfollowing an ELMo-style regime ( §\n2.1) as well as\nby ﬁne-tuning the language model ( §2.2).\n2.1 ELMo augmentation\nThe ELMo approach of\nPeters et al. (2018) forms\ncontextualized word embeddings based on lan-\nguage model representations without adjusting\nthe actual language model parameters. Speciﬁ-\ncally , the ELMo module contains a set of parame-\nters λ1 . . . λL, γ to form a linear combination of\nthe L layers of the language model: ELMo =\nγ ∑ L\ni=0\n1\nZ exp(λi)hk where γ is a learned scalar,\nZ is a constant to normalize the exp(λi) to sum\nto one and hk is the output of the k-th language\nmodel layer; the module also considers the input\nword embeddings of the language model. W e also\napply layer normalization ( Ba et al. , 2016) to each\nhk before computing ELMo vectors.\nW e experiment with an ELMo module to input\ncontextualized embeddings either to the encoder\n(SR C -ELM O ) or the decoder ( TG T-ELM O ). This\nprovides word representations speciﬁc to the cur-\nrent input sentence and these representations have\nbeen trained on much more data than is available\nfor the text generation task.\n2.2 Fine-tuning approach\nFine-tuning the pre-trained representations ad-\njusts the language model parameters by the learn-\ning signal of the end-task (\nRadford et al. , 2018;\nDevlin et al. , 2018). W e replace learned input\nword embeddings in the encoder network with the\noutput of the language model ( SR C -FT ). Speciﬁ-\ncally , we use the language model representation of\nthe layer before the softmax and feed it to the en-\ncoder. W e also add dropout to the language model\noutput. Tuning separate learning rates for the lan-\nguage model and the sequence to sequence model\nmay lead to better performance but we leave this\nto future work. However, we do tune the number\nof encoder blocks N as we found this important to\nobtain good accuracy for this setting. W e apply the\nsame strategy to the decoder: we input language\nmodel representations to the decoder network and\nﬁne-tune the language model when training the se-\nquence to sequence model ( TG T-FT ).\n3 Experimental setup\n3.1 Datasets\nPre-training. W e train language models on two\nlanguages: One model is estimated on the Ger-\nman newscrawl distributed by WMT’18 compris-\ning 260M sentences or 6B tokens. Another model\nis trained on the English newscrawl data compris-\ning 193M sentences or 5B tokens. W e learn a joint\nByte-Pair-Encoding (BPE; Sennrich et al., 2016)\nvocabulary of 37K types on the German and En-\nglish newscrawl and train the language models\nwith this vocabulary .\nMachine translation. W e consider two bench-\nmarks: Most experiments are run on the WMT’18\nEnglish-German (en-de) news translation task and\nwe validate our ﬁndings on the WMT’18 English-\nTurkish (en-tr) news task. For WMT’18 English-\nGerman, the training corpus consists of all avail-\nable bitext excluding the ParaCrawl corpus and we\nremove sentences longer than 250 tokens as well\nas sentence-pairs with a source/target length ra-\ntio exceeding 1.5. This results in 5.18M sentence\npairs. W e tokenize all data with the Moses tok-\nenizer (\nKoehn et al. , 2007) and apply the BPE vo-\ncabulary learned on the monolingual corpora.\nFor WMT’18 English-Turkish, we use all of the\navailable bitext comprising 208K sentence-pairs\nwithout any ﬁltering. W e develop on newstest2017\nand test on newstest2018. For en-tr we only exper-\niment with adding representations to the encoder\nand therefore apply the language model vocabu-\nlary to the source side. For the target vocabulary\nwe learn a BPE code with 32K merge operations\non the Turkish side of the bitext. Both datasets are\nevaluated in terms of case-sensitive de-tokenized\nBLEU (\nPapineni et al. , 2002; Post, 2018).2\nSummarization. W e consider the CNN-\nDailyMail abstractive document summarization\ntask comprising over 280K news articles paired\nwith multi-sentence summaries. CNN-DailyMail\nis a widely used dataset for abstractive text\nsummarization. Following (\nSee et al. , 2017),\nwe report results on the non-anonymized\nversion of CNN-DailyMail rather than the\nentity-anonymized version (\nHermann et al. , 2015;\nNallapati et al. , 2016) because the language model\nwas trained on full text. Articles are truncated to\n400 tokens (\nSee et al. , 2017) and we use a BPE\nvocabulary of 32K types ( Fan et al. , 2017). W e\nevaluate in terms of F1-Rouge, that is Rouge-1,\nRouge-2 and Rouge-L (\nLin, 2004).3\n3.2 Language model pre-training\nW e consider two types of architectures: a bi-\ndirectional language model to augment the se-\nquence to sequence encoder and a uni-directional\nmodel to augment the decoder. Both use\nself-attention (\nV aswani et al. , 2017) and the uni-\ndirectional model contains N = 12 transformer\nblocks, followed by a word classiﬁer to predict\nthe next word on the right. The bi-directional\nmodel solves a cloze-style token prediction task\nat training time (\nBaevski et al. , 2019). The model\nconsists of two towers, the forward tower oper-\nates left-to-right and the tower operating right-\nto-left as backward tower; each tower contains\n2 sacreBLEU signatures: BLEU+case.mixed+lang.en-\n{de,tr}+numrefs.1+smooth.exp+test.wmt18+tok.13a\n+version.1.2.1\n3 W e use the following parameters for\nROUGE-1.5.5.pl: -m -a -n 2\n160K 320K 640K 1280K 2560K 5186K\n− 1\n0\n1\n2\n3\n4\n5\n6\nBitext tokens\nBLEU delta wrt baseline\nSH A R ED SR C -ELM O SR C -FT\nTG T-ELM O TG T-FT SR C -ELM O +SH D EM B\nFigure 1: BLEU difference to a bitext-only baseline when add ing pre-trained language model representations\nto a neural machine translation model in different simulate d bitext settings. Results are based on averaging\nnewstest2012-2017 of WMT English-German translation.\nN = 12 transformer blocks. The forward\nand backward representations are combined via\na self-attention module and the output of this\nmodule is used to predict the token at posi-\ntion i. The model has access to the entire in-\nput surrounding the current target token. Mod-\nels use the standard settings for the Big Trans-\nformer (\nV aswani et al. , 2017). The bi-directional\nmodel contains 353M parameters and the uni-\ndirectional model 190M parameters. Both models\nwere trained for 1M steps using Nesterov’s accel-\nerated gradient (\nSutskever et al. , 2013) with mo-\nmentum 0.99 following Baevski and Auli (2018).\nThe learning rate is linearly warmed up from\n10− 7 to 1 for 16K steps and then annealed us-\ning a cosine learning rate schedule with a single\nphase to 0.0001 (\nLoshchilov and Hutter , 2016).\nW e train on 32 Nvidia V100 SXM2 GPUs and\nuse the NCCL2 library as well as the torch dis-\ntributed package for inter-GPU communication.\nTraining relies on 16-bit ﬂoating point opera-\ntions (\nOtt et al. , 2018) and it took six days for the\nbi-directional model and four days for the uni-\ndirectional model.\n3.3 Sequence to sequence model\nW e use the transformer implementation of the\nfairseq toolkit (\nOtt et al. , 2019). The WMT en-de\nand en-tr experiments are based on the Big Trans-\nformer sequence to sequence architecture with 6\nblocks in the encoder and decoder. For abstractive\nsummarization we use a base transformer model\n(\nV aswani et al. , 2017). W e tune dropout values of\nbetween 0.1 and 0.4 on the validation set. Models\nare optimized with Adam (\nKingma and Ba , 2015)\nusing β1 = 0.9, β2 = 0.98, and ǫ = 1e − 8\nand we use the same learning rate schedule as\nV aswani et al. (2017); we perform 10K-200K de-\npending on bitext size. All models use label\nsmoothing with a uniform prior distribution over\nthe vocabulary ǫ = 0 .1 (\nSzegedy et al. , 2015;\nPereyra et al. , 2017). W e run experiments on 8\nGPUs and generate translations with a beam of\nsize 5.\n4 Results\n4.1 Machine translation\nW e ﬁrst present a comparison of the various strate-\ngies in different simulated parallel corpus size set-\ntings. For each experiment, we tune the dropout\napplied to the language model representations,\nand we reduce the number of optimizer steps for\nsmaller bitext setups as models converge faster;\nall other hyper-parameters are equal between se-\ntups. Our baseline is a Big Transformer model\nand we also consider a variant where we share to-\nken embeddings between the encoder and decoder\n(SH A R ED ; Inan et al., 2016; Press & W olf, 2016).\nFigure\n1 shows results averaged over six test\nsets relative to the baseline which does not share\nsource and target embeddings (Appendix\nA shows\n160K 640K 5186K\nbaseline 21.4 33.1 40.1\nSR C -ELM O 26.6 35.6 41.8\nSR C -FT 24.3 34.9 40.8\nTG T-ELM O 21.3 31.9 40.5\nTG T-FT 24.2 31.4 38.8\nSR C -ELM O +SH D EM B 29.0 36.2 41.8\nT able 1: BLEU on newstest2018 of WMT English-\nGerman in three simulated bitext size scenarios.\nnews2017 news2018\nbaseline 9.8 9.5\nSR C -ELM O 12.0 11.3\nSR C -ELM O +SH D EM B 12.9 11.8\nT able 2: WMT English-Turkish translation results in\nterms of BLEU on newstest2017 (valid) and new-\nstest2018 (test) with ELMo inputs to the encoder.\na detailed breakdown). SH A R ED performs very\nwell with little labeled data but the gains erode to\npractically zero in large bitext settings. Pre-trained\nlanguage model representations are most effective\nin low bitext setups. The best performing strategy\nis ELMo embeddings input to the encoder ( SR C -\nELM O ). This improves the baseline by 3.8 BLEU\nin the 160K bitext setting and it still improves the\n5.2M setting by over 1 BLEU.\nW e further improve SR C -ELM O by sharing\nlearned word representations in the decoder\nby tying input and output embeddings ( SR C -\nELM O +SH D EM B ). This conﬁguration performs\neven better than SR C -ELM O with a gain of 5.3\nBLEU in the 160K setup. Sharing decoder embed-\ndings is equally applicable to SR C -FT . Language\nmodel representations are much less effective in\nthe decoder: TG T-FT improves the 160K bitext\nsetup but yields no improvements thereafter and\nTG T-ELM O performs even worse. W e conjecture\nthat pre-trained representations give much easier\nwins in the encoder. T able\n1 shows additional re-\nsults on newstest2018.\nPre-trained representations mostly impacts the\ntraining time of the sequence to sequence model\n(see Appendix\nB): SR C -ELM O slows throughput\nduring training by about 5.3x and SR C -FT is\neven slower because of the need to backpropa-\ngate through the LM for ﬁne-tuning (9.2x). How-\never, inference is only 12-14% slower than the\nRO U G E\n1 2 L\nLead-3 40.34 17.70 36.57\nSee et al. (2017) 39.53 17.28 36.38\nGehrmann et al. (2018) 41.22 18.68 38.34\nbaseline 40.07 17.61 36.78\nSR C -ELM O +SH D E M B 41.56 18.94 38.47\nT able 3: Abstractive summarization results on CNN-\nDailyMail. ELMo inputs achieve a new state of the art.\nbaseline when adding pre-trained embeddings to\nthe encoder ( SR C -ELM O , SR C -FT ). This is be-\ncause the LM computation can be paralelized for\nall input tokens. Inference is much slower when\nadding representations to the decoder because the\nLM needs to be invoked repeatedly . Our current\nimplementation does not cache LM operations for\nthe previous state and can be made much faster.\nThe baseline uses a BPE vocabulary estimated\non the language model corpora ( §\n3). Appendix A\nshows that this vocabulary actually leads to sligtly\nbetter performance than a joint BPE code learned\non the bitext as is usual.\nNext, we validate our ﬁndings on the WMT’18\nEnglish-Turkish task for which the bitext is truly\nlimited (208K sentence-pairs). W e use the lan-\nguage model vocab for the the English side of the\nbitext and a BPE vocabulary learned on the Turk-\nish side. T able\n2 shows that ELMo embeddings for\nthe encoder improve English-Turkish translation.\n4.2 Abstractive summarization\nFollowing\nSee et al. (2017), we experiment on\nthe non-anonymized version of CNN-DailyMail.\nWhen generating summaries, we follow standard\npractice of tuning the maximum output length and\ndisallow repeating the same trigram (\nPaulus et al. ,\n2017; Fan et al. , 2017). For this task we train\nlanguage model representations on the combina-\ntion of newscrawl and the CNN-DailyMail train-\ning data. T able\n3 shows that pre-trained em-\nbeddings can signiﬁcantly improve on top of a\nstrong baseline transformer. W e also compare to\nGehrmann et al. (2018) who use a task-speciﬁc ar-\nchitecture compared to our generic sequence to se-\nquence baseline. Pre-trained representations are\ncomplementary to their method.\n5 Conclusion\nW e presented an analysis of different strategies to\nadd pre-trained language model representations to\nsequence to sequence models for neural machine\ntranslation and abstractive document summariza-\ntion. Adding pre-trained representations is very\neffective for the encoder network and while re-\nturns diminish when more labeled data becomes\navailable, we still observe improvements when\nmillions of examples are available. In future re-\nsearch we will investigate ways to improve the de-\ncoder with pre-trained representations.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv,\nabs/1607.06450.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv, abs/1809.10853.\nAlexei Baevski, Sergey Edunov, Y inhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. arXiv.\nAndrew M. Dai and Quoc V . Le. 2015.\nSemi-supervised sequence learning. arXiv,\nabs/1511.01432.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\nAngela Fan, David Grangier, and Michael Auli. 2017.\nControllable abstractive summarization. arXiv,\nabs/1711.05217.\nSebastian Gehrmann, Y untian Deng, and Alexander M\nRush. 2018. Bottom-up abstractive summarization.\narXiv, abs/1808.10792.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Y oshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv, abs/1503.03535.\nKarl Moritz Hermann, T om ´ aˇ s Ko ˇ cisk ´ y, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. T eaching ma-\nchines to read and comprehend. In Proc. of NIPS.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. T ying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv,\nabs/1611.01462.\nDiederik P . Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Proc. of\nICLR.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, W ade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProc. of ACL Demo Session.\nChin-Y ew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In W orkshop on T ext Sum-\nmarization Branches Out.\nIlya Loshchilov and Frank Hutter. 2016. SGDR:\nstochastic gradient descent with restarts. arXiv,\nabs/1608.03983.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond. In Proc. of CONLL.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proc. of NAACL\nSystem Demonstrations.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. of WMT.\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. BLEU: a method for automatic\nevaluation of machine translation. In Proc. of ACL.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv, abs/1705.04304.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-\nularizing neural networks by penalizing conﬁdent\noutput distributions. In International Conference on\nLearning Representations (ICLR) W orkshop.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of ACL.\nJason Phang, Thibault Fevry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv,\nabs/1811.01088.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv, abs/1804.08771.\nOﬁr Press and Lior W olf. 2017. Using the output em-\nbedding to improve language models. In Proc. of\nEACL.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nhttps://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf\n.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proc. of ACL.\nRico Sennrich, Barry Haddow , and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. of ACL.\nIlya Sutskever, James Martens, George E. Dahl, and\nGeoffrey E. Hinton. 2013. On the importance of ini-\ntialization and momentum in deep learning. In Proc.\nof ICML.\nChristian Szegedy, V incent V anhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew W ojna. 2015. Re-\nthinking the Inception Architecture for Computer\nV ision. arXiv preprint arXiv:1512.00567.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nY ou Need. In Proc. of NIPS.\nBarret Zoph, Deniz Y uret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proc. of EMNLP.\nA Detailed WMT English-German Results\nbitext method 2012 2013 2014 2015 2016 2017 2018 A vg\n160K\nbaseline 13.2 15.7 13.5 15.7 18.6 14.8 21.4 16.1\nS H A R E D 15.3 18.2 16.7 19.0 21.6 18.2 24.9 19.1\nS H A R E D+bitext-BPE 15.1 17.9 16.2 18.9 22.0 18.0 25.2 19.0\nS R C -E L M O 16.0 19.4 17.1 19.9 23.0 18.7 26.6 20.1\nS R C -F T 15.3 18.5 16.6 18.9 20.8 17.6 24.3 18.9\nT G T-E L M O 13.3 16.4 14.1 16.2 18.8 14.9 21.3 16.4\nT G T-F T 14.7 17.2 15.8 18.4 21.4 16.9 24.2 18.4\nS R C -E L M O +S H D E M B 17.4 20.8 18.6 21.5 24.9 20.3 29.0 21.8\n320K\nbaseline 17.2 20.4 18.1 21.2 25.0 19.6 28.9 21.5\nS H A R E D 18.1 21.1 19.1 22.4 26.3 21.2 30.6 22.7\nS H A R E D+bitext-BPE 17.6 20.6 19.1 22.3 26.1 20.8 29.9 22.3\nsrc-elmo 18.8 22.3 21.1 24.0 27.5 22.2 32.5 24.1\nS R C -F T 19.0 22.5 20.9 23.5 26.9 22.2 32.1 23.9\nT G T-E L M O 16.7 20.7 18.2 20.9 24.1 19.4 28.0 21.1\nT G T-F T 16.1 19.4 17.1 20.0 23.1 18.5 26.3 20.1\nS R C -E L M O +S H D E M B 19.5 22.9 21.2 24.0 27.4 22.4 32.3 24.2\n640K\nbaseline 19.2 22.9 21.2 24.5 27.9 22.4 33.1 24.5\nS H A R E D 19.9 23.4 22.1 25.1 28.8 23.0 34.1 25.2\nS H A R E D+bitext-BPE 19.4 22.8 21.7 24.9 28.4 22.9 33.6 24.8\nsrc-elmo 21.0 24.3 23.4 26.5 30.0 24.6 35.6 26.5\nS R C -F T 20.5 24.0 22.9 26.1 29.1 24.4 34.9 26.0\nT G T-E L M O 18.9 22.6 20.8 24.2 27.5 22.3 31.9 24.0\nT G T-F T 18.2 21.8 20.6 23.7 27.0 21.8 31.4 23.5\nS R C -E L M O +S H D E M B 21.2 25.1 23.9 26.7 30.2 24.7 36.2 26.9\n1280K\nbaseline 20.9 24.6 23.6 26.5 30.5 24.7 36.2 26.7\nS H A R E D 21.1 24.6 24.6 27.6 31.0 25.2 37.3 27.3\nS H A R E D+bitext-BPE 20.5 24.0 23.9 26.2 30.6 24.8 36.2 26.6\nsrc-elmo 22.1 25.7 25.7 28.5 31.7 26.3 38.2 28.3\nS R C -F T 21.3 25.2 25.3 28.5 31.1 26.2 37.4 27.9\nT G T-E L M O 20.9 24.4 23.6 26.6 30.3 24.9 36.1 26.7\nT G T-F T 20.1 23.7 22.4 25.2 29.1 23.6 34.4 25.5\nS R C -E L M O +S H D E M B 22.3 26.0 26.3 28.9 32.6 26.8 38.6 28.8\n2560K\nbaseline 21.7 25.6 25.4 28.2 32.3 26.2 39.1 28.4\nS H A R E D 22.2 25.9 25.7 28.3 32.1 26.3 38.9 28.5\nS H A R E D+bitext-BPE 21.8 25.5 25.5 27.9 32.1 26.0 38.6 28.2\nsrc-elmo 22.9 27.0 27.0 30.0 33.4 28.0 40.0 29.8\nS R C -F T 22.2 26.4 26.3 29.5 32.4 27.3 39.3 29.1\nT G T-E L M O 21.8 25.7 25.8 28.5 32.3 26.6 39.3 28.6\nT G T-F T 21.5 25.3 24.5 27.0 30.2 25.2 36.8 27.2\nS R C -E L M O +S H D E M B 23.1 27.2 27.1 29.7 33.7 27.9 40.0 29.8\n5186K\nbaseline 23.1 26.8 27.7 30.1 33.6 27.9 40.1 29.9\nS H A R E D 22.6 26.6 27.7 30.5 33.4 28.2 40.2 29.9\nS H A R E D+bitext-BPE 22.5 26.0 27.0 29.7 33.4 27.7 40.6 29.6\nsrc-elmo 23.7 27.8 28.7 31.1 34.5 29.2 41.8 31.0\nS R C -F T 23.1 27.0 27.8 30.5 33.7 28.3 40.8 30.2\nT G T-E L M O 22.9 26.6 26.9 29.5 33.8 27.7 40.5 29.7\nT G T-F T 22.3 26.1 26.1 28.9 32.5 26.5 38.8 28.7\nS R C -E L M O +S H D E M B 23.4 28.0 28.8 31.2 34.5 28.7 41.8 30.9\nT able 4: BLEU on newstest2012 to newstest2018 of WMT English -German translation in varioius simulated bitext\nsize scenarios (cf. Figure 1).\nB T raining and inference speed\ntrain (tok/sec) inference (tok/sec)\nSH A R ED 528,802 2,334\nSR C -ELM O 100,636 2,011\nSR C -FT 57,753 2,080\nTG T-ELM O 142,525 259\nTG T-FT 95,313 299\nT able 5: Training and inference speed of models trained on WM T English-German. Training speed based on 32\nV100 GPUs. Inference speed measured on a single V100 and by ba tching up to 12K source or target tokens.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8605923056602478
    },
    {
      "name": "Computer science",
      "score": 0.8257944583892822
    },
    {
      "name": "Automatic summarization",
      "score": 0.8234705924987793
    },
    {
      "name": "Natural language processing",
      "score": 0.6947109699249268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6712893843650818
    },
    {
      "name": "Language model",
      "score": 0.5879844427108765
    },
    {
      "name": "Sentence",
      "score": 0.5831238031387329
    },
    {
      "name": "Inference",
      "score": 0.5567806363105774
    },
    {
      "name": "BLEU",
      "score": 0.5477544069290161
    },
    {
      "name": "Encoder",
      "score": 0.5451830625534058
    },
    {
      "name": "Translation (biology)",
      "score": 0.5442623496055603
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5202149748802185
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.46702197194099426
    },
    {
      "name": "Speech recognition",
      "score": 0.3421672284603119
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}