{
    "title": "Extracting Training Data from Large Language Models",
    "url": "https://openalex.org/W3112689365",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221782472",
            "name": "Carlini, Nicholas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3163579884",
            "name": "Tramer, Florian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2913157063",
            "name": "Wallace, Eric",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221782474",
            "name": "Jagielski, Matthew",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287908655",
            "name": "Herbert-Voss, Ariel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2330663980",
            "name": "Lee, Katherine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224145415",
            "name": "Roberts, Adam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3189469367",
            "name": "Brown Tom",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751273816",
            "name": "Song, Dawn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287915914",
            "name": "Erlingsson, Úlfar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742597437",
            "name": "Oprea, Alina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222913930",
            "name": "Raffel, Colin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2473418344",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2884943453",
        "https://openalex.org/W2930926105",
        "https://openalex.org/W3027379683",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2036963181",
        "https://openalex.org/W3087503988",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1970606468",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W2947686949",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W2109426455",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2096633407",
        "https://openalex.org/W3102407811",
        "https://openalex.org/W3100352836",
        "https://openalex.org/W72496981",
        "https://openalex.org/W3035261884",
        "https://openalex.org/W2949461276",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2208157769",
        "https://openalex.org/W2967985550",
        "https://openalex.org/W3137695714",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W2971008823",
        "https://openalex.org/W3096738375",
        "https://openalex.org/W3035034338",
        "https://openalex.org/W2784621220",
        "https://openalex.org/W3106051020",
        "https://openalex.org/W46679369",
        "https://openalex.org/W3101678025",
        "https://openalex.org/W2053637704",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3034287667",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W3167352803",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2786233556",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2612281133"
    ],
    "abstract": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
    "full_text": "Extracting Training Data from Large Language Models\nNicholas Carlini1 Florian Tramèr2 Eric Wallace3 Matthew Jagielski4\nAriel Herbert-V oss5,6 Katherine Lee1 Adam Roberts1 Tom Brown5\nDawn Song3 Úlfar Erlingsson7 Alina Oprea4 Colin Raffel1\n1Google 2Stanford 3UC Berkeley 4Northeastern University 5OpenAI 6Harvard 7Apple\nAbstract\nIt has become common to publish large (billion parameter)\nlanguage models that have been trained on private datasets.\nThis paper demonstrates that in such settings, an adversary can\nperform a training data extraction attackto recover individual\ntraining examples by querying the language model.\nWe demonstrate our attack on GPT-2, a language model\ntrained on scrapes of the public Internet, and are able to extract\nhundreds of verbatim text sequences from the model’s training\ndata. These extracted examples include (public) personally\nidentiﬁable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our\nattack is possible even though each of the above sequences\nare included in just one document in the training data.\nWe comprehensively evaluate our extraction attack to un-\nderstand the factors that contribute to its success. Worryingly,\nwe ﬁnd that larger models are more vulnerable than smaller\nmodels. We conclude by drawing lessons and discussing pos-\nsible safeguards for training large language models.\n1 Introduction\nLanguage models (LMs)—statistical models which assign a\nprobability to a sequence of words—are fundamental to many\nnatural language processing tasks. Modern neural-network-\nbased LMs use very large model architectures (e.g., 175 bil-\nlion parameters [7]) and train on massive datasets (e.g., nearly\na terabyte of English text [55]). This scaling increases the\nability of LMs to generate ﬂuent natural language [53,74,76],\nand also allows them to be applied to a plethora of other\ntasks [29, 39, 55], even without updating their parameters [7].\nAt the same time, machine learning models are notorious\nfor exposing information about their (potentially private) train-\ning data—both in general [47, 65] and in the speciﬁc case of\nlanguage models [8, 45]. For instance, for certain models it\nis known that adversaries can apply membership inference\nattacks [65] to predict whether or not any particular example\nwas in the training data.\nGPT-2\nEast Stroudsburg Stroudsburg...\nPrefix\n---  Corporation Seabank Centre\n------ Marine Parade Southport\nPeter W--------- \n-----------@---.------------.com\n+-- 7 5--- 40-- \nFax: +-- 7 5--- 0--0\nMemorized text\nFigure 1: Our extraction attack. Given query access to a\nneural network language model, we extract an individual per-\nson’s name, email address, phone number, fax number, and\nphysical address. The example in this ﬁgure shows informa-\ntion that is all accurate so we redact it to protect privacy.\nSuch privacy leakage is typically associated withoverﬁtting\n[75]—when a model’s training error is signiﬁcantly lower\nthan its test error—because overﬁtting often indicates that a\nmodel has memorized examples from its training set. Indeed,\noverﬁtting is a sufﬁcient condition for privacy leakage [72]\nand many attacks work by exploiting overﬁtting [65].\nThe association between overﬁtting and memorization has—\nerroneously—led many to assume that state-of-the-art LMs\nwill not leak information about their training data. Because\nthese models are often trained on massive de-duplicated\ndatasets only for a single epoch [7, 55], they exhibit little\nto no overﬁtting [53]. Accordingly, the prevailing wisdom has\nbeen that “the degree of copying with respect to any given\nwork is likely to be, at most,de minimis” [71] and that models\ndo not signiﬁcantly memorize any particular training example.\n1\narXiv:2012.07805v2  [cs.CR]  15 Jun 2021\nContributions. In this work, we demonstrate that large lan-\nguage models memorize and leak individual training exam-\nples. In particular, we propose a simple and efﬁcient method\nfor extracting verbatim sequences from a language model’s\ntraining set using only black-box query access. Our key in-\nsight is that, although training examples do not have notice-\nably lower losses than test examples onaverage, certain worst-\ncase training examples are indeed memorized.\nIn our attack, we ﬁrst generate a large, diverse set of high-\nlikelihood samples from the model, using one of three general-\npurpose sampling strategies. We then sort each sample using\none of six different metrics that estimate the likelihood of\neach sample using a separate reference model (e.g., another\nLM), and rank highest the samples with an abnormally high\nlikelihood ratio between the two models.\nOur attacks directly apply to any language model, including\nthose trained on sensitive and non-public data [10,16]. We use\nthe GPT-2 model [54] released by OpenAI as a representative\nlanguage model in our experiments. We choose to attack\nGPT-2 to minimize real-world harm—the GPT-2 model and\noriginal training data source are already public.\nTo make our results quantitative, we deﬁne a testable def-\ninition of memorization. We then generate 1,800 candidate\nmemorized samples, 100 under each of the3×6 attack conﬁg-\nurations, and ﬁnd that over 600 of them are verbatim samples\nfrom the GPT-2 training data (conﬁrmed in collaboration with\nthe creators of GPT-2). In the best attack conﬁguration, 67%\nof candidate samples are verbatim training examples. Our\nmost obviously-sensitive attack extracts the full name, phys-\nical address, email address, phone number, and fax number\nof an individual (see Figure 1). We comprehensively analyze\nour attack, including studying how model size and string fre-\nquency affects memorization, as well as how different attack\nconﬁgurations change the types of extracted data.\nWe conclude by discussing numerous practical strategies to\nmitigate privacy leakage. For example, differentially-private\ntraining [1] is theoretically well-founded and guaranteed to\nproduce private models if applied at an appropriate record\nlevel, but it can result in longer training times and typically\ndegrades utility. We also make recommendations, such as\ncarefully de-duplicating documents, that empirically will help\nto mitigate memorization but cannot prevent all attacks.\n2 Background & Related Work\nTo begin, we introduce the relevant background on large\n(billion-parameter) neural network-based language models\n(LMs) as well as data privacy attacks.\n2.1 Language Modeling\nLanguage models are a fundamental building block of cur-\nrent state-of-the-art natural language processing pipelines\n[12, 31, 50, 52, 55]. While the unsupervised objectives used\nto train these models vary, one popular choice is a “next-step\nprediction” objective [5, 31, 44, 52]. This approach constructs\na generative model of the distribution\nPr(x1,x2,..., xn),\nwhere x1,x2,..., xn is a sequence of tokens from a vocabulary\nV by applying the chain rule of probability\nPr(x1,x2,..., xn) =Πn\ni=1Pr(xi |x1,..., xi−1).\nState-of-the-art LMs use neural networks to estimate this\nprobability distribution. We let fθ(xi |x1,..., xi−1) denote\nthe likelihood of token xi when evaluating the neural net-\nwork f with parameters θ. While recurrent neural networks\n(RNNs) [26, 44] used to be a common choice for the neu-\nral network architecture of LMs, attention-based models [4]\nhave recently replaced RNNs in state-of-the-art models. In\nparticular, Transformer LMs [70] consist of a sequence of at-\ntention layers and are the current model architecture of choice.\nBecause we believe our results are independent of the exact\narchitecture used, we will not describe the Transformer archi-\ntecture in detail here and instead refer to existing work [3].\nTraining Objective. A language model is trained to max-\nimize the probability of the data in a training set X. In this\npaper, each training example is a text document—for example,\na speciﬁc news article or webpage from the internet. Formally,\ntraining involves minimizing the loss function\nL(θ) =−logΠn\ni=1 fθ(xi |x1,..., xi−1)\nover each training example in the training dataset X. Because\nof this training setup, the “optimal” solution to the task of\nlanguage modeling is to memorize the answer to the ques-\ntion “what token follows the sequence x1,..., xi−1?” for ev-\nery preﬁx in the training set. However, state-of-the-art LMs\nare trained with massive datasets, which causes them to not\nexhibit signiﬁcant forms of memorization: empirically, the\ntraining loss and the test loss are nearly identical [7, 53, 55].\nGenerating Text. A language model can generate new\ntext (potentially conditioned on some preﬁx x1,..., xi)\nby iteratively sampling ˆxi+1 ∼fθ(xi+1|x1,..., xi) and then\nfeeding ˆxi+1 back into the model to sample ˆxi+2 ∼\nfθ(xi+2|x1,..., ˆxi+1). This process is repeated until a desired\nstopping criterion is reached. Variations of this text generation\nmethod include deterministically choosing the most-probable\ntoken rather than sampling (i.e., “greedy” sampling) or setting\nall but the top-n probabilities to zero and renormalizing the\nprobabilities before sampling (i.e., top-n sampling1 [18]).\nGPT-2. Our paper focuses on the GPT variant of Trans-\nformer LMs [7,52,54]. Speciﬁcally, we demonstrate our train-\ning data extraction attacks on GPT-2, a family of LMs that\n1For notational clarity, we write top-n instead of the more common top-k\nbecause we will use the constant k for a separate purpose.\n2\nwere all trained using the same dataset and training algorithm,\nbut with varying model sizes. GPT-2 uses a word-pieces [61]\nvocabulary with a byte pair encoder [22].\nGPT-2 XL is the largest model with 1.5 billion parameters.\nFor the remainder of this paper, the “GPT-2” model refers\nto this 1.5 billion parameter model or, when we speciﬁcally\nindicate this, its Small and Medium variants with 124 million\nand 334 million parameters, respectively.\nThe GPT-2 model family was trained on data scraped from\nthe public Internet. The authors collected a dataset by follow-\ning outbound links from the social media website Reddit. The\nwebpages were cleaned of HTML, with only the document\ntext retained, and then de-duplicated at the document level.\nThis resulted in a ﬁnal dataset of 40GB of text data, over\nwhich the model was trained for approximately 12 epochs.2\nAs a result, GPT-2 does not overﬁt: the training loss is only\nroughly 10% smaller than the test loss across all model sizes.\n2.2 Training Data Privacy\nIt is undesirable for models to remember any details that are\nspeciﬁc to their (potentially private) training data. The ﬁeld\nof training data privacy develops attacks (to leak training data\ndetails) and defenses (to prevent leaks).\nPrivacy Attacks. When models are not trained with\nprivacy-preserving algorithms, they are vulnerable to numer-\nous privacy attacks. The least revealing form of attack is the\nmembership inference attack [28, 47, 65, 67]: given a trained\nmodel, an adversary can predict whether or not a particular\nexample was used to train the model. Separately, model inver-\nsion attacks [21] reconstruct representative views of a subset\nof examples (e.g., a model inversion attack on a face recog-\nnition classiﬁer might recover a fuzzy image of a particular\nperson that the classiﬁer can recognize).\nTraining data extraction attacks, like model inversion at-\ntacks, reconstruct training datapoints. However, training data\nextraction attacks aim to reconstruct verbatim training exam-\nples and not just representative “fuzzy” examples. This makes\nthem more dangerous, e.g., they can extract secrets such as\nverbatim social security numbers or passwords. Training data\nextraction attacks have until now been limited to small LMs\ntrained on academic datasets under artiﬁcial training setups\n(e.g., for more epochs than typical) [8, 66, 68, 73], or settings\nwhere the adversary has a priori knowledge of the secret they\nwant to extract (e.g., a social security number) [8, 27].\nProtecting Privacy. An approach to minimizing memoriza-\ntion of training data is to apply differentially-private training\ntechniques [1, 9, 43, 60, 64]. Unfortunately, training models\nwith differentially-private mechanisms often reduces accu-\nracy [34] because it causes models to fail to capture the long\n2Personal communication with the GPT-2 authors.\ntails of the data distribution [19,20,67]. Moreover, it increases\ntraining time, which can further reduce accuracy because cur-\nrent LMs are limited by the cost of training [35, 38, 55]. As\na result, state-of-the-art LMs such as GPT-2 [53], GPT-3 [7],\nand T5 [55] do not apply these privacy-preserving techniques.\n3 Threat Model & Ethics\nTraining data extraction attacks are often seen as theoretical\nor academic and are thus unlikely to be exploitable in practice\n[71]. This is justiﬁed by the prevailing intuition that privacy\nleakage is correlated with overﬁtting [72], and because state-\nof-the-art LMs are trained on large (near terabyte-sized [7])\ndatasets for a few epochs, they tend to not overﬁt [53].\nOur paper demonstrates that training data extraction attacks\nare practical. To accomplish this, we ﬁrst precisely deﬁne\nwhat we mean by “memorization”. We then state our threat\nmodel and our attack objectives. Finally, we discuss the ethical\nconsiderations behind these attacks and explain why they are\nlikely to be a serious threat in the future.\n3.1 Deﬁning Language Model Memorization\nThere are many ways to deﬁne memorization in language\nmodeling. As mentioned earlier, memorization is in many\nways an essential component of language models because\nthe training objective is to assign high overall likelihood to\nthe training dataset. LMs must, for example, “memorize” the\ncorrect spelling of individual words.\nIndeed, there is a research direction that analyzes neural\nnetworks as repositories of (memorized) knowledge [51, 59].\nFor example, when GPT-2 is prompted to complete the sen-\ntence “My address is 1 Main Street, San Francisco CA”, it\ngenerates “94107”: a correct zip code for San Francisco, CA.\nWhile this is clearly memorization in some abstract form,we\naim to formalize our deﬁnition of memorization in order to\nrestrict it to cases that we might consider “unintended” [8].\n3.1.1 Eidetic Memorization of Text\nWe deﬁne eidetic memorization as a particular type of mem-\norization.3 Informally, eidetic memorization is data that has\nbeen memorized by a model despite only appearing in a small\nset of training instances. The fewer training samples that con-\ntain the data, the stronger the eidetic memorization is.\nTo formalize this notion, we ﬁrst deﬁne what it means for\na model to have knowledge of a string s. Our deﬁnition is\nloosely inspired by knowledge deﬁnitions in interactive proof\nsystems [24]: a model fθ knows a string s if s can be extracted\nby interacting with the model. More precisely, we focus on\nblack-box interactions where the model generates s as the\nmost likely continuation when prompted with some preﬁx c:\n3Eidetic memory (more commonly called photographic memory) is the\nability to recall information after seeing it only once.\n3\nDeﬁnition 1 (Model Knowledge Extraction) A string s is\nextractable4 from an LM fθ if there exists a preﬁx c such that:\ns ←argmax\ns′: |s′|=N\nfθ(s′|c)\nWe abuse notation slightly here to denote by fθ(s′|c) the\nlikelihood of an entire sequence s′. Since computing the most\nlikely sequence s is intractable for large N, the argmax in\nDeﬁnition 1 can be replaced by an appropriate sampling strat-\negy (e.g., greedy sampling) that reﬂects the way in which the\nmodel fθ generates text in practical applications. We then\ndeﬁne eidetic memorization as follows:\nDeﬁnition 2 (k-Eidetic Memorization) A string s is k-\neidetic memorized (for k ≥1) by an LM fθ if s is extractable\nfrom fθ and s appears in at most k examples in the training\ndata X: |{x ∈X : s ⊆x}|≤k.\nKey to this deﬁnition is what “examples” means. For GPT-\n2, each webpage is used (in its entirety) as one training exam-\nple. Since this deﬁnition counts the number of distinct training\nexamples containing a given string, and not the total number\nof times the string occurs, a string may appear multiple times\non one page while still counting as k = 1 memorization.\nThis deﬁnition allows us to deﬁne memorization as a spec-\ntrum. While there is no deﬁnitive value ofk at which we might\nsay that memorization is unintentional and potentially harm-\nful, smaller values are more likely to be so. For any givenk,\nmemorizing longer strings is also “worse” than shorter strings,\nalthough our deﬁnition omits this distinction for simplicity.\nFor example, under this deﬁnition, memorizing the correct\nspellings of one particular word is not severe if the word oc-\ncurs in many training examples (i.e., k is large). Memorizing\nthe zip code of a particular city might be eidetic memorization,\ndepending on whether the city was mentioned in many train-\ning examples (e.g., webpages) or just a few. Referring back to\nFigure 1, memorizing an individual person’s name and phone\nnumber clearly (informally) violates privacy expectations, and\nalso satisﬁes our formal deﬁnition: it is contained in just a\nfew documents on the Internet—and hence the training data.\n3.2 Threat Model\nAdversary’s Capabilities. We consider an adversary who\nhas black-box input-output access to a language model. This\nallows the adversary to compute the probability of arbitrary\nsequences fθ(x1,..., xn), and as a result allows the adversary\nto obtain next-word predictions, but it does not allow the\nadversary to inspect individual weights or hidden states (e.g.,\nattention vectors) of the language model.\n4This deﬁnition admits pathological corner cases. For example, many\nLMs when when prompted with“Repeat the following sentence: _____. ”will\ndo so correctly. This allows any string to be “known” under our deﬁnition.\nSimple reﬁnements of this deﬁnition do not solve the issue, as LMs can also\nbe asked to, for example, down-case a particular sentence. We avoid these\npathological cases by prompting LMs only with short preﬁxes.\nThis threat model is highly realistic as many LMs are\navailable through black-box APIs. For example, the GPT-\n3 model [7] created by OpenAI is available through black-box\nAPI access. Auto-complete models trained on actual user data\nhave also been made public, although they reportedly use\nprivacy-protection measures during training [10].\nAdversary’s Objective. The adversary’s objective is to ex-\ntract memorized training data from the model. The strength\nof an attack is measured by how private (formalized as being\nk-eidetic memorized) a particular example is. Stronger attacks\nextract more examples in total (both more total sequences,\nand longer sequences) and examples with lower values of k.\nWe do not aim to extracttargeted pieces of training data, but\nrather indiscriminately extract training data. While targeted\nattacks have the potential to be more adversarially harmful,\nour goal is to study the ability of LMs to memorize data\ngenerally, not to create an attack that can be operationalized\nby real adversaries to target speciﬁc users.\nAttack Target. We select GPT-2 [54] as a representative\nLM to study for our attacks. GPT-2 is nearly a perfect target.\nFirst, from an ethical standpoint, the model and data are public,\nand so any memorized data that we extract is already public.5\nSecond, from a research standpoint, the dataset (despite being\ncollected from public sources) was never actually released\nby OpenAI. Thus, it is not possible for us to unintentionally\n“cheat” and develop attacks that make use of knowledge of\nthe GPT-2 training dataset.\n3.3 Risks of Training Data Extraction\nTraining data extraction attacks present numerous privacy\nrisks. From an ethical standpoint, most of these risks are miti-\ngated in our paper because we attack GPT-2, whose training\ndata is public. However, since our attacks would apply to any\nLM, we also discuss potential consequences of future attacks\non models that may be trained on private data.\nData Secrecy. The most direct form of privacy leakage oc-\ncurs when data is extracted from a model that was trained\non conﬁdential or private data. For example, GMail’s auto-\ncomplete model [10] is trained on private text communica-\ntions between users, so the extraction of unique snippets of\ntraining data would break data secrecy.\nContextual Integrity of Data. The above privacy threat\ncorresponds to a narrow view of data privacy as data secrecy.\n5Since the training data is sourced from the public Web, all the outputs\nof our extraction attacks can also be found via Internet searches. Indeed,\nto evaluate whether we have found memorized content, we search for the\ncontent on the Internet and are able to ﬁnd these examples relatively easily.\n4\nA broader view of the privacy risks posed by data extrac-\ntion stems from the framework of data privacy as contextual\nintegrity [48]. That is, data memorization is a privacy in-\nfringement if it causes data to be used outside of its intended\ncontext. An example violation of contextual integrity is shown\nin Figure 1. This individual’s name, address, email, and phone\nnumber are not secret—they were shared online in a speciﬁc\ncontext of intended use (as contact information for a software\nproject)—but are reproduced by the LM in a separate context.\nDue to failures such as these, user-facing applications that use\nLMs may inadvertently emit data in inappropriate contexts,\ne.g., a dialogue system may emit a user’s phone number in\nresponse to another user’s query.\nSmall-k Eidetic Risks. We nevertheless focus on k-eidetic\nmemorization with a smallk value because it makes extraction\nattacks more impactful.While there are cases where large-k\nmemorization may still matter (for example, a company may\nrefer to the name of an upcoming product multiple times in\nprivate—and even though it is discussed often the name itself\nmay still be sensitive) we study the small-k case.\nMoreover, note that although we frame our paper as an “at-\ntack”, LMs will output memorized dataeven in the absence of\nan explicit adversary. We treat LMs as black-box generative\nfunctions, and the memorized content that we extract can be\ngenerated through honest interaction with the LM. Indeed, we\nhave even discovered at least one memorized training exam-\nple among the 1,000 GPT-3 samples that OpenAI originally\nreleased in its ofﬁcial repository [49].\n3.4 Ethical Considerations\nIn this paper, we will discuss and carefully examine speciﬁc\nmemorized content that we ﬁnd in our extraction attacks. This\nraises ethical considerations as some of the data that we ex-\ntract contains information about individual users.\nAs previously mentioned, we minimize ethical concerns by\nusing data that is already public. We attack the GPT-2 model,\nwhich is available online. Moreover, the GPT-2 training data\nwas collected from the public Internet [54], and is in principle\navailable to anyone who performs the same (documented)\ncollection process as OpenAI, e.g., see [23].\nHowever, there are still ethical concerns even though the\nmodel and data are public. It is possible—and indeed we\nﬁnd it is the case—that we might extract personal informa-\ntion for individuals from the training data. For example, as\nshown in Figure 1, we recovered a person’s full name, ad-\ndress, and phone number. In this paper, whenever we succeed\nin extracting personally-identifying information (usernames,\nphone numbers, etc.) we partially mask out this content with\nthe token . We are aware of the fact that this does not\nprovide complete mediation: disclosing that the vulnerability\nexists allows a malicious actor to perform these attacks on\ntheir own to recover this personal information.\nJust as responsible disclosure still causes some (limited)\nharm, we believe that the beneﬁts of publicizing these attacks\noutweigh the potential harms. Further, to make our attacks\npublic, we must necessarily reveal some sensitive information.\nWe contacted the individual whose information is partially\nshown in Figure 1 to disclose this fact to them in advance\nand received permission to use this example. Our research\nﬁndings have also been disclosed to OpenAI.\nUnfortunately, we cannot hope to contact all researchers\nwho train large LMs in advance of our publication. We thus\nhope that this publication will spark further discussions on the\nethics of memorization and extraction among other companies\nand research teams that train large LMs [2, 36, 55, 63].\n4 Initial Training Data Extraction Attack\nWe begin with a simple strawman baseline for extracting\ntraining data from a language model in a two-step procedure.\n• Generate text. We generate a large quantity of data by\nunconditionally sampling from the model (Section 4.1).\n• Predict which outputs contain memorized text. We\nnext remove the generated samples that are unlikely to\ncontain memorized text using a membership inference\nattack (Section 4.2).\nThese two steps correspond directly to extracting model\nknowledge (Deﬁnition 1), and then predicting which strings\nmight be k-eidetic memorization (Deﬁnition 2).\n4.1 Initial Text Generation Scheme\nTo generate text, we initialize the language model with a one-\ntoken prompt containing a special start-of-sentence token and\nthen repeatedly sample tokens in an autoregressive fashion\nfrom the model (see Section 2.1 for background). We hope\nthat by sampling according to the model’s assigned likelihood,\nwe will sample sequences that the model considers “highly\nlikely”, and that likely sequences correspond to memorized\ntext. Concretely, we sample exactly 256 tokens for each trial\nusing the top-n strategy from Section 2.1 with n = 40.\n4.2 Initial Membership Inference\nGiven a set of samples from the model, the problem of training\ndata extraction reduces to one of membership inference: pre-\ndict whether each sample was present in the training data [65].\nIn their most basic form, past membership inference attacks\nrely on the observation that models tend to assign higher con-\nﬁdence to examples that are present in the training data [46].\nTherefore, a potentially high-precision membership inference\nclassiﬁer is to simply choose examples that are assigned the\nhighest likelihood by the model.\nSince LMs are probabilistic generative models, we follow\nprior work [8] and use a natural likelihood measure: the per-\n5\nplexity of a sequence measures how well the LM “predicts”\nthe tokens in that sequence. Concretely, given a sequence of\ntokens x1,..., xn, the perplexity is deﬁned as\nP = exp\n(\n−1\nn\nn\n∑\ni=1\nlog fθ(xi|x1,..., xi−1)\n)\nThat is, if the perplexity is low, then the model is not very\n“surprised” by the sequence and has assigned on average a\nhigh probability to each subsequent token in the sequence.\n4.3 Initial Extraction Results\nWe generate 200,000 samples using the largest version of\nthe GPT-2 model (XL, 1558M parameters) following the text\ngeneration scheme described in Section 4.1. We then sort\nthese samples according to the model’s perplexity measure\nand investigate those with the lowest perplexity.\nThis simple baseline extraction attack can ﬁnd a wide va-\nriety of memorized content. For example, GPT-2 memorizes\nthe entire text of the MIT public license, as well as the user\nguidelines of Vaughn Live, an online streaming site. While\nthis is “memorization”, it is onlyk-eidetic memorization for\na large value of k—these licenses occur thousands of times.\nThe most interesting (but still not eidetic memorization for\nlow values of k) examples include the memorization of popu-\nlar individuals’ Twitter handles or email addresses (omitted\nto preserve user privacy). In fact, all memorized content we\nidentify in this baseline setting is likely to have appeared in\nthe training dataset many times.\nThis initial approach has two key weaknesses that we can\nidentify. First, our sampling scheme tends to produce a low\ndiversity of outputs. For example, out of the 200,000 samples\nwe generated, several hundred are duplicates of the memo-\nrized user guidelines of Vaughn Live.\nSecond, our baseline membership inference strategy suffers\nfrom a large number of false positives, i.e., content that is\nassigned high likelihood but is not memorized. The majority\nof these false positive samples contain “repeated” strings (e.g.,\nthe same phrase repeated multiple times). Despite such text\nbeing highly unlikely, large LMs often incorrectly assign high\nlikelihood to such repetitive sequences [30].\n5 Improved Training Data Extraction Attack\nThe proof-of-concept attack presented in the previous section\nhas low precision (high-likelihood samples are not always in\nthe training data) and low recall (it identiﬁes no k-memorized\ncontent for low k). Here, we improve the attack by incorporat-\ning better methods for sampling from the model (Section 5.1)\nand membership inference (Section 5.2).\n5.1 Improved Text Generation Schemes\nThe ﬁrst step in our attack is to randomly sample from the lan-\nguage model. Above, we used top-n sampling and conditioned\nthe LM on the start-of-sequence token as input. This strategy\nhas clear limitations [32]: it will only generate sequences that\nare likely from beginning to end. As a result, top-n sampling\nfrom the model will cause it to generate the same (or similar)\nexamples several times. Below we describe two alternative\ntechniques for generating more diverse samples from the LM.\n5.1.1 Sampling With A Decaying Temperature\nAs described in Section 2.1, an LM outputs the probability of\nthe next token given the prior tokens Pr(xi |x1,..., xi−1). In\npractice, this is achieved by evaluating the neural networkz =\nfθ(x1,..., xi−1) to obtain the “logit” vectorz, and then com-\nputing the output probability distribution as y = softmax(z)\ndeﬁned by softmax(z)i = exp(zi)/∑n\nj=1 exp(zj).\nOne can artiﬁcially “ﬂatten” this probability distribution\nto make the model less conﬁdent by replacing the output\nsoftmax(z) with softmax(z/t), for t > 1. Here, t is called the\ntemperature. A higher temperature causes the model to be\nless conﬁdent and more diverse in its output.\nHowever, maintaining a high temperature throughout the\ngeneration process would mean that even if the sampling\nprocess began to emit a memorized example, it would likely\nrandomly step off the path of the memorized output. Thus,\nwe use a softmax temperature that decays over time, starting\nat t = 10 and decaying down to t = 1 over a period of the\nﬁrst 20 tokens (≈10% of the length of the sequence). This\ngives a sufﬁcient amount of time for the model to “explore”\na diverse set of preﬁxes while also allowing it to follow a\nhigh-conﬁdence paths that it ﬁnds.\n5.1.2 Conditioning on Internet Text\nEven when applying temperature sampling, there are still\nsome preﬁxes that are unlikely to be sampled but nevertheless\noccur in actual data. As a ﬁnal strategy, our third sampling\nstrategy seeds the model with preﬁxes from our own scrapes\nof the Internet. This sampling strategy ensures that we will\ngenerate samples with a diverse set of preﬁxes that are similar\nin nature to the type of data GPT-2 was trained on.\nWe follow a different data collection process as used in\nGPT-2 (which follows Reddit links) in order to reduce the like-\nlihood that our dataset has any intersection with the model’s\ntraining data. In particular, we select samples from a subset\nof Common Crawl6 to feed as context to the model.7\n6http://commoncrawl.org/\n7It is possible there is some intersection between these two datasets, effec-\ntively allowing this strategy to “cheat”. We believe this does not considerably\naffect results. First, any overlap between the two datasets is rare on average.\nSecond, because we only use between the ﬁrst 5 to 10 tokens of each sample,\nany possible overlap will be small in absolute terms.\n6\n 200,000 LM \nGenerationsLM (GPT-2) Sorted \nGenerations\n(using one of 6 metrics)\nDeduplicate\nTraining Data Extraction Attack\nPreﬁxes\nEvaluation\nInternet \nSearch\nChoose \nTop-100\nCheck \nMemorization\nMatch\nNo\nMatch\nFigure 2: Workﬂow of our extraction attack and evaluation. 1) Attack. We begin by generating many samples from GPT-2\nwhen the model is conditioned on (potentially empty) preﬁxes. We then sort each generation according to one of six metrics and\nremove the duplicates. This gives us a set of potentially memorized training examples. 2) Evaluation. We manually inspect\n100 of the top-1000 generations for each metric. We mark each generation as either memorized or not-memorized by manually\nsearching online, and we conﬁrm these ﬁndings by working with OpenAI to query the original training data. An open-source\nimplementation of our attack process is available at https://github.com/ftramer/LM_Memorization.\nAs in prior work [55], we perform basic data-sanitization\nby removing HTML and JavaScript from webpages, and we\nde-duplicate data on a line-by-line basis. This gives us a\ndataset of 50MB of text. We randomly sample between 5 and\n10 tokens of context from this scraped data and then continue\nLM generation with top-n sampling as in Section 4.1.\n5.2 Improved Membership Inference\nPerforming membership inference by ﬁltering out samples\nwith low likelihood has poor precision due to failures in the\nunderlying language model: there are many samples that are\nassigned spuriously high likelihood. There are predominantly\ntwo categories of such samples:\n• Trivial memorization. We identify many cases where\nGPT-2 outputs content that is uninteresting because of\nhow common the text is. For example, it repeats the num-\nbers from 1 to 100 with high probability.\n• Repeated substrings. One common failure mode of LMs\nis their propensity to repeatedly emit the same string over\nand over [30, 37]. We found many of the high-likelihood\nsamples that are not memorized are indeed repeated texts\n(e.g., “I love you. I love you. . . ”).\nOur insight is that we can ﬁlter out these uninteresting (yet\nstill high-likelihood samples) by comparing to a second LM.\nGiven a second model that accurately captures text likelihood,\nwe should expect it will also assign high likelihood to these\nforms of memorized content. Therefore, a natural strategy\nfor ﬁnding more diverse and rare forms of memorization\nis to ﬁlter samples where the original model’s likelihood is\n“unexpectedly high” compared to a second model. Below we\ndiscuss four methods for achieving this.\nComparing to Other Neural Language Models. Assume\nthat we have access to a second LM that memorizes a different\nset of examples than GPT-2. One way to achieve this would be\nto train a model on a disjoint set of training data, in which case\nit is unlikely that the two models will memorize the same data\nfor small k. An alternate strategy is to take a much smaller\nmodel trained on the same underlying dataset: because smaller\nmodels have less capacity for memorization, we conjecture\nthat there are samples that are k-eidetic memorized (for small\nk) by the largest GPT-2 model, but which are not memorized\nby smaller GPT-2 models. Speciﬁcally, we use the Small\n(117M parameters) and Medium (345M parameters) models.\nComparing to zlib Compression. It is not necessary that\nwe compare to another neural LM; any technique that quan-\ntiﬁes some notion of “surprise” for a given sequence can be\nuseful. As a simple baseline method, we compute the zlib [41]\nentropy of the text: the number of bits of entropy when the\nsequence is compressed with zlib compression. We then use\nthe ratio of the GPT-2 perplexity and the zlib entropy as our\nmembership inference metric. Although text compressors are\nsimple, they can identify many of the examples of trivial mem-\norization and repeated patterns described above (e.g., they are\nexcellent at modeling repeated substrings).\nComparing to Lowercased Text. Instead of detecting\nmemorization by comparing one model to another model,\nanother option detects memorization by comparing the per-\nplexity of the model to the perplexity of the same model on a\n“canonicalized” version of that sequence. Speciﬁcally, we mea-\nsure the ratio of the perplexity on the sample before and after\nlowercasing it, which can dramatically alter the perplexity of\nmemorized content that expects a particular casing.\n7\nPerplexity on a Sliding Window. Sometimes a model is\nnot conﬁdent when the sample contains one memorized sub-\nstring surrounded by a block of non-memorized (and high\nperplexity) text. To handle this, we use the minimum perplex-\nity when averaged over a sliding window of 50 tokens.8\n6 Evaluating Memorization\nWe now evaluate the various data extraction methods and\nstudy common themes in the resulting memorized content.\n6.1 Methodology\nAn overview of our experimental setup is shown in Figure 2.\nWe ﬁrst build three datasets of 200,000 generated samples\n(each of which is 256 tokens long) using one of our strategies:\n• Top-n (§4.1) samples naively from the empty sequence.\n• Temperature(§5.1.1) increases diversity during sampling.\n• Internet (§5.1.2) conditions the LM on Internet text.\nWe order each of these three datasets according to each of\nour six membership inference metrics:\n• Perplexity: the perplexity of the largest GPT-2 model.\n• Small: the ratio of log-perplexities of the largest GPT-2\nmodel and the Small GPT-2 model.\n• Medium: the ratio as above, but for the Medium GPT-2.\n• zlib: the ratio of the (log) of the GPT-2 perplexity and the\nzlib entropy (as computed by compressing the text).\n• Lowercase: the ratio of perplexities of the GPT-2 model\non the original sample and on the lowercased sample.\n• Window: the minimum perplexity of the largest GPT-2\nmodel across any sliding window of 50 tokens.\nFor each of these 3 ×6 = 18 conﬁgurations, we select 100\nsamples from among the top-1000 samples according to the\nchosen metric.9 This gives us 1,800 total samples of poten-\ntially memorized content. In real-world attacks, adversaries\nwill look to uncover large amounts of memorized content and\nthus may generate many more samples. We focus on a smaller\nset as a proof-of-concept attack.\nData De-Duplication. To avoid “double-counting” memo-\nrized content, we apply an automated fuzzy de-duplication\nstep when we select the 100 samples for each conﬁguration.\nGiven a sample s, we deﬁne the trigram-multiset of s, de-\nnoted tri(s) as a multiset of all word-level trigrams ins (with\nwords split on whitespace and punctuation characters). For\nexample, the sentence “my name my name my name” has\ntwo trigrams (“my name my” and ”name my name”) each of\n8Chosen after a cursory hyper-parameter sweep and manual analysis.\n9To favor low-ranked samples, while also exploring some of the higher-\nranked samples, we select the 100 samples so that the fraction of selected\nsamples with rank below k is\n√\nk/1000.\nmultiplicity 2. We mark a sample s1 as a duplicate of another\nsample s2, if their trigram multisets are similar, speciﬁcally if\n|tri(s1) ∩tri(s2)|≥|tri(s1)|/2.\nEvaluating Memorization Using Manual Inspection.\nFor each of the 1,800 selected samples, one of four authors\nmanually determined whether the sample contains memo-\nrized text. Since the training data for GPT-2 was sourced\nfrom the public Web, our main tool is Internet searches. We\nmark a sample as memorized if we can identify a non-trivial\nsubstring that returns an exact match on a page found by a\nGoogle search.\nValidating Results on the Original Training Data. Fi-\nnally, given the samples that we believe to be memorized,\nwe work with the original authors of GPT-2 to obtain lim-\nited query access to their training dataset. To do this we sent\nthem all 1,800 sequences we selected for analysis. For efﬁ-\nciency, they then performed a fuzzy 3-gram match to account\nfor memorization with different possible tokenizations. We\nmarked samples as memorized if all 3-grams in the mem-\norized sequence occurred in close proximity in the training\ndataset. This approach eliminates false negatives, but has false\npositives. It can conﬁrm that our samples are memorized but\ncannot detect cases where we missed memorized samples.\nIn some experiments below, we report exact counts for how\noften a particular sequence occurs in the training data. We\nobtained these counts by asking the GPT-2 authors to perform\na separate grep over the entire dataset to get an exact count.\n6.2 Results\nIn total across all strategies, we identify 604 unique memo-\nrized training examples from among the 1,800 possible can-\ndidates, for an aggregate true positive rate of 33.5% (our best\nvariant has a true positive rate of 67%). Below, we categorize\nwhat types of content is memorized by the model, and also\nstudy which attack methods are most effective.\nCategories of Memorized Content. We manually grouped\nthe memorized samples into different categories (a descrip-\ntion of these categories is in Appendix A). The results are\nshown in Table 1. Most memorized content is fairly canonical\ntext from news headlines, log ﬁles, entries from forums or\nwikis, or religious text. However, we also identify a signiﬁcant\namount of unique data, containing 128-bit UUIDs, (correctly-\nresolving) URLs containing random substrings, and contact\ninformation of individual people and corporations. In Sec-\ntion 6.3, we study these cases in more detail.\nEfﬁcacy of Different Attack Strategies. Table 2 shows\nthe number of memorized samples broken down by the dif-\nferent text generation and membership inference strategies.\n8\nCategory Count\nUS and international news 109\nLog ﬁles and error reports 79\nLicense, terms of use, copyright notices 54\nLists of named items (games, countries, etc.) 54\nForum or Wiki entry 53\nValid URLs 50\nNamed individuals (non-news samples only) 46\nPromotional content (products, subscriptions, etc.) 45\nHigh entropy (UUIDs, base64 data) 35\nContact info (address, email, phone, twitter, etc.) 32\nCode 31\nConﬁguration ﬁles 30\nReligious texts 25\nPseudonyms 15\nDonald Trump tweets and quotes 12\nWeb forms (menu items, instructions, etc.) 11\nTech news 11\nLists of numbers (dates, sequences, etc.) 10\nTable 1: Manual categorization of the 604 memorized training\nexamples that we extract from GPT-2, along with a descrip-\ntion of each category. Some samples correspond to multiple\ncategories (e.g., a URL may contain base-64 data). Categories\nin bold correspond to personally identiﬁable information.\nSampling conditioned on Internet text is the most effective\nway to identify memorized content, however, all generation\nschemes reveal a signiﬁcant amount of memorized content.\nFor example, the baseline strategy of generating with top-n\nsampling yields 191 unique memorized samples, whereas\nconditioning on Internet text increases this to 273.\nAs discussed earlier, looking directly at the LM perplexity\nis a poor membership inference metric when classifying data\ngenerated with top-n or temperature sampling: just 9% and\n3% of inspected samples are memorized, respectively. The\ncomparison-based metrics are signiﬁcantly more effective at\npredicting if content was memorized. For example, 67% of\nInternet samples marked by zlib are memorized.\nFigure 3 compares the zlib entropy and the GPT-2 XL\nperplexity for each sample, with memorized examples high-\nlighted. Plots for the other strategies are shown in Figure 4 in\nAppendix B. Observe that most samples fall along a diagonal,\ni.e., samples with higher likelihood under one model also have\nhigher likelihood under another model. However, there are\nnumerous outliers in the top left: these samples correspond to\nthose that GPT-2 assigns a low perplexity (a high likelihood)\nbut zlib is surprised by. These points, especially those which\nare extreme outliers, are more likely to be memorized than\nthose close to the diagonal.\nThe different extraction methods differ in the type of mem-\norized content they ﬁnd. A complete breakdown of the data is\ngiven in Appendix A; however, to brieﬂy summarize:\n1 2 3 4 5 6 7 89\nGPT-2 Perplexity\n100\n200\n300\n400\n500\n600\n700\n800zlib Entropy\nAll Samples\nSelected\nMemorized\nFigure 3: The zlib entropy and the perplexity of GPT-2 XL for\n200,000 samples generated with top-n sampling. In red, we\nshow the 100 samples that were selected for manual inspec-\ntion. In blue, we show the 59 samples that were conﬁrmed\nas memorized text. Additional plots for other text generation\nand detection strategies are in Figure 4.\n1. The zlib strategy often ﬁnds non-rare text (i.e., has a high\nk-memorization). It often ﬁnds news headlines, license\nﬁles, or repeated strings from forums or wikis, and there\nis only one “high entropy” sequence this strategy ﬁnds.\n2. Lower-casing ﬁnds content that is likely to have irregular\ncapitalization, such as news headlines (where words are\ncapitalized) or error logs (with many uppercase words).\n3. The Small and Medium strategies often ﬁnd rare content.\nThere are 13 and 10 high entropy examples found by us-\ning the Small and Medium GPT-2 variants, respectively\n(compared to just one with zlib).\n6.3 Examples of Memorized Content\nWe next manually analyze categories of memorized content\nthat we ﬁnd particularly compelling. (Additional examples\nare presented in Appendix C.) Recall that since GPT-2 is\ntrained on public data, our attacks are not particularly severe.\nNevertheless, we ﬁnd it useful to analyze what we are able to\nextract to understand the categories of memorized content—\nwith the understanding that attacking a model trained on a\nsensitive dataset would give stronger results.\nPersonally Identiﬁable Information. We identify numer-\nous examples of individual peoples’ names, phone numbers,\naddresses, and social media accounts.\n9\nInference\nStrategy\nText Generation Strategy\nTop-n Temperature Internet\nPerplexity 9 3 39\nSmall 41 42 58\nMedium 38 33 45\nzlib 59 46 67\nWindow 33 28 58\nLowercase 53 22 60\nTotal Unique 191 140 273\nTable 2: The number of memorized examples (out of 100\ncandidates) that we identify using each of the three text gen-\neration strategies and six membership inference techniques.\nSome samples are found by multiple strategies; we identify\n604 unique memorized examples in total.\nWe ﬁnd 46 examples that contain individual peoples’\nnames. When counting occurrences of named individuals,\nwe omit memorized samples that relate to national and in-\nternational news (e.g., if GPT-2 emits the name of a famous\npolitician, we do not count this as a named individual here).\nWe further ﬁnd 32 examples that contain some form of contact\ninformation (e.g., a phone number or social media handle).\nOf these, 16 contain contact information for businesses, and\n16 contain private individuals’ contact details.\nSome of this memorized content is exclusive to just a few\ndocuments. For example, we extract the usernames of six\nusers participating in an IRC conversation that appeared in\nexactly one training document.\nURLs. We identify 50 examples of memorized URLs that\ncorrectly resolve to live webpages. Many of these URLs con-\ntain uncommon pieces of text, such as random numbers or\nbase-64 encoded strings. We also identify several URLs that\nresolve correctly but we cannot identify their source (and we\nthus do not count them as “memorized” in our evaluation).\nCode. We identify 31 generated samples that contain snip-\npets of memorized source code. Despite our ability to recover\nthe source code verbatim, we are almost always unable to\nrecover the original authorship notices or terms of use. Often,\nthis information is given either before the code itself or in a\nLICENSE ﬁle that appears separately. For many of these sam-\nples, we can also extend their length and recover thousands\nof lines of (near verbatim) source code (see Section 6.4).\nUnnatural Text. Memorization is not limited to natural-\nlooking text. We ﬁnd 21 instances of random number se-\nquences with at least 50 bits of entropy.10 For example, we\n10We estimate the entropy through manual analysis by guessing the entropy\nspace given the format of the string.\nMemorized\nString\nSequence\nLength\nOccurrences in Data\nDocs Total\nY2... ...y5 87 1 10\n7C... ...18 40 1 22\nXM... ...WA 54 1 36\nab... ...2c 64 1 49\nff... ...af 32 1 64\nC7... ...ow 43 1 83\n0x... ...C0 10 1 96\n76... ...84 17 1 122\na7... ...4b 40 1 311\nTable 3: Examples of k = 1 eidetic memorized, high-\nentropy content that we extractfrom the training data. Each\nis contained in just one document. In the best case, we extract\na 87-characters-long sequence that is contained in the training\ndataset just 10 times in total, all in the same document.\nextract the following UUID:\n1e4bd2a8-e8c8-4a62-adcd-40a936480059\nfrom the model; a Google search for this string identiﬁes just\n3 documents containing this UUID, and it is contained in just\none GPT-2 training document (i.e., it is 1-eidetic memorized).\nOther memorized random number sequences include UUIDs\ncontained in only a few documents (not listed to preserve\nprivacy), git commit hashes, random IDs used for ad tracking,\nand product model numbers.\nTable 3 gives nine examples of k = 1 eidetic memorized\ncontent, each of which is a random sequences between 10\nand 87 characters long. In each of these cases, the memorized\nexample is contained in exactly one training document, and\nthe total number of occurrences within that single document\nvaries between just 10 and 311.\nData From Two Sources. We ﬁnd samples that contain\ntwo or more snippets of memorized text that are unrelated to\none another. In one example, GPT-2 generates a news article\nabout the (real) murder of a woman in 2013, but then attributes\nthe murder to one of the victims of a nightclub shooting in\nOrlando in 2016. Another sample starts with the memorized\nInstagram biography of a pornography producer, but then goes\non to incorrectly describe an American fashion model as a\npornography actress. This type of generation is not k-eidetic\nmemorization (these independent pieces of information never\nappear in the same training documents), but it is an example\nof a contextual integrity violation.\nRemoved Content. Finally, GPT-2 memorizes content that\nhas since been removed from the Internet, and is thus now\nprimarily accessible through GPT-2. We are aware of this\ncontent as it is still cached by Google search, but is no longer\n10\npresent on the linked webpage. Some of this data is not par-\nticularly interesting in its own right, e.g., error logs due to a\nmisconﬁgured webserver that has since been ﬁxed. However,\nthe fact that this type of memorization occurs highlights that\nLMs that are trained entirely on (at-the-time) public data may\nend up serving as an unintentional archive for removed data.\n6.4 Extracting Longer Verbatim Sequences\nIn our previous experiments, we extract strings of 256 tokens\nin length. Here, we brieﬂy investigate if we can extract longer\nsequences. In particular, we extend the length of some of the\nmemorized sequences by seeding the model with each sample\nand continuing to generate. To do this, we apply a beam-\nsearch-like decoding method introduced in prior work [8]\ninstead of greedy decoding which often fails to generate long\nverbatim sequences.\nWe can extend many of the memorized samples. For exam-\nple, we identify a piece of source code taken from a repository\non GitHub. We can extend this snippet to extract an entire\nﬁle, namely 1450 lines of verbatim source code. We can\nalso extract the entirety of the MIT, Creative Commons, and\nProject Gutenberg licenses. This indicates that while we have\nextracted 604 memorized examples, we could likely extend\nmany of these to much longer snippets of memorized content.\n6.5 Memorization is Context-Dependent\nConsistent with recent work on constructing effective\n“prompts” for generative LMs [7, 62], we ﬁnd that the memo-\nrized content is highly dependent on the model’s context.\nFor example, GPT-2 will complete the prompt “3.14159”\nwith the ﬁrst 25 digits of π correctly using greedy sampling.\nHowever, we ﬁnd that GPT-2 “knows” (under Deﬁnition 2)\nmore digits of π because using the beam-search-like strategy\nintroduced above extracts 500 digits correctly.\nInterestingly, by providing the more descriptive prompt\n“pi is 3.14159”, straight greedy decoding gives the ﬁrst799\ndigits of π—more than with the sophisticated beam search.\nFurther providing the context “e begins 2.7182818, pi begins\n3.14159”, GPT-2 greedily completes the ﬁrst 824 digits of π.\nThis example demonstrates the importance of the context:\nin the right setting, orders of magnitude more extraction is\nfeasible than when the context is just slightly suboptimal.\nWe ﬁnd that this holds true for our memorized examples as\nwell. None of the 273 extracted samples found using Internet\nconditioning can be reliably reproduced when using the same\npreﬁx initially provided to GPT-2 that produced this sample.\nHowever, nearly all can be reproduced with high probability\nif we provided the entire sequence of data up to (but not\nincluding) the beginning of the memorized content.\nThe important lesson here is that our work vastly under-\nestimates the true amount of content that GPT-2 memorized.\nThere are likely prompts that would identify much more mem-\norized content, but because we stick to simple prompts we do\nnot ﬁnd this memorized content.\n7 Correlating Memorization with\nModel Size & Insertion Frequency\nThus far, we have shown that language models can memorize\nverbatim training strings, even when they are trained for few\nepochs and achieve small train-test accuracy gaps. A natural\nquestion is how many times a string must appear for it to be\nmemorized (i.e.,k in Deﬁnition 2). Prior work has investigated\nLM memorization by varying the number of times particular\n“canary” tokens were inserted into a training dataset [8]. The\nmain limitation of this approach is that it is synthetic: canaries\nare inserted artiﬁcially after the dataset has been collected\nand may not be representative of natural data.\nHere, we study how well GPT-2 memorizesnaturally oc-\ncurring canaries in the training data. In particular, we consider\na piece of memorized content with the following preﬁx:\n{\"color\":\"fuchsia\",\"link\":\"https://www.\nreddit.com/r/The_Donald/comments/\nThe reddit.com URL above is completed by a speciﬁc\n6-character article ID and a title. We located URLs in this\nspeciﬁc format in a single document on pastebin.com. Each\nURL appears a varying number of times in this document,\nand hence in the GPT-2 training dataset. 11 Table 4 shows\na subset of the URLs that appear more than once, and their\nrespective counts in the document. 12 This allows us to ask\nthe question: how many times must an example appear in the\ntraining dataset for us to extract it?\nMethods. We attempt two approaches to extract URLs of\nthis format, and run three variants of GPT-2 (XL, Medium, and\nSmall). The two approaches vary the “difﬁculty” of the attack,\nso even if the more difﬁcult fails the easier may succeed.\nFirst, we directly prompt each variant of GPT-2 with the\npreﬁx above, and use top-n sampling to generate 10,000 pos-\nsible extensions. Then, we test whether any of the URLs in\nthe training document were among those that were emitted\nby GPT-2. We count a URL as emitted if it matches verbatim\nwith one of the 10,000 generations.\nSome URLs are not extractable with this technique, and\nso we make the problem easier for GPT-2 by additionally\nproviding GPT-2 the 6-character random token that begins\neach URL. Given this additional preﬁx, we then sample from\n11The purpose of this text dump was to tag users of Reddit who posted\nfrequently on speciﬁc topics. In doing so, this page repeats some of the same\nlinks many times because many users comment on the same links.\n12We conﬁrmed with OpenAI that the counts here are within 5% of the\ntrue counts of these URLs in the training data.\n11\nOccurrences Memorized?\nURL (trimmed) Docs Total XL M S\n/r/ 51y/milo_evacua... 1 359 ✓ ✓ 1/2\n/r/ zin/hi_my_name... 1 113 ✓ ✓\n/r/ 7ne/for_all_yo... 1 76 ✓ 1/2\n/r/ 5mj/fake_news_... 1 72 ✓\n/r/ 5wn/reddit_admi... 1 64 ✓ ✓\n/r/ lp8/26_evening... 1 56 ✓ ✓\n/r/ jla/so_pizzagat... 1 51 ✓ 1/2\n/r/ ubf/late_night... 1 51 ✓ 1/2\n/r/ eta/make_christ... 1 35 ✓ 1/2\n/r/ 6ev/its_ofﬁcia... 1 33 ✓\n/r/ 3c7/scott_adams... 1 17\n/r/ k2o/because_his... 1 17\n/r/ tu3/armynavy_ga... 1 8\nTable 4: We show snippets of Reddit URLs that appear a\nvarying number of times in a single training document. We\ncondition GPT-2 XL, Medium, or Small on a prompt that\ncontains the beginning of a Reddit URL and report a ✓ if\nthe corresponding URL was generated verbatim in the ﬁrst\n10,000 generations. We report a1/2 if the URL is generated by\nproviding GPT-2 with the ﬁrst 6 characters of the URL and\nthen running beam search.\nthe model using the beam search procedure. This task is eas-\nier in two ways: we have ﬁrst provided more context and\nadditionally use a higher recall sampling strategy.\nResults. Table 4 summarizes the key results. Under the\nmore difﬁcult of the two approaches, the full-sized 1.5 billion\nparameter GPT-2 model emits all examples that are inserted\n33 times or more, the medium-sized 345 million parameter\nmemorizes half of the URLs, and the smallest 117 million\nparameter model memorizes none of these URLs.\nWhen given the additional context and using beam search,\nthe medium model can emit four more URLs, and the small\nmodel only emits the one URL that was inserted 359 times.\nThese results illustrate two fundamental lessons in LM\nmemorization. First, larger models memorize signiﬁcantly\nmore training data: even hundreds of millions of parameters\nare not enough to memorize some of the training points. The\nability of LMs to improve with model size has been exten-\nsively studied [35, 38]; we show a negative trend where these\nimprovements come at the cost of decreased privacy. Second,\nfor the largest LM, complete memorization occurs after just\n33 insertions. This implies that any potentially sensitive infor-\nmation that is repeated a non-trivial amount of times is at risk\nfor memorization, even if it was only repeated multiple times\nin a single training document.\n8 Mitigating Privacy Leakage in LMs\nNow that we have shown that memorized training data can\nbe extracted from LMs, a natural question is how to mitigate\nthese threats. Here we describe several possible strategies.\nTraining With Differential Privacy. Differential privacy\n(DP) [13, 14] is a well-established notion of privacy that of-\nfers strong guarantees on the privacy of individual records in\nthe training dataset. Private machine learning models can be\ntrained with variants of the differentially private stochastic gra-\ndient descent (DP-SGD) algorithm [1] which is widely imple-\nmented [17, 25]. Large companies have even used DP in pro-\nduction machine learning models to protect users’ sensitive\ninformation [15,69]. The tradeoffs between privacy and utility\nof models have been studied extensively: differentially-private\ntraining typically prevents models from capturing the long\ntails of the data distribution and thus hurts utility [19, 20, 67].\nIn the content of language modeling, recent work demon-\nstrates the privacy beneﬁts of user-level DP models [56]. Un-\nfortunately, this work requires labels for which users con-\ntributed each document; such labels are unavailable for data\nscraped from the open Web. It may instead seem natural to\naim for DP guarantees at the granularity of individual web-\npages, but rare snippets of text (e.g., an individual’s name\nand contact information as in Figure 1) might appear in more\nthan one webpage. It is thus unclear how to apply DP in a\nprincipled and effective way on Web data.\nCurating the Training Data. One cannot manually vet the\nextremely large training datasets used for training LMs. How-\never, there are methods to limit the amount of sensitive con-\ntent that is present, e.g., by identifying and ﬁltering personal\ninformation or content with restrictive terms of use [11, 58].\nAside from attempting to remove sensitive content, it is\nalso important to carefully de-duplicate the data. Many lan-\nguage modeling datasets are de-duplicated at the document-\nor paragraph-level, which means that a single document can\nstill contain many repeated occurrences of a sensitive piece\nof content. We envision more sophisticated strategies to de-\nduplicate the training data, or limit the contribution of any\nsingle source of training data.\nIt is also vital to carefully source the training data. Many of\nthe potentially-sensitive training examples that we extracted\n(e.g., individuals’ personal information) came from websites\nthat are known to host sensitive content, e.g.,pastebin is the\n12th most popular domain in GPT-2’s training set.\nOverall, sanitizing data is imperfect—some private data\nwill always slip through—and thus it serves as a ﬁrst line of\ndefense and not an outright prevention against privacy leaks.\nLimiting Impact of Memorization on Downstream Appli-\ncations. In many downstream applications, e.g., dialogue\n12\nsystems [76] and summarization models [29], LMs are ﬁne-\ntuned on task-speciﬁc data. On the positive side, this ﬁnetun-\ning process may cause the LM to “forget” [42, 57] some of\nthe data that is memorized during the pre-training stage. On\nthe negative side, ﬁne-tuning may introduce its own privacy\nleakages if the task-speciﬁc data also contains private infor-\nmation. An interesting direction for future work is to explore\nhow memorization is inherited by ﬁne-tuned models.\nDownstream applications built on top of language models\ncould also attempt to ﬁlter out generated text that contains\nmemorized content, if such content can be reliably detected\n(e.g., using various membership inference strategies).\nAuditing ML Models for Memorization. Finally, after\nmitigating privacy leaks, it is vital to audit models to empiri-\ncally determine the privacy level they offer in practice [33].\nAuditing is important even when using differential privacy,\nas it can complement theoretical upper bounds on privacy\nleakage [1]. We envision using our proposed methods, as well\nas existing attacks [8, 33, 65, 72], to audit LMs.\n9 Lessons and Future Work\nExtraction Attacks Are a Practical Threat. Prior work\nshows that (100×to 1000×smaller) language models poten-\ntially memorize training data in semi-realistic settings [8, 73].\nOur results show that state-of-the-art LMs do memorize their\ntraining data in practice, and that adversaries can extract this\ndata with simple techniques. Our attacks are practical even\nwhen the data contains a given sequence only a few times.\nAs our attacks interact with a language model as a black-\nbox, our results approximate the worst-case behavior of lan-\nguage models when interacting with benign users. In particu-\nlar, among 600,000 (honestly) generated samples, our attacks\nﬁnd that at least 604 (or 0.1%) contain memorized text.\nNote that this is likely an extremely loose lower bound. We\nonly manually inspected1,800 potential candidate memorized\nsamples; if we had started with more candidates we would\nlikely have identiﬁed signiﬁcantly more memorized content.\nDeveloping improved techniques for extracting memorized\ndata, including attacks that are targeted towards speciﬁc con-\ntent, is an interesting area for future work.\nMemorization Does Not Require Overﬁtting. It is often\nbelieved that preventing overﬁtting (i.e., reducing the train-\ntest generalization gap) will prevent models from memorizing\ntraining data. However, large LMs have no signiﬁcant train-\ntest gap, and yet we still extract numerous examples verbatim\nfrom the training set. The key reason is that even though\non average the training loss is only slightly lower than the\nvalidation loss, there are still some training examples that have\nanomalously low losses. Understanding why this happens is\nan important problem for future work [6, 40].\nLarger Models Memorize More Data. Throughout our\nexperiments, larger language models consistently memorized\nmore training data than smaller LMs. For example, in one\nsetting the 1.5 billion parameter GPT-2 model memorizes\nover 18×as much content as the124 million parameter model\n(Section 7). Worryingly, it is likely that as LMs become bigger\n(in fact they already are100×larger than the GPT-2 model we\nstudy [7]), privacy leakage will become even more prevalent.\nMemorization Can Be Hard to Discover. Much of the\ntraining data that we extract is only discovered when prompt-\ning the LM with a particular preﬁx. Currently, we simply\nattempt to use high-quality preﬁxes and hope that they might\nelicit memorization. Better preﬁx selection strategies [62]\nmight identify more memorized data.\nAdopt and Develop Mitigation Strategies. We discuss\nseveral directions for mitigating memorization in LMs, in-\ncluding training with differential privacy, vetting the training\ndata for sensitive content, limiting the impact on downstream\napplications, and auditing LMs to test for memorization. All\nof these are interesting and promising avenues of future work,\nbut each has weaknesses and are incomplete solutions to\nthe full problem. Memorization in modern LMs must be ad-\ndressed as new generations of LMs are emerging and becom-\ning building blocks for a range of real-world applications.\n10 Conclusion\nFor large language models to be widely adopted, they must\naddress the training data memorization problems that we have\nidentiﬁed. Our extraction attacks are practical and efﬁcient,\nand can recover hundreds of training examples from a model,\neven when they are contained in just one training document.\nOur analysis is best viewed as a cautionary tale of what\ncould happen when training large LMs on sensitive data. Even\nthough our attacks target GPT-2 (which allows us to ensure\nthat our work is not harmful), the same techniques apply\nto any LM. Moreover, because memorization gets worse as\nLMs become larger, we expect that these vulnerabilities will\nbecome signiﬁcantly more important in the future.\nThere will therefore need to be techniques developed to\nspeciﬁcally address our attacks. Training with differentially-\nprivate techniques is one method for mitigating privacy leak-\nage, however, we believe that it will be necessary to develop\nnew methods that can train models at this extreme scale (e.g.,\nbillions of parameters) without sacriﬁcing model accuracy\nor training time. More generally, there are many open ques-\ntions that we hope will be investigated further, including why\nmodels memorize, the dangers of memorization, and how to\nprevent memorization.\n13\nAcknowledgements\nWe are grateful for comments on early versions of this paper\nby Dan Boneh, Andreas Terzis, Carey Radebaugh, Daphne Ip-\npolito, Christine Robson, Kelly Cooke, Janel Thamkul, Austin\nTarango, Jack Clark, Ilya Mironov, and Om Thakkar. Florian\nTramèr is supported by NSF award CNS-1804222.\nSummary of Contributions\n• Nicholas, Dawn, Ariel, Tom, Colin and Úlfar proposed the\nresearch question of extracting training data from GPT-2\nand framed the threat model.\n• Colin, Florian, Matthew, and Nicholas stated the memoriza-\ntion deﬁnitions.\n• Florian, Ariel, and Nicholas wrote code to generate candi-\ndate memorized samples from GPT-2 and verify the ground\ntruth memorization.\n• Florian, Nicholas, Matthew, and Eric manually reviewed\nand categorized the candidate memorized content.\n• Katherine, Florian, Eric, and Colin generated the ﬁgures.\n• Adam, Matthew, and Eric ran preliminary investigations in\nlanguage model memorization.\n• Nicholas, Florian, Eric, Colin, Katherine, Matthew, Ariel,\nAlina, Úlfar, Dawn, and Adam wrote and edited the paper.\n• Tom, Adam, and Colin gave advice on language models\nand machine learning background.\n• Alina, Úlfar, and Dawn gave advice on the security goals.\nReferences\n[1] Martín Abadi, Andy Chu, Ian Goodfellow, H Brendan\nMcMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\nDeep learning with differential privacy. In ACM CCS,\n2016.\n[2] Daniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al.\nTowards a human-like open-domain chatbot. arXiv\npreprint arXiv:2001.09977, 2020.\n[3] Jay Alammar. The illustrated transformer. Visualizing\nMachine Learning One Concept at a Time, 2018.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. Neural machine translation by jointly learning to\nalign and translate. In ICLR, 2015.\n[5] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. A neural probabilistic language model.\nJMLR, 2003.\n[6] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith,\nand Kunal Talwar. When is memorization of irrele-\nvant training data necessary for high-accuracy learning?\narXiv preprint arXiv:2012.06421, 2020.\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n[8] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. The secret sharer: Evaluating and\ntesting unintended memorization in neural networks. In\nUSENIX Security Symposium, 2019.\n[9] Kamalika Chaudhuri and Claire Monteleoni. Privacy-\npreserving logistic regression. In NIPS, 2009.\n[10] Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan\nWang, Andrew M Dai, Zhifeng Chen, Timothy Sohn,\nand Yonghui Wu. Gmail smart compose: Real-Time\nassisted writing. In KDD, 2019.\n[11] Andrea Continella, Yanick Fratantonio, Martina Lindor-\nfer, Alessandro Puccetti, Ali Zand, Christopher Kruegel,\nand Giovanni Vigna. Obfuscation-Resilient Privacy\nLeak Detection for Mobile Apps Through Differential\nAnalysis. In NDSS, 2017.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nNAACL, 2019.\n[13] C Dwork, F McSherry, K Nissim, and A Smith. Cali-\nbrating noise to sensitivity in private data analysis. In\nTCC, 2006.\n[14] Cynthia Dwork. Differential privacy: A survey of results.\nIn TAMC, 2008.\n[15] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.\nRAPPOR: Randomized aggregatable privacy-preserving\nordinal response. In ACM CCS, 2014.\n[16] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko,\nSusan M Swetter, Helen M Blau, and Sebastian Thrun.\nDermatologist-level classiﬁcation of skin cancer with\ndeep neural networks. Nature, 2017.\n[17] Facebook. Opacus. https://github.com/pytorch/\nopacus.\n[18] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchi-\ncal neural story generation. In ACL, 2018.\n14\n[19] Vitaly Feldman. Does learning require memorization?\nA short tale about a long tail. In STOC, 2020.\n[20] Vitaly Feldman and Chiyuan Zhang. What neural net-\nworks memorize and why: Discovering the long tail via\ninﬂuence estimation. In NeurIPS, 2020.\n[21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.\nModel inversion attacks that exploit conﬁdence informa-\ntion and basic countermeasures. In ACM CCS, 2015.\n[22] Philip Gage. A new algorithm for data compression. C\nUsers Journal, 12(2):23–38, 1994.\n[23] Aaron Gokaslan and Vanya Cohen. OpenWeb-\nText corpus. http://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\n[24] Shaﬁ Goldwasser, Silvio Micali, and Charles Rackoff.\nThe knowledge complexity of interactive proof systems.\nSICOMP, 1989.\n[25] Google. Tensorﬂow Privacy. https://github.com/\ntensorflow/privacy.\n[26] Alex Graves. Generating sequences with recurrent neu-\nral networks. arXiv preprint arXiv:1308.0850, 2013.\n[27] Peter Henderson, Koustuv Sinha, Nicolas Angelard-\nGontier, Nan Rosemary Ke, Genevieve Fried, Ryan\nLowe, and Joelle Pineau. Ethical challenges in data-\ndriven dialogue systems. In Proceedings of the 2018\nAAAI/ACM Conference on AI, Ethics, and Society, pages\n123–129, 2018.\n[28] Sorami Hisamoto, Matt Post, and Kevin Duh. Member-\nship inference attacks on sequence-to-sequence models:\nIs my data in your machine translation system? InTACL,\n2020.\n[29] Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, and\nYejin Choi. Efﬁcient adaptation of pretrained trans-\nformers for abstractive summarization. arXiv preprint\narXiv:1906.00138, 2019.\n[30] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. In\nICLR, 2020.\n[31] Jeremy Howard and Sebastian Ruder. Universal lan-\nguage model ﬁne-tuning for text classiﬁcation. In ACL,\n2018.\n[32] Daphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. Automatic detection of gener-\nated text is easiest when humans are fooled. In ACL.\n[33] Matthew Jagielski, Jonathan Ullman, and Alina Oprea.\nAuditing differentially private machine learning: How\nprivate is private SGD? In NeurIPS, 2020.\n[34] Bargav Jayaraman and David Evans. Evaluating differ-\nentially private machine learning in practice. InUSENIX\nSecurity Symposium, 2019.\n[35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scal-\ning laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[36] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. Bart: Denois-\ning sequence-to-sequence pre-training for natural lan-\nguage generation, translation, and comprehension.arXiv\npreprint arXiv:1910.13461, 2019.\n[37] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. A diversity-promoting objective func-\ntion for neural conversation models. In NAACL, 2016.\n[38] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt\nKeutzer, Dan Klein, and Joseph E Gonzalez. Train large,\nthen compress: Rethinking model size for efﬁcient train-\ning and inference of transformers. In ICML, 2020.\n[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[40] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue\nBu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and\nKai Chen. Understanding membership inferences\non well-generalized learning models. arXiv preprint\narXiv:1802.04889, 2018.\n[41] Jean loup Gailly and Mark Adler. zlib compression\nlibrary.\n[42] Michael McCloskey and Neal J Cohen. Catastrophic\ninterference in connectionist networks: The sequential\nlearning problem. In Psychology of learning and moti-\nvation. 1989.\n[43] H Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. Learning differentially private recurrent\nlanguage models. In ICLR, 2018.\n[44] Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. Recurrent neural net-\nwork based language model. In Interspeech, 2010.\n[45] Randall Munroe. Predictive models. https://xkcd.\ncom/2169/, 2019.\n15\n[46] Milad Nasr, Reza Shokri, and Amir Houmansadr. Ma-\nchine learning with membership privacy using adversar-\nial regularization. In ACM SIGSAC, 2018.\n[47] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-\nprehensive privacy analysis of deep learning: Passive\nand active white-box inference attacks against central-\nized and federated learning. In IEEE S&P, 2019.\n[48] Helen Nissenbaum. Privacy as contextual integrity.\nWashington Law Review, 2004.\n[49] OpenAI. Language models are few-shot learners.\nhttps://github.com/openai/gpt-3, 2020.\n[50] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations.\nIn NAACL, 2018.\n[51] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Sebas-\ntian Riedel. Language models as knowledge bases? In\nEMNLP, 2019.\n[52] Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding by\ngenerative pre-training, 2018.\n[53] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela\nAmodei, Jack Clark, Miles Brundage, and Ilya Sutskever.\nBetter language models and their implications. OpenAI\nBlog, 2019.\n[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\n[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer. InJMLR,\n2020.\n[56] Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Françoise\nBeaufays. Training production language models without\nmemorizing user data. arXiv preprint arXiv:2009.10031,\n2020.\n[57] Roger Ratcliff. Connectionist models of recognition\nmemory: constraints imposed by learning and forgetting\nfunctions. Psychological review, 1990.\n[58] Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud\nLegout, and David Choffnes. ReCon: Revealing and con-\ntrolling PII leaks in mobile network trafﬁc. In MobiSys,\n2016.\n[59] Adam Roberts, Colin Raffel, and Noam Shazeer. How\nmuch knowledge can you pack into the parameters of a\nlanguage model? In EMNLP, 2020.\n[60] Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang,\nand Nina Taft. Learning in a large function space:\nPrivacy-preserving mechanisms for SVM learning. Pri-\nvacy and Conﬁdentiality, 2012.\n[61] Rico Sennrich, Barry Haddow, and Alexandra Birch.\nNeural machine translation of rare words with subword\nunits. In ACL, 2016.\n[62] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric\nWallace, and Sameer Singh. AutoPrompt: Eliciting\nknowledge from language models with automatically\ngenerated prompts. In EMNLP, 2020.\n[63] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter lan-\nguage models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[64] Reza Shokri and Vitaly Shmatikov. Privacy-preserving\ndeep learning. In ACM CCS, 2015.\n[65] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-\ntaly Shmatikov. Membership inference attacks against\nmachine learning models. In IEEE S&P, 2017.\n[66] Congzheng Song and Ananth Raghunathan. Information\nleakage in embedding models. In ACM CCS, 2020.\n[67] Congzheng Song and Vitaly Shmatikov. Auditing data\nprovenance in text-generation models. In KDD, 2018.\n[68] Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews,\nand Françoise Beaufays. Understanding unintended\nmemorization in federated learning. arXiv preprint\narXiv:2006.07490, 2020.\n[69] Abhradeep Guha Thakurta, Andrew H. Vyrros,\nUmesh S. Vaishampayan, Gaurav Kapoor, Julien Freudi-\nger, Vivek Rangarajan Sridhar, and Doug Davidson.\nLearning new words, 2017. US Patent 9,594,741.\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS,\n2017.\n[71] Kit Walsh. USPTO request for comments on intellectual\nproperty protection for artiﬁcial intelligence innovation\n– public comment by the electronic frontier founda-\ntion. https://www.uspto.gov/sites/default/\nfiles/documents/Electronic%20Frontier%\n20Foundation_RFC-84-FR-58141.PDF , 2020.\n16\n[72] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and\nSomesh Jha. Privacy risk in machine learning: Analyz-\ning the connection to overﬁtting. In IEEE CSF, 2018.\n[73] Santiago Zanella-Béguelin, Lukas Wutschitz, Shruti\nTople, Victor Rühle, Andrew Paverd, Olga Ohrimenko,\nBoris Köpf, and Marc Brockschmidt. Analyzing infor-\nmation leakage of updates to natural language models.\nIn ACM CCS, 2020.\n[74] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan\nBisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.\nDefending against neural fake news. In NeurIPS, 2019.\n[75] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\nRecht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. ICLR, 2017.\n[76] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu,\nand Bill Dolan. DialoGPT: Large-scale generative pre-\ntraining for conversational response generation. In ACL\nDemo Track, 2020.\nA Categorization of Memorized Data\nTable 5 describes the high-level categories that we assigned\nto the 604 memorized samples extracted from GPT-2. Note\nthat a single sample can belong to multiple categories. Tables\n6 and 7 (omitted for space) show the categorization broken\ndown by attack strategy.\nB Distribution of Model Perplexities\nFigure 4 shows the distribution of the perplexities of samples\ngenerated with each of our three text generation strategies and\nordered based on our six membership inference strategies.\nC Additional Case Studies of Memorization\nHere we present additional results from our manual analysis\nof the memorized content.\nMemorized Leaked Podesta Emails from WikiLeaks.\nWe identify several memorized URLs that originated from\nthe leaked Podesta Emails available on WikiLeaks13. There\nis only one training document that contains these memorized\nURLs. Due to the nature of email, the text of one message is\noften included in subsequent replies to this email. As a result,\na URL that is used (intentionally) only once can be included\nin the dataset tens of times due to the replies.\n13https://en.wikipedia.org/wiki/Podesta_emails\nMemorized Donald Trump Quotes and Tweets. The\nGPT-2 training dataset was collected when the 2016 US Pres-\nidential election was often in the news. As a result, we ﬁnd\nseveral instances of memorized quotes from Donald Trump,\nboth in the form of ofﬁcial remarks made as President (found\nin the ofﬁcial government records), as well as statements made\non Twitter.\nMemorized Promotional Content. We extract memorized\nsamples of promotional content, such as advertisements for\nbooks, beauty products, software products. One of these sam-\nples includes a link to an author’s valid Patreon account, along\nwith a list of named and pseudonymous prior donors.\nMemorized Number Sequences. We identify many ex-\namples where GPT-2 emits common number sequences.\nNearly ten examples contain the integers counting\nup from some speciﬁc value. We also ﬁnd exam-\nples of GPT-2 counting the squares 1, 2, 4, 8, 16,\n25, 36, Fibonacci numbers 1, 1, 2, 3, 5, 8, 13, 21,\n34, 55, 89, 144, 233, 377, 610, 987, or digits of π,\n3.14159265358979323846264. None of these examples\nshould be unexpected, but the quantity of memorized number\nsequences was surprising to us.\nMemorized News Headlines. Numerous memorized text\nsnippets are verbatim copies of news articles and headlines.\nA large number of these memorized samples are attributed\nto a single source: thehill.com, an American news website.\nInterestingly, most of these samples follow the exact same\ntemplate: (1) they contain a list of different news headlines\nseparated by a “pipe” symbol (|), (2) the sample begins with\ntwo merged words, e.g., “TrumpJesuit”, (3) the headline list\nends with the all-caps word “MORE”, and (4) the sample\ncontains the all-caps word “ADVERTISEMENT”.\nWe indeed ﬁnd pages on the Web that contain copies of\nheadlines from thehill.com under this exact template. The\npeculiarities of these snippets likely contributed to their mem-\norization. For example, the token TrumpJesuit does not appear\nin any other context on the entire Web.\nMemorized Base-64 Content. One particularly interesting\nform of memorization that we identify is the ability of GPT-2\nto emit base-64 encoded content. For example, we extract out\nof the model the following sequence:\nbWFzdGVyfGltYWdlc3w3OTkxOXxpbWFnZS9wbmd\n8aW1hZ2VzL2hkZS9oMDQvODg0NTY3MjYxMTg3MC\n5wbmd8ZmFkMTMlNmFiYWJhZjFiMjJlYTAyNzU0Z\nwhich decodes to the sequence “master|images|79919|image\n/png|images/hde/h04/8845672611870.png|...”. Despite our at-\ntempts, we are unable to identify where this content originates.\n17\n(a) Top-n (2.6% duplicates)\n(b) Internet (7.1% duplicates)\n(c) Temperature (0.6% duplicates)\nFigure 4: For each of our three text generation strategies (Top-n, Internet and Temperature), we generate 200,000 samples using\nGPT-2 and apply a de-duplication procedure. The two left-most plots show the distribution of perplexities for the full sample, and\nthe most likely window of 50 tokens. The remaining plots compare the distribution of perplexities of GPT-2 to other measure of\nsample likelihood: zlib entropy, perplexity under GPT-2 Small and GPT-2 Medium, and perplexity of lower-cased samples. Each\nplot highlights the 100 samples we selected for manual inspection (red) and the subset that was conﬁrmed as memorized (blue).\nCategory Count Description\nUS and international\nnews\n109 General news articles or headlines, mostly\nabout US politics\nLog ﬁles and error\nreports\n79 Logs produced by software or hardware\nLicense, terms of\nuse, copyright\nnotices\n54 Software licenses or website terms of use,\ncopyright for code, books, etc.\nLists of named items 54 Ordered lists, typically alphabetically, of\ngames, books, countries, etc.\nForum or Wiki entry 53 User posts on online forums or entries in\nspeciﬁc wikis\nValid URLs 50 A URL that resolves to a live page\nNamed individuals 46 Samples that contain names of real individu-\nals. We limit this category to non-news sam-\nples. E.g., we do not count names of politi-\ncians or journalists within news articles\nPromotional content 45 Descriptions of products, subscriptions,\nnewsletters, etc.\nHigh entropy 35 Random content with high entropy, e.g.,\nUUIDs Base64 data, etc.\nCategory Count Description\nContact info 32 Physical addresses, email addresses, phone\nnumbers, twitter handles, etc.\nCode 31 Snippets of source code, including\nJavaScript\nConﬁguration ﬁles 30 Structured conﬁguration data, mainly for\nsoftware products\nReligious texts 25 Extracts from the Bible, the Quran, etc.\nPseudonyms 15 Valid usernames that do not appear to be tied\nto a physical name\nDonald Trump\ntweets and quotes\n12 Quotes and tweets from Donald Trump, of-\nten from news articles\nWeb forms 11 Lists of user menu items, Website instruc-\ntions, navigation prompts (e.g., “please enter\nyour email to continue”)\nTech news 11 News related to technology\nLists of numbers 10 Lists of dates, number sequences, π, etc.\nSports news 9 News related to sports\nMovie synopsis, cast 5 List of actors, writers, producers. Plot syn-\nopsis.\nPornography 5 Content of pornographic nature, often lists\nof adult ﬁlm actors.\nTable 5: Descriptions for the categories of memorized text. Categories in bold correspond to personally identiﬁable information.\n18\nCategory Count\nUS and international news 88\nForum or Wiki entry 34\nLicense, terms of use, copyright notice 28\nNamed individuals 25\nPromotional content 18\nLists of named items 15\nContact info 20\nDonald Trump tweets and quotes 12\nPseudonyms 7\nValid URLs 7\nSports news 6\nMovie synopsis or cast 6\n(a) Top-n (191 samples)\nCategory Count\nLog ﬁles and error reports 86\nLists of named items 53\nValid URLs 40\nLicense, terms of use, copyright notice 36\nHigh entropy 33\nConﬁguration ﬁles 32\nCode 29\nNamed individuals 18\nPromotional content 14\nContact info 12\nPseudonyms 11\nForum or Wiki entry 9\nUS and international news 7\nTech news 7\nPornography 5\nWeb forms 5\nLists of numbers 5\n(b) Internet (273 samples)\nCategory Count\nUS and international news 31\nReligious texts 28\nLicense, terms of use, copyright notice 24\nPromotional content 20\nForum or Wiki entry 17\nNamed individuals 12\nLists of named items 12\nValid URLs 12\nTech news 8\nContact info 8\nHigh entropy 6\nLists of numbers 6\n(c) Temperature (140 samples)\nTable 6: Memorized content found in samples produced by each of the our three text generation strategies. We show categories\nwith at least 5 samples.\nCategory Count\nLicense, terms of use, copyright notice 11\nLists of named items 8\nLog ﬁles and error reports 7\nValid URLs 6\nLists of numbers 5\n(a) Perplexity (51 samples)\nCategory Count\nUS and international news 21\nLists of named items 18\nLicense, terms of use, copyright notice 16\nPromotional content 11\nValid URLs 11\nLog ﬁles and error reports 10\nNamed individuals 8\nHigh entropy 8\nForum or Wiki entry 7\nConﬁguration ﬁles 6\nCode 6\n(b) Window (119 samples)\nCategory Count\nUS and international news 40\nLicense, terms of use, copyright notice 31\nLists of named items 17\nForum or Wiki entry 14\nNamed individuals 13\nPromotional content 13\nContact info 12\nLog ﬁles and error reports 11\nValid URLs 10\nCode 10\nTech news 6\nConﬁguration ﬁles 6\nPseudonyms 5\n(c) zlib (172 samples)\nCategory Count\nUS and international news 39\nLog ﬁles and error reports 29\nLists of named items 17\nForum or Wiki entry 12\nNamed individuals 11\nLicense, terms of use, copyright notice 10\nHigh entropy 9\nConﬁguration ﬁles 6\nPromotional content 5\nTech news 5\n(d) Lowercase (135 samples)\nCategory Count\nLog ﬁles and error reports 17\nForum or Wiki entry 15\nReligious texts 14\nValid URLs 13\nHigh entropy 13\nLists of named items 12\nLicense, terms of use, copyright notice 12\nPromotional content 11\nConﬁguration ﬁles 11\nNamed individuals 11\nother 9\nUS and international news 9\nContact info 8\nDonald Trump tweets and quotes 7\nCode 6\n(e) Small (141 samples)\nCategory Count\nValid URLs 17\nLog ﬁles and error reports 14\nUS and international news 13\nContact info 12\nReligious texts 12\nNamed individuals 11\nPromotional content 11\nHigh entropy 10\nForum or Wiki entry 9\nLists of named items 8\nLicense, terms of use, copyright notice 8\nCode 5\nDonald Trump tweets and quotes 5\n(f) Medium (116 samples)\nTable 7: Memorized content found using our six membership inference strategies. We show categories with at least 5 samples.\n19"
}