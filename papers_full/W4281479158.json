{
    "title": "Prompt Tuning for Discriminative Pre-trained Language Models",
    "url": "https://openalex.org/W4281479158",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2062353949",
            "name": "Yuan Yao",
            "affiliations": [
                "Tsinghua University",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2275601388",
            "name": "Bowen Dong",
            "affiliations": [
                "Center for Information Technology",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2004093727",
            "name": "Ao Zhang",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2104573188",
            "name": "Zhengyan Zhang",
            "affiliations": [
                "Tsinghua University",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2506913441",
            "name": "Ruobing Xie",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2051269448",
            "name": "Zhiyuan Liu",
            "affiliations": [
                "Center for Information Technology",
                "Beijing Academy of Artificial Intelligence",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2143226789",
            "name": "Leyu Lin",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2157167650",
            "name": "Maosong Sun",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Tsinghua University",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2101039141",
            "name": "Jianyong WANG",
            "affiliations": [
                "Tsinghua University",
                "Center for Information Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W2970745243",
        "https://openalex.org/W3126074026",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W2919420119",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3102663935",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3098324846",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2028175314",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3468 - 3473\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nPrompt Tuning for Discriminative Pre-trained Language Models\nYuan Yao1, Bowen Dong1, Ao Zhang2, Zhengyan Zhang1,\nRuobing Xie3, Zhiyuan Liu1,4,5,6†, Leyu Lin3, Maosong Sun1,4,5,6†, Jianyong Wang1\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology\n2Department of Computer Science, National University of Singapore, Singapore\n3WeChat Search Application Department, Tencent, China\n4Institute for Artificial Intelligence, Tsinghua University, Beijing, China\n5Institute Guo Qiang, Tsinghua University, Beijing, China\n6International Innovation Center of Tsinghua University, Shanghai, China\nyaoyuanthu@163.com dongbw18@mails.tsinghua.edu.cn\nAbstract\nRecent works have shown promising results of\nprompt tuning in stimulating pre-trained lan-\nguage models (PLMs) for natural language\nprocessing (NLP) tasks. However, to the\nbest of our knowledge, existing works focus\non prompt-tuning generative PLMs that are\npre-trained to generate target tokens, such as\nBERT (Devlin et al., 2019). It is still unknown\nwhether and how discriminative PLMs, e.g.,\nELECTRA (Clark et al., 2020), can be effec-\ntively prompt-tuned. In this work, we present\nDPT, the first prompt tuning framework for\ndiscriminative PLMs, which reformulates NLP\ntasks into a discriminative language model-\ning problem. Comprehensive experiments on\ntext classification and question answering show\nthat, compared with vanilla fine-tuning, DPT\nachieves significantly higher performance, and\nalso prevents the unstable problem in tuning\nlarge PLMs in both full-set and low-resource\nsettings. The source code and experiment de-\ntails of this paper can be obtained fromhttps:\n//github.com/thunlp/DPT.\n1 Introduction\nRecent years have witnessed the great success of\nthe pre-training-then-fine-tuning paradigm in natu-\nral language processing (NLP) (Devlin et al., 2019;\nYang et al., 2019; Clark et al., 2020; Lan et al.,\n2020; Raffel et al., 2020). Typically, language mod-\nels are first pre-trained on large-scale corpora via\nself-supervised generative or discriminative tasks\nto learn universal text representations, and then\nfine-tuned to adapt to downstream tasks (Qiu et al.,\n2020; Xu et al., 2021). However, the significant gap\n† Corresponding authors: Z.Liu (liuzy@tsinghua.edu.cn),\nM.Sun (sms@tsinghua.edu.cn)\nbetween the objective forms of model pre-training\nand fine-tuning hinders taking full advantage of\nPLMs in downstream tasks (Liu et al., 2021).\nPrompt tuning has recently shown its effective-\nness in stimulating the capability of PLMs by trans-\nforming downstream tasks into the same form as\npre-training (Petroni et al., 2019; Brown et al.,\n2020; Schick and Schütze, 2021; Gao et al., 2021;\nLiu et al., 2021). However, to the best of our knowl-\nedge, existing works focus on prompt-tuning gen-\nerative PLMs (i.e., PLMs pre-trained by generat-\ning target textual tokens from the context, such as\nBERT (Devlin et al., 2019) and GPT (Brown et al.,\n2020)). It is still unknown whether and how dis-\ncriminative PLMs can be effectively prompt-tuned\n(i.e., PLMs pre-trained by discriminating replaced\ntokens, such as ELECTRA (Clark et al., 2020) and\nWKLM (Xiong et al., 2020)). Since discriminative\nPLMs typically enjoy competitive performance and\nsuperior computational efficiency compared with\ntheir generative counterparts (Clark et al., 2020), it\ncan be especially appealing to prompt-tuning dis-\ncriminative PLMs.\nIn this work, we present DPT, the first prompt\ntuning framework for discriminative PLMs. DPT\nreformulates downstream tasks into a discrimina-\ntive language modeling problem, maximally mit-\nigating the gap between model pre-training and\ntuning. Specifically, as shown in Figure 1, mod-\nels are asked to discriminate correct answer tokens\n(e.g., correct labels for text classification, or answer\nspans for question answering) from the input to-\nkens based on the reused discriminative language\nmodeling head, where the objective form is identi-\ncal to pre-training.\nTo evaluate DPT, we conduct comprehensive\nexperiments on text classification and question an-\n3468\nYou may to make up a book on the movie subject[CLS]\n… … DLM\nHead\noriginal\nreplaced\nDLM\nHead\noriginal\nreplaced… …\n… … … …\n[SEP]\n(b) Fine-tuning\nA graceful movie[CLS] [SEP]\nCLS\nHead\npositive\nneutral\nA graceful movie[CLS] [SEP]\n(a) DLM-based Pre-training\n. Class positive negative: neutral, , .\n.\n…\n.\n(c) DLM-based Prompt Tuning (Our Approach)\nDLM\nHead\noriginal\nreplaced\n…\nDLM\nHead\noriginal\nreplaced\nfeel compelled\nDLM\nHead\noriginal\nreplaced\n…\n…\nInput TemplateInput\nnegative\nFigure 1: Illustration of (a) discriminative language modeling (DLM) based pre-training with the DLM head,\n(b) vanilla fine-tuning with a new classification (CLS) head, and (c) our DPT prompt tuning approach that\nreformulates NLP tasks into a discriminative language modeling problem. DPT fills the input text into the template\ncontaining answer candidates, and discriminates whether each answer candidate is correct (i.e., original), or incorrect\n(i.e., replaced) based on the reused DLM head.\nswering in both full-set and low-resource settings.\nExperimental results show that despite its sim-\nplicity, DPT significantly outperforms vanilla fine-\ntuning (e.g., 4.1% accuracy improvement in the\nlow-resource SST-5 evaluation). Moreover, previ-\nous works have shown that fine-tuning large PLMs\ncan be highly unstable and even produce divergent\nresults (Devlin et al., 2019; Dodge et al., 2020),\nwhich undermines the practicality of large PLMs.\nWe show that DPT also addresses the unstable prob-\nlem in tuning large discriminative PLMs.\nThe contributions of our work are summarized\nas follows: (1) We present the first prompt tuning\nframework for discriminative PLMs. (2) Compre-\nhensive experimental results on text classification\nand question answering demonstrate the effective-\nness of the proposed prompt tuning framework.\n2 Preliminary\nIn this work, without loss of generality, we take\nELECTRA (Clark et al., 2020) as a representative\nexample of discriminative PLMs, while applying\nDPT to other discriminative PLMs is also appli-\ncable. Here we introduce the main procedure of\npre-training and fine-tuning, and we refer readers\nto the paper (Clark et al., 2020) for more details.\nPre-training. During pre-training, a generator first\ncorrupts the text via token replacement. Then the\ndiscriminator is asked to detect the replaced tokens,\nby classifying each token into binary categories,\ni.e., {original, replaced}, as shown in Figure 1.\nFinally, the generator is discarded and the discrimi-\nnator is fine-tuned on downstream tasks.\nVanilla Fine-tuning. (1) During fine-tuning, to\nperform text classification , a new classification\nhead is typically introduced to classify the hid-\nden representation of the [CLS] token in the last\nlayer (Clark et al., 2020). (2) For general multi-\nspan question answering, the answer could be mul-\ntiple spans from the input text (Dasigi et al., 2019;\nDua et al., 2019). State-of-the-art fine-tuning ap-\nproaches formulate the task as a sequence-labeling\nproblem, and classify each input token into binary\nlabels based on a new classification head, indicat-\ning whether the token belongs to the answer or\nnot (Segal et al., 2020; Ye et al., 2020).\nNote that the classification head typically intro-\nduces new parameters, and learning the parameters\nfrom scratch usually requires a large amount of la-\nbeled data. Moreover, previous works have shown\nthat fine-tuning large PLMs can be highly unstable,\nand even produce divergent results (Devlin et al.,\n2019; Dodge et al., 2020). As a result, multiple\nfine-tuning trials are usually needed to find a good\nrandom seed that leads to a stably fine-tuned PLM,\nwhich undermines the practicality of large PLMs.\n3 Methodology\nIn this section, we introduce the framework of DPT\nfor prompt-tuning discriminative PLMs. We first\nintroduce DPT using text classification as the run-\nning example, and then illustrate its application in\nquestion answering.\nDLM-based Reformulation. DPT reformulates\nNLP tasks into a dscriminative language modeling\nproblem, maximally mitigating the gap between\npre-training and tuning. Specifically, as shown in\nFigure 1 (c), for a text classification task with class\nset C = {c1, c2, . . . , cn}, DPT defines a template\nthat contains all answer candidates T (·; C). Given\n3469\nan input text x (e.g., “A graceful movie.”), DPT\nfills the input text into the template as follows:\nT (x; C) = [CLS]x Class: c1, c2, . . . , cn.[SEP] (1)\nIntuitively, T (x; C) can be understood as cre-\nating a virtual context that assumes all candidate\nclasses are correct for the input text x. It is then\nstraightforward for discriminative PLMs to decide\nwhether each class candidate token is proper in the\ncontext, by classifying the tokens into original (i.e.,\ncorrect), or replaced (i.e., incorrect) based on the\nreused DLM head. In our experiments, we find\nthat the order of classes in template has minimal\ninfluence on the performance, and a random order\ncan produce good prompt-tuning results.\nDPT Training. After template filling, T (x; C)\nis fed into PLMs to obtain the hidden represen-\ntations {h[CLS], h1, h2, . . . ,hm, h[SEP]}. PLMs\nare then prompted to discriminate whether each\nclass is correct. Specifically, we compute the score\nof class ci based on the representation of the corre-\nsponding token ti as:1\ns(ci) = 1 − σ(h⊤\nDLMhti ), (2)\nwhere hDLM is the reused DLM head, and σ(·) is\nthe sigmoid activation. Note that in Equation 2,\nthe computation of class scores is different from\nthe vanilla fine-tuning approaches which encour-\nage large inner products between the correct an-\nswer and classification head (Devlin et al., 2019;\nClark et al., 2020). The rationale is that during\npre-training, discriminative PLMs are typically re-\nquired to produce large inner products for the re-\nplaced tokens (i.e., incorrect ones), and small in-\nner products for the original tokens (i.e., correct\nones) (Clark et al., 2020), and therefore Equation 2\nbetter fits the semantics in pre-training. In our ex-\nperiments, we find this simple operation can lead\nto significantly better results in prompt-tuning dis-\ncriminative PLMs. After obtaining the class score,\nthe model is optimized as:\nL =\nX\ni\n[−yi log s(ci) −(1 −yi) log(1−s(ci))], (3)\nwhere yi ∈ {0, 1} indicates the ground-truth la-\nbel. Since DPT tunes PLMs by reusing the pre-\ntrained DLM head in the same objective form as\npre-training, compared with vanilla fine-tuning, we\n1If the class name consists of multiple tokens, the repre-\nsentation of the first token is used.\nexpect DPT will lead to more sample efficient and\nstable tuning results.\nDPT for Question Answering.Besides text clas-\nsification, DPT can also be applied for the question\nanswering task. Given a question and a paragraph,\ndirectly concatenating them without additional tem-\nplates can already create a good prompting context.\nThen similar to text classification, we ask PLMs to\ndiscriminate whether each token in the paragraph\nis part of the answer (i.e., original), or not (i.e.,\nreplaced) based on the reused DLM head. During\ninference, we threshold the token scores to obtain\nmultiple answer spans.\n4 Experiments\nIn this section, we empirically evaluate DPT on the\ntask of text classification and question answering.\nDatasets. We evaluate DPT on four widely used\ntext classification datasets, including SST-2, SST-\n5, TREC and AGNews. For question answering,\nwe adopt the challenging QUOREF dataset, where\nfor each question, there may exist multiple answer\nspans in the paragraph. We refer readers to Sec-\ntion B for more dataset details.\nEvaluation Protocols. We evaluate the models\nunder two settings, including (1) full-set setting,\nwhere the full training data is available, and (2)low-\nresource setting, where only 10% of the full train-\ning data for each dataset is available. We report the\naccuracy for text classification, and exact match\n(EM) and F1 score for question answering. To ac-\ncount for the unstable problem of baseline models,\nwe report the average results from 3 best random\nseeds among 10 trials.\nBaselines. We compare DPT with several strong\nbaseline models, including vanilla fine-tuning of\nELECTRA (Clark et al., 2020), BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019). The\nfine-tuning of ELECTRA adopts the identical dis-\ncriminative PLM to our model, and serves as the\nmost direct baseline for comparison.\nMain Results.We report the main results in Table 1\nand Table 2, from which we observe that: (1) DPT\nsignificantly improves the performance of discrim-\ninative PLMs. The improvements are consistent\nacross different tasks and datasets, as well as base\nand large models. (2) Previous works show that de-\nspite the significant improvements in low-resource\nsetting, template-based prompt tuning typically can\nonly approach fine-tuning performance in full-set\n3470\nPLM Tuning Full-set Setting Low-resource Setting\nApproach SST-2 SST-5 TREC AGNews SST-2 SST-5 TREC AGNews\nBase\nBERT FT 91.32 53.41 95.93 93.68 86.91 42.46 86.73 90.23\nRoBERTa FT 94.69 56.09 95.27 93.92 91.23 50.41 91.07 90.25\nELECTRA FT 94.38 56.60 94.87 93.70 91.68 49.40 88.40 89.17\nELECTRA DPT (Ours) 95.26 58.34 96.27 94.22 93.83 53.48 93.93 90.60\n∆ +0.88 +1.74 +1.40 +0.52 +2.15 +4.08 +5.53 +1.43\nLarge\nBERT FT 93.32 54.10 96.73 94.89 90.77 50.89 94.73 92.93\nRoBERTa FT 95.46 56.80 96.80 95.26 94.27 51.41 95.20 93.41\nELECTRA FT 95.72 58.27 97.13 94.80 93.74 53.65 94.00 92.33\nELECTRA DPT (Ours) 96.58 60.69 98.07 95.38 96.09 57.00 95.67 93.58\n∆ +0.86 +2.42 +0.94 +0.58 +2.35 +3.35 +1.67 +1.25\nTable 1: Experimental results on text classification. Full-set setting: 100% data, Low-resource setting: 10% data.\nFT: fine-tuning, DPT: discriminative prompt tuning. ∆: Improvements of DPT over fine-tuning ELECTRA.\nPLM Tuning Full Set Low Resource\nApproach EM F1 EM F1\nBERT FT 75.67 79.99 53.02 61.36\nRoBERTa FT 78.29 84.56 59.31 67.56\nELECTRA FT 77.79 83.72 54.29 63.71\nELECTRA DPT (Ours) 79.66 86.03 63.65 73.09\n∆ +1.87 +2.31 +9.36 +9.38\nTable 2: Experimental results of ELECTRAlarge on\nQUOREF multi-span question answering dataset.\nTuning Approach SST-2 SST-5 TREC AGNews\nFine-tuning 91.68 49.40 88.40 89.17\nDPT (σ) 92.16 50.96 88.00 90.29\nDPT (1−σ) 93.83 53.48 93.93 90.60\nTable 3: Ablation on reuse forms of DLM head based\non ELECTRAbase in low-resource setting.\nsetting (Gao et al., 2021). In comparison, we note\nthat DPT can improve the performance in both low-\nresource and full-set settings. The reason is that\nDPT enables PLMs to jointly model the input text\nand class candidates for better text understanding.\nIn summary, DPT is effective in improving the per-\nformance of discriminative PLM tuning.\nTuning Stability.Previous works have commonly\nobserved the instability of fine-tuning large genera-\ntive PLMs (Devlin et al., 2019; Dodge et al., 2020).\nSome works attempt to alleviate the problem by\ncareful initialization and optimization (Zhang et al.,\n2021), or intermediate fine-tuning on other large-\nscale datasets (Phang et al., 2018). To investigate\nthe tuning stability of discriminative PLMs, we\ntune ELECTRAlarge using fine-tuning and DPT\nfrom 10 random seeds. From the results in Fig-\nure 2, we observe that: (1) Similar to generative\nPLMs, fine-tuning large discriminative PLMs is\nFT DPT\n48.0\n56.0\n64.0\n72.0\n80.0\n88.0\n96.0\nSST-2\nFT DPT\n30.0\n36.0\n42.0\n48.0\n54.0\n60.0\nSST-5\nFT DPT\n16.0\n32.0\n48.0\n64.0\n80.0\n96.0\nTREC\n(a) Full-set Setting (100 % data)\nFT DPT\n48.0\n56.0\n64.0\n72.0\n80.0\n88.0\n96.0\nSST-2\nFT DPT\n24.0\n30.0\n36.0\n42.0\n48.0\n54.0\n60.0\nSST-5\nFT DPT\n24.0\n36.0\n48.0\n60.0\n72.0\n84.0\n96.0\nTREC\n(b) Low-resource Setting (10 % data)\nFigure 2: Performance distribution of ELECTRAlarge\nusing fine-tuning and DPT from 10 seeds.\nalso highly unstable, and can even frequently pro-\nduce divergent results (e.g., nearly 20% accuracy\nfor 5-way classification in SST-5 in low-resource\nsetting). The problem is exacerbated by sparse data\nin low-resource setting, but remains even in full-set\nsetting. (2) DPT achieves significantly more stable\ntuning results in both full-set and low-resource set-\ntings, where all tuning trials converged and closely\napproach the best performance. This is due to the\nreuse of DLM head parameters and identical objec-\ntive forms to pre-training.\nAblation Study. In DPT, different from conven-\ntional fine-tuning approaches, correct labels are\nencouraged to have small inner products with clas-\nsifiers (as indicated by the 1 − σ in Equation 2).\nWe evaluate DPT using conventional score com-\nputation (i.e., σ), and report the results in Table 3.\nThe significant drop in performance shows that a\nproper form of reusing DLM head is crucial to the\n3471\nresults of prompt-tuning discriminative PLMs.\n5 Conclusion and Future Work\nIn this work, we present a simple and effective\nprompt tuning approach for discriminative PLMs.\nWe note directly performing large-scale classifica-\ntion (e.g., for hundreds of classes) with DPT may\nbe computationally inefficient. In future, we plan\nto address the problem by classifying text follow-\ning class hierarchies, where each hierarchical layer\ntypically consists of a moderate number of classes.\n6 Acknowledgement\nThis work is suooprted by the Natural Science\nFoundation of China (NSFC) and the German Re-\nsearch Foundation (DFG) in Project Crossmodal\nLearning, NSFC 61621136008 / DFC TRR-169,\nInstitute Guo Qiang at Tsinghua University, and\nInternational Innovation Center of Tsinghua Uni-\nversity, Shanghai, China.\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of ICLR.\nPradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A.\nSmith, and Matt Gardner. 2019. QUOREF: A read-\ning comprehension dataset with questions requiring\ncoreferential reasoning. In Proceedings of EMNLP-\nIJCNLP, pages 5924–5931.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of NAACL-HLT, pages 2368–2378.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL-IJCNLP , pages\n3816–3830.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proceedings\nof ICLR.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In Proceedings of EMNLP-\nIJCNLP, pages 2463–2473.\nJason Phang, Thibault Févry, and Samuel R Bowman.\n2018. Sentence encoders on STILTS: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nSCTS, pages 1–26.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21:140:1–140:67.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of NAACL-HLT,\npages 2339–2352.\nElad Segal, Avia Efrat, Mor Shoham, Amir Globerson,\nand Jonathan Berant. 2020. A simple and effective\nmodel for answering multi-span questions. In Pro-\nceedings of EMNLP, pages 3074–3080.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of EMNLP, page 1631–1642.\nEllen M. V oorhees and Dawn M. Tice. 2000. Building\na question answering test collection. In Proceedings\nof SIGIR, pages 200–207.\n3472\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of ICLR.\nHan Xu, Zhang Zhengyan, et al. 2021. Pre-trained\nmodels: Past, present and future. arXiv preprint\narXiv:2106.07139.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of NeurIPS,\npages 5754–5764.\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng\nLi, Maosong Sun, and Zhiyuan Liu. 2020. Corefer-\nential reasoning learning for language representation.\nIn Proceedings of EMNLP, pages 7170–7186.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein-\nberger, and Yoav Artzi. 2021. Revisiting few-sample\nBERT fine-tuning. In Proceedings of ICLR.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of NIPS, pages 649–657.\nA Implementation Details\nIn this work, we take ELECTRA (Clark et al., 2020)\nas an representative example of discriminative\nPLMs, including (1) ELECTRAbase with 768 di-\nmensional hidden representations, 12 encoding lay-\ners and 110M parameters, and (2) ELECTRAlarge\nwith 1, 024 dimensional hidden representations, 24\nencoding layers and 340M parameters.\nFor text classification tasks, we follow the hy-\nperparameters in Clark et al. (2020), and train the\nbase models for 10 epochs with learning rate 2e-5\nand batchsize 32 on 2 GeForce RTX 2080 Ti GPUs.\nAnd we train the large models for 10 epochs with\nlearning rate 2e-5 and batchsize 8 on 2 GeForce\nRTX 2080 Ti GPUs. For question answering, we\nfollow the hyparameters in Segal et al. (2020), and\ntrain the large models for 20 epochs with learning\nrate 5e-6 and batchsize 2 on 6 GeForce RTX 2080\nTi GPUs. During inference, a token is considered\nas part of the answer if its score is lower than 0.6.\nB Dataset Details\nWe evaluate DPT on four popular text classifica-\ntion datasets, including SST-2 (Socher et al., 2013),\nSST-5 (Socher et al., 2013), TREC (V oorhees and\nTice, 2000) and AGNews (Zhang et al., 2015). For\nquestion answering task, we adopt the challenging\nQUOREF dataset (Dasigi et al., 2019), where there\nmay exist multiple answers in the paragraph for\nFT DPT\n20.0\n30.0\n40.0\n50.0\n60.0\n70.0\n80.0\n90.0\n100.0\nAGNews\n(a) Full Set.\nFT DPT\n20.0\n30.0\n40.0\n50.0\n60.0\n70.0\n80.0\n90.0\n100.0\nAGNews (b) Low Resource.\nFigure 3: Performance distribution of ELECTRAlarge\nusing fine-tuning and DPT from 10 seeds.\neach question. Specifically, QUOREF contains\n21, 817 questions and 4, 225 paragraphs, where\neach question has 1.15 answers on average. The\naverage length for the questions and paragraphs\nare 15.49 and 325.68 respectively. We report the\nresults on the validation set for QUOREF, since\nits test set is not publicly available, and report the\nresults on the test set for the other datasets.\nC Further Results of Tuning Stability\nWe report the performance distribution of AGNews\nin Figure 3. We observe that the unstable problem\nof fine-tuning large discriminative PLMs remains\neven for the large-scale AGNews dataset with 120K\ntraining samples. The results show the advantage\nof DPT in stably tuning discriminative PLMs.\n3473"
}