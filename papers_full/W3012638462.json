{
  "title": "Fully Quantized Transformer for Machine Translation",
  "url": "https://openalex.org/W3012638462",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3023596895",
      "name": "Gabriele Prato",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2981480042",
      "name": "Ella Charlaix",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2258670661",
      "name": "Mehdi Rezagholizadeh",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2754526845",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2608554408",
    "https://openalex.org/W2460130460",
    "https://openalex.org/W2140660536",
    "https://openalex.org/W2787752464",
    "https://openalex.org/W2748428003",
    "https://openalex.org/W2777406049",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W3133056632",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2405920868",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2950458216",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2892090442",
    "https://openalex.org/W1992348535",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2979314664",
    "https://openalex.org/W2512629640",
    "https://openalex.org/W2767785892",
    "https://openalex.org/W2002016471",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2950894517",
    "https://openalex.org/W2557257847",
    "https://openalex.org/W2963689957",
    "https://openalex.org/W2319920447",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1935978687",
    "https://openalex.org/W2198190323",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2952369090",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2613332842",
    "https://openalex.org/W2951978180",
    "https://openalex.org/W2949961122",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2952191002",
    "https://openalex.org/W2141155619",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2889847962",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2952344559"
  ],
  "abstract": "State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1–14\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1\nFully Quantized Transformer for Machine Translation\nGabriele Prato\nMila, Universit´e de Montr´eal\npratogab@mila.quebec\nElla Charlaix\nHuawei Noah’s Ark Lab\nella.charlaix@huawei.com\nMehdi Rezagholizadeh\nHuawei Noah’s Ark Lab\nmehdi.rezagholizadeh@huawei.com\nAbstract\nState-of-the-art neural machine translation\nmethods employ massive amounts of param-\neters. Drastically reducing computational\ncosts of such methods without affecting per-\nformance has been up to this point unsuccess-\nful. To this end, we propose FullyQT: an all-\ninclusive quantization strategy for the Trans-\nformer. To the best of our knowledge, we are\nthe ﬁrst to show that it is possible to avoid any\nloss in translation quality with a fully quan-\ntized Transformer. Indeed, compared to full-\nprecision, our 8-bit models score greater or\nequal BLEU on most tasks. Comparing our-\nselves to all previously proposed methods, we\nachieve state-of-the-art quantization results.\n1 Introduction\nThe idea of using neural networks for machine\ntranslation was only recently proposed (Kalchbren-\nner and Blunsom, 2013; Sutskever et al., 2014; Cho\net al., 2014). Nonetheless, the approach became the\nstate-of-the-art in the ﬁeld (Ahmed et al., 2017; Ott\net al., 2018; Edunov et al., 2018). A key element of\nthis success was to allow the decoder to attend to all\nhidden states of the encoder (Bahdanau et al., 2014).\nA few variations to this additive attention mecha-\nnism have been proposed, such as multiplicative\nand self-attention (Luong et al., 2015; Cheng et al.,\n2016; Lin et al., 2017). The latter formed the basis\nof the Transformer network (Vaswani et al., 2017),\nwhich achieved state-of-the-art results in machine\ntranslation. Inspiring a new wave of work, numer-\nous natural language processing tasks reached new\nheights (Devlin et al., 2018; Liu et al., 2019). Un-\nfortunately, these models use an enormous amount\nof parameters. Inference on resource-limited hard-\nware such as edge-devices is thus impractical.\nA solution to reduce the computational burden\nof these networks is to lower numerical precision.\nConsequently, numerical values can be represented\nusing fewer bits (Tang and Kwan, 1993; March-\nesi et al., 1993). This method called quantization\nhas the advantage of providing good compression\nrates with minimal loss in accuracy. It is also con-\nveniently supported by most hardware. Properly\nquantizing the Transformer would allow computa-\ntional speed gains at inference, as well as deploy-\nment on more constrained devices.\nIn this work, we propose a quantization-aware\ntraining strategy for the entire Transformer architec-\nture. Our method is easy to implement and results\nare consistent with the full-precision Transformer.\nWe test our approach on multiple translation tasks\nsuch as WMT14 EN-FR and WMT14 EN-DE and\nobtain state-of-the-art quantization results. In com-\nparison with full-precision, our quantized models\nscore equal or higher BLEU on most tasks. We are,\nto the best of our knowledge, the ﬁrst to show that\nthe Transformer architecture can be fully quantized\nwithout impairing translation quality. We also per-\nform an ablation study and show that quantizing\nspeciﬁc components of the Transformer improves\nBLEU score.\n2 Background\nIn this section, we review a broad spectrum of quan-\ntization and pruning methods for neural network\ncompression.\n2.1 Quantization\nOver the years, a large range of methods have been\nproposed to quantize neural networks. These in-\nclude, among many others, binary (Courbariaux\net al., 2016), ternary (Lin et al., 2015; Li et al.,\n2016), uniform (Jacob et al., 2017) and learned\n(Zhang et al., 2018) quantization. These methods\ncan be universally applied to any type of neural\nnetwork. To maintain performance though, spe-\nciﬁc architectures usually require custom tailored\nquantization schemes.\n2\nSeveral recent work explore recurrent neural net-\nwork (Jordan, 1990) quantization. Ott et al. (2016)\npropose an exponential quantization method for\nRNN weights. They ﬁnd ternary and exponen-\ntial quantization to work well on language mod-\neling and speech recognition, while binary weights\nseemed ineffective. Hubara et al. (2016) quantize\nweights and activations of both RNNs and LSTMs\n(Hochreiter and Schmidhuber, 1997) to 2, 4 and 6-\nbit. Meanwhile, He et al. (2016) propose modiﬁca-\ntions to the gates and interlinks of quantized LSTM\nand GRU (Cho et al., 2014) cells, as well as a bal-\nanced quantization method for weights. Wu et al.\n(2016) successfully quantize a stacked sequence-to-\nsequence LSTM to 8-bit without any loss in trans-\nlation quality. Most recently, Wang et al. (2018)\npropose applying different quantization methods\nfor different RNN components.\nWith regards to CNNs (LeCun et al., 1989), var-\nious works have also explored quantizing these\nmodels. Gong et al. (2014) compare matrix factor-\nization, binarization, k-means clustering, product\nquantization and residual quantization of CNNs.\nWu et al. (2015) apply quantization to both kernels\nand fully connected layers of convolutional neural\nnetworks. Rastegari et al. (2016) propose using bi-\nnary weighted ﬁlters on AlexNet (Krizhevsky et al.,\n2012). Testing their method on ImageNet, they\nshow classiﬁcation accuracy to be on par with full-\nprecision. For faster inference and training, Zhou\net al. (2016) use low bitwidth weights, activations\nand gradients on CNNs.\nQuantization has been applied in tandem with\nother compression methods. Han et al. (2015)\ncombine pruning, quantization, weight sharing and\nHuffman coding. In another line of work, Polino\net al. (2018) employ quantization with knowledge\ndistillation (Hinton et al., 2015) for higher com-\npression rates. Moreover, Chen et al. (2018) blend\nquantization with block based low-rank matrix ap-\nproximation of embeddings.\n2.2 Pruning\nThe pruning of neural networks for model com-\npression has also been largely explored. LeCun\net al. (1990) were the ﬁrst to propose a Hessian\nbased method to prune neural net weights. Hassibi\net al. (1994) later improved the method. More\nrecently, See et al. (2016) show that pruning a\nfully trained model and then retraining it can in-\ncrease performance over the original non-pruned\nmodel. Gradually pruning in tandem with train-\ning has also been shown to increase performance\n(Zhu and Gupta, 2017). To avoid sparse matrices,\nLiu et al. (2017) prune nodes instead of weights.\nThey apply a penalty in the loss on the γparame-\nters of batch normalization layers. With a similar\nobjective, Narang et al. (2017b) make better use of\nhardware by applying pruning and weight decay in\nblocks to minimize the number of loaded weight\nmatrix chunks.\nSimilarly to quantization, pruning methods have\nalso been adapted to speciﬁc architectures. Liu et al.\n(2015) propose an efﬁcient sparse matrix multipli-\ncation algorithm for CNNs. As for RNNs, Narang\net al. (2017a) show sparse pruning to work well on\nthe architecture. In order to maintain dimension\nconsistency, Wen et al. (2017) propose to prune\nall basic LSTM structures concurrently. Lastly,\nPark et al. (2018) introduce simple recurrent units\n(SRUs) for easy pruning of RNNs.\n3 FullyQT\n3.1 Quantization Methodology\nOur quantization scheme was chosen to be uniform,\nmeaning that the step size between two quantized\nvalues is constant. This choice, which is an addi-\ntional constraint, was made for practical reasons. It\nindeed simpliﬁes all computations required during\ninference, enabling the exploitation of hardware\nresources more efﬁciently. If the performance with\nuniform quantization is already on par with full-\nprecision, then more weighty methods are unneces-\nsary. A brief overview of uniform quantization is\ngiven in this section. For more details, we refer the\nreader to Jacob et al. (2017).\nGiven an element xof a tensor X, we apply the\nquantization function Q:\nQ(x) =\n⌊clamp(x; xmin, xmax) −xmin\ns\n⌉\n∗s + xmin (1)\ns = xmax −xmin\n2k −1 (2)\nwhere xmin and xmax deﬁnes the endpoints of\nthe quantization interval. When quantization is\napplied to weights, these values are respectively\nmin(X) and max(X). However, when quantiza-\ntion is applied to activations, those values are run-\nning estimates. The latter are computed during\ntraining, where for every forward pass, the xmin\nand xmax variables are updated via an exponential\nmoving average with a momentum of 0.9. The\nclamp function associates all values outside of the\n3\n[xmin,xmax] range to the closest endpoint and ⌊·⌉\nrepresents rounding to the nearest integer. The\nvalue kis simply the bit precision. For example, in\nthe context of 8-bit quantization, k= 8.\nDuring backpropagation, we use the straight-\nthrough estimator (Hinton, 2012) and set the gra-\ndients of clamped values to zero. Once training\nis ﬁnished, sand xmin are frozen along with the\nweights.\n3.2 What to Quantize\nWe choose to quantize all operations which can\nprovide a computational speed gain at inference. In\nthis regard, we quantize all matrix multiplications,\nmeaning that the inputs and weights of MatMuls\nwill both be k-bit quantized. The other operations\nwe quantize are divisions, but only if both the nu-\nmerator and denominator are second or higher rank\ntensors. For all other operations, such as sums,\nthe computational cost added by the quantization\noperation outweighs the beneﬁt of performing the\noperation with reduced precision. Hence, we do\nnot quantize such operations.\nMore precisely, we quantize all weights of the\nTransformer, excluding biases. The latter are\nsummed with the INT32 output of matrix multi-\nplications and thus provide no additional compu-\ntational efﬁciency from being quantized. Further-\nmore, the memory space of biases is insigniﬁcant\nin comparison to the weight matrices, representing\nless than 0.1% of total weights. For positional em-\nbeddings, these are ﬁxed and can thus be quantized\nonce prior to training. The γ weights of Layer-\nNorms are also quantized. As for activations, we\nquantize the sum of the input embeddings with the\npositional encodings in both the encoder and de-\ncoder. In the Multi-Head Attention, we quantize\nthe (Q,K,V ) input, the softmax’s numerator, the\nsoftmax’s denominator, the softmax’s output and\nthe Scaled Dot-Product Attention’s output. At infer-\nence, the softmax does not need to be computed in\nfull-precision. Indeed, the exponential function can\ninstead be replaced with a step function. For the\nposition-wise feed-forward networks, we quantize\nthe output of the ReLUs and of the feed-forwards\nthemselves. Finally, for all LayerNorms, we quan-\ntize the numerator x−µ, the denominator\n√\nσ2 + ϵ,\ntheir quotient and the output of the LayerNorm. A\nvisual guide is provided in appendix A.\n3.3 Bucketing\nInstead of using a single set of (s,xmin) per quan-\ntized tensor, we can quantize subsets of the latter\nwith each its own set of (s,xmin) (Alistarh et al.,\n2016). Even though this adds more scalars, the\nmemory cost is insigniﬁcant overall. Furthermore,\nthe added ﬂexibility can greatly alleviate the preci-\nsion loss resulting from all values being mapped to\na single low numerical precision domain.\nWe use this bucketing method for all weight ma-\ntrices, with a number of subset equal to the output\ndimension. For activations, we use bucketing when\nquantizing: the sum of input embeddings with the\npositional encoding, the Q,K,V inputs, the Scaled\nDot-Product Attention’s output, the feed-forward’s\noutput, the LayerNorm’s numerator, quotient and\noutput.\n3.4 Dealing with Zeros\nUnlike Jacob et al. (2017), we do not nudge the do-\nmain so that the zero value gets perfectly mapped.\nThe only zero values which we have to deal with\nare the padding, the Softmax numerator and output,\nthe output of ReLU layers and dropouts. Since\npadding has no effect on the ﬁnal output, we com-\npletely ignore these values when quantizing. For\nReLUs and the Softmax’s numerator and output,\nwe ﬁx their xmin to 0, which guarantees the per-\nfect mapping of the value. Finally, quantization\nis applied before any dropout operation. Indeed,\neven though the zeros added to the output of the\nquantization layer might not be part of the domain,\nthis only happens during training.\n4 Related Work\nRecently, simple quantization solutions have been\napplied to the Transformer. Cheong and Daniel\n(2019) apply k-means quantization and binariza-\ntion with two centroids over the weights of the\nnetwork. For both methods, a look up table asso-\nciated with each quantized layer is used to map\nindices to their corresponding centroids. Similarly,\nFan (2019) compares binary, 4 and 8-bit uniform\nquantization of the Transformer weights. A big dis-\nadvantage with quantizing only the weights of a net-\nwork is that operations must still be performed in\nfull-precision. Even though the parameters’ mem-\nory usage is reduced, these constantly have to be\nconverted back to full-precision. Achieving quanti-\nzation of both weights and activations is much more\nbeneﬁcial. The ﬁrst attempt at doing so for the\n4\nTransformer applies 8-bit quantization on weights\nand inputs of feed forward layers and binarizes the\n(Q,K) input of the Multi-Head Attention (Tierno,\n2019). The scaling factor √dk is approximated by\na constant which can be computed as a right bitshift.\nThe method resulted in a huge drop in translation\naccuracy. Achieving better performance, Bhan-\ndare et al. (2019) quantize certain MatMul oper-\nations and use the KL divergence to estimate the\nmost suited parameters for each quantization range.\nThey restrain from quantizing all MatMuls, report-\ning poorer results in accuracy. Aside from trans-\nlation, the concurrent work by Zafrir et al. (2019)\nquantizes the embedding and fully connected layers\nof BERT (Devlin et al., 2018). The Softmax and\nLayerNorm operations are kept in full-precision.\nOn the GLUE benchmark, their loss in accuracy is\nminimal compared to the original model.\nAll of these methods omit quantizing the whole\nTransformer architecture, resulting in suboptimal\ncomputational efﬁciency. Furthermore, these solu-\ntions all fail to avoid impairing translation quality.\nOur method achieves both.\n5 Experiments\nIn this section, we present the results of our full\nquantization scheme on various tasks. We ﬁrst\ncompare our method on a machine translation setup.\nThen we present the results of numerous ablation\nstudies. We also compare the impact of delaying\nquantization on translation quality. Finally, we\nevaluate our method on two language model tasks\nand experiment with node pruning.\n5.1 Full Quantization\nWe apply our quantization strategy on both the\nbase and big Transformer (Vaswani et al., 2017).\nThe training setup of all presented models is the\nsame as in the original paper, with the excep-\ntion that the dropout ratio is set to 0.1 in all\ncases. We refer readers to the original paper\nfor experimental details. Our models were ﬁrst\nevaluated on the WMT 2014 / 2017 English-to-\nGerman and WMT 2014 English-to-French trans-\nlation tasks. Reported perplexity is per token and\nBLEU was measured with multi-bleu.pl1 on\nthe newstest20142 test set. We used beam\n1https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl\n2https://www.statmt.org/wmt14/\ntranslation-task.html\nsearch with a beam size of 4 and a length penalty\nof 0.6. Unlike Vaswani et al. (2017), no checkpoint\naveraging was performed.\nWe compare our results with the original Trans-\nformer and other 8-bit quantization methods in Ta-\nble 1. All models are base Transformers. Original\nuncompressed size is the same in all cases. Most\nwork do not report their compressed model size.\nFor those, we give lower bounds based on their\nreports. Our BLEU score was computed on the test\nset using the checkpoint with the highest validation\naccuracy over 2 million training steps. Validation\nwas computed every training epoch. Models were\ntrained once. Our objective was to train quantized\nmodels up to convergence. Very similar BLEU\nscores can be obtained with much fewer training\n(see below). As for other methods, Cheong and\nDaniel (2019) retrain for 10k steps a 200k steps\npretrained Transformer. Fan (2019) also does the\nsame but does not mention the number of retrain-\ning steps. Bhandare et al. (2019) and the original\nTransformer paper both do not mention the number\nof training steps. Out of all methods, we are the\nonly one quantizing every component of the model\n(see section 4 for details).\nIn Table 2, we show performance of our method\non the WMT14 EN-DE and WMT14 EN-FR for\na ﬁxed amount of training steps. We compare our\nresults with two full-precision Transformers: base\nand big variants. We also evaluate two other quan-\ntization approaches. The ﬁrst one is the ”default”\napproach, which is to naively quantize every pos-\nsible operation. The second approach applies our\nquantization strategy post-training (see section 5.3).\nIn all cases except for post-quantization, BLEU\nwas computed on the test set using the checkpoint\nwhich scored the highest accuracy on the valida-\ntion set. Towards the end of training, we ran one\nvalidation epoch for every 100 training steps. Base-\nlines and FullyQT 8-bit results were averaged over\n5 trials. Standard deviation of the BLEU scores\ndid not seem higher for any method and ranged\nbetween 0.09 and 0.51. Training with quantiza-\ntion was about twice as slow as with the baselines.\nAs for post-training quantization, the BLEU score\nwas computed on the test set using the best vali-\ndation performance out of 20 trials. The default\napproach’s nan in the EN-FR task is due to nu-\nmerical instability. By quantizing every operation,\nzeros in the LayerNorm’s denominator are more\nfrequent.\n5\nMethod Fully Size (Gb) Compr. BLEU\nQuantized [EN-DE, EN-FR] EN-DE (2014) EN-FR EN-DE (2017)\nVaswani et al. (2017) [2.02, 1.94] 1x 27.3 38.1 -\nCheong and Daniel (2019) 0.69 2.92x - - 27.38\nBhandare et al. (2019) ≥0.96 ≤2.1x 27.33 - -\nFan (2019) ≥0.51 ≤3.99x 26.94 - -\nFullyQT ✓ [0.52, 0.50] 3.91x 27.60 39.91 27.60\nTable 1: Our quantization strategy achieves better BLEU scores than all other quantization methods for the Trans-\nformer on the WMT14 EN-DE, WMT14 EN-FR and WMT17 EN-DE test set.\nModel Method Precision EN-DE EN-FR\nPPL BLEU Size (Gb) Compr. PPL BLEU Size (Gb) Compr.\nBase Baseline 32-bit 4.95 26.46 2.02 1x 3.21 38.34 1.94 1x\nDefault Approach 8-bit 74.04 0.21 0.52 3.91x nan 0 0.50 3.91x\nPost-Quantization 8-bit 4.97 26.44 0.52 3.91x 3.26 38.30 0.50 3.91x\nFullyQT 8-bit 4.94 26.38 0.52 3.91x 3.23 38.41 0.50 3.91x\nPost-Quantization 6-bit 6.00 24.84 0.39 5.18x 3.98 35.02 0.37 5.17x\nFullyQT 6-bit 5.09 26.98 0.39 5.18x 3.38 37.07 0.37 5.17x\nFullyQT 4-bit 11.96 18.32 0.26 7.66x 48.21 1.59 0.25 7.64x\nBig Baseline 32-bit 4.38 27.13 6.85 1x 2.77 40.54 6.69 1x\nPost-Quantization 8-bit 4.27 26.55 1.74 3.95x 2.78 39.78 1.69 3.95x\nFullyQT 8-bit 4.57 26.96 1.74 3.95x 2.80 40.25 1.69 3.95x\nPost-Quantization 6-bit 5.12 24.86 1.31 5.24x 3.08 37.92 1.28 5.24x\nFullyQT 6-bit 4.78 26.76 1.31 5.24x 2.87 39.59 1.28 5.24x\nFullyQT 4-bit 33.11 10.22 0.88 7.79x 42.42 2.81 0.86 7.79x\nTable 2: Performance of our quantization method on the WMT14 EN-DE and WMT14 EN-FR test set for a ﬁxed\nnumber of training steps.\nModel Method Precision EN-CS RU-EN ES-EN\nPPL BLEU PPL BLEU PPL BLEU\nBase Baseline 32-bit 6.90 22.71 3.56 32.62 5.59 29.99\nFullyQT 8-bit 6.81 23.06 3.53 33.08 5.60 29.88\nBig Baseline 32-bit 7.41 22.22 3.57 32.22 5.32 30.06\nFullyQT 8-bit 7.17 22.49 3.66 31.74 5.35 30.15\nTable 3: Evaluation of our quantization method on the WMT14 EN-CS, WMT14 RU-EN and WMT14 ES-EN\ntranslation datasets.\nResults on additional translation datasets can be\nfound in Table 3. All models were trained follow-\ning the same setup as WMT14 EN-FR and WMT14\nEN-DE. V ocabulary size is set to 32k for all mod-\nels. Since there is no test set for WMT14 ES-EN,\nwe used the validation set as a test set and omitted\ncomputing any validation epochs during training.\nLooking at all conducted experiments, includ-\ning section 5.3, translation quality of the 8-bit Ful-\nlyQT models seems to be on par with full-precision.\nMost of the time, the highest BLEU was scored by\nthe quantized model. For example in the case of\nWMT14 EN-DE, the maximum BLEU FullyQT\nbase 8-bit obtained was 26.98, while the baseline’s\nhighest was 26.64. As for the big models, the max\nFullyQT scored was 27.95, whereas the baseline’s\nwas 27.43. We looked at training and validation\ncurves to see if quantization had any effect, but saw\nno discernible difference.\nAll models use full-precision biases, sand xmin.\nThis amounts to 11.61 Mb in the base models and\n23.15 Mb in the big models. In the case of 8-bit,\nthese represent less than 2.35% of the total size.\nWithout bucketing, this would amount to 2.18 Mb\nand 4.35 Mb respectively. We believe the small\nincrease in model size to be worth it. Indeed, in\nsection 5.2, we show that training without bucket-\ning leads to poorer translation.\n6\nAlthough 6-bit quantization seems to perform\nwell, the compression advantage over 8-bit is usu-\nally lost. Most hardware store INT6 using either 8\nor 32 bits. Dedicated hardware is needed to get the\nfull compression advantage. Unless 6-bit quantiza-\ntion results in better models, 8-bit seems like the\nbest choice for most hardware.\n5.2 Ablation Studies\nTo better understand which operations are more\nsensitive to quantization, we evaluate such effect\non single operations of the Transformer. By this,\nwe mean quantizing the operation of a module for\nall Transformer layers. Table 4 shows results on\nthe WMT14 EN-FR translation task for 8-bit pre-\ncision. The effect of bucketing was also evaluated.\nBLEU was computed on the test set after 100k steps\nof training. In 24 out of 27 experiments, perfor-\nmance was better than our full-precision baseline of\n38.34 BLEU. Solely quantizing the LayerNorm’s\ndenominator with no bucketing results in poor per-\nformance. The latter also cannot be bucketed since\nall dimensions of the variance tensor vary per batch.\nTo successfully quantize this element without caus-\ning any loss in performance, we suspect quantizing\nother elements in the network helps.\nTo further validate our quantization scheme, we\nevaluated four models trained with alterations to\nour design choices. Results on the WMT14 EN-FR\ntask are presented in Table 5. All models are 8-bit\nquantized base Transformers. Training procedure\nis the same as in section 5.1.\n5.3 Delaying Quantization\nOur method’s goal is to increase computational ef-\nﬁciency when inferring with the Transformer. To\nthis end, our quantization scheme only requires us\nto learn sand xmin. Although we do so throughout\nthe whole training, this is not a necessity. Quanti-\nzation could also be applied later during training.\nResults for different starting points are compared\nin Table 6. The earliest we start quantizing is at\n100 steps, since we need at least a few steps to\nassess the running estimates. All models were eval-\nuated on the WMT14 EN-DE and WMT14 EN-FR\ntranslation tasks. BLEU was measured on the test\nset using the checkpoint which scored the high-\nest accuracy on the validation set during training.\nValidation was computed every 100 training steps\ntowards the end of training. From our observed\nresults, quantizing the model early on seems prefer-\nable.\nLearning quantization parameters adds a signiﬁ-\ncant computational cost during training. A major\nadvantage to delaying quantization is to perform\nmore training steps in the same given amount of\ntime. Therefore, when training time is a constraint,\na possible strategy is to train a model without quan-\ntization, perform more training steps and ﬁnally\npost-quantize the model. By the latter, we mean\nkeeping all weights ﬁxed and compute the sand\nxmin over a few hundred steps. This is another\nadvantage, since many trials can be performed in\nsearch of the best performing candidate. We found\npost-quantization BLEU scores to vary by about\n0.2 BLEU.\n5.4 Language Modeling\nTo evaluate if our quantization scheme generalizes\nwell to other tasks, we evaluate it on two language\nmodeling datasets: WikiText-2 and WikiText-103.\nAs the setup, we use PyTorch’s language model-\ning toy example3. The task consists of predicting\nthe sequence {xt+1,··· ,xt+n+1}from the input\nsequence {xt,··· ,xt+n}. We trained four Trans-\nformer models, each with different precision. All\nmodels consist of two Transformer encoder lay-\ners, with the embedding and hidden size set to 200.\nMulti-Head Attention has two heads with key and\nvalue size 64. The ﬁnal word projection layer’s\nweights are shared with the embedding layer. Mod-\nels were trained for 10 epochs with a batch size\nof 20 and sequence length of 35. Learning rate is\nset to 5, dropout to 0.2 and gradient clipping to\n0.25. Loss is computed on every element of the\noutput sequence. Results are presented in Table 7.\nValidation was computed every epoch to determine\nthe best candidate. Loss and perplexity are com-\nputed on the test set and averaged over 10 trials\nfor WikiText-2 and 3 trials for WikiText-3. See\nfootnote 3 for any extra details.\n6 Pruning Useless Nodes\nWe experiment with node pruning our Transformer\nmodels. Once the model is fully trained and quan-\ntized, we can further compress it by removing use-\nless nodes. By useless, we mean nodes which do\nnot cause any loss in translation quality when re-\nmoved. We choose to prune nodes instead of in-\ndependently pruning weights. The latter method\nusually requires special hardware or software to\n3https://github.com/pytorch/examples/\ntree/master/word_language_model\n7\nModule Quantized Activation No Bucketing Bucketing\nPPL BLEU PPL BLEU\nEncoder (Input Embedding + Positional Encoding) 3.20 38.61 3.20 39.08\nDecoder (Input Embedding + Positional Encoding) 3.20 39.35 3.20 39.36\nMulti-HeadAttention\nInput(Q,K,V) 3.21 39.06 3.21 39.29\nLayerNorm Output 3.21 39.09 3.20 38.78\nScaledDot-ProductAttention\nSoftmax Numerator 3.20 39.32 3.21 39.01\nSoftmax Denominator 3.21 39.35 3.21 39.11\nSoftmax Output 3.22 39.41 3.22 38.87\nOutput 3.21 38.73 3.21 39.02\nFeed-forward\nReLU Output 3.21 39.43 3.22 38.93\nFeed-forward Output 3.54 38.03 3.20 39.27\nLayerNorm Output 3.21 38.67 3.21 39.04\nLayerNorm\nNumerator 3.53 37.75 3.21 38.86\nDenominator 1748 0 - -\nQuotient 3.22 38.97 3.21 39.02\nTable 4: Effect of quantizing single activations of the Transformer. Results are on the WMT14 EN-FR test set.\nMethod PPL BLEU\nNo Bucketing 3.49 37.14\nNo Gradient Clipping 2549.30 0\nNo LayerNorm Denominator Quantization 3.22 38.29\n8-bit Quantized Weights, Full-precision Activations 3.20 38.36\nTable 5: Variations to our quantization scheme evaluated on the WMT14 EN-FR translation task.\nQuantization Start EN-DE EN-FR\n(training step) PPL BLEU PPL BLEU\nNever quantized 4.95 26.46 3.21 38.34\n100 4.67 26.98 3.23 38.55\n10000 4.99 26.63 3.21 38.62\n50000 4.98 26.84 3.21 38.50\n80000 5.03 26.41 3.21 38.43\nPost-Quantization 4.45 25.50 3.22 37.96\nTable 6: Impact of delaying quantization. Results are\non the WMT14 EN-DE and WMT14 EN-FR test set.\nleverage sparse weight matrices. Pruning nodes\nresults in concretely shrunken models. When get-\nting rid of a node, we remove its corresponding\nset of weights from the layer outputting it and the\nfollowing layer receiving the node as input.\nThe only nodes of the Transformer which can\nbe removed without causing alterations to other\ncomponents of the network are the nodes in be-\ntween the two layers of each feed-forward network.\nFortunately, these consist of a substantial portion\nof the model’s weights. In the case of the base\nTransformer, for a respective vocabulary of size\n37000 and 32000, 39.96% and 41.65% of the total\nweights are owned by the feed-foward networks.\nThis number grows to 47.03% and 48.18% in the\nbig Transformer.\nTo evaluate which nodes can be safely pruned\nwithout affecting translation quality, we estimate\nxmax for each node of the ReLU output over a few\nhundred steps. This is done on the training set,\nusing the fully trained model and keeping all other\nweights frozen. These xmax are computed before\nquantizing the ReLU output and do not replace the\nones used by the quantization process. Figure 3 in\nthe appendix shows the histogram of these running\nestimates for one ReLU layer in the encoder and\none in the decoder. All other ReLU layers share the\nsame pattern, where in the encoder there are always\nmultiple xmax close to 0. This does not happen in\nthe decoder.\nOnce the running estimates are computed, we\nprune its corresponding node if xmax <zσ where\nzis a hyperparameter and σthe standard deviation\nof the layer’s xmax. We empirically found z =\n0.025 to work well, with higher thresholds causing\nBLEU to quickly decay. No retraining of the model\nis performed after pruning nodes.\nUsing this method, we can further compress\nthe Transformer without affecting BLEU scores.\nOur approach has the advantage of being adaptive,\n8\nPrecision Size (Mb) Compression WikiText-2 WikiText-103\nLoss PPL Loss PPL\n32-bit 243.04 1x 5.65 284.15 5.91 369.20\n8-bit 61.93 3.92x 5.64 282.67 5.94 377.79\n6-bit 46.75 5.20x 5.64 281.48 5.93 376.44\n4-bit 31.57 7.70x 5.65 284.26 5.94 378.67\nTable 7: Evaluation of our quantization method on the WikiText-2 and WikiText-103 language modeling tasks.\nModel Precision Method\nEN-DE EN-FR\nPPL BLEU Nodes Pruned Total PPL BLEU Nodes Pruned Total\nin Encoder FF Compr. in Encoder FF Compr.\nBase 8-bit No pruning 4.39 27.60 0% 3.95x 2.90 39.91 0% 3.95x\nL1-norm ﬁxed5.57 23.99 13.57% 4.02x 4.38 29.01 9.47% 3.99x\nxmaxﬁxed 4.57 27.33 13.57% 4.02x 3.18 39.40 9.47% 3.99x\nxmaxadaptive 4.40 27.60 13.57% 4.02x 2.90 39.91 9.47% 3.99x\n6-bit No pruning 5.09 26.98 0% 5.25x 3.38 37.07 0% 5.24x\nL1-norm ﬁxed6.97 20.81 12.06% 5.31x 4.19 31.64 9.62% 5.28x\nxmaxﬁxed 5.41 26.20 12.06% 5.31x 3.68 36.91 9.62% 5.28x\nxmaxadaptive 5.09 26.98 12.06% 5.31x 3.38 37.07 9.62% 5.28x\nBig 8-bit No pruning 4.24 27.95 0% 3.97x 2.80 40.17 0% 3.97x\nL1-norm ﬁxed5.80 22.65 26.39% 4.21x 4.16 28.85 28.41% 4.24x\nxmaxﬁxed 4.47 27.43 26.39% 4.21x 2.91 39.40 28.41% 4.24x\nxmaxadaptive 4.25 27.95 26.39% 4.21x 2.80 40.17 28.41% 4.24x\n6-bit No pruning 4.78 26.76 0% 5.28x 2.87 39.59 0% 5.28x\nL1-norm ﬁxed7.73 17.32 29.96% 5.64x 7.88 15.09 22.66% 5.54x\nxmaxﬁxed 4.92 26.86 29.96% 5.64x 2.91 39.25 22.66% 5.54x\nxmaxadaptive 4.78 26.76 29.96% 5.64x 2.87 39.59 22.66% 5.54x\nTable 8: Comparison of our adaptive pruning scheme versus ﬁxed rate pruning methods for equal pruning propor-\ntions. Total compression accounts for quantization combined with pruning.\nmeaning the number of nodes pruned per layer will\ndiffer as opposed to a ﬁxed pruning ratio method.\nFor example, in the case of the big Transformer\ntrained on WMT14 EN-FR, 169 nodes were pruned\nin the ﬁrst ReLU of the encoder, while in the sec-\nond, 1226 were pruned. Nodes in the decoder rarely\ngot pruned, at most 4 in the whole decoder. Results\nare presented in Table 8. Reported results are aver-\naged on the test set over a few trials. BLEU varied\nby about 0.01−0.02.\nOther approaches usually decide the ratio ﬁrst\nand then prune. We compared with two such meth-\nods. For each task, we ﬁx their ratio to the average\npercentage of nodes pruned by our method and only\nprune nodes in the encoder. The ﬁrst ﬁxed pruning\nmethod uses L1-norm to sort nodes in ascending\nweight order, while the second sorts the xmax, also\nin ascending order.\n7 Conclusion\nWe proposed a full quantization strategy for the\nTransformer architecture. Our objective was to ex-\nploit hardware resources as efﬁciently as possible,\nquantizing all operations which could provide a\ncomputational speed gain.\nWith FullyQT, we achieve higher BLEU scores\nthan all other quantization methods for the Trans-\nformer on multiple translation tasks and avoid any\nloss in BLEU compared to full-precision. Specif-\nically, out of 35 experiments, 8-bit quantization\nperformed better than full-precision in 21 cases.\nIf instead of minimizing inference time, one\nwants to maximize translation accuracy, then ap-\nplying quantization to only certain components of\nthe Transformer seems to be the best option. In-\ndeed, our ablation study showed than BLEU score\ncould increase even more when only speciﬁc ele-\nments of the Transformer were quantized. Further\ngains might be possible, but supplementary exper-\niments would be necessary to determine the best\ncombination.\nWe plan on extending our work to variations of\nthe Transformer, as well as further exploring the\ncompression of these networks.\n9\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted Transformer Network\nfor Machine Translation. arXiv e-prints, page\narXiv:1711.02132.\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka,\nand Milan V ojnovic. 2016. QSGD: Communication-\nEfﬁcient SGD via Gradient Quantization and Encod-\ning. arXiv e-prints, page arXiv:1610.02132.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural Machine Translation by Jointly\nLearning to Align and Translate. arXiv e-prints,\npage arXiv:1409.0473.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi\nKarkada, Vivek Menon, Sun Choi, Kushal Datta,\nand Vikram Saletore. 2019. Efﬁcient 8-Bit\nQuantization of Transformer Neural Machine\nLanguage Translation Model. arXiv e-prints, page\narXiv:1906.00532.\nPatrick Chen, Si Si, Yang Li, Ciprian Chelba, and\nCho-Jui Hsieh. 2018. Groupreduce: Block-wise\nlow-rank approximation for neural language model\nshrinking. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31, pages 10988–10998. Curran Associates,\nInc.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong Short-Term Memory-Networks for Machine\nReading. arXiv e-prints, page arXiv:1601.06733.\nRobin Cheong and Robel Daniel. 2019. transform-\ners.zip: Compressing Transformers with Pruning\nand Quantization. Technical report, Stanford Uni-\nversity, Stanford, California.\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. arXiv\ne-prints, page arXiv:1406.1078.\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Binarized\nNeural Networks: Training Deep Neural Networks\nwith Weights and Activations Constrained to +1 or\n-1. arXiv e-prints, page arXiv:1602.02830.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training\nof Deep Bidirectional Transformers for Lan-\nguage Understanding. arXiv e-prints , page\narXiv:1810.04805.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding Back-Translation at\nScale. arXiv e-prints, page arXiv:1808.09381.\nChaofei Fan. 2019. Quantized Transformer. Technical\nreport, Stanford University, Stanford, California.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing Deep Convolutional\nNetworks using Vector Quantization. arXiv e-prints,\npage arXiv:1412.6115.\nSong Han, Huizi Mao, and William J. Dally.\n2015. Deep Compression: Compressing Deep\nNeural Networks with Pruning, Trained Quantiza-\ntion and Huffman Coding. arXiv e-prints, page\narXiv:1510.00149.\nBabak Hassibi, David G. Stork, and Gregory Wolff.\n1994. Optimal brain surgeon: Extensions and per-\nformance comparisons. In J. D. Cowan, G. Tesauro,\nand J. Alspector, editors, Advances in Neural In-\nformation Processing Systems 6, pages 263–270.\nMorgan-Kaufmann.\nQinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong\nYao, Xinyu Zhou, and Yuheng Zou. 2016. Effec-\ntive Quantization Methods for Recurrent Neural Net-\nworks. arXiv e-prints, page arXiv:1611.10176.\nGeoffrey Hinton. 2012. Neural networks for machine\nlearning. Coursera, video lectures.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the Knowledge in a Neural Network.\narXiv e-prints, page arXiv:1503.02531.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9:1735–\n80.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Quantized\nNeural Networks: Training Neural Networks with\nLow Precision Weights and Activations. arXiv e-\nprints, page arXiv:1609.07061.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2017. Quantiza-\ntion and Training of Neural Networks for Efﬁcient\nInteger-Arithmetic-Only Inference. arXiv e-prints,\npage arXiv:1712.05877.\nMichael I. Jordan. 1990. Artiﬁcial neural networks.\nchapter Attractor Dynamics and Parallelism in a\nConnectionist Sequential Machine, pages 112–127.\nIEEE Press, Piscataway, NJ, USA.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\n10\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1700–1709, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classiﬁcation with deep con-\nvolutional neural networks. In F. Pereira, C. J. C.\nBurges, L. Bottou, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems\n25, pages 1097–1105. Curran Associates, Inc.\nY . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. 1989. Back-\npropagation applied to handwritten zip code recog-\nnition. Neural Computation, 1(4):541–551.\nYann LeCun, John S. Denker, and Sara A. Solla. 1990.\nOptimal brain damage. In D. S. Touretzky, editor,\nAdvances in Neural Information Processing Systems\n2, pages 598–605. Morgan-Kaufmann.\nFengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary\nWeight Networks. arXiv e-prints , page\narXiv:1605.04711.\nZhouhan Lin, Matthieu Courbariaux, Roland Memise-\nvic, and Yoshua Bengio. 2015. Neural Networks\nwith Few Multiplications. arXiv e-prints, page\narXiv:1510.03009.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos\nSantos, Mo Yu, Bing Xiang, Bowen Zhou, and\nYoshua Bengio. 2017. A Structured Self-attentive\nSentence Embedding. arXiv e-prints , page\narXiv:1703.03130.\nBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall\nTappen, and Marianna Pensky. 2015. Sparse con-\nvolutional neural networks. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-Task Deep Neural Net-\nworks for Natural Language Understanding. arXiv\ne-prints, page arXiv:1901.11504.\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao\nHuang, Shoumeng Yan, and Changshui Zhang.\n2017. Learning Efﬁcient Convolutional Networks\nthrough Network Slimming. arXiv e-prints, page\narXiv:1708.06519.\nMinh-Thang Luong, Hieu Pham, and Christopher D.\nManning. 2015. Effective Approaches to Attention-\nbased Neural Machine Translation. arXiv e-prints,\npage arXiv:1508.04025.\nM. Marchesi, G. Orlandi, F. Piazza, and A. Uncini.\n1993. Fast neural networks without multipliers.\nIEEE Transactions on Neural Networks, 4(1):53–62.\nSharan Narang, Erich Elsen, Gregory Diamos, and\nShubho Sengupta. 2017a. Exploring Sparsity in\nRecurrent Neural Networks. arXiv e-prints, page\narXiv:1704.05119.\nSharan Narang, Eric Undersander, and Gregory Di-\namos. 2017b. Block-Sparse Recurrent Neural Net-\nworks. arXiv e-prints, page arXiv:1711.02782.\nJoachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu,\nand Yoshua Bengio. 2016. Recurrent Neural Net-\nworks With Limited Numerical Precision. arXiv e-\nprints, page arXiv:1608.06902.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling Neural Machine Trans-\nlation. arXiv e-prints, page arXiv:1806.00187.\nJinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin,\nand Wonyong Sung. 2018. Fully neural network\nbased speech recognition on mobile and embedded\ndevices. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31, pages 10620–10630. Curran Associates,\nInc.\nAntonio Polino, Razvan Pascanu, and Dan Alistarh.\n2018. Model compression via distillation and quan-\ntization. arXiv e-prints, page arXiv:1802.05668.\nMohammad Rastegari, Vicente Ordonez, Joseph Red-\nmon, and Ali Farhadi. 2016. XNOR-Net: ImageNet\nClassiﬁcation Using Binary Convolutional Neural\nNetworks. arXiv e-prints, page arXiv:1603.05279.\nAbigail See, Minh-Thang Luong, and Christopher D.\nManning. 2016. Compression of Neural Machine\nTranslation Models via Pruning. arXiv e-prints,\npage arXiv:1606.09274.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27, pages\n3104–3112. Curran Associates, Inc.\nC. Z. Tang and H. K. Kwan. 1993. Multilayer feed-\nforward neural networks with single powers-of-two\nweights. IEEE Transactions on Signal Processing,\n41(8):2724–2727.\nAndrew Tierno. 2019. Quantized Transformer. Techni-\ncal report, Stanford University, Stanford, California.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. arXiv e-prints, page arXiv:1706.03762.\nPeiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dong-\nsheng Wang, and Yuan Xie. 2018. Hitnet: Hy-\nbrid ternary recurrent neural network. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-\nBianchi, and R. Garnett, editors,Advances in Neural\nInformation Processing Systems 31, pages 604–614.\nCurran Associates, Inc.\n11\nWei Wen, Yuxiong He, Samyam Rajbhandari, Minjia\nZhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran\nChen, and Hai Li. 2017. Learning Intrinsic Sparse\nStructures within Long Short-Term Memory. arXiv\ne-prints, page arXiv:1709.05027.\nJiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu,\nand Jian Cheng. 2015. Quantized Convolutional\nNeural Networks for Mobile Devices. arXiv e-\nprints, page arXiv:1512.06473.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Łukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google’s\nNeural Machine Translation System: Bridging the\nGap between Human and Machine Translation.\narXiv e-prints, page arXiv:1609.08144.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8Bit BERT.\narXiv e-prints, page arXiv:1910.06188.\nDongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and\nGang Hua. 2018. LQ-Nets: Learned Quantization\nfor Highly Accurate and Compact Deep Neural Net-\nworks. arXiv e-prints, page arXiv:1807.10029.\nShuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou,\nHe Wen, and Yuheng Zou. 2016. DoReFa-Net:\nTraining Low Bitwidth Convolutional Neural Net-\nworks with Low Bitwidth Gradients. arXiv e-prints,\npage arXiv:1606.06160.\nMichael Zhu and Suyog Gupta. 2017. To prune,\nor not to prune: exploring the efﬁcacy of prun-\ning for model compression. arXiv e-prints, page\narXiv:1710.01878.\n12\nA FullyQT Visual Guide\nFigure 1 and 2.\nB Node Pruning Running Estimate\nFigure 3.\n13\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nInput\nEmbedding\nAdd & Norm\nPositional\nEncoding\nN× \nInputs\nMasked\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nOutput\nEmbedding\nAdd & Norm\nPositional\nEncoding\nN× \nOutputs\n(shifted right)\nMulti-Head\nAttention\nAdd & Norm\nLinear\nSoftmax\nOutput\nProbabilities\nQuantize\nReLU \nX \nQuantize \nLinear \nQuantize \nLinear \nFigure 1: (left) Fully Quantized Transformer, (right) Feed-forward.\nMatMul\nScale\nMask (opt.)\nSoftmax\nMatMul\nQ K V \nQuantize\nQuantizeQuantize Quantize\nLinear Linear\nScaled Dot-Product\nAttention\nLinear Linear\nScaled Dot-Product\nAttention\nConcat\nh \nK Q V \nLinearLinear\nQuantize\nLinear\nQuantizeQuantizeQuantize\nFigure 2: (left) Scaled Dot-Product Attention, (right) Multi-Head Attention.\n14\nFigure 3: xmax histogram of a ReLU layer in the encoder (left) and decoder (right), one xmax per output node.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8724536895751953
    },
    {
      "name": "Transformer",
      "score": 0.8521589040756226
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.7562394142150879
    },
    {
      "name": "Computer science",
      "score": 0.7016118764877319
    },
    {
      "name": "Translation (biology)",
      "score": 0.44959911704063416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.430819571018219
    },
    {
      "name": "Algorithm",
      "score": 0.3943553864955902
    },
    {
      "name": "Computer engineering",
      "score": 0.34334754943847656
    },
    {
      "name": "Engineering",
      "score": 0.131597101688385
    },
    {
      "name": "Voltage",
      "score": 0.11575835943222046
    },
    {
      "name": "Electrical engineering",
      "score": 0.11263984441757202
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ],
  "cited_by": 8
}