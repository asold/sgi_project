{
  "title": "DeepMetaForge: A Deep Vision-Transformer Metadata-Fusion Network for Automatic Skin Lesion Classification",
  "url": "https://openalex.org/W4389961133",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3105808210",
      "name": "Sirawich Vachmanus",
      "affiliations": [
        "Mahidol University"
      ]
    },
    {
      "id": "https://openalex.org/A213474391",
      "name": "Thanapon Noraset",
      "affiliations": [
        "Mahidol University"
      ]
    },
    {
      "id": "https://openalex.org/A3036986949",
      "name": "Waritsara Piyanonpong",
      "affiliations": [
        "Ramathibodi Hospital",
        "Mahidol University"
      ]
    },
    {
      "id": "https://openalex.org/A2971414132",
      "name": "Teerapong Rattananukrom",
      "affiliations": [
        "Mahidol University",
        "Ramathibodi Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A206342214",
      "name": "Suppawong Tuarob",
      "affiliations": [
        "Mahidol University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3128646645",
    "https://openalex.org/W2154553317",
    "https://openalex.org/W4280617741",
    "https://openalex.org/W6712165758",
    "https://openalex.org/W2965873243",
    "https://openalex.org/W6714702107",
    "https://openalex.org/W3093647740",
    "https://openalex.org/W2889264069",
    "https://openalex.org/W2955165645",
    "https://openalex.org/W4225310474",
    "https://openalex.org/W3011885901",
    "https://openalex.org/W3133356497",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W6842542540",
    "https://openalex.org/W3157022347",
    "https://openalex.org/W4353091525",
    "https://openalex.org/W4285290199",
    "https://openalex.org/W3198143721",
    "https://openalex.org/W2100111947",
    "https://openalex.org/W2095641976",
    "https://openalex.org/W2023272005",
    "https://openalex.org/W2057343036",
    "https://openalex.org/W3005190725",
    "https://openalex.org/W2789357239",
    "https://openalex.org/W4297851380",
    "https://openalex.org/W2966495887",
    "https://openalex.org/W2768611607",
    "https://openalex.org/W4206369442",
    "https://openalex.org/W3105645498",
    "https://openalex.org/W4306768101",
    "https://openalex.org/W3127886489",
    "https://openalex.org/W3151500074",
    "https://openalex.org/W2986609444",
    "https://openalex.org/W2290883490",
    "https://openalex.org/W4307231856",
    "https://openalex.org/W4210634901",
    "https://openalex.org/W4206055728",
    "https://openalex.org/W4220826373",
    "https://openalex.org/W4289260297",
    "https://openalex.org/W4320913615",
    "https://openalex.org/W3008195728",
    "https://openalex.org/W2958919462",
    "https://openalex.org/W2234281713",
    "https://openalex.org/W2064741553",
    "https://openalex.org/W2990259914",
    "https://openalex.org/W2910440362",
    "https://openalex.org/W2959492430",
    "https://openalex.org/W4255958015",
    "https://openalex.org/W2086478842",
    "https://openalex.org/W2899625073",
    "https://openalex.org/W2997636549",
    "https://openalex.org/W3133754732",
    "https://openalex.org/W3148150040",
    "https://openalex.org/W3081326413",
    "https://openalex.org/W3026316716",
    "https://openalex.org/W3212386989",
    "https://openalex.org/W4312912923",
    "https://openalex.org/W6746023985",
    "https://openalex.org/W4229003689",
    "https://openalex.org/W3215150387",
    "https://openalex.org/W3157833636",
    "https://openalex.org/W2910954186",
    "https://openalex.org/W3121732873",
    "https://openalex.org/W3081500256",
    "https://openalex.org/W2979841293",
    "https://openalex.org/W2061253660",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6766263406",
    "https://openalex.org/W62900058",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4320930577",
    "https://openalex.org/W2411420082",
    "https://openalex.org/W2399625966"
  ],
  "abstract": "Skin cancer is a dangerous form of cancer that develops slowly in skin cells. Delays in diagnosing and treating these malignant skin conditions may have serious repercussions. Likewise, early skin cancer detection has been shown to improve treatment outcomes. This paper proposes DeepMetaForge, a deep-learning framework for skin cancer detection from metadata-accompanied images. The proposed framework utilizes BEiT, a vision transformer pre-trained as a masked image modeling task, as the image-encoding backbone. We further propose merging the encoded metadata with the derived visual characteristics while compressing the aggregate information simultaneously, simulating how photos with metadata are interpreted. The experiment results on four public datasets of dermoscopic and smartphone skin lesion images reveal that the best configuration of our proposed framework yields 87.1&#x0025; macro-average F1 on average. The empirical scalability analysis further shows that the proposed framework can be implemented in a variety of machine-learning paradigms, including applications on low-resource devices and as services. The findings shed light on not only the possibility of implementing telemedicine solutions for skin cancer on a nationwide scale that could benefit those in need of quality healthcare but also open doors to many intelligent applications in medicine where images and metadata are collected together, such as disease detection from CT-scan images and patients&#x2019; expression recognition from facial images.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nDeepMetaForge: A Deep\nVision-Transformer Metadata-Fusion\nNetwork for Automatic Skin Lesion\nClassification\nSIRAWICH VACHMANUS1, THANAPON NORASET1, WARITSARA PIYANONPONG2,\nTEERAPONG RATTANANUKROM2, and SUPPAWONG TUAROB1 (Member, IEEE)\n1Faculty of Information and Communication Technology, Mahidol University, Nakhon Pathom 73170 Thailand\n2Division of Dermatology, Faculty of Medicine, Ramathibodi Hospital, Mahidol University, Thailand\nCorresponding author: Suppawong Tuarob (e-mail: suppawong.tua@mahidol.edu).\nThis research paper is supported by Specific League Funds from Mahidol University.\nABSTRACT\nSkin cancer is a dangerous form of cancer that develops slowly in skin cells. Delays in diagnosing and\ntreating these malignant skin conditions may have serious repercussions. Likewise, early skin cancer\ndetection has been shown to improve treatment outcomes. This paper proposes DeepMetaForge, a deep-\nlearning framework for skin cancer detection from metadata-accompanied images. The proposed framework\nutilizes BEiT, a vision transformer pre-trained as a masked image modeling task, as the image-encoding\nbackbone. We further propose merging the encoded metadata with the derived visual characteristics\nwhile compressing the aggregate information simultaneously, simulating how photos with metadata are\ninterpreted. The experiment results on four public datasets of dermoscopic and smartphone skin lesion\nimages reveal that the best configuration of our proposed framework yields 87.1% macro-average F1 on\naverage. The empirical scalability analysis further shows that the proposed framework can be implemented\nin a variety of machine-learning paradigms, including applications on low-resource devices and as services.\nThe findings shed light on not only the possibility of implementing telemedicine solutions for skin cancer\non a nationwide scale that could benefit those in need of quality healthcare but also open doors to many\nintelligent applications in medicine where images and metadata are collected together, such as disease\ndetection from CT-scan images and patients‚Äô expression recognition from facial images.\nINDEX TERMS Image-Metadata Fusion, Deep Learning, Skin Lesion Classification\nI. INTRODUCTION\nSkin cancer is among the most common cancerous diseases\nin many countries [1]. Early identification of skin cancer,\nwhile still not prevalent, is critical for improving treatment\noutcomes and may lead to lower mortality rates [2]. In\nmedical practice, some benign and malignant conditions are\ndifficult to distinguish from each other, as their dermatologic\nmanifestations are very resembling and can be wrongly diag-\nnosed as one another. For instance, skin-colored and pearly-\nrolled edge nodules on facial skin can be diagnosed clinically\nwith cutaneous basal cell carcinoma (BCC) [3], but the other\nskin neoplasm can be a mimicker, such as SCC, amelanotic\nmelanoma, and trichoepithelioma. Another example includes\nsquamous cell carcinoma (SCC) [3], which is clinically\nsimilar to actinic keratosis (AK), amelanotic melanoma,\nBCC, warts, spindle cell tumor, traumatic wounds, or other\nbenign tumors [4], while melanoma, the deadliest form of\nskin cancer [5], might be challenging to differentiate from\nseborrheic keratosis, melanocytic nevus, pigmented BCC,\npigmented AK, lentigo, angiokeratoma, dermatofibroma, or\nsome vascular abnormalities [6]. Therefore, due to the variety\nof treatments and prognoses, these dermatological disorders\nneed actual diagnoses. Dermoscopy is a noninvasive, in\nvivo diagnostic procedure used largely to examine cutaneous\nlesions. Traditionally, such a procedure has been utilized\nas a guide to distinguish between benign and malignant\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nskin lesions before performing a skin biopsy to provide a\ndefinitive diagnosis.\nThe initial presentation of these skin conditions can cause\nanxiety in different aspects and encourage patients with\nsuspected skin conditions to visit hospitals for professional\ndiagnoses. These circumstances, causing the sheer influx of\nconcerned patients, have posed bottleneck problems in many\nhospitals with insufficient dermatologists, causing hospital\ncongestion that may lead to a decline in the quality of medical\ncare [7]. Furthermore, studies have shown that patients who\nlive far from major hospitals, particularly in rural regions\nor with poor incomes, identified long waits and long travel\ndistances as barriers to receiving proper dermatologic care\n[8]. As a result, people in rural areas are more likely to\ndisregard their health concerns until it is too late, owing\nto the expenses of communication and hospitalization, as\nwell as their hectic routines [9, 10]. These issues behoove\ntelemedicine solutions or other detection technologies that\ncan facilitate diagnoses in patients with suspected skin dis-\norders without requiring them to visit major hospitals. Such\nsystems could potentially be used by non-dermatologists, es-\npecially primary care providers who work in rural areas, with\nguidance from artificial intelligence and remote professional\ndermatologists, allowing concerning patients in rural areas to\nreceive mainstream treatment in a timely manner. Developing\na telemedicine system for skin cancer detection, however,\nrequires intelligent components that not only automatically\nand reliably distinguish malignant from benign skin diseases\nbut also demand reasonable processing resources.\nPrevious literature has utilized deep learning technolo-\ngies to produce classification models for skin cancer de-\ntection from images [11]. Furthermore, recent discoveries\nhave shown that patients‚Äô metadata could provide additional\nuseful information when appropriately integrated into the\nclassifiers‚Äô training processes. The majority of the previ-\nously proposed metadata-fusing methods either concatenate\nthe encoded metadata to the image embedding [12] or use\nmetadata to guide visual feature extraction [13]. Although\nthese techniques may be logical from the standpoint of the\nmodel, they fail to replicate the way in which people under-\nstand the meaning of pictures in conjunction with metadata.\nThis is particularly relevant in the context of diagnosing\nskin lesions, where dermatologists rely on a combination of\nvisual and metadata cues altogether rather than sequentially.\nNevertheless, most of these approaches were only validated\non a single dataset, limiting their evidence of generalizability\nand robustness against varying image quality and metadata\ncompositions from diverse data sources. Furthermore, the\npreponderance of past research on metadata-fusing method-\nologies for skin lesion classification has merely examined the\nefficacy of their proposed models without evaluating their\nscalability in the context of system implementation. The\nultimate goal of our research is to establish a system that\ncan be accessible by both patients and healthcare providers\nthroughout the country, particularly those whose physical ac-\ncess to major hospitals is hindered. Therefore, the realization\naspects of the proposed algorithms are equally important.\nIn this paper, we propose DeepMetaForge, a visual\ntransformer-based deep-learning framework for skin lesion\nclassification using both images and patient metadata. The\nproposed framework utilizes the BEiT [14] backbone, which\nuses the self-attention mechanism to pre-train the model as a\nmasked image modeling (MIM) task, inspired by the masked\nlanguage modeling (MLM) task that was found successful\nfor pre-training transformer-based language models [15]. To\nour knowledge, we are the first to evaluate the BEiT back-\nbone on the skin lesion classification problem. Furthermore,\nwe propose the Deep Metadata Fusion Module (DMFM)\nthat combines the visual features extracted from BEiT and\nmetadata encoded by a convolutional neural network (CNN)\nwhile simultaneously compressing and decompressing the\namalgamated information, similar to the process of forging\nmetal decoration onto a steel plate. We hypothesize that\nfusing the metadata while compressing the visual features\nallows the metadata to blend in with the image informa-\ntion more effectively. This process intuitively aligns with\nhow humans perceive semantics from metadata-accompanied\nimages, where metadata is interpreted simultaneously while\ndigesting the image content rather than sequentially. The ex-\nperiment results on four public datasets demonstrate that our\nproposed DeepMetaForge framework exhibits superior per-\nformance compared to the best image classification backbone\nand the state-of-the-art metadata-fusing approach by a large\nmargin. In addition, the scalability analysis found that the\nBEiT backbone utilized in the proposed framework is scal-\nable and could potentially be implemented in telemedicine\napplications where the framework can be run on both low-\nresource devices and as API services.\nIn future directions, the proposed framework could be\ngeneralized to research problems outside of imaging der-\nmatology in which data consists of pictures and associated\nmetadata, such as artwork interpretation and document figure\ncategorization. Furthermore, the next generation of the BEiT\nbackbone (BEiT-v3) uses Multiway Transformers to perform\nmasked ‚Äúlanguage‚Äù modeling on images, texts, and image-\ntext pairs, allowing such a backbone to be used in a variety of\nvision-language downstream tasks, including visual question\nanswering, visual reasoning, and image captioning [16]. Such\ntechnologies could give rise to many innovative, intelligent\nmedical innovations, such as the ability to explain models‚Äô\ndecisions with natural language responses and to retrieve\nhand-written medical notes with natural language queries.\nIn summary, this research‚Äôs key contributions are as fol-\nlows:\n‚Ä¢ We propose a novel framework for skin lesion classi-\nfication, DeepMetaForge, that uses BEiT as the back-\nbone image encoder as well as a novel Deep Metadata\nFusion Module that combines the visual features with\nencoded metadata while compressing the amalgamated\ninformation. Such a novel concept is inspired by an\nintuition that humans comprehend metadata and images\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nsimultaneously while distilling a conclusion rather than\nsequentially.\n‚Ä¢ We empirically evaluate our proposed framework on\nfour public skin lesion image datasets and compare the\nclassification performance with stand-alone backbones\nand state-of-the-art metadata-fusing methods for skin\nlesion classification [12, 17, 18].\n‚Ä¢ We empirically investigate the scalability of the pro-\nposed framework by analyzing the trade-off between\nthe efficacy and efficiency of the models. The insights\nshed light on the implementation aspect when extending\nthe proposed framework in a real-world telemedicine\nsystem.\n‚Ä¢ We present parameter sensitivity analyses that impact\nthe performance of the proposed framework, such as the\nimpacts of different metadata modes, compression ra-\ntios, module components, and backbone configurations.\nThe rest of this article is organized as follows. Section\nII discusses the background of skin lesion classification\nfrom images and relevant approaches to addressing such\na task. Section III explains the proposed DeepMetaForge\nframework, including the network architecture, the Deep\nMetadata Fusion module, the datasets used for validation,\nand the evaluation protocol. Section IV reports highlighted\nexperiment results as well as relevant discussions. Finally,\nSection V concludes the paper.\nII. BACKGROUND AND RELATED WORK\nAutomatic skin lesion classification has been a long-standing\nresearch problem in computational sciences and medicine\n[19]. The ability to automatically recognize cancerous skin\nconditions from images could prove helpful in building\ncomputer-aided systems for dermatologists. Furthermore,\ntelemedicine applications could adopt such technologies to\nenable medical doctors to early diagnose patients who do not\nhave access to hospitals with guided information from arti-\nficial intelligence. Such early-stage detection of malignant\nskin diseases could vastly increase the chances of successful\ntreatment [20]. Various computer vision and machine learn-\ning techniques have been proposed to address skin lesion\nclassification problems. The early techniques focused on\nextracting discriminative characteristics from images, includ-\ning segmentation, lesion border, color, and other texture-\nbased features [21]. These extracted features can then be\nused to train traditional machine-learning models such as\nSupport Vector Machine (SVM) [22], neural networks [23],\nand CART [24].\nDeep learning technologies have evolved in recent years to\nease end-to-end training of machine learning models while\nreducing the need for human expertise to engineer features\n[25]. The availability of public datasets has expedited the\ngrowth of emerging deep-learning techniques by facilitating\nstandard validation benchmarks. In many applications, in-\ncluding skin lesion classification, studies have reported deep\nlearning approaches to perform superior to the traditional ma-\nchine learning models trained with engineered features [26,\n27]. In addition, the use of advanced deep learning techniques\nhas facilitated the analysis and acquisition of knowledge from\nvarious kinds of images, therefore effectively tackling com-\nplex issues, including the segmentation of retinal layers [28]\nand the identification of objects in infrared thermal images\n[29, 30] where immune-based intelligent techniques can be\nused for diagnosis [31] and feature extraction [32] tasks.\nThis section reviews recent deep-learning techniques applied\nto skin lesion classification problems. The first subsection\ndiscusses the techniques that utilize only image information.\nThen, since a novelty of our proposed network architecture\nis the ability to forge metadata onto image embedding, we\nalso discuss relevant studies that utilize patients‚Äô metadata to\nenhance skin lesion classification in the second subsection.\nA. SKIN LESION CLASSIFICATION USING ONLY\nIMAGES\nMalignant skin lesion recognition from images can be framed\nas an image classification problem where conventional off-\nthe-shelf pre-trained image embedding models can be di-\nrectly applied. Jiahao et al. [33] evaluated the applicability of\nVGG-16, ResNet-50, and EfficientNet-B5 on the ISIC 2020\ndataset and found EfficientNet-B5 to yield the best AUC-\nROC. Similarly, Zhang and Wang [34] found DenseNet-\n201 to perform the best when compared with VGG-16 and\nResNet-50 on the ISIC 2020 dataset.\nWhile pre-trained image models could be conveniently\napplied to the skin lesion datasets, the performance gaps still\nexisted that called for the invention of more advanced net-\nwork architectures. Zhang et al. [35] proposed to optimize the\nconvolutional neural networks (CNN) with an improved ver-\nsion of the whale optimization algorithm [36] for skin cancer\ndetection. Dermquest and DermIS were used as the bench-\nmark datasets to compare their proposed method with an\nordinary CNN, VGG-16, LIN, Inception-v3, and ResNet-50.\nLiu et al. [37] proposed incorporating doctors‚Äô perspectives\nwhen diagnosing skin cancer, including zooming, observing,\nand comparing into their proposed clinical-inspired network\n(CI-Net) using ResNet as the backbone. The evaluation was\nconducted on the ISIC 2016 - 2020 and PH2 datasets. Kaur\net al. [38] developed LCNet, a novel end-to-end CNN-based\nframework that incorporates image resizing, oversampling,\nand augmentation specifically designed for melanoma skin\nlesion classification. Their method was tested on ISIC 2016,\nISIC 2017, and ISIC 2020 datasets and was compared with\nconventional image classification backbones such as ResNet-\n18, Inception-v3, and AlexNet. Recently, Reis et al. [39]\ninvented InSiNet, a deep convolutional approach for skin can-\ncer detection and segmentation. They specifically raised the\nconcern that typical deep learning image encoding backbones\ncan be extensive in size and consume high computational\nresources and proposed a new network with few parame-\nters, resulting in a relatively lightweight model capable of\ncropping, segmenting lesions, and removing hair noises from\nthe input images. Their method was evaluated on the ISIC\n2018, 2019, and 2020 datasets, comparing against Inception-\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nv3, DenseNet-201, ResNet-152v2, and EfficientNet-B0.\nWhile deep learning techniques have been proposed to\nsolve the skin lesion classification problem, a significant\ndrawback of such approaches would be the lack of ex-\nplainability when making predictions [40]. In the area of\nautomatic skin cancer diagnosis, many attempts were made\nto explain the decisions from deep learning models. L√≥pez-\nLabraca et al. [41] designed an interpretable system that\nallows CNN neurons to identify visual features and analyzes\nactivation units to extract useful information for decision-\nmaking. Using their proposed system, they found that a small\nsubset of channels was deemed relevant for a dermatologist\ncompared to those of the baseline system. Recently, Rezk\net al. [42] introduced an interpretable skin cancer diagnostic\nmethod that uses a skin lesion taxonomy to gradually acquire\ndermatological knowledge from a modified DNN architec-\nture. These models were trained using clinical photographs\nthat could be readily accessible to non-specialist healthcare\nprofessionals. The empirical investigations showed that the\napplied taxonomy is helpful for enhancing classification\naccuracy, comprehending the reasoning behind illness di-\nagnosis, and identifying diagnostic mistakes. In addition,\nthey used a sophisticated gradient-based class activation map\napproach that illustrates visual explanations when the model\nmakes decisions.\nThe above-mentioned literature has investigated and pro-\nposed methods to improve skin lesion classification from\nimages. Different studies focused on addressing challenges\nposed by the available skin lesion datasets, such as devel-\noping novel network architectures specific to the problem\nat hand, handling data imbalance, improving image quality,\nand enabling the models to provide valuable explanations\nwhen making decisions. However, all such methods only\nrely on skin lesion images as the sole information sources.\nTypically, during clinical dermoscopy, dermatologists also\ncollect additional information about the patients, such as\ngender, age, and behavior (e.g., smoking, drinking, etc.), and\nabout the diagnosed lesions, such as anatomical location,\ncolor, and bleeding. This research aims to investigate whether\nsuch metadata could help to improve the classification per-\nformance when incorporated into the network architecture\nwhile learning the image information. Therefore, the follow-\ning subsection discusses relevant literature that attempts to\nuse patients‚Äô metadata to improve skin lesion classification\nperformance.\nB. METHODS UTILIZING BOTH IMAGES AND METADATA\nOftentimes, images are accompanied by metadata for com-\nplemented information. Fusing different data sources to en-\nhance model performance has long been investigated in com-\nputer vision [43, 44, 45, 46]. While traditional image classi-\nfication models only require images as inputs, several studies\nhave found that incorporating metadata during the training\nprocess could be helpful [47, 48, 49]. Integrating metadata\ninto image classification could be as simple as concatenating\nmetadata to the image features, training a dedicated learner\nwith metadata and combining the probability outputs with\nthose from image classifiers, or fusing metadata into the\nnetwork architecture. Boutell and Luo [50] used a Bayesian\nnetwork to incorporate camera metadata, such as brightness,\nflash, and subject distance, for scene image classification.\nZhu et al. [51] extracted text lines from an image and\ncombined them with the image content. Experimenting with\nan SVM classifier yielded an improvement from 81.3% to\n90.1% in terms of accuracy. Yang et al. [47] enhanced theme\nclassification using images from maps and their metadata\nsuch as name, title, keywords, and abstract. Langenberg et al.\n[48] used traffic lights‚Äô contextual metadata to assign each\ntraffic light to its appropriate lane. Ellen et al. [49] employed\ngeometric, geo-temporal, and hydrographic context metadata\nto improve plankton image classification. Lee et al. [52]\nproposed to combine deep learning models and traditional\nhand-crafted visual metadata features for biomedical image\nmodality classification. Jony et al. [53, 54] fused image\nfeatures and metadata to detect flooding in Flickr images.\nIn classifying skin lesions, as many public datasets contain\npatient information with lesion photos, the literature has\ndeveloped approaches for exploiting such extra metadata to\nenhance classification performance. While a study reported\nthat integrating patients‚Äô metadata, such as age, gender, and\nanatomical site, using a weighted average, concatenation-\nbased, and squeeze-and-excitation (SE) approaches did not\noverall improve the skin lesion classification performance in\na case study of 431 patients [55], some reported otherwise,\nespecially those experimenting their proposed methods on\nlarger datasets. Nunnari et al. [56] proposed concatenating\nprobability from the image classifier with metadata when\ntraining traditional machine learning models. However, if\nneural networks are used as the primary classifier, then\nthey proposed to concatenate one-hot encoded metadata with\nthe image embedding layer. Their method yielded a 19.1%\naccuracy improvement on the ISIC 2019 dataset compared\nto classifiers trained only with image information. They also\nfound that among the metadata attributes, age provided the\nmost discriminative information, followed by body location\nand gender. Gessert et al. [12] proposed to encode metadata\nwith two dense layers with dropout options and ReLU acti-\nvation functions. The encoded metadata is then concatenated\nwith the image embedding from the EfficientNet backbone.\nThe choice of EfficientNet was particularly explored due to\nits ability to scale the model‚Äôs width and depth according\nto the associated input size, leading to better classification\nresults while using fewer parameters compared to other tradi-\ntional image encoding backbones. Their approach was eval-\nuated on the ISIC 2019 dataset, yielding better performance\nthan SENet-154, ResNext WSL, and EfficientNet classifiers\ntrained only with image information. Ningrum et al. [17]\nfocused on developing algorithms for melanoma detection\nfrom dermoscopic images using low-resource devices. Sim-\nilar to Gessert et al. [12]‚Äôs work, they proposed to encode\npatients‚Äô metadata with a layer of the artificial neural net-\nwork before concatenating it to the image embedding from\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\na CNN model. Their method was evaluated on the ISIC\n2019 dataset and was shown to be superior to using the\nCNN model alone. In addition to simply concatenating one-\nhot encoded metadata to image embedding, Li et al. [57]\nproposed to embed metadata with two neural networks using\nReLU and Sigmoid as activation functions, then fused the\nembedded metadata with the image embedding using the\nmultiplication function. While their method was shown to be\nsuperior to the concatenation-based approach, the improve-\nment was marginal on the ISIC 2018 dataset. Furthermore,\ntheir concatenation baseline simply used the one-hot encoded\nmetadata as-is without encoding it first, as done in their\nproposed multiplication-based method.\nAnother scheme of fusing metadata into visual information\nis using metadata to guide the image feature extraction with-\nout changing the dimension of visual feature maps, similar\nto the attention mechanism [58]. Pacheco and Krohling [13]\nproposed MetaBlock that uses an attention mechanism to\nsupervise visual feature extraction. The block first encodes\nthe patients‚Äô metadata and multiplies it with the image em-\nbedding where the hyperbolic tangent gate is used. These\nintermediate parameters are concatenated with another set of\nencoded metadata features, where the sigmoid gate is used\nto produce the final supervised visual features. Their method\nwas evaluated on the ISIC 2019 and PAD-UFES-20 datasets.\nSimilarly, Pundhir et al. [59] proposed a multiplication-\nbased scheme to combine patients‚Äô metadata with encoded\nlesion images. However, their proposed Hyperpametric Meta\nFusion Block multiples image embedding with encoded\nmetadata using Leaky ReLU as the activation function then\nconcatenates the intermediate parameters with another en-\ncoded metadata, after which the Swish [60] activation is used\nto produced supervised features. Their method was evalu-\nated on the PAD-UFES-20 dataset, varying backbones such\nas MobileNet-v2, VGGNet-13, ResNet-50, EfficientNet-B4,\nand DenseNet-121. Cai et al. [61] proposed SLE (Soft Label\nEncoder), where metadata attributes are encoded as soft la-\nbels rather than binary values as done by the one-hot encoder.\nThe soft labels are then used in a mutual attention decoder to\nproduce the final embedding for classification with a fully\nconnected layer. Their proposed method was evaluated on\nthe ISIC 2018 dataset. Recently, Tang et al. [62] presented\nFusionM4Net, a multi-stage multi-modal learning algorithm\nfor multi-label skin disease classification, which consists of\ntwo stages: first, learning feature information from clinical\nand dermoscopy pictures, and second, combining patient\nmetadata and decision information from the two-modality\nimages. Experiments conducted on the Seven-Point Checklist\n(SPC) dataset demonstrated that FusionM4Net was more\naccurate than any other existing state-of-the-art approach. A\nweakness of their approach is its dependence on two image\nsources (i.e., dermoscopic and clinical pictures), which may\nnot be available in other datasets or practical scenarios.\nWhile many studies have proposed various ways to merge\npatients‚Äô metadata into skin lesion classifiers‚Äô learning mech-\nanisms, including concatenation-based, multiplication-based,\nand attention-based methods, the above-mentioned metadata-\nfusion approaches still fall short of analyses in several prac-\ntical aspects. First, most of the proposed algorithms only\nvalidated their methods on one dataset, while generalizability\nmust be evidenced by controlled experiments on multiple\ndata sources with diverse characteristics. Second, it is our\noverarching intention to develop a telemedicine system ac-\ncessible to healthcare practitioners and patients, especially\nthose in suburban areas in developing countries where physi-\ncal access to modern medical equipment is limited and expe-\nrienced dermatologists are scarce. In doing so, the scalability\naspect of the classification models must be explored to de-\ntermine the appropriate deployment options to implement. In\nthis paper, we propose a deep vision-transformer metadata-\nfusing framework for skin lesion classification. The novelty\nof our framework lies in two folds. First, we adopted BEiT\n(Bidirectional Encoder representation from Image Trans-\nformers), which uses a masked image modeling task to pre-\ntrain vision transformers [14]. Second, we propose a novel\nDeep Metadata Fusion Module (DMFM) to merge encoded\nmetadata onto the image embedding during the compres-\nsion, mimicking the process of ‚Äúforging‚Äù decoration onto a\nmetal piece, hence the name DeepMetaForge. Our proposed\nmethod is generalizable as evidenced by the experiments on\nfour publicly available skin lesion datasets, comparing with\nboth stand-alone image encoding backbones and the state-of-\nthe-art metadata-fusing methods proposed by Gessert et al.\n[12] and Ningrum et al. [17]. Furthermore, we analyze the\nscalability of our proposed framework by studying the trade-\noff between classification performance and computational\nresource consumption.\nIII. METHODOLOGY\nThe novelty of our proposed DeepMetaForge framework is\nthe use of BEiT backbone and the metadata forging mech-\nanism. This section first discusses the proposed network\narchitecture in detail. Then, the rest of this section walks\nthrough the experiment setup, including datasets, computa-\ntional environments, and evaluation protocol.\nA. NETWORK ARCHITECTURE\nThis section provides an overview of our network archi-\ntecture, depicted in Fig. 1. Our network features a multi-\nmodal base that can merge feature maps from various inputs.\nThe network structure comprises three main parts: the image\nencoder, the meta encoder, and the combining module. The\nimage encoder utilizes the state-of-the-art vision transformer\nbackbone, BEiT [14], designed to extract high-level visual\nfeatures from images. On the other hand, the metadata en-\ncoder employs a convolutional network structure to extract\nthe feature map from the input data. To combine the feature\nmaps from the image encoder and metadata encoder, we\nuse a merging module called Deep Metadata Fusion Module\n(DMFM). This module effectively combines two feature\nmaps to produce a more comprehensive and informative\nrepresentation of the input. Overall, our network architecture\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nImage Encoder\nMeta Encoder\nDMFM Classifier OutputImage\nMetadata\nAge Sex . Site\n35 M . head\n40 F . neck\nFIGURE 1: Overall DeepMetaForge network architecture.\nis designed to take advantage of the strengths of both the\nimage and metadata while effectively fusing them using a\nnovel deep feature-level merging module.\n1) Image Encoder:\nOur proposed architecture uses the Bidirectional Encoder\nrepresentation from Image Transformers (BEiT) [14] as the\nimage encoder. It has been shown to outperform existing\nsupervised pre-training models in masked image modeling\ntasks. The BEiT image encoder, illustrated in Fig. 2, tok-\nenizes the original image into small visual tokens and ran-\ndomly masks some image patches. These masked patches\nare then fed into the backbone transformer. In summary, the\nBEiT image encoder is a highly effective tool for pre-training\nvision transformers and an excellent choice for our network\narchitecture.\nBEiT Encoder\nMasked Image ModelingHead\n246\nBlockwise Masking\nInput Image\n[S]\n [M] [M] [M] [M] [M]\n210 543 876 11109 12 151413 16\n+ + + + + + + + + + + + + + + + +\nTokenizer\n660 246 948 192\n454 279 868 297\n495 471 659 831\n625 964 411 257\nVisual Token\n‚Ñé2\nùêø ‚Ñé3\nùêø ‚Ñé6\nùêø ‚Ñé7\nùêø ‚Ñé14\nùêø\n948 279 868 964\nImage Patches\nFIGURE 2: Illustration of the BEiT network structure,\nadapted from [14].\n2) Metadata Encoder:\nThe metadata encoder in our architecture is based on a\nconvolutional neural network (CNN), consisting of two sets\nof convolutional layers, each including a one-dimensional\nconvolutional filter, a normalizer, and an activation function.\nThe computation of the metadata encoder is described by\n(1), where œï represents the one-dimensional convolutional\nfilter, Œæ represents the one-dimensional batch normalizer, and\nœÅ represents the Rectified Linear Unit (ReLU) activation\nfunction.\ny = œÅ[Œæ (œï (x))] (1)\nLet y ‚àà R and x ‚àà R denote the feature map from a set of\nconvolutional layers and the input feature map. The metadata\nencoder is one of the critical components of our architecture,\nas it extracts the feature map from the input data. By utilizing\na CNN structure, we can effectively extract relevant features\nfrom the input, which can be integrated with the feature map\ngenerated by the image encoder to create a comprehensive\nrepresentation of the input data. The first CNN converts the\ninput metadata into a feature map with 256 channels. The\nsecond converts the output of the first set into the same\nshape as the output of the image encoder. This allows us to\nmerge the feature map generated by the meta encoder with\nthe feature map generated by the image encoder using our\nproposed DMFM, resulting in a more comprehensive and\ninformative representation of the input data.\n3) Deep Metadata Fusion Module (DMFM):\nThe Deep Metadata Fusion Module (DMFM) is a merging\ncomponent in our architecture that fuses the feature maps\ngenerated by the image encoder and metadata encoder. This\nmodule is adapted from the Fused Module, introduced by\nVachmanus et al. [63], to handle multi-modal visual in-\nformation. However, the Fused module was designed for\ntwo-dimensional feature maps, while DMFM merges one-\ndimensional feature maps by removing the convolution with\nrate branch (cbr). Equation (2) defines the calculation of\nDMFM. Let ÀÜx ‚àà Rc represent the concatenation result of the\nfeature maps from the image encoder and metadata encoder,\nand ÀÜz ‚àà R2c denote the output feature map from DMFM, the\nasterisk symbol ( ‚àó) represents the concatenation operation,\nand z is calculated according to Equation (3).\nÀÜz = z ‚àó ÀÜx (2)\nz = ¬µ2(¬µ1 (ÀÜx)) (3)\nThe function ¬µ(x) is described by Equation (4), where œÜ\nis the dropout operation with a 0.4 probability.\n¬µ(x) = œÜ[œÅ(œï (x))] (4)\nThe result of ¬µ1 is a feature map with Œ≥\nc channels, where\nŒ≥ denotes the compression ratio. The function ¬µ2 converts\nthe feature map back to the input shape of the module.\nAs a result, the feature map shape is roughly four times\nlarger than the input shape. In our architecture, we use a\ncompression ratio of 8. The DMFM plays an essential role\nin our architecture, allowing us to merge the feature maps\ngenerated by the image encoder and meta encoder into a more\ncomprehensive and informative representation of the input\ndata.\nThe DMFM‚Äôs operation is designed to imitate how a skin\nlesion image with metadata is comprehended. Specifically,\none would understand the metadata and the associated skin\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nlesion presented in the image altogether concomitantly while\ndeducing a conclusion about whether it is a cancerous lesion.\nOur concept contradicts the popular concatenation-based\nmethod [12], where the simple concatenation of visual and\nmetadata features are fed to the fully connected layer for\nclassification, hence forcing the classifier to learn the two\ninformation pieces sequentially rather than simultaneously.\n4) Classification Layer:\nThe classification layer of the network is tasked with reduc-\ning the channel of the fused feature map from the DMFM\nto a binary classification based on whether the skin lesion is\nbenign or malignant. This layer comprises a fully connected\nlayer, a one-dimensional batch normalizer, and a Rectified\nLinear Unit (ReLU) activation function. As previously de-\nscribed, the output of this layer is the ultimate output of\nthe whole network structure and is vital for making an\naccurate prediction since it converts the DMFM-generated\nfeature map into a binary classification. By applying a fully\nconnected layer, the feature map‚Äôs dimension is efficiently\ndecreased to provide a reliable prediction of the skin condi-\ntion.\nThe primary objective of the proposed network is to\nenable the detection of malignant skin lesions by binary\nclassification rather than focusing on multiclass classification\nfor more detailed identification of specific forms of skin\ncancer. This design choice is based on the following justi-\nfications. First, our primary objective is to use the model\ninside a telemedicine framework, with the primary purpose\nof screening individuals exhibiting symptoms of skin cancer\nto determine the need for further diagnosis by professional\ndermatologists. Hence, precise identification of malignant\nskin lesions is crucial. Framing the problem as a multiclass\nclassification task, while providing more details about the\npredicted skin conditions, could not only be unnecessary for\nthe objective but also result in poorer classification accuracy\n[64]. Furthermore, the primary contribution of this study is\nin the novel network architecture, which necessitates empir-\nical validation for its suitability in accommodating diverse\ndatasets with various types of skin conditions. Hence, to pro-\nvide an equitable evaluation of the network‚Äôs efficacy across\nvarious datasets, it is essential that the prediction task remains\nconstant, hence enabling a comparative examination of the\ninfluence of input data on the accuracy of the model. Finally,\nframing the problem as a benign-vs-malignant classification\ntask allows a fair comparison with several reputable methods\naddressing the skin lesion classification [12, 17, 18] that also\naddressed the problem using binary classification methods.\nNevertheless, extending the proposed network for multiclass\nclassification is trivial and can be achieved by modifying the\noutput layers as needed.\nB. DATASETS\nWe evaluate our proposed network architecture on four pub-\nlicly available skin lesion datasets: ISIC 2020 [65], PAD-\nUFES-20 [66], SKINL2 [67], and PH2 [68]. The images\nTABLE 1: Statistics of the datasets used in this research.\nDataset Camera\n# Metadata \nNumeric\nAÔøΩributes\n# Metadata \nCategorical\nAÔøΩributes\n# Benign\nSamples\n# Malignant\nSamples\nPH2 Dermoscopy 0 13 203                  76                      \nSKINL2 Dermoscopy 1 3 160                  40                      \nPAD-UFES-20 Smartphone 3 17 405                  1,089                \nISIC 2020 Dermoscopy 1 2 31,954            575                    \nFIGURE 3: Example skin lesion images from each dataset.\nBy column: ISIC 2020, PAD-UFES-20, PH2, and SKINL2.\nBy row: Malignant and Benign.\nin the PAD-UFES-20 dataset were taken with smartphones,\nwhile the others were with dermoscopy cameras. All of these\ndatasets include both images and metadata, which provide\ncomplementary information for skin lesion classification.\nThe dataset contains a wide range of metadata, each tai-\nlored to specific categories. For example, the PH2 dataset\ncontains the Histological Diagnosis, Asymmetry, Pigment\nNetwork, Dots/Globules, Streaks, Regression Areas, Blue-\nWhitish Veil, as well as color characteristics such as White,\nRed, Light-Brown, Dark-Brown, Blue-Gray, and Black. The\nSKINL2 dataset provides information on Gender, Age, Foto-\ntype, and Melanocytic attributes. The PAD-UFES-20 dataset\nincludes data on smoking and drinking habits, age, pesticide\nexposure, gender, skin cancer history, cancer history, access\nto piped water and sewage systems, Fitzpatrick score, region,\nlesion diameter measurements, as well as factors like itchi-\nness, growth, pain, changes, bleeding, elevation, and biopsy\nstatus. Lastly, the ISIC 2020 dataset encompasses informa-\ntion regarding gender, age approximation, anatomical site\nof the lesion, and diagnosis. These comprehensive datasets\noffer valuable insights for a wide range of dermatological\nand epidemiological research. A detailed description of each\ndataset is provided in Table 1. Example images from each\ndataset are depicted in Fig. 3.\nEach dataset is divided into a training set, which comprises\n70% of the data, a 10% validation set, and a testing set, which\ncomprises the remaining 20%. To further enhance the relia-\nbility of our results, we employ 5-fold cross-validation for\nall experiments conducted in this paper. During the training\nphase, we randomly split the training set into eight parts,\nwith one part designated as a validation set for hyperpa-\nrameter tuning. To improve the model‚Äôs performance, we\napply pre-processing techniques such as resizing all images\nto 224√ó224, with some tests conducted using 384, depending\non the pre-trained models‚Äô architectures. Data augmentation\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntechniques such as normalization, random flipping, and ran-\ndom rotation are performed to allow the model to handle a\nrange of diverse inputs.\nBy validating our proposed architecture on these various\ndatasets, we can show its efficacy in accurately classifying\nbenign and malignant skin lesions in a wide range of scenar-\nios. The use of these publicly available datasets is essential\nfor demonstrating the practical generalizability of our pro-\nposed architecture to diverse sources of skin lesion images\n(i.e., smartphones and dermoscopy cameras) and different\ncompositions of metadata details.\nC. COMPUTATIONAL ENVIRONMENTS\nDuring the training process, RGB images are resized to a\nheight and width of 224 to meet the requirements of the\nbackbone. The experiment is conducted on a single Linux\nmachine with NVIDIA Geforce RTX 3090 GPU and Intel\ni7-12700K CPU with 32 GB of memory. The deep learning\nframework Pytorch is utilized to construct and train the\nnetwork model. The network encoder is pre-trained on each\noriginal publication, while the other layers‚Äô parameters are\nrandomly initialized. To re-adjust the weights of the train-\ning loss for each class, a cross-entropy function is used as\nEquation (5), wherem represents the number of classes,p the\npredicted probability observation, and q the binary indicator\n(0 or 1).\nLoss = ‚àí 1\nm\nX\nm\nqlog(p) + (1‚àí q)log(1 ‚àí p) (5)\nMomentum SGD is used as the optimization algorithm,\nwhose parameter is set to 0.9. The initial learning rate\n( rateini) is set to 0.001 with a decay of 0.90 at every five\nepochs, and the learning rate ( lr) is calculated by Equation\n(6), where epochs denotes the global step epochs of training\nand dpe is the decay per epoch.\nlr = rateini √ó decay\nepochs\ndpe (6)\nD. EVALUATION METHODS\nThe network models are evaluated using a variety of met-\nrics, including precision, recall, F1 score, accuracy, MCC,\nsensitivity, specificity, and NPV , to provide a comprehensive\nanalysis of its effectiveness in skin lesion classification. Let\nthe malignant class be referred to as the positive class and\nthe benign class as the negative class. Precision measures\nthe proportion of true positives among all predicted positives,\nwhile recall measures the proportion of true positives among\nall actual positives. The F1 score combines precision and\nrecall to provide a balanced evaluation of the network‚Äôs\nperformance. In this research, we present precision, recall,\nand F1 of both the positive and negative classes. The macro-\naverage F1 is simply the average of F1 scores from both\nclasses, used to represent the overall classification efficacy.\nAccuracy measures the proportion of correct predictions\namong all predictions, while MCC, or Matthews correlation\n0 2 4 6 8 10\nIteration (Epoch)\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy\nTraining\nValidation\nFIGURE 4: Comparison of the accuracy of training and\nvalidation across training iterations (epochs) using the BEiT-\nbase-224 model on the ISIC 2020 dataset.\ncoefficient, considers true and false positives and negatives\nto evaluate the performance of the network. Additionally,\nsensitivity and specificity measure the proportion of true\npositive and true negative predictions, respectively, and NPV ,\nor negative predictive value, measures the proportion of true\nnegative predictions among all negative predictions.\nIV. EXPERIMENT, RESULTS, AND DISCUSSION\nIn this section, we evaluate the effectiveness of the proposed\nnetwork structure in comparison to other existing approaches\nfor skin lesion classification. Our evaluation is conducted on\nmultiple publicly available datasets, as described in Section\nIII-B. In addition to evaluating the overall performance of the\nproposed network, we also assess the construction and opti-\nmization of the DMFM, including the evaluation of individ-\nual components using ablation studies, as well as determining\nthe optimal compression ratio for merging feature maps\ngenerated from different sources. By thoroughly evaluating\nthe proposed approach, we can identify the strengths and\nweaknesses of the network and provide insights for future\nimprovements in skin lesion classification.\nDuring the training process, the primary goal is to identify\nthe optimal training weights, which is achieved by closely\nmonitoring the accuracy of the validation set. To address\npotential overfitting issues, a simultaneous check is kept on\nboth the validation accuracy and the training accuracy. In Fig.\n4, the relationship between accuracy and training iterations\nacross epochs using the BEiT-base-224 model on the ISIC\n2020 dataset is illustrated. Given the dataset‚Äôs substantial\nsize, it can be observed that the training accuracy stabilizes\nshortly after the first epoch. Both the training and validation\naccuracy values consistently remain at approximately 98-\n99%.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nA. THE DMFM OPTIMIZATION\nThe DMFM is primarily determined by the compression\nratio, denoted as Œ≥, which controls the size of the feature\nmap in the module. The feature map is compressed to a\nsmaller shape during the merging process to facilitate the\ncombination of the different feature maps. To determine the\noptimal compression ratio, we varied the value ofŒ≥ according\nto powers of 2 to avoid any remainder during the separation\nprocess. Specifically, we tested compression ratios of 1, 2, 4,\n8, 16, 32, and 64 in our experiments.\nIn this experiment, the performance of the DMFM was\ntested on the ISIC 2020 dataset, which is the largest dataset\nused in this research. The primary objective was to evaluate\nthe DMFM‚Äôs capability to merge feature maps generated\nfrom various sources, including both image and metadata,\neffectively. The evaluation metrics used were precision, re-\ncall, and F1 score for each class. By testing the DMFM\non different datasets while varying the compression ratio,\nwe could determine the optimal configuration for accurately\nclassifying skin lesions.\nTABLE 2: Comparison between different compression ratios\non the ISIC 2020 dataset.\nF1 F1\n1 0.997 0.799 0.898 0.993 0.800 0.770 0.997 0.996\n2 0.995 0.629 0.812 0.991 0.636 0.623 0.998 0.993\n4 0.996 0.666 0.831 0.992 0.667 0.671 0.998 0.994\n8 0.997 0.843 0.920 0.995 0.846 0.850 0.997 0.997\n16 0.996 0.735 0.866 0.992 0.748 0.694 0.997 0.995\n32 0.995 0.559 0.777 0.990 0.602 0.510 0.998 0.991\n64 0.996 0.710 0.853 0.992 0.726 0.638 0.998 0.994\nNPVComp.\nRaÔøΩo\nBenign Malignant Macro\nAvg F1 Accuracy MCC SensiÔøΩvity SpeciÔ¨Åcity\nThe results of the experiment evaluating the performance\nof the DMFM on the ISIC2020 dataset are presented in Table\n2. To ease analyses, the comparison of F1 scores is shown\nin Fig.5. Our primary objective was to determine the opti-\nmal compression ratio to effectively merge the feature maps\ngenerated from different sources, including both images and\nmetadata, and accurately classify skin lesions as benign or\nmalignant. The results show that the best compression ratio\nis 8, which produces the highest macro-average F1 score of\n92.0%, roughly 18.4% higher than the lowest F1 score ob-\ntained in the experiment. In addition to the average F1 score,\nwe also analyzed the precision and recall values for each class\nto evaluate the performance of the DMFM with varying com-\npression ratios. The results show that the compression ratio\nof 8 generates the highest recall value for the malignant class,\nwhich is about 66.9% higher than the lowest recall value\nobtained in the experiment. Moreover, this compression ratio\nprovides a well-balanced performance between precision and\nrecall for both the benign and malignant classes, which is\nimportant in accurately classifying skin lesions. The results\nof this experiment demonstrate that the DMFM with a com-\npression ratio of 8 is the most efficient configuration to merge\nmetadata features into image data features for accurate skin\nlesion classification. By varying the compression ratio and\nevaluating the performance on different datasets, we can\n10 0 10 1 10 2 \nC o m p r e s s i o n R a t i o \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \nF 1 - s c o r e \nB e n i g n \nM a l i g n a n t \nM a c r o a v e r a g e \nFIGURE 5: Comparison of F1 scores from different com-\npression ratios. The blue, orange, and green lines represent\nthe F1 scores of the benign class, malignant class, and macro\naverage between the two classes, respectively.\ndetermine the optimal configuration for the DMFM to effec-\ntively forge feature maps generated from different sources.\nThese findings contribute to the development of an effective\nand accurate system for skin lesion classification, which can\naid in the early detection and treatment of skin cancer.\nB. ABLATION STUDIES ON DMFM\nAblation studies analyze the impact of individual compo-\nnents on a model‚Äôs performance. In DMFM, they can provide\ninsights into the compression branch and skip feature map.\nBy removing one component at a time and analyzing per-\nformance, we can understand their importance and how they\nwork together. This can guide the design of more effective\nmodels for skin lesion classification.\nTable 3 displays the results of our experiment assessing\nthe performance of the DMFM on the ISIC2020 dataset. To\nfurther investigate the impact of individual components on\nthe overall performance of the model, we conducted an abla-\ntion study by removing the compression branch and the skip\nfeature map, one component at a time. We then evaluated the\nperformance on the same dataset to determine the importance\nof each component with respect to skin lesion classification\nperformance. The results indicate that the combination of\nthe compression branch and skip feature map yields the\nhighest F1 score among all combinations. Notably, the pro-\nposed compression branch with the compression ratio ( Œ≥)\ncan improve the efficiency of concatenation by 31.2%. Ad-\nditionally, the precision of the malignant class was improved\nby approximately 1.8 times. These findings demonstrate the\nimpact of both components on achieving accurate skin lesion\nclassification and highlight the effectiveness of the proposed\ncompression branch in improving the DMFM‚Äôs performance.\nAnother crucial aspect in developing the DMFM architec-\nture involves exploring various methods of merging feature\nmaps. To achieve optimal skin lesion classification, another\nexperiment was conducted with different merging operations\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3: Comparison of the classification performance when using DMFM with different module components on the ISIC\n2020 dataset. ‚àÜ F1 denotes the relative performance difference (improvement) of the best-performing method compared to\neach other approach.\nComp.\nBranch Skip F1 F1\nw/o w 0.979 0.423 0.701 31.3% 0.960 0.456 0.751 0.964 0.995\nw w/o 0.991 0.341 0.666 38.1% 0.983 0.341 0.333 0.999 0.984\nw w 0.997 0.843 0.920 - 0.995 0.846 0.850 0.997 0.997\nAccuracy\nModule Components Benign Malignant Macro\nAvg F1 ‚àÜ F1 MCC SensiÔøΩvity SpeciÔ¨Åcity NPV\nTABLE 4: Comparison of the classification performance when using DMFM with different feature map merging methods on the\nISIC 2020 dataset. ‚àÜ F1 denotes the relative performance difference (improvement) of the best-performing method compared\nto each other approach.\nF1 F1\n0.994 0.471 0.732 25.6% 0.988 0.471 0.439 0.998 0.989\n0.994 0.608 0.801 14.9% 0.989 0.610 0.605 0.998 0.991\n0.997 0.843 0.920 - 0.995 0.846 0.850 0.997 0.997\nNPVMerging Components\nSummaÔøΩon\nMulÔøΩplicaÔøΩon\nConcatenaÔøΩon\nAccuracy MCC SensiÔøΩvity SpeciÔ¨ÅcityBenign Malignant Macro\nAvg F1 ‚àÜ F1\nfor ÀÜx in Equation (2), including summation, multiplication,\nand concatenation. These operations play a pivotal role in\nfusing the information from the compression branch and\nthe skip feature map. Through rigorous testing and analysis,\nthe aim is to determine which merging technique yields the\nbest results, ultimately refining the DMFM architecture and\nadvancing the field of skin lesion classification.\nTable 4 presents the experimental results for various merg-\ning operations of DMFM on the ISIC2020 dataset. The\nfindings indicate that employing the concatenation operation\nyields the highest F1 score compared to the other operations.\nThis results in approximately a 25.6% improvement over the\nsummation operation and a 14.9% increase over the multipli-\ncation operation. Consequently, the concatenation operation\nwas the optimal choice for merging with DMFM.\nC. IMAGE ENCODER BACKBONE COMPARISON\nThe proposed network architecture comprises two encoders\nthat extract different feature maps: the metadata encoder\nand the image encoder. The metadata encoder uses a basic\nCNN without pre-trained weights, while the image encoder\nemploys a large network structure that can extract or scope\ninto multiple values in the image. To optimize training time\nand computational resources, the image encoder is initialized\nwith pre-trained weights specific to our task. In this section,\nwe conducted experiments to determine the optimal network\nbackbone for the image encoder by testing three well-known\nnetwork structures: the ResNext [69], EfficientNet [70], and\nBEiT [14] models. Our primary objective was to identify the\nmost suitable model for accurately classifying skin lesions.\nThe results of our experiments, presented in Table 5,\ndemonstrate that the ResNext model achieved an F1 score\nof approximately 85-86%, while EfficientNet ranges from\n50-87% (depending on the model size), and BEiT achieves\nan F1 score of 90-92% on the ISIC 2020 dataset. The BEiT\nmodel outperforms the other models on the malignant class,\nresulting in the highest F1 score. Therefore, we conclude\nthat the BEiT model is the most suitable image-encoding\nbackbone for our proposed architecture. The high F1 score\nobtained using the BEiT model demonstrates its effective-\nness in extracting image features and combining them with\nmetadata to accurately classify skin lesions.\nD. IMPACT OF METADATA INTEGRATION\nMetadata, which includes patient information such as age,\ngender, anatomical location of the lesion, and histopatho-\nlogical information, is an essential factor that can provide\nadditional information about a skin lesion that is not cap-\ntured by image data alone. Incorporating metadata into skin\nlesion classification can be critical in accurately classifying\nskin lesions. For example, age can be a significant factor\nin identifying malignant melanoma, while the location of\nthe lesion can provide insight into its potential malignancy.\nBy combining metadata with image data, we can obtain a\nmore comprehensive understanding of the lesion, leading\nto improved accuracy in classification. In our study, we\naimed to investigate the impact of metadata on skin lesion\nclassification by comparing the performance of our proposed\nmodel with and without metadata. We also tested the model\nusing dummy metadata, where all data slots were set to zero,\nto evaluate the effect of metadata on model efficiency. By\nanalyzing the results across different datasets, we aimed to\ngain insights into the importance of metadata in accurately\nclassifying skin lesions and identify the optimal approach for\nincorporating metadata into the model.\nOur analysis revealed that incorporating metadata im-\nproved classification performance on most datasets for the\naverage F1 score, as presented in Table 6. However, the\ndegree of improvement varied depending on the dataset\nused. For instance, on the ISIC 2020 dataset, the use of\nmetadata improved the performance by 87.3%, while on the\nPAD-UFES-20 dataset, the improvement was only 1.51%.\nFurthermore, we observed that the effect of metadata was\nmore prominent in the efficient detection of malignant skin\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5: Classification performance of the DMFM with different image-encoding backbones on the ISIC 2020 dataset.‚àÜ F1\ndenotes the relative performance difference (improvement) of the best-performing method compared to each other approach.\nF1 F1\nResNext 50 0.996 0.715 0.855 8.66% 0.992 0.731 0.685 0.9972 0.994\nResNext 101 0.995 0.728 0.861 7.89% 0.991 0.735 0.732 0.9955 0.995\nEÔ¨ÉcientNet b0 0.994 0.343 0.669 39.02% 0.987 0.343 0.355 0.9987 0.989\nEÔ¨ÉcientNet b1 0.991 0.084 0.537 72.92% 0.983 0.117 0.050 0.9995 0.983\nEÔ¨ÉcientNet b2 0.993 0.355 0.674 37.91% 0.986 0.397 0.270 0.9991 0.987\nEÔ¨ÉcientNet b3 0.991 0.026 0.509 82.74% 0.982 0.052 0.014 0.9999 0.983\nEÔ¨ÉcientNet b4 0.996 0.747 0.872 6.62% 0.992 0.751 0.706 0.9972 0.995\nEÔ¨ÉcientNet b5 0.995 0.575 0.785 18.41% 0.990 0.585 0.520 0.9982 0.991\nEÔ¨ÉcientNet b6 0.996 0.658 0.827 12.40% 0.992 0.660 0.656 0.9977 0.994\nEÔ¨ÉcientNet b7 0.995 0.561 0.778 19.44% 0.990 0.574 0.496 0.9986 0.991\nBEiT-base-224 0.997 0.843 0.920 0.99% 0.995 0.846 0.850 0.9973 0.997\nBEiT-base-384 0.997 0.861 0.929 - 0.995 0.862 0.927 0.9959 0.999\nBEiT-large-224 0.997 0.833 0.915 1.57% 0.994 0.832 0.873 0.9961 0.998\nBEiT-large-384 0.996 0.796 0.896 3.73% 0.993 0.794 0.816 0.9957 0.997\nMCC SensiÔøΩvity SpeciÔ¨Åcity NPVAccuracyBackbone Benign Malignant Macro\nAvg F1 ‚àÜ F1\nTABLE 6: Classification performance comparison between using only image information with the BEiT backbone (None), our\nproposed network with dummy metadata (Dummy), and our proposed network with actual metadata (Actual). ‚àÜ F1 denotes\nthe relative performance difference (improvement) of the best-performing method compared to each other approach.\nF1 F1\nNone 0.948 0.754 0.851 10.10% 0.915 0.728 0.700 0.969 0.930\nDummy 0.558 0.343 0.451 107.94% 0.545 0.120 0.650 0.519 0.787\nActual 0.975 0.899 0.937 - 0.960 0.879 0.900 0.975 0.976\nNone 0.915 0.749 0.832 4.72% 0.875 0.698 0.753 0.920 0.919\nDummy 0.707 0.360 0.534 63.28% 0.630 0.121 0.382 0.723 0.738\nActual 0.926 0.817 0.871 - 0.896 0.755 0.830 0.920 0.938\nNone 0.605 0.880 0.742 1.51% 0.816 0.502 0.926 0.521 0.726\nDummy 0.338 0.445 0.392 92.31% 0.479 0.012 0.441 0.578 0.288\nActual 0.633 0.874 0.754 - 0.813 0.524 0.889 0.610 0.695\nNone 0.864 0.118 0.491 87.32% 0.765 0.188 0.841 0.764 0.996\nDummy 0.647 0.026 0.337 173.27% 0.534 -0.033 0.344 0.538 0.976\nActual 0.997 0.843 0.920 - 0.995 0.846 0.850 0.997 0.997\nNone 0.833 0.625 0.729 19.39% 0.843 0.529 0.805 0.793 0.893\nDummy 0.563 0.294 0.428 103.29% 0.547 0.055 0.454 0.589 0.697\nActual 0.883 0.858 0.871 - 0.916 0.751 0.867 0.876 0.901\nPH2\n SKINL2 \nPAD-\nUFES-20\nISIC 2020\nAvgerage\nAccuracy MCC SensiÔøΩvity SpeciÔ¨Åcity NPV‚àÜ F1Dataset Metadata Benign Malignant Macro\nAvg F1\nlesions. However, in the case of the PAD-UFES-20 dataset,\nthe improvement from integrating metadata is only 1.51%,\nlower than those of other datasets. This may be due to the\ntypes of metadata that were not related to the diagnosis of the\ndisease. In addition, PAD-UFES-20 is the only dataset whose\nimages were taken by smartphones. Therefore, the reason\nfor the small performance improvement could be due to the\nimage characteristics themselves. Further investigation into\nthe actual causes is needed to find a theoretical explanation\nfor such a phenomenon.\nIn Fig. 6, we used GradCAM [71] to visualize the regions\nof interest (ROIs) of the network for image classification\nwithout metadata. The red areas in the bottom images rep-\nresent the ROIs of the model. When comparing two samples,\nthe network primarily focuses on the edges of the lesions.\nThis visualization shows that using only image input cannot\nproduce precise results, as the network‚Äôs ROIs do not always\ncorrespond to the main information in the image. This can\nlead to misclassification of benign samples.\nWe also tested the model using dummy metadata, where\nall attributes‚Äô values are replaced with zeros, to evaluate the\nimpact of metadata on model efficiency. Our results showed\nthat the use of dummy metadata decreased the classification\nefficiency by about 41.2%, indicating that relevant and mean-\ningful metadata is crucial for improving the performance\nof skin lesion classification. This also indicates the pro-\nposed network cannot tolerate missing metadata. This makes\nsense since the network was trained with actual metadata;\ntherefore, the absence thereof during model evaluation could\nfalsely guide the model‚Äôs prediction.\nIn conclusion, our study evaluated the impact of incorpo-\nrating metadata in skin lesion classification using our pro-\nposed model. By comparing the model‚Äôs performance with\nand without metadata across different datasets, we demon-\nstrated that metadata could improve classification efficiency,\nespecially for malignant detection. However, the effect of\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6: EfficientNet B7 ROI visualization of benign\nsamples using the GradCAM technique: Top Row - Input\nImages, Bottom Row - ROI Visualization, Left Column -\npredicted as benign, Right Column - predicted as malignant.\nmetadata on performance varies depending on the type of\nmetadata and the dataset used. Therefore, it is crucial to\ncarefully select and incorporate relevant metadata into the\nmodel to improve its efficiency in accurately classifying skin\nlesions. Further research is needed to determine the most\nappropriate types of metadata and the optimal approaches for\nincorporating them into skin lesion classification models.\nE. NETWORKS COMPARISON\nIn the skin lesion classification domain, metadata has been\nshown to improve the overall classification performance in\naddition to using the image data alone. Various researchers\nhave proposed new techniques for integrating metadata into\ntheir learning mechanisms and have continued to improve\nupon each other‚Äôs work over time. In this study, we compared\nour proposed technique to three state-of-the-art methods,\nJasil (2023) [18] + DMF, Ningrum (2021) [17], and Gessert\n(2020) [12], which also fuse metadata into their network\narchitectures for skin lesion classification. The image en-\ncoding modules for these three baselines were reproduced\nas reported in their publications. Note that the skin lesion\nclassification method proposed by Jasil and Ulagamuthalvi\n[18] does not inherently integrate image metadata into the\nnetwork, making it difficult to compare with other metadata-\nfusing networks. Therefore, their proposed network was used\nas the image-encoder component in our proposed DMF\nnetwork, hence referred to as Jasil (2023) + DMF . By\ncomparing the performance of these methods, we can gain\ninsights into the strengths and weaknesses of each approach\nand identify areas for further improvement.\nThe results of our comparison study are presented in Table\n7. By comparing the performance of the proposed Deep-\nMetaForge (BEiT + DMF) network and three other published\napproaches (Jasil (2023) [18], Ningrum (2021) [17], and\nFIGURE 7: Prediction Outputs by BEiT with DMFM, with\nProbability Scores.\nGessert (2020) [12]), across all datasets, except for PH2, our\nDeepMetaForge network achieved the highest macro-average\nF1 score. As discussed earlier, the relevance and quality of\nmetadata can have a significant impact on classification per-\nformance. This suggests that DMF is able to leverage meta-\ndata effectively to improve classification accuracy. Therefore,\nwhile the efficiency of only image classification for the\nmalignant class is higher than that of the baseline networks\n(slightly higher than DMF), the DMF network outperformed\nGessert (2020) [12] by 34.29%, Ningrum (2021) [17] by\n75.15%, and Jasil (2023) [18] by approximately 19.89% in\nterms of average F1 across all datasets.\nIt is worth noting that the Ningrum (2021) method per-\nforms relatively inferior to other methods, despite their better\nperformance reported in their publication [17]. One explana-\ntion could be that they only experimented on a subset of 1,200\nimages from the ISIC 2019 dataset with roughly 30% of\npositive samples and whose characteristics may be different\nfrom the whole dataset. Furthermore, our experiment, whose\nresults are reported in Table 7, used the ISIC 2020 dataset,\nwhich is more comprehensive than the 2019 version while\nalso presenting a severe data imbalance problem with only\n1.78% positive samples.\nIn conclusion, our proposed DMF network demonstrates\nthe highest efficiency compared to the other previously pro-\nposed metadata-fusing networks and establishes itself as the\nstate-of-the-art method for image-metadata classification in\nskin lesion classification. Our results suggest that incorporat-\ning metadata into the classification system can improve the\naccuracy of the diagnosis, and the proposed network provides\nan effective approach for integrating metadata with image\ndata for skin lesion classification.\nF. EVALUATION OF VARIOUS NETWORK\nCOMBINATIONS\nWhile the proposed DeepMetaForge network features the\nBEiT backbone, this plug-and-play configuration could be\neasily changed to other image-encoding backbones, such as\nResNext and EfficientNet, for portability. Figure 7 illustrates\nexample prediction outputs. Therefore, this section provides\na comprehensive evaluation of the proposed network archi-\ntecture using different backbones, compared with the state-\nof-the-art Gessert (2020) method [12] whose backbones are\nalso varied for a fair comparison. Furthermore, the perfor-\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 7: Classification performance comparison between different metadata fusing networks (i.e., Ningrum (2021), Gessert\n(2020), Jasil (2023) + DMF and our proposed DeepMetaForge network architecture (BEiT + DMF)) on the selected datasets.‚àÜ\nF1 denotes the relative performance difference (improvement) of the best-performing method compared to each other approach.\nF1 F1\nGessert (2020) 0.924 0.700 0.812 18.02% 0.880 0.635 0.725 0.919 0.993\nNingrum (2021) 0.889 0.000 0.444 115.64% 0.800 0.000 0.000 1.000 0.800\nJasil (2023) + DMF 0.983 0.934 0.958 - 0.973 0.918 0.950 0.979 0.988\nBEiT + DMF 0.975 0.899 0.937 2.27% 0.960 0.879 0.900 0.975 0.976\nGessert (2020) 0.798 0.359 0.579 50.60% 0.706 0.209 0.387 0.826 0.798\nNingrum (2021) 0.848 0.200 0.524 66.36% 0.750 0.133 0.200 0.933 0.783\nJasil (2023) + DMF 0.844 0.617 0.731 19.24% 0.781 0.480 0.632 0.837 0.859\nBEiT + DMF 0.926 0.817 0.871 - 0.896 0.755 0.830 0.920 0.938\nGessert (2020) 0.471 0.651 0.561 34.34% 0.611 0.172 0.601 0.560 0.434\nNingrum (2021) 0.848 0.200 0.524 43.85% 0.750 0.133 0.200 0.933 0.783\nJasil (2023) + DMF 0.613 0.827 0.720 4.66% 0.761 0.451 0.784 0.699 0.546\nBEiT + DMF 0.633 0.874 0.754 - 0.813 0.524 0.889 0.610 0.695\nGessert (2020) 0.956 0.327 0.642 43.44% 0.918 0.407 0.927 0.918 0.999\nNingrum (2021) 0.992 0.000 0.496 85.47% 0.985 0.000 0.000 1.000 0.985\nJasil (2023) + DMF 0.991 0.000 0.496 85.72% 0.982 0.000 0.000 1.000 0.982\nBEiT + DMF 0.997 0.843 0.920 - 0.995 0.846 0.850 0.997 0.997\nGessert (2020) 0.787 0.509 0.648 34.29% 0.779 0.356 0.660 0.806 0.806\nNingrum (2021) 0.894 0.100 0.497 75.15% 0.821 0.067 0.100 0.967 0.838\nJasil (2023) + DMF 0.858 0.595 0.726 19.89% 0.874 0.462 0.591 0.879 0.844\nBEiT + DMF 0.883 0.858 0.871 - 0.916 0.751 0.867 0.876 0.901\nSpeciÔ¨Åcity NPV\nPH2\n SKINL2 \nPAD-\nUFES-20\n‚àÜ F1 Accuracy MCC SensiÔøΩvityDataset Network\nArchitecture\nBenign Malignant\nISIC 2020\nAverage\nMacro\nAvg F1\nmance of selected image-encoding backbones alone is also\nreported for reference. Using different image encoders allows\none to explore the effectiveness of different feature extraction\nmethods in skin lesion classification. By varying the back-\nbone, we can also evaluate the adaptability of each network\narchitecture on different backbones that implement different\narchitectures.\nThe evaluation of the proposed network model against the\nstate-of-the-art approaches and only image classification was\nconducted using a variety of metrics, including F1, accu-\nracy, and MCC. ‚àÜ F1 denotes the performance difference\nrelative to the proposed DeepMetaForge network with the\nBEiT backbone. The results presented in Table 8, and the\ncomparison of F1 score are shown in Fig. 8, show that, in\nmost datasets, the proposed method outperformed the other\nbaselines in many of the evaluation metrics. However, there\nwere some evaluating metrics, such as sensitivity and speci-\nficity, in which the DeepMetaForge network with BEiT im-\nage encoder backbone did not achieve the highest score in the\nSKINL2 and PAD-UFES-20 datasets. It is important to note\nthat these metrics focus only on each class of classification,\nand the difference between the best-performing method and\nthe DMF with BEiT backbone is only marginal. On average,\nthe proposed DMF network with BEiT outperforms the base-\nlines and other configurations in all aspects. Specifically, the\nbest configuration of the proposed network outperforms the\nbest image-encoding backbone and metadata-fusing state-of-\nthe-art methods by 19.39% and 8.49%, respectively.\nThis research provides valuable insights into the effective-\nness of different network models and their ability to classify\nskin lesions accurately, which can have a significant impact\non the development of more efficient and accurate diagnosis\nand treatment methods. Specifically, the experiment results\non the four different datasets support our conjecture that\nfusing the metadata with visual features while compressing\nthe fused information to extract the low-level representation\nis an effective approach to combining information from two\ndifferent sources for skin lesion classification.\nG. SCALABILITY ANALYSIS\nSkin lesion classification models can be used in telemedicine\napplications to help dermatologists make informed decisions\nefficiently. These systems should be accessible via diverse\nplatforms and available to patients and healthcare practition-\ners, especially those in rural areas. Evaluating the scalability\nof the proposed method is crucial to ensure its accessibility,\nusability, and reliability across different settings and scenar-\nios.\nIn the previous part, we conducted an extensive evaluation\nand comparison of several skin lesion classification networks\nto determine the most suitable model for this task. We found\nthat the proposed DeepMetaForge network with the BEiT im-\nage encoder backbone was the most efficient for the selected\ndatasets. However, it is also essential to examine the trade-off\nbetween the models‚Äô efficacy and resource consumption in\ndifferent scenarios to ensure its practical application in real-\nworld solutions. Thus, in this part, we evaluate the proposed\nDeepMetaForge network on the ISIC 2020 dataset while\nvarying different BEiT backbone models with different hy-\nperparameter settings. With a thorough understanding of the\nmodel‚Äôs efficacy-efficiency tradeoff, developers can choose\nthe right model for specific applications.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 8: Classification performance comparison between different metadata fusing methods (i.e., None, Gessert (2020), and\nour proposed DeepMetaForge network) and different backbone image encoders (i.e., ResNext50, EfficientNet-B7, and BEiT)\non the selected datasets. ‚àÜ F1 denotes the relative performance difference (improvement) of the best-performing method\ncompared to each other approach.\nF1 F1\nResNext50 0.929 0.587 0.758 23.58% 0.880 0.586 0.475 0.981 0.885\nEÔ¨ÉcientNet-B7 0.914 0.676 0.795 17.89% 0.865 0.601 0.700 0.906 0.925\nBEiT (base) 0.948 0.754 0.851 10.10% 0.915 0.728 0.700 0.969 0.930\nResNext50 0.962 0.769 0.865 8.30% 0.935 0.755 0.725 0.988 0.940\nEÔ¨ÉcientNet-B7 0.924 0.700 0.812 15.41% 0.880 0.635 0.725 0.919 0.993\nBEiT (base) 0.969 0.871 0.920 1.86% 0.950 0.850 0.875 0.969 0.970\nResNext50 0.963 0.845 0.904 3.68% 0.940 0.810 0.825 0.969 0.957\nEÔ¨ÉcientNet-B7 0.947 0.675 0.811 15.62% 0.910 0.646 0.650 0.975 0.923\nBEiT (base) 0.975 0.899 0.937 - 0.960 0.879 0.900 0.975 0.976\nResNext50 0.908 0.708 0.808 7.87% 0.860 0.642 0.634 0.945 0.875\nEÔ¨ÉcientNet-B7 0.879 0.680 0.780 11.79% 0.824 0.559 0.684 0.877 0.881\nBEiT (base) 0.915 0.749 0.832 4.72% 0.875 0.698 0.753 0.920 0.919\nResNext50 0.842 0.129 0.486 79.35% 0.738 0.100 0.147 0.960 0.760\nEÔ¨ÉcientNet-B7 0.798 0.359 0.579 50.60% 0.706 0.209 0.387 0.826 0.798\nBEiT (base) 0.919 0.800 0.859 1.41% 0.885 0.729 0.843 0.900 0.941\nResNext50 0.882 0.571 0.726 19.96% 0.818 0.512 0.518 0.930 0.847\nEÔ¨ÉcientNet-B7 0.837 0.522 0.679 28.25% 0.767 0.426 0.476 0.875 0.823\nBEiT (base) 0.926 0.817 0.871 - 0.896 0.755 0.830 0.920 0.938\nResNext50 0.507 0.802 0.655 15.13% 0.718 0.313 0.784 0.538 0.483\nEÔ¨ÉcientNet-B7 0.366 0.829 0.597 26.14% 0.731 0.227 0.895 0.289 0.511\nBEiT (base) 0.605 0.880 0.742 1.51% 0.816 0.502 0.926 0.521 0.726\nResNext50 0.349 0.677 0.513 46.86% 0.613 0.145 0.671 0.454 0.392\nEÔ¨ÉcientNet-B7 0.471 0.651 0.561 34.34% 0.611 0.172 0.601 0.560 0.434\nBEiT (base) 0.607 0.851 0.729 3.32% 0.785 0.467 0.848 0.617 0.615\nResNext50 0.577 0.857 0.717 5.06% 0.787 0.439 0.880 0.536 0.629\nEÔ¨ÉcientNet-B7 0.560 0.849 0.704 7.00% 0.775 0.415 0.868 0.526 0.608\nBEiT (base) 0.633 0.874 0.754 - 0.813 0.524 0.889 0.610 0.695\nResNext50 0.849 0.129 0.489 88.39% 0.743 0.191 0.824 0.740 0.996\nEÔ¨ÉcientNet-B7 0.832 0.078 0.455 102.49% 0.716 0.098 0.598 0.718 0.989\nBEiT (base) 0.864 0.118 0.491 87.32% 0.765 0.188 0.841 0.764 0.996\nResNext50 0.951 0.393 0.672 36.91% 0.910 0.451 0.890 0.911 0.997\nEÔ¨ÉcientNet-B7 0.956 0.327 0.642 43.44% 0.918 0.407 0.927 0.918 0.999\nBEiT (base) 0.979 0.423 0.701 31.27% 0.960 0.456 0.751 0.964 0.995\nResNext50 0.996 0.715 0.855 7.60% 0.992 0.731 0.685 0.997 0.994\nEÔ¨ÉcientNet-B7 0.995 0.561 0.778 18.28% 0.990 0.574 0.496 0.999 0.991\nBEiT (base) 0.997 0.843 0.920 - 0.995 0.846 0.850 0.997 0.997\nResNext50 0.798 0.556 0.677 28.54% 0.800 0.433 0.679 0.801 0.810\nEÔ¨ÉcientNet-B7 0.748 0.566 0.657 32.59% 0.784 0.372 0.719 0.698 0.826\nBEiT (base) 0.833 0.625 0.729 19.39% 0.843 0.529 0.805 0.793 0.893\nResNext50 0.776 0.492 0.634 37.29% 0.799 0.363 0.608 0.828 0.772\nEÔ¨ÉcientNet-B7 0.787 0.509 0.648 34.29% 0.779 0.356 0.660 0.806 0.806\nBEiT (base) 0.868 0.736 0.802 8.49% 0.895 0.626 0.829 0.863 0.880\nResNext50 0.854 0.747 0.801 8.73% 0.884 0.623 0.727 0.858 0.857\nEÔ¨ÉcientNet-B7 0.834 0.652 0.743 17.16% 0.860 0.515 0.622 0.844 0.836\nBEiT (base) 0.883 0.858 0.871 - 0.916 0.751 0.867 0.876 0.901\nMalignant Macro\nAvg F1 NPV\nPH2\nNone\nGessert (2020)\nDeepMetaForge\n‚àÜ F1 Accuracy MCC SensiÔøΩvity SpeciÔ¨ÅcityDataset Metadata-Fusing\nMethod Backbone Benign\nSKINL2\nNone\nGessert (2020)\nDeepMetaForge\nAvgerage\nNone\nGessert (2020)\nDeepMetaForge\nPAD-\nUFES-20\nNone\nGessert (2020)\nDeepMetaForge\nISIC 2020\nNone\nGessert (2020)\nDeepMetaForge\nWe also analyzed the effect of the training dataset sizes\non the model efficiency. We varied different training sizes,\ni.e., 10%, 30%, 50%, 70%, 90%, and 100%, and observed\nthe performance on the test set. Fig. 9 plots the F1-score\nof the Malignant class and the macro-average F1-score as\nthe function of training dataset size. The results show that\ntraining data size has a direct impact on performance, which\nbegins to plateau when using over 80% of the training data.\nHowever, using the full dataset size still yields the optimal\nperformance.\nIn this experiment, we compared the performance of differ-\nent image encoder backbones by evaluating their parameter\nsize, model size, training time, memory usage, and predicting\ntime. The results, presented in Table 9, show that the larger\nsize of the image encoder backbone yields a higher number of\nthe model‚Äôs parameters and also affects the model‚Äôs physical\nsize (i.e., storage space on HDD). The large version of BEiT\nis about three times larger than the base one. The training\ntime per iteration with a 16-batch-size of 384-input type took\nlonger than the 224-input type. Additionally, the predicted\nmemory reserve of the GPU for this type of network ranged\nbetween 2-3 GB. Due to the model‚Äôs size, the larger size\nresulted in a longer predicting time, with the larger size being\naround three times longer than the base one, and the 384-\ninput type being around three times longer than the 224-input\ntype. Although the classification performance (macro-avg F1\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nPH2 SKINL2 PAD-UFES-20 ISIC20200.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMacroaverageF1-score\nOnlyImage-ResNext50\nOnlyImage-E/ cientNetB7\nOnlyImage-BEiTbase224\nGessert-ResNext50\nGessert-E/ cientNetB7\nGessert-BEiTbase224\nDMF-ResNext50\nDMF-E/ cientNetB7\nDMF-BEiTbase224\nFIGURE 8: Comparison of macro-avg F1 scores from different network architectures and image encoding backbones on all\ndatasets.\nTABLE 9: Efficacy-efficiency tradeoff of the proposed DeepMetaForge network using different configurations of the BEiT\nbackbones on the ISIC 2020 dataset.\nBackbone Parameter\nSize (M)\nModel\nSize\n(MB)\nTraining\nTime/\nIteraÔøΩon (s)\nTest \nMemory \nUsage (GB)\nAverage \nTesÔøΩng \nTime (ms)\nMacro-\nAvg\nF1\nAccuracy MCC\nBEiT-base-224 9.31 389.0      0.079 2.07 4.1 0.920 0.995 0.846\nBEiT-base-384 9.33 418.1      0.286 2.16 10.13 0.929 0.995 0.862\nBEiT-large-224 31.1 1,265.5   0.257 2.87 10.91 0.915 0.994 0.833\nBEiT-large-384 31.16 1,324.2   0.9 2.9 30.19 0.896 0.993 0.794\n0 20 40 60 80 100 \nD a t a s e t s i z e ( % ) \n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1 \nF 1 - s c o r e \nM a l i g n a n t \nM a c r o a v e r a g e \nFIGURE 9: F1-score vs. dataset size on ISIC 2020 dataset\nusing the proposed model.\nand accuracy) and memory usage of these backbones do not\ndiffer much, we suggest using the BEiT-base-224 backbone\nfor those who seek to adopt the proposed framework in\na resource-limited environment, such as offline smartphone\napplications. Such a base model only consumes roughly 2GB\nof memory to operate, which can be accommodated by many\nmodern smartphones. Once loaded in the memory, this base\nversion also takes 4.1 ms to grade an input sample, which\nshould be fast enough for real-time applications. However,\nif computation resources are not of critical concern, then\nthe BEiT-base-384 version, which can encode larger images,\nhence yielding slightly better performance, is recommended.\nIt is worth noting that the larger versions, such as BEiT-large-\n224 and BEiT-large-384, do not improve the classification\nefficacy but consume roughly 40% more memory to operate.\nTherefore, adopting such overkilling large models for skin\nlesion detection applications is not encouraged. Regardless,\nmore investigation must be done to find ways to tweak these\nlarger models to improve the classification performance.\nH. LIMITATIONS\nAlthough the proposed network architecture has demon-\nstrated encouraging performance on the four chosen datasets\nof skin lesion images, the scope of the presented study pri-\nmarily examined the performance of the proposed network,\nwith limited emphasis on other essential processes involved\nin deploying the model in practical systems. Such procedures\ninclude handling data imbalance, augmenting training data\nto enhance its quality, performing image segmentation to\neliminate background noise, and applying image filtering\nto better certain characteristics of different skin conditions.\nResearchers and professionals interested in adopting the pro-\nposed techniques should thoroughly examine techniques for\ndata preprocessing to further enhance the model‚Äôs perfor-\nmance.\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nV. CONCLUSION AND FUTURE WORK\nIn this experiment, we proposed a novel network architecture,\nDeepMetaForge, for skin lesion classification, incorporating\nimage and metadata information to improve classification\naccuracy. The proposed architecture features BEiT image-\nencoding backbone and the novel Deep Metadata Fusion\nModule (DMFM) that integrates visual and metadata features\nwhile blending them together simultaneously. We evaluated\nthe performance of the DeepMetaForge network, along with\nother state-of-the-art approaches, on four datasets comprising\nskin lesion images taken from both dermoscopy and smart-\nphone cameras. The results demonstrated that the proposed\nnetwork with the BEiT image encoder backbone not only\ngeneralized well to different image sources and metadata\ncompositions but also outperformed other networks in terms\nof F1, accuracy, and MCC, making it a suitable model for\nskin lesion classification when images and their metadata are\navailable. A scalability analysis was conducted to investigate\nhow the required computation resources would impact the\nclassification performance of the proposed approach. This\nwork can be extended by framing the problem as multiclass\nclassification or object detection tasks, which can have sig-\nnificant implications for pre-screening skin lesion problems\nin remote areas. Future work can also focus on adapting the\nnetwork to meet the specific needs of remote communities,\nultimately improving public healthcare in underdeveloped\nand developing countries.\nACKNOWLEDGMENT\nThis research paper is supported by Specific League Funds\nfrom Mahidol University.\nREFERENCES\n[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. So-\nerjomataram, A. Jemal, and F. Bray, ‚ÄúGlobal cancer\nstatistics 2020: Globocan estimates of incidence and\nmortality worldwide for 36 cancers in 185 countries,‚Äù\nCA: a cancer journal for clinicians, vol. 71, no. 3, pp.\n209‚Äì249, 2021.\n[2] A. Stang and K. H. J√∂ckel, ‚ÄúDoes skin cancer screening\nsave lives? a detailed analysis of mortality time trends\nin schleswig-holstein and germany,‚Äù Cancer, vol. 122,\nno. 3, pp. 432‚Äì437, 2016.\n[3] M. Zambrano-Rom√°n, J. R. Padilla-Guti√©rrez, Y . Valle,\nJ. F. Mu√±oz-Valle, and E. Vald√©s-Alvarado, ‚ÄúNon-\nmelanoma skin cancer: A genetic update and future\nperspectives,‚Äù Cancers, vol. 14, no. 10, pp. 1‚Äì20, 2022.\n[4] A. F. Jerant, J. T. Johnson, C. D. Sheridan, and T. J.\nCaffrey, ‚ÄúEarly detection and treatment of skin cancer,‚Äù\nAmerican Family Physician, vol. 62, no. 2, pp. 357‚Äì\n368, 2000.\n[5] L. E. Davis, S. C. Shalin, and A. J. Tackett, ‚ÄúCurrent\nstate of melanoma diagnosis and treatment,‚Äù Cancer\nbiology & therapy, vol. 20, no. 11, pp. 1366‚Äì1379,\n2019.\n[6] B. G. Goldstein and A. O. Goldstein, ‚ÄúDiagnosis and\nmanagement of malignant melanoma,‚Äù American Fam-\nily Physician, vol. 63, no. 7, pp. 1359‚Äì1369, 2001.\n[7] D. J. Brailer, A theory of congestion in general hospi-\ntals. University of Pennsylvania, 1992.\n[8] M. E. Cyr, D. Boucher, S. A. Korona, B. J. Guthrie, and\nJ. C. Benneyan, ‚ÄúA mixed methods analysis of access\nbarriers to dermatology care in a rural state,‚Äù Journal of\nAdvanced Nursing, vol. 77), no. 1, pp. 355‚Äì366, 2020.\n[9] A. Wattanapisit and U. Saengow, ‚ÄúPatients‚Äô perspec-\ntives regarding hospital visits in the universal health\ncoverage system of thailand: a qualitative study,‚Äù Asia\nPacific family medicine, vol. 17, no. 1, pp. 1‚Äì8, 2018.\n[10] P. Jia, F. Wang, and I. M. Xierali, ‚ÄúDifferential effects of\ndistance decay on hospital inpatient visits among sub-\npopulations in florida, usa,‚Äù Environmental monitoring\nand assessment, vol. 191, pp. 1‚Äì16, 2019.\n[11] K. Hauser, A. Kurz, S. Haggenm√ºller, R. C. Maron,\nC. von Kalle, J. S. Utikal, F. Meier, S. Hobelsberger,\nF. F. Gellrich, M. Sergon et al., ‚ÄúExplainable artificial\nintelligence in skin cancer recognition: A systematic\nreview,‚Äù European Journal of Cancer, vol. 167, pp. 54‚Äì\n69, 2022.\n[12] N. Gessert, M. Nielsen, M. Shaikh, R. Werner, and\nA. Schlaefer, ‚ÄúSkin lesion classification using ensem-\nbles of multi-resolution efficientnets with meta data,‚Äù\nMethodsX, vol. 7, p. 100864, 2020.\n[13] A. G. Pacheco and R. A. Krohling, ‚ÄúAn attention-based\nmechanism to combine images and metadata in deep\nlearning models applied to skin cancer classification,‚Äù\nIEEE journal of biomedical and health informatics,\nvol. 25, no. 9, pp. 3554‚Äì3563, 2021.\n[14] H. Bao, L. Dong, S. Piao, and F. Wei, ‚ÄúBeit: Bert\npre-training of image transformers,‚Äù arXiv preprint\narXiv:2106.08254, 2021.\n[15] K. S. Kalyan, A. Rajasekharan, and S. Sangeetha,\n‚ÄúAmmu: a survey of transformer-based biomedical pre-\ntrained language models,‚Äù Journal of biomedical infor-\nmatics, p. 103982, 2021.\n[16] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu,\nK. Aggarwal, O. K. Mohammed, S. Singhal, S. Som\net al., ‚ÄúImage as a foreign language: Beit pretraining\nfor all vision and vision-language tasks,‚Äù arXiv preprint\narXiv:2208.10442, 2022.\n[17] D. N. A. Ningrum, S.-P. Yuan, W.-M. Kung, C.-C. Wu,\nI.-S. Tzeng, C.-Y . Huang, J. Y .-C. Li, and Y .-C. Wang,\n‚ÄúDeep learning classifier with patient‚Äôs metadata of der-\nmoscopic images in malignant melanoma detection,‚Äù\nJournal of Multidisciplinary Healthcare, pp. 877‚Äì885,\n2021.\n[18] S. P. G. Jasil and V . Ulagamuthalvi, ‚ÄúA hybrid CNN\narchitecture for skin lesion classification using deep\nlearning,‚Äù Soft Computing, Mar. 2023. [Online]. Avail-\nable: https://doi.org/10.1007/s00500-023-08035-w\n[19] O. Mehta, Z. Liao, M. Jenkinson, G. Carneiro, and\nJ. Verjans, ‚ÄúMachine learning in medical imaging‚Äì\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nclinical applications and challenges in computer vi-\nsion,‚Äù Artificial Intelligence in Medicine: Applications,\nLimitations and Future Directions, pp. 79‚Äì99, 2022.\n[20] D. C. Ara√∫jo, A. A. Veloso, R. S. de Oliveira Filho, M.-\nN. Giraud, L. J. Raniero, L. M. Ferreira, and R. A. Bitar,\n‚ÄúFinding reduced raman spectroscopy fingerprint of\nskin samples for melanoma diagnosis through machine\nlearning,‚Äù Artificial Intelligence in Medicine, vol. 120,\np. 102161, 2021.\n[21] I. Maglogiannis and C. N. Doukas, ‚ÄúOverview of ad-\nvanced computer vision systems for skin lesions charac-\nterization,‚Äù IEEE transactions on information technol-\nogy in biomedicine, vol. 13, no. 5, pp. 721‚Äì733, 2009.\n[22] M. d‚ÄôAmico, M. Ferri, and I. Stanganelli, ‚ÄúQualita-\ntive asymmetry measure for melanoma detection,‚Äù in\n2004 2nd IEEE International Symposium on Biomedi-\ncal Imaging: Nano to Macro (IEEE Cat No. 04EX821).\nIEEE, 2004, pp. 1155‚Äì1158.\n[23] S. E. Umbaugh, R. H. Moss, and W. V . Stoecker,\n‚ÄúApplying artificial intelligence to the identification of\nvariegated coloring in skin tumors,‚Äù IEEE engineering\nin medicine and biology magazine, vol. 10, no. 4, pp.\n57‚Äì62, 1991.\n[24] M. Wiltgen, A. Gerger, and J. Smolle, ‚ÄúTissue\ncounter analysis of benign common nevi and malignant\nmelanoma,‚Äù International journal of medical informat-\nics, vol. 69, no. 1, pp. 17‚Äì28, 2003.\n[25] A. Iosifidis and A. Tefas, ‚ÄúDeep learning for visual\ncontent analysis,‚Äù p. 115806, 2020.\n[26] T.-C. Pham, C.-M. Luong, M. Visani, and V .-D. Hoang,\n‚ÄúDeep cnn and data augmentation for skin lesion classi-\nfication,‚Äù in Intelligent Information and Database Sys-\ntems: 10th Asian Conference, ACIIDS 2018, Dong Hoi\nCity, Vietnam, March 19-21, 2018, Proceedings, Part II\n10. Springer, 2018, pp. 573‚Äì582.\n[27] H. K. Gajera, D. R. Nayak, and M. A. Zaveri, ‚ÄúA\ncomprehensive analysis of dermoscopy images for\nmelanoma detection via deep cnn features,‚Äù Biomedical\nSignal Processing and Control, vol. 79, p. 104186,\n2023.\n[28] L. Ngo, J. Cha, and J.-H. Han, ‚ÄúDeep neural network\nregression for automated retinal layer segmentation in\noptical coherence tomography images,‚Äù IEEE transac-\ntions on image processing, vol. 29, pp. 303‚Äì312, 2019.\n[29] X. Yu, Z. Zhou, Q. Gao, D. Li, and K. R√≠ha, ‚ÄúIn-\nfrared image segmentation using growing immune field\nand clone threshold,‚Äù Infrared physics & technology,\nvol. 88, pp. 184‚Äì193, 2018.\n[30] X. Yu and X. Tian, ‚ÄúA fault detection algorithm for\npipeline insulation layer based on immune neural net-\nwork,‚Äù International Journal of Pressure Vessels and\nPiping, vol. 196, p. 104611, 2022.\n[31] X. Yu, Y . Lu, and Q. Gao, ‚ÄúPipeline image diagnosis\nalgorithm based on neural immune ensemble learning,‚Äù\nInternational Journal of Pressure Vessels and Piping,\nvol. 189, p. 104249, 2021.\n[32] Z. Zhou, B. Zhang, and X. Yu, ‚ÄúImmune coordination\ndeep network for hand heat trace extraction,‚Äù Infrared\nPhysics & Technology, vol. 127, p. 104400, 2022.\n[33] W. Jiahao, J. Xingguang, W. Yuan, Z. Luo, and Z. Yu,\n‚ÄúDeep neural network for melanoma classification in\ndermoscopic images,‚Äù in 2021 IEEE International Con-\nference on Consumer Electronics and Computer Engi-\nneering (ICCECE), 2021, pp. 666‚Äì669.\n[34] Y . Zhang and C. Wang, ‚ÄúSiim-isic melanoma classifi-\ncation with densenet,‚Äù in 2021 IEEE 2nd International\nConference on Big Data, Artificial Intelligence and\nInternet of Things Engineering (ICBAIE). IEEE, 2021,\npp. 14‚Äì17.\n[35] N. Zhang, Y .-X. Cai, Y .-Y . Wang, Y .-T. Tian, X.-L.\nWang, and B. Badami, ‚ÄúSkin cancer diagnosis based\non optimized convolutional neural network,‚Äù Artificial\nintelligence in medicine, vol. 102, p. 101756, 2020.\n[36] S. Mirjalili and A. Lewis, ‚ÄúThe whale optimization\nalgorithm,‚Äù Advances in engineering software, vol. 95,\npp. 51‚Äì67, 2016.\n[37] Z. Liu, R. Xiong, and T. Jiang, ‚ÄúCi-net: clinical-inspired\nnetwork for automated skin lesion recognition,‚Äù IEEE\nTransactions on Medical Imaging, 2022.\n[38] R. Kaur, H. GholamHosseini, R. Sinha, and M. Lind√©n,\n‚ÄúMelanoma classification using a novel deep convolu-\ntional neural network with dermoscopic images,‚Äù Sen-\nsors, vol. 22, no. 3, p. 1134, 2022.\n[39] H. C. Reis, V . Turk, K. Khoshelham, and S. Kaya,\n‚ÄúInsinet: a deep convolutional approach to skin cancer\ndetection and segmentation,‚Äù Medical & Biological En-\ngineering & Computing, pp. 1‚Äì20, 2022.\n[40] P. Messina, P. Pino, D. Parra, A. Soto, C. Besa, S. Uribe,\nM. And√≠a, C. Tejos, C. Prieto, and D. Capurro, ‚ÄúA\nsurvey on deep learning and explainability for auto-\nmatic report generation from medical images,‚Äù ACM\nComputing Surveys (CSUR), vol. 54, no. 10s, pp. 1‚Äì40,\n2022.\n[41] J. L√≥pez-Labraca, I. Gonz√°lez-D√≠az, F. D√≠az-de Mar√≠a,\nand A. Fueyo-Casado, ‚ÄúAn interpretable cnn-based cad\nsystem for skin lesion diagnosis,‚Äù Artificial Intelligence\nin Medicine, vol. 132, p. 102370, 2022.\n[42] E. Rezk, M. Eltorki, and W. El-Dakhakhni, ‚ÄúInter-\npretable skin cancer classification based on incremen-\ntal domain knowledge learning,‚Äù Journal of Healthcare\nInformatics Research, pp. 1‚Äì25, 2023.\n[43] M. B. Alatise and G. P. Hancke, ‚ÄúA review on chal-\nlenges of autonomous mobile robot and sensor fusion\nmethods,‚Äù IEEE Access, vol. 8, pp. 39 830‚Äì39 846,\n2020.\n[44] H. Dhayne, R. Haque, R. Kilany, and Y . Taher, ‚ÄúIn\nsearch of big medical data integration solutions-a com-\nprehensive survey,‚Äù IEEE Access, vol. 7, pp. 91 265‚Äì\n91 290, 2019.\n[45] L. Leng, M. Li, C. Kim, and X. Bi, ‚ÄúDual-source\ndiscrimination power analysis for multi-instance con-\ntactless palmprint recognition,‚Äù Multimedia tools and\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\napplications, vol. 76, pp. 333‚Äì354, 2017.\n[46] L. Leng and J. Zhang, ‚ÄúPalmhash code vs. palmphasor\ncode,‚Äù Neurocomputing, vol. 108, pp. 1‚Äì12, 2013.\n[47] Z. Yang, Z. Gui, H. Wu, and W. Li, ‚ÄúA latent feature-\nbased multimodality fusion method for theme classifi-\ncation on web map service,‚Äù IEEE Access, vol. 8, pp.\n25 299‚Äì25 309, 2019.\n[48] T. Langenberg, T. L√ºddecke, and F. W√∂rg√∂tter, ‚ÄúDeep\nmetadata fusion for traffic light to lane assignment,‚Äù\nIEEE Robotics and Automation Letters, vol. 4, no. 2,\npp. 973‚Äì980, 2019.\n[49] J. S. Ellen, C. A. Graff, and M. D. Ohman, ‚ÄúImproving\nplankton image classification using context metadata,‚Äù\nLimnology and Oceanography: Methods, vol. 17, no. 8,\npp. 439‚Äì461, 2019.\n[50] M. Boutell and J. Luo, ‚ÄúPhoto classification by integrat-\ning image content and camera metadata,‚Äù in Proceed-\nings of the 17th International Conference on Pattern\nRecognition, 2004. ICPR 2004., vol. 4. IEEE, 2004,\npp. 901‚Äì904.\n[51] Q. Zhu, M.-C. Yeh, and K.-T. Cheng, ‚ÄúMultimodal\nfusion using learned text concepts for image catego-\nrization,‚Äù in Proceedings of the 14th ACM international\nconference on Multimedia, 2006, pp. 211‚Äì220.\n[52] S. L. Lee, M. R. Zare, and H. Muller, ‚ÄúLate fusion\nof deep learning and handcrafted visual features for\nbiomedical image modality classification,‚Äù IET image\nprocessing, vol. 13, no. 2, pp. 382‚Äì391, 2019.\n[53] R. I. Jony, A. Woodley, and D. Perrin, ‚ÄúFlood detection\nin social media images using visual features and meta-\ndata,‚Äù in 2019 Digital Image Computing: Techniques\nand Applications (DICTA). IEEE, 2019, pp. 1‚Äì8.\n[54] ‚Äî‚Äî, ‚ÄúFusing visual features and metadata to detect\nflooding in flickr images,‚Äù in 2020 Digital Image Com-\nputing: Techniques and Applications (DICTA). IEEE,\n2020, pp. 1‚Äì8.\n[55] J. H√∂hn, E. Krieghoff-Henning, T. B. Jutzi, C. von\nKalle, J. S. Utikal, F. Meier, F. F. Gellrich, S. Hobels-\nberger, A. Hauschild, J. G. Schlager et al., ‚ÄúCombining\ncnn-based histologic whole slide image analysis and\npatient data to improve skin cancer classification,‚Äù Eu-\nropean Journal of Cancer, vol. 149, pp. 94‚Äì101, 2021.\n[56] F. Nunnari, C. Bhuvaneshwara, A. O. Ezema, and\nD. Sonntag, ‚ÄúA study on the fusion of pixels and patient\nmetadata in cnn-based classification of skin lesion im-\nages,‚Äù in Machine Learning and Knowledge Extraction:\n4th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 Inter-\nnational Cross-Domain Conference, CD-MAKE 2020,\nDublin, Ireland, August 25‚Äì28, 2020, Proceedings 4.\nSpringer, 2020, pp. 191‚Äì208.\n[57] W. Li, J. Zhuang, R. Wang, J. Zhang, and W.-S. Zheng,\n‚ÄúFusing metadata and dermoscopy images for skin\ndisease diagnosis,‚Äù in 2020 IEEE 17th international\nsymposium on biomedical imaging (ISBI). IEEE,\n2020, pp. 1996‚Äì2000.\n[58] M.-H. Guo, T.-X. Xu, J.-J. Liu, Z.-N. Liu, P.-T. Jiang,\nT.-J. Mu, S.-H. Zhang, R. R. Martin, M.-M. Cheng, and\nS.-M. Hu, ‚ÄúAttention mechanisms in computer vision:\nA survey,‚Äù Computational Visual Media, vol. 8, no. 3,\npp. 331‚Äì368, 2022.\n[59] A. Pundhir, S. Dadhich, A. Agarwal, and B. Raman,\n‚ÄúTowards improved skin lesion classification using\nmetadata supervision,‚Äù in 2022 26th International Con-\nference on Pattern Recognition (ICPR). IEEE, 2022,\npp. 4313‚Äì4320.\n[60] P. Ramachandran, B. Zoph, and Q. V . Le, ‚ÄúSearching\nfor activation functions,‚Äù CoRR, vol. abs/1710.05941,\n2017. [Online]. Available: http://arxiv.org/abs/1710.\n05941\n[61] G. Cai, Y . Zhu, Y . Wu, X. Jiang, J. Ye, and D. Yang,\n‚ÄúA multimodal transformer to fuse images and metadata\nfor skin disease classification,‚Äù The Visual Computer,\npp. 1‚Äì13, 2022.\n[62] P. Tang, X. Yan, Y . Nan, S. Xiang, S. Krammer, and\nT. Lasser, ‚ÄúFusionm4net: A multi-stage multi-modal\nlearning algorithm for multi-label skin lesion classifi-\ncation,‚Äù Medical Image Analysis, vol. 76, p. 102307,\n2022.\n[63] S. Vachmanus, A. A. Ravankar, T. Emaru, and\nY . Kobayashi, ‚ÄúMulti-modal sensor fusion-based se-\nmantic segmentation for snow driving scenarios,‚Äù IEEE\nSensors Journal, vol. 21, no. 15, pp. 16 839‚Äì16 851,\n2021.\n[64] T. J. D. Berstad, M. Riegler, H. Espeland, T. de Lange,\nP. H. Smedsrud, K. Pogorelov, H. K. Stensland, and\nP. Halvorsen, ‚ÄúTradeoffs using binary and multiclass\nneural network classification for medical multidisease\ndetection,‚Äù in 2018 IEEE International Symposium on\nMultimedia (ISM). IEEE, 2018, pp. 1‚Äì8.\n[65] V . Rotemberg, N. Kurtansky, B. Betz-Stablein, L. Caf-\nfery, E. Chousakos, N. Codella, M. Combalia, S. Dusza,\nP. Guitera, D. Gutman et al., ‚ÄúA patient-centric dataset\nof images and metadata for identifying melanomas us-\ning clinical context,‚Äù Scientific data, vol. 8, no. 1, p. 34,\n2021.\n[66] A. G. Pacheco, G. R. Lima, A. S. Salomao, B. Krohling,\nI. P. Biral, G. G. de Angelo, F. C. Alves Jr, J. G.\nEsgario, A. C. Simora, P. B. Castro et al., ‚ÄúPad-ufes-\n20: A skin lesion dataset composed of patient data and\nclinical images collected from smartphones,‚Äù Data in\nbrief, vol. 32, p. 106221, 2020.\n[67] S. M. de Faria, J. N. Filipe, P. M. Pereira, L. M. Tavora,\nP. A. Assuncao, M. O. Santos, R. Fonseca-Pinto, F. San-\ntiago, V . Dominguez, and M. Henrique, ‚ÄúLight field\nimage dataset of skin lesions,‚Äù in 2019 41st Annual\nInternational Conference of the IEEE Engineering in\nMedicine and Biology Society (EMBC). IEEE, 2019,\npp. 3905‚Äì3908.\n[68] T. Mendon√ßa, P. M. Ferreira, J. S. Marques, A. R.\nMarcal, and J. Rozeira, ‚ÄúPh 2-a dermoscopic image\ndatabase for research and benchmarking,‚Äù in 2013 35th\nannual international conference of the IEEE engineer-\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nS. Vachmanus et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ning in medicine and biology society (EMBC). IEEE,\n2013, pp. 5437‚Äì5440.\n[69] S. Xie, R. Girshick, P. Doll√°r, Z. Tu, and K. He,\n‚ÄúAggregated residual transformations for deep neural\nnetworks,‚Äù in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017, pp.\n1492‚Äì1500.\n[70] M. Tan and Q. Le, ‚ÄúEfficientnet: Rethinking model scal-\ning for convolutional neural networks,‚Äù in International\nconference on machine learning. PMLR, 2019, pp.\n6105‚Äì6114.\n[71] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell,\nD. Parikh, and D. Batra, ‚ÄúGrad-cam: Why did\nyou say that? visual explanations from deep\nnetworks via gradient-based localization,‚Äù CoRR,\nvol. abs/1610.02391, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1610.02391\nSIRAWICH VACHMANUS received the M.E.\nand Ph.D. degrees from Hokkaido University,\nJapan, in 2019 and 2022, respectively. He is cur-\nrently a lecturer at the Faculty of Information and\nCommunication Technology, Mahidol University,\nThailand. His research interests include computer\nvision, deep learning, machine learning, sensor\nfusion, and artificial intelligence for robots.\nTHANAPON NORASET is a faculty member at\nthe Faculty of Information and Communication\nTechnology, Mahidol University, Thailand. He re-\nceived his BSc degree from the same faculty in\n2007. He received his Ph.D. degree in Computer\nScience from Northwestern University, USA, in\n2017. His research work and interests are in the\nfield of natural language processing and machine\nlearning.\nWARITSARA PIYANONPONG received the\nM.D. degree from the Faculty of Medicine, Tham-\nmasat University. She then obtained a diploma in\nfamily medicine from Trang Hospital. Currently,\nshe is a dermatology resident at the Faculty of\nMedicine, Ramathibodi Hospital, Mahidol Univer-\nsity, Thailand. She has a broad interest in derma-\ntology research, which also includes the role of\nartificial intelligence in dermatology.\nTEERAPONG RATTANANUKROM received\nthe M.D. degree from Khon Kaen University Med-\nical School, the M.Sc. degree in Dermatology\nfrom the University of Hertfordshire, UK, and the\nDiploma of The Thai Board of Dermatology from\nthe Faculty of Medicine, Ramathibodi Hospital,\nMahidol University. Currently, he is a clinical\ninstructor at the Division of Dermatology, Depart-\nment of Medicine, Faculty of Medicine, Ramath-\nibodi Hospital, Mahidol University, Thailand. His\nresearch interests include the treatment of cutaneous lymphoma, melanoma,\nand non-melanoma skin cancers, as well as surgical techniques in Mohs\nsurgery.\nSUPPAWONG TUAROB received his PhD in\ncomputer science and engineering and MS in in-\ndustrial engineering both from the Pennsylvania\nState University and his BSE and MSE both in\ncomputer science and engineering from the Uni-\nversity of Michigan-Ann Arbor. Currently, he is\nan Associate Professor of Computer Science at\nthe Faculty of Information and Communication\nTechnology, Mahidol University, Thailand. His re-\nsearch involves data mining in large-scale schol-\narly, social media, and healthcare domains, as well as applications of\nintelligent technologies for social good.\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3345225\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Metadata",
  "concepts": [
    {
      "name": "Metadata",
      "score": 0.862403154373169
    },
    {
      "name": "Computer science",
      "score": 0.8029863238334656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6057651042938232
    },
    {
      "name": "Deep learning",
      "score": 0.5541622638702393
    },
    {
      "name": "Machine learning",
      "score": 0.42890581488609314
    },
    {
      "name": "Scalability",
      "score": 0.4276782274246216
    },
    {
      "name": "Skin cancer",
      "score": 0.4256798326969147
    },
    {
      "name": "Computer vision",
      "score": 0.3600575923919678
    },
    {
      "name": "Cancer",
      "score": 0.1921614110469818
    },
    {
      "name": "Database",
      "score": 0.13443875312805176
    },
    {
      "name": "Medicine",
      "score": 0.12330344319343567
    },
    {
      "name": "World Wide Web",
      "score": 0.09845888614654541
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ]
}