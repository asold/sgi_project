{
  "title": "TFPose: Direct Human Pose Estimation with Transformers",
  "url": "https://openalex.org/W3158818292",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2368493484",
      "name": "Mao, Weian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3186157715",
      "name": "Ge, Yongtao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360656043",
      "name": "Shen, Chunhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079118816",
      "name": "Tian, Zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A780421308",
      "name": "Wang Xinlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1902389368",
      "name": "Wang Zhi-Bin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W1537698211",
    "https://openalex.org/W3116978317",
    "https://openalex.org/W2949359214",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2113325037",
    "https://openalex.org/W2950735449",
    "https://openalex.org/W3112160422",
    "https://openalex.org/W3108516375",
    "https://openalex.org/W2080873731",
    "https://openalex.org/W2768872714",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3010812598",
    "https://openalex.org/W2952819818",
    "https://openalex.org/W2612947062",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2143487029",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W2988211500",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2591767619",
    "https://openalex.org/W2952422028",
    "https://openalex.org/W2784710428",
    "https://openalex.org/W2769331938",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W2907137919",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2889332130",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W2983035142",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2953258117",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2907715846"
  ],
  "abstract": "We propose a human pose estimation framework that solves the task in the regression-based fashion. Unlike previous regression-based methods, which often fall behind those state-of-the-art methods, we formulate the pose estimation task into a sequence prediction problem that can effectively be solved by transformers. Our framework is simple and direct, bypassing the drawbacks of the heatmap-based pose estimation. Moreover, with the attention mechanism in transformers, our proposed framework is able to adaptively attend to the features most relevant to the target keypoints, which largely overcomes the feature misalignment issue of previous regression-based methods and considerably improves the performance. Importantly, our framework can inherently take advantages of the structured relationship between keypoints. Experiments on the MS-COCO and MPII datasets demonstrate that our method can significantly improve the state-of-the-art of regression-based pose estimation and perform comparably with the best heatmap-based pose estimation methods.",
  "full_text": "TFPose: Direct Human Pose Estimation with Transformers\nWeian Mao†∗, Yongtao Ge †∗, Chunhua Shen †*, Zhi Tian †, Xinlong Wang †, Zhibin Wang ‡\n†The University of Adelaide ‡Alibaba Group\nAbstract\nWe propose a human pose estimation framework that\nsolves the task in the regression-based fashion. Unlike pre-\nvious regression-based methods, which often fall behind\nthose state-of-the-art methods, we formulate the pose es-\ntimation task into a sequence prediction problem that can\neffectively be solved by transformers. Our framework is\nsimple and direct, bypassing the drawbacks of the heatmap-\nbased pose estimation. Moreover, with the attention mech-\nanism in transformers, our proposed framework is able to\nadaptively attend to the features most relevant to the tar-\nget keypoints, which largely overcomes the feature mis-\nalignment issue of previous regression-based methods and\nconsiderably improves the performance. Importantly, our\nframework can inherently take advantages of the structured\nrelationship between keypoints. Experiments on the MS-\nCOCO and MPII datasets demonstrate that our method\ncan signiﬁcantly improve the state-of-the-art of regression-\nbased pose estimation and perform comparably with the\nbest heatmap-based pose estimation methods.\nCode is available at: https://git.io/AdelaiDet\n1. Introduction\nHuman pose estimation requires the computer to obtain\nthe human keypoints of interest in an input image and plays\nan important role in many computer vision tasks such as\nhuman behavior understanding.\nExisting mainstream methods solving the task can be\ngenerally categorized into heatmap-based (Figure 1 top) and\nregression-based methods (Figure. 1 bottom). Heatmap-\nbased methods often ﬁrst predict a heatmap or a classiﬁca-\ntion score map with fully convolutional networks (FCNs),\nand then the body joints are located by the peak’s loca-\ntions in the heatmap or the score map. Most pose es-\ntimation methods are heatmap-based because it has rela-\ntively higher accuracy. However, the heatmap-based meth-\nods may suffer the following issues. 1) A post-processing\n(e.g., the “taking-maximum” operation) is needed. The\n*First two authors contributed equally. CS is the corresponding author.\n……\n(X1,Y1)\n(X2,Y2)\n(XK-1,YK-1)\n(XK,YK)\n……\n(X1,Y1)\n(X2,Y2)\n(XK-1,YK-1)\n(XK,YK)\n(a) Heatmap-based method\n……\n(X1,Y1)\n(X2,Y2)\n(XK-1,YK-1)\n(XK,YK)\n……\n(X1,Y1)\n(X2,Y2)\n(XK-1,YK-1)\n(XK,YK)\n(b) Regression-based method\nFigure 1 – Comparison of mainstream pose estimation\npipelines. (a) Heatmap-based methods. (b) Regression-based\nmethods.\npost-processing might not be differentiable, making the\nframework not end-to-end trainable. 2) The resolution of\nheatmaps predicted by the FCNs is usually lower than the\nresolution of the input image. The reduced resolution re-\nsults in a quantization error and limits the precision of the\nkeypoint’s localization. This quantization error might be\nsolved by shifting the output coordinates according to the\nvalue of the pixels near the peak, but it makes the frame-\nwork much more complicated and introduces more hyper-\nparameters. 3) The ground truth heatmaps need to be man-\nually designed and heuristically tuned, which might cause\nmany noises and ambiguities contained in the ground-truth\nmaps, as show in [21, 31, 41].\nIn contrast, the regression-based methods usually di-\nrectly map the input image to the coordinates of body joints\nwith a FC (fully-connected) prediction layer, eliminating\nthe need for heatmaps. The pipeline of regression-based\nmethods is much more straightforward than heatmap-based\nmethods as in principle pose estimation is a kind of regres-\nsion tasks such as object detection. Moreover, the regssion-\nbased method can bypass the aforementioned drawbacks of\nheatmap-based methods, thus being more promising.\narXiv:2103.15320v1  [cs.CV]  29 Mar 2021\nHowever, there are only a few research works focus-\ning on regression-based methods because regression-based\nmethods often have inferior performance to heatmap-based\nmethods. The reasons may be four-fold. First, in order to\nreduce the network parameters in the FC layer, in the Deep-\nPose [46], a global average pooling is applied to reduce the\nfeature map resolution before the FC layer. This global av-\nerage pooling destroys the spatial structure of the convo-\nlutional feature maps, and signiﬁcantly deteriorates the per-\nformance. Next, as shown in DirectPose [44] and SPM [35],\nin regression-based methods, the convolutional features and\npredictions are misaligned, which results in low localiza-\ntion precsion of the keypoints. Moreover, regression-based\nmethods only regress the coordinates of body joints and\ndoes not take account of the structured dependency between\nthese keypoints [41].\nRecently, we have witnessed the rise of vision transform-\ners [6, 15, 54]. The transformers are originally designed for\nthe sequence-to-sequence tasks, which inspires us to for-\nmulate the single person pose estimation to the problem\nof predicting K-length sequential coordinates, where K is\nthe number of body joints for one person. This leads to a\nsimple and novel regression-based pose estimation frame-\nwork, termed TFPose ( i.e., Transformer-based Pose Esti-\nmation). As shown in Figure 2, taking as inputs the fea-\nture maps of CNNs, the transformer sequentially predict K\ncoordinates. TFPose can bypass the aforementioned difﬁ-\nculties of regression-based methods. First, it does not need\nthe global average pooling as in DeepPose [46]. Second,\ndue to the multi-head attention mechanism, our method can\navoid the feature misalignment between the convolutional\nfeatures and predictions. Third, since we predict the key-\npoints in the sequential way, the transformer can naturally\ncapture the structured dependency between the keypoints,\nresulting in improved performance.\nWe summarize the main contributions as follows.\n• TFPose is the ﬁrst transformer-based pose estimation\nframework. Our proposed framework adapts to the\nsimple and straightforward regression-based methods,\nwhich is end-to-end trainable and can overcome many\ndrawbacks of the heatmap-based methods.\n• Moreover, our TFPose can naturally learn to exploit\nthe structured dependency between the keypoints with-\nout heuristic designs, e.g., in [41]. This results in im-\nproved performance and better interpretability.\n• TFPose achieves greatly advance the state-of-the-art\nof regression-based methods, making the regression-\nbased methods comparable to the state-of-the-art\nheatmap-based ones. For example, we improve\nthe previously best regression-based method Sun et\nal. [42] by 4.4% AP on the COCO keypoint detection\ntask, and Aiden et al. [34] by 0.9% PCK on the MPII\nbenchmark.\n2. Related Work\nTransformers in computer vision. After being proposed\nin [47], Transformers have achieved signiﬁcant progress\nin NLP (Natural Language Processing) [2, 14]. Recently,\nTransformers have also attracted much attention in com-\nputer vision community. For basic image classiﬁcation\ntask, ViT [15] apply a pure Transformer to sequential im-\nage patches. Expect for image classiﬁcation, vision Trans-\nformer is also widely applied to object detection [6,54], seg-\nmentation [48, 49], pose estimation [22, 23, 28], low-level\nvision task [8]. More details, we refer to [16]. Specially,\nDETR [6] and Deformable DETR [54] formulate the object\ndetection task to predict a box set so that object detection\nmodel can be trained end-to-end; the Transformer appli-\ncations in both 3D Hand Pose Estimation [22, 23] and 3D\nhuman pose estimation [22, 23] show that Transformer is\nsuitable for modeling human pose.\nHeatmap-based 2D pose estimation. Heatmap-based 2D\npose estimation methods [4, 5, 9, 10, 17, 26, 33, 40, 51] per-\nform the state-of-the-art accuracy in 2D human pose esti-\nmation. Recently, most work, including both top-down and\nbottom up, are heatmap-based methods. [33] ﬁrstly propose\nhourglass-style framework and hourglass-style framework\nalso be widely applied, such as, [4,26]. [40] propose a novel\nnetwork architecture for heatmap-based 2D pose estima-\ntion and achieve a excellent performance. [10] propose a\nnew bottom-up method achieve impressive performance in\nCrowdPose dataset [25] and improved by [31]. [4] propose a\nefﬁcient network achieving the the-state-of-art performance\nin COCO keypoint detection dataset [29]. However, [42,44]\nargue that heatmap-based methods cannot be trained end-to-\nend, due to the ”taking-maximum” operation. Recently, the\nnoise and ambiguity in the ground truth heatmap are found\nby [31, 41]. [21] ﬁnds the heatmap data processing applied\nby most previous work is biased and proposed an new unbi-\nased data processing method.\nRegression-based 2D pose estimation. 2D human pose\nestiamtion is naturally a regression problem [42]. How-\never, regression based methods are not accurate as well as\nheatmap-based methods, thus there are just a few works\n[7, 35, 41, 42, 44, 46] for it. Apart from that, although some\nmethods, such as G-RMI [36], apply regression method to\nreduce the quantization errors casued by heatmap, they are\nessentially heatmap-based methods. There are some work\npoint out the reason of the bad performance of regression-\nbased method. Directpose [44] points out the feature mis-\nalignment issue and propose a mechanism to align the fea-\nture and the predictions; [41] indicates regression-based\nmethod cannot learn the structure-aware information well\nand proposal a hand-design model for pose estimation to\n2\n……\n+\nK× Query\nPosition embedding\nCNN Transformer\nEncoder\nTransformer\nDecoder\n……\nK× (X,Y)\nFigure 2 – Overall pipeline of TFPose.The model directly predicts a sequence of keypoint coordinates in parallel by combining a com-\nmon CNN with a transformer architecture. A transformer decoder takes as input a ﬁx number of keypoint queries and encoder output.\nThen, we pass the output embedding of the decoder to a multi-layer feed forward network that predicts ﬁnal keypoint coordinates.\nforce regression-based method learn the structure-aware in-\nformation better; Sun et al. [42] propose integral regression,\nwhich shares the merits of both heatmap representation and\nregression approaches, to avoid non-differentiable postpro-\ncessing and quantization error issues.\n3. Our Approach\n3.1. TFPose Architecture\nThis work focus on the single pose estimation task. Fol-\nlowing previous works, we ﬁrst apply a person detector to\nobtain the bounding boxes of persons. Then, according to\nthe detected boxes, each person is cropped from the input\nimage. We denote the cropped image by I ∈ Rh×w×3,\nwhere h, w is the height and the width of the image, re-\nspectively. With the cropped image with a single person,\nthe previous heatmap-based methods apply a convolutional\nneural network Fto the patch to predict keypoint heatmaps\nH ∈Rh×w×k(Hk for kth joint) of this person, where k is\nthe number of the predicted keypoint. Formally, we have\nH = F(I). (1)\nEach pixel of H represents the probability that the body\njoints locate at this pixel. To obtain the joints’ coordinates\nJ ∈R2×k(Jk for kthjoint), those methods usually use the\n“taking-maximum” operation to obtain the locations with\npeak activations. Formally, let p be the spatial locations on\nH, and it can be formulated as\nJk = arg max\np\n(Hk(p)). (2)\nNote that in the heatmap-based methods, the localization\nprecision of p is up to the resolution of H, which is often\nmuch lower than the resolution of the input and thus causes\nthe quantization errors. Moreover, the arg max operation\nhere is not differential, making the pipeline not end-to-end\ntrainable. In TFPose, we instead treat J as a K-length se-\nquence and directly map the input I to the body joints’ co-\nordinates J. Formally,\nJ = F(I), (3)\nwhere Fis composed of three main components: a stan-\ndard CNN backbone to extract multi-level feature represen-\ntations, a feature encoder to capture and fuse multi-level\nfeatures and a coarse-to-ﬁne decoder to generate the a se-\nquence of keypoint coordinates. It is illustrated in Figure 2.\nNote that our TFPose is fully differentiable and the localiza-\ntion precision is not limited by the resolution of the feature\nmaps.\n3.2. Transformer Encoder\nAs shown in Figure 3, the backbone extracts multi-level\nfeatures of the input image. The multi-level feature maps\n+\n+\n+\n+\n……\nFlattenPosition embedding\nC2 C3 C4 C5\nEp\nEl2 El3 El4 El5\n……\nFlatten\nF0\nFE0\nFigure 3 – Positional encoding. This ﬁgure illustrates the po-\nsitional embeddings to the input F0 of the transformer. El\ni rep-\nresents the level embeddings depicting which level a feature\nvector comes from. EP represents the pixel embedding depict-\ning the spatial location of a feature vector on the feature maps.\nWe use FE\n0 to denote F0 with position embedding. Following\n[54], both F0 and FE\n0 are the inputs of the transformer.\n3\nare denoted by C2, C3, C4 and C5, respectively, whose\nstrides are 4, 8, 16 and 32, respectively. We separately ap-\nply a 1 ×1 convolution to these feature maps so that they\nhave the same number of the output channels. These feature\nmaps are ﬂatten and concatenated together, which results in\nthe input F0 ∈Rn×c to the ﬁrst encoder in the transformer\n, where nis the number of the pixel in the F0. Here, we use\nFi denotes the output to the i-th encoder in the transformer.\nFollowing [47, 54], F0 is added with the positional embed-\ndings and we denote F0 with the positional embeddings by\nFE\n0 . The details of the positional embeddings will be dis-\ncussed in Section 3.2. Afterwards, both F0 and FE\n0 are sent\nto the transformer to compute the memory M ∈ Rn×c.\nWith the memory M, a query matrix Q ∈RK×c will be\nused in the transformer decoder to obtain the K body joints’\ncoordinates J ∈RK×2.\nWe follow Deformable DETR [54] to design the encoder\nin our transformer. As mentioned before, beforeF0 is taken\nas inputs, each feature vector of F0 is added with the posi-\ntional embeddings. Following Deformable DETR, we use\nboth level embedding EL\nl ∈R1×c and pixel position em-\nbeddings EP ∈Rn×c. The former encodes the level where\nthe feature vector comes from, and the latter is the feature\nvector’s spatial location on the feature maps. As shown in\nFigure 3, all the feature vectors from level lare added with\nEL\nl and then the feature vectors are added with their pixel\nposition embeddings Ep, where Ep is the 2-D cosine posi-\ntional embeddings corresponding to the 2-D location of the\nfeature vector on the feature maps.\nIn TFPose, we use NE = 6 encoder layers. For eth\nencoder layer, as shown in Figure 4, the previous encoder\nlayer’s outputs will be taken as the input of this layer. Fol-\nlowing Deformable DETR, we also compute the pixel-to-\npixel attention between the output vectors of each encoder\nlayer (denoted by ‘p2p attention’). After NE transformer\nencoder layers are applied, we can obtain the memory M.\n3.3. Transformer Decoder\nIn the decoder, we aim to decode the desired keypoint\ncoordinates from the memory M. As mentioned before, we\nuse a query matrix Q ∈ RK×c to achieve this. Q is es-\nsentially an extra learnable matrix, which is jointly updated\nwith the model parameters during training and each row of\nwhich corresponds to a keypoint. In TFPose, we have ND\ntransformer decoder layers. As shown in Figure 4, each de-\ncoder layer takes as input the memoryM and the outputs of\nthe previous decoder layer Qd−1 ∈RK×c. The ﬁrst layer\ntakes as inputs M and the matrix Q. Similarly, Qd−1 is\nadded with the positional embeddings. The result is denoted\nby QE\nd−1. The Qd−1 and QE\nd−1 will be sent to the query-to-\nquery attention module (denoted as ‘q2q attention’), which\naims to model the dependency between human body joints.\nThe q2q attention module useQd−1, QE\nd−1 and QE\nd−1 as val-\nues, queries and keys, respectively. Later, the output of the\nq2q attention module and M used to compute the pixel-to-\nquery attention (denoted as ‘p2q attention’) with the value\nbeing the former and query being the latter. Then, an MLP\nwill be applied to the output of p2q attention the output of\nthe decoder Qd. The keypoint coordinates are predicted by\napplying an MLP with output channels being 2 to each row\nof Qd.\nInstead of simply predicting the keypoint coordinates in\nthe ﬁnal decoder layer, inspired by [7, 20, 54], we require\nall the decoder layers to predict the keypoint coordinates.\nSpeciﬁcally, we let the ﬁrst decoder layer directly predict\nthe target coordinates. Then, every other decoder layer re-\nﬁnes the predictions of its previous decoder layer by pre-\ndicting reﬁnements ∆ˆyd ∈RK×2. In that way, the keypoint\ncoordinates can be progressively reﬁned. Formally, letˆyd−1\nbe the keypoint coordinates predicted by the (d−1)-th de-\ncoder layer, the predictions of the d-th decoder layer are\nˆyd = σ(σ−1(ˆyd−1) + ∆ˆyd), (4)\nwhere σ and σ−1 denote the sigmoid and inverse sigmoid\nfunction, respectively. ˆy0 is a randomly-initialized matrix\nand jointly updated with model parameters during training.\n3.4. Training Targets and Loss Functions\nThe loss functions of TFPose consist of two parts. The\nﬁrst part is the L1 regression loss. Let y ∈RK×2 be the\nground-truth coordinates. The regression loss is formulated\nas,\n……\nPixel-to-Pixel\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\n+\nQuery-to-Query\nAttention\nAdd & Norm\nPixel-to-Query\nAttention\nFeed\nForward\nAdd & Norm\nPosition embedding\nMLP\nNE× ND×\n(Xi,Yi)\nQ\nF0\nDeconv\nHˆ\nMMC5\nFigure 4 – Transformer architecture. During training, de-\nconvolution modules are used to upsamle transformer encoder\noutput (MC5 ) for for auxiliary loss. During testing, only output\nTransformer decoder. ‘Norm’ represent normalization; ( Xi,\nYi) represent the coordinate for ith keypoint.\n4\nLreg =\nND∑\nd=1\n||y−ˆyd||, (5)\nwhere ND is the number of the decoders, and every de-\ncoder layer is supervised with the target keypoint coordi-\nnates. The second part is an auxiliary loss Laux. Follow-\ning DirectPose [44], we use the auxiliary heatmap learn-\ning during training 1, which can result in better perfor-\nmance. In order to use the heatmap leanrning, we gather\nthe feature vectors that were C5 from M and reshape these\nvectors into the original spatial shape. The result is de-\nnoted by MC5 ∈ R(h/32)×(w/32)×c. Similar to simple\nbaseline [51], we apply 3×deconvolution to MC5 to up-\nsample the feature maps by 8 and generate the heatmap\nˆH ∈R(h/4)×(w/4)×K. Then, we compute the mean square\nerror (MSE) loss between the predicted and ground-truth\nheatmaps. The ground-truth heatmaps are generated by fol-\nlowing [33, 52]. Formally, the auxiliary loss function is\nLaux = ||H−ˆH||2, (6)\nWe sum the two loss functions to obtain the ﬁnal overall\nloss\nLoverall = Lreg + λLaux, (7)\nwhere λis a constant and used to balance the two losses.\n4. Experiments\n4.1. Implementation details.\nDatasets. We conduct a number of ablation experiments on\ntwo mainstream pose estimation datasets.\nOur experiments are mainly conducted on COCO2017\nKeypoint Detection [50] benchmark, which contains about\n250K person instances with 17 keypoints. Following com-\nmon settings, we use the same person detector in Simple\nBaseline [52] for COCO evaluation. We report results on\nthe val set for ablation studies and compare with other\nstate-of-the-art methods on the test-dev set. The Average\nPrecision (AP) based on Object Keypoint Similarity (OKS)\nis employed as the evaluation metric.\nBesides COCO dataset, we also report results on MPII\ndataset [1]. MPII is a popular benchmark for single person\n2D pose estimation, which has 25K images. In total, there\nare 29Kannotated poses for training, and another7Kposes\nfor testing. The Percentage of Correct Keypoints (PCK)\nmetric is used for evaluation.\nModel settings. Unless speciﬁed, ResNet-18 [18] is used\nas the backbone in ablation study. The size of input image\nis 256 ×192 or 384 ×288. The weights pre-trained on Ima-\ngeNet [13] are used to initialize the ResNet backbone. The\n1The heatmap branch is removed in inference.\n0 25000 50000 75000 100000 125000 150000 175000\niter\n0.0\n0.2\n0.4\n0.6\n0.8accuracy\nwo_aux_loss\nwith_aux_loss\nFigure 5 – Convergence curves of TFPose superivsed by dif-\nferent kinds of losses on COCO val2017 set. ”wo aux loss”\nindicates only regression loss is employed. ”with aux loss” in-\ndicates both regression loss and auxiliary loss are employed.\nrest parts of our network are initialized with random param-\neters. For the Transformer, we adopt Defermable Attention\nModule proposed in [54] and the same hyper-parameters are\nused.\nTraining. All the models are optimized by AdamW [30]\nwith a base learning rate of 4 ×10−3. β1 and β2 are set to\n0.9 and 0.999. Weight decay is set to10−4. λis set to 50 by\ndefault for balancing the regression loss and auxiliary loss.\nUnless speciﬁed, all the experiments use a cosine learning\nschedule with base learning rate 4 ×10−3. Learning rate of\nthe Transformers and the linear projections for predicting\nkeypoints offsets is decreased by a factor of 10. For data\naugmentation, random rotation ([−40◦,40◦]), random scale\n([0.5,1.5]), ﬂipping and half body data augmentation [50]\nare applied. For auxiliary loss, we follow Unbiased Data\nProcessing (UDP) [21] to generate unbiased ground truth.\n4.2. Ablation Study\nQuery-to-query attention. In the proposed TFPose, query-\nto-query attention is designed to capture structure-aware in-\nformation cross all the keypoints. Unlike [41] which uses a\nhand-design method to explicitly force the model to learn\nthe structure-aware information, query-to-query attention\nmodels human body structure implicitly. To study the effect\nof query-to-query attention, we report the results of remov-\ning the query-to-query attention in all decoder layers. As\nshown in Table 1, the proposed query-to-query attention im-\nprove the performance by 1.3% AP with only 0.1 GFLOPs\nmore computational cost.\nConﬁgurations of Transformer decoder. Here we study\nthe effect of width and depth of the decoder. Speciﬁcally,\nwe conduct experiments by varying the number of channels\n5\nq2q GFLOPs APkp APkp\n50 APkp\n75 APkp\nM APkp\nL\n3.51 63.2 85.1 69.9 60.3 70.2\n✓ 3.61 64.5 85.2 71.2 61.5 71.5\nTable 1– The effect of query-to-query attention in decoder lay-\ners on COCO val2017 set. ”q2q” indicates whether add query-\nto-query attention in the decoder. In this experiment, we set\nthe number of transformer encoder layers: NE = 0, and trans-\nformer decoder layers: ND = 6. As shown in the table, de-\ncoder with query-to-query attention have better performance.\nC GFLOPs APkp APkp\n50 APkp\n75 APkp\nM APkp\nL\n128 2.28 63.2 85.0 69.8 60.6 69.8\n256 3.61 64.5 85.2 71.2 61.5 71.5\nTable 2 – The effect of the number of channels of the input\nfeatures to the Transformer encoder on COCO val2017 set. C\nis the number of channels. In this experiment, we set the num-\nber of transformer encoder layers: NE = 0, and transformer\ndecoder layers: ND = 6.\nND GFLOPs APkp APkp\n50 APkp\n75 APkp\nM APkp\nL\n1 6.32 65.7 86.3 73.4 63.0 72.4\n2 6.41 66.9 86.5 74.0 64.2 73.8\n3 6.50 67.1 86.6 74.2 64.5 73.9\n4 6.59 67.2 86.6 74.2 64.6 74.0\n5 6.68 67.2 86.6 74.2 64.6 74.1\n6 6.77 67.2 86.6 74.2 64.6 74.1\nTable 3 – Ablation study of different numbers of decoder lay-\ners on COCO val2017 set. ND is the number of decoder layers\nused for reﬁning the location of key-points. ”GFLOPs” indi-\ncates the computational cost. In this experiment, we set the\nnumber of transformer encoder layers: NE = 6.\nof the input features and the number of decoder layers in\nTransformer decoder.\nAs shown in Table 2, Transformers with 256-channel\nfeature maps is 1.3% AP higher than 128-channels ones.\nMoreover, we change the number of decoder layers. As\nshown in Table 3, the performance grows at the ﬁrst three\nlayers and saturates at the fourth decoder layer.\nAuxiliary loss. As shown in previous works [15,54,54], the\ntransformer modules may converge slower. To mitigate this\nissue, we adopt the deformable attention module proposed\nin [54]. Apart from that, we propose an auxiliary loss to ac-\ncelerate the convergence speed of TFPose. Here, we inves-\ntigate the effect of the auxiliary loss. In this experiment, the\nﬁrst model is only supervised by regression loss; the second\nmodel is supervised by both regression loss and auxiliary\nloss. As shown in Figure 5 and Table 4, the auxiliary loss\ncan signiﬁcantly accelerates the convergence speed of TF-\nPose and boost the performance by a large margin ( +2.3%\nAP).\naux GFLOPs APkp APkp\n50 APkp\n75 APkp\nM APkp\nL\n6.76 67.2 86.6 74.2 64.6 74.1\n✓ 6.76 69.5 87.5 76.5 66.1 77.0\nTable 4 – Ablation study of effectiveness of auxiliary loss on\nCOCO val2017 set. ”aux” indicts whether using auxiliary loss.\nIn this experiment, we set the number of transformer encoder\nlayers: NE = 6, and transformer decoder layers: ND = 6.\n4.3. Discussions on TFPose\nVisualization of sampling keypoints. To study how the\nDeformable Attention Module locate the body joints, we vi-\nsualize the sampling locations of the module on the feature\nmaps C3 . In Deformable Attention Module, there are 8 at-\ntention heads and every head will sample 4 points on every\nfeature map. So for the C3 feature map, there are 32 sam-\npling points. As shown in Figure 7, the sampling points (red\ndot) are all densely located nearby the ground truth (yellow\ncircle). This visualization shows that TFPose can address\nthe feature mis-alignment issue in a sense, and supervises\nthe CNN with dense pixel information.\nVisualization of query-to-query attention. To further\nstudy how the query-to-query self-attention module works,\nwe visualize the attention weights of the query-to-query\nself-attention. As shown in Figure 6, there are two obvi-\nous patterns of attention: the ﬁrst attention pattern is that\nthe symmetric joints (e.g. left shoulder and right shoulder)\nare more likely to attend to each other, and the second atten-\ntion pattern is that the adjacent joints ( e.g. eyes, nose, and\nmouth) are more likely to attend to each other.\nTo have a better understanding of this attention pattern,\nwe also visualize the attention graph between each keypoint\naccording to the attention maps in the supplementary. This\nattention pattern suggests that TFPose can employ the con-\ntext and structured relationship between the body joints to\nlocate and classify the types of body joints.\n4.4. Comparison with State-of-the-art Methods\nIn this section, we compare TFPose with previous state-\nof-the-art 2D pose estimation methods on COCO val split,\nCOCO test-dev split and MPII [1]. We compare these\nmethod in terms of both accuracy and computational cost.\nThe results of our proposed TFPose and other state-of-the-\nart methods are listed in Table 5, Table 6 and Table 7.\nResults on COCO val. set. As shown in Table 5, with sim-\nilar computational cost, TFPose with 4 encoder layers and\nResNet-50 surpass the previous regression-based method\nDeepPose with ResNet-101 (70.5% AP vs. 56.0% AP) by\na large margin and even has much better performance than\nDeepPose with ResNet-152 (70.5% AP vs. 58.3% AP). Be-\nsides, TFPose also outperform many heatmap-based meth-\nods, for example, 8-stage Hourglass [33](70.5% AP vs.\n6\nNoseL eyeR eyeL earR ear\nL shoulderR shoulder\nL elbowR elbowL wristR wristL hipR hipL kneeR kneeL ankleR ankle\nNose\nL eye\nR eye\nL ear\nR ear\nL shoulder\nR shoulder\nL elbow\nR elbow\nL wrist\nR wrist\nL hip\nR hip\nL knee\nR knee\nL ankle\nR ankle\nNoseL eyeR eyeL earR ear\nL elbowR elbowL wristR wristL hipR hipL kneeR kneeL ankleR ankle\nNose\nL eye\nR eye\nL ear\nR ear\nL shoulder\nR shoulder\nL elbow\nR elbow\nL wrist\nR wrist\nL hip\nR hip\nL knee\nR knee\nL ankle\nR ankle\nL shoulderR shoulder\nDecoder layer 2 Decoder layer 3\nFigure 6 – Visualization of the attention weights of the q2q attention. We average the attention maps over the whole COCO2017val\ndataset. The left map is the attention weights of the second decoder layer. The right map is the attention weights of the third decoder\nlayer. ‘L’ means the joints are in the left. ‘R’ means the joints are in the right. The horizontal axis and the vertical axis represent the\ninput query and key of the attention module, respectively. Multi-head attention computes the attention weights between each pair of the\nqueries and keys. The query attends more to the key with a higher attention weight.\nFigure 7 – Visualisation of the sampling point on feature map. There are 17 queries for 17 keypoints. We visualize 12 body joints\nqueries (not including facial joints). Each image correspond to a body joints. Red dot represent the sampling point; yellow circle\nrepresent the ground truth.\n67.1% AP), CPN [9](70.5% AP vs. 69.4% AP) by a large\nmargin. It is also important to note that TFPose with 4 en-\ncoder layers and ResNet-50 outperforms the strong baseline\nSimpleBaseline [52] with ResNet-50 (70.5% AP vs. 70.4%\nAP) with lower computational cost (7.68 GFLOPs vs. 8.9\nGFLOPs).\nResults on COCO test-dev set. As shown in Table 6, TF-\nPose achieves the best result among regression-based meth-\nods. Especially, TFPose with 6 encoder layers and ResNet-\n50 achieves 70.9% AP, which is higher than the Int. Reg.\n[42] (67.8% AP), and our computational cost is lower than\nthe Int. Reg. (9.15 GFLOPs vs. 11.0 GFLOPs). More-\nover, with the same bacbone ResNet-50, our TFPose even\nachieves better performance than the strong heatmap-based\nmethod SimpleBaseline (70.5% vs. 70.0% AP) with less\ncomputational complexity (7.7 GFLOPS vs. 8.9 GFLOPS).\nAdditionally, the results of TFPose are also close to the\nbest reported pose estimation results. For exmaple, the per-\nformance of TFPose (72.2% AP) is close to the ResNet-\nInception based CPN(72.1% AP) and ResNet-152 based\nSimpleBaseline (73.7% AP). Note that they use much larger\nbackbones than ours.\n7\nModels Backbone Input Size GFLOPs AP(OKS)\nDeepPose [46] ResNet-101 256 × 192 7.6 56.0\nDeepPose [46] ResNet-152 256 × 192 11.3 58.3\n8-stage Hourglass [33] - 256 × 192 19.5 66.9\n8-stage Hourglass [33] - 256 × 256 25.9 67.1\nCPN [9] ResNet-50 256 × 192 6.2 68.6 (69.4)\nCPN [9] ResNet-50 384 × 288 13.9 70.6 (71.6)\nSimpleBaseline [52] ResNet-50 256 × 192 8.9 70.4\nSimpleBaseline [52] ResNet-50 384 × 288 20.0 72.2\nOurs (ND = 4) ResNet-50 256 × 192 7.7 70.5\nOurs (ND = 6) ResNet-50 256 × 192 9.2 71.0\nOurs (ND = 6) ResNet-50 384 × 288 20.4 72.4\nTable 5 – Comparisons with previous works on the COCOval split. For CPN, the results in the brackets are with online hard keypoints\nmining. All the reported methods use person detectors with similar performance. Speciﬁcally, Hourglass and CPN use the person\ndetector with 55.3% AP on COCO. Others use the person detector with 56.4% AP. DeepPose is implemented by the mmpose [12].\nFlipping test is applied for all model. ND represents the number of encoder layers.\nMethod Backbone Input Size GFLOPs APkp APkp\n50 APkp\n75 APkp\nM APkp\nL\nHeatmap-based methods\nAE [32] HourGlass [33] - - 56.6 81.8 61.8 49.8 67.0\nMask R-CNN [17] ResNet-50 - - 62.7 87.0 68.4 57.4 71.1\nCMU-Pose [5] VGG-19 [39] - - 64.2 86.2 70.1 61.0 68.8\nG-RMI [36] ResNet-101 - - 64.9 85.5 71.3 62.3 70.0\nHigherHRNet†‡ [10] HRNet-W48 - - 70.5 89.3 77.2 66.6 75.8\nCPN [9] ResNet-Ince. 384 × 288 29.2 72.1 91.4 80.0 68.7 77.2\nSimpleBaseline† [51] ResNet-50 256 × 192 8.9 70.0 90.9 77.9 66.8 75.8\nSimpleBaseline† [51] ResNet-152 384 × 288 35.6 73.7 91.9 81.1 70.3 80.0\nHRNet† [40] HRNet-W32 384 × 288 16.0 74.9 92.5 82.8 71.3 80.9\nRegression-based methods\nDeepPose† [46] ResNet-101 256 × 192 7.69 57.4 86.5 64.2 55.0 62.8\nDeepPose† [46] ResNet-152 256 × 192 11.34 59.3 87.6 66.7 56.8 64.9\nDirectpose [44] ResNet-50 - - 62.2 86.4 68.2 56.7 69.8\nDirectpose [44] ResNet-101 - - 63.3 86.7 69.4 57.8 71.2\nSPM [35] HourGlass [33] - - 66.9 88.5 72.9 62.6 73.1\nInt. Reg. [42] ResNet-101 256 × 256 11.0 67.8 88.2 74.8 63.9 74.0\nOurs†(ND = 4) ResNet-50 256 × 192 7.7 70.5 90.4 78.7 67.6 76.8\nOurs†(ND = 6) ResNet-50 256 × 192 9.2 70.9 90.5 79.0 68.1 77.0\nOurs†(ND = 6) ResNet-50 384 × 288 20.4 72.2 90.9 80.1 69.1 78.8\nTable 6– Comparisons with state-of-the-art methods on COCOtest-dev set. † and ‡ denote ﬂipping and multi-sacle testing, respectively.\nInput size and the GFLOPs are shown for the single person pose estimation methods. ’ResNet-Ince.’ represent the ResNet inception.\nThe Simple baseline(ResNet-50) is tested with the ofﬁcial code. ND represents the number of encoder layers.\nResults on MPII test set. On the MPII benchmark, TF-\nPose also achieves the best results among the regression-\nbased methods. As shown in Table 7, TFPose with ResNet-\n50 is higher than the method proposed by Aiden et al. [34]\n(90.4% vs. 89.5%) with the same backbone. TFPose is also\ncomparable to heatmap-based methods.\n5. Conclusion\nWe have proposed a novel pose estimation framework\nnamed TFPose built upon Transformers, which largely im-\nproves the performance of the regression-based pose es-\ntimation and bypasses the drawbacks of heatmap-based\nmethods such as the non-differentiable post-processing and\nquantization error. We have shown that with the attention\nmechanism, TFPose can naturally capture the structured re-\nlationship between the body joints, resulting in improved\nperformance. Extensive experiments on the MS-COCO and\nMPII benchmarks show that TFPose can achieve state-of-\nthe-art performance among regression-based methods and\nis comparable to the best heatmap-based methods.\n8\nMethod Head Sho. Elb. Wri. Hip Knee Ank. Total\nHeatmap-based methods\nPishchulin et al. [37] 74.3 49.0 40.8 34.1 36.5 34.4 35.2 44.1\nTompson et al. [45] 95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6\nHu et al. [19] 95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4\nLifshitz et al. [27] 97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0\nRaf et al. [38] 97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3\nBulat et al. [3] 97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7\nChu et al. [11] 98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5\nKe et al. [24] 98.5 96.8 92.7 88.4 90.6 89.3 86.3 92.1\nTang et al. [43] 98.4 96.9 92.6 88.7 91.8 89.4 86.2 92.3\nZhang et al. [53] 98.6 97.0 92.8 88.8 91.7 89.8 86.6 92.5\nRegression-based methods\nCarreira et al. [7] 95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3\nSun et al. [41] 97.5 94.3 87.0 81.2 86.5 78.5 75.4 86.4\nAiden et al. (ResNet-50) [34] 97.8 96.0 90.0 84.3 89.8 85.2 79.7 89.5\nOurs (ResNet-50) 98.0 95.9 91.0 86.0 89.8 86.6 82.6 90.4\nTable 7 – MPII human pose test set PCKh accuracies. For our model, the number of encoder layers ND is set to 6.\nAcknowledgements\nThe authors would like to thank Alibaba Group for the\ndonation of GPU cloud computing resources.\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2d human pose estimation: New benchmark\nand state of the art analysis. In Proc. IEEE Conf. Comput.\nVis. Pattern Recog., pages 3686–3693, 2014. 5, 6\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. In Proc. Adv. Neural\nInform. Process. Syst., 2020. 2\n[3] Adrian Bulat and Georgios Tzimiropoulos. Human pose es-\ntimation via convolutional part heatmap regression. In Proc.\nEur. Conf. Comput. Vis., pages 717–732, 2016. 9\n[4] Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin,\nAngang Du, Haoqian Wang, Xiangyu Zhang, Xinyu Zhou,\nErjin Zhou, and Jian Sun. Learning delicate local represen-\ntations for multi-person pose estimation. In Proc. Eur. Conf.\nComput. Vis., pages 455–472. Springer, 2020. 2\n[5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\nYaser Sheikh. Openpose: realtime multi-person 2d pose es-\ntimation using part afﬁnity ﬁelds. IEEE Trans. Pattern Anal.\nMach. Intell., 43(1):172–186, 2019. 2, 8\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Proc. Eur. Conf.\nComput. Vis., pages 213–229. Springer, 2020. 2\n[7] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-\ntendra Malik. Human pose estimation with iterative error\nfeedback. In Proc. IEEE Conf. Comput. Vis. Pattern Recog.,\npages 4733–4742, 2016. 2, 4, 9\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 2\n[9] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang\nZhang, Gang Yu, and Jian Sun. Cascaded pyramid network\nfor multi-person pose estimation. In Proc. IEEE Conf. Com-\nput. Vis. Pattern Recog., pages 7103–7112, 2018. 2, 7, 8\n[10] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi,\nThomas S Huang, and Lei Zhang. Higherhrnet: Scale-aware\nrepresentation learning for bottom-up human pose estima-\ntion. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages\n5386–5395, 2020. 2, 8\n[11] Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L\nYuille, and Xiaogang Wang. Multi-context attention for hu-\nman pose estimation. In Proc. IEEE Conf. Comput. Vis. Pat-\ntern Recog., pages 1831–1840, 2017. 9\n[12] MMPose Contributors. Openmmlab pose estimation tool-\nbox and benchmark. https://github.com/open-mmlab/\nmmpose, 2020. 8\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proc. IEEE Conf. Comput. Vis. Pattern Recog.,\npages 248–255. Ieee, 2009. 5\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 6\n[16] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\n9\njing Xu, Yixing Xu, et al. A survey on visual transformer.\narXiv preprint arXiv:2012.12556, 2020. 2\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proc. Int. Conf. Comput. Vis., pages\n2961–2969, 2017. 2, 8\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc. IEEE\nConf. Comput. Vis. Pattern Recog., pages 770–778, 2016. 5\n[19] Peiyun Hu and Deva Ramanan. Bottom-up and top-down\nreasoning with hierarchical rectiﬁed gaussians. In Proc.\nIEEE Conf. Comput. Vis. Pattern Recog., pages 5600–5609,\n2016. 9\n[20] Tao Hu, Honggang Qi, Jizheng Xu, and Qingming Huang.\nFacial landmarks detection by self-iterative regression based\nlandmarks-attention network. volume 32, 2018. 4\n[21] Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. The\ndevil is in the details: Delving into unbiased data processing\nfor human pose estimation. In Proc. IEEE Conf. Comput.\nVis. Pattern Recog., pages 5700–5709, 2020. 1, 2, 5\n[22] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan. Hand-\ntransformer: Non-autoregressive structured modeling for 3d\nhand pose estimation. In Proc. Eur. Conf. Comput. Vis. ,\npages 17–33. Springer, 2020. 2\n[23] Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and Junsong\nYuan. Hot-net: Non-autoregressive transformer for 3d hand-\nobject pose estimation. In Proc. ACM Int. Conf. Multimedia,\npages 3136–3145, 2020. 2\n[24] Lipeng Ke, Ming-Ching Chang, Honggang Qi, and Siwei\nLyu. Multi-scale structure-aware network for human pose es-\ntimation. In Proc. Eur. Conf. Comput. Vis., September 2018.\n9\n[25] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu\nFang, and Cewu Lu. Crowdpose: Efﬁcient crowded scenes\npose estimation and a new benchmark. In Proc. IEEE Conf.\nComput. Vis. Pattern Recog., pages 10863–10872, 2019. 2\n[26] Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yum-\ning Du, Tianzi Xiao, Gang Yu, Hongtao Lu, Yichen Wei,\nand Jian Sun. Rethinking on multi-stage networks for human\npose estimation. arXiv preprint arXiv:1901.00148, 2019. 2\n[27] Ita Lifshitz, Ethan Fetaya, and Shimon Ullman. Human pose\nestimation using deep consensus voting. In Proc. Eur. Conf.\nComput. Vis., pages 246–260, 2016. 9\n[28] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-\nman pose and mesh reconstruction with transformers. arXiv\npreprint arXiv:2012.09760, 2020. 2\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nProc. Eur. Conf. Comput. Vis. , pages 740–755. Springer,\n2014. 2\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In Proc. Int. Conf. Learn. Represent., 2019. 5\n[31] Zhengxiong Luo, Zhicheng Wang, Yan Huang, Tieniu\nTan, and Erjin Zhou. Rethinking the heatmap regres-\nsion for bottom-up human pose estimation. arXiv preprint\narXiv:2012.15175, 2020. 1, 2\n[32] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-\ntive embedding: End-to-end learning for joint detection and\ngrouping. arXiv preprint arXiv:1611.05424, 2016. 8\n[33] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation. In Proc. Eur.\nConf. Comput. Vis. , pages 483–499. Springer, 2016. 2, 5,\n6, 8\n[34] Aiden Nibali, Zhen He, Stuart Morgan, and Luke Prender-\ngast. Numerical coordinate regression with convolutional\nneural networks. arXiv preprint arXiv:1801.07372, 2018. 2,\n8, 9\n[35] Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng\nYan. Single-stage multi-person pose machines. In Proc. Int.\nConf. Comput. Vis., pages 6951–6960, 2019. 2, 8\n[36] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander\nToshev, Jonathan Tompson, Chris Bregler, and Kevin Mur-\nphy. Towards accurate multi-person pose estimation in the\nwild. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\npages 4903–4911, 2017. 2, 8\n[37] Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, and\nBernt Schiele. Strong appearance and expressive spatial\nmodels for human pose estimation. In Proc. Int. Conf. Com-\nput. Vis., pages 3487–3494, 2013. 9\n[38] Umer Raﬁ, Bastian Leibe, Juergen Gall, and Ilya Kostrikov.\nAn efﬁcient convolutional network for human pose estima-\ntion. In Proc. Brit. Mach. Vis. Conf., volume 1, page 2, 2016.\n9\n[39] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In Proc.\nInt. Conf. Learn. Represent., 2015. 8\n[40] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog. ,\npages 5693–5703, 2019. 2, 8\n[41] Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.\nCompositional human pose regression. In Proc. Int. Conf.\nComput. Vis., pages 2602–2611, 2017. 1, 2, 5, 9\n[42] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen\nWei. Integral human pose regression. In Proc. Eur. Conf.\nComput. Vis., pages 529–545, 2018. 2, 3, 7, 8\n[43] Wei Tang, Pei Yu, and Ying Wu. Deeply learned composi-\ntional models for human pose estimation. InProc. Eur. Conf.\nComput. Vis., September 2018. 9\n[44] Zhi Tian, Hao Chen, and Chunhua Shen. Directpose: Di-\nrect end-to-end multi-person pose estimation. arXiv preprint\narXiv:1911.07451, 2019. 2, 5, 8\n[45] Jonathan Tompson, Arjun Jain, Yann LeCun, and Christoph\nBregler. Joint training of a convolutional network and a\ngraphical model for human pose estimation. arXiv preprint\narXiv:1406.2984, 2014. 9\n[46] Alexander Toshev and Christian Szegedy. Deeppose: Human\npose estimation via deep neural networks. In Proc. IEEE\nConf. Comput. Vis. Pattern Recog., pages 1653–1660, 2014.\n2, 8\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Proc. Adv. Neural\nInform. Process. Syst., pages 5998–6008, 2017. 2, 4\n10\n[48] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. MaX-DeepLab: End-to-end panoptic\nsegmentation with mask transformers. In Proc. IEEE Conf.\nComput. Vis. Pattern Recog., 2021. 2\n[49] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In Proc.\nIEEE Conf. Comput. Vis. Pattern Recog., 2021. 2\n[50] Zhicheng Wang, Wenbo Li, Binyi Yin, Qixiang Peng, Tianzi\nXiao, Yuming Du, Zeming Li, Xiangyu Zhang, Gang Yu, and\nJian Sun. Mscoco keypoints challenge 2018. In Proc. Eur.\nConf. Comput. Vis., volume 5, 2018. 5\n[51] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\nfor human pose estimation and tracking. In Proc. Eur. Conf.\nComput. Vis., pages 466–481, 2018. 2, 5, 8\n[52] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\nfor human pose estimation and tracking. In Proc. Eur. Conf.\nComput. Vis., September 2018. 5, 7, 8\n[53] Hong Zhang, Hao Ouyang, Shu Liu, Xiaojuan Qi, Xiaoy-\nong Shen, Ruigang Yang, and Jiaya Jia. Human pose esti-\nmation with spatial contextual information. arXiv preprint\narXiv:1901.01760, 2019. 9\n[54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable Transform-\ners for end-to-end object detection. InProc. Int. Conf. Learn.\nRepresent., 2021. 2, 3, 4, 5, 6\nA. Qualitative Results of TFPose\nWe show more qualitative results in Figure 10. TFPose\nworks reliably under various challenging cases.\nB. Visualization of Transformer Attentions\nB.1. Query-to-query Attention\nWe observe two obvious query-to-query attention pat-\nterns in different decoder layers, termed symmetric pattern\nand adjacent pattern, respectively. Both patterns exist in\nall decoder layers, we illustrate them separately for conve-\nnience. For symmetric pattern, Figure 8 demonstrates that\nthe correlation between all pairs of symmetric joints in the\nthird decoder layer. For adjacent pattern, Figure 9 explicitly\nshows that adjacent joints attend to each other in the second\ndecoder layer.\nB.2. Multi-scale Deformable Attention\nWe visualize the learned multi-scale deformable atten-\ntion modules for better understanding. As shown in Fig-\nure 11 and Figure 12, the visualization indicates that TF-\nPose looks at context information surround the ground truth\njoint. More concretely, the sampling points near the ground\ntruth joint have higher attention weight (denoted as red),\nwhile the sampling points far from the ground truth joint\nown lower attention weight (denoted as blue).\n11\nNoseL eyeR eyeL earR ear\nL elbowR elbowL wristR wristL hipR hipL kneeR kneeL ankleR ankle\nNose\nL eye\nR eye\nL ear\nR ear\nL shoulder\nR shoulder\nL elbow\nR elbow\nL wrist\nR wrist\nL hip\nR hip\nL knee\nR knee\nL ankle\nR ankle\nL shoulderR shoulder\nDecoder layer 3\nHead\nShoulder\nElbow\nWrist\nHip\nAnkle\nKnee\nFigure 8 – The pattern of symmetric joints. As shown in the right graph, left shoulder and right shoulder are symmetric joints and they\nattend to each other. The same pattern can be found in other body joints including left elbow and right elbow, left hip and right hip etc.\nNoseL eyeR eyeL earR ear\nL elbowR elbowL wristR wristL hipR hipL kneeR kneeL ankleR ankle\nNose\nL eye\nR eye\nL ear\nR ear\nL shoulder\nR shoulder\nL elbow\nR elbow\nL wrist\nR wrist\nL hip\nR hip\nL knee\nR knee\nL ankle\nR ankle\nL shoulderR shoulder\nDecoder layer 2\nHead\nShoulder\nElbow\nWrist Hip\nAnkle\nKnee\nFigure 9 – The pattern of adjacent joints. As shown in the right graph, left shoulder attend to its adjacent joints including right shoulder,\nleft elbow, and head. The same pattern can be found in other body joints, e.g.., elbow and wrist.\n12\nFigure 10 – Qualitative results of TFPose with ResNet-50 on COCO2017 val set (single-model and singe-scale testing). The joints in\nupper body are represented by green and the joints in lower body are represented by blue.\n13\nLow High\nFigure 11 – Visualization of right shoulder’s pixel-to-query attention in the last decoder layer. For readability, we draw the sampling\npoints and attention weights from C3 feature map in different pictures. Each sampling point is marked as a ﬁlled circle whose color\nindicates its corresponding weight. The ground truth joint is shown as yellow cross marker.\n14\nLow High\nFigure 12 – Visualization of right knee’s pixel-to-query attention in the last decoder layer. For readability, we draw the sampling points\nand attention weights from C3 feature map in different pictures. Each sampling point is marked as a ﬁlled circle whose color indicates\nits corresponding weight. The ground truth joint is shown as yellow cross marker.\n15",
  "topic": "Pose",
  "concepts": [
    {
      "name": "Pose",
      "score": 0.7935072183609009
    },
    {
      "name": "Computer science",
      "score": 0.7112448811531067
    },
    {
      "name": "Transformer",
      "score": 0.6995078325271606
    },
    {
      "name": "Regression",
      "score": 0.6651958227157593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6058053970336914
    },
    {
      "name": "Estimation",
      "score": 0.5253771543502808
    },
    {
      "name": "Machine learning",
      "score": 0.5039498209953308
    },
    {
      "name": "Task (project management)",
      "score": 0.4830506443977356
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44121965765953064
    },
    {
      "name": "Regression analysis",
      "score": 0.42945414781570435
    },
    {
      "name": "Mathematics",
      "score": 0.1383986473083496
    },
    {
      "name": "Voltage",
      "score": 0.10105660557746887
    },
    {
      "name": "Engineering",
      "score": 0.0998242199420929
    },
    {
      "name": "Statistics",
      "score": 0.08455637097358704
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}