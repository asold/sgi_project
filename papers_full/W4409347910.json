{
  "title": "Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?",
  "url": "https://openalex.org/W4409347910",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5073224776",
      "name": "Michael-Andrei Panaitescu-Liess",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5016281162",
      "name": "Zora Che",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2144918093",
      "name": "Bang An",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2106949872",
      "name": "Yuancheng Xu",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A4290383299",
      "name": "Pankayaraj, Pathmanathan",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2224443342",
      "name": "Souradip Chakraborty",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2100863713",
      "name": "Sicheng Zhu",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2122996091",
      "name": "Tom Goldstein",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2102932341",
      "name": "Furong Huang",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5073224776",
      "name": "Michael-Andrei Panaitescu-Liess",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5016281162",
      "name": "Zora Che",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2144918093",
      "name": "Bang An",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2106949872",
      "name": "Yuancheng Xu",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A4290383299",
      "name": "Pankayaraj, Pathmanathan",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2224443342",
      "name": "Souradip Chakraborty",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2100863713",
      "name": "Sicheng Zhu",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2122996091",
      "name": "Tom Goldstein",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2102932341",
      "name": "Furong Huang",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389009541",
    "https://openalex.org/W6848955896",
    "https://openalex.org/W4360891421",
    "https://openalex.org/W4387561008",
    "https://openalex.org/W6849706158"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper, we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation, we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content, thereby addressing a critical concern in the deployment of LLMs. However, we also find that watermarking can have unintended consequences on Membership Inference Attacks (MIAs), which aim to discern whether a sample was part of the pretraining dataset and may be used to detect copyright violations. Surprisingly, we find that watermarking adversely affects the success rate of MIAs, complicating the task of detecting copyrighted text in the pretraining dataset. These results reveal the complex interplay between different regulatory measures, which may impact each other in unforeseen ways. Finally, we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications.",
  "full_text": "Can Watermarking Large Language Models Prevent\nCopyrighted Text Generation and Hide Training Data?\nMichael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu, Pankayaraj\nPathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein, Furong Huang\nUniversity of Maryland, College Park\n{mpanaite, zche, bangan, ycxu, pan, schakra3, sczhu, tomg, furongh}@umd.edu\nAbstract\nLarge Language Models (LLMs) have demonstrated impres-\nsive capabilities in generating diverse and contextually rich\ntext. However, concerns regarding copyright infringement\narise as LLMs may inadvertently produce copyrighted ma-\nterial. In this paper, we first investigate the effectiveness of\nwatermarking LLMs as a deterrent against the generation of\ncopyrighted texts. Through theoretical analysis and empiri-\ncal evaluation, we demonstrate that incorporating watermarks\ninto LLMs significantly reduces the likelihood of generating\ncopyrighted content, thereby addressing a critical concern in\nthe deployment of LLMs. However, we also find that water-\nmarking can have unintended consequences on Membership\nInference Attacks (MIAs), which aim to discern whether a\nsample was part of the pretraining dataset and may be used to\ndetect copyright violations. Surprisingly, we find that water-\nmarking adversely affects the success rate of MIAs, compli-\ncating the task of detecting copyrighted text in the pretraining\ndataset. These results reveal the complex interplay between\ndifferent regulatory measures, which may impact each other\nin unforeseen ways. Finally, we propose an adaptive tech-\nnique to improve the success rate of a recent MIA under wa-\ntermarking. Our findings underscore the importance of devel-\noping adaptive methods to study critical problems in LLMs\nwith potential legal implications.\nIntroduction\nIn recent years, Large Language Models (LLMs) have\npushed the frontiers of natural language processing by facil-\nitating sophisticated tasks like text generation, translation,\nand summarization. With their impressive performance,\nLLMs are increasingly integrated into various applications,\nincluding virtual assistants, chatbots, content generation,\nand education. However, the widespread usage of LLMs\nbrings forth serious concerns regarding potential copyright\ninfringements. Addressing these challenges is critical for\nthe ethical and legal deployment of LLMs.\nCopyright infringement involves unauthorized usage of\ncopyrighted content, which violates the intellectual prop-\nerty rights of copyright owners, potentially undermining\ncontent creators’ ability to fund their work, and affecting\nthe diversity of creative outputs in society. Additionally,\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nviolators can face legal consequences, including lawsuits\nand financial penalties. For LLMs, copyright infringement\ncan occur through (1) generation of copyrighted content\nduring deployment and (2) illegal usage of copyrighted\nworks during training. Ensuring the absence of copyrighted\ncontent in the vast training datasets of LLMs is challenging.\nMoreover, legal debates around generative AI copyright in-\nfringement vary by region, complicating compliance further.\nCurrent lawsuits against AI companies for unautho-\nrized use of copyrighted content (e.g., Andersen v. Stability\nAI Ltd, NYT v. OpenAI) highlight the urgent need for\nmethods to address these challenges. In this paper, we focus\non studying the effects of watermarking LLMs on two\ncritical issues: (1) preventing the generation of copyrighted\ncontent, and (2) detecting copyrighted content in training\ndata. We show that watermarking can significantly impact\nboth the generation of copyrighted text and the detection of\ncopyrighted content in training data.\nFirstly, we observe that current LLM output water-\nmarking techniques can significantly reduce the probability\nof LLMs generating copyrighted content, by tens of orders\nof magnitude. Our empirical results focus on two recent\nwatermarking methods: UMD (Kirchenbauer et al. 2023)\nand Unigram-Watermark (Zhao et al. 2023). Both methods\nsplit the vocabulary into two sets (green and red) and bias\nthe model towards selecting tokens from the green set\nby altering the logits distribution, thereby embedding a\ndetectable signal. We provide both empirical and theoretical\nresults to support our findings.\nSecondly, we demonstrate that watermarking techniques\ncan decrease the success rate of Membership Inference\nAttacks (MIAs), which aim to detect whether a piece of\ncopyrighted text was part of the training dataset. Since\nMIAs exploit the model’s output, their performance can\nsuffer under watermarking due to changes in the probability\ndistribution of output tokens. Our comprehensive empirical\nstudy, including 5 recent MIAs and 5 LLMs, shows that the\nAUC of detection methods can be reduced by up to 16% in\nthe presence of watermarks.\nFinally, we propose an adaptive method designed to\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n25002\nFigure 1: Illustration of the effect of LLM watermarking on generation of copyrighted content. We observe that watermarking\ncan make it more than 1020 times less likely for Llama-30B to generate copyrighted content.\nenhance the success rate of a recent MIA (Shi et al. 2023)\nin detecting copyright violations under watermarking.\nThis method applies a correction to the model’s output to\naccount for the perturbations introduced by watermarks. By\nincorporating knowledge about the watermarking scheme,\nwe improve the detection performance for pretraining data,\ncounteracting the obfuscation caused by watermarking. Our\ncontribution underscores the importance of continuously\ndeveloping adaptive attack methodologies to keep pace with\nadvances in defense mechanisms.\nThe rest of the paper is organized as follows. In the\n“Related Work” section, we review prior research on LLM\nwatermarking and copyright. The “Setup and Notations”\nsection formally introduces the problems we study. We\nthen present our first two contributions and introduce\nthe adaptive version of the Min-K% Prob membership\ninference attack in the following three sections. Finally, we\nprovide concluding remarks in the last section. Additional\nexperiments, theoretical results, and a discussion on the\nlimitations of our work are included in the appendix 1.\nRelated Work\nWatermarks for LLMs. Language model watermarking\ntechniques embed identifiable markers into output text to\ndetect AI-generated content. Recent strategies incorpo-\nrate watermarks during the decoding phase of language\nmodels (Zhao et al. 2023; Kirchenbauer et al. 2023).\nAaronson (2023) develops the Gumbel watermark, which\nemploys traceable pseudo-random sampling for generating\nsubsequent tokens. Kirchenbauer et al. (2023) splits the\nvocabulary into red and green lists according to preceding\ntokens, biasing the generation towards green tokens. Zhao\net al. (2023) employs a fixed grouping strategy to develop\na robust watermark with theoretical guarantees. Liu et al.\n(2024) proposes to generate watermark logits based on the\npreceding tokens’ semantics rather than their token IDs to\nboost the robustness. Kuditipudi et al. (2023) and Christ,\nGunn, and Zamir (2023) explore watermark methods that\ndo not change the output textual distribution.\nCopyright. Copyright protection in the age of AI has\ngained importance, as discussed by Ren et al. (2024). Vyas,\nKakade, and Barak (2023) addresses content protection\nthrough near access-freeness (NAF) and developed learning\nalgorithms for generative models to ensure compliance\nunder NAF conditions. Prior works focus on training\n1The appendix is available in the arXiv version of the paper\n(https://arxiv.org/abs/2407.17417).\nalgorithms to prevent copyrighted text generation (Vyas,\nKakade, and Barak 2023; Chu, Song, and Yang 2024),\nwhereas our work emphasizes lightweight, inference-time\nalgorithms. Other works have studied copyright in machine\nlearning from a legal perspective. Hacohen et al. (2024)\nutilizes a generative model to determine the generic charac-\nteristics of works to aid in defining the scope of copyright.\nElkin-Koren et al. (2023) demonstrates that copying does\nnot necessarily constitute copyright infringement and\nargues that existing detection methods may detract from the\nfoundational purposes of copyright law.\nAdditionally, we include a discussion on memorization and\nmembership inference in the appendix.\nSetup and Notations\nDefinitions\nLet D be a training dataset, C be all the copyrighted texts,\nand CD be all the copyrighted texts that are part of D. We\ngive definitions for the following setups.\nVerbatim Memorization of Copyrighted Content.\nFor a fixed k ∈ N, Carlini et al. (2022) defines a string\ns as being memorized by a model if s is extractable with\na prompt p of length k using greedy decoding and the\nconcatenation p ⊕ s ∈ D. We adopt a similar definition\nfor verbatim memorization of copyrighted content but\nemploy a continuous metric to measure it. Specifically,\nwe measure verbatim memorization of a text c ∈ C using\nthe perplexity of the model on the copyrighted text cp\nwhen given the prefix p as a prompt (where cp repre-\nsents the text c after removing its prefix p). Note that for\ncp = c(1)\np ⊕ c(2)\np ⊕ ··· ⊕c(n)\np we compute the perplex-\nity using the following formula perplexity(cp|p) =\u0000Qn\ni=1 P(c(i)\np |p ⊕ c(0)\np ⊕ c(1)\np ⊕ ··· ⊕c(i−1)\np\n\u0001− 1\nn , where\nc(0)\np is the empty string. In our experiments, p is either an\nempty string or the first 10, 20, or 100 tokens of c. Lower\nperplexity thereby indicate higher levels of memorization.\nMIAs for Copyrighted Training Data Detection. MIAs\nare privacy attacks aiming to detect whether a sample was\npart of the training set. We define an MIA for copyrighted\ndata as a binary classifier A(·), which ideally outputs\nA(x) = 1, ∀x ∈ CD and A(x) = 0, ∀x ∈ C − CD. In\npractice, A(·) is defined by thresholding a metric (e.g., per-\nplexity), i.e., A(x) = 1, ∀x such that perplexity(x) < t\nand 0, otherwise. Since the threshold t needs to be set, prior\nwork (Shi et al. 2023) uses AUC (Area Under the ROC\n25003\nCurve) as an evaluation metric which is independent of t.\nNote that we employ the same metric in our experiments.\nLLM Watermarking. Watermarking LLMs consists\nof introducing signals during its training or inference that\nare difficult to detect by humans without the knowledge of a\nwatermark key but can be detected using an algorithm if the\nkey is known. We focus our paper on recent methods that\nemploy logits distribution changes as a way of inserting wa-\ntermark signals during the decoding process (Kirchenbauer\net al. 2023; Zhao et al. 2023).\nMIAs\nCurrent MIAs for detecting training data rely on thresh-\nolding various heuristics that capture differences in output\nprobabilities for each token between data included in the\ntraining set and data that was not. Below, we present an\noverview of these heuristics.\nPerplexity. This metric distinguishes between data\nused to train the model (members) and data that was not\n(non-members), as members are generally expected to have\nlower perplexity.\nSmaller Ref, Lowercase and Zlib (Carlini et al. 2021).\nSmaller Ref is defined as the ratio of the log-perplexity\nof the target LLM on a sample to the log-perplexity of a\nsmaller reference LLM on the same sample. Lowercase\nrepresents the ratio of the log-perplexity of the target LLM\non the original sample to the log-perplexity of the LLM on\nthe lowercase version of the sample. Zlib is defined as the\nratio of the log-perplexity of the target LLM on a sample to\nthe zlib entropy of the same sample.\nMin-K% Prob(Shi et al. 2023). This heuristic computes the\naverage of the minimum K% token probabilities outputted\nby the LLM on the sample. Note that this method requires\ntuning K, so in all our experiments we chose the best result\nover K% ∈ {5%,10%, 20%, 30%, 40%, 50%, 60%}.\nLLM Watermarking Methods\nUMD (Kirchenbauer et al. 2023) splits the vocabulary into\ntwo sets (green and red) and biases the model towards the\ngreen tokens by altering the logit distribution. The hash\nof the previous token’s ID serves as a seed for a pseudo-\nrandom number generator used to split the vocabulary into\nthese two groups. For a “hard” watermark, the model is\nforced not to sample from the red list at all. For a “soft”\nwatermark, a positive bias δ is added to the logits of the\ngreen tokens before sampling. We focus our empirical\nevaluation on “soft” watermarks as they are more suitable\nfor LLM deployment due to their smaller impact on the\nquality of the generated text.\nUnigram-Watermark (Zhao et al. 2023) employs a\nsimilar approach of splitting the vocabulary into two sets\nand biasing the model towards one of the two sets. However,\nthe split remains consistent throughout the generation. This\nchoice is made to provide a provable improvement against\nparaphrasing attacks (Krishna et al. 2024).\nWatermarking LLMs Prevents\nCopyrighted Text Generation\nIn this section, we study the effect of LLM watermarking\ntechniques on verbatim memorization. We discuss the their\nimplications for preventing copyrighted text generation.\nDatasets. We consider 4 versions of the WikiMIA\nbenchmark (Shi et al. 2023) with 32, 64, 128, and 256\nwords in each sample and only consider the samples that\nwere very likely part of the training set of all the models\nwe consider (labeled as 1 in Shi et al. (2023)). We consider\nthese subsets as a proxy for text that was used in the training\nset, and the model may be prone to verbatim memorization.\nFrom now on, we refer to this subset as the “training sam-\nples” or “training texts”. Similarly, we consider BookMIA\ndataset (Shi et al. 2023), which contains samples from\ncopyrighted books.\nMetric. We measure the relative increase in perplexity\non the generation of training samples by the watermarked\nmodel compared to the original model. We report the\nincrease in both the minimum and average perplexity over\nthe training samples. Note that a large increase in perplexity\ncorresponds to a large decrease in the probability of gen-\nerating that specific sample, as shown later in this section.\nWhen computing the perplexity, we prompt the model with\nan empty string, the first 10, and the first 20 tokens of the\ntargeted training sample, respectively. In the BookMIA\ndataset, we designate the initial 100 or 256 tokens as the\nprompt. This is because each BookMIA sample contains\n512 words, which is larger than the sample size in WikiMIA.\nModels. We conduct our empirical evaluation on 5\nrecent LLMs: Llama-30B (Touvron et al. 2023), GPT-\nNeoX-20B (Black et al. 2022), Llama-13B (Touvron et al.\n2023), Pythia-2.8B (Biderman et al. 2023) and OPT-2.7B\n(Zhang et al. 2022).\nEmpirical Evaluation\nIn Table 1, we show the increase in perplexity on the\ntraining samples when the model is watermarked relative to\nthe unwatermarked model. We observe that for Llama-30B,\nUnigram-Watermark induces a relative increase of 4.1 in\nthe minimum and 34.1 in the average perplexity. Note that\na relative increase of 4.1 in perplexity for a sample makes\nit more than 4.3 × 1022 times less likely to be generated.\nThis is based on a sample with only 32 tokens, which is\nlikely a lower bound since the number of tokens is typically\nlarger than the number of words. We observe consistent\nresults over several models and prompt lengths. For all\nexperiments, unless otherwise specified, we use a fixed\nstrength parameter δ = 10 for watermark methods and a\nfixed percentage of 50% green tokens. All the results are\naveraged over 5 runs with different seeds for the watermark\nmethods. We include additional results on WikiMIA-64,\n25004\nLlama-30B Llama-13B\nP. Min. Avg. Min. Avg.\n0 3.3 31.2 4.9 34.3\nUMD 10 2.8 28.7 3.5 31.9\n20 2.4 30.1 3.5 33.4\n0 4.1 34.1 5.0 36.6\nUnigram 10 3.0 31.7 4.0 34.3\n20 2.4 31.5 3.4 34.0\nTable 1: Measuring the reduction in verbatim memoriza-\ntion of training texts on WikiMIA-32. We report the relative\nincrease in both the minimum and average perplexity be-\ntween the watermarked and unwatermarked models, where\nlarger values correspond to less memorization. Note that “P.”\nstands for “prompt length”.\nLlama-30B Llama-13B\nP. Min. Avg. Min. Avg.\n0 1.5 33.7 2.4 41.2\nUMD 10 1.5 33.6 2.3 41.0\n20 1.4 33.5 2.3 40.8\n100 1.3 32.9 1.9 40.3\n0 1.6 36.4 2.4 44.5\nUnigram 10 1.6 36.3 2.4 44.3\n20 1.5 36.1 2.3 44.2\n100 1.4 35.5 1.8 43.6\nTable 2: Measuring the reduction in verbatim memoriza-\ntion of training texts on BookMIA. We report the relative\nincrease in both the minimum and average perplexity be-\ntween the watermarked and unwatermarked models, where\nlarger values correspond to less memorization. Note that “P.”\nstands for “prompt length”.\nWikiMIA-128 and WikiMIA-256 in Tables 7, 8 and 9,\nrespectively, in the appendix. We observe that our findings\nare consistent across models and splits of WikiMIA. Finally,\nwe include the complete version of Table 1 in the appendix\n(Table 6), which shows results for additional models and\nrandom logit perturbations with the same strength as the\nwatermarking methods. Overall, the additional results are\nconsistent with our previous findings.\nIn Figure 2, as well as Figure 5 from the appendix,\nwe study the influence of the strength of the watermark δ\non the relative increase in both the minimum and average\nperplexity on the WikiMIA-32 training samples. In this\nexperiment, we also consider a baseline of generating text\nfreely to study the impact of watermarks on the quality of\ntext relative to the impact on training samples’ generation\n(here, perplexity is computed by an unwatermarked model).\nAll the results are averaged over 5 runs with different seeds\nfor the watermark methods. In the case of free generation,\nwe generate 100 samples for 5 different watermarking\nFigure 2: We study how the watermark strength (under the\nUMD scheme) affects the average and the minimum per-\nplexity of training samples from WikiMIA-32, as well as the\nquality of generated text.\nseeds and average the results. The length of the generated\nsamples is up to 42 tokens, which is approximately 32\nwords in the benchmark (on a token-to-word ratio of\n4 : 3). The results show an exponential increase in the\nperplexity of the training samples with the increase in\nwatermark strength, while the generation quality is\naffected at a slower rate. This suggests that even if there\nis a trade-off between protecting the generation of text\nmemorized verbatim and generating high-quality text, find-\ning a suitable watermark strength for each particular\napplication is possible. Examples of generated samples\nat varying watermark strengths are provided in the appendix.\nApproximate Memorization. Informally, we consider\na training sample approximately memorized by a model\nif, given its prefix, it is possible to generate a completion\nthat is similar enough to the ground truth completion. In\nour experiments, we use models fine-tuned on a subset\nof BookMIA (details provided in the appendix) and we\nconsider Normalized Edit Similarity (referred to as edit\nsimilarity from now on) and BLEU score as similarity\nmeasures, as in (Ippolito et al. 2023). Note that we consider\nboth word-level and token-level variants for the BLEU\nscore. The range for each metric is between 0 and 1, where\nvalues close to 1 represent similar texts. In all experiments,\nsince all the samples are 512 words long, we consider the\nfirst 256 words as the prefix and the last 256 words as\nthe ground truth completion. We present the results for\nedit similarity with the UMD watermark in Figure 3, and\n25005\nthe complete results—using all metrics and including the\nUnigram watermark—in Figure 7, averaged over 20 runs\nwith different random seeds. Note that the duplication factor\n(shown on x-axis) represents the number of times the target\ncopyrighted text is duplicated. We observe that for high\nlevels of memorization, a strong watermark significantly\nreduces the similarity between the generated completion\nand the ground truth (copyrighted) one.\nFigure 3: Edit similarity between the generated completion\nand the ground truth when considering different watermark\nstrengths and memorization levels.\nTakeaways. Watermarking significantly increases the\nperplexity of generating training texts, reducing verbatim\nmemorization likelihood. This is achieved with only a\nmoderate impact on the overall quality of generated text.\nThis suggests that watermark strength can be effectively\ntailored to balance verbatim memorization and text quality\nfor specific applications. Finally, we believe that our\nfindings on WikiMIA—which does not necessarily contain\ncopyrighted data—directly extend to the generation of\ncopyrighted text verbatim, as this constitutes a form of\nverbatim memorization of the training data. To confirm, we\nrun similar experiments on a dataset containing copyrighted\ndata (BookMIA) and include the results in the Table 2.\nAdditionally, we consider finetuning Llama-7B (Touvron\net al. 2023) on BookMIA while controlling memorization\nby duplicating training samples. Detailed information about\nthis experiment is provided in the appendix.\nImpact of Watermarking on\nPretraining Data Detection\nDatasets. We revisit the WikiMIA benchmark as discussed\nin the previous section. We consider the full datasets,\nrather than the subset of samples that were part of the\ntraining for models we study. Additionally, we consider\nthe BookMIA benchmark, which contains copyrighted texts.\nMetrics. We follow the prior work (Shi et al. 2023;\nDuarte et al. 2024) and report the AUC and AUC drop to\nstudy the detection performance of the MIAs. Note that this\nmetric has the advantage of not having to tune the threshold\nfor the detection classifier.\nFigure 4: AUC drop due to watermarking for each MIA\nwhen varying the strength of the watermark.\nModels. We conduct experiments on the same LLMs\nas in the previous section. Additionally, for the Smaller\nRef method that requires a smaller reference model along\nwith the target LLM, we consider Llama-7B, Neo-125M,\nPythia-70M, and OPT-350M as references.\nEmpirical Evaluation\nIn Table 4, we show the AUC for the unwatermarked and\nwatermarked models using the UMD scheme, as well as the\ndrop between the two. We observe that watermarking re-\nduces the AUC (drop shown in bold in the table) by up to\n14.2% across 4 detection methods and 5 LLMs. All the ex-\nperiments on watermarked models are run with 5 different\nseeds and we report the mean and standard deviation of the\nresults. We also report the AUC drop, which is computed\nby the difference between the AUC for the unwatermarked\nmodel and the mean AUC over the 5 runs for the water-\nmarked model. Additionally, while the experiments from Ta-\nble 4 are conducted on WikiMIA-256, we observe similar\ntrends for WikiMIA-32, WikiMIA-64, and WikiMIA-128 in\nthe appendix. We also study the impact of the watermark’s\nstrength on the AUC drop for Llama-30B in Figure 4 and\nfor the other models in Figure 6 from the appendix. Note\nthat we considered WikiMIA-256 for these experiments. We\nobserve that higher watermark strengths generally induce\nlarger AUC drops.\nIn addition to the 4 detection methods, we also consider\nSmaller Ref attack, which we include in Table 13 of the\nappendix. We consider different variations, including an un-\nwatermarked reference model and a watermarked one with\na similar strength but a different seed or with both strength\nand seed changed in comparison to the watermarked target\nmodel. The baseline is an unwatermarked model with an\nunwatermarked reference model. We observe the AUC\ndrops in all scenarios (up to 16.4%), which is consistent\nwith our previous findings.\n25006\nLlama-30B Llama-13B\n85.4% 68. 2%\nPPL 84.7 ± 1.4% 67. 6 ± 2.5%\n0.7% 0.6%\n87.9% 77. 6%\nLowercase 80.9 ± 3.1% 67. 2 ± 4.0%\n7.0% 10.4%\n82.5% 62. 5%\nZlib 77.8 ± 1.2% 57. 1 ± 2.0%\n4.7% 5.4%\n85.1% 70. 2%\nMin-K% Prob 85.0 ± 1.0% 68. 5 ± 0.1%\n0.1% 1.7%\nTable 3: AUC of each MIA for the unwatermarked (top of\neach cell), watermarked models (middle of each cell), and\nthe drop between the two (bottomof each cell) on BookMIA\nusing UMD scheme.\nWe also experiment with several percentages of green\ntokens for a fixed watermark strength of δ = 10. We show\nthe results in Table 14 of the appendix. We observe that for\nall models, in at least 80% of the cases all of the attacks’\nAUCs are negatively affected (positive drop value), sug-\ngesting that, in general, finding a watermarking scheme\nthat reduces the success rates of the current MIAs is\nnot a difficult task. Note that the experiments are run on\nWikiMIA for UMD scheme and the results are averaged\nover 5 watermark seeds.\nTakeaways. Watermarking can significantly reduce\nthe success of membership inference attacks (MIAs), with\nAUC drops up to16.4%. By varying the percentage of green\ntokens as well as the watermark’s strength, we observe that\nwatermarking schemes can be easily tuned to negatively\nimpact the detection success rates of MIAs. Finally, we\nconduct experiments on the BookMIA dataset and observe\nresults consistent with our previous findings. These results\nare included in Table 3.\nImproving Detection Performance with\nAdaptive Min-K% Prob\nThis section demonstrates how an informed, adaptive\nattacker can improve the success rate of a recent MIA,\nMin-K% Prob. Our main idea is that an attacker with\nknowledge of the watermarking technique (including\ngreen-red token lists and watermark’s strength δ) can\nreadjust token probabilities. This is possible even without\nadditional information about the logit distribution, relying\nsolely on the probability of each token from the target\nsample given the preceding tokens. Our approach relies on\ntwo key assumptions. First, knowledge of the watermarking\nscheme, which aligns with assumptions made in prior work\non public watermark detection (Kirchenbauer et al. 2023).\nSecond, access to the probability of each token in a sample,\ngiven the previous tokens—an assumption also made by the\nMin-K% Prob method (Shi et al. 2023).\nThreat model. (1) The attacker’s goal is to infer whether\nspecific samples are part of the training set or not. In our\nsetting, the attacker is not malicious, as the goal is to\ndetect copyright violations. (2) Regarding the attacker’s\nknowledge, we assume the attacker knows the watermarking\nmethod and its parameters (green and red lists, and the\nwatermark strength), which aligns with the assumption\nmade in prior work by Kirchenbauer et al. (2023) for public\nwatermark detection. (3) As for the attacker’s capabilities,\nwe assume they can access the probabilities for each token\nin the given samples, similar to what a copyright auditor\nmay have access to. This also mirrors the assumption made\nby Shi et al. (2023) in the context of training data detection.\nOur method described in Algorithm 1 is based on the\nobservation that if the denominator of softmax function\n(i.e., P\ni ezi, where zi is the logit for the i-th vocabulary)\ndoes not vary significantly when generating samples with\nthe watermarked model (and similarly for the unwater-\nmarked model), then we can readjust the probabilities of\nthe green tokens by “removing” the bias δ. More precisely,\nassuming the approximation for the denominator of softmax\nis good, then the probability for each token ti in an unwa-\ntermarked model will be around eLi\nc , where Li is the logit\ncorresponding to the token ti and c is a constant. However,\nfor a watermarked model, if the token ti is green, then the\nprobability would be approximated by eLi+δ\nd , where d is\nagain a constant, while in the case ti is red the probability\nwill be around eLi\nd . To compensate for the bias introduced\nby watermarking, we divide the probability of green tokens\nby eδ and this way we end up with probabilities that are\njust a scaled (by c\nd ) version of the probabilities from the\nunwatermarked model. The scaling factor will not affect the\norders between the samples when computing the average\nof the minimum K% log-probabilities as long as the tested\nsentences are approximately the same length, which is an\nassumption made by Shi et al. (2023) as well.\nDespite the strong assumption we assumed regarding\nthe approximation of the denominator, empirical results\nshow that our method effectively improves the success rate\nof Min-K% under watermarking. We show results for two\nLLMs in Table 5 and include the complete results for 5\nLLMs in Table 17 from the appendix. We observe that our\nmethod improves over the baseline in 95% of the cases, and\nthe increase is as high as 4.8% (averaged over 5 runs).\nFinally, we also consider adaptive versions of the Lowercase\nand Zlib methods. Our findings show that these adaptive\nmethods outperform the baselines in at least 80% of cases.\nDetailed results are provided in the appendix.\nTakeaways. We demonstrate that an adaptive attacker\ncan leverage the knowledge of a watermarking scheme to\nincrease the success rates of recent MIAs.\n25007\nLlama-30B NeoX-20B Llama-13B Pythia-2.8B OPT-2.7B\n72.0% 71. 3% 71. 2% 67. 8% 60. 5%\nPPL 70.6 ± 1.9% 64. 7 ± 2.3% 70. 0 ± 2.6% 64. 4 ± 1.9% 54. 9 ± 2.2%\n1.4% 6.6% 1.2% 3.4% 5.6%\n68.1% 68. 2% 65. 5% 62. 9% 58. 9%\nLowercase 63.8 ± 4.5% 55. 4 ± 5.5% 61. 6 ± 3.8% 58. 7 ± 3.2% 49. 7 ± 2.9%\n4.3% 14.2% 3.9% 4.2% 9.2%\n72.7% 73. 2% 73. 1% 69. 2% 62. 7%\nZlib 72.0 ± 1.6% 66. 6 ± 2.0% 71. 6 ± 2.3% 66. 1 ± 1.2% 58. 1 ± 1.8%\n0.7% 6.6% 1.5% 3.1% 4.6%\n71.8% 78. 0% 72. 9% 71. 0% 65. 5%\nMin-K% Prob 70.5 ± 1.8% 76. 2 ± 2.1% 70. 4 ± 3.2% 69. 5 ± 1.6% 63. 1 ± 3.4%\n1.3% 1.8% 2.5% 1.5% 2.4%\nTable 4: AUC of each MIA for the unwatermarked (top of each cell), watermarked models (middle of each cell), and the drop\nbetween the two (bottom of each cell) on WikiMIA-256 using UMD scheme.\nAlgorithm 1: Adaptive Min-K% Prob\nRequire : Tokenized target sample t = t1 ⊕ t2 ⊕ ... ⊕ tn, access to the probability of the target (watermarked) LLM f\nto generate ti given the i − 1 previous tokens and t0 (empty string) f(ti|t0 ⊕ t1 ⊕ ... ⊕ ti−1) (similar\nassumption as Min-K% Prob algorithm), K, we assume we know the watermarking scheme (e.g., for public\nwatermark detection purposes), i.e. we know the green and red lists as well as δ.\nOutput : Adjusted average of the minimum K% token probabilities when generating t1 ⊕ t2 ⊕ ... ⊕ tn\nadj prob ← {} ▷ The set of adjusted probabilities\nfor i ∈ 1, 2, . . . , ndo\npf (ti) ← f(ti|t0 ⊕ t1 ⊕ ... ⊕ ti−1) ▷ Probability of ti when the model is watermarked\nif ti is green then\nadj prob ← adj prob ∪{pf (ti)\neδ } ▷ Adjust the probability if the token is green\nelse\nadj prob ← adj prob ∪{pf (ti)}\nend\nend\nk = floor (n · K%) ▷ Find the number of token probabilities to keep\nadj k prob ← min k(adj prob) ▷ Select the minimum k probabilities\nreturn mean(log(adj k prob) ) ▷ Return the mean of the minimum k log-probabilities\nLlama-30B Llama-13B\nWikiMIA Not adapt. 66.2% 64. 5%\n32 Adapt. 68.5% 66.3%\nWikiMIA Not adapt. 64.4% 62. 8%\n64 Adapt. 67.3% 64.9%\nWikiMIA Not adapt. 70.0% 68. 9%\n128 Adapt. 73.1% 71.0%\nWikiMIA Not adapt. 70.5% 70. 4%\n256 Adapt. 71.3% 72.4%\nTable 5: We show the AUC of Min-%K Prob (referred as\n“Not adapt.”) and our method (referred as “Adapt.”) when\nusing UMD watermarking scheme. We highlight the cases\nwhen our method improves over the baseline.\nConclusion and Discussion\nWatermarking LLMs has unintended consequences on\nmethods towards copyright protection. Our experiments\ndemonstrate that while watermarking may be a promising\nsolution to prevent copyrighted text generation, watermark-\ning also complicates membership inference attacks that may\nbe employed to detect copyright abuses. Watermarking can\nbe a double-edged sword for copyright regulators since it\npromotes compliance during generation time, while making\ntraining time copyright violations harder to detect. We hope\nour work furthers the discussion around watermarking and\ncopyright issues for LLMs.\nAcknowledgements\nPanaitescu-Liess, Che, An, Xu, Pathmanathan, Chakraborty,\nZhu, and Huang are supported by DARPA Transfer from Im-\nprecise and Abstract Models to Autonomous Technologies\n25008\n(TIAMAT) 80321, National Science Foundation NSF-IIS-\n2147276 FAI, DOD-AFOSR-Air Force Office of Scientific\nResearch under award number FA9550-23-1-0048, Adobe,\nCapital One and JP Morgan faculty fellowships.\nReferences\nAaronson, S. 2023. Simons institute talk on watermarking\nof large language models.\nBiderman, S.; Schoelkopf, H.; Anthony, Q. G.; Bradley,\nH.; O’Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.;\nPrashanth, U. S.; Raff, E.; et al. 2023. Pythia: A suite for an-\nalyzing large language models across training and scaling.\nIn International Conference on Machine Learning, 2397–\n2430. PMLR.\nBlack, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.;\nGolding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.;\net al. 2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nCarlini, N.; Ippolito, D.; Jagielski, M.; Lee, K.; Tramer, F.;\nand Zhang, C. 2022. Quantifying memorization across neu-\nral language models. arXiv preprint arXiv:2202.07646.\nCarlini, N.; Tramer, F.; Wallace, E.; Jagielski, M.; Herbert-\nV oss, A.; Lee, K.; Roberts, A.; Brown, T.; Song, D.; Erlings-\nson, U.; et al. 2021. Extracting training data from large\nlanguage models. In 30th USENIX Security Symposium\n(USENIX Security 21), 2633–2650.\nChrist, M.; Gunn, S.; and Zamir, O. 2023. Unde-\ntectable watermarks for language models. arXiv preprint\narXiv:2306.09194.\nChu, T.; Song, Z.; and Yang, C. 2024. How to Protect Copy-\nright Data in Optimization of Large Language Models? In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 38, 17871–17879.\nDuarte, A. V .; Zhao, X.; Oliveira, A. L.; and Li, L. 2024. DE-\nCOP: Detecting Copyrighted Content in Language Models\nTraining Data. arXiv preprint arXiv:2402.09910.\nElkin-Koren, N.; Hacohen, U.; Livni, R.; and Moran, S.\n2023. Can Copyright be Reduced to Privacy?arXiv preprint\narXiv:2305.14822.\nHacohen, U.; Haviv, A.; Sarfaty, S.; Friedman, B.; Elkin-\nKoren, N.; Livni, R.; and Bermano, A. H. 2024. Not All\nSimilarities Are Created Equal: Leveraging Data-Driven Bi-\nases to Inform GenAI Copyright Disputes. arXiv preprint\narXiv:2403.17691.\nIppolito, D.; Tram`er, F.; Nasr, M.; Zhang, C.; Jagielski, M.;\nLee, K.; Choquette-Choo, C. A.; and Carlini, N. 2023. Pre-\nventing generation of verbatim memorization in language\nmodels gives a false sense of privacy. In Proceedings of\nthe 16th International Natural Language Generation Con-\nference, 28–53. Association for Computational Linguistics.\nKirchenbauer, J.; Geiping, J.; Wen, Y .; Katz, J.; Miers, I.;\nand Goldstein, T. 2023. A watermark for large language\nmodels. In International Conference on Machine Learning,\n17061–17084. PMLR.\nKrishna, K.; Song, Y .; Karpinska, M.; Wieting, J.; and Iyyer,\nM. 2024. Paraphrasing evades detectors of ai-generated text,\nbut retrieval is an effective defense. Advances in Neural In-\nformation Processing Systems, 36.\nKuditipudi, R.; Thickstun, J.; Hashimoto, T.; and Liang, P.\n2023. Robust distortion-free watermarks for language mod-\nels. arXiv preprint arXiv:2307.15593.\nLiu, A.; Pan, L.; Hu, X.; Meng, S.; and Wen, L. 2024. A\nSemantic Invariant Robust Watermark for Large Language\nModels. In The Twelfth International Conference on Learn-\ning Representations.\nRen, J.; Xu, H.; He, P.; Cui, Y .; Zeng, S.; Zhang, J.; Wen,\nH.; Ding, J.; Liu, H.; Chang, Y .; et al. 2024. Copyright Pro-\ntection in Generative AI: A Technical Perspective. arXiv\npreprint arXiv:2402.02333.\nShi, W.; Ajith, A.; Xia, M.; Huang, Y .; Liu, D.; Blevins,\nT.; Chen, D.; and Zettlemoyer, L. 2023. Detecting pre-\ntraining data from large language models. arXiv preprint\narXiv:2310.16789.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971.\nVyas, N.; Kakade, S. M.; and Barak, B. 2023. On provable\ncopyright protection for generative models. In International\nConference on Machine Learning, 35277–35299. PMLR.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhao, X.; Ananth, P.; Li, L.; and Wang, Y .-X. 2023. Provable\nrobust watermarking for ai-generated text. arXiv preprint\narXiv:2306.17439.\n25009",
  "topic": "Digital watermarking",
  "concepts": [
    {
      "name": "Digital watermarking",
      "score": 0.7883747816085815
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6751497387886047
    },
    {
      "name": "Computer science",
      "score": 0.6281895637512207
    },
    {
      "name": "Computer security",
      "score": 0.44686055183410645
    },
    {
      "name": "Training set",
      "score": 0.4247279167175293
    },
    {
      "name": "Language model",
      "score": 0.4181053638458252
    },
    {
      "name": "Internet privacy",
      "score": 0.37917229533195496
    },
    {
      "name": "Natural language processing",
      "score": 0.3763999342918396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34523332118988037
    },
    {
      "name": "Image (mathematics)",
      "score": 0.13056045770645142
    },
    {
      "name": "Geography",
      "score": 0.07963263988494873
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}