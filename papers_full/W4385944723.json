{
  "title": "Gromov-Wasserstein unsupervised alignment reveals structural correspondences between the color similarity structures of humans and large language models",
  "url": "https://openalex.org/W4385944723",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Kawakita, Genji",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Zeleznikow-Johnston, Ariel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2317985547",
      "name": "Tsuchiya Naotsugu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565512717",
      "name": "Oizumi Masafumi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2140053249",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2000512760",
    "https://openalex.org/W2564693100",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4316015337",
    "https://openalex.org/W4386796473",
    "https://openalex.org/W2417433140",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W3022227446",
    "https://openalex.org/W4385727593",
    "https://openalex.org/W4282053625",
    "https://openalex.org/W4221159014",
    "https://openalex.org/W4229796189",
    "https://openalex.org/W4287245321",
    "https://openalex.org/W3092472394",
    "https://openalex.org/W4402512774",
    "https://openalex.org/W4286892233",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W4313706437",
    "https://openalex.org/W2153633111",
    "https://openalex.org/W2793477525",
    "https://openalex.org/W2963472233",
    "https://openalex.org/W2027842533",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Large Language Models (LLMs), such as the General Pre-trained Transformer (GPT), have shown remarkable performance in various cognitive tasks. However, it remains unclear whether these models have the ability to accurately infer human perceptual representations. Previous research has addressed this question by quantifying correlations between similarity response patterns of humans and LLMs. Correlation provides a measure of similarity, but it relies pre-defined item labels and does not distinguish category- and item- level similarity, falling short of characterizing detailed structural correspondence between humans and LLMs. To assess their structural equivalence in more detail, we propose the use of an unsupervised alignment method based on Gromov-Wasserstein optimal transport (GWOT). GWOT allows for the comparison of similarity structures without relying on pre-defined label correspondences and can reveal fine-grained structural similarities and differences that may not be detected by simple correlation analysis. Using a large dataset of similarity judgments of 93 colors, we compared the color similarity structures of humans (color-neurotypical and color-atypical participants) and two GPT models (GPT-3.5 and GPT-4). Our results show that the similarity structure of color-neurotypical participants can be remarkably well aligned with that of GPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to the methodological advancements of comparing LLMs with human perception, and highlight the potential of unsupervised alignment methods to reveal detailed structural correspondences. This work has been published in Scientific Reports, DOI: https://doi.org/10.1038/s41598-024-65604-1.",
  "full_text": "Gromov-Wasserstein unsupervised alignment reveals structural\ncorrespondences between the color similarity structures of\nhumans and large language models\nGenji Kawakita1*, Ariel Zeleznikow-Johnston2,3, Naotsugu Tsuchiya2,3,4,5,\nMasafumi Oizumi6*\n1Department of Bioengineering, Imperial College London, London, UK.\n2School of Psychological Sciences, Monash University, Melbourne, Australia.\n3Turner Institute for Brain and Mental Health, Monash University, Melbourne, Australia.\n4Center for Information and Neural Networks (CiNet), National Institute of Information\nand Communications Technology (NICT), Osaka, Japan.\n5Department of Qualia Structure, ATR Computational Neuroscience Laboratories, Kyoto,\nJapan.\n6Graduate School of Arts and Science, The University of Tokyo, Tokyo, Japan.\n*Corresponding author(s). E-mail(s): g.kawakita22@imperial.ac.uk;\nc-oizumi@g.ecc.u-tokyo.ac.jp;\nContributing authors: ariel.zeleznikow-johnston@monash.edu;\nnaotsugu.tsuchiya@monash.edu;\nAbstract\nLarge Language Models (LLMs), such as the General Pre-trained Transformer (GPT), have shown\nremarkable performance in various cognitive tasks. However, it remains unclear whether these models\nhave the ability to accurately infer human perceptual representations. Previous research has addressed\nthis question by quantifying correlations between similarity response patterns of humans and LLMs. Cor-\nrelation provides a measure of similarity, but it relies pre-defined item labels and does not distinguish\ncategory- and item- level similarity, falling short of characterizing detailed structural correspondence\nbetween humans and LLMs. To assess their structural equivalence in more detail, we propose the use of\nan unsupervised alignment method based on Gromov-Wasserstein optimal transport (GWOT). GWOT\nallows for the comparison of similarity structures without relying on pre-defined label correspondences\nand can reveal fine-grained structural similarities and differences that may not be detected by simple\ncorrelation analysis. Using a large dataset of similarity judgments of 93 colors, we compared the color\nsimilarity structures of humans (color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of color-neurotypical participants\ncan be remarkably well aligned with that of GPT-4 and, to a lesser extent, to that of GPT-3.5.These\nresults contribute to the methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed structural correspondences.\nThis work has been published in Scientific Reports, DOI: 10.1038/s41598-024-65604-1.\nKeywords: Large Language Models, unsupervised alignment, Gromov-Wasserstein optimal transport, color\nsimilarity structures\nLarge Language Models (LLMs) have demonstrated remarkable performance in a variety of cognitive tasks\n[1–3]. These LLMs, based on the Transformer architecture, use self-attention mechanisms to effectively\nprocess and generate sequences of data [4]. Among LLMs, the General Pre-trained Transformer (GPT) series\ndeveloped by OpenAI has received considerable public attention with the introduction of the ChatGPT\nconversational interface [5, 6]. Recently introduced GPT models can generate human-like responses to\nprompts, and are reported to excel at tasks assessing Theory of Mind abilities [7].\n1\narXiv:2308.04381v3  [q-bio.NC]  13 Aug 2024\nThese observations raise two intriguing questions: To what extent can Large Language Models accurately\ninfer human perceptual representations, and how can we effectively compare LLMs and human perceptual\nrepresentations? Previous research addressed this by comparing human similarity judgments with those\ngenerated by GPT across various modalities [8]. They found correlations as high as ρ = 0.8 between human\nand LLM color similarity judgments.\nTo evaluate the similarity between representational structures, a supervised approach known as Repre-\nsentational Similarity Analysis (RSA) has been widely used in neuroscience to compare different similarity\nmatrices obtained from behavioral, neural, and neural network model data [9, 31]. The supervised approach\n(or supervised alignment method in general [17]) assumes that an element in one similarity structure (for\nexample, the color ’red’) corresponds to the same element in another similarity structure. It then quantifies\nthe degree of similarity between the different structures, assuming a one-to-one correspondence as defined\nby the item labels. Previous studies comparing the similarity structures of humans and LLMs also use this\nsupervised approach [8, 15, 16].\nWhile high correlations suggest remarkable representational similarity between humans and LLMs, inter-\npreting their significance is challenging. First, correlation values lack appropriate controls, such as simple\ncolor space models (e.g., RGB or LAB), for comparison. These controls serve as baselines to determine\nwhether the high correlations between human and LLM similarity judgments are truly indicative of sophis-\nticated representational similarity or if they can be achieved by simpler models. Second, high correlations\nmay indicate only coarse category-level alignment without capturing fine-grained structural correspondence\n[9, 10].\nRegarding the first point, simple correlation can only be interpreted relative to other representational\nmodels, especially because simple correlation does not provide an absolute measure of representational\nequivalence. For example, even if correlation values in the range of 0.7-0.8 seem impressively high in the\ncontext of a color similarity judgment task, this does not necessarily mean that such values can only be\nachieved by sophisticated neural network models such as GPT. If simpler color space models, such as RGB or\nLAB, can achieve similar correlation levels with human judgments, the significance of a high human-to-GPT\ncorrelation becomes less pronounced.\nRegarding the second point, simple correlation does not necessarily entail a fine-item-level structural\ncorrespondence between two similarity structures. For example, previous studies using representational\nsimilarity analysis (RSA) [9], a common method for assessing perceptual representational similarity via\nsimple correlations between similarity matrices, have shown that high correlation values (e.g.,ρ = 0.95) may\nindicate only coarse category-level correspondence, even while fine-item-level alignment is completely absent\n(e.g., Fig. 3 in [10]). Thus, the mere presence of a high correlation does not clarify whether the structures\nhave a fine-item-level alignment or simply a coarse-category-level correspondence.\nTo address these limitations, we propose using an unsupervised alignment approach to assess a more\ndetailed level of structural correspondence between the similarity structures of humans and LLMs. In unsu-\npervised alignment, the correspondence between items in two similarity structures is not assumed. Instead,\nthe correspondences need to be discovered through an alignment procedure (Fig. 1a), since information about\nexternal item labels is not used. After alignment, external labels are used only to evaluate the alignment\n(Fig. 1b).\nFor unsupervised alignment, Gromov-Wasserstein optimal transport (GWOT) [11] (Fig. 1c) has recently\nemerged as a promising method for comparing and aligning similarity structures. GWOT has been success-\nfully applied in various contexts, such as aligning word embedding spaces across languages [18], single-cell\nmulti-omics data [19]. The unsupervised optimal transport method has revealed the structural correspon-\ndence of the similarity structures of colors across individuals [14] and objects [10], facilitating a broad\nstructural exploration of human perceptual structures. These advances in unsupervised alignment tech-\nniques provide new ways to understand the extent to which LLMs can accurately infer human perceptual\nstructures.\nUsing GWOT, we compared the color similarity structures of color-neurotypical and color-atypical\nhuman participants with those of GPT-3.5, GPT-4, and color space models. Our larger 93-color dataset,\ncompared to the 23 colors in [8], allows studying higher-dimensional color similarity. We also contrasted\nGPT-4 and GPT-3.5 to explore the effects of visual input integration and model/data size. As baselines, we\nconsidered color-atypical individuals and simple color space models (RGB and LAB). The inclusion of RGB\nand LAB is important to rule out the possibility that GPT responses are based solely on these models.\nOur results show that the color similarity structure of color-neurotypical participants can be remarkably\nwell aligned with that of GPT-4 and, to a lesser extent, with that of GPT-3.5. In contrast, the color sim-\nilarity structures of color-neurotypical participants could not be aligned with those of color space models,\ndespite reasonably high correlation values. These findings suggest a strong fine-item-level structural cor-\nrespondence between color-neurotypical human participants and the recent GPT models, but not between\n2\nFig. 1 Schematic of unsupervised alignment. (a) Unsupervised alignment of similarity structures without external\nlabels, based only on similarity relations. (b) Evaluation of unsupervised alignment using external labels. (c) Schematic of\nGromov-Wasserstein optimal transport. The elements of matrices D and D′ are the dissimilarities between the items. Γ is the\ntransportation matrix, where each element indicates the probability of an item in one similarity structure corresponding to\nanother item in the other similarity structure. Modified from [14].\ncolor-neurotypical human participants and color-space models. The results provide insights into LLMs’ abil-\nity to capture human color perception and demonstrate the utility of unsupervised alignment methods in\nrevealing detailed structural similarities and differences between human and LLM representations.\nResults\nColor dissimilarity matrices\nIn this study, we compared the similarity structures of 93 colors obtained from human participants (color-\nneurotypical and color-atypical participants) with those of large language models (GPT-3.5 and GPT-4). As\nfor the human participants data, we used a large-scale dataset including 426 color-neurotypical participants\nand 257 color-atypical participants, which we previously collected [14]. Color-atypical participants in this\nstudy refer to individuals with red-green color blindness, who were screened using a modified online Ishihara\ntest (see supplementary material for details on inclusion criteria and screening procedure). In the color\nsimilarity judgement experiment, human participants were asked to rate the perceived similarity between\npairs of colors drawn from a set of 93 color stimuli. For each pair, participants provided a rating on a scale\nfrom 0 to 7, with 0 indicating that the colors were perceived as very similar and 7 indicating that the colors\nwere perceived as very different. The details of the experimental design and procedure can be found in the\nsupplementary information. We obtained the dissimilarity matrices of 93 colors for the color-neurotypical\nand color-atypical participants group by simply averaging the similarity judgement responses for each color\npair from all the participants in each participant group as shown in Fig. 2.\nTo obtain responses from GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4-0314), we used a prompt that\nrepresented each color as a HEX code in line with a previous study [8] (see Methods for the details). By\ncollecting a complete set of similarity judgements of 93 colors with the same prompt, we obtained the\ndissimilarity matrices of GPT-3.5 and GPT-4 as shown in Fig. 2. By visual inspection, we can clearly see\nthat the dissimilarity matrix of GPT-4 is very similar to that of the color-neurotypical participants.\nTo examine the possibility that LLMs rely on established color space models when judging color simi-\nlarity, we computed the dissimilarity matrices for the 93 colors using two representative color space models,\nRGB and LAB color space models, as shown in Fig. 2 (see Methods for the details). These simple color\nspace models served as baselines to evaluate how well large language models can approximate the human\ncolor similarity structures.\n3\nFig. 2 Color dissimilarity matrices.Displayed are the dissimilarity matrices of 93 colors from the color-neurotypical\nparticipants group, the color-atypical participants group, GPT-4, GPT-3.5, and the RGB and LAB color space models. All\nmatrices are normalized to have values between 0 and 1, where 0 means the no difference between colors and 1 means the\nmaximum difference for each dissimilarity matrix.\nCorrelations between color dissimilarity matrices\nBefore evaluating the structural correspondences of the color dissimilarity matrices between humans and\nLLMs as per the unsupervised alignment analysis, we first computed the similarity between them by sim-\nply computing correlations (the Spearman correlation) between them. This analysis corresponds to the\nconventional representational similarity analysis [9], which implicitly assumes correspondence between the\nsame colors across different similarity structures. Fig. 3a summarizes the correlations between the simi-\nlarity matrices of the color-neurotypical participants group, the color-atypical participants group, GPT-4,\nGPT-3.5, and the RGB and LAB color spaces.\nThis analysis yields the following three findings:\n1. The dissimilarity matrix of the human color-neurotypical participant group is the closest to that of GPT-\n4 (ρ = 0.77) among the models considered (GPTs and color space models). In addition, other models\nsuch as GPT-3.5 ( ρ = 0.62), the RGB color space model ( ρ = 0.60), and the LAB color space model\n(ρ = 0.71) also show reasonable correlation with the similarity structure of the color-neurotypical group.\n2. GPT-3.5 shows lower correlations with the dissimilarity matrix of the human color-neurotypical group\nthan GPT-4. In addition, the dissimilarity matrix of GPT-3.5 shows lower correlations with the color\nspace models than GPT-4.\n3. The human color-atypical group shows relatively low correlations with the other dissimilarity matrices,\nsuggesting that the similarity structure of the color-atypical group is significantly different from that of\nthe the color-neurotypical group, the GPTs, and the color space models.\nThe scatter plots of similarity ratings of all the pairs of the dissimilarity matrices are available in Fig. S1,\nproviding a visual aid for understanding the correlation trends between each pair of groups.\nUnsupervised alignment of color similarity structures\nWe then evaluated the extent to which the color similarity structures of humans and LLMs could be aligned\nin an unsupervised manner using the Gromov-Wasserstein Optimal Transport (GWOT) algorithm. In Fig.\n3b, we summarized the matching rates of the unsupervised GWOT alignment between all pairs of the\ndissimilarity matrices shown in Fig. 2. In the following, the results of the most important pairs (color-\nneurotypical vs. GPT-4, GPT-3.5, LAB) are explained in more detail. The detailed results of the other pairs\nare shown in the Supplementary Figs. S2 and S3.\n4\nFig. 3 Evaluating the similarity of similarity structures in a supervised and unsupervised method.(a) Con-\nventional representational similarity analysis based on Spearman correlations. Spearman correlations between the dissimilarity\nmatrices obtained from the color-neurotypical participants group (abbreviated by TYP), the color-atypical participants group\n(abbreviated by ATYP), GPT-4, GPT-3.5, and the RGB and LAB color spaces are shown. (b) Matching rates of unsupervised\nalignment based on GWOT between the dissimilarity matrices.\nUnsupervised alignment with GPT-4\nFirst, we showed the results of the unsupervised alignment between the color similarity structures of the\nhuman color-neurotypical participants and GPT-4 in Fig. 4. We applied entropic GWOT to the two dis-\nsimilarity matrices shown in Fig. 4a. Since entropic GWOT is a non-convex optimization problem involving\nhyperparameter search of ϵ, which controls the degree of entropy regularization, we performed a total of\n500 optimization iterations with different ϵ values and initialization of transportation plans to search for\na global optimum. The points in Fig. 4b correspond to the local minimum found in each iteration of the\noptimization performed on different ϵ values. Across different ϵ values, we selected the local minimum with\nthe lowest GWD as the optimal solution (indicated by the red circle in Fig. 4b).\nFrom the optimization process, we obtained the optimal transportation plan Γ between the human color-\nneurotypical participants and GPT4 (Fig. 4c). As shown in Fig. 4c, most of the diagonal elements in Γ\nhave high values, indicating that most of the colors in the color-neurotypical participants correspond with\na high probability to the same colors in GPT-4. To quantitatively assess the degree of correspondence, we\ncomputed the matching rate of the 93 colors (see Methods for details), which was 91.4% (Fig. 3b). As can be\nseen in Fig. 4b, the local minima with low GWD (in the y-axis) tend to yield a high matching rate (points\nwith yellowish color), which is necessary for unsupervised alignment to achieve a high matching rate.\nTo visually inspect the degree of the unsupervised alignment, we draw the embeddings of the color-\nneurotypical participants and the aligned embeddings of GPT-4 in Fig. 4d (See Supplementary Movies S1\nfor the animation of the aligned embeddings). As detailed in Methods, we aligned the embeddings of GPT-\n4 to those of the color-neurotypical participants by solving a Procrustes-type problem using the optimized\ntransportation plan Γ obtained through GWOT. Each color represents the label of a corresponding external\ncolor stimulus. Note that even though the color labels are shown in Fig. 4d, this is only for the visualization\npurpose and the whole alignment procedure is performed in a purely unsupervised manner without relying\non the color labels. As depicted in Fig. 4d, identical colors from the color-neurotypical participants and GPT-\n4 are located in close proximity to each other. This shows that GPT-4 has a color similarity structure that\nis strikingly similar to that of the color-neurotypical participants, allowing for the successful unsupervised\nalignment.\nWhile the main results presented above were obtained using GPT-4 with text input, we also observed\nqualitatively similar results when using GPT-4 Vision, which takes visual input (color patches) instead of\ntext descriptions. The unsupervised alignment between GPT-4 Vision and color-neurotypical participants\nrevealed a high degree of structural similarity in their color representations, albeit with a slightly lower\nmatching rate compared to GPT-4 with text input. See Supplementary Text 2 and Supplementary Figures\nS4 and S5 for detailed results and visualizations of the GPT-4 Vision analysis.\nUnsupervised alignment with GPT-3.5\nNext, for comparison with GPT-4, we showed the results of the unsupervised alignment between the color\nsimilarity structures of the human color-neurotypical participants and GPT-3.5 in Fig. 5 (See Supplementary\n5\nFig. 4 Unsupervised alignment between the color similarity structure of the human color-neurotypical par-\nticipants and that of GPT-4. (a) Dissimilarity matrices of 93 colors from the human color-neurotypical participants\n(abbreviated by TYP) and GPT-4. (b) The optimization results over 500 iterations with different ϵ values. GWD values of\nlocal minima represented by points are shown with respect to ϵ. Colors represent the matching rate of unsupervised alignment.\n(c) Optimal transportation plan Γ between the dissimilarity matrices of TYP and GPT-4. (d) Aligned embeddings of TYP\nand GPT-4 plotted in the embedded space of TYP.\nMovies S1 for the animation of the aligned embeddings). The results are presented in the same format as\nFig. 4 and the analysis procedure is also the same as explained in the previous section.\nIn contrast to the case of GPT-4, we found that the matching rate of the optimal solution (shown in the\nred circle in Fig. 5b) is much lower, 11.8%, than that of the GPT-4, 91.4%. However, this is still significantly\nhigher than the chance level (1.08%). We can also see that the optimal transportation plan Γ in Fig. 5c is\n“roughly” diagonal, i.e., the diagonal elements or neighboring elements to diagonal elements of Γ tend to\nhave large values. This roughly diagonal appearance of Γ means that similar colors correspond to each other\nbetween the color-neurotypical participants and GPT-3.5 with a high probability. This is also confirmed by\nthe aligned embeddings shown in Fig. 5d, where the embeddings of similar colors from the color-neurotypical\nparticipants and GPT-3.5 are located close together. These results indicate that the similarity matrix of\nGPT-3.5, although less well aligned with the color-neurotypical similarity matrix than GPT-4, contains\nsome structural features that can be aligned with the color-neurotypical similarity matrix.\nUnsupervised alignment with color space models\nTo provide a baseline comparison, we showed the results of the unsupervised alignment between the color\nsimilarity structures of the human color-neurotypical participants and the LAB color space model in Fig. 6\n(See Supplementary Movies S1 for the animation of the aligned embeddings). The results are presented in\nthe same format as Figs. 4 and 5.\nIn contrast to the both cases of GPT-4 and GPT-3.5, we found that the matching rate of the optimal\nsolution (shown in the red circle in Fig. 6b) is very low, 4.30%, which is close to the chance level (1.08%). We\nalso found that the appearance of the optimal transportation plan Γ (Fig. 6c) is qualitatively different from\nthose of GPT-4 (Fig. 4c) and GPT-3.5 (Fig. 5c). The optimal transportation plan (Fig. 6c) is not lined up\ndiagonally, i.e., the diagonal elements or the neighboring elements to the diagonal elements of Γ are small.\nThe aligned embeddings shown in Fig. 6d are also quite different from those of GPT-4 (Fig. 4d) and GPT-\n3.5 (Fig. 5d), i.e., the embeddings of similar colors are not closely positioned, indicating that similar colors\nare not correctly aligned by the unsupervised alignment. We also obtained the similar results for the RGB\ncolor space model. The matching rate between the color-neurotypical participants and RGB is 5.38% (Fig.\n3b) and the optimal transportation plan Γ does not show roughly diagonal appearance (Supplementary Fig.\nS2).\nUnsupervised alignment with color-atypical participants\nAs another negative control, we also showed the results of the unsupervised alignment with the color-\natypical participants in Supplementary Fig. S2 and Fig. 3b. As shown in Fig. 3b, the similarity structure of\nthe color-atypical participants is not aligned with either GPT-4 or the color-neurotypical participants (the\nmatching rate is 1.08% and 7.53%, respectively). Note, however, that the correlations between the color-\natypical participants and GPT-4 and the color-neurotypical participants are reasonably high, ρ = 0.67 and\nρ = 0.57, respectively (Fig. 3a). The subtle structural difference caused by red-green color deficiency is likely\nto prevent the successful unsupervised alignment between the color-atypical participants and GPT-4 and\nthe color-neurotypical participants (see also [14]).\n6\nFig. 5 Unsupervised alignment between the color similarity structure of the human color-neurotypical par-\nticipants and that of GPT-3.5. (a) Dissimilarity matrices of 93 colors from the human color-neurotypical participants\n(abbreviated by TYP) and GPT-3.5. (b) The optimization results over 500 iterations with different ϵ values. GWD values of\nlocal minima represented by points are shown with respect to ϵ. Colors represent the matching rate of unsupervised alignment.\n(c) Optimal transportation plan Γ between the dissimilarity matrices of TYP and GPT-3.5. (d) Aligned embeddings of TYP\nand GPT-3.5 plotted in the embedded space of TYP.\nComparison between conventional representational similarity analysis and\nunsupervised alignment\nFinally, we mention several important differences between the results of conventional representational simi-\nlarity analysis and the unsupervised alignment based on GWOT. Comparing the Figs. 3a and b, we observe\nthat the unsupervised alignment method was able to reveal more nuanced structural differences that were\nnot observable using conventional representational similarity analysis. For example, the Spearman correla-\ntions between the color-neurotypical participants and the color space models (ρ = 0.60 for RGB andρ = 0.71\nfor LAB) are reasonably high. In particular, the correlation of LAB ( ρ = 0.71) is close to the correlation\nof GPT-4 ( ρ = 0.77) and higher than the correlation of GPT-3 ( ρ = 0.62). However, as we showed in the\nprevious section, the matching rates of the unsupervised alignment between the color-neurotypical partici-\npants and the color space models are very low (5.38% for RGB and 4.30% for LAB), almost at chance level.\nThis suggests that human color similarity structures are not adequately captured by color space models,\nbut are remarkably well captured by GPT-4 and is captured to some extent by GPT-3.5. Such nuanced\nstructural differences between the human color similarity structure, the color space models, and GPT-4 or\nGPT-3.5 cannot be detected by conventional representational similarity analysis, which is based on simple\ncorrelation between similarity structures.\nDiscussion\nThe primary objective of our work was to present a methodological advancement beyond simple correlation,\nbrought by unsupervised alignment technique, Gromov-Wasserstein Optimal Transport (GWOT), for com-\nparing the color similarity structures of humans and large language models (LLMs). Unlike previous studies\nwith simple correlation analysis, our GWOT technique revealed more nuanced structural similarities and\ndifferences.\nSpecifically, among the models considered (GPT-4, GPT-3.5, RGB, and LAB), GPT-4 had a color sim-\nilarity structure that most closely resembled that of the color-neurotypical participants with the highest\nmatching rate with the color-neurotypical participants (91.4%). Compared to GPT-4, GPT-3.5 is less well\naligned with the color-neurotypical participants, but it still demonstrates a significant degree of alignment\n(11.8%), outperforming the RGB and LAB color space models. Despite reasonably high correlation coef-\nficients ( ρ = 0 .60 for RGB, ρ = 0 .71 for LAB), somewhat surprisingly, the similarity structures of the\ncolor space models could not be aligned with that of human color-neurotypical participants in an unsuper-\nvised manner (5.38% for RGB and 4.30% for LAB). These results indicate that our unsupervised alignment\nmethod can reveal nuanced structural similarities and differences between the similarity structures that are\nnot discernible by simple correlation analysis. The qualitative difference between the GPTs and the color\nspace models means that neither GPT-4 nor GPT-3.5 similarity judgments are the simple reflections of the\nRGB and LAB color space models. Rather, GPTs reflect something learned from massive textual data, or\nthe combination of textual and visual data in the case of GPT-4. Despite our finding that color similar-\nity structure of GPT-4 is remarkably well aligned with that of human color-neurotypical participants, it\nremains unclear whether GPT-4 maintains similar internal representations of color to humans. A valuable\ndirection for future research would be to directly extract the embeddings of colors in GPT-4 and evaluate\nthe similarity structures computed as distance matrices between these embeddings. However, note that the\nGPT-4 embeddings were not available at this time (January 2024).\n7\nFig. 6 Unsupervised alignment between the color similarity structure of the human color-neurotypical partic-\nipants and that of LAB. (a) Dissimilarity matrices of 93 colors from the human color-neurotypical participants (abbreviated\nby TYP) and LAB. (b) The optimization results over 500 iterations with different ϵ values. GWD values of local minima rep-\nresented by points are shown with respect to ϵ. Colors represent the matching rate of unsupervised alignment. (c) Optimal\ntransportation plan Γ between the dissimilarity matrices of TYP and LAB. (d) Aligned embeddings of TYP and LAB plotted\nin the embedded space of TYP.\nFor the GPT-human comparison studies including this study [8], it is important to consider the potential\ninfluence of cultural factors, such as language. This consideration is important even in the case of color\ndiscrimination and categorization [30]. While the exact details of GPT’s training data are not publicly\navailable, it is presumed that the model was primarily trained on English language data. Similarly, the human\nparticipants in our study were recruited from an English-speaking region. Given that both the GPT models\nand the human participants were from the same language/cultural background, it is possible that this shared\nbackground contributed to the strong alignment of their color similarity structures. Future research could\nexplore how color similarity structures may differ across cultures and how the alignment between human\nand LLM color perception might be affected by cultural factors. Additionally, investigating the performance\nof LLMs trained on data from diverse cultural and linguistic backgrounds could provide insights into the\nrole of culture in shaping color perception and categorization in both humans and AI models.\nWhile this study focused on comparing the similarity structures of colors as an initial tractable attempt,\nfuture research could explore other sensory modalities across a broader range of tasks (e.g., visual object\nsimilarity judgment tasks [23, 24]). This could provide a more comprehensive understanding of the extent to\nwhich large language models accurately capture the similarity structures inherent in human perception. Some\nstudies have already begun to compare the similarity structures of LLMs and humans in other domains based\non simple correlation [8, 15, 16]. Our unsupervised alignment method may provide a novel computational\ntool to explore more detailed structural differences or similarities between human cognition and LLMs that\ncannot be detected by simple correlation analysis.\nMethods\nCollecting responses from large language models\nTo obtain responses from GPT-3.5 ( gpt-3.5-turbo) and GPT-4 ( gpt-4-0314), we used a prompt that\nrepresented each color as a HEX code in line with a previous study [8].\nThe prompt we used is as follows:\nPeople described pairs of colors using their hex codes. Rate the dissimilarity of the pair of colors: Color 1:[HEX\ncode] and Color 2:[HEX code] on a scale of 0-7 with 0-1 being Very Similar, 2-3 being Similar, 4-5 being Different,\nand 6-7 being Very Different. Your rating should be any real number between 0 and 7. Your answer should be\nonly the rating in the form of a number. No explanation is needed.\nThe temperature parameter, which determines the degree of randomness in GPT responses, was set to\n0.7. We ran 5 trials for each model, collecting a complete set of similarity judgment responses using the\nsame prompt for each trial. We averaged the responses of the similarity judgments for all possible pairs of\n93 colors over 5 trials and obtained the dissimilarity matrices of 93 colors from GPT-3.5 and GPT-4.\nDissimilarity matrix of color space models\nTo obtain the dissimilarity matrix of the RGB color space model, we computed the Euclidean distance as\na measure of dissimilarity between each color pair within the 3-dimensional RGB space. For the LAB color\nspace model, we used the CIEDE 2000 color difference formula [20] implemented in the Python package\ncolormath (delta_e_cie2000) as a measure of the dissimilarity between colors. Then, by computing the\n8\ndissimilarities of all the pairs of 93 colors, we obtained the dissimilarity matrices of the RGB and LAB color\nspace models.\nComparing Color Similarity Structures\nConventional representational similarity analysis\nTo compare the color similarity structures between humans and GPTs in a supervised manner using exter-\nnal color label information, we used the conventional representational similarity analysis (RSA) approach.\nWe computed Spearman correlations between all pairs of similarity matrices (considering only the upper-\ntriangular elements of each matrix). It is important to note that this analysis inherently assumes a\ncorrespondence between the same colors across different similarity structures.\nUnsupervised alignment using Gromov-Wasserstein optimal transport\nTo evaluate the similarity between the color similarity structures in an unsupervised manner, i.e., without\nmaking any assumptions about the correspondence of colors between different similarity structures, we\nused the Gromov-Wasserstein optimal transport (GWOT) algorithm. GWOT is an unsupervised alignment\nmethod that identifies the optimal transport plan between point clouds in two domains without requiring\ninformation about the correspondence between each item. The algorithm optimizes the Gromov-Wasserstein\ndistance (GWD),\nGWD = min\nΓ\nX\ni,j,k,l\n(Dij − D′\nkl)2ΓikΓjl, (1)\nwhich quantifies the correspondence between the similarity structures in the two domains (Fig. 1C). In\nour problem setting, Dij denotes the dissimilarity between color i and j in one similarity matrix D, while\nD′\nkl denotes the dissimilarity between color k and l in another similarity matrix D′. We normalized each\nsimilarity matrix D so that the values range between 0 and 1. Solving the minimization problem of GWD\nyields the optimal transportation plan, represented by the matrix Γ ∗, which effectively aligns the color\nstructures in the two domains in an unsupervised manner. An element of the matrix Γ ik can be interpreted\nas the “probability” that the i-th color in one domain corresponds to the k-th color in the other domain.\nEfficient optimization of GWD can be achieved by adding an entropy-regularization term, H(Γ), as\nshown in the following equation.\nGWDϵ = min\nΓ\nX\ni,j,k,l\n(Dij − D′\nkl)2ΓikΓjl + ϵH(Γ). (2)\nThis addition has been proven to enhance optimization efficiency [12].\nThe optimization problem in Eq. 2 is non-convex, meaning that the optimal solutions found by the\nalgorithm are local optima, with no guarantee of achieving the global optimum. To find good local minima,\nwe conducted hyperparameter tuning on ϵ and performed random initialization of the matrix Γ using the\nGWTune toolbox that we developed [10]. This toolbox uses Optuna [21] for hyperparameter tuning and\nPython Optimal Transport (POT) [22] for GWOT optimization. We used ϵ values ranging from 10 −4 to\n10−1 with logarithmic spacing (500 different ϵ values), in line with previous works [14]. For each value of\nϵ, we used a randomly initialized matrix Γ. After finding the optimized Γ for each ϵ value, we selected the\nsolution that minimizes the GWD without the entropy term (Eq. 1) as the optimal transportation plan.\nEvaluation of unsupervised alignment\nTo assess the degree of agreement between two similarity structures, we calculated the matching rate between\nthe two dissimilarity matrices using color labels. For each color, we consider it as a match if the transportation\nplan assigns the highest probability between the same colors in the two similarity matrices, because the\ntransportation plan Γij represents the probability or weight of transporting the i-th color in matrix 1 to the\nj-th color in matrix 2. More precisely, for each colori from matrix 1, if Γij is the highest among j ∈ {1, ..., n}\nand the i-th color in matrix 1 and the j-th color in matrix 2 are the same, then we consider the i-th color\nin matrix 1 to be a match with the same color, j, in matrix 2.\nWe denote the color labels in the two dissimilarity matrices as c1 and c2 respectively. The matching rate\nor accuracy is calculated by comparing the transportation plan Γ with these labels. For each color i in the\nmatrix 1, denoted by c1i, the matching condition can be formalized as:\nMatch(i) =\n(\n1, if Γij = maxj∈{1,...,n}(Γij) and c1i = c2j\n0, otherwise (3)\n9\nThis function indicates whether the i-th color in the matrix 1 c1i matches with the same color in the matrix\n2 c2j. The matching rate is then the percentage of colors in the matrix 1 that match with the same color in\nthe matrix 2, which can be calculated as:\nMatching Rate =\nPn\ni=1 Match(i)\nn (4)\nVisualization of unsupervised alignment\nTo visually assess the degree of similarity between the two color similarity structures in an unsupervised\nmanner, we obtained 3-dimensional embeddings of 93 colors. To derive the color embeddings, we applied\nmultidimensional scaling (MDS) to the similarity matrices, yielding 3-dimensional embeddings. We then\naligned a pair of embeddings, denoted X and Y , using the orthogonal rotation matrix Q. This matrix was\nobtained by solving a Procrustes-type problem using the optimized transportation plan Γ ∗ derived from\nGWOT.\nmin\nQ\n∥X − QY Γ∗∥2\nF , (5)\nwhere ∥·∥ F is the Frobenius norm ∥A∥F =\nqP\ni,j a2\nij. A solution to the problem can be found through the\nsingular value decomposition of X(Y Γ∗)⊤.\nReferences\n[1] Devlin J, Chang M-W, Lee K, and Toutanova K. BERT: Pre-training of deep bidirectional transformers\nfor language understanding. In Proceedings of the Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4171–4186, 2018.\n[2] Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee YT, Li Y, Lund-\nberg S, Nori H, Palangi H, Ribeiro MT, and Zhang Y. Sparks of artificial general intelligence: Early\nexperiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.\n[3] Binz M and Schulz E. Using cognitive psychology to understand GPT-3. Proc. Natl. Acad. Sci,\n120(6):e2218523120, 2023.\n[4] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser  L, and Polosukhin I.\nAttention is all you need. InAdvances in Neural Information Processing System, pages 5998–6008, 2017.\n[5] Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G,\nAskell A, Agarwal S, Herbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu\nJ, Winter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish\nS, Radford A, Sutskever I, and Amodei D. Language models are Few-Shot learners. In Advances in\nNeural Information Processing Systems, pages 1877–1901, 2020.\n[6] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[7] Kosinski M. Theory of mind may have spontaneously emerged in large language models. arXiv preprint\narXiv:2302.02083, 2023.\n[8] Marjieh R, Sucholutsky I, van Rijn P, Jacoby N, and Griffiths TL. Large language models predict\nhuman sensory judgments across six modalities. arXiv preprint arXiv:2302.01308, 2023.\n[9] Kriegeskorte N and Kievit RA. Representational geometry: integrating cognition, computation, and\nthe brain. Trends Cogn. Sci, pages 401–412, 2013.\n[10] Sasaki M, Takeda K, Abe K, and Oizumi M. Toolbox for Gromov-Wasserstein optimal transport:\nApplication to unsupervised alignment in neuroscience. bioRxiv, 2023.\n[11] M´ emoli F. Gromov–Wasserstein distances and the metric approach to object matching.Found Comput\nMath 11, pages 417–487, 2011.\n[12] Peyr´ e G, Cuturi M, and Solomon J. Gromov-Wasserstein averaging of kernel and distance matrices. In\nInternational Conference on Machine Learning, pages 2664–2672, 2016.\n[13] Peyr´ e G and Cuturi M. Computational optimal transport. arXiv preprint arXiv:1803.00567, 2020.\n10\n[14] Kawakita G, Zeleznikow-Johnston A, Takeda K, Tsuchiya N, and Oizumi M. Is my “red” your\n“red”?: Unsupervised alignment of qualia structures via optimal transport. PsyArXiv preprint,\ndoi:10.31234/osf.io/h3pqm, 2023.\n[15] Marjieh R, van Rijn P, Sucholutsky I, Sumers TR, Lee H, Griffiths TL, and Jacoby N. Words are all you\nneed? capturing human sensory similarity with textual descriptors. arXiv preprint arXiv:2206.04105,\n2022.\n[16] Marjieh R, Sucholutsky I, Sumers TR, Jacoby N, and Griffiths TL. Predicting human similarity\njudgments using large language models. arXiv preprint arXiv:2202.04728, 2022.\n[17] Williams A, Kunz E, Kornblith S, and Linderman S. Generalized shape metrics on neural representa-\ntions. Advances in Neural Information Processing Systems, 2021.\n[18] Alvarez-Melis D and Jaakkola TS. Gromov-Wasserstein alignment of word embedding spaces. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n1881–1890, 2018.\n[19] Demetci P, Santorella R, Sandstede B, Noble WS, and others. Gromov-Wasserstein optimal transport\nto align single-cell multi-omics data. bioRxiv, 2020.\n[20] Sharma G, Wu W, and Dalal EN. The CIEDE2000 color-difference formula: Implementation notes,\nsupplementary test data, and mathematical observations. Color Res. Appl., 30(1):21–30, 2005.\n[21] Akiba T, Sano S, Yanase T, Ohta T, and Koyama M. Optuna: A next-generation hyperparameter opti-\nmization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, 2623-2631, 2019.\n[22] Flamary R, Courty N, Gramfort A, Alaya MZ, Boisbunon A, Chambon S, Chapel L, Corenflos A,\nFatras K, Fournier N, Gautheron L, Gayraud NTH, Janati H, Rakotomamonjy A, Redko I, Rolet A,\nSchutz A, Seguy V, Sutherland DJ, Tavenard R, Tong A, and Vayer T. Pot: Python optimal transport.\nJournal of Machine Learning Research22, 1–8 (2021).\n[23] Hebart MN, Zheng CY, Pereira F, and Baker CI. Revealing the multidi- mensional mental rep-\nresentations of natural objects underlying human similarity judgements. Nature Human Behaviour,\n4(11):1173–1185, 2020.\n[24] Hebart MN, Contier O, Teichmann L, Rockter AH, Zheng CY, Kidder A, Corriveau A, Vaziri-Pashkam\nM, and Baker CI. THINGS-data: A multimodal collection of large-scale datasets for investigating object\nrepresentations in brain and behavior. eLife, 12:e82580, 2023.\n[25] Epping GP, Fisher EL, Zeleznikow-Johnston A, Pothos E, and Tsuchiya N. A quantum geometric model\nof color similarity judgements. Cognitive Science, 47: e13231, 2023.\n[26] Zeleznikow-Johnston A, Aizawa Y, Yamada M, and Tsuchiya N. Are color experiences the same across\nthe visual field? J Cogn Neurosci, 35(4):509–542, 2023.\n[27] Birch J. Efficiency of the Ishihara test for identifying red-green colour deficiency. Ophthalmic and\nPhysiological Optics, 17(5):403–408, 1997.\n[28] Pouw A, Karanjia R, and Sadun A. A method for identifying color vision deficiency malingering.Graefes\nArch Clin Exp Ophthalmol, 255(3):613–618, 2017.\n[29] Saji N, Imai M, and Asano M. Acquisition of the meaning of the word orange requires understanding of\nthe meanings of red, pink, and purple: Constructing a lexicon as a connected system. Cognitive Science,\n44(1):e12813, 2020.\n[30] Winawer J, Witthoft N, Frank MC, Wu L, Wade AR, Boroditsky L. Russian Blues Reveal Effects of\nLanguage on Color Discrimination. Proceedings of the National Academy of Sciences of the United\nStates of America. 2007;104(19):7780-85.\n[31] Roads BD, Love BC. Modeling Similarity and Psychological Space. Annual Review of Psychology. 2024\nJan;75:215-40.\n11\nAuthor Contributions Statement\nG.K. and M.O. conceived the idea. G.K. ran experiments to collect the data from GPTs. A.Z. and N. T.\ndesigned and performed experiments to collect the data from human participants. G.K. and M.O. analyzed\nthe data. G.K. and M.O. wrote the initial draft of the manuscript. All authors reviewed the manuscript,\nread and approved its final version.\nData availability\nData for the behavioral experiments is available at https://osf.io/9xwr2/.\nCode availability\nCode for the behavioral experiments is available at https://osf.io/9xwr2/. Code for the data analysis is\navailable at https://oizumi-lab.github.io/GWTune/.\nAcknowledgments\nG.K. and M.O. were supported by JST Moonshot R&D Grant Number JPMJMS2012. N.T. and M.O. were\nsupported by Japan Promotion Science, Grant-in-Aid for Transformative Research Areas Grant Numbers\n20H05710, 23H04830 (N.T.) and 20H05712, 23H04834 (M.O.). N.T. was supported by Australian Research\nCouncil (DP180104128, DP180100396). N.T. and A.Z. were supported by National Health Medical Research\nCouncil (APP1183280) and Foundational Question Institute (FQXi-RFP-CPW-2017) and Fetzer Franklin\nFund, a donor advised fund of Silicon Valley Community Foundation. We thank Dominik Kirsten-Parsch\nand Lonni Gomes for their help in collecting the color dissimilarity data.\n12",
  "topic": "Similarity (geometry)",
  "concepts": [
    {
      "name": "Similarity (geometry)",
      "score": 0.659476637840271
    },
    {
      "name": "Neurotypical",
      "score": 0.6041507720947266
    },
    {
      "name": "Equivalence (formal languages)",
      "score": 0.5777373313903809
    },
    {
      "name": "Perception",
      "score": 0.5662827491760254
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4461456835269928
    },
    {
      "name": "Correlation",
      "score": 0.41239404678344727
    },
    {
      "name": "Psychology",
      "score": 0.3653671145439148
    },
    {
      "name": "Computer science",
      "score": 0.33765918016433716
    },
    {
      "name": "Cognitive psychology",
      "score": 0.33600738644599915
    },
    {
      "name": "Mathematics",
      "score": 0.26450806856155396
    },
    {
      "name": "Developmental psychology",
      "score": 0.18839365243911743
    },
    {
      "name": "Neuroscience",
      "score": 0.09942150115966797
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Autism",
      "score": 0.0
    },
    {
      "name": "Autism spectrum disorder",
      "score": 0.0
    },
    {
      "name": "Discrete mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I90023481",
      "name": "National Institute of Information and Communications Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I165953009",
      "name": "Tokyo University of the Arts",
      "country": "JP"
    }
  ],
  "cited_by": 9
}