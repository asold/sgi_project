{
    "title": "Improving Multiple Documents Grounded Goal-Oriented Dialog Systems via Diverse Knowledge Enhanced Pretrained Language Model",
    "url": "https://openalex.org/W4285207898",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2498704594",
            "name": "Yunah Jang",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2142025243",
            "name": "Dongryeol Lee",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2100272918",
            "name": "Hyung Joo Park",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2518850399",
            "name": "Taegwan Kang",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2109292562",
            "name": "Hwanhee Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108911784",
            "name": "Hyunkyung Bae",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2304483808",
            "name": "Kyomin Jung",
            "affiliations": [
                "Seoul National University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3099590177",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3153094109",
        "https://openalex.org/W3202390784",
        "https://openalex.org/W2980207396",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W2898875342",
        "https://openalex.org/W3213758350",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W2807873315",
        "https://openalex.org/W3104777900",
        "https://openalex.org/W2950457956",
        "https://openalex.org/W2995183464",
        "https://openalex.org/W3167673057",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2891826200",
        "https://openalex.org/W2971236040",
        "https://openalex.org/W2988647680",
        "https://openalex.org/W3034999214"
    ],
    "abstract": "Yunah Jang, Dongryeol Lee, Hyung Joo Park, Taegwan Kang, Hwanhee Lee, Hyunkyung Bae, Kyomin Jung. Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering. 2022.",
    "full_text": "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 136 - 141\nMay 26, 2022 ©2022 Association for Computational Linguistics\nImproving Multiple Documents Grounded Goal-Oriented Dialog Systems\nvia Diverse Knowledge Enhanced Pretrained Language Model\nYunah Jang1 Dongryeol Lee1 Hyungjoo Park1 Taegwan Kang1\nHwanhee Lee1 Hyunkyung Bae1 Kyomin Jung2\n1Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea\n2Automation and Systems Research Institute, Seoul National University, Seoul, Korea\n{vn2209, drl123, harry0816, zd9370, wanted1007, hkbae, kjung}@snu.ac.kr\nAbstract\nIn this paper, we mainly discuss about our sub-\nmission to MultiDoc2Dial task, which aims to\nmodel the goal-oriented dialogues grounded in\nmultiple documents. The proposed task is split\ninto grounding span prediction and agent re-\nsponse generation. The baseline for the task\nis the retrieval augmented generation model,\nwhich consists of a dense passage retrieval\nmodel for the retrieval part and the BART\nmodel for the generation part. The main chal-\nlenge of this task is that the system requires a\ngreat amount of pre-trained knowledge to gen-\nerate answers grounded in multiple documents.\nTo overcome this challenge, we adopt multi-\ntask learning, data augmentation, model pre-\ntraining and contrastive learning to enhance our\nmodel’s coverage of pretrained knowledge. We\nexperiment with various settings of our method\nto show the effectiveness of our approaches.\nOur final model achieved 37.78 F1 score, 22.94\nSacreBLEU, 36.97 Meteor, 35.46 RougeL, a\ntotal of 133.15 on DialDoc Shared Task at ACL\n2022 released test set.\n1 Introduction\nRecently, deep learning-based dialog systems have\nattracted much attention from academia and the\nindustry. The main challenge of dialog systems\nis to generate fluent responses consistent with the\nusers’ text input. As Pre-trained Language Models\n(PLMs) (e.g., BART (Lewis et al., 2019) and GPT2\n(Radford et al., 2019)) have emerged, dialog sys-\ntems have taken advantage of PLMs (Zhao et al.,\n2020; Wu et al., 2019; Budzianowski and Vulic,\n2019), which can enhance the quality of dialog re-\nsponse by applying implicit language knowledge.\nHowever, these systems lack knowledge of spe-\ncific topics and thus show weakness in conducting\nan in-depth conversation with humans. There have\nbeen various works for knowledge-grounded dia-\nlogue systems to address this problem. (Kim et al.,\n2020; Zhan et al., 2021) Knowledge grounded di-\nalogue models are capable of generating precise\nresponses based on both the dialogue context and\nexternal sources. Therefore, researchers have usu-\nally constructed dialogue flows grounded in related\ndocuments (Dinan et al., 2018; Zhou et al., 2018b)\nor knowledge graphs (Moon et al., 2019; Zhou\net al., 2018a; Tuan et al., 2019). In particular, Feng\net al. (2020) have introduced the Doc2Dial tasks\nfor goal-oriented document-grounded dialog sys-\ntems. Compared to previous works, Doc2dial has\nintroduced a more challenging setting with multi-\nturn queries and aims to generate natural language\nresponses from relevant grounding document. On\ntop of that, they also propose the MultiDoc2Dial\ndataset (Feng et al., 2021) ,which is built upon the\nDoc2Dial dataset. MultiDoc2Dial dataset is more\nclosely related to real-life scenarios than the prior\nwork since the agent generates responses based on\nmultiple documents as grounding knowledge. Due\nto its multi-document setting, utilizing knowledge\nhas become more complex.\nTo utilize external knowledge in dialogue, knowl-\nedge grounded models generally consist of a re-\ntrieval model and a generative model. Recently,\nthe Retrieval Augmented Generation (RAG) model\n(Lewis et al., 2020a) has been proposed to leverage\nboth parametric (Raffel et al., 2019; Lewis et al.,\n2019) and non-parametric memory (Lewis et al.,\n2020b; Xiao et al., 2020) methods by combining\npre-trained seq2seq models and the dense vector\nindex of grounding documents. However, the RAG\nmodel lacks knowledge related to question answer-\ning and dialogue generation.\nIn this paper, our team JPL proposes four ap-\nproaches to enhance RAG’s diverse knowledge:\nmulti-task learning, data augmentation, pretrain-\ning and contrastive learning. Multi-task learning,\nextra pretraining on conversational question an-\nswering datasets, and data augmentation enhance\nthe model’s task-oriented knowledge. Contrastive\n136\nDPR\nBART\nDPR\nBART\nContrastive Learning \nPretraining \nMulti-task Learning \nData AugmentationExtra\nHard-negative\nCoQA\nDoc2Dial \nRAG\nGrounding+Grounding Synonym\nGeneration\nFigure 1: Our training pipeline We utilize four methods to cultivate the RAG model’s diverse knowledge. To\nenhance model’s task-agnositic knowledge, we add a hard negative sample for contrastive learning on the DPR\nretriever module. Pretraining BART with conversational QA datasets, data augmentation on grounding task, and\nmulti-task learning improves task-specific knowledge for the final RAG model.\nlearning for the DPR retriever module strengthen\ntask-agnostic knowledge. We participate in the\nsecond DialDoc shared task held by ACL, Multi-\nDoc2Dial: Modeling Dialogues Grounded in Mul-\ntiple Documents (Feng et al., 2021). These meth-\nods cultivate the dialogue model’s capability to\nuse complex external knowledge on top of PLM’s\ninherent power.\nSplits Train Val Test\nDialogues 3474 661 661\nQueries 21453 4201 4094\nPassages(struct) 4110\nTable 1: Dataset Statistics We split documents by using\nstructural information from markup tags integrated in\nHTML files.\n2 Shared Task\n2.1 Dataset\nIn this shared task, we focus on the MultiDoc2Dial\ndataset (Feng et al., 2021), which contains conver-\nsations that are grounded in multiple documents.\nThe dataset is constructed based on the Doc2Dial\ndataset, the dataset for the prior shared task at the\nDialDoc 2021 workshop. Unlike its predecessor,\neach dialogue in the MultiDoc2Dial dataset has\nmultiple segments with different grounding docu-\nments for adjacent segments. The dataset consists\nof 4800 dialogues with an average of 14 turns that\nare grounded in 488 documents from four different\ndomains (dmv, ssa, studentaid, va). Details of the\nMultiDoc2Dial dataset are given in Table 1.\n2.2 Multidoc2dial\nFor the evaluations on MultiDoc2Dial dataset, two\nsub-tasks are proposed. Task 1 aims to predict the\ngrounding span for the next agent response. For\ntask 1, we get (1) current user turn, (2) dialogue\nhistory, (3) the entire set of documents from all\ndomains as input. For the output, we aim to figure\nout the most relevant grounding text span from one\ndocument for the next agent response. Task 2 aims\nto generate agent response in natural language. For\ntask 2, we get (1) current user turn, (2) dialogue\nhistory, (3) the entire set of documents from all\ndomain as an input.\n2.3 Baseline Model\nIn this shared task, the author proposed a base-\nline model based on the HuggingFace RAG. 1 For\nthe retriever part, DPR (Karpukhin et al., 2020)\nwas given in the form of both finetuned DPR en-\ncoders by author 2 and the original Facebook DPR.\n3 The generator module of the baseline is BART-\nlarge from the HuggingFace.4 Our final submission\nmodel is composed of our own fine-tuned DPR\nand Bart-large pretrained with conversational QA\ndatasets.\n3 Methodology\nWe use four methods to enhance the model’s ability\nto efficiently utilize external grounding knowledge\nespecially on dialogue modeling.\n1https://huggingface.co/docs/transformers/master/model_doc\n/rag\n2https://huggingface.co/sivasankalpp\n3https://github.com/facebookresearch/DPR\n4https://huggingface.co/facebook/bart-large\n137\n3.1 Multi-task Learning\nMulti-task learning improves the model’s perfor-\nmance when different tasks share information or\nsemantics. If the tasks have a higher correlation, it\nis likely for the model to benefit more from multi-\ntask learning. The final goal of the proposed task is\nto generate natural language responses, which cor-\nresponds to the generation task. Figure 2 presents\nthe similarity between the ground truth of each task.\nFrom this statistic, it is clear that two tasks share\nmuch semantic information.\nIn order to implement multi-task learning, we\nfirst train the model on the grounding task with\nprefix \"TASK1: \" added to the input string for\nthe generator. Then, using the last checkpoint, we\ncontinue training the model on the generation task\nwith prefix \"TASK2: \" concatenated to each input\nstring.\n0.0 0.2 0.4 0.6 0.8 1.0\nrouge_score\n0\n100\n200\n300\n400\n500\n600\n700\n800number of instances\nrouge-l-r\nrouge-l-p\nrouge-l-f\nFigure 2: Similarity score of ground truth answer on\ngrounding and generation task\n3.2 Data Augmentation\nTo enhance the adaptability of the RAG model to\nthe dataset, we attempt to increase the amount of\ndata for finetuning. For each dialogue query in the\noriginal dataset, we apply the synonym augmenter\nfrom nlpaug5. The synonym augmenter randomly\nchanges some words in the input to similar words\nbased on WordNet6. We exclude ’[SEP]’, ’agent:’\n’user:’ since these words are special tokens for the\ntask.\n3.3 Pretraining on Conversational QA\nDatasets\nTo enhance the generative performance of the\nmodel, we pretrain the RAG generator on two\ndatasets.\n5https://github.com/makcedward/nlpaug\n6https://wordnet.princeton.edu/\nCoQA The first dataset is the CoQA dataset\n(Reddy et al., 2018), a conversational QA dataset\ngrounded in a diverse range of documents. Because\nMultiDoc2Dial is not a large dataset, there is al-\nways a possibility of underfitting. CoQA, with its\n127k questions, can provide us with much-needed\nextra data for our generator. As the format of the\nCoQA dataset (grounding document, then ques-\ntions) is different from the input format of our\nBART model (query and dialogue context, fol-\nlowed by the grounding document), we reformat\nthe dataset to fit our needs before training.\nDoc2Dial The second dataset is the Doc2Dial\ndataset (Feng et al., 2020), a goal-oriented\ndocument-grounded dialogue dataset which is ex-\ntremely similar to the MultiDoc2Dial dataset. As\nmentioned above, most of the instances in the\nMultiDoc2Dial dataset are formed by modifying\nDoc2Dial instances to fit a multi-document setting.\nAlong with the existence of a single grounding doc-\nument, this extreme similarity of content makes it\nan ideal candidate to train our generator without\nrelying on the proper functioning of the retriever.\nTherefore, we can expect pretraining the generator\non the Doc2Dial dataset to boost the generative\ncapabilities of our model. As with CoQA, we refor-\nmat the dataset to fit the input of our BART model\nbefore training.\nFor both datasets, we do not cut down the\ngrounding document to fit the maximum input\nlength of our model. This may have resulted in\ntruncation of the relevant span in some instances,\nand remains an area of possible improvement.\n3.4 Contrastive Learning\nTo enhance the retrieval performance of the model,\nwe adopt data augmentation to increase the number\nof hard negative contexts in the DPR training data.\nWe apply the antonym augmenter from nlpaug 7.\nThe antonym augmenter takes positive contexts,\nwhich is the correct grounding document for the\ndialogue, as input. Based on WordNet antonym,\nthe augmenter switches some words in the inputs\nto their respective antonyms and outputs the aug-\nmented sentences. We consider these outputs as\nthe hard negative contexts and added them to the\noriginal dataset. We use the augmented dataset to\nfinetune DPR.\n7https://github.com/makcedward/nlpaug\n138\n4 Experiments\n4.1 Training Details\nWe fine-tune RAG by following the default hyper-\nparameter settings from the baseline code.8 Due to\nhardware shortage, there are minor modifications;\nwe set the gradient accumulation step as 2 and re-\nduce the training and evaluation batch size to 4 and\n1, respectively. We only report results of utilizing\ndocument structural information for segmentation\nsince it shows better results in our experimental set-\ntings. The retrieved documents are not re-ranked\nsince this method doesn’t benefit the model perfor-\nmance.\n4.2 Results and Analysis\nModel F1 EM S_Bleu\nbaseline 34.69 3.86 20.63\n+Multi-task learning 34.85 3.98 19.86\n+Data Augmentation 33.55 3.28 19.01\nTable 2: RAG Fine-tuning Methods Results Models\nare evaluated with F1, Exact Match, and sacreBLEU\nscores. The baseline model is composed of the released\nversion of finetuned DPR9 and BART-large on the Hug-\ngingFace.\n4.2.1 RAG Fine-tuning Methods\nIn this section, the Facebook released version of\nDPR and BART-large in the HuggingFace consti-\ntute the baseline model.\nMulti-task Learning We sequentially fine-tune\nthe model on the grounding and generation tasks.\nTable 2 shows the results for multi-task learning.\nThere are improvements in the F1 and EM score\nusing multi-task learning, even though considering\nthe fact that the model was trained on the genera-\ntion task for a much shorter time. We expect the\nmodel to show better results with more extended\ntraining.\nData Augmentation For data augmentation,\nwe apply synonym transformation to the original\ndataset, attaining twice the baseline size. Table 2\npresents the result for data augmentation on gen-\neration task. We have observed that applying data\naugmentation to the generation task degraded the\nperformance. However, by utilizing augmented\ndata on the grounding task, the model achieves\na 40.55 F1 score and a 23.49 exact match score.\nCompared to our baseline model implementation\n8https://github.com/IBM/multidoc2dial\ntrained with the original grounding task data, train-\ning with augmented data improved +0.5 F1 score\nand +0.64 exact match score. These results demon-\nstrate that synonym data augmentation on the gen-\neration task’s gold answers does not provide the\nmodel with any informative knowledge for the gen-\neration task. Therefore, we include augmented data\nonly on grounding task during multi-task learning.\nModel F1 EM S_Bleu\nbaseline 34.69 3.86 20.63\n+CoQA 35.08 4.02 20.37\n+CoQA&Doc2Dial 35.34 4.09 20.63\nDPR(adv_nq) 34.05 3.57 19.76\n+DPR(+hard neg) 35.09 3.83 20.87\nTable 3: Module Specific Methods Results We\nevaluate models with F1, Exact Match, and sacre-\nBLEU scores. +CoQA&Doc2Dial reports results for\nBART-large pretrained on CoQA and Doc2Dial dataset.\nDPR(adv_nq) is the RAG model composed of our\nown fine-tuned DPR using shared task configuration.\n+DPR(+hard_negative) corresponds to results for RAG\nwith our fine-tuned DPR version with an extra hard neg-\native sample.\n4.2.2 Module Specific Methods\nThis section mainly discusses results for module-\nspecific training methods. We fine-tune RAG’s\nretriever and pretrain generator, DPR and BART,\nwith contrastive learning and conversational QA\ndatasets. We set the baseline model as the same\nconfiguration with section 4.2.1.\nPretraining We pretrain BART-large on CoQA\nand Doc2Dial before integrating it into RAG. We\ntrain 10 epochs for each dataset using hyperparam-\neters suggested by the DialDoc2021 baseline code\non subtask2.10 Table 3 shows the result for pretrain-\ning. We report two results; pretrained on CoQA\nonly and pretrained on both CoQA and Doc2Dial.\nBoth datasets enhanced the model performance in\nterms of F1 and EM scores. There is extra room\nfor improvement since we pretrain BART only for\na few epochs due to long training time and limited\nresources.\nContrastive Learning We fine-tune DPR using\nthe settings implemented by the shared task. We\nfine-tune the recently released version of DPR,\ncheckpoint.retriever.single-adv-\nhn.nq.bert-base-encoder, for 50 epochs\n10https://github.com/doc2dial/sharedtask-dialdoc2021\n139\non our new DPR dataset with one extra hard nega-\ntive sample generated by antonym augmentation.\nTable 3 reports the results for contrastive learning.\nDespite using the same hyperparameters for DPR,\nthere is degradation in the score for fine-tuning\non our setting. However, after adding another\nhard negative sample, the model shows better\nperformance on the shared task.\n4.2.3 Leaderboard Submission\nOur final model for DialDoc shared task at ACL\n2022 utilizes all four suggested methods in this\npaper. We only participate in MultiDoc2Dial-\nseen-domain task which training data and test data\nshare the same domains for the grounding docu-\nments. Our best performing model achieves 37.78\nF1 score, 22.94 SacreBLEU, 36.97 Meteor, 35.46\nRougeL, a total of 133.15 on the officially released\ntest set (MDD-SEEN).\n5 Conclusion\nIn this paper, we explain our submissions to the\nMultiDoc2Dial shared task. We utilize various con-\nversational QA datasets and methods to improve\nthe given baseline model. Our RAG model is com-\nposed of DPR for the retriever and BART for the\ngenerator. We train DPR with contrastive learning\nwith an extra hard negative sample. BART is pre-\ntrained on conversational QA datasets, CoQA and\nDoc2Dial. On the end-to-end level, we implement\nmulti-task learning to utilize model knowledge ob-\ntained from the previous grounding task that is\ntrained on augmented data. All of the mentioned\ntechniques enhance the model performance com-\npared to the suggested baseline model.\nAcknowledgements\nK. Jung is with ASRI, Seoul National Univer-\nsity, Korea. This work was supported by the\nNational Research Foundation of Korea (NRF)\ngrant funded by the Korea government (No.\n2021R1A2C2008855)\nReferences\nPawel Budzianowski and Ivan Vulic. 2019. Hello, it’s\nGPT-2 - how can I help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. CoRR, abs/1907.05774.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nSong Feng, Siva Sankalp Patel, Hui Wan, and Sachin-\ndra Joshi. 2021. Multidoc2dial: Modeling dia-\nlogues grounded in multiple documents. CoRR,\nabs/2109.12595.\nSong Feng, Hui Wan, R. Chulaka Gunasekara,\nSiva Sankalp Patel, Sachindra Joshi, and Luis A.\nLastras. 2020. doc2dial: A goal-oriented document-\ngrounded dialogue dataset. CoRR, abs/2011.06623.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nByeongchang Kim, Jaewoo Ahn, and Gunhee Kim.\n2020. Sequential latent knowledge selection for\nknowledge-grounded dialogue.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. CoRR, abs/1910.13461.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\nYih, Tim Rocktäschel, Sebastian Riedel, and\nDouwe Kiela. 2020a. Retrieval-augmented gener-\nation for knowledge-intensive NLP tasks. CoRR,\nabs/2005.11401.\nPatrick S. H. Lewis, Pontus Stenetorp, and Sebastian\nRiedel. 2020b. Question and answer test-train over-\nlap in open-domain question answering datasets.\nCoRR, abs/2008.02637.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. OpenDialKG: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854, Florence, Italy. Associa-\ntion for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2018. Coqa: A conversational question answering\nchallenge. CoRR, abs/1808.07042.\n140\nYi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee.\n2019. Dykgchat: Benchmarking dialogue genera-\ntion grounding on dynamic knowledge graphs. arXiv\npreprint arXiv:1910.00610.\nQingyang Wu, Yichi Zhang, Yu Li, and Zhou Yu.\n2019. Alternating recurrent dialog model with\nlarge-scale pre-trained language models. CoRR,\nabs/1910.03756.\nJinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung\nBui, Tong Sun, and Jiawei Han. 2020. Open-domain\nquestion answering with pre-constructed question\nspaces. CoRR, abs/2006.08337.\nHaolan Zhan, Lei Shen, Hongshen Chen, and Hainan\nZhang. 2021. CoLV: A collaborative latent variable\nmodel for knowledge-grounded dialogue generation.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2250–2261, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nXueliang Zhao, Wei Wu, Can Xu, Chongyang Tao,\nDongyan Zhao, and Rui Yan. 2020. Knowledge-\ngrounded dialogue generation with pre-trained lan-\nguage models. CoRR, abs/2010.08824.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018a. Common-\nsense knowledge aware conversation generation with\ngraph attention. In IJCAI, pages 4623–4629.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018b. A dataset for document grounded\nconversations. arXiv preprint arXiv:1809.07358.\n141"
}