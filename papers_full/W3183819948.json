{
    "title": "Multimodal Item Categorization Fully Based on Transformer",
    "url": "https://openalex.org/W3183819948",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5100333391",
            "name": "Lei Chen",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Rakuten (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5008787624",
            "name": "Houwei Chou",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Rakuten (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5067274223",
            "name": "Yandi Xia",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Rakuten (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5058709950",
            "name": "Hirokazu Miyake",
            "affiliations": [
                "Massachusetts Institute of Technology",
                "Rakuten (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3087568487",
        "https://openalex.org/W3108561697",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3049632937",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3117216739",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2951092385",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3035485997",
        "https://openalex.org/W2900753462",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3104609290",
        "https://openalex.org/W2953607269",
        "https://openalex.org/W4298094466",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "The Transformer has proven to be a powerful feature extraction method and has gained widespread adoption in natural language processing (NLP). In this paper we propose a multimodal item categorization (MIC) system solely based on the Transformer for both text and image processing. On a multimodal product data set collected from a Japanese e-commerce giant, we tested a new image classification model based on the Transformer and investigated different ways of fusing bi-modal information. Our experimental results on real industry data showed that the Transformer-based image classifier has performance on par with ResNet-based classifiers and is four times faster to train. Furthermore, a cross-modal attention layer was found to be critical for the MIC system to achieve performance gains over text-only and image-only models.",
    "full_text": "Proceedings of the 4th Workshop on e-Commerce and NLP (ECNLP 4), pages 111–115\nAugust 5, 2021. ©2021 Association for Computational Linguistics\n111\nMultimodal Item Categorization Fully Based on Transformers\nLei Chen∗, Hou Wei Chou∗, Yandi Xia, Hirokazu Miyake\nRakuten Institute of Technology\nBoston, MA, USA\n{lei.a.chen,houwei.chou,yandi.xia,hirokazu.miyake}@rakuten.com\nAbstract\nThe Transformer has proven to be a power-\nful feature extraction method and has gained\nwidespread adoption in natural language pro-\ncessing (NLP). In this paper we propose a\nmultimodal item categorization (MIC) system\nsolely based on the Transformer for both text\nand image processing. On a multimodal\nproduct data set collected from a Japanese e-\ncommerce giant, we tested a new image clas-\nsiﬁcation model based on the Transformer and\ninvestigated different ways of fusing bi-modal\ninformation. Our experimental results on real\nindustry data showed that the Transformer-\nbased image classiﬁer has performance on par\nwith ResNet-based classiﬁers and is four times\nfaster to train. Furthermore, a cross-modal at-\ntention layer was found to be critical for the\nMIC system to achieve performance gains over\ntext-only and image-only models.\n1 Introduction\nItem categorization (IC) is a core technology in\nmodern e-commerce. Since there can be millions\nof products and hundreds of labels in e-commerce\nmarkets, it is important to be able to map these\nproducts to their locations in a product category tax-\nonomy tree efﬁciently and accurately so that buyers\ncan easily ﬁnd the products they need. Therefore,\nIC technology with high accuracy is needed to cope\nwith this demanding task.\nProducts can contain text (such as titles) and\nimages. Although most IC research has focused\non using text-based cues, images of products also\ncontain useful information. For example, in some\nsub-areas like fashion, the information conveyed\nthrough images is richer and more accurate than\nthrough the text channel. In this paper, we propose\nan MIC model entirely based on the Transformer\narchitecture (Vaswani et al., 2017) for achieving\n∗Equal contributor\na simpliﬁcation of the model and faster training\nspeed. We conducted experiments on real product\ndata collected from an e-commerce giant in Japan\nto (a) test the performance of the Transformer-\nbased product image classiﬁcation, and (b) system-\natically compare several bi-modal fusion methods\nto jointly use both text and image cues.\n2 Related works\n(Zahavy et al., 2016) is a seminal work on MIC\nwhere multi-label classiﬁcation using both titles\nand images was conducted on products listed on the\nWalmart.com website. They used a convolutional\nneural network to extract representations from both\ntitles and images, then designed several policies\nto fuse the outputs of the two models. This led\nto improved performance over individual models\nseparately. Since this work, further research has\nbeen conducted on MIC such as (Wirojwatanakul\nand Wangperawong, 2019; Nawaz et al., 2018).\nRecently, a MIC data challenge was organized\nin the SIGIR’20 e-commerce workshop1. Rakuten\nFrance provided a dataset containing about 99K\nproducts where each product contained a title, an\noptional detailed description, and a product image.\nThe MIC task was to predict 27 category labels\nfrom four major genres: books, children, house-\nhold, and entertainment. Several teams submitted\ntheir MIC systems (Bi et al., 2020; Chordia and\nVijay Kumar, 2020; Chou et al., 2020). A common\nsolution was to ﬁne-tune pre-trained text and im-\nage encoders to serve as feature extractors, then\nuse a bi-modal fusion mechanism to combine pre-\ndictions. Most teams used the Transformer-based\nBERT model (Devlin et al., 2019) for text feature\nextraction and ResNet (He et al., 2016) for image\nfeature extraction, including the standard ResNet-\n1https://sigir-ecom.github.io/\necom2020/data-task.html\n112\n152 and the recently released Big Transfer (BiT)\nmodel (Kolesnikov et al., 2020). For bi-modal fu-\nsion, the methods used were more diverse. Roughly\nin order of increasing complexity, the methods in-\ncluded simple decision-level late fusion (Bi et al.,\n2020), highway network (Chou et al., 2020), and\nco-attention (Chordia and Vijay Kumar, 2020). It\nis interesting to note that the winning team used\nthe simplest decision-level late fusion method.\nIn other recent work, a cross-modal attention\nlayer which used representations from different\nmodalities to be the key and query vectors to\ncompute attention weights was studied. In (Zhu\net al., 2020), product descriptions and images were\njointly used to predict product attributes, e.g., color\nand size, and their values in an end-to-end fashion.\nIn addition, based on the fact that product images\ncan contain information not clearly aligned with or\neven contradicting the information conveyed in the\ntext, a special gate was used to control the contribu-\ntion of the image channel. A similar idea was used\nin (Sun et al., 2020) on multimodal named entity\nrecognition research on Twitter data.\nAlthough the ﬁeld has converged on using\nTransformer-based models for processing text in\nrecent years, ResNet-based image processing is\nstill the dominant approach in MIC research. One\nimmediate difﬁculty in combining the two types of\nmodels is the big gap between the training speeds.\nOwing to the superior parallel running capability\nenabled by self-attention in the Transformer archi-\ntecture, text encoder training is much faster than the\nimage encoder, and the training bottleneck of the\nMIC system becomes solely the image encoder. In\naddition, using two different deep learning architec-\ntures simultaneously makes building and maintain-\ning MIC systems more complex. One solution is to\nuse Transformers as the encoder of choice for both\nmodalities. Furthermore, a detailed comparison of\ndifferent fusion methods on large-scale multimodal\nindustry product data is still missing. Our work\naddresses these two directions of research.\n3 Model\nOur MIC model is depicted in Figure 12. It consists\nof feature extraction components using a Trans-\nformer on uni-modal channels (i.e., text titles and\nimages), a fusion part to obtain multimodal repre-\nsentations, and a Multi-Layer Perceptron (MLP)\n2The image of the can of tea is from https://item.\nrakuten.co.jp/kusurinokiyoshi/10016272/\nWord_1, ... Word_N 1 2\n4 5\n3\n6\n7 8 9\nBERT V iT \nv_0 v_1 h_2 ... v_9h_0 h_1 h_2 ... h_N\nCLS P_1 P_2 ... P_9CLS W_1 W_2 ... W_N\nMLP MLP\nLate fusion\n(decision)\nEarly fusion\n(Shallow)\nMLP\nCross-modal\nattention\nMLP\nEarly fusion\n(Cross-Attention)\n(a) (b) (c)\nalpha 1-alpha\ncategory\ncategory category\nFigure 1: Our Transformer-based MIC system consists\nof a BERT model to extract textual information and a\nViT model to extract visual information. Three differ-\nent types of multimodal fusion methods are compared,\nincluding (a) late fusion, (b) early fusion by concatenat-\ning textual and image representations (shallow), and (c)\nearly fusion by using a cross-modal attention. Wide ar-\nrows indicate that the entire sequence, e.g., h0 to hN,\nis used in the computation. For illustration we show\n3 ×3 patches for ViT but in our actual implementation\na higher P was used.\nhead to make ﬁnal predictions.\n3.1 BERT text model\nWe ﬁne-tuned a Japanese BERT model (Devlin\net al., 2019) trained on Japanese Wikipedia data.\nThe BERT model encodes a textual product title,\nx = ([CLS],x1,...,x N ), into text representation\nsequence h = (h0,h1,...hN ), where hi is a vector\nwith a dimension of 768.\n3.2 ViT image model\nAlthough originally developed for NLP applica-\ntions, in recent years the Transformer architec-\nture (Vaswani et al., 2017) has been increasingly\napplied to the computer vision domain. For ex-\nample, (Han et al., 2020) is a recent survey paper\nlisting many newly emerging visual models using\nthe Transformer.\nAmong the many visual Transformer models\nwe used the ViT model (Dosovitskiy et al., 2020),\nwhich is a pure Transformer that is applied directly\non an image’sP ×P patch sequence. ViT utilizes\nthe standard Transformer’s encoder part as an im-\nage classiﬁcation feature extractor and adds a MLP\nhead to determine the image labels. The ViT model\n113\nwas pre-trained using a supervised learning task on\na massive image data set. The size of the super-\nvised training data set impacts ViT performance\nsigniﬁcantly. When using Google’s in-house JFT\n300M image set, ViT can reach a performance su-\nperior to other competitive ResNet (He et al., 2016)\nmodels.\nThe ViT model encodes the product image. Af-\nter converting a product image to P ×P patches,\nViT converts these patches to visual tokens. After\nadding a special [CLS] visual token to represent the\nentire image, the M = P ×P + 1long sequence\nis fed into a ViT model to output an encoding as\nv = (v0,v1,v2,...vM ), where M = P ×P.\n3.3 Multimodal fusion\nThe fusion method plays an important role in MIC.\nIn this paper we compared three methods, corre-\nsponding to Figure 1 (a), (b), and (c).\n3.3.1 Late fusion\nThe simplest fusion method is combining the deci-\nsions made by individual models directly (Bi et al.,\n2020; Chou et al., 2020). We used weights αand\n1 −αto interpolate the probabilities estimated by\nBERT and ViT models. The αvalue was chosen\nusing a held-out set.\n3.3.2 Early fusion – shallow\nThe [CLS] token, the ﬁrst token of every input\nsequence to BERT and ViT, is used to provide a\nglobal representation. Therefore we can concate-\nnate the two encoded [CLS] tokens to create a mul-\ntimodal output. The concatenated feature vectors\nare sent to an MLP head for predicting multi-class\ncategory labels. This method is called a shallow\nfusion (Siriwardhana et al., 2020).\n3.3.3 Early fusion – cross-modal attention\nA cross-modal attention layer provides a more so-\nphisticated fusion between text and image chan-\nnels (Zhu et al., 2020; Sun et al., 2020). Cross-\nmodal attention is computed by combining Key-\nValue (K-V) pairs from one modality with the\nQuery (Q) from the other modality. In addition,\n(Zhu et al., 2020) used a gate to moderate potential\nnoise from the visual channel.\nSpeciﬁcally, the multimodal representation h′is\ncomputed from the addition of the self-attention\n(SA) version of text representation h and the cross-\nmodal attention version by considering the visual\nrepresentation v as\nh′= SA(h,h,h) +VG ⊙SA(h,v,v), (1)\nwhere\nSA(q,k,v) =softmax\n((WQq)(WKk)T\n√dk\n)\nWV v,\n(2)\nVGi = σ(W1hi + W2v0 + b), (3)\nWQ, WK, and WV are trainable query, key, and\nvalue parameters, dk is the dimension of the key\nvectors, and the visual gate, VG, can be learned\nfrom both the local text representations hi and\nglobal visual representation v0, with W1, W2, and\nbas trainable parameters. The category label pre-\ndiction ˆyis determined as\nˆy= softmax\n(\nW3\n∑\ni\nh′\ni\n)\n, (4)\nwhere W3 is a trainable parameter.\n4 Experiment\n4.1 Setup\nData set: Our data consisted of about 500,000\nproducts from a large e-commerce platform in\nJapan, focusing on three major product categories.\nOur task, a multi-class classiﬁcation problem, was\nto predict the leaf-level product categories from\ntheir Japanese titles and images. Further details of\nour data set are shown in the left part of Table 1.\nWe used the macro-averaged F1-score to evaluate\nmodel performance.\nModels: We compared the following models.\n• Text-only: Japanese BERT model3 ﬁne-tuned\non product titles.\n• Image-BiT: BiT image model (Kolesnikov\net al., 2020) ﬁne-tuned on product images.\nIn particular, we used BiT-M.4 BiT showed\na considerable performance advantage than\nother conventional ResNet models in the SI-\nGIR’20 MIC data challenge (Chou et al.,\n2020).\n3https://huggingface.co/cl-tohoku/\nbert-base-japanese-whole-word-masking\n4https://tfhub.dev/google/bit/\nm-r152x4/1\n114\nRoot genre Class size Train size Test size Ma-F1 (BiT) Ma-F1 (ViT)\nBeverages (B) 32 29,269 7,332 0.666 0.610\nAppliances (A) 280 200,552 50,283 0.574 0.639\nMen’s Fashion (M) 70 228,148 57,077 0.715 0.733\nTable 1: Summary of our data set obtained from a large e-commerce platform in Japan. Right two columns report\nimage classiﬁcation macro-F1 values using BiT and ViT models, respectively.\n• Image-ViT: ViT image model (Dosovitskiy\net al., 2020) ﬁne-tuned on product images. We\nused ViT-L-16.5 16 means that we used 16 ×\n16 patches when feeding images.\n• Fusion: The late fusion method described in\nSection 3.3.1 and depicted in Figure 1 (a),\nthe early fusion method described in Sec-\ntion 3.3.2 and depicted in Figure 1 (b), and\nthe cross-modal fusion method described in\nSection 3.3.3 and depicted in Figure 1 (c).\nImplementation details: Our models were imple-\nmented in PyTorch using a GPU for training and\nevaluation. The AdamW optimizer (Loshchilov\nand Hutter, 2017) was used. Tokenization was per-\nformed with MeCab.6\n4.2 Result\nTable 1 reports on macro-F1 values for the\nthree genres using the ResNet-based BiT vs.\nTransformer-based ViT. ViT shows higher perfor-\nmance compared to BiT on two of the three genres.\nIn addition, consistent with the speed advantage\nreported in (Dosovitskiy et al., 2020), we also ob-\nserved that the training for ViT is about four times\nfaster than BiT. This is critical for an MIC system\ndeployable in industry since image model training\ntime is the main bottleneck.\nModel F1 (B) F1 (A) F1 (M)\nText-BERT 0.718 0 .733 0 .802\nImage-ViT 0.610 0 .639 0 .733\nFusion-late 0.725 0 .709 0 .814\nFusion-early 0.714 0 .726 0 .788\nFusion cross-modal 0.729 0.740 0.815\nTable 2: Macro-F1 on the three product genres. Uni-\nmodal models, i.e., BERT text model and ViT image\nmodel, and different fusion models are compared.\nTable 2 reports on uni-modal model perfor-\nmance, i.e., text-BERT and image-ViT separately,\n5https://github.com/asyml/\nvision-transformer-pytorch\n6https://taku910.github.io/mecab/\nas well as the results of fusing these models in\nvarious ways. We found that the early (shallow)\nfusion method leads to poor model performance.\nOne possible reason is that product images used in\ne-commerce product catalogs sometimes do not ap-\npear to be clearly related to its corresponding titles.\nFor example, a bottle of wine may be packaged in\na box and its image only shows the box. We also\nfound that late (decision) fusion does not lead to\nconsistent gains. In the appliance genre, we found\nthat the fused model was worse than the text model.\nOn the other hand, the cross-modal attention fusion\nmethod showed consistent gains over both the text\nand image models separately on all three genres.\n5 Discussion\nAlthough various approaches have been explored in\nMIC research, we found that a MIC system built en-\ntirely out of the Transformer architecture was miss-\ning. Combining the well-established BERT text\nmodel and the newly released ViT image model,\nwe proposed an all-Transformer MIC system on\nJapanese e-commerce products. From experiments\non real industry product data from an e-commerce\ngiant in Japan, we found that the ViT model can be\nﬁne-tuned four times faster than BiT and can have\nimproved performance. Furthermore, fusing both\ntext and image inputs in an MIC setup using the\ncross-modal attention fusion method led to model\nperformance better than each model separately, and\nwe found that this fusion method worked better\nthan late fusion and the early (shallow) fusion of\nsimply concatenating representations from the two\nmodalities.\nThere are several directions to extend the cur-\nrent work in the future, including (1) considering\njointly modeling texts and images in one Trans-\nformer model like FashionBERT (Gao et al., 2020),\nand (2) using self-training to go beyond the limit\ncaused by the size of labeled image data for the\nimage model.\n115\nReferences\nYe Bi, Shuo Wang, and Zhongrui Fan. 2020. A Multi-\nmodal Late Fusion Model for E-Commerce Product\nClassiﬁcation. arXiv preprint arXiv:2008.06179.\nV . Chordia and B.G. Vijay Kumar. 2020. Large Scale\nMultimodal Classiﬁcation Using an Ensemble of\nTransformer Models and Co-Attention. In Proc. SI-\nGIR’20 e-Com workshop.\nH. Chou, Y .H. Lee, L. Chen, Y . Xia, and W.T. Chen.\n2020. CBB-FE, CamemBERT and BiT Feature Ex-\ntraction for Multimodal Product Classiﬁcation and\nRetrieval. In Proc. SIGIR’20 e-Com workshop.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs]. ArXiv:\n1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, and Sylvain Gelly. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nDehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng\nLi, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashion-\nbert: Text and image matching with adaptive loss for\ncross-modal retrieval. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages\n2251–2260.\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, and Yixing Xu. 2020. A Survey on Vi-\nsual Transformer. arXiv preprint arXiv:2012.12556.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nJoan Puigcerver, Jessica Yung, Sylvain Gelly, and\nNeil Houlsby. 2020. Big Transfer (BiT): General\nVisual Representation Learning. arXiv:1912.11370\n[cs]. ArXiv: 1912.11370.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nShah Nawaz, Alessandro Calefati, Muhammad Kam-\nran Janjua, Muhammad Umer Anwaar, and Ignazio\nGallo. 2018. Learning fused representations for\nlarge-scale multimodal classiﬁcation. IEEE Sensors\nLetters, 3(1):1–4.\nShamane Siriwardhana, Andrew Reis, Rivindu\nWeerasekera, and Suranga Nanayakkara. 2020.\nTuning “BERT-like” Self Supervised Models to\nImprove Multimodal Speech Emotion Recognition.\nLin Sun, Jiquan Wang, Yindu Su, Fangsheng Weng,\nYuxuan Sun, Zengwei Zheng, and Yuanyi Chen.\n2020. RIV A: A Pre-trained Tweet Multimodal\nModel Based on Text-image Relation for Multi-\nmodal NER. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 1852–1862.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, \\Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30:5998–6008.\nPasawee Wirojwatanakul and Artit Wangperawong.\n2019. Multi-Label Product Categorization Us-\ning Multi-Modal Fusion Models. arXiv preprint\narXiv:1907.00420.\nTom Zahavy, Alessandro Magnani, Abhinandan Krish-\nnan, and Shie Mannor. 2016. Is a picture worth a\nthousand words? A Deep Multi-Modal Fusion Ar-\nchitecture for Product Classiﬁcation in e-commerce.\narXiv preprint arXiv:1611.09534.\nTiangang Zhu, Yue Wang, Haoran Li, Youzheng Wu,\nXiaodong He, and Bowen Zhou. 2020. Multi-\nmodal Joint Attribute Prediction and Value Ex-\ntraction for E-commerce Product. arXiv preprint\narXiv:2009.07162."
}