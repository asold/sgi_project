{
  "title": "KR-BERT: A Small-Scale Korean-Specific Language Model",
  "url": "https://openalex.org/W3047738520",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1549786878",
      "name": "Lee， Sang-ah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2723240327",
      "name": "Jang, Hansol",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Baik, Yunmee",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Park, Suzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2403843205",
      "name": "Shin， Hyopil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W2806577156",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2990098892",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "Since the appearance of BERT, recent works including XLNet and RoBERTa utilize sentence embedding models pre-trained by large corpora and a large number of parameters. Because such models have large hardware and a huge amount of data, they take a long time to pre-train. Therefore it is important to attempt to make smaller models that perform comparatively. In this paper, we trained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and dataset. Since Korean is one of the morphologically rich languages with poor resources using non-Latin alphabets, it is also important to capture language-specific linguistic phenomena that the Multilingual BERT model missed. We tested several tokenizers including our BidirectionalWordPiece Tokenizer and adjusted the minimal span of tokens for tokenization ranging from sub-character level to character-level to construct a better vocabulary for our model. With those adjustments, our KR-BERT model performed comparably and even better than other existing pre-trained models using a corpus about 1/10 of the size.",
  "full_text": "arXiv:2008.03979v2  [cs.CL]  11 Aug 2020\nKR-BERT∗: A Small-Scale Korean-Speciﬁc Language Model\nSangah Lee, Hansol Jang, Y unmee Baik, Suzi Park, Hyopil Shin\nSeoul National University\nAbstract\nSince the appearance of BER T , recent works in-\ncluding XLNet and RoBER T a utilize sentence em-\nbedding models pre-trained by large corpora and\na large number of parameters. Because such mod-\nels have large hardware and a huge amount of\ndata, they take a long time to pre-train. Therefore\nit is important to attempt to make smaller mod-\nels that perform comparatively. In this paper, we\ntrained a Korean-speciﬁc model KR-BER T , utiliz-\ning a smaller vocabulary and dataset. Since Ko-\nrean is one of the morphologically rich languages\nwith poor resources using non-Latin alphabets, it is\nalso important to capture language-speciﬁc linguis-\ntic phenomena that the Multilingual BER T model\nmissed. W e tested several tokenizers including our\nBidirectionalW ordPiece T okenizer and adjusted the\nminimal span of tokens for tokenization ranging\nfrom sub-character level to character-level to con-\nstruct a better vocabulary for our model. With those\nadjustments, our KR-BER T model performed com-\nparably and even better than other existing pre-\ntrained models using a corpus about 1/10 of the\nsize.\n1 Introduction\nThe Bidirectional Encoder Representations from Transform -\ners (BER T) Multilingual model [Devlin et al., 2018 ] were\npre-trained on Wikipedia data for 104 languages and has\ndemonstrated powerful performances on a wide range of tasks\nin Natural Language Processing. Although it is meant to act\nas a universal language model, it does not always work well\nfor non-English downstream tasks. This is due to the fact tha t\nit cannot fully capture the speciﬁc linguistic characteris tics of\nevery language. T o accommodate for this, we have prepared\na single language model that incorporates this knowledge fo r\nKorean.\nDeveloping a BER T model for Korean NLP tasks is chal-\nlenging for two reasons. First, Korean is an agglutinative l an-\nguage, which is morphologically richer than inﬂectional la n-\nguages such as German or French. Secondly, its writing sys-\n∗ https://github.com/snunlp/KR-BERT\ntem, Hangul, is composed of more than 10,000 syllable char-\nacters. For these reasons, a proper BER T model for Korean\nrequires both a vocabulary and a tokenizer that can effectiv ely\ntreat a variety of complex word forms.\nAdditionally, the scale of the multilingual BER T model is\ntoo large to be applied to single-language tasks. The model i s\ntrained on data combining texts from 104 languages and uti-\nlizes a vocabulary including characters from those 104 lan-\nguages, which results in larger number of parameters. While\nthere are other existing large-scale single language model s\nsuch as XLNet or RoBER T a, such large models require a\nhuge amount of training data and resources, which makes\nit difﬁcult to train them for languages with fewer resources .\nTherefore it is important to scale the model down while still\nmaintaining comparable performances, as recent works such\nas ALBER T and DistilBER T do.\nIn this paper, we construct our small scale single language\nmodel by utilizing a smaller number of parameters and vo-\ncabulary entries, as well as using less training data. W e bui lt\na Korean BPE vocabulary with sub-characters as the small-\nest units instead of characters, and propose the Bidirectio nal-\nW ordPiece T okenizer to capture the characteristics of the K o-\nrean language. Our KR-BER T model, which we trained from\nscratch on our own corpus, outperforms the BER T -base Mul-\ntilingual model on four downstream tasks, speciﬁcally sent i-\nment analysis, Question-Answering, Named Entity Recogni-\ntion (NER), and Paraphrase Detection. The model shows the\nmost consistent results and is comparable to other Korean-\nlanguage BER T models such as KorBER T and KoBER T , de-\nspite its small scale.\n2 Related W ork\n2.1 Models after BERT\nSince the powerful performance that BER T demonstrated on\na wide range of tasks in NLP , various improved BER T mod-\nels have been shown. There is a trend towards bigger mod-\nels such as XLNet and RoBER T a which utilize a larger cor-\npora than the 16GB BER T and a large number of parameters.\nOn the other hand, smaller models such as DistilBER T and\nALBER T are implemented to overcome hardware limitations\nand longer training times of the huge models. T able 1 shows\na quick overview of existing models mentioned above.\nBERT XLNet RoBERT a DistilBERT ALBERT\nThe # of\nParameters (M)\nBase: 110\nLarge: 340\nBase: 110\nLarge: 340\nBase: 110\nLarge: 355 Base: 66 Base: 12\nLarge: 18\nData 16GB BER T data\n3.3B words\n13GB BER T data\n+ over 130GB\nadditional data\n33B words\nBER T data\n+ over 140GB\nadditional data\nBER T data BER T data\nMethod\nBER T\n(Bidirectional\nTransformer with\nMLM & NSP)\nBidirectional\nTransformer\nwith Permutation\nbased modeling\nBER T without NSP BER T Distillation\nFactorized Embedding,\nCross-layer\nParameter Sharing,\nSOP\nT able 1: An overview of existing BERT -based models. MLM, NSP , and SOP in the table stand for the tasks related to BERT: Mask ed Language\nModeling, Next Sentence Prediction, and Sentence Order Pre diction.\nKorBERT KoBERT Kakao NLP T eam\n(Park, 2018)\nKoreanCharacter\nBERT\nKalBert\nbased on ALBERT\nDistilKoBert\n(based on DistilBERT)\nT okenizer\nmorphome-level\nand character-level\n(W ordPiece)\ncharacter-level\n(SentencePiece) morpheme-level character-level\n(modiﬁed W ordPiece)\nmorpheme-level\n(BPE) wihtout tag\ncharacter-level\n(SentencePiece)\nData 23GB 25M sents,\n324M words Munjong Corpus its own\nKorean Dataset 6GB 6GB\nAdditional\nInformation\nDetails below Details below -\n7477 vocab\nSmall: 3 layers\nBase: full layers\n47471 vocab\n3 layers\n(reducing 12 layers\nof KoBER T)\nT able 2: An overview of existing Korean-speciﬁc models\n2.2 Language-speciﬁc BERT models\nMultilingual BER T is pre-trained on Wikipedia data for\n104 languages. Although the multilingual BER T shows re-\nmarkable cross-lingual ability, a variety of single-langu age\nBER T models are suggested for improvement. Language-\nspeciﬁc BER T models are trained on German 1, Italian\n[Polignano et al., 2019 ]2 , Finnish [V irtanen et al., 2019 ]3 ,\nand Japanese [Kikuta, 2019 ]4 . In addition, the CamemBER T\nmodel for French [Martin et al., 2019 ] is a RoBER T a model\n[Liu et al., 2019 ] that was trained on a French corpus of a\nsimilar size to the training data of the multilingual BER T , a nd\nthe Chinese BER T model trained on a Chinese Wikipedia is\na combination of BER T , BER T with Whole W ord Masking,\nand RoBER T a models [Cui et al., 2019 ]5 .\n2.3 Korean\nSubword Segmentation and T okenization\nBPE and its Applications. A typical BER T model uses ei-\nther the W ordPiece tokenizer [Wu et al., 2016 ] or the Senten-\ncePiece tokenizer [Kudo, 2018 ], which are both based on the\nByte-Pair-Encoding (BPE) algorithm [Sennrich et al., 2016 ].\n1 https://deepset.ai/german-bert\n2 https://github.com/marcopoli/AlBERT o-it\n3 https://github.com/TurkuNLP/FinBERT\n4 https://github.com/yoheikikuta/bert-japanese\n5 https://github.com/ymcui/Chinese-BERT -wwm\nLanguage-speciﬁc V ocabulary and Linguistic Structure.\nKim et al. [2019] suggest the use of a subword segmentation\nalgorithm in order to reﬂect the linguistic structure of Kor ean.\nThey utilize Korean linguistic characteristics such as par ticles\nand normalize subwords based on mutual information.\nPossibilities for tokenization of Korean include morpheme -\nlevel, character-level, and sub-character level models, h ow-\never, no sub-character level model has been presented so far .\nRecent Korean BERT models\nRecent Korean BER T pre-training models that have been\nopenly released include KorBER T (Electronics and T elecom-\nmunications Research Institute 2019) 6 and KoBER T (SK\nT elecom 2019) 7. The details of these models are examined\nin sections 4 and 5. Park [2018]8 reports that the Kakao NLP\nT eam is pre-training a BER T model with a Korean corpus.\nSome Korean models are presented on Github: KoreanChar-\nacterBert, KalBert and DistilKoBert. T able 2 shows the over -\nall comparison of those models.\n6 http://aiopen.etri.re.kr/service dataset.php\n7 https://github.com/SKTBrain/KoBERT\n8 http://docs.likejazz.com/bert\n3 The Need for a Small-scale\nLanguage-speciﬁc Model\nThe multilingual BER T model shows weak performance\ndealing with the Korean texts considering its scale. Also, t he\nlanguage-speciﬁc BER T models trained on other languages\nshow better performances than multilingual BER T model on\ndownstream tasks. W e analyze the disadvantages of the mul-\ntilingual BER T model below .\n3.1 Limit of Corpus Domain\nWhile the multilingual BER T model was trained on\nWikipedia texts from 104 languages, the language-speciﬁc\nBER T models like German BER T or French CamemBER T\nwere trained on several diverse data sources, including leg al\ndata, news articles, and so forth. The multilingual BER T is\ntherefore limited in its domain with respect to language us-\nage, since Wikipedia articles have different linguistic pr oper-\nties from other user-generated content, such as blogs or use r\ncomments [Ferschke, 2014; Habernal and Gurevych, 2017 ].\nThis weakness is especially notable for noisy user-generat ed\ntexts. T o solve this problem, we can expand the variety of dat a\nsources that the model is trained on.\n3.2 Considering Language-speciﬁc Properties\nRare “Character” Problem\nIn the process of subword tokenization of the transformer\nmechanism, misspelled or rare words can be handled for lan-\nguages written in Latin alphabets such as English by simply\nseparating the word into letters. For example, a token such a s\n‘lol’ which is not present in the multilingual BER T vocabu-\nlary can be tokenized into subword units such as l ##o ##l.\nThis is possible because the 26 characters of the Latin alpha -\nbet can be included in the whole vocabulary without signif-\nicantly increasing vocabulary size of the model. In contras t,\na Korean word such as 힉 hik “eek” can not be decomposed\nin the same manner due to the whole word being treated as\na single character, not present in the multilingual BER T vo-\ncabulary. Because Korean is organized by syllable and not by\nletter, augmenting the vocabulary, as is done with the Latin\nalphabet, would greatly increase the vocabulary size.\nThere are a total of 11,172 existing syllables in the written\nsystem of the language. However, only 1,187 syllables out of\nthem are included in the vocabulary of multilingual BER T ,\nwhich means that the excluded 9,985 syllables can not be an-\nalyzed properly. For this reason, we need a new BER T vo-\ncabulary that is able to solve this problem in order to improv e\nresults on Korean NLP tasks.\nInadequacy for Morphologically Rich Languages\nBecause Korean is an agglutinative language with generally\none feature encoded per sufﬁx, the morphology is much richer\nthan English. Fusional languages with grammatical afﬁxes\npotentially encoding several different features, such as F rench\nor German, also have more complex morphology than En-\nglish. However, for an agglutinative language there will be\ngreater difﬁculties in representing the vocabulary becaus e of\nits morphological complexity. Speciﬁcally, there is a lack of\nrepresentation for verb conjugations. Unlike for English, in\nlanguages such as Japanese and Korean, each verb conjuga-\ntion can have dozens of different forms.\nLack of Meaningful T okens\nFor the German language, it was found that the vocabulary\nof the multilingual BER T model contains a large number of\nindividual subword units that do not have a clear semantic\nmeaning. A similar type of problem is present for Korean, in\nwhich most words are tokenized as single characters, rather\nthan morpheme-like units. This signiﬁes that the multiling ual\nmodel would have difﬁculty with properly representing mul-\ntiple languages.\nT o solve these problems, we implemented the language-\nspeciﬁc vocabulary and tokenizer which can deal with Korean\ntexts and the language-speciﬁc properties of Korean. W e pro -\npose two levels of vocabularies: syllable character and sub -\ncharacter. One Korean character, which corresponds to one\nsyllable, can be decomposed into smaller sub-character uni ts,\nor graphemes. It is possible to construct the vocabulary mor e\nefﬁciently using sub-characters than cramming all the 11,1 72\nsyllables into the vocabulary.\n3.3 Large Scale of the Model\nExisting bigger models utilize very large amounts of data fo r\ntraining: the multilingual BER T model collected Wikipedia\ntexts from 104 languages, XLNet used 130GB and RoBER T a\nused 160GB of data. They also have a large number of param-\neters with multilingual BER T utilizing 167M and RoBER T a\nusing 355M parameters. These large-scale models require a\nhuge amount of resources in order to perform. W e hope to\nmaintain comparable performances using a smaller vocabu-\nlary, fewer parameters, and less training data.\n4 Models\nIn order to resolve the problems discussed above, we design\nthe Korean-speciﬁc KR-BER T model, with the process de-\nscribed below . It has a relatively small data size for pretra in-\ning, but still performs comparably to the existing Korean-\nspeciﬁc models. W e suggest both character and sub-characte r\nmodels with two tokenizers: BER T original tokenizer and our\nBidirectionalW ordPiece tokenizer. Our new vocabulary and\nBidirectinalW ordPiece tokenizer are designed to reﬂect fe a-\ntures of Korean language.\n4.1 Subcharacter T ext Representation\nKorean text is basically represented with Hangul syllable\ncharacters, which can be decomposed into sub-characters, o r\ngraphemes. T o accommodate such characteristics, we traine d\na new vocabulary and BER T model on two different repre-\nsentations of a corpus: syllable characters and sub-charac ters.\nWhen the BPE algorithm is applied to each of these two rep-\nresentations, the resulting tokenization can be different . For\nexample, a syllable character 뜀 ttwim “jumping” can be de-\ncomposed into two sub-character units ㄸㅟ ttwi “jump” and\nㅁ m “-ing” [Park and Shin, 2018 ].\nUnlike KorBER T and KoBER T , the KR-BER T model uses\na sub-character representation in addition to a character r ep-\nresentation. This method has the advantage of allowing us to\ncapture the common aspects of various Korean verb/adjectiv e\nCharacter representation (default) Sub-character repres entation (decomposed)\n갔 kass “went” ㄱk ㅏa “go” ㅆss “-ed”\n감 kam “going” ㄱk ㅏa “go” ㅁm “-ing”\n간 kan “that has/have gone” ㄱk ㅏa “go” ㄴn “that has/have . . . ”\n갈 kal “that will go” ㄱk ㅏa “go” ㄹl “that will . . . ”\nT able 3: T wo representation methods of the verb conjugation s of 가\nka “go”\nconjugation forms, since many grammatical morphemes are\nrealized only by partial elements of a syllable character.\nAs seen in T able 3, a sub-character representation can de-\ntect these elements without a morphological analyzer, whil e\nthe character representation cannot do so. It can capture wh at\nthe multilingual BER T cannot detect.\n4.2 Subword V ocabulary\nW e used the BPE algorithm to obtain a ﬁxed-size vocabulary.\nDifferent vocabulary sizes were experimented with, rangin g\nfrom 8,000 to 20,000. For each vocabulary size, we measured\nMasked LM Accuracy at 100,000 steps for the BER T pre-\ntraining stage. The best results were obtained with a vocabu -\nlary size of 10,000.\nAfter this step, we heuristically added special symbols fre -\nquently used in user-generated Korean textual data as well a s\nseveral other languages (Latin alphabet, Kana, Kanji etc.) to\nthe vocabulary.\nIn T able 4 we present the vocabulary sizes for our model,\ncomparing with multilingual BER T and several previous\nstudies on Korean pre-trained BER T . KR-BER T has a vo-\ncabulary of 16424 character tokens consisting of both sin-\ngle characters and concatenated characters as a subword\nunit. KR-BER T’s sub-character vocabulary has 12356 tokens\nwhich were constructed in the same manner. Our vocabulary\nsize is much smaller than multilingual BER T and KorBER T\nand similar to KoBER T .\n4.3 Subword T okenization\nW e used the W ordPiece tokenization [Wu et al., 2016 ] and\nour BidirectionalW ordPiece tokenization.\nBaselines\nThe baselines we used are the original W ordPiece tokeniza-\ntion and the SentencePiece tokenization [Kudo, 2018 ]. As\nin the original BER T model, the W ordPiece algorithm was\nused for tokenization. This resulted in the original corpus be-\ning segmented into subword level tokens. Multilingual BER T\nand KorBER T also used this algorithm. On the other hand,\nKoBER T used the SentencePiece algorithm as a comparison\nof subword tokenization methods.\nThe W ordPiece T okenizer used by multilingual BER T\nadopts the Byte-Pair-Encoding (BPE) [Sennrich et al., 2016 ]\nalgorithm. This algorithm is deterministic, breaking down a\nsentence into one unique sequence. In contrast, the Sentenc e-\nPiece T okenizer uses a Unigram Language Model. This prob-\nabilistic model investigates all possible segmentations a nd se-\nlects the one with the highest probability.\nMultilingual\nBERT KorBERT KoBERT KR-BERT\ncharacter\nKR-BERT\nsub-character\nvocabulary size 119547 30797 8002 16424 12367\nparameter size 167,356,416 109,973,391 92,186,880 99,265,066 96,145,23 3\ndata size\n-\n(The Wikipedia data\nfor 104 languages)\n23GB\n4.7B morphemes\n-\n25M sentences,\n324M words\n2.47GB\n20M sentences,\n233M words\n2.47GB\n20M sentences,\n233M words\nT able 4: Comparison of the vocabulary size, the number of par ame-\nters, and the training data size between the models trained o n Korean\ntexts\nMultilingual\nBERT KorBERT KoBERT KR-BERT\ncharacter\nKR-BERT\nsub-character\nwords\n(Hangul)\n1664\n(1.391%)\n12047\n(39.117%)\n4489\n(56.098%)\n7352\n(44.764%)\n6606\n(53.416%)\nsubwords\n(Hangul)\n1609\n(1.346%)\n8023\n(26.051%)\n2678\n(33.467%)\n3840\n(23.380%)\n2140\n(17.304%)\nsymbols and\nother languages\n116170\n(97.175%)\n10720\n(34.808%)\n830\n(10.372%)\n5227\n(31.825%)\n3616\n(29.239%)\nspecial tokens 5\n(0.004%)\n7\n(0.023%)\n5\n(0.062%)\n5\n(0.030%)\n5\n(0.040%)\ntotal 119547 30797 8002 16424 12367\nT able 5: V ocabulary composition of the models trained on Kor ean\ntexts\nBidirectionalW ordPiece T okenizer\nW e use the BidirectionalW ordPiece model to reduce search\ncosts while maintaining the possibility of choice. This mod el\napplies BPE in both forward and backward directions to ob-\ntain two candidates and chooses the one that has a higher fre-\nquency.\nIn the Korean language, typical noun phrases and verb\nphrases form in different ways. In a noun declension, a long\nstem is followed by one or two short particles. On the con-\ntrary, a verb conjugation has a short stem, to which multiple\nendings are attached. In the case of noun phrases, it is advan -\ntageous to match the longer subword unit from the left, while\nthe backward match is more appropriate for verb phrases,\nhence our use of a bidirectional tokenizer.\nOur tokenizer is designed to tokenize the data with the cor-\nresponding vocabulary, depending on the data representati on:\ncharacter or sub-character.\n4.4 Comparison with Other Korean Models\nW e compare our KR-BER T with Multilingual BER T , Kor-\nBER T , and KoBER T .\nT able 4 shows the comparison of the vocabulary size, the\nnumber of parameters, and the size of the training data be-\ntween the models. KorBER T has a large vocabulary, many\nparameters, and a large data size which results in a high\nmemory requirement and longer training time. Meanwhile,\nKoBER T has a smaller vocabulary and fewer parameters than\nKR-BER T , but our model has a smaller data size.\nT able 5 represents the vocabulary composition of each\nmodel. As the proportion of words and subwords in Hangul\nshows, a Korean-speciﬁc model such as KorBER T , KoBER T\nand our KR-BER T is perferable to the multilingual BER T .\nT able 6 compares the tokenization (segmentation) results.\nMultilingual BER T often results in the [UNK] token. T okens\nfrom KR-BER T models retain the original meanings well,\nMultilingual\nBERT\nKorBERT\ncharacter KoBERT\nKR-BERT\ncharacter\nW ordPiece\nKR-BERT\ncharacter\nBidirectionalW ordPiece\nKR-BERT\nsub-character\nW ordPiece\nKR-BERT\nsub-character\nBidirectionalW ordPiece\nnayngcangko\n“refrigerator” nayng#cang#ko nayng#cang#ko nayng#cang#ko nayngcangko n ayngcangko nayngcangko nayngcangko\nchwupta\n“cold” [UNK] chwup#ta chwup#ta chwup#ta chwup#ta chwu#pta chwu#p ta\npaytsalam\n“seaman” [UNK] payt#salam payt#salam payt#salam payt#salam pay#t# salam pay#t#salam\nmaikhu\n“microphone” ma#i#khu mai#khu ma#i#khu maikhu maikhu maikhu maikhu\nT able 6: T okenization Examples of Korean-speciﬁc models\nproving the superiority of our tokenizer and vocabularies.\nFrequent word 냉장고nayngcangko ‘refrigerator’ is only tok-\nenized by KR-BER T . It seems that the heuristically added vo-\ncabularies and sub-character level segmentation of our mod -\nels improve the tokenization. Furthermore, KR-BER T models\nare more robust when it comes to foreign loan words ( 마이크\nmaikhu ‘microphone’) and, especially for sub-character mo d-\nels, conjugation. For example, 춥다 chwupta ‘cold’ is seg-\nmented as its conjugation form 추 chwu and ㅂ다 pta in our\nKR-BER T sub-character model. 뱃사람 paytsalam ‘seaman’\nis segmented as 배 pay ‘boat’ and 사람 salam ‘man’ with ㅅ\nt between the tokens due to a phonological rule.\n5 Experiments and Results\nOur experiments include pre-training the BER T model on\ndata obtained from Korean Wikipedia and news articles, fol-\nlowed by running experiments on the downstream tasks of\nsentiment classiﬁcation, question answering, named entit y\nrecognition and paraphrase detection. The results are com-\npared against the baseline of the pre-trained multilingual\nBER T model and the existing Korean-only BER T models.\n5.1 Dataset and Preprocessing\nFor data preprocessing, we extracted text from the Ko-\nrean Wikipedia dump using WikiExtractor, omitting meta-\ndata and history. W e also extracted crawled news articles,\nsuch as Chosun Ilbo. After this, we tokenized all the text\ninto sentences using the Natural Language T oolkit (NL TK)\n[Loper and Bird, 2002 ].\nFor our sub-character version, we decomposed all Hangul\nsyllable characters into their sub-characters using Unico de’s\nNormalization Form D.\n5.2 Results\nMasked LM Accuracy\nW e present the training results of the masked language mod-\nels in T able 7. The masked LM accuracy of multilingual\nBER T and KorBER T were not speciﬁed. W e observe that our\ncharacter based model and sub-character based model have\nhigher accuracy than KoBER T , and when using a Bidirec-\ntionalW ordPiece tokenizer we can also get slightly better l an-\nguage model accuracy.\nModel Masked LM Accuracy\nMultilingual BERT n/a\nKorBERT n/a\nKoBERT 0.750\nKR-BERT character W ordPiece 0.773\nKR-BERT character BidirectionalW ordPiece 0.779\nKR-BERT sub-character W ordPiece 0.761\nKR-BERT sub-character BidirectionalW ordPiece 0.769\nT able 7: Masked LM Accuracy\nDownstream tasks\nW e tested sentiment classiﬁcation, question answering,na med\nentity recognition and paraphrase detection using Naver\nSentiment Movie Corpus, KorQuAD, KorNER, and Korean\nPaired Question dataset.\nW e use a batch size of 32 and 64 and ﬁne-tuned for 5\nepochs over the data for all tasks. For each task, we selected\nthe best batch size, the best epochs, and the best ﬁne-tuning\nlearning rate (among 1e-3, 2e-5, 5e-5) on the dev set.\nResults are presented in T able 8. KR-BER T performs com-\nparably or better than the other Korean based model, and it\nhas more stable higher results than others.\nIn the NSMC task, while KorBER T shows the highest ac-\ncuracy, all the KR-BER T models more consistently show high\naccuracies compared to KoBER T and multilingual BER T .\nKR-BER T character W ordPiece model, followed by sub-\ncharacter BidirectionalW ordPiece model, achieves the hig h-\nest F1 score at the KorQuAD. At the KorNER task, all the\nKR-BER T sub-character models appear to be the best models\nwith the original W ordPiece tokenizer model recording the\nhighest and our BidirectionalW ordPiece tokenizer model fo l-\nlowing the second highest. KR-BER T character models had\nthe next highest score, displaying big differences between\nKR-BER T and the other models. On the Paraphrase detec-\ntion task, KorBER T is the most accurate model, but only by a\nnarrow margin and the KR-BER T character W ordPiece model\nreveals the second highest accuracy.\nAlthough for NSMC and Paraphrase Detection, our KR-\nBER T model has lower results than the KorBER T model, the\nresults are comparable and it still obtains about 7% improve -\nment over the KorBER T model for KorQuAD and KorNER.\nModel NSMC (Acc.) KorQuAD (F1) KorNER (F1) Paraphrase\nDetection (Acc.)\nMultilingual BERT (Google) 87.08 89.58 61.52 79.55\nKorBERT (ETRI) 89.84 83.73 59.43 93.79\nKoBERT (SKT) 89.01 n/a n/a 91.03\nKR-BERT character W ordPiece 89.34 89.92 64.97 93.54\nKR-BERT character BidirectionalW ordPiece 89.38 89.18 64.50 92.74\nKR-BERT sub-character W ordPiece 89.20 89.31 66.64 93.14\nKR-BERT sub-character BidirectionalW ordPiece 89.34 89.78 66.28 92.74\nT able 8: Performances on Downstream T asks\nMultilingual\nBERT\nKorBERT\ncharacter KoBERT\nKR-BERT\ncharacter\nW ordPiece\nKR-BERT\ncharacter\nBidirectionalW ordPiece\nKR-BERT\nsub-character\nW ordPiece\nKR-BERT\nsub-character\nBidirectionalW ordPiece\niyenghwa\n“thismovie”\ni#yeng#hwa\n(incorrect)\niyenghwa\n(incorrect)\ni#yengwha\n(correct)\niyeng#hwa\n(incorrect)\ni#yenghwa\n(correct)\niyeng#hwa\n(incorrect)\ni#yenghwa\n(correct)\ncaymisnunteyng\n“It’s amusing”\n[UNK]\n(incorrect)\ncay#mis#nun#teyng\n(incorrect)\ncay#mis#nun#teyng\n(incorrect)\ncay#mis#nun#teyng\n(incorrect)\ncay#mis#nun#teyng\n(incorrect)\ncaymi#s#nuntey#ng\n(correct)\ncaymi#s#nuntey#ng\n(correct)\nT able 9: T okenization Examples of Korean-speciﬁc models\n5.3 Analysis of Downstream T asks\nFor all of the KR-BER T models, the character W ordPiece\nmodel achieved remarkable results on all of the downstream\ntasks. Our new KR-BER T sub-character BidirectionalW ord-\nPiece model does not perform as well but still shows good\nresults consistently, especially for the NSMC and NER tasks .\nThe BidirectionalW ordPiece tokenizer models display high er\naccuracy for the NSMC, and the NER the F1 score from\nits sub-character models are the highest. W e emphasize that\nNSMC, the dataset for sentiment classiﬁcation, is noisy use r-\ngenerated data consisting of real comments from users. Ad-\nditionally, the NER task contains a lot of out-of-vocabular y\n(OOV) words, mostly consisting of low-frequency noun\nstems. The high performance of KR-BER T with the Bidirec-\ntionalW ordPiece tokenizer shows that this tokenizer and su b-\ncharacter level representation are able to effectively tac kle the\nOOV problem, or the incorrect spacing problem. An example\nof this from the NSMC dataset can be seen in T able 9.\nLike other movie review data, the NSMC contains vari-\nous types of non-standard language use, including deliber-\nate spelling errors exempliﬁed in T able 9. The phrase, 재밋\n는뎅 cay.mis.nun.teyng, whose standard form is 재밌는데\ncay.miss.nun.tey “It’s amusing, ” should be easily classiﬁ ed\nif tokenized properly to capture the meaningful words 재미\ncaymi “amusing” and 는데 nuntey “it’s, ” as in the KR-BER T\nsub-character models are able to do.\nBecause NER and paraphrase detection consists of rela-\ntively formal data, models using the W ordPiece tokenizer\nhave higher scores than models using BidirectionalW ord-\nPiece tokenizer.\nEspecially in NER which has more low frequency proper\nnouns than other datasets, sub-character based models get\nmuch higher F1 scores because they can capture the unknown\nwords more effectively. T o look into the OOV problem in\nmore detail, T able 10 shows the [UNK] ratio for each model,\nModel # of [UNK] total # of tokens [UNK] ratio\nMultilingual BERT 3461 337741 0.01024\nKorBERT 16 300437 0.00005\nKR-BERT character\nW ordPiece 2336 293418 0.00796\nKR-BERT character\nBidirectionalW ordPiece 2336 293441 0.00796\nKR-BERT sub-character\nW ordPiece 45 293266 0.00015\nKR-BERT sub-character\nBidirectionalW ordPiece 45 293467 0.00015\nT able 10: [UNK] ratio of each model for KorNER data\nrepresenting the number of words tokenized as [UNK] di-\nvided by the total number of words.\nBecause KorQuAD and paraphrase detection consists of\nrelatively formal data, models using the W ordPiece tokeniz er\nhave much higher scores than models using the Bidirection-\nalW ordPiece tokenizer.\n6 Conclusion\nW e have demonstrated that our Korean-speciﬁc KR-BER T\nmodel can effectively deal with Korean NLP tasks. The KR-\nBER T model shows a higher performance than multilingual\nBER T in both Masked LM accuracy and downstream tasks,\nwhich means it captures language-speciﬁc linguistic pheno m-\nena. Compared to other Korean models, KR-BER T has higher\nor comparable results in downstream tasks. W e introduced a\nsub-character based model and the BidirectionalW ordPiece\ntokenizer to deal with a morphologically rich language with\npoor resources, and can conﬁrm that a sub-character based\nKorean model and BidirectionalW ordPiece tokenizer have ef -\nﬁcacy in certain datasets. Our contribution was to test a sub -\ncharacter based Korean model using its morphological fea-\ntures and the BidirectionalW ordPiece tokenizer with a smal l\npre-training corpus. For future work, we plan to continue\nmaking much smaller and faster Korean language models by\ncombining current approaches with distilling methods.\nReferences\n[Cui et al., 2019 ] Y iming Cui, W anxiang Che, Ting Liu,\nBing Qin, Ziqing Y ang, Shijin W ang, and Guoping Hu.\nPre-training with whole word masking for chinese bert.\narXiv preprint arXiv:1906.08101, 2019.\n[Devlin et al., 2018 ] Jacob Devlin, Ming-W ei Chang, Ken-\nton Lee, and Kristina T outanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Ferschke, 2014 ] Oliver Ferschke. The quality of content in\nopen online collaboration platforms: Approaches to NLP-\nsupported information quality management in W ikipedia.\nPhD thesis, T echnische Universit¨ at, 2014.\n[Habernal and Gurevych, 2017 ] Ivan Habernal and Iryna\nGurevych. Argumentation mining in user-generated web\ndiscourse. Computational Linguistics , 43(1):125–179,\n2017.\n[Kikuta, 2019 ] Y ohei Kikuta. Bert pretrained\nmodel trained on japanese wikipedia articles.\nhttps://github.com/yoheikikuta/bert-japanese, 2019.\n[Kim et al., 2019 ] Mansu Kim, Y unkon Kim, Y eonsoo Lim,\nand Eui-Nam Huh. Advanced subword segmentation and\ninterdependent regularization mechanisms for korean lan-\nguage understanding. In 2019 Third W orld Conference\non Smart T rends in Systems Security and Sustainablity\n(W orldS4), pages 221–227. IEEE, 2019.\n[Kudo, 2018 ] T aku Kudo. Subword regularization: Improv-\ning neural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics (V ol-\nume 1: Long P apers), pages 66–75, Melbourne, Australia,\nJuly 2018. Association for Computational Linguistics.\n[Liu et al., 2019 ] Y inhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and V eselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[Loper and Bird, 2002 ] Edward Loper and Steven Bird.\nNltk: the natural language toolkit. arXiv preprint\ncs/0205028, 2002.\n[Martin et al., 2019 ] Louis Martin, Benjamin Muller, Pedro\nJavier Ortiz Su ´ arez, Y oann Dupont, Laurent Romary,\n´Eric V illemonte de la Clergerie, Djam ´ e Seddah, and Benoˆ ıt\nSagot. Camembert: a tasty french language model. arXiv\npreprint arXiv:1911.03894, 2019.\n[Park and Shin, 2018 ] Suzi Park and Hyopil Shin.\nGrapheme-level awareness in word embeddings for\nmorphologically rich languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), 2018.\n[Park, 2018 ] Sang-kil Park. Looking into bert (bert\nthophapoki). http://docs.likejazz.com/bert, 2018.\n[Polignano et al., 2019 ] Marco Polignano, Pierpaolo Basile,\nMarco de Gemmis, Giovanni Semeraro, and V alerio\nBasile. Alberto: Italian bert language understanding mode l\nfor nlp challenging tasks based on tweets. In Proceedings\nof the Sixth Italian Conference on Computational Linguis-\ntics (CLiC-it 2019), volume 2481. CEUR, 2019.\n[Sennrich et al., 2016 ] Rico Sennrich, Barry Haddow , and\nAlexandra Birch. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics\n(V olume 1: Long P apers), pages 1715–1725, Berlin, Ger-\nmany, August 2016. Association for Computational Lin-\nguistics.\n[V irtanen et al., 2019 ] Antti V irtanen, Jenna Kanerva, Rami\nIlo, Jouni Luoma, Juhani Luotolahti, T apio Salakoski,\nFilip Ginter, and Sampo Pyysalo. Multilingual\nis not enough: Bert for ﬁnnish. arXiv preprint\narXiv:1912.07076 , 2019.\n[Wu et al., 2016 ] Y onghui Wu, Mike Schuster, Zhifeng\nChen, Quoc V Le, Mohammad Norouzi, W olfgang\nMacherey, Maxim Krikun, Y uan Cao, Qin Gao, Klaus\nMacherey, et al. Google’s neural machine translation sys-\ntem: Bridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144, 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8331381678581238
    },
    {
      "name": "Language model",
      "score": 0.7673225998878479
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6549755334854126
    },
    {
      "name": "Natural language processing",
      "score": 0.5863965749740601
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5645704865455627
    },
    {
      "name": "Vocabulary",
      "score": 0.5352625846862793
    },
    {
      "name": "Sentence",
      "score": 0.4909570515155792
    },
    {
      "name": "Construct (python library)",
      "score": 0.47317153215408325
    },
    {
      "name": "Embedding",
      "score": 0.4701792299747467
    },
    {
      "name": "Lexical analysis",
      "score": 0.4587235152721405
    },
    {
      "name": "Linguistics",
      "score": 0.2750268578529358
    },
    {
      "name": "Programming language",
      "score": 0.057371169328689575
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}