{
    "title": "WTASR: Wavelet Transformer for Automatic Speech Recognition of Indian Languages",
    "url": "https://openalex.org/W4310295314",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2188110119",
            "name": "Tripti Choudhary",
            "affiliations": [
                "GLA University"
            ]
        },
        {
            "id": "https://openalex.org/A1979243048",
            "name": "Vishal Goyal",
            "affiliations": [
                "GLA University"
            ]
        },
        {
            "id": "https://openalex.org/A2109659196",
            "name": "Atul Bansal",
            "affiliations": [
                "Chandigarh University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2091432990",
        "https://openalex.org/W2321786725",
        "https://openalex.org/W2026303233",
        "https://openalex.org/W811578723",
        "https://openalex.org/W2547793174",
        "https://openalex.org/W2963920996",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2886180730",
        "https://openalex.org/W1995562189",
        "https://openalex.org/W2749128330",
        "https://openalex.org/W3016010032",
        "https://openalex.org/W2963414781",
        "https://openalex.org/W6734241735",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W3161873870",
        "https://openalex.org/W2980109192",
        "https://openalex.org/W4200525300",
        "https://openalex.org/W2593910181"
    ],
    "abstract": "Automatic speech recognition systems are developed for translating the speech signals into the corresponding text representation. This translation is used in a variety of applications like voice enabled commands, assistive devices and bots, etc. There is a significant lack of efficient technology for Indian languages. In this paper, an wavelet transformer for automatic speech recognition (WTASR) of Indian language is proposed. The speech signals suffer from the problem of high and low frequency over different times due to variation in speech of the speaker. Thus, wavelets enable the network to analyze the signal in multiscale. The wavelet decomposition of the signal is fed in the network for generating the text. The transformer network comprises an encoder decoder system for speech translation. The model is trained on Indian language dataset for translation of speech into corresponding text. The proposed method is compared with other state of the art methods. The results show that the proposed WTASR has a low word error rate and can be used for effective speech recognition for Indian language.",
    "full_text": "BIG DATA MINING AND ANALYTICS\nISSN 2096-0654 08/10 pp 85 – 91\nVolume 6, Number 1, March 2023\nDOI: 10.26599/BDMA.2022.9020017\n\rC The author(s) 2023. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nWTASR: Wavelet Transformer for Automatic Speech Recognition of\nIndian Languages\nTripti Choudhary\u0003, Vishal Goyal, and Atul Bansal\nAbstract: Automatic speech recognition systems are developed for translating the speech signals into the\ncorresponding text representation. This translation is used in a variety of applications like voice enabled commands,\nassistive devices and bots, etc. There is a signiﬁcant lack of efﬁcient technology for Indian languages. In this paper,\nan wavelet transformer for automatic speech recognition (WTASR) of Indian language is proposed. The speech\nsignals suffer from the problem of high and low frequency over different times due to variation in speech of the\nspeaker. Thus, wavelets enable the network to analyze the signal in multiscale. The wavelet decomposition of the\nsignal is fed in the network for generating the text. The transformer network comprises an encoder decoder system\nfor speech translation. The model is trained on Indian language dataset for translation of speech into corresponding\ntext. The proposed method is compared with other state of the art methods. The results show that the proposed\nWTASR has a low word error rate and can be used for effective speech recognition for Indian language.\nKey words:transformer; wavelet; automatic speech recognition (ASR); Indian language\n1 Introduction\nAutomatic speech recognition (ASR) is a signiﬁcant area\nof research under the pattern recognition ﬁeld. ASR\ncomprises of multiple technologies for transforming the\nspeech signals into its corresponding text. The objective\nof ASR is to enable machines to translate the speech\nsignal into textual form. Many researchers worldwide\nare working on this problem to further improve efﬁciency\nand accuracy. And even the organizations like Amazon,\nApple, Google, IBM, etc. have also developed high end\nspeech recognition systems for English language[1]. The\ndevelopment of ASR for Indian languages is limited.\nThus, there is a great need of development of algorithms\nfor Indian languages. The speech signals comprise of a\n\u000fTripti Choudhary and Vishal Goyal are with the Department\nof Electronics and Communication, GLA University, Mathura\n281406, India. E-mail: triptichoudhary06@gmail.com; vishal.\ngoyal@gla.ac.in.\n\u000fAtul Bansal is with Chandigarh University, Mohali 140413,\nIndia. E-mail: atul.bansal@cumail.in.\n* To whom correspondence should be addressed.\nManuscript received: 2022-05-31; revised: 2022-06-06;\naccepted: 2022-06-21\nlot of heterogeneity in terms of language, speaker’s voice,\nvariations in the channel and so on. This heterogeneity\nmay be owed to various factors like the gender, accent,\nage, environmental conditions and also the speed of\nthe speaker. The ASR systems must be trained in a\nmanner to overcome all these limitations. In this regard\nthe training data length and the device with which the\nsignal is recorded also play important roles. An ASR\nsystem is considered to be efﬁcient if it is able to translate\nthe speech into its corresponding text despite all these\nchallenges. The training data for Indian languages are\nstill scarce and the text corpus for Indian languages is\nlimited[2]. Thus, the Indian languages need efﬁcient ASR\nsystems that can perform the recognition in such limited\nresources. Many researchers are working in the ﬁeld\nof ASR and multiple techniques are being applied to\nspeech-to-text conversion. Artiﬁcial neural networks\n(ANN) have been widely used for providing speech\nrecognition systems[3]. Hybrid hidden Markov model\n(HMM) is also being used by many researchers for the\npurpose of ASR[4]. Speech recognition models mainly\nfall under two categories: acoustic model and language\nmodel. In case of acoustic model, sound signals are\n86 Big Data Mining and Analytics, March2023, 6(1): 85–91\nanalyzed and converted into text or any other phonetic\nrepresentation[5]. However, the language models work\ntowards discovering the grammar, words, and sentence\nstructure of any language. Multiple machine learning\nand HMM based techniques are used traditionally for\nASR[6–10].\nBut with the advancement of deep learning models\nin the last decade, deep learning based solutions have\nreplaced these traditional techniques[11]. Different deep\nnetworks like convolutional neural networks (CNN) and\nrecurrent neural networks (RNN) are used for ASR[12]. In\nRef. [13], an encoder decoder RNN is presented for\nASR. The encoder comprises of multiple long short-term\nmemory (LSTM) layers. These layers are pre-trained to\nidentify phonemes, graphemes, and words. A residual\n2D-CNN is also used for speech recognition in which the\nresidual block comprises of connections amid previous\nand next layers[14].\nIn speech recognition the number of frames in an\naudio signal is much higher as compared to other\nforms. Therefore, the CNN model was modiﬁed, and\nthe transformer network evolved. Transformers are\nwidely used in various natural language processing\n(NLP) applications. One of the most successful areas is\nspeech recognition. Transformers provide the ability of\nsequence-to-sequence translation[15]. Many researchers\nhave used transformer in speech recognition and\ntranslation for different languages [16]. The speech\nrecognition systems for Indian language are very few.\nThere are a lot of use cases that require ASR for\nIndian language[17]. Thus, in this paper a transformer\nmodel for Hindi language speech recognition is\nproposed. The transformer model is augmented with\nwavelets for feature extraction. The wavelet can\nanalyze the acoustic signal at multiscale. Thus, the\nfeatures are extracted using discrete wavelet transform\n(DWT). These features are fed in the transformer model\nto generate the corresponding text. The transformer\nmodel is trained using Indian dataset for efﬁcient speech-\nto-text translation.\nThe paper is organized in ﬁve sections. Introduction\nto the research problem and literature review is proposed\nin Section 1. Section 2 introduces the proposed\nmethodology. Section 3 details the experiments. The\nresults are discussed in Section 4. Section 5 gives the\nconclusion of the proposed work.\n2 Proposed Methodology\nThe proposed methodology is shown in Fig. 1. The\nmethod hybrids the power of wavelet feature extraction\nwith a transformer deep learning network for speech-\nto-text conversion. Wavelet transform can change the\n“scale” parameter to ﬁnd different frequencies in the\nsignal along with their location. So, now we know which\nfrequencies exist in the time signal and where they exist.\nSmaller scale means that wavelets are squished. So, they\ncan capture higher frequencies. On the other hand, a\nlarger scale can capture lower frequencies. Therefore,\nthe use of wavelets overcomes the problem of pitch\nand frequency of the audio signal. As shown in the\nblock diagram, there are two main stages: wavelet\nfeature extraction and the transformer network. These\nare discussed in the following sections.\n2.1 Wavelet feature extraction\nWavelet transforms (WT) are extremely useful for\nthe analysis of signals as they are able to perform\nmultiscale analysis. More explicitly, dissimilar to the\nshort-time Fourier trasform (STFT) that gives uniform\ntime resolution to all frequencies, DWT gives high\ntime resolution and low recurrence resolution for high\nfrequencies, and high recurrence resolution and low time\nresolution for low frequencies. In that regard, it is like the\nhuman ear which displays comparable time-recurrence\nresolution qualities. DWT is a unique instance of WT\nthat gives a minimal portrayal of a sign on schedule\nand recurrence that can be ﬁgured proﬁciently[18]. The\ndiscrete wavelet transform is used for audio signals.\nT.a;b/D 1pa\nZ 1\n\u00001\nx.t/ \u0002 .t\u0000b/\na dt (1)\nwhere ais scale or dilation parameter, bis location of\nwavelet,  is wavelet function, and xis the signal.\nThe scale is used to represent the spread or squish of\nthe wavelet.\n2.2 Transformer network for speech recognition\nTransformer networks are widely used for speech\nrecognition tasks. A speech transformer is made up\nof two main parts, i.e. encoder and decoder. The\ntask of encoder is to take a speech feature sequence\n(x1, x2, . . . , xT / and transform it into a hidden\nrepresentation H = ( h1, h2, . . . , hL/. The decoder\nworks in contrast to the encoder. It takes the input\nH and transforms it into the character sequence\n(y1;y2;:::;y S /of the corresponding text. The decoder\nconsiders the previous output when predicting the next\ncharacter of the sequence. Conventionally, spectrogram\ninputs and word embeddings were used for speech-to-\nTripti Choudhary et al.: WTASR: Wavelet Transformer for Automatic Speech Recognition of Indian Languages 87\nWavelet decomposition\n2D \nconvolution\nPositional \nencoding\n Layer norm\nMulti-head attention\nFeed forward network \nLayer norm\nLayer norm\nLayer norm\n \nMulti-head attention\nLayer norm\nMulti head attention\nFeed forward nework\nLayer norm\nLayer norm\nLinear\nSoftmax\nOutput probabilities\nCharacter sequence\nCharacter embedding\nPositional encoding\nOutput\nSpeech \nsignal\nEncoder\nDecoder\nFig. 1 Proposed methodology.\ntext conversion. But the transformer network replaces\nthese by using the concept of multi-head attention and\nposition wise feed forward networks.\nThe encoder and decoder comprise of N transformer\nlayers. The encoder layers work continuously for\nreﬁning the input sequence representation. These layers\ncombine multi-head self-attention and frame-level afﬁne\ntransformations for the reﬁning process. Self-attention\nrefers to the process which communicates the different\npositions of input sequences to compute representations\nfor the inputs.\nThe input for computing self-attention is a\ncombination of three components: keys (K/, values (V/,\nand queries (Q/. The attention value is computed using\nscaled dot product as shown in Eq. (1).\nAttention .Q;K;V /Dsoftmax\n\u0012QKT\npdk\n\u0013\nV (2)\nwhere Q2Rnq\u0002dq ;K 2Rnk\u0002dk and V 2Rnv\u0002dv are\nthe queries, keys, and values, whereddenotes dimension\nand ndenotes the sequence lengths, dq Ddk and nk D\nnv:\nThe output of a query is calculated by computing the\nweighted sum of the values. The weight of the query\nis calculated through the query function along with\nthe related key. The multiple attentions are combined\ntogether using multi-head attention. It is calculated by\ntaking the product of head number ( h/and scaled dot-\nproduct Attention. The multi-head attention is computed\nusing Eq. (3).\nMultiHead .Q;K;V /D\nConcat.head1;head2;:::; headh/W o (3)\nwhere\nheadi DAttention.QWQ\ni ;KW K\ni ;VW V\ni /:\nThe dimension of Q;K; and V is same as that of\ndmodel ; the projection matrices WQ\ni 2 Rdmodel \u0002dq ,\n88 Big Data Mining and Analytics, March2023, 6(1): 85–91\nWK\ni 2 Rdmodel \u0002dk , WV\ni 2 Rdmodel \u0002dv , Wo 2\nRhdv\u0002dmodel .\nThe decoder carries out multi-head attention in two\nrounds. Firstly, self-attention is computed based on the\nprevious output sequence generated (QDKDV/. In\nthe second round, attention of the output of the encoder\nﬁnal layer is computed. The output character sequence\nat each layer is predicted by making use of the previous\nlayer.\nThe ﬂowchart of the proposed techniques is shown in\nFig. 2. The input feature sequence used here is wavelets.\nThe wavelet features are fed into the transformer\nnetwork. The transformer network comprises of 2D\nconvolution layers along with normalization layer and\nReLU activation function. Further 2D max pooling is\ndone. This goes as an input sequence to the encoder\nfollowed by the decoder. The decoder generates the\ncorresponding character sequence.\n3 Experiment\nThe experiments are conducted using the speech dataset\nfor Hindi Language. The length of the training data\nis 95.05 hours and testing data length is 5.55 hours\n(http://www.openslr.org/103/). The dataset comprises\nof unique sentences from Hindi stories. The data has\nhigh variability with a total of 78 different speakers. The\nsampling rate of the audio is 8 kHz with an encoding\nof 16 bit. The vocabulary size is 6542 including both\ntraining and testing datasets. Some sample speech text\nis as follows:\nयह\n है\n मोटा राजा\nमोटे राजा का है दुबला कु \u0000ा\nमोटा राजा व दुबला कु \u0000ा घूमने िनकले\nवह उसके पीछे भागा\nThe model is developed using the Python Keras\nmodule. Thr transformer model is trained using GPU\nsupport in Google Colab. The optimization is done using\nthe Adam Optimizer. The learning rate is initialized to\n10 and the number of epochs used is 100. Each speech\nsignal was decomposed using wavelet transform. A\nsample wavelet decomposition of the signal is shown in\nFig. 3.\n4 Result\nThe performance of the proposed system is computed\nusing the word error rate (WER). It is a metric used for\nspeech recognition or machine translation system. WER\nis computed as follows.\nConv2D\nLayer norm + ReLU \n2D max pooling \nEncoder \n2× \nDecoder Layer norm + ReLU \nConv1D \nEmbeddings \nOutput\ntext\nयह है मोटा राजा\nInput \n4× \nFig. 2 Proposed model for ASR.\nTripti Choudhary et al.: WTASR: Wavelet Transformer for Automatic Speech Recognition of Indian Languages 89\nFig. 3 Wavelet decomposition.\nWERD.SCI CD/=N\nwhere S denotes number of substitutions, D denotes\ndeletions, and I denotes insertions.\n\u000fSubstitutions. When the system transcribes one\nword in place of another. Transcribing the ﬁfth word\nas “this” instead of “the” is an example of a substitution\nerror.\n\u000fDeletions. When the system misses a word entirely.\nIn the example, the system deleted the ﬁrst word “well”.\n\u000fInsertions. When the system adds a word into the\ntranscript that the speaker did not say, such as “or”\ninserted at the end of the example.\nThe proposed method is compared with other state-of-\nthe-art methods for Indian language. The WER of these\nsystems is recorded and shown in the following Table 1.\nTable 1 shows that the WER for WTSAR is less than\n5% and thus it can be considered for practical uses.\nThe WER of other methods is higher than the proposed\nmethod (see Fig. 4). The performance can be further\nimproved by increasing the length of training data.\nTable 1 WER for Indian language.\n(%)\nMethod Hindi Marathi\nCNN 6.3 6.1\nRNN 5.9 6.2\nTransformer 5.2 5.0\nWTASR 4.8 4.9\n5 Conclusion\nTransformer networks are being widely used for\nASR. In this paper a wavelet enabled transformer\nmodel for Indian language is proposed. The proposed\nmodel overcomes the variations of speech signals like\nvariability in voice, gender, speed of utterance, etc. The\nwavelets can analyze a signal at multiscale. With the\nuse of wavelets, the features are extracted from speech\nsignals. These features consider the variability of speech\nsignals. These features are used by the Transformer\nmodel to predict the corresponding character sequence.\nThe limited data sources available for Indian language\nmake it signiﬁcant to develop an efﬁcient model. The\nperformance of the proposed model is compared with\nother state-of-the-art methods. The model gives a\nsigniﬁcant WER for Indian language and may be used\nfor multiple ASR applications.\nFig. 4 Graph of WER for Indian languages.\n90 Big Data Mining and Analytics, March2023, 6(1): 85–91\nReferences\n[1] L. Deng, G. Hinton, and B. Kingsbury, New types of deep\nneural network learning for speech recognition and related\napplications: An overview, in Proc. 2013 IEEE Int. Conf.\non Acoustics, Speech and Signal Processing, Vancouver,\nCanada, 2013, pp. 8599–8603.\n[2] S. R. Shahamiri and S. S. B. Salim, A multi-views multi-\nlearners approach towards dysarthric speech recognition\nusing multi-nets artiﬁcial neural networks, IEEE Trans.\nNeural Syst. Rehabil. Eng., vol. 22, no. 5, pp. 1053–1063,\n2014.\n[3] S. R. Shahamiri and S. S. B. Salim, Artiﬁcial neural\nnetworks as speech recognisers for dysarthric speech:\nIdentifying the best-performing set of MFCC parameters\nand studying a speaker-independent approach, Adv. Eng.\nInf., vol. 28, no. 1, pp. 102–110, 2014.\n[4] H. Bourlard and N. Morgan, Connectionist Speech\nRecognition: A Hybrid Approach. Boston, MA, USA:\nKluwer Academic Publishers, 1994.\n[5] C. Espa˜na-Bonet and J. A. R. Fonollosa, Automatic speech\nrecognition with deep neural networks for impaired speech,\nin Proc. 3rd Int. Conf. on Advances in Speech and\nLanguage Technologies for Iberian Languages, Lisbon,\nPortugal, 2016, pp. 97–107.\n[6] H. Sak, A. W. Senior, K. Rao, and F. Beaufays, Fast\nand accurate recurrent neural network acoustic models for\nspeech recognition, in Proc. 16th Annu. Conf. of the Int.\nSpeech Communication Association, Dresden, Germany,\n2015, pp. 1468–1472.\n[7] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, Listen, attend and\nspell: A neural network for large vocabulary conversational\nspeech recognition, in Proc. 2016 IEEE Int. Conf. on\nAcoustics, Speech and Signal Processing, Shanghai, China,\n2016, pp. 4960–4964.\n[8] C. C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P.\nNguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E.\nGonina, et al., State-of-the-art speech recognition with\nsequence-to-sequence models, in Proc. 2018 IEEE Int.\nConf. on Acoustics, Speech and Signal Processing, Calgary,\nCanada, 2018, pp. 4774–4778.\n[9] T. Hori, J. Cho, and S. Watanabe, End-to-end speech\nrecognition with word-based Rnn language models, in\nProc. 2018 IEEE Spoken Language Technology Workshop,\nAthens, Greece, 2018, pp. 389–396.\n[10] O. Abdel-Hamid, A. R. Mohamed, H. Jiang, L. Deng, G.\nPenn, and D. Yu, Convolutional neural networks for speech\nrecognition, IEEE/ACM Trans. Audio Speech Lang Process.,\nvol. 22, no. 10, pp. 1533–1545, 2014.\n[11] B. Vachhani, C. Bhat, B. Das, and S. K. Kopparapu, Deep\nautoencoder based speech features for improved dysarthric\nspeech recognition, in Proc. 18th Annu. Conf. of the Int.\nSpeech Communication Association, Stockholm, Sweden,\n2017, pp. 1854–1858.\n[12] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S.\nKoo, and S. Kumar, Transformer transducer: A streamable\nspeech recognition model with transformer encoders and\nRNN-T loss, in Proc. 2020 IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing, Barcelona, Spain, 2020, pp.\n7829–7833.\n[13] K. Rao, H. Sak, and R. Prabhavalkar, Exploring\narchitectures, data and units for streaming end-to-end\nspeech recognition with RNN-transducer, in Proc. 2017\nIEEE Automatic Speech Recognition and Understanding\nWorkshop, Okinawa, Japan, 2017, pp. 193–199.\n[14] Y . Wang, X. Deng, S. Pu, and Z. Huang, Residual\nconvolutional CTC networks for automatic speech\nrecognition, arXiv preprint arXiv: 1702.07793, 2017.\n[15] L. Dong, S. Xu, and B. Xu, Speech-transformer: A\nno-recurrence sequence-to-sequence model for speech\nrecognition, in Proc. 2018 IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing, Calgary, Canada, 2018,\n5884–5888.\n[16] X. Chen, Y . Wu, Z. Wang, S. Liu, and J. Li, Developing\nreal-time streaming transformer transducer for speech\nrecognition on large-scale dataset, in Proc. 2021 IEEE Int.\nConf. on Acoustics, Speech and Signal Processing, Toronto,\nCanada, 2021, 5904–5908.\n[17] A. Singh, V . Kadyan, M. Kumar, and N. Bassan, ASRoIL:\nA comprehensive survey for automatic speech recognition\nof Indian languages, Artif. Intell. Rev., vol. 53, no. 5, pp.\n3673–3704, 2020.\n[18] S. Jaglan, S. Dhull, and K. K. Singh, Tertiary wavelet model\nbased automatic epilepsy classiﬁcation system, Int. J. Intell.\nUnmanned. Syst., doi: 10.1108/ijius-10-2021-0115.\nVishal Goyal is currently a professor\nat the Department of Electronics &\nCommunication, GLA University, Mathura,\nIndia. He obtained the PhD in 2016\nfrom GLA University, Mathura, India. He\nhas participated in several high-proﬁle\nconferences and published many high-\nquality journal papers. In addition to his\nacademic career, he held several managerial positions in GLA\nUniversity.\nTripti Choudhary is a research scholar\nat the Department of Electronics &\nCommunication, GLA University, Mathura,\nIndia. She obtained the MS degree\nin 2012. Currently, she is working\non low-resource Indian languages and\npublished several good-quality papers on\nthe Automatic Speech recognition domain.\nTripti Choudhary et al.: WTASR: Wavelet Transformer for Automatic Speech Recognition of Indian Languages 91\nAtul Bansal is currently a professor at\nChandigarh University, Punjab, India.\nPrior to his recent appointment at\nChandigarh University, he was a professor\nin the Department of Electronics &\nCommunication, GLA University, Mathura,\nIndia. He received the BS degree from\nPunjab Technical University, India in 2001.\nHe received the MS degree from Indian Institute of Technology,\nDelhi, India, and the PhD degree from Thapar Institute of\nEngineering & Technology, Punjab, India in 2015. He published\nseveral papers in preferred journals and chapters in books and\nparticipated in a range of forums on the electronics domain. He\nalso presented various academic as well as research-based papers\nat several national and international conferences."
}