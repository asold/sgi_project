{
  "title": "Leaf disease severity classification with explainable artificial intelligence using transformer networks",
  "url": "https://openalex.org/W4362465518",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5089635776",
      "name": "Revanasiddappa Bandi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103903062",
      "name": "Suma Swamy",
      "affiliations": [
        "Agency for Science, Technology and Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2913631090",
    "https://openalex.org/W2145392172",
    "https://openalex.org/W2361749505",
    "https://openalex.org/W1938887115",
    "https://openalex.org/W4298148810",
    "https://openalex.org/W2589650048",
    "https://openalex.org/W4238975334",
    "https://openalex.org/W1597793043",
    "https://openalex.org/W4231734873",
    "https://openalex.org/W3067356005",
    "https://openalex.org/W2153110463",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W1676314349",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3049600440",
    "https://openalex.org/W4226101455",
    "https://openalex.org/W4285088909",
    "https://openalex.org/W2796438033",
    "https://openalex.org/W4311248875",
    "https://openalex.org/W6799166919",
    "https://openalex.org/W2995304823",
    "https://openalex.org/W4319741943",
    "https://openalex.org/W2969933026",
    "https://openalex.org/W4240839154",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3033272228",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3163473558",
    "https://openalex.org/W4293416723",
    "https://openalex.org/W3029515339",
    "https://openalex.org/W2008532403",
    "https://openalex.org/W4250533120",
    "https://openalex.org/W3195199539",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3025800305",
    "https://openalex.org/W3106505022",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3122887924",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2985778816",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W3172544793",
    "https://openalex.org/W3019724610",
    "https://openalex.org/W3132971810",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2913766929"
  ],
  "abstract": "The agricultural sector is considered India's most crucial sector.In India, farming is the most common occupation and a significant source of revenue [1].India is second in the world in terms of population, with 70% of its people living in villages and relying primarily on agriculture for their subsistence.Farmers cultivate a wide variety of crops based on numerous factors such as crop conditions, the environment, soil conditions, local farming practices, new variants of pathogens and various illnesses, etc.",
  "full_text": "International Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                            \nISSN (Print): 2394-5443   ISSN (Online): 2394-7454 \nhttp://dx.doi.org/10.19101/IJATEE.2022.10100136 \n278 \n \nLeaf disease severity classification with explainable artificial intelligence using \ntransformer networks  \n \nRevanasiddappa Bandi1*, Suma Swamy2 and Arvind C. S3 \nAssistant Professor, Department of Computer Science, Dr. Ambedkar Institute of Technology, Bengaluru, \nKarnataka, India, 5600561  \nProfessor, Department of Computer Science, Sir. M. Visvesvaraya Institute of Technology, Bengaluru, Karnataka, \nIndia, 5621572 \nSenior Researcher, SBIC, Agency for Science, Technology and Research (A*STAR) Singapore, 1386323  \n  \nReceived: 12-November-2022; Revised: 27-March-2023; Accepted: 29-March-2023 \n©2023 Revanasiddappa Bandi et al. This is an open access article distributed under the Creative Commons Attribution  (CC BY) \nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly  \ncited. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n1.Introduction \nThe agricultural sector is considered India’s most \ncrucial sector. In India, farming is the most common \noccupation and a significant source of revenue [1]. \nIndia is second in the world in terms of population, \nwith 70% of its people living in villages and relying \nprimarily on agriculture for their subsistence. \nFarmers cultivate a wide variety of crops based on \nnumerous factors such as crop conditions, the \nenvironment, soil conditions, local farming p ractices, \nnew variants of pathogens and various illnesses, etc.  \n \n \n \n \n*Author for correspondence \nPresently farmers are incurring losses due to changes \nin climatic conditions, plant disease is one of the \nmain challenges to food security since it significantly \nlowers crop quality and output [2].   \n \nTherefore, it has been difficult to diagnose diseases \ncorrectly and discover them early. A variety of \nmethods, including soil management, crop rotation, \nirrigation, genetic modification, harvesting methods, \nprecision farming, weed control, and pest and disease \ncontrol, are used to boost the yield. So, it is cr ucial to \nidentify pesticides and diseases early if you want to \nboost a crop's production. The traditional method of \nidentifying plant diseases is by visual inspection [3]. \nThis can result in incorrect disease identification, and \nResearch Article \nAbstract  \nAgribusiness is the main source of income for roughly 70% of p eople who reside in rural areas. India is the world's \nsecond-largest producer of pulses, textile raw materials, spices, coconuts, and other agricultural products. India's gross \ndomestic product (GDP) is significantly impacted by the agriculture industry. T echnology advancements help the \nagricultural industry to forecast various elements, such as soil quality, crop quality, and, disease detection to boost crop \nyield. Disease detection is one of the essential tasks that have to be carried out in agriculture. The early identification of \nthe leaf disease helps to prevent further spread to other leaves in the plant by which the yield can be improved. In this \nwork, plant leaf disease detection and stage classification are performed based on the severity of leaf in fection. A deep \nlearning model, you only look once version5 (YOLOv5) is used to detect plant leaf disease then background of the \ndiseased leaf is removed using U2 -Net architecture followed by stage classification performed using vision transformer \n(ViT) for classifying it as different stages such as low, moderate, and high. A recommendation solution has been provided \nto mitigate the leaf disease. YOLOv5 was trained using different open -source datasets namely 1) PlantDoc and 2) \nPlantvillage. This work mainly  concentrates on the apple leaf for performing stage classification. The YOLO v5 gives a \nmaximum f1-score of 0.57 at a confidence score of 0.2 and the vision transformer with a background image gives an f1 -\nscore of 0.758 and without a background image, 0.908 f1-score is achieved. \n \nKeywords \nYou only look once version5(YOLOV5), Vision transformer (ViT), Computer -aided disease detection system (CADS), \nRegion proposal network (RPN), Natural language processing (NLP), Explainable artificial intelligence (XAI), D eep \nconvolutional neural network (DCNN). \n \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n279          \n \nthe use of inexperienced pe sticides can have long -\nterm negative effects on the crops and soil. \n \nAs a result, there is a need for a computer -aided \ndisease detection system (CADS) [4] powered by \nartificial intelligence (AI) to assist farmers to \nimprove their crop yield. CADS consists of computer \nvision, machine learning, and a decision support \nsystem that can detect disease using imaging \ninformation. CADS systems for plant disease \ndetection using deep learning architectures were \ndeveloped on high computational resourcing \nplatforms by u sing open -source datasets such as \nPlantDoc [5], PlantVillage [6], and also several \ncustom-based plant disease datasets. CADS system \nwith transfer learning methodology was developed to \ndetect black rot, bacterial plaque, and rust diseases \nusing region propo sal network (RPN) resulting in \nimproved accuracy compared to the traditional \nmethod [7] with high run time complexity. \n \nDeep learning techniques have been used in recent \nyears to accurately classify leaf diseases. However, \nthe lack of interpretability of t hese models has \nlimited their adoption in practical applications. \nExplainable artificial intelligence (XAI) aims to \nprovide insight into the decision -making process of \nAI models, making them more transparent and \ntrustworthy. Transformer networks, a type of  deep \nlearning architecture, have been used in various \nnatural language processing (NLP) tasks and have \nshown promising results in computer vision tasks [8]. \nTransformers in machine learning is made up of \nnumerous layers of self -attention. The most recent \ndevelopments in computer vision, which meet state -\nof-the-art standard accuracy with enhanced parameter \nefficiency, are only one example of how machine \nlearning advancements show great potential for a \ngeneral learning technique that can be utilized for a \nnumber of data modalities [9].  \n \nThe main objective of this paper is to accurately \nidentify and classify plant diseases using deep \nlearning networks and to offer appropriate treatments \nto stop the disease from spreading to other plant \nleaves. Some researchers used outdated traditional \nprocedures, or they made use of slow, time -\nconsuming features. According to the literature \nreview conducted, the work's motivation is to \nperform stage classification based on the proportion \nof disease severity, which was not taken into account \nin the past. He nce, this issue has been taken into \naccount in this work. This study uses you only look \nonce version 5 (YOLOv5), a deep learning technique, \nto detect plant diseases and vision transformer (ViT) \nclassifier is used to perform the stage classification. If \nit is between 0 to 30 then the leaf disease is in the low \nseverity stage, if the severity is between 31 to 60 then \nthe leaf disease is in the moderate stage and if the \npercentage of severity is 61 to 100 then the leaf \ndisease is in the high severity stage. \n \nThe organization of the paper is as follows: section 1 \naddresses the introduction of agriculture background, \nchallenges of the previous literature, motivation of \nthe work, objectives of the paper, and contribution of \nthe paper. Section 2 explains the litera ture review \ncarried out on plant disease detection and \nclassification, section 3 discusses the different \nmethods adopted for the leaf disease severity \nclassification, experimental setup for leaf disease \ndetection training, and multistage leaf disease \nclassification, and section 4 describes the results \ncarried out for performance evaluation of leaf disease \ndetection using plantdoc dataset, training and testing \nanalysis of leaf disease detection, performance \nanalysis of background removal technique and \nmultistage leaf disease classification, end -to-end leaf \ndisease detection and classification results for apple \nrust and apple scab and finally comparative analysis \nof multistage leaf disease classification with and \nwithout background using ViT. \n \n2.Literature review  \nThe major factors that affect leaf disease include \nchanges in climatic conditions such as temperature, \nhumidity, soil moisture, light intensity, PH level, etc., \nand pathogenic organisms such as viruses, bacteria, \nfungi, and insects. Seve ral authors discussed how to \nmitigate leaf disease using different techniques. \n \nThe textural statistics application for the diagnosis of \nplant leaf disease was described by Dhaygude and \nKumbhar [10] using various procedures such as \ntransforming an image fr om red green blue (RGB) to \nhue saturation value (HSV), hiding and removing \ngreen pixels based on their threshold value, 32 ×32 \npatch size segmentation is used to separate the \nrelevant segments, and texture statistics are then \ncalculated using the color occu rrence technique to \nassess the existence of disease. The literature review \nis limited in terms of providing information on the \nspecific vision -based algorithm and neural networks \ncan be used to improve the classification process, as \nwell as any potential c hallenges or drawbacks \nassociated with their implementation. \n \nRevanasiddappa Bandi et al. \n280 \n \nSethy et al. [11] described K -means clustering, \nmulticlass support vector machine (Multiclass -SVM), \nand particle swarm optimization (PSO) as used to \ndiagnose and detect rice leaf disease. For fe ature \nextraction, gray level cooccurrence matrix (GLCM) \n[12] has been used. PSO increased the detection \naccuracy to 97.91%. The study's generalizability may \nbe limited as it only collected a small number of \nspecimens with a limited variations of infected r ice \nleaves. \n \nKhirade and Patil [13] proposed the use of image \nprocessing and the internet of things to detect plant \ndiseases, specifically on hill banana leaves, by taking \nimages with a camera sensor and transmitting them to \ncloud storage using the Raspber ry Pi3 model. The \nstudy used random forest classifier from GLCM \nfeatures to detect three diseases namely, Black \nSigatoka, Bunchy Top virus, and Yellow Sigatoka. \nThis study is limited to the hill banana dataset and \nthey have not performed performance evalua tion on \nthe dataset. \n \nCao et al. [14] suggested a multi-scaled convolutional \nobject detection network. They acquired multi -scaled \nfeatures using deep convolutional networks and \nincorporated deformable convolutional structures to \ncounteract geometric altera tions. Experiments \ndemonstrate that the proposed framework \nsignificantly increases the accuracy of recognizing \nsmall target objects with geometric deformation and \nthe speed/accuracy trade -off. This paper can be \nextended limited to the networks in the field  of video \nobject detection. \n \nGuo et al. [15] proposes a deep learning -based \nmathematical model for plant disease detection and \nrecognition, using a RPN and Chan -Vese algorithm \n[16] to recognize and localize leaves and segment \nimages based on symptoms. The model outperformed \nconventional approaches in detecting rust, bacterial \nplaque, and black rot diseases with an accuracy of \n83.57%. The Chan -Vese algorithm's iterative \ncalculation time could be shortened, though. To \nexpedite training and iteration, future r esearch will \nconstruct zero initial sets using neural networks. The \nsuggested algorithm has important effects on \necological preservation, sustainable agricultural \nproduction, and intelligent agriculture. The Chan -\nVese technique requires perpetual iterative  \ncalculation and takes a lengthy time, which is not \nconducive to the method's ability to produce quick \nidentification results. \nHassan et al. [17] explore the use of deep convolution \nneural network (DCNN) models for the purpose of \nidentifying and diagnosing  plant diseases from the \nleaves. The models were developed using depth \nseparable convolution to cut down on the number of \nparameters and computational costs. The models \nwere trained using a dataset that included 14 plant \nspecies and 38 different disease cl asses. The \noutcomes demonstrated high disease classification \naccuracy rates that outperformed custom -feature-\nbased methods and needed less training time than \nprevious deep learning models. Mobile devices can \nalso be employed with the MobileNetV2 architectu re \nthat was used in this study. The paper concludes that \nDCNN models have promising potential for the \nefficient identification of plant diseases, including in \nreal-time agricultural systems. \n \nRashid et al. [18] address the challenges in spotting \npotato lea f (leaflet) diseases (PLD) in their early \nphases due to differences in crop category, signs of \nthe disease, and environmental variables. While \nseveral machine learning methods have been created \nto identify potato foliage diseases, they are only \napplicable in certain areas. In order to identify early \nblight and late blight diseases, the article suggests a \nmulti-level deep learning model for potato leaf \ndisease identification that makes use of image \nsegmentation and a convolutional neural network. \nThe model b eat state -of-the-art models in terms of \naccuracy and processing cost on a dataset gathered \nfrom the Central Punjab area of Pakistan, achieving a \n99.75% accuracy rate. The study only concentrates \non identifying a specific disease on a leaf, which is a \nlimitation of the article. A limitation of the paper is \nthat the study only focuses on detecting a single \ndisease on a leaf and does not include an assessment \nof disease location or severity. In addition, the PLD \ndataset could be improved, and further research is \nneeded to develop an IoT -based monitoring system, \nwebsite, and mobile application. \n \nYOLOv1 proposed by Redmon et al. [19] uses the \nDarknet framework. It was trained on the ImageNet -\n1000 dataset with an input picture size of 224×224 \nand can identify objects at a rate of 45 frames per \nsecond. The limitation is that it cannot generalize the \nobjects if the image has various dimensions or \nidentify the objects accurately when they are tiny. \n \nYOLOv2 and YOLO9000 proposed by Redmon and \nFarhadi [20] employs D arknet 19 design, which has \n19 convolutional layers, 5 max -pooling layers, and a \nSoftMax layer. It can conduct batch normalization \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n281          \n \nand anchor boxes on input images up to 448 ×448. \nAdditionally, the mean average precision (mAP) of \nmulti-scale training has in creased by 2% by \nintroducing batch normalization to the convolutional \nlayers in the design. On Visual Object Classes \nChallenge-2007 (VOC 2007), YOLOv2 receives 76.8 \nmAP at 67 FPS. YOLOv2 achieves 78.6 mAP at 40 \nFPS, beating cutting -edge techniques like fas ter \nregion-based convolutional neural networks (Faster \nR-CNN). Despite having detection data for only 44 of \nthe 200 classes, YOLO9000 receives 19.7 mAP on \nthe ImageNet detection validation set. The algorithm \nhas limited capacity to identify items of differ ent \nshapes and dimensions. YOLO9000 finds it difficult \nto display certain products, like eyeglasses. \n \nYOLOv3 was proposed by Redmon and Farhadi [21] \nand employs the Darknet -53 network, which consists \nof 53 convolutional layers, as its feature generator. \nWith an input image size of 320 ×320, it primarily \nconsists of 3x3 and 1x1 filters with shortcut links and \nuses feature pyramid network (FPN) to make class \nforecasts. More than 80 distinct items can be \nrecognized by YOLOv3 in a single image. The error \nrate can be significantly decreased and runs in 22 ms \nat mAP of 28.2, three times as quickly as single shot \ndetector (SSD) but with the same accuracy. YOLOv3 \nstruggles with little objects that appear in groupings, \nsuch as flocks of birds. \n \nYOLOv4 was proposed b y Bochkovskiy et al. [22] \nemploys cross -stage partial architecture \n(CSPDarknet53), a bag of freebies, and a bag of \nspecials as architectural styles. Features like cross -\niteration batch normalization (CBN), pan aggregation \nnetwork (PAN), weighted -residual-connections \n(WRC), and cross -stage-partial connections (CSP) \nare applicable to the majority of tasks, models, and \ndatasets. YOLOv4 was rated as one of the best \nmodels for speed and accuracy for the Common \nObjects in Context (COCO) dataset, even if its \noverall accuracy lagged behind that of EfficientDet \nlargest model. YOLOv4 Struggles to recognize close \nobjects because each grid can suggest only two \nbounding boxes. \n \nYOLOv5, a creation of Ultralytics open -source \nresearch on cutting -edge vision AI techniques, w as \ndescribed in [23] with a 416x416 input image size \nand features like auto-learning bounding box anchors, \n16-bit floating point precision, and new model \nconfiguration files. Object detection has been carried \nout precisely and effectively using .yaml files , cross-\nstage partial networks as the backbone, preliminary \nassessment metrics, etc. \n \nFaster R-CNN were discussed by Ren et al. [24] for \nreal-time object detection using RPN. Rapid R -CNN \nleverages the RPN's fully trained region proposals to \nmake accurate detections. They merged two networks \nby merging RPN and Faster R -CNN into a single \nnetwork and sharing their convolutional features. The \nRPN component gives instructions to the unified \nnetwork on where to conduct attention -based \nsearches. Using the PASCAL V OC2007, VOC2012 \n[25, 26], and Microsoft (MS) COCO datasets [27], \nthe proposed approach running on a graphics \nprocessing unit (GPU), delivers state-of-the-art object \ndetection accuracy with just 300 proposals per image. \nThe foundation of the top -scoring sub missions in a \nnumber of tracks in the ImageNet Large Scale Visual \nRecognition Challenge (ILSVRC) and COCO 2015 \ncompetitions are faster R-CNN and RPN. \n \nTan and Le [28] proposed a novel scaling method \nthat scales all depth, breadth, and resolution \nparameters equally using a simple yet immensely \npowerful compound coefficient. By scaling up \nMobile Nets and ResNet, the authors demonstrated \nthe viability of this approach. In order to advance \neven further, neural architecture search was used to \nrebuild a fresh basic network, scale it up, and develop \nthe Efficient Nets family of models, which \noutperforms prior ConvNets in terms of accuracy and \nproductivity. For instance, the proposed EfficientNet -\nB7 performs better than the most well -known \nConvNet since it is 6.1 ti mes quicker and 8.4 times \nsmaller at inference. In addition to obtaining cutting -\nedge 84.3% top -1 accuracy on ImageNet. The \nEfficient Net model can be scaled up extremely \nsuccessfully using ImageNet and five regularly used \ntransfer learning datasets, outpe rforming state-of-the-\nart accuracy with orders of magnitude fewer \nparameters and floating -point operations per second \n(FLOPS). \n \nMalik et al. [29] explained the sugarcane crop disease \ndetection using deep learning models such as VGG -\n19, RseNet -34, and Resne t-50. Five sugarcane \ndiseases have been taken into consideration, and \nimages of the situations were taken using cameras at \nvarious resolutions and illumination levels. The \nproposed model was developed using data from the \nsugarcane industry and demonstrated  robustness in \nidentifying complicated patterns by achieving \n93.20% accuracy on testing data and 76.40% on data \nfrom online sources. The limitation of this study is \nRevanasiddappa Bandi et al. \n282 \n \nthat, a need for a larger dataset for both tasks, which \nis believed to be crucial for improving the results. \n \nToda and Okura [30] described various layer -wise \nand neuron -wise visualization approaches. Training \nis carried out by using publicly accessible datasets. \nThis paper demonstrated how neural networks can \nacquire the texture and color of le sions that are \nunique to each disease. They deleted the layers from \nthe network that were not helping the development of \nan effective model, reducing the parameters by 75% \nwithout affecting the accuracy. They have used \nlimited training data, overfitting the training data, and \nlack transparency in the decision -making process of \ndeep neural networks. \n \nNagasubramanian et al. [31] explained an innovative \nmethod, a 3D deep convolutional neural network \n(DCNN) that instantly incorporates hyperspectral \ndata and the  trained model to generate \nphysiologically relevant justifications. The authors \nconcentrated on charcoal rot; a fungal infection \ntransmitted through soil that reduces the production \nof soybean crops globally.3D DCNN has an infected \nclass F1 score of 0.87 and a classification accuracy of \n95.73%, was built on hyperspectral imaging of \ninoculated and mock -inoculated stem images. In \naddition to offering excellent accuracy, it also \nprovides physiological insight into model predictions, \nincreasing model predictions' credibility. The study \nhas a potential limitation of using a smaller dataset \nsize, but the model's robustness is tested through \nfivefold cross-validation.  \n \nLiu and Wang [32] described a method for early \ndetection of tomato leaf spots u sing the \nMobileNetv2-YOLOv3 model with GIoU bounding \nbox regression loss function. The proposed system \nshows improvements in detection accuracy and \nvalidates real-time detection. The hypothetical results \ndemonstrate high F1 score (94.13%), probability \n(92.53%), and an average Intersection over Union \n(IOU) value (89.92%). The system also achieves a \nhigh detection speed of 246 frames per GPU and a \nfast extrapolation speed of 16.9ms for one 416×416 \nframe. The proposed algorithm should be \nimplemented on a computer or mobile application for \npractical use in real crop farming so that farmers can \neasily get support for their crops anywhere and \nanytime. \n \nMalik et al. [33] explained a hybrid deep learning \nmodel which uses a stacked ensemble learning \ntechnique to combine MobileNet and visual geometry \ngroup-16 (VGG -16) models to generate a composite \nmodel. Authors considered four categories of \nsunflower diseases. They have also created a dataset \nusing Google images and the proposed model \noutperformed other models in the  dataset with 89.2% \naccuracy. A limitation of the paper is that there are \ndifferent species of sunflower plants and some \ndiseases can have similar symptoms, making their \nidentification and classification difficult. \n \nTammina [34] explained transfer learning  which is \nused to address a variety of classification, regression, \nand clustering-related problems. This work used one \nof the trained models, VGG -16 with DCNN, to \nrecognize images. The basic convolutional neural \nnetwork model's validation accuracy is 72.40  percent. \nThe model's accuracy increased to 79.20% with the \nhelp of image augmentation. Finally, the VGG -16 \npre-trained model, which was created using a sizable \nimage dataset and improved with images, achieved \nan accuracy of 95.40%. \n \nSandler et al. [35], d escribed MobileNetV2 which \nimproves the cutting -edge performance of mobile \nmodels across a variety of model sizes, workloads, \nand benchmarks. A novel framework called Single \nShot Detector Lite (SSDLite), a mobile DeepLabv3 \nversion of DeepLabv3 that is buil t on an inverted \nresidual structure with shortcut connections between \nthe thin bottleneck layers is described in this study. It \nalso demonstrates how to build mobile semantic \nsegmentation models. The intermediate expansion \nlayer uses lightweight depth -wise convolutions as a \nsource of non -linearity to filter features. When used \nwith the ImageNet dataset, the suggested architecture \nadvances the state of the art for a number of \nperformance measures. Our network outperforms \nstate-of-the-art Realtime detectors f or the job of \nobject identification on the COCO dataset in terms of \naccuracy and model complexity. Interestingly, when \ncombined with the SSDLite detection module, the \nproposed solution uses 20 fewer computations and 10 \nfewer parameters than YOLOv2. \n \nDenseNet and EfficientNet, Deep convolutional \nneural network models, were described by Srinidhi et \nal. [36] and used to successfully detect four kinds of \napple plant diseases from images of apple plant \nleaves (leaflets). The different categories are scab, \nrust, healthy, and several diseases. The dataset for \napple leaf disease is improved in this work using data \naugmentation and image annotation techniques such \nas Canny Edge Detection, Blurring, and Flipping. \nBased on an improved dataset, models using \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n283          \n \nEfficientNetB7 and DenseNet are recommended, \nachieving an accuracy of 99.8% and 99.75%, \nrespectively, and resolving convolutional neural \nnetwork shortcomings. Models may become even \nmore accurate and dependable by using powerful \nvalidation techniques. \n \nSibiya an d Sumbwanyambe [37] introduced the \nbenefits of fuzzy logic principles. The current study \naims to update the algorithm used in the Leaf Doctor \napplication, which is used to determine the severity \nof plant leaf diseases. This strategy will develop the \ntechnology of precision agriculture by introducing an \nalgorithm that might be used in smartphone programs \nlike the Leaf Doctor app. Applications developed \nusing the technique presented in this paper will help \nbeginner users and non -plant pathologists grasp the \nestimated severity of the disease. For smartphone \napps that don't employ fuzzy logic, this study \nrequired less time and is recommended in order to \nacquire relevant findings. The recommended \napproach for color threshold segmentation was tested \nusing a Fiji i mage. The fuzzy logic inference system \nwas modeled and tested using LabVIEW software \n[38]. It is indicated that Leaf Doctor is one program \nthat, in subsequent upgrades, will employ the \nsuggested methodology. \n \nShi et al. [39] outlined different studies on \nconvolution neural network (CNN) based plant \ndisease severity assessment in terms of classical CNN \nframeworks, improved CNN architectures, and CNN \nbased segmentation networks, depending on the \nnetwork architecture. The study also provided a \ndetailed compara tive analysis of the advantages and \ndisadvantages of each approach. Moreover, the study \ninvestigated common methods for acquiring datasets \nand performance evaluation metrics for CNN models. \nOverall, this literature survey provides insights into \nthe current  state of research on the use of deep \nlearning techniques for plant disease severity \nassessment and highlights potential areas for future \nresearch. \n \nAbd et al. [40] introduced a new deep learning \ntechnique called ant colony optimization with \nconvolution neural network (ACO -CNN) for disease \ndetection and classification in plant leaves. The \ntechnique uses ant colony optimization to enhance \nthe accuracy of disease diagnosis. The proposed \nmethod subtracts geometries of color, texture, and \nleaf arrangement from the provided images using a \nCNN classifier. The study uses several effectiveness \nmetrics to compare the proposed approach with \nexisting techniques and demonstrates that the \nproposed approach outperforms existing methods \nwith improved accuracy rates. The st udy also \npresents the steps involved in disease detection, \nincluding image acquisition, image separation, noise \nremoval, and classification. However, the paper does \nnot consider the stage classification of plant disease. \n  \nMohammed and Yusoff [41] provides an overview of \nvarious techniques available for achieving success in \nmachine learning, deep learning, and image \nprocessing. It emphasizes the importance of training \nand testing models with more datasets to increase the \naccuracy of recognition rates. The r eview also \nhighlights the need for new and improved deep \nlearning methods to classify plant diseases. While \nseveral methods have been used in the past, neural \nnetworks like CNN appear to be the best technique \ndue to their flexibility and feature extractor property. \nUnlike previous models like naive bayes and support \nvector machine (SVM), CNN can learn additional \nfeatures from images to provide better output. \nTherefore, CNN is the best choice for research work \nin image processing, machine learning, and deep \nlearning due to its ability to learn and extract features \nfrom images for reliable output. \n \nThe literature review discussed above has \ndemonstrated that plant disease diagnosis is being \nsolved using various image processing, machine \nlearning and deep -learning techniques. However, \nmost of the research work carried out either talks \nabout the detecti on or classification of plant diseases \nand few have used image processing methods for \nstage identification which does not provide a \ngeneralized solution for deployment. Hence there is a \nneed for a robust and novel pipeline for plant disease \nstage classification. In this research work, an end -to-\nend deployable solution is developed that does leaf \ndetection using YOLOv5 architecture and ViT \narchitecture [42] that does stage classification. The \nrecommendation engine provides a mitigation \nsolution based on the severity of the disease. Ground \ntruth for stage classification was done by an expert \nplant pathologist. To reduce time and burden, an \nunsupervised clustering -based ground -truth \ngeneration methodology was developed. Also, a \ncomparative study of F1 score for  multistage leaf \ndisease classification with and without background \nusing ViT classifier and its interpretability on disease \nstage classification was performed on an apple leaf \ndisease.  \n  \n \nRevanasiddappa Bandi et al. \n284 \n \n3.Methods \nThe perception of the end -to-end multistage leaf \ndisease d etection and classification system is shown \nin Figure 1.  Multi-stage pipeline consists of six \ndifferent stages. (i) Data acquisition from PlantDoc \nand Plantvillage datasets (ii) Data preprocessing was \nperformed using resize, augmentation, and contrast \nenhancement techniques. (iii) Disease detection \nmodel was trained using different parameters and \nhyperparameter tuning (iv) Background removal was \nperformed to accurately classify the image (v) Stage \nclassification was accomplished based on the severity \nof leaf infection. (vi) recommendation solution was \nprovided to mitigate the leaf disease. In this research \nwork, apple leaf data was used for multistage \nclassification. The ground truth fo r di fferent disease \nstages. (i)  low (ii) moderate and (iii) high, were \ngenerated using the mean -shift clustering technique \nand were validated by experts. \n \n \nFigure 1 The perception of multistage plant leaf disease detection and classification \n \n3.1Data Acquisition \nIn this research work, two open -source datasets were \nused for plant leaf disease detection and \nclassification.   \nPlantdoc: A dataset for visual plant disease \ndetection: PlantDoc dataset [4] contains cropped leaf \nimages used for standardizing classification models. \nThe researchers used this dataset for training image \nclassification models, VGG -16 InceptionV3 and \nobject identif ication models, MobileNet, and Faster -\nRCNN. The dataset can be used to improve general \nagricultural computer vision tasks, health crop \ncategorization, and plant disease classification. The \nPlantDoc dataset contains 2572 images which include \n13 species and 27 classes out of which 17 classes are \ndiseased and 10 classes are healthy. The different \nclasses and number of images in a particular class are \ndepicted in Table 1 . These images are used for \ntraining the YOLOv5 model to perform leaf region \ndetection and its disease type. \nThe two sample classes of apple leaf disease i.e., \napple rust leaf and apple scab leaf are depicted in \nFigure 2. \n \n \nFigure 2  Apple leaf diseases from the PlantDoc \ndataset \n \n \n\nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n285          \n \nTable 1 PlantDoc dataset used for leaf disease detection  \nAttributes/Classes Ranges/No. of Images \nTomato two spotted spider mites leaf(leaflet) 2 \nTomato mosaic virus leaf(leaflet) 54 \nCherry leaf(leaflet) 57 \nBell pepper leaf(leaflet) 61 \nTomato leaf(leaflet) 63 \nGrape leaf(leaflet) black rot 64 \nSoyabean leaf(leaflet) 65 \nCorn Gray leaf(leaflet) spot 68 \nGrape leaf(leaflet) 69 \nBell pepper leaf(leaflet) spot 71 \nTomato yellow virus leaf(leaflet) 76 \nApple rust leaf(leaflet) 88 \nTomato early blight leaf(leaflet) 88 \nApple Leaf(leaflet) 91 \nTomato leaf(leaflet) mold 91 \nApple Scab Leaf(leaflet) 93 \nStrawberry leaf(leaflet) 96 \nPotato leaf(leaflet) late blight 105 \nTomato bacterial spot leaf(leaflet) 110 \nPeach leaf(leaflet) 111 \nTomato late blight leaf(leaflet) 111 \nBlueberry leaf(leaflet) 115 \nCorn rust leaf(leaflet) 116 \nPotato leaf(leaflet) early blight 116 \nRaspberry leaf(leaflet) 119 \nSquash Powdery mildew leaf(leaflet) 130 \nTomato septoria leaf(leaflet) spot 151 \nCorn leaf(leaflet) blight 191 \nHealthy 847 \nUnhealthy  1725 \nTotal 2572 \n \nPlantVillage: Dataset of diseased plant leaf images \nand corresponding labels: The PlantVillage dataset \n[5] contains 54,309 carefully labeled images on both \nhealthy and diseased plant leaves of 14 different \ncrops. \n \nThese data mark the start of a  continuous \ncrowdsourcing effort to enable computer vision \nmethods to assist in resolving the issue of crop plant \nyield losses caused by viral diseases. This dataset is \nused for plant disease detection systems and this is a \npopular dataset that is used by many researchers. In \nthis work samples of the datasets were used for stage \nclassification using ViT. Typical images of apple rust \nand apple scab from the plantvillage dataset are \nshown in Figure 3. \n \n3.2Data Pre-processing \nBefore the model training, resize the images to \n416×416 size as it is a prerequisite for YOLOv5 \narchitecture [43]. To avoid overfitting and to create \ngeneralization, data augmentation techniques have \nbeen applied like (i) flip (ii) rotate, and (iii) contra st \nenhancement. \n \n \nFigure 3  Apple leaf diseases from the plantvillage \ndataset \n \nTo train the YOLOv5 model, the dataset is split into \ntwo parts 90% for training and 10% for testing the \nmodel. Table 2  shows the number of training and \ntesting images considered for the YOLOv5 model \ntraining of each class disease of PlantDoc. \n\nRevanasiddappa Bandi et al. \n286 \n \nTable 2 Training and testing data for training leaf disease detection using plantDoc dataset  \nClass name Training images Testing images \nTomato two spotted spider mites leaf(leaflet) 2 0 \nTomato mosaic virus leaf(leaflet) 44 10 \nCherry leaf(leaflet) 47 10 \nBell pepper leaf(leaflet) 53 8 \nTomato leaf(leaflet) 55 8 \nGrape leaf(leaflet) black rot 56 8 \nGrape leaf(leaflet) 57 12 \nSoyabean leaf(leaflet) 57 8 \nBell pepper leaf(leaflet) spot 62 9 \nCorn Gray leaf(leaflet) spot 64 4 \nTomato yellow virus leaf(leaflet) 70 6 \nApple rust leaf(leaflet) 78 10 \nTomato early blight leaf(leaflet) 79 9 \nApple Leaf(leaflet) 82 9 \nApple Scab Leaf(leaflet) 83 10 \nTomato leaf(leaflet) mold 85 6 \nStrawberry leaf(leaflet) 88 8 \nPotato leaf(leaflet) late blight 97 8 \nTomato bacterial spot leaf(leaflet) 101 9 \nTomato late blight leaf(leaflet) 101 10 \nPeach leaf(leaflet) 102 9 \nBlueberry leaf(leaflet) 104 11 \nCorn rust leaf(leaflet) 106 10 \nPotato leaf(leaflet) early blight 108 8 \nRaspberry leaf(leaflet) 112 7 \nSquash Powdery mildew leaf(leaflet) 124 6 \nTomato septoria leaf(leaflet) spot 140 11 \nCorn leaf(leaflet) blight 179 12 \nHealthy 757 90                 \nUnhealthy    1579  146             \nTotal   2336  236 \n \n3.3Leaf disease detection  \nLeaf disease is detected using one of the most \nadvanced object identification techniques YOLOv5. \nThe YOLOv5 uses a single neural network to process \nthe entire image, then separates it into parts along \nwith the bounding box probabilities of each \ncomponent. The network takes an image as input and \nproduces a set of bounding boxes and class \nprobabilities as output. The bounding boxes represent \nthe location of the objects in the image, while the \nclass probabilities represent the likelihood of each \nobject belonging to a specific class. One of the main \nimprovements of YOLOv5 is its architecture. It uses \na scaled approach, where the network is scaled up or \ndown depending on the size of the input image. This \nallows YOLOv5 to be more accurate and faster than \nprevious versions of YOLO. There are two choices \nfor training the YOLOv5 model. a) Activation and \nOptimization Function: Leaky rectified linear unit \n(ReLU) and Sigmoid are used as activation functions. \nStochastic gradient descent (SGD), and adaptive \nmoment estimati on (ADAM) are used as optimizer \nfunctions. In YOLOv5, the final detection layer uses \nthe sigmoid activation function whereas the \nmiddle/hidden layers use the Leaky ReLU activation \nfunction. b) Loss Function: A compound loss is \ncalculated for the YOLO based  on the objectness \nscore, class probability score, and bounding box \nregression score. For the loss computation of class \nprobability and object score, Ultralytics employed the \nBinary Cross-Entropy with Logits Loss function from \nthe PyTorch library. The loss  function of YOLO is \ndescribed in Equation 1. \nloss = lbox+ lcls+ lobj   (1) \n \nwhere lbox is a bounding box regression function, \nlcls is a loss function of classification and lobj is a \nloss function of confidence. The regression loss \nfunction of the bounding  box is written as in \nEquation 2. \nlbox = coord   ∑ ∑     \n    \n     \n  \n   \nbj(2 - wiX hi ) [( xi   -  ̂i\nj)2 \n+ (yi -  ̂i\nj)2 + (wi -  ̂i\nj ) 2 (hi -  ̂i\nj)2]  (2) \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n287          \n \nThe classification loss function is written as in \nEquation 3. \nlcls= class \n∑ ∑     \n    \n     \n  \n   \n∑  (        (  ̂   (               (3) \n \nThe confidence loss function is written as in Equation \n4. \nlobj = noobj ∑ ∑     \n      \n     \n  \n   \n (ci -  ̂j )2+  obj \n∑ ∑     \n    \n     \n  \n   \n  (ci-  ̂j )2   (4) \n \nwhere  ̂  ̂ is the target’s real central coordinate,  ̂  ̂ \nis the target width and height, and λcoord is the \nposition loss coefficient, λclass is the category loss \ncoefficient. If the anchor box at ( i, j) contains targets \nthen the value of  Ii,j\nobj is 1 otherwise it is 0.  Pi (c) is \nthe target’s category probability  and  ̂   (     is the \ncategory’s actual value. the total number of \ncategories C is represented by the length of the two. \nThe input image is passed to the YOLOv5 model \nwhich processes it and outputs the detected image \nwith a confidence score as shown in Figure 4. \n \n \nFigure 4 Leaf disease detection using YOLOv5 \n \n3.4Multistage infection ground truth preparation \nAlgorithm 1 explains the steps taken to prepare the \nmultistage infection before ground truth preparation. \nInput to the algorithm will be apple leaf images from \nthe plantvillage dataset and output will be three \ndifferent stages of severity of the apple leaf  disease. \nIn the first step, the mean-shift clustering technique is \napplied to the input images to get the region of \nclusters. In step 2 canny -edge detection method is \napplied to get the leaf and edges of the infected \nregion. In step 3 calculate the contou r to know the \narea of the diseased part in the leaf. In step 4 different \nmetrics are calculated such as leaf infection area, \ntotal area, perimeter, and percentage of the infection \non the leaf finally images are grouped into three \ncategories low, moderate, and high.  \n \nAlgorithm 1: Multi -Stage Infection Ground Truth \nPreparation Algorithm \nInput: Apple Leaf Disease Images from PlantVillage \nDataset \nOutput: Different stage identification as ground truth \nof apple leaf disease dataset for classification  \nStep 1:   Apply mean -shift clustering to get disease \nregion clusters [44]. \nStep 2:  Canny-edge detection is applied to get leaf \nand infect region edges [45]. \nStep 3: Find the contour of the leaf to know the area \nof the diseased part in the leaf. \nStep 4: Leaf infection area, total area, and perimeter \nare calculated, as the percentage of the infection on \nthe leaf. \nStep 5:  Based on Step 4  three different severity \ncategories are grouped as, (i) low (ii) moderate (iii) \nhigh severity \nimages. \n \n3.5Background removal \nA digital image processing technique called \nbackground removal can be used to separate a \npicture's components into interesting and undesirable \nareas. Before further analysis and processing, \nbackground reduction is necessary for many \napplications of image p rocessing and computer \nvision. To reduce false classification results, the \nbackground removal technique is adopted using U2 -\nNet architecture [46]. It is a two -level nested U -\nstructure, consisting of 11 stages on the top and each \nstage is configured by a re sidual U -block (RSU) on \nthe bottom as shown in Figure 5 (a). Background \nremoval was performed on the apple leaf using U2 -\nNet architecture where the RGB image of the apple \nleaf is converted into the binary image and then \npassed on to the U2 -Net model which r emoves the \nbackground of the binary image using residual U -\nblocks. Figure 5(b) shows the removal of background \nnoise from the leaf image which is not required for \nthe classification and it will affect the accuracy of the \nclassifying image. Initially, the i nput image is passed \nto the U2 -Net model, it converts to a binary image \nthen background noise is removed for the binary \nimage, after that background noise is removed for the \nRGB image to get the desired output image. \n \n\nRevanasiddappa Bandi et al. \n288 \n \n \nFigure 5(a) Illustration 0f U2-Net architecture \n \n \nFigure 5(b) Plant leaf image background removal using U2-Net architecture \n \n3.6Multistage classification  \nThe proposed multi -stage classification is performed \nby using deep learning architecture ViT. It is an \nimage classification technique and it employs a \ntransformer-like architecture that gives patches over \nthe images. A flattened vector of pixel values from  \npatches of size 16×16 makes up the input sequence, \nand each of these patches is embedded linearly. \nThese embedded patches are added up and the result \nis passed to the standard transformer encoder as \nshown in Figure 6.  The ViT is the intersection \nbetween N LP and computer vision. It has achieved \nhigh performance for object detection, classification \nof images, computer vision applications, and \nsemantic segmentation. Transformers applied directly \non image patches that are in sequence can work well \non image classification tasks. \n \nAlgorithm2 explains about steps taken for inference \nleaf disease detection and stage classification. Input \nto the algorithm is a plant image from the Plantdoc \ndataset and output is detected disease and their \nseverity. In step 1 the inpu t image is passed to the \nleaf disease detection model using YOLOv5 to detect \nthe image. In step 2 check the image for healthy or \ndiseased. If it is diseased find the region of interest \n(ROI) and perform the background removal on the \ninput image in step 3 and step 4 respectively and \nfinally apply a vision transformer on the background \nremoved image to get the severity of the disease. \n\nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n289          \n \nAlgorithm 2: Inference Leaf Disease Detection \nand Stage Classification \nInput: Plant leaf Image I(x,y) from PlantDoc dataset. \nOutput: Detected Disease I D(x,y) , Severity of \nDisease SID(x,y) \nPD → Plant Disease type \nIROI(X,Y) → Region of Interest of Diseased Image \nBR()→ Background Removed Image \nViT()→ Vision Transformer \nIROI BR(x,y) → Region of Interest of Background Removed \nImage \nStep 1: Input the image I(x,y) to disease detection model \nusing YOLOv5 to detect the image \nStep 2: Check the image for healthy or diseased \nStep 3: If it is diseased, find the region of interest of Diseased \nImage IROI (X,Y) \nStep 4: Perform background removal on IROI(X,Y) \nStep 5: Apply VIT on ROI of background removed \nimage to get the severity of disease SID(x,y) \n \n \nFigure 6 Multistage classification using vision transformer \n \n3.7Experimental setup \nThe proposed research work was conducted using \nopen-source google collab with K80 NVIDIA GPU. \nPytorch library with python 3.7. The Hardware \nrequirements of the proposed work include an Intel \nCore i7 -12th generation system, 1TB Hard Disk, \n32GB RAM, and Software resources such as \nWindows 10 Operating System, google collab with \nK80 NVIDIA GPU as Software Tool, Python 3.7 \nCoding Language and Pytorch Library. \n3.7.1Training parameters and hyperparameters \nThe input image size for YOLOv5 should be of size \n416×416 dimension and a batch size of 50 i.e., \namount of training data in one iteration, 715 epochs \nare used to train the model, with learning rates of 0.1 \nand 0.01 to learn the model, and 0.0005 of weight \ndecay is applied. Different training parameters and \nhyperparameters for training YOLOv5 usi ng the \nPlantDoc dataset are shown in Table 3. \nTable 3  Training parameters/hyperparameters for \nleaf disease detection using YOLOv5 \nS. No Training parameters Values \n1 Image size 416 × 416 \n2 Batch size 50 \n3 Epochs 715 \n4 Learning rate lr 0 0.01 \nlr f 0.1 \n5 Weight decay 0.0005 \n \n3.7.2Multistage leaf disease classification \nFor finding the severity of the plant leaf disease, \nmultistage classification is performed on the \nplantvillage dataset. The different training parameters \nused for the plantvillage dataset for classifying the \nstage of the disease using ViT are shown in Table 4. \nThe input image size for ViT should be of size \n224×224 dimension and batch size is 64 and a total \nnumber of 200 epochs are taken to train the model, \nlearning rate of 2e -5 is u sed to learn the model and \n\nRevanasiddappa Bandi et al. \n290 \n \n0.7 of gamma is applied to check how close the \ntraining reaches and seed value of 42 has been used. \nThe apple leaf disease class has been chosen for \ntraining and testing the data using ViT classifier by \nconsidering with backgroun d and without \nbackground removed images. Table 5  and 6 shows \nthe statistics of the data used for training, testing, and \nvalidation and the number of images used should be \nequal for the ViT classifier. Different stages of apple \nscab and rust leaf are low, m oderate, and high. The \ndata imbalance problem is faced in this stage because \nthe model requires an equal number of images for \ntraining, testing, and validation. So, here we have \nused data augmentation techniques like rotation, flip \nand rotate at different angles to maintain an equal \nnumber of images for each step. \n \nTable 4 Training parameters/ hyperparameters for leaf disease classification using ViT \nS. No. Training parameters Values \n1 Image size 224 x 224 \n2 Batch size 64 \n3 Epochs 200 \n4 Learning rate 2e-5 \n5 Gamma 0.7 \n6 Seed 42 \n \nTable 5  Training, testing, and validation data for multistage leaf disease classification using plantvillage dataset \nwith background images \nType of disease class Stage of the disease Train Test Validation \nApple Scab Leaf Apple Scab Low 217 217 217 \nApple Scab Moderate 216 216 216 \nApple Scab High 197 197 197 \nApple Rust Leaf Apple Rust Low 216 216 216 \nApple Rust Moderate 203 203 203 \nApple Rust High 211 211 211 \n              Total Images 1260 1260 1260 \n \nTable 6  Training, testing, and validation data for multistage leaf disease classification using plantvillage dataset \nwithout background images \nType of disease class Stage of the disease  Train Test Validation \nApple Scab Leaf Apple Scab Low 221 221 221 \nApple Scab Moderate 199 199 199 \nApple Scab High 202 202 202 \nApple Rust Leaf Apple Rust Low 187 187 187 \nApple Rust Moderate 227 227 227 \nApple Rust High 216 216 216 \n                Total Images 1252 1252 1252 \n \n4.Results \nThe main goal of the research work is to find the best \nstate of the model for plant leaf disease detection and \nperform multi -stage classification based on severity \nlevel. Experimental results show that the training is \ndone using both original datasets as well as \nsupplemented datasets. So, this model can be \ninstalled for real-time prediction. \n \n4.1Performance evaluation \nThe trained model performance is evaluated by using \na confusion matrix as depicted in Figure 7.  The \ncalculation is performed based on the actual value v/s \npredicted value that belongs to different classes. In \nleaf disease detection training using YOLOv5 the \nmAP values generated for the classes apple scab, \napple leaf (leaflet), and apple rust are 0.5 51, 0.258, \nand 0.245 respectively at a confidence score of 0.7. \n \n \n \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n291          \n \n \nFigure 7 Confusion matrix of leaf disease detection using plantdoc dataset \n \n(a)Leaf disease detection training performance \nF1 score is used as a statistical measure to rate \nperformance. The F1 score is applicable for any point \non the receiver operating characteristic (ROC) curve. \nFor the F1 score to be high, both precision and recall \nvalues should be high. Figure 8 (a) shows the F1 \ncurve of leaf disease detection training which consists \nof 27 different classes of the PlantDoc dataset the \nconfidence score is plotted on the x -axis and the F1 \nscore on the y -axis. The grey color in the graph \nindicates different classes of F1 scores and  the blue \ncolor indicates the overall F1 score of all the classes. \nThe maximum F1 score is 0.57 at a 0.223 confidence \nscore. A precision and recall curve are a plot of a \ngraph which are drawn based on recall at the x -axis \nand precision at the y -axis. It is  also called a \nPrecision-Recall (PR) curve and is used for different \nprobability thresholds. PR curve uses class imbalance \nwhich is estimated on the baseline to inform how \nwell the model shall perform, for the given specific \nimbalance. The grey color in th e graph indicates \ndifferent classes' PR values and the blue color \nindicates the overall PR value of all the classes. It is \nobserved that the graph has a descending trend which \nmeans if the threshold is lower than you will have \nmore false positive predictio ns and if the threshold is \nhigh then you will have more false negative \npredictions. The PR-score is 0.623 at a mAP of 0.5 as \nshown in Figure 8(b). \n \nb) Accuracy and Loss graph  \nIn order to better understand the performance of leaf \ndisease detection using YO LOv5, find the different \nlosses such as classification loss(cls_loss), objectness \nloss (obj_loss) is the confidence of object presence \nwhich is 0.028874, and bounding box regression loss \n(box_loss) is 0.03059 as shown in Table 7. Since the \ndataset contains  multiple classes the classification \nerror is 0.006629. Precision measures the percentage \nof accurate bbox forecasts and how much of the true \nbbox was properly predicted is measured by a recall, \nthe precision and recall values are 0.66002 and \n0.54152 respectively. A mAP is an evaluation metric \nused in the object detection model and it is calculated \nby fixing the confidence threshold. mAP 0.5 refers to \nthe mAP at a threshold of 0.5 for IoU  which is \n0.61814. The average mAP over various IoU \nthresholds, from 0.5 to 0.95, is represented by the \nnotation mAP 0.5:0.95 which is 0.46136. The \nvalidation or testing box_loss(error), obj_loss(error), \nand cls_loss(error) is 0.031752, 0.011021, and \n0.041854 respectively. The same values have been \nrepresented using different graphs as training loss, \naccuracy metrics, and testing or validation loss as \nshown in Figure 9. \n\nRevanasiddappa Bandi et al. \n292 \n \n \na) F1 curve for PlantDoc Dataset                                                                      b) PR curve for PlantDoc \nFigure 8 F1 curve and PR curve of leaf disease detection training \n \nTable 7 Accuracy and loss values of leaf disease detection \nAccuracy/Loss Value \nTraining/box_loss(error) 0.028874 \nTraining/obj_loss(error) 0.03059 \nTraining/cls_loss(error) 0.006629 \nMetrics/precision 0.66002 \nMetrics/recall 0.54152 \nMetrics/mAP_0.5 0.61814 \nMetrics/mAP_0.5:0.95 0.46136 \nValidation/box_loss(error) 0.031752 \nValidation/obj_loss(error) 0.011021 \nValidation/cls_loss(error) 0.041854 \n \n \nFigure 9 Accuracy and loss graph of leaf disease detection training \n \n \n\nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n293          \n \nc) Leaf disease detection testing analysis \nThe leaf disease detection testing analysis can be \nshown using the boxplot which gives confidence \nscores representation at threshold 0.3, 0.5, and 0.7 on \nthe x-axis and mAP on the y -axis as shown in Figure \n10. The confidence score or classification threshold is \nnothing but how confident the machine learning \nmodel is that the appropriate intent was assigned. \nBased on how the neural networks function, the score \ncan range from 0 to 1. Typically, a score for each \nuser input is calculated for each intent, and th e \nhighest score is returned as the outcome. It is \nobserved that the best Average mAP of 0.443 at a \nconfidence score of 0.7 as there are no outliers and \ninter quartile range (IQR) is high. Table 8  shows \nconfidence score values and corresponding average \nmAP at the different thresholds. The box confidence \nscore is nothing but a minimum confidence threshold, \nthe model has detected an object. \n \n \nFigure 10 Box plot of leaf disease detection testing on plantdoc dataset \n \nTable 8 Inter quartile range of average map for \ndifferent confidence scores of leaf disease detection \ntesting \nS. No. Confidence score Average mAP \n1 0.3 0.2345 \n2 0.5 0.2593 \n3 0.7 0.443 \n \n4.2Performance analysis of background removal \ntechnique \nTo evaluate the performance of background removal \nmanually, two classes of apple diseased leaves are \nconsidered, apple scab and apple rust for finding a \nbackground of the image using U2 -Net architecture. \nTo know the performance of U2 -Net, the input image \nwas annotated using an open -source labeling tool \ncalled LabelMe [47] to get the binary image as shown \nin Figure 11 . The dice score or dice coefficient is \nused for determining the background removal \nperformance which is a statistical tool used to \ndetermine h ow similar two samples of data are. In \nthis study, the dice score for two classes of apple \ndiseased leaves is calculated using the sum of the \nintersection of both images to the sum of two images. \nThe dice score of apple scab is 0.750+ - 0.097 and \napple rust is 0.756 + - 0.093 depicted in the box plot \nas shown in Figure 12. \n \n \nFigure 11 Apple scab leaf RGB image and binary image with background \n\nRevanasiddappa Bandi et al. \n294 \n \n \nFigure 12 Dice score of apple scab and apple rust using U2-Net \n \n4.3Performance analysis of multistage leaf disease \nclassification \nClassification metrics are employed to assess a \nclassification model, they indicate if the classification \nis good or unsatisfactory. The various measures \ninclude, 1) Accuracy: It is calculated as the \nproportion of acc urate forecasts to all predictions. 2) \nConfusion matrix: It is a table that contrasts model \npredictions with the ground truth. 3) Precision: is \nmeasured by how many accurate forecasts are \nproduced 4) Recall: is a metric for how many out of \nall the positive  cases in the data that the classifier \ncorrectly predicted. 5) F1  Score: the weighted \naverage of Recall and Precision. Therefore, this score \ntakes both false positives and false negatives into \naccount represented in Equation 5. \nF1 Score =   \n                \n                  (5) \n \nThe performance of multistage leaf disease \nclassification is calculated through the above \nmeasures. The output of a classification algorithm is \nshown and summarized in a confusion matrix as \nshown in Figure 13.  The overall F1  score of \nmultistage leaf disease classification with and \nwithout background are 0.75842 and 0.90846 as \ndepicted in Figure 14. \n \n \n \n \n4.4Integration of leaf disease detection and \nseverity classification \nFor the integration model, an input image is passed \nthen it will detect and identify the type of image as \napple rust or apple scab then the contour of the image \nwill be shown, A contour image is a visualization of \nan object where the edges or boundaries of  the \nobjects or surfaces are highlighted using lines or \ncurves of a specific color. In other words, a contour \nimage represents the shape and structure of an object \nor surface by showing its outermost boundaries. The \ncontours are a useful tool for both form  analysis and \nobject detection and recognition. Finally, the heat \nmap image is produced, it is a graphical \nrepresentation of data that uses color coding to \nvisualize the intensity or density of values in a two -\ndimensional matrix or table. In a heat map, ea ch cell \nof the matrix is assigned a color that represents the \nvalue it contains, with colors ranging from low to \nhigh intensity. The colors can be any gradient, but \ntypically, red or yellow colors are used to represent \nhigh values, while green or blue colo rs are used to \nrepresent low values. The leaf severity is calculated \nbased on the percentage of infection on the particular \nleaf using the ViT classifier. In this work, we have \ntested our model for two classes of apple leaf disease \nnamely apple rust and ap ple scab. The end -to-end \npipeline of the leaf disease detection and \nclassification results for two samples of the apple rust \nleaf and apple scab with severity and \nrecommendation solutions are shown in Figure 15  \nand Figure 16. \n\nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n295          \n \n \nFigure 13 Confusion matrix of multistage leaf disease classification with and without background  \n \n\nRevanasiddappa Bandi et al. \n296 \n \n \nFigure 14 Precision, recall, and F1 score of multistage leaf disease classification with and without background  \n \n4.5Comparative analysis \nThis section addresses the performance of two classes \nof apple leaf diseases i.e. apple rust and apple scab \nwith their severity levels as low, moderate, and high. \nThe performance standards precision, recall and F1 \nscore have been calculated for each of the  severity \nlevels with the support value being considered. Table \n9 and Table 10  depicts the performance values of \nmultistage leaf disease classification with background \nand without background. The complete F1 score of \nclassification with background is 0.758, and without \nbackground is 0.910. Table 11 shows the comparison \nresults of the F1 score for multistage leaf disease \nclassification with and without background using \nViT. \n \n \n\nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n297          \n \n \nFigure 15 End-to-end leaf disease detection and classification results for apple rust \n \n \nFigure 16 End-to-end leaf disease detection and classification results for apple scab \n\nRevanasiddappa Bandi et al. \n298 \n \nTable 9 Precision, recall, and F1 score of Multistage Leaf Disease Classification with background \n \nTable 10 Precision, recall, and F1 score of multistage leaf disease classification without background \n \nTable 11 Comparison of F1 Score for multistage leaf disease classification with and without background using ViT  \nS. No. Model Dataset used F1 score \n1 Multistage classification using ViT with \nbackground \nApple leaf disease from plantvillage \ndataset 0.758 \n2 Multistage classification using ViT without \nbackground \nApple leaf disease from plantvillage \ndataset 0.910 \n \n5.Discussion  \nIn this section, a detailed discussion is made based on \nthe results obtained in the above sections. The \nexperimental study performed is multifold. First, \ndisease detection is performed using YOLOv5 deep \nlearning model. Second, background removal is \nperformed using U2-Net in order to correctly classify \nthe diseased leaf. Third, leaf disease stage \nclassification is performed by using the ViT \nclassifier. Fourth, the performance analysis of leaf \ndisease detection, background removal technique, \nseverity classification, and, finally the comparison \nstudy is discussed. The disease detection training \nperformance on the PlanDoc dataset is represented \nthrough a confusion matrix (see Figure 7), where the \naverage mAP values generated for classes apple \nleaf(leaflet), apple scab leaf(leaflet), and apple rust \nleaf(leaflet), are 0.258,0.551 and 0.245 respectively \nat a confidence score of 0.7. Similarly, the \nperformance is shown by the F1 curve and PR -curve \n(see Figure 8) where the maximum F1 score is 0.57 \nat a confidence score of 0.223 by considering all the \nclasses of PlantDoc dataset and the PR score is 0.623 \nat 0.5 mAP. The different values of accuracy and loss \nof leaf disease detection using YOLOv5 are \ncalculated ( see Figure 9 ). The training losses are \ncalculated such as box loss is 0.028874, object loss is \n0.03059, and classification loss is 0.0066293. \nSimilarly, validation losses are calculated and their \nvalues are 0.031752, 0.011021, and 0.041854 \nrespectively. The accuracy value is 0.66002, recall is \n0.54152, mAP_0.5 is 0.61814 and mAP_0.5:0.95 is \n0.46136. The performance of background removal is \nperformed by using the dice score metric. The Dice \nscore of apple scab is 0.750+ -0.0977 and apple rust is \n0.756+-0.093 (see Figure 12). Finally, the integration \nresults have been shown for apple rust and apple scab \nin which first the input image is passed to the \nintegration model which will identify as either apple \nrust or apple scab, and then a contour image is \ngenerated which represents the severity of leaf than, \nthe heat map image is generated which represents the \ncolor pixel value of hue or intensity and finally the \nseverity of the leaf is shown with a scientific name of \nthe disease and the recommended solution for that  \nparticular disease which will be helpful for the \nfarmers to take the action against the disease. \n \nThe comparative analysis is performed by \nconsidering precision, recall, and F1 score with \nsupport value as performance standards. F1  score of \ndifferent severity levels of apple rust disease with the \nbackground are apple_rust_high, apple_rust_low, and \napple_rust_moderate is 0.668, 0.736, and 0.694. \nSimilarly, the F1 score of different severity levels of \napple scab disease with the background are \nS. No. Classes Precision     Recall   F1 score   Support \n1 Apple_rust_High        0.854 0.545 0.668 115 \n2 Apple_rust_Low        0.853 0.648 0.736 140 \n3 Apple_rust_Moderate               0.556 0.916 0.694 186 \n4 Apple_Scab_High        0.951 0.583 0.723 115 \n5 Apple_Scab_Low        0.897 0.931 0.919 202 \n6 Apple_Scab_Moderate        0.707 0.931 0.813 201 \n Accuracy   0.758 959 \nS. No. Classes Precision     Recall   F1 score   Support \n1 Apple_rust_High        0.844 0.958 0.898 207 \n2 Apple_rust_Low        0.923 0.903 0.913 169 \n3 Apple_rust_Moderate               0.955 0.903 0.913 194 \n4 Apple_Scab_High        0.902 0.921 0.914 186 \n5 Apple_Scab_Low        0.862 0.936 0.898 207 \n6 Apple_Scab_Moderate        0.988 0.874 0.928 174 \n Accuracy   0.910 1137 \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n299          \n \napple_scab_high, apple_scab_low, and \napple_scab_moderate is 0.723, 0.919, and 0.813. So,  \nthe overall F1 score of multistage classification of \napple leaf with background is 0.738 (see Table 9 ). \nThe F1 score of different severity levels of apple rust \ndisease without backgr ound are apple_rust_high, \napple_rust_low, and apple_rust_moderate is 0.898, \n0.913, and 0.913. Similarly, the F1 score of different \nseverity levels of apple scab disease without \nbackground are apple_scab_high, apple_scab_low, \nand apple_scab_moderate is  0.914, 0.898, and 0.928. \nSo, the overall F1 score of multistage classification of \napple leaf without background is 0.910 (see Table \n10). By comparing both results, the F1 score of \nmultistage classification of apple leaf without \nbackground outperforms with a background. \n \nImplications: The implications of this research work \nare as follows. \nImproved accuracy in leaf disease classification: The \nproposed deep learning approach achieves high \naccuracy in classifying the severity of leaf diseases in \nplants. This could lead to more effective and timely \nmanagement of plant diseases, potentially leading to \nincreased crop yields and reduced economic losses. \n \nExplainability in AI: An explicable AI strategy offers \ninsights into the model's decision -making process. \nThis is important in many applications, such as \nagriculture and healthcare, where it is critical to know \nwhy a model made a certain decision. Transferability \nof transformer -based models: Modern methods for \nNLP tasks are used in this article, including the \ntransformer network design. The success of this \napproach in classifying leaf diseases suggests that \ntransformer-based models can be applied to other \nimage classification tasks, as well.  Advancements in \nplant disease detection: By enabling early \nidentification and management of plant disease, the \napplication of deep learning to plant disease detection \nhas the potential to revolutionize agriculture. This \ncould lead to reduced pesticide use, lower costs, and \nincreased yields. \n \nOverall, leaf disease severity classification with XAI \nusing transformer networks has implications for \nvarious fields, including agriculture, AI, and \nexplainability. It demonstrates t he potential of deep \nlearning approaches in plant disease detection and \nmanagement, while also highlighting the importance \nof XAI in ensuring trust and transparency in AI \nsystems. \nLimitation: This work is limited to the severity \nclassification of apple lea f diseases, apple rust, and \napple scab. In Appendix I, an exhaustive list of \nacronyms is provided. \n \n6.Conclusion and future work  \nThe primary goal of the proposed work is to find the \nmost cutting-edge deep-learning model for plant leaf \ndisease detection and stage classification based on the \nseverity of the plant leaf. Real -time plant leaf disease \ndetection and stage classification us ing XAI \nmethodology has been implemented in this research \nwork. This has given an insight into the classification \nand also provided expert recommendations through \nthe suggested solution to mitigate the leaf disease to \nincrease the end -users' trust and supp ort. The multi -\nclass apple plant leaf diseases are detected and the \nbest classification stage is identified as it belongs to \nlow, moderate, and high severity based on the \nseverity of the infection and a real -time application \ncan be inferred using deep lear ning techniques. \nYOLOv5 with various confidence scores was used in \nthe tests using the PlantDoc dataset to detect leaf \ndisease. The maximum detection accuracy for plant \nleaf disease is 0.443 mAP with a confidence score of \n0.7. Background removal is perform ed using U2 -Net \narchitecture for both Apple Scab and Apple Rust \ndiseased leaf. The mean value of the Apple Scab leaf \nand Apple Rust leaf are 0.750 and 0.756 and the \nstandard deviation (STD) is 0.097 and 0.093 \nrespectively. Gradient -weighted class activatio n \nmapping (Grad -CAM), an explainable AI method, \nwas applied to the projected output using a YOLOv5-\nbased validation to foster trust with an F1 score of \n91% on both the original and enhanced datasets, the \nViT was determined to be the best classification \nmodel for diagnosing the infected leaf. The client -\nserver interface is used by the end -to-end model. \nThrough the offered client -server interface, users can \nupload photos of infected leaves. The end -user \nreceived a professional recommendation based on the \nclassification and analysis. The proposed system can \nmake a redolent contribution to agriculture research. \n \nFuture work:  Even though high accuracy is \nachieved on the classification dataset, however on the \nreal-time datasets, one can measure recognizable \nimprovements in accuracy. The system can be \nimproved with many different plant species with \ndifferent leaf diseases, and it is expected to be \nimproved significantly with more and more training \ndata in the future.  \n \n \nAcknowledgment \nNone. \nRevanasiddappa Bandi et al. \n300 \n \nConflicts of interest \nThe authors have no conflicts of interest to declare. \n \nAuthor’s contribution statement \nRevanasiddappa Bandi: Worked on the literature survey, \nmethodology, implementation, result analysis, and \nmanuscript preparation. Suma Swamy:  Worked on the \nmanuscript correction. Arvind C.  S: Worked on the \nproblem formulation and manuscript correction. \n \nReferences \n[1] https://agricoop.nic.in/Documents/annual-report-2020-\n21.pdf. Accessed 19 February 2023. \n[2] Raza A, Razzaq A, Mehmood SS, Zou X, Zhang X, \nLv Y, et al.  Impact of climate change on crops \nadaptation and strategies to tackle its outcome: a \nreview. Plants. 2019; 8(2):1-29. \n[3] López MM, Bertolini E, Olmos A, Caruso P, Gorris \nMT, Llop P, et al . Innovative tools for detection of \nplant pathogenic viruses and bacteria. International \nMicrobiology. 2003; 6:233-43. \n[4] Liang Q, Xiang S, Hu Y, Coppola G, Zhang D, Sun \nW. PD2SE -Net: computer-assisted plant disease \ndiagnosis and severity esti mation network. Computers \nand Electronics in Agriculture. 2019; 157:518-29. \n[5] Singh D, Jain N, Jain P, Kayal P, Kumawat S, Batra \nN. PlantDoc: a dataset for visual plant disease \ndetection. In  proceedings of the 7th ACM IKDD \nCoDS and 25th COMAD 2020 (pp. 249-53). \n[6] https://www.kaggle.com/datasets/abdallahalidev/plant\nvillage-dataset. Accessed 10 February 2023. \n[7] Fang Y, Ramasamy RP. Current and prospective \nmethods for plant disease detection. Biosensors. 2015;  \n5(3):537-61. \n[8] Mou L, Meng Z, Yan R, Li G, Xu Y, Zhang L, et al. \nHow transferable are neural networks in NLP \napplications? Proceedings of the conference on \nempirical methods in natural language processing  \n2016 (pp. 479-89). Association for Computational \nLinguistic. \n[9] Dhanya VG, Subeesh A, Kushwaha NL, Vishwakarma \nDK, Kumar TN, Ritika G, et al. Deep learning based \ncomputer vision approaches for smart agricultural \napplications. Artificial Intelligence in Agriculture. \n2022; 6:211-9. \n[10] Dhaygude SB, Kumbhar NP. Agricultural plant leaf \ndisease detection using image processing. \nInternational Journal of Advanced Research in \nElectrical, Electronics and Instrument ation \nEngineering. 2013; 2(1):599-602. \n[11] Sethy PK, Barpanda NK, Rath AK. Detection and \nidentification of rice leaf diseases using multiclass \nSVM and particle swarm optimization technique. \nInternational Journal of Innovative Technology and \nExploring Engineering. 2019; 8(6S2):2278-3075. \n[12] Sebastian VB, Unnikrishnan A, Balakrishnan K. Gray \nlevel co-occurrence matrices: generalisation a nd some \nnew features. International Journal of Computer \nScience, Engineering and Information Technology.  \n2012; 2(2):151-7. \n[13] Khirade SD, Patil AB. Plant disease detection using \nimage processing. In  international conference on \ncomputing communication control and automation \n2015 (pp. 768-71). IEEE. \n[14] Cao D, Chen Z, Gao L. An improved object detection \nalgorithm based on multi -scaled and deformable \nconvolutional neural networks. Human -centric \nComputing and Information Sciences. 2020;  10(1):1-\n22. \n[15] Guo Y, Zhang J, Yin C, Hu X, Zou Y, Xue Z, et al. \nPlant dis ease identification based on deep learning \nalgorithm in smart farming. Discrete Dynamics in \nNature and Society. 2020; 2020:1-11. \n[16] Cohen R. The chan -vese algorithm. Computer Vision \nand Pattern Recognition. 2011:1-18. \n[17] Hassan SM, Maji AK, Jasiński M, Leonowicz Z, \nJasińska E. Identification of plant -leaf diseases using \nCNN and transfer -learning approach. Electronics. \n2021; 10(12):1-19. \n[18] Rashid J, Khan I, Ali G, Almotiri SH, Al ghamdi MA, \nMasood K. Multi-level deep learning model for potato \nleaf disease recognition. Electronics. 2021;  10(17):1-\n27. \n[19] Redmon J, Divvala S, Girshick R, Farhadi A. You \nonly look once: unified, real-time object detection. In \nproceedings of the conference on computer vision and \npattern recognition 2016 (pp. 779-88). IEEE. \n[20] Redmon J, Farhadi A. Y olo9000: better, faster, \nstronger. In proceedings of the conference on \ncomputer vision and pattern recognition 2017 (pp. \n7263-71). IEEE. \n[21] Redmon J, Fa rhadi A. Yolov3: an incremental \nimprovement. Computer Vision and Pattern \nRecognition. 2018:1-6. \n[22] Bochkovskiy A, Wang CY, Liao HY. Yolov4: optimal \nspeed and accuracy of object detection. Computer \nVision and Pattern Recognition. 2020:1-17. \n[23] https://github.com/ultralytics/yolov5. Accessed 10 \nFebruary 2023. \n[24] Ren S, He K, Girshick R, Sun J. Faster R-CNN: \ntowards real -time object detection with region \nproposal networks. IEEE Transactions on Pattern \nAnalysis and Machine Intelligence. 2016:1-14.  \n[25] http://www.pascal-\nnetwork.org/challenges/VOC/voc2007/index.html. \nAccessed 15 February 2023. \n[26] http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc20\n12/index.html. Accessed 15 February 2023. \n[27] Lin TY, Maire M, Belongie S, Hays J, Perona P, \nRamanan D, et al. Mic rosoft coco: common objects in \ncontext. In computer vision –ECCV, 13th European \nconference, Zurich, Switzerland, proceedings 2014 \n(pp. 740-55). Springer International Publishing. \n[28] Tan M, Le Q. Efficientnet: rethinking model scaling \nfor convolutional neural networks. In  international \nconference on mac hine learning 2019 (pp. 6105 -14). \nPMLR. \n[29] Malik HS, Dwivedi M, Omkar SN, Javed T, Bakey A, \nPala MR, et al. Disease recognition in sugarcane crop \nusing deep learning. In  advances in artificial \nInternational Journal of Advanced Technology and Engineering Exploration, Vol 10(100)                                                                                                             \n301          \n \nintelligence and data engineering 2021 (pp. 189 -206). \nSpringer Singapore. \n[30] Toda Y, Okura F. How convolutional neural networks \ndiagnose plant disea se. Plant Phenomics. 2019 . \n2019:1-14. \n[31] Nagasubramanian K, Jones S, Singh AK, Sarkar S, \nSingh A, Ganapathysubramanian B. Plant disease \nidentification using explainable 3D  deep learning on \nhyperspectral images. Plant Methods. 2019; 15:1-10. \n[32] Liu J, Wang X. Early recognition of tomato gray lea f \nspot disease based on MobileNetv2 -Yolov3 model. \nPlant Methods. 2020; 16:1-6. \n[33] Malik A, Va idya G, Jagota V, Eswaran S, Sirohi A, \nBatra I, et al . Design and evaluation of a hybrid \ntechnique for detecting sunflower leaf disease using \ndeep learning approach. Journal of Food Quality. \n2022; 2022:1-12. \n[34] Tammina S. Transfer learning using vgg -16 with deep \nconvolutional neural network for classifying images. \nInternational Journal of Scientific and Research \nPublications. 2019; 9(10):143-50. \n[35] Sandler M, Howard A, Zhu M, Zhmoginov A, Chen \nLC. Mobilenetv2: inverted residuals and linear \nbottlenecks. In proceedings of the conference on \ncomputer vision and pattern recognition 2018 (pp. \n4510-20). IEEE \n[36] Srinidhi VV, Sahay A, Deeba K. Plant pathology \ndisease detection in apple leaves using deep \nconvolutional neural networks: apple leaves disease \ndetection using efficientnet and densenet. In  5th \ninternational conference on computing methodologies \nand communication 2021 (pp. 1119-27). IEEE. \n[37] Sibiya M, Sumbwanyambe M. Automatic fuzzy logic -\nbased maize common rust disea se severity predictions \nwith thresholding and deep learning. Pathogens. 2021;  \n10(2):1-17. \n[38] Shiferaw D, Hailegnaw A, Assefa A, Abebe D, Dagne \nE, Fekadie G. LabVIEW based fuzzy logic contr oller \nfor PLC. In  international conference on computer, \ncontrol, electrical, and electronics engineering 2019 \n(pp. 1-4). IEEE. \n[39] Shi T, Liu Y, Zheng X, Hu K, Huang H, Liu H, Huang \nH. Recent advances in plant disease severity \nassessment using convolutional neural networks. \nScientific Reports. 2023; 13(1):1-14. \n[40] Abd AYM, Caro OJ, Bravo LM, Kaur C, Al AMS, \nBala BK. Leaf disease identification and classification \nusing optimized deep learning. Measurement: Sensors. \n2023; 25:1-6. \n[41] Mohammed L, Yusoff Y. Detection and classification \nof plant leaf diseases using digital image processing \nmethods: a review. ASEAN Engineering Journal. \n2023; 13(1):1-9. \n[42] Han K, Wang Y, Chen H, Chen X, Guo J, Liu Z, et al. \nA survey on vision transformer. IEEE Transactions on \nPattern Analysis and Machine Intelligence. 2022;  \n45(1):87-110. \n \n[43] Xu R, Lin H, Lu K, Cao L, Liu Y. A forest fire \ndetection system based on ensemble learning. Forests. \n2021; 12(2):1-16. \n[44] Bo S, Jing Y. Image clustering using mean shift \nalgorithm. In  fourth international conference on \ncomputational intelligence and communication \nnetworks 2012 (pp. 327-30). IEEE. \n[45] Canny J. A computational approach to edge detection. \nIEEE Transactions on Pattern Analysis and Machine \nIntelligence. 1986; 8(6):679-98. \n[46] Qin X, Zhang Z, Huang C, Dehghan M, Zaiane OR, \nJagersand M. U2 -Net: going deeper with nested U -\nstructure for salient object detection. Pattern \nRecognition. 2020; 106(2020):1-12. \n[47] Arvind CS, Prajwal R, Bhat PN, Sreedevi A, \nPrabhudeva KN. Fish detection and tracking in \npisciculture environment using deep instance \nsegmentation. In TENCON region 10 conference 2019 \n(pp. 778-83). IEEE. \n \nRevanasiddappa Bandi  received a \nB.E. degree in Computer Science & \nEngineering from PDA College of \nEngineering, Gulbarga, Karnataka, \nIndia, in the year 2008 and M. Tech in \nComputer Science & Engineering from \nDr. Ambedkar Institute of Technology, \nBangalore, India in the year 2010, \ncurrently pursuing a Ph.D. degree from the  Visvesvaraya \nTechnological University, Belagavi, India. He is currently \nworking as Assistant Professor with the Department of \nComputer Science and Engineering at Dr. Ambedkar \nInstitute of Technology, Bangalore, India. His major \nresearch areas are the Inte rnet of Things, Smart \nAgriculture, Machine Learning, and Artificial Intelligence. \nEmail: revanasiddappa.cs@drait.edu.in \n \nDr. Suma Swamy received a B.E. \ndegree in Electronics from Shivaji \nUniversity, Maharashtra, India, in 1990 \nand M. Tech in Electronics from \nVisvesvaraya Technological \nUniversity, Belagavi, India in year \n2006. She has completed Ph.D. degree \nfrom Anna University, Chennai, I ndia \nin year 2014. She joined various different engineering \ncolleges at different positions. She is currently working as \nProfessor with the Department of Computer Science and \nEngineering at Sir. M Visvesvaraya Institute of \nTechnology, Bangalore, India. Her major research areas are \nSpeech Processing, Data Mining, Big Data, Machine \nLearning, Natural Language Processing  and IoT. She has \nmany national and international research publications. \nEmail: sumaswamy_cs@sirmvit.edu \n \n \n \n \n \n \n \n \nRevanasiddappa Bandi et al. \n302 \n \nArvind C. S is a Senior Researcher in \nthe Bioinformatic Institute (BII) at the \nAgency for Science, Technology and \nResearch (A*STAR), Singapore & \nIEEE Senior Member. He has received \nhis Master of Technology degree in \nComputer Science Engineering from \nVisvesvaraya Technological University \n(VTU), India. He has worked on AI for Mobile Platform, \nVision-Based Driver Assistance Autonomous Vehicle \nProgram, Biomedical Imaging, Internet of Things (IoT). He \nhas published over 20 referred articles in IEEE, Elsevier \nand Springer Jo urnals and several patents. His current \nresearch interests include Artificial Intelligence, AI for \nRadiology, Deep Generative Models, Reinforcement \nLearning and optimization.  \nEmail: csarvind2000@gmail.com      \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAppendix I \nS. No. Abbreviation Description \n1 ACO-CNN Ant Colony Optimization with \nConvolution Neural Network  \n2 ADAM Adaptive Moment Estimation \n3 AI Artificial Intelligence \n4 CADS Computer-Aided Disease Detection \nSystem  \n5 CBN Cross-Iteration Batch Normalization \n6 CNN Convolution Neural Network \n7 COCO Common Objects in Context \n8 CSP Cross-Stage-Partial Connections  \n9 DCNN Deep Convolutional Neural Network \n10 Faster R-CNN Faster Region -Based Convolutional \nNeural Networks \n11 FLOPS Floating-Point Operations Per \nSecond \n12 FPN Feature Pyramid Network  \n13 GDP Gross Domestic Product \n14 GLCM Gray Level Cooccurrence Matrix \n15 GPU Graphics Processing Unit \n16 Grad-CAM Gradient-Weighted Class Activation \nMapping  \n17 HSV Hue Saturation Value \n18 ILSVRC ImageNet Large Scale Visual \nRecognition Challenge  \n19 IOU Intersection Over Union \n20 IQR Inter Quartile Range  \n21 mAP Mean Average Precision  \n22 Multiclass-\nSVM  \nMulticlass Support Vector Machine  \n23 MLP Multi-Layer Perceptron Layer \n24 MS Microsoft \n25 NLP Natural Language Processing \n26 PAN Pan Aggregation Network \n27 PLD Potato Leaf Diseases \n28 PR Precision-Recall  \n29 PSO Particle Swarm Optimization  \n30 ReLU Rectified Linear Unit  \n31 RGB Red Green Blue \n32 ROC Receiver Operating Characteristic  \n33 ROI Region of Interest \n34 RPN Region Proposal Network \n35 RSU Residual U-Block  \n36 SGD Stochastic Gradient Descent \n37 SSD Single Shot Detector  \n38 STD Standard Deviation  \n39 SVM Support Vector Machine  \n40 VGG Visual Geometry Group  \n41  ViT  Vision Transformer \n42 VOC Visual Object Classes Challenge \n43 WRC Weighted-Residual-Connections  \n44 XAI Explainable Artificial Intelligence \n45 YOLO You Only Look Once \n \n ",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.539925754070282
    },
    {
      "name": "Computer science",
      "score": 0.49238622188568115
    },
    {
      "name": "Disease",
      "score": 0.4221005141735077
    },
    {
      "name": "Machine learning",
      "score": 0.36887913942337036
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33287882804870605
    },
    {
      "name": "Medicine",
      "score": 0.29473310708999634
    },
    {
      "name": "Pathology",
      "score": 0.13532570004463196
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    }
  ]
}