{
    "title": "Utilizing Large Language Models for Named Entity Recognition in Traditional Chinese Medicine against COVID-19 Literature: Comparative Study (Preprint)",
    "url": "https://openalex.org/W4388940332",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5060232948",
            "name": "Xu Tong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5049574875",
            "name": "Nina Smirnova",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5006264539",
            "name": "Sharmila Upadhyaya",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5031363707",
            "name": "Ran Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5092442081",
            "name": "Jack H. Culbert",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5006235960",
            "name": "Chao Sun",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5076734567",
            "name": "Wolfgang Otto",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5033632230",
            "name": "Philipp Mayr",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3109416268",
        "https://openalex.org/W4220860654",
        "https://openalex.org/W3164834607",
        "https://openalex.org/W3209540526",
        "https://openalex.org/W3116124068",
        "https://openalex.org/W4224324845",
        "https://openalex.org/W4378469337",
        "https://openalex.org/W4323650985",
        "https://openalex.org/W4362700315",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W4362679551",
        "https://openalex.org/W4361806442",
        "https://openalex.org/W4387299960",
        "https://openalex.org/W4376167008",
        "https://openalex.org/W4252608096",
        "https://openalex.org/W4221159609",
        "https://openalex.org/W4295991004",
        "https://openalex.org/W4254034712",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4366999773",
        "https://openalex.org/W4376865066",
        "https://openalex.org/W4382536554",
        "https://openalex.org/W4321524373",
        "https://openalex.org/W4385988359",
        "https://openalex.org/W4320854883",
        "https://openalex.org/W4385963580",
        "https://openalex.org/W4381827011",
        "https://openalex.org/W4323709074",
        "https://openalex.org/W4366595110",
        "https://openalex.org/W4220814225",
        "https://openalex.org/W4220813551",
        "https://openalex.org/W3048090083",
        "https://openalex.org/W4318819520",
        "https://openalex.org/W3096498527",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3206067900",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W3002281830",
        "https://openalex.org/W3020786614",
        "https://openalex.org/W4285219308",
        "https://openalex.org/W4361994586",
        "https://openalex.org/W3213591530",
        "https://openalex.org/W2033591223"
    ],
    "abstract": "<sec> <title>BACKGROUND</title> Recent advances in large language models (LLMs) have shown remarkable performance on various downstream tasks in zero- and few-shot scenarios, shedding light on named entity recognition (NER) in low-resource domains. Traditional Chinese medicine (TCM) against COVID-19 has been a new research topic and led to niche research literature. NER techniques are crucial for extracting and utilizing the rich knowledge in such literature. </sec> <sec> <title>OBJECTIVE</title> To explore and compare the performance of ChatGPT and other state-of-the-art LLMs on domain-specific NER tasks covering different entity types and domains in TCM against COVID-19 literature. </sec> <sec> <title>METHODS</title> We established a dataset of 389 articles on TCM against COVID-19, and manually annotated 48 of them with 6 types of entities belonging to 3 domains as the ground truth, against which the NER performance of LLMs can be assessed. We then performed NER tasks for the 6 entity types using ChatGPT (GPT-3.5 and GPT-4) and 4 state-of-the-art BERT-based question-answering (QA) models (RoBERTa, MiniLM, PubMedBERT and SciBERT) without prior training on the specific task. A domain fine-tuned model (GSAP-NER) was also applied for a comprehensive comparison on one of the entity types. In the task setup, we considered two different matching methods, namely exact match and fuzzy match, to have better coverage of potential application scenarios. The evaluation metrics we used are Precision, Recall, and F-1 in both exact match and fuzzy match. </sec> <sec> <title>RESULTS</title> The overall performance of LLMs varied significantly in exact match and fuzzy match. In the fuzzy match, ChatGPT surpassed BERT-based QA models in 5 out of 6 tasks, while in exact match, BERT-based QA models outperformed ChatGPT in 5 out of 6 tasks but with a smaller F-1 difference. GPT-4 showed a significant advantage over other models in fuzzy match, especially on the entity type of TCM formula and the Chinese patent drug (TFD) and ingredient (IG), achieving a higher F-1 score of 0.814 and 0.689, respectively. Although GPT-4 outperformed BERT-based models on entity type of herb (HB, 0.324), target (TG, 0.319), and research method (RM, 0.443), none of the F-1 scores exceeded 0.5. GSAP-NER, outperformed GPT-4 in terms of F-1 by a slight margin (0.45 vs 0.433) on RM. ChatGPT achieved considerably higher recalls than precisions, particularly in the fuzzy match. </sec> <sec> <title>CONCLUSIONS</title> The NER performance of LLMs is highly dependent on the entity type, and their performance varies across application scenarios. ChatGPT could be a good choice for scenarios where high recall is favored, e.g., for domain novices obtaining an extensive overview of the field. However, for knowledge acquisition in rigorous scenarios, neither ChatGPT nor BERT-based QA models are off-the-shelf tools for professional practitioners. Besides, these BERT-based models are open-source, and thus more accessible for scientific inquiry and hence worth further exploration. </sec>",
    "full_text": null
}