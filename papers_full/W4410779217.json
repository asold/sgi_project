{
  "title": "Situating governance and regulatory concerns for generative artificial intelligence and large language models in medical education",
  "url": "https://openalex.org/W4410779217",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2121377473",
      "name": "Michael Tran",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A1859952413",
      "name": "Chinthaka Balasooriya",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2132490811",
      "name": "Jitendra Jonnagaddala",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A3140595255",
      "name": "Gilberto Ka-Kit Leung",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Neeraj Mahboobani",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2013582931",
      "name": "Subha Ramani",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2105106171",
      "name": "Joel Rhee",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2115977333",
      "name": "Lambert Schuwirth",
      "affiliations": [
        "Flinders University"
      ]
    },
    {
      "id": "https://openalex.org/A5117714528",
      "name": "Neysan Sedaghat Najafzadeh-Tabrizi",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2127126379",
      "name": "Carolyn Semmler",
      "affiliations": [
        "University of Adelaide"
      ]
    },
    {
      "id": "https://openalex.org/A4201704819",
      "name": "Zoie SY Wong",
      "affiliations": [
        "UNSW Sydney",
        "St. Luke's International University",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2121377473",
      "name": "Michael Tran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1859952413",
      "name": "Chinthaka Balasooriya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132490811",
      "name": "Jitendra Jonnagaddala",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3140595255",
      "name": "Gilberto Ka-Kit Leung",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Neeraj Mahboobani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013582931",
      "name": "Subha Ramani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105106171",
      "name": "Joel Rhee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115977333",
      "name": "Lambert Schuwirth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117714528",
      "name": "Neysan Sedaghat Najafzadeh-Tabrizi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127126379",
      "name": "Carolyn Semmler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4201704819",
      "name": "Zoie SY Wong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937037822",
    "https://openalex.org/W4407821436",
    "https://openalex.org/W4399567248",
    "https://openalex.org/W4409050072",
    "https://openalex.org/W3008689577",
    "https://openalex.org/W4324020772",
    "https://openalex.org/W4322758367",
    "https://openalex.org/W4392769984",
    "https://openalex.org/W3189751383",
    "https://openalex.org/W4387602412",
    "https://openalex.org/W4395055094",
    "https://openalex.org/W4367626063",
    "https://openalex.org/W4387144847",
    "https://openalex.org/W3138351336",
    "https://openalex.org/W4238449808",
    "https://openalex.org/W4396766974",
    "https://openalex.org/W4400519915",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W3116286104",
    "https://openalex.org/W4378084879",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W3007086729",
    "https://openalex.org/W4392851477",
    "https://openalex.org/W4321369174",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4385667886",
    "https://openalex.org/W4394956892",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4399354087",
    "https://openalex.org/W2939970150",
    "https://openalex.org/W3097948114",
    "https://openalex.org/W4400168802",
    "https://openalex.org/W4376866715",
    "https://openalex.org/W3196768570",
    "https://openalex.org/W4367039164",
    "https://openalex.org/W4386347795",
    "https://openalex.org/W4393318738",
    "https://openalex.org/W4390546602",
    "https://openalex.org/W4393403769",
    "https://openalex.org/W4387949478",
    "https://openalex.org/W3008960112",
    "https://openalex.org/W4401046213",
    "https://openalex.org/W4393868488",
    "https://openalex.org/W4362557750",
    "https://openalex.org/W4225405007",
    "https://openalex.org/W3123730699",
    "https://openalex.org/W3167015399",
    "https://openalex.org/W3039044431",
    "https://openalex.org/W2942858056",
    "https://openalex.org/W4366587927",
    "https://openalex.org/W4206299767",
    "https://openalex.org/W4391277750",
    "https://openalex.org/W4360938479",
    "https://openalex.org/W4385607650",
    "https://openalex.org/W4406465563",
    "https://openalex.org/W4400530818",
    "https://openalex.org/W2088061011",
    "https://openalex.org/W2096020754",
    "https://openalex.org/W4391582176",
    "https://openalex.org/W4400949264",
    "https://openalex.org/W4366989525",
    "https://openalex.org/W4392863142",
    "https://openalex.org/W4292088805",
    "https://openalex.org/W4393862318",
    "https://openalex.org/W4400115569",
    "https://openalex.org/W4401888862",
    "https://openalex.org/W2990549216",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4378469229",
    "https://openalex.org/W4379054888",
    "https://openalex.org/W2397005921",
    "https://openalex.org/W4389559236",
    "https://openalex.org/W270637301",
    "https://openalex.org/W4403179622",
    "https://openalex.org/W4365136203",
    "https://openalex.org/W4385715939"
  ],
  "abstract": "Generative artificial intelligence (GenAI) and large language models represent gains in educational efficiency and personalisation of learning. These are balanced against the considerations of the learning process, authentic assessment, and academic integrity. A pedagogical approach helps situate these concerns, and informs various types of governance and regulatory approaches. In this review we identify current and emerging issues regarding GenAI in medical education including pedagogical considerations, emerging roles, and trustworthiness. Potential measures to address specific regulatory concerns are explored.",
  "full_text": "npj |digital medicine Review\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01721-z\nSituating governance and regulatory\nconcerns for generative artiﬁcial\nintelligence and large language models in\nmedical education\nCheck for updates\nMichael Tran1 , Chinthaka Balasooriya1, Jitendra Jonnagaddala1,G i l b e r t oK a - K i tL e u n g2,\nNeeraj Mahboobani3, Subha Ramani4,J o e lR h e e1, Lambert Schuwirth5,\nNeysan Sedaghat Najafzadeh-Tabrizi1, Carolyn Semmler6 &Z o i eS YW o n g1,2,7\nGenerative artiﬁcial intelligence (GenAI) and large language models represent gains in educational\nefﬁciency and personalisation of learning. These are balanced against the considerations of the\nlearning process, authentic assessment, and academic integrity. A pedagogical approach helps\nsituate these concerns, and informs various types of governance and regulatory approaches. In this\nreview we identify current and emerging issues regarding GenAI in medical education including\npedagogical considerations, emerging roles, and trustworthiness. Potential measures to address\nspeciﬁc regulatory concerns are explored.\nGenerative artiﬁcial intelligence (GenAI), refers to deep-learning models\nthat can generate high-quality content based on the data on which it was\ntrained\n1. Large language models are a category of foundation models trained\non immense amounts of data. Although not inherently able to understand\ntext and data, these models are ableto generate natural, human-like\nlanguage\n2 which is perceived as“conversational” by users. GenAI differs\nfrom extractive AI technologies that excel in accessing, collating, prioritiz-\ning, adapting, and using information under narrow circumstances3.\nGenAI including large language models (LLMs), provides learners and\neducators in medicine and health sciences with previously-unimaginable\nopportunities for teaching and learning. The scope of potential applications,\nand the efﬁciency with which this could be completed, is rapidly increasing.\nThe utility of its capacity to utilize multi-modal approaches has been\ndemonstrated in cardiac electrophysiology education\n4 and digital pathology5,6.\nAI in general is a complex social, cultural, and material artifact whose\nm e a n i n ga n dp l a c ec o n t i n u et ob ec o n s t r ucted by different stakeholders. There\nremains a paucity of information regarding the development, deployment and\ncommercialization7 of these models and their applications and services based\nupon them8. Unsurprisingly, there has been growing consternation among\neducators, professional bodies, and governments regarding the potential need\nfor regulation, in its various forms, and control of the inﬂuence of this tech-\nnology. It often feels like attempts to provide regulatory frameworks and\nlegislation are reactionary and ineffectu a l ,i nt h ef a c eo fr a p i dg l o b a lp r o g r e s s\nunbounded by any speciﬁc institutional or sovereign authority. Several\nguidelines, ethical considerations9, statement papers, and recommendations\nhave been published in recent years10, including primers for AI3,r e c o m -\nmendations for workforce implications11, and considerations, especially\nethical12, regarding the integration of AI in medical curricula13.\nEnthusiasm for, and trepidation of the future role of GenAI in medical\neducation must to be considered in the context of our evolving under-\nstanding of pedagogical principles and best practices. The impact of GenAI\ncannot be ignored, as it risks multi-level harms ranging from a lack of\nstructures to ensuring scholarly integrity, stagnation, and irrelevance of\nlearning approaches\n14. There is likely a need for sustainable and adaptable\nresponses to GenAI in learning, teaching and assessment15.T h i sr e v i e w\nseeks to situate our current understanding of the impact of GenAI in\nundergraduate medical education, within a pedagogical framework, to\ninform regulatory concerns. A narrow focus recognizes the differences\nbetween undergraduate, postgraduate, and continuing professional educa-\ntion requirements. We outline the concerns of GenAI and LLMs in medical\neducation, pedagogical considerations, and emerging roles, and present a\ndiscussion regarding the regulation and preservation of academic integrity.\nA summary of the key considerations and concerns regarding GenAI and\nLLMs is provided in Fig.1.\n1University of New South Wales, Kensington, NSW, Australia.2The University of Hong Kong, Hong Kong, PR China.3Department of Imaging and Interventional\nRadiology, Faculty of Medicine, The Chinese University of Hong Kong (CUHK), Hong Kong, PR China.4Brigham and Women’s Hospital and Harvard Medical\nSchool, Boston, MA, USA.5Flinders University, Adelaide, SA, Australia.6The University of Adelaide, Adelaide, SA, Australia.7St Luke’s International University,\nChuo, Japan. e-mail: Michael.m.tran@unsw.edu.au\nnpj Digital Medicine|           (2025) 8:315 1\n1234567890():,;\n1234567890():,;\nPedagogical considerations for generative AI in medi-\ncal education\nGenAI and learning\nThe learning process is perhaps the biggest consideration when situating\nGenAI within a pedagogical approach. Educability, that is, the ability of\nlearners to utilize any and all previously-learned information in meaningful\nways, distinguishes their learning capacity from machines\n16.G e n A Ii sl i k e l y\nto be a useful adjunctive tool in medical education but is unlikely to replace\nall the experiences and social interactions that are important for the\ndevelopment of empathetic and contextually aware learners in con-\nstructivist and experiential frameworks. It is important to include infor-\nmation on GenAI in education for all students and educators. Without the\nopportunity to learn about the ethical use of GenAI, learners are more\nsusceptible to engaging in inappropriate use of GenAI\n17. A recent scoping\nreview identiﬁed the need for further research in three key areas to improve\nour understanding of the role of GenAI in medical education. These include\n(i) developing learners’ skills to evaluate GenAI critically, (ii) rethinking\nassessment methodology, and (iii) studying human– AI interactions18.\nWhat learning processes, in the medical student“journey,” are likely to\nbe impacted (adversely or positively) by LLMs and GenAI? Students learn\nthrough a combination of different means, and the theoretical approaches to\nclassify these have revolved around cognitive psychology, humanistic psy-\nchology, and social anthropology. Social constructivism, as an epistemolo-\ngical framework, outlines learning through the construction of knowledge\nand interactions with others by linking new information to that previously\nlearned and incorporating new experiences into a knowledge base. It is not\nsimply the transmission of knowledge from the external world to the\nlearner\n19. A complementary learning theory, experiential learning, deﬁnes\nlearning as a process whereby knowledge is created through the transfor-\nmation of experience. Different learners pass through phases of reﬂective\nobservation, abstract conceptualization, and active experimentation in their\nown preferred order\n20. Therefore, the context in which learning is experi-\nenced and knowledge is acquired is critical. Whilst offering“efﬁciency,” how\nGenAI is situated with respect to the“context” of learning and the transition\nfrom “novice” to “expert” is not yet fully understood. Cognitive psychology\noffers many theories and explanations of how expertise develops, and is\nfostered via learning21. As our understanding and experience of GenAI\ngrows, it may well be that weﬁnd it ﬁts within existing frameworks or\ndemands a novel approach to understanding its role in this transition. The\nextant literature identiﬁes instances where GenAI can accelerate learning in\nnovices\n22 but may also accelerate skill decay and hinder skill acquisition23.A\ntheoretical counterargument posits that GenAI itself could play the role of\n“the more knowledgeable other” in the social constructivist framework24.\nOperating in a metaphorical contextual vacuum, GenAI is unlikely to bypass\nthe process of exposure and experience inlearning. Its inability to teach the\nintegration of contextual and external information, comprehend sensory\nand nonverbal cues, cultivate rapport and interpersonal interaction, and\nalign with overarching medical education and patient care goals\n25 remains a\nkey limitation for the current generation of LLMs.\nClinical reasoning and GenAI\nThe key difference between extractive and GenAI is that the latter leverages\nmachine learning, such as neural networks, to generate new content. This\nmethod is based on the relationships andpatterns found in existing datasets,\nFig. 1 |Summary of the key concerns and considerations for GenAI and LLMs from\nmedical education, pedagogy, and regulatory perspectives. The overlap in concerns\nregarding GenAI and LLMs between each perspective is demonstrated in the Venn\ndiagram. Unknown biases and trustworthiness are concerns shared across all three\nperspectives.\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 2\nwhich can be broad and varied. GenAI can autonomously and rapidly\nproduce large volumes of content and has the capacity to be“imaginative”\nand “disruptive” in its innovation. Via a“black box” mechanism involving\nmultiple layers of neural networks (which remains opaque even to devel-\nopers and difﬁcult for computer scientists to explain), GenAI can create\nsubstantially larger output than the input provided10,26. The concerns are\nslightly different for extractive AIcompared to GenAI. The former, with\nmore utility for diagnostic processes, has a stronger expectation of being\ncorrect, while the latter must be plausible and useful. The strengths and\nweaknesses of GenAI and extractive and algorithmic forms of AI are\ndistinct.\nThis““black box” component of GenAI may be problematic if trying to\nteach clinical reasoning and decision-making skills in medical education,\nwhich means necessarily involving opaque, partial and ambiguous\nsituations\n27. When prompted, GenAI might offer a plausible explanation of\nits decision-making process, but this explanation is not necessarily an\naccurate, or comprehensible representation of how GenAIactuallymade its\ndecisions. It could also be argued that the ways in which humans recognize\nand solve problems and engage in clinical reasoning are unclear. This\nopacity is problematic when considering that the formative process of\nlearning clinical decision-making, including understanding when and why\nerrors occur, has implications for legal liability and accountability in medical\npractice\n14. LLMs can impact critical evaluation and analytical thinking and,\nwhen used inappropriately, could negatively impact students” ability to\ndiscriminate valuable information from inaccurate and irrelevant inputs28.\nJust as the Internet has led to the externalization of factual knowledge, there\nare concerns that LLMs could externalize medical reasoning. The response\nto the readily-available nature of a vast amount of information was to place\ngreater emphasis on debate and discussion and knowledge“management,”\nrather than memorization28. With an improvement in the capacity to assist\nwith clinical reasoning28, LLMs are likely to promote further changes to\neducational methods and the needto reconceptualize assessment.\nIn being unable to account for patient context in its formulation, there\nis a risk that GenAI may reduce complex patient experiences to linear\nproblem-solving interventions, promising “solutionism,” and risking\nobjectifying patients, based on potentially-biased learning of patient\npopulations that are“most studied” or “most prevalent in the literature.\nRecalculating patient illness experiences into solution-based computational\nterms risks ignoring the beneﬁts of dialog and the complex and often\nunpredictable patient experiences\n29. Algorithmic and extractive AI tech-\nnologies may excel at diagnostic components of consultations that are akin\nto data reduction and categorization tasks. By contrast, developing a\ntreatment plan is context-dependent, imbued with uncertainty and more\nnuanced, a scenario more suited to GenAI capabilities. Uncertainty in its\nmany guises cannot be avoided in medical education and clinical practice. In\nsynthesizing large bodies of knowledge, GenAI may obfuscate or overstate\nuncertainty, and learners will need to develop skills to understand not only\ntheir own but also technological reactions\n30. Although GenAI and LLMs, as\nadaptive educational systems, may improve the efﬁciency and interactivity\nof the learning experience, their unknown impact on learners’attention and\nother cognitive and metacognitiveabilities need to be considered31.\nHere we highlight the need to acknowledge both the strengths and\nlimitations of GenAI in medical education. Educators and learners are well-\nadvised to consider the implications of the output of GenAI will be un-\nscaffolded and not peremptorily veriﬁed. This presents a unique facet to the\nconstructivist and experiential approaches that underpin much medical\neducation pedagogy.\nAssessment and GenAI\nDespite the beneﬁts of LLMs being able to synthesize and personalize\ninformation for learners3, there is much consternation with respect to the\nuse of LLMs to subvert current assessment processes32. Using GenAI in this\nmanner (when explicitly disallowed inthe task description) is an academic\ndishonesty. A more vexing concern is whether learners will become overly\nor completely reliant on these technologies and what can be gained or\nsacriﬁced using GenAI as an educator. There is a potential risk of denying\nlearners the formative experiences and important skills such as critical\nthinking necessary in the journey from novice to“expert.”\nGenAI has progressed to the point where LLMs can pass licensure\nexaminations in many undergraduate\n33 and postgraduate specialty training\nprograms34,35. However, ongoing challenges to medical education from\nLLMs include ensuring the accuracy and contemporaneousness of infor-\nmation, reducing bias\n36, ensuring accountability28, minimizing learner over-\nreliance, preventing patient privacy exposure, safeguarding data security,\nenhancing the cultivation of empathy, and maintaining academic integrity37.\nWith existing and potential changes for learners and educators, it remains\nimportant to consider what place GenAI has within broader educational\naims and pedagogy in the context of itspotential, limitations, and bound-\naries. We need to consider what makes educators and learners unique and\nwhether GenAI can support or supplant this in working towards the goal of\ncreating competent and empathetic doctors. Empirical studies evaluating\nthe use of GenAI and LLMs in medical education and their efﬁcacy in\ndeveloping competencies in health professional training are scarce. These\nstudies have focused on the use of GenAI for learning support\n38 or auto-\nmated assessments of clinical skills, but there has been limited use of theory\nor conceptual frameworks\n39.\nCurriculum and assessment redesign encompassing future-focused\ncompetencies recognizesthat new skills will be required for novel models of\ncare. Learners should be proﬁcient in understanding the origins and\ndevelopment of technologies that they will be using in their clinical work, in\nresearch, and in continuing learning and professional development. New\nareas of technical competence will be essential for learners to work in AI-\nintegrated healthcare environmentsto deliver patient care, communicate\nwith other health professionals, and effectively manage large amounts of\npopulation-wide data that will become increasingly available\n40.I ti s\nimpossible to address the potential impacts of GenAI use in medical edu-\ncation without acknowledging the intersection with clinician training and\nclinical care.\nEmerging roles of GenAI in medical education\nFor learners\nArtiﬁcial intelligence is likely to impact medical education methods by\nproducing intelligent and personalized systems to identify and respond to\ngaps in students’ knowledge, adaptable virtual facilitators in constructivist\nlearning approaches, mining data, and providing intelligent feedback to\nlearners3,41. GenAI not only delivers content but also enables adaptive\nlearning, provides information and feedback, creates individualized learning\npathways, supports competency-based assessment, and potentially provides\nand manages programmatic assessment data42. For learners, this indivi-\ndualized learning can be customizable in depth, tone, and style of output,\nmaking it an ideal personalized teaching assistant\n28.\nStudents can beneﬁt from improved practical skills43, robust selection\nprocesses and research assistance44. Recent research on the medical student\nperspective suggests that GenAI is good atfacilitating differential diagnosis\nbrainstorming, providing interactive practice cases, and aiding in multiple-\nchoice question review25. LLMs can be used to create interactive and\nengaging simulations. For example, students may use LLMs to have con-\nversations with simulated patients, allowing them to practice taking patient\nhistories or assessing diagnoses and discussing treatment plans\n28.\nFor educators\nFrom an educator’s perspective, LLMs may help shape medical curriculum\ndevelopment and engender changes in teaching methodologies41. It has been\ndemonstrated that GenAI with expert human guidance can also produce\nassessment items for medical examinations\n45. Human-developed questions\nstill retain a higher discriminatory power46.T h i si sp o t e n t i a l l yd u et oh u m a n\nassessors being more adept at generating items with higher construct\nvalidity and also being more closely aligned with a priori knowledge such as\nlecture material. GenAI may help reduce the administrative burden on\neducators\n42, with help in assessment and attendance tracking3.A n a l o g o u st o\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 3\n“precision medicine,” educators can foster“precision education” by lever-\naging data to individualize training and assessment. Data can inform the\nstrategic deployment of educational resources and strengthen the link\nbetween practice and education, and educators can advocate the develop-\nment of appropriate tools\n42.\nTraditional assessment methodologies are increasingly at risk of\nobsolescence12, necessitating a paradigm shift towards assessment mod-\nalities that are more resistant to unapproved GenAI assistance, such as\ncontinuous in-person assessment of practical or clinical skills, and oral\nexaminations47. A contrasting and perhaps more realistic view accepts that\nstudents are more likely to use GenAI and will be working in healthcare\nenvironments that have been transformed by the integration of GenAI.\nAssessment may need to be better at evaluating whether students can use\nGenAI with a complete understanding of its strengths and limitations,\ndemonstrating its effective and safe use in their own learning and in patient\ncare. The reliance on traditional written tasks, which are susceptible to\ncompletion by GenAI without genuine student engagement or learning,\nunderscores the urgency for educators to redesign assessments\n41.E d u c a t o r s\nmay need to rethink and re-deﬁne “authenticity” and “originality” in\nassessment that incorporates the use of GenAI48,49. Competency frameworks\nneed to be reconsidered and updated to consider 21st century realities. The\nabilities that students and future clinicians require to adequately meet\npatients’healthcare needs will be impacted by AI-enabled systems\n50.T h e r e\nremains a need to improve the digital literacy of future physicians while\nincorporating patients’ views with the increasing use of GenAI\ntechnologies\n50. This highlights the need for new assessment strategies which\npermit authenticity of the learner’s voice, discourage over-reliance on\nGenAI for completion and which prepare the future workforce for work-\nplaces where they will need to navigateGenAI and technology competently.\nImpacts on the educator– learner relationship\nLearners have expressed a lack of conﬁdence in being able to inform others\nabout the features and risks of GenAI applications due to a lack of formal\ninstruction about the use of such programs\n51. Unsurprisingly, there is a\ndemand for structured GenAI training, particularly in terms of reducing\nmedical errors and ethical issues\n51. One apparent deﬁciency of GenAI and\nLLMs identiﬁed by medical students was the reduction in the humanistic\naspect of medicine. The nature of learning is likely to evolve with the\nintroduction of GenAI in education, as well as the roles of educators, what is\ndemanded of them, and the relationships they have with students. It may\nwell be that there is greater emphasis on reinforcing human skills, com-\nmunication, empathy, professionalism, and contextualizing and indivi-\ndualizing treatment strategies for patients. The opportunities, challenges,\nand considerations for GenAI are summarized in Table1.\nTrustworthiness and the intersection of GenAI use in\nmedical education and clinical practice\nUnderlying some concerns about GenAI is perhaps the belief that seeking\nautonomous input from GenAI will necessarily result in nefarious\noutcomes52. In healthcare settings, one factor delaying the translation of\nGenAI and its potential beneﬁts to patient care and education is whether\nlearners, educators, clinicians, and patients would trust it. The patient’sv o i c e\nregarding their needs and expectations is not always fully considered in the\napplication of GenAI in healthcare\n53. Studies of patient perceptions about\nthe use of GenAI in healthcare have concluded that most are comfortable\nwith its involvement but would prefer theﬁnal plans and management to be\napproved and delivered by humans. Trusting the decision-making capacity\nof the clinician is based on the pre-existing trust that patients have with their\nphysicians\n54. The mistrust of GenAI is perhaps secondary to its inability to\nexplain its rationale55 and decisions, that are not fully transparent42.T h i si s\ncomplicated by the understanding that GenAI technology has inherent\nbiases that join human biases in shaping the diagnostic process, in a\npotentially non-neural manner56. Where GenAI and LLMs perform with\nsome degree of autonomy, from an ethical perspective, this gives them a\nmoral agency that needs to be accounted for\n57. Blind acceptance of AI\ndecisions is another potential source of mistrust, where GenAI output\nreplaces, rather than augments, human decision making. A greater under-\nstanding of the“permissible” ways in which GenAI could augment human\nprocesses may help learners, users,and patients to ensure that the use of\nGenAI remains responsible.\nWhen decisions are subjective or the variables change, human judg-\nment is trusted more because of people’s capacity for empathy. Even when\nTable 1 | Key opportunities, challenges, and pedagogical considerations of GenAI and large language models\nLearners\nPositive Impacts Challenges\n Personalized learning systems that provide intelligence feedback\nand virtual facilitation in constructivist learning approaches\n Creation of interactive and engaging learning simulations and\ninteractive practice cases\n Brainstorming of differential diagnoses\n Application of AI in MCQ review and exam preparation\n Need to ensure accuracy and contemporaneousness of information, reduce bias, ensure\naccountability, minimize learner over-reliance, retain empathy\n The “black box” output mechanics of neural networks is difﬁcult to explain and so the basis of\nany decision-making process is opaque and will impact critical reasoning, thinking and\ndecision-making skills\nEducators\nPositive Impacts Challenges\n Reduced administrative burden with help in assessment and\nattendance tracking\n Ability to manage large volumes of programmatic assessment data\n Use of GenAI to produce assessment items for examinations\n GenAI can pass licensure examinations and could potentially subvert assessment processes\nand contribute to academic dishonesty\n In using GenAI, there is a need to safeguard data security and patient privacy, and help maintain\nacademic integrity\n Teaching and assessment will need to change to keep assessment original and authentic\n Future assessment may need to be developed to evaluate whether students can safely and\nknowledgeably use GenAI\nPedagogical Considerations\n It is unclear if GenAI can provide the social context and experiences often thought to be required to acquire knowledge and develop understanding\n Efﬁciency could contribute to expertise by providing augmented capacity to access information.\n GenAI at this stage cannot teach the integration of contextual and external information, comprehend sensory and nonverbal cues, cultivate rapport and interpersonal\ninteraction, and align with overarching medical education and patient care goals\n Algorithmic approaches in AI risks objectifying patients and experiences to simple problem-solving approaches, when this is rarely straightforward in healthcare\n It is unclear if GenAI can accurately describe its approach to uncertainty\n Curriculum redesign is required to capture future-focussed competencies including working with GenAI\n Formal instruction regarding GenAI in medical education is likely to be necessary to consider the operation, features, limitations, and ethical issues with emerging\ntechnology\n The role of educators in learning is likely to change and may involve reinforcing uniquely human skills\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 4\nGenAI systems outperform human doctors, trust in GenAI does not\nincrease58. This mistrust is even greater where factors affecting diagnoses\nmay be behavioral, and case-speciﬁc, such as mental health59. Another driver\nof consumer resistance to medical AI is the phenomenon of“uniqueness\nneglect.” This indicates that GenAI systems are less able than human pro-\nviders to account for consumers’unique characteristics and circumstances,\nand drive consumer resistance to medical AI60. The human ability to\ncombine contextual awareness withknowledge leads to the perception of\nsuperiority in planning, managing,and achieving favorable results59.T ot h i s\nend, GenAI and LLMs should retain an assistive role in clinical encounters,\nand medical education needs to adaptto ensure that future doctors are\nprepared for an GenAI-assisted work environment to preserve doctor-\npatient relationships\n61. GenAI and LLMs use quantiﬁable datasets, and there\nis a risk that patients themselves are reduced to data points, neglecting their\nexperiences and individual context. Patients and healthcare professionals\nshould consider this and encourage patient empowerment by expressing\ntheir individual circumstances\n62. Medical training should prepare doctors to\noperate dynamically in being able to adapt to a range of technologically-\nenabled, or not, environments. Therewill be patients who may lack digital\nliteracy with whom the nature of interaction would differ to those who\noperate in more AI-enabled environments and who may have consulted\nindependently with GenAI technologies to understand their medical con-\nditions. The digitally-literate physician will be able to navigate and address\nthe diverse needs of all patient groups.\nA further consequence of using GenAI as an adjunct to a healthcare\nprovider’s work, is that administrative tasks may be made less onerous for\nclinicians, reducing burnout, and allowing for greater time and connection\nwith the humanistic side of medicine. The counterargument is that there\nexists a potentially increased burden with higher throughput of patient\nconsultations and increased cognitiveload with monitoring and correcting\nthe output of GenAI processes. Democratization of patient care could also\nbe achieved by providing patients with access to their information in a\ntimely and comprehensible fashion\n63. Considering the doctor-patient-AI\n“triad” relationship represents a paradigm shift and calls for further research\nto better understand how GenAI inﬂuences the doctor-patient relationship\nand respective autonomies, to ensure that ethical practice remains present64.\nTechnological mediation may inhibitthe development of trust in a doctor-\npatient relationship, and this too, would beneﬁt from further research and\nunderstanding. As a mediator placed between the doctor and patient, GenAI\nsystems can inhibit tacit understanding of the patient’s health and well-\nbeing and encourage both clinicians andpatients to discuss health solely in\nmeasurable quantities or machine interpretable terms65.\nOften, models are developed without input from the people who will\nultimately use them, namely students, practitioners, and patients. GenAI\nmodels also have no intrinsic ability to use context or meaning to inform\noutput and decisions, which is problematic because context critically\ndetermines the quality of outcomes for patients\n52. This contextual awareness\nis likely to improve with newer generations of GenAI, but it will be critical\nfor any underlying bias within the material that the GenAI has“learned”\nfrom to be either eliminated or mitigated, as this will inform the technology’s\ncapacity to make inferences about the patient context. A codesigned\napproach that considers which tasks are more efﬁcient with GenAI ele-\nments, which should be learner andstudent-led, is likely to be more\nproductive66.\nThe U.S. Department of Health and Human Services has identiﬁed six\nprinciples of trustworthy AI67, including LLMs being robust and reliable, fair\nand impartial, transparent and explainable, responsible and accountable,\nsafe and secure. and ensuring privacy and consent. However, it is unclear\nwhat would make GenAI trustworthy in clinical practice and without a clear\nunderstanding, the development of effective implementation strategies will\nbe impaired in the healthcare setting\n68 GenAI and LLMs are likely to con-\ntinue to develop in ways that beneﬁtp a r t i c u l a rg r o u ps (especially com-\nmercial), but without a high level of trustworthiness, they are unlikely to be\nacceptable to all aspects of health professions. Evaluating and ensuring the\npresence of these underlying“foundations” of the trustworthiness of GenAI\ntechnologies by health professionals, possibly as part of the responsibility for\nself-regulation, may be required to shape GenAI development in equitable\nand acceptable ways. These are considerations that those involved in\nmedical education. particularly learners and educators, need to heed and\ndevelop personal approaches to. Further research including all stakeholders,\nespecially patients, learners, and educators, into the foundations of trust-\nworthiness and how these features in future AI-enabled workspaces will be\ncritical.\nIs regulation necessary?\nGiven the speed and unpredictability ofinnovation, quantum of investment,\nand lack of technical information, it is almost impossible to forecast the\nopportunities and risks of GenAI accurately. LLMs raise questions about the\nopportunities and risks of widespread adoption; scope and adequacy of\nnational strategic planning and policies;ﬁtness of legal and regulatory\napproaches, and implications of increasing geopolitical competition and\ngeo-speciﬁcr e g u l a t i o n s\n8. Regulation needs to be deﬁned within the context\nof this review.\n(i) With regard to the medical device functionality of GenAI in clinical\nwork, a legal deﬁnition of regulation is appropriate where it represents\nrules, or directives, designed to control and govern conduct. Oversight\nwould be the domain of government departments responsible for the\nimplementation and use of therapeutic goods and devices.\n(ii) Regarding medical education, regulation refers to accreditation and\nvalidation, that is, formal processes to ensure that standards for quality\nand competency are met. Oversight would be local and context-\ndependent.\nThere are several challenges associated with attempts to regulate\ntechnology. The perceived risks of harm are tempered by social norms,\nmarket pressure, and the coding architecture (design, structure, and orga-\nnization of the codebase). Adapting formal regulation may be one element\nfor ensuring safe and ethical GenAI use. A stepped approach to GenAI\nregulation recognizes that a new technology does not necessarily imply the\nneed for new rules. Where there are risks from the use of GenAI that warrant\nsome form of regulation, identifyingwhich component or process requires\nregulation will be important and the codesign of any framework with all\nstakeholders will be critical\n69. Existing legal frameworks may address and\nmitigate some risks of patient-facing GenAI use69 however speciﬁc contexts\nfor GenAI and LLM use will require may require speciﬁcr e g u l a t o r y\nattention.\nRegulation and GenAI use in medical education\nPreserving academic integrity\nDetecting the misuse of LLMs for plagiarism where there is no augmenta-\ntion of learner abilities\n44 remains challenging given the lack of transparency,\nfrom both GenAI programs and the algorithms used by detection tools. The\nability of GenAI to pass high-stakes examinations\n70 highlights an issue with\nreliance on“single-shot” examinations and their inherent difﬁculties with\ngeneralizability71, being limited assessments of knowledge. Programmatic\nassessment72, which focuses on a wide variety of assessment tasks, including\nworkplace-based assessments, is potentially more resistant to the unau-\nthorized use of GenAI. GenAI can help organize the wealth of performance\nevidence that accompanies programmatic assessment, visualizing and\ninterpreting it in a manner that informs future learning and identifying\nsignals in performance evidence thatwould steer additional diagnostic\nassessments or learning experiences\n42. There will be an onus placed on\neducators to rethink how the utility of GenAI can be maximized73 while\nmitigating concerns about its potential misuse.\nGenAI, students, and clinicians are likely to have an interdependent\nrelationship. Bearman and Ajjawi provided a framework to work with, and\nnot fear,“black boxes”27. Orienting students to quality standards and pro-\nviding meaningful interactions with GenAI systems would (i) permit an\nunderstanding of the social regulating boundaries around GenAI (ii) pro-\nmote learner interactions with GenAI while building evaluative judgment in\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 5\nweighing GenAI’s contribution and (iii) encouraging understanding of the\nevaluative, ethical, and practical necessities of working with“black boxes”27.\nJust as learners will use GenAI to an increasing degree, it will continue to rely\non high-quality input from users\n74, including students and clinicians. Initial\ntraining with GenAI is unlikely to be sufﬁcient as a standalone endeavor and\nadditional training is likely to be required as theﬁeld evolves. By establishing\nframeworks for its adoption and education early, this process becomes more\nfeasible in the future.\nAs with any source material, understanding the nature of veracity and\napplicability of information to their own learning and eventually patient care\nneeds to be emphasized. Learners should continuously critique and question\nGenAI-generated outputs in biomedical knowledge and the pathophysiol-\nogy of the disease\n13,75. This would prevent GenAI-generated information\nfrom acting as an automated crutch for clinical decision-making76,w h i c h\ncould hamper the development of clinical reasoning abilities77.\nSpeciﬁc concerns regarding GenAI use in medical education include\nalgorithmic bias, overreliance, plagiarism, misinformation, inequity, priv-\nacy, and copyright concerns\n9,41. Many practical guidelines regarding the\nregulation of GenAI agree on factors that require regulatory oversight\nincluding: transparency, bias, content validity, data protection, excessive\n(and non-consensual) data collection,data ownership, informed consent,\nensuring that users remain empowered and establishing accountability9,78.\nWhere information is drawn from by GenAI programs is also of importance\nwith respect to intellectual property and copyright protection.\nRegulation and GenAI use in clinical practice\nIf GenAI and LLMs are used in clinical settings, there is ambiguity regarding\nthe responsibility for medical diagnoses, whether it is a GenAI or a\nhealthcare professional. Calls have been made for users of GenAI to be\nguided by ethical principles, which practically and legally may involve\nreforming the categories of medical malpractice, vicarious liability, and\nproduct liability, as well as the ancillary duties healthcare providers\n62.M a n y\nof these recommendations fall under the rubric of“soft law,” presenting self-\nregulating obligations and codes of conduct that are not legally enforceable\nbut are considered“good practice.” With the introduction of GenAI sys-\ntems, there is potentially an argument for some aspects, such as the duty to\nwarn of limitations and obtain informed consent, to be reallocated to“hard\nlaw,” becoming legal obligations related to disclosure of information.\nAlthough regulations regarding therapeutic goods and devices focus\nmostly on patient safety, they do not necessarily guarantee it\n79.N o ta l lA I\ntools with regulatory authorization are not necessarily clinically validated80\nand if GenAI is implemented poorly, it may add to doctors’ burden of\nresponsibility and potentially exposed o c t o r st ot h er i s k so fp o o rd e c i s i o n -\nmaking. Alternatively, GenAI implemented with a responsible design,\ninformed by cognitive science, would allow doctors to ofﬂoad many of their\ncognitive tasks to GenAI when appropriate and focus their attention on\npatients\n52. Responsible GenAI requires the development of legal frameworks\nto protect patients and consumers frompotential harm arising from poorly\ndeveloped GenAI and inappropriate deployment in socio-technical systems.\nMost importantly, patients and consumers have the right to be informed\nabout the limitations of GenAI to allow them to decide which aspects of their\nlives could beneﬁtf r o mi t52 and the choice to opt-out of systems employ-\ning GenAI.\nAs previously discussed, the use of GenAI during medical training may\nresult in inadequate development of critical thinking and clinical reasoning\nskills, which may threaten patient safety several years later, as the learner\nstarts to take on greater responsibility for patient care. On the other hand,\ntraining of the future generation without adequate recognition of the role of\nGenAI in their future practice and the new competencies that are required is\nlikely to result in graduates who are underprepared for their clinical roles\nwhich may ultimately adopt such technologies. Clinicians are likely going to\nneed to understand and keep pace with patient use of GenAI as well.\nGenAI currently operates in a regulatory framework that is patchwork,\nat best. One call for legislation is based on human-rights, with concerns for\nemerging harms from GenAI centered on privacy, algorithmic\ndiscrimination, automation bias, misinformation, and disinformation\n81.\nLegislation does exist to regulate GenAI usage in speciﬁcs e t t i n g so rc i r -\ncumstances; however, many gaps still exist.\nRegulation, applied with the intent of supporting safe innovation, may\nalso to some degree, incur human and economic opportunity costs in also\npotentially restricting progress and innovation. This reinforces the overall\nmessage that regulation, in whichever form it is present, needs to be with\npurpose and should assure educators, learners, medical professionals, and\npatients that LLMs can be used without causing harm or compromising data\nor privacy\n82.\nLevels of regulation\nConsideration of the different types of, and levels at which regulation may\napply will inform how individuals, institutions, accrediting bodies, national\ngovernments, and global organizations manage the establishment of the\nacceptable use of GenAI in medical education to ensure safe and ethical\npractice. The ecological framework allows the consideration of regulatory\nprinciples at the micro, meso, and macro levels and has been used to syn-\nthesize and unify existing learning theories to model the roles of artiﬁcial\nintelligence in promoting learning processes\n83. The ecologicalf r a m e w o r k\nnot only identiﬁes increasingly broader levels of inﬂuence but also considers\nthe relationships across different levels. Any regulatory effort is unlikely to\nsucceed without all levels interacting to some degree. At this nascent stage\nhowever, the most readily-applicable action is likely to be at the micro level,\nthat of the individual learner and the educator. This framework is sum-\nmarized in Fig.2.\nMicro: individual learners and educators\nRegulation at the micro level, including individual learners and educators,\nwould predominantly involve degrees of self-regulation. Regulatory\nresponsibility for the use of GenAI in medical education will likely need to\nfocus on developing robust strategies to counter or address opacity and\ninexplicability, data privacy and security, fairness and bias, reliability\n84,\nprotection of intellectual property, assurance of quality control and stan-\ndardization, informed consent, data ownership, over-reliance on GenAI\nmodels, and continuous monitoring and validation\n82. Educators and lear-\nners should be encouraged to develop personal and morally-informed\nstrategies, akin to a personal code ofconduct, to manage these issues and be\nready to state how these have been addressed, or not, when using GenAI.\nIdeally, learners will be empowered to increase their knowledge and skills to\nuse a range of emerging digital health systems, analyze the data emanating\nfrom them, and evaluate information for trustworthiness and relevance\n47.\nThis would not only ensure that students are adept at leveraging GenAI in\ntheir future careers but also emphasizethe importance of critical thinking\nand maintaining integrity and professional standards in their work\n47,d e s p i t e\nthe convenience of readily-generated information14.\nMeso: institutions and accrediting bodies\nAt this level, there is the intersection of regulatory processes governing\ntherapeutic goods and devices as well aseducational accreditation. Profes-\nsional health education curricula will need to evolve to include compre-\nhensive teaching on the ethical and appropriate use of GenAI\n9 and critical\nappraisal of information created with it. Institution-level approaches to\nGenAI may be retroﬁtted to existing national guidelines85. Similarly, insti-\ntutional policies may be developedbased on guidelines that have been\ndeveloped. It would be the responsibility of tertiary educational institutions\nand professional colleges overseeing pre-vocational and postgraduate\nvocational training to develop frameworks appropriate to their accreditation\nand validation processes. Individual professional organizations such as the\nRoyal Australian College of General Practitioners, have also developed\nevolving position statements to guide clinicians\n86. The latter position\nstatement outlines various concerns and issues and makes legally non-\nbinding recommendations but calls on general practitioners to be cognizant\nof technological advances and their ethical and clinical implications, calling\nfor individual responsibility with GenAI use. These reminders should be\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 6\nreinforced at the medical student and learner levels to encourage forward\nthinking about the ethical challenges that GenAI systems will pose. Enabling\nthis self-reﬂection would emphasize on faculty development and educator’s\nupskilling to safely and productively engage with GenAI87.\nThe Australian Health Practitioner Regulation Agency, responsible\nfor clinician accreditation in Australia, reminded practitioners to consider\ntheir professional obligations when using GenAI in practice, particularly\nwith respect to accountability, understanding, transparency, and\ninformed consent\n88. Some recommendations have called for self-\nregulation at an industry level, with a codesign process between stake-\nholders, developers and users encouraging transparency and potentially\nincreasing public trust\n89,90.\nMacro: national and international organizations\nMost LLMs have been released globally. Ideally, a global approach from\nregulators is required; however, proactive regulation is impossible with the\nproverbial cat being already out of the bag. Broader regulation at the national\nand international macro level is challenging and likely lags signiﬁcantly\nbehind GenAI research and development. Theﬁrst international conven-\ntion was recently signed by the Council of Europe\n91.The Bletchley\nDeclaration, signed by 28 countries and the European Union, at an AI Safety\nSummit, establishes a shared understanding of the opportunities and risks\nposed by frontier artiﬁcial intelligence. The aim of this declaration was to\npromote increased transparency by private actors developing frontier AI\ncapabilities, appropriate evaluation metrics, tools for safety testing, and\ndeveloping relevant public sector capability and scientiﬁc research while\nacknowledging that approaches would“differ” with respect to applicable\nlegal frameworks\n92. A similar declaration from the United Nations93 has\ncited concerns about human rights infringements and inequity with GenAI\ntechnology, but apart from development of an independent international\nscientiﬁc panel, intergovernmental and multi-stakeholder policy dialog\ntacitly acknowledges the difﬁculties in enforcement. Some regulatory\napproaches include risk-based approaches, where compliance obligations\nare proportionate to the level of risk, medical or otherwise, posed by the use\nof GenAI technology. These include sector-agnostic and sector-speciﬁc\nrules and regulations, depending on a particular sector’su s eo fG e n A I ;a n d\npolicy alignment, incorporating GenAI-related rule making within existing\nframeworks for cybersecurity, data privacy, and intellectual property\nprotection.\nNational governments have recognized that there is low public trust in\nGenAI systems which can in turn slowadoption and public acceptance. The\nrisk-based approach seeks, through greater testing, transparency, and\noversight, to pre-emptively mitigate potential negative impacts from GenAI\nand LLMs that could be difﬁcult or impossible to reverse\n94. GenAI systems\nare being developed and deployed at a speed and scale that will outpace the\ncapacity of the legislative frameworks. A map of potential GenAI risks may\nneed to be developed to be answered by future GenAI regulations to ensure\nthat it can account for and handle new risks, potential. and actual alike\n95.\nGovernment-level organizations are calling on those developing and\ndeploying GenAI in high-risk contexts to take their own proactive steps to\nensure user and consumer safety94.\nOther national approaches have included mooting the legal protection\nof human rights in the USA96, national AI strategies in the UK97,H o n g\nKong98, white papers in Japan99 and voluntary standards in Australia100.\nLegal regulatory approaches for therapeutic devices are required to\naccount for the unique differences in the development and distribution of\nLLMs compared to other existing medical technologies. To safeguard\npatient care, Mesko and Topol suggested that a regulatory body only has to\ndesign regulations for LLMs if either the developers of LLMs claim that their\nLLM can be used for medical purposes, or if LLMs are developed for,\nadapted, modiﬁed, or directed toward speciﬁc medical purposes\n82.S u c h\nadaptation or use of LLMs for medical purposes may not always be explicitly\nstated, or even intended, by developers. Even if the currently widespread\nLLMs do not fall into either category, further iterations of the medical\nalternatives of LLMs speciﬁcally trained on medical data and databases will\nprobably occur. A participatory approach to AI governance, informed by\nthe micro and end-user levels will be more effective than overarching top-\ndown regulations.\nThere is little by way of international oversight governing the use of\nGenAI in medical education. An initial advancement of the meso-level\napproaches would see institutions and accrediting bodies collaborating and\nadopting shared strategies at a national level. It remains to be seen if global\ngovernance would be necessary, or even feasible, in medical education.\nUnderpinning regulatory concerns is an understandable focus on\npatient safety, privacy, transparency, and ongoing trust in the healthcare\nprofession. However, this safety cannot be guaranteed if learners, the future\nworkforce, are deﬁcient in clinical reasoning and critical thinking skills\nbecause of, or when operating within, GenAI-integrated environments.\nFig. 2 |A depiction of regulatory levels for GenAI and LLMs within the ecological\nframework. The micro-level consists of learners and educators, the meso-level of\ninstitutions, industry bodies and accrediting organizations and the macro-level of\nnational and international organizations. There is intercalation of the regulatory\nconcerns, strategies and frameworks across these different levels.\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 7\nAccountability lies with the end-user of any GenAI technology, as it would\nwith any therapeutic good or device, and navigating the challenges that\nGenAI represent is an important learning skill. A broader view of“regula-\ntion” with participation from all stakeholders, will help ensure that\naccrediting bodies, education providers and students will understand and\nconsider how GenAI and LLMs affect learning, development of knowledge\nand skills and attainment of competency in practice.\nConclusion\nThe intersection of the role of GenAI in medical education and clinical use\nhinges on issues of governance and regulation. Currently, GenAI is unlikely\nto become fully autonomous and unreservedly accepted by the wider medical\ncommunity and by patients due to issuesof trustworthiness and the as-yet\nunknown impacts on the doctor-patient relationship, despite promised gains\nin efﬁciency and personalization of outcomes. Its use and inputs need to be\nconstantly moderated and updated by humas to ensure the veracity and\nutility of its output, retainits generative capacity andprevent model collapse.\nT h ei m p l i c a t i o n so ft h i si nm e d i c a le d u c a t i o ns h o u l db ec o n s i d e r e di nt h e\ncontext of the learning process, authentic assessment, and the preservation of\nacademic integrity. Regulation, in its different guises, applied thoughtfully at\ndifferent levels, will guide users towards safe, appropriate, and equitable use\nof these technologies. In place of blind acceptance, a balanced and considered\ncollaboration between humans, GenAI, and governance will permit\nadvancements in learning possibilities and efﬁciencies without over-\nregulation stiﬂing innovation and progress.\nData availability\nNo datasets were generated or analyzed during the current study.\nReceived: 27 September 2024; Accepted: 14 May 2025;\nReferences\n1. IBM. What is Generative AI? [cited 2024 10 Sep 2024]; Available\nfrom: https://research.ibm.com/blog/what-is-generative-AI (2024).\n2. IBM. What are Large Language Models (LLMs)? [cited 2024 10 Sep\n2024]; Available from:https://www.ibm.com/topics/large-\nlanguage-models (2024).\n3. Masters, K. Arti ﬁcial intelligence in medical education.Med. Teach.\n41, 976–980 (2019).\n4. Hanycz, S. A. & Antiperovitch, P. A practical review of generative AI in\ncardiac electrophysiology medical education.J. Electrocardiol.90,\n153903 (2025).\n5. Lu, M.Y. et al. A Multimodal generative AI Copilot for human\npathology. Nature (2024).\n6. Wang, X. et al. Foundation model for predicting prognosis and\nadjuvant therapy beneﬁt from digital pathology in GI cancers.J. Clin.\nOncol. p. JCO2401501 (2025).\n7. Eynon, R. & Young, E. Methodology, legend, and rhetoric: the\nconstructions of AI by academia, industry, and policy groups for\nlifelong learning.Sci., Technol., Hum. Values46, 166–191 (2020).\n8. Bell, G., Burgess, J., Thomas, J. & Sadiq, S. Rapid Response\nInformation Report: Generative AI— Language Models (LLMs) and\nMultimodal Foundation Models (MFMs), (Australian Council of\nLearned Academies, 2023).\n9. Masters, K. Ethical use of artiﬁcial intelligence in health professions\neducation: AMEE Guide No. 158.Med Teach.45, 574–584 (2023).\n10. Tolsgaard, M. G. et al. The fundamentals of Artiﬁcial Intelligence in\nmedical education research: AMEE Guide No. 156.Med. Teach.45,\n565–573 (2023).\n11. Reznick, R. Harris, K., Horsley, T. & Mohsen, H. Task force report on\nartiﬁcial intelligence and emerging digital technologies, Royal\nCollege of Physicians and Surgeons of Canada (2020).\n12. Gordon, M. et al. A scoping review of artiﬁcial intelligence in medical\neducation: BEME Guide No. 84.Med Teach.46, 446–470 (2024).\n13. Lee, J., Wu, A. S., Li, D. & Kulasegaram, K. M. Artiﬁcial intelligence in\nundergraduate medical education: a scoping review.Acad. Med.96,\nS62–S70 (2021).\n14. Alam, F., Lim, M. A. & Zulkipli, I. N. Integrating AI in medical\neducation: embracing ethical usage and critical understanding.\nFront. Med.10, 1279707 (2023).\n15. AAIN Generative AI Working Group,AIN Generative Artiﬁcial\nIntelligence Guidelines(Austraian Academic Integrity Network, 2023).\n16. Valiant, L.\nThe Importance of Being Educable: A New Theory of\nHuman Uniqueness(Princeton University Press, 2024).\n17. Foltynek, T. et al. ENAI recommendations on the ethical use of\nartiﬁcial intelligence in education.Int. J. Educ. Integr.19 (2023).\n18. Preiksaitis, C. & Rose, C. Opportunities, challenges, and future\ndirections of generative artiﬁcial intelligence in medical education:\nscoping review.JMIR Med Educ.9, e48785 (2023).\n19. Dong, H., Lio, J., Sherer, R. & Jiang, I. Some learning theories for\nmedical educators.Med. Sci. Educ.31, 1157–1172 (2021).\n20. Kolb, D. A. Experiential Learning: Experience as the Source of\nLearning and Development(Prentice-Hall, 1984).\n21. Ericsson, K. A. & Staszewski, J. J. skilled memory and expertise:\nmechanisms of exceptional performance. In: Klahr, D., Kotovsky, K.\nEditors Complex Information Processing, 235–267 (Lawrence\nErlbaum Associates, 1989).\n22. Wang, S. et al. Artiﬁcial intelligence in education: a systematic\nliterature review.Expert Syst. Appl.252 (2024).\n23. Macnamara, B. N. et al. Does using artiﬁcial intelligence assistance\naccelerate skill decay and hinder skill development without\nperformers’awareness? Cogn. Res. Princ. Implic.9, 46 (2024).\n24. Tran, M., Balasooriya, C. & Semmler, C., Rhee, J. Generative artiﬁcial\nintelligence: the‘more knowledgeable other’in a social\nconstructivist framework of medical education. npj DIgit. Med.\nUnder Review (2025).\n25. Safranek, C. W., Sidamon-Eristoff, A. E., Gilson, A. & Chartash, D.\nThe role of large language models in medical education: applications\nand implications.JMIR Med. Educ.9, e50945 (2023).\n26. Linardatos, P., Papasteranopoulos, V.& Kotsiantis, S.Explainable AI: a\nreview of machine learning interpretability methods.Entropy23(2020).\n27. Bearman, M. & Ajjawi, R. Learning to work with the black box:\nPedagogy for a world with artiﬁcial intelligence.Br. J. Educ. Technol.\n54, 1160–1173 (2023).\n28. Clusmann, J. et al. The future landscape of large language models in\nmedicine. Commun. Med.3, 141 (2023).\n29. van der Niet, A. G. & Bleakley, A. Where medical education meets\nartiﬁcial intelligence:‘Does technology care?Med Educ.55,3 0–36\n(2021).\n30. Reddy, S. Generative AI in healthcare: an implementation science\ninformed translational path on application, integration and\ngovernance. Implement Sci.\n19, 27 (2024).\n31. Mykhailov, D. Philosophical dimension of today’s educational\ntechnologies: framing ethical landscape of the smart education\ndomain. NaUKMA Res. Pap. Philo. Relig. Stud. 68–75 (2023).\n32. Moritz, S., Romeike, B., Stosch, C. & Tolks, D. Generative AI (gAI) in\nmedical education: Chat-GPT and co.GMS J. Med. Educ.40, Doc54\n(2023).\n33. Brin, D. et al. Comparing ChatGPT and GPT-4 performance in\nUSMLE soft skill assessments.Sci. Rep.13, 16492 (2023).\n34. Alfertshofer, M. et al. Sailing the seven seas: a multinational\ncomparison of ChatGPT’s performance on medical licensing\nexaminations. Ann. Biomed. Eng.52, 1542–1545 (2024).\n35. Lucas, H. C., Upperman, J. S. & Robinson, J. R. A systematic review\nof large language models and their implications in medical\neducation. Med. Educ.(2024).\n36. Zack, T. et al. Assessing the potential of GPT-4 to perpetuate racial\nand gender biases in health care: a model evaluation study.Lancet\nDigit. Health6, e12–e22 (2024).\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 8\n37. Li, Z. et al. Large language models and medical education: a\nparadigm shift in educator roles.Smart Learning Environ. 11(2024).\n38. Chan, K. S. & Zary, N. Applications and challenges of implementing\nartiﬁcial intelligence in medical education: integrative review.JMIR\nMed. Educ.5, e13930 (2019).\n39. Tolsgaard, M. G., Boscardin, C. K., Park, Y. S., Cuddy, M. M. &\nSebok-Syer, S. S. The role of data science and machine learning in\nHealth Professions Education: practical applications, theoretical\ncontributions, and epistemic beliefs.Adv. Health Sci. Educ. Theory\nPract. 25, 1057–1086 (2020).\n40. Balasooriya, C. et al. Learning, teaching and assessment in health\nprofessional education and scholarship in the next 50 years.FoHPE\n25 (2024).\n41. Abd-Alrazaq, A. et al. Large language models in medical education:\nopportunities, challenges, and future directions.JMIR Med. Educ.9,\ne48291 (2023).\n42. Lomis, K. et al. Artiﬁcial Intelligence for Health Professions\nEducators. NAM Perspect.2021 (2021).\n43. Nagi, F. et al. Applications of artiﬁcial intelligence (AI) in medical\neducation: a scoping review.Stud. Health Technol. Inf.305, 648–651\n(2023).\n44. Boscardin, C. K., Gin, B., Golde, P. B. & Hauer, K. E. ChatGPT and\nGenerative artiﬁcial intelligence for medical education: potential\nimpact and opportunity.Acad. Med99,2 2–27 (2024).\n45. Artsi, Y. et al. Large language models for generating medical\nexaminations: systematic review.BMC Med. Educ.24, 354 (2024).\n46. Laupichler, M. C., Rother, J. F., Grunwald Kadow, I. C., Ahmadi, S. &\nRaupach, T. Large language models in medical education:\ncomparing ChatGPT- to human-generated exam questions.Acad.\nMed 99, 508–512 (2024).\n47. Scott, K. & Hart, J. Digital technologies in health: implications for\nhealth professional education.FoHPE. 25 (2024).\n48. Pearce, J. & Chiavaroli, N. Rethinking assessment in response to\ngenerative artiﬁcial intelligence.Med, Educ.57, 889–891 (2023).\n49. Fawns, T. & Schuwirth, L. Rethinking the value proposition of\nassessment at a time of rapid development in generative artiﬁcial\nintelligence. Med. Educ.58,1 4–16 (2024).\n50. Rampton, V., Mittelman, M. & Goldhahn, J. Implications of artiﬁcial\nintelligence for medical education.\nLancet Digit. Health2, e111–e112\n(2020).\n51. Jackson, P. et al. Artiﬁcial intelligence in medical education -\nperception among medical students.BMC Med. Educ.24, 804\n(2024).\n52. Australian Academy of Technological Sciences and Engineering\n(ATSE) and A.I.f.M.L. (AIML), Responsible AI: Your questions\nanswered. 2023: Canberra, Adelaide.\n53. Moy, S. et al. Patient perspectives on the use of artiﬁcial intelligence\nin health care: a scoping review.J. Patient Cent. Res Rev.11,5 1–62\n(2024).\n54. Mikkelsen, J. G., Sorensen, N. L., Merrild, C. H., Jensen, M. B. &\nThomsen, J. L. Patient perspectives on data sharing regarding\nimplementing and using artiﬁcial intelligence in general practice - a\nqualitative study.BMC Health Serv. Res23, 335 (2023).\n55. Khullar, D. et al. Perspectives of patients about artiﬁcial intelligence\nin health care.JAMA Netw. Open5, e2210309 (2022).\n56. Kudina, O. & de Boer, B. Co-designing diagnosis: Towards a\nresponsible integration of Machine Learning decision-support\nsystems in medical diagnostics.J. Eval. Clin. Pract.27, 529–536\n(2021).\n57. Mykhailov, D. A moral analysis of intelligent decision-support\nsystems in diagnostics through the lens of Luciano Floridi’s\ninformation ethics.Hum. Aff.31, 149–164 (2021).\n58. Juravle, G., Boudouraki, A., Terziyska, M. & Rezlescu, C. Trust in\nartiﬁcial intelligence for medical diagnoses.Prog. Brain Res.253,\n263–282 (2020).\n59. Candelon, F., di Carlo, R, C., De Bondt, M. & Evgenious, T. AI\nregulation is coming.Harvard Bus. Rev.(2021).\n60. Longoni, C., Bonezzi, A. & Morewedge, C. K. Resistance to medical\nartiﬁcial intelligence.J. Consum. Res.46, 629–650 (2019).\n61. Sauerbrei, A., Kerasidou, A., Lucivero, F. & Hallowell, N. The impact\nof artiﬁcial intelligence on the person-centred, doctor-patient\nrelationship: some problems and solutions.BMC Med. Inf. Decis.\nMak. 23, 73 (2023).\n62. de Boer, B. & Kudina, O. What is morally at stake when using\nalgorithms to make medical diagnoses? Expanding the discussion\nbeyond risks and harms.Theor. Med. Bioeth.45, 245–266 (2021).\n63. Kingsford, P. A. & Ambrose, J. A. Artiﬁcial intelligence and the\ndoctor-patient relationship.Am. J. Med137, 381–382 (2024).\n64. Lorenzini, G., Arbelaez Ossa, L., Shaw, D. M. & Elger, B. S. Artiﬁcial\nintelligence and the doctor-patient relationship expanding the\nparadigm of shared decision making.Bioethics 37, 424–429 (2023).\n65. Mittelstadt, B. The Impact of Artiﬁcial Intelligence on the Doctor-\nPatient Relationship(Council of Europe 2021).\n66. Mittermaier, M., Raza, M. & Kvedar, J. C. Collaborative strategies for\ndeploying AI-based physician decision support systems: challenges\nand deployment approaches.NPJ Digit. Med.6, 137 (2023).\n67. U.S. Department of Health and Human Services. Artiﬁcial\nIntelligence (AI) at HHS. [cited 10 Sep 2024]; Available from:https://\nwww.hhs.gov/programs/topic-sites/ai/index.html (2024).\n68. Jonnagaddala, J. & Wong, Z. S. Privacy preserving strategies for\nelectronic health records in the era of large language models.NPJ\nDigit. Med.8, 34 (2025).\n69. Productivity Commission, Australian Government. Making the most\nof the AI opportunity: The challenges of regulating AI: Canberra\n(2024).\n70. Nikolic, S. et al. ChatGPT, Copilot, Gemini, SciSpace and Wolfram\nversus higher education assessments: an updated multi-institutional\nstudy of the academic integrity impacts of Generative Artiﬁcial\nIntelligence (GenAI) on assessment, teaching and learning in\nengineering. Austr. J. Eng. Educ. 29,1 –28 (2024).\n71. Schuwirth, L. The need for national licensing examinations.Med.\nEduc. 41, 1022–1023 (2007).\n72. Schuwirth, L. W. & Van der Vleuten, C. P. Programmatic assessment:\nfrom assessment of learning to assessment for learning.Med.\nTeach. 33, 478–485 (2011).\n73. Bhanji, F. et al. Competence by design: the role of high-stakes\nexaminations in a competence based medical education system.\nPerspect. Med. Educ.13,6 8–74 (2024).\n74. Shumailov, I. et al. AI models collapse when trained on recursively\ngenerated data.Nature 631, 755–759 (2024).\n75. De Angelis, L. et al. ChatGPT and the rise of large language models:\nthe new AI-driven infodemic threat in public health.Front. Public\nHealth 11, 1166120 (2023).\n76. Xu, X., Chen, Y. & Miao, J. Opportunities, challenges, and future\ndirections of large language models, including ChatGPT in medical\neducation: a systematic scoping review.J. Educ. Eval. Health Prof.\n21, 6 (2024).\n77. Ngo, B., Nguyen, D. & vanSonnenberg, E. The cases for and against\nartiﬁcial intelligence in the medical school curriculum.Radio. Artif.\nIntell. 4, e220074 (2022).\n78. Franco D\n’Souza, R., Mathew, M., Mishra, V. & Surapaneni, K. M.\nTwelve tips for addressing ethical concerns in the implementation of\nartiﬁcial intelligence in medical education.Med. Educ. Online29,\n2330250 (2024).\n79. Fleisher, L. A. & Economou-Zavlanos, N. J. Artiﬁcial Intelligence can\nbe regulated using current patient safety procedures and\ninfrastructure in hospitals.JAMA Health Forum5, e241369 (2024).\n80. Chouffani El Fassi, S. et al. Not all AI health tools with regulatory\nauthorization are clinically validated.Nat. Med.30, 2718–2720\n(2024).\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 9\n81. Australian Human Rights Commission. Australia Needs AI\nRegulation [cited 3 Dec 2024]; Available from:https://humanrights.\ngov.au/about/news/australia-needs-ai-regulation (2023).\n82. Mesko, B. & Topol, E. J. The imperative for regulatory oversight of\nlarge language models (or generative AI) in healthcare.NPJ Digit.\nMed. 6, 120 (2023).\n83. GIbson, D., Kovanovic, V., Ifenthaler, D., Dexter, S. & Feng, S.\nLearning theories for artiﬁcial intelligence promoting learning\nprocesses. Br. J. Educ. Technol.54, 1125–1146 (2023).\n84. Yu, H. & Guo, Y. Generative artiﬁcial intelligence empowers\neducational reform: current status, issues, and prospects.Front.\nEduc. 8 (2023).\n85. Gniel, H. AI: A Regulatory Perspective(Australian Government\nTertiary Education Quality and Standards Agency, 2023).\n86. The Royal Australian College of General Practitioners. Artiﬁcial\nintelligence in primary care [cited 2024 20/09/2024]; Available from:\nhttps://www.racgp.org.au/advocacy/position-statements/view-all-\nposition-statements/clinical-and-practice-management/artiﬁcial-\nintelligence-in-primary-care (2024).\n87. Knopp, M. I. et al. AI-enabled medical education: threads of change,\npromising futures, and risky realities across four potential future\nworlds. JMIR Med Educ.9, e50373 (2023).\n88. Australian Health Practitioner Regulation Agency. Meeting your\nprofessional obligations when using Artiﬁcial Intelligence in\nhealthcare [cited 3 Dec 2024]; Available from:https://www.ahpra.\ngov.au/Resources/Artiﬁcial-Intelligence-in-healthcare.aspx (2024).\n89. Australian Government Digital Transformation Agency, Policy for the\nResponsible Use of AI in Government, Commonwealth of Australia\n(Digital Transformation Agency) (2024).\n90. The White House. Fact Sheet: Executive Order on Safe, Secure, and\nTrustworthy Artiﬁcial Intelligence [cited 13 Dec 2024]; Available from:\nhttps://www.whitehouse.gov/brieﬁng-room/statements-releases/\n2023/10/30/fact-sheet-president-biden-issues-executive-order-\non-safe-secure-and-trustworthy-artiﬁcial-intelligence/ (2023).\n91. Council of Europe, Council of Europe Framework Convention on\nArtiﬁcial Intelligence and Human Rights, Democracy and the Rule of\nLaw, Council of Europe Treaty Series No. 225 (2024).\n92. Department for Science, I.T., The Bletchley Declaration by Countries\nAttending the AI Safety Summit, 1–2 November 2023, Department\nfor Science, Innovation & Technology (2023).\n93. United Nations AI Advisory Body, Governing AI for Humanity (2024).\n94. Australian Government Department of Industry, S.a.R., Safe and\nresponsible AI in Australia consultation: Australian Government’s\ninterim response. Commonwealth of Australia (2024).\n95. Wellner, G. A postphenomenological guide to AI regulation.J. Hum.-\nTechnol. Relat.2,1 –18 (2024).\n96. The White House. Blueprint for an AI Bill of Rights: Making\nAutomated Systems Work for the American People [cited 20 Sep\n2024]; Available from:https://www.whitehouse.gov/ostp/ai-bill-of-\nrights/ (2024).\n97. Government of the United Kingdom, National AI Strategy, HM\nGovernment (2021).\n98. Digital Policy Of ﬁce: The Government of the Hong Kong Special\nAdministrative Region of the People’s Republic of China, Ethical\nArtiﬁcial Intelligence Framework (2024).\n99. Ministry of Education, C., Sports, Science and Technology - Japan,\nWhite Paper on Science, Technology, and Innovation: How AI will\ntransform Science, Technology and Innovation. (Ministry of\nEducation, Culture, Sports, Science and Technology, 2024).\n100. Australian Government Department of Industry, S.a.R., Voluntary AI\nSafety Standard. (Commonwealth of Australia, 2024).\nAuthor contributions\nEach author, M.T., C.B., J.J., G.L., N.M., S.R., J.R., L.S., N.S., C.S. and Z.W.\ncontributed to the planning, writing and review of the manuscript. All authors\nhave read and approved theﬁnal manuscript.\nCompeting interests\nThe following authors are afﬁliated with this journal (npj digital medicine) as\nEditors. They have excused themselves from the editorial process and\nhandling of this article to remain impartial. (1) J.J. (2) Z.W.\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nMichael Tran.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01721-z Review\nnpj Digital Medicine|           (2025) 8:315 10",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7518407106399536
    },
    {
      "name": "Corporate governance",
      "score": 0.5587558746337891
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4208067059516907
    },
    {
      "name": "Computer science",
      "score": 0.3307023048400879
    },
    {
      "name": "Business",
      "score": 0.2719639539718628
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ]
}