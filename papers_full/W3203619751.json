{
  "title": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting",
  "url": "https://openalex.org/W3203619751",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288205693",
      "name": "Grigsby, Jake",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1062931711",
      "name": "Wang Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2028669061",
      "name": "Nguyen, Nam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2272233770",
      "name": "Qi, Yanjun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W2747599906",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3092172514",
    "https://openalex.org/W3028192203",
    "https://openalex.org/W2981613521",
    "https://openalex.org/W3000386982",
    "https://openalex.org/W3037973456",
    "https://openalex.org/W3005326476",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3153837934",
    "https://openalex.org/W3023489204",
    "https://openalex.org/W3202141913",
    "https://openalex.org/W2963172229",
    "https://openalex.org/W2952042565",
    "https://openalex.org/W3173001334",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3090339165",
    "https://openalex.org/W2963358464",
    "https://openalex.org/W3094981855",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3017895332",
    "https://openalex.org/W3097294131",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3022643593",
    "https://openalex.org/W2114001875",
    "https://openalex.org/W3163905744",
    "https://openalex.org/W3181262653",
    "https://openalex.org/W3016635207",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3034749137",
    "https://openalex.org/W2947014533",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W3166021751",
    "https://openalex.org/W2603648311",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W2962752580",
    "https://openalex.org/W2890096158",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3093761440",
    "https://openalex.org/W3192937766",
    "https://openalex.org/W2957988017",
    "https://openalex.org/W2607045400",
    "https://openalex.org/W2798058877",
    "https://openalex.org/W3103682594"
  ],
  "abstract": "Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a \"spatiotemporal sequence\" formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achieves competitive results on benchmarks from traffic forecasting to electricity demand and weather prediction while learning spatiotemporal relationships purely from data.",
  "full_text": "Long-Range Transformers for Dynamic Spatiotemporal\nForecasting\nJake Grigsby\nUniversity of Virginiaâ€ \njcg6dn@virginia.edu\nZhe Wang\nUniversity of Virginia\nzw6sg@virginia.edu\nNam Nguyen\nIBM Research\nnnguyen@us.ibm.com\nYanjun Qi\nUniversity of Virginia\nyanjun@virginia.edu\nABSTRACT\nMultivariate time series forecasting focuses on predicting future\nvalues based on historical context. State-of-the-art sequence-to-\nsequence models rely on neural attention between timesteps, which\nallows for temporal learning but fails to consider distinct spatial re-\nlationships between variables. In contrast, methods based on graph\nneural networks explicitly model variable relationships. However,\nthese methods often rely on predefined graphs that cannot change\nover time and perform separate spatial and temporal updates with-\nout establishing direct connections between each variable at ev-\nery timestep. Our work addresses these problems by translating\nmultivariate forecasting into a â€œspatiotemporal sequence\" formu-\nlation where each Transformer input token represents the value\nof a single variable at a given time. Long-Range Transformers can\nthen learn interactions between space, time, and value information\njointly along this extended sequence. Our method, which we call\nSpacetimeformer, achieves competitive results on benchmarks\nfrom traffic forecasting to electricity demand and weather predic-\ntion while learning spatiotemporal relationships purely from data.\n1 INTRODUCTION\nMultivariate forecasting models attempt to predict future outcomes\nbased on historical context; jointly modeling a set of variables al-\nlows us to interpret dependency relationships that provide early\nwarning signs of changes in future behavior. A simple example is\nshown in Figure 1a. Time Series Forecasting (TSF) models typically\ndeal with a small number of variables with long-term temporal\ndependencies that require historical recall and distant forecast-\ning. This is commonly handled by encoder-decoder sequence-to-\nsequence (seq2seq) architectures based on recurrent networks or\none-dimensional convolutions. Current state-of-the-art TSF models\nsubstitute classic seq2seq architectures for neural-attention-based\nmechanisms. However, these models represent the value of multi-\nple variables per timestep as a single input token. This lets them\nlearn â€œtemporal attention\" amongst timesteps but can ignore the dis-\ntinct spatial relationships that exist between variables. A temporal\nattention network is shown in Figure 1b.\nIn contrast, spatial-temporal methods aim to capture the rela-\ntionships between multiple variables. These models typically in-\nvolve alternating applications of temporal sequence processing and\nspatial message passing based on Graph Neural Network (GNN)\nWork done in part while an intern at IBM Research. Now at UT Austin, email:\ngrigsby@cs.utexas.edu.\nTT-c T+h-10\n0\n10\nT-c T-1 T T+1T+hQueryToken\nT T+1T+h\nT-c\nT-c\nT-1 T T+1T+h\nT T+1T+hT-1\na)\n10 10 0 ? ?\n1 1 -9 ? ?\n-9 -8 -5 ? ?\n0 ? ?\n1 1 -9 ? ?\n-9 -8 -5 ? ?\n-9\n10 10\n0\n? ?\n-9 -8 -5 ? ?\n1 1 ? ?\nT-1 T+1\nT-c T-110 10\nT-c\n101\nT-1\n101\nT\n0 -9\nT+1\n? ?\nT+h\n? ?-9 -8 -5 ? ?\nb) Temporal Attention\nc) Spatial Graph and TemporalAttention\nd) Spatio-temporal Attention\nQueryToken\nQueryToken\nContextTarget\nLess Attention MoreAttention\nFigure 1: Attention in multivariate forecasting. (a) A three-\nvariable sequence with three context points and two target\npoints to predict. (b) Temporal attention in which each to-\nken contains all three variables. Darker blue lines between\ngrey tokens represent increasing attention. (c) Temporal at-\ntention with spatial interactions modeled within each to-\nkenâ€™s timestep by known spatial graphs (shown in black).\n(d) Spatiotemporal attention in which each variables at each\ntimestep is a separate token. In practice this graph is densely\nconnected but edges have been cut for readability. All figures\nin this paper are best viewed in color.\ncomponents that rely on ground-truth variable relationships that\nare provided in advance or determined by heuristics. However,\narXiv:2109.12218v3  [cs.LG]  18 Mar 2023\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nhardcoded graphs can be difficult to define in domains that do not\nhave clear physical relationships between variables. Even when\nthey do exist, predefined graphs can create fixed ( static) spatial\nstructure. Variable relationships may change over time and can be\nmore accurately modeled by a context-dependent (dynamic) graph.\nA temporal attention network with spatial graph layers is depicted\nin Figure 1c.\nThis paper proposes a general-purpose multivariate forecaster\nwith the long-term prediction ability of a time series model and the\ndynamic spatial modeling of a GNN without relying on a hardcoded\ngraph. Let ğ‘ be the number of variables we are predicting and ğ¿\nbe the sequence length in timesteps. We flatten multivariate in-\nputs of shape (ğ¿,ğ‘ )into long sequences of shape (ğ¿Ã—ğ‘,1)where\neach input token isolates the value of a single variable at a given\ntimestep. The resulting input allows Transformer architectures to\nlearn attention networks across both space and time jointly, cre-\nating the â€œspatiotemporal attention\" mechanism shown in Figure\n1d. Spatiotemporal attention learns dynamic variable relationships\npurely from data, while an encoder-decoder Transformer architec-\nture enables accurate long-term predictions. Our method avoids\nTSF/GNN domain knowledge by representing the multivariate fore-\ncasting problem in a raw format that relies on end-to-end learning;\nthe cost of that simplicity is the engineering challenge of train-\ning Transformers at long sequence lengths. We explore a variety\nof strategies, including fast attention, hybrid convolutional archi-\ntectures, local-global and shifted-window attention, and a custom\nspatiotemporal embedding scheme. Our implementation is efficient\nenough to run on standard resources and scales well with high-\nmemory GPUs. Extensive experiments demonstrate the benefits of\nTransformers with spatiotemporal attention in benchmarks from\ntraffic forecasting to electricity production, temperature prediction,\nand metro ridership. We show that a single approach can achieve\nhighly competitive results against specialized baselines from both\nthe time series forecasting and spatial-temporal GNN literature. We\nopen-source a large codebase that includes our model, datasets, and\nthe groundwork for future research directions.\n2 BACKGROUND AND RELATED WORK\n2.1 Long-Range Transformers\nFor brevity, we assume the reader is familiar with the Transformer\narchitecture [65]. A brief overview of the self-attention mechanism\ncan be found in Appendix A.1. This paper will focus on the inter-\npretation of Transformers as a learnable message passing graph\namongst a set of inputs [28, 42, 86]. Let A âˆˆRğ¿ğ‘Ã—ğ¿ğ‘˜ be the atten-\ntion scores of a query sequence of length ğ¿ğ‘ and a key sequence\nof length ğ¿ğ‘˜. A acts much like the adjacency matrix of a graph\nbetween the tokens of the two sequences, where A[ğ‘–,ğ‘— ]denotes\nthe strength of the connection between the ğ‘–th token of the query\nsequence and the ğ‘—th token in the key sequence.\nBecause attention involves matching each query to the entire\nkey sequence, its runtime and memory use grows quadratically\nwith the length of its input. As a result, the research community\nhas raced to develop and evaluate Transformer variants for longer\nsequences [63]. Many of these methods introduce heuristics to spar-\nsify the attention matrix. For example, we can attend primarily to\nadjacent input tokens [35], select global tokens [22], increasingly\ndistant tokens [83, 8] or a combination thereof [86, 89]. While these\nmethods are effective, their inductive biases about the structure of\nthe trained attention matrix are not always compatible with tasks\noutside of NLP. Another approach looks to approximate attention\nin sub-quadratic time while retaining its flexibility [66, 79, 95, 54].\nParticularly relevant to this work is the Performer [9]. Performer\napproximates attention in linear space and time with a kernel of ran-\ndom orthogonal features and enables the long-sequence approach\nthat is central to our work. For a thorough survey of efficient atten-\ntion mechanisms, see [62].\n2.2 Time Series Forecasting and Transformers\nDeep learning approaches to TSF are generally based on a seq2seq\nframework in which acontext window of theğ‘most recent timesteps\nis mapped to a target window of predictions for a time horizon of â„\nsteps into the future. Let ğ‘¥ğ‘¡ be a vector of timestamp values (the\nday, month, year, etc.) at timeğ‘¡and ğ‘¦ğ‘¡ be a vector of variable values\nat time ğ‘¡. Given a context sequence of time inputs (ğ‘¥ğ‘‡âˆ’ğ‘,...,ğ‘¥ ğ‘‡)\nand variables (ğ‘¦ğ‘‡âˆ’ğ‘,...,ğ‘¦ ğ‘‡)up to time ğ‘‡, we output another se-\nquence of variable values (^ğ‘¦ğ‘‡+1,..., ^ğ‘¦ğ‘‡+â„)corresponding to our\npredictions at future timesteps (ğ‘¥ğ‘‡+1,...,ğ‘¥ ğ‘‡+â„).\nThe most common class of deep TSF models are based on a combi-\nnation of Recurrent Neural Networks (RNNs) and one-dimensional\nconvolutions (Conv1Ds) [3, 60, 57, 32]. More related to the proposed\nmethod is a recent line of work on attention mechanisms that aim\nto overcome RNNsâ€™ autoregressive training and difficulty in in-\nterpreting long-term patterns [27, 35, 88, 72, 50]. Notable among\nthese is theInformer [92] - a general encoder-decoder Transformer\narchitecture for TSF. Informer takes the sequence timestamps\n(ğ‘¥ğ‘‡âˆ’ğ‘,...,ğ‘¥ ğ‘‡+â„)and embeds them in a higher dimension ğ‘‘. The\nvariable values - with zeros replacing the unknown target sequence\n(ğ‘¦ğ‘‡âˆ’ğ‘,...,ğ‘¦ ğ‘‡,0ğ‘‡+1 ..., 0ğ‘‡+â„)- are mapped to the same dimension\nğ‘‘. The time (ğ‘¥) and variable (ğ‘¦) components sum to create an input\nsequence of ğ‘+â„tokens, written in matrix form as Z âˆˆR(ğ‘+â„)Ã—ğ‘‘.\nThe encoder processes the sub-sequence Z[0 ...ğ‘ ]while the de-\ncoder observes the target sequence Z[ğ‘+1 ...ğ‘ +â„]. The outputs\nof the decoder are treated as predictions and error is minimized\nby regression to the true sequence values. Note that Informer out-\nputs the entire prediction sequence directly in one forward pass, as\nopposed to decoder-only generative models that output each token\niteratively (e.g., large language models). This has the advantage of\nreducing compute by reusing the encoder representation in each\ndecoder layer and minimizing error accumulation in autoregressive\npredictions.\nFollowing Informer, a rapidly expanding line of work has looked\nto improve upon the benchmark results of Transformers in TSF.\nThese methods adjust the model architecture [45, 43] and re-introduce\nclassic time series domain knowledge such as series decomposi-\ntion, auto-correlation, or computation in frequency space [93, 70].\nThese time series biases are often used as a domain-specific form\nof efficient attention [71, 43, 15], as many long-term TSF tasks nat-\nurally extend beyond the sequence limits of default Transformers.\nIt is important to note that time series Transformers use one input\ntoken per timestep, so that the embedding of the token at time ğ‘¡\nrepresents ğ‘ distinct variables at that moment in time. This is in\ncontrast with domains like natural language processing in which\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\neach token represents just one unified idea (e.g., a single word)\n[59]. The message-passing graph that results from attention over a\nmultivariate sequence learns patterns across time while keeping\nvariables grouped together (Fig. 1b). This setup forces the variables\nwithin each token to receive the same amount of information from\nother timesteps, despite the fact that variables may have distinct\npatterns or relationships to each other. Ideally, we would have a\nway to model these kinds of variable relationships.\n2.3 Spatial-Temporal Forecasting and GNNs\nMultivariate TSF involves two dimensions of complexity: the fore-\ncast sequenceâ€™s duration, ğ¿, and the number of variables ğ‘ consid-\nered at each timestep. As ğ‘ grows, it becomes increasingly impor-\ntant to model the relationships between each variable. The multi-\nvariate forecasting problem can be reformulated as the prediction of\na sequence of graphs, where at each timestep ğ‘¡ we have a graph Gğ‘¡\nconsisting of ğ‘ variable nodes connected by weighted edges. Given\na context sequence of graphs (Gğ‘‡âˆ’ğ‘,..., Gğ‘‡), we must predict the\nnode values of the target sequence (Gğ‘‡+1,..., Gğ‘‡+â„).\nGraph Neural Networks (GNNs) are a category of deep learn-\ning techniques that aim to explicitly model variable relationships\nas interactions along a network of nodes [74]. Earlier work uses\ngraph convolutional layers [48] to pass information amongst the\nvariables of each timestep while using standard sequence learning\narchitectures (e.g., RNNs [36], Conv1Ds [85], dilated Conv1Ds [76])\nto adapt those representations across time. More recent work has\nextended this formula by replacing the spatial and/or temporal\nlearning mechanisms with attention modules [6, 80, 77]. Temporal\nattention with intra-token spatial graph learning is depicted in Fig-\nure 1c. Existing GNN-based methods have a combination of three\ncommon shortcomings:\n(1) Their spatial modules require predefined graphs denoting\nvariable relationships. This can make it more difficult to\nsolve abstract forecasting problems where relationships are\nunknown and must be discovered from data.\n(2) They perform separate spatial and temporal updates in al-\nternating layers. This creates information bottlenecks that\nrestrict spatiotemporal message passing.\n(3) Their spatial graphs remain static across timesteps. Vari-\nable relationships can often change over time and should be\nmodeled by a dynamic graph.\nAppendix A.2 provides a detailed overview of related work\nin the crowded field of spatial-temporal GNNs, and includes\na categorization of existing methods according to these three\nkey differences. Our goal is to develop a seq2seq time series model\nwith true spatiotemporal message passing on a dynamic graph that\ncan be competitive with GNNs despite not using predefined variable\nrelationships.\n3 SPATIOTEMPORAL TRANSFORMERS\n3.1 Spatiotemporal Sequences\nWe begin by building upon the Informer-style encoder-decoder\nTransformer framework. As discussed in Sec. 2.2, Informer gener-\nates ğ‘‘-dimensional embeddings of the sequence \u0000(ğ‘¥ğ‘‡âˆ’ğ‘,ğ‘¦ğ‘‡âˆ’ğ‘),...,\n(ğ‘¥ğ‘‡,ğ‘¦ğ‘‡),(ğ‘¥ğ‘‡+1,0ğ‘‡+1),..., (ğ‘¥ğ‘‡+â„,0ğ‘‡+â„)\u0001, with the result expressed\n101001 1 -9-9 -8 -5T-c T-1 T\nycontext\nxcontext\n1010 0 1 1 -9 -9 -8 -5T-c T-1 T T-c T-1 T T-c T-1 T0 0 0 1 1 1 2 2 20 1 2 0 1 2 0 1 2\n101001 1 -9-9 -8 -5T-c T-1 T0 1 2\nValue\nTimePositionValueTime\nPositionVariable\nInput\tDataA Temporal\tSequence\nSpatiotemporal\tSequence3\nB\nC\nFigure 2: Embedding Multivariate Data (a) The example con-\ntext sequence from Fig. 1a. (b) Standard â€œTemporal\" embed-\nding input sequence where each column will become a to-\nken. (c) Flattened spatiotemporal input sequence for Fig. 1d\nwith position and variable indices.\nin matrix form as Z âˆˆR(ğ‘+â„)Ã—ğ‘‘. We propose to modify the token\nembedding input sequence by flattening each multivariateğ‘¦ğ‘¡ vector\ninto ğ‘ scalars with a copy of its timestamp ğ‘¥ğ‘¡, leading to a new\nsequence: \u0000(ğ‘¥ğ‘‡âˆ’ğ‘,ğ‘¦0\nğ‘‡âˆ’ğ‘),..., (ğ‘¥ğ‘‡âˆ’ğ‘,ğ‘¦ğ‘\nğ‘‡âˆ’ğ‘),..., (ğ‘¥ğ‘‡,ğ‘¦0\nğ‘‡),...,\n(ğ‘¥ğ‘‡,ğ‘¦ğ‘\nğ‘‡ ),(ğ‘¥ğ‘‡+1,00\nğ‘‡+1),..., (ğ‘¥ğ‘‡+1,0ğ‘\nğ‘‡+1),..., (ğ‘¥ğ‘‡+â„,0ğ‘\nğ‘‡+â„)\u0001. Embed-\nding this longer sequence results in a Zâ€²âˆˆRğ‘(ğ‘+â„)Ã—ğ‘‘. When we\npass Zâ€²through a Transformer, the attention matrixA âˆˆRğ‘(ğ‘+â„)Ã—ğ‘(ğ‘+â„)\nthen represents a spatiotemporal graph with a direct path between\nevery variable at every timestep (see Appendix A.2 Fig. 5). In addi-\ntion, we are now learning spatial relationships that do not rely on\npredefined variable graphs and can change dynamically according\nto the time and variable values of the input data. This concept is\ndepicted in Figure 1d. There are two important questions left to\nanswer:\n(1) How do we embed this sequence so that the attention net-\nwork parameters can accurately interpret the information\nin each token?\n(2) Are we able to multiply the sequence length by a factor of\nğ‘ and still scale to real-world datasets?\n3.2 Spatiotemporal Embeddings\nRepresenting Time and Value. The input embedding module of\na TSF Transformer determines the way the (ğ‘¥,ğ‘¦)sequence con-\ncepts in Sec 3.1 are implemented in practice. We create input se-\nquences consisting of the values of our time series variables (ğ‘¦) and\ntime information (ğ‘¥). Although many seq2seq TSF models discard\nexplicit time information, we use Time2Vec layers [29] to learn\nseasonal characteristics. Time2Vec maps ğ‘¥ğ‘¡ to sinusoidal patterns\nof learned offsets and wavelengths. This helps represent periodic\nrelationships that extend past the limited length of the context se-\nquence. The concatenated variable values and time embeddings are\nthen projected to the input dimension of the Transformer model\nwith a feed-forward layer. We refer to the resulting output as the\nâ€œvalue+time embedding. \"\nRepresenting Position. Transformers are permutation invari-\nant, meaning they cannot interpret the order of input tokens by\ndefault. This is fixed by adding a position embedding to the tokens;\nwe opt for the fully learnable position embedding variant where we\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nTarget Embedding\nNorm\nLocal Self Attention\nNorm\nGlobal Self Attention\nNorm\nLocal Cross Attention\nNorm\nGlobal Cross Attention\nContext Embedding\nNorm\nLocal Self Attention\nNorm\nGlobal Self Attention\nLinear\nNorm\nEncoder Sequence\nLinear\nNorm\nNorm\nLinearNorm\n101 -9101 -8 0 -9 -5\n0 1 2 0 1 2 0 1 2\nT-cT-cT-cT-1T-1T-1T T T\n? ? ? ? ? ?\n0 1 2 0 1 2\nT+1T+1T+1T+hT+hT+h\nValue\nVariable\nTime\n9 0 -8 8 1 -6\nTarget Embedding\nPredictions:\nEncoder Output\nFigure 3: The Spacetimeformer architecture for joint spa-\ntiotemporal learning applied to the sequence shown in Fig-\nure 1a. This architecture creates a practical implementation\nof the spatiotemporal attention concept in Figure 1d.\ninitialize ğ‘‘-dimensional embedding vectors for each timestep up to\nthe maximum sequence length. An example of the way input time\nseries data is organized for standard TSF Transformers is shown in\nFigure 2a and 2b. Existing models use a variety of implementation\ndetails, but the end result is a ğ‘‘-dimensional token that represents\nthe values of all ğ‘ variables simultaneously.\nRepresenting Space. We create spatiotemporal sequences by\nflattening the ğ‘ variables into separate tokens (Fig. 2c). Each token\nis given a copy of the time information ğ‘¥ for its value+time embed-\nding and assigned the position embedding index corresponding to\nits original timestep order, so that each position now appears ğ‘\ntimes. We differentiate between the variables at each timestep with\nan additional â€œvariable embedding. \" We initializeğ‘ ğ‘‘-dimensional\nembedding vectors for each variable index, much like the position\nembedding. Note that this means the variable representations are\nrandomly initialized and learned end-to-end during training. The\ninput pattern of position and variable indices is represented by\nFigure 2c. The use of two learnable embedding sets (space and time)\ncreates interesting parallels between our time series model and\nTransformers in computer vision. Further discussion and small-\nscale experiments on image data using our model are in Appendix\nA.3.\nRepresenting Missing Data. Many real-world applications in-\nvolve missing data. Existing work often ignores timesteps that have\nany missing variable values (wasting valuable data) or replaces\nthem with an arbitrary scalar that can confuse the model by being\nunpredictable or ambiguous. For example, popular traffic bench-\nmarks (Sec. 4.3) replace missing values with zeros, so that it is\nunclear whether traffic was low or was not recorded. Embedding\neach variable in its own separate token gives us the flexibility to\nleave values missing in the data pipeline, replace them with zeros\nin the forward pass, and then tell the model when values were orig-\ninally missing with a binary â€œgiven embedding. \" The â€œvalue+time\",\nvariable, position, and â€œgiven\" embeddings sum to create the final\nspatiotemporal input sequence.\n3.3 Scaling to Long Sequence Lengths\nOur embedding scheme in Sec 3.2 converts a sequence-to-sequence\nproblem of length ğ¿ into a new format of length ğ¿ğ‘. Standard\nTransformer layers have a maximum sequence length of less than\n1,000; the spatiotemporal sequence lengths considered in our ex-\nperimental results exceed 26,000. Clearly, additional optimization\nis needed to make these problems feasible.\nScaling with Fast-Attention . However, we are fortunate that\nthere is a rapidly-developing field of research in long-sequence\nTransformers (Sec. 2.1). The particular choice of attention mech-\nanism is quite flexible, although most results in this paper use\nPerformer FAVOR+ attention [9] - a linear approximation of atten-\ntion via a random kernel method. The direct (non-iterative) output\nformat of our model does not require causal masking, which can be\na key advantage when dealing with approximate attention mecha-\nnisms that have difficulty masking an attention matrix that they\nnever explicitly compute. However, there are some domains (Ap-\npendix B.6) with variable-length sequences and padding that make\nmasking an important consideration.\nScaling with Initial and Intermediate Conv1Ds . In low-resource\nsettings, we can also look for ways to learn shorter representations\nof the input with strided convolutions. â€œInitial\" convolutions are\napplied to the value+time embedding of the encoder. â€œIntermedi-\nate\" convolutions occur between attention layers and are a key\ncomponent of the Informer architecture. However, our flattened\nspatiotemporal sequences lay out variables in an arbitrary order\n(Fig. 2c). We rearrange the input so that each variable can be passed\nthrough a convolutional block independently and then recombined\ninto the longer spatiotemporal sequence.\nScaling with Shifted Attention Windows . We can split the\ninput into smaller chunks or â€œwindows\" across time, and perform\nspatiotemporal attention amongst the tokens of each window sepa-\nrately. Subsequent layers can shift the window boundaries and let\ninformation spread between distant windows as the network gets\ndeeper. The shifted window approach is inspired by the connection\nbetween Vision Transformers and spatiotemporal attention [44]\n(see Appendix A.3).\nThese scaling methods can be mixed and matched based on avail-\nable GPU memory. Our goal is to learn spatiotemporal graphs across\nentire input sequences. Therefore, we try use as few optimizations\nas possible, even though convolutions and windowed attention\nhave shown promise as a way to improve predictions despite not\nbeing a computational necessity. Most results in this paper were\ncollected using fast attention alone with less than 40GBs of GPU\nmemory. Strided convolutions are only necessary on the largest\ndatasets. Shifted window attention saves meaningful amounts of\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nmemory when using quadratic attention, so we primarily use it\nwhen we need to mask padded sequences.\n3.4 Spacetimeformer\nLocal and Global Architecture . We find that attention over a\nlonger multivariate sequence can complicate learning in problems\nwith large ğ‘. We add some architectural bias towards the sequence\nof each tokenâ€™s own variable with â€œlocal\" attention modules in each\nencoder and decoder layer. In a local layer, each token attends to\nthe timesteps of its own spatial variable. Note that this does not\nmean we are simplifying the â€œglobal\" attention layer by separating\ntemporal and spatial attention, as is common in spatial-temporal\nmethods (Appendix A.2). Rather, tokens attend to every token in\ntheir own variableâ€™s sequence and then to every token in the entire\nspatiotemporal global sequence. We use a Pre-Norm architecture\n[78] and BatchNorm [26] normalization. Figure 3 shows a one-layer\nencoder-decoder architecture.\nOutput and Time Series Tricks . The final feed-forward layer\nof the decoder outputs a sequence of predictions that can be folded\nback into the original input format of (ğ‘¥ğ‘¡,^ğ‘¦ğ‘¡). We can then optimize\na variety of forecasting loss functions, depending on the particular\nproblem and the baselines we are comparing against. Our goal is to\ncreate a general multivariate sequence model, so we try to avoid\nadding domain specific tricks whenever possible. However, we in-\nclude features from the recent Transformer TSF literature such as\nseasonal decomposition, input normalization, and the ability to\npredict the target sequence as the net difference from the output of\na simple linear model. These tricks are turned off by default and\nonly used in non-stationary domains where distribution shift is a\nmajor concern; we return to this briefly in the Experiments section\nand in detail in Appendix B.4. We nickname our full model the\nSpacetimeformer for clarity in experimental results. More imple-\nmentation details are listed in Appendix B.2, including explanations\nof several techniques that are not used in the main experimental\nresults but are part of our open-source code release.\n4 EXPERIMENTS\nOur experiments are designed to answer the following questions:\n(1) Is our model competitive with seq2seq methods on highly\ntemporal tasks that require long-term forecasting?\n(2) Is our model competitive with graph-based methods on\nhighly spatial tasks, despite not having access to a predefined\nvariable graph?\n(3) Does our spatiotemporal sequence formulation let Trans-\nformers learn meaningful variable relationships?\nWe compare against representative methods from the TSF and\nGNN literature in addition to ablations of our model. MTGNN [75]\nis a GNN method that is capable of learning its graph structure\nfrom data. LinearAR is a basic linear model that iteratively predicts\neach timestep of the target sequence as a linear combination of the\ncontext sequence and previous outputs. We also include a standard\nencoder-decoder LSTM [24] and the RNN/Conv1D-based LSTNet\n[32]. The most important baseline is an ablation of our method\nsimilar to Informer that controls for implementation details to\nmeasure the impact of spatiotemporal attention; this model uses\nthe Spacetimeformer architecture with the more standard tem-\nporal sequence embedding (see Fig. 2b). We refer to this as the\nTemporal model1. These baselines are included in our open-source\nrelease and use the same training loop and evaluation process. We\nadd Time2Vec information to baseline inputs when applicable be-\ncause Time2Vec has been shown to improve the performance of a\nvariety of sequence models [29]. Time series forecasting tricks like\ndecomposition and input normalization have been implemented for\nall methods. We also provide reference results from existing work\nwhen they are available, including several spatial-temporal GNN\nmodels with predefined graph information (Appendix A.2) and re-\ncent TSF Transformers (Sec. 2.2). We report evaluation metrics as\nthe average over all the timesteps of the target sequence, and the\naverage of at least three training runs.\n4.1 Toy Examples\nWe begin with a binary multivariate copy task similar to those used\nto evaluate long-range dependencies in memory-based sequence\nmodels [21]. However, we add an extra challenge and shift each\nvariableâ€™s output by a unique number of timesteps (visualized in\nAppendix B.3 Fig. 7). The shifted copy task was created because it\nrequires each variable to attend to different timesteps; Temporalâ€™s\nattention is fundamentally unable to do this, and instead resorts\nto learning one variable relationship per attention head until it\nruns out of heads and produces blurry outputs. The attention heads\nare visualized in Appendix B.3 Fig. 8 while an example sequence\nresult is shown in Appendix B.3 Fig. 9. Spatiotemporal attention is\ncapable of learning allğ‘variable relationships in one attention head\n(Appendix B.3 Fig. 10), leading to an accurate output (Appendix B.3\nFig. 11).\nNext we look at a more traditional forecasting setup inspired by\n[59] consisting of a multivariate sine-wave sequence with strong\ninter-variable dependence. Several ablations of our method are\nconsidered. This dataset creates a less extreme example of the effect\nin the shifted copy task, where Temporal models are forced to\ncompromise their attention over timesteps in a way that reduces\npredictive power over variables with such distinct frequencies. Our\nmethod learns an uncompromising spatiotemporal relationship\namong all tokens to generate the most accurate predictions. Dataset\ndetails and results can be found in Appendix B.3.\n4.2 Time Series Forecasting\nNY-TX Weather. We continue with more realistic time series do-\nmains where we must learn to forecast a relatively small number\nof variables ğ‘ with unknown relationships over a long duration\nğ¿. First we evaluate on a custom dataset of temperature values\ncompiled from the ASOS Weather Network [49]. We use three sta-\ntions in central Texas and three in eastern New York to create two\nalmost unrelated spatial sub-graphs. Temperature values are taken\nat one-hour intervals, and we investigate the impact of sequence\nlength by predicting 40, 80, and 160 hour target sequences. The re-\nsults are shown in Table 1. Our spatiotemporal embedding scheme\nprovides the most accurate forecasts, and its improvement over the\n1One key detail that cannot be applied to the Temporal model is the use of local\nattention layers, because there is no concept of local vs. global when tokens represent\na combination of variables.\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nLinearAR LSTM MTGNN Temporal Spacetimeformer\n40 hours\nMSE 18.84 14.29 13.32 13.29 12.49\nMAE 3.24 2.84 2.67 2.67 2.57\nRRSE 0.40 0.355 0.34 0.34 0.33\n80 hours\nMSE 23.99 18.75 19.27 19.99 17.9\nMAE 3.72 3.29 3.31 3.37 3.19\nRRSE 0.45 0.40 0.41 0.41 0.40\n160 hours\nMSE 28.84 22.11 24.28 24.16 21.35\nMAE 4.13 3.63 3.78 3.77 3.51\nRRSE 0.50 0.44 0.46 0.46 0.44\nTable 1: NY-TX Weather Results.\nTemporal method appears to increase over longer sequences where\nthe lack of flexibility that comes from grouping the two geographic\nregions together may become more relevant. MTGNN learns spatial\nrelationships, but temporal consistency can be difficult without\ndecoder attention; its convolution-only output mechanism begins\nto struggle at the 80 and 160 hour lengths.\nETTm1. The Transformer TSF literature (Sec. 2.2) has settled\non a set of common benchmarks for experimental comparison\nin long-term forecasting, including a dataset of electricity trans-\nformer temperature (ETT) series introduced by [92]. We compare\nagainst Informer and a selection of follow-up methods on the\nmultivariate minute resolution variant in Table 2. At first glance,\nSpacetimeformer offers meaningful improvements to Informer\nand is competitive with later variants that make use of additional\ntime series domain knowledge. However, our work on ETTm1\nand similar benchmarks revealed that these results have less to\ndo with advanced attention mechanisms or model architectures\nthan they do with robustness to the distribution shift caused by\nnon-stationary datasets. This is highlighted by the performance\nof simple linear models like LinearAR, which closely matches the\naccuracy of ETSFormer. In fact, a few tricks allow most models\nto achieve near state-of-the-art performance; reversible instance\nnormalization [30], for example, is enough to more than halve the\nprediction error of the LSTM baseline - noticeably outperforming\nthe original Informer results. This discussion is continued in detail\nin Appendix B.4 with experiments on ETTm1 and another common\nbenchmark. In addition, we create a custom task to investigate the\nway time series models handle different kinds of distributional shift.\n4.3 Spatial-Temporal Forecasting\nAL Solar. We turn our focus to problems on the GNN end of the spa-\ntiotemporal spectrum whereğ‘ approaches or exceedsğ¿. The AL So-\nlar dataset consists of solar power production measurements taken\nat 10-minute intervals from 137 locations. We predict 4-hour hori-\nzons, leading to the longest spatiotemporal sequences of our main\nexperiments; the results are shown in Table 3. Spacetimeformer\nis significantly more accurate than the TSF baselines. We speculate\nthat this is due to an increased ability to forecast unusual changes in\npower production due to weather or other localized effects. MTGNN\nlearns similar spatial relationships, but its temporal predictions are\nnot as accurate.\nPrediction Length\n24 48 96 288 672\nLSTM 0.63 0.94 0.91 1.12 1.56\nInformer 0.37 0.50 0.61 0.79 0.93\nPyraformer 0.49 0.66 0.71\nYFormer 0.36 0.46 0.57 0.59 0.66\nPreformer 0.40 0.43 0.45 0.49 0.54\nAutoformer 0.40 0.45 0.46 0.53 0.54\nETSFormer 0.34 0.38 0.39 0.42 0.45\nLinearAR 0.33 0.37 0.39 0.44 0.48\nSpacetimeformer 0.34 0.38 0.40 0.45 0.52\nTable 2: ETTm1 test set normalized MAE. Additional results\nand discussion provided in Appendix B.4.\nLinearAR LSTNet LSTM MTGNN TemporalSpacetimeformer\nMSE 14.3 15.09 10.59 11.40 9.94 7.75\nMAE 2.29 2.08 1.56 1.76 1.60 1.37\nTable 3: AL Solar Results.\nTraffic Prediction. Next, we experiment on two datasets com-\nmon in GNN research. The Metr-LA and Pems-Bay datasets consist\nof traffic speed readings at 5 minute intervals, and we forecast the\nconditions for the next hour. For these experiments we include\nresults directly from the literature (Appendix A.2) to get a better\ncomparison with GNN-based spatial models that used predefined\nroad graphs. The results are listed in Table 4. Our method clearly\nseparates itself from TSF models and enters the performance band\nof dedicated GNN methods on both datasets (without needing pre-\ndefined graphs).\nHZMetro. [40] experiment with a dataset of passenger arrivals\nand departures at metro stations in Hangzhou, China for a total of\n160 variables recorded in 15 minute time intervals. We forecast the\nnext hour and compare against their published results in Table 5.\nSpacetimeformer again shows that it can be as effective as graph-\nbased methods in spatial domains without requiring predefined\ngraphs. We list the bounds of several trials because the performance\ngap between methods on this dataset is relatively slim.\nSpatiotemporal Attention Patterns. Standard Transformers\nlearn sliding attention patterns that resemble convolutions. Our\nmethod learns distinct connections between variables - this leads\nto attention diagrams that tend to be structured in â€œvariable blocks\"\ndue to the way we flatten our input sequence (Fig. 2c). Figure 4 pro-\nvides an annotated example for the NY-TX weather dataset. Some\nattention heads convincingly recover the ground-truth relationship\nbetween input variables. Our methodâ€™s GNN-level performance\nin highly spatial tasks like traffic forecasting supports a similar\nconclusion when facing more complex graphs.\nAblations and Node Classification. Finally, we perform sev-\neral ablation experiments to measure the importance of design\ndecisions in our embedding mechanism and model architecture\nusing the NY-TX Weather and Metr-LA Traffic datasets. Results\nand analysis can be found in Appendix B.5.\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nTime Series Models ST-GNN Models\nLinearAR LSTM Temporal DCRNN MTGNN STAWnet ST-GRAT Graph\nWaveNet\nTraffic\nTrans. Spacetimeformer\nMetr-LA\nMAE 4.71 3.87 3.59 3.03 3.08 3.06 3.03 3.09 2.83 2.86\nMSE 94.11 47.5 52.73 37.88 39.05 38.73 39.77 39.84 32.82 38.27\nMAPE 12.7 10.7 10.7 8.27 8.30 8.34 8.25 8.42 7.70 7.80\nPems-Bay\nMAE 2.24 2.41 2.49 1.59 1.64 1.61 1.62 1.63 1.53 1.61\nMSE 27.62 25.49 27.27 13.69 13.98 13.47 13.85 13.87 13.25 13.99\nMAPE 4.98 5.81 6.12 3.61 3.66 3.63 3.65 3.67 3.49 3.63\nTable 4: Traffic Forecasting Results. Results for italicized models taken directly from published work.\nLSTM Temporal ASTGCN DCRNN GCRNN Graph-WaveNet PVGCN Spacetimeformer\nMAE 29.1 29.0 28.0 26.1 26.1 26.5 23.8 25.7 Â±.3\nRMSE 51.3 48.5 47.2 44.64 44.5 44.8 40.1 44.7 Â±1.6\nTable 5: HZMetro Ridership Prediction Results. Italicized model results provided by [40].\n\u0019\u0013\u0003+RXUV\u0003\n$JR 3UHVHQW\nÂ«\u0011\u0011\u0011\u0011\u0011\u0011\nÂ«\u0011\u0011\u0011\u0011\n\u0011\u0011\n7HPSRUDO\n6SDWLRWHPSRUDO\nFigure 4: Discovering Spatial Relationships from Data: We\nforecast the temperature at three weather stations in Texas\n(lower left, blue diamonds) and three stations in New York\n(upper right, purple triangles). Temporal attention stacks all\nsix time series into one input variable and attends across\ntime alone (upper left). Our method recovers the correct\nspatial relationship between the variables along with the\nstrided temporal relation (lower right) (Dark blue shaded en-\ntries â†’more attention).\n5 CONCLUSION AND FUTURE DIRECTIONS\nThis paper has presented a unified method for multivariate forecast-\ning based on the application of a custom long-range Transformer\narchitecture to elongated spatiotemporal input sequences. Our ap-\nproach jointly learns temporal and spatial relationships to achieve\ncompetitive results on long-sequence time series forecasting, and\ncan scale to high dimensional spatial problems without relying\non a predefined graph. We see several promising directions for\nfuture development. First, there is room to scale Spacetimeformer\nto much larger domains and model sizes. This could be accom-\nplished with additional computational resources or by making bet-\nter use of optimizations like windowed attention and convolutional\nlayers that were underutilized in our experimental results. Next,\nour main experiments focus on established benchmarks with rela-\ntively static spatial relationships and a standard multivariate input\nformat. While it was necessary to verify that our model is com-\npetitive in popular domains like traffic forecasting, we feel that\nnew applications with more rapid changes in variable behavior\ncould take better advantage our fully dynamic and learnable graph.\nOur embedding scheme also enables flexible input formats with\nirregularly/unevenly sampled series and exogenous variables (Ap-\npendix B.2). Finally, we see an opportunity to experiment with\nmulti-dataset generalization as is common for Transformers in\nmany other areas of machine learning. Appendix B.6 provides fur-\nther discussion of this direction, and the foundation for this work\nis included in our open-source release.\nREFERENCES\n[1] Ali Araabi and Christof Monz. Optimizing Transformer for\nLow-Resource Neural Machine Translation . 2020. arXiv: 2011.\n02266 [cs.CL].\n[2] Lei Bai et al. â€œAdaptive graph convolutional recurrent net-\nwork for traffic forecastingâ€. In: Advances in neural informa-\ntion processing systems 33 (2020), pp. 17804â€“17815.\n[3] Anastasia Borovykh, Sander Bohte, and Cornelis W Ooster-\nlee. â€œConditional time series forecasting with convolutional\nneural networksâ€. In: arXiv preprint arXiv:1703.04691 (2017).\n[4] Tom B. Brown et al. Language Models are Few-Shot Learners .\n2020. doi: 10.48550/ARXIV.2005.14165. url: https://arxiv.\norg/abs/2005.14165.\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\n[5] Khac-Hoai Nam Bui, Jiho Cho, and Hongsuk Yi. â€œSpatial-\ntemporal graph neural network for traffic forecasting: An\noverview and open research issuesâ€. In: Applied Intelligence\n(2021), pp. 1â€“12.\n[6] Ling Cai et al. â€œTraffic transformer: Capturing the continu-\nity and periodicity of time series for traffic forecastingâ€. In:\nTransactions in GIS 24.3 (2020), pp. 736â€“755.\n[7] Vitor Cerqueira, Luis Torgo, and Igor MozetiÄ. â€œEvaluating\ntime series forecasting models: An empirical study on per-\nformance estimation methodsâ€. In: Machine Learning 109.11\n(2020), pp. 1997â€“2028.\n[8] Rewon Child et al. â€œGenerating long sequences with sparse\ntransformersâ€. In: arXiv preprint arXiv:1904.10509 (2019).\n[9] Krzysztof Choromanski et al. Rethinking Attention with Per-\nformers. 2021. arXiv: 2009.14794 [cs.LG].\n[10] Xiangxiang Chu et al. â€œTwins: Revisiting the design of spatial\nattention in vision transformersâ€. In: Advances in Neural\nInformation Processing Systems 34 (2021).\n[11] Razvan-Gabriel Cirstea et al. â€œTowards spatio-temporal aware\ntraffic time series forecastingâ€. In: 2022 IEEE 38th Interna-\ntional Conference on Data Engineering (ICDE) . IEEE. 2022,\npp. 2900â€“2913.\n[12] Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding . 2019. arXiv: 1810.\n04805 [cs.CL].\n[13] Xiaoyi Dong et al. â€œCswin transformer: A general vision\ntransformer backbone with cross-shaped windowsâ€. In:arXiv\npreprint arXiv:2107.00652 (2021).\n[14] Alexey Dosovitskiy et al. â€œAn image is worth 16x16 words:\nTransformers for image recognition at scaleâ€. In:arXiv preprint\narXiv:2010.11929 (2020).\n[15] Dazhao Du, Bing Su, and Zhewei Wei. Preformer: Predic-\ntive Transformer with Multi-Scale Segment-wise Correlations\nfor Long-Term Time Series Forecasting . 2022. doi: 10.48550/\nARXIV.2202.11356. url: https://arxiv.org/abs/2202.11356.\n[16] et al. Falcon WA. â€œPyTorch Lightningâ€. In: GitHub. Note:\nhttps://github.com/PyTorchLightning/pytorch-lightning3 (2019).\n[17] Angela Fan, Edouard Grave, and Armand Joulin. Reducing\nTransformer Depth on Demand with Structured Dropout . 2019.\narXiv: 1909.11556 [cs.LG].\n[18] Yuchen Fang et al. â€œSpatio-Temporal meets Wavelet: Disen-\ntangled Traffic Flow Forecasting via Efficient Spectral Graph\nAttention Networkâ€. In: arXiv e-prints (2021), arXivâ€“2112.\n[19] Tryambak Gangopadhyay et al. â€œSpatiotemporal attention\nfor multivariate time series prediction and interpretationâ€. In:\nICASSP 2021-2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE. 2021, pp. 3560â€“\n3564.\n[20] Rakshitha Godahewa et al. â€œMonash time series forecasting\narchiveâ€. In: arXiv preprint arXiv:2105.06643 (2021).\n[21] Alex Graves, Greg Wayne, and Ivo Danihelka. â€œNeural turing\nmachinesâ€. In: arXiv preprint arXiv:1410.5401 (2014).\n[22] Qipeng Guo et al. â€œStar-transformerâ€. In:arXiv preprint arXiv:1902.09113\n(2019).\n[23] Shengnan Guo et al. â€œAttention Based Spatial-Temporal Graph\nConvolutional Networks for Traffic Flow Forecastingâ€. In:\nProceedings of the AAAI Conference on Artificial Intelligence\n33.01 (July 2019), pp. 922â€“929. doi: 10.1609/aaai.v33i01.\n3301922. url: https://ojs.aaai.org/index.php/AAAI/article/\nview/3881.\n[24] Sepp Hochreiter and JÃ¼rgen Schmidhuber. â€œLong Short-Term\nMemoryâ€. In: Neural Comput. 9.8 (Nov. 1997), pp. 1735â€“1780.\nissn: 0899-7667. doi: 10 . 1162 / neco . 1997 . 9 . 8 . 1735.url:\nhttps://doi.org/10.1162/neco.1997.9.8.1735.\n[25] Xiao Shi Huang et al. â€œImproving Transformer Optimization\nThrough Better Initializationâ€. In: Proceedings of the 37th\nInternational Conference on Machine Learning . Ed. by Hal\nDaumÃ© III and Aarti Singh. Vol. 119. Proceedings of Machine\nLearning Research. PMLR, July 2020, pp. 4475â€“4483. url:\nhttps://proceedings.mlr.press/v119/huang20f.html.\n[26] Sergey Ioffe and Christian Szegedy. Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal\nCovariate Shift. 2015. doi: 10.48550/ARXIV.1502.03167. url:\nhttps://arxiv.org/abs/1502.03167.\n[27] Tomoharu Iwata and Atsutoshi Kumagai. â€œFew-shot Learn-\ning for Time-series Forecastingâ€. In:arXiv preprint arXiv:2009.14379\n(2020).\n[28] Chaitanya Joshi. â€œTransformers are Graph Neural Networksâ€.\nIn: The Gradient (2020).\n[29] Seyed Mehran Kazemi et al. â€œTime2vec: Learning a vector\nrepresentation of timeâ€. In: arXiv preprint arXiv:1907.05321\n(2019).\n[30] Taesung Kim et al. â€œReversible instance normalization for ac-\ncurate time-series forecasting against distribution shiftâ€. In:\nInternational Conference on Learning Representations . 2021.\n[31] Louis Kirsch et al. â€œGeneral-purpose in-context learning by\nmeta-learning transformersâ€. In:arXiv preprint arXiv:2212.04458\n(2022).\n[32] Guokun Lai et al. Modeling Long- and Short-Term Temporal\nPatterns with Deep Neural Networks . 2018. arXiv: 1703.07015\n[cs.LG].\n[33] Shiyong Lan et al. â€œDSTAGNN: Dynamic Spatial-Temporal\nAware Graph Neural Network for Traffic Flow Forecastingâ€.\nIn: Proceedings of the 39th International Conference on Ma-\nchine Learning . Ed. by Kamalika Chaudhuri et al. Vol. 162.\nProceedings of Machine Learning Research. PMLR, July 2022,\npp. 11906â€“11917. url: https://proceedings.mlr.press/v162/\nlan22a.html.\n[34] Mengzhang Li and Zhanxing Zhu. â€œSpatial-Temporal Fusion\nGraph Neural Networks for Traffic Flow Forecastingâ€. In:\nProceedings of the AAAI Conference on Artificial Intelligence\n35.5 (May 2021), pp. 4189â€“4196. url: https://ojs.aaai.org/\nindex.php/AAAI/article/view/16542.\n[35] Shiyang Li et al. â€œEnhancing the locality and breaking the\nmemory bottleneck of transformer on time series forecast-\ningâ€. In: Advances in Neural Information Processing Systems\n32 (2019), pp. 5243â€“5253.\n[36] Yaguang Li et al. Diffusion Convolutional Recurrent Neural\nNetwork: Data-Driven Traffic Forecasting . 2018. arXiv: 1707.\n01926 [cs.LG].\n[37] Yawei Li et al. â€œLocalvit: Bringing locality to vision trans-\nformersâ€. In: arXiv preprint arXiv:2104.05707 (2021).\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\n[38] Bryan Lim et al. Temporal Fusion Transformers for Inter-\npretable Multi-horizon Time Series Forecasting . 2020. arXiv:\n1912.09363 [stat.ML].\n[39] Aoyu Liu and Yaying Zhang. â€œSpatial-Temporal Interactive\nDynamic Graph Convolution Network for Traffic Forecast-\ningâ€. In: arXiv preprint arXiv:2205.08689 (2022).\n[40] Lingbo Liu et al. â€œPhysical-virtual collaboration modeling for\nintra-and inter-station metro ridership predictionâ€. In: IEEE\nTransactions on Intelligent Transportation Systems (2020).\n[41] Liyuan Liu et al. Understanding the Difficulty of Training\nTransformers. 2020. arXiv: 2004.08249 [cs.LG].\n[42] Pengfei Liu et al. â€œContextualized non-local neural networks\nfor sequence learningâ€. In: Proceedings of the AAAI Confer-\nence on Artificial Intelligence . Vol. 33. 01. 2019, pp. 6762â€“6769.\n[43] Shizhan Liu et al. â€œPyraformer: Low-complexity pyramidal\nattention for long-range time series modeling and forecast-\ningâ€. In: International Conference on Learning Representations .\n2021.\n[44] Ze Liu et al. Swin Transformer: Hierarchical Vision Trans-\nformer using Shifted Windows . 2021. doi: 10.48550/ARXIV.\n2103.14030. url: https://arxiv.org/abs/2103.14030.\n[45] Kiran Madhusudhanan et al. Yformer: U-Net Inspired Trans-\nformer Architecture for Far Horizon Time Series Forecasting .\n2021. doi: 10.48550/ARXIV.2110.08255. url: https://arxiv.\norg/abs/2110.08255.\n[46] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assi-\nmakopoulos. â€œThe M4 Competition: 100,000 time series and\n61 forecasting methodsâ€. In: International Journal of Forecast-\ning 36.1 (2020). M4 Competition, pp. 54â€“74. issn: 0169-2070.\ndoi: https : / / doi . org / 10 . 1016 / j . ijforecast . 2019 . 04 . 014.\nurl: https://www.sciencedirect.com/science/article/pii/\nS0169207019301128.\n[47] Toan Q Nguyen and Julian Salazar. â€œTransformers with-\nout tears: Improving the normalization of self-attentionâ€.\nIn: arXiv preprint arXiv:1910.05895 (2019).\n[48] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.\nLearning Convolutional Neural Networks for Graphs . 2016.\narXiv: 1605.05273 [cs.LG].\n[49] NOAA. National Weather Service Automated Surface Observ-\ning System (ASOS) . 2021. url: https://www.weather.gov/\nasos/.\n[50] Boris N. Oreshkin et al. Meta-learning framework with ap-\nplications to zero-shot time-series forecasting . 2020. arXiv:\n2002.02887 [cs.LG].\n[51] Cheonbok Park et al. â€œST-GRAT: A Novel Spatio-temporal\nGraph Attention Networks for Accurately Forecasting Dy-\nnamically Changing Road Speedâ€. In: Proceedings of the 29th\nACM International Conference on Information & Knowledge\nManagement (Oct. 2020). doi: 10.1145/3340531.3411940. url:\nhttp://dx.doi.org/10.1145/3340531.3411940.\n[52] Adam Paszke et al. â€œPyTorch: An Imperative Style, High-\nPerformance Deep Learning Libraryâ€. In: Advances in Neural\nInformation Processing Systems 32 . Ed. by H. Wallach et al.\n2019, pp. 8024â€“8035.\n[53] Yanjun Qin et al. â€œDMGCRN: Dynamic Multi-Graph Convo-\nlution Recurrent Network for Traffic Forecastingâ€. In: arXiv\npreprint arXiv:2112.02264 (2021).\n[54] Zhen Qin et al. cosFormer: Rethinking Softmax in Attention .\n2022. doi: 10.48550/ARXIV.2202.08791. url: https://arxiv.\norg/abs/2202.08791.\n[55] Alex Rogozhnikov. â€œEinops: Clear and Reliable Tensor Ma-\nnipulations with Einstein-like Notationâ€. In: International\nConference on Learning Representations . 2022. url: https :\n//openreview.net/forum?id=oapKSVM2bcj.\n[56] Benedek Rozemberczki et al. PyTorch Geometric Temporal:\nSpatiotemporal Signal Processing with Neural Machine Learn-\ning Models . 2021. arXiv: 2104.07788 [cs.LG].\n[57] David Salinas, Valentin Flunkert, and Jan Gasthaus. DeepAR:\nProbabilistic Forecasting with Autoregressive Recurrent Net-\nworks. 2019. arXiv: 1704.04110 [cs.AI].\n[58] Sheng Shen et al. PowerNorm: Rethinking Batch Normaliza-\ntion in Transformers . 2020. arXiv: 2003.07845 [cs.CL].\n[59] Shun-Yao Shih, Fan-Keng Sun, and Hung-yi Lee. â€œTemporal\npattern attention for multivariate time series forecastingâ€.\nIn: Machine Learning 108.8 (2019), pp. 1421â€“1441.\n[60] Slawek Smyl. â€œA hybrid method of exponential smoothing\nand recurrent neural networks for time series forecastingâ€.\nIn: International Journal of Forecasting 36.1 (2020), pp. 75â€“85.\n[61] Chao Song et al. â€œSpatial-Temporal Synchronous Graph Con-\nvolutional Networks: A New Framework for Spatial-Temporal\nNetwork Data Forecastingâ€. In: Proceedings of the AAAI Con-\nference on Artificial Intelligence 34.01 (Apr. 2020), pp. 914â€“921.\ndoi: 10.1609/aaai.v34i01.5438. url: https://ojs.aaai.org/index.\nphp/AAAI/article/view/5438.\n[62] Yi Tay et al. Efficient Transformers: A Survey . 2020. doi: 10.\n48550/ARXIV.2009.06732. url: https://arxiv.org/abs/2009.\n06732.\n[63] Yi Tay et al. Long Range Arena: A Benchmark for Efficient\nTransformers. 2020. arXiv: 2011.04006 [cs.LG].\n[64] Chenyu Tian and Wai Kin Chan. â€œSpatial-temporal attention\nwavenet: A deep learning framework for traffic prediction\nconsidering spatial-temporal dependenciesâ€. In: IET Intelli-\ngent Transport Systems 15.4 (2021), pp. 549â€“561.\n[65] Ashish Vaswani et al. â€œAttention is all you needâ€. In:Advances\nin neural information processing systems . 2017, pp. 5998â€“6008.\n[66] Sinong Wang et al.Linformer: Self-Attention with Linear Com-\nplexity. 2020. arXiv: 2006.04768 [cs.LG].\n[67] Wenhai Wang et al. â€œPyramid vision transformer: A versa-\ntile backbone for dense prediction without convolutionsâ€.\nIn: Proceedings of the IEEE/CVF International Conference on\nComputer Vision . 2021, pp. 568â€“578.\n[68] Yuhu Wang et al. â€œTVGCN: Time-variant graph convolu-\ntional network for traffic forecastingâ€. In: Neurocomputing\n471 (2022), pp. 118â€“129.\n[69] Chen Weikang et al. â€œSpatial-Temporal Adaptive Graph Con-\nvolution with Attention Network for Traffic Forecastingâ€. In:\narXiv preprint arXiv:2206.03128 (2022).\n[70] Gerald Woo et al. â€œETSformer: Exponential Smoothing Trans-\nformers for Time-series Forecastingâ€. In:arXiv preprint arXiv:2202.01381\n(2022).\n[71] Haixu Wu et al. Autoformer: Decomposition Transformers\nwith Auto-Correlation for Long-Term Series Forecasting . 2021.\ndoi: 10.48550/ARXIV.2106.13008. url: https://arxiv.org/abs/\n2106.13008.\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\n[72] Sifan Wu et al. â€œAdversarial Sparse Transformer for Time\nSeries Forecastingâ€. In: (2020).\n[73] Zhen Wu et al. UniDrop: A Simple yet Effective Technique to\nImprove Transformer without Extra Cost . 2021. arXiv: 2104.\n04946 [cs.CL].\n[74] Zonghan Wu et al. â€œA Comprehensive Survey on Graph\nNeural Networksâ€. In: IEEE Transactions on Neural Networks\nand Learning Systems 32.1 (Jan. 2021), pp. 4â€“24. issn: 2162-\n2388. doi: 10.1109/tnnls.2020.2978386. url: http://dx.doi.org/\n10.1109/TNNLS.2020.2978386.\n[75] Zonghan Wu et al. Connecting the Dots: Multivariate Time\nSeries Forecasting with Graph Neural Networks . 2020. arXiv:\n2005.11650 [cs.LG].\n[76] Zonghan Wu et al. â€œGraph wavenet for deep spatial-temporal\ngraph modelingâ€. In: arXiv preprint arXiv:1906.00121 (2019).\n[77] Zonghan Wu et al. TraverseNet: Unifying Space and Time in\nMessage Passing . 2021. arXiv: 2109.02474 [cs.LG].\n[78] Ruibin Xiong et al.On Layer Normalization in the Transformer\nArchitecture. 2020. arXiv: 2002.04745 [cs.LG].\n[79] Yunyang Xiong et al. NystrÃ¶mformer: A NystrÃ¶m-Based Al-\ngorithm for Approximating Self-Attention . 2021. arXiv: 2102.\n03902 [cs.CL].\n[80] Mingxing Xu et al.Spatial-Temporal Transformer Networks for\nTraffic Flow Forecasting . 2021. arXiv: 2001.02908 [eess.SP].\n[81] Jiexia Ye et al. â€œMulti-stgcnet: A graph convolution based\nspatial-temporal framework for subway passenger flow fore-\ncastingâ€. In: 2020 International joint conference on neural net-\nworks (IJCNN). IEEE. 2020, pp. 1â€“8.\n[82] Xue Ye et al. â€œMeta Graph Transformer: A Novel Framework\nfor Spatialâ€“Temporal Traffic Predictionâ€. In:Neurocomputing\n491 (2022), pp. 544â€“563. issn: 0925-2312. doi: https://doi.\norg/10.1016/j.neucom.2021.12.033. url: https://www.\nsciencedirect.com/science/article/pii/S0925231221018725.\n[83] Zihao Ye et al. â€œBp-transformer: Modelling long-range con-\ntext via binary partitioningâ€. In:arXiv preprint arXiv:1911.04070\n(2019).\n[84] Xueyan Yin et al. â€œSTNN: A Spatial-Temporal Graph Neu-\nral Network for Traffic Predictionâ€. In: 2021 IEEE 27th In-\nternational Conference on Parallel and Distributed Systems\n(ICPADS). IEEE. 2021, pp. 146â€“152.\n[85] Bing Yu, Haoteng Yin, and Zhanxing Zhu. â€œSpatio-Temporal\nGraph Convolutional Networks: A Deep Learning Frame-\nwork for Traffic Forecastingâ€. In: Proceedings of the Twenty-\nSeventh International Joint Conference on Artificial Intelli-\ngence (July 2018). doi: 10.24963/ijcai.2018/505. url: http:\n//dx.doi.org/10.24963/ijcai.2018/505.\n[86] Manzil Zaheer et al. â€œBig Bird: Transformers for Longer\nSequences.â€ In: NeurIPS. 2020.\n[87] Ailing Zeng et al. Are Transformers Effective for Time Series\nForecasting? 2022. doi: 10.48550/ARXIV.2205.13504. url:\nhttps://arxiv.org/abs/2205.13504.\n[88] George Zerveas et al. â€œA Transformer-based Framework for\nMultivariate Time Series Representation Learningâ€. In:arXiv\npreprint arXiv:2010.02803 (2020).\n[89] Hang Zhang et al. Poolingformer: Long Document Modeling\nwith Pooling Attention . 2021. arXiv: 2105.04371 [cs.CL].\n[90] Chuanpan Zheng et al. â€œGman: A graph multi-attention net-\nwork for traffic predictionâ€. In: Proceedings of the AAAI con-\nference on artificial intelligence . Vol. 34. 01. 2020, pp. 1234â€“\n1241.\n[91] Chuanpan Zheng et al.Spatio-Temporal Joint Graph Convolu-\ntional Networks for Traffic Forecasting . 2021. arXiv: 2111.13684\n[cs.LG].\n[92] Haoyi Zhou et al. â€œInformer: Beyond efficient transformer\nfor long sequence time-series forecastingâ€. In: Proceedings of\nAAAI. 2021.\n[93] Tian Zhou et al.FEDformer: Frequency Enhanced Decomposed\nTransformer for Long-term Series Forecasting . 2022. doi: 10.\n48550/ARXIV.2201.12740. url: https://arxiv.org/abs/2201.\n12740.\n[94] Wangchunshu Zhou et al.Scheduled DropHead: A Regulariza-\ntion Method for Transformer Models . 2020. arXiv: 2004.13342\n[cs.CL].\n[95] Chen Zhu et al. Long-Short Transformer: Efficient Transform-\ners for Language and Vision . 2021. arXiv: 2107.02192[cs.CV].\nA ADDITIONAL BACKGROUND AND\nRELATED WORK\nA.1 Transformers and Self Attention\nThe Transformer [65] is a deep learning architecture for sequence-\nto-sequence prediction that is widely used in natural language\nprocessing (NLP) [12]. Transformers operate on two sequences of\nğ‘‘-dimensional vectors, represented in matrix form as X âˆˆRğ¿ğ‘¥ Ã—ğ‘‘\nand Z âˆˆRğ¿ğ‘§ Ã—ğ‘‘, where ğ¿ğ‘¥ and ğ¿ğ‘§ are sequence lengths. The primary\ncomponent of the model is the attention mechanism that updates\nthe representation of tokens in X with information from Z. Tokens\nin Z are mapped to key vectors with learned parametersğ‘Šğ¾, while\ntokens in X generate query vectors withğ‘Šğ‘„. The dot-product simi-\nlarity between query and key vectors is re-normalized to determine\nthe attention matrix ğ´(X,Z)âˆˆ Rğ¿ğ‘¥ Ã—ğ¿ğ‘§ :\nğ´(X,Z)= softmax\n\u0012ğ‘Šğ‘„(X)(ğ‘Šğ¾(Z))ğ‘‡\nâˆš\nğ‘‘\n\u0013\n(1)\nAs mentioned in the main text, our method focuses on the inter-\npretation of attention as a form of message passing along a dynam-\nically generated adjacency matrix [28, 42, 86], where ğ´(X,Z)[ğ‘–,ğ‘— ]\ndenotes the strength of the connection between the ğ‘–th token in X\nand the ğ‘—th token in Z. The information passed along the edges of\nthis graph are value vectors of sequence Z, generated with parame-\nters ğ‘Šğ‘‰. We create a new representation of sequence X according\nto the attention-weighted sum of ğ‘Šğ‘‰(Z):\nAttention(X,Z)âˆˆ Rğ¿ğ‘¥ Ã—ğ‘‘ := ğ´(X,Z)ğ‘Šğ‘‰(Z) (2)\nLet Z correspond to the context sequence of the time series\nforecasting framework (Sec. 2.2) and X be the target sequence. In\nan encoder-decoder Transformer, a stack of consecutive encoder\nlayers observe the context sequence and perform self-attention\nbetween that sequence and itself (Attention(Z,Z)), leading to an\nupdated representation of the context sequence Zâ€². Decoder layers\nprocess the target sequence X and alternate between self-attention\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\n(Attention(X,X)) and cross-attention with the output of the en-\ncoder (Attention(X,Zâ€²)). Each encoder and decoder layer also in-\ncludes normalization, fully connected layers, and residual connec-\ntions applied to each token [65, 78]. The output of the last decoder\nlayer is passed through a linear transformation to generate a se-\nquence of predictions.\nA.2 Spatial-Temporal Graph Neural Networks\nHere we expand upon the discussion of spatial-temporal related\nwork summarized by Section 2.3. The method proposed in this pa-\nper is focused on â€œspatiotemporal\" forecasting, where we learn both\nspatial relationships amongst multiple variables and temporal rela-\ntionships across multiple timesteps. There are countless papers on\nsequence models for forecasting that learn representations across\ntime, and a growing literature on Graph Neural Network (GNN)\nmodels that learn representations across space. At the intersection\nof seq2seq TSF and GNNs is the spatial-temporal GNN (ST-GNN)\nliterature centered around graph operations over a sequence of\nvariable nodes. The spatial-temporal literature can be difficult to\ncategorize due to heavy overloading of vocabulary like â€œspatial\nattention\" and concurrent publication of similar methods. In this\nsection, we try to abstract away implementation details and catego-\nrize existing works based on the ways they learn representations\nof multivariate data.\nWe begin with non-GNN seq2seq time series models as a point of\nreference. Informer, for example, groups ğ‘ variables per timestep\ninto ğ‘‘ dimensional vector representations, and then uses self atten-\ntion to share information between timesteps (Sec. 2.2). This makes\nmodels like Informer and later variants (Autoformer, ETSFormer,\netc.) purely temporal methods. The Temporal category is not spe-\ncific to self attention but to any method that learns patterns across\ntime. For example, the LSTM model used in our experiments learns\ntemporal patterns by selectively updating a compressed represen-\ntation of past timesteps given the current timestep.\nST-GNNs reformulate multivariate forecasting as the prediction\nof a sequence of graphs. A graph at timestep ğ‘¡, denoted Gğ‘¡, has\na set of ğ‘ nodes (Vğ‘¡) connected by edges ( Eğ‘¡). The ğ‘–th node ğ‘£ğ‘–\nğ‘¡\ncontains a vector of values/attributes ğ‘¦ğ‘–\nğ‘¡, with the attributes of\nall ğ‘ nodes represented by a matrix ğ‘Œğ‘¡. Nodes are connected by\nweighted edges that can be represented by an adjacency matrix\nAğ‘¡ âˆˆRğ‘Ã—ğ‘. In traffic forecasting, for example, nodes are road\nsensors with the current traffic velocity as attributes and weighted\nedges corresponding to road lengths. Using graph notation, the fore-\ncasting problem can be written as the prediction of future node val-\nues [ğ‘Œğ‘‡+1,ğ‘Œğ‘‡+2,...,ğ‘Œ ğ‘‡+â„]given previous graphs [Gğ‘‡âˆ’ğ‘,..., Gğ‘‡],\nwhere â„is the horizon and ğ‘ is the context length. In practice, we\ntypically look to learn a parameterized function ğ‘“ğœƒ from past node\nvalues and a fixed (static) adjacency matrix:\n[ğ‘Œğ‘‡+1,ğ‘Œğ‘‡+2,...,ğ‘Œ ğ‘‡+â„]= ğ‘“ğœƒ([ğ‘Œğ‘‡âˆ’ğ‘,...,ğ‘Œ ğ‘‡; A]) (3)\nTraffic Transformer[6] is a prototypical example of using the\ngraph-based formulation with self-attention components. Traffic\nTransformer uses the predefined adjacency matrix A to perform\na graph convolution, sharing information between the nodes of\neach timestep according to the hard-coded spatial relationships.\nThe node representations are then passed through a more standard\nTransformer component that handles temporal learning. The overall\narchitecture is one example of the â€œspatial, then temporal\" pattern\nthat is common in ST-GNNs, and is depicted in Figure 1c. Once\nagain, this framework does not require an attention mechanism that\nlooks like a Transformer - it just needs a way to share information\nacross space and time in an alternating fashion.DCRNN, for example,\nis a foundational work in this literature that merges diffusion based\ngraph layers with the recurrent temporal processing of an RNN.\nOne drawback of graph convolutions on hardcoded adjacency\nmatrices is their inability to adapt spatial connections for specific\npoints in time. In our traffic forecasting example, roadway connec-\ntions may change due to closure, accidents, or high volume. More\nformally, some domains require an extension of Eq. 3 to a dynamic\nsequence of adjacency matrices:\n[ğ‘Œğ‘‡+1,...,ğ‘Œ ğ‘‡+â„]= ğ‘“ğœƒ([(ğ‘Œğ‘‡âˆ’ğ‘,Ağ‘‡âˆ’ğ‘),..., (ğ‘Œğ‘‡,Ağ‘‡)]) (4)\nHowever it is rare to have such a sequence available, so practical\nimplementations often rely on attention mechanisms to re-weight\ntheir spatial relationships based on time and node attributes, as in\nASTGCN [23]. This can allow for some dynamic adaptation of the\nspatial graph, although we are still unable to learn edges that were\nnot provided in advance. GMAN [90] enables a fully dynamic spatial\ngraph by using A to create a spatiotemporal embedding for each\nnode ğ‘£ âˆˆVğ‘¡. Spatial attention modules based on these embeddings\ncombine with more typical temporal self attention for accurate\npredictions.\nWhile adjacency matrices may be available for traffic forecast-\ning where road networks are clearly defined, many multivariate\ndomains have unknown spatial relationships that need to be discov-\nered from data. One approach - used byGraph WaveNet[76], MTGNN\n[75], AGCRN [2], and others - is to randomly initialize trainable node\nembeddings and use the similarity scores between them to construct\na learned adjacency matrix. However, these graphs are updated\nby gradient descent and are then static after training is complete.\nMethods like STAWnet [64] learn dynamic graphs by making the\nspatial relationships dependent on both the node embeddings and\nthe time/value of the current input.\nMore recent work takes spatial-temporal learning a step further.\nRather than alternating between spatial and temporal layers, fully\nspatiotemporal methods spread information across space and time\njointly by adding edges between the graphs of multiple timesteps.\nWhen using alternating spatial/temporal layers, information from\npast timesteps of neighboring nodes must take an indirect route\nthrough the representation of another node. In other words, a spa-\ntial layer must store irrelevant information in a node just so that it\ncan be moved by a temporal layer to the timestep where it is rele-\nvant and vice versa. This effect is most evident in attention-based\nmodels where two-step message passing relies on the queries/keys\nof unrelated nodes, but also occurs in recurrent/convolutional mod-\nels due to the need to compress information into a fixed amount\nof space. A illustrative example using self-attention terminology is\nprovided in Figure 5. STSGCN [34], STJGCN [91], and TraverseNet\n[77] expand their predefined graphs by connecting node neighbor-\nhoods across short segments of time. Spacetimeformer processes\njoint spatiotemporal relationships across a learnable dynamic graph,\nproviding the most flexible and assumption-free combination of the\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nST-GNN literature. We accomplish this by leveraging efficient at-\ntention mechanisms to dynamically generate the adjacency matrix\nof a densely connected graph between all nodes at all timesteps.\nA\nC D\nByt\nyt+1\nyt+1\nyt\nC Spatial Attn\nTemporal, then SpatialSpatial, then Temporal\nSpatiotemporal\n0 0\n11\nFigure 5: â€œSpatial and Temporal\" vs. Spatiotemporal Atten-\ntion. We depict a two variable series with purple (top) and\nred (bottom) variable sequences. Nodes corresponding to\ntimesteps ğ‘¡and ğ‘¡+1 and have been labeled A, B, C, D for sim-\nplicity. Suppose that the information at node A is important\nto the representation we want to produce at node D. Alter-\nnating temporal and spatial layers force this information to\ntake an indirect route through the attention mechanism of\nan unrelated token (e.g., B or C). The A â†’D path is then de-\npendent on the similarity of B and A or C and A, as well as\nany other tokens involved in Bâ€™s temporal attention or Câ€™s\nspatial attention. In contrast, true spatiotemporal attention\ncreates a direct path A â†’D with no risk of information loss.\nIn summary, the ST-GNN literature has three key methodological\ndifferences that define modelsâ€™ flexibility and accuracy:\n(1) The type of graph used in learning. Are we performing spa-\ntial and temporal updates in an alternating fashion, or can\nwe learn relationships across space and time jointly with a\ntrue spatiotemporal graph?\n(2) The requirement of a predefined adjacency matrix based on\nknown relationships between nodes.\n(3) The ability to dynamically adapt spatial relationships accord-\ning to the current timestep and node values.\nWe categorize related work according to these characteris-\ntics in Table 6 . For more background on ST-GNNs, see [5].\nA.3 Connection to Vision Transformers\nSpacetimeformerâ€™s architecture and embedding scheme prompt\nsome interesting parallels with work on Transformers in computer\nvision. Both domains involve two dimensional data (rows/columns\nof pixels in vision and space/time in forecasting). In fact, another\nway to look at the standard multivariate forecasting problem (Sec.\n2.2) is as the completion of the rightmost columns of a grayscale\nimage given the leftmost columns. We let ğ‘¥ be the scalar index\nof a column and ğ‘¦ be the vector of pixel values for that column.\nThe number of variables corresponds to the number of rows in the\nimage. An example of using Spacetimeformer to complete MNIST\nimages given the first 10 columns of pixels is shown in Figure 6.\nOur model solves this problem with the same approach used in all\nthe other forecasting results presented in this paper. The image-\ncompletion perspective can be an intuitive way to identify the\nproblem with standard â€œtemporal\" attention in TSF Transformers.\nIf faced with the image completion task in Figure 6, we would be\nunlikely to try and model all of the rows of the image together and\nperform attention over columns - but that is exactly what models\nlike Informer end up doing. Pixel shapes can have complex local\nstructure and each region should have the freedom to attend to\ndistinct parts of the image that are most relevant. We run similar\nspatiotemporal (Spacetimeformer) and temporal ( Temporal) ar-\nchitectures on MNIST and see an 8% reduction in prediction error.\nHowever, MNIST is a simple dataset and we would expect a much\nlarger gap in performance on higher-resolution images.\nThe key difference between image and time series data is that\nboth dimensions of vision data have meaningful order while the\norder of our spatial axis must be assumed to be arbitrary. This\ncan limit our ability to apply common vision techniques to reduce\nsequence length. For example, the original Vision Transformer [14]\nestablished a convention of â€œpatching\" the image into a grid of small\n(16 Ã—16) pixel squares. The input to the Transformer then becomes\na feed-forward layerâ€™s projection of regions of pixels rather than\nevery pixel individually - dramatically reducing the input length\nof the self-attention mechanism. Square patches of multivariate\ntime series data would arbitrarily group multiple variables together\nbased on the order their columns appeared in the dataset. However,\nwe do have the option to patch along rectangular ( 1 Ã—ğ‘˜) slices\nof time. The initial convolution approach (Sec. 3.3) used on large\ndatasets can be seen as a kind of overlapping patching to reduce\nsequence length.\nAnother common theme in Vision Transformer work is the com-\nbination of attention and convolutions. Convolutions provide an\narchitectural bias towards groups of adjacent pixels while attention\nallows for global connections even in shallow layers where convolu-\ntional receptive fields are small [37]. Spacetimeformer â€œintermedi-\nate convolutions\" (Sec. 3.3) rearrange the flattened spatiotemporal\nattention sequence to a Conv1D input format to get a similar effect.\nSome Vision Transformers create a bias towards nearby pixels with\nefficient attention mechanisms that resemble convolutions over lo-\ncal regions but form sparse hierarchies across the full image [13, 10,\n67]. Spacetimeformerâ€™s local attention layers can be interpreted\nas a version of this approach. However, our model also contains\ntrue global layers that would be the equivalent of attention between\neach pixel and every other pixel in an image - something that is not\nusually attempted in vision architectures. An interesting technique\nrelated to the local vs. global approach and convolutional networks\nis Shifted Window Attention [44]. Attention is performed only\namongst the pixels or patches of a â€œwindow\" or neighborhood of\npixels, but the window boundaries are redrawn in each layer so that\ninformation can spread between distant windows as the network\ngets deeper. This is directly analogous to the expanding receptive\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nMethod Message Passing\nType\nPredefined\nSpatial Graph\nDynamic\nSpatial Graph\nInformer [92] Temporal âœ— âœ—\nTFT [38] Temporal âœ— âœ—\nDCRNN [36] Spatial + Temporal âœ“ âœ—\nTSE-SC [6] Spatial + Temporal âœ“ âœ—\nPVCGN [40] Spatial + Temporal âœ“ âœ—\nMulti-STGCnet [81] Spatial + Temporal âœ“ âœ—\nSTFGNN [61] Spatial + Temporal âœ“ âœ—\nASTGCN [23] Spatial + Temporal âœ“ âœ“ -\nST-GRAT [51] Spatial + Temporal âœ“ âœ“ -\nDMGCRN [53] Spatial + Temporal âœ“ âœ“ -\nSTWave [18] Spatial + Temporal âœ“ âœ“ -\nMGT [82] Spatial + Temporal âœ“ âœ“ -\nSTTN [80] Spatial + Temporal âœ“ âœ“\nSTIDGCN [39] Spatial + Temporal âœ“ âœ“\nGMAN [90] Spatial + Temporal âœ“ âœ“\nSTNN [84] Spatial + Temporal âœ“ âœ“\nGraph WaveNet [76] Spatial + Temporal âœ— âœ—\nMTGNN [75] Spatial + Temporal âœ— âœ—\nAGCRN [2] Spatial + Temporal âœ— âœ—\nDSTAGNN [33] Spatial + Temporal âœ— âœ“-\nST-WA [11] Spatial + Temporal âœ— âœ“\nTVGCN [68] Spatial + Temporal âœ— âœ“\nSTAAN [69] Spatial + Temporal âœ— âœ“\nTPA-LSTM [59] Spatial + Temporal âœ— âœ“\nSTAWNet [64] Spatial + Temporal âœ— âœ“\nSTAM [19] Spatial + Temporal âœ— âœ“\nSTSGCN [34] Short-Term Spatiotemporal âœ“ âœ—\nTraverseNet [77] Short-Term Spatiotemporal âœ“ âœ“ -\nSTJGCN [91] Short-Term Spatiotemporal âœ“ âœ“\nSpacetimeformer Spatiotemporal âœ— âœ“\nTable 6: Spatial-Temporal forecasting related work categorized by graph type (Fig 5), the requirement of a hard-coded variable\ngraph, and the ability to dynamically adapt spatial relationships across time. â€œShort-Term Spatiotemporal\" refers to spatiotem-\nporal graphs that are restricted to a short range of timesteps. The â€œ âœ“-\" rating in the dynamic spatial graph column indicates\nmodels that re-weight their adjacency matrix without creating new connections.\nfield that results from down-sampled convolutional architectures.\nSpacetimeformer implements shifted windowed attention in one\ndimension where neighborhoods of data are defined by slices in\ntime. This mechanism is not used in the primary experimental re-\nsults because fast attention provides sufficient memory savings.\nIn general, the overlap between vision and time series techniques\nappears to be a promising direction for the future scalability of\nspatiotemporal forecasting. Our public codebase provides fast setup\nfor time series models to train on the MNIST task in Fig. 6. We also\ninclude a more difficult CIFAR-10 task where the imagesâ€™ rows and\ncolumns have been flattened into a sequence with three variables\ncorresponding to the red, green, and blue color channels.\n Figure 6: Image completion as a multivariate forecasting\nproblem. Spacetimeformer learns to complete images given\nthe leftmost columns.\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nB ADDITIONAL RESULTS AND DETAILS\nB.1 Real-World Dataset Details\nDetails of the real-world datasets used in our experimental results\nare listed below. Descriptions of toy datasets are deferred to the\nappendix sections where their results are discussed. We try to follow\nthe train/val/test splits of prior work where possible to ensure a\nfair comparison. Splits are often determined by dividing the series\nby a specific point in time, so that the earliest data is the train set\nand the most recent data is the test set. This evaluation scheme\nhelps measure the future predictive power of the model and is\nespecially important in non-stationary datasets [7]. Datasets are\neither released directly with our source code or made available for\ndownload in the proper format.\nVariables\n(ğ‘)\nLength\n(ğ¿)\nSize\n(Timesteps)\nNY-TX Weather 6 800 569,443\nAL Solar 137 192 52,560\nMetr-LA Traffic 207 24 34,272\nPems-Bay Traffic 325 24 52,116\nETTm1 7 1344 69,680\nHZMetro 160 8 1650\nWeather 21 1440 52,696\nTable 7: Real-World Dataset Summary. Sequence length (ğ¿) is\nreported as the largest combined length of the context and\ntarget windows used in results.\nâ€¢NY-TX Weather (new). We obtain hourly temperature readings\nfrom the ASOS weather network. We use three stations located in\ncentral Texas and three more located hundreds of miles away in\neastern New York. The data covers the years1949âˆ’2021, making\nthis a very large dataset by TSF standards. Many of the longest\nactive stations are airport weather stations. We use the airport\ncodes ACT (Waco, TX), ABI (Abilene, TX), AMA (Amarillo, TX),\nALB (Albany, New York) as well as the two largest airports in the\nNew York City metro, LGA and JFK. The NY-TX benchmark was\ncreated for this paper, and has two key advantages when evaluat-\ning timeseries Transformers. First, it has far more timesteps than\npopular datasets; Transformers are data-hungry, but we can more\nsafely ignore overfitting issues here. Second, this dataset is almost\nperfectly stationary; popular datasets are not, and this creates\nan evaluation problem that is not related to the modeling power\nof the architecture. This issue has been widely overlooked and\nallows basic linear models that are more robust to non-stationary\nto outperform recent methods that are more powerful on paper.\nFor more on this see [87] and Appendix B.4.\nâ€¢Metr-LA Traffic [36]. A popular benchmark dataset in the GNN\nliterature consisting of traffic measurements from Los Angeles\nhighway sensors at 5-minute intervals over 4 months in 2012.\nBoth the context and target sequence lengths are set to 12. We\nuse the same train/test splits as [36].\nâ€¢AL Solar [32] . A popular benchmark dataset in the time series\nliterature consisting of solar power production measurements\nacross the state of Alabama in 2006. We use a context length of\n168 and a target length of 24.\nâ€¢Pems-Bay Traffic [36] . Similar to Metr-LA but covering the\nBay Area over 6 months in 2017. The context and target lengths\nare 12 and we use the standard train/test split.\nâ€¢HZMetro [40]. A dataset of the number of riders arriving and\ndeparting from 80 metro stations in Hangzhou, China during\nthe month of January 2019. The (ğ¿Ã—ğ‘,1)input format of our\nmodel requires the arrivals and departures to be separated into\ntheir own variable nodes, leading to a total of 160 variables. The\ncontext and target windows are set to a length of4 as in previous\nwork.\nâ€¢ETT [92] . An electricity transformer dataset covering 2016 âˆ’\n2018. We use the ETTm1 variant which is logged at one minute\nintervals to provide as much data as possible. We evaluate on the\nset of target sequence lengths {24,48,96,288,672}established\nby Informer.\nâ€¢Weather [92]. A German dataset of21 weather-related variables\nrecorded every 10 minutes in 2020. We use the same set of target\nsequence lengths and context window selection approach as in\nETTm1.\nB.2 Code and Implementation Details\nThe code for our work is open-sourced and available on GitHub at\nQData/spacetimeformer. All models are implemented in PyTorch\n[52] and the training process is conducted with PyTorch Lightning\n[16]. Our LSTNet and MTGNN implementations are based on public\ncode [56] and verified by replicating experiments from the original\npapers. Generic models like LSTM and LinearAR are implemented\nfrom scratch and we made an effort to ensure the results are com-\npetitive. The code for the Spacetimeformer model was originally\nbased on the Informer open-source release.\nData Preprocessing . As mentioned in the previous section,\ntrain/val/test splits are based on existing work or determined by a\ntemporal splits where the most recent data forms the test set. Data\nis organized into time sequences ( ğ‘¥) and variable values (ğ‘¦). We\nfollow established variable normalization schemes of prior work to\nensure a fair comparison, and default to z-score normalization in\nother cases. Most real-world datasets use ğ‘¥ values that correspond\nto date/time information. We represent calendar dates by splitting\nthe information into separate year, month, day, hour, minute, and\nsecond values and then re-scaling each to be âˆˆ[0,1]. This works\nout so that only the year value is unbounded, but we divide by the\nlatest year present in the training set. We discard time variables\nthat are not robust or prone to overfitting. For example, a dataset\nthat only spans two months would drop the month and year values.\nEmbedding. The time variablesğ‘¥are passed through aTime2Vec\nlayer [29]. If ğ‘¥ is a time representation with three elements {hour,\nminute, second}, the Time2Vec output would be shape (3,ğ‘˜)where\nğ‘˜ is the time embedding dimension. The first of the ğ‘˜ elements has\nno activation function while the remaining ğ‘˜âˆ’1 use a sine func-\ntion with trainable parameters to represent periodic patterns. After\nflattening (3,ğ‘˜)â†’( 3 Ã—ğ‘˜,), the time embedding is concatenated\nwith its corresponding ğ‘¦ value. When using our spatiotemporal\nembedding, the y value will be a single scalar and time values are\nduplicated to account for the flattened sequence. Note that we use\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nterminology like â€œflatten\" because most datasets in practice are\nstructured such that there are ğ‘ variables sampled at the same\nmoment in time, and the conversion to the spatiotemporal format\nlooks like we have laid out the rows of a dataframe end-to-end.\nHowever, embedding the value of each variable at every timestep\nas its own separate token gives us a lot of flexibility to use alternate\ndataset formats where variables may be sampled at different inter-\nvals. We do not take advantage of this in our experimental results\nbecause it is not relevant to common benchmark datasets and is\nnot applicable to all the baselines we consider.\nThe combined value and time are projected to the Transformer\nmodel dimension (ğ‘‘) with a single feed-forward layer. We experi-\nmented with two approaches to the position embedding. The first re-\npurposes Time2Vec to learn a periodic ğ‘‘-dimensional embeddings -\nessentially learning the frequency hyperparameters of the original\nfixed position embedding [65] automatically. We also experimented\nwith fully learnable lookup-table-style position embeddings com-\nmonly used for word embeddings in natural language processing.\nBoth are provided in the code and appeared to lead to similar perfor-\nmance. However, we decided that the fully learnable option was the\nsafer choice to make sure the position embedding had the freedom\nto differentiate itself from the other components that make up our\nembedding scheme. Position indices are repeated as necessary to\naccount for the flattened ğ‘¦values. Variable embeddings are imple-\nmented similarly with indices assigned arbitrarily from 0,...,ğ‘ .\nThe repeating pattern of tokensâ€™ variable and position indices is\nbest explained by Figure 2.\nâ€œGiven\" embeddings are a third lookup-table layer with two en-\ntries that indicate whether the ğ‘¦value for a token contains missing\nvalues. They are only used in the encoderâ€™s embedding layer be-\ncause all of the values are missing/empty in the decoder sequence.\nThe value+time, variable, position, and given embeddings sum to\ncreate the final ğ‘‘-dimensional embedding.\nArchitecture and Training Loop. There is significant empiri-\ncal work investigating technical improvements to the Transformer\narchitecture and training routine [25, 41, 73, 94, 1, 17]. We incorpo-\nrate some of these techniques to increase performance and hyperpa-\nrameter robustness while retaining simplicity. A Pre-Norm architec-\nture [78] is used to forego the standard learning rate warmup phase.\nWe also find that replacing LayerNorm with BatchNorm is advanta-\ngeous in the time series domain. [58] argue thatBatchNorm is more\npopular in computer vision applications because reduced training\nvariance improves performance over the LayerNorms that are the\ndefault in NLP. Our experiments add empirical evidence that this\nmay also be the case in time series problems. We also experiment\nwith PowerNorm [58] and ScaleNorm [47] layers with mixed results.\nAll four variants are included as a configurable hyperparameter in\nour open-source release.\nThe encoder and decoder have separate embedding layers to\nenable the context sequence to contain a different set of input vari-\nables than those we are forecasting in the target sequence. Dropout\ncan be applied to: 1) query, key, and value layer parameters. 2) The\noutput of embedding layers. 3) The attention matrix (when applica-\nble). 4) The feed-forward layers at the end of each encoder/decoder\nlayer. Local attention layers are implemented withrearrange-style\noperations [55] and can be used with any efficient attention variant.\nThe choice of attention implementation is flexible across the archi-\ntecture, allowing progress in the sub-field of long-range attention\nto improve future scalability. We default to the ReLU version of\nPerformer [9] due to its memory savings and compatibility with\nboth self and cross attention. However, we also experimented with\nNystromformer [79] and ProbSparse attention [92].\nMetrics. For completeness, the evaluation metrics used in our\nresults tables are listed below.ğ‘¦ğ‘›\nğ‘¡ and ^ğ‘¦ğ‘›\nğ‘¡ correspond to the true and\npredicted values of the ğ‘›th variable at timestep ğ‘¡, respectively. Â¯ğ‘¦is\nshorthand for the mean value of ğ‘¦.\nMSE := 1\nâ„ğ‘\nğ‘âˆ‘ï¸\nğ‘›=1\nğ‘‡+â„âˆ‘ï¸\nğ‘¡=ğ‘‡\n(ğ‘¦ğ‘›\nğ‘¡ âˆ’^ğ‘¦ğ‘›\nğ‘¡ )2 (5)\nMAE := 1\nâ„ğ‘\nğ‘âˆ‘ï¸\nğ‘›=1\nğ‘‡+â„âˆ‘ï¸\nğ‘¡=ğ‘‡\n(ğ‘¦ğ‘¡ âˆ’^ğ‘¦ğ‘¡) (6)\nRMSE :=\nvut\n1\nâ„ğ‘\nğ‘âˆ‘ï¸\nğ‘›=1\nğ‘‡+â„âˆ‘ï¸\nğ‘¡=ğ‘‡\n(ğ‘¦ğ‘¡ âˆ’^ğ‘¦ğ‘¡)2 (7)\nRRSE :=\nvutÃğ‘\nğ‘›=1\nÃğ‘‡+â„\nğ‘¡=ğ‘‡ (ğ‘¦ğ‘¡ âˆ’^ğ‘¦ğ‘¡)2\nÃğ‘\nğ‘›=1\nÃğ‘‡+â„\nğ‘¡=ğ‘‡ (ğ‘¦ğ‘¡ âˆ’Â¯ğ‘¦)2 (8)\nMAPE := 1\nâ„ğ‘\nğ‘âˆ‘ï¸\nğ‘›=1\nğ‘‡+â„âˆ‘ï¸\nğ‘¡=ğ‘‡\n\f\f\f\f\n(ğ‘¦ğ‘¡ âˆ’^ğ‘¦ğ‘¡)\nğ‘¦ğ‘¡\n\f\f\f\f (9)\nThe main detail to note here is that we are reporting the average\nover the length of the forecast sequence ofâ„timesteps. However, it\nis somewhat common (especially in the ST-GNN datasets) to report\nmetrics for multiple timesteps independently to see how accuracy\nchanges as we get further from the known context. We take the\nmean of all reported timesteps to recover the metrics of related\nwork for Tables 4 and 5. All metrics discard missing values.\nAs mentioned in Sec 3.4, the output of our model is a sequence\nof predictions that can be restored to the original (ğ¿,ğ‘ ) input for-\nmat. This lets us choose from several different loss functions. Most\ndatasets use either Mean Squared Error (MSE) or Mean Absolute\nError (MAE). We occasionally compare Root Relative Squared Error\n(RRSE) and Mean Absolute Percentage Error (MAPE) - though they\nare never used as an objective function. We train all models with\nearly stopping and learning rate reductions based on validation\nperformance.\nHyperparameters and Compute. Due to the large number of\nexperiments and baselines used in our results, we choose to defer\nhyperparamter information to the source code by providing the nec-\nessary training commands. This lets us include the settings of minor\ndetails that have not been explained in writing.Spacetimeformerâ€™s\nlong-sequence attention architecture is mainly constrained by GPU\nmemory. Most results in this paper were gathered on 1 âˆ’4 12GB\ncards, although larger A100 80GB GPUs were used in some later\nresults (e.g., Pems-Bay). We hope to provide training commands\nthat serve as a competitive alternative for resource-constrained set-\ntings. Training is relatively fast due to small dataset sizes, with the\nlongest results being Metr-LA and Pems-Bay at roughly 5 hours.\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nB.3 Toy Experiments\nShifted Copy Task. Copying binary input sequences is a common\ntest of long-sequence or memory-based models [21]. We generate\nbinary masks with shape (ğ¿,ğ‘ ), where elements are set to 1 with\nsome probability ğ‘. Each of the ğ‘ variables is associated with\na â€œshift\" value. The target sequence is a duplicate of the context\nsequence with each variable zero-padded and translated forward\nin time by its shift value. An example with ğ¿ = 100, ğ‘ = 5, and\nshifts of {0,5,10,15,25}is shown in Figure 7. The rows have been\ncolor-coded to make the shift easier to identify.\nFigure 7: Shifted Copy Task. Copy a binary sequence with\neach row shifted by increasing amounts top to bottom (col-\norized for visualization).\nFirst we train a standard (â€œTemporal\") Transformer on an eight\nvariable version of the shifted copy task. An example of the correct\noutput (bottom) and predicted sequence (top) is shown in Figure 8.\nWhile the first few rows appear to be accurately copied, the lower\nvariable outputs are blurry or missing entirely.\nFigure 8: Temporal attention discrete copy example. Top im-\nage shows predicted output sequence; bottom shows ground-\ntruth output sequence. Standard attention forces each col-\numn to share attention graphs, leading to a â€œblurry\" copy\noutput on variables with high shift.\nTo try and understand where the model is going wrong, we\nvisualize the cross attention patterns of several heads in the first\ndecoder layer. The results are shown in Figure 9. To perform this\ntask correctly, the tokens of each element in the target sequence\nneed to learn to attend to a specific position in the context sequence.\nThe problem is that each variable needs to attend to different posi-\ntions due to their temporal shift. The standard Transformer groups\nall of the variables into the same token, making it very difficult to\npick timesteps to attend to that are useful for all of them. Instead it\ncorrectly learns the shift of a single variable per attention head and\neventually runs out of heads, leading to the inaccurate results in\nthe bottom half of the output. Note that in this problem the correct\nattention pattern would form a line where the distance from the\ndiagonal corresponds to a variableâ€™s shift value.\nNext we run the same Transformer with spatiotemporal atten-\ntion and see a roughly 17Ã—reduction in MAE , with outputs that are\nnear-perfect reproductions of the input (Figure 10). By flattening\nthe context and target tokens into a spatiotemporal graph, the vari-\nables of each timestep are given independent attention mechanisms.\nEach attention head is capable of learningğ‘2 relationships between\nvariables. Of course, this toy problem only has ğ‘ important spatial\nrelationships (each variable only needs to attend to itself). Figure\n11 shows the spatiotemporal pattern with attention shifting along\nthe diagonal.\nDependent Sine Waves. Next we recreate a version of the toy\ndataset used to emphasize the necessity of spatial modeling in [59].\nWe generate ğ· sequences where sequence ğ‘–at timestep ğ‘¡ is defined\nby:\nğ‘Œğ‘–\nğ‘¡ = ğ‘ ğ‘–ğ‘›(2ğœ‹ğ‘–ğ‘¡\n64 )+ 1\nğ·+1\nğ·âˆ‘ï¸\nğ‘—=1,ğ‘—â‰ ğ‘–\nğ‘ ğ‘–ğ‘›(2ğœ‹ğ‘—ğ‘¡\n64 ) (10)\nWe map 2,000 timesteps to a sequence of daily calendar dates\nbeginning on Jan 1, 2000. We set ğ· = 20 and use a context length\nof 128 and a target length of 32. The final quarter of the time series\nis held out as a test set.\nSeveral ablations of our method are considered. Temporal modi-\nfies the spatiotemporal embedding as discussed at the end of Sec\n3.4. ST Local skips the global attention layers but includes spa-\ntial information in the embedding. The â€œDeeper\" variants attempt\nto compensate for the additional parameters of the local+global\nattention architecture of our full method. All models use a small\nTransformer model and optimize for MSE. The results are shown\nin Table 8. The Temporal embedding is forced to compromise its\nattention over timesteps in a way that reduces predictive power\nover variables with such distinct frequencies. Standard (â€œFull\") at-\ntention fits in memory with the Temporal embedding but is well\napproximated by Performer. Our method learns an uncompromis-\ning spatiotemporal relationship among all tokens to generate the\nmost accurate predictions by all three metrics.\nTemporalTemporal\n(Deeper)\nTemporal\n(Deeper&\nFull Attn)\nST LocalST Local\n(Deeper)Spacetimeformer\nMSE 0.006 0.010 0.005 0.021 0.014 0.003\nMAE 0.056 0.070 0.056 0.104 0.090 0.042\nRRSE 0.093 0.129 0.094 0.180 0.153 0.070\nTable 8: Toy Dataset Results\nB.4 Distributional Shift and Time Series\nTransformers\nWhile collecting baseline results on ETT and other datasets used\nin recent Transformer TSF work we noticed significant gaps in\nperformance between train and test set prediction error. However,\nthe apparent overfitting effect showed few signs of improvement\nwhen given additional data and smaller, highly-regularized archi-\ntectures. Recently, [87] released an investigation into the failures\nof Transformers in long-term TSF. They show that a simple model,\nâ€œDLinear, \" can outperform Transformers in a variety of common\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nFigure 9: Temporal attention fails with too few heads. Because each variable (row) requires an independent attention graph, a\nstrong optimizer will attempt to put one relationship on each attention head. However, we quickly run out of heads and are\nleft with an inaccurate copy. In a more complex problem, Temporal attention could require as many as ğ‘2 heads to accurately\nmodel every variable relationship.\nFigure 10: Spatiotemporal attention represents ğ‘2 relation-\nships per head. Thanks to its spatiotemporal sequence em-\nbedding, Spacetimeformer can represent the underlying\nvariable relationship in a single accurate head. This capabil-\nity spares room for optimization errors and multiple rela-\ntionships between variables in more complex problems.\nFigure 11: Spacetimeformer outputs an accurate copy (top)\nof the input sequence (bottom) despite the row shift.\nbenchmarks. Their DLinear model is a slightly more advanced ver-\nsion of our LinearAR baseline, which predicts each timestep as a\nlinear combination of the previous timesteps. We verify a similar\nresult with the tiny LinearAR model in Table 9 - nearly matching\nthe performance of ETSFormer on the ETTm1 dataset.\nComparisons between the NY-TX Weather dataset - whereLinearAR\nperforms worse than our Transformer baselines - and ETTm1 show\nthe latter dataset has a much larger gap between the magnitude\nof the train and test sequences. This type of distributional shift is\ncommon in non-stationary time series and is caused by the use of\nthe most recent data as a test set; long-term trends that begin in\nthe train set can continue into the test set and alter the distribution\nof our inputs. We hypothesize that distribution shift is a key part\nof the Transformerâ€™s test set inaccuracy and that linear models are\nless effected because their outputs are a more direct combination of\nthe magnitude of their inputs. Reversible Instance Normalization\n(RevIN) [30] normalizes a modelâ€™s input based on the statistics of\neach context sequence so that its parameters are less sensitive to\nchanges in scale. The predicted output can then be inverse normal-\nized to the original range. We experiment with RevIN as a way to\ncombat distribution shift on both ETTm1 and the popular Weather\ndataset. The results are displayed in Tables 9 and 10. Input normal-\nization improves the performance of all models and even makes\nLSTM competitive with advanced Transformers. This may suggest\nthat the improved performance of recent methods may not be the\nresult of more complex architectures but from improved resistance\nto distributional shift. We investigate this further by implementing\nseasonal decomposition - another common detail of Transformers\nin time series. Seasonal decomposition separates a series into short\nand long-term trends to be processed independently, and is also\nincluded in the DLinear model [87]. We evaluate its impact on the\nWeather dataset in Table 10. Performance is comparable to instance\nnormalization, potentially because decomposition has the effect of\nstandardizing inputs by transforming them into a difference from a\nmoving average. The Spacetimeformer results in Table 2 use both\nseasonal decomposition and reversible instance normalization for\nfurther improved results.\nHowever, input normalization is not always enough, especially\nwhen patterns are in fact scale-dependent. For a brief example\nconsider a continuous version of the copy task in the previous\nsection using sine curves instead of binary masks. We randomly\ngenerate ğ‘ different context and target sequences according to:\nJake Grigsby, Zhe Wang, Nam Nguyen, and Yanjun Qi\nğ‘¦(ğ‘¡)= ğ‘sin(ğ‘ğ‘¡ +ğ‘)+ğ‘‘+ğœ–(ğ‘¡),ğœ–(ğ‘¡)âˆ¼N( 0,.1)\nwhere ğ‘,ğ‘,ğ‘ and ğ‘‘ are parameters drawn from distributions that\ncan vary between the train and test sets. Our experiments show\nthat RevIN-style input normalization can reduce the generalization\ngap for out-of-distribution ğ‘‘to nearly zero for LSTM, temporal and\nspatiotemporal attention models. However, if we shift the curve\nbetween the context and target such that:\nğ‘¦context(ğ‘¡)= ğ‘sin(ğ‘ğ‘¡ +ğ‘)+ğ‘‘+ğœ–(ğ‘¡),ğœ–(ğ‘¡)âˆ¼N( 0,.1)\nğ‘¦target (ğ‘¡)= ğ‘¦ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ‘¡)+( 1 +|ğ‘‘|)2\nRevIN is unable to generalize. This is because when we pass\nour model a normalized input we are discarding the magnitude (ğ‘‘)\ninformation necessary to make accurate predictions. An example\nof a Spacetimeformer+RevIN prediction is plotted in Figure 12.\nThe behavior of many real-world time series also changes with\nthe overall magnitude of the variable and more work is needed to\nmake large Transformer TSF models robust to small, non-stationary\ndatasets.\nFigure 12: Modeling Without Magnitude. Input normaliza-\ntion removes the context information that is needed to\nmake scale-dependent predictions.\nB.5 Ablations\nAblation results are listed in Table 11. During our development\nprocess, we became interested in the amount of spatial information\nthat makes it through the encoder and decoder layers. We created\na way to measure this by adding a softmax classification layer to\nthe encoder and decoder output sequences. This layer interprets\neach token and outputs a prediction for the time series variable it\noriginated from. Importantly, we detach this classification loss from\nthe forecasting loss our model is optimizing; only the classification\nlayer is trained on this objective. Classification accuracy begins\nnear zero but spikes upwards of 99% in all problems within a few\nhundred gradient updates due to distinct (randomly initialized)\nvariable embeddings that are well-preserved by the residual Pre-\nNorm Transformer architecture. In TSF tasks like NY-TX weather,\nthis accuracy is maintained throughout training. However, in more\nPrediction Length\n24 48 96 288 672\nLSTM 0.63 0.94 0.91 1.12 1.56\nRepeat Last 0.54 0.76 0.78 0.83 0.87\nInformer 0.37 0.50 0.61 0.79 0.93\nPyraformer 0.49 0.66 0.71\nYFormer 0.36 0.46 0.57 0.59 0.66\nPreformer 0.40 0.43 0.45 0.49 0.54\nAutoformer 0.40 0.45 0.46 0.53 0.54\nETSFormer 0.34 0.38 0.39 .42 .45\nLSTM + RevIN 0.37 0.44 0.47 0.53 0.55\nTemporal + RevIN .32 0.40 0.44 0.50 0.55\nSpatiotemporal + RevIN 0.34 0.40 0.45 0.50 0.57\nLinearAR 0.33 .37 .39 0.44 0.48\nTable 9: Normalized Mean Absolute Error (MAE) on ETTm1\ntest set. â€œRepeat Last\" is a simple heuristic that predicts the\nmost recent value in the context sequence for the duration\nof the forecast.\nPrediction Length\n96 192 336 720\nInformer 0.38 0.54 0.52 0.74\nLSTM (from Autoformer) 0.41 0.44 0.45 0.52\nAutoformer 0.34 0.37 0.40 0.43\nLinear Shared 0.24 0.29 0.32 0.36\nLinear Ind Decomp 0.22 0.27 0.31 0.36\nLSTM (Ours) 0.30 0.34 0.41 0.53\nLSTM RevIN 0.23 0.26 0.32 0.36\nLSTM Decomp 0.23 0.27 0.30 0.36\nTransformer (Ours) 0.24 0.33 0.40 0.42\nTransformer RevIN 0.23 0.27 0.35 0.37\nTransformer Decomp 0.24 0.27 0.31 0.37\nTable 10: Normalized Mean Absolute Error (MAE) on the\nWeather test set. â€œLinear Ind Decomp\" adds independent pa-\nrameters for each variable to a linear model with seasonal\ndecomposition.\nspatial tasks like Metr-LA, accuracy begins to decline over time.\nBecause this decline corresponds to increased forecasting accuracy,\nwe interpret this as a positive indication that related nodes are\ngrouped with similar variable embeddings that become difficult for\na single-layer classification model to distinguish.\nThe relative importance of space and time embedding infor-\nmation changes based on whether the problem is more temporal\nlike NY-TX Weather or spatial like traffic forecasting. While global\nspatiotemporal attention is key to state-of-the-art performance on\nMetr-LA, it is interesting to see that local attention on its own is\na competitive baseline. We investigate this further by removing\nboth global attention and the variable embeddings that differentiate\ntraffic nodes, which leads to the most inaccurate predictions. This\nLong-Range Transformers for Dynamic Spatiotemporal Forecasting\nimplies that the local attention graph is adapting to each node based\non the static spatial information learned by variable embeddings.\nMAE Classification Acc.\n(%)\nNY-TX Weather (ğ¿= 200,ğ‘ = 6)\nFull Spatiotemporal 2.57 99\nNo Local Attention 2.62 99\nNo Variable Embedding 2.66 45\nTemporal Embedding/Attention 2.67 -\nNo Value Embedding 3.83 99\nNo Time Embedding 4.01 99\nMetr-LA Traffic (ğ¿= 24,ğ‘ = 207)\nFull Spatiotemporal 2.83 58\nNo Global Attention 3.00 46\nNo Time Embedding 3.11 50\nTemporal Embedding/Attention 3.14 -\nNo Local Attention 3.27 54\nNo Variable Embedding 3.29 2\nNo Global Attention, No Variable Emb. 3.48 1\nTable 11: Ablation Results.\nB.6 Future Directions in Multi-series\nGeneralization\nSome time series prediction problems - particularly competitions\nlike the M series [46] and Kaggle - are based on a large collection\nof semi-independent series that do not occur on the same time\ninterval and therefore cannot be cast in the standard multivariate\nformat. For example, we might have the sales data of a collection of\nthousands of different products sold in different stores several years\napart. ML-based solutions use a univariate context/target window\nthat is trained jointly across batches from every series. Implicitly,\nthese models need to infer the behavior of the current series they\nare observing from this limited context. Long-sequence Transform-\ners create an opportunity for genuine â€œin-context\" learning [31, 4]\nin the time series domain, where the context sequence isall existing\ndata for a given varible up until the current moment, and the target\nsequence is all future values we might want to predict. A model\ntrained in this format would have all available information to iden-\ntify the characteristics of the current series and learn generalizable\npatterns for more accurate forecasting. The main implementation\ndifference here is the varying number of samples per series, which\nrequires the use of sequence padding and masking and can limit\nour choice of efficient attention variant. For multi-series problems,\nwe revert to vanilla (quadratic) attention with arbitrary masking,\nand make use of shifted-window attention (Sec. 3.3) to reduce GPU\nmemory usage. Our open-source code release includes fully imple-\nmented training routines for the Monash dataset [20], M4 dataset\n[46], and Wikipedia site traffic Kaggle competition. Preliminary\nresults suggest this method can be competitive with leaderboard\nscores of ML-based approaches on the M4 and Wikipedia compe-\ntitions, and we hope that our codebase can be a starting point for\nfurther development on this topic. The multivariate attention ca-\npabilities of Spacetimeformer allow an extension of meta-TSF to\ndatasets of multiple multivariate time series.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6846439838409424
    },
    {
      "name": "Transformer",
      "score": 0.5877817869186401
    },
    {
      "name": "Multivariate statistics",
      "score": 0.5320656895637512
    },
    {
      "name": "Sequence learning",
      "score": 0.47713518142700195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47248902916908264
    },
    {
      "name": "Sequence (biology)",
      "score": 0.47033318877220154
    },
    {
      "name": "Artificial neural network",
      "score": 0.4588620662689209
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.4363541603088379
    },
    {
      "name": "Machine learning",
      "score": 0.3911808431148529
    },
    {
      "name": "Data mining",
      "score": 0.3230990767478943
    },
    {
      "name": "Mathematics",
      "score": 0.12773650884628296
    },
    {
      "name": "Engineering",
      "score": 0.12180870771408081
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    }
  ],
  "cited_by": 77
}