{
    "title": "The Moral Landscape of General-Purpose Large Language Models",
    "url": "https://openalex.org/W4391219908",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4221741208",
            "name": "Giada Pistilli",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6791555646",
        "https://openalex.org/W2017561954",
        "https://openalex.org/W6621741755",
        "https://openalex.org/W7011547903",
        "https://openalex.org/W4283167130",
        "https://openalex.org/W2101493843",
        "https://openalex.org/W7073618765",
        "https://openalex.org/W2251410821",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4292166681",
        "https://openalex.org/W1580915421",
        "https://openalex.org/W4365388135",
        "https://openalex.org/W4287018479",
        "https://openalex.org/W3176075285",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4300696238",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4223578676",
        "https://openalex.org/W1500866872"
    ],
    "abstract": null,
    "full_text": "82 DOI: 10.1201/9781003320791-9 \n \nThe Moral Landscape7 of General-Purpose \nLarge Language Models \nGiada Pistilli \nSorbonne University, Paris, France \nINTRODUCTION \nThe confusion around the term “Artifcial General Intelligence” (AGI), often trapped \nand disputed between the marketing and research felds, deserves to be defned and \nanalyzed from an ethical perspective. In 1980, American philosopher John Searle \npublished an article in which he argued against what was then called “strong AI.” \nFollowing the legacy of Alan Turing, the question Searle posed was: “Is a machine \ncapable of thinking?” (Searle, 1980). To briefy summarize the experiment, the phi-\nlosopher illustrated a thought experiment known today as “the Chinese room” to \nattempt to answer his question. The thought experiment consists of imagining a room \nin which Artifcial Intelligence (AI) has at its disposal a set of documents (knowl-\nedge base) with Chinese sentences in it. A native Chinese speaker enters the room \nand begins to converse with this AI; the latter can answer, considering it can easily \nfnd which sentence corresponds to the questions asked. The American philosopher’s \nargument is simple: although AI can provide answers in Chinese, it has no back-\nground knowledge of the language. In other words, the syntax is not a suffcient \ncondition for the determination of semantics. \nAlthough the term “strong AI” seems to be replaced by “AGI” nowadays, the two \nterms do not mean the same thing. More importantly, there is still a lot of confusion \namong pioneers and AI practitioners. Machine Learning (ML) engineer Shane Legg \ndescribes AGI as “AI systems that aim to be quite general, for example, as general as \nhuman intelligence” (Legg and Hutter, 2007). This defnition seems to be a philo-\nsophical position rather than an engineering argument.\n1 Nevertheless, in this chapter \nI will not discuss human intelligence, a topic arousing debates for centuries in many \nsocial sciences (e.g., epistemology, philosophy of mind, cognitive psychology, \nanthropology, etc.), but rather AGI capabilities. Therefore, the interpretation I will \nuse to the term “Artifcial General Intelligence” points to AI systems as increasingly \nspecialized in precise tasks, specifcally in processing natural language.\n2 The idea is \nthen to scale exponentially the capabilities of a given AI system. In this sense, I will \nnot discuss the possibility of theoretical physics to realize this idea but rather its \nphilosophical implications and, specifcally, its moral implications. \nThis chapter has been made available under a CC-BY -NC license. \n 83 The Moral Landscape of General-Purpose Large Language Models \nTherefore, this chapter wishes to foster the development of Human-Centered \nArtifcial Intelligence (HCAI), understood as systems created by humans for humans, \nwith the primary objective being enhancing human well-being. My analysis will \ntherefore try to shed light on specifc issues related to General-Purpose Large \nLanguage Models, emphasizing ethical tensions and highlighting potential solutions \nto be explored. \nNATURAL LANGUAGE PROCESSING (NLP) \nBefore discussing the AGI moral implications, it is essential to situate our arguments \nand clarify a few technical details. \nNatural Language Processing (NLP) is a subfeld of linguistics, computer science, \nand nowadays also of AI that focuses on the interactions between human and machine \nlanguage. Initially based on a symbolic recognition system (called symbolic AI), \nlearning in NLP today refers more to statistical probability methods in Neural NLP. \nNLP systems based on Machine Learning algorithms are increasingly popular, and \none type of learning is making waves: Transformers (Vaswani et al., 2017). We can \nsee the entry of the Transformers architecture as a revolutionary moment for NLP, as \nit allows models to scale more easily. Based on the idea of self-attention, it allows the \nmachine to focus on specifc parts of the text sequence and weigh the importance of \neach word to make its prediction. This technique attempts to mimic human cognitive \nattention. As Wittgenstein would say, a word only makes sense in its context \n(Wittgenstein, 1953). Similarly, Transformers, in the pretraining phase, make con-\nnections between words. The principle is to use a very large dataset and focus the \nattention of the model on a small but important part of it, depending on the context \n(Vaswani et al., 2017). \nFor example, BERT (Bidirectional Encoder Representations from Transformers) \n(Devlin et al., 2018) is a powerful language model that leverages the capabilities of \ntransformer-based architecture and is trained on a vast corpus of textual data. This \nmodel is designed to provide a contextual understanding of words present in a sen-\ntence, which makes it an ideal choice for NLP tasks such as question answering, \nsentiment analysis, and text classifcation. One of the most prominent applications of \nBERT is in Google’s search engine. Google has employed BERT to better compre-\nhend the purpose of a user’s query, thereby delivering results that are supposed to be \nmore relevant to the user’s intent. However, this mechanism has not been spared from \ncriticism and its potential malfunction, such as highlighting irrelevant information, \nconveying false information, or discrimination (Noble, 2018). \nThe use of language patterns in search engines is accelerating; these new human– \ncomputer interaction technologies will change how we approach information and its \nresearch. For example, Google announced they would soon introduce their new \n“experimental AI service,” Bard, powered by their language model LaMDA \n(Language Model for Dialogue Applications) (Pichai, 2023). That same language \nmodel caused much noise last summer because the engineer who was testing it said \nhe believed LaMDA was sentient (Tiku, 2022). Many scholars revolted, including \nme, and we tried to call attention to how certain conversations are what a journalist \ncalled the “Sentient AI Trap” (Johnson, 2022). \n \n \n \n \n84 Human-Centered AI \nGENERATIVE PRE-TRAINED TRANSFORMER 3 (GPT-3) \nTo illustrate our point, we will take the GPT-3 3 language model as a case study, \ngiven its scope and multiple missions. My arguments only wish to be a philosophical \nconceptual basis for thinking about ethical issues related to Large Language Models \n(LLMs) and asking questions for the future. \nAs reported by the Ada Lovelace Institute (2023) and OpenAI’s latest blog post \nabout AGI (Altman, 2023), GPT-3 makes a good candidate for our analysis given its \n“general-purpose” capabilities and scope—even though the frontier between AGI \nand “general-purpose” remains yet unclear. \nGPT-3, Generative Pre-trained Transformer 3, is an autoregressive language \nmodel that uses deep learning to produce human-like text (Broackman et al., 2020). \nOpenAI’s API\n4 can be applied to virtually any task that involves understanding or \ngenerating natural language. On their API webpage, there is a spectrum of models \nwith different power levels suitable for various tasks. Examples of GPT-3 models \nare: chat (it simulates an AI assistant to converse with), Q&A (where you can ask \nquestions on any topic and get answers), summarize for a second grader (makes a \nsummary in simple words of a provided text), classifcation (you write lists and ask \nfor categories to be associated with them), and much more. \nGPT-3’s ability to multitask makes it a good example of progress toward some-\nthing that would appear as Artifcial General Intelligence. Moreover, OpenAI’s strat-\negy for selling access to GPT-3 is also noteworthy, given the hype generated around \nits potential applications and use cases. While guardrails like content flters exist, \ntheir effectiveness can be limited in practice: given the statistical nature of AI sys-\ntems, it is a deterministic approach to a probabilistic system. If we add to this the \nhuman unpredictability concerning the use of these models, the approaches taken in \nthe context of GPT-3 remain limited. \nUSE CASE APPLICATIONS \nIf we look at concrete use cases of such AI models, there are numerous examples of \napplication of GPT-3 in fnal products. For example, the French company Algolia\n5 \nand its cloud search API for websites and mobile applications. Algolia provides a \nrange of features, including search-as-you-type suggestions, faceted search, and geo-\nspatial search, as well as the ability to index and search through large amounts of \ndata in real time. \nAnother use case of GPT-3’s API is the company Copy.ai.\n6 Copy.ai is an \nAI-powered writing assistant that helps users generate high-quality written content. \nThe company’s AI technology uses advanced NLP algorithms to analyze large \namounts of text data and generate new written content that is similar in style and tone \nto the input provided by the user. \nNevertheless, can we genuinely trust these systems when we implement them \nwithin fnal products and market them, advertised as lightly as marketing a new \nsmartphone? The confdent and compelling outputs of GPT-3 run the risk of ensnar-\ning its users in the art of rhetoric. Its latest successor, the overreported ChatGPT,\n7 is \n \n \n85 The Moral Landscape of General-Purpose Large Language Models \nfagrant proof of the dangers due to the question of trusting what it produces as con-\ntent.8 This means that the fallibility of LLMs like GPT-3 or ChatGPT and their inher-\nent unreliability in generating content necessitate systematic human oversight over \nthe information produced in its outputs. \nIf we cannot trust the content produced by a language model, what will happen \nwhen it is impossible to distinguish human content from AI-generated content? Will \nit be necessary for users—who are consumers of online content—to distinguish the \nreal from the fake and the artifcial from the human? What impact and moral conse-\nquences will this lack of distinction have? \nTHE PROBLEM OF ARTIFICIAL “GENERAL-PURPOSE” \nINTELLIGENCE (AGI) \nLet us now imagine the extension of the capabilities of language models, having a \nmultitude of goals as the primary—but general—purpose. As seen above, there are \nseveral defnitions of what an AGI is. Another interesting defnition for our analysis \nis the one proposed by Goertzel and Pennacin in their 2007 book Artifcial General \nIntelligence: \nArtifcial General Intelligence (AGI) refers to AI research in which “intelligence” is \nunderstood as a general-purpose capability, not restricted to any narrow collection of \nproblems or domains and including the ability to broadly generalize to fundamentally \nnew areas. \n(Goertzel & Pennachin, 2007) \nThe various defnitions of AGI often recall a cross-cutting capability of the lan-\nguage model, defned as “general-purpose.” Moreover, in their latest blog post “How \nshould AI systems behave, and who should decide?,\n9” they open with the sentence \n“OpenAI’s mission is to ensure that artifcial general intelligence (AGI 10) benefts \nall of humanity.”11 If we are taking GPT-3 as a case study, it is because OpenAI \ndefnes its API as follows: “unlike most AI systems which are designed for one use-\ncase, the API today provides a general-purpose “text in, text out” interface, allowing \nusers to try it on virtually any English language task” (Broackman et al., 2020). The \nsimplicity of using this type of AI system is that users can exploit them with almost \nno computer skills. Users simply have to write their request in natural language in \nthe prompt.\n12 GPT-3 will respond with content generation that attempts to match the \nanswer (“text-out”) to the question (“text-in”). Although lowering the barrier of entry \nto certain technological tools is welcome, questions remain about the safety and the \npotential risks associated with their use. \nSELECTED ETHICAL CONCERNS REGARDING GENERAL-PURPOSE \nLARGE LANGUAGE MODELS \nDeveloping general-purpose LLMs without a specifc objective but rather with a \nwide range of capabilities, with the intention of moving toward AGI, gives rise to \n \n  \n  \n86 Human-Centered AI \nseveral ethical concerns on various levels. I will not explore all ethical concerns but \nrather focus on three in particular. \n1. The frst ethical tension we face is related to the innumerable capabilities of \nthe AI model. In moral philosophy, which deals with defning, suggesting, \nand evaluating the choices and actions that put individuals in a situation of \nwell-being, it isn’t easy to morally assess an artifact with an assortment of \ndifferent scopes. Moreover, the capacities of a Large Language Model like \nGPT-3 are often defned but can multiply with its use. Given the breadth of \npossible uses in natural language, the model’s capabilities can be infnite if \nnot defned a priori and framed by its developers. If the goal of an AGI is to \nno longer recognize itself in a list of skills but rather to have an infnity of \nthem, the situation becomes highly complex to keep under control. It won’t \nbe easy to assess and make value judgments about something whose full \nrange of capabilities is still unknown. Also, it will be challenging to control \npossible malicious uses, to name a few: phishing, fake product reviews, \nmisinformation, and disinformation, etc. One example comes from a study \nby the Government Technology Agency of Singapore. The researchers used \nGPT-3 in conjunction with other AI products focused on personality analy-\nsis to generate phishing emails tailored to their colleagues’ backgrounds \nand traits. The researchers found that more people clicked the links in the \nAI-generated messages than the human-written ones by a signifcant mar -\ngin (Hay Newman, 2021). Moreover, GPT-3 has also been used to create \ncontent for online farms, which often repurpose news from established sites \nto attract ad revenue. Some of these AI-powered sites have been caught \nspreading false information (Vincent, 2023). \nTherefore, I argue that in order to make a moral judgment about a tech-\nnological artifact, it is essential to know and defne its goals. In the absence \nof these conditions, ethics will hardly fnd its usefulness. Calculating the \nrisks, consequences, context, and model use would be very challenging or \neven impossible if its capabilities and use cases were infnite. \nMoreover, without going into the psychology and characteristics of hu-\nman intelligence, there is confusion among AGI pioneers between the latter \nand Human-Level AI (Goertzel, 2014). Nils Nilsson described the AGI as \na machine capable of autonomous learning; the question emerging here is: \nwithout a priori fxed limits, how can control be exercised over its pos-\nsible and various uses? (Nilsson, 2010) What safeguards are in place to \nprevent abuse and misuse? Furthermore, what are the limits set on the ma-\nchine learning of this AGI? Given these technologies’ are state of the art, the \ncurrent state of moral analysis around these systems often seems to dwell \non the technical limits of machine or human intelligence. Quid about the \nboundaries of the latter’s capabilities? \n2. Secondly, as already pointed out by Goetze and Abramson in their paper \n“Bigger isn’t better” (2021), by sociologist Antonio Casilli’s studies of \n“click workers” (2019) and researcher Kate Crawford (2021), there is an \nethical concern related to social justice. Crowdwork, often used to train \n \n  \n87 The Moral Landscape of General-Purpose Large Language Models \nsuch large models, does not guarantee the quality of the dataset and perpetu-\nates wage inequalities. \nCrowdworkers are generally extremely poorly paid for their time; ineligible for \nbenefts, overtime pay, and legal or union protections; vulnerable to exploitation \nby work requesters […]. Moreover, many crowdworkers end up trapped in this \nsituation due to a lack of jobs in their geographic area for people with their quali-\nfcations, compounded with other effects of poverty. \n(Goetze & Abramson, 2021) \nFor example, the famous ImageNet dataset was labeled by an equally re-\nnowned crowdwork: Amazon’s Mechanical Turk, which offers tailored ser-\nvices to adjust and improve AI systems’ data and knowledge bases while \ntraining them to enable automation (Crawford, 2021). The way these Large \nLanguage Models are trained is a bit obscure and raises issues of social \njustice and relevance when annotating data that will need to feed a globally \ntargeted AI model. This set of issues raised seems to refer to the logic of \nwhat some contemporary philosophers call the “technoeconomy” (Sadin, \n2018). According to this logic, the economy would fnd itself driving tech-\nnical and technological developments, seeking to minimize their costs to \nproduce maximum benefts. \nAnother example concerns the latest scandal related to OpenAI’s cre-\nation of a safety system for ChatGPT that could detect and flter out toxic \nlanguage. OpenAI contracted with an outsourcing frm in Kenya to label \ntens of thousands of text snippets, many of which contained explicit and dis-\nturbing content, such as descriptions of child sexual abuse, murder, suicide, \nand torture (Perrigo, 2023). The workers who labeled the data were report-\nedly paid less than $2 per hour, which raises ethical concerns about fair \ncompensation and worker exploitation, as reported by the abovementioned \nscholars. This case also illustrates how even the most benevolent intentions \nmay yield limited actions and results if the subsequent implementation fails \nto consider the ethical implications relating to social justice. \n3. This last argument allows us to make a transition to our third ethical problem: \nlanguage. Speaking of Natural Language Processing and Large Language \nModels, it is inevitable to talk about it. I argue that the language-related \nproblem in Large Language Models is of two different natures. The frst is \nthe diffculty in controlling the text generation (“text-out”) produced by the \nmodel. As an example, GPT-3 has a content flter to warn the user when con-\nfronted with content that is unsafe (text containing profane, discriminatory, \nor hateful language) or sensitive (the text could be talking about a sensitive \ntopic, something political, religious, or talking about a protected class such \nas race or nationality). As mentioned above, this content flter is inaccurate \nand unsatisfactory, as the content generated by GPT-3 is often toxic. Within \nthe context of language models and their role in shaping communication, \nit is imperative to remember that the values conveyed by language are fun-\ndamental in guiding human behavior and action (Habermas, 1990). Thus, \nthe implicit values that exist within a language model may be transmitted \n 88 Human-Centered AI \nthrough its use. Recent empirical research has demonstrated that the values \nthat are embedded in the GPT-3 training data are predominantly refective \nof American values, rather than those of other cultural contexts (Johnson \net al., 2022). \nRegardless, it still will be diffcult to tame this titan under these AGI \nconditions of “general-purpose.” In this case, the limits are not only ethi-\ncal but also technical. Text generation being a probabilistic calculation of \nwhich word will follow within the same sentence, GPT-3 will always be in \nthe condition to give different answers from each other, according to the \nexamples inserted in its prompt. Therefore, if the text-in already presents \ntoxicity, fnding it in the text-out will be easy. Differently, if in the prompt \nthere are no toxic contents, there will always be the probability that GPT-3 \nanswers with a text-out containing toxic elements. Once again, the ethical \nproblem here is related to the vastness of the language model and the desire \nto open it up to a multitude of capabilities. \nThe second nature of the language-related ethical problem when it \ncomes to Large Language Models is the absence of diversity. Diversity is \nunderstood not just as a representation of gender and ethnicity but also as \nan actual language (Spanish, Portuguese, Danish, etc.). In fact, according \nto OpenAI, 93% of the training data was in English. The next most repre-\nsented language was French (1.8%), followed by German (1.5%), Spanish \n(0.8%), Italian (0.6%), and so on (Brown et al., 2020). Researchers have \nalready begun to explore the multilingual capabilities of GPT-3, noting, \nfor example, how it works poorly in minority languages such as Catalan \n(Armengol-Estapé et al., 2021). Since the absence of a piece of data is as \nimportant as its presence, the very scarce presence of languages other than \nEnglish leads us to some rather negative considerations, given the multilin-\ngual and universal nature that an AI model like AGI is intended to take. The \noverwhelming and cumbersome omnipresence of the English language is a \nserious problem that needs to be addressed as soon as possible if we want \nto make AI accessible to everyone. Because GPT-3 is a system that uses \nnatural language to function and provide answers, orienting it exclusively \nto English and the values that revolve around American culture will not do \njustice to the pluralism of values in which we live in our diverse societies. \nThe risk of implicitly promoting a monoculture fostered by large American \nindustries is indisputable. The danger here is twofold: on the one hand, the \npropagation of the monoculture may be permeated by the implicit or explic-\nit values of the industries developing these AI systems. On the other hand, \nthis same monoculture can be promoted and shared, implicitly or explicitly, \nthrough the value systems belonging to the culture dominating these new \ntechnological developments. One striking example is related to the recent \ntestimony of the Facebook whistleblower. During her testimony, Frances \nHaugen pointed out that the lack of moderation tools in languages other \nthan English allowed users of the online platform to freely share content in \nviolation of Facebook’s internal policy (Hao, 2021). \n \n \n89 The Moral Landscape of General-Purpose Large Language Models \nPOTENTIAL SOLUTIONS TO BE EXPLORED \nFirst, a challenging but fundamental question must be asked: what then is the ultimate \ngoal of these Large Language Models? What is the purpose of AGI? Since in ethics \nthe “I do it because I can” paradigm can’t stand, we should be able to defne “the” \npurpose clearly and not settle for the vague “general-purpose.” In its absence, it will \nbe diffcult to fnd a justifcation and, consequently, evaluate it morally. Considering \nthe advances and the current state of the art of machine learning technologies, auto-\nmating it more and more can only be desirable after well-framed safeguards have \nbeen put in place. If this can still be part of building an AGI, developing ex-ante \nwell-structured capabilities limits would be necessary. \nSecondly, we need to start shedding light on these dark processes behind the AI \nindustry regarding our social justice issue. The “black box” is not only found within \nthe algorithms but also in the exploitative processes that often bind the poorest part \nof the world to make us believe that these processes are automated—but they are not. \nThe demand for human labor to produce the datasets needed to run these Large \nLanguage Models grows exponentially. As a result, national and international institu-\ntions need to start asking questions quickly, in order to bring answers and a clear \nlegislative framework for these new “data labeler-proletarians.” \nFinally, the issue related to language is, in my opinion, one of the thorniest to deal \nwith. Aside from the concealed hypocrisy found among the AGI pioneers, who sell \ntheir products as being “universal,” the problem here is structural. Today we’re talk-\ning about Large Language Models, but I’d like to point out that the entire Internet \necosystem is governed by the English language and an American monoculture that \npermeates every corner of it. Today we are facing a diffculty that we can turn into a \npossibility: we can fx this kind of problem in language models and try to integrate \nthe feedback from its users as much as possible. The process will undoubtedly be \nlonger, but it could be the beginning of a fruitful collaboration. In addition, it might \nhelp to change the paradigm of AGI and make it rather “narrow AI”: oriented toward \nspecifc capabilities and circumscribed to its context. In this way, each context could \nappropriate its model and make it its own, thus ensuring a plurality of values relevant \nto its social context. \nFrom an ethical standpoint, developing narrow and culturally based AI models \nmay offer several benefts compared to pursuing AGI. One noteworthy advantage is \nthat these models are tailored to specifc contexts and can accommodate the needs \nand values of specifc communities. By doing so, these models could mitigate the \nrisk of perpetuating biases and unintended consequences, as they are aligned with \nlocal ethical and cultural norms. By focusing on narrow AI models, stakeholders can \nensure that the development process is more controllable, transparent, and subject to \ngreater scrutiny and accountability. Furthermore, given the dominance of English in \nthe AI ecosystem, prioritizing the development of language models for non-English \nlanguages is essential to ensure that diverse linguistic and cultural perspectives are \nrepresented in the discourse. Taken together, prioritizing the development of narrow \nand culturally based AI models can address ethical concerns related to AI and pro-\nmote the technology’s ethical use in ways that align with local values and needs. \n 90 Human-Centered AI \nThe argument presented here is exemplifed by grassroots organizations such as \nMasakhane,13 aimed at fostering research in NLP specifcally for African languages. \nRemarkably, even though African languages constitute nearly 2,000 of the total \nworld languages, they are scarcely represented within technological platforms. \nAnother pertinent example can be observed in the endeavors of Te Hiku Media,\n14 a \nnonproft Māori radio station. This pioneering grassroots initiative within the domain \nof NLP concentrates on the safeguarding of minority languages, while concurrently \nensuring the control and sovereignty of the community’s data (Hao, 2022). \nAnother virtuous example of projects addressing the issue of language and data \ngovernance is the BigScience open science workshop\n15 and their approach to multi-\nlingualism. In their paper “Data governance in the age of large-scale data-driven \nlanguage technology” (Jernite et al., 2022), the authors present their defnition of \ndata governance as “the set of processes and policies that govern how data is col-\nlected, stored, accessed, used, and shared” (p. 1). The advent of machine translation \nsystems and LLMs presents unique ethical opportunities and challenges. The authors \nillustrate the implications of these challenges and opportunities; for example, the \nethical concerns of utilizing biased or sensitive data, the privacy issues of exposing \npersonal or confdential information, the quality issues of utilizing low-resource or \nnoisy data, and the diversity gaps of underrepresenting certain languages or groups. \nThe authors also propose some guiding principles for data governance, such as estab-\nlishing unambiguous data ownership and consent mechanisms, developing data qual-\nity metrics and standards, promoting data diversity and inclusion, and fostering \ncollaboration and transparency among stakeholders. \nCONCLUSION \nIn conclusion, we have seen how technical problems often go hand in hand with \nethical issues. In pursuit of the genuine development of HCAI, a paradigm where AI \nsystems are tailored to conform to human values, and invariably ensure human ben-\neft, it is imperative to address the highlighted ethical tensions. Moreover, given the \ninterdisciplinary nature of the scientifc domain of artifcial intelligence, these ethi-\ncal problems cannot be solved without the help of engineers. And when I talk about \nphilosophers and engineers working together, it also means that engineers shouldn’t \nmake themselves out to be ethicists without the right expertise and knowledge. \nIndeed, philosophy has been asking questions of this order for thousands of years; \nits experience can serve us not to make the same mistakes, but more importantly \nto well formulate the right questions to ask in this new and evolving technological \ncontext. The heightened attention being paid to moral philosophy is of paramount \nimportance and represents an urgent concern. Nonetheless, as has become evident in \ncontemporary times, the complex challenges posed by emerging technologies cannot \nbe resolved through technical efforts alone. In this regard, the social sciences and \nhumanities are called upon to play a critical role in helping this discipline. Because \nwhile science serves to describe reality, it is ethics that ultimately guides the way in \nwhich this reality ought to be constructed in the future. \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n91 The Moral Landscape of General-Purpose Large Language Models \nNOTES \n1 In this chapter, I do not make the distinction between ethics and morality, both having the \nsame etymology coming from Greek and Latin respectively. \n2 Language is defned as “natural” when it belongs specifcally to humans (e.g., Chinese, \nSpanish, and German), as opposed to the “artifcial” language of machines (e.g., different \ncode languages). \n3 Although ChatGPT and GPT-4 would have been even more relevant objects of analysis, \nthis chapter and its related research have been elaborated months before their release. \n4 An API, or Application Programming Interface, is a set of rules and protocols for accessing \na web-based software application or web tool. \n5 https://www.algolia.com/about/ \n6 https://www.copy.ai \n7 This improved version of GPT-3, also developed by OpenAI, is focused only on the ques-\ntion-and-answer task, thus not relevant to our analysis. Accessible to anyone on condition \nof signing up on the platform, it can be accessed at the following link: https://chat.openai. \ncom/chat \n8 To explore the issue of trust further, I recommend reading this recent article appeared in \nNature: https://www.nature.com/articles/d41586-023-00423-4 \n9 https://openai.com/blog/how-should-ai-systems-behave/ \n10 Their defnition of AGI reads: “By AGI, we mean highly autonomous systems that outper-\nform humans at most economically valuable work.” \n11 Ibid. \n12 A prompt is a set of initial input given to a large language model to generate output based \non the provided context. \n13 https://www.masakhane.io/ \n14 https://tehiku.nz/ \n15 https://bigscience.huggingface.co/ \nREFERENCES \nAltman, S. (2023). Planning for AGI and beyond. OpenAI blog. Retrieved from https://openai. \ncom/blog/planning-for-agi-and-beyond \nArmengol-Estapé, J., Bonet, O. D., & Melero, M. (2021). On the Multilingual Capabilities of \nVery Large-Scale English Language Models. arXiv, abs/2108.13349. \nBroackman, G., et al. (2020). OpenAI API. OpenAI Blog. Retrieved from https://openai.com/ \nblog/openai-api \nBrown, T. B., et al. (2020). Language Models are Few-Shot Learners. arXiv, arXiv:2005.14165. \nCasilli, A. (2019). En attentant les robots. Editions Seuil, Paris. \nCrawford, K. (2021). Atlas of AI. Yale University Press, Yale. \nDevlin, J., et al. (2018). Bert: Pre-training of deep bidirectional transformers for language \nunderstanding. arXiv preprint, arXiv:1810.04805. \nGoertzel, B. (2014). Artifcial General Intelligence: Concept, State of the Art, and Future \nProspects. Journal of Artifcial General Intelligence, 5(1), 1–46. \nGoertzel, B., & Pennachin, C. (2007). Artifcial General Intelligence. Springer-Verlag, Berlin \nHeidelberg, Berlin. \nGoetze, T. S., & Abramson, D. (2021). Bigger Isn’t Better: The Ethical and Scientifc Vices \nof Extra-Large Datasets in Language Models. In WebSci ’21 Proceedings of the 13th \nAnnual ACM Web Science Conference (Companion Volume). \nHabermas, J. (1990). Moral consciousness and communicative action. MIT Press. \n \n \n  \n  \n \n \n92 Human-Centered AI \nHao, K. (2021). The Facebook whistleblower says its algorithms are dangerous. Here’s why. \nMIT Technology Review. Retrieved from https://www.technologyreview.com/2021/ \n10/05/1036519/facebook-whistleblower-frances-haugen-algorithms/ \nHao, K. (2022). A new vision of artifcial intelligence for the people. MIT Technology \nReview. Retrieved from https://www.technologyreview.com/2022/04/22/1050394/ \nartifcial-intelligence-for-the-people/ \nHay Newman, L. (2021). AI Wrote Better Phishing Emails Than Humans in a Recent Test. \nWired. Retrieved from https://www.wired.com/story/ai-phishing-emails/ \nJernite, Y ., et al. (2022). Data governance in the age of large-scale data-driven language tech-\nnology. In 2022 ACM Conference on Fairness, Accountability, and Transparency. \nJohnson, K. (2022). LaMDA and the Sentient AI Trap. Wired. Retrieved from https://www. \nwired.com/story/lamda-sentient-ai-bias-google-blake-lemoine/ \nJohnson, R. L., Pistilli, G., Menédez-González, N., Duran, L. D. D., Panai, E., Kalpokiene, \nJ., & Bertulfo, D. J. (2022). The Ghost in the Machine has an American accent: value \nconfict in GPT-3. arXiv preprint, arXiv:2203.07785. \nLegg, S., & Hutter, M. (2007). Universal Intelligence: A Defnition of Machine Intelligence. \nMinds and Machines, 17(4), 391–444. \nNilsson, N. J. (2010). Quest for Artifcial Intelligence: A History of Ideas and Achievements. \nCambridge University Press, Cambridge. \nNoble, S. U. (2018). Algorithms of Oppression: How Search Engines Reinforce Racism. New \nYork University Press, New York. https://doi.org/10.18574/nyu/9781479833641. \n001.0001 \nPerrigo, B. (2023). Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour \nto Make ChatGPT Less Toxic. Time. Retrieved from https://time.com/6247678/ \nopenai-chatgpt-kenya-workers/ \nPichai, S. (2023). An important next step on our AI journey. Google Blog. Retrieved from \nhttps://blog.google/technology/ai/bard-google-ai-search-updates/ \nSadin, E. (2018). Le technolibéralisme nous conduit à un ‘avenir régressif’. Hermès, La \nRevue, 80(1), 255–258. \nSearle, J. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, 3(3), 417–424. \ndoi:10.1017/S0140525X00005756 \nTiku, N. (2022). The Google engineer who thinks the company’s AI has come to life. The \nWashington Post. Retrieved from https://www.washingtonpost.com/technology/2022/ \n06/11/google-ai-lamda-blake-lemoine/ \nVaswani, A. et al. (2017). Attention is All you Need. ArXiv abs/1706.03762. \nVincent, J. (2023). AI is being used to generate whole spam sites. The Verge. Retrieved from \nhttps://www.theverge.com/2023/5/2/23707788/ai-spam-content-farm-misinformation-\nreports-newsguard \nWittgenstein, L. (1953). Philosophische Untersuchungen (Philosophical Investigations), \ntranslated by G. E. M. Anscombe, 1953. "
}