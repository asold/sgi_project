{
  "title": "Patton: Language Model Pretraining on Text-Rich Networks",
  "url": "https://openalex.org/W4385571564",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2158505021",
      "name": "Bowen Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117585308",
      "name": "Wentao Zhang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2023856972",
      "name": "Yu Zhang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A1976615132",
      "name": "Yu Meng",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2119870721",
      "name": "Xinyang Zhang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2054618842",
      "name": "Qi Zhu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W2135389419",
    "https://openalex.org/W3037208489",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3126284168",
    "https://openalex.org/W4288275971",
    "https://openalex.org/W3005552578",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3036446966",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W3103427817",
    "https://openalex.org/W3212640459",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W4321648960",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4385562555",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3172205429"
  ],
  "abstract": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval.However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks.Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure.We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7005–7020\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPATTON\n : Language Model Pretraining on Text-Rich Networks\nBowen Jin1, Wentao Zhang1, Yu Zhang1, Yu Meng1,\nXinyang Zhang1, Qi Zhu1, Jiawei Han1\n1University of Illinois at Urbana-Champaign, IL, USA\n{bowenj4,wentao4,yuz9,yumeng5,xz43,qiz3,hanj}@illinois.edu\nAbstract\nA real-world text corpus sometimes comprises\nnot only text documents, but also semantic\nlinks between them (e.g., academic papers in\na bibliographic network are linked by citations\nand co-authorships). Text documents and se-\nmantic connections form a text-rich network,\nwhich empowers a wide range of downstream\ntasks such as classification and retrieval. How-\never, pretraining methods for such structures\nare still lacking, making it difficult to build one\ngeneric model that can be adapted to various\ntasks on text-rich networks. Current pretrain-\ning objectives, such as masked language mod-\neling, purely model texts and do not take inter-\ndocument structure information into considera-\ntion. To this end, we propose our PretrAining\non TexT-Rich NetwOrk framework PATTON .\nPATTON 1 includes two pretraining strategies:\nnetwork-contextualized masked language mod-\neling and masked node prediction, to capture\nthe inherent dependency between textual at-\ntributes and network structure. We conduct\nexperiments on four downstream tasks in five\ndatasets from both academic and e-commerce\ndomains, where PATTON outperforms baselines\nsignificantly and consistently.\n1 Introduction\nTexts in the real world are often interconnected\nthrough links that can indicate their semantic\nrelationships. For example, papers connected\nthrough citation links tend to be of similar topics;\ne-commerce items connected through co-viewed\nlinks usually have related functions. The texts and\nlinks together form a type of network called a text-\nrich network, where documents are represented\nas nodes, and the edges reflect the links among\ndocuments. Given a text-rich network, people are\nusually interested in various downstream tasks (e.g.,\ndocument/node classification, document retrieval,\nand link prediction) (Zhang et al., 2019; Wang et al.,\n1Code is available at https://github.com/\nPeterGriffinJin/Patton\n2019; Jin et al., 2023a). For example, given a com-\nputer science academic network as context, it is\nintuitively appealing to automatically classify each\npaper (Kandimalla et al., 2021), find the authors of\na new paper (Schulz et al., 2014), and provide pa-\nper recommendations (Küçüktunç et al., 2012). In\nsuch cases, pretraining a language model on a given\ntext-rich network which can benefit a great number\nof downstream tasks inside this given network is\nhighly demanded (Hu et al., 2020b).\nWhile there have been abundant studies on build-\ning generic pretrained language models (Peters\net al., 2018; Devlin et al., 2019; Liu et al., 2019;\nClark et al., 2020), they are mostly designed for\nmodeling texts exclusively, and do not consider\ninter-document structures. Along another line of\nresearch, various network-based pretraining strate-\ngies are proposed in the graph learning domain to\ntake into account structure information (Hu et al.,\n2020a,b). Yet, they focus on pretraining graph\nneural networks rather than language models and\ncannot easily model the rich textual semantic in-\nformation in the networks. To empower language\nmodel pretraining with network signals, LinkBERT\n(Yasunaga et al., 2022) is a pioneering study that\nputs two linked text segments together during pre-\ntraining so that they can serve as the context of\neach other. However, it simplifies the complex net-\nwork structure into node pairs and does not model\nhigher-order signals (Yang et al., 2021). Overall,\nboth existing language model pretraining methods\nand graph pretraining methods fail to capture the\nrich contextualized textual semantic information\nhidden inside the complex network structure.\nTo effectively extract the contextualized seman-\ntics information, we propose to view the knowledge\nencoded inside the complex network structure from\ntwo perspectives: token-level and document-level.\nAt the token level, neighboring documents can help\nfacilitate the understanding of tokens. For example,\nin Figure 1, based on the text information of neigh-\n7005\nExperience the ultimate in \nsmoothness and creaminess \nwith Dove \nFerrero Rocher Collection, \nFine Hazelnut Milk Chocolates \n…\n…\n…\nSoap Bar? \nChocolate? \n…\nh\nHERSHEY'S Assorted \nChocolate Candy Mix \nOlay Moisture Coconut \nOasis Body Wash \n L'Oreal Paris Elvive \nRepairing Shampoo \nDove ’s Smooth formula will \nleave you feeling refreshed \nFigure 1: An illustration of a text-rich network (a prod-\nuct item co-viewed network). At the token level, from\nnetwork neighbors, we can know that the “Dove” at\nthe top is a personal care brand and the “Dove” at the\nbottom is a chocolate brand. At the document level,\nreferring to the edge in the middle, we can learn that the\nchocolate from “Hershey’s” should have some similarity\nwith the chocolate from “Ferrero”.\nbors, we can know that the “Dove” at the top refers\nto a personal care brand, while the “Dove” at the\nbottom is a chocolate brand. At the document level,\nthe two connected nodes can have quite related\noverall textual semantics. For example, in Fig-\nure 1, the chocolate from “Hershey’s” should have\nsome similarity with the chocolate from “Ferrero”.\nAbsorbing such two-level hints in pretraining can\nhelp language models produce more effective rep-\nresentations which can be generalized to various\ndownstream tasks.\nTo this end, we propose PATTON , a method to\ncontinuously pretrain language models on a given\ntext-rich network. The key idea of PATTON is\nto leverage both textual information and network\nstructure information to consolidate the pretrained\nlanguage model’s ability to understand tokens and\ndocuments. Building on this idea, we propose two\npretraining strategies: 1) Network-contextualized\nmasked language modeling: We randomly mask\nseveral tokens within each node and train the lan-\nguage model to predict those masked tokens based\non both in-node tokens and network neighbors’ to-\nkens. 2) Masked node prediction: We randomly\nmask some nodes inside the network and train the\nlanguage model to correctly identify the masked\nnodes based on the neighbors’ textual information.\nWe evaluate PATTON on both academic domain\nnetworks and e-commerce domain networks. To\ncomprehensively understand how the proposed pre-\ntraining strategies can influence different down-\nstream tasks, we conduct experiments on classifi-\ncation, retrieval, reranking, and link prediction.\nIn summary, our contributions are as follows:\n• We propose the problem of language model pre-\ntraining on text-rich networks.\n• We design two strategies, network contextual-\nized MLM and masked node prediction to train\nthe language model to extract both token-level\nand document-level semantic correlation hidden\ninside the complex network structure.\n• We conduct experiments on four downstream\ntasks in five datasets from different domains,\nwhere PATTON outperforms pure text/graph pre-\ntraining baselines significantly and consistently.\n2 Preliminaries\nDefinition 2.1. Text-Rich Networks (Yang et al.,\n2021; Jin et al., 2023b). A text-rich network can\nbe denoted asG= (V,E,D), where V, Eand Dare\nnode set, edge set, and text set, respectively. Each\nvi ∈V is associated with some textual information\ndvi ∈D. For example, in an academic citation\nnetwork, v ∈ Vare papers, e ∈ Eare citation\nedges, and d∈D are the content of the papers. In\nthis paper, we mainly focus on networks where the\nedges can provide semantic correlation between\ntexts (nodes). For example, in a citation network,\nconnected papers (cited papers) are likely to be\nsemantically similar.\nProblem Definition. (Language Model Pretrain-\ning on Text-rich Networks.) Given a text-rich net-\nwork G= (V,E,D), the task is to capture the self-\nsupervised signal on Gand obtain a G-adapted lan-\nguage model MG. The resulting language model\nMG can be further finetuned on downstream tasks\nin G, such as classification, retrieval, reranking, and\nlink prediction, with only a few labels.\n3 P ATTON\n3.1 Model Architecture\nTo jointly leverage text and network information in\npretraining, we adopt the GNN-nested Transformer\narchitecture (called GraphFormers) proposed in\n(Yang et al., 2021). In this architecture, GNN mod-\nules are inserted between Transformer layers. The\nforward pass of each GraphFormers layer is as fol-\nlows.\nz(l)\nx = GNN({H(l)\ny [CLS]|y∈Nx}), (1)\n˜H(l)\nx = Concate(z(l)\nx ,H(l)\nx ), (2)\n˜H(l)′\nx = LN(H(l)\nx + MHAasy(˜H(l)\nx )), (3)\nH(l+1)\nx = LN(˜H(l)′\nx + MLP(˜H(l)′\nx )), (4)\nwhere H(l)\nx is token hidden states in the l-th layer\nfor node x, Nx is the network neighbor set ofx, LN\n7006\nis the layer normalization operation andMHAasy is\nthe asymmetric multihead attention operation. For\nmore details, one can refer to (Yang et al., 2021).\n3.2 Pretraining P ATTON\nWe propose two strategies to help the language\nmodels understand text semantics on both the token\nlevel and the document level collaboratively from\nthe network structure. The first strategy focuses on\ntoken-level semantics learning, namely network-\ncontextualized masked language modeling; while\nthe second strategy emphasizes document-level se-\nmantics learning, namely masked node prediction.\nStrategy 1: Network-contextualized Masked\nLanguage Modeling (NMLM). Masked lan-\nguage modeling (MLM) is a commonly used strat-\negy for language model pretraining (Devlin et al.,\n2019; Liu et al., 2019) and domain adaptation (Gu-\nrurangan et al., 2020). It randomly masks several\ntokens in the text sequence and utilizes the sur-\nrounding unmasked tokens to predict them. The\nunderlying assumption is that the semantics of each\ntoken can be reflected by its contexts. Trained\nto conduct masked token prediction, the language\nmodel will learn to understand semantic correla-\ntion between tokens and capture the contextualized\nsemantic signals. The mathematical formulation of\nMLM is as follows,\nLMLM = −\n∑\ni∈Mt\nlog p(wi|Hi), (5)\nwhere Mt is a subset of tokens which are replaced\nby a special [MASK] token and p(wi|Hi) is the\noutput probability of a linear head fhead which\ngives predictions to wi (from the vocabulary W)\nbased on contextualized token hidden states {Hi}.\nSuch token correlation and contextualized se-\nmantics signals also exist and are even stronger in\ntext-rich networks. Text from adjacent nodes in\nnetworks can provide auxiliary contexts for text se-\nmantics understanding. For example, given a paper\ntalking about “Transformers” and its neighboring\npapers (cited papers) in the academic network on\nmachine learning, we can infer that “Transformers”\nhere is a deep learning model rather than an elec-\ntrical engineering component by reading the text\nwithin both the given paper and the neighboring pa-\npers. In order to fully capture the textual semantic\nsignals in the network, the language model needs\nto not only understand the in-node text token corre-\nlation but also be aware of the cross-node semantic\ncorrelation.\nWe extend the original in-node MLM to network-\ncontextualized MLM, so as to facilitate the lan-\nguage model to understand both in-node token cor-\nrelation and network-contextualized text semantic\nrelatedness. The training objective is shown as\nfollows.\nLNMLM = −\n∑\ni∈Mt\nlog p(wi|Hx,zx),\np(wi|Hx,zx) =softmax(q⊤\nwihi),\n(6)\nwhere zx denotes the network contextualized token\nhidden state in Section 3.1 and hi = H(L)\nx [i] (if i\nis inside node x). Lis the number of layers. qwi\nrefers to the MLM prediction head for wi. Since\nthe calculation of hi is based on Hx and zx, the\nlikelihood will be conditioned on Hx and zx.\nStrategy 2: Masked Node Prediction (MNP).\nWhile network-contextualized MLM focuses more\non token-level semantics understanding, we pro-\npose a new strategy called “masked node predic-\ntion”, which helps the language model understand\nthe underlying document-level semantics correla-\ntion hidden in the network structure.\nConcretely, we dynamically hold out a subset of\nnodes from the network ( Mv ⊆V), mask them,\nand train the language model to predict the masked\nnodes based on the adjacent network structure.\nLMNP = −\n∑\nvj∈Mv\nlog p(vj|Gvj ),\np(vj|Gvj ) =softmax(h⊤\nvj hNvj )\n(7)\nwhere Gvj = {hvk |vk ∈ Nvj }are the hidden\nstates of the neighbor nodes in the network andNvj\nis the set of neighbors of vj. In particular, we treat\nthe hidden state of the last layer of[CLS] as a repre-\nsentation of node level, that is, hvj = H(L)\nvj [CLS].\nBy performing the task, the language model will\nabsorb document semantic hints hidden inside the\nnetwork structure (e.g., contents between cited pa-\npers in the academic network can be quite semanti-\ncally related, and text between co-viewed items in\nthe e-commerce network can be highly associated).\nHowever, directly optimizing masked node pre-\ndiction can be computationally expensive since we\nneed to calculate the representations for all neigh-\nboring nodes and candidate nodes for one predic-\ntion. To ease the computation overload, we prove\nthat the masked node prediction task can be theo-\nretically transferred to a computationally cheaper\npairwise link prediction task.\n7007\n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n濂濁激濁 \n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n濁濂濄 \nĸ 濄濦濙濨濦濕濝濢濝濢濛 \n…\n…\nķ 澵澔濈濙濬濨澡濦濝濗濜澔濂濙濨濫濣濦濟 \n澽濢濤濩濨 \n濂濁激濁 \n濁濂濄 \n濂濁激濁 \n濁濂濄 \nĹ 澺濝濢濙濨濩濢濝濢濛 \n…\n濗濣濢濨濙濬濨 \n濏澷激濇濑 濏澷激濇濑 濏澷激濇濑 \n澸濣濗澔澥 澸濣濗澔澦 澸濣濗澔澧 \n濖澢澔濁濂濄 \n濒\n濕澢澔濂濁激濁 \nOn the [mask] \nand risks of … \n濒\n濫濣濦濘澔濨濣濟濙濢澔濜濝濘濘濙濢澔濧濨濕濨濙 \n濏澷激濇濑澔濨濣濟濙濢澔濜濝濘濘濙濢澔濧濨濕濨濙 \n濢濙濝濛濜濖濣濦澔濕濛濛濦濙濛濕濨濝濣濢澔 \n濜濝濘濘濙濢澔濧濨濕濨濙 \n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n激濝濢濟澔濄濦濙濘濝濗濨濝濣濢 \n瀖\n瀖\n瀖\n濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 \n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n濆濙濦濕濢濟濝濢濛 \n瀖\n瀖\n瀖\n濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 \n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n濆濙濨濦濝濙濪濕濠 \n瀖\n瀖\n瀖\n濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 濥濸瀃瀅濸瀆濸瀁瀇濴瀇濼瀂瀁 \n澻濦濕濤濜澡濖濕濧濙濘澔澵濛濛濦濙濛濕濨濝濣濢 \n澷濠濕濧濧濝濚濝濗濕濨濝濣濢 \n濖濿濴瀆瀆濼濹濼濶濴瀇濼瀂瀁澳 \n濻濸濴濷 \n瀖\n瀖\n瀖\n濖濿濴瀆瀆濼濹濼濶濴瀇濼瀂瀁澳 \n濻濸濴濷 \n濏澷激濇濑 濏澷激濇濑 \n澸濣濗澔澨 澸濣濗澔澩 \n… …\n… …\n… …\n… …\n…\nFigure 2: Overall pretraining and finetuning procedures for PATTON . We have two pretraining strategies: network-\ncontextualized masked language modeling (NMLM) and masked node prediction (MNP). Apart from output layers,\nthe same architectures are used in both pretraining and finetuning (in our experiment, we have 12 layers). The same\npretrained model parameters are used to initialize models for different downstream tasks. During finetuning, all\nparameters are updated.\nTheorem 3.2.1. Masked node prediction is equiva-\nlent to pairwise link prediction.\nProof: Given a set of masked nodes Mv, the likeli-\nhood of predicting the masked nodes is∏\nv[MASK]∈Mv\np(v[MASK] = vi|vk ∈Nv[MASK] )\n∝\n∏\nv[MASK]∈Mv\np(vk ∈Nv[MASK] |v[MASK] = vi)\n=\n∏\nv[MASK]∈Mv\n∏\nvk∈Nv[MASK]\np(vk|v[MASK] = vi)\n=\n∏\nv[MASK]∈Mv\n∏\nvk∈Nv[MASK]\np(vk ←→vi)\nIn the above proof, the first step relies on the Bayes’\nrule, and we have the assumption that all nodes\nappear uniformly in the network, i.e., p(v[MASK] =\nvi) = p(v[MASK] = vj). In the second step, we\nhave the conditional independence assumption of\nneighboring nodes generated given the center node,\ni.e., p(vk,vs|v[MASK] = vi) =p(vk|v[MASK] = vi) ·\np(vs|v[MASK] = vi).\nAs a result, the masked node prediction objective\ncan be simplified into a pairwise link prediction\nobjective, which is\nLMNP = −\n∑\nvj∈Mv\n∑\nvk∈Nvj\nlog p(vj ↔vk)\n= −\n∑\nvj∈Mv\n∑\nvk∈Nvj\nlog\nexp(h⊤\nvj hvk )\nexp(h⊤vj hvk ) +∑\nu′ exp(h⊤vj hv′u ),\n(8)\nwhere v′\nu stands for a random negative sample.\nIn our implementation, we use “in-batch negative\nsamples” (Karpukhin et al., 2020) to reduce the\nencoding cost.\nJoint Pretraining. To pretrain PATTON , we opti-\nmize the NMLM objective and the MNP objective\njointly:\nL= LNMLM + LMNP. (9)\nThis joint objective will unify the effects of NMLM\nand MNP, which encourages the model to conduct\nnetwork-contextualized token-level understanding\nand network-enhanced document-level understand-\ning, facilitating the joint modeling of texts and net-\nwork structures. We will show in Section 4.6 that\nthe joint objective achieves superior performance\nin comparison with using either objective alone.\n3.3 Finetuning P ATTON\nLast, we describe how to finetune PATTON for\ndownstream tasks involving encoding for text in\nthe network and text not in the network. For text\nin the network (thus with neighbor information),\nwe will feed both the node text sequence and the\nneighbor text sequences into the model; while for\ntexts not in the network (thus neighbor information\nis not available), we will feed the text sequence into\nthe model and leave the neighbor text sequences\nblank. For both cases, the final layer hidden state\nof [CLS] is used as text representation following\n(Devlin et al., 2019) and (Liu et al., 2019).\n7008\nTable 1: Dataset Statistics.\nDataset #Nodes #Edges #Fine-Classes #Coarse-ClassesMathematics 490,551 2,150,584 14,271 18Geology 431,834 1,753,762 7,883 17Economics 178,670 1,042,253 5,205 40Clothes 889,225 7,876,427 2,771 9Sports 314,448 3,461,379 3,034 16\n4 Experiments\n4.1 Experimental Settings\nDataset. We perform experiments on both aca-\ndemic networks from Microsoft Academic Graph\n(MAG) (Sinha et al., 2015) and e-commerce net-\nworks from Amazon (McAuley et al., 2015). In\nacademic networks, nodes are papers and there\nwill be an edge between two papers if one cites\nthe other; while in e-commerce networks, nodes\ncorrespond to items, and item nodes are linked\nif they are frequently co-viewed by users. Since\nMAG and Amazon both have multiple domains,\nwe select three domains from MAG and two do-\nmains from Amazon. In total, five datasets are used\nin the evaluation (i.e., MAG-Mathematics, MAG-\nGeology, MAG-Economics, Amazon-Clothes and\nAmazon-Sports). The statistics of all the datasets\ncan be found in Table 1. Fine-classes are all the\ncategories in the network-associated node category\ntaxonomy (MAG taxonomy and Amazon product\ncatalog), while coarse-classes are the categories at\nthe first layer of the taxonomy.\nPretraining Setup. The model is trained for\n5/10/30 epochs (depending on the size of the net-\nwork) on 4 Nvidia A6000 GPUs with a total batch\nsize of 512. We set the peak learning rate as 1e-5.\nNMLM pretraining uses the standard 15% [MASK]\nratio. For our model and all baselines, we adopt a\n12-layer architecture. More details can be found in\nthe Appendix A.\nBaselines. We mainly compare our method with\ntwo kinds of baselines, off-the-shelf pretrained\nlanguage models and language model continuous\npretraining methods. The first category includes\nBERT (Devlin et al., 2019), SciBERT (Beltagy\net al., 2019), SPECTER (Cohan et al., 2020), Sim-\nCSE (Gao et al., 2021), LinkBERT (Yasunaga et al.,\n2022) and vanilla GraphFormers (Yang et al., 2021).\nBERT (Devlin et al., 2019) is a language model pre-\ntrained with masked language modeling and next\nsentence prediction objectives on Wikipedia and\nBookCorpus. SciBERT (Beltagy et al., 2019) uti-\nlizes the same pretraining strategies as BERT but\nis trained on 1.14 million paper abstracts and full\ntext from Semantic Scholar. SPECTER (Cohan\net al., 2020) is a language model continuously pre-\ntrained from SciBERT with a contrastive objective\non 146K scientific papers. SimCSE (Gao et al.,\n2021) is a contrastive learning framework and we\nperform the experiment with the models pretrained\nfrom both unsupervised settings (Wikipedia) and\nsupervised settings (NLI). LinkBERT (Yasunaga\net al., 2022) is a language model pretrained with\nmasked language modeling and document relation\nprediction objectives on Wikipedia and BookCor-\npus. GraphFormers (Yang et al., 2021) is a GNN-\nnested Transformer and we initialize it with the\nBERT checkpoint for a fair comparison. The sec-\nond category includes several continuous pretrain-\ning methods (Gururangan et al., 2020; Gao et al.,\n2021). We perform continuous masked language\nmodeling starting from the BERT checkpoint (de-\nnoted as BERT.MLM) and the SciBERT check-\npoint (denoted as SciBERT.MLM) on our data, re-\nspectively. We also perform in-domain supervised\ncontrastive pretraining with the method proposed in\n(Gao et al., 2021) (denoted as SimCSE.in-domain).\nAblation Setup. For academic networks, we pre-\ntrain our model starting from the BERT-base 2\ncheckpoint (PATTON ) and the SciBERT 3 check-\npoint (Sci PATTON ) respectively; while for e-\ncommerce networks, we pretrain our model from\nBERT-base only (PATTON ). Furthermore, we con-\nduct ablation studies to validate the effectiveness\nof both the NMLM and the MNP strategies. The\npretrained model with NMLM removed and that\nwith MNP removed are called “w/o NMLM” and\n“w/o MNP”, respectively. In academic networks,\nthe ablation study is done on SciPATTON , while in\ne-commerce networks, it is done on PATTON .\nWe demonstrate the effectiveness of our frame-\nwork on four downstream tasks, including classifi-\ncation, retrieval, reranking, and link prediction.\n4.2 Classification\nIn this section, we conduct experiments on 8-shot\ncoarse-grained classification for nodes in the net-\nworks. We use the final layer hidden state of\n[CLS] token from language models as the rep-\nresentation of the node and feed it into a lin-\near layer classifier to obtain the prediction re-\nsult. Both the language model and the classi-\nfier are finetuned. The experimental results are\nshown in Table 2. From the result, we can find\n2https://huggingface.co/bert-base-uncased\n3https://huggingface.co/allenai/scibert_scivocab_uncased\n7009\nTable 2: Experiment results on Classification. We show the meanstd of three runs for all the methods.\nMethod Mathematics Geology Economics Clothes SportsMacro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1\nBERT 18.140.07 22.040.32 21.970.87 29.630.36 14.170.08 19.770.12 45.101.47 68.542.25 31.880.23 34.580.56\nGraphFormers18.690.52 23.240.46 22.640.92 31.021.16 13.681.03 19.001.44 46.271.92 68.972.46 43.770.63 50.470.78\nSciBERT 23.500.64 23.102.23 29.491.25 37.821.89 15.910.48 21.320.66 - - - -SPECTER 23.370.07 29.830.96 30.400.48 38.540.77 16.160.17 19.840.47 - - - -SimCSE (unsup)20.120.08 26.110.39 38.780.19 38.550.17 14.540.26 19.070.43 42.702.32 58.720.34 41.910.85 59.190.55\nSimCSE (sup)20.390.07 25.560.00 25.660.28 33.890.40 15.030.53 18.641.32 52.820.87 75.540.98 46.690.10 59.190.55\nLinkBERT 15.780.91 19.751.19 24.080.58 31.320.04 12.710.12 16.390.22 44.942.52 65.334.34 35.600.33 38.300.09\nBERT.MLM 23.440.39 31.750.58 36.310.36 48.040.69 16.600.21 22.711.16 46.980.84 68.000.84 62.210.13 75.430.74\nSciBERT.MLM23.340.42 30.110.97 36.940.28 46.540.40 16.280.38 21.410.81 - - - -SimCSE.in-domain25.150.09 29.850.20 38.910.08 48.930.14 18.080.22 23.790.44 57.030.20 80.160.31 65.570.35 75.220.18\nPATTON 27.580.03 32.820.01 39.350.06 48.190.15 19.320.05 25.120.05 60.140.28 84.880.09 67.570.08 78.600.15\nSciPATTON 27.350.04 31.700.01 39.650.10 48.930.06 19.910.08 25.680.32 - - - -w/o NMLM 25.910.45 27.792.07 38.780.19 48.480.17 18.860.23 24.250.26 56.680.24 80.270.17 65.830.28 76.240.54\nw/o MNP 24.790.65 29.441.50 38.000.73 47.821.06 18.690.59 25.631.44 47.351.20 68.502.60 64.231.53 76.031.67\nthat: 1) PATTON and SciPATTON consistently out-\nperform baseline methods; 2) Continuous pre-\ntraining method (BERT.MLM, SciBERT.MLM,\nSimCSE.in-domain, PATTON , and SciPATTON ) can\nhave better performance than off-the-shelf PLMs,\nwhich demonstrates that domain shift exists be-\ntween the pretrained PLM domain and the target\ndomain, and the adaptive pretraining on the target\ndomain is necessary. More detailed information on\nthe task can be found in Appendix B.\n4.3 Retrieval\nThe retrieval task corresponds to 16-shot fine-\ngrained category retrieval, where given a node, we\nwant to retrieve category names for it from a very\nlarge label space. We follow the widely-used DPR\n(Karpukhin et al., 2020) pipeline to finetune all\nthe models. In particular, the final layer hidden\nstates of [CLS] token are utilized as dense repre-\nsentations for both node and label names. Negative\nsamples retrieved from BM25 are used as hard neg-\natives. The results are shown in Table 3. From\nthe result, we can have the following observations:\n1) PATTON and Sci PATTON consistently outper-\nform all the baseline methods; 2) Continuously\npretrained models can be better than off-the-shelf\nPLMs in many cases (SciBERT and SPECTER per-\nform well on Mathematics and Economics since\ntheir pretrained corpus includes a large number of\nComputer Science papers, which are semantically\nclose to Mathematics and Economics papers) and\ncan largely outperform traditional BM25. More\ndetailed information on the task can be found in\nAppendix C.\n4.4 Reranking\nThe reranking task corresponds to the 32-shot fine-\ngrained category reranking. We first adopt BM25\n(Robertson et al., 2009) and exact matching as the\nretriever to obtain a candidate category name list\nfor each node. Then, the models are asked to rerank\nall the categories in the list based on their similarity\nto the given node text. The way to encode the node\nand category names is the same as that in retrieval.\nUnlike retrieval, reranking tests the ability of the\nlanguage model to distinguish among candidate\ncategories at a fine-grained level. The results are\nshown in Table 4. From the result, we can find\nthat PATTON and SciPATTON consistently outper-\nform all baseline methods, demonstrating that our\npretraining strategies allow the language model to\nbetter understand fine-grained semantic similarity.\nMore detailed information on the task can be found\nin Appendix D.\n4.5 Link Prediction\nIn this section, we perform the 32-shot link predic-\ntion for nodes in the network. Language models are\nasked to give a prediction on whether there should\nexist an edge between two nodes. It is worth not-\ning that the edge semantics here (“author overlap”\n4 for academic networks and “co-purchased” for\ne-commerce networks) are different from those in\npretraining (“citation” for academic networks and\n“co-viewed” for e-commerce networks). We utilize\nthe final layer [CLS] token hidden state as node rep-\nresentation and conduct in-batch evaluations. The\nresults are shown in Table 5. From the result, we\ncan find that PATTON and SciPATTON can outper-\nform baselines and ablations in most cases, which\nshows that our pretraining strategies can help the\nlanguage model extract knowledge from the pre-\ntrained text-rich network and apply it to the new\nlink type prediction. More detailed information on\nthe task can be found in Appendix E.\n4Two papers have at least one same author.\n7010\nTable 3: Experiment results on Retrieval. We show the meanstd of three runs for all the methods.\nMethod Mathematics Geology Economics Clothes SportsR@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100\nBM25 20.76 24 .55 19 .02 20 .92 19 .14 22 .49 15 .76 15 .88 22 .00 23 .96\nBERT 16.730.17 22.660.18 18.820.39 25.940.39 23.950.25 31.540.21 40.771.68 50.401.41 32.371.09 43.320.96\nGraphFormers16.650.12 22.410.10 18,920.60 25.940.39 24.480.36 32.160.40 41.772.05 51.262.27 32.390.89 43.291.12\nSciBERT 24.700.17 33.550.31 23.710.89 30.940.95 29.800.66 38.660.52 - - - -SPECTER 23.860.25 31.110.31 26.561.05 34.041.32 31.260.15 40.790.11 - - - -SimCSE (unsup)17.910.26 23.190.29 20.450.20 26.820.26 25.830.23 33.420.28 44.900.35 54.760.38 38.810.35 49.300.44\nSimCSE (sup)20.290.41 26.230.51 22.340.49 29.630.55 28.070.38 36.510.37 44.690.59 54.700.77 40.310.43 50.550.41\nLinkBERT 17.250.30 23.210.47 17.140.75 23.050.74 22.690.30 30.770.36 28.662.97 37.793.82 31.970.54 41.770.67\nBERT.MLM 20.690.21 27.170.25 32.130.36 41.740.42 27.130.04 36.000.14 52.411.71 63.721.79 54.100.81 63.140.83\nSciBERT.MLM20.650.21 27.670.32 31.650.71 40.520.76 29.230.67 39.180.73 - - - -SimCSE.in-domain24.540.05 31.660.09 33.970.07 44.090.19 28.440.31 37.810.27 61.420.84 72.250.86 53.770.22 63.730.30\nPATTON 27.440.15 34.970.21 34.940.23 45.010.28 32.100.51 42.190.62 68.620.38 77.540.19 58.630.31 68.530.55\nSciPATTON 31.400.52 40.380.66 40.690.52 51.310.48 35.820.69 46.050.69 - - - -w/o NMLM 30.850.14 39.890.23 39.290.07 49.590.11 35.170.31 46.070.20 65.600.26 75.190.32 57.050.14 67.220.12\nw/o MNP 22.470.07 30.200.15 31.280.89 40.540.97 29.540.36 39.570.57 60.200.73 69.850.52 51.730.41 60.350.78\nTable 4: Experiment results on Reranking. We show the meanstd of three runs for all the methods.\nMethod Mathematics Geology Economics Clothes SportsNDCG@5 NDCG@10 NDCG@5 NDCG@10 NDCG@5 NDCG@10 NDCG@5 NDCG@10 NDCG@5 NDCG@10\nBERT 37.150.64 44.760.59 56.591.18 68.210.96 42.650.70 53.550.76 62.190.63 72.000.70 44.680.56 57.540.55\nGraphFormers37.850.32 47.890.69 58.321.22 69.911.19 41.820.65 52.670.76 62.110.87 72.020.73 44.490.71 57.350.50\nSciBERT 40.730.50 53.220.51 57.041.05 69.470.92 43.240.79 55.220.67 - - - -SPECTER 38.950.67 52.170.71 57.790.69 69.570.46 43.411.10 55.801.02 - - - -SimCSE (unsup)32.340.43 42.590.44 49.601.04 61.511.03 36.370.67 47.180.76 57.031.27 68.161.04 43.290.16 55.410.09\nSimCSE (sup)34.850.60 44.760.59 48.070.54 59.790.51 37.010.40 48.050.44 52.740.55 64.280.52 42.000.09 53.920.13\nLinkBERT 38.501.15 50.741.12 59.570.96 71.410.93 44.001.12 55.780.95 58.241.93 70.481.58 48.451.02 61.631.01\nBERT.MLM 39.240.47 51.180.35 60.580.29 72.520.28 44.300.68 55.840.69 60.510.31 71.360.28 45.704.49 57.084.60\nSciBERT.MLM39.030.48 52.340.39 62.010.55 74.580.47 46.430.21 58.600.21 - - - -SimCSE.in-domain40.370.30 53.800.24 61.130.75 73.890.57 45.270.13 58.330.13 64.810.49 75.770.24 50.050.62 62.560.29\nPATTON 42.080.17 55.300.17 61.410.62 74.020.49 46.520.53 59.250.44 66.260.81 77.010.55 52.160.44 64.960.37\nSciPATTON 47.100.49 60.860.55 63.480.25 75.860.18 51.190.33 63.860.34 - - - -w/o NMLM 41.430.16 55.280.21 62.841.79 75.361.43 46.052.04 59.391.91 63.711.11 74.750.81 52.120.13 65.350.14\nw/o MNP 43.560.53 57.140.52 62.420.47 74.910.40 48.070.30 60.570.32 63.880.47 74.010.36 47.810.56 59.680.54\n4.6 Ablation Study\nWe perform ablation studies to validate the effec-\ntiveness of the two strategies in Tables 2-5. The\nfull method is better than each ablation version in\nmost cases, except R@100 on Economy retrieval,\nNDCG@10 on Sports reranking, and link predic-\ntion on Amazon datasets, which indicates the im-\nportance of both strategies.\n4.7 Pretraining Step Study\nWe conduct an experiment on the Sports dataset\nto study how the pretrained checkpoint at different\npretraining steps can perform on downstream tasks.\nThe result is shown in Figure 3. From the figure, we\ncan find that: 1) The downstream performance on\nretrieval, reranking, and link prediction generally\nimproves as the pretraining step increases. This\nmeans that the pretrained language model can learn\nmore knowledge, which can benefit these down-\nstream tasks from the pretraining text-rich network\nas the pretraining step increases. 2) The down-\nstream performance on classification increases and\nthen decreases. The reason is that for downstream\nclassification, when pretrained for too long, the\npretrained language model may overfit the given\ntext-rich network, which will hurt classification\nperformance.\n4.8 Scalability Study\nWe run an experiment on Sports to study the time\ncomplexity and memory complexity of the pro-\nposed pretraining strategies. The model is pre-\ntrained for 10 epochs on four Nvidia A6000 GPU\ndevices with a total training batch size set as 512.\nWe show the result in Table 6. From the result, we\ncan find that: 1) Pretraining with the MNP strategy\nis faster and memory cheaper than pretraining with\nthe NMLM strategy. 2) Combining the two strate-\ngies together will not increase the time complexity\nand memory complexity too much, compared with\nNMLM pretraining only.\nFurther model studies on finetune data size can\nbe found in Appendix F.\n5 Attention Map Study\nWe conduct a case study by showing some attention\nmaps of PATTON and the model without pretraining\non four downstream tasks on Sports. We randomly\npick a token from a random sample and plot the\nself-attention probability of how different tokens (x-\n7011\nTable 5: Experiment results on Link Prediction. We show the meanstd of three runs for all the methods.\nMethod Mathematics Geology Economics Clothes SportsPREC@1 MRR PREC@1 MRR PREC@1 MRR PREC@1 MRR PREC@1 MRR\nBERT 6.600.16 12.960.34 6.240.76 12.961.34 4.120.08 9.230.15 24.170.41 34.200.45 16.480.45 25.350.52\nGraphFormers6.910.29 13.420.34 6.521.17 13.341.81 4.160.21 9.280.28 23.790.69 33.790.66 16.690.36 25.740.48\nSciBERT 14.080.11 23.620.10 7.150.26 14.110.39 5.011.04 10.481.79 - - - -SPECTER 13.440.5 21.730.65 6.850.22 13.370.34 6.330.29 12.410.33 - - - -SimCSE (unsup)9.850.10 16.280.12 7.470.55 14.240.89 5.720.26 11.020.34 30.510.09 40.400.10 22.990.07 32.470.06\nSimCSE (sup)10.350.52 17.010.72 10.100.04 17.800.07 5.720.26 11.020.34 35.420.06 46.070.06 27.070.15 37.440.16\nLinkBERT 8.050.14 13.910.09 6.400.14 12.990.17 2.970.08 6.790.15 30.330.56 39.590.64 19.830.09 28.320.04\nBERT.MLM 17.550.25 29.220.26 14.130.19 25.360.20 9.020.09 16.720.15 42.710.31 54.540.35 29.360.09 41.600.05\nSciBERT.MLM22.440.08 34.220.05 16.220.03 27.020.07 9.800.00 17.720.01 - - - -SimCSE.in-domain33.550.05 46.070.07 24.560.06 36.890.11 16.770.10 26.930.01 60.410.03 71.860.06 49.170.04 63.480.03\nPATTON 70.410.11 80.210.04 44.760.05 57.710.04 57.040.05 68.350.04 58.590.12 70.120.12 46.680.09 60.960.23\nSciPATTON 71.220.17 80.790.10 44.950.24 57.840.25 57.360.26 68.710.31 - - - -w/o NMLM 71.040.13 80.600.07 44.330.23 57.290.22 56.640.25 68.120.16 60.300.03 71.670.07 49.720.06 63.760.04\nw/o MNP 63.060.23 74.260.11 33.840.60 47.020.65 44.460.03 57.050.04 49.620.06 61.610.01 36.050.20 49.780.25\n5k 15k 25k  35k   45k    55k     65k\nstep\n0.76\n0.77\n0.78\n0.79\n0.80Micro-F1\n(a) Classification\n5k 15k 25k  35k   45k    55k     65k\nstep\n0.54\n0.56\n0.58R@50 (b) Retrieval\n5k 15k 25k  35k   45k    55k     65k\nstep\n0.61\n0.62\n0.63\n0.64\n0.65NDCG@10 (c) Reranking\n5k 15k 25k  35k   45k    55k     65k\nstep\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47PREC@1 (d) Link Prediction\nFigure 3: Pretrain step study on Amazon-Sports. The downstream performance on retrieval, reranking and link\nprediction generally improves when pretrained for longer, while the performance on classification improves and\nthen drops.\nTable 6: Time scalability and memory scalability study\non Amazon-Sports.\nAttribute NMLM MNP NMLM+MNP\nTime 15h 37min 14h 53min 15h 39min\nMemory 32,363MB 30,313MB 32,365MB\naxis), including neighbor virtual token ([n_CLS])\nand the first eight original text tokens ([tk_x]), will\ncontribute to the encoding of this random token\nin different layers (y-axis). The result is shown\nin Figure 4. From the result, we can find that the\nneighbor virtual token is more deactivated for the\nmodel without pretraining, which means that the\ninformation from neighbors is not fully utilized\nduring encoding. However, the neighbor virtual\ntoken becomes more activated after pretraining,\nbringing more useful information from neighbors\nto enhance center node text encoding.\n6 Related Work\n6.1 Pretrained Language Models\nPretrained language models have been very suc-\ncessful in natural language processing since they\nwere introduced (Peters et al., 2018; Devlin et al.,\n2019). Follow-up research has made them stronger\nby scaling them up from having millions of pa-\nrameters (Yang et al., 2019; Lewis et al., 2020;\nClark et al., 2020) to even trillions (Radford et al.,\n2019; Raffel et al., 2020; Brown et al., 2020). An-\nother way that these models have been improved\nis by using different training objectives, including\nmasked language modeling (Devlin et al., 2019),\nauto-regressive causal language modeling (Brown\net al., 2020), permutation language modeling (Yang\net al., 2019), discriminative language modeling\n(Clark et al., 2020), correcting and contrasting\n(Meng et al., 2021) and document relation mod-\neling (Yasunaga et al., 2022). However, most of\nthem are designed for modeling texts exclusively,\nand do not consider the inter-document structures.\nIn this paper, we innovatively design strategies to\ncapture the semantic hints hidden inside the com-\nplex document networks.\n6.2 Domain Adaptation in NLP\nLarge language models have demonstrated their\npower in various NLP tasks. However, their per-\nformance under domain shift is quite constrained\n(Ramponi and Plank, 2020). To overcome the neg-\native effect caused by domain shift, continuous\npretraining is proposed in recent works (Gururan-\ngan et al., 2020), which can be further categorized\ninto domain-adaptive pretraining (Han and Eisen-\nstein, 2019) and task-specific pretraining (Howard\nand Ruder, 2018). However, existing works mainly\nfocus on continuous pretraining based on textual in-\n7012\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n(a) Classification w/o pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n (b) Retrieval w/o pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n (c) Reranking w/o pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n (d) Link Prediction w/o pre-\ntrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n(e) Classification w/ pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n (f) Retrieval w/ pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n (g) Reranking w/ pretrain\n[n_CLS]\n[tk_1]\n[tk_2]\n[tk_3]\n[tk_4]\n[tk_5]\n[tk_6]\n[tk_7]\n[tk_8]\nT okens\n11109876543210\n# Transformer Layer\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n (h) Link Prediction w/ pretrain\nFigure 4: Attention map study on Amazon-Sports. [n_CLS] refers to network neighbor virtual token and [tk_x]s refer\nto word tokens. [n_CLS] is more activated after pretraining (PATTON ), which means that more useful information\nfrom network neighbors is extracted to enhance center node text encoding.\nformation, while our work tries to conduct pretrain-\ning utilizing textual signal and network structure\nsignal simultaneously.\n6.3 Pretraining on Graphs\nInspired by the recent success of pretrained lan-\nguage models, researchers are starting to explore\npretraining strategies for graph neural networks\n(Hu et al., 2020b; Qiu et al., 2020; Hu et al., 2020a).\nFamous strategies include graph autoregressive\nmodeling (Hu et al., 2020b), masked component\nmodeling (Hu et al., 2020a), graph context predic-\ntion (Hu et al., 2020a) and constrastive pretraining\n(Qiu et al., 2020; Velickovic et al., 2019; Sun et al.,\n2020). These works conduct pretraining for graph\nneural network utilizing network structure informa-\ntion and do not consider the associated rich textual\nsignal. However, our work proposes to pretrain the\nlanguage model, adopting both textual information\nand network structure information.\n7 Conclusions\nIn this work, we introduce PATTON , a method to\npretrain language models on text-rich networks.\nPATTON consists of two objectives: (1) a network-\ncontextualized MLM pretraining objective and (2)\na masked node prediction objective, to capture the\nrich semantics information hidden inside the com-\nplex network structure. We conduct experiment\non four downstream tasks and five datasets from\ntwo different domains, where PATTON outperforms\nbaselines significantly and consistently.\nAcknowledgments\nWe thank anonymous reviewers for their valu-\nable and insightful feedback. Research was sup-\nported in part by US DARPA KAIROS Program\nNo. FA8750-19-2-1004 and INCAS Program No.\nHR001121C0165, National Science Foundation\nIIS-19-56151, IIS-17-41317, and IIS 17-04532,\nand the Molecule Maker Lab Institute: An AI Re-\nsearch Institutes program supported by NSF under\nAward No. 2019897, and the Institute for Geospa-\ntial Understanding through an Integrative Discov-\nery Environment (I-GUIDE) by NSF under Award\nNo. 2118329. Any opinions, findings, and con-\nclusions or recommendations expressed herein are\nthose of the authors and do not necessarily rep-\nresent the views, either expressed or implied, of\nDARPA or the U.S. Government. The views and\nconclusions contained in this paper are those of the\nauthors and should not be interpreted as represent-\ning any funding agencies.\n7013\nLimitations\nIn this work, we mainly focus on language model\npretraining on homogeneous text-rich networks and\nexplore how pretraining can benefit classification,\nretrieval, reranking, and link prediction. Interest-\ning future studies include 1) researching how to\nconduct pretraining on heterogeneous text-rich net-\nworks and how to characterize the edges of differ-\nent semantics; 2) exploring how pretraining can\nbenefit broader task spaces including summariza-\ntion and question answering.\nEthics Statement\nWhile it has been shown that PLMs are powerful in\nlanguage understanding (Devlin et al., 2019; Lewis\net al., 2020; Raffel et al., 2020), there are studies\nhighlighting their drawbacks such as the presence\nof social bias (Liang et al., 2021) and misinforma-\ntion (Abid et al., 2021). In our work, we focus\non pretraining PLMs with information from the\ninter-document structures, which could be a way to\nmitigate bias and eliminate the contained misinfor-\nmation.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent anti-muslim bias in large language models.\nIn AIES.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: Pretrained language model for scientific text. In\nEMNLP.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nICLR.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S. Weld. 2020. SPECTER:\nDocument-level Representation Learning using\nCitation-informed Transformers. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. NAACL.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In EMNLP.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nadapt language models to domains and tasks. ACL.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsuper-\nvised domain adaptation of contextualized embed-\ndings for sequence labeling. EMNLP.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nACL.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zit-\nnik, Percy Liang, Vijay Pande, and Jure Leskovec.\n2020a. Strategies for pre-training graph neural net-\nworks. ICLR.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei\nChang, and Yizhou Sun. 2020b. Gpt-gnn: Gener-\native pre-training of graph neural networks. In KDD.\nBowen Jin, Yu Zhang, Yu Meng, and Jiawei Han. 2023a.\nEdgeformers: Graph-empowered transformers for\nrepresentation learning on textual-edge networks. In\nICLR.\nBowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. 2023b.\nHeterformer: Transformer-based deep node represen-\ntation learning on heterogeneous text-rich networks.\nKDD.\nBharath Kandimalla, Shaurya Rohatgi, Jian Wu, and\nC Lee Giles. 2021. Large scale subject category\nclassification of scholarly papers with deep attentive\nneural networks. Frontiers in research metrics and\nanalytics, 5:600382.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. EMNLP.\nOnur Küçüktunç, Erik Saule, Kamer Kaya, and Ümit V\nÇatalyürek. 2012. Recommendation on academic\nnetworks using direction aware citation analysis.\narXiv preprint arXiv:1205.1143.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2020. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\nACL.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understanding\nand mitigating social biases in language models. In\nICML.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n7014\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. 2015. Image-based recom-\nmendations on styles and substitutes. In SIGIR.\nYu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett,\nJiawei Han, Xia Song, et al. 2021. Coco-lm: Cor-\nrecting and contrasting text sequences for language\nmodel pretraining. NeurIPS.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang,\nHongxia Yang, Ming Ding, Kuansan Wang, and Jie\nTang. 2020. Gcc: Graph contrastive coding for graph\nneural network pre-training. In KDD.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nAlan Ramponi and Barbara Plank. 2020. Neural unsu-\npervised domain adaptation in nlp—a survey. COL-\nING.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval.\nChristian Schulz, Amin Mazloumian, Alexander M Pe-\ntersen, Orion Penner, and Dirk Helbing. 2014. Ex-\nploiting citation networks for large-scale author name\ndisambiguation. EPJ Data Science.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Hsu, and Kuansan Wang. 2015. An\noverview of microsoft academic service (mas) and ap-\nplications. In Proceedings of the 24th international\nconference on world wide web, pages 243–246.\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian\nTang. 2020. Infograph: Unsupervised and semi-\nsupervised graph-level representation learning via\nmutual information maximization. ICLR.\nPetar Velickovic, William Fedus, William L Hamil-\nton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm.\n2019. Deep graph infomax. ICLR.\nXiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang\nYe, Peng Cui, and Philip S Yu. 2019. Heterogeneous\ngraph attention network. In WWW.\nJunhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo\nLi, Defu Lian, Sanjay Agrawal, Amit Singh,\nGuangzhong Sun, and Xing Xie. 2021. Graphform-\ners: Gnn-nested transformers for representation learn-\ning on textual graph. In NeurIPS.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. NeurIPS.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. ACL.\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram\nSwami, and Nitesh V Chawla. 2019. Heterogeneous\ngraph neural network. In KDD.\n7015\nA Pretrain Settings\nTo facilitate the reproduction of our pretraining\nexperiment, we provide the hyperparameter config-\nuration in Table 7. All reported continuous pretrain-\ning and in-domain pretraining methods use exactly\nthe same set of hyperparameters for pretraining\nfor a fair comparison. All GraphFormers (Yang\net al., 2021) involved methods have the neighbor\nsampling number set as 5. Paper titles and item\ntitles are used as text associated with the nodes\nin the two kinds of networks, respectively. (For\nsome items, we concatenate the item title and de-\nscription together since the title is too short.) Since\nmost paper titles (88%) and item titles (97%) are\nwithin 32 tokens, we set the max length of the in-\nput sequence to be 32. The models are trained\nfor 5/10/30 epochs (depending on the size of the\nnetwork) on 4 Nvidia A6000 GPUs with a total\nbatch size of 512. The total time cost is around 24\nhours for each network. Code is available athttps:\n//github.com/PeterGriffinJin/Patton.\nB Classification\nTask. The coarse-grained category names for aca-\ndemic networks and e-commerce networks are\nthe first-level category names in the network-\nassociated category taxonomy. We train all the\nmethods in the 8-shot setting (8 labeled train-\ning samples and 8 labeled validation samples for\neach class) and test the models with hundreds of\nthousands of new query nodes (220,681, 215,148,\n85,346, 477,700, and 129,669 for Mathematics,\nGeology, Economics, Clothes, and Sports respec-\ntively). Detailed information on all category names\ncan be found in Table 8-12.\nFinetuning Settings. All reported methods use\nexactly the same set of hyperparameters for fine-\ntuning for a fair comparison. The median results\nof three runs with the same set of three different\nrandom seeds are reported. For all the methods,\nwe finetune the model for 500 epochs in total. The\npeak learning rate is 1e-5, with the first 10% steps\nas warm-up steps. The training batch size and the\nvalidation batch size are both 256. During training,\nwe validate the model every 25 steps and the best\ncheckpoint is utilized to perform prediction on the\ntest set. The experiments are carried out on one\nNvidia A6000 GPU.\nC Retrieval\nTask. The retrieval task corresponds to fine-\ngrained category retrieval. Given a node in the\nnetwork, we aim to retrieve its fine-grained labels\nfrom a large label space. We train all the com-\npared methods in the 16-shot setting (16 labeled\nqueries in total) and test the models with tens of\nthousands of new query nodes (38,006, 33,440,\n14,577, 95,731, and 34,979 for Mathematics, Geol-\nogy, Economics, Clothes, and Sports, respectively).\nThe fine-grained label spaces for both academic net-\nworks and e-commerce networks are constructed\nfrom all the labels in the network-associated tax-\nonomy 5 6 . The statistics of the label space for all\nnetworks can be found in Table 1.\nFinetuning Settings. We finetune the models\nwith the widely-used DPR pipeline (Karpukhin\net al., 2020). All reported methods use exactly\nthe same set of hyperparameters for finetuning for\na fair comparison. The median results of three runs\nwith the same set of three different random seeds\nare reported. For all the methods, we finetune the\nmodel for 1,000 epochs with the training data. The\npeak learning rate is 1e-5, with the first 10% steps\nas warm-up steps. The training batch size is 128.\nThe number of hard BM25 negative samples 7 is\nset as 4. We utilize the faiss library 8 to perform\nan approximate search for nearest neighbors. The\nexperiments are carried out on one Nvidia A6000\nGPU.\nD Reranking\nTask. The reranking task corresponds to fine-\ngrained category reranking. Given a retrieved cate-\ngory list for the query node, we aim to rerank all\ncategories within the list. We train all the methods\nin the 32-shot setting (32 training queries and 32\nvalidation queries) and test the models with 10,000\nnew query nodes and candidate list pairs. The cat-\negory space in reranking is the same as that in\nretrieval. In our experiment, the retrieved category\nlist is constructed with BM25 and exact matching\nof category names.\nFinetuning Settings. All reported methods use\nexactly the same set of hyperparameters for fine-\n5https://www.microsoft.com/en-\nus/research/project/academic/articles/visualizing-the-\ntopic-hierarchy-on-microsoft-academic/\n6http://jmcauley.ucsd.edu/data/amazon/links.html\n7https://github.com/dorianbrown/rank_bm25\n8https://github.com/facebookresearch/faiss\n7016\nTable 7: Pretraining hyper-parameter configuration.\nParameter Mathematics Geology Economics Clothes Sports\nMax Epochs 30 10 30 5 10\nPeak Learning Rate 1e-5 1e-5 1e-5 1e-5 1e-5\nBatch Size 512 512 512 512 512\nWarm-Up Epochs 3 1 3 0.5 1\nSequence Length 32 32 32 32 32\nAdamϵ 1e-8 1e-8 1e-8 1e-8 1e-8\nAdam(β1,β2) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999)\nClip Norm 1.0 1.0 1.0 1.0 1.0\nDropout 0.1 0.1 0.1 0.1 0.1\nTable 8: Class names of MAG-Mathematics.\n0 mathematical optimization 5 econometrics 10 control theory 15 computational science\n1 mathematical analysis 6 mathematical physics 11 geometry 16 mathematics education\n2 combinatorics 7 statistics 12 applied mathematics 17 arithmetic\n3 algorithm 8 pure mathematics 13 operations research\n4 algebra 9 discrete mathematics 14 mathematical economics\ntuning for a fair comparison. The median results\nof three runs with the same set of three different\nrandom seeds are reported. For all the methods,\nwe finetune the model for 1,000 epochs in total\nwith the training data. The peak learning rate is\n1e-5, with the first 10% steps as warm-up steps.\nThe training batch size and validation batch size\nare 128 and 256, respectively. During training, the\nmodel is validated every 1,000 steps and the best\ncheckpoint is utilized to conduct inference on the\ntest set. The experiments are carried out on one\nNvidia A6000 GPU.\nE Link Prediction\nTask. The task aims to predict if there should\nexist an edge with specific semantics between two\nnodes. It is worth noting that the semantics of the\nedge here is different from the semantics of the\nedge in the pretraining text-rich network. In aca-\ndemic networks, the edge semantics in the pretrain-\ning network is “citation”, while the edge semantics\nin downstream link prediction is “author overlap”\n9. In e-commerce networks, the edge semantics in\nthe pretraining network is “co-viewed”, while the\nedge semantics in the prediction of the downstream\nlink is “co-purchased”. We train all the methods in\nthe 32-shot setting (32 training labeled pairs and 32\nvalidation labeled pairs) and test the models with\n10,000 new node pairs. We utilize in-batch sam-\nples as negative samples in training to finetune the\nmodel and in testing to evaluate the performance\n9Two papers have at least one same author.\nof the methods.\nFinetuning Settings. All reported methods use\nexactly the same set of hyperparameters for fine-\ntuning for a fair comparison. The median results\nof three runs with the same set of three different\nrandom seeds are reported. For all the methods,\nwe finetune the model for 200 epochs in total. The\npeak learning rate is 1e-5, with the first 10% step\nas warm-up steps. The training batch size and val-\nidation batch size are 128 and 256, respectively.\nDuring training, we validate the model in 20 steps\nand use the best checkpoint to perform the predic-\ntion on the test set. The experiments are carried out\non one Nvidia A6000 GPU.\nF Finetuning Data Size Study\nWe conduct a parameter study to explore how ben-\neficial our pretraining method is to downstream\ntasks with different amounts of finetuning data on\nthe four tasks on Sports. The results are shown in\nFigure 5, where we can find that: 1) As finetuning\ndata increases, the performance of both PATTON\nand the model without pretraining (GraphForm-\ners) improves. 2) The performance gap between\nPATTON and the model without pretraining (Graph-\nFormers) becomes smaller as finetuning data in-\ncreases, but PATTON is consistently better than the\nmodel without pretraining (GraphFormers).\n7017\nTable 9: Class names of MAG-Geology.\n0 geomorphology 5 paleontology 10 petrology 15 mining engineering\n1 seismology 6 climatology 11 geotechnical engineering 16 petroleum engineering\n2 geochemistry 7 atmospheric sciences 12 soil science\n3 mineralogy 8 geodesy 13 earth science\n4 geophysics 9 oceanography 14 remote sensing\nTable 10: Class names of the MAG-Economics\n0 mathematical economics 10 economy 20 development economics 30 economic policy\n1 labour economics 11 monetary economics 21 international trade 31 market economy\n2 finance 12 operations management 22 keynesian economics 32 environmental economics\n3 econometrics 13 actuarial science 23 positive economics 33 classical economics\n4 macroeconomics 14 industrial organization 24 agricultural economics 34 management science\n5 microeconomics 15 political economy 25 international economics 35 management\n6 economic growth 16 commerce 26 demographic economics 36 welfare economics\n7 financial economics 17 socioeconomics 27 neoclassical economics 37 economic system\n8 public economics 18 financial system 28 natural resource economics 38 environmental resource management\n9 law and economics 19 accounting 29 economic geography 39 economic history\nTable 11: Class names of Amazon-Clothes.\n0 girls 3 luggage 5 fashion watches 7 boys\n1 men 4 baby 6 shoes 8 adidas\n2 novelty\nTable 12: Class name of Amazon-Sports.\n0 accessories 4 cycling 8 golf 12 paintball & airsoft\n1 action sports 5 baby 9 hunting & fishing & game room 13 racquet sports\n2 boating & water sports 6 exercise & leisure sports 10 outdoor gear 14 snow sports\n3 clothing 7 fan shop 11 fitness 15 team sports\n0 2000 4000 6000 8000\ndata\n0.5\n0.6\n0.7\n0.8Macro-F1 GraphFormers\nOurs\n(a) Classification\n0 20000 40000 60000\ndata\n0.4\n0.6\n0.8R@50\nGraphFormers\nOurs (b) Retrieval\n0 10000 20000 30000\ndata\n0.5\n0.6\n0.7\n0.8NDCG@5 GraphFormers\nOurs (c) Reranking\n0 10000 20000 30000\ndata\n0.2\n0.3\n0.4\n0.5PREC@1 GraphFormers\nOurs (d) Link Prediction\nFigure 5: Finetuning data size study on Amazon-Sports. As finetuning data size increases, both the performance\nof our proposed PATTON and the model without pretraining (GraphFormers) improves. PATTON consistently\noutperforms the language model without pretraining (GraphFormers).\n7018\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the Limitations section, which is after 6. Conclusion.\n□\u0013 A2. Did you discuss any potential risks of your work?\nIn the Ethics Statement section, which is after the Limitations section.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and 1. Introduction section.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4. Experiment section\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4. Experiment section\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n4. Experiment section, Appendix B, C, D, E, F\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4. Experiment section, Appendix B, C, D, E, F\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\n4. Experiment section, Appendix B, C, D, E, F\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4. Experiment section, Appendix B, C, D, E, F\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4. Experiment section, Appendix B, C, D, E, F\nC □\u0013 Did you run computational experiments?\nIn 4. Experiment section.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.1, 4.8, Appendix B, C, D, E, F\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7019\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.1, Appendix B, C, D, E, F\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.1, 4.2, 4.3, 4.4, 4.5, Appendix B, C, D, E, F\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.1, 4.2, 4.3, 4.4, 4.5, Appendix B, C, D, E, F\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n7020",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8453697562217712
    },
    {
      "name": "Natural language processing",
      "score": 0.6823835968971252
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6306573748588562
    },
    {
      "name": "Artificial intelligence",
      "score": 0.597382664680481
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.527488112449646
    },
    {
      "name": "Language model",
      "score": 0.5152466893196106
    },
    {
      "name": "Node (physics)",
      "score": 0.4131471514701843
    },
    {
      "name": "Information retrieval",
      "score": 0.3566718101501465
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}