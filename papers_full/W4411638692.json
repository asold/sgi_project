{
  "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
  "url": "https://openalex.org/W4411638692",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2554743009",
      "name": "Janice Ahn",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A2904271762",
      "name": "Rishu Verma",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A3160512288",
      "name": "Renze Lou",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A2108392518",
      "name": "Di Liu",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A1970544490",
      "name": "Wenpeng Yin",
      "affiliations": [
        "Temple University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4382567098"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics:\nStudent Research Workshop, pages 225–237\nMarch 21-22, 2024c⃝2024 Association for Computational Linguistics\nLarge Language Models for Mathematical Reasoning:\nProgresses and Challenges\nJanice Ahn♠ Rishu Verma♠ Renze Lou♠ Di Liu♢\nRui Zhang♠ and Wenpeng Yin♠\n♠The Pennsylvania State University ♢ Temple University\n{jfa5672, wenpeng}@psu.edu; diliu@temple.edu\nAbstract\nMathematical reasoning serves as a cornerstone\nfor assessing the fundamental cognitive capa-\nbilities of human intelligence. In recent times,\nthere has been a notable surge in the devel-\nopment of Large Language Models (LLMs)\ngeared towards the automated resolution of\nmathematical problems. However, the land-\nscape of mathematical problem types is vast\nand varied, with LLM-oriented techniques un-\ndergoing evaluation across diverse datasets and\nsettings. This diversity makes it challenging\nto discern the true advancements and obsta-\ncles within this burgeoning field. This survey\nendeavors to address four pivotal dimensions:\ni) a comprehensive exploration of the various\nmathematical problems and their correspond-\ning datasets that have been investigated; ii) an\nexamination of the spectrum of LLM-oriented\ntechniques that have been proposed for math-\nematical problem-solving; iii) an overview of\nfactors and concerns affecting LLMs in solving\nmath; and iv) an elucidation of the persisting\nchallenges within this domain. To the best of\nour knowledge, this survey stands as one of the\nfirst extensive examinations of the landscape\nof LLMs in the realm of mathematics, provid-\ning a holistic perspective on the current state,\naccomplishments, and future challenges in this\nrapidly evolving field.\n1 Introduction\nMathematical reasoning is crucial to human intel-\nligence, driving ongoing efforts in the AI commu-\nnity to autonomously tackle math challenges. This\npursuit inherently calls for an augmentation of AI\ncapabilities, delving into the intricate realms of tex-\ntual comprehension, image interpretation, tabular\nanalysis, symbolic manipulation, operational logic,\nand a nuanced grasp of world knowledge. As the\nAI landscape evolves, the endeavor to empower\nmachines with a comprehensive understanding of\ndiverse mathematical facets becomes not only a tes-\ntament to technological prowess but also a pivotal\nstride towards achieving a more generalized and\nadept AI.\nIn recent times, the landscape of AI has been\nreshaped by the ascendancy of Large Language\nModels (LLMs) as formidable tools for automating\nintricate tasks. Notably, LLMs have proven to be\npotent assets in unraveling the nuances of mathe-\nmatical problem-solving (Romera-Paredes et al.,\n2023; Imani et al., 2023). Their language capabili-\nties fuel focused exploration in utilizing them for\nmathematical reasoning, uncovering fresh insights\ninto the synergy between language and logic.\nHowever, amid this progress, the current state\nof LLM-oriented research in mathematics presents\na complex panorama. Diverse mathematical prob-\nlem types pose a formidable challenge, exacerbated\nby the varied evaluation metrics, datasets, and set-\ntings employed in the assessment of LLM-oriented\ntechniques (Testolin, 2023; Lu et al., 2023c). The\nlack of a unified framework hampers our ability to\ngauge the true extent of progress achieved and im-\npedes a coherent understanding of the challenges\nthat persist in this evolving field.\nThis survey endeavors to cast a spotlight on the\nmultifaceted landscape of LLMs in the realm of\nmathematics. We plan to traverse four crucial di-\nmensions: a meticulous exploration of math prob-\nlem types and the datasets associated with them;\nan in-depth analysis of the evolving techniques em-\nployed by LLMs in mathematical problem-solving;\nan examination of factors that affect the LLMs solv-\ning math problems; and a critical discussion on the\npersisting challenges that loom over this burgeon-\ning field.\nTo our knowledge, this survey marks one of the\nfirst comprehensive examinations of LLMs specif-\nically tailored for mathematics. By weaving to-\ngether insights from various dimensions, we aim to\nprovide a holistic understanding of the current state\nof affairs in LLM-driven mathematical reasoning,\nshedding light on achievements, challenges, and\n225\nthe uncharted territories that await exploration in\nthis captivating intersection of language and logic.\n2 Related Work\nTo the best of our knowledge, the existing literature\non summarizing mathematical research, particu-\nlarly within the context of LLMs, remains limited.\nNotably, Chang et al. (2023) conducted a compre-\nhensive evaluation of LLMs, incorporating an ex-\namination of their performance in mathematical\nproblem-solving, albeit with a relatively brief ex-\nploration of the mathematical field. Conversely,\nboth (Testolin, 2023) and (Lu et al., 2023c) delved\ninto the application of Deep Learning in the domain\nof mathematical reasoning. Our work distinguishes\nitself on three fronts: firstly, we concentrate on\nLLMs, providing a more in-depth analysis of their\nvarious advancements; secondly, beyond merely\nreporting progress, we engage in a thorough discus-\nsion of the challenges inherent in this trajectory;\nand thirdly, we extend our scrutiny to encompass\nthe perspective of mathematics pedagogy. In do-\ning so, we contribute a nuanced perspective that\nseeks to broaden the understanding of LLMs in the\ncontext of mathematical research.\nThe only work contemporaneous with ours is\n(Liu et al., 2023b). In comparison, our contribution\nlies in: i) not only introducing various methods\nbut also paying more attention to various factors\naffecting model performance; ii) taking a broader\nperspective on the progress of LLM in the field\nof mathematics, elucidating not only from the AI\nperspective but also from the perspective of ed-\nucation. It emphasizes that the pursuit of model\nperformance alone, while neglecting human factors,\nis something that needs attention.\n3 Math Problems & Datasets\nThis section concisely overviews prominent math-\nematical problem types and associated datasets,\nspanning ARITHMETIC , MATH WORD PROB -\nLEMS , GEOMETRY , AUTOMATED THEOREM\nPROVING , and MATH IN VISION CONTEXT .\n3.1 Arithmetic\nThis category of problems entails pure mathemati-\ncal operations and numerical manipulation, devoid\nof the need for the model to interpret text, images,\nor other contextual elements. An illustrative exam-\nple is presented below, where “ Q” denotes ques-\ntions and “A” for answers.\nQ: 21 + 97\nA: 118\nThe dataset MATH-140 (Yuan et al., 2023) con-\ntains 401 arithmetic expressions for 17 groups.\n3.2 Math Word Problems\nMATH WORD PROBLEMS (MWP ) are mathemati-\ncal exercises or scenarios presented in the form of\nwritten or verbal descriptions rather than straight-\nforward equations in ARITHMETIC . These prob-\nlems require individuals to decipher the informa-\ntion provided, identify relevant mathematical con-\ncepts, and formulate equations or expressions to\nsolve the given problem. MWP often reflect real-\nworld situations, allowing individuals to apply\nmathematical principles to practical contexts. Solv-\ning these problems typically involves critical think-\ning, problem-solving skills, and the application of\nmathematical operations to find a solution.\nMWP invariably comprise a question (Q) and\nits corresponding final answer (A) (referred to as\nQuestion-Answer). However, the presence or ab-\nsence of additional clues can give rise to various\nversions of these problems. Variations may emerge\nbased on factors such as the availability of an equa-\ntion (E; referred to as Question-Equation-Answer)\nor the provision of a step-by-step rationale ( R;\nQuestion-Rationale-Answer) to guide the problem-\nsolving process.\nQuestion-Answer. The instance of this type of\nMWP consists of a question (Q) and the final an-\nswer (A), such as:\nQ: Lily received $20 from her mum. After\nspending $10 on a storybook and $2.5 on\na lollipop, how much money does she have\nleft?\nA: $7.5\nQuestion-Equation-Answer. Compared with\nQuestion-Answer, this MWP type provides the\nequation solution, such as\nQ: Jack had 8 pens and Mary had 5 pens.\nJack gave 3 pens to Mary. How many pens\ndoes Jack have now?\nE: 8 −3\nA: 5 (optional)\nQuestion-Rationale-Answer. This type of\nMWP includes answers and reasoning paths, akin\nto the Chain-of-Thought method, which explicates\nreasoning steps rather than defining problem types\n226\nNAME SIZE LEVEL NOTE\nQ-A\nCMATH (Wei et al., 2023) 1.7K E Chinese; grade 1-6\nSAT-MATH (Zhong et al., 2023) 220 H Multi-choice\nQuestion-Equation-Answer\nSVAMP (Patel et al., 2021) 1K E Three types of variations\nASD IV (Miao et al., 2020) 2.3K E Problem type and grade level annotated\nMAWPS (Koncel-Kedziorski et al., 2016) 3.3K E Extension of ADDSUB, MULTI ARITH , etc.\nPARA MAWPS (Raiyan et al., 2023) 16K E Paraphrased, adversarial MAWPS\nSINGLE EQ (Koncel-Kedziorski et al., 2015) 508 E\nADDSUB (Hosseini et al., 2014) 395 E Only addition and subtraction\nMULTI ARITH (Roy and Roth, 2015) 600 E Multi-step reasoning\nDRAW-1K (Upadhyay and Chang, 2017) 1K E\nMATH23K (Wang et al., 2017) 23K E Chinese\nAPE210K (Zhao et al., 2020) 210K E Chinese\nK6 (Yang et al., 2023) 600 E Chinese; grade 1-6\nCM17K (Qin et al., 2021) 17K M H Chinese; grade 6-12\nQuestion-Rationale-Answer\nCARP (Zhang et al., 2023a) 4.9K M Chinese\nGSM8K (Cobbe et al., 2021) 8.5K M Linguistically diverse\nMATH (Hendrycks et al., 2021) 12.5K H Problems are put into difficulty levels 1-5\nPRM800K (Lightman et al., 2023) 12K H MATH w/ step-wise labels\nMATHQA (Amini et al., 2019) 37K C GRE examinations; have quality concern\nAQUA (Ling et al., 2017) 100K C GRE&GMAT questions\nARB (Sawada et al., 2023) 105 C Contest problems and university math proof\nGHOSTS (Frieder et al., 2023) 709 C\nTHEOREM QA-M ATH (Chen et al., 2023b) 442 C Theorem as rationale\nLILA (Mishra et al., 2022) 132K H Incorporates 20 existing datasets\nMATH-INSTRUCT (Yue et al., 2023) 260K H Instruction-following style\nTABMWP (Lu et al., 2023b) 38K H Tabular MWP; below the College level\nTable 1: Datasets for Math Word Problems.\nE = Elementary, M = Middle School, H = High School, C = College, H = Hybrid\n(Wei et al., 2022). The rationale guides correct\nproblem-solving and serves as a valuable reference\nfor model training, including fine-tuning and\nfew-shot learning.\nQ: Beth bakes 4, or 2 dozen batches of\ncookies in a week. If these cookies are\nshared amongst 16 people equally, how\nmany cookies does each person consume?\nR: Beth bakes 4 2 dozen batches of\ncookies for a total of 4 ∗2 =<< 4 ∗2 =\n8 >> 8 dozen cookies. There are 12\ncookies in a dozen and she makes 8 dozen\ncookies for a total of12∗8 =<< 12∗8 =\n96 >> 96 cookies. She splits the 96\ncookies equally amongst 16 people so\nthey each eat 96/16 =<< 96/16 = 6>>\n6 cookies.\nA: 6\nTable 1 lists most datasets that are summarized\nin three categories: Question-Answer, Question-\nEquation-Answer, and Question-Rationale-Answer.\nIn addition to the above three MWP types of con-\nventional styles, recent work studied MWP in\ngiven tables and even MWP generation.\nTabular MWP. TABMWP (Lu et al., 2023b) is\nthe first dataset to study MWP over tabular context\non open domains and is the largest in terms of data\nsize. Each problem in TABMWP is accompanied\nby a tabular context, which is represented in three\nformats: an image, a semi-structured text, and a\nstructured table.\nBEADS $/KILOGRAM\nheart-shaped 3\nrectangular 2\nspherical 2\noval 2\nTable 2: Table for the tabular MWP example.\nT: Table 2\nQ: Henrik bought 2.5 kilograms of oval\nbeads. How much did he spend? (Unit:\n$)\nA: 5\n227\nMWP Generation. Instead of deriving the an-\nswer for a given math question, this type of mathe-\nmatical reasoning tries to generateMWP questions.\nFor example, Wang et al. (2021) fine-tuned GPT-\n2 (Radford et al., 2019) on equation-to-MWP in-\nstances for MWP generation. The effectiveness of\nGPT-3’s question-generation capabilities was as-\nsessed by Zong and Krishnamachari (2023), who\ninstructed the model to generate a question similar\nto a provided MWP question. Deb et al. (2023) an-\nalyzed a group of LLMs (GPT-4, GPT-3.5, PaLM-\n2 (Anil et al., 2023), and LLaMa (Touvron et al.,\n2023a)), and found a significant drop in accuracy\nfor backward reasoning compared to forward rea-\nsoning. Norberg et al. (2023) used GPT-4 to rewrite\nhuman-written MWP , reporting optimal readabil-\nity, lexical diversity, and cohesion scores, although\nGPT-4 rewrites incorporated more low-frequency\nwords.\n3.3 Geometry\nCompared with MWP , GEOMETRY problems in-\nvolve a distinct set of challenges. While MWP of-\nten requires logical reasoning and arithmetic op-\nerations, geometry problems demand a spatial un-\nderstanding of shapes, sizes, and their interrela-\ntionships. Solving geometry problems typically\nentails applying geometric principles, theorems,\nand formulas to analyze and deduce properties of\ngeometric figures. Furthermore, current geometry\napproaches mainly rely on symbolic methods and\npredefined search heuristics, highlighting the spe-\ncialized strategies required in this domain (Trinh\net al., 2024). This contrast in problem-solving\napproaches highlights the multifaceted nature of\nmathematical challenges and the varied skill sets\nrequired in different mathematical domains. An\nexample can be seen as follows and Table 3 lists\nmainstream datasets.\na \nc \nb h \nQ: a=7 inches; b=24 inches; c=25 inches;\nh=5.4 inches; What is its area? (Unit:\nsquare inches)\nA: 24.03\nNAME SIZE\nGEOSHADER (Alvin et al., 2017) 102\nGEOS (Seo et al., 2015) 186\nGEOS++ (Sachan et al., 2017) 1.4K\nGEOS-OS (Sachan and Xing, 2017) 2.2K\nGEOMETRY 3K (Lu et al., 2021) 3K\nGEOQA (Chen et al., 2021a) 5K\nUNIGEO (Chen et al., 2022) 14.5K\nTable 3: Geometry datasets\n3.4 Automated theorem proving\nIn the specialized area of Automated Theorem\nProving (ATP), the inherent challenges are unique\nand encompass a wide spectrum, akin to those\nfound in distinct mathematical fields. ATP’s core\nfocus is on autonomously constructing proofs for\nspecified conjectures, requiring a blend of logical\nanalysis and a profound grasp of formal languages,\nsupported by an extensive knowledge base. Its\napplication is crucial in areas like the validation\nand development of both software and hardware\nsystems.\nFor example, the MINI F2F dataset (Zheng et al.,\n2022) stands out in ATP, featuring a series of com-\nplex Olympiad-level mathematical problems, de-\nsigned to evaluate theorem-proving systems includ-\ning Metamath (Yu et al., 2023), Lean (Han et al.,\n2022), and Isabelle (Wenzel et al., 2008). In a\nsimilar vein, the HOList benchmark (Bansal et al.,\n2019), with its comprehensive array of theorem\nstatements from various corpora, sets a sequential\nproving challenge for ATP systems, where each\ntheorem must be proved using only the lemmas\npreceding it. Additionally, the COQGYM dataset\n(Yang and Deng, 2019) provides a broad ATP en-\nvironment, showcasing a rich collection of more\nthan 71,000 proofs penned by humans, all within\nthe framework of the Coq proof assistant. These\ndatasets illustrate the diverse methodologies and\nskillsets necessary in ATP, reflecting the multi-\nfaceted nature of solving mathematical problems.\n3.5 Math in vision-language context\nCHART QA (Masry et al., 2022), with 9.6K human-\nwritten questions and 23.1K model-generated ques-\ntions have explored a variety of complex reasoning\nquestions that involve several logical and arithmetic\noperations over charts. MATHVISTA (Lu et al.,\n2023a): size: 6K; it features seven types of mathe-\nmatical reasoning: algebraic reasoning, arithmetic\n228\nreasoning, geometry reasoning, logical reasoning,\nnumeric common sense, scientific reasoning, and\nstatistical reasoning. In addition, fine-grained meta-\ndata are available, including question type, answer\ntype, language, source, category, task, grade level,\nand visual context.\n4 Methodologies\nWe summarize these methods into three progressive\nlevels: i) Prompting frozen LLMs, ii) Strategies en-\nhancing frozen LLMs, and iii) Fine-tuning LLMs.\n4.1 Prompting frozen LLMs\nWe organize prior work by typical LLMs.\nGPT-3. Zong and Krishnamachari (2023) eval-\nuated the use of GPT-3, a 175B parameter trans-\nformer model for three related challenges pertain-\ning to math word problems: i) classifying word\nproblems, ii) extracting equations from word prob-\nlems, and iii) generating word problems.\nChatGPT. Shakarian et al. (2023) reported the\nfirst independent evaluation of ChatGPT on MWP,\nand found that ChatGPT’s performance changes\ndramatically based on the requirement to show its\nwork. Cheng and Zhang (2023) assessed Chat-\nGPT, OpenAI’s latest conversational chatbot and\nLLM, on its performance in elementary-grade arith-\nmetic and logic problems, and found that Chat-\nGPT performed better than previous models such\nas InstructGPT (Ouyang et al., 2022) and Minerva\n(Lewkowycz et al., 2022).\nGPT-4. Wu et al. (2023) adapted and evaluated\nseveral existing prompting methods to the usage\nof GPT-4, including a vanilla prompt, Program-\nof-Thoughts prompt (Chen et al., 2023a), and Pro-\ngram Synthesis prompt (Drori et al., 2022). The\nstudy by Gu (2023) investigated the capability of\nGPT-4 to actively engage in math-oriented brain-\nstorming sessions. This includes tasks like iden-\ntifying new research problems, refining problem\nformulations, and suggesting potential methods or\nunconventional solutions, all achieved through it-\nerative ideation with a human partner—a common\npractice in collaborative brainstorming with other\nprofessionals.\nGPT4V & Bard. Lu et al. (2023a) presented\nMATHVISTA , a benchmark of evaluating math-\nematical reasoning in visual context, conducted\na comprehensive, quantitative evaluation of three\nLLMs (i.e, ChatGPT, GPT-4, Claude-2 (Bai et al.,\n2022)), two proprietary large multimodal mod-\nels (LMMs) (i.e., GPT4V , Bard), and seven\nopen-source LMMs, with Chain-of-Thought and\nProgram-of-Thought.\nMultiple. Wei et al. (2023) evaluated a variety\nof popular LLMs, including both commercial and\nopen-source options, aiming to provide a bench-\nmark tool for assessing the following question:\nto what grade level of Chinese elementary school\nmath do the abilities of popular LLMs correspond?\n4.2 Strategies enhancing frozen LLMs\nPreprocessing the math question. An et al.\n(2023a) explored ChatGPT for the datasetSVAMP\nand observed that substituting numerical expres-\nsions with English expressions can elevate the per-\nformance.\nMore advanced prompts. Chain-of-thought\n(Wei et al., 2022), the first time to steer the\nLLMs to do step-by-step math reasoning, Self-\nConsistency (Wang et al., 2023) tried multiple\nChain-of-Thought reasoning paths and leverage the\nconsistency mechanism to discover a more proba-\nble answer. Zhou et al. (2023a) proposed a novel\nand effective prompting method, explicit code-\nbased self-verification, to further boost the mathe-\nmatical reasoning potential of GPT-4 Code Inter-\npreter. This method employs a zero-shot prompt\non GPT-4 Code Interpreter to encourage it to use\ncode to self-verify its answers.\nUsing external tool. Yamauchi et al. (2023) em-\nployed an external tool, specifically the Python\nREPL, to correct errors in Chain-of-Thought. Their\ndemonstration highlighted that integrating Chain-\nof-Thought and Python REPL using a markup\nlanguage improves the reasoning capabilities of\nChatGPT. In a related context, He-Yueya et al.\n(2023) introduced an approach that merges an\nLLM, Codex (Chen et al., 2021b), capable of pro-\ngressively formalizing word problems into vari-\nables and equations, with an external symbolic\nsolver adept at solving the generated equations.\nProgram-of-Thought (Chen et al., 2023a) separates\nthe computational aspect from the reasoning by\nutilizing a Language Model (primarily Codex) to\narticulate the reasoning procedure as a program.\nThe actual computation is delegated to an external\ncomputer, responsible for executing the generated\nprograms to arrive at the desired answer.\n229\nImproving the whole interaction. Wu et al.\n(2023) introduced MathChat, a conversational\nframework designed for chat-based LLMs. In\nthis framework, math problems from the MATH\ndataset are resolved through a simulated conversa-\ntion between the model and a user proxy agent.\nConsidering more comprehensive factors in eval-\nuation. While accuracy is crucial in evaluating\nLLMs for math problem-solving, it shouldn’t be the\nsole metric. Other important dimensions include:\ni) Confidence Provision: Imani et al. (2023)’s\n”MathPromper” boosts LLM performance and con-\nfidence by generating algebraic expressions, pro-\nviding diverse prompts, and evaluating consensus\namong multiple runs. ii) Verifiable Explanations:\nGaur and Saunshi (2023) used concise, verifiable\nexplanations to assess LLM reasoning, revealing\ntheir proficiency in zero-shot solving of symbolic\nMWPand their ability to produce succinct explana-\ntions.\n4.3 Fine-tuning LLMs\nLearning to select in-context examples. As in-\ndicated by prior research, few-shot GPT-3’s perfor-\nmance is susceptible to instability and may decline\nto near chance levels due to the reliance on in-\ncontext examples. This instability becomes more\npronounced when dealing with intricate problems\nsuch as TABMWP . In addressing this issue, Lu\net al. (2023b) introduced PROMPTPG, which can\nautonomously learn to select effective in-context\nexamples through policy gradient interactions with\nthe GPT-3 API, eliminating the need for manually\ndesigned heuristics.\nGenerating intermediate steps. Nye et al.\n(2021) initiated the fine-tuning of decoder-only\nLLMs, ranging from 2M to 137B in size. Their\napproach involved training these models to solve\ninteger addition and polynomial evaluation by gen-\nerating intermediate computation steps into a des-\nignated “scratchpad.” In a related effort, Zhang\net al. (2023b) introduced a fine-tuning strategy for\nGPT-2 or T5, enabling them to produce step-by-\nstep solutions with a combination of textual and\nmathematical tokens leading to the final answer.\nAdditionally, Yang et al. (2023) applied a step-by-\nstep strategy in fine-tuning a series of GLM models\n(Zeng et al., 2023), specifically tailored for solving\ndistinct Chinese mathematical problems. Minerva,\ndeveloped by Lewkowycz et al. (2022), enhances\nLLMs’ ability to generate intermediate steps in\ncomplex math problems. Its fine-tuning of diverse\ndatasets enables nuanced, step-by-step problem-\nsolving, demonstrating advanced handling of intri-\ncate mathematical concepts.\nLearning an answer verifier. OpenAI re-\nsearchers, per Cobbe et al. (2021), fine-tuned a\nGPT-3 model of 175B as a verifier, assigning\nprobabilities to solution candidates. In explor-\ning reexamination processes for MWP solving,\nBin et al. (2023) introduced Pseudo-Dual Learn-\ning, involving solving and reexamining modules.\nFor MWP solution, Zhu et al. (2023) developed a\ncooperative reasoning-induced PLM, with GPT-J\n(Wang and Komatsuzaki, 2021) generating paths\nand DeBERTa-large (He et al., 2021) supervising\nevaluation. Google researchers, as per Liu et al.\n(2023c), observed improved correctness in LLMs\nwith multiple attempts, which hints that LLMs\nmight generate correct solutions while struggling\nto differentiate between accurate and inaccurate\nones. They sequentially fine-tuned their PaLM 2\nmodel (Anil et al., 2023) as a solution generator,\nevaluator, and generator again.\nLearning from enhanced dataset. Emulating\nthe error-driven learning process observed in hu-\nman learning, An et al. (2023b) conducted fine-\ntuning on various open-source LLMs within the\nLLaMA (Touvron et al., 2023a), LLaMA-2 (Tou-\nvron et al., 2023b), CodeLLaMA (Rozi `ere et al.,\n2023), WizardMath (Luo et al., 2023), MetaMath\n(Yu et al., 2023), and Llemma (Azerbayev et al.,\n2023) families. This fine-tuning utilized mistake-\ncorrection data pairs generated by GPT-4. To\nmitigate over-reliance on knowledge distillation\nfrom LLM teachers, Liang et al. (2023a) fine-\ntuned LLaMA-7B on existing mathematical prob-\nlem datasets that exhibit diverse annotation styles.\nIn a related approach, Raiyan et al. (2023) demon-\nstrated that training on linguistic variants of prob-\nlem statements and implementing a voting mecha-\nnism for candidate predictions enhance the math-\nematical reasoning and overall robustness of the\nmodel.\nTeacher-Student knowledge distillation. Liang\net al. (2023b) utilized GPT-3 to coach a more\nefficient MWP solver (RoBERTa-based encoder-\ndecoder (Liu et al., 2019)). They shifted the focus\nfrom explaining existing exercises to identifying\nthe student model’s learning needs and generating\nnew, tailored exercises. The resulting smaller LLM\n230\nachieves competitive accuracy on the SVAMP\ndataset with significantly fewer parameters com-\npared to state-of-the-art LLMs.\nFinetuning on many datasets. Mishra et al.\n(2022) conducted fine-tuning on a series of GPT-\nNeo2.7B causal language models (Black et al.,\n2021) using LILA, a composite of 20 existing math\ndatasets. Similarly, Yue et al. (2023) created “Math-\nInstruct”, a meticulously curated instruction tun-\ning dataset. Comprising 13 math datasets with\nintermediate Chain-of-Thought and Program-of-\nThought rationales, this dataset was used to fine-\ntune Llama (Touvron et al., 2023a,b; Rozi`ere et al.,\n2023) models across different scales. The result-\ning models demonstrate unprecedented potential in\ncross-dataset generalization.\nMath solver ensemble. Yao et al. (2023) incor-\nporated a problem typing subtask that combines\nthe strengths of the tree-based solver and the LLM\nsolver (ChatGLM-6B (Zeng et al., 2023)).\n5 Analysis\n5.1 LLMs’s robustness in math\nPatel et al. (2021) provided strong evidence that the\npre-LLM MWP solvers, mostly LSTM-equipped\nencoder-decoder models, rely on shallow heuristics\nto achieve high performance on some simple bench-\nmark datasets, then introduced a more challenging\ndataset, SVAMP, created by applying carefully\nchosen variations over examples sampled from\npreceding datasets. Stolfo et al. (2023) observed\nthat, among non-instruction-tuned LLMs, the larger\nones tend to be more sensitive to changes in the\nground-truth result of a MWP, but not necessarily\nmore robust. However, a different behavior exists\nin the instruction-tuned GPT-3 models, which show\na remarkable improvement in both sensitivity and\nrobustness, although the robustness reduces when\nproblems get more complicated. Wei et al. (2023)\nassessed the robustness of several top-performing\nLLMs by augmenting the original problems in the\ncurated CMATH dataset with distracting informa-\ntion. Their findings reveal that GPT-4 can maintain\nrobustness while other models fail.\nZhou et al. (2023b) proposed a new dataset RO-\nBUST MATH to evaluate the robustness of LLMs in\nmath-solving ability. Extensive experiments show\nthat (i) Adversarial samples from higher-accuracy\nLLMs are also effective for attacking LLMs with\nlower accuracy; (ii) Complex MWPs (such as more\nsolving steps, longer text, more numbers) are more\nvulnerable to attack; (iii) We can improve the ro-\nbustness of LLMs by using adversarial samples in\nfew-shot prompts.\n5.2 Factors in influencing LLMs in math\nThe comprehensive evaluation conducted by Yuan\net al. (2023) encompasses OpenAI’s GPT series,\nincluding GPT-4, ChatGPT2, and GPT-3.5, along\nwith various open-source LLMs. This analysis\nmethodically examines the elements that impact the\narithmetic skills of LLMs, covering aspects such as\ntokenization, pre-training, prompting techniques,\ninterpolation and extrapolation, scaling laws, Chain\nof Thought (COT), and In-Context Learning (ICL).\nTokenization. This research underscores tok-\nenization’s critical role in LLMs’ arithmetic perfor-\nmance (Yuan et al., 2023). Models like T5, lacking\nspecialized tokenization for arithmetic, are less ef-\nfective than those with advanced methods, such as\nGalactica (Taylor et al., 2022) and LLaMA, which\nshow superior accuracy in arithmetic tasks. This\nindicates that token frequency in pre-training and\nthe method of tokenization are key to arithmetic\nproficiency.\nPre-training Corpus. Enhanced arithmetic skills\nin LLMs correlate with the inclusion of code and\nLATEX in pre-training data (Yuan et al., 2023).\nGalactica, heavily utilizing LATEX, excels in arith-\nmetic tasks, while models like Code-DaVinci-002,\nbetter at reasoning, lags in arithmetic, highlight-\ning a distinction between arithmetic and reasoning\nskills.\nPrompts. The nature of input prompts greatly\naffects LLMs’ arithmetic performance (Liu et al.,\n2023a; Lou et al., 2023). Without prompts, perfor-\nmance drops (Yuan et al., 2023). Models like Chat-\nGPT, which respond well to instructional system-\nlevel messages, demonstrate the importance of\nprompt type. Instruction tuning in pre-training also\nemerges as a significant factor (Yue et al., 2023).\nModel Scale. There’s a noted correlation be-\ntween parameter count and arithmetic capability\nin LLMs (Yuan et al., 2023). Larger models gen-\nerally perform better, but a performance plateau\nis observed, as shown by Galactica’s similar out-\ncomes at 30B and 120B parameters. However, this\ndoesn’t always mean superior performance, with\nsmaller models like ChatGPT occasionally outper-\nforming larger ones.\n231\n5.3 Perspectives of mathematics pedagogy\nWhile machine learning emphasizes LLMs’\nproblem-solving abilities in mathematics, in prac-\ntical education, their primary role is to aid learn-\ning. Thus, the focus shifts from mere mathematical\nperformance to a crucial consideration of LLMs’\nunderstanding of students’ needs, capabilities, and\nlearning methods.\nAdvantages of deploying LLMs in math edu-\ncation. Educators have observed the following\nbenefits of leveraging LLMs for math education. (i)\nLLMs foster critical thinking and problem-solving\nskills, as they provide comprehensive solutions and\npromote rigorous error analysis (Matzakos et al.,\n2023); (ii) Educators and students prefer LLM-\ngenerated hints because of their detailed, sequen-\ntial format and clear, coherent narratives (Gattupalli\net al., 2023); (iii) LLMs introduce a conversational\nstyle in problem-solving , an invaluable asset in\nmath education (Gattupalli et al., 2023); (iv) The\nimpact of LLMs extends beyond mere computa-\ntional assistance, offering deep insights and under-\nstanding spanning diverse disciplines like Algebra,\nCalculus, and Statistics (Rane, 2023).\nDisadvantages of deploying LLMs in math edu-\ncation. (i) Potential for misinterpretation. Misin-\nterpretation of students’ queries or errors in provid-\ning explanations by LLMs could lead to confusion.\nInaccurate responses might result in the reinforce-\nment of misconceptions, impacting the quality of\neducation (Yen and Hsu, 2023). (ii) Limited un-\nderstanding of individual learning styles. LLMs\nmay struggle to cater to diverse learning styles, as\nthey primarily rely on algorithms and might not\nfully grasp the unique needs of each student. Some\nlearners may benefit more from hands-on activi-\nties or visual aids that LLMs may not adequately\naddress. Gresham (2021) proposed that hints pro-\nduced by GPT-4 could be excessively intricate for\nyounger students who have shorter attention spans.\n(iii) Privacy and data security issues. Deploying\nLLMs involves collecting and analyzing substan-\ntial amounts of student data. Privacy concerns may\narise if proper measures are not in place to safe-\nguard this data from unauthorized access or misuse.\n6 Challenges\nData-driven & limited generalization. The pre-\nvailing trend in current research revolves around\nthe curation of extensive datasets. Despite this\nemphasis, there is a noticeable lack of robust gener-\nalization across various datasets, grade levels, and\ntypes of math problems. Examining how humans\nacquire math-solving skills suggests that machines\nmay need to embrace continual learning to enhance\ntheir capabilities.\nLLMs’ brittleness in math reasoning. The\nfragility of LLMs in mathematical reasoning is\nevident across three dimensions. Firstly, when pre-\nsented with questions expressed in varying textual\nforms (comprising words and numbers), LLMs ex-\nhibit inconsistent performance. Secondly, for iden-\ntical questions, an LLM may yield different final\nanswers through distinct reasoning paths during\nmultiple trials. Lastly, pre-trained math-oriented\nLLMs are susceptible to attacks from adversarial\ninputs, highlighting their vulnerability in the face\nof manipulated data.\nHuman-oriented math interpretation. The cur-\nrent LLM-oriented math reasoning, such as chain-\nof-thoughts, does not take into account the needs\nand comprehension abilities of users, such as stu-\ndents. As an example, Yen and Hsu (2023) discov-\nered that GPT-3.5 had a tendency to misinterpret\nstudents’ questions in the conversation, resulting\nin a failure to deliver adaptive feedback. Addi-\ntionally, research conducted by Gresham (2021)\nrevealed that GPT-4 frequently overlooks the prac-\ntical comprehension abilities of younger students.\nIt tends to generate overly intricate hints that even\nconfuse those students. Consequently, there is a\npressing need for increased AI research that ac-\ntively incorporates human factors into its design,\nensuring future developments align more closely\nwith the nuanced requirements of users.\n7 Conclusion\nThis survey on LLMs for Mathematics delves into\nvarious aspects of LLMs in mathematical reason-\ning, including their capabilities and limitations.\nThe paper discusses different types of math prob-\nlems, datasets, and the persisting challenges in the\ndomain. It highlights the advancements in LLMs,\ntheir application in educational settings, and the\nneed for a human-centric approach in math edu-\ncation. We hope this paper will guide and inspire\nfuture research in the LLM community, fostering\nfurther advancements and practical applications in\ndiverse mathematical contexts.\n232\nReferences\nChris Alvin, Sumit Gulwani, Rupak Majumdar, and\nSupratik Mukhopadhyay. 2017. Synthesis of solu-\ntions for shaded area geometry problems. In Proceed-\nings of the Thirtieth International Florida Artificial\nIntelligence Research Society Conference, FLAIRS\n2017, Marco Island, Florida, USA, May 22-24, 2017,\npages 14–19. AAAI Press.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of NAACL-HLT , pages\n2357–2367.\nJisu An, Junseok Lee, and Gahgene Gweon. 2023a.\nDoes chatgpt comprehend the place value in num-\nbers when solving math word problems? In Pro-\nceedings of the Workshop ”Towards the Future of\nAI-augmented Human Tutoring in Math Learning”\nco-located with The 24th International Conference\non Artificial Intelligence in Education (AIED 2023),\nTokyo, Japan, July 3, 2023, volume 3491 of CEUR\nWorkshop Proceedings, pages 49–58.\nShengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng,\nJian-Guang Lou, and Weizhu Chen. 2023b. Learning\nfrom mistakes makes LLM better reasoner. CoRR,\nabs/2310.20689.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hern´andez\n´Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan A. Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl´ement Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D ´ıaz,\nNan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier\nGarcia, Sebastian Gehrmann, Lucas Gonzalez, and\net al. 2023. Palm 2 technical report. CoRR,\nabs/2305.10403.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,\nMarco Dos Santos, Stephen McAleer, Albert Q.\nJiang, Jia Deng, Stella Biderman, and Sean Welleck.\n2023. Llemma: An open language model for mathe-\nmatics.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBenjamin Mann, and Jared Kaplan. 2022. Train-\ning a helpful and harmless assistant with rein-\nforcement learning from human feedback. CoRR,\nabs/2204.05862.\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, Chris-\ntian Szegedy, and Stewart Wilcox. 2019. Holist: An\nenvironment for machine learning of higher-order\ntheorem proving.\nYi Bin, Wenhao Shi, Yujuan Ding, Yang Yang, and See-\nKiong Ng. 2023. Solving math word problems with\nreexamination. CoRR, abs/2310.09590.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,\nYi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\n2023. A survey on evaluation of large language mod-\nels. CoRR, abs/2307.03109.\nJiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin,\nChongyu Chen, and Xiaodan Liang. 2022. Unigeo:\nUnifying geometry logical reasoning via reformu-\nlating mathematical expression. In Proceedings of\nEMNLP, pages 3313–3323.\nJiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,\nLingbo Liu, Eric P. Xing, and Liang Lin. 2021a.\nGeoqa: A geometric question answering benchmark\ntowards multimodal numerical reasoning. In Find-\nings of ACL/IJCNLP , volume ACL/IJCNLP 2021,\npages 513–523.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pond ´e de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021b. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2023a. Program of thoughts\n233\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. Transactions on\nMachine Learning Research.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan,\nXueguang Ma, Jianyu Xu, Xinyi Wang, and Tony\nXia. 2023b. Theoremqa: A theorem-driven question\nanswering dataset. In Proceedings of EMNLP, pages\n7889–7901.\nVincent Cheng and Yu Zhang. 2023. Analyzing Chat-\nGPT’s mathematical deficiencies: Insights and con-\ntributions. In Proceedings of the 35th Conference\non Computational Linguistics and Speech Processing\n(ROCLING 2023), pages 188–193.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nAniruddha Deb, Neeva Oza, Sarthak Singla, Dinesh\nKhandelwal, Dinesh Garg, and Parag Singla. 2023.\nFill in the blank: Exploring and enhancing LLM\ncapabilities for backward reasoning in math word\nproblems. CoRR, abs/2310.01991.\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard\nTang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda\nChen, Sunny Tran, Newman Cheng, et al. 2022. A\nneural network solves, explains, and generates uni-\nversity math problems by program synthesis and few-\nshot learning at human level. Proceedings of the Na-\ntional Academy of Sciences, 119(32):e2123433119.\nSimon Frieder, Luca Pinchetti, Ryan-Rhys Grif-\nfiths, Tommaso Salvatori, Thomas Lukasiewicz,\nPhilipp Christian Petersen, Alexis Chevalier, and\nJulius Berner. 2023. Mathematical capabilities of\nchatgpt. CoRR, abs/2301.13867.\nSai Gattupalli, William Lee, Danielle Allessio, Danielle\nCrabtree, Ivon Arroyo, Beverly Woolf, and Beverly\nWoolf. 2023. Exploring pre-service teachers’ per-\nceptions of large language models-generated hints in\nonline mathematics learning.\nVedant Gaur and Nikunj Saunshi. 2023. Reasoning in\nlarge language models through symbolic math word\nproblems. In Findings of ACL, pages 5889–5903.\nGina Gresham. 2021. Exploring exceptional education\npreservice teachers’ mathematics anxiety. Interna-\ntional Journal for the Scholarship of Teaching and\nLearning, 15.\nSophia Gu. 2023. Llms as potential brainstorming\npartners for math and science problems. CoRR,\nabs/2310.10677.\nJesse Michael Han, Jason Rute, Yuhuai Wu, Edward W.\nAyers, and Stanislas Polu. 2022. Proof artifact co-\ntraining for theorem proving with language models.\nIn Proceedings of ICLR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In Proceedings of\nICLR.\nJoy He-Yueya, Gabriel Poesia, Rose E. Wang, and\nNoah D. Goodman. 2023. Solving math word prob-\nlems by combining language models with symbolic\nsolvers. CoRR, abs/2304.09102.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the MATH dataset. In Proceed-\nings of NeurIPS.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn Proceedings of EMNLP, pages 523–533. ACL.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathprompter: Mathematical reasoning using large\nlanguage models. In Proceedings of ACL, pages 37–\n42.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Trans. Assoc. Comput. Linguistics, 3:585–597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nNAACL, pages 1152–1157.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models.\nZhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao,\nQingkai Zeng, Xiangliang Zhang, and Dong Yu.\n2023a. Mint: Boosting generalization in mathemat-\nical reasoning via multi-view fine-tuning. CoRR,\nabs/2307.07951.\nZhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Pe-\nter Clark, Xiangliang Zhang, and Ashwin Kalyan.\n2023b. Let GPT be a math tutor: Teaching math\nword problem solvers with customized exercise gen-\neration. CoRR, abs/2305.14386.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl\nCobbe. 2023. Let’s verify step by step. CoRR,\nabs/2305.20050.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of ACL, pages 158–167.\n234\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nWentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding,\nJunsong Li, Jiayi Zeng, Mengliang He, Qin Chen,\nBo Jiang, Aimin Zhou, and Liang He. 2023b.\nMathematical language models: A survey. CoRR,\nabs/2312.07622.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nYixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-\nReyes, and Peter J. Liu. 2023c. Improving large lan-\nguage model fine-tuning for solving math problems.\nCoRR, abs/2310.10047.\nRenze Lou, Kai Zhang, and Wenpeng Yin. 2023. Is\nprompt all you need? no. a comprehensive and\nbroader view of instruction learning. arXiv preprint\narXiv:2303.10475.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. 2023a.\nMathvista: Evaluating math reasoning in visual con-\ntexts with gpt-4v, bard, and other large multimodal\nmodels. CoRR, abs/2310.02255.\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. 2021.\nInter-gps: Interpretable geometry problem solving\nwith formal language and symbolic reasoning. In\nProceedings of ACL/IJCNLP, pages 6774–6786.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2023b. Dynamic prompt learn-\ning via policy gradient for semi-structured mathemat-\nical reasoning. In Proceedings of ICLR.\nPan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and\nKai-Wei Chang. 2023c. A survey of deep learning\nfor mathematical reasoning. In Proceedings of ACL,\npages 14605–14631.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\nCoRR, abs/2308.09583.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R.\nJoty, and Enamul Hoque. 2022. Chartqa: A bench-\nmark for question answering about charts with visual\nand logical reasoning. In Findings of ACL, pages\n2263–2279.\nNikolaos Matzakos, Spyridon Doukakis, and Maria\nMoundridou. 2023. Learning mathematics with large\nlanguage models: A comparative study with com-\nputer algebra systems and other tools. International\nJournal of Emerging Technologies in Learning (iJET),\n18(20):51–71.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Proceedings\nof ACL, pages 975–984.\nSwaroop Mishra, Matthew Finlayson, Pan Lu, Leonard\nTang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-\nhit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark,\nand Ashwin Kalyan. 2022. LILA: A unified bench-\nmark for mathematical reasoning. In Proceedings of\nEMNLP, pages 5807–5832.\nKole Norberg, Husni Almoubayyed, Stephen E. Fanc-\nsali, Logan De Ley, Kyle Weldon, April Murphy, and\nSteven Ritter. 2023. Rewriting math word problems\nwith large language models. In Proceedings of the\nWorkshop on Empowering Education with LLMs -\nthe Next-Gen Interface and Content Generation 2023\nco-located with 24th International Conference on Ar-\ntificial Intelligence in Education (AIED 2023), Tokyo,\nJapan, July 7, 2023 , volume 3487 of CEUR Work-\nshop Proceedings, pages 163–172.\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-\nAri, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten\nBosma, David Luan, Charles Sutton, and Augustus\nOdena. 2021. Show your work: Scratchpads for inter-\nmediate computation with language models. CoRR,\nabs/2112.00114.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of NAACL-\nHLT, pages 2080–2094.\nJinghui Qin, Xiaodan Liang, Yining Hong, Jianheng\nTang, and Liang Lin. 2021. Neural-symbolic solver\nfor math word problems with auxiliary tasks. In\nProceedings of ACL/IJCNLP, pages 5870–5881.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSyed Rifat Raiyan, Md. Nafis Faiyaz, Shah Md. Jawad\nKabir, Mohsinul Kabir, Hasan Mahmud, and\n235\nMd Kamrul Hasan. 2023. Math word problem solv-\ning by generating linguistic variants of problem state-\nments. CoRR, abs/2306.13899.\nNitin Rane. 2023. Enhancing mathematical capabili-\nties through chatgpt and similar generative artificial\nintelligence: Roles and challenges in solving mathe-\nmatical problems. SSRN Electronic Journal.\nBernardino Romera-Paredes, Mohammadamin\nBarekatain, Alexander Novikov, Matej Balog,\nM Pawan Kumar, Emilien Dupont, Francisco JR\nRuiz, Jordan S Ellenberg, Pengming Wang, Omar\nFawzi, et al. 2023. Mathematical discoveries from\nprogram search with large language models. Nature,\npages 1–3.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of EMNLP,\npages 1743–1752.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Man-\nish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,\nWenhan Xiong, Alexandre D ´efossez, Jade Copet,\nFaisal Azhar, Hugo Touvron, Louis Martin, Nico-\nlas Usunier, Thomas Scialom, and Gabriel Synnaeve.\n2023. Code llama: Open foundation models for code.\nCoRR, abs/2308.12950.\nMrinmaya Sachan, Avinava Dubey, and Eric P. Xing.\n2017. From textbooks to knowledge: A case study in\nharvesting axiomatic knowledge from textbooks to\nsolve geometry problems. In Proceedings of EMNLP,\npages 773–784.\nMrinmaya Sachan and Eric P. Xing. 2017. Learn-\ning to solve geometry problems from natural lan-\nguage demonstrations in textbooks. In Proceedings\nof *SEM @ACM, pages 251–261.\nTomohiro Sawada, Daniel Paleka, Alexander Havrilla,\nPranav Tadepalli, Paula Vidas, Alexander Kranias,\nJohn J. Nay, Kshitij Gupta, and Aran Komatsuzaki.\n2023. ARB: advanced reasoning benchmark for large\nlanguage models. CoRR, abs/2307.13692.\nMin Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren\nEtzioni, and Clint Malcolm. 2015. Solving geometry\nproblems: Combining text and diagram interpretation.\nIn Proceedings of EMNLP, pages 1466–1476.\nPaulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and\nLakshmivihari Mareedu. 2023. An independent eval-\nuation of chatgpt on mathematical word problems\n(MWP). In Proceedings of the AAAI 2023 Spring\nSymposium on Challenges Requiring the Combina-\ntion of Machine Learning and Knowledge Engineer-\ning (AAAI-MAKE 2023), Hyatt Regency, San Fran-\ncisco Airport, California, USA, March 27-29, 2023,\nvolume 3433 of CEUR Workshop Proceedings.\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Sch ¨olkopf, and Mrinmaya Sachan. 2023. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models. In Pro-\nceedings of ACL, pages 545–561.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, An-\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A large language model for science.\nCoRR, abs/2211.09085.\nAlberto Testolin. 2023. Can neural networks do arith-\nmetic? A survey on the elementary numerical skills\nof state-of-the-art deep learning models. CoRR,\nabs/2303.07735.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur´elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur ´elien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nTrieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang\nLuong. 2024. Solving olympiad geometry without\nhuman demonstrations. Nature.\nShyam Upadhyay and Ming-Wei Chang. 2017. An-\nnotating derivations: A new evaluation strategy and\ndataset for algebra word problems. In Proceedings\nof EACL, pages 494–504.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. In Proceedings of ICLR.\n236\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of EMNLP, pages 845–854.\nZichao Wang, Andrew S. Lan, and Richard G. Baraniuk.\n2021. Math word problem generation with mathe-\nmatical consistency and problem context constraints.\nIn Proceedings of EMNLP, pages 5986–5999.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nProceedings of NeurIPS.\nTianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and\nBin Wang. 2023. CMATH: can your language model\npass chinese elementary school math test? CoRR,\nabs/2306.16636.\nMakarius Wenzel, Lawrence C Paulson, and Tobias\nNipkow. 2008. The isabelle framework. In Theo-\nrem Proving in Higher Order Logics: 21st Interna-\ntional Conference, TPHOLs 2008, Montreal, Canada,\nAugust 18-21, 2008. Proceedings 21 , pages 33–38.\nSpringer.\nYiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li,\nErkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng,\nQingyun Wu, and Chi Wang. 2023. An empirical\nstudy on challenging math problem solving with GPT-\n4. CoRR, abs/2306.01337.\nRyutaro Yamauchi, Sho Sonoda, Akiyoshi Sannai, and\nWataru Kumagai. 2023. LPML: llm-prompting\nmarkup language for mathematical reasoning. CoRR,\nabs/2309.13078.\nKaiyu Yang and Jia Deng. 2019. Learning to prove\ntheorems via interacting with proof assistants.\nZhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang,\nZehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. 2023.\nGPT can solve mathematical problems without a cal-\nculator. CoRR, abs/2309.03241.\nJie Yao, Zihao Zhou, and Qiufeng Wang. 2023. Solving\nmath word problem with problem type classification.\nIn Proceedings of NLPCC, volume 14304, pages 123–\n134.\nAn-Zi Yen and Wei-Ling Hsu. 2023. Three questions\nconcerning the use of large language models to facil-\nitate mathematics learning. CoRR, abs/2310.13615.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T. Kwok, Zhenguo\nLi, Adrian Weller, and Weiyang Liu. 2023. Meta-\nmath: Bootstrap your own mathematical questions\nfor large language models. CoRR, abs/2309.12284.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nand Songfang Huang. 2023. How well do large lan-\nguage models perform in arithmetic tasks? CoRR,\nabs/2304.02015.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao\nHuang, Huan Sun, Yu Su, and Wenhu Chen. 2023.\nMammoth: Building math generalist models through\nhybrid instruction tuning. CoRR, abs/2309.05653.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130B: an open bilingual pre-trained model. In\nProceedings of ICLR.\nBeichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin\nZhao, Jing Sha, Shijin Wang, and Ji-Rong Wen.\n2023a. Evaluating and improving tool-augmented\ncomputation-intensive math reasoning. arXiv\npreprint arXiv:2306.02408.\nMengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi\nFeng, and Andrew S. Lan. 2023b. Interpretable math\nword problem solution generation via step-by-step\nplanning. In Proceedings of ACL, pages 6858–6877.\nWei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and\nJingming Liu. 2020. Ape210k: A large-scale and\ntemplate-rich dataset of math word problems.\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu.\n2022. Minif2f: a cross-system benchmark for formal\nolympiad-level mathematics.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. 2023. Agieval: A human-centric\nbenchmark for evaluating foundation models. CoRR,\nabs/2304.06364.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun\nLuo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song,\nMingjie Zhan, and Hongsheng Li. 2023a. Solving\nchallenging math word problems using GPT-4 code\ninterpreter with code-based self-verification. CoRR,\nabs/2308.07921.\nZihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan\nYe, Wei Liu, Wei Wang, Xiaowei Huang, and Kaizhu\nHuang. 2023b. Mathattack: Attacking large lan-\nguage models towards math solving ability. CoRR,\nabs/2309.01686.\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,\nYongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-\njiu Yang. 2023. Solving math word problems via\ncooperative reasoning induced language models. In\nProceedings of ACL, pages 4471–4485.\nMingyu Zong and Bhaskar Krishnamachari. 2023. Solv-\ning math word problems concerning systems of equa-\ntions with GPT-3. In Proceedings of AAAI, pages\n15972–15979.\n237",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7118847370147705
    },
    {
      "name": "Automated reasoning",
      "score": 0.4156542420387268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40569913387298584
    },
    {
      "name": "Cognitive science",
      "score": 0.3826567828655243
    },
    {
      "name": "Natural language processing",
      "score": 0.3753470778465271
    },
    {
      "name": "Programming language",
      "score": 0.324604332447052
    },
    {
      "name": "Management science",
      "score": 0.32384830713272095
    },
    {
      "name": "Engineering",
      "score": 0.09048786759376526
    },
    {
      "name": "Psychology",
      "score": 0.0858214795589447
    }
  ],
  "institutions": []
}