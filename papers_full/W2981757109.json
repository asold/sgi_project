{
  "title": "Depth-Adaptive Transformer",
  "url": "https://openalex.org/W2981757109",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288859917",
      "name": "Elbayad, Maha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222615513",
      "name": "Gu, Jiatao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222857661",
      "name": "Grave, Edouard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359259",
      "name": "Auli, Michael",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2767421475",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963393494",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W2964062240",
    "https://openalex.org/W2772087644",
    "https://openalex.org/W3204130541",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less than a quarter of the decoder layers.",
  "full_text": "DEPTH -ADAPTIVE TRANSFORMER\nMaha Elbayad∗\nUniv. Grenoble Alpes\nJiatao Gu, Edouard Grave, Michael Auli\nFacebook AI Research\nABSTRACT\nState of the art sequence-to-sequence models for large scale tasks perform a ﬁxed\nnumber of computations for each input sequence regardless of whether it is easy or\nhard to process. In this paper, we train Transformer models which can make out-\nput predictions at different stages of the network and we investigate different ways\nto predict how much computation is required for a particular sequence. Unlike\ndynamic computation in Universal Transformers, which applies the same set of\nlayers iteratively, we apply different layers at every step to adjust both the amount\nof computation as well as the model capacity. On IWSLT German-English trans-\nlation our approach matches the accuracy of a well tuned baseline Transformer\nwhile using less than a quarter of the decoder layers.\n1 I NTRODUCTION\nThe size of modern neural sequence models (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al.,\n2019) can amount to billions of parameters (Radford et al., 2019). For example, the winning entry\nof the WMT’19 news machine translation task in English-German used an ensemble totaling two\nbillion parameters (Ng et al., 2019). While large models are required to do better on hard examples,\nsmall models are likely to perform as well on easy ones, e.g., the aforementioned ensemble is prob-\nably not required to translate a short phrase such as \"Thank you\". However, current models apply\nthe same amount of computation regardless of whether the input is easy or hard.\nIn this paper, we propose Transformers which adapt the number of layers to each input in order to\nachieve a good speed-accuracy trade off at inference time. We extend Graves (2016; ACT) who\nintroduced dynamic computation to recurrent neural networks in several ways: we apply different\nlayers at each stage, we investigate a range of designs and training targets for the halting module and\nwe explicitly supervise through simple oracles to achieve good performance on large-scale tasks.\nUniversal Transformers (UT) rely on ACT for dynamic computation and repeatedly apply the same\nlayer (Dehghani et al., 2018). Our work considers a variety of mechanisms to estimate the network\ndepth and applies a different layer at each step. Moreover, Dehghani et al. (2018) ﬁx the number\nof steps for large-scale machine translation whereas we vary the number of steps to demonstrate\nsubstantial improvements in speed at no loss in accuracy. UT uses a layer which contains as many\nweights as an entire standard Transformer and this layer is applied several times which impacts\nspeed. Our approach does not increase the size of individual layers. We also extend the resource\nefﬁcient object classiﬁcation work of Huang et al. (2017) and Bolukbasi et al. (2017) to structured\nprediction where dynamic computation decisions impact future computation. Related work from\ncomputer vision includes Teerapittayanon et al. (2016); Figurnov et al. (2017) and Wang et al. (2018)\nwho explored the idea of dynamic routing either by exiting early or by skipping layers.\nWe encode the input sequence using a standard Transformer encoder to generate the output sequence\nwith a varying amount of computation in the decoder network. Dynamic computation poses a chal-\nlenge for self-attention because omitted layers in prior time-steps may be required in the future.\nWe experiment with two approaches to address this and show that a simple approach works well\n(§2). Next, we investigate different mechanisms to control the amount of computation in the de-\ncoder network, either for the entire sequence or on a per-token basis. This includes multinomial\nand binomial classiﬁers supervised by the model likelihood or whether the argmax is already correct\nas well as simply thresholding the model score (§3). Experiments on IWSLT14 German-English\n∗Work done during an internship at Facebook AI Research.\n1\narXiv:1910.10073v4  [cs.CL]  14 Feb 2020\nState Copied state CnClassiﬁer Copy\nDecoder depth\nDecoding step\nC1\nC2\nC3\n(a) Aligned training\n×M\nDecoder depth\nDecoding step\nC2\nC1\nC3 (b) Mixed training\nFigure 1: Training regimes for decoder networks able to emit outputs at any layer. Aligned training\noptimizes all output classiﬁers Cn simultaneously assuming all previous hidden states for the current\nlayer are available. Mixed training samples M paths of random exits at which the model is assumed\nto have exited; missing previous hidden states are copied from below.\ntranslation (Cettolo et al., 2014) as well as WMT’14 English-French translation show that we can\nmatch the performance of well tuned baseline models at up to 76% less computation (§4).\n2 A NYTIME STRUCTURED PREDICTION\nWe ﬁrst present a model that can make predictions at different layers. This is known as anytime\nprediction for computer vision models (Huang et al., 2017) and we extend it to structured prediction.\n2.1 T RANSFORMER WITH MULTIPLE OUTPUT CLASSIFIERS\nWe base our approach on the Transformer sequence-to-sequence model (Vaswani et al., 2017). Both\nencoder and decoder networks contain N stacked blocks where each has several sub-blocks sur-\nrounded by residual skip-connections. The ﬁrst sub-block is a multi-head dot-product self-attention\nand the second a position-wise fully connected feed-forward network. For the decoder, there is an\nadditional sub-block after the self-attention to add source context via another multi-head attention.\nGiven a pair of source-target sequences (x,y), xis processed with the encoder to give representa-\ntions s= (s1,...,s |x|). Next, the decoder generates ystep-by-step. For every new token yt input\nto the decoder at time t, the N decoder blocks process it to yield hidden states (hn\nt )1≤n≤N :\nh0\nt = embed(yt), h n\nt = blockn(hn−1\n≤t ,s), (1)\nwhere blockn is the mapping associated with the nth block and embed is a lookup table.\nThe output distribution for predicting the next token is computed by feeding the activations of the\nlast decoder layer hN\nt into a softmax normalized output classiﬁer W:\np(yt+1|hN\nt ) = softmax(WhN\nt ) (2)\nStandard Transformers have a single output classiﬁer attached to the top of the decoder network.\nHowever, for dynamic computation we need to be able to make predictions at different stages of the\nnetwork. To achieve this, we attach output classiﬁers Cn parameterized by Wn to the output hn\nt of\neach of the N decoder blocks:\n∀n, p(yt+1|hn\nt ) = softmax(Wnhn\nt ) (3)\nThe classiﬁers can be parameterized independently or we can share the weights across theN blocks.\n2.2 T RAINING MULTIPLE OUTPUT CLASSIFIERS\nDynamic computation enables the model to use any of the N exit classiﬁers instead of just the ﬁnal\none. Some of our models can choose a different output classiﬁer at each time-step which results in\nan exponential number of possible output classiﬁer combinations in the sequence length.\n2\nWe consider two possible ways to train the decoder network (Figure 1). Aligned training optimizes\nall classiﬁers simultaneously and assumes all previous hidden states required by the self-attention\nare available. However, at test time this is often not the case when we choose a different exit for\nevery token which leads to misaligned states. Instead, mixed training samples several sequences of\nexits for a given sentence and exposes the model to hidden states from different layers.\nGenerally, for a given output sequencey, we have a sequence of chosen exits(n1,...,n |y|) and we\ndenote the block at which we exit at time tas nt.\n2.2.1 A LIGNED TRAINING\nAligned training assumes all hidden states hn−1\n1 ,...,h n−1\nt are available in order to compute self-\nattention and it optimizes N loss terms, one for each exit (Figure 1a):\nLLn\nt = logp(yt|hn\nt−1), LLn =\n|y|∑\nt=1\nLLn\nt , Ldec(x,y) =− 1∑\nn ωn\nN∑\nn=1\nωn LLn . (4)\nThe compound loss Ldec(x,y) is a weighted average of N terms w.r.t. to (ω1,...ω N ). We found\nthat uniform weights achieve better BLEU compared to other weighing schemes (c.f . Appendix A).\nAt inference time, not all time-steps will have hidden states for the current layer since the model\nexited early. In this case, we simplycopy the last computed state to all upper layers, similar to mixed\ntraining (§2.2.2). However, we do apply layer-speciﬁc key and value projections to the copied state.\n2.2.2 M IXED TRAINING\nAligned training assumes that all hidden states of the previous time-steps are available but this as-\nsumption is unrealistic since an early exit may have been chosen previously. This creates a mismatch\nbetween training and testing. Mixed training reduces the mismatch by training the model to use hid-\nden states from different blocks of previous time-steps for self-attention. We sample M different\nexit sequences (n(m)\n1 ,...n (m)\n|y| )\n1≤m≤M\nand evaluate the following loss:\nLL(n1,...,n |y|) =\n|y|∑\nt=1\nlog p(yt|hnt\nt−1), Ldec(x,y) =−1\nM\nM∑\nm=1\nLL(n(m)\n1 ,...,n (m)\n|y| ). (5)\nWhen nt < N, we copy the last evaluated hidden state hn\nt to the subsequent layers so that the\nself-attention of future time steps can function as usual (see Figure 1b).\n3 A DAPTIVE DEPTH ESTIMATION\nWe present a variety of mechanisms to predict the decoder block at which the model will stop\nand output the next token, or when it should exit to achieve a good speed-accuracy trade-off. We\nconsider two approaches: sequence-speciﬁc depth decodes all output tokens using the same block\n(§3.1) while token-speciﬁc depth determines a separate exit for each individual token (§3.2).\nWe model the distribution of exiting at time-step t with a parametric distribution qt where qt(n)\nis the probability of computing block1,..., blockn and then emitting a prediction with Cn. The\nparameters of qt are optimized to match an oracle distribution q∗\nt with cross-entropy:\nLexit(x,y) =\n∑\nt\nH(q∗\nt (x,y),qt(x)) (6)\nThe exit loss ( Lexit) is back-propagated to the encoder-decoder parameters. We simultaneously\noptimize the decoding loss (Eq. (4)) and the exit loss (Eq. (6)) balanced by a hyper-parameter αto\nensure that the model maintains good generation accuracy. The ﬁnal loss takes the form:\nL(x,y) =Ldec(x,y) +αLexit(x,y), (7)\nIn the following we describe for each approach how the exit distribution qt is modeled (illustrated\nin Figure 2) and how the oracle distribution q∗\nt is inferred.\n3\nState Copied state 1 Halting decision CnClassiﬁer Copy\nDecoder depth\nDecoding step\ns 2\nC2 C2 C2\n(a) Sequence-speciﬁc depth\nDecoder depth\nDecoding step\n2\nC2\n1\nC1\n3\nC3 (b) Token-speciﬁc - Multinomial\nDecoder depth\nDecoding step\nC\nS\nC2\nS\nC1\nC\nC\nC3 (c) Token-speciﬁc - Geometric-like\nFigure 2: Variants of the adaptive depth prediction classiﬁers. Sequence-speciﬁc depth uses a multi-\nnomial classiﬁer to choose an exit for the entire output sequence based on the encoder outputs(2a).\nIt then outputs a token at this depth with classiﬁer Cn. The token-speciﬁc multinomial classiﬁer\ndetermines the exit after the ﬁrst block and proceeds up to the predicted depth before outputting the\nnext token (2b). The token geometric-like classiﬁer (2c) makes a binary decision after every block\nto dictate whether to continue (C) to the next block or to stop (S) and emit an output distribution.\n3.1 S EQUENCE -SPECIFIC DEPTH :\nFor sequence-speciﬁc depth, the exit distribution qand the oracle distribution q∗are independent of\nthe time-step so we drop subscript t. We condition the exit on the source sequence by feeding the\naverage sof the encoder outputs to a multinomial classiﬁer:\ns= 1\n|x|\n∑\nt\nst, q (n|x) = softmax(Whs+ bh) ∈RN , (8)\nwhere Wh and bh are the weights and biases of the halting mechanism. We consider two oracles to\ndetermine which of the N blocks should be chosen. The ﬁrst is based on the sequence likelihood\nand the second looks at an aggregate of the correctly predicted tokens at each block.\nLikelihood-based: This oracle is based on the likelihood of the entire sequence after each block\nand we optimize it with the Dirac delta centered around the exit with the highest sequence likelihood.\nq∗(x,y) =δ(arg max\nn\nLLn).\nWe add a regularization term to encourage lower exits that achieve good likelihood:\nq∗(x,y) =δ(arg max\nn\nLLn −λn). (9)\nCorrectness-based: Likelihood ignores whether the model already assigns the highest score to\nthe correct target. Instead, this oracle chooses the lowest block that assigns the largest score to\nthe correct prediction. For each block, we count the number of correctly predicted tokens over\nthe sequence and choose the block with the most number of correct tokens. A regularization term\ncontrols the trade-off between speed and accuracy.\nCn = #{t|yt = arg max\ny\np(y|hn\nt−1)}, q ∗(x,y) =δ(arg max\nn\nCn −λn). (10)\nOracles based on test metrics such as BLEU are feasible but expensive to compute since we would\nneed to decode every training sentence N times. We leave this for future work.\n3.2 T OKEN -SPECIFIC DEPTH :\nThe token-speciﬁc approach can choose a different exit at every time-step. We consider two options\nfor the exit distribution qt at time-step t: a multinomial with a classiﬁer conditioned on the ﬁrst\ndecoder hidden state h1\nt and a geometric-like where an exit probability χn\nt is estimated after each\nblock based on the activations of the current block hn\nt .\n4\nMultinomial qt:\nqt(n|x,y<t) = softmax(Whh1\nt + bh), (11)\nThe most probable exit arg maxqt(n|x,y<t) is selected at inference.\nGeometric-like qt:\n∀n∈[1..N−1], χn\nt = sigmoid(w⊤\nh hn\nt +bh), (12)\nqt(n|x,y<t)=\n\n\n\nχn\nt\n∏\nn′<n\n(1−χn′\nt ), if n<N\n∏\nn′<N\n(1−χn′\nt ), otherwise (13)\nwhere, d is the dimension of the decoder states, Wh ∈RN×d and wh ∈Rd are the weights of\nthe halting mechanisms, and bh their biases. During inference the decoder exits when the halting\nsignal χn\nt exceeds a threshold τn which we tune on the valid set to achieve a better accuracy-speed\ntrade-off. If thresholds (τn)1≤n<N have not been exceeded, then we default to exiting at block N.\nThe two classiﬁers are trained to minimize the cross-entropy with respect to either one the following\noracle distributions:\nLikelihood-based: At each time-step t, we choose the block whose exit classiﬁer has the highest\nlikelihood plus a regularization term weighted by λto encourage lower exits.\nq∗\nt (x,y) =δ(arg max\nn\nLLn\nt −λn) (14)\nThis oracle ignores the impact of the current decision on the future time-steps and we therefore\nconsider smoothing the likelihoods with an RBF kernel.\nκ(t,t′) =e−|t−t′|2\nσ , ˜LLn\nt =\n|y|∑\nt′=1\nκ(t,t′) LLn\nt′, q ∗\nt (x,y) =δ(arg max\nn\n˜LLn\nt −λn), (15)\nwhere we control the size of the surrounding context withσthe kernel width. We refer to this oracle\nas LL(σ,λ) including the case where we only look at the likelihood of the current token withσ→0.\nCorrectness-based: Similar to the likelihood-based oracle we can look at the correctness of the\nprediction at time-step tas well as surrounding positions. We deﬁne the target q∗\nt as follows:\nCn\nt = 1 [yt = arg max\ny\np(y|hn\nt−1)], ˜Cn\nt =\n|y|∑\nt′=1\nκ(t,t′) Cn\nt , (16)\nq∗\nt (x,y) =δ(arg max\nn\n˜Cn\nt −λn). (17)\nConﬁdence thresholding Finally, we consider thresholding the model predictions (§2), i.e., exit\nwhen the maximum score of the current output classiﬁer p(yt+1|hn\nt ) exceeds a hyper-parameter\nthreshold τn. This does not require training and the thresholds τ = (τ1,...,τ N−1) are simply\ntuned on the valid set to maximize BLEU. Concretely, for 10k iterations, we sample a sequence of\nthresholds τ ∼U(0,1)N−1, decode the valid set with the sampled thresholds and then evaluate the\nBLEU score and computational cost achieved with this choice of τ. After 10k evaluations we pick\nthe best performing thresholds, that is τ with the highest BLEU in each cost segment.\n4 E XPERIMENTS\n4.1 E XPERIMENTAL SETUP\nWe evaluate on several benchmarks and measure tokenized BLEU (Papineni et al., 2002):\nIWSLT’14 German to English (De-En). We use the setup of Edunov et al. (2018) and train on\n160K sentence pairs. We useN = 6blocks, a feed-forward network (ffn) of intermediate-dimension\n5\nUniform n= 1 n= 2 n= 3 n= 4 n= 5 n= 6 Average\nBaseline - 34.2 35.3 35.6 35.7 35.6 35.9 35.4\nAligned (ωn = 1) 35.5 34.1 35.5 35.8 36.1 36.1 36.2 35.6\nMixed M = 1 34.1 32.9 34.3 34.5 34.5 34.6 34.5 34.2\nMixed M = 3 35.1 33.9 35.2 35.4 35.5 35.5 35.5 35.2\nMixed M = 6 35.3 34.2 35.4 35.8 35.9 35.8 35.9 35.5\nTable 1: Aligned vs. mixed training on IWSLT De-En. We report valid BLEU for a uniformly\nsampled exit n∼U([1..6]) at each token, a ﬁxed exitn∈[1..6] for all tokens, as well as the average\nBLEU over the ﬁxed exits. As baseline we show six standard Transformer models with 1-6 blocks.\n1024, 4 heads, dropout 0.3, embedding dimension denc = 512for the encoder and ddec = 256for\nthe decoder. Embeddings are untied with 6 different output classiﬁers. We evaluate with a single\ncheckpoint and a beam of width 5.\nWMT’14 English to French (En-Fr). We also experiment on the much larger WMT’14 English-\nFrench task comprising 35.5m training sentence pairs. We develop on 26k held out pairs and test\non newstest14. The vocabulary consists of 44k joint BPE types (Sennrich et al., 2016). We use\na Transformer big architecture and tie the embeddings of the encoder, the decoder and the output\nclassiﬁers ((Wn)1≤n≤6; §2.1). We average the last ten checkpoints and use a beam of width 4.\nModels are implemented in fairseq (Ott et al., 2019) and are trained with Adam (Kingma & Ba,\n2015). We train for 50k updates on 128 GPUs with a batch size of 460k tokens for WMT’14 En-Fr\nand on 2 GPUs with 8k tokens per batch for IWSLT’14 De-En. To stabilize training, we re-normalize\nthe gradients if the norm exceeds gclip = 3.\nFor models with adaptive exits, we ﬁrst train without exit prediction ( α = 0 in Eq. (7)) using the\naligned mode (c.f . §2.2.1) for 50k updates and then continue training withα̸= 0until convergence.\nThe exit prediction classiﬁers are parameterized by a single linear layer (Eq. (8)) with the same input\ndimension as the embedding dimension,e.g., 1024 for a big Transformer; the output dimension isN\nfor a multinomial classiﬁer or one for geometric-like. We exit when χt,n > 0.5 for geometric-like\nclassiﬁers.\n4.2 T RAINING MULTIPLE OUTPUT CLASSIFIERS\nWe ﬁrst compare the two training regimes for our model (§2.2). Aligned training performs self-\nattention on aligned states (§2.2.1) and mixed training exposes self-attention to hidden states from\ndifferent blocks (§2.2.2).\nWe compare the two training modes when choosing either a uniformly sampled exit or a ﬁxed exit\nn= 1,..., 6 at inference time for every time-step. The sampled exit experiment tests the robustness\nto mixed hidden states and the ﬁxed exit setup simulates an ideal setting where all previous states\nare available. As baselines we show six separate standard Transformers with N ∈[1..6] decoder\nblocks. All models are trained with an equal number of updates and mixed training withM=6 paths\nis most comparable to aligned training since the number of losses per sample is identical.\nTable 1 shows that aligned training outperforms mixed training both for ﬁxed exits as well as for ran-\ndomly sampled exits. The latter is surprising since aligned training never exposes the self-attention\nmechanism to hidden states from other blocks. We suspect that this is due to the residual connections\nwhich copy features from lower blocks to subsequent layers and which are ubiquitous in Transformer\nmodels (§2). Aligned training also performs very competitively to the individual baseline models.\nAligned training is conceptually simple and fast. We can process a training example with N exits\nin a single forward/backward pass while M passes are needed for mixed training. In the remaining\npaper, we use the aligned mode to train our models. Appendix A reports experiments with weighing\nthe various output classiﬁers differently but we found that a uniform weighting scheme worked\nwell. On our largest setup, WMT’14 English-French, the training time of an aligned model with six\noutput classiﬁers increases only marginally by about 1% compared to a baseline with a single output\nclassiﬁer keeping everything else equal.\n6\n(a) Token-specﬁc\n1 2 3 4 5 6\n33.5\n34.0\n34.5\n35.0\n35.5\nAverage exit (AE)\nBLEU BaselineAlignedTok-C MultinomialTok-LL MultinomialTok-C Geometric-likeTok-LL Geometric-like\n(b) Sequence-speciﬁc depth\n1 2 3 4 5 6\n33.5\n34.0\n34.5\n35.0\n35.5\nAverage exit (AE)\nBLEU\nBaselineAlignedSeq-LLSeq-C\n(c) Conﬁdence thresholding\n1 2 3 4 5 6\n33.5\n34.0\n34.5\n35.0\n35.5\nAverage exit (AE)\nBLEU\nBaselineAlignedTok-C Geometric-likeTok-LL Geometric-likeConﬁdence thresholding\nFigure 3: Trade-off between speed (average exit or AE) and accuracy (BLEU) for depth-adaptive\nmethods on the IWSLT14 De-En test set.\n0 0.2 0.41\n2\n3\n4\n5\n6\nRegularization parameterλ\nAverage exit (AE)\nσ→0σ= 1\n(a) Effect of λon AE\n0 1 2 33.5\n4\n4.5\n5\n5.5\n6\nRBF kernel widthσ\nAverage exit (AE)λ= 0.01λ= 0.05\n(b) Effect of σon AE\nFigure 4: Effect of the hyper-parameters σand λon the average exit (AE) measured on the valid set\nof IWSLT’14 De-En.\n4.3 A DAPTIVE DEPTH ESTIMATION\nNext, we train models with aligned states and compare adaptive depth classiﬁers in terms of BLEU\nas well as computational effort. We measure the latter as the average exit per output token (AE).\nAs baselines we use again six separate standard Transformers with N ∈[1..6] with a single output\nclassiﬁer. We also measure the performance of the aligned mode trained model for ﬁxed exits n ∈\n[1..6]. For the adaptive depth token-speciﬁc models (Tok), we train four combinations: likelihood-\nbased oracle (LL) + geometric-like, likelihood-based oracle (LL) + multinomial, correctness based\noracle (C) + geometric-like and correctness-based oracle (C) + multinomial. Sequence-speciﬁc\nmodels (Seq) are trained with the correctness oracle (C) and the likelihood oracle (LL) with different\nvalues for the regularization weightλ. All parameters are tuned on the valid set and we report results\non the test set for a range of average exits.\nFigure 3 shows that the aligned model (blue line) can match the accuracy of a standard 6-block\nTransformer (black line) at half the number of layers ( n = 3) by always exiting at the third block.\nThe aligned model outperforms the baseline for n= 2,..., 6.\nFor token speciﬁc halting mechanisms (Figure 3a) the geometric-like classiﬁers achieves a better\nspeed-accuracy trade-off than the multinomial classiﬁers (ﬁlled vs. empty triangles). For geometric-\nlike classiﬁers, the correctness oracle outperforms the likelihood oracle (Tok-C geometric-like vs.\nTok-LL geometric-like) but the trend is less clear for multinomial classiﬁers. At the sequence-level,\nlikelihood is the better oracle (Figure 3b).\nThe rightmost Tok-C geometric-like point ( σ = 0, λ = 0.1) achieves 34.73 BLEU at AE = 1.42\nwhich corresponds to similar accuracy as the N = 6 baseline at 76% fewer decoding blocks.\n7\n(a) BLEU vs. AE (test)\n1 2 3 4 5 6\n41.5\n42.0\n42.5\n43.0\n43.5\n44.0\nAverage exit (AE)\nBLEU BaselineAlignedSeq-LLTok-C PoissonTok-LL PoissonConﬁdence thresholding\n(b) BLEU vs. FLOPs (test)\n2 3 4 5\n·108\n41.5\n42.0\n42.5\n43.0\n43.5\n44.0\nAverage FLOPs\nBLEU BaselineAlignedSeq-LLTok-C PoissonTok-LL PoissonConﬁdence thresholding\nFigure 5: Speed and accuracy on the WMT’14 English-French benchmark (c.f . Figure 3).\nThe best accuracy of the aligned model is 34.95 BLEU at exit 5 and the best comparable Tok-C\ngeometric-like conﬁguration achieves 34.99 BLEU at AE = 1.97, or 61% fewer decoding blocks.\nWhen ﬁxing the budget to two decoder blocks, Tok-C geometric-like with AE = 1.97 achieves\nBLEU 35, a 0.64 BLEU improvement over the baseline ( N = 2) and aligned which both achieve\nBLEU 34.35.\nConﬁdence thresholding (Figure 3c) performs very well but cannot outperform Tok-C geometric-\nlike.\nAblation of hyper-parameters In this section, we look at the effect of the two main hyper-\nparameters on IWSLT’14 De-En:λthe regularization scale (c.f . Eq. (9)), and the RBF kernel width\nσ used to smooth the scores ( c.f . Eq. (15)). We train Tok-LL Geometric-like models and evaluate\nthem with their default thresholds (exit if χn\nt > 0.5). Figure 4a shows that higher values of λlead\nto lower exits. Figure 4b shows the effect ofσfor two values of λ. In both curves, we see that wider\nkernels favor higher exits.\n4.4 S CALING THE ADAPTIVE -DEPTH MODELS\nFinally, we take the best performing models form the IWSLT benchmark and test them on the large\nWMT’14 English-French benchmark. Results on the test set (Figure 5a) show that adaptive depth\nstill shows improvements but that they are diminished in this very large-scale setup. Conﬁdence\nthresholding works very well and sequence-speciﬁc depth approaches improve only marginally over\nthe baseline. Tok-LL geometric-like can match the best baseline result of BLEU 43.4 ( N = 6) by\nusing only AE = 2.40 which corresponds to 40% of the decoder blocks; the best aligned result\nof BLEU 43.6 can be matched with AE = 3.25. In this setup, Tok-LL geometric-like slightly\noutperforms the Tok-C counterpart.\nConﬁdence thresholding matches the accuracy of the N=6 baseline with AE 2.5 or 59% fewer de-\ncoding blocks. However, conﬁdence thresholding requires computing the output classiﬁer at each\nblock to determine whether to halt or continue. This is a large overhead since output classiﬁers pre-\ndict 44k types for this benchmark (§4.1). To better account for this, we measure the average number\nof FLOPs per output token (details in Appendix B). Figure 5b shows that the Tok-LL geometric-like\napproach provides a better trade-off when the overhead of the output classiﬁers is considered.\n4.5 Q UALITATIVE RESULTS\nThe exit distribution for a given sample can give insights into what a Depth-Adaptive Transformer\ndecoder considers to be a difﬁcult task. In this section, for each hypothesis ˜y, we will look at\nthe sequence of selected exits (n1,...,n |˜y|) and the probability scores (p1,...p |˜y|) with pt =\np(˜yt|hnt\nt−1) i.e. the conﬁdence of the model in the sampled token at the selected exit.\nFigures 6 and 7 show hypotheses from the WMT’14 En-Fr and IWSLT’14 De-En test sets, respec-\ntively. For each hypothesis we state the exits and the probability scores. In Figure 6a, predicting\n8\nChi@@\nrac\n,\nle\nPremier\nministre\n,\nétait\nprésent\n.\n</s>\n1\n2\n3\n4\n5\n6Exit\n0.2\n0.4\n0.6\n0.8\n1\n(a) Src: Chi@@rac , the Prime Minister , was there .\nRef: Chi@@rac , Premier ministre , est là .\nMais\nles\npassagers\nne\ndevraient\npas\ns’\nattendre\nà\ndes\nchangements\nimmédiats\n.\n</s>\n1\n2\n3\n4\n5\n6\n0.2\n0.4\n0.6\n0.8\n1\nScore\n(b) Src: But passengers shoul@@dn’t expect changes to\nhappen immediately .\nRef: Mais les passagers ne devraient pas s’ attendre à\ndes changements immédiats .\nFigure 6: Examples from the WMT’14 En-Fr test set (newstest14) with Tok-LL geometric-like depth\nestimation. Token exits are in blue and conﬁdence scores are in gray. The ‘ @ @’ are due to BPE or\nsubword tokenization. For each example the source ( Src) and the reference ( Ref) are provided in\nthe caption.\nyou\ncan\nper@@\nform\nthis\ntrick\nto\nyour\nfriends\nand\nneighb@@\nors\n.\nthank\nyou\n.\n</s>\n1\n2\n3\n4\n5\n6Exit\n0.2\n0.4\n0.6\n0.8\n1\nScore\n(a) Src: diesen trick können sie ihren freunden und nachbarn vor@@führen . danke .\nRef: there is a trick you can do for your friends and neighb@@ors . thanks .\nFigure 7: Example from the IWSLT’14 De-En test set with Tok-LL geometric-like depth estimation.\nSee Figure 6 for more details.\n‘présent’ (meaning ‘present’) is hard. A straightforward translation is ‘était là’ but the model chooses\n‘present’ which is also appropriate. In Figure 6b, the model uses more computation to predict the\ndeﬁnite article ‘les’ since the source has omitted the article for ‘passengers’.\nA clear trend in both benchmarks is that the model requires less computation near the end of de-\ncoding to generate the end of sequence marker </s>and the preceding full-stop when relevant. In\nFigure 8, we show the distribution of the exits at the beginning and near the end of test set hypothe-\nses. We consider the beginning of a sequence to be the ﬁrst 10% of tokens and the end as the last\n10% of tokens. The exit distributions are shown for three models on WMT’14 En-Fr: Model 1 has\nan average exit of AE = 2.53, Model2 exits at AE = 3.79 on average and Model3 with AE = 4.68.\nWithin the same models, deep exits late are used at the beginning of the sequence and early exits are\nselected near the end. For heavily regularized models such as Model1 with AE = 2.53, the disparity\nbetween beginning and end is less severe as the model exits early most of the time. Model 2 and\nModel3 are less regularized (higher AE) and tend to use late exits at the beginning of the sequence\nand early exits near the end. On the other hand, the more regularized Model1 with AE = 2.53 exits\n1 2 3 4 5 6\n0.2\n0.4\n0.6\nFrequency\nBeginning\nEnd\n1 2 3 4 5 6\nExit\n1 2 3 4 5 6\nModel1 Model2 Model3\nFigure 8: WMT’14 En-Fr test set: exit distributions in the beginning (relative-position: rpos<0.1)\nand near the end (rpos>0.9) of the hypotheses of three models.\n9\n1 2 3 4 5 6\n0.0-0.1\n0.1-0.2\n0.2-0.3\n0.3-0.4\n0.4-0.5\n0.5-0.6\n0.6-0.7\n0.7-0.8\n0.8-0.9\n0.9-1.0Conﬁdence\n1 2 3 4 5 6\nExit\n1 2 3 4 5 6\nModel1 Model2 Model3\nFigure 9: Joint histogram of the exits and the conﬁdence scores for 3 Tok-LL geometric-like models\non newstest14.\nearly most of the time. There is also a correlation between the model probability and the amount\nof computation, particularly in models with low AE . Figure 9 shows the joint histogram of the\nscores and the selected exit. For both Model 1 and Model2, low exits (n ≤2) are used in the high\nconﬁdence range [0.8 −1] and high exits ( n ≥4) are used in the low-conﬁdence range [0 −0.5].\nModel3 has a high average exit ( AE = 4.68) so most tokens exit late, however, in low conﬁdence\nranges the model does not exit earlier than n= 5.\n5 C ONCLUSION\nWe extended anytime prediction to the structured prediction setting and introduced simple but effec-\ntive methods to equip sequence models to make predictions at different points in the network. We\ncompared a number of different mechanisms to predict the required network depth and ﬁnd that a\nsimple correctness based geometric-like classiﬁer obtains the best trade-off between speed and ac-\ncuracy. Results show that the number of decoder layers can be reduced by more than three quarters\nat no loss in accuracy compared to a well tuned Transformer baseline.\nACKNOWLEDGMENTS\nWe thank Laurens van der Maaten for fruitful comments and suggestions.\nREFERENCES\nTolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks\nfor efﬁcient inference. In Proc. of ICML, 2017.\nM. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Federico. Report on the 11th iwslt evaluation\ncampaign. In IWSLT, 2014.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. In Proc. of ICLR, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proc. of NAACL, 2019.\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical\nstructured prediction losses for sequence to sequence learning. In Proc. of NAACL, 2018.\nMichael Figurnov, Artem Sobolev, and Dmitry P. Vetrov. Probabilistic adaptive computation time.\nIn ArXiv preprint, 2017.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional\nsequence to sequence learning. In Proc. of ICML, 2017.\nAlex Graves. Adaptive computation time for recurrent neural networks. In ArXiv preprint, 2016.\n10\nGao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q Weinberger.\nMulti-scale dense networks for resource efﬁcient image classiﬁcation. In Proc. of ICLR, 2017.\nD. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fair’s\nwmt19 news translation task submission. In Proc. of WMT, 2019.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL,\n2019.\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: a method for automatic evaluation of\nmachine translation. In Proc. of ACL, 2002.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. In Technical report, OpenAI., 2019.\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\nunits. In Proc. of ACL, 2016.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via\nearly exiting from deep neural networks. In ICPR, 2016.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Proc. of NeurIPS, 2017.\nXin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning dy-\nnamic routing in convolutional networks. In Proc. of ECCV, 2018.\n11\nAPPENDIX A L OSS SCALING\nIn this section we experiment with different weights for scaling the output classiﬁer losses. Instead\nof uniform weighting, we bias towards speciﬁc output classiﬁers by assigning higher weights to their\nlosses. Table 2 shows that weighing the classiﬁers equally provides good results.\nUniform n= 1 n= 2 n= 3 n= 4 n= 5 n= 6 Average\nBaseline - 34.2 35.3 35.6 35.7 35.6 35.9 35.4\nωn = 1 35.5 34.1 35.5 35.8 36.1 36.1 36.2 35.6\nωn = n 35.3 32.2 35.0 35.8 36.0 36.2 36.3 35.2\nωn = √n 35.4 33.3 35.2 35.8 35.9 36.1 36.1 35.4\nωn = 1/√n 35.6 34.5 35.4 35.7 35.8 35.8 35.9 35.5\nωn = 1/n 35.3 34.7 35.3 35.5 35.7 35.8 35.8 35.5\n(a) IWSLT De-En - Valid\nUniform n= 1 n= 2 n= 3 n= 4 n= 5 n= 6 Average\nBaseline - 33.7 34.6 34.6 34.6 34.6 34.8 34.5\nωn = 1 34.4 33.2 34.4 34.8 34.9 35.0 34.9 34.5\nωn = n 34.2 31.4 33.8 34.7 34.8 34.8 34.9 34.1\nωn = √n 34.4 32.5 34.1 34.8 34.9 35.0 35.1 34.4\nωn = 1/√n 34.6 33.7 34.3 34.6 34.8 34.8 34.9 34.5\nωn = 1/n 34.2 33.8 34.3 34.5 34.6 34.7 34.7 34.4\n(b) IWSLT De-En - Test\nTable 2: Aligned training with different weights (ωn) on IWSLT De-En. For each model we report\nBLEU on the dev set evaluated with a uniformly sampled exit n ∼U([1..6]) for each token and a\nﬁxed exit n ∈[1..6] throughout the sequence. The average corresponds to the average BLEU over\nthe ﬁxed exits.\nGradient scaling Adding intermediate supervision at different levels of the decoder results in\nricher gradients for lower blocks compared to upper blocks. This is because earlier layers affect\nmore loss terms in the compound loss of Eq. (4). To balance the gradients of each block in the\ndecoder, we scale up the gradients of each loss term (−LLn) when it is updating the parameters\nof its associated block ( blockn with parameters θn) and revert it back to its normal scale before\nback-propagating it to the previous blocks. Figure 10 and Algorithm 1 illustrate this gradient scaling\nprocedure. The θn are updated withγn-ampliﬁed gradients from the block’s supervision and(N−n)\ngradients from the subsequent blocks. We choose γn = γ(N −n) to control the ratio γ:1 as the\nratio of the block supervision to the subsequent blocks’ supervisions.\nTable 3 shows that gradient scaling can beneﬁt the lowest layer at the expense of higher layers.\nHowever, no scaling generally works very well.\nhn−1 blockn; θn blockn+1; θn+1 blockn+2; θn+2 ... blockN ; θN\nγN∇LLNγn+2∇LLn+2γn+1∇LLn+1γn∇LLn\n∇LLN∇LLn+2\n∇LLN\n∇LLn+1\n∇LLn+2\n∇LLN\nFigure 10: Illustration of gradient scaling.\n12\nAlgorithm 1 Pseudo-code for gradient scaling (illustrated for a single step t)\n1: for n∈1..N do\n2: hn\nt = blockn(hn−1\nt )\n3: p(yt+1|hn\nt ) = softmax(Wnhn\nt )\n4: p(yt+1|hn\nt ) =SCALE _GRADIENT (p(yt+1|hn\nt ),γn)\n5: if n<N then hn\nt = SCALE _GRADIENT (hn\nt , 1\nγn+1\n)\n6: end for\n7: function SCALE _GRADIENT (Tensor x, scale γ)\n8: return γx+ (1−γ)STOP _GRADIENT (x)\n9: ⊿STOP _GRADIENT in PyTorch with x.detach().\n10: end function\nUniform n= 1 n= 2 n= 3 n= 4 n= 5 n= 6 Average\nBaseline - 34.2 35.3 35.6 35.7 35.6 35.9 35.4\n∅ 35.5 34.1 35.5 35.8 36.1 36.1 36.2 35.6\nγ = 0.3 35.1 33.7 34.7 35.3 35.7 35.8 36.0 35.2\nγ = 0.5 35.4 34.8 35.4 35.6 35.6 35.7 35.6 35.4\nγ = 0.7 34.9 34.6 35.1 35.1 35.2 35.4 35.3 35.1\nγ = 0.9 34.9 34.8 35.3 35.3 35.3 35.4 35.5 35.3\nγ = 1.1 35.1 34.9 35.2 35.3 35.3 35.3 35.3 35.2\n(a) IWSLT De-En - Valid\nUniform n= 1 n= 2 n= 3 n= 4 n= 5 n= 6 Average\nBaseline - 33.7 34.6 34.6 34.6 34.6 34.8 34.5\n∅ 34.4 33.2 34.4 34.8 34.9 35.0 34.9 34.5\nγ = 0.3 34.2 32.8 33.9 34.3 34.6 34.8 35.0 34.2\nγ = 0.5 34.5 33.8 34.2 34.6 34.5 34.7 34.7 34.6\nγ = 0.7 34.0 33.7 34.2 34.3 34.3 34.3 34.3 34.2\nγ = 0.9 34.1 34.0 34.2 34.3 34.4 34.4 34.4 34.3\nγ = 1.1 34.2 34.0 34.3 34.3 34.3 34.3 34.2 34.2\n(b) IWSLT De-En - Test\nTable 3: Aligned training with different gradient scaling ratios γ : 1on IWSLT’14 De-En. For each\nmodel we report the BLEU4 score evaluated with a uniformly sampled exit n∼U([1..6]) for each\ntoken and a ﬁxed exit n∈[1..6]. The average corresponds to the average BLEU4 of all ﬁxed exits.\n13\nAPPENDIX B FLOPS APPROXIMATION\nThis section details the computation of the FLOPS we report. The per token FLOPS are for the\ndecoder network only since we use an encoder of the same size for all models. We breakdown\nthe FLOPS of every operation in Algorithm 2 (blue front of the algorithmic statement). We omit\nnon-linearities, normalizations and residual connections. The main operations we account for are\ndot-products and by extension matrix-vector products since those represent the vast majority of\nFLOPS (we assume batch size one to simplify the calculation).\nParameters\ndd decoder embedding dimension.\nde encoder embedding dimension.\ndf The feed-forward network dimension.\n|x| source length.\nt Current time-estep (t≥1).\nV output vocabulary size.\nOperation FLOPS\nDot-product (d) 2d−1\nLinear din →dout 2dindout\nTable 4: FLOPS of basic operations, key parameters and variables for the FLOPS estimation.\nWith this breakdown, the total computational cost at time-step tof a decoder block that we actually\ngo through, denoted with FC, is:\nFC(x,t) = 12d2\nd + 4df dd + 4tdd + 4|x|dd + 4[ [FirstCall] ]|x|ddde,\nwhere the cost of mapping the source’ keys and values is incurred the ﬁrst time the block is called\n(ﬂagged with FirstCall). This occurs at t = 1for the baseline model but it is input-dependent with\ndepth adaptive estimation and may never occur if all tokens exit early.\nIf skipped, a block still has to compute the keys and value of the self-attention block so the self-\nattention of future time-steps can function. We will denote this cost with FS and we haveFS = 4d2\nd.\nDepending on the halting mechanism, an exit prediction cost, denoted wit FP, is added:\nSequence-speciﬁc depth: FP(t,q(t)) = 2[ [t= 1] ]Ndd\nToken-speciﬁc Multinomial: FP(t,q(t)) = 2Ndd\nToken-speciﬁc Geometric-like: FP(t,q(t)) = 2ddq(t)\nConﬁdence thresholding: FP(t,q(t)) = 2q(t)Vdd\nFor a set of source sequences {x(i)}i∈Iand generated hypotheses {y(i)}i∈I, the average ﬂops per\ntoken is:\nBaseline (N blocks): 1∑\ni|y(i)|\n∑\ni\n∑|y(i)|\nt=1\n(\nN FC(x(i),t) + 2Vdd\n)\nAdaptive depth: 1∑\ni|y(i)|\n∑\ni\n∑|y(i)|\nt=1\n(\nq(t)FC(x(i),t) + (N −q(t))FS + FP(t,q(t)) + 2Vdd\n)\nIn the case of conﬁdence thresholding the ﬁnal output prediction cost ( 2Vdd) is already accounted\nfor in the exit prediction cost FP.\n14\nAlgorithm 2 Adaptive decoding with Tok-geometric-like\n1: Input: source codes s, incremental state\n2: Initialization: t= 1, y1 = <s>\n3: for n∈1 ...N do\n4: FirstCall[n] = True. ⊿A ﬂag signaling if the source’ keys and values should be evaluated.\n5: end for\n6: while yt ̸= </s>do\n7: Embed the last output token yt.\n8: for n∈1 ...N do\n9: ⊿Self-attention.\n10: - Map the input into a key (k) and value (v). FLOPS=4d2\nd\n11: - Map the input into a query q. FLOPS=2d2\nd\n12: - Score the memory keys with qto get the attention weights α. FLOPS=4tdd\n13: - Map the attention output. FLOPS=2d2\nd\n14: ⊿Encoder-Decoder interaction.\n15: if FirstCall[n] then\n16: Map the source states into keys and values for the nth block. FLOPS=4|x|dedd\n17: FirstCall[n] = False\n18: end if\n19: - Map the input into a query q. FLOPS=2d2\nd\n20: - Score the memory keys with qto get the attention weights α. FLOPS=4|x|dd\n21: - Map the attention output. FLOPS=2d2\nd\n22: Feed-forward network. FLOPS=4dddf\n23: Estimate the halting probability χt,n. FLOPS=2dd\n24: if χt,n >0.5 then\n25: Exit the loop (Line 8)\n26: end if\n27: end for\n28: if n<N then\n29: ⊿Skipped blocks.\n30: for ns ∈n+ 1...N do\n31: Copy and map the copied state into a key (k) and value (v). FLOPS=4d2\nd\n32: end for\n33: end if\n34: Project the ﬁnal state and sample a new output token. FLOPS=2Vdd\n35: t++\n36: end while\n15",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5357303619384766
    },
    {
      "name": "Computer science",
      "score": 0.3915402293205261
    },
    {
      "name": "Electrical engineering",
      "score": 0.27768585085868835
    },
    {
      "name": "Engineering",
      "score": 0.154870867729187
    },
    {
      "name": "Voltage",
      "score": 0.11840090155601501
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210114444",
      "name": "Meta (United States)",
      "country": "US"
    }
  ]
}