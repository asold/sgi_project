{
    "title": "“Let me explain!”: exploring the potential of virtual agents in explainable AI interaction design",
    "url": "https://openalex.org/W3041723710",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2483837154",
            "name": "Katharina Weitz",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2786066099",
            "name": "Dominik Schiller",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2785769430",
            "name": "Ruben Schlagowski",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2098964097",
            "name": "Tobias Huber",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2099360608",
            "name": "Elisabeth André",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2483837154",
            "name": "Katharina Weitz",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2786066099",
            "name": "Dominik Schiller",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2785769430",
            "name": "Ruben Schlagowski",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2098964097",
            "name": "Tobias Huber",
            "affiliations": [
                "University of Augsburg"
            ]
        },
        {
            "id": "https://openalex.org/A2099360608",
            "name": "Elisabeth André",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3010300896",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2122889832",
        "https://openalex.org/W3404557",
        "https://openalex.org/W114912394",
        "https://openalex.org/W1999478155",
        "https://openalex.org/W2143331706",
        "https://openalex.org/W2963847595",
        "https://openalex.org/W4243947120",
        "https://openalex.org/W2143074722",
        "https://openalex.org/W2162651730",
        "https://openalex.org/W2044140042",
        "https://openalex.org/W2586602577",
        "https://openalex.org/W2113362740",
        "https://openalex.org/W4243342770",
        "https://openalex.org/W2962790223",
        "https://openalex.org/W2321972046",
        "https://openalex.org/W2963095307",
        "https://openalex.org/W2657631929",
        "https://openalex.org/W2963609017",
        "https://openalex.org/W2818161558",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2616247523",
        "https://openalex.org/W2901782613",
        "https://openalex.org/W2069743909",
        "https://openalex.org/W2008535554",
        "https://openalex.org/W2889467522",
        "https://openalex.org/W2736191430",
        "https://openalex.org/W2951134183",
        "https://openalex.org/W2953923985",
        "https://openalex.org/W2900728424",
        "https://openalex.org/W2618099328",
        "https://openalex.org/W2903036152",
        "https://openalex.org/W2407023693",
        "https://openalex.org/W2808078667",
        "https://openalex.org/W3102564565",
        "https://openalex.org/W2797583228",
        "https://openalex.org/W2251135874",
        "https://openalex.org/W2110171129",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2807015674",
        "https://openalex.org/W1488545818",
        "https://openalex.org/W1498892783"
    ],
    "abstract": "Abstract While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user’s trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants’ feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.",
    "full_text": "Journal on Multimodal User Interfaces (2021) 15:87–98\nhttps://doi.org/10.1007/s12193-020-00332-0\nORIGINAL PAPER\n“Let me explain!”: exploring the potential of virtual agents in\nexplainable AI interaction design\nKatharina Weitz 1 · Dominik Schiller 1 · Ruben Schlagowski 1 · Tobias Huber 1 · Elisabeth André 1\nReceived: 2 November 2019 / Accepted: 16 June 2020 / Published online: 9 July 2020\n© The Author(s) 2020\nAbstract\nWhile the research area of artiﬁcial intelligence beneﬁted from increasingly sophisticated machine learning techniques in\nrecent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In\nthis paper, we explore the effects of incorporating virtual agents into explainable artiﬁcial intelligence (XAI) designs on the\nperceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for\nkeyword classiﬁcation. As a result of this experiment, we found that the integration of virtual agents leads to increased user\ntrust in the XAI system. Furthermore, we found that the user’s trust signiﬁcantly depends on the modalities that are used within\nthe user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined\nwith a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the\nparticipants’ feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction\nwith the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design.\nBased on these results, we discuss current limitations and interesting topics for further research in the ﬁeld of XAI. Moreover,\nwe present design recommendations for virtual agents in XAI systems for future projects.\nKeywords Explainable artiﬁcial intelligence · Interpretable artiﬁcial intelligence · Virtual agents · Human-agent interaction ·\nDeep learning · Trust\n1 Introduction\nThe research area of artiﬁcial intelligence beneﬁted from\nincreasingly sophisticated machine learning techniques in\nrecent years. As an effect, a variety of use cases for these new\ntechnologies found their way into the everyday lives of a mul-\ntitude of users. As an example, automatic speech recognition\nis already powering a new generation of voice assistants like\nAmazon’s Alexa, Google’s Assistant or Apple’s Siri.\nWhile those advancements are leading to improved and\nmore intuitive ways of interacting with AI systems, the\nunderlying algorithms are growing in complexity and there-\nElectronic supplementary material The online version of this article\n(https://doi.org/10.1007/s12193-020-00332-0 ) contains\nsupplementary material, which is available to authorized users.\nB Katharina Weitz\nweitz@hcm-lab.de\n1 Department of Computer Science, Human-Centered\nMultimedia, Augsburg University, Universitätsstraße 6a,\nAugsburg, Germany\nfore decreasing the system’s comprehensibility. This is not\nonly complicating matters for machine learning engineers\nand practitioners, who are working on improving the perfor-\nmance of their models, but also proposes some new chal-\nlenges when it comes to end-user related human-computer\ninteraction. Evidence suggests that a lack of transparency,\nwith respect to the decisions of an AI system, might have\na negative impact on the trustworthiness of a system. This\nlack of trustworthiness can also decrease the overall user-\nexperience [ 13,35].\nThe reemerging research ﬁeld of XAI [ 11] investigates\napproaches to address this problem. One goal of XAI is\nthe development of innovative explanation algorithms which\nare promising to grant new insights into state of the art\nmachine learning black box models, and thereby helping\nthe user better understand and trust an AI system [ 4,16,19].\nMany approaches are relying on visualisation techniques\nlike saliency maps to highlight the parts of the input that\nwere most relevant for the decision of a model. Although\nthose efforts achieved remarkable progress in recent years,\nconcerns have been expressed that the development of expla-\n123\n88 Journal on Multimodal User Interfaces (2021) 15:87–98\nnation methods has been focused too much on building\nsolutions for AI-experts while neglecting end-users [ 21].\nWeitz et al. [ 44] concluded that those approaches are not\nyet at a point where they can be utilized to beneﬁt the user\ndirectly.\nA potential next step that steers explainable artiﬁcial\nintelligence research into a more user-centered direction\nhas been proposed by De Graaf and Malle [ 6]. They sug-\ngested that humans are approaching the explanations of an\nAI system with the same attitude of expectation, they are\nemploying towards another human. Therefore the genera-\ntion of explanations within the bounds of the conceptual\nand linguistic framework of human behaviour could greatly\nimprove the transparency and explainability of AI systems\ntowards end-users. V an Mulken et al. [ 38] additionally found\nout that personiﬁed agents can have positive effects on the\nperceived difﬁculty of processing technical information in\nhuman–computer interaction while not decreasing the over-\nall objective performance of the user.\nSimilar results were found in Weitz et al. [ 45], where\nwe showed that end-users trust an AI system for speech\nrecognition more when a virtual agent is presenting XAI\nvisualisations.\nBased on the results of this study, our paper evaluates in\ndetail which aspects of a virtual agent are relevant to sup-\nport XAI visualisations. To this end, we focus on assessing\nthe effect of different levels of anthropomorphism/human-\nlikeness of an agent (voice, visualisation, and the content of\nwhat is said).\nFor this evaluation we conducted a user study in which a\nvirtual agent presented XAI visualisations to users of a simple\nArtiﬁcial Neural Network (ANN)-based speech recognition\nmodel, which classiﬁes audio keywords based on visual rep-\nresentations of the audio signal (spectrograms). To this end,\nwe split the participants into three groups, which interacted\nwith different versions of the same virtual agent (text, voice,\nand visual presence), and a baseline group without a vir-\ntual agent. We are aiming to examine the following research\nquestions:\n1. Does the usage of a virtual agent positively impact the\nperceived trustworthiness of AI systems like deep neural\nnetworks?\n2. Which of the three modalities of a virtual agent that we\ntested (pure information in form of text, voice, and visual\npresence) are important for an impact on the perceived\ntrustworthiness of an AI system?\n3. How are the presented XAI visualisations perceived and\nrated by users?\n4. How does the use of virtual agents affect the perception\nof the presented XAI visualisations?\nIn order to answer the ﬁrst and second research ques-\ntion, we formulated a directional hypothesis that is evaluated\nwithin the scope of a contrast analysis. To calculate the effect\nsize, we used the recommendations for contrast analyses\nfrom Perugini et al. [ 24].\nFor our hypothesis we assume a linear trend, which means\nthat the general trust increases depending on the virtual agent\ngroup where the baseline group without agent has the lowest\ngeneral trust score, followed by the text agent group, the voice\nagent group, and the virtual agent group with the highest\nscores in general trust.\nThe third and fourth research question will be evalu-\nated qualitatively as well as quantitatively by performing an\nANOV A to determine the impact of the different virtual agent\nmodalities on the rating of XAI visualisations of the partici-\npants.\nOverall, our paper contains three contributions:\n1. We present a novel XAI interaction design where we\nemploy a virtual agent to present XAI visualisations for\na simple ANN-based speech recognition model which\nclassiﬁes audio keywords.\n2. We conducted a user-study to empirically verify the\nimpact of the human-likeness of a virtual agent on the\nhelpfulness of XAI visualisations and perceived trust in\nthe system.\n3. Based on the results of this study we are presenting sug-\ngestions to improve the integration of virtual agents in\nXAI interaction designs.\nThe remainder of this paper is divided into six sections. Sec-\ntion 2 introduces the related work with respect to current\nstate of the art XAI systems and explanations from the stand-\npoint of human-like interactions. In Sect. 3 we describe the\nimplemented speech recognition system and the XAI algo-\nrithm LIME [ 26] that we used for our study. Then in Sect. 4\nwe present our experimental setup in detail, followed by the\nresults of our study in Sect. 5. Finally, we are closing the\npaper with a discussion of results in Sect. 6 as well as a ﬁnal\nconclusion and an outlook for future work in Sect. 7.\n2 Related work\nIn this section we are presenting an overview of related work\nin the area of explainable AI. We split this overview in two\nsubsections regarding the technical aspects of XAI as well\nas the human interactive aspects of explanations in general.\n2.1 Explainable AI\nCurrent state of the art approaches for artiﬁcial intelligence\nare increasingly relying on the deployment of Machine\n123\nJournal on Multimodal User Interfaces (2021) 15:87–98 89\nLearning (ML) models across a wide range of applica-\ntion areas. Speciﬁcally, Deep Learning methods, which are\nable to automatically learn abstract high level features from\nlow-level or even raw input, are continuously growing in pop-\nularity. On the one hand such models are often able to push\nresults further beyond the state of the art in human-computer-\ninteraction related prediction problems like automatic speech\nrecognition [47], activity recognition [ 42], or sensing a users\naffective state [ 41]. On the other hand, they are confronted\nwith a trade-off between accuracy and comprehensibility,\nwhere the inner workings of the applied models are becom-\ning increasingly more complex and therefore opaque, which\nmakes them impossible to comprehend for humans [ 29].\nThis problem is being addressed by recent research within\nthe realm of XAI and interpretable AI. Since both terms are\noften used synonymously [ 10], we will subsume work from\nboth ﬁelds under the term XAI. [ 18] pointed out that there are\nvarying deﬁnitions for the exact goal of XAI in the literature.\nFor this paper, we adopt the view of Gilpin et al. [ 10], who\nstate that the goal of XAI is the description of a system,\nits internal states, and its decisions in such a way that this\ndescription can be understood by human beings.\nWe focus on XAI approaches that attempt to shed light on\nspeciﬁc decisions of incomprehensible ML models by visu-\nally highlighting parts of the input data according to their\nrelevance for that decision. XAI visualisations are suitable\nfor classiﬁcations on the basis of visual input, which is eas-\nier to interpret for humans than raw data [ 22]. One of the ﬁrst\napproaches for creating XAI visualisations was to use gra-\ndients with respect to the input [ 34] to measure how much\na small change in each input part would affect the predic-\ntion. Later, the Grad-CAM algorithm [ 31] used gradients\nwith respect to intermediate results of the prediction model\nto achieve the same goal. Another method to determine rel-\nevance is Layerwise-Relevance Propagation [ 2], which uses\nthe intermediate results of the model during the prediction to\ncalculate the contribution of each part of the input to the over-\nall prediction. While these concepts can theoretically be used\non various machine learning models, the deployed algorithms\nare often optimized for neural networks and require adjust-\nments for other models. A different take on the explanation\nof opaque machine learning models is proposed by Ribeiro et\nal. [26] in the form of the LIME framework. In contrast to the\naforementioned methods, model-agnostic approaches like\nLIME have the advantage of being universally applicable,\nindependent of the input data or the utilized machine learn-\ning algorithm. LIME addresses the task of generating XAI\nvisualisations by training a simple machine learning model\nto approximate the underlying model locally. To accomplish\nthis, the image to be explained is divided into superpixels by\na segmentation algorithm. These superpixels are then ran-\ndomly grayed out to create a new dataset with perturbed\nsample instances. This dataset is used to train a more explain-\nable model, which uses the presence of superpixels as input,\nto predict the decisions of the original model. By analyzing\nthis explainable model, the effect that each superpixel has on\nthe overall prediction of the original model can be assessed.\n2.2 Explanations and human-like interactions\nRecent research indicates that modern XAI approaches,\nwhich are aiming at unraveling the non-linear maze of state\nof the art machine learning models, are not quite meeting the\nrequirements to impart enough insightful information to the\nend-user.\nWhile Ribeiro et al. [ 26] used three different tasks to\ndemonstrate the usefulness of LIME for human end-users,\neach of these tasks was tailored to show a speciﬁc problem.\nIt is unclear whether these results generalize to more com-\nplex problems. Layerwise-Relevance Propagation and Grad-\nCAM were evaluated in more general settings. Alqaraawi et\nal. [ 1] conducted a user study investigating the usefulness\nof Layerwise-Relevance Propagation visualisations on the\n2012 version of the PASCAL Visual Object Classes dataset\nand Selvaraju et al. [ 32] used the 2007 version of the same\ndataset to evaluate Grad-CAM. In both studies, the partic-\nipants were able to correctly guess the model’s prediction\nbased on the XAI visualisations about 60 % of the time\n(60.07% for LRP , 61.23% for Grad-CAM). While this was\nbetter than the respective comparison groups, it is not a good\nresult by itself. This indicates that there is potential for XAI\nvisualisations but that they are not yet on a level where they\ncan be used on their own.\nA promising way to increase the accessibility of XAI\nvisualisations is to incorporate them into a human-like inter-\naction. De Graaf and Malle [ 6] hypothesized that people\nare applying human traits to AI systems and will, there-\nfore, expect explanations within the conceptual and linguistic\nframework used to explain human behaviours. They argue\nthat people are more likely to form a correct mental model\nof an AI system and recalibrate their trust in the system\nif it communicates explanations in a human-like way. The\nchallenging nature of this task is reﬂected by the recent\ngrowth in research addressing that topic. Richardson and\nRosenfeld [ 27] are proposing a taxonomy of interpretabil-\nity to answer the questions of why-, what-, how-, and when\na machine learning driven human-agent system should gen-\nerate an explanation for the user. However, they focused on\nself-explaining agents instead of investigating how agents\ncan improve existing XAI methods.Broekens et al. [ 3]i n v e s -\ntigated the effect of different types of explanations for agents\nthat are driven by a belief-desire-intention model (BDI-\nAgents). Similar to the work presented in this paper, they\nconducted a user study to investigate explainability in the\ncontext of BDI-Agents. As a result of their study, they came\nup with a set of guidelines for the future development of\n123\n90 Journal on Multimodal User Interfaces (2021) 15:87–98\nexplainable BDI-Agents. For example, they point out that\nthe overall goal an agent aims to achieve should be pre-\nsented transparently and that certain additional information\nis required, depending on the type of action the agent takes.\nMiller [20] conducted an interdisciplinary survey with the\ngoal of exploring ways to deﬁne, generate, select, present, and\nevaluate explanations with a focus on XAI. He came up with\nfour major ﬁndings regarding the properties of explanations\nin human-like interactions:\n– Explanations are contrastive : People tend\nto not ask why something happened, but rather why\nsomething happened instead of something else. They\nare therefore implicitly creating a reference between the\nactual occurrence of an event and their own expectations.\n– Explanations are selected| : People are rarely\nexpecting an explanation to be covering all potential\ncauses. An explanation is rather communicated by high-\nlighting one speciﬁc reason.\n– Probabilities probably don’t matter| :\nCausal explanations are more important than pure cor-\nrelations. Therefore, explanations with the highest prob-\nabilities are not necessarily the best explanations for a\nuser.\n– Explanations are social| : Explanations are a\ntransfer of knowledge as part of a conversation or inter-\naction. This also involves queries by the interlocutor that\nreceives the explanation as well as adaption to this his or\nher preferences (e.g., style of communication or available\nbackground knowledge).\nBased on these works we argue that one should take inspi-\nration from the human explanation process when developing\nXAI interaction designs.\n3 Keyword classiﬁer and explainable AI\nimplementation\nOut of the XAI approaches previously discussed in Sect. 2,\nwe chose the LIME framework by Ribeiro et al. [ 26]t o\nexplain the automatic recognition of spoken keywords within\nour user study. The underlying algorithm creates XAI visu-\nalisations for any classiﬁcation or regression based system\nby approximating predictions locally with an an explainable\nmodel.\nAs we stated before, the LIME algorithm only creates\ninterpretable visualisations. However, it is often difﬁcult to\nextract meaningful information from the visual representa-\ntion of raw audio data (i.e., a wave form). We therefore chose\nto present our XAI visualisations in the form of highlighted\nspectrograms (see Fig. 1). Spectrograms are visual represen-\ntations (images) of audio samples and display sound pressure\nFig. 1 A spectrogram of an audio sample (left), its segmentation into\nsuperpixels (center) and the output for the user containing LIME visu-\nalisations and additional phoneme information (right)\nlevels as pixel values over the dimensions time (x-axis) and\nfrequency (y-axis). Figure 1 illustrates the spectrogram for\nthe spoken input word ’house’ on the left as an example.\nThese spectrograms are calculated from the respective audio\nsignal and used as input for our classiﬁcation model.\nAs prediction model we used the neural network architec-\nture proposed by Sainath and Parada [ 28]. This ANN-based\nclassiﬁcation model uses a convolutional neural network to\ngenerate abstract features based on mel-frequency cepstrum\ncoefﬁcients (MFCCs) which are derived from the spectro-\ngrams of the raw audio wave forms. These features are then\nfed into a fully-connected layer which ﬁnally predicts the tar-\nget class, which is one of the keywords (labels) of the training\ndataset (see Fig. 2).\nWe trained our model on the speech command dataset\nprovided by Warden [ 43]. This dataset consists of instances\nfrom 35 different spoken words and was speciﬁcally designed\nto train and evaluate audio keyword classiﬁcation systems.\nThe comparably high ratio of samples per class to the over-\nall number of classes and the high variance with respect to\nspeakers and sound-quality, enabled us to train a fairly robust\nmodel for our speciﬁc use case.\nTo generate a visual explanation for a speciﬁc prediction\n(keyword) of our classiﬁcation model, the input-spectrogram\nis ﬁrst segmented by the Felzenszwalb’s algorithm for image\nsegmentation [ 7]( s e eF i g . 1, centered image) into so-called\nsuperpixels. Subsequently, the generated superpixels are ran-\ndomly greyed out to conceal the visual information they\ncontain. Afterwards, a more explainable model is trained to\npredict the decisions of the original model on those perturbed\nimages based on a binary vector that encodes which super-\npixels were grayed out in the input image. Analyzing this new\nmodel enables us to assess the effect that each superpixel has\n123\nJournal on Multimodal User Interfaces (2021) 15:87–98 91\nFig. 2 XAI visualisation of the spoken keyword “seven”. With every XAI visualisation the predicted label and the prediction accuracy of the speech\nrecognition system were displayed\non the overall prediction of the model. Finally, superpixels\nthat are found to have a signiﬁcant impact in favor of a spe-\nciﬁc label are highlighted green for the user, whereas red\nhighlighted segments speak against the predicted label (see\nFig. 1, right image).\nTo further enhance the explainability of the LIME visual-\nisations, we are also presenting a phoneme based segmenta-\ntion of the input-word to the user. Phonemes are small units of\nsound that can be used to distinguish one word from another.\nTherefore they are particularly well suited to assist with the\nestablishment of a relation between the way humans under-\nstand spoken language and the visualisations provided by\nour system. The phoneme segmentation of the spectrogram\nis generated through the WebMAUS tool developed by Kisler\net al. [ 15]. An example of this segmentation for the spoken\nword ’house’ can be seen in Fig. 1 in the right image.\n4S t u d y\nTo investigate the effect of agents in combination with XAI\nvisualisations, we conducted a user study with 60 partici-\npants. Each participant was given the same ten prescribed\nEnglish keywords (i.e., dog, four, happy, core, on, right,\neleven, two, seven, cat) to speak into our speech recogni-\ntion system. Only eight of those keywords were part of the\ntraining data, whereas the remaining two words (i.e., core\nand eleven) were unknown to the classiﬁcation system and\nwould therefore be wrongly classiﬁed for sure. The intention\nbehind this was to verify that the generated explanations help\nthe user understand both correct and incorrect predictions. In\norder to reduce statistical deviations of the prediction model\nand the explanation framework we chose keywords which\nwe found to reliably produce comprehensible explanations\nin advance. Prior to the test, the supervisor introduced the\nsimple graphical user interface (GUI) to the participants and\na textual cover story provided detailed instructions about\nhow to read the systems’ explanations and spectrograms.\nThen, every participant interacted with the GUI and spoke\na predeﬁned and ﬁxed sequence of the ten chosen keywords\ninto a microphone. After each recording, the audio data was\nclassiﬁed by the model and a XAI visualisation for this clas-\nsiﬁcation was displayed together with the predicted label and\nFig. 3 Modalities of the conducted user study. Four different groups\nreceived information to understand the prediction of a speech recogni-\ntion system\nthe prediction accuracy of the speech recognition system (see\nFig. 3). For wrong classiﬁcations, the XAI visualisations\nfor the three predictions with the highest probability were\npresented. Before continuing to the next keyword, the par-\nticipants rated the helpfulness (‘not helpful’, ‘helpful’, and\n‘don’t know’) of the XAI visualisation in a questionnaire. To\nexamine the inﬂuence of the human-likeness of a virtual agent\non the XAI interaction design, some participants received\ninformation by a virtual agent in addition to the XAI visual-\nisations. To this end, we split the 60 participants evenly into\nfour test groups of 15: text agent group (only textual informa-\ntion), voice agent group (only information via voice), virtual\nembodied agent group (visual presence and voice), and a\nno agent group (see Table 1). The no agent group received\nonly the XAI visualisations without further commentary. The\n123\n92 Journal on Multimodal User Interfaces (2021) 15:87–98\nTable 1 Demographic information of the participants\nCharacteristic Agent No\nText V oice Virtual Agent\nn 15 15 15 15\nAge\nM 25.7 25.0 28.2 27.27\nSD 3.99 5.6 8.6 5.19\nGender\nMale 12 11 10 12\nFemale 3 4 5 3\nExperience\nV oice assistants 11 13 10 8\nAudio processing 5 4 7 5\nVirtual agents 5 4 6 6\nother three groups received additional information in varying\nmodalities from a virtual agent named Gloria (see Fig. 4).\nThe information given by the agent was selected dynami-\ncally from a set of phrases that were designed by our team in\nadvance. These phrases were designed to communicate the\nfollowing information:\n– Acknowledgement of user inputs, e.g., “Ok the system\ngot that!”\n– Comments on the prediction accuracy of the neural net-\nwork, e.g., “The system was pretty sure you said seven!”\n– Comments on important phonemes within the output of\nthe XAI framework, e.g., “Phoneme number two was\nfound to have a particularly positive effect towards the\nprediction.”\nThe text agent group received only the textual output of Glo-\nria’s comments in a separate GUI. The voice agent group,\nin contrast, received the same information via text-to-speech\nprovided by Amazon Polly.\n1 The third group saw, in addition\nto the speech output, the virtual presence of a 3D-character\ndesigned by the Charamel GmbH, 2 which lip-synced the\nphrases and performed body gestures while communicating\n(see Fig. 5).\nAfter the experiment, all participants rated their impres-\nsion of and their trust in the system and answered the Trust\nin Automation (TiA) questionnaire [ 14]. Additionally, we\nused a combination of 7-point Likert scales and open form\nquestionnaires to collect qualitative and quantitative user\nfeedback.\n3 Furthermore, the user’s individual impressions\n1 https://aws.amazon.com/de/polly/ .\n2 https://vuppetmaster.de/.\n3 The translated version of the german questionnaire can be found in\nsupplementary material.\nFig. 4 Schematics of the used speech recognition system. (1) A spectro-\ngram is generated from the raw audio wave form. (2) The spectrogram\nis used to calculate 20 MFCCs. (3) The MFCCs are fed into a convo-\nlutional neural network. (4) The learned features are then forwarded to\nthe fully connected layers of the network. (5) Finally the output of the\nnetwork is mapped to the corresponding target class\nFig. 5 Setup of the experiment for participants in the virtual embodied\nagent group\nof Gloria were queried, if the participant was part of one\nof the virtual agent groups. The participants rated how they\nperceived Gloria in terms of her helpfulness (i.e., “The infor-\nmation Gloria gave me helped me to understand the decisions\nof the system”), comprehensibility (i.e., “Gloria’s answers\nare understandable”), trustworthiness (i.e., “Gloria is trust-\nworthy”), interaction (i.e., “I would interact with Gloria\nagain), and likability (i.e., “I liked Gloria”). Participants of\nthe text agent group also were asked to asses how often they\nhad read the text information of Gloria on a 7-point Likert\nscale (1 = never, 7 = always). The results of our study will be\npresented in the next section of this paper.\n5 Results\nIn this section, we describe the results of our study starting\nwith a comparison of trust values between the different test-\ngroups. To calculate the required sample size for the test-\ngroup comparison, we performed an a-priori power analysis.\n123\nJournal on Multimodal User Interfaces (2021) 15:87–98 93\nWith a desired power of 0.80, an alpha value of 0.05 and an\neffect size of 0.45 (based on the large effect size resulted in\nWeitz et al. [ 45]), we calculated a required sample size of\n60, which would result in a expected power of 0.82. After\nevaluating the results, the actual effect size of 0.42 showed\nthat an actual power of 0.75 was achieved. In addition to the\ngroup comparison, we report the evaluation of our virtual\nagent Gloria, followed by the ratings and the feedback for\nthe XAI visualisations.\n5.1 Test-group comparison on trust\nTo answer our ﬁrst and second research question, we evalu-\nated the general trust value by examining the data from the\nTiA questionnaire using a contrast analysis, depending on\nthe hypothesis stated in Sect. 1. Contrast analysis is a speciﬁc\nway of analysis for testing directional hypotheses (planned\ncontrasts), that uses linear contrast coefﬁcients to weight\nthe means of the groups that are compared. This method\noffers insights into group differences as it gives the possibil-\nity to deﬁne speciﬁc and more precise comparisons between\ngroups. We speciﬁcally chose this methodology, since it leads\nto a higher power, makes post-hoc testing obsolete and the\neffect-sizes are easier to interpret. The results of our contrast\nanalysis showed a linear trend R\n2 = .16, F (3,56) = 3.45,\np = .02, indicating that as the human-likeness of the agent\nincreases, general trust increased proportionately.\nThe planned contrast revealed that the human-likeness sig-\nniﬁcantly increased in the text agent ( M = 4.89, SD = 0.95),\nvoice agent ( M = 5.12, SD = 0.79), and virtual embodied\nagent group ( M = 5.42, SD = 0.69), compared to the no\nagent group ( M = 4.48, SD = 0.86), b = .68, ( t) = 3.19, p =\n.001, f = 0.42 (medium effect). 4\nThese ﬁndings support our hypothesis about a linear trend\nof the observed user trust regarding the chosen modalities,\nrising from no agent group over text and speech groups up\nunto virtual embodied agent group.\n5.2 Agent evaluation\nSecond, we analysed how the agent Gloria was perceived by\nthe participants in the three groups with agent (text agent,\nvoice agent, and virtual embodied agent). The evaluation\nof the agent Gloria covered the following areas: sympathy,\nrepeated interaction, trustworthiness, comprehensibility of\nher statements and helpfulness in understanding the system’s\ndecision (see Fig. 6). Participants evaluated each area on a\n7-point Likert scale (1 = disagree, 7 = fully agree). For each\nitem Gloria received the lowest average rating by the par-\nticipants of the text agent group. For being comprehensible,\n4 For calculating the effect size, we used the recommendations for con-\ntrast analyses from Perugini et al. [ 24]\nFig. 6 Evaluation of ﬁve different aspects of the virtual agent Gloria.\nThe rating was scaled between 1=disagree to 7=fully agree\ntrustworthy, and likable Gloria received the highest average\nratings from the voice agent group. Participants in the voice\nagent group also most often wanted to interact with Gloria\nagain. The highest rating for Gloria being helpful was given\nby the virtual embodied agent group.\nAs a result of the evaluation of the open questions, two\nareas were found to be assessed positively by the participants:\n– Appearance of the virtual agent: Facial expressions, voice\nand gestures were emphasized as appealing.\n– Interactions with the virtual agent: The participants indi-\ncated that they found verbalization of the visualisation\n(e.g., the reference to relevant phonemes) supportive.\nParticipants within the embodied agent group mentioned that\nthe body gestures of Gloria (e.g., pointing on the spectro-\ngram) were perceived as helpful to draw attention to the XAI\nvisualisation.\n5.3 Evaluation of explanations\nTo answer the third and fourth of our research questions,\nthe participants gave feedback at the end of the study as\nto whether the given XAI visualizations were sufﬁcient and\nwhich aspects or further explanations they would ﬁnd help-\nful. The ANOV A reveals that the difference between the four\ngroups were not signiﬁcant, F (1, 58) = 0.47, p = .495, which\nmeans the ratings of the LIME visualisations do not differ\nbetween the four groups. Figure 7 displays the ratings of the\nparticipants on whether the given XAI visualisations were\nsufﬁcient. Additionally, it can be seen that the average rat-\nings of each group did not reach values above 5. This shows\nthat there is still room for improvement within the XAI meth-\nods used in our study.\nMany participants stated that they would have found\ndetailed information in linguistic form (see some examples\nin Table 2) and comparative information helpful. Here, visual\nas well as linguistic comparisons were mentioned by the par-\n123\n94 Journal on Multimodal User Interfaces (2021) 15:87–98\nFig. 7 Rating of the participants whether the displayed XAI visuali-\nsations were sufﬁcient. The rating was scaled between 1 = disagree to\n7 = fully agree. Error bars represent the standard error\nticipants. Also the analysis of the feedback suggests that\nparticipants would have liked to see more interaction with\nthe virtual agent as well as with the XAI visualisations (e.g.,\nclicking on superpixels or a label to get more detailed infor-\nmation).\n6 Discussion\nThe primary goal of our user study was to examine whether\na user interface featuring a virtual agent has a positive effect\non the perceived trustworthiness of an ANN-based classiﬁca-\ntion model for an end-user. Here, we investigated whether the\nmodalities (pure information in form of text, voice, or visual\npresence), that were chosen for the communication of the\nclassiﬁer’s prediction results and their XAI visualisations,\nhad a signiﬁcant impact on the perceived trustworthiness.\nFurthermore, we examined the overall perceived quality of\nthe generated XAI visualisations. For this purpose, we anal-\nysed additional feedback from the four different participant\ngroups.\nWithin the ﬁrst subsection, we discuss our ﬁndings regard-\ning perceived user trust. In the second subsection, we discuss\nthe free-form feedback of the participants as well as our ﬁnd-\nings regarding the effects of our virtual agents on user ratings\nof the XAI visualisations.\n6.1 Agent-user interface design and perceived trust\nExamining the results of our study, we were able to empiri-\ncally verify our hypotheses that\n– The users’ trust in an ANN-based classiﬁcation model\nbeneﬁted from additional text output given by a virtual\nagent.\n– The users’ trust in an ANN-based classiﬁcation model\nbeneﬁted from speech output provided by the virtual\nagent compared to text output.\n– The users’ trust in an ANN-based classiﬁcation model\nbeneﬁted from the provided visual presence of a vir-\ntual agent performing additional lip-synchronisation and\nbody gestures compared to raw speech output.\nOur results are contrasting the study by V an Mulken et\nal. [ 39], in which no signiﬁcant increase in trustworthiness\nthrough the personiﬁcation of user interfaces could be deter-\nmined. They argued that this might have been caused by an\ninsufﬁcient quality of virtual agents at that time. This sugges-\ntion provides a possible explanation for our deviating result,\nsince the advancements in technology enabled us to employ\na more lifelike and realistic virtual agent in our study. This is\nreﬂected in the ratings of our agent, which are all well above\naverage in the voice agent and virtual embodied agent group\n(see Fig. 6).\nOur study examined the relationship between the “human-\nlikeness” of a virtual agent and how this inﬂuences perceived\nuser trust. The overall impression from our results is that\nthe more human-like XAI interactions appear, the more the\nusers tend to trust the classiﬁcation model whose predictions\nare explained. As virtual embodied agents offer simulated\nhuman-like behaviour, such as lip synchronization and body\nlanguage along with speech output, their potential for trust-\noriented XAI interaction-design seems intuitive, but it was\nnot yet veriﬁed prior to this study. Our study gives ﬁrst indica-\ntions that design choices of a virtual agent inﬂuence humans’\ntrust in an AI system. This information is a crucial step in\nestablishing appropriate trust [ 17] in AI systems in the future.\nKnowing the means by which a user’s trust can be inﬂuenced\nmight help to increase awareness towards such methods.\nHowever, when analysing trust one has to be careful, since\ntrust is a complex concept that can be inﬂuenced by vari-\nous aspects. Hoff and Bashir [ 12] presented a three-layered\nframework, consisting of dispositional trust, situational trust,\nand learned trust. In our study we focused primarily on the sit-\nuational trust which is strongly dependent on the situational\ncontext. This context is further divided into external and\ninternal factors. External factors include task difﬁculty (i.e.,\nspectrograms), the type of system (i.e., text-, voice-, virtual\nembodied agent vs. no agent), and system complexity (i.e.,\nANN). Among others, internal factors include subject matter\n(e.g., background in signal processing) and self-conﬁdence.\nWhile inﬂuences attributable to dispositional and learned\ntrust were not explicitly addressed in our study, these could\nbe used in further work to make more precise statements\nabout perceived trust.\n123\nJournal on Multimodal User Interfaces (2021) 15:87–98 95\nTable 2 Evaluation of the LIME explanations\nKind of information Example feedback of the users\nLinguistic information “Detailed answers for wrong words”\n“A verbal explanation of why some sounds were not understood”\n“Explanations for the individual case, if something is not recognized and\nwhat exactly the problem was.”\n“To tell me which phoneme had a very beneﬁcial effect on the prediction, this\ncould be used more.”\n“How does the system work in the background?”\nComparative information “Comparisons of similar sounding words”\n(visual & linguistic) “In case of incorrect predictions, additional windows with analysis of the\ncorrect label.”\n“More detailed explanation of what should be heard and what was actually\nheard (in the diagram).”\n“In case of wrong classiﬁcation also visualisation of the actual class would be\nhelpful.”\n“It is not clear what the word would look like if it were spoken perfectly.”\nAnswers from participants to the question which further explanations they would have found helpful\n6.2 XAI visualisation feedback\nBesides the impact of virtual agents on the perceived\ntrustworthiness of the ANN-based classiﬁcation model, we\nwanted to investigate (1) how the presented XAI visualisa-\ntions are perceived and rated by participants and (2) how\nvirtual agents affect this perception of XAI visualisations.\nWe found that\n– Participants wanted more information in linguistic form.\n– Participants asked for comparative information in visual\nas well as linguistic form.\n– Participants would have preferred further interaction with\nthe system (e.g., to ask questions).\nAs the ratings of the visual explanations were not particularly\nhigh (average around 4 with a maximum rating of 7), there\nis still a high potential for improvement regarding the visual\nexplanations we used in our experiment. A cause for this may\nbe the complexity of the visual explanations, as they require\nsome basic understanding of spectrograms and how to read\nthem.\nFrom the results of our study, a tendency can be observed\nregarding the participants’ rating of the quality of the XAI\nvisualisations (see Fig. 7), where the no agent and text agent\ngroup rated the XAI visualisations as less sufﬁcient than par-\nticipants in the voice agent and virtual embodied agent group.\nThis result reﬂects the ﬁndings on user’s trust towards the sys-\ntem discussed in the previous subsection. A possible cause\nfor this might be a cognitive bias such as the halo effect [ 37].\nThe halo effect states that a positive impression of a person\nabout an object in one area positively inﬂuences their opin-\nion in other areas. In our study, the perceived trustworthiness\nof the ANN-based classiﬁcation model could have positively\ninﬂuenced the ratings of the participants towards the XAI\nvisualisations. The aforementioned observation provides ﬁrst\nindications that cognitive biases may occur during interac-\ntion with XAI systems. Whether and to what extent cognitive\nbiases inﬂuence the perception of XAI should therefore be\nthe focus of further studies.\nA result from our free-form feedback showed that partic-\nipants wished for more linguistic explanations. This aligns\nwith the social characteristic of explanations found by Miller\n[20], since it underlines the participants need for selective\ninformation and causality within the explanation. Our sim-\nple implementation of linguistic explanations in the text,\nvoice, and virtual embodied agent groups, which highlights\nthe most relevant phoneme to the user, already illustrates the\nusefulness of this concept. This corresponds to the ﬁndings\nof Siebers and Schmid [ 33], who suggested that adding tex-\ntual explanations can redirect the focus of the user towards\nimportant areas, and of Schmid [ 30], who pointed out that\nadditional textual explanations enable the inclusion of causal\nrelations among other information. Park et al. [ 23] introduced\na concept to generate such explanations for a visual ques-\ntion answering system by using recurrent neural networks\nto generate textual explanations based on an input image, a\nquestion, and visual explanations of the predicted answer.\nIn the same way one could use the visual explanations we\nimplemented in this paper to generate additional linguistic\nexplanations for the agent which correspond to the speciﬁc\ninput.\nIn addition to linguistic explanations, the supplementary\nuse of advanced body gestures could help the agent to point\n123\n96 Journal on Multimodal User Interfaces (2021) 15:87–98\nat certain regions of the visualisation more precisely and thus\nsimulate a more natural behaviour. To achieve this, one could\nbuild up on the already existing body of work that addresses\nthe topic of automatic gesture generation [ 5,9,25].\nAnother aspect that emerged from the evaluation of the\nfree form questionnaire was that the participants wanted\ninformation that was prepared in such a way that particularly\nintuitive comparisons could be made. A possible cause for\nthis might be the speciﬁc way of integrating XAI visualisa-\ntions in our system, which does not show the visualisation of\nthe correct keyword in the case of misclassiﬁcation. Instead,\nwe displayed only three visualisations corresponding to the\ntop predictions of our classiﬁer. In some cases those visualisa-\ntions did not contain the word that was actually spoken by the\nparticipant. Here, participants missed additional information\nwhich would have enabled them to interpret the explana-\ntion in the correct context. This insight supports the thesis\nof Miller [ 20], according to which people prefer to ask why\none prediction was made instead of another. To enable such a\ncomparison, an explanation design could beneﬁt from addi-\ntionally displaying example explanations of inputs that have\nbeen classiﬁed correctly.\nThe participants feedback suggest that they would have\npreferred to interact more with the system, for example, to\nask questions when they do not understand something. This\ninsight corresponds to Miller’s ﬁndings stating that explana-\ntions have social characteristics since they represent a transfer\nof knowledge in the context of conversations [ 20]. Conver-\nsations are one of the most important ways for people to\nexchange and share knowledge [ 8] and therefore are one of\nthe main characteristics of human-to-human explanations.\nThis characteristic has also been investigated in human-\ncomputer interaction by Susan Robinson and Henderer [ 36].\nThey found that users most often reacted to utterances of a\nconversational agent with queries. It would be interesting to\nexperiment with the application of more mature conversa-\ntional agent architectures in this area, since those should be\nable to to respond adequately to questions and also to deal\nwith queries from the user. Modern neural network based\narchitectures like the ones proposed by Wu et al. [ 46] and\nVinyals and Le [40] are already enabling natural user adaptive\nconversations with a virtual agent. Combining such conver-\nsational capabilities with the textual explanation approaches,\nlike the one by Park et al. [ 23] we mentioned before, could\nlead to a more natural interaction and improved transfer of\nknowledge.\n7 Conclusion\nWithin this paper we explored the potential of virtual agents\nto explain the decisions of an ANN-based classiﬁcation\nmodel to end-users. To this end, we conducted a user-study\nin which we presented XAI visualisations of the decisions\nfrom a speech recognition system to the user. While one test\ngroup only received the XAI visualisations, three test groups\nwere presented additionally different modalities of a virtual\nagent (text, voice, or virtual presence). The results of our\nstudy show a linear trend of the user’s perceived trust in the\nused ANN-based classiﬁcation model regarding the chosen\nmodalities, rising from no-agent group over text and speech\ngroups up unto virtual embodied agent group. By analyzing\nthe participants’ free-form feedback, we additionally found\nthat:\n– End-users want additional linguistic explanations.\n– End-users want explanations to be suitable for intuitive\ncomparisons.\n– End-users want to interact with the agent, e.g., by asking\nquestions.\nThe results of our study are inline with our initial assump-\ntion that the end-users’ experience could beneﬁt from a more\nhuman-like XAI interaction design. Based on our ﬁndings,\nwe argue that there lies vast potential in the use of virtual\nagents to achieve this design goal.\nAcknowledgements Open Access funding provided by Projekt DEAL.\nThis work has received funding from the DFG under project number\n392401413 (DEEP) and from the BMBF within the project “VIV A ”,\nGrant Number 16SV7960.\nCompliance with ethical standards\nConﬂicts of interest The authors declare that they have no conﬂict of\ninterest.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Alqaraawi A, Schuessler M, Weiß P , Costanza E, Berthouze N\n(2020) Evaluating saliency map explanations for convolutional\nneural networks: a user study. arXiv:2002.00772\n2. Bach S, Binder A, Montavon G, Klauschen F, Müller KR,\nSamek W (2015) On pixel-wise explanations for non-linear clas-\nsiﬁer decisions by layer-wise relevance propagation. PloS One\n10(7):e0130140. https://doi.org/10.1371/journal.pone.0130140\n123\nJournal on Multimodal User Interfaces (2021) 15:87–98 97\n3. Broekens J, Harbers M, Hindriks K, V an Den Bosch K, Jonker C,\nMeyer JJ (2010) Do you get it? user-evaluated explainable BDI\nagents. In: German conference on multiagent system technologies.\nSpringer, pp 28–39\n4. Chen JYC, Procci K, Boyce M, Wright J, Garcia A, Barnes MJ\n(2014) Situation awareness-based agent transparency. US Army\nResearch Laboratory\n5. Chiu CC, Marsella S (2011) How to train your avatar: a data\ndriven approach to gesture generation. In: International workshop\non intelligent virtual agents. Springer, pp 127–140, https://doi.org/\n10.1007/978-3-642-23974-8_14\n6. De Graaf MMA, Malle BF (2017) How people explain action (and\nautonomous intelligent systems should too). In: AAAI 2017 fall\nsymposium on AI-HRI, pp 19–26\n7. Felzenszwalb PF, Huttenlocher DP (2004) Efﬁcient graph-based\nimage segmentation. Int J Comput Vis 59(2):167–181. https://doi.\norg/10.1023/B:VISI.0000022288.19776.77\n8. Garrod S, Pickering MJ (2004) Why is conversation so easy?\nTrends Cogn Sci 8(1):8–11. https://doi.org/10.1016/j.tics.2003.10.\n016\n9. Gatt A, Paggio P (2014) Learning when to point: a data-driven\napproach. In: Proceedings of COLING 2014, the 25th interna-\ntional conference on computational linguistics: Technical Papers,\npp 2007–2017\n10. Gilpin LH, Bau D, Y uan BZ, Bajwa A, Specter M, Kagal L (2018)\nExplaining explanations: an approach to evaluating interpretability\nof machine learning. arXiv:1806.00069\n11. Gunning D (2017) Explainable artiﬁcial intelligence (XAI).\nDefense Advanced Research Projects Agency (DARPA)\n12. Hoff KA, Bashir M (2015) Trust in automation: integrating\nempirical evidence on factors that inﬂuence trust. Hum Factors\n57(3):407–434. https://doi.org/10.1177/0018720814547570\n13. Hoffman JD, Patterson MJ, Lee JD, Crittendon ZB, Stoner HA,\nSeppelt BD, Linegang MP (2006) Human-automation collabo-\nration in dynamic mission planning: a challenge requiring an\necological approach. In: Proceedings of the human factors and\nergonomics society annual meeting, vol 50(23), pp 2482–2486.\nhttps://doi.org/10.1177/154193120605002304\n14. Jian JY , Bisantz AM, Drury CG (2000) Foundations for an empir-\nically determined scale of trust in automated systems. Int J Cognit\nErgon. https://doi.org/10.1207/S15327566IJCE0401_04\n15. Kisler T, Reichel U, Schiel F (2017) Multilingual processing of\nspeech via web services. Comput Speech Lang 45:326–347. https://\ndoi.org/10.1016/j.csl.2017.01.005\n16. Lane HC, Core MG, V an Lent M, Solomon S, Gomboc D (2005)\nExplainable artiﬁcial intelligence for training and tutoring. Uni-\nversity of Southern California/Institute for Creative Technologies,\nTech. rep\n17. Lee JD, See KA (2004) Trust in automation: designing for appro-\npriate reliance. Hum Factors 46(1):50–80\n18. Lipton ZC (2018) The mythos of model interpretability. Commun\nACM 61(10):36–43. https://doi.org/10.1145/3233231\n19. Mercado JE, Rupp MA, Chen JY , Barnes MJ, Barber D, Procci K\n(2016) Intelligent agent transparency in human-agent teaming for\nmulti-UxV management. Hum Factors 58(3):401–415. https://doi.\norg/10.1177/0018720815621206\n20. Miller T (2018) Explanation in artiﬁcial intelligence: insights from\nthe social sciences. Artif Intell 267:1–38. https://doi.org/10.1016/\nj.artint.2018.07.007\n21. Miller T, Howe P , Sonenberg L (2017) Explainable AI: beware of\ninmates running the asylum. In: IJCAI International joint confer-\nence on artiﬁcial intelligence, arXiv:1712.00547\n22. Montavon G, Samek W, Müller KR (2017) Methods for interpret-\ning and understanding deep neural networks. Digit Signal Process\n73:1–15. https://doi.org/10.1016/j.dsp.2017.10.011\n23. Park DH, Hendricks LA, Akata Z, Rohrbach A, Schiele B, Darrell\nT, Rohrbach M (2018) Multimodal explanations: Justifying deci-\nsions and pointing to the evidence. In: 2018 IEEE conference on\ncomputer vision and pattern recognition, CVPR 2018, Salt Lake\nCity, UT, USA, June 18-22, 2018, pp 8779–8788, https://doi.org/\n10.1109/CVPR.2018.00915\n24. Perugini M, Gallucci M, Costantini G (2018) A practical primer\nto power analysis for simple experimental designs. Int Rev Soc\nPsychol 31(1):20. https://doi.org/10.5334/irsp.181\n25. Ravenet B, Clavel C, Pelachaud C (2018) Automatic nonverbal\nbehavior generation from image schemas. In: Proceedings of the\n17th international conference on autonomous agents and multia-\ngent systems, pp 1667–1674\n26. Ribeiro MT, Singh S, Guestrin C (2016) Why should i trust you?\nExplaining the predictions of any classiﬁer. In: Proceedings of the\n22Nd ACM SIGKDD international conference on knowledge dis-\ncovery and data mining. ACM, pp 1135–1144, https://doi.org/10.\n1145/2939672.2939778\n27. Richardson A, Rosenfeld A (2018) A survey of interpretability and\nexplainability in human-agent systems. In: Proceedings of the 2nd\nworkshop of explainable artiﬁcial intelligence, pp 137–143\n28. Sainath TN, Parada C (2015) Convolutional neural networks for\nsmall-footprint keyword spotting. Proc Interspeech 2015:1478–\n1482\n29. Samek W, Wiegand T, Müller KR (2017) Explainable artiﬁ-\ncial intelligence: understanding, visualizing and interpreting deep\nlearning models. arXiv preprint arXiv:1708.08296 pp 1–8\n30. Schmid U (2018) Inductive programming as approach to compre-\nhensible machine learning. In: Proceedings of the 7th workshop on\ndynamics of knowledge and belief (DKB-2018) and the 6th work-\nshop KI & Kognition (KIK-2018), co-located with 41st German\nconference on artiﬁcial intelligence, vol 2194\n31. Selvaraju RR, Das A, V edantam R, Cogswell M, Parikh D, Batra\nD (2017) Grad-cam: visual explanations from deep networks via\ngradient-based localization. In: The IEEE international conference\non computer vision (ICCV) 2017, pp 618–626\n32. Selvaraju RR, Cogswell M, Das A, V edantam R, Parikh D, Batra\nD (2020) Grad-cam: visual explanations from deep networks via\ngradient-based localization. Int J Comput Vis 128(2):336–359\n33. Siebers M, Schmid U (2018) Please delete that! why should I?\nExplaining learned irrelevance classiﬁcations of digital objects.\nKI - Künstliche Intelligenz. https://doi.org/10.1007/s13218-018-\n0565-5\n34. Simonyan K, V edaldi A, Zisserman A (2013) Deep inside con-\nvolutional networks: visualising image classiﬁcation models and\nsaliency maps. http://arxiv.org/abs/1312.6034, arXiv:1312.6034\n35. Stubbs K, Hinds PJ, Wettergreen D (2007) Autonomy and common\nground in human-robot interaction: a ﬁeld study. IEEE Intell Syst\n22(2):42–50. https://doi.org/10.1109/MIS.2007.21\n36. Susan Robinson MI David Traum, Henderer J (2008) What would\nyou ask a conversational agent? observations of human-agent dia-\nlogues in a museum setting. In: Proceedings of the 6th international\nconference on language resources and evaluation (LREC’08),\nEuropean Language Resources Association\n37. Thorndike EL (1920) A constant error in psychological ratings. J\nAppl Psychol 4(1):25–29\n38. V an Mulken S, André E, Müller J (1998) The persona effect: how\nsubstantial is it? In: People and computers XIII. Springer, pp 53–66,\nhttps://doi.org/10.1007/978-1-4471-36057_4\n39. V an Mulken S, André E, Müller J (1999) An empirical study on the\ntrustworthiness of life-like interface agents. In: Human–Computer\ninteraction: communication, cooperation, and application design,\nproceedings of 8th international conference on human–computer\ninteraction, 1999, pp 152–156\n40. Vinyals O, Le QV (2015) A neural conversational model. arXiv\npreprint arXiv:1506.05869\n123\n98 Journal on Multimodal User Interfaces (2021) 15:87–98\n41. Wagner J, Schiller D, Seiderer A, André E (2018) Deep learning\nin paralinguistic recognition tasks: are hand-crafted features still\nrelevant? Proc Interspeech 2018:147–151\n42. Wang J, Chen Y , Hao S, Peng X, Hu L (2018) Deep learning for\nsensor-based activity recognition: a survey. Pattern Recognit Lett\n119:3–11. https://doi.org/10.1016/j.patrec.2018.02.010\n43. Warden P (2018) Speech commands: a dataset for limited-\nvocabulary speech recognition. arXiv:1804.03209v1\n44. Weitz K, Hassan T, Schmid U, Garbas JU (2019a) Deep-\nlearned faces of pain and emotions: Elucidating the differences\nof facial expressions with the help of explainable ai methods. tm-\nTechnisches Messen 86(7-8):404–412, https://doi.org/10.1515/\nteme-2019-0024\n45. Weitz K, Schiller D, Schlagowski R, Huber T, André E (2019b)\n“Do you trust me?”: Increasing user-trust by integrating virtual\nagents in explainable ai interaction design. In: Proceedings of the\n19th ACM international conference on intelligent virtual agents.\nACM, New Y ork, NY , USA, IV A ’19, pp 7–9, https://doi.org/10.\n1145/3308532.3329441\n46. Wu J, Ghosh S, Chollet M, Ly S, Mozgai S, Scherer S (2018)\nNadia: Neural network driven virtual human conversation agents.\nIn: Proceedings of the 18th international conference on intelli-\ngent virtual agents. ACM, pp 173–178, https://doi.org/10.1145/\n3267851.3267860\n47. Zhang Z, Geiger J, Pohjalainen J, Mousa AED, Jin W, Schuller B\n(2018) Deep learning for environmentally robust speech recogni-\ntion: an overview of recent developments. ACM Trans Intell Syst\nTechnol (TIST) 9(5):49:1–49:28, https://doi.org/10.1145/3178115\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}