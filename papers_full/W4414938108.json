{
  "title": "Age and gender distortion in online media and large language models",
  "url": "https://openalex.org/W4414938108",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4222028498",
      "name": "Guilbeault, Douglas",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": null,
      "name": "Delecourt, Solène",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A4222028494",
      "name": "Desikan, Bhargav Srinivasa",
      "affiliations": [
        "Land Sea Air Autonomy (United States)",
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2168920989",
    "https://openalex.org/W2195118980",
    "https://openalex.org/W3199234880",
    "https://openalex.org/W4220762274",
    "https://openalex.org/W4391823781",
    "https://openalex.org/W2486389606",
    "https://openalex.org/W936083337",
    "https://openalex.org/W2108193005",
    "https://openalex.org/W2026208346",
    "https://openalex.org/W1970639446",
    "https://openalex.org/W2968614544",
    "https://openalex.org/W2924325295",
    "https://openalex.org/W2504931247",
    "https://openalex.org/W4401688049",
    "https://openalex.org/W4306856047",
    "https://openalex.org/W2887598086",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W3009728556",
    "https://openalex.org/W2038960993",
    "https://openalex.org/W3021515550",
    "https://openalex.org/W2761999826",
    "https://openalex.org/W2121561731",
    "https://openalex.org/W2048945236",
    "https://openalex.org/W2158382220",
    "https://openalex.org/W284511401",
    "https://openalex.org/W2783328238",
    "https://openalex.org/W2114687821",
    "https://openalex.org/W3120536948",
    "https://openalex.org/W4379769139",
    "https://openalex.org/W4220816919",
    "https://openalex.org/W2510725918",
    "https://openalex.org/W2115651492",
    "https://openalex.org/W2783489916",
    "https://openalex.org/W2592232824",
    "https://openalex.org/W1965804146",
    "https://openalex.org/W2172241916",
    "https://openalex.org/W2019464758",
    "https://openalex.org/W4312722235",
    "https://openalex.org/W67208201",
    "https://openalex.org/W2118690925",
    "https://openalex.org/W4403680681",
    "https://openalex.org/W2550925836",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W2791423782",
    "https://openalex.org/W4391463878",
    "https://openalex.org/W2086806158",
    "https://openalex.org/W4367836140",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W2149252982",
    "https://openalex.org/W3159059608",
    "https://openalex.org/W4285090338",
    "https://openalex.org/W2794635328",
    "https://openalex.org/W3125829154",
    "https://openalex.org/W3103365499"
  ],
  "abstract": null,
  "full_text": "Nature | Vol 646 | 30 October 2025 | 1129\nArticle\nAge and gender distortion in online media \nand large language models\nDouglas Guilbeault1 ✉, Solène Delecourt2 & Bhargav Srinivasa Desikan3,4\nAre widespread stereotypes accurate1–3 or socially distorted4–6? This continuing \ndebate is limited by the lack of large-scale multimodal data on stereotypical \nassociations and the inability to compare these to ground truth indicators. Here we \novercame these challenges in the analysis of age-related gender bias7–9, for which age \nprovides an objective anchor for evaluating stereotype accuracy. Despite there being \nno systematic age differences between women and men in the workforce according to \nthe US Census, we found that women are represented as younger than men across \noccupations and social roles in nearly 1.4 million images and videos from Google, \nWikipedia, IMDb, Flickr and YouTube, as well as in nine language models trained on \nbillions of words from the internet. This age gap is the starkest for content depicting \noccupations with higher status and earnings. We demonstrate how mainstream \nalgorithms amplify this bias. A nationally representative pre-registered experiment \n(n = 459) found that Googling images of occupations amplifies age-related gender \nbias in participants’ beliefs and hiring preferences. Furthermore, when generating \nand evaluating resumes, ChatGPT assumes that women are younger and less \nexperienced, rating older male applicants as of higher quality. Our study shows how \ngender and age are jointly distorted throughout the internet and its mediating \nalgorithms, thereby revealing critical challenges and opportunities in the fight against \ninequality.\nAlthough few deny that stereotypes, or generalizations about social \ngroups10–12, are harmful, a fundamental question remains contested: \nare common stereotypes accurate1–3 or socially distorted4–6? Some \nargue that commonplace stereotypes accurately capture observable \naspects of social groups; otherwise, they would not gain such wide-\nspread adoption1–3. Yet, others argue that stereotypes are often exag-\ngerated or illusory4–6. Assessing stereotype accuracy is challenging \nbecause stereotypes involve not only statistical associations (such as \nexpected correlations among the features of a social group) but also \nnormative judgements (such as that one group is superior to another) \nfor which there is no well-defined ground truth 10–12. Even for statis-\ntical associations, identifying the ground truth is difficult. In some \ncases, this stems from disagreement on how to measure the ground \ntruth, such as enduring debates over how to measure intelligence13  \n(a heavily stereotyped characteristic14). Yet, even when there is agree-\nment on the relevant constructs, there is often a lack of large-scale, \nquantifiable cultural data for measuring stereotypical associations \nand comparing these to ground truth indicators. As a result, research \non stereotypes often yields inconsistent findings, calling into ques-\ntion the pervasiveness and impact of these biases. In this study, we \novercame these limitations in the analysis of age-related gender bias, \nwhich not only involves biological age as an objective anchor for evalu-\nating stereotype accuracy, but which can also be linked to large-scale \nstatistical biases in how the ages of women and men are depicted  \nonline.\nOn the one hand, evidence abounds that older women face a dual \nbias at the intersection of gender and age. Policy reports8,15,16, media \ncoverage17 and workplace interviews7,18 indicate that older women are \ndiscriminated against in hiring and promotion across industries (known \nas ‘gendered ageism’7,18,19). This is related to a general statistical bias \ntowards associating women with expectations of youth. From enter-\ntainment media to the workplace, women face persistent pressure to \nappear young, which results in a ‘beauty tax’ with sizeable financial and \ntime costs20. This bias also manifests in everyday language. Women in \nacademia21 and industry22 are more likely than men to be referred to \nusing infantilizing pronouns (such as ‘girls’). These patterns suggest \nthat age-related gender expectations may form a culture-wide statis-\ntical bias that influences people’s perceptions of others throughout \nsociety23,24.\nOn the other hand, the statistical association between women and \nyouth contradicts observable socioeconomic realities. Since the 1960s, \nwomen have consistently outlived men in the USA by as much as 8 years, \na gap that has been increasing25,26. Census data on occupations present \nsimilarly puzzling trends (Supplementary Fig. 1). Over the past decade, \nthere has been no correlation between the fraction of women in an occu-\npation and its median age, according to the US census (Extended Data \nFig. 1 and Supplementary Table 1). There were also no clear differences \nin the age distribution of women and men throughout the workforce \nfrom 2009 to the present (Supplementary Fig. 2 and Supplementary \nTable 2). Moreover, recent surveys failed to observe gendered ageism \nhttps://doi.org/10.1038/s41586-025-09581-z\nReceived: 14 October 2024\nAccepted: 29 August 2025\nPublished online: 8 October 2025\nOpen access\n Check for updates\n1Graduate School of Business, Stanford University, Stanford, CA, USA. 2Haas School of Business, University of California, Berkeley, Berkeley, CA, USA. 3Autonomy Institute, London, UK. 4Oxford \nInternet Institute, University of Oxford, Oxford, UK. ✉e-mail: dguilb@stanford.edu\n1130 | Nature | Vol 646 | 30 October 2025\nArticle\nin certain organizational settings and even suggest that older women \nmay be less affected by stereotypes than older men27,28. These inconsist-\nent findings resonate with critiques against claims of enduring gender \ninequality, such as research showing declines in gender stereotypes \nover the last century in online text29,30, as well as studies showing that \nhiring across industries increasingly favours women31,32. This dissonant \nlandscape raises the question of whether age-related gender bias is an \norganization-specific or industry-specific problem, or whether it is \na culture-wide distortion that continues to reflect and contribute to \nsystemic inequalities.\nWe argue that this uncertainty is fuelled by the lack of (1) culture-wide \nmultimodal data on the associations between gender and age and  \n(2) computational methodologies for comparing these associations \nwith ground truth indicators. So far, only a handful of studies have \nexamined age–gender associations in small-scale surveys and inter-\nviews with professional women7,18,27,28,33 or in sparse, non-representative \nobservational studies of particular industries, such as celebrities and \nathletes in entertainment media 34–38. However, failing to observe \nage-related gender bias in limited samples of a few contexts does not \nindicate a lack of prominence on a culture-wide scale. Social biases in \nhow people categorize the world frequently emerge only at scale39,40 \nand can manifest as exaggerated or even illusory beliefs41. This suggests \nthe alternative view that skewed associations between gender and age \ncan emerge as a large-scale statistical bias that distorts socioeconomic \nrealities despite inconsistencies across small-scale contexts.\nAlthough a number of recent studies revealed the exaggeration of \nmale representation in online texts and images6,42, no comparable analy-\nses exist for tracking age-related representations of gender. T o address \nthis gap, we produced a large-scale culture-wide dataset on age–gender \nassociations across modalities, including images, videos and textual \ndata, collected from popular sources of digital media. We began by \nexamining the gender and age associations of all social categories in \nthe English language (n = 3,495) in more than 1.3 million images and \nthousands of videos from Google, Wikipedia, IMDb, Flickr, YouTube \nand a random sample of the world-wide web (see ‘Image and video data-\nsets’ in Methods for details on pre-processing and post-processing). \nWe benchmarked online images of occupations against the US census \ndata to examine whether they exaggerate the association between \nwomen and youth. We went beyond the visual modalities by examining \nage-related gender bias in nine popular language models trained on \nbillions of words from across the internet, including Reddit, Google \nNews, Wikipedia and Twitter (see ‘Measuring age and gender in online \ntext’ in Methods). By examining age-related gender bias in large-scale \ninternet data, our study was uniquely poised to examine the role that \nmainstream algorithms play in reinforcing this bias. We examined \nalgorithmic amplification in both the image and textual modalities \nthrough (1) the study of the psychological effects of using Google Image \nsearch and (2) the use of ChatGPT to generate and evaluate resumes \nin the workplace.\nAge–gender distortions in visual content\nAcross all image datasets spanning five popular online platforms, \nwomen are consistently represented as younger than men, regard-\nless of whether the age and gender of faces are measured using human \njudgements, machine learning or ground truth data. First, we ana -\nlysed 657,035 images from the Google search engine associated with \n3,495 social categories, in which the gender and age of images were \nclassified by human coders6 (see ‘Image data collection procedure’ in \nMethods; all categories were examined using retrieved Google Images \ncontaining human faces). We found that women in Google Images \nwere coded as significantly younger than men, both for non-gendered \nsearches (such as ‘doctor’ or ‘banker’; mean difference in age groups, \nMdiff = 0.37; t = −73.84; P = 2.2 × 10−16; n = 3,434 categories; Fig. 1a) and \ngendered searches (such as searching ‘female doctor’ and ‘male doctor’; \nMdiff = 0.29; t = −36.52; P = 2.2 × 10−16; n = 2,960 categories; Fig. 1b). Rep-\nlicating this method over Wikipedia revealed that women in Wikipedia \nimages were also coded as significantly younger than men (Mdiff = 0.71; \nt = −39.62; P = 2.2 × 10−16; n = 1,251 categories; Fig. 1c).\nThese results are robust to collecting Google Images from different \ncountries around the world (Supplementary Fig. 3) and controlling for \n(1) the demographic characteristics and subjectivity of human coders \n(Supplementary Figs. 4 and 5 and Supplementary Tables 3–5); (2) the \nlinguistic features of social categories (Supplementary Figs. 6 and 7 \nand Supplementary Table 6), such as word polysemy, word gender con-\nnotation, word age connotation and word frequency in Google Search \nand in everyday language; (3) the visual features of the images, includ-\ning the number of faces per image, the number of images associated \nwith each category overall, whether the image repeats across searches \nand whether the image is photographic or displays a digital avatar \n(Supplementary Table 7); and (4) whether the faces in each image are \ncropped before classification (Supplementary Fig. 8), as well as statisti-\ncal biases in the cropping algorithm itself (Supplementary Fig. 9). We \nconfirm that these results reflect images from a wide range of websites \n(Supplementary Fig. 10).\nNext, we analysed the 2018 IMDb–Wiki dataset 43 and the 2014 \nCross-Age Celebrity Dataset (CACD)44 consisting of Google Images, \neach of which provides the true gender and age of the celebrities \ndepicted using their public bio pages and time-stamped photographs. \nFigure 1 shows that female celebrities are, on average, 6.5 years younger \nthan men on IMDb (t = −169.9; P = 2.2 × 10−16; n = 451,562 images; Fig. 1d), \n3.27 years younger on Wikipedia (t = 10.64; P = 2.2 × 10−16; n = 57,972 \nimages; Fig. 1e) and 5.35 years younger in Google Images (t = −90.92; \nP = 2.2 × 10−16; n = 149,889 images; Fig. 1f). In all cases, the most common \n(modal) age for women is in their 20s, whereas in images from IMDb \nand Google, the most common ages for men are 40 years and 50 years, \nrespectively. These analyses show that age-related gender bias online \nis not an artefact of human perceptions of gender and age, because it is \nreplicated using verified objective information about the age and gen-\nder of those depicted. That age-based gender bias replicates strongly \nin the context of celebrities is concerning, given the salient role that \ncelebrities play in reinforcing stereotypes45.\nFinally, we explored age biases in the representation of women and \nmen using prominent image datasets developed for training machine \nlearning algorithms. The age and gender classifications in these data-\nsets were provided by computer vision algorithms constructed by the \nresearch teams that released these datasets. We found that women were \nautomatically classified as significantly younger than men in the 2017 \nUTK46 dataset consisting of a diverse sample of images from across the \nworld-wide web (Mdiff = 5.12 years; t = −19.9; P = 2.2 × 10−16; n = 24,106 \nimages; Fig. 1g), the 2014 Adience dataset47 consisting of images from \nFlickr (Mdiff = 0.18; t = −6.52; P = 6.79 × 10−11; n = 17,492 images; Fig. 1h) \nand the 2008 Labeled Faces in the Wild (LFW) dataset48 consisting of \na random sample of images from Google News (Mdiff = 0.84; t = −44.89; \nP = 2.2 × 10−16; n = 13,143 images; Fig. 1i) (two-tailed Student’s t-test). \nThese findings further generalized our results beyond the perceptions \nof human coders.\nA remaining question is whether age-related gender bias is also \nobserved in online videos, which increasingly dominate the world-wide \nweb49. Although an exhaustive analysis of online videos is beyond the \nscope of this study, we analysed two open-source datasets of screen-\nshots from YouTube videos that provide compelling support for our \ntheory. First, we examined the correlation between gender and age in \nthe 2011 YouTube Faces dataset50, consisting of 3,645 faces of celebrities \nextracted from 3,425 YouTube videos. Women in the YouTube Faces \ndataset appear significantly younger than men according to machine \nlearning classifications (Mdiff = 0.87; t = −25.68; P = 2.2 × 10−16; n = 3,645 \nimages; Fig. 1j). We also analysed a more recent dataset of YouTube \nvideos called the 2022 CelebV-HQ dataset51, consisting of 35,666 images \ncollected by identifying public lists of celebrities on Wikipedia and \nNature | Vol 646 | 30 October 2025 | 1131\n0\n10\n20\n30\n0–2\n4–6\n8–12\n15–20\n25–32\n38–48\n48–53\n60–100\nFlickr\n(2014)\n0\n2\n4\n6\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nUTK\n(2017)\n0\n1\n2\n3\n10\n20\n30\n40\n50\n60\nGoogle\n(ref. 44)\n0\n10\n20\n30\n40\n0–11\n12–17\n18–24\n25–34\n35–54\n55–74\n+75\n0–11\n12–17\n18–24\n25–34\n35–54\n55–74\n+75\nAge of face (years)\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nFemale\nMale\nGoogle\n(ref. 6)\n0\n25\n50\n75\n12–17\n18–24\n25–34\n35–54\n55–74\nAge of face (years)\nAge of face (years)\nAge of face (years)\nAge of face (years)\nAge of face (years)\nAge of face (years)\nGoogle\n(ref. 6; gendered)\n0\n10\n20\n30\n40\nAge of face (years)\nWikipedia\n(ref. 68)\nFraction of faces (%)Fraction of faces (%)\nFraction of faces (%)\nFraction of faces (%)\nFraction of faces (%)Fraction of faces (%)\nFraction of faces (%)\nFraction of faces (%)\nFraction of faces (%)\nFraction of faces (%)\na b c\nf g h\nWikipedia\n(ref. 43)\nd e\n0\n10\n20\n30\nBaby\nBaby\nChild\nChild\nTeen\nTeen\nAdult\nAdult\nSenior\nSenior\nLabeled faces in the wild\n(ref. 48)\ni\n0\n20\n40\n60\nYouTube\n(2011) j\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nAge of face (years)\nAge of face (years)\nIMDb\n(ref. 43)\n100\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFig. 1 | Women are represented as significantly younger than men in more \nthan 1.3 million images and thousands of videos across 7 online sources.  \na–j, The age of either female or male faces according to the top 100 Google \nImages associated with 3,434 social categories ( n = 161,484 images) (a), the top \n100 Google Images retrieved using gendered searches (such as by searching \n‘female athlete’ or ‘male athlete’) shown for all non-gendered categories in \nWordNet (n = 2,960 categories; 495,551 images) (b), 1,251 categories in Wikipedia \n(from the Srinivasan et al. 68 dataset; 14,709 images) (c), celebrities identified  \nby the top 100,000 most popular pages on IMDb ( n = 451,570 images)  \n(d), biographical Wikipedia pages describing the same celebrities in the IMDb–\nWiki dataset ( n = 57,932 images) (e), the top 50 most popular celebrities from \n1951 to 2004 as they appear in Google Images, according to the CACD (n = 149,889 \nimages) (f), a random sample across the world-wide web (the 2017 UTK dataset; \nn = 20,000 images) (g), a random sample from Flickr (the 2014 Adience dataset; \nn = 26,580 images) ( h), a random sample of images from online news websites \n(the 2008 LFW dataset; n  = 13,233 images) (i ) and images of the same people \nidentified in the LFW dataset, extracted across 3,425 YouTube videos in 2011 \n(n = 5,000 images) ( j). The method for coding age and gender varies by dataset; \na–c rely on human coders; panels d ,e,g rely on ground truth measures of the \nage and gender of celebrities posted publicly online; f ,h–j rely on automated \ndeep learning classifications of gender and age. Solid gold and blue lines \nindicate the average age for female and male faces, respectively.\n1132 | Nature | Vol 646 | 30 October 2025\nArticle\nautomatically collecting the top 10 YouTube videos associated with \neach celebrity. Although this dataset contains only a binary measure \nof age (faces are coded as either young = 1 or old = 0), we can still test \nour theory by comparing the fraction of women and men coded as \nyoung. Only 20% of men were classified as young compared with 33% of \nwomen, marking a significantly higher rate of youthful presentations \nfor women (P = 2.2 × 10−16; two-tailed proportion test).\nComparing with the census\nWe compared these findings to available industry-level ground truth \ndata to measure the extent to which online images distort the underly-\ning sociodemographic realities of age (occupation-level census data \ncontaining both gender and age information are unavailable; see ‘Com-\nparing online images with the census’ in Methods). We matched 867 \nsocial categories from our Google Images (Fig. 1a) dataset to occupa-\ntional categories in the US census. Although gender–age associations \nin Google Images and census data are correlated at the industry level \n(r = 0.13; confidence interval = 0.11–0.15; P = 2.2 × 10−16; two-tailed Pear-\nson’s correlation; Extended Data Fig. 2 and Supplementary Tables 8 \nand 9), Google Images consistently display exaggerated and, in some \ncases, inverted trends that consistently amplify the association between \nwomen and youth. Extended Data Fig. 3 presents the absolute age gap \nbetween women and men in each industry, vertically ranked in terms \nof the magnitude of this gap while also placing the older gender on the \nright side. In the sales, resources and management industries, Google \nImages consistently presented the highest age gap relative to all census \nyears (P < 0.001 for all pairwise comparisons; two-tailed Student’s \nt-test). Moreover, in each of these industries, Google Images displayed \nmen as older than women, whereas women were older than men for \neach of the census years examined in the sales industry and for two \nof the years in the resources industry. In the production and service \nindustry, the magnitude of the age gap captured by Google Images was \nnot higher than all census years; yet, the bias towards representing men \nas older was stable. In each census year, women were older than men \nin the production and service industries. It was only in Google Images \nthat men were older than women in these industries, suggesting sys-\ntematic age and gender distortions that associate women with youth.\nRelationship to social status\nGiven the observational and large-scale nature of these analyses, it \nis challenging to identify the mechanisms driving these age–gender \nassociations. Nevertheless, numerous patterns in our data were relevant \nwhen considering sociologically relevant factors. One such considera-\ntion pertains to the hypothesis that gender stereotypes are most salient \nin high-status and prestigious occupations, which play a prominent role \nin reinforcing gender expectations and norms of desirability52,53.  To \ntest this, we recruited a nationally representative US sample from Pro-\nlific (n = 1,002) to evaluate the status and prestige of 867 occupations \nmatched between our Google Image data (Fig. 1a) and the US census \nfrom 2015 to 2022 (see ‘Collecting judgements of occupational status’ in \nMethods). Occupations rated as higher status were more likely to elicit \nGoogle Images in which men were older than women (Extended Data \nFig. 4a; r = 0.08; t = 11.28; P = 2.2 × 10−16; two-tailed Pearson’s correla-\ntion; n = 867 occupations). We reproduced this correlation using the \nobjective measure of occupational prestige54 of the US Bureau of Labor \nStatistics (Extended Data Fig. 4b; r = 0.11; t = 2.5; P = 0.01; two-tailed \nPearson’s correlation; n = 532 occupations could be matched). Next, \nwe showed that the probability of men appearing as older in Google \nImages is significantly higher for occupations associated with higher \nmedian earnings (Extended Data Fig. 4c; r = 0.11; t = 7.39; P = 1.07 × 10−13; \ntwo-tailed Pearson’s correlation; n = 4,444 pairwise comparisons at the \ncensus year level from 2015 to 2022; yearly earnings logged). We found \nthat the gender pay gap16,55, or the extent to which men earn more than \nwomen in the same occupation, is associated with the digital age gap, \nor the extent to which men appear older than women in Google Images \n(Extended Data Fig. 2d; r = 0.04; t = 7 = 3.05; P = 0.002; two-tailed Pear-\nson’s correlation; n = 4,444 pairwise comparisons at the census year \nlevel from 2015 to 2022; yearly earnings logged). These results were \nrobust to numerous statistical controls (Supplementary Figs. 11 and \n12 and Supplementary Tables 10–13) and resonate with long-standing \nconcerns about disparities in how genders are perceived and evaluated \nin the workplace.\nAge–gender distortions in online text\nA natural suspicion is that age-related gender bias in online images \nand videos may be driven by affordances of visual communication, \nsuch as image filters and cosmetics, which do not generalize to \nother modalities. Here we show that comparably salient patterns of \nage-related gender bias are readily observable in massive bodies of \ninternet text data beyond the visual modality. We begin by analysing \ngender–age associations in GPT-2 Large56, the largest open-source \nlanguage model of OpenAI trained on billions of tokens of text data \nfrom across the internet (see ‘Measuring age and gender in online text’ \nin Methods; Supplementary Tables 14 and 15). As shown in Fig. 2, the \nrepresentations of GPT-2 Large exhibit a strong correlation between \nthe extent to which a social category is associated with men and older \nages (r = 0.87; t = 105.57; P = 2.2 × 10−16; two-tailed Pearson’s correla-\ntion). These results are robust to alternative methods for extracting \nage and gender associations (Supplementary Fig. 13), as well as to a \nrange of statistical controls, including word frequency, gender, age \nand polysemy (Supplementary Fig. 14 and Supplementary Table 16). \n0\n0.25\n0.50\n0.75\n1.00\n0\n0.25\n0.50\n0.75\n1.00\nGender association\n(female–male dimension)\nAge association\n(young–old dimension)\n(Older)\n(Male)\nIntern\nCook\nHomeopath Secretary\nNovice\nDirector of\n research\nChairman of\n  the board\nElected\nof/f_icial\nMilitary\npersonnel\nChief of\nstaff\nFig. 2 | Women are represented as significantly younger than men in billions \nof words scraped from the internet, as encoded by the largest open-source \nmodel of OpenAI (GPT-2 Large). Correlation between age and gender \nassociations for 3,495 social categories in GPT-2 Large. The horizontal axis \npresents the gender association from 0 (female) to 1 (male), and the vertical \naxis presents the age association from 0 (young) to 1 (old). The trend line shows \nthe linear prediction according to an ordinary least squares regression. The \norange highlighted categories illustrate some of the categories that have the \nyoungest and most female associations, whereas the blue highlighted categories \nillustrate some of the categories that have the oldest and most male associations.\nNature | Vol 646 | 30 October 2025 | 1133\nThese associations are significantly predictive of ground truth age \ndistributions by gender and occupation in the census, affirming their \nempirical coherence (Supplementary Tables 17 and 18). These results \nare not unique to GPT-2 Large. We replicated these patterns across \neight different canonical and popular language models that vary in \ntheir training data and algorithmic training methods (Supplementary \nFigs. 15 and 16).\nAmplification via Google Search\nThe systematic distortion of age–gender associations in online images, \nvideos and text across popular platforms that we have identified raises \nconcerns about how mainstream algorithms trained on these data \nmight amplify the spread of this bias. We begin by examining possible \nalgorithmic amplification in the visual modality to investigate whether \nexposure to visual content from the Google search engine amplifies \nage-related gender bias in people’s beliefs.\nT o answer this question, we report the results of a pre-registered \nexperiment. We recruited a nationally representative sample of US \nparticipants from Prolific (n = 500), who were tasked with using Google \nto search for images of occupations related to science, technology and \nthe arts (Extended Data Fig. 5; ‘Participant pool’ in Methods). In total, \n459 participants completed the task. Each participant used Google \nto retrieve descriptions of 22 randomly selected occupations from \na set of 54 (‘participant experience’ in Methods). The participants \nwere randomized into treatment or control condition. In the treat-\nment condition (hereafter ‘image condition’), the participants used \nGoogle Images to search for images of occupations, which they then \nuploaded to our survey. After uploading an image for an occupation, the \nparticipants were asked to label the gender of the image they uploaded \nand then to estimate the average age of someone in this occupation. \nThe participants were also asked to rate their willingness to hire the \nperson depicted in their uploaded image. In the control condition, the \nparticipants used Google Images to search for and upload images of \nbasic unrelated categories (such as apple and guitar). After uploading \na random image, the control participants were asked to estimate the \naverage age of someone in a randomly selected occupation from the \nsame set. The control participants were also asked to rate the ideal hir-\ning age of someone in each occupation, as well as which gender (‘male’ \nor ‘female’) is most likely to belong to each occupation. This design \nallowed us to evaluate the treated participants’ age estimates of occu-\npations after uploading an image of a man or woman compared with \n(1) the control participants’ age estimates formed without exposure to \nimages of occupations and (2) the control participants’ age estimates \nconditional on their belief about which gender is most common in each  \noccupation.\nWe began by testing the prediction that exposure to online images of \noccupations primes age-related gender bias in the participants’ beliefs. \nT o test this prediction, we evaluated whether uploading an image of a \nwoman (man) for each occupation is associated with a lower (higher) \nage estimate compared with the average age estimate of each occupa-\ntion provided by those in the control condition who did not encounter \nonline images of each occupation before providing their estimates.  \nFigure 3a shows that the participants who uploaded an image of a \nwoman estimated the average age of an occupation to be 5.46 years \nyounger than those who uploaded an image of a man (t  = −19.07; \nP = 2.2 × 10−16; Student’s t-test), holding occupation constant. More-\nover, uploading an image of a woman led the participants to estimate \na significantly lower age for each occupation (by 1.75 years) compared \nwith the control participants (t = −11.32; P = 2.2 × 10−16), whereas upload-\ning an image of a man led the participants to estimate a significantly \nhigher age for each occupation (by 0.64 years) compared with those in \nthe control condition (t = 3.42; P = 0.0006; Student’s two-tailed t-test).\nNotably, these results hold when controlling for the participants’ \ngender and age, as well as whether their own demographics matched \nthose of the people depicted in the images they uploaded (Supple -\nmentary Tables 19 and 20). Supplementary analyses further demon-\nstrate that the participants’ estimates of the average ages of people in \noccupations are significantly correlated with the median age of these \noccupations according to the US census, indicating that the partici-\npants’ age judgements were coherent and consistent with ground truth \nsociodemographic distributions (Supplementary Tables 21 and 22).\n0\n0.02\n0.04\n0.06\n–20 –10 0 10 20 30\nEstimated age relative to control\nDensity\nImage uploaded\nFemale\nMale\n35\n36\n37\n38\n39\nControl Image\nCondition\nEstimated age (years)\nMale\nFemale\nPartial effect plot\n(with occupation and subject /f_ixed effects)\n25\n30\n35\n40\n45\n–1.0 -0.5 0 0.5 1.0\nGender associations\nPerceived ideal hiring age (years)\nGender ratings (control condition)\nImage uploads (image condition)\na b c\nMaleFemale\nFig. 3 | Googling for images of occupations amplifies age-based gender \ninequality in people’s beliefs.  The participants ( n = 459) from a nationally \nrepresentative sample were randomized either to the ‘image’ condition, in \nwhich they googled for images of occupations (n = 54), or the ‘control’ condition, \nin which they googled for image-based descriptions of random categories \n(such as apple) unrelated to occupations. a , Average age of each occupation, as \nestimated by the participants in the image condition, broken down by whether \nthey uploaded a female or male image of the occupation, centred relative to the \naverage age of each occupation provided across all the participants in the \ncontrol condition. b , Partial effect plot that controls for occupation and \nparticipant fixed effects while predicting the average age provided for each \noccupation depending on the gender of the image uploaded by the participants \nin the image condition or the gender the participants most associated with \neach occupation in the control condition. Data points display mean values,  \nand error bars indicate 95% confidence intervals. c , Correlation between the \ngender association and perceived ideal hiring age of each occupation (averaged \nacross all the participants in the control condition). The gender association  \nof each occupation was measured separately according to the participants’ \nmanual gender ratings in the control condition and the gender distribution of \nthe images uploaded by the participants in the image condition. Data points \nshow the average gender association and perceived ideal hiring age for each \noccupation according to each measure. Error bands show 95% confidence \nintervals.\n1134 | Nature | Vol 646 | 30 October 2025\nArticle\nNext, we leveraged the control condition to examine the effect of \nexposure to online images depicting occupations, above and beyond \nthe participants’ existing biases about the gender composition of \noccupations. Specifically, we tested whether the participants in the \ntreatment condition reported younger (older) ages when uploading \nimages of women (men) for each occupation compared with the age \nestimates provided by the control participants, who reported believing \nthat women (men) most often belonged to a given occupation. Figure 3b \nshows that the control participants who believed women are most \nlikely to belong to a given occupation also estimated the average age \nof people in this occupation to be significantly younger (by 2.15 years), \nevidencing a baseline pattern of age-related gender bias in people’s \njudgements (β [male] = 2.15 years; standard error = 0.38; t  = 5.55; \nP = 2.98 × 10−8). However, Fig. 3b further shows that this age gap is even \nhigher among those in the treatment condition. The participants who \nuploaded an image of a woman for a given occupation reported esti-\nmating the age of people in this occupation to be significantly younger \nthan the control participants who already believed that this occupation \nis female-skewed. This analysis controls for the specific occupation \nbeing evaluated, as well as the participants’ idiosyncratic judgements \nthrough occupation and participant fixed effects (β[gender × condi-\ntion] = −0.84; standard error = 0.31; t = −2.69; P = 0.007). These results \nindicate that exposure to online images significantly exacerbates the \nperceived age gap between women and men, particularly by increasing \nthe association between women and youth.\nWe conclude these experimental analyses by examining the practical \nconsequences of this age-based gender bias by evaluating its impact on \nwomen’s and men’s perceived fit across occupations. Figure 3c shows \nthat occupations that are more associated with women (men) are sig-\nnificantly correlated with the participants reporting lower (higher) \nrecommended ages for whom to hire in this occupation. Figure 3c  \nshows that the control participants’ perceived ideal age for hiring is \nstrongly and positively correlated with the extent to which each occupa-\ntion is associated with men, as measured by (1) the control participants’ \nmanual gender ratings of occupations (r = 0.58; P = 3.52 × 10−6) and  \n(2) the gender associations in the images uploaded by the participants \nin the image condition (r = 0.45; P = 0.0006; Pearson’s two-tailed cor-\nrelation). These analyses provide evidence that age-related gender \nassociations mediate people’s judgements of who is best to hire for a \ngiven occupation, with a preference towards hiring younger women \nand older men.\nThe above results are highly robust to whether (1) the participants \nprovided gender ratings of occupations without estimating age (as \ncaptured by a separately replicated experiment 6; Supplementary \nFig. 17) and (2) the participants rated hireability using a Likert scale \n(Supplementary Fig. 18; see Supplementary Tables 23 and 24 for a full \nsummary of all of our pre-registered hypotheses and the associated \nanalyses and results). All of our main pre-registered hypotheses were \nstrongly supported. Note that the framing of our study was updated \nin response to the review process, with no changes to the reporting \nof the experimental design or statistical results (see Supplementary \nTable 24 and the associated discussion for details).\nAmplification through large language models\nBecause popular artificial intelligence tools such as ChatGPT are trained \non internet data, we further propose that ChatGPT will exhibit signifi-\ncant age-based gender bias in its textual representations and evalua-\ntions of occupations in professional resumes. Identifying age-related \nbias in ChatGPT would highlight a potential pathway through which \nthis bias is widely propagated, given that more than 400 million people \nand two million businesses use ChatGPT weekly57. By adapting prompt \nengineering techniques developed for auditing biases in ChatGPT’s \nresume generation58, we prompted ChatGPT (v.GPT-4o mini) to cre-\nate nearly 40,000 resumes for 54 occupations using 16 unique female \nand male names that were normalized to control for name popularity, \nfamiliarity, ethnicity and perceived age group, such that the male and \nfemale names were maximally similar along these dimensions (the same \nnames were used in a recent auditing study58) (see ‘Prompt design’ in \nMethods). This experiment consisted of two phases: resume genera-\ntion and resume evaluation (Extended Data Fig. 6). All resumes were \ngenerated and evaluated in June 2024.\nWe began by examining the resume generation phase, in which we \nprompted ChatGPT to generate resumes across 54 occupations while \nvarying the prompt across three conditions: (1) the control condition; \n(2) the control–gender condition; and (3) the treatment condition. In \nthe control condition, we prompted ChatGPT to generate 50 resumes \nfor each of the 54 occupations without specifying the name or gender \nof the applicant, resulting in 2,700 unique resumes. In the control–\ngender condition, we replicated the control condition, except that we \nalso asked ChatGPT to include the gender of applicants in the resumes \ngenerated. Finally, in the treatment condition, we replicated the design \nof the control condition, except that our prompt included a specific \nname for the applicant and asked ChatGPT to generate a resume for the \nnamed applicant applying for the specified occupation (in this condi-\ntion, ChatGPT was not asked to explicitly identify the gender of the \nnamed applicant). We used the same occupations in our image search \nexperiment. In the treatment condition, we prompted ChatGPT 20 sepa-\nrate times for each name–gender–occupation prompt combination, \nyielding a total of 34,560 resumes and 17,280 resumes for each gender \ngroup. The resume features generated by ChatGPT were highly coher-\nent and stable (Supplementary Fig. 19 and Supplementary Table 25).\nNext, we evaluated the consequences that age-based gender biases in \nChatGPT’s representations of resumes can have on its practical applica-\ntion as a hiring tool. We focused on one of ChatGPT’s most popular uses \nin the workplace: to evaluate, score and rank resumes to expedite hiring \nprocesses by focusing human recruiters on resumes with top scores58. \nWe prompted ChatGPT to evaluate each of the resumes generated in \nthe first phase by providing a score between 1 and 100 to indicate the \nquality of each resume. All reported results are robust to altering the \nmodel temperature of ChatGPT (Supplementary Fig. 20).\nWe began by examining how altering the gender of a target appli-\ncant’s name affects the resumes that ChatGPT generates. We compared \nthe resumes that ChatGPT generated in the treatment condition for \nfemale or male names while using a linear regression to control for the \napplicant name and occupation. As Fig. 4a indicates, when ChatGPT \ngenerated a resume for a female name, it generated resumes with sig-\nnificantly lower ages (by 1.6 years; t = 20.5; P = 7.09 × 10−93), more recent \ngraduation dates (by 1.3 years; t = 12.5; P = 1.18 × 10−35) and fewer years \nof relevant experience (by 0.92 years; t = 5.39; P = 6.97 × 10−8) compared \nwith male names (Student’s t-test; n = 34,560 resumes). Compared \nwith the control condition, the resumes ChatGPT generated for female \n(male) applicants were significantly younger (older) and less (more) \nexperienced than the resumes generated for the same occupations \nwithout any gender or name (all at the P  < 0.00001 level; Student’s \ntwo-tailed t-test). Thus, ChatGPT exhibits age-based assumptions about \nwomen and men that are highly consistent with stereotypical associa-\ntions relating to gendered ageism.\nThese patterns of age-based gender bias in ChatGPT were replicated \nin the control–gender condition, in which ChatGPT generated its own \nname and gender classification for each occupation. When ChatGPT \ngenerated a male applicant for a given resume, this applicant was more \nlikely to be older (by 1.3 years; t = 17.3; P = 2.2 × 10−16) and to have gradu-\nated less recently (by 1.2 years; t = 7.10; P = 2.29 × 10−12) than when it \ngenerated a resume for a female applicant, holding occupation constant \n(Student’s t-test; n = 2,500 resumes). Thus, the effects we observed were \nnot dependent on the specific names used to prompt ChatGPT or on \nthe overall prompt design in the treatment condition.\nWe next evaluated the consequences this bias had on how ChatGPT \nevaluated the quality of resumes. Across all occupations, Fig. 4b shows \nNature | Vol 646 | 30 October 2025 | 1135\nthat ChatGPT’s judgements of resume quality were significantly and \npositively correlated with the age of the applicant that ChatGPT initially \ngenerated for the resume (r = 0.27; P = 2.2 × 10−16; t = 51.59; Pearson’s \ntwo-tailed correlation; n = 39,560 resumes). This result equally holds \nin a linear regression when we controlled for the occupation and name \nused to generate each application (β[age] = 0.04; t = 6.3; P = 2.96 × 10−10). \nFinally, we tested whether ChatGPT exhibits a preference not only for \nolder applicants but also for older men specifically, consistent with \nthe predictions of gendered ageism6,7. We used linear regression to \npredict ChatGPT’s judgements of resume quality using an interac -\ntion between the age and gender of the applicant while holding occu-\npation and applicant name constant. As shown in Fig. 4c, the model \nidentified a highly significant and positive interaction between being \nmale and older, indicating that the benefit of older age on ChatGPT’s \njudgements of resume quality is even greater if the applicant is male \nrather than female (β[male × age] = 0.04; t = 6.61; P = 3.66 × 10−11). This \ninteraction effect is robust to altering ChatGPT’s model temperature \n(Supplementary Fig. 20).\nDiscussion\nIn this study, we have provided large-scale evidence that age-related \ngender bias pervades online media, including images, videos and texts \nacross major platforms, and that the bias towards representing women \nas younger distorts ground truth realities on the actual ages of women \nand men throughout society. Our findings raise an alarm about the \nalgorithmic amplification of age-related gender bias on the internet, \nespecially considering that many mainstream machine learning algo-\nrithms are trained on these public datasets. Many of the image and \ntext datasets examined in this study are used extensively as canonical \ntraining and benchmark datasets for developing artificial intelligence \napplications. Enormous harm can be caused by latent social biases \nthat lurk in popular machine learning tools59,60, and algorithmic bias \ntypically arises from contaminated training data. Our study provides \ndirect evidence that age-related gender bias is amplified by two of the \nmost widely used algorithms today: the Google Image search engine \nand ChatGPT. Although companies such as Google and OpenAI invest \nheavily in reducing stereotypical content in their products 61, most \nstudies focus on single dimensions of bias, such as gender-based or \nrace-based biases. Our study highlighted the critical need to account \nfor multimodal and multidimensional forms of bias62, which are more \nchallenging to detect but not less consequential in how people and \nalgorithms represent the social world. The intersectional statistical \nbias we identified between gender and age may interact with other \nbiases, such as relating to how women and men are depicted in terms \nof warmth and competence, revealing a promising direction for future \nresearch63,64.\nHow might the digital distortion of age-related gender associations \nnegatively affect women and men? Our results highlighted several key \nways in which older women are likely to be disadvantaged by this bias. \n94\n95\n96\n97\n98\n20\n40\n60\n80\nApplicant age (years)\nFemale\nMale\nYears of\nexperience\nYears since\ngraduation\nApplicant\nage\n–0.5\n0\n0.5\n1.0\n1.5\nEffect of male name on resume feature\n85\n90\n95\n100\n20\n30\n40\n50\n60\n70\nApplicant age (years)\nResume score\nResume score\na b\nc\n+1.6 years\n+1.3 years\n+0.9 years\nFig. 4 | Effect of gender and age on ChatGPT’s generation and evaluation  \nof resumes.  a, Partial effect plot in an ordinary least squares regression \ndisplaying the effect of a male applicant name (versus a female applicant name) \non (1) applicant age; (2) years since the applicant’s graduation; and (3) the \nnumber of years of applicant’s relevant experience, while controlling for name \nand occupation. Only resumes from the treatment condition were examined in \nthis analysis (n = 34,560), because this ensures that all resumes have either a \nmale or female name and were produced through the same prompt. Error bars \nindicate 95% confidence intervals. b , Linear correlation between applicant age \nand ChatGPT’s rating of resume quality across all resumes ( n = 39,560) from all \nconditions. Data points display the raw distribution of scores for each resume, \nwith one data point per resume. The trend line reflects a standard bivariate \nlinear trend. c , Partial effect plot displaying the interaction effect between \napplicant age and applicant gender on ChatGPT’s rating of resume quality, with \nfixed effects for applicant name, occupation and phase 1 condition (data from \nthe control condition were excluded because of the lack of applicant gender; \nn = 37,060 resumes used in total). Error bands display 95% confidence intervals. \nChatGPT’s temperature was set to its default value of 0.7.\n1136 | Nature | Vol 646 | 30 October 2025\nArticle\nFor example, when generating resumes, ChatGPT not only assumes that \nwomen are younger, but also that they have less overall experience. Con-\nsequently, ChatGPT is biased towards giving lower scores to resumes \nfrom younger women compared with older women while giving the \nhighest scores to older men. Yet, ChatGPT also gives higher scores to \nresumes from young women than from young men, suggesting that \nyoung men may also be disadvantaged by this dual bias (Supplementary \nFig. 18). However, a selection bias favouring younger women and older \nmen may further reinforce gender inequalities at the systemic level, \nwhereby women are preferentially hired into roles with lower status and \nauthority but denied mobility, whereas older men continue to enjoy \ntop positions. This resonates with our finding that online content is \nmost likely to depict men as older than women for occupations with \nhigher status and wealth.\nA critical direction for future research is to investigate the causal \nmechanisms through which age-related gender bias seeps into and \nspreads through the images, videos and text of distinct platforms, \neach with its own unique audiences and distribution channels. Our \nresults about objective differences in the ages of male and female \ncelebrities visualized on IMDb, Wikipedia and Google probably reflect \nindustry-specific mechanisms related to status dynamics, hiring biases \nand the objectification of women in entertainment media. Yet, these \nindustry-specific drivers do not account for how strongly women and \nyouth are semantically associated in massive bodies of online text from \ndiverse sources, let alone in ChatGPT’s text-based representations and \nrankings of job candidates. A fascinating question for future work is \nto explore whether the aesthetic norms and hiring biases of entertain-\nment media spill over into the distortion of age–gender associations \nthroughout social life. A related question concerning supply-side fac-\ntors concerns whether age-related gender bias in popular algorithms \nstems from inequalities in the gender of data contributors online. Stud-\nies suggest that Reddit users65 and Wikipedia editors66 are dispropor-\ntionately male, and textual data from these platforms are frequently \nmined for training artificial intelligence models. Training artificial \nintelligence on datasets with greater gender equality in data contribu-\ntors may provide an effective mitigation strategy.\nThis study highlights the increasingly prominent role of internet \nculture and algorithms in mediating our representation of the social \nworld. As a recent review article emphasized67, previous studies have \nbeen limited in their ability to concretely measure the diverse cultural \nmeanings of age both in terms of its biological basis and its relevance \nto cultural notions of life stages (such as ‘youth’ and ‘childhood’). The \nstrength of our approach is that it captures the cultural meanings of \nage and gender along many dimensions, from concrete time-stamped \nimages to verbal descriptions of age categories across social roles \nand contexts. We revealed that the cultural meanings of gender and \nage are deeply linked on a massive scale across information modali -\nties and in ways that reflect socioeconomic inequalities. Compelling \nevidence that these patterns are socially constructed comes from \nour finding that online associations between gender and age heavily \ndistort the measurable ground truth reality of how people of different \ngenders and ages are distributed throughout society. The extent to \nwhich algorithms entrench distortions of social reality en masse and \nto which this can be corrected is a vital topic for future research on \ninternet policy and human cultural evolution. The methods we propose \nfor measuring widespread stereotyped representations online and \nfor grounding them in verifiable sociodemographic realities mark a \ncrucial step in the fight against pervasive cultural inequalities, both \nonline and beyond.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-025-09581-z.\n1. Madon, S., Guyll, M., Hilbert, S. J., Kyriakatos, E. & Vogel, D. L. Stereotyping the stereotypic: \nwhen individuals match social stereotypes. J. Appl. Social Psychol. 36, 178–205 (2006).\n2. Jussim, L. et al. in Handbook of Prejudice, Stereotyping, and Discrimination 2nd edn  \n(ed. Nelson, T. D.) 31–63 (Psychology Press, 2015).\n3. Jussim, L., Crawford, J. T. & Rubinstein, R. S. Stereotype (In)accuracy in perceptions of \ngroups and individuals. Curr. Dir. Psychol. Sci. 24, 490–497 (2015).\n4. Puddifoot, K. How Stereotypes Deceive Us (Oxford Univ. Press, 2021).\n5. Bai, X., Fiske, S. T. & Griffiths, T. L. Globally inaccurate stereotypes can result from locally \nadaptive exploration. Psychol. Sci. 33, 671–684 (2022).\n6. Guilbeault, D. et al. Online images amplify gender bias. Nature 626, 1049–1055 (2024).\n7. Itzin, C. & Phillipson, C. in Gender, Culture and Organizational Change (eds Itzen, C. & \nNewman, J.) 84–95 (Routledge, 1995).\n8. Older Women: Inequality at the Intersection of Age and Gender https://data.unwomen.\norg/publications/older-women-inequality-intersection-age-and-gender (United Nations, \n2022).\n9. Diehl, A., Dzubinski, L. M. & Stephenson, A. L. Women in leadership face ageism at every \nage. Harvard Business Review https://hbr.org/2023/06/women-in-leadership-face- \nageism-at-every-age (2023).\n10. Allport, G. W. The Nature of Prejudice (Addison-Wesley, 1954).\n11. Tajfel, H. in Intergroup Relations: Essential Readings (eds Hogg, M. A. & Abrams, D.) 132–145 \n(Psychology Press, 2001).\n12. Fiske, S. T. & Tablante, C. B. in APA Handbook of Personality and Social Psychology,  \nVolume 1: Attitudes and Social Cognition (eds Mikulincer, M. et al.) 457–507 (American \nPsychological Association, 2015).\n13. Bartholomew, D. J. Measuring Intelligence: Facts and Fallacies (Cambridge Univ. Press, \n2004).\n14. Nosek, B. A. et al. National differences in gender–science stereotypes predict national sex \ndifferences in science and math achievement. Proc. Natl Acad. Sci. USA 106, 10593–10597 \n(2009).\n15. Global Report on Ageism www.who.int/publications-detail-redirect/9789240016866 \n(Demographic Change and Healthy Ageing & World Health Organization, 2021).\n16. Gender pay gap widens as women age: women consistently earn less than men.  \nUnited States Census Bureau www.census.gov/library/stories/2022/01/gender-pay- \ngap-widens-as-women-age.html (2022).\n17. Cecco, L. Anger as Lisa LaFlamme dropped as Canada TV anchor after going grey.  \nThe Guardian https://webstories.theguardian.com/stories/uk/2022/aug/22/anger-as- \nlisa-laflamme-dropped-as-canada-tv-anchor-after-going-grey/ (2022).\n18. Spedale, S., Coupland, C. & Tempest, S. Gendered ageism and organizational routines at \nwork: the case of day-parting in television broadcasting. Organ. Stud. 35, 1585–1604 \n(2014).\n19. Duncan, C. & Loretto, W. Never the right age? Gender and age-based discrimination in \nemployment. Gender Work Organ. 11, 95–115 (2004).\n20. Ramati-Ziber, L., Shnabel, N. & Glick, P. The beauty myth: prescriptive beauty norms for \nwomen reflect hierarchy-enhancing motivations leading to discriminatory employment \npractices. J. Pers. Soc. Psychol. 119, 317–343 (2020).\n21. MacArthur, H. J., Cundiff, J. L. & Mehl, M. R. Estimating the prevalence of gender-biased \nlanguage in undergraduates’ everyday speech. Sex Roles 82, 81–93 (2020).\n22. Miller, K. L. I’m a manager, but to my boss and colleagues, I’m a ‘girl’. Washington Post \nwww.washingtonpost.com/business/economy/im-a-manager-but-to-my-boss-and- \ncolleagues-im-a-girl/2019/05/10/d18fb3ea-71d0-11e9-9eb4-0828f5389013_story.html \n(2019).\n23. Ridgeway, C. L. in Framed by Gender: How Gender Inequality Persists in the Modern World \n(ed. Ridgeway, C. L.) 32–55 (Oxford Univ. Press, 2011).\n24. Martin, A. E., Guevara Beltran, D., Koster, J. & Tracy, J. L. Is gender primacy universal?  \nProc. Natl Acad. Sci. USA 121, e2401919121 (2024).\n25. Shmerling, R. Why men often die earlier than women. Harvard Health www.health.\nharvard.edu/blog/why-men-often-die-earlier-than-women-201602199137 (2016).\n26. Ducharme, J. Why U.S. women now live 6 years longer than men. Time Magazine https://\ntime.com/6334873/u-s-life-expectancy-gender-gap/ (2023).\n27. Chatman, J. A., Sharps, D., Mishra, S., Kray, L. J. & North, M. S. Agentic but not warm: \nage-gender interactions and the consequences of stereotype incongruity perceptions for \nmiddle-aged professional women. Organ. Behav. Hum. Decis. Processes 173, 104190 \n(2022).\n28. Martin, A. E., North, M. S. & Phillips, K. W. Intersectional escape: older women elude \nagentic prescriptions more than older men. Pers. Soc. Psychol. Bull. 45, 342–359 \n(2019).\n29. Garg, N., Schiebinger, L., Jurafsky, D. & Zou, J. Word embeddings quantify 100 years of \ngender and ethnic stereotypes. Proc. Natl Acad. Sci. USA 115, E3635–E3644 (2018).\n30. Jones, J. J., Amin, M. R., Kim, J. & Skiena, S. Stereotypical gender associations in language \nhave decreased over time. Sociol. Sci. 7, 1–35 (2020).\n31. Williams, W. M. & Ceci, S. J. National hiring experiments reveal 2:1 faculty preference for \nwomen on STEM tenure track. Proc. Natl Acad. Sci. USA 112, 5360–5365 (2015).\n32. Chan, J. & Wang, J. Hiring preferences in online labor markets: evidence of a female hiring \nbias. Manage. Sci. 64, 2973–2994 (2018).\n33. Fiske, S. T. Prejudices in Cultural Contexts: Shared stereotypes (gender, age) versus \nvariable stereotypes (race, ethnicity, religion). Perspect. Psychol. Sci. 12, 791–799 (2017).\n34. Handy, J. & Davy, D. Gendered ageism: older women’s experiences of employment \nagency practices. Asia Pac. J. Hum. Resour. 45, 85–99 (2007).\n35. Messner, M. A., Duncan, M. C. & Jensen, K. Separating the men from the girls: the \ngendered language of televised sports. Gend. Soc. 7, 121–137 (1993).\n36. Lincoln, A. E. & Allen, M. P. Double jeopardy in Hollywood: age and gender in the careers \nof film actors, 1926–1999. Sociol. Forum 19, 611–631 (2004).\nNature | Vol 646 | 30 October 2025 | 1137\n37. Hoegele, D., Schmidt, S. L. & Torgler, B. The importance of key celebrity characteristics \nfor customer segmentation by age and gender: does beauty matter in professional \nfootball? Rev. Manag. Sci. 10, 601–627 (2016).\n38. Edström, M. Visibility patterns of gendered ageism in the media buzz: a study of the \nrepresentation of gender and age over three decades. Fem. Media Stud. 18, 77–93 (2018).\n39. Martin, D. et al. The spontaneous formation of stereotypes via cumulative cultural \nevolution. Psychol. Sci. 25, 1777–1786 (2014).\n40. Guilbeault, D., Baronchelli, A. & Centola, D. Experimental evidence for scale-induced \ncategory convergence across populations. Nat. Commun. 12, 327 (2021).\n41. Mastroianni, A. M. & Gilbert, D. T. The illusion of moral decline. Nature 618, 782–789 \n(2023).\n42. Bailey, A. H., Williams, A. & Cimpian, A. Based on billions of words on the internet, \npeople = men. Sci. Adv. 8, eabm2463 (2022).\n43. Rothe, R., Timofte, R. & Van Gool, L. Deep expectation of real and apparent age from a \nsingle image without facial landmarks. Int. J. Comput. Vis. 126, 144–157 (2018).\n44. Chen, B.-C., Chen, C.-S. & Hsu, W. H. in Computer Vision – ECCV 2014 (eds Fleet, D. et al.) \n768–783 (Springer, 2014).\n45. Porter, C. & Serra, D. Gender differences in the choice of major: the importance of female \nrole models. Am. Econ. J. Appl. Econ. 12, 226–254 (2020).\n46. Zhang, Z., Song, Y. & Qi, H. Age progression/regression by conditional adversarial \nautoencoder. In Proc. IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR) 4352–4360 (IEEE Computer Society, 2017).\n47. Eidinger, E., Enbar, R. & Hassner, T. Age and gender estimation of unfiltered faces.  \nIEEE Trans. Inf. Forensics Secur. 9, 2170–2179 (2014).\n48. Huang, G. B., Ramesh, M., Berg, T. & Learned-Miller, E. Labeled Faces in the Wild: A \nDatabase for Studying Face Recognition in Unconstrained Environments https://inria.hal.\nscience/inria-00321923v1/preview/Huang_long_eccv2008-lfw.pdf#page=2 (University of \nMassachusetts and Stony Brook University, 2008).\n49. McGrady, R., Zheng, K., Curran, R., Baumgartner, J. & Zuckerman, E. Dialing for videos: a \nrandom sample of YouTube. J. Quant. Descr. Digital Media 3, 1–85 (2023).\n50. Wolf, L., Hassner, T. & Maoz, I. Face recognition in unconstrained videos with matched \nbackground similarity. In Proc. CVPR 2011 529–534 (IEEE, 2011).\n51. Zhu, H. et al. CelebV-HQ: a large-scale video facial attributes dataset. In Computer  \nVision – ECCV 2022 (eds Bertino, E. et al.) 650–667 (Springer, 2022).\n52. Ridgeway, C. L. & Diekema, D. in Gender, Interaction, and Inequality (ed. Ridgeway, C. L.) \n157–180 (Springer, 1992).\n53. Ridgeway, C. L. Why status matters for inequality. Am. Sociol. Rev. 79, 1–16 (2014).\n54. Occupational prestige ratings. GitHub https://occupational-prestige.github.io/opratings/\nindex.html.\n55. Kochhar, R. The enduring grip of the gender pay gap. Pew Research Center www.\npewresearch.org/social-trends/2023/03/01/the-enduring-grip-of-the-gender-pay-gap/ \n(2023).\n56. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog 1 \nhttps://cdn.openai.com/better-language-models/language_models_are_unsupervised_\nmultitask_learners.pdf (2019).\n57. Kant, R. OpenAI’s weekly active users surpass 400 million. Reuters www.reuters.com/\ntechnology/artificial-intelligence/openais-weekly-active-users-surpass-400-million- \n2025-02-20/ (2025).\n58. Armstrong, L., Liu, A., MacNeil, S. & Metaxa, D. The silicon ceiling: auditing GPT’s race and \ngender biases in hiring. In Proc. 4th ACM Conference on Equity and Access in Algorithms, \nMechanisms, and Optimization 1–18 (Association for Computing Machinery, 2024).\n59. Lambrecht, A. & Tucker, C. Algorithmic bias? An empirical study of apparent gender- \nbased discrimination in the display of STEM career ads. Manage. Sci. 65, 2966–2981 \n(2019).\n60. Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an \nalgorithm used to manage the health of populations. Science 366, 447–453 (2019).\n61. Sambasivan, N. Moving toward a gender equitable internet. Google https://blog.google/\ntechnology/next-billion-users/towards-gender-equity-online/ (2019).\n62. Charlesworth, T. E. S., Ghate, K., Caliskan, A. & Banaji, M. R. Extracting intersectional \nstereotypes from embeddings: developing and validating the Flexible Intersectional \nStereotype Extraction procedure. Proc. Natl Acad. Sci. USA 3, pgae089 (2024).\n63. Fiske, S. T. Stereotype content: warmth and competence endure. Curr. Dir. Psychol. Sci. \n27, 67–73 (2018).\n64. Sun, L. et al. Smiling women pitching down: auditing representational and presentational \ngender biases in image-generative AI. J. Comput.-Mediated Commun. 29, zmad045 \n(2024).\n65. Duggan, M. & Smith, A. 6% of online adults are reddit users. Pew Research Center  \nwww.pewresearch.org/internet/2013/07/03/6-of-online-adults-are-reddit-users/ (2013).\n66. Antin, J., Yee, R., Cheshire, C. & Nov, O. Gender differences in Wikipedia editing. In Proc. \n7th International Symposium on Wikis and Open Collaboration 11–14 (Association for \nComputing Machinery, 2011).\n67. Johfre, S. & Saperstein, A. The social construction of age: concepts and measurement. \nAnnu. Rev. Sociol. 49, 339–358 (2023).\n68. Srinivasan, K., Raman, K., Chen, J., Bendersky, M. & Najork, M. WIT: Wikipedia-based image \ntext dataset for multimodal multilingual machine learning. In Proc. 44th International ACM \nSIGIR Conference on Research and Development in Information Retrieval 2443–2449 \n(Association for Computing Machinery, 2021).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution-\nNonCommercial-NoDerivatives 4.0 International License, which permits any \nnon-commercial use, sharing, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material.  \nYou do not have permission under this licence to share adapted material derived from this \narticle or parts of it. The images or other third party material in this article are included in the \narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is  \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain \npermission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle\nMethods\nIn this section, we detail the methods used in all parts of our analyses, \nincluding our observational comparisons of gender and age bias in \nonline images, videos and texts, as well as our Google Image search \nexperiment and our resume audit of ChatGPT. The pre-registration for \nour online image search experiment is available at https://osf.io/x9scm. \nThis experiment was a successful replication of a previous study with \na nearly identical design; the pre-registration of this previous study \nis available at https://osf.io/2b58d. This study was approved by the \nethics review board at the University of California, Berkeley, where \nthis study was conducted.\nObservational methods\nIn what follows, we describe our observational methodology for collect-\ning and analysing large bodies of images, videos and text online. With \nregard to the crowdsourcing methods applied to analysing our main \nGoogle and Wikipedia Image datasets, many of the methods described \nbelow (including the procedure and demographic details of the coder \npopulation) were reproduced from the original data collection sum-\nmary provided as part of the first publication of these datasets 6. In \naddition to this reproduced description, we include information on \nhow age classifications of these images were collected, because this \nfeature was not explored or discussed as part of the original publica-\ntion of these datasets6.\nImage data collection procedure\nOur crowdsourcing methodology consisted of four steps. We began \nby identifying all social categories in WordNet 69, a canonical lexi -\ncal database of English. WordNet captures 3,495 social categories, \nincluding occupations (such as doctor) and generic social roles (such \nas neighbour). We then gathered online images associated with each \nsocial category from Google and Wikipedia. Next, we applied the \nOpenCV deep learning module in Python to automatically extract \nthe face from each image. Cropping faces helped us ensure that each \nface in each image was separately classified in a standardized manner \nwhile avoiding subjective biases in coders’ decisions for which face to \nfocus on and categorize in each image. Finally, we hired 6,392 human \ncoders from Amazon’s Mechanical Turk to manually classify the gen-\nder of the faces. Each face was classified by three unique annotators \n(as per established methodology 6,70,71), so that the gender of each \nface (‘male’ or ‘female’) could be identified on the basis of the major-\nity (modal) gender classification across three coders. (We also gave \ncoders the option of labelling the gender of faces as ‘non-binary’ , but \nthis option was only chosen in 2% of cases. Therefore, we excluded \nthese data from our main analyses and recollected all classifications \nuntil each face was associated with three unique coders using either \nthe male or female label.) Although coders were asked to label the \ngender of the faces presented, our measure was agnostic to which \nfeatures the coders used to determine their gender classifications. \nThey may have used facial features and features relating to the aes -\nthetics of expressed gender, such as hair or accessories. In terms of \nage, each face was classified as belonging to one of the following \nage bins (the ordinal ranking of each bin is indicated in parenthe -\nses): (1) 0–11, (2) 12–17, (3) 18–24, (4) 25–34, (5) 35–54, (6) 55–74 and  \n(7) 75+. Because the greater number of classification options for age \nled to fewer images associated with a majority-preferred age clas -\nsification, we identified the age of each face by taking the average of \nthe ordinal age bin judgements across the three coders. Each search \nwas implemented from a fresh Google account with no previous \nhistory. Searches were run in August 2020 by ten distinct data serv -\ners in New York City. This study was approved by the institutional \nreview board at the University of California, Berkeley, where this \npart of the study was conducted. All participants provided informed  \nconsent.\nT o collect images from Google, we followed a previous study by \nretrieving the top 100 images that appeared when using each of the \n3,495 categories to search for images using the public Google Images \nsearch engine. (Google provides roughly 100 images for its initial \nsearch results.) Across the non-gendered and gendered searches, \n3,489 categories could be associated with images containing faces in \nthe Google Image search engine (specifically, 3,434 categories for the \nnon-gendered searches and 2,960 for the gendered searches). T o col-\nlect images from Wikipedia, we identified the images associated with \neach social category in the 2021 Wikipedia-based Image T ext (WIT) \nDataset72. WIT maps all images over Wikipedia to textual descriptions \non the basis of the title, content and metadata of the active Wikipedia \narticles in which they appear. We were able to associate 1,251 social \ncategories from WordNet with images in WIT (across all English articles) \nthat supported stable classification as human faces with detectable \nages, according to our coders. The coders identified 18% of images as \nnot containing human faces, and these were removed from our analyses. \nWe also asked all annotators to complete an attention check, which \ninvolved providing the correct answer to the common-sense question \n(What is the opposite of the word ‘down’?) using the following options: \n‘fish’ , ‘up’ , ‘monk’ and ‘apple’ . We removed the data from all annotators \nwho failed an attention check (15%) and continued collecting classifi-\ncations until each image was associated with the judgements of three \nunique coders, all of whom passed the attention check.\nDemographics of human coders\nThe human coders were all US-based adults fluent in English. Supple-\nmentary Table 3 indicates that our main results are robust to control-\nling for the demographic composition of our human coders. Among \nour coders, 44.2% identified as being female, 50.6% as male, 3.2% as \nnon-binary and the remaining preferred not to disclose. In terms of age \n(in years), 42.6% identified as 18–24, 22.9% as 25–34, 32.5% as 35–54, 1.6% \nas 55–74 and less than 1% as over 75. In terms of race, 46.8% identified \nas Caucasian, 11.6% as African American, 17% as Asian, 9% as Hispanic, \n10.3% as Native American and the remaining as either mixed race or \npreferred not to disclose. In terms of political ideology, 37.2% identi-\nfied as conservative, 33.8% as liberal, 20.3% as independent, 3.9% as \nother and the remaining preferred not to disclose. In terms of annual \nincome, 14.3% reported making less than US$10,000, 33.4% reported \nUS$10,000–50,000, 22.7% reported US$50,000–75,000, 14.9% \nreported US$75,000–100,000, 10.5% reported US$100,000–150,000, \n2.8% reported US$150,000–250,000, less than 1% reported more than \nUS$250,000 and the remaining preferred not to disclose. In terms of \nthe highest level of education acquired by the annotator, 2.7% selected \n‘below high school’ , 17.5% selected ‘high school’ , 29.2% selected ‘techni-\ncal/community college’ , 34.5% selected ‘undergraduate degree’ , 14.8% \nselected ‘master’s degree’ , less than 1% selected ‘doctorate degree’ and \nthe remaining preferred not to disclose.\nImage and video datasets\nT o measure age-related gender bias in online images and videos, we \nanalysed a range of open-source datasets collected either for social \nscience research or for training face recognition algorithms, none \nof which examined or reported correlations between the gender and \nage of the people depicted. In total, we examined more than one mil-\nlion images from five main online sources: Google, Wikipedia, IMDb, \nFlickr and YouTube, as well as the Common Crawl (created by randomly \nscraping content from across the world-wide web), each with distinct \nways of sourcing and aggregating data. We measured gender and age \nusing a variety of techniques, including human judgements, machine \nlearning and ground truth data on the self-reported gender and true \ntime-stamped age of the people depicted. Our statistical analyses did \nnot control for multiple comparisons because all tests were theoreti-\ncally guided and did not involve an agnostic permutation over a set \nof pairwise comparisons. Although we examined many datasets, our \nmain analyses examined a single correlation (between gender and age) \nwithin each dataset separately. We now describe each of these datasets.\nFirst, we used large-scale crowdsourcing to identify age-related \ngender bias in a new dataset of images from Google and Wikipedia \n(which was originally collected for a recently published study that \ndid not examine age-related classifications)6. This dataset6 contains \nthe top 100 Google images associated with each of the 3,435 social \ncategories contained within WordNet69, a lexical ontology that maps the \ntaxonomic structure of the English language. These categories include \noccupations (such as ‘physicist’) and generic social roles (such as ‘col-\nleague’). For each category, this dataset contains the top 100 images \nthat appear in Google Images when searching for (1) the category on \nits own (such as ‘doctor’); (2) the female version of the category (such \nas ‘female doctor’); and (3) the male version of the category (such as \n‘male doctor’). The gendered searches were completed only for the \n2,960 non-gendered categories (for example, the searches did not \ninclude ‘male aunt’). Altogether, this yielded 657,035 unique images \ncontaining faces from Google. Searches were run from ten distinct \ndata servers in New York City. Because Google is known to customize \nsearch results on the basis of the location from which the search is run72, \nwe show that our results are robust to replicating this data collection \npipeline while collecting Google Images from six distinct cities around \nthe world (Supplementary Fig. 3).\nThis dataset also leveraged human coders to classify the age and \ngender of faces in Wikipedia images associated with as many WordNet \nsocial categories as possible in the 2021 WIT Dataset68. WIT maps all \nimages over Wikipedia to textual descriptions on the basis of the title, \ncontent and metadata of the active Wikipedia articles in which they \nappear. WIT includes images of 1,251 social categories from WordNet \nacross all English Wikipedia articles, in total yielding 14,709 faces.\nWe hired 6,392 human annotators from Amazon’s Mechanical Turk to \nclassify the gender and age of the faces in these images. Each face was \nclassified by three unique annotators6,70,71 so that the gender of each \nface (male or female) could be identified on the basis of the majority \ngender classification across three coders. (We also gave coders the \noption of identifying the gender of faces as non-binary, but this option \nwas chosen in less than 2% of cases. Therefore, we excluded these data \nfrom our main analyses.) In terms of age, each face was classified as \nbelonging to one of the following age bins (in years): (1) 0–11, (2) 12–17, \n(3) 18–24, (4) 25–34, (5) 35–54, (6) 55–74 and (7) 75+. Because the greater \nnumber of classification options for age led to fewer images with a \nmajority-preferred classification, we identified the age of each face by \ntaking the average of the ordinal age bin judgements across the three \ncoders (our main results hold when using the modal age judgement; \nSupplementary Fig. 4). Our findings continued to hold when controlling \nfor annotator demographics and intercoder agreement, which was high \nin our sample (Supplementary Fig. 5 and Supplementary Table 3). We \nalso conducted a separate validation task, in which the true gender and \nage of the faces being classified were known. The results indicate that \nour coders exhibited reliable and accurate gender and age judgements, \nwith no biases as a function of gender (Supplementary Tables 4 and 5). \nSensitivity tests further showed that even if our coders were hypotheti-\ncally biased in their ability to estimate age as a function of gender, this \nwould not disrupt the statistical significance or directionality of our \nfindings (Supplementary Fig. 6).\nWe extended our findings by examining age-related gender bias \nin two large corpora of online images collected from three main \nwebsites (IMDb, Wikipedia and Google) for which the self-identified \ngender and true age of the faces were objectively inferred. This exten-\nsion allowed us to examine whether women are objectively younger \nthan men in online images, without depending on age predictions \nfrom human coders or machine learning algorithms. The first cor -\npus was the 2018 IMDb–Wiki dataset43, which consisted of more than \nhalf a million images of celebrities from IMDb and Wikipedia on the \nbasis of those depicted in the top 100,000 most visited IMDb pages.  \nEach image in this dataset was time-stamped for when the photograph \nwas taken, allowing the age of each face to be inferred on the basis of  \nthe celebrity’s date of birth, which is publicly available through their \nopen profile on IMDb and Wikipedia. This dataset yielded 451,570 \nimages from IMDb and 57,932 images from Wikipedia. The second cor-\npus was the 2014 CACD44, which consisted of 163,446 images collected \nfrom the Google Image search engine depicting 2,000 celebrities, \ncomprising the top 50 most popular celebrities each year from 1951 to \n1990. The creators of CACD collected time-stamped images by using \nGoogle Image search to retrieve images associated with each celebrity \nfrom 2004 to 2013 (for example, by searching ‘Emma Watson 2004’ \nthrough ‘Emma Watson 2013’). We merged the CACD and IMDb–Wiki \ndataset43 to identify the gender of 1,825 celebrities in the CACD (50% \nare female celebrities). All images from both datasets containing ages \nbelow 0 and above 100 were removed to maximize data quality. Each \ndataset identified the exact age of the celebrities at the time they were \ndepicted in each photograph by determining the date of birth and \ngender of each celebrity on their public IMDb and Wikipedia pages \nand then by comparing this information to the time-stamped date of \nwhen each photograph was taken.\nFinally, we examined images from four publicly available training \ndatasets widely used to train automated face recognition algorithms. In \nthese canonical datasets, the gender and age classifications were on the \nbasis of a combination of automated machine learning classifications \nand verification through human annotation. This includes the 2017 UTK \ndataset46 consisting of 20,000 images scraped randomly from across \nthe world-wide web using search engines and public repositories, the \n2014 Adience dataset47 consisting of 26,580 images randomly sampled \nfrom Flickr, a public image-based social media platform, and the 2008 \nLFW48 dataset consisting of 13,233 images randomly scraped from \nonline news websites. Finally, we examined images of faces extracted \nfrom screenshots of YouTube videos using two datasets. The first \nwas the 2011 YouTube Faces dataset50 consisting of 3,425 YouTube \nvideos and 3,645 images of celebrities. The second one was the 2022 \nCelebV-HQ51 dataset consisting of 35,666 images formed by identifying \npublic lists of celebrities on Wikipedia and automatically collecting the \ntop 10 YouTube videos associated with each celebrity.\nComparing online images with the census\nWe were able to match 867 social categories from our main Google \nimage (Fig. 1a) dataset to occupational categories in the US census. \nThe US Bureau of Labor Statistics recently released a breakdown of the \nmedian age of each gender, from 2019 to 2023, across five industries: \nsales, services, natural resources and construction, production and \ntransportation and management. The census assigns each occupation \nto one of these industries, allowing those occupations matched in our \nGoogle image dataset to be assigned a census industry. We estimated \nthe relationship between gender and age at the industry level by aver-\naging the age associations in Google Images across all occupations \nwithin a given industry (averaged within each occupation and then \nacross occupations at the industry level). The census age groupings \nare highly similar to the age groupings the coders used when judging \nfaces. Supplementary Tables 8 and 9 present the robustness of our \nresults to a range of statistical controls.\nCollecting judgements of occupational status\nWe collected a nationally representative sample of 1,002 US-based \nparticipants who provided their subject evaluations of the status and \nprestige of occupations. Each participant evaluated 20 randomly sam-\npled occupations from a broader set of 867 WordNet social catego-\nries that could be matched with corresponding occupations in the \nUS census. Through randomization, each category was evaluated by \n27 unique participants on average (minimum of 15 participants). For \neach occupation, the participants rated (1) its status using the following \nscale (How would you rate the social status of someone belonging to \nArticle\nthis occupation? −2, very negative; −1, negative; 0, neutral; 1, positive; \n2, very positive) and (2) its prestige using the following scale (T o what \nextent do you agree that it is prestigious to belong to this occupation? \n−2, strongly disagree; −1, disagree; 0, neutral; 1, agree; 2, strongly agree). \nWe also asked the participants to rate the status/prestige through the \nstandard question from the general social survey, which asked them to \nplace occupations on a ladder containing 10 rungs, where the bottom \nrung indicates occupations with very low status, income, education and \nprestige, whereas the highest rung indicates occupations with very high \nstatus, income, education and prestige (Supplementary Fig. 11). The \nparticipants’ answers across all three questions were highly correlated \n(all paired Pearson’s correlations above 0.85; Supplementary Fig. 9). \nIn our main results shown in Extended Data Fig. 4, we first averaged all \nparticipants’ judgements of each occupation across the (1) status and \n(2) prestige question and then assigned each occupation a single status \nscore by taking the mean of its average status and prestige score. In the \nSupplementary Information, we show that all of our results hold when \nexamining each question separately and when examining the partici-\npants’ judgements using the standard social status question from the \nGeneral Social Survey (GSS) (Supplementary Fig. 11 and Supplementary \nTables 10–13). Note that Prolific’s nationally representative sample \nof the US population size allows for a maximum of 800 participants. \nHowever, this sample size was not large enough to gain sufficiently \npowered judgements across all 867 occupational categories; therefore, \nan extra sample of US participants was recruited until all occupations \nreached a minimum of 15 evaluations from independent participants. \nAll results are robust to a range of statistical controls (Supplementary \nTables 10–13).\nMeasuring age and gender in online text\nT o measure age-related gender bias in large bodies of internet text, we \nleveraged word embedding models trained on massive amount of inter-\nnet data. These models were designed to construct a high-dimensional \nvector space on the basis of the co-occurrence of words (for example, \nwhether two words appear in the same sentence), such that words with \nsimilar meanings are closer in this vector space. T echnically, these \nembedding spaces also capture higher-order similarities on the basis \nof whether words co-occur in similar linguistic contexts (that is, in asso-\nciation with related sets of words), without requiring words to directly \nappear together. We harnessed recent advances in natural language \nprocessing to extract demographic dimensions in word embedding \nmodels that capture the extent to which existing demographics under-\nlie the cultural connotations of categories. We identified both gender \nand age dimensions. We briefly describe this methodology below.\nWord embedding models leverage the frequency of word co-  \noccurrences in text to position words in an n-dimensional space such \nthat words that frequently co-occur together are more closely located \nin this n-dimensional space. The ‘embedding’ for a given word identi-\nfies the specific position of this word in this n-dimensional space. The \ncosine distance between word embeddings in this n-dimensional space \nprovides a robust measure of semantic similarity that captures the \nsimilarity of the semantic contexts in which words appear6. T o extract a \ngender dimension in word embedding space, we harnessed the ‘geom-\netry of culture’ method of Kozlowski et al.73. This method was originally \ndeveloped for static embedding models such as Word2Vec and GloVe; \ntherefore, we incorporated key adjustments that enable its application \nto contextualized embeddings through generative transformer models \nsuch as GPT-2 Large. We identified two clustered regions in the word \nembedding space corresponding to conventional representations of \nfemales and males. Specifically, the female cluster consisted of ‘woman’ , \n‘her’ , ‘she’ , ‘female’ and ‘girl’ , whereas the male cluster consisted of ‘man’ , \n‘his’ , ‘he’ , ‘male’ and ‘boy’ . For each social category in WordNet, we cal-\nculated the average cosine distance between this category and both \nthe female and male clusters. Each category was associated with two \nnumbers: its cosine distance with the female cluster (averaged across \nits cosine distance with each term in the female cluster) and its cosine \ndistance with the male cluster (averaged across its cosine distance \nwith each term in the male centroid). Taking the difference between \nthe cosine distance of a category with the female and male centroids \nallowed each category to be positioned along a −1 (female) to 1 (male) \nscale in the embedding space. Although we recognize that gender is \nfundamentally non-binary, we built upon a previous study that lever-\naged this binary framework73 to identify biases in the extent to which \npeople associate concepts with men or women.\nThe issue with applying this approach to contextualized embeddings \nis that the embedding associated with an individual word can be sharply \ndifferent from the embedding associated with this word within a larger \ncontext, for example, within a surrounding sentence. For this reason, \nwe modified the geometry of culture method by creating male and \nfemale poles consisting of many parallel sentences that vary only in \nwhether they mention the corresponding male or female version of a \npronoun. For example, the male pole consists of sentences such as ‘he \nis a boy’ and ‘his hobbies are very masculine’ , whereas the analogues \nof these sentences in the female pole are ‘she is a girl’ and ‘her hobbies \nare very feminine’ . Fifty sentences were used to form each pole. All \nsentences used are provided in Supplementary Tables 14 and 15. We \nconducted key robustness tests to verify the validity of our methods \nand the robustness of our results to the use of different sentences along \nthe gender pole (Supplementary Fig. 13). In our supplementary analyses \ninvolving static embedding models, we used the original geometry of \nculture approach.\nWe used this same approach to construct an age dimension in word \nembedding models. For static embedding models, we identified two \nclustered regions in the word embedding space corresponding to \nyounger and older ages. Specifically, the younger cluster consisted of \nthe words ‘child’ , ‘teenager’ and ‘adolescent’ , whereas the older clus-\nter consisted of the words ‘adult’ , ‘senior’ and ‘elder’ . All results are \nhighly robust to increasing the number of words used to construct \nthis age dimension. For example, our results replicate when defining \nthe younger cluster using the words ‘young’ , ‘youth’ , ‘childhood’ , ‘child’ , \n‘baby’ , ‘infant’ , ‘teen’ , ‘teenager’ and ‘adolescent’ , as well as when defin-\ning the older cluster using the words ‘old’ , ‘elder’ , ‘elderly’ , ‘adulthood’ , \n‘adult’ , ‘senior’ , ‘parent’ , ‘retired’ and ‘aged’ . We used the same technique \nto sort categories along a −1 (young) to 1 (old) scale in the embedding \nspace. Similarly, to examine age associations in contextualized embed-\nding models, we generated 50 sentences that hold everything constant \nwhile varying whether the age term involved indicates a young or old \nage (see Supplementary Table 15 for a full list of the age sentences used \nto create the contextualized age pole).\nIn all cases examining static models, to compute the distances \nbetween the vectors of social categories represented by bigrams \n(such as ‘professional dancer’), we used the Phrases class in the gensim \nPython package, which provided a pre-built function for identifying \nand calculating distances for bigram embeddings. This method works \nby identifying an n-dimensional vector of middle positions between \nthe vectors corresponding separately to each word in the bigram (for \nexample, ‘professional’ and ‘dancer’). This technique then treats the \nmiddle vector as the singular vector corresponding to the bigram ‘pro-\nfessional dancer’ and is thereby used to calculate the distances from \nother category vectors. This method is not necessary in contextual \nlanguage models, which provide unique embeddings for n-grams as \ndistinct from their component words.\nOnce the corresponding demographic dimensions were constructed \nfor each model, we evaluated the correlation between gender and age \nassociations across 3,495 social categories from WordNet (the same \ncategories examined in our image analyses above). T o simplify the \npresentation of how this gender and age dimensions are correlated, we \nused min-max normalization to convert the gender dimension into a 0 \n(female) to 1 (male) association, which, in effect, represents the extent \nto which each category carries male associations relative to all other \ncategories. We applied the same approach to produce a normalized 0 \n(young) to 1 (old) dimension, which captures the extent to which each \ncategory is associated with older ages relative to all other categories. \nThe supplementary analyses showed that our results are highly robust \nto varying our technique for constructing the age and gender dimen-\nsions (Supplementary Fig. 13 and Supplementary Tables 14 and 15).\nIn the main text, we present our results while analysing the largest \nopen-source large language model from OpenAI (GPT-2 Large56), for \nwhich word embeddings can be robustly and transparently extracted \nand examined. GPT-2 Large is one of the largest and most popular \nopen-source language models, trained on billions of words from the \n2019 WebT ext dataset, which primarily comprises Reddit data and \nthe diverse web content (including articles and books) to which these \nReddit data are linked. In the supplementary analyses, we showed \nthat these results replicate when examining a wide range of models, \nincluding Word2Vec, GloVe, BERT, FastT ext, RoBERTa and GPT-4, all \nof which vary in their dimensionality and data sources, as well as the \nyear in which their training data were collected, ranging from 2013 to \n2023. We focus our main results on GPT-2 Large, not only because of its \nscale and popularity, but also because its open-source nature allows \nus to transparently access and analyse its word embeddings. GPT-4, by \ncontrast, is a closed-source model that relies on using OpenAI’s private \napplication programming interface, which limits the interpretability \nof our method. Nevertheless, supplementary analyses showed that \nour results replicate when examining this closed-source model (Sup-\nplementary Figs. 14 and 15).\nExperimental methods with human participants\nParticipant pool. We invited a nationally representative sample of \nparticipants (n = 500) from Prolific. Prolific is a popular online panel \nfor social science research that provides prescreening functionality \nspecifically for recruiting a nationally representative sample of the \nUSA along the dimensions of sex, age and ethnicity. The participants \nwere invited to partake in the study only if they were based in the USA, \nwere fluent English speakers and were over 18 years old. A total of 52% \nof participants were female (no participants identified as non-binary). \nThe average age of participants was 45.2 (45.9 for women; 44.6 for men). \nOur sample size was selected to emulate the sample size of a recent \nexperiment with a highly similar design, which effectively measured \nstatistically powered outcomes6. There was an attrition rate of 9.2% \nof participants (which is within the common range of attrition for \nonline experiments), such that 459 participants completed the task. \nOur results only examined data from the participants who completed \nthe experiment to ensure data quality. All the participants provided \ninformed consent before participating. This experiment was run on \n10 November 2023.\nParticipant experience. Extended Data Fig. 4 presents a schematic \nof the full experimental design. This experiment was approved by the \nInstitutional Review Board at the University of California, Berkeley. \nIn this experiment, the participants were randomized to one of two \nconditions: (1) the image condition (in which they used the Google \nImage search engine to retrieve images of occupations) and (2) the \ncon trol condition (in which they used the Google Image search eng-\nine to retrieve images of random, non-gendered categories, such as \n‘apple’). In the image condition, after uploading an image for a given \noccupation, the participants were asked to label the gender of the image \nthey uploaded and then to estimate the average age of someone in this \noccupation. The participants in the image condition were also asked \nto rate their willingness to hire the person depicted in their uploaded  \nimage (Supplementary Fig. 18). After uploading a given random  \nimage, the control participants were then asked to estimate the average \nage of someone in a randomly selected occupation from the same set. \nThe control participants were also asked to rate the ideal hiring age of \nsomeone in each occupation, as well as which gender (male or female) \nwas most likely to belong to each occupation. This design allowed us \nto evaluate the treated participants’ age estimates after uploading \nan image of a man or woman compared with (1) the control partici-\npants’ age estimates that were formed without exposure to images of  \noccupations and (2) the control participants’ age estimates conditional \non which gender they think is most common in each occupation. All \nparticipants regardless of condition completed this sequence for 22 \noccupations (randomly sampled from a set of 54 occupations held \nconstant across conditions). On average, the participants took 35 min \nto complete the task. All participants were compensated at the rate of \nUS$15 per hour for their time.\nThe occupations that the participants identified were selected to \ninclude professions from science, technology, engineering and math-\nematics and the liberal arts. We used the exact list of occupations lever-\naged in a recent experiment of a similar design (focusing only on gender, \nnot age) to compare results and evaluate replicability6. Here is the full \npre-registered set: immunologist, mathematician, harpist, painter, \npiano player, aeronautical engineer, applied scientist, geneticist, astro-\nphysicist, professional dancer, fashion model, graphic desig ner, \nhygienist, educator, intelligence analyst, logician, intelligence agent, \nfinancial analyst, chief executive officer, clarinetist, chiropractor, com-\nputer expert, intellectual, climatologist, systems analyst, program-\nmer, poet, astronaut, professor, automotive engineer, cardiologist, \nneurobiologist, English professor, number theorist, marine engineer, \nbookkeeper, dietician, model, trained nurse, cosmetic surgeon, fashion \ndesigner, nurse practitioner, art teacher, singer, interior decorator, \nmedia consultant, art student, dressmaker, English teacher, literary \nagent, social worker, screen actor, editor-in-chief and schoolteacher.\nExperimental methods with ChatGPT\nPrompt design. The control condition in the resume generation phase \nused the following prompt (adapted from a recent audit study58): ‘Write \na detailed and concise resume for someone applying for a position in the \nrole of [occupation] in the United States. Include the following sections: \ndate of birth in month/day/year format, education with graduate date \nin month/day/year format, total number of years of relevant experience \nin numeric format and a numbered list of relevant skills’ . This prompt \ndiffers from that of a previous study by prompting ChatGPT to include \nthe applicants’ age and years of experience. We prompted ChatGPT 50 \nseparate times for each occupation, resulting in 2,700 unique resumes \nin the control condition.\nThe control–gender condition in the resume generation phase repli-\ncated the design of the control condition, except it also asked ChatGPT \nto include each applicant’s gender in the resumes generated across \nthe 54 occupations. This led to the following updated prompt: ‘Write \na detailed and concise resume for someone applying for a position \nin the role of [occupation] in the United States. Include the follow -\ning sections: gender of the applicant (male or female), date of birth in \nmonth/day/year format, education with graduate date in month/day/\nyear format, total number of years of relevant experience in numeric \nformat and a numbered list of relevant skills’ . We prompted ChatGPT \n50 separate times for each gender–occupation prompt combination, \nyielding 2,700 unique resumes in this condition.\nThe treatment condition in the resume generation phase replicated \nthe design of the control condition, except that its prompt included \na specific name for the applicant and asked ChatGPT to generate a \nresume for this named applicant applying for the specified occupation. \nWe used 16 female names and 16 male names, all normalized in terms \nof popularity, familiarity, ethnicity and perceived age group, such that \nthe male and female names were maximally similar along these dimen-\nsions. These names were developed and deployed by Armstrong et al.58 \nfor a related auditing task that did not involve requesting explicit age \ninformation to examine age-related gender bias. The female and male \nnames validated by Armstrong et al.58 included four names for the fol-\nlowing ethnic identities: Hispanic, Asian, white people and Black people  \nArticle\n(four names per gender per ethnicity yielded 16 names for each gen-\nder). The prompt in this condition read: ‘[name] is applying for a posi-\ntion in the role of [occupation] in the United States. Write a detailed \nand concise resume for [name]. Include the following sections: date \nof birth in month/day/year format, education with graduate date in \nmonth/day/year format, total number of years of relevant experience \nin numeric format and a numbered list of relevant skills’ . We prompted \nChatGPT with this request 20 separate times for each name–gender– \noccupation prompt combination, yielding 34,560 resumes in total \nand 17,280 resumes for each gender group. This excludes the rare \ncases where ChatGPT failed to produce a resume (for example, by \ngenerating an empty JavaScript Object Notation structure without \nany resume-related information).\nIn the resume evaluation phase, we prompted ChatGPT to evaluate \neach resume from the resume generation phase by providing a score \nbetween 1 and 100 to indicate the quality of each resume. Specifically, \nwe deployed the following prompt (adapted from Armstrong et al.58): \n‘Given the following job title and resume, rate the resume with a score \nbetween 1 and 100 to capture the quality of the resume and its fit with \nthe job title. 1 is a low score, while 100 is a high score. Only return a score’ .\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nAll data associated with this study can be downloaded from GitHub \nat https://github.com/drguilbe/distortion_age_gender_online/. The \npre-registration for our experiment is available at https://osf.io/x9scm.\nCode availability\nAll codes associated with this study can be downloaded from GitHub \nat https://github.com/drguilbe/distortion_age_gender_online/.\n \n69. Devitt, A. & Vogel, C. The topology of WordNet: some metrics. In Proc. GWC-04, 2nd \nGlobal WordNet Conference 106–111 (Global WordNet Association, 2004).\n70. Kay, M., Matuszek, C. & Munson, S. A. Unequal representation and gender stereotypes in \nimage search results for occupations. In Proc. 33rd Annual ACM Conference on Human \nFactors in Computing Systems 3819–3828 (Association for Computing Machinery, 2015).\n71. Metaxa, D., Gan, M. A., Goh, S., Hancock, J. & Landay, J. A. An image of society: gender \nand racial representation and impact in image search results for occupations. Proc. ACM \nHum. Comput. Interact. 5, 26:1–26:23 (2021).\n72. Vlasceanu, M. & Amodio, D. M. Propagation of societal gender inequality by internet \nsearch algorithms. Proc. Natl Acad. Sci. USA 119, e2204529119 (2022).\n73. Kozlowski, A. C., Taddy, M. & Evans, J. A. The geometry of culture: analyzing the meanings \nof class through word embeddings. Am. Sociol. Rev. 84, 905–949 (2019).\nAcknowledgements We acknowledge the Haas MORS group for their helpful comments  \non this project. We also thank T. Hull for his assistance with data collection. This project was \npartially funded by grants from the Fisher Center for Business Analytics and the Center for \nEquity, Gender & Leadership, awarded to D.G. and S.D., as well as the Barbara and Gerson Bakar \nFellowship awarded to D.G., all through the University of California, Berkeley.\nAuthor contributions D.G. and S.D. designed the project. D.G. analysed the data. D.G. and \nB.S.D. developed the algorithmic methods and collected the data. D.G. wrote the paper. S.D. \nand B.S.D. assisted with editing the paper.\nCompeting interests The authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-025-09581-z.\nCorrespondence and requests for materials should be addressed to Douglas Guilbeault.\nPeer review information Nature thanks April H. Bailey and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permissions information is available at http://www.nature.com/reprints.\nExtended Data Fig. 1 | The lack of correlation between female representation \nin an occupation and its associated median age according to the U.S. Bureau \nof Labor Statistics.  Female representation is measured by the percentage of \nwomen employed in an occupation. Panel ( A) shows the raw data (with each \ndata point showing a single occupation) for 2012 (the correlation is non-  \nsignificant; r = −0.021, CI = [−0.12, 0.08], p  = 0.70, Pearson Correlation, \ntwo-tailed, n = 536 occupations). Panel ( B) shows the raw data (with each data \npoint showing a single occupation) for 2023 (the correlation is non-significant; \nr = −0.046, CI = [−0.14, 0.05], p = 0.36, Pearson Correlation, two-tailed, n = 594 \noccupations). For all census years for which this data is provided in this format \n(from 2011 to 2023), there is not a single year with a statistically significant \ncorrelation between the fraction of women in an occupation and its associated \nmedian age (Table S1). Error bands show 95% confidence intervals.\nArticle\nExtended Data Fig. 2 | Benchmarking Google Images of occupations \nagainst Census data.  Comparing the average age of women and men across \nindustries in the U.S. Census (from 2019 to 2023) to the average perceived age \nof people in occupations from these same industries according to Google \nImages. The shape of the points indicates the data source, and the color of the \npoints indicates the associated gender ( N = 867 matched occupations).\nExtended Data Fig. 3 | Comparing the gender-age gap in Google images with \nthe gender-age gap across industries in the U.S. Census (from 2019 to 2023).  \nEach panel shows the age gap between each gender for each industry separately. \nThe midpoint of the age gap for each industry and data source is centered at 0 \nto help visually compare the magnitude of the age gap across datasets for each \nindustry. Negative values along the horizontal axis indicate the gender that is \nassociated with the lower age (relative to the midpoint), whereas positive \nvalues indicate the gender that is associated with the older age (relative to the \nmidpoint). The color of the point indicates which gender falls on each side  \nof the gender gap. Bold lines indicate cases where men are associated with a \nhigher age than women for a given data source in a given industry; dotted lines \nindicate cases where women are older than men.\nArticle\nExtended Data Fig. 4 | Status effects on the gender-age gap in Google \nImages.  The age gap for occupations in Google Images is predicted by the \nperceived status of occupations, as well as by the median yearly earnings of \noccupations and the gender pay gap by occupation according to U.S. Census \ndata from 2015 to 2022. Google image data is from Guilbeault et al. (2024; see \nFig. 1a) and is based on 866 social categories matched to occupations in the U.S. \ncensus. In all panels, data points are presented as mean values and confidence \nintervals display 95% confidence intervals. ( A) The correlation between the \nperceived status of an occupation and the probability that men appear older \nthan women in Google images of the occupation (status perceptions are \naveraged across a nationally representative U.S. sample, n = 1,002 participants; \nan average of 27 participants rated each of occupations; data shown in six \nevenly spaced bins). Examples of occupations in the lowest (highest) 5% of \nperceived social status according to this measure are provided at the bottom  \nof the figure. (B) The correlation between the U.S. Bureau of Labor Statistics’ \nmeasures of occupational prestige (shown in quartiles) and the probability  \nthat men appear older than women in Google images of the occupation (532 \noccupations could be matched). ( C) The logged median yearly earnings for an \noccupation (shown in quartiles) predict the probability that men appear older \nthan women in Google images of the occupation. ( D) The pay gap in median \nearnings for an occupation by gender (shown in quartiles) predicts the age gap \nin perceived age between men and women in Google images of the occupation. \nFor (B) and (C), data are shown for the 753 occupations that could be associated \nwith yearly earnings across Census years, 2015 to the present.\nExtended Data Fig. 5 | Schematic represention of the design for the Google \nImage search experiment. A nationally representative US sample of participants \n(n = 500) were randomized into one either the Google image condition, and the \nControl condition (in which they were asked to use the Google Images search \nengine to retrieve images of random, non-gendered categories, such  \nas guitar or apple). After uploading an image for either an occupation (Image \ncondition) or random distractor category (Control condition), participants \nindicated the average age of people in the target occupations (from 0 to 100).  \nIn the Control condition, participants were asked to indicate which gender they \nassociate with a randomly selected occupation after uploading a description \nfor an unrelated category. Participants completed this sequence for 22 unique \noccupations (randomly sampled from a set of 54 occupations). Participants in \neach condition were asked additional questions after inputting their age \nestimate for each occupation. Specifically, in the Control condition, participants \nwere also asked to use the same age slider to indicate the ideal age of a new hire \nin the given occupation. Control participants were also asked to indicate which \ngender they most associate with the given occupation by selecting either \n“men”, “women”, or “don’t know”. In the Image condition, participants were \nasked to indicate the perceived gender of the face they uploaded for a given \noccupation (using the same gender options indicated above); and they were \nalso asked to rate their willingness to hire the person depicted for this occupation \nusing a 7-point Likert.\nArticle\nExtended Data Fig. 6 | Schematic representation of the method applied for \nthe resume generation phase of the ChatGPT audit experiment. ChatGPT \nwas prompted to generate resumes for 54 occupations in each of three \nconditions: (i) the Control condition, (ii) the Control-gender condition, and  \n(iii) the Treatment condition. In the Control condition, ChatGPT generated 50 \nresumes for each occupation without specifying the name or gender of the \napplicant, yielding 2,700 resumes. In the Control-gender condition, the design \nwas identical to the Control condition except that ChatGPT was also prompted \nto specify the gender of applicants in the resumes it generated. In the Treatment \ncondition, the design of the Control condition was replicated except that \nChatGPT was prompted to generate a resume for a named applicant whose \nname was selected from a list of 16 male and 16 female names spanning four \nethnicities and normed for popularity, familiarity, and perceived age group.  \nIn the Treatment condition, ChatGPT was prompted 20 separate times for each \nname-gender-occupation prompt combination, yielding 34,560 resumes in \ntotal and 17,280 resumes for each gender group.\n1 nature portfolio  |  reporting summaryApril 2023\nCorresponding author(s): Douglas Guilbeault\nLast updated by author(s): Aug 20, 2025\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection Our team developed custom Python code (Python version 3.12.0) for collecting the observational data from Google and Wikipedia reflected in \npanels A, B, and C of figure 1. The remaining data reflected in panels D-J of figure 1 were already collected and published by other teams, and \nwe detail and cite these sources directly in the main text (many of these datasets are established training sets for computer vision models). \nExcept for the word2vec model we trained on a recent sample of news we collected using Crawl Feeds (see figure S14), all other language \nmodels examined are either publicly available via Python packages (e.g., gensim) or via publicly available APIs as in the case of GPT models. \nThe experimental data reflected in figure 3 were collected using a survey instrument developed in Qualtrics and a panel of participants from \nProlific. \nData analysis All data analyses were conducted using custom code written by our team in either R or Python (Python version 3.12.0; R version 4.4.2). All \ndata and code for replicating the analyses in our paper are publicly available at the following github: https://github.com/drguilbe/\ndistortion_age_gender_online. Our statistical analyses do not control for multiple comparisons since all tests were theoretically motivated \nand, when experimental, preregistered -- and none of our analyses involved an agnostic permutation over a set of pairwise comparisons. \nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\n2 nature portfolio  |  reporting summaryApril 2023\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \n \nAll data and code associated with this study can be publicly downloaded at: https://github.com/drguilbe/distortion_age_gender_online\nResearch involving human participants, their data, or biological material\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), \nand sexual orientation and race, ethnicity and racism.\nReporting on sex and gender In this study, we examine \"gender\" as a socially constructed category specifying a manner of identification and not sex as a \nbiologically determined phenotypic category. Our measurements of gender - either in images, videos, or text - are in no way \nintended or framed to reify gender as biologically determined or as an objectively detectable and static attribute of specific \nindividuals or of people in general. Instead, we study gender as a perception -- i.e., as (an often biased) judgment of people \nor of social categories in general (e.g., occupations). Accordingly, we measure gender in a number of ways, none of which \nconstitute a fully objective ground truth, especially when understanding gender as both a fluid mode of self-identification and \nas a context-dependent perception by others. In the context of images, we measure gender using (i) classification judgments \naggregated across human coders, (ii) machine learning classifications, and (iii) self-identified gender ascriptions based on \npublicly available biographical profiles. In the case of language models, we study gender at the level of social categories (e.g., \noccupations) by examining the extent to which a given category is associated with men or women in these language models' \nhigh dimensional embedding space. We adopt a binary partitioning of gender (female and male) when extracting gender \nassociations from language models not because we believe or intend to convey an essentialist view of gender as intrinsically \nbinary. We maintain that gender is highly fluid and exists in a complex, fluid, multidimensional continuum. However, in order \nto compare our image data against established methods for measuring gender associations in language models (specifically, \nthe \"geometry of culture\" method), we adopt the binary partitioning of gender in embedding space that this prior work uses \nand validates via a series of robustness tests. \nReporting on race, ethnicity, or \nother socially relevant \ngroupings\nIn this study, we do not directly examine socially constructed categorizations relating to race or ethnicity. In our resume audit \nstudy of ChatGPT, we adopt the methods of recent work which provided a set of names for men and women that were \nnormalized by familiarity and chosen to be representative across the following four ethnic categories (according to their \ncategorizations): (Black or African American, Hispanic or Latinx, Asian, and White. See the original paper that provided this set \nof names here: https://arxiv.org/abs/2405.04412\nPopulation characteristics For this experiment, we recruited a nationally representative sample of participants (n = 500) from the popular \ncrowdsourcing platform Prolific, which provides a panel of high-quality human participants for online research. Our sample \nsize was selected to emulate the sample size of a recent experiment with a highly similar design, which effectively measured \nstatistically powered outcomes (see Guilbeault et al., 2024 in Nature). 459 participants completed the task, exhibiting an \nattrition rate of 9.2%. We only examine data from participants who completed the experiment. To recruit a nationally \nrepresentative sample, we used Prolific’s pre-screening functionality designed to provide a nationally representative sample \nof the USA along the dimensions of sex, age, and ethnicity. Participants were invited to partake in the study only if they were \nbased in the USA, fluent English speakers and aged more than 18 years. A total of 52% of participants were female (no \nparticipants identified as non-binary). The average age of participants was 45.2 (45.9 for women, 44.6 for men). All \nparticipants provided informed consent before participating. This experiment was run on November 10th, 2023.\nRecruitment For the experimental component, our sampling strategy was a random sample from Prolific’s nationally representative panel \n(n = 500; see “Population Characteristics”). \nEthics oversight This study was approved by the Ethics Review Board IRB at the University of California, Berkeley, where this study was \nconducted. \nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\n3 nature portfolio  |  reporting summaryApril 2023\nBehavioural & social sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nStudy description This study has three main components. The first is an observational analysis of the gender and age associations of social categories in \nimages, videos, and textual data from popular online platforms, including Google, Wikipedia, Flickr, IMDb (Internet Movie Database), \nand Youtube. The second component is an online experiment in which human participants were tasked with using Google Images to \nsearch for images of occupations. This experiment tested the effects of exposure to online images of occupations on people’s beliefs \nabout the age and hireability of men and women in these occupations. The third component of this study is a resume audit of \nChatGPT. For this audit, we prompted ChatGPT to generate resumes across a number of occupations (yielding over 40k resumes), \nwhile varying whether the resume was generated for a male or female applicant (based on the name of the target applicant). We \nthen measured how varying the name of the applicant impacted the age and level of experience that ChatGPT assigned to this \napplicant, as well as ChatGPT’s overall score for the quality of the resume it generated. \nResearch sample The research sample for the experimental component of this study consists of a national representative sample of the U.S. as curated \nby the crowdsourcing platform Prolific. The details on Prolific’s U.S. nationally representative sample are provided by Prolific at the \nfollowing link: https://researcher-help.prolific.com/en/article/95c345 \n \nTo create a representative U.S. sample, Prolific takes the intended sample size and strategies it across three demographics: age, sex, \nand ethnicity. Prolific uses census data from the U.S. Census Bureau to divide the sample into subgroups with the same proportions \nas the national U.S. population. This means, for example, that a nationally representative sample contains the same proportion of \n28-37 year old Asian women as the national population (to the extent possible). Using this representative sample is important for our \nexperiment, which does not make any demographic-specific predictions around the effects of Google Image search on gender-age \nbias. In this way, based on these available resources, our findings are positioned to generalize across the aforementioned \ndemographic characteristics.  \nSampling strategy For the experimental component of our study, our sampling strategy was a random sample from Prolific’s nationally representative \npanel (n = 500, see “Research Sample”). In terms of the statistical method used to determine sample size, this sample size replicated \nthe sample size from a recent publication by our team which presented statistically significant effects of Image search with a \ncomparable sample size (see Guilbeault et al. 2024 in Nature). Otherwise, no power tests were used to select this sample size; we \nbased our judgment on the prime facie validity of reproducing this recent sample size from Guilbeault et al. 2024.  \n \nFor the observational component of this study, our sample of Google images was collected through the following standardized \nprocedure (this description is copied from the Guilbeault et al. 2024 Nature reporting summary, which first introduced this dataset). \nWe started by using each of the social categories in Wordnet to automatically search and retrieve the top 100 images in Google \ncorresponding to each social category in Google Images (Google provides roughly 100 images by default for its initial search results \non a given search query). Each search was implemented from a fresh Google account with no prior history to avoid the uncontrolled \neffects of Google’s recommendation algorithm, which customized search results based on browsing history. Searches were run by 10 \ndistinct serves in New York City. All image data from Google was collected in August 2020.  \n \nThe sources of the language models are transparently described in the paper and appendix. All models were available via Python \n(e.g., genism package) or via public API in the case of GPT models. One of the models we examine in the appendix was trained on a \nrecent sample of online news that our team collected. We compiled a dataset of 2,717,000 randomly sampled news articles \npublished in English across various topics between January 2021 and August 2023. These articles were sourced from the following \nprominent online news sources: 1,000,000 articles from the BBC; 500,000 articles from the Huffington Post; 480,000 articles from \nCNBC; 400,000 articles from Bloomberg; 160,000 articles from Time Magazine; 150,000 12 articles from Techcrunch; and 27,000 \narticles from CNN. These datasets were purchased from the online web-scraping service, Crawl Feeds (https://crawlfeeds.com/). \nData collection The online observational data – text, images, and videos – all derived from publicly available repositories. The experimental data \ncollected was implemented using a survey instrument designed in Qualtrics and deployed over Prolific. \nTiming All of our main image data from Google was collected in August 2020. The timing associated with the training and release of all \nlanguage models is described in the manuscript and appendix; similarly for the timing associated with the already published image \nand video training sets that we examine. One of the models we examine in the appendix was trained on a recent sample of online \nnews that our team collected. We compiled a dataset of 2,717,000 randomly sampled news articles published in English across \nvarious topics between January 2021 and August 2023. The experiment reported in figure 2 was conducted on November 10th, 2023. \nThe resume audit study of ChatGPT was conducted between July and August of 2024. \nData exclusions No data were excluded from the observational analyses. For our experimental data, we only examined data associated with \nparticipants who successfully completed the task (459/500). \nNon-participation No participants declined to participate in this task based on our records. \nRandomization In the experiment, participants were evenly randomized into either the control or the image condition. \nReporting for specific materials, systems and methods\n4 nature portfolio  |  reporting summaryApril 2023\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nPlants\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nNovel plant genotypes NA\nSeed stocks NA\nAuthentication NA\nPlants",
  "topic": null,
  "concepts": []
}