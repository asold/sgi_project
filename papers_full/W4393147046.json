{
  "title": "SECap: Speech Emotion Captioning with Large Language Model",
  "url": "https://openalex.org/W4393147046",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4375850666",
      "name": "Yaoxun Xu",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2146313505",
      "name": "Hangting Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2119302463",
      "name": "Jianwei Yu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4319606629",
      "name": "Qiaochu Huang",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2097219887",
      "name": "Zhiyong Wu",
      "affiliations": [
        "Tsinghua University",
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2639420037",
      "name": "Shi Xiong Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122432211",
      "name": "Guangzhi Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102936830",
      "name": "Yi Luo",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2896189801",
      "name": "Rongzhi Gu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4375850666",
      "name": "Yaoxun Xu",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2146313505",
      "name": "Hangting Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2119302463",
      "name": "Jianwei Yu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4319606629",
      "name": "Qiaochu Huang",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2097219887",
      "name": "Zhiyong Wu",
      "affiliations": [
        "Tsinghua University",
        "Chinese University of Hong Kong",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2639420037",
      "name": "Shi Xiong Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122432211",
      "name": "Guangzhi Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102936830",
      "name": "Yi Luo",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2896189801",
      "name": "Rongzhi Gu",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W6752051073",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3035060230",
    "https://openalex.org/W6802852670",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W6797810191",
    "https://openalex.org/W3169320628",
    "https://openalex.org/W2959133507",
    "https://openalex.org/W2969889150",
    "https://openalex.org/W3187963534",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2799331981",
    "https://openalex.org/W2111926505",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2923793772",
    "https://openalex.org/W2561826558",
    "https://openalex.org/W3206857696",
    "https://openalex.org/W2954841306",
    "https://openalex.org/W4280582601",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W2603621985",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3176445421",
    "https://openalex.org/W3206041894",
    "https://openalex.org/W2074788634",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4291566970",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4287665141",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3036928441",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3015190346",
    "https://openalex.org/W4361864998",
    "https://openalex.org/W4389519587",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4367061106",
    "https://openalex.org/W2803832867"
  ],
  "abstract": "Speech emotions are crucial in human communication and are extensively used in fields like speech synthesis and natural language understanding. Most prior studies, such as speech emotion recognition, have categorized speech emotions into a fixed set of classes. Yet, emotions expressed in human speech are often complex, and categorizing them into predefined groups can be insufficient to adequately represent speech emotions. On the contrary, describing speech emotions directly by means of natural language may be a more effective approach. Regrettably, there are not many studies available that have focused on this direction. Therefore, this paper proposes a speech emotion captioning framework named SECap, aiming at effectively describing speech emotions using natural language. Owing to the impressive capabilities of large language models in language comprehension and text generation, SECap employs LLaMA as the text decoder to allow the production of coherent speech emotion captions. In addition, SECap leverages HuBERT as the audio encoder to extract general speech features and Q-Former as the Bridge-Net to provide LLaMA with emotion-related speech features. To accomplish this, Q-Former utilizes mutual information learning to disentangle emotion-related speech features and speech contents, while implementing contrastive learning to extract more emotion-related speech features. The results of objective and subjective evaluations demonstrate that: 1) the SECap framework outperforms the HTSAT-BART baseline in all objective evaluations; 2) SECap can generate high-quality speech emotion captions that attain performance on par with human annotators in subjective mean opinion score tests.",
  "full_text": "SECap: Speech Emotion Captioning with Large Language Model\nYaoxun Xu1, Hangting Chen2, Jianwei Yu2,*, Qiaochu Huang1,\nZhiyong Wu1,3,*\n, Shi-Xiong Zhang2, Guangzhi Li2, Yi Luo2, Rongzhi Gu2\n1Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n2Tencent AI Lab\n3The Chinese University of Hong Kong, Hong Kong SAR, China\nxuyx22@mails.tsinghua.edu.cn, {erichtchen,tomasyu}@tencent.com, zywu@sz.tsinghua.edu.cn\nAbstract\nSpeech emotions are crucial in human communication and\nare extensively used in fields like speech synthesis and natural\nlanguage understanding. Most prior studies, such as speech\nemotion recognition, have categorized speech emotions into a\nfixed set of classes. Yet, emotions expressed in human speech\nare often complex, and categorizing them into predefined\ngroups can be insufficient to adequately represent speech\nemotions. On the contrary, describing speech emotions di-\nrectly by means of natural language may be a more effective\napproach. Regrettably, there are not many studies available\nthat have focused on this direction. Therefore, this paper pro-\nposes a speech emotion captioning framework namedSECap,\naiming at effectively describing speech emotions using natu-\nral language. Owing to the impressive capabilities of large\nlanguage models in language comprehension and text gener-\nation, SECap employs LLaMA as the text decoder to allow\nthe production of coherent speech emotion captions. In addi-\ntion, SECap leverages HuBERT as the audio encoder to ex-\ntract general speech features and Q-Former as the Bridge-Net\nto provide LLaMA with emotion-related speech features. To\naccomplish this, Q-Former utilizes mutual information learn-\ning to disentangle emotion-related speech features and speech\ncontents, while implementing contrastive learning to extract\nmore emotion-related speech features. The results of objec-\ntive and subjective evaluations demonstrate that: 1) the SE-\nCap framework outperforms the HTSAT-BART baseline in\nall objective evaluations; 2) SECap can generate high-quality\nspeech emotion captions that attain performance on par with\nhuman annotators in subjective mean opinion score tests.\nIntroduction\nSpeech communication plays a pivotal role in people’s daily\nlife in terms of transmitting information and establishing\nconnections. As one of the core carriers of interpersonal\ncommunication, speech not only undertakes the function of\nverbal communication but also deeply involves the transmis-\nsion of emotions and intentions. Recognizing and interpret-\ning speech emotions precisely is crucial for enhancing com-\nmunication effects. Therefore, how to extract the speaker’s\nemotional information accurately from speech has gradually\nbecome an important topic in the field of speech processing.\n*Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nPrevious research has typically approached speech emo-\ntion acquisition as a categorization task, known as speech\nemotion recognition (SER) (El Ayadi, Kamel et al. 2011;\nNwe, Foo, and De Silva 2003; Jiang et al. 2019), where emo-\ntions like fear and happiness are assigned to discrete cate-\ngories. In recent years, the performance of such SER tasks\nhas made great progress thanks to the emergence of innova-\ntive model architectures.\nHowever, traditional SER exhibits limitations, because\nsingle-word labels often lack nuances, failing to convey de-\ntailed emotional information like intensity and fluctuations.\nSpeech emotions are typically multifaceted, encompassing\ndiverse affective states (e.g., simultaneous happiness and\nnervousness) within one utterance. Classifying speech into a\nsingle emotion category may inadequately capture authentic\nemotion. Additionally, the inherently subjective perception\nof emotions leads to potential variability in emotion classifi-\ncation among individuals interpreting complicated speech.\nConsidering the limitations of speech emotion classifica-\ntion, employing natural language sentences rather than la-\nbels could be a promising strategy to describe speech emo-\ntions more precisely. Motivated by the recent progress of the\nAutomated Audio Captioning (AAC) task (Han et al. 2021;\nChen et al. 2020; Ye et al. 2021) which employs natural lan-\nguage to describe acoustic events in audio, we present the\nSpeech Emotion Captioning (SEC) task and propose an in-\nnovative SECap framework, comprising an audio encoder,\na Bridge-Net, and a text decoder, to characterize human\nspeech emotions using natural language. To our knowledge,\nthis is among the pioneering works in this direction.\nIn the SEC task, there are two primary challenges to ad-\ndress: firstly, how to extract the emotion-related speech fea-\ntures from the original speech inputs; and secondly, how to\ngenerate high-quality, human-like speech emotion descrip-\ntions. For the first challenge, limited speech data with emo-\ntion captions makes training the audio encoder from scratch\nchallenging. Inspired by the success of pre-trained model in\nSER (Mohamed and Aly 2021) tasks, we utilize HuBERT\n(Hsu et al. 2021) as SECap’s audio encoder for robust speech\nfeature extraction. However, directly using the frame-level\nHuBERT features can be computationally heavy. To address\nthis, inspired by BLIP-2 (Li et al. 2023), we employ Q-\nFormer as Bridge-Net to compress HuBERT features. While\nboth acoustic and content information within HuBERT fea-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19323\nFigure 1: Comparison of Speech Emotion Recognition\n(SER) model and the proposed SECap. The SER model gen-\nerates emotion labels, while SECap generates natural lan-\nguage emotion descriptions derived from the speech.\ntures are related to speech emotion, acoustic information is\ntypically more directly related to speech emotion, and con-\ntent information can be easily obtained through transcrip-\ntion. Therefore, in the Bridge-Net, we aim to separately ex-\ntract emotion-related acoustic information from HuBERT\nfeatures while eliminating content information. Thus, we\nemploy Speech-Caption Contrastive Learning and Speech-\nTranscription Mutual Information Learning to train Bridge-\nNet to better extract emotion-related acoustic information.\nFor the second challenge, due to the advances in large lan-\nguage models (LLMs) and their impressive natural language\nunderstanding capabilities, such as GPT-4 (OpenAI 2023),\nwe employ LLaMA (Touvron et al. 2023) as text decoder\nfor generating fluent and coherent speech emotion captions\nbased on Q-Former-extracted speech features. Concurrently,\nwe use LLaMA to guide Q-Former training, enabling bet-\nter projection of speech emotion features into LLaMA, ulti-\nmately yielding higher-quality speech emotion captions.\nAs for evaluation, we design both subjective and objective\nevaluation metrics based on the AAC task to better assess\nthe quality of speech emotion captions generated by SE-\nCap. To facilitate a more effective comparison, we choose\nthe HTSAT-BART model (Mei et al. 2023), which performs\nexceptionally well in the AAC task, as our baseline. Ex-\nperimental results demonstrate that SECap outperforms the\nHTSAT-BART model across all objective metrics. In the\nsubjective mean opinion score (MOS) test, the quality of\nspeech emotion captions generated by SECap surpasses that\nof human labels (i.e., 3.77 vs. 3.39 MOS score) and are on\npar with human annotations (i.e., 3.77 vs. 3.85 MOS score).\nOur main contributions are as follows:\n• We propose the task of Speech Emotion Captioning\n(SEC), which, to our knowledge, stands among the pi-\noneering efforts to characterize speech emotions using\nnatural language.\n• We introduce SECap 1 to tackle the SEC task, which\ncomprises a HuBERT-based audio encoder, a Q-Former-\nbased Bridge-Net, and a LLaMA-based text decoder.\n• Experimental results show that SECap is capable of gen-\nerating suitable and fluent speech emotion captions that\nare on par with human-labeled speech emotion captions.\n1Codes, models and results: https://github.com/thuhcsi/SECap\nRelated Work\nSpeech Emotion Recognition\nSpeech Emotion Recognition (SER) entails detecting and\nclassifying emotions in spoken language, ultimately cate-\ngorizing them into specific labels. From the perspective of\npattern recognition, SER (Khalil et al. 2019) can be divided\ninto three components: feature extraction, feature selection,\nand feature classification. Extracted features include Mel-\nFrequency Cepstral Coefficients (MFCC) (Dahake, Shaw\net al. 2016), and Linear Predictive Coding (LPC) (Chat-\ntopadhyay et al. 2020), among others. Owing to the ad-\nvancement of deep learning, feature classifiers have evolved\nfrom methods like Linear Discriminant Analysis (LDA) (Liu\net al. 2018) to neural network architectures such as CNN and\nTransformer (Vaswani et al. 2017). Our approach describes\nspeech emotions with natural sentences rather than confin-\ning them to specific categories.\nLarge Language Model\nLarge Language Models (LLMs) (Singhal et al. 2023; Ko-\njima et al. 2022; Black et al. 2022) have revolutionized nat-\nural language understanding. By analyzing vast textual data,\nLLMs learn linguistic patterns and generate natural prose.\nOpen-source models like BLOOM (Scao et al. 2022) and\nChatGLM (Du et al. 2022) have fostered growth in the LLM\ncommunity. Also, researchers explore LLMs’ performance\nin multimodal interactions, aspiring for models capable of\nmanaging audio, vision, and text modalities, reflecting hu-\nman daily interactions. The first approach uses LLMs as\ntask orchestrators, connecting downstream models like Au-\ndioGPT (Huang et al. 2023) and HuggingGPT (Shen et al.\n2023) for specialized tasks. The second approach positions\nLLMs as multitask processors, mapping modal tasks to a\nunified space. For example, BLIP-2 maps images to text\nspace using Q-Former, while Video-LLaMA (Zhang, Li, and\nBing 2023) maps audio and vision modalities via Q-Former.\nAutomated Audio Captioning\nAutomated audio captioning (AAC) (Xu, Wu, and Yu 2022;\nKoh, Fuzhao, and Siong 2022) is a crucial task in the audio\ndomain, describing ambient sounds using natural language.\nUnlike audio tagging (Xu et al. 2017) or sound event detec-\ntion (Bilen et al. 2020), AAC requires identifying specific\nevents and describing them naturally. After the emergence\nof this task, the encoder-decoder (Sutskever, Vinyals, and\nLe 2014) framework has been the dominant solution to this\nproblem. Methods like AudioClip (Guzhov et al. 2022) and\nCLAP (Wu et al. 2023) employ contrastive learning to map\naudio and text, boosting the encoder-decoder connection.\nMethod\nInspired by the AAC task, SECap employs encoder-decoder\narchitecture, as illustrated in Figure 1. The audio encoder\nextracts speech features, and Bridge-Net extracts emotion-\nrelated speech features and transforms them into the text\ndecoder’s feature space. The text decoder then generates\nspeech emotion captions based on these features.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19324\nFigure 2: Framework of the proposed SECap\nIn this section, we will begin by providing an overview\nof the SECap structure. Following that, we will elaborate on\nthe two key aspects of the Bridge-Net design for obtaining\nemotion-related representations. Lastly, we will describe the\noverall training process of SECap.\nModel Architecture\nAs illustrated in Figure 2, SECap utilizes a HuBERT-\nbased audio encoder, a Q-Former-based Bridge-Net, and a\nLLaMA-based text decoder.\nThe HuBERT is to derive speech embedding for its pow-\nerful speech feature extraction capability. However, frame-\nlevel HuBERT features can lead to heavy computation cost.\nWe employ Q-Former-based Bridge-Net to compress fea-\ntures. Meanwhile, acoustic information is more directly as-\nsociated with speech emotion, while content information is\nobtainable from transcriptions. Thus, the Bridge-Net is used\nto extract emotion-related acoustic information and elimi-\nnate content information. We employ LLaMA as the text\ndecoder for generating speech emotion captions, leveraging\nits exceptional language comprehension capabilities. Align-\ning with LLaMA’s input format, we position L-Embedding\nbetween the “BOS” and a prompt. This method constrains\nLLaMA’s output space via the prompt, yielding more accu-\nrate speech emotion captions.\nQ-Former Owing to the redundancy of HuBERT speech\nfeatures, Q-Former is designed and adopted to compress and\nextract emotion-related speech features which consists of\nself-attention, cross-attention, and linear layers. Q-queries\nare learnable parameters for extracting speech embedding.\nLet q ∈ Rnq×dq represent the Q-queries, where nq is the\nnumber of Q-queries, dq is the dimension of Q-queries and\nS ∈ Rns×Ts×ds represent the speech embedding, where ns\nis the batch size, Ts is the number of time steps, and ds is\nthe dimension of speech embedding. We first input the Q-\nqueries q ∈ Rnq×dq into the self-attention mechanism:\nAself = softmax\n\u0012qWqself (qWkself )T\n√dk\n\u0013\nqWvself (1)\nwhere Wqself ∈ Rdq×dk , Wkself ∈ Rdq×dk , and Wvself ∈\nRdq×dv are the learnable weight matrices for queries, keys,\nand values in the self-attention mechanism, and dk and dv\nare the dimensions of keys and values. The output of the self-\nattention mechanism Aself ∈ Rnq×dv are then used as the\nqueries for the cross-attention mechanism, while the speech\nembedding S ∈ Rns×Ts×ds serve as keys and values:\nAcross = softmax\n\u0012AselfWq(SWk)T\n√dk\n\u0013\nSWv (2)\nwhere Across ∈ Rns×nq×dv represents the cross-attention\noutput, while Wq ∈ Rdv×dk , Wk ∈ Rds×dk , and Wv ∈\nRds×dv are the learnable weight matrices for queries, keys,\nand values in the cross-attention mechanism.\nThis approach enables the attention mechanism to retrieve\nfeatures related to Q-queries within the speech embedding.\nSpecifically, the output of the Q-Former, denoted as the Q-\nEmbedding Qe ∈ Rns×nq×dq , maintains a fixed length that\nis independent of the length of the input speech. This fixed-\nlength representation leads to improved generalization per-\nformance across speech inputs of varying lengths.\nObtain Emotion-Related Representations\nTo provide LLaMA with more content-unrelated and\nemotion-related speech features, we simultaneously incor-\nporate both the human-labeled speech emotion captions and\nthe transcriptions. As depicted in Figure 3, these are passed\nthrough a Q-Former that is largely consistent with the orig-\ninal, except for the absence of the cross-attention module.\nThis process yields C-Embedding Qc ∈ Rns×Tc×dq and T-\nEmbedding Qt ∈ Rns×Tt×dq , where Tc and Tt denote the\nlength of the caption and the transcription, respectively. We\nemploy Speech-Transcription Mutual Information Learning\nto disentangle speech features from speech content. Addi-\ntionally, Speech-Caption Contrastive Learning is utilized to\nextract more emotion-related speech features.\nSpeech-Transcription Mutual Information Learning\n(STMIL) The speech content can potentially impact emo-\ntion assessment, for example, expressing joyous statements\ncalmly. To minimize the correlation between speech features\nand content, thereby mitigating the speech content’s impact\non LLaMA’s speech emotion caption generation, we propose\nSpeech-Transcription Mutual Information Learning. As il-\nlustrated in Figure 3, we introduce both Speech Embed-\nding and Trans Embedding into the Q-Former simultane-\nously, yielding Q-EmbeddingQe and T-EmbeddingQt. This\nenables the comparison of speech and its content within a\nunified representation space. To evaluate the correlation be-\ntween Qe and Qt, we adopt mutual information I(Qt; Qe)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19325\nFigure 3: The figure presents Q-Former decoupling audio representation and content information using Speech-Transcription\nMutual Information Learning with speech features (Q-Embedding) and speech transcription features (T-Embedding). Addition-\nally, it obtains more emotion-related audio representation through Speech-Caption Contrastive Learning with speech features\n(Q-Embedding) and speech emotion caption features (C-Embedding).\nas the metric:\nI(Qt; Qe) =\nX\nqt∈Qt\nX\nqe∈Qe\np(qt, qe) log p(qt, qe)\np(qt)p(qe) (3)\nwhere p(qt, qe) represents the joint probability distribution\nof Qt and Qe, and p(qt) and p(qe) denote the marginal prob-\nability distributions of Qt and Qe, respectively.\nHowever, direct computation of mutual information be-\ntween Qe and Qt is infeasible due to their unknown, high-\ndimensional nature. While prior methods such as MINE\n(Belghazi et al. 2018) and infoNCE (Van Den Oord, Vinyals\net al. 2017) can estimate the lower bound of mutual infor-\nmation, they are not suitable for controlling the minimiza-\ntion process. Following vCLUB (Cheng et al. 2020), we use\nEquation (4) to estimate the upper bound of mutual informa-\ntion and employ it as a loss function to reduce the correlation\nbetween speech features and content.\nU(Qt; Qe) = 1\nn2\nnX\ni=1\nnX\nj=1\n\u0014\nlog q(yi|xi)\nq(yj|xi)\n\u0015\n(4)\nThe equation includes conditional probabilities q(yi|xi) and\nq(yj|xi), representing the probabilities of the i-th and j-th\nQe samples given the i-th Qt sample. The logarithm cap-\ntures the dissimilarity between Qe conditioned on Qt, and\nsumming over all pairwise combinations provides the upper\nbound mutual information measure between Qe and Qt.\nSpeech-Caption Contrastive Learning (SCCL) Due to\nthe high dimensionality and redundancy in speech represen-\ntations, speech features contain abundant information such\nas content and background noise, with only a fraction be-\ning emotion-related. To alleviate the complexity of process-\ning speech features by LLaMA, we aim for Q-Former to ex-\ntract features highly correlated with speech emotion caption,\nconsequently bridging the gap between speech features and\ntext modality. As illustrated in Figure 3, our objective is to\nminimize the distance between Qe and C-Embedding Qc,\nprompting Q-Former to extract more emotion-related fea-\ntures and progressively approach the text modality. Drawing\ninspiration from CLAP (Wu et al. 2023), we employ a con-\ntrastive learning approach to accurately represent distances\nbetween Qe of distinct speech samples, ensuring that speech\nwith similar emotions yield closer Qe distances, while those\nwith dissimilar emotions result in farther Qe distances.\nTo mitigate the influence of similar emotions in nega-\ntive samples during contrastive learning, we partition the\ndataset into N distinct categories based on human-labeled\nspeech emotion labels. This guarantees substantial differ-\nences in speech emotion captions across categories, thereby\nenhancing the model’s discriminative capacity throughout\nthe learning process. During each training step, we select\nK speech-caption pairs from each of the N sets, ensuring\nthat for each Qe (referred to as ei), there is 1 corresponding\nQc (referred to as di),(K −1) Qc with similar emotions (re-\nferred to as pi), and (NK −K) Qc with dissimilar emotions\n(referred to as ui).\nWe opt to use cosine similarity S to measure the distance\nbetween Qe and Qc. For enhanced contrastive learning, we\ndesign the training method as follows:\nL(Qc; Qe) =\nNKX\ni=1\nh\nw1(1 − S(ei, di)) +w2\nK−1X\nj=1\n(1−\nS(ei, pij)) +w3\nNK−KX\nj=1\nReLU(S(ei, uij) − m)\ni\n(5)\nwhere the weighting coefficients w1, w2, and w3 con-\ntrol the contribution of each term in the loss function. The\nthreshold value m is the margin to control the distance be-\ntween speech feature Qe and irrelevant speech emotion cap-\ntion feature Qc.\nTraining Process\nTo enhance speech emotion caption generation with\nLLaMA, we devise a two-stage training process. The first\nstage compresses HuBERT-extracted speech features to ob-\ntain emotion-relevant attributes, while the subsequent stage\naligns these features with LLaMA’s representation space.\nIn the first training stage, we combine STMIL and SCCL\nfor collaborative training as in Figure 3, while keeping the\nHuBERT model frozen. Inspired by BLIP-2, we initialize\nthe Q-Former using pre-trained parameters from BERT base\n(Devlin et al. 2019). Specifically, the training loss is:\nLT1 = wT1 × U(Qt; Qe) +wT2 × L(Qc; Qe) (6)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19326\nwhere the weighting coefficients wT1 and wT2 control the\ncontribution of STMIL and SCCL.\nIn the second training stage, we fine-tune the Q-Former\nand the projection layer to effectively integrate Q-Former-\nextracted speech features into LLaMA. Meanwhile, the pa-\nrameters of LLaMA and HuBERT remain frozen. We insert\na “BOS” token before the L-Embedding to align with the in-\nference format. To improve SECap’s generalization ability,\nwe devise 30 semantically akin sentences, each instructing\nto “portray the speaker’s emotion in a single Chinese sen-\ntence.” During training, we randomly choose a sentence to\nconcatenate after the L-Embedding. Subsequently, we ap-\npend the human-labeled speech caption C after the prompt\nand employ the teacher-forcing approach to enable LLaMA\ngenerate caption ˆC. Cross-entropy loss (CELoss) is then\nadopted as the training objective:\nLT2 = CELoss(C, ˆC) (7)\nDataset\nDue to the lack of publicly available SEC datasets, we utilize\nan internal dataset called EMOSpeech. EMOSpeech dataset\nconsists of 5 female and 2 male speakers, totaling 41.6 hours\nof speech covering 30526 sentences, sampled at a rate of\n24kHz. Each speech in EMOSpeech has three to five human-\nlabeled speech emotion captions and human-labeled speech\nemotion labels provided by different annotators, along with\nits corresponding transcription.\nAs for labeling, we begin with 50 sample audio clips for\nindependent annotator labeling and hold a discussion ses-\nsion for annotators to review annotations and establish stan-\ndardized rules based on collective input. The annotation pro-\ncess has three levels: identifying overall emotion using a\nsingle word, describing emotion intensity, and providing a\ncomprehensive sentence considering emotion, volume, and\nspeech rate. With these guidelines, annotators consistently\nlabel the dataset. To ensure annotation quality, we conduct\nconsistency checks by randomly selecting 5 out of every 100\nclips for review by other annotators, upholding high stan-\ndards throughout the dataset construction.\nUpon constructing the EMOSpeech dataset, we randomly\nselect 600 sentences for testing, 600 sentences for validation,\nand the remaining 29,326 sentences for training2.\nEvaluation Metric\nSince there is no existing method to evaluate speech emotion\ncaptions, we devise both objective and subjective evaluation\nmethods based on the nature of the SEC task.\nObjective Evaluation\nIn this study, we initially adopt objective evaluation metrics\nfor the AAC task, containing BLEU1 (Papineni et al. 2002),\nBLEU4, METEOR (Banerjee and Lavie 2005), ROUGE l\n(Lin 2004), CIDEr (Wang and Chan 2019), and SPICE (An-\nderson et al. 2016). However, these metrics primarily focus\n2Please refer to project’s GitHub repository for detailed dataset\nconstruction process, where the test set is also publicly available.\non word-level matching. To more effectively assess the sim-\nilarity between two Chinese emotion captions at the sen-\ntence level, we employ sentence similarity evaluation met-\nrics in conjunction with the aforementioned criteria. The first\nmodel (Ming 2022) is based on MACBERT (Cui et al. 2021)\nand trained on Chinese STS-B (Cer et al. 2017), while the\nsecond model (Reimers and Gurevych 2019) is finetuned on\nTencent Cloud. Their evaluation indicators are denoted as\nSIM1 and SIM2, respectively.\nSubjective Evaluation\nIn the subjective scoring method, we develop a three-stage\nscoring criterion to reduce variability due to evaluators’ in-\nconsistent understanding of emotions. The first step involves\ndetermining whether the generated sentence describes an\nemotion. The second step assesses if the generated sentence,\nwhen summarized into an emotion, matches the speech. The\nthird step evaluates whether the generated sentence aligns\nwith the speech in terms of the emotion’s intensity.\nTo be specific, we have devised a scoring method similar\nto the Mean Opinion Score (MOS) used in Text-to-Speech\nsystems, with ratings ranging from 1 to 5, where 1 represents\nthe worst and 5 represents the best.\nResults and Analysis\nExperiment Setup\nOur experiments are conducted exclusively on the EMO-\nSpeech dataset. We choose the HuBERT-large model, pre-\ntrained on the 10k-hour WenetSpeech (Zhang et al. 2022) L\nsubset, as the audio encoder. Due to the original LLaMA’s\nlimited proficiency in understanding Chinese, we choose an\nenhanced version of LLaMA (Cui, Yang, and Yao 2023)\nfinetuned with Chinese datasets as the text decoder3.\nTrainable Params Total Params\nTraining stage 1 100M 517M\nTraining stage 2 103M 7.4B\nTable 1: The number of trainable parameters and the total\nparameters during the two stages of the training process\nPerformance Analysis\nThis experiment aims to demonstrate the effectiveness of\nSECap. Given speech content’s impact on emotion, we\nincorporate transcriptions as additional input and design\nmultiple comparison groups for thorough analysis. Specif-\nically, when incorporating transcriptions, we either use the\nraw transcriptions processed through the LLaMA tokenizer\nor apply the T-Embedding processed through a projection\nlayer as an extra input, which is then concatenated between\nBOS Emb and L-Embedding. Subsequently, we append the\nPrompt Emb and feed it into LLaMA. In some of the com-\nparison groups, speech features are not used, the former\nprocessed transcriptions are directly concatenated between\nBOS Emb and Prompt Emb.\n3Specific experimental details are given at GitHub repository.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19327\nModel ID Input Modality SIM1 SIM2 BLEU1 BLUE4 METEOR ROUGE l CIDEr SPICEText Audio\nHTSAT-BART #1 — Q-Emb 59.62 53.19 32.74 3.05 14.61 23.64 2.21 2.17\nSECap\n#2 Raw Trans — 56.49 22.38 0.014 0.00 2.67 4.38 0.00 0.00\n#3 T-Emb — 65.90 62.24 25.97 4.28 15.98 23.37 18.37 2.58\n#4 Raw Trans Q-Emb 69.24 67.50 29.59 5.36 16.99 25.16 28.51 5.77\n#5 T-Emb Q-Emb 69.66 70.02 33.62 7.25 18.44 27.18 33.82 5.96\n#6 — Q-Emb 71.95 70.51 36.08 8.12 19.30 28.49 34.81 6.49\nTable 2: Objective experiment results on performance analysis. “Raw Trans” denotes raw transcription of speech.“Q-Emb”\ndenotes Q-Embedding.“T-Emb” denotes T-Embedding. “—” refers to not included. Higher scores mean better performance.\nObjective Evaluation Table 2 illustrates consistent trends\nin most metrics across various experiments. Therefore, our\nanalysis will primarily focus on the insights obtained from\nthe two Chinese sentence similarity models.\nAs depicted in Table 2, when incorporating only speech\nfeatures, the proposed SECap (#6) surpasses the HTSAT-\nBART (#1) baseline across all objective metrics, signifying\nits ability to generate more natural, human-like speech emo-\ntion captions compared to the HTSAT-BART model.\nCompared to raw transcription (#2), employing T-\nEmbedding (#3) improves SIM values by 16.66% and\n178.11%. Since LLaMA has not been previously trained\non the EMOSpeech dataset, it lacks prior knowledge of the\ndataset’s captions, resulting in unconstrained output space.\nHowever, T-Embedding imposes greater constraints, extract-\ning more emotion-related features, and resulting in relatively\naccurate speech emotion captions.\nUsing only Q-Embedding (#6), we observe a 9.18% and\n13.29% SIM value increase compared to relying solely on T-\nEmbedding (#3). As identical sentences can convey different\nemotions, relying only on speech content (i.e. transcription)\nmay not sufficiently reflect speech emotions, while speech\nsignals better represent speech emotions. Upon incorporat-\ning Q-Embedding and the raw transcription (#4), the SIM\nvalues relatively decrease by 3.77% and 4.27% compared to\nonly using Q-Embedding (#6). However, replacing the raw\ntranscription (#4) with T-Embedding (#5) shows a relative\nimprovement of 0.61% and 3.73% in SIM values. Despite\nthis increase, the SIM values remain lower than those ob-\ntained when using only Q-Embedding (#6).\nConsistent with text-only modality, T-Embedding out-\nperforms raw transcription in extracting emotion features\nfrom transcription, providing greater constraints for LLaMA\nwhile reducing conflicts with Q-Embedding. However, inte-\ngrating both audio and text modalities into the model may\nincrease the difficulty for LLaMA in processing the infor-\nmation, as text and audio could contain similar, unrelated,\nor even contradictory information. Consequently, LLaMA\nmust balance these features, potentially impeding its ability\nto optimally leverage information from both modalities and\naffecting the model’s assessment of speech emotion.\nSubjective Evaluation In the subjective experiment, we\nrandomly select a test set of 50 sentences. We apply all the\nmethods listed in Table 2 to generate corresponding speech\nemotion captions. To enable a more comprehensive compar-\nFigure 4: Subjective experiment results on performance\nanalysis\nison, we also include human-labeled speech emotion labels\n(Human Label) and emotion categories (SER Model Label)\nidentified by a competitive pre-trained Chinese SER model\nfrom HuggingFace4. We request evaluators to score the nine\ntypes of texts according to the Subjective Evaluation details\nprovided in Evaluation Metric section. 15 evaluators partic-\nipate in the tests, with the results presented in Figure 4.\nAs seen in Figure 4, human-labeled speech emotion cap-\ntions surpass both human-labeled speech emotion labels and\nSER Model Labels. This outcome aligns with the SEC task’s\naim to represent emotions more comprehensively and ac-\ncurately within a single sentence. Notably, the best SECap\nmodel outperforms human-labeled speech emotion labels\nand is on par with human-labeled speech emotion captions.\nFurthermore, Figure 4 reveals that SECap’s performance\nis suboptimal with solely raw transcription. However, other\nSECap input methods outperform human-labeled emotion\nlabels and the baseline in subjective evaluation metrics,\ndemonstrating SECap’s ability to generate suitable speech\nemotion captions deemed more representative of emotions.\nHowever, employing both Q-Embedding and T-Embedding\nas input generates superior subjective evaluation outcomes\ncompared to using solely Q-Embedding.\nWe suppose that objective metrics, relying on predefined\nrules, may differ from subjective evaluations based on hu-\nman perception and understanding. Discrepancies can arise\n4https://huggingface.co/xmj2002/hubert-base-ch-speech-\nemotion-recognition\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19328\nModel SIM1Audio Encoder Bridge-Net Text Decoder\nHTSAT Linear BART 59.62±0.22\nHuBERT Linear BART 63.95±0.27\nHTSAT Linear LLaMA 64.94±0.06\nHuBERT Linear LLaMA 68.62±0.32\nHuBERT Q-Former LLaMA 71.95±0.04\nTable 3: Experiment results on the ablation study of different\nmodel components\nas evaluators focus on intricate details, such as emotional\nexpression’s naturalness and contextual information, which\nare challenging for objective metrics to capture. Addition-\nally, we observed that evaluators initially concentrated on\nspeech content. In cases of content-emotion conflicts, evalu-\nators tended to assign higher scores to content-relevant cap-\ntions. For example, uttering “I’m feeling terrible today” in a\nflat tone with only speech embedding might yield a caption\ndescribing the flat tone, while incorporating text embedding\ncould result in a caption combining sadness and flatness.\nAblation Study on Different Model Components\nThis experiment aims to explore the effect of different model\ncomponents on generating speech emotion captions. Con-\nsidering that HTSAT-BART differs from SECap in both the\naudio encoder, the Bridge-Net, and the text decoder, to bet-\nter analyze each component, we construct the model using\ndifferent audio encoders, text decoders, and Bridge-Nets, re-\nspectively. In the previous experiment, we find that the two\ntext similarity models exhibit the same trend despite differ-\nent experiments. Therefore, in this experiment as well as\nsubsequent experiments, we only use the first text similar-\nity model introduced in the Evaluation Metric section for\nthe evaluation of objective indicators.\nTable 3 reveals that by replacing the text decoder with\nLLaMA and retaining the audio encoder, the HTSAT-\nBART’s SIM value increases by 8.92%, indicating LLaMA’s\nsuperior text generation capabilities compared to BART.\nSimilarly, substituting the audio encoder with HuBERT\nwhile maintaining the text decoder results in a 7.26% SIM\nvalue increase, suggesting HuBERT’s robust speech fea-\nture extraction abilities compared to HTSAT. Replacing both\ncomponents yields a 15.10% SIM value increase. Further-\nmore, exchanging the linear layer with Q-Former signifi-\ncantly enhances high-quality speech emotion caption gen-\neration, accompanied by a 4.85% SIM value increase.\nEvidently, HuBERT is more suitable for speech fea-\nture extraction compared to HTSAT, and LLaMA demon-\nstrates superior text comprehension and generation capabil-\nities compared to BART. By further extracting speech fea-\ntures using the Q-Former, speech features that better align\nwith emotional aspects can be conveyed to LLaMA, ulti-\nmately resulting in more accurate speech emotion captions.\nComparison of Training Methods\nThis experiment seeks to investigate the impact of various\ntraining methods for the Q-Former on generating speech\nMethod SIM1STMIL SCCL Freeze\nHTSAT-BART % % % 63.95±0.27\nSECap\n% % % 67.29±0.22\n! % % 68.75±0.12\n% ! % 69.40±0.11\n! ! % 71.95±0.04\n! ! ! 57.92±0.19\nTable 4: Experiment results on the effect of different meth-\nods of training the Q-Former\nemotion captions. While maintaining the audio encoder as\nHuBERT and the text decoder as LLaMA, we conduct a\nseries of comparisons, including whether to use STMIL or\nSCCL in the first training stage, and whether to freeze the\nQ-Former in the second training stage.\nTable 4 indicates that incorporating STMIL or SCCL indi-\nvidually during the first training stage results in a 2.17% and\n3.14% SIM value increase, respectively, compared to omit-\nting this stage, suggesting that disentangling content infor-\nmation or extracting extra emotion-related speech features\nenhances caption quality. Moreover, using both methods si-\nmultaneously yields a 6.92% SIM value increase. Without\nboth STMIL and SCCL, Q-Former lacks speech feature in-\nsight, and limited EMOSpeech risks overfitting. Compared\nto employing either method alone, employing both STMIL\nand SCCL leads to the rise of SIM values by 4.65% and\n3.67%, respectively, highlighting that concurrent utilization\nof both methods bolsters speech feature extraction, generat-\ning more precise speech emotion captions.\nUpon finalizing the initial training phase, a 19.50% SIM\nvalue reduction is observed by freezing Q-Former and solely\ntraining the projection layer, compared to the unfrozen\nQ-Former. As LLaMA lacks involvement in guiding Q-\nFormer’s training, the extracted features might inadequately\nalign with LLaMA’s input, and the projection layer’s adapt-\nability fails to offset this misalignment.\nConclusion\nTo better represent speech emotions, we introduce an in-\nnovative task called speech emotion captioning, which\nuses natural language descriptions rather than singular\nlabels to characterize speech emotions. Our proposed\nmodel, SECap, integrates a HuBERT-based audio encoder, a\nLLaMA-based text decoder, and a Q-Former-based Bridge-\nNet. The Q-Former effectively disentangles speech fea-\ntures and speech content information through Speech-\nTranscription Mutual Information Learning, while extract-\ning more emotion-related speech features via Speech-\nCaption Contrastive Learning. Impressively, SECap gener-\nates high-quality speech emotion captions, with its perfor-\nmance on par with human annotators. This pioneering task\nand method provide a fresh perspective on speech emotion\nunderstanding, fostering a more comprehensive approach to\nanalyzing and interpreting speech emotional expressions.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19329\nAcknowledgements\nThis work is supported by National Natural Science\nFoundation of China (62076144), Shenzhen Science\nand Technology Program (WDZC20220816140515001,\nJCYJ20220818101014030) and Tencent AI Lab Rhino-Bird\nFocused Research Program (RBFR2023015).\nReferences\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. Spice: Semantic propositional image caption evalua-\ntion. In Computer Vision–ECCV 2016: 14th European Con-\nference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part V 14, 382–398. Springer.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, 65–72.\nBelghazi, M. I.; Baratin, A.; Rajeshwar, S.; Ozair, S.; Ben-\ngio, Y .; Courville, A.; and Hjelm, D. 2018. Mutual infor-\nmation neural estimation. In International conference on\nmachine learning, 531–540. PMLR.\nBilen, C ¸ .; Ferroni, G.; Tuveri, F.; Azcarreta, J.; and\nKrstulovi´c, S. 2020. A framework for the robust evalua-\ntion of sound event detection. In ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 61–65. IEEE.\nBlack, S.; Biderman, S.; Hallahan, E.; Anthony, Q.; Gao, L.;\nGolding, L.; He, H.; Leahy, C.; McDonell, K.; Phang, J.;\net al. 2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nCer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,\nL. 2017. SemEval-2017 Task 1: Semantic Textual Similar-\nity Multilingual and Crosslingual Focused Evaluation. In\nProceedings of the 11th International Workshop on Seman-\ntic Evaluation (SemEval-2017), 1–14. Vancouver, Canada:\nAssociation for Computational Linguistics.\nChattopadhyay, S.; Dey, A.; Basak, H.; et al. 2020. Opti-\nmizing speech emotion recognition using manta-ray based\nfeature selection. arXiv preprint arXiv:2009.08909.\nChen, K.; Wu, Y .; Wang, Z.; Zhang, X.; Nian, F.; Li, S.; and\nShao, X. 2020. Audio Captioning Based on Transformer and\nPre-Trained CNN. In DCASE, 21–25.\nCheng, P.; Hao, W.; Dai, S.; Liu, J.; Gan, Z.; and Carin, L.\n2020. Club: A contrastive log-ratio upper bound of mutual\ninformation. In International conference on machine learn-\ning, 1779–1788. PMLR.\nCui, Y .; Che, W.; Liu, T.; Qin, B.; and Yang, Z. 2021.\nPre-training with whole word masking for chinese bert.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 29: 3504–3514.\nCui, Y .; Yang, Z.; and Yao, X. 2023. Efficient and effective\ntext encoding for chinese llama and alpaca. arXiv preprint\narXiv:2304.08177.\nDahake, P. P.; Shaw, K.; et al. 2016. Speaker depen-\ndent speech emotion recognition using MFCC and Sup-\nport Vector Machine. In 2016 International Conference on\nAutomatic Control and Dynamic Optimization Techniques\n(ICACDOT), 1080–1084.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335.\nEl Ayadi, M.; Kamel, M. S.; et al. 2011. Survey on speech\nemotion recognition: Features, classification schemes, and\ndatabases. Pattern recognition, 44(3): 572–587.\nGuzhov, A.; Raue, F.; Hees, J.; and Dengel, A. 2022. Au-\ndioclip: Extending clip to image, text and audio. In ICASSP\n2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 976–980. IEEE.\nHan, Q.; Yuan, W.; Liu, D.; Li, X.; and Yang, Z. 2021.\nAutomated Audio Captioning with Weakly Supervised Pre-\nTraining and Word Selection Methods. In DCASE, 6–10.\nHsu, W.-N.; Bolte, B.; Tsai, Y .-H. H.; Lakhotia, K.;\nSalakhutdinov, R.; and Mohamed, A. 2021. Hubert: Self-\nsupervised speech representation learning by masked pre-\ndiction of hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29: 3451–3460.\nHuang, R.; Li, M.; Yang, D.; Shi, J.; Chang, X.; Ye, Z.; Wu,\nY .; Hong, Z.; Huang, J.; Liu, J.; et al. 2023. Audiogpt: Un-\nderstanding and generating speech, music, sound, and talk-\ning head. arXiv preprint arXiv:2304.12995.\nJiang, P.; Fu, H.; Tao, H.; Lei, P.; and Zhao, L. 2019. Paral-\nlelized convolutional recurrent neural network with spectral\nfeatures for speech emotion recognition. IEEE Access, 7:\n90368–90377.\nKhalil, R. A.; Jones, E.; Babar, M. I.; Jan, T.; Zafar, M. H.;\nand Alhussain, T. 2019. Speech emotion recognition us-\ning deep learning techniques: A review. IEEE Access, 7:\n117327–117345.\nKoh, A.; Fuzhao, X.; and Siong, C. E. 2022. Automated\naudio captioning using transfer learning and reconstruction\nlatent space similarity regularization. In ICASSP 2022-2022\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 7722–7726. IEEE.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. Blip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19330\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, Z.-T.; Xie, Q.; Wu, M.; Cao, W.-H.; Mei, Y .; and Mao,\nJ.-W. 2018. Speech emotion recognition based on an im-\nproved brain emotion learning model. Neurocomputing,\n309: 145–156.\nMei, X.; Meng, C.; Liu, H.; Kong, Q.; Ko, T.; Zhao, C.;\nPlumbley, M. D.; Zou, Y .; and Wang, W. 2023. Wavcaps:\nA chatgpt-assisted weakly-labelled audio captioning dataset\nfor audio-language multimodal research. arXiv preprint\narXiv:2303.17395.\nMing, X. 2022. text2vec: A Tool for Text to Vector.\nMohamed, O.; and Aly, S. A. 2021. Arabic speech emo-\ntion recognition employing wav2vec2. 0 and hubert based\non baved dataset. arXiv preprint arXiv:2110.04425.\nNwe, T. L.; Foo, S. W.; and De Silva, L. C. 2003. Speech\nemotion recognition using hidden Markov models. Speech\ncommunication, 41(4): 603–623.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Inui,\nK.; Jiang, J.; Ng, V .; and Wan, X., eds., Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, 3980–3990.\nAssociation for Computational Linguistics.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn´e, R.; Luccioni, A. S.; Yvon, F.; Gall´e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nShen, Y .; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang,\nY . 2023. Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\net al. 2023. Large language models encode clinical knowl-\nedge. Nature, 1–9.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to\nSequence Learning with Neural Networks. In Ghahramani,\nZ.; Welling, M.; Cortes, C.; Lawrence, N. D.; and Wein-\nberger, K. Q., eds.,Advances in Neural Information Process-\ning Systems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Montreal,\nQuebec, Canada, 3104–3112.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nVan Den Oord, A.; Vinyals, O.; et al. 2017. Neural dis-\ncrete representation learning. Advances in neural informa-\ntion processing systems, 30.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Q.; and Chan, A. B. 2019. Describing like hu-\nmans: on diversity in image captioning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 4195–4203.\nWu, Y .; Chen, K.; Zhang, T.; Hui, Y .; Berg-Kirkpatrick, T.;\nand Dubnov, S. 2023. Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-\ncaption augmentation. In ICASSP 2023-2023 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 1–5. IEEE.\nXu, X.; Wu, M.; and Yu, K. 2022. A comprehensive\nsurvey of automated audio captioning. arXiv preprint\narXiv:2205.05357.\nXu, Y .; Huang, Q.; Wang, W.; Foster, P.; Sigtia, S.; Jackson,\nP. J.; and Plumbley, M. D. 2017. Unsupervised feature learn-\ning based on deep models for environmental audio tagging.\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 25(6): 1230–1241.\nYe, Z.; Wang, H.; Yang, D.; and Zou, Y . 2021. Improving\nthe Performance of Automated Audio Captioning via Inte-\ngrating the Acoustic and Semantic Information. In Font,\nF.; Mesaros, A.; Ellis, D. P. W.; Fonseca, E.; Fuentes, M.;\nand Elizalde, B., eds., Proceedings of the 6th Workshop on\nDetection and Classification of Acoustic Scenes and Events\n2021 (DCASE 2021), Online, November 15-19, 2021, 40–\n44.\nZhang, B.; Lv, H.; Guo, P.; Shao, Q.; Yang, C.; Xie, L.;\nXu, X.; Bu, H.; Chen, X.; Zeng, C.; et al. 2022. Wenet-\nspeech: A 10000+ hours multi-domain mandarin corpus for\nspeech recognition. In ICASSP 2022-2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), 6182–6186. IEEE.\nZhang, H.; Li, X.; and Bing, L. 2023. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding. arXiv preprint arXiv:2306.02858.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19331",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8669109344482422
    },
    {
      "name": "Computer science",
      "score": 0.5885266065597534
    },
    {
      "name": "Speech recognition",
      "score": 0.45554155111312866
    },
    {
      "name": "Linguistics",
      "score": 0.4340091347694397
    },
    {
      "name": "Natural language processing",
      "score": 0.4120092988014221
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2683897912502289
    },
    {
      "name": "Philosophy",
      "score": 0.040106773376464844
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3131625388",
      "name": "University Town of Shenzhen",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    }
  ],
  "cited_by": 24
}