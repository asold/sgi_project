{
  "title": "Large language model enhanced corpus of CO2 reduction electrocatalysts and synthesis procedures",
  "url": "https://openalex.org/W4394013963",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2120035901",
      "name": "Xueqing Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2065795670",
      "name": "Gao Yang",
      "affiliations": [
        "National Center for Nanoscience and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2275644038",
      "name": "Ludi Wang",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2078532592",
      "name": "Wenjuan Cui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2127112418",
      "name": "Jiamin Huang",
      "affiliations": [
        "National Center for Nanoscience and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098036377",
      "name": "Yi Du",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences",
        "Institute for Advanced Study",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1995576685",
      "name": "Bin Wang",
      "affiliations": [
        "National Center for Nanoscience and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120035901",
      "name": "Xueqing Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2065795670",
      "name": "Gao Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2275644038",
      "name": "Ludi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078532592",
      "name": "Wenjuan Cui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127112418",
      "name": "Jiamin Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098036377",
      "name": "Yi Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995576685",
      "name": "Bin Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2972593701",
    "https://openalex.org/W3025104221",
    "https://openalex.org/W4380434108",
    "https://openalex.org/W1992591486",
    "https://openalex.org/W2888253280",
    "https://openalex.org/W2884430236",
    "https://openalex.org/W4292939109",
    "https://openalex.org/W3036481679",
    "https://openalex.org/W3047398431",
    "https://openalex.org/W4205394401",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W4361298520",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W4353015365",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W6600329640",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W3015467311",
    "https://openalex.org/W4281559792",
    "https://openalex.org/W2961421843",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W274251997",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W1524134026",
    "https://openalex.org/W2121244856",
    "https://openalex.org/W3043647281",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W2251329024",
    "https://openalex.org/W6954632539",
    "https://openalex.org/W6954887340",
    "https://openalex.org/W6955453396",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2964090065",
    "https://openalex.org/W6604896550",
    "https://openalex.org/W1973406306",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4394013963"
  ],
  "abstract": null,
  "full_text": "1Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdata\nLarge language model enhanced \ncorpus of CO2 reduction \nelectrocatalysts and synthesis \nprocedures\nXueqing Chen  1,2,5, Yang Gao  3,5, Ludi Wang  1,5, Wenjuan Cui  1, Jiamin Huang3, Yi Du  1,2,4 ✉  \n& Bin Wang  3 ✉\nCO2 electroreduction has garnered significant attention from both the academic and industrial \ncommunities. Extracting crucial information related to catalysts from domain literature can help \nscientists find new and effective electrocatalysts. Herein, we used various advanced machine learning, \nnatural language processing techniques and large language models (LLMs) approaches to extract \nrelevant information about the CO2 electrocatalytic reduction process from scientific literature. By \napplying the extraction pipeline, we present an open-source corpus for electrocatalytic CO2 reduction. \nThe database contains two types of corpus: (1) the benchmark corpus, which is a collection of 6,985 \nrecords extracted from 1,081 publications by catalysis postgraduates; and (2) the extended corpus, \nwhich consists of content extracted from 5,941 documents using traditional NLP techniques and LLMs \ntechniques. The Extended Corpus I and II contain 77,016 and 30,283 records, respectively. Furthermore, \nseveral domain literature fine-tuned LLMs were developed. Overall, this work will contribute to the \nexploration of new and effective electrocatalysts by leveraging information from domain literature \nusing cutting-edge computer techniques.\nBackground & Summary\nCO2 electroreduction has garnered significant attention from both the academic and industrial communities, \nowing to its potential to effectively mitigate greenhouse gas emissions while simultaneously producing fuels and \nchemicals1–3. Its widespread adoption relies heavily on the development of efficient and reliable electrocatalysts. \nOver the past three decades, scientists have invested substantial efforts in the development of CO 2 reduction \nelectrocatalysts4,5; However, this trial-and-error approach has proven to be time-consuming and labor-intensive. \nConsequently, it becomes pivotal in accelerating catalyst development to establish a comprehensive database \nfor CO\n2 electroreduction, which should encompass various information pertaining to the composition, syn -\nthesis, regulation, and performance of catalysts. Given the substantial workload involved, the manual annota-\ntion method by domain experts is deemed unreasonable. In recent years, emerging artificial intelligence (AI) \ntechnologies have exhibited tremendous potential in facilitating the construction of realm-specific datasets\n6,7. \nExtracting crucial information related to catalysts from domain literature is the initial step toward accelerating \ncatalyst development using AI technologies. Traditionally, Named Entity Recognition (NER) methods have been \nemployed for text mining and information retrieval\n8–11. However, NER often necessitates the establishment of \nalgorithms tailored to specific tasks, which are typically undertaken by scientists or engineers with expertise \nin coding, data structures, and computer algorithms. Therefore, this approach is labor-intensive. Furthermore, \nNER algorithms are closely tied to their assigned tasks, lacking generalizable ability and thus making direct \ntransfer to other tasks challenging. Additionally, extracted information tends to be intricate, heterogeneous, and \ndiverse in the field of catalysis, leading to unsatisfied NER performance and reduced accuracy\n12. Therefore, the \n1Laboratory of Big Data Knowledge, computer network i nformation center, chinese Academy of Sciences, \nBeijing, 100083, China. 2University of Chinese Academy of Sciences, Beijing, 100049, China. 3cAS Key Laboratory \nof nanosystem and Hierarchical fabrication, national center for nanoscience and technology (ncnS t), Beijing, \n100190, China. 4Hangzhou Institute for Advanced Study, UCAS, Hangzhou, 310000, China. 5these authors \ncontributed equally: Xueqing chen, Yang Gao, Ludi Wang. ✉e-mail: duyi@cnic.cn; wangb@nanoctr.cn\nDaTa DESCr IPTOr\nOPEN\n\n2Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\ndevelopment and utilization of more general and robust methods for extracting domain knowledge are becom-\ning increasingly imperative.\nRecently, the emergence of large language models (LLMs), especially the widely acclaimed ChatGPT, has \nbrought new prospects to the field of NER tasks13. It can be effectively operated by domain scientists who may \nnot be well-versed in computer algorithms. However, ChatGPT is susceptible to information hallucinations, \na glaring issue that significantly undermines its reliability in scientific domains 14–16. Prompt engineering has \nproven to be a potential solution to mitigate the problem of artificial hallucinations 17–19. For instance, Zheng \net al. employed prompt engineering to guide ChatGPT in automating text mining for the synthesis conditions \nof metal-organic frameworks 17. Nevertheless, the utility of this approach for more diverse and complex tasks \nwithin the catalytic science domain remains an area warranting further exploration. Moreover, the high demand \nfor computing resources in LLMs also limits their application in various fields. The training and application of \nLLMs usually require a tremendous amount of computational power, which are not only expensive to purchase \nbut also consume substantial amounts of electricity.\nIn recent work, our team has developed a text-mining pipeline to construct a dataset describing the CO\n2 \nreduction process catalyzed by copper-based electrocatalysts, which specifically includes material, regulation \nmethod, product, Faradaic efficiency and relevant conditions\n12. In the current work, we built a more advanced \nextraction pipeline based on the knowledge system of CO 2 electrocatalytic reduction (Fig.  1), which uses var-\nious advanced machine learning, natural language processing techniques and large language models (LLMs) \napproaches to extract relevant information about the CO\n2 electrocatalytic reduction process from scientific \nliterature. In addition, for the purpose of providing a more detailed and complete guidance scheme for mate -\nrials scientists to develop new catalysts, we designed a set of synthesis actions with predefined properties and \na deep-learning sequence to sequence model based on the transformer architecture, which converts unstruc\n-\ntured experimental procedure text into structured action sequences. By applying the extraction pipeline, we \npresent an open-source corpus for electrocatalytic CO\n2 reduction. The database contains two types of corpus: \n(1) the benchmark corpus, which is a collection of 6,086 records extracted from 1,081 publications by cataly -\nsis postgraduates; and (2) the extended corpus, which consists of content extracted from the abstract of 5,941 \ndocuments using traditional natural language processing techniques and large language models techniques. \nRespectively, the Extended Corpus I contains 77,016 records and the Extended Corpus II contains 30,283 \nrecords. In addition, we extracted 476 synthesis procedures for catalytic materials from 2,176 full-text docu\n-\nments, and the extracted information includes target and preparation materials, synthesis operations and the \nquantity of materials involved in them, and operation properties. The Extended Corpus was evaluated and \nrevised by domain experts. This work provides a valuable resource to accelerate research into CO\n2 reduction by \nsupplying structured information and datasets ready for further analysis and hypothesis generation. The tools \nand datasets created could significantly reduce the time and resources required for literature review and data \ngathering, allowing scientists to focus on innovation and experimentation.\nMethods\nThe schematic overview of the extraction pipeline is shown in Fig. 1. We first searched the literature related to the \nelectrocatalytic CO2 reduction process following a series of filtering criteria. For scientific article retrieval and \npreprocessing, the raw archived corpus was parsed and organized in paragraphs. After paragraph classification, \nFig. 1 The schematic overview of dataset construction pipeline. (a) The process of literature search filtering and \nparagraph classification. (b) The top panel shows the schematic diagram of the standard text mining process: \n<i> expert annotation to build a baseline corpus; <ii> extraction of critical information from the literature \ntext and construction of an extended corpus; <iii> store in a database for future data mining. The bottom panel \nshows an example of converting a synthesis sentence into action sequences. The key components of an action \nsequence such as starting and target material, synthesis steps and their conditions are found and extracted from \nthe paragraph by different text mining algorithms (see Methods). (c) The entity types and their relationships \nextracted from the literature. The final constructed dataset can provide guidance for practical experimental work.\n3Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nthe paragraphs related to the concrete synthesis procedures were automatically selected. The extracted infor -\nmation includes the materials, the target products, their quantities as well as the synthesis operations and their \nattributes. We then constructed action sequences for each synthesis action in a predefined format. Finally based \non the the system of knowledge defined by domain experts, we published a manually annotated baseline corpus \nand an automatically annotated extended corpus. The final generated dataset can be used for domain data min\n-\ning and further downstream NLP tasks, as well as provide guidance to material domain scientists for practical \nexperimental work.\nContent acquisition.  Scientific publications used in this work are journal articles published by Elsevier, \nthe Royal Society of Chemistry, American Chemical Society, Wiley, Acta Physico-Chimica Sinica & University \nChemistry Editorial Office (Peking University), MDPI, the Electrochemical Society, Springer Nature, etc. For \neach publisher, the journals relevant to materials science were manually selected. We used regular expression \nmatching\n20 to obtain the dois of relevant literature in the field of CO2 electrocatalytic reduction. Specifically, we \nsearched and exported metadata for more than 27,000 articles by using the keywords “CO 2” , “Reduction” , and \n“Electro*” as subject indexes on the Web of Science website. The exported literature metadata was then filtered \nstep by step according to expert-defined rules. The title of every article was queried for words “CO 2” , “carbon \ndioxide” or “CO(2)” , which yielded 9,850 articles. The abstract of every article was queried for words “electroc” or \n“electror” , which yielded 6,973 articles. Finally the domain experts performed manual filtration to exclude arti-\ncles whose titles contained words that were not relevant to the topic, including: “photoc” , “light” , “visible” , “solar” , \n“microbial” , “bacteria” , “culture” , etc. we eventually obtained 5,941 summary texts of the literature related to the \nwork on CO\n2 electrocatalytic reduction and scraped the full text of 2,776 papers from the web. We finally acquired \nthe literature in PDF format and used the PyMuPDF tool, a PDF parsing tool 21, to automatically process these \nliterature data to obtain their metadata such as title, authors, abstract, etc. and the full text in json format. Since \nthe processed document contains irrelevant tags, we developed a data cleaning method for parsing the article tag \nstrings into consistently formatted text paragraphs while retaining the same chapter and paragraph structure as \nthe original paper.\nParagraph classification. We used the Transformers Bidirectional Encoder Representation (BERT) model \nto identify paragraphs containing descriptions of synthesis methods. MatBERT is a BERT model22 specifically for \nmaterial science texts, pre-trained on over 2 million papers in a self-supervised manner, i.e. by predicting masked \nwords based on the context around the target sentence. After training the BERT model, we used a paragraph clas-\nsification method based on semi-supervised learning\n23. First we applied latent Dirichlet allocation (LDA)24 on the \n12,643 articles in the field of photoelectrocatalysis to identify the experimental steps implicit in sentences. Then \nwe collected all the paragraphs from the literature and manually labelled the paragraphs describing the synthesis \nprotocol. The training data ultimately included 760 training examples, with 228 positive examples and 532 nega-\ntive examples. We applied the random decision forest (RF) algorithm\n25, a supervised machine learning method, to \nbinary classify the training data. This step yielded 476 synthesis paragraphs from a total of 2,776 articles.\nEntity annotation. In order to improve the quality of the training data based on the automatically extracted \nmodels, we generated a higher-quality dataset, also known as a gold standard corpus26, by manually annotating a \nportion of the sentences from the abstracts and body of literature related to CO2 electroreduction. We developed \nan annotation framework based on the doccano annotation tool27. Annotators can open the framework in a web \nbrowser and browse through the sentences of the material literature. The page displays the sentence to be anno-\ntated along with predefined entity types and related descriptions. The annotator can add new entities, reorder \nthem or edit them by opening a separate view. To ensure consistency between annotators, detailed annotation \nguidelines are provided.\nEntity extraction.  In our previous study, we extracted nine types of entities in the literature based on the \nconstructed electrocatalytic reduction system, including material, regulation method, product, faradaic efficiency, \ncell setup, electrolyte, synthesis method, current density, and voltage. Some of these entity labels are provided with \nmore detailed labelling subclasses to ensure that materials scientists have access to more complete information. \nIn the current construction of the CO\n2 electrocatalysis literature dataset, We have updated the categories of the \ntag subcategories according to the new knowledge system. In addition, we added information on the material \nsynthesis process, which converted unstructured scientific paragraphs describing catalytic materials synthesis \ninto pre-defined “coded recipes” of synthesis. The recipes includes not only the starting materials and final target \nproducts but also the synthesis actions and their attributes.\nConstruction of extended corpus.  Traditional entity extraction methods follow the pattern of “expert \nannotation, model training, model application” and use automatic extraction models to build a wider and larger \ncorpus of lower quality, also known as a silver standard corpus(SSC)\n26. The Large Language Models (LLMs) such \nas GPT-3, GPT-3.5, and GPT-4 have been used for this purpose 28–30. Its emergency provides a new paradigm \nfor natural language processing modelling, i.e., building prompts with a small amount of expert annotation to \ndirectly fine-tune GPT models that have been pre-trained on large-scale data. Traditional NER methods are less \ngeneral, but have higher domain confidence, while large models may produce uncontrollable illusions. Herein, \nin this paper, we used two model training approaches separately to generate an extended corpus based on the \nconstruction standard of the silver standard corpus(SSC).\nEntity extraction using traditional NER methods. Regarding the hierarchical structure of entity labelling, \nwe designed a two-step entity recognition model which consists of coarse-grained entity recognition and \nfine-grained entity classification. In the first step, we used the SciBERT model 31 to convert each word token \n4Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\ninto an embedding vector. The embedding vector was then passed to a bi-directional long-short-term memory \nneural network with a conditional random-field top layer(BiLSTM-CRF) 32,33 to identify which class of entity \nlabels the corresponding token was. Considering that the representations of some entities usually have regu -\nlarities, such as the chemical formula expressions of material entities and the numerical expressions of faradaic \nefficiency entities, we proposed a regular rule-based approach to assist the deep learning model\n34. The results of \nthe two models were selected using a voting scheme26. In the second step, each coarse-grained type entity was \nclassified into finer-grained entity classes using a classification algorithm combining dictionary and maximum \nentropy model. The dictionary-based recognizers used lists of words built on expert-annotated data\n35. For data \nthat cannot be matched, the word embedding vectors, context vectors, word cluster clustering information and \ncoarse-grained entity category information for each entity were passed through a simple mapping function. The \nfinal mapping results were used as entity features for classification probability prediction through a maximum \nentropy model.\nA typical synthesis procedure in the electrocatalytic reduction literature contains information on the pre\n-\npared and target materials, synthesis operations and operating conditions. These items are organized into mate-\nrial synthesis “recipes” and are extracted from the synthesis paragraph as shown in Fig. 2. Our extraction process \nconsists of multiple algorithms that analyze the passages and identify the relevant materials, the synthesis actions \nperformed, and the condition information associated with those synthetic actions. The method used in each step \nof the extraction process is described in detail below.\nStep 1: Materials entity recognition. The first step is the labelling of the preparation material. The synthesis \nof the target material involves the names of all the reagents that need to be prepared. We used pattern matching \nagainst a database of common reagent names and then used a plain Bayesian classifier to determine whether a \ncandidate phrase is a reagent name, excluding some specific phrases\n36. Through iterative trials, we eventually \nchose reagent names from the Reaxys database and non-reagent-name texts from the Brown English language \ncorpus to train the classifier.\nStep 2: Synthesis actions To identify and classify synthesis actions described in passages, we imple\n-\nmented an algorithm that combines Recurrent Neural Networks (RNN) and rule-based sentence dependency \ntree parsing\n22. The neural network labelled the sentences in the synthetic passages into nine categories: NOT \nOPERATION, ADDING, HEATING, CURING, ELECTROCHEMICAL ANODIZATION, FILTERING, \nDRYING, DIPPING and REACITON, which are the main operations in catalytic materials synthesis. We used \nChemDataExtractor’s ChemWordTokenizer\n37 to tokenize the lemmatized sentences. For each synthesis \naction obtained, we used the SpaCy library 38 to parse the syntactic information of the dependency subtree for \nlinguistic features of the tokens, such as their lexical properties and their dependency on the root token.\nStep 3: Synthesis action conditions For each synthesis action, we used dependency tree parsing and \nrule-based regular expression methods 39 to extract the relevant attributes of the synthesis action, such as \nheating time, heating temperature, and potential voltage values. In addition, if there were materials involved \nsuch as ADDING and REACTION operations, we used pattern-matching techniques to extract the names and \nFig. 2 Schematic diagram of the process of converting a synthetic paragraph into action sequences.\n5Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\ncorresponding quantities of the reagents involved. For example, one of the patterns used for finding solutions is \n“a/an XX solution containing Reagent” in which “Reagent” represents a phrase previously tagged as a reagent. \nAn example phrase that would be matched by this pattern is “an aqueous solution containing HAuCl\n4(10 mol, \n125 mL)” . The contents of the parentheses are regularly matched to the corresponding quantities of the reagents.\nEntity extraction using LLMs. In previous study, we attempted to construct a corpus using an NLP model, \nbut the accuracy of the intelligent model is easily affected by the volume of training data. Herein, we demon -\nstrate that LLMs, including original LLMs and fine-tuned LLMs, can act as assistants to collaborate with human \nresearchers, facilitating entity recognition and text mining to accelerate the research process.\nIn the realm of catalyst-related tasks, LLM’s performance can be significantly enhanced by employing prompt \nengineering (PE) which can steer LLMs toward generating precise and pertinent information. Although LLMs, \nincluding fine-tuned LLMs, can answer general questions, their knowledge depth, accuracy and timeliness are \nlimited in vertical domain filed. To solve this problem, we use vector databases to enhance the reasoning ability \nof LLMs in vertical domains. Vector databases can transform literature and data into vector representations by \nembedding vectors. Sci-BERT\n31 was used as embedding model for construct the vector database.\nFigure 3 shows the process of knowledge extraction using LLMs and vector database. Firstly, we processed \nand cleaned the full text of 12,643 photoelectrocatalytic scientific literature, and used them for LLMs fine-tuning. \nIn this step, we chose Vicuna-33b-v1.3 as the basic LLMs. Secondly, we extracted the title, abstract and doi from \narticles associated with standard corpus, then we use Sci-BERT as the embedding model to transform title and \nabstract into vector. When performing entity recognition, user first input the text to be extracted, embedding \nmodel transform it into vectors. Then the similar articles will be obtained by calculating the vector distance, and \nwill be used to generate precise and pertinent information, which be shown in Fig. 4. The prompt will be input \nto the fine-tuned LLMs for entity recognition.\nData records\nThe both types of datasets constructed in this work are available in ScienceDB, a public, general-purpose data \nrepository designed to serve data to researchers, research projects/teams, journals, institutions, universities, \nand others. The metadata contained in the article dataset includes: article DOI, year of publication, and title. \nEach record corresponds to the process of CO\n2 electrocatalytic reduction and its metadata includes: the entity \nextracted from the paper, the label of the entity, and the sentence in which the entity is located. In addition, the \ndatasets for the catalytic material synthesis methods are available as a single json. Each record corresponds to a \nsynthesis procedure extracted from a paragraph and is represented as a separate json object. The metadata for \neach reaction includes the DOI of the paper from which the reaction is extracted as well as a fragment of the \ncorresponding synthesis paragraph, the target product, the preparative material used in the reaction, and a tree \nof seven types of synthesis operations and their corresponding conditions. Table 1 gives extended details of all \nthe dataset format.\nThe sequence of synthesis steps for the reaction (if specified in a paragraph) is listed as a data structure with \nthe following fields: the original paragraph in the text (synthesis_paragraph), its type (operation_\nstring) specified by the classification algorithm (see Methods), and the conditions associated with this oper\n-\nation step (conditions). We classified the types of operations involved in the synthesis of catalyst materials \ninto eight categories and give detailed descriptions of the types of operations and condition attributes in Table 2.\nThe corpus is publicly available at Science Data Bank (ScienceDB), which is a public, general-purpose data \nrepository aiming to provide data services for researchers, research projects/teams, journals, institutions, \nFig. 3 The schematic overview of extraction using LLMs and vector database.\n6Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nuniversities, etc. The benchmark corpus is publicly available at https://doi.org/10.57760/sciencedb.1329040. The \nextended corpus I and extended corpus II are publicly available at https://doi.org/10.57760/sciencedb.1329241, \nwhere include other extendedcorpuscorpus exacted by LLM model. The two types of Corpus are provided as \na file in CSV format, and the details of them are shown in Table 3. A complete dataset of 476 catalytic material \nsynthesis processes is publicly available at https://doi.org/10.57760/sciencedb.1329342.\ntechnical Validation\nExtraction accuracy. To demonstrate the utility of the extended corpus, we first evaluated the model against \nother current state-of-the-art traditional entity extraction methods. We selected several generic neural network \ntagging models, including bi-directional LSTM layers with conditional random field (CRF) layer33,43,44, bi-direc-\ntional recurrent neural network Bi-GRU45, and BERT model with CRF layer. We then chose a multi-feature based \nmaximum entropy machine learning model46 using two types of features, Parts-of-Speech features generated by \nGENIA Parts-of-Speech Tagger47and lexical features. Table 4 shows the results of the experimental comparison. \nFig. 4 The prompt using in the entity extraction.\nData Description Data Key Label Data Type\nDOI of the original paper doi string\nTitle of the original paper title string\nEntity extracted from the paper entity string\nLabel of the entity entity_label string\nSentence where the entity is located context string\nTarget material data target_string list of strings\nEquipment where the reaction is operated hardware list of strings\nMaterial data of the preparation process reagent list of strings\nSequence of synthesis steps and corresponding conditions operation\nlist of Objects (dict):\n-string: string\n-vessel: string\n-reagent: list of Objectsa\n-speed: string\n-temp: list of Objects\nb\n-time: list of Objectsb\n-potential: list of Objectsb\n-condition: string\n-stir: boolean\n-reflux: boolean\nTable 1. Format of each data record: description, key label, data type.\n7Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nWe found that our constructed entity extraction model consistently outperforms other methods, achieving an \noverall F1 score of 85.16 in recognizing four coarse-grained categories of entities. This also demonstrated an \nadvantage in the subsequent classification of fine-grained entities.\nTo estimate the quality of the synthesis process dataset, we had a human expert test 100 randomly selected \nentries. The human expert manually extracted the information provided in the synthesis paragraphs and com-\npared the results with those extracted by the pipeline. Table 5 presents the accuracy statistics, which include the \nprecision, recall, and F1 scores calculated from the test entries.\nWe also validated the entity recognition results of the LLMs in this paper. We validate the answers of the \nLLMs by an expert with 160 randomly selected entries, and ensure that each category has 20 test data. The eval-\nuation result is shown in Table 6. The Count means the total amount of samples from different categories, the \nCorrect means the number of correctly identified entities, and the Existence means the number of entities of this \ntype does exist in the text input to the large model. It is worth mentioning that if there is indeed no correspond-\ning entity in the text input to the large model, the situation where the large model answers empty should also be \nconsidered as correct recognition. Therefore, we use Modified Correct to remove the above influence. Ultimately, \nwe utilize Modified Correct and Count to calculate the evaluation of LLMs, which is Modified accuracy . Using \nlarge models for entity recognition also causes significant time loss. We used two NVIDIA A100 GPU graphics \nprocessing units for entity recognition, and cost almost 10 hours to process 5,941 literature abstracts.\nFrom the results, we can see that the LLMs perform better in entity extraction for numerical classes (faradaic \nefficiency, potential, etc.), but perform poorly in entity extraction for descriptive classes. This may be due to the \nobjectivity of data entities, which reduces the possibility of hallucinations in large models.\nDataset mining. To present the recent trends in the development of CO 2 reduction electrocatalysts, we \nshowcased and analyzed the information in the database. Firstly, we demonstrated the publication trends of CO2 \nreduction electrocatalysts over the past 30 years. As depicted in Fig. 5a, articles on CO2 reduction electrocatalysts \nhave experienced a rapid surge since 2010, indicating the burgeoning interest of scientists in this field. Figure 5b \nillustrates the proportional distribution of various types of CO 2 reduction electrocatalysts. It is evident that the \ncurrent research predominantly focuses on E (single metal), E/C (metal-carbon composites), E-M (binary or \nternary metal systems), and EOx (metal oxides), with a notable increase in attention toward E/C in recent years.\nIn addition to the overall development of electrocatalysts, another intriguing aspect lies in the correlation \nbetween catalysts and products, which is crucial for product-oriented catalyst design. Figure 6 presents an allu-\nvial plot illustrating the intricate associations between catalysts and products. Notably, for clarity, less reported \ncatalyst categories have not been included. E/C and E-M are favorable choices for generating CO, while E-M \nand EO\nx exhibit the capability for formic acid production. For C 2 products, such as C 2H4 and C2H5OH, both \nOperation Type Condition attributes Data description\nADDING\n-left_reagent The name and quantity of materials involved in the operation\n-right_reagent The name and quantity of materials involved in the operation\n-speed Speed of adding operations\n-stir Stirring or not during addition operation\nHEATING\n-vessel Vessel in which the heating operation takes place\n-temp Final temperature for heating operations\n-time Time for heating operations\n-stir Stirring or not during heating\n-reflux Whether the heating process requires reflux\nCURING\n-condition Curing conditions, deliberately stated in the paragraph\n-temp Temperature during curing operation\n-time Time for curing\nELECTROCHEMICAL ANODIZATION\n-reagent Name of electrode solution\n-potential Potential values for anodic oxidation reactions\n-time Time for the electrochemical anodization\nFILTERING\n-condition Filter conditions, the original sentence text extracted directly\n-reagent Name of the reagent being filtered\nDRYING -condition Dry conditions, the original sentence text extracted directly\nDIPPING\n-left_reagent Name of material to be dipped\n-right_reagent Name of material immersed in\n-time Time for the immersion\nREACTION\n-left_reagent Name and quantity of materials involved in the reaction\n-right_reagent Name and quantity of materials involved in the reaction\n-temp Temperature at the time of reaction\n-time Time for the reaction\n-reflux Whether reflux is required for the reaction\nTable 2. Format of each synthesis operation record: operation type, condition attributes, data description.\n8Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nE and EOx are viable options. Furthermore, Fig. 6 also reveals some potential research topics that warrant fur-\nther exploration. For instance, although a few catalysts demonstrate the ability to produce C 3 products, such \nas n-propanol and acetone, the optimal catalysts have yet to be well-established. While composite systems are \ngaining increasing attention, their advantages over individual compounds remain to be fully elucidated.\nCorpus Type\nBenchmark Corpus Extended Corpus I Extended Corpus IIEntity Type\nMaterial 1,092 18,184 5,977\nRegulation method 1,086 35,780 5,488\nProduct (including the second and third product) 1,340 19,080 6,700\nFaradaic efficiency (including the Faradaic efficiency of second and third product) 1,135 3,496 3,152\nCell setup 435 — 170\nElectrolyte 475 — 3,919\nSynthesis method 228 476 —\nCurrent density 393 — 3,852\nVoltage 801 — 1,025\nTotal 6,985 77,016 30,283\nTable 3. Summary of the three corpus.\nEntity(freq. in test set) MaxEnt BiLSTM-CRF BiGRU-CRF BERT-CRF BERT-BiLSTM-CRF\nMATERIAL(92) 40.12 50.43 52.01 59.96 60.59\nMETHOD(97) 38.25 46.89 49.67 57.12 58.02\nPRODUCT(94) 70.21 82.45 86.12 91.10 92.34\nFARADAIC EFFICIENCY(62) 88.16 91.18 91.98 94.56 96.82\nMacro-avg F1 51.26 66.90 68.02 71.48 73.90\nMicro-avg F1 68.89 81.02 82.33 82.73 85.16\nTable 4. Compare the F1 scores of entity recognition in various models.\nPipeline Component Extraction Method F1:(precision | recall)\nArticle filtering Regular match 0.88:(0.84 | 0.93)\nSynthesis paragraph classification BERT classification 0.80:(0.82 | 0.78)\nMaterials entity recognition BiLSTM + CRF (BERT embedding) & Regular match\n0.96:(0.98 | 0.94) - materials\n0.84:(0.86 | 0.82) - targets\nSynthesis actions BiLSTM (Word2Vec embeddings) 0.89: (0.92 | 0.86)\nSynthesis conditions Rule-based\n-Temperature 0.95: (0.98 | 0.93)\n-Time 0.96: (0.98 | 0.94)\n-Potential 0.88: (0.91 | 0.86)\nMaterial quantities Rule-based 0.90: (0.93 | 0.87)\nTable 5. Accuracy of synthesis information extraction models.\nEntity Count Correct Existence Modified Correct Modified accuracy\nMATERIAL 20 15 17 15 75%\nMETHOD 20 13 19 13 65%\nPRODUCT 20 17 17 17 85%\nFARADAIC EFFICIENCY 20 11 11 18 90%\nELECTROLYTE 20 9 10 10 50%\nPOTENTIAL 20 7 7 16 80%\nCURRENT DENSITY 20 7 7 12 60%\nCELL SETUP 20 6 6 9 45%\nOVERALL 160 85 94 110 68.75%\nTable 6. The evaluation of entity recognition of LLMs.\n9Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nMoreover, the type of metal, particularly the presence of Cu, is crucial for the performance of catalysts in \nCO2 electroreduction. Therefore, we annotated whether the catalysts contained Cu in the database. To illustrate \nthis contrast clearly, we generated doughnut charts to display the percentage of different products from several \ntypes of catalysts with or without Cu. As shown in Fig.  7a, the majority of the products for E/C are CO, while \nCu/C can generate various C 1 and C2 products. For single metal systems (Fig.  7b), the primary products of E \nFig. 5 (a) Histograms of the number of publications of CO2 reduction electrocatalysts over the past thirty years. \n(b) Stacked histograms of the percentage of CO2 reduction electrocatalysts in the last ten years.\nFig. 6 Alluvial plot illustrating the relationships between catalysts and products.\n10Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nare C1 products, whereas Cu yields predominantly C2 products. In the case of binary or ternary metal systems, \nCu-M exhibits a stronger capability for producing C2 products compared to E-M. Regarding metal oxides, the \nproducts of EOx are predominantly formic acid, while CuOx yields primarily C2H4. These findings underscore \nthe significant impact of the presence of Cu on the selectivity of C2 products for catalysts.\nThe choice of synthesis method also has a significant impact on the performance of catalysts, so we analyzed \nthe correlation between catalysts and synthesis methods. As shown in Fig. 8, thermal treatment and solvother-\nmal methods are the two most widely used material synthesis methods. In addition, different catalysts also have \ntheir conventional synthesis methods. For example, the synthesis of Cu/C, which usually refers to carbon-coated \nmetal nanoparticles or anchored single atoms, is mainly through thermal treatment. The synthesis of E and E-M \nis mainly electrochemical methods, especially electrochemical reduction treatment. For EO\nx and its composites, \nthe solvothermal method, wet chemical method, and electrochemical method are commonly used methods. \nThis analysis is helpful for the screening of target catalyst synthesis methods.\nThe database encompasses various catalyst types and diverse regulation strategies, which can be utilized \nto guide the design and optimization of novel catalysts. One feasible approach involves integrating multiple \nFig. 7 Doughnut charts showing the percentage of different products of catalysts with or without Cu.\nFig. 8 Heatmap showing the number of publications of CO2 electrocatalysts with different synthesis methods.\n11Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nstrategies by drawing inspiration from well-performing catalysts and regulation methods in the literature, thus \nfacilitating the development of highly efficient catalysts. For example, CuS serves as a potential efficient catalyst \nfor C\n2H4 production, while nano-sized polymer coatings can enhance the selectivity of C2H4. Consequently, CuS \nnanoparticles coated with an a-few-nm-thick polymer layer represent an effective method for selectively pro -\nducing C2H4. Similarly, coupling Cu2O nanocrystals with (111) facets with functionalized graphene nanosheets \ncan be employed for C2H5OH production. Furthermore, utilizing fine-tuned domain LLMs is also a viable strat-\negy for developing novel catalysts, and further efforts are required in fine-tuning LLMs and prompt engineering.\nCode availability\nThe scripts utilized to parse articles and extract entities are home-written codes which are publicly available at the \ngithub repository https://github.com/cxqwindy/CO2_reduction_electrocatalysts_db. The underlying machine-\nlearning libraries used in this project are all open-source: rxn4chemistry(rxn4chemistry), ChemDataExtractor \n(chemdataextractor.org)\n37, gensim (radimrehurek.com)48, PyMuPDF(PyMuPDF), Pytorch (www.pytorch.org) \nand scikit-learn (scikit-learn.org)49.\nReceived: 18 December 2023; Accepted: 22 March 2024;\nPublished: xx xx xxxx\nreferences\n 1. Birdja, Y . Y . et al. Advances and challenges in understanding the electrocatalytic conversion of carbon dioxide to fuels. Nat. Energy \n4, 732–745 (2019).\n 2. Zhong, M. et al. Accelerated discovery of CO2 electrocatalysts using active machine learning. Nature 581, 178–183 (2020).\n 3. Gao, Y ., Wang, L., Chen, X., Du, Y . & Wang, B. Revisiting electrocatalyst design by a knowledge graph of Cu-based catalysts for CO2 \nreduction. ACS Catal. 13, 8525–8534 (2023).\n 4. Qiao, J., Liu, Y ., Hong, F . & Zhang, J. A review of catalysts for the electroreduction of carbon dioxide to produce low-carbon fuels. \nChem. Soc. Rev. 43, 631–675 (2014).\n 5. Zheng, T., Jiang, K. & Wang, H. Recent advances in electrochemical CO2-to-CO conversion on heterogeneous catalysts. Adv. Mater. \n30, 1802066 (2018).\n 6. Butler, K. T., Davies, D. W ., Cartwright, H., Isayev, O. & Walsh, A. Machine learning for molecular and materials science. Nature 559, \n547–555 (2018).\n 7. Peng, J. et al. Human- and machine-centred designs of molecules and materials for sustainability and decarbonization. Nat. Rev. \nMater. 7, 991–1009 (2022).\n 8. He, T. et al. Similarity of precursors in solid-state synthesis as text-mined from scientific literature. Chem. Mater. 32, 7861–7873 \n(2020).\n 9. Huang, S. & Cole, J. M. A database of battery materials auto-generated using ChemDataExtractor. Sci. Data 7, 260 (2020).\n 10. Paula, A. J. et al . Machine learning and natural language processing enable a data-oriented experimental design approach for \nproducing biochar and hydrochar from biomass. Chem. Mater. 34, 979–990 (2022).\n 11. Kononova, O. et al. Text-mined dataset of inorganic materials synthesis recipes. Sci. data 6, 203 (2019).\n 12. Wang, L. et al. A corpus of CO2 electrocatalytic reduction process extracted from the scientific literature. Sci. Data 10, 175 (2023).\n 13. Wang, S. et al. GPT-NER: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428 (2023).\n 14. Alkaissi, H. & McFarlane, S. I. Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus 15 (2023).\n 15. Bang, Y. et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In  \nProceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific \nChapter of the Association for Computational Linguistics (Volume 1: Long Papers), 675-718 (2023).\n 16. Azamfirei, R., Kudchadkar, S. R. & Fackler, J. Large language models and the perils of their hallucinations. Crit. Care 27, 1–2 (2023).\n 17. Zheng, Z., Zhang, O., Borgs, C., Chayes, J. T. & Y aghi, O. M. ChatGPT chemistry assistant for text mining and the prediction of MOF \nsynthesis. J. Am. Chem. Soc. 145, 18048–18062 (2023).\n 18. Kumar, K. Geotechnical parrot tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. \narXiv preprint arXiv:2304.02138 (2023).\n 19. Polak, M. P . & Morgan, D. Extracting accurate materials data from research papers with conversational language models and prompt \nengineering. arXiv preprint arXiv:2303.05352 (2023).\n 20. Hiszpanski, A. M. et al. Nanomaterial synthesis insights from machine learning of scientific articles by extracting, structuring, and \nvisualizing knowledge. J. Chem. Inf. Model. 60, 2876–2887 (2020).\n 21. Liu, R. & McKie, J. Pymupdf. Available at http://pymupdf.readthedocs.io/en/latest/ (2018).\n 22. Cruse, K. et al. Text-mined dataset of gold nanoparticle synthesis procedures, morphologies, and size entities. Sci. Data 9, 234 \n(2022).\n 23. Huo, H. et al. Semi-supervised machine-learning classification of materials synthesis procedures. npj Comput. Mater. 5, 62 (2019).\n 24. Blei, D. M., Ng, A. Y . & Jordan, M. I. Latent dirichlet allocation. J. Mach. Learn. Res. 3, 993–1022 (2003).\n 25. Breiman, L. Random forests. Mach. Learn. 45, 5–32 (2001).\n 26. Rebholz-Schuhmann, D. et al . The calbc silver standard corpus for biomedical named entities-a study in harmonizing the \ncontributions from four independent named entity taggers. In LREC 2010-7th International Conference on Language Resources and \nEvaluation (CELI Language & Informat Technol; European Media Lab GmBH; Quaero; META, 2010).\n 27. Nakayama, H., Kubo, T., Kamura, J., Taniguchi, Y . & Liang, X. doccano: Text annotation tool for human. Software available from \nhttps://github.com/doccano/doccano 34 (2018).\n 28. Brown, T. et al. Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020).\n 29. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog 1, 9 (2019).\n 30. Radford, A. et al. Improving language understanding by generative pre-training. (2018).\n 31. Beltagy, I., Lo, K. & Cohan, A. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing \n(EMNLP-IJCNLP),3615-3620 (2019).\n 32. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997).\n 33. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. & Dyer, C. Neural architectures for named entity recognition. In  \nProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies,260-270 (2016).\n 34. Corbett, P . & Murray-Rust, P . High-throughput identification of chemistry in life science texts. In International Symposium on \nComputational Life Science, 107–118 (Springer, 2006).\n 35. Hettne, K. M. et al. A dictionary to identify small molecules and drugs in free text. Bioinformatics 25, 2983–2991 (2009).\n\n12Scientific  Data |          (2024) 11:347  | https://doi.org/10.1038/s41597-024-03180-9\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 36. Vaucher, A. C. et al. Automated extraction of chemical synthesis actions from experimental procedures. Nat. Commun.  11, 3601 \n(2020).\n 37. Swain, M. C. & Cole, J. M. ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific \nliterature. J. Chem. Inf. Model. 56, 1894–1904 (2016).\n 38. Honnibal, M. & Johnson, M. An improved non-monotonic transition system for dependency parsing. In Proceedings of the 2015 \nconference on empirical methods in natural language processing, 1373–1378 (2015).\n 39. Teller, V . Speech and language processing: an introduction to natural language processing, computational linguistics, and speech \nrecognition (2000).\n 40. Wang, L. et al. Benchmark corpus of CO2 reduction electrocatalysts and synthesis procedures, ScienceDB, https://doi.org/10.57760/\nsciencedb.13290 (2023).\n 41. Wang, L. et al. The extended corpus of CO2 reduction electrocatalysts and synthesis procedures, ScienceDB, https://doi.org/10.57760/\nsciencedb.13292 (2023).\n 42. Wang, L. et al . A complete dataset of 476 catalytic material synthesis processes. ScienceDB at https://doi.org/10.57760/\nsciencedb.13293 (2023).\n 43. Ma, X. & Hovy, E. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF . arXiv preprint arXiv:1603.01354 (2016).\n 44. Plank, B., Søgaard, A. & Goldberg, Y . Multilingual part-of-speech tagging with bidirectional long short-term memory models and \nauxiliary loss. arXiv preprint arXiv:1604.05529 (2016).\n 45. Chung, J., Gulcehre, C., Cho, K. & Bengio, Y . Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv \npreprint arXiv:1412.3555 (2014).\n 46. Borthwick, A. E. A maximum entropy approach to named entity recognition (New Y ork University, 1999).\n 47. Tsuruoka, Y . & Tsujii, J. Bidirectional inference with the easiest-first strategy for tagging sequence data. In proceedings of human \nlanguage technology conference and conference on empirical methods in natural language processing, 467–474 (2005).\n 48. Řehřek, R. & Sojka, P . Software framework for topic modelling with large corpora. (2010).\n 49. Pedregosa, F . et al. Scikit-learn: Machine learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011).\nacknowledgements\nThis work was supported by the National Key Research and Development Plan of China under Grant No. \n2022YFF0712200, 2022YFF0711900 and 2021YFA1202802, the Natural Science Foundation of China under \nGrant No. T2322027, Information Science Database in National Basic Science Data Center under Grant \nNo.NBSDC-DB-25, the Y oung Elite Scientists Sponsorship Program by Beijing Association for Science and \nTechnology (BYESS2023410), the CAS Pioneer Hundred Talents Program and Y outh Innovation Promotion \nAssociation CAS.\nauthor contributions\nAll authors contributed substantively to the work presented in this paper. Conception and Supervision: B. Wang, \nY . Du; Data acquisition: L. Wang, X. Chen, Y . Du; Data validation: Y . Gao, B. Wang, J. Huang; Technical validation: \nL. Wang, X. Chen; Dataset mining: Y . Gao, B. Wang; Writing and Proof reading: L. Wang, Y . Gao, X. Chen, B. \nWang, Y . Du.\nCompeting interests\nThe authors declare no competing interests.\nadditional information\nCorrespondence and requests for materials should be addressed to Y .D. or B.W .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2024",
  "topic": "Pipeline (software)",
  "concepts": [
    {
      "name": "Pipeline (software)",
      "score": 0.7305957674980164
    },
    {
      "name": "Computer science",
      "score": 0.6958702802658081
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6655216217041016
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6549085378646851
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.516555666923523
    },
    {
      "name": "Natural language processing",
      "score": 0.5050507187843323
    },
    {
      "name": "Process (computing)",
      "score": 0.48823121190071106
    },
    {
      "name": "Information extraction",
      "score": 0.46886876225471497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4688001275062561
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210108629",
      "name": "Computer Network Information Center",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210141483",
      "name": "National Center for Nanoscience and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 12
}