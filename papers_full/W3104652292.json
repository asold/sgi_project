{
    "title": "On the Sub-layer Functionalities of Transformer Decoder",
    "url": "https://openalex.org/W3104652292",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2095776957",
            "name": "Yilin Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2155672199",
            "name": "Longyue Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105557964",
            "name": "Shuming Shi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993564419",
            "name": "Prasad Tadepalli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131064080",
            "name": "Stefan Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2126985900",
            "name": "Zhaopeng Tu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2990555152",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2563574619",
        "https://openalex.org/W2896060389",
        "https://openalex.org/W2892205701",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2145094598",
        "https://openalex.org/W3034939458",
        "https://openalex.org/W2964189376",
        "https://openalex.org/W3035072529",
        "https://openalex.org/W2951977278",
        "https://openalex.org/W2971287409",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2971154170",
        "https://openalex.org/W2971473121",
        "https://openalex.org/W2962926939",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2962822108",
        "https://openalex.org/W2952682849",
        "https://openalex.org/W2970290486",
        "https://openalex.org/W2964093087",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W2970810442",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2997574889",
        "https://openalex.org/W2773621464",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2912351236",
        "https://openalex.org/W2962777840",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2903728819",
        "https://openalex.org/W2963651521",
        "https://openalex.org/W2017257315",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2962931466",
        "https://openalex.org/W1869752048",
        "https://openalex.org/W2156985047",
        "https://openalex.org/W2773956126",
        "https://openalex.org/W2970511364",
        "https://openalex.org/W2963069010",
        "https://openalex.org/W1411230545",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W3037932933"
    ],
    "abstract": "There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages – developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance – a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4799–4811\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4799\nOn the Sub-layer Functionalities of Transformer Decoder\nYilin Yang∗\nOregon State University\nyilinyang721@gmail.com\nLongyue Wang\nTencent AI Lab\nvinnylywang@tencent.com\nShuming Shi\nTencent AI Lab\nshumingshi@tencent.com\nPrasad Tadepalli\nOregon State University\ntadepall@oregonstate.edu\nStefan Lee\nOregon State University\nleestef@oregonstate.edu\nZhaopeng Tu\nTencent AI Lab\nzptu@tencent.com\nAbstract\nThere have been signiﬁcant efforts to interpret\nthe encoder of Transformer-based encoder-\ndecoder architectures for neural machine trans-\nlation (NMT); meanwhile, the decoder re-\nmains largely unexamined despite its critical\nrole. During translation, the decoder must\npredict output tokens by considering both the\nsource-language text from the encoder and the\ntarget-language preﬁx produced in previous\nsteps. In this work, we study how Transformer-\nbased decoders leverage information from the\nsource and target languages – developing a\nuniversal probe task to assess how informa-\ntion is propagated through each module of\neach decoder layer. We perform extensive ex-\nperiments on three major translation datasets\n(WMT En-De, En-Fr, and En-Zh). Our anal-\nysis provides insight on when and where de-\ncoders leverage different sources. Based on\nthese insights, we demonstrate that the resid-\nual feed-forward module in each Transformer\ndecoder layer can be dropped with minimal\nloss of performance – a signiﬁcant reduction\nin computation and number of parameters, and\nconsequently a signiﬁcant boost to both train-\ning and inference speed.\n1 Introduction\nTransformer models have advanced the state-of-\nthe-art on a variety of natural language pro-\ncessing (NLP) tasks, including machine transla-\ntion (Vaswani et al., 2017), natural language in-\nference (Shen et al., 2018), semantic role label-\ning (Strubell et al., 2018), and language represen-\ntation (Devlin et al., 2019). However, so far not\nmuch is known about the internal properties and\nfunctionalities it learns to achieve its superior per-\nformance, which poses signiﬁcant challenges for\nhuman understanding of the model and potentially\ndesigning better architectures.\n∗Work done when interning at Tencent AI Lab.\nRecent efforts on interpreting Transformer mod-\nels mainly focus on assessing the encoder represen-\ntations (Raganato et al., 2018; Yang et al., 2019;\nTang et al., 2019a) or interpreting the multi-head\nself-attentions (Li et al., 2018; V oita et al., 2019;\nMichel et al., 2019). At the same time, there have\nbeen few attempts to interpret the decoder side,\nwhich we believe is also of great interest, and\nshould be taken into account while explaining the\nencoder-decoder networks. The reasons are three-\nfold: (a) the decoder takes both source and target as\ninput, and implicitly performs the functionalities of\nboth alignment and language modeling, which are\nat the core of machine translation; (b) the encoder\nand decoder are tightly coupled in that the output\nof the encoder is fed to the decoder and the training\nsignals for the encoder are back-propagated from\nthe decoder; and (c) recent studies have shown that\nthe boundary between the encoder and decoder is\nblurry, since some of the encoder functionalities\ncan be substituted by the decoder cross-attention\nmodules (Tang et al., 2019b).\nIn this study, we interpret the Transformer de-\ncoder by investigating when and where the decoder\nutilizes source or target information across its stack-\ning modules and layers. Without loss of generality,\nwe focus on the representation evolution1 within\na Transformer decoder. To this end, we introduce\na novel sub-layer2 split with respect to their func-\ntionalities: Target Exploitation Module(TEM) for\nexploiting the representation from translation his-\ntory, Source Exploitation Module (SEM) for ex-\nploiting the source-side representation, and Infor-\nmation Fusion Module (IFM) to combine represen-\ntations from the other two (§2.2).\nFurther, we design a universal probing scheme\n1By “evolution”, we denote the progressive trend from the\nﬁrst layer till the last.\n2Throughout this paper, we use the terms “sub-layer” and\n“module” interchangeably.\n4800\nto quantify the amount of speciﬁc information em-\nbedded in network representations. By probing\nboth source and target information from decoder\nsub-layers, and by analyzing the alignment error\nrate (AER) and source coverage rate, we arrive at\nthe following ﬁndings:\n•SEM guides the representation evolution\nwithin NMT decoder (§3.1).\n•Higher-layer SEMs accomplish the functional-\nity of word alignment, while lower-layer ones\nconstruct the necessary contexts (§3.2).\n•TEMs are critical to helping SEM build word\nalignments, while their stacking order is not\nessential (§3.2).\nLast but not least, we conduct a ﬁne-grained analy-\nsis on the information fusion process within IFM.\nOur key contributions in this work are:\n1. We introduce a novel sub-layer split of Trans-\nformer decoder with respect to their functionali-\nties.\n2. We introduce a universal probing scheme from\nwhich we derive aforementioned conclusions\nabout the Transformer decoder.\n3. Surprisingly, we ﬁnd that the de-facto usage of\nresidual FeedForward operations are not efﬁcient,\nand could be removed in totality with minimal\nloss of performance, while signiﬁcantly boosting\nthe training and inference speeds.\n2 Preliminaries\n2.1 Transformer Decoder\nNMT models employ an encoder-decoder architec-\nture to accomplish the translation process in an end-\nto-end manner. The encoder transforms the source\nsentence into a sequence of representations, and\nthe decoder generates target words by dynamically\nattending to the source representations. Typically,\nthis framework can be implemented with a recur-\nrent neural network (RNN) (Bahdanau et al., 2015),\na convolutional neural network (CNN) (Gehring\net al., 2017), or a Transformer (Vaswani et al.,\n2017). We focus on the Transformer architecture,\nsince it has become the state-of-the-art model on\nmachine translation tasks, as well as various text\nunderstanding (Devlin et al., 2019) and genera-\ntion (Radford et al., 2019) tasks.\nTarget \nEmbedding\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\nTarget Exploitation Module (TEM) \nto exploit information from translation \nhistory\nSource Exploitation Module (SEM) \nto exploit information from source-side\nInformation Fusion Module (IFM) \nto combine both sides of information \nfor prediction.\nTarget \nEmbedding\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\nSAN\nEAN\nFFN\nTarget \nEmbedding\nMulti-Head \nAttention\nAdd & Norm\n⊕ ⇘Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\nSAN\nEAN\nFFN\nFigure 1: A sub-layer splitting of Transformer decoder\nwith respect to their functionalities.\nSpeciﬁcally, the decoder is composed of a stack\nof N identical layers, each of which has three sub-\nlayers, as illustrated in Figure 1. A residual con-\nnection (He et al., 2016) is employed around each\nof the three sub-layers, followed by layer normal-\nization (Ba et al., 2016) (“Add & Norm”). The ﬁrst\nsub-layer is a self-attention module that performs\nself-attention over the previous decoder layer:\nCn\nd = LN\n(\nATT(Qn\nd, Kn\nd, Vn\nd ) +Ln−1\nd\n)\nwhere ATT(·) and LN(·) denote the self-attention\nmechanism and layer normalization. Qn\nd, Kn\nd, and\nVn\nd are query, key and value vectors that are trans-\nformed from the (n-1)-th layer representation Ln−1\nd .\nThe second sub-layer performs attention over the\noutput of the encoder representation:\nDn\nd = LN\n(\nATT(Cn\nd, KN\ne , VN\ne ) +Cn\nd\n)\nwhere KN\ne and VN\ne are transformed from the top\nencoder representation LN\ne . The ﬁnal sub-layer is\na position-wise fully connected feed-forward net-\nwork with ReLU activations:\nLn\nd = LN\n(\nFFN(Dn\nd) +Dn\nd\n)\nThe top decoder representation LN\nd is then used to\ngenerate the ﬁnal prediction.\n2.2 Sub-Layer Partition\nIn this work, we aim to reveal how a Transformer\ndecoder accomplishes the translation process uti-\nlizing both source and target inputs. To this end,\n4801\nwe split each decoder layer into three modules with\nrespect to their different functionalities over the\nsource or target inputs, as illustrated in Figure 1:\n•Target Exploitation Module (TEM) consists of\nthe self-attention operation and a residual con-\nnection, which exploits the target-side translation\nhistory from previous layer representations.\n•Source Exploitation Module (SEM) consists only\nof the encoder attention, which dynamically se-\nlects relevant source-side information for genera-\ntion.\n•Information Fusion Module (IFM) consists of\nthe rest of the operations, which fuse source and\ntarget information into the ﬁnal layer representa-\ntion.\nCompared with the standard splits (Vaswani et al.,\n2017), we associate the “Add&Norm” operation\nafter encoder attention with the IFM, since it starts\nthe process of information fusion by a simple addi-\ntive operation. Consequently, the functionalities of\nthe three modules are well-separated.\n2.3 Research Questions\nModern Transformer decoder is implemented as\nmultiple identical layers, in which the source and\ntarget information are exploited and evolved layer-\nby-layer. One research question arises naturally:\nRQ1. How do source and target information\nevolve within the decoder layer-by-layer and\nmodule-by-module?\nIn Section 3.1, we introduce a universal probing\nscheme to quantify the amount of information em-\nbedded in decoder modules and explore their evo-\nlutionary trends. The general trend we ﬁnd is that\nhigher layers contain more source and target in-\nformation, while the sub-layers behave differently.\nSpeciﬁcally, the amount of information contained\nby SEMs would ﬁrst increase and then decrease.\nIn addition, we establish that SEM guides both\nsource and target information evolution within the\ndecoder.\nSince SEMs are critical to the decoder repre-\nsentation evolution, we conduct a more detailed\nstudy into the internal behaviors of the SEMs. The\nexploitation of source information is also closely\nrelated to the inadequate translation problem – a\nkey weakness of NMT models (Tu et al., 2016).\nWe try to answer the following research question:\nRQ2. How does SEM exploit the source infor-\nmation in different layers?\nIn Section 3.2, we investigate how the SEMs trans-\nform the source information to the target side in\nterms of alignment accuracy and coverage ratio (Tu\net al., 2016). Experimental results show that higher\nlayers of SEM modules accomplish word align-\nment, while lower layer ones exploit necessary con-\ntexts. This also explains the representation evo-\nlution of source information: lower layers collect\nmore source information to obtain a global view of\nsource input, and higher layers extract less aligned\nsource input for accurate translation.\nOf the three sub-layers, IFM modules concep-\ntually appear to play a key role in merging source\nand target information – raising our ﬁnal question:\nRQ3. How does IFM fuse source and target\ninformation on the operation level?\nIn Section 3.3, we ﬁrst conduct a ﬁne-grained anal-\nysis of the IFM module on the operation level, and\nﬁnd that a simple “Add&Norm” operation performs\njust as well at fusing information. Thus, we sim-\nplify the IFM module to be only one Add&Norm\noperation. Surprisingly, this performs similarly to\nthe full model while signiﬁcantly reducing the num-\nber of parameters and consequently boosting both\ntraining and inference speed.\n3 Experiments\nData To make our conclusions compelling, all ex-\nperiments and analysis are conducted on three rep-\nresentative language pairs. For English⇒German\n(En⇒De), we use WMT14 dataset that consists\nof 4.5M sentence pairs. The English ⇒Chinese\n(En⇒Zh) task is conducted on WMT17 cor-\npus, consisting of 20.6M sentence pairs. For\nEnglish⇒French (En⇒Fr) task, we use WMT14\ndataset that comprises 35.5M sentence pairs. En-\nglish and French have many aspects in common\nwhile English and German differ in word order, re-\nquiring a signiﬁcant amount of reordering in trans-\nlation. Besides, Chinese belongs to a different lan-\nguage family compared to the others.\nModels We conducted the experiments on the\nstate-of-the-art Transformer (Vaswani et al., 2017),\nand implemented our approach with the open-\nsource toolkit FairSeq (Ott et al., 2019). We fol-\n4802\nBush talkhold a with Sharon .Input 2\n0.5 0.30.10.40.20.20.3\nInput 1\nforce-decoding\nOutput\nProbing Model\nFigure 2: Illustration of the information probing model,\nwhich reads the representation of a decoder module\n(“Input 1”) and the word sequence to recover (“Input\n2”), and outputs the generation probability (“Output”).\nlow the setting of Transformer-Base in Vaswani\net al. (2017), which consists of 6 stacked en-\ncoder/decoder layers with the model size being\n512. We train our models on 8 NVIDIA P40 GPUs,\nwhere each is allocated with a batch size of 4,096\ntokens. We use Adam optimizer (Kingma and Ba,\n2015) with 4,000 warm-up steps.3\n3.1 Representation Evolution Across Layers\nIn order to quantify and visualize the representation\nevolution, we design a universal probing scheme to\nquantify the source (or target) information stored\nin network representations.\nTask Description Intuitively, the more the\nsource (or target) information stored in a network\nrepresentation, the more probably a trained re-\nconstructor could recover the source (or target)\nsequence. Since the lengths of source sequence\nand decoder representations are not necessarily the\nsame, the widely-used classiﬁcation-based probing\napproaches (Belinkov et al., 2017; Tenney et al.,\n2019b) cannot be applied to this task. Accordingly,\nwe cast this task as a generation problem – evaluat-\ning the likelihood of generating the word sequence\nconditioned on the input representation.\nFigure 2 illustrates the architecture of our prob-\ning scheme. Given a representation sequence\nfrom decoder H = {h1, . . . ,hM }and the source\n(or target) word sequence to be recovered x =\n{x1, . . . , xN }the recovery likelihood is calculated\nas the perplexity (i.e. negative log-likelihood) of\nforced-decoding the word sequence:\nPPL (x|H) =\nN∑\nn=1\n−log P(xn|x<n, H) (1)\n3More implementation details are in Sec A.1.\nThe lower the recovery perplexity, the more the\nsource (or target) information stored in the repre-\nsentation. The probing model can be implemented\nas any architecture. For simplicity, we use a one-\nlayer Transformer decoder. We train the probing\nmodel to recover both source and target sequence\nfrom all decoder sub-layer representations. During\ntraining, we ﬁx the NMT model parameters and\ntrain the probing model on the MT training set to\nminimize the recovery perplexity in Equation 1.\nTask Discussion The above probing scheme is a\ngeneral framework applicable to probing any given\nsequence from a network representation. When we\nprobe for the source sequence, the probing model is\nanalogous to an auto-encoder (Bourlard and Kamp,\n1988; Vincent et al., 2010), which reconstructs the\noriginal input from the network representations.\nWhen we probe for the target sequence, we ap-\nply an attention mask to the probing decoder to\navoid direct copying from the input of translation\nhistories. Contrary to source probing, the target\nsequence is never seen by the model.\nIn addition, our proposed scheme can also be\napplied to probe linguistic properties that can be\nrepresented in a sequential format. For instance,\nwe could probe source constituency parsing infor-\nmation, by training a probing model to recover the\nlinearized parsing sequence (Vinyals et al., 2015).\nDue to space limitations, we leave the linguistic\nprobing to future work.\nProbing Results Figure 3 shows the results of\nour information probing conducted on the heldout\nset. We have a few observations:\n•The evolution trends of TEM and IFM are\nlargely the same. Speciﬁcally, the curve of\nTEM is very close to that of IFM shifted up by\none layer. Since TEM representations are two\noperations (self-attn. and Add&Norm) away\nfrom the previous layer IFM, this observation\nindicates TEMs do not signiﬁcantly affect the\namount of source/target information. 4\n•SEM guides both source and target informa-\ntion evolution. While closely observing the\ncurves, the trend of layer representations (i.e.\nIFM) is always led by that of SEM. For ex-\nample, as the PPL of SEM transitions from\n4 TEM may change the order or distribution of\nsource/target information, which are not captured by our prob-\ning experiments.\n4803\nDecoder Layer\n1 2 3 4 5 6\nEnglish-Chinese\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nDecoder Layer\n1 2 3 4 5 6\nDecoder Layer\n1 2 3 4 5 6\nEnglish-French\nEnglish-German\n(a) En-De\nDecoder Layer\n1 2 3 4 5 6\nEnglish-Chinese\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nDecoder Layer\n1 2 3 4 5 6\nDecoder Layer\n1 2 3 4 5 6\nEnglish-French\nEnglish-German\n (b) En-Zh\nDecoder Layer\n1 2 3 4 5 6\nEnglish-Chinese\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nDecoder Layer\n1 2 3 4 5 6\nDecoder Layer\n1 2 3 4 5 6\nEnglish-French\nEnglish-German\n (c) En-Fr\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n(d) En-De\nDecoder Layer\n1 2 3 4 5 6\nEnglish-Chinese\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nDecoder Layer\n1 2 3 4 5 6\nDecoder Layer\n1 2 3 4 5 6\nEnglish-French\nEnglish-German\n (e) En-Zh\nDecoder Layer\n1 2 3 4 5 6\nEnglish-Chinese\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nDecoder Layer\n1 2 3 4 5 6\nDecoder Layer\n1 2 3 4 5 6\nEnglish-French\nEnglish-German\n (f) En-Fr\nFigure 3: Evolution trends of source (upper panel) and\ntarget (bottom panel) information embedded in the de-\ncoder modular representations across layers. Lower\nperplexity (“PPL”) denotes more information embed-\nded in the representations.\ndecreases to increases, the PPL of IFM slows\ndown the decreases and starts increasing as an\naftermath. This is intuitive: in machine trans-\nlation, source and target sequences should\ncontain equivalent information, thus the tar-\nget generation should largely follow the lead\nof source information (from SEM representa-\ntions) to guarantee its adequacy.\n•For IFM, the amount of target information\nconsistently increases in higher layers – a\nconsistent decrease of PPL in Figures 3(d-\nf). While source information goes up in the\nlower layers, it drops in the highest layer (Fig-\nures 3(a-c)).\nSince SEM representations are critical to decoder\nevolution, we turn to investigate how SEM exploit\nsource information, in the hope of explaining the\ndecoder information evolution.\n3.2 Exploitation of Source Information\nIdeally, SEM should accurately and fully incorpo-\nrate the source information for the decoder. Ac-\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDecoder Layer\n1 2 3 4 5 6\nEn-Zh\nEn-De\nCoverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nEn-Zh\nEn-De\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDecoder Layer\n1 2 3 4 5 6\nEn-De\nEn-Zh\nCoverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nEn-De\nEn-Zh\n(a) Word Alignment\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDecoder Layer\n1 2 3 4 5 6\nEn-Zh\nEn-De\nCoverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nEn-Zh\nEn-De\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDecoder Layer\n1 2 3 4 5 6\nEn-De\nEn-Zh\nCoverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nEn-De\nEn-Zh\n (b) Cumulative Coverage\nFigure 4: Behavior of the SEM in terms of (a) align-\nment quality measured in AER (the lower, the better),\nand (b) the cumulative coverage of source words.\ncordingly, we evaluate how well SEMs accomplish\nthe expected functionality from two perspectives.\nWord Alignment. Previous studies generally in-\nterpret the attention weights of SEM as word align-\nments between source and target words, which can\nmeasure whether SEMs select the most relevant\npart of source information for each target token (Tu\net al., 2016; Li et al., 2019; Tang et al., 2019b).\nWe follow previous practice to merge attention\nweights from the SEM attention heads, and to ex-\ntract word alignments by selecting the source word\nwith the highest attention weight for each target\nword. We calculate the alignment error rate (AER)\nscores (Och and Ney, 2003) for word alignments\nextracted from SEM of each decoder layer.\nCumulative Coverage. Coverage is commonly\nused to evaluate whether the source words are fully\ntranslated (Tu et al., 2016; Kong et al., 2019). We\nuse the above extracted word alignments to identify\nthe set of source words Ai, which are covered (i.e.,\naligned to at least one target word) at each layer.\nWe then propose a new metriccumulative coverage\nratio C≤i to indicate how many source words are\ncovered by the layers ≤i:\nC≤i = |A1 ∪···∪ Ai|\nN (2)\nwhere N is the number of total source words. This\nmetric indicates the completeness of source infor-\nmation coverage till layer i.\nDataset We conducted experiments on two\nmanually-labeled alignment datasets: RWTH En-\n4804\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n(a) Alignment\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n (b) Cumulative Coverage\nFigure 5: Effects of the stacking order of TEM and\nSEM on the En-De dataset.\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n(a) Alignment\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n (b) Cumulative Coverage\nFigure 6: Effects of the stacking order of TEM and\nSEM on the En-Zh dataset.\nDe5 and En-Zh (Liu and Sun, 2015). The align-\nments are extracted from NMT models trained on\nthe WMT En-De and En-Zh dataset.\nResults Figure 4 demonstrates our results on\nword alignment and cumulative coverage. We\nﬁnd that the lower-layer SEMs focus on gather-\ning source contexts (rapid increase of cumulative\ncoverage with poor word alignment), while higher-\nlayer ones play the role of word alignment with\nthe lowest AER score of less than 0.4 at the 5th\nlayer. The 4th layer and the 3rd layer separate the\ntwo roles for En-De and En-Zh respectively. Corre-\nspondingly, they are also the turning points (PPL\nfrom decreases to increases) of source information\nevolution in Figure 3 (a,b). Together with conclu-\nsions from Sec. 3.1, we demonstrate the general\npattern of SEM: SEM tends to cover more source\ncontent and gain increasing amount of source in-\n5https://www-i6.informatik.rwth-aachen.\nde/goldAlignment\nDecoder En-De En-Zh En-Fr\nTEM ⇒SEM ⇒IFM 27.45 32.24 40.39\nSEM ⇒TEM ⇒IFM 27.61 33.62 40.89\nSEM ⇒IFM 22.76 30.06 37.56\nTable 1: Effects of the stacking order of decoder sub-\nlayers on translation quality in terms of BLEU score.\nDepth En-De En-Zh En-Fr Ave.\n6 27.45 32.24 40.39 33.36\n4 27.52 31.35 40.37 33.08\n12 27.64 32.50 40.44 33.53\nTable 2: Effects of various decoder depths on transla-\ntion quality in terms of BLEU score.\nformation up to a turning point of 3rd or 4th layer,\nafter which it starts only attending to the most rele-\nvant source tokens and contains decreasing amount\nof total source information.\nTEM Modules Since TEM representations serve\nas the query vector for encoder attention opera-\ntions (shown in Figure 1), we naturally hypothesize\nthat TEM is helping SEM on building alignments.\nTo verify that, we remove TEM from the decoder\n(“SEM⇒IFM”), which signiﬁcantly increases the\nalignment error from 0.37 to 0.54 (in Figure 5),\nand leads to a serious decrease of translation per-\nformance (BLEU: 27.45 ⇒22.76, in Table 1) on\nEn-De, while results on En-Zh also conﬁrms it (in\nFigure 6). This indicates that TEM is essential for\nbuilding word alignment.\nHowever, reordering the stacking of TEM and\nSEM (“SEM⇒TEM⇒IFM”) does not affect the\nalignment or translation qualities (BLEU: 27.45\nvs. 27.61). These results provide empirical sup-\nport for recent work on merging TEM and SEM\nmodules (Zhang et al., 2019).\nRobustness to Decoder Depth To verify the ro-\nbustness of our conclusions, we vary the depth\nof NMT decoder and train it from scratch. Ta-\nble 2 demonstrates the results on translation qual-\nity, which generally show that more decoder layers\nbring better performance. Figure 7 shows that SEM\nbehaves similarly regardless of depth. These results\ndemonstrate the robustness of our conclusions.\n3.3 Information Fusion in Decoder\nWe now turn to the analysis of IFM. Within the\nTransformer decoder, IFM plays the critical role of\nfusing the source and target information by merg-\n4805\nEnglish-Chinese\nCoverage Ratio\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nEnglish-German\nCoverage Ratio\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n(a) Word Alignment\nEnglish-Chinese\nCoverage Ratio\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nEnglish-German\nCoverage Ratio\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n (b) Cumulative Coverage\nFigure 7: Effects of decoder depths on SEM behaviors\non the En-De task.\nModel Self-Attn. Enc-Attn. FFN\nBase 6.3M 6.3M 12.6M\nBig 25.2M 25.2M 50.4M\nTable 3: Number of parameters taken by three major\noperations within Transformer Base and Big decoder.6\ning representations from SEM and TEM. To study\nthe information fusion process, we conduct a more\nﬁne-grained analysis on IFM at the operation level.\nFine-Grained Analysis on IFM As shown in\nFigure 8(a), IFM contains three operations:\n•Add-NormI linearly sums and normalizes the\nrepresentations from SEM and TEM;\n•Feed-Forward non-linearly transforms the\nfused source and target representations;\n•Add-NormII again linearly sums and normal-\nizes the representations from the above two.\nIFM Analysis Results Figures 8 (b) and (c) re-\nspectively illustrate the source and target infor-\nmation evolution within IFM. Surprisingly, Add-\nNormI contains a similar amount of, if not more,\nsource (and target) information than Add-NormII,\nwhile the Feed-Forward curve deviates signiﬁcantly\nfrom both. This indicates that the residual Feed-\nForward operation may not affect the source (and\ntarget) information evolution, and one Add&Norm\noperation may be sufﬁcient for information fusion.\nSimpliﬁed Decoder To empirically demonstrate\nwhether one Add&Norm operation is already sufﬁ-\ncient, we remove all other operations, leaving just\none Add&Norm operation for the IFM. The archi-\ntectural change is illustrated in Figure 9(b), and we\ndub it the “simpliﬁed decoder”.\n(a) Three Operations in IFM\n(b) Source PPL\n (c) Target PPL\nFigure 8: Illustration of (a) three operations within\nIFM, and (b,c) the source and target information evo-\nlution within IFM on En-De task.\nSimpliﬁed Decoder Results Table 4 reports the\ntranslation performance of both architectures on\nall three major datasets, while Figure 10 illustrates\nthe information evolution of both on WMT En-\nDe. We ﬁnd the simpliﬁed model reaches com-\nparable performance with only a minimal drop of\n0.1-0.3 BLEU on En-De and En-Fr, while observ-\ning 0.9 BLEU gains on En-Zh.7 To further assess\nthe translation performance, we manually evalu-\nate 100 translations sampled from the En-Zh test\nset. On the scale of 1 to 5, we ﬁnd that the sim-\npliﬁed decoder obtains a ﬂuency score of 4.01 and\nan adequacy score of 3.87, which is approximately\nequivalent to that of the standard decoder, i.e. 4.00\nfor ﬂuency and 3.86 for adequacy (in Table 5).\nOn the other hand, since the simpliﬁed decoder\ndrops the operations (FeedForward) with most pa-\nrameters (shown in Table 3), we also expect a sig-\nniﬁcant increase on training and inference speeds.\nFrom Table 4, we conﬁrm a consistent boost of\nboth training and inference speeds by approxi-\nmately 11-14%. To demonstrate the robustness, we\nalso conﬁrm our ﬁndings under Transformer big\nsettings (Vaswani et al., 2017), whose results are\n6As a comparison, the total number of parameters in Base\nand Big models are 62.9M and 213.9M respectively on En-De.\n7Simpliﬁed models are trained with the same hyper-\nparameters as standard ones, which may be suboptimal as\nthe number of parameters is signiﬁcantly reduced.\n4806\nTarget \nEmbedding\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\nTarget \nEmbedding\nMulti-Head \nAttention\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\n(a) Standard\nTarget \nEmbedding\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities\nTarget \nEmbedding\nMulti-Head \nAttention\nAdd & Norm\n⊕ ⇘ Positional \nEncoding\nN×\nMulti-Head \nAttention\nAdd & Norm\nSoftmax\nOutput \nProbabilities (b) Simpliﬁed\nFigure 9: Illustration of (a) the standard decoder, and\n(b) the simpliﬁed decoder with simpliﬁed IFM.\nshown in Section A.2. The lower PPL in Figure 10\nsuggests that the simpliﬁed model also contains\nconsistently more source and target information\nacross its stacking layers.\nOur results demonstrate that a single Add&Norm\nis indeed sufﬁcient for IFM, and the simpliﬁed\nmodel reaches comparable performance with a sig-\nniﬁcant parameter reduction and a noticable 11-\n14% boost on training and inference speed.\n4 Related Work\nInterpreting Encoder Representations Previ-\nous studies generally focus on interpreting the en-\ncoder representations by evaluating how informa-\ntive they are for various linguistic tasks (Conneau\net al., 2018; Tenney et al., 2019b), for both RNN\nmodels (Shi et al., 2016; Belinkov et al., 2017;\nBisazza and Tump, 2018; Blevins et al., 2018) and\nTransformer models (Raganato et al., 2018; Tang\net al., 2019a; Tenney et al., 2019a; Yang et al.,\n2019). Although they found that a certain amount\nof linguistic information is captured by encoder\nrepresentations, it is still unclear how much en-\ncoded information is used by the decoder. Our\nwork bridges this gap by interpreting how the Trans-\nformer decoder exploits the encoded information.\nInterpreting Encoder Self-Attention In recent\nyears, there has been a growing interest in inter-\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nEnDe-w/oFFN\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM --standard\nIFM --simplified\n(a) Source PPL\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nEnDe-w/oFFN\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nSource PPL\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nDecoder Layer\n1 2 3 4 5 6\nIFM --standard\nIFM --simplified (b) Target PPL\nFigure 10: Comparison of IFM information evolution\nbetween the standard and simpliﬁed decoder on En-De.\nDecoder BLEU #Train. #Infer.En-De\nStandard 27.45 63.93K 65.35\nSimpliﬁed 27.29 71.08K 72.93\n△ -0.16 +11.18% +11.60%\nEn-Zh\nStandard 32.24 32.49K 38.55\nSimpliﬁed 33.15 36.59K 54.06\n△ +0.91 +12.62% +40.23%\nEn-Fr\nStandard 40.39 68.28K 58.97\nSimpliﬁed 40.07 76.03K 67.23\n△ -0.32 +11.35% +14.01%\nTable 4: Performance of the simpliﬁed Base decoder.\n“#Train” denotes the training speed (words per second)\nand “#Infer.” denotes the inference speed (sentences\nper second). Results are averages of three runs.\npreting the behaviors of attention modules. Previ-\nous studies generally focus on the self-attention in\nthe encoder, which is implemented as multi-head\nattention. For example, Li et al. (2018) showed\nthat different attention heads in the encoder-side\nself-attention generally attend to the same position.\nV oita et al. (2019) and Michel et al. (2019) found\nthat only a few attention heads play consistent and\noften linguistically-interpretable roles, and others\ncan be pruned. Geng et al. (2020) empirically vali-\ndated that a selective mechanism can mitigate the\nproblem of word order encoding and structure mod-\neling of encoder-side self-attention. In this work,\nwe investigated the functionalities of decoder-side\nattention modules for exploiting both source and\ntarget information.\nInterpreting Encoder Attention The encoder-\nattention weights are generally employed to inter-\npret the output predictions of NMT models. Re-\ncently, Jain and Wallace (2019) showed that atten-\n4807\nModel Fluency Adequacy\nStandard (Base) 4.00 3.86\nSimpliﬁed (Base) 4.01 3.87\nTable 5: Human evaluation of translation performance\nof both standard and simpliﬁed decoders on 100 sam-\nples from En-Zh test set, on the scale of 1 to 5.\ntion weights are weakly correlated with the con-\ntribution of source words to the prediction. He\net al. (2019) used the integrated gradients to bet-\nter estimate the contribution of source words. Re-\nlated to our work, Li et al. (2019) and Tang et al.\n(2019b) also conducted word alignment analysis\non the same De-En and Zh-En datasets with Trans-\nformer models8. We use similar techniques to ex-\namine word alignment in our context; however, we\nalso introduce a forced-decoding-based probing\ntask to closely examine the information ﬂow.\nUnderstanding and Improving NMT Recent\nwork started to improve NMT based on the ﬁnd-\nings of interpretation. For instance, Belinkov et al.\n(2017, 2018) pointed out that different layers prior-\nitize different linguistic types, based on which Dou\net al. (2018) and Yang et al. (2019) simultaneously\nexposed all of these signals to the subsequent pro-\ncess. Dalvi et al. (2017) explained why the decoder\nlearns considerably less morphology than the en-\ncoder, and then explored to explicitly inject mor-\nphology in the decoder. Emelin et al. (2019) argued\nthat the need to represent and propagate lexical fea-\ntures in each layer limits the model’s capacity, and\nintroduced gated shortcut connections between the\nembedding layer and each subsequent layer. Wang\net al. (2020) revealed that miscalibration remains\na severe challenge for NMT during inference, and\nproposed a graduated label smoothing that can im-\nprove the inference calibration. In this work, based\non our information probing analysis, we simpliﬁed\nthe decoder by removing the residual feedforward\nmodule in totality, with minimal loss of translation\nquality and a signiﬁcant boost of both training and\ninference speeds.\n5 Conclusions\nIn this paper, we interpreted NMT Transformer\ndecoder by assessing the evolution of both source\n8We ﬁnd our results are more similar to that of Tang et al.\n(2019b). Also, our results are reported on the En ⇒De and\nEn⇒Zh directions, while they report results in the inverse\ndirections.\nand target information across layers and modules.\nTo this end, we investigated the information func-\ntionalities of decoder components in the transla-\ntion process. Experimental results on three major\ndatasets revealed several ﬁndings that help under-\nstand the behaviors of Transformer decoder from\ndifferent perspectives. We hope that our analysis\nand ﬁndings could inspire architectural changes for\nfurther improvements, such as 1) improving the\nword alignment of higher SEMs by incorporating\nexternal alignment signals; 2) exploring the stack-\ning order of SEM, TEM and IFM sub-layers, which\nmay provide a more effective way to transform in-\nformation; 3) further pruning redundant sub-layers\nfor efﬁciency.\nSince our analysis approaches are not limited\nto the Transformer model, it is also interesting to\nexplore other architectures such as RNMT (Chen\net al., 2018), ConvS2S (Gehring et al., 2017), or\non document-level NMT (Wang et al., 2017, 2019).\nIn addition, our analysis methods can be applied\nto other sequence-to-sequence tasks such as sum-\nmarization and grammar error correction, whose\nsource and target sides are in the same language.\nWe leave those tasks for future work.\nAcknowledgments\nTadepalli acknowledges the support of DARPA un-\nder grant number N66001-17-2-4030. The authors\nthank the anonymous reviewers for their insightful\nand helpful comments.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In ICLR.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. What do neural ma-\nchine translation models learn about morphology?\nIn ACL.\nYonatan Belinkov, Llu ´ıs M `arquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass. 2018.\nEvaluating layers of representation in neural ma-\nchine translation on part-of-speech and semantic tag-\nging tasks. arXiv preprint arXiv:1801.07772.\nArianna Bisazza and Clara Tump. 2018. The lazy en-\ncoder: A ﬁne-grained analysis of the role of mor-\nphology in neural machine translation. In EMNLP.\n4808\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018.\nDeep rnns encode soft hierarchical syntax. In ACL.\nHerv´e Bourlard and Yves Kamp. 1988. Auto-\nassociation by multilayer perceptrons and singular\nvalue decomposition. Biological Cybernetics.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Mike Schuster, Zhifeng Chen,\net al. 2018. The best of both worlds: Combining\nrecent advances in neural machine translation. ACL.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nYou Can Cram into A Single $&!#∗Vector: Prob-\ning Sentence Embeddings for Linguistic Properties.\nACL.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, and Stephan V ogel. 2017. Understanding\nand improving morphological learning in the neural\nmachine translation decoder. In IJCNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi,\nand Tong Zhang. 2018. Exploiting deep representa-\ntions for neural machine translation. In EMNLP.\nDenis Emelin, Ivan Titov, and Rico Sennrich. 2019.\nWidening the representation bottleneck in neural ma-\nchine translation with lexical shortcuts. In Proceed-\nings of the Fourth Conference on Machine Transla-\ntion (Volume 1: Research Papers).\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In ICML.\nXinwei Geng, Longyue Wang, Xing Wang, Bing Qin,\nTing Liu, and Zhaopeng Tu. 2020. How Does Selec-\ntive Mechanism Improve Self-Attention Networks?\nIn ACL.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In CVPR.\nShilin He, Zhaopeng Tu, Xing Wang, Longyue Wang,\nMichael Lyu, and Shuming Shi. 2019. Towards un-\nderstanding neural machine translation with word\nimportance. In EMNLP.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. In NAACL.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nXiang Kong, Zhaopeng Tu, Shuming Shi, Eduard\nHovy, and Tong Zhang. 2019. Neural machine trans-\nlation with adequacy-oriented learning. In AAAI.\nJian Li, Zhaopeng Tu, Baosong Yang, Michael R Lyu,\nand Tong Zhang. 2018. Multi-head attention with\ndisagreement regularization. EMNLP.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In ACL.\nYang Liu and Maosong Sun. 2015. Contrastive unsu-\npervised word alignment with non-local features. In\nAAAI.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In\nNeurIPS.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL-HLT.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. arXiv.\nAlessandro Raganato, J¨org Tiedemann, et al. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In BlackboxNLP Work-\nshop.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2018. DiSAN:\nDirectional Self-Attention Network for RNN/CNN-\nFree Language Understanding. In AAAI.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In\nEMNLP.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In EMNLP.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019a.\nEncoders help you disambiguate word senses in neu-\nral machine translation. In EMNLP.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019b.\nUnderstanding neural machine translation by sim-\npliﬁcation: The case of encoder-free models. In\nRANLP.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical nlp pipeline. In ACL.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2019b. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In ICLR.\n4809\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie,\nYoshua Bengio, and Pierre-Antoine Manzagol. 2010.\nStacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local de-\nnoising criterion. Journal of Machine Learning Re-\nsearch.\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. In NIPS.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In ACL.\nLongyue Wang, Zhaopeng Tu, Xing Wang, and Shum-\ning Shi. 2019. One model to learn both: Zero pro-\nnoun prediction and translation. In EMNLP.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu.\n2017. Exploiting cross-sentence context for neural\nmachine translation. In EMNLP.\nShuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu.\n2020. On the Inference Calibration of Neural Ma-\nchine Translation. In ACL.\nBaosong Yang, Jian Li, Derek Wong, Lidia S. Chao,\nXing Wang, and Zhaopeng Tu. 2019. Context-\nAware Self-Attention Networks. In AAAI.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Assessing\nthe ability of self-attention networks to learn word\norder. In ACL.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. In EMNLP.\n4810\nA Additional Results\nA.1 Implementation Details\nAll transformer models are selected based on their\nloss on validation set, while evaluated and reported\non the test set. For En-De and En-Fr models,\nwe used newstest2013 as validation set and new-\nstest2014 as test set. For En-Zh models, we used\nnewsdev2016 as validation set and newstest2017\nas test set.\nAll three datasets follow the prepossessing steps\nfrom FairSeq9, which uses Moses tokenizer10, with\na joint BPE of 40000 steps, while does not include\nlower-casing nor true-casing.\nAll models are evaluated with a beam size of\n10. Before evaluating the BLEU score, we apply a\npostprocessing step, where En-De and En-Fr gen-\nerations apply compound word splitting11, and En-\nZh generations apply Chinese word splitting (into\nChinese characters). All generations are then eval-\nuated with Moses multi-bleu.perl script12 against\nthe golden references.\nA.2 Transformer Big Results\nWe also compare the performance of the standard\nand simpliﬁed decoder under Transformer Big set-\nting. Big models are trained on 4 NVIDIA V100\nchips, where each is allocated with a batch size of\n8,192 tokens. Other training schedules and hyper-\nparameters are the same as standard (Vaswani et al.,\n2017). Also, our Transformer Base models are all\ntrained with full precision (FP32), while Big mod-\nels are all trained with half precision (FP16) for\nfaster training.\nTransformer Big results are shown in Table. 6.\nWe could observe a more severe BLEU score drop\nwith a more signiﬁcant speed boosting under Big\nsetting. This is very intuitive, compared to Base\nsetting, the simpliﬁed decoder drops more parame-\nters, while still trained under the same schedule as\nstandard, thus escalating the training discrepancy.\nUnfortunately due to the resource limitation, we\n9https://github.com/pytorch/fairseq/\nblob/master/examples/translation/\nprepare-wmt14en2de.sh\n10https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ntokenizer/mosestokenizer/tokenizer.py\n11https://gist.github.com/myleott/\nda0ea3ce8ee7582b034b9711698d5c16\n12https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl\nDecoder BLEU #Train. #Infer.En-De\nStandard 28.66 103.7K 74.3\nSimpliﬁed 28.20 125.2K 90.5\n△ -0.46 +20.7% +21.8%\nEn-Zh\nStandard 34.48 71.3K 30.5\nSimpliﬁed 34.35 82.6K 46.0\n△ -0.13 +15.8% +50.8%\nEn-Fr\nStandard 42.48 113.8K 65.7\nSimpliﬁed 42.19 138.1K 80.9\n△ -0.29 +21.4% +23.1%\nTable 6: Performance of the simpliﬁed Big decoder.\n“#Train” denotes the training speed (words per second)\nand “#Infer.” denotes the inference speed (sentences\nper second).\n(a) Source PPL\n (b) Target PPL\nFigure 11: Illustration of the source and target informa-\ntion evolution within IFM on En-Zh.\ncould not afford hyper-parameter tuning for Trans-\nformer.\nA.3 Additional En-Zh and En-Fr Plots\nAll experiments are conducted on three datasets\n(En-De, En-Zh and En-Fr), where we have similar\nﬁndings. Due to space limits, we mainly demon-\nstrate results on En-De task in our paper. In this\nsection, we provide additional results on En-Zh and\nEn-Fr if applicable.\n4811\nEnglish-Chinese\nCoverage Ratio\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nEnglish-German\nCoverage Ratio\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n(a) Word Alignment\nEnglish-Chinese\nCoverage Ratio\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nEnglish-German\nCoverage Ratio\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n6 Layer\n4 Layer\n12 Layer\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n2 4 6 8 10 12\n (b) Cumulative Coverage\nFigure 12: Effects of decoder depths on SEM behaviors\non En-Zh.\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n(a) Word Alignment\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n (b) Cumulative Coverage\nFigure 13: Comparison between standard and simpli-\nﬁed model on SEM behaviors on En-De.\nTarget PPL\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nDecoder Layer\n1 2 3 4 5 6\nIFM\nSEM\nTEM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n(a) Word Alignment\nEnglish-Chinese\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\nEnglish-German\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nStandard\nSimplified\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n AER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\nAER\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\n Coverage Ratio\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDecoder Layer\n1 2 3 4 5 6\nTEM=>SEM=>IFM\nSEM=>TEM=>IFM\nSEM=>IFM\n (b) Cumulative Coverage\nFigure 14: Comparison between standard and simpli-\nﬁed model on SEM behaviors on En-Zh."
}