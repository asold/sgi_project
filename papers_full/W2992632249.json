{
  "title": "Semantic Mask for Transformer based End-to-End Speech Recognition",
  "url": "https://openalex.org/W2992632249",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2356177428",
      "name": "Wang Cheng-yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100338995",
      "name": "Wu Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2463346314",
      "name": "Du Yujiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133789678",
      "name": "Li Jinyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2010893203",
      "name": "Liu Shu-jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101202197",
      "name": "Lu Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2368042017",
      "name": "Ren, Shuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747981504",
      "name": "Ye, Guoli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151703812",
      "name": "Zhao Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou, Ming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251321385",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963362078",
    "https://openalex.org/W2994771587",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2085628288",
    "https://openalex.org/W2977728428",
    "https://openalex.org/W2740433069",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2250357346",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W1494198834"
  ],
  "abstract": "Attention-based encoder-decoder model has achieved impressive results for both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This approach takes advantage of the memorization capacity of neural networks to learn the mapping from the input sequence to the output sequence from scratch, without the assumption of prior knowledge such as the alignments. However, this model is prone to overfitting, especially when the amount of training data is limited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic mask based regularization for training such kind of end-to-end (E2E) model. The idea is to mask the input features corresponding to a particular output token, e.g., a word or a word-piece, in order to encourage the model to fill the token based on the contextual information. While this approach is applicable to the encoder-decoder framework with any type of neural network architecture, we study the transformer-based model for ASR in this work. We perform experiments on Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art performance on the test set in the scope of E2E models.",
  "full_text": "Semantic Mask for Transformer based End-to-End Speech Recognition\nChengyi Wang♦, Yu Wu♦,\n(Alphabetical Order) Yujiao Du‡∗, Jinyu Li†, Shujie Liu♦, Liang Lu†, Shuo Ren♦,\nGuoli Ye†, Sheng Zhao†, Ming Zhou♦\n♦Microsoft Research Asia, Beijing\n†Microsoft Speech and Language Group\n‡Beijing University of Posts and Telecommunications\n{v-chengw, Wu.Yu, jinyli, shujliu, lial, v-shure, guoye, Sheng.Zhao,\nmingzhou}@microsoft.com\nmagicbubble95@gmail.com\nAbstract\nAttention-based encoder-decoder model has achieved impres-\nsive results for both automatic speech recognition (ASR) and\ntext-to-speech (TTS) tasks. This approach takes advantage\nof the memorization capacity of neural networks to learn the\nmapping from the input sequence to the output sequence from\nscratch, without the assumption of prior knowledge such as the\nalignments. However, this model is prone to overﬁtting, espe-\ncially when the amount of training data is limited. Inspired by\nSpecAugment and BERT, in this paper, we propose a seman-\ntic mask based regularization for training such kind of end-to-\nend (E2E) model. The idea is to mask the input features cor-\nresponding to a particular output token, e.g., a word or a word-\npiece, in order to encourage the model to ﬁll the token based\non the contextual information. While this approach is appli-\ncable to the encoder-decoder framework with any type of neu-\nral network architecture, we study the transformer-based model\nfor ASR in this work. We perform experiments on Librispeech\n960h and TedLium2 data sets, and achieve the state-of-the-art\nperformance on the test set in the scope of E2E models.\nIndex Terms: End to End ASR, Transformer, Semantic Mask\n1. Introduction\nEnd-to-end (E2E) acoustic models, particularly with the\nattention-based encoder-decoder framework [1], have achieved\na competitive recognition accuracy in a wide range of speech\ndatasets [2]. This model directly learns the mapping from the\ninput acoustic signals to the output transcriptions without de-\ncomposing the problems into several different modules such\nas lexicon modeling, acoustic modeling and language model-\ning as in the conventional hybrid architecture. While this kind\nof E2E approach signiﬁcantly simpliﬁes the speech recognition\npipeline, the weakness is that it is difﬁcult to tune the strength of\neach component. One particular problem from our observations\nis that the attention based E2E model tends to make grammati-\ncal errors, which indicates that the language modeling power of\nthe model is weak, possibly due to the small amount of train-\ning data, or the mismatch between the training and evaluation\ndata. However, due to the jointly model approach in the at-\ntention model, it is unclear how to improve the strength of the\nlanguage modeling power, i.e., attributing more weights to the\nprevious output tokens in the decoder, or to improve the strength\nof the acoustic modeling power, i.e., attributing more weights to\nthe context vector from the encoder.\n∗Works are done during internship at Microsoft\nWhile an external language model may be used to mitigate\nthe weakness of the language modeling power of an attention-\nbased E2E model, by either re-scoring the hypothesis or through\nshallow or deep fusion [3], the improvements are usually lim-\nited, and it incurs additional computational cost. Inspired by\nSpecAgument [4] and BERT [5], we propose a semantic mask\napproach to improve the strength of the language modeling\npower in the attention-based E2E model, which, at the same\ntime, improves the generalization capacity of the model as\nwell. Like SpecAugment, this approach masks out partial of the\nacoustic features during model training. However, instead of\nusing a random mask as in SpecAugment, our approach masks\nout the whole patch of the features corresponding to an output\ntoken during training, e.g., a word or a word-piece. The moti-\nvation is to encourage the model to ﬁll in the missing token (or\ncorrect the semantic error) based on the contextual information\nwith less acoustic evidence, and consequently, the model may\nhave a stronger language modeling power and is more robust to\nacoustic distortions.\nIn principle, our approach is applicable to the attention-\nbased E2E framework with any type of neural network encoder.\nTo constrain our research scope, we focus on the transformer ar-\nchitecture [6], which is originally proposed for neural machine\ntranslation. Recently, it has been shown that the transformer\nmodel can achieve competitive or even higher recognition accu-\nracy compared with the recurrent neural network (RNN) based\nE2E model for speech recognition [7]. Compared with RNNs,\nthe transformer based encoder can capture the long-term corre-\nlations with a computational complexity of O(1), instead of us-\ning many steps of back-propagation through time (BPTT) as in\nRNNs. We evaluate our transformer model with semantic mask-\ning on Librispeech and TedLium datasets. We show that seman-\ntic masking can achieve signiﬁcant word error rate reduction\n(WER) on top of SpecAugment, and we report the lowest WERs\non the test sets of the Librispeech corpus with an E2E model.\nWe release our code, training scripts, and pre-train models at\nhttps://github.com/MarkWuNLP/SemanticMask\n2. Related Work\nAs aforementioned, our approach is closely related to SpecAug-\nment [4], which applies a random mask to the acoustic features\nto regularize an E2E model. However, our masking approach is\nmore structured in the sense that we mask the acoustic signals\ncorresponding to a particular output token. Besides the beneﬁt\nin terms of model regularization, our approach also encourages\narXiv:1912.03010v2  [cs.CL]  16 Mar 2020\nFigure 1: An example of semantic mask\nthe model to reconstruct the missing token based on the con-\ntextual information, which improves the power of the implicit\nlanguage model in the decoder. The masking approach operates\nas the output token level is also similar to the approach used\nin BERT [5], but with the key difference that our approaches\nworks in the acoustic space.\nIn terms of the model structure, the transformer-based E2E\nmodel has been investigated for both attention-based framework\n[8, 7] as well as RNN-T based models [9, 10]. Our model struc-\nture generally follows [11], with a minor difference that we used\na deeper CNN before the self-attention blocks. We used a joint\nCTC/Attention loss to train our model following [7].\n3. Semantic Masking\n3.1. Masking Strategy\nOur masking approach requires the alignment information in\norder to perform the token-wise masking as shown in Figure\n1. There are multiple speech recognition toolkits available to\ngenerate such alignments. In this work, we used the Montreal\nForced Aligner1trained with the training data to perform forced-\nalignment between the acoustic signals and the transcriptions to\nobtain the word-level timing information. During model train-\ning, we randomly select a percentage of the tokens and mask\nthe corresponding speech segments in each iteration. Following\n[5], in our work, we randomly sample 15% of the tokens and\nset the masked piece to the mean value of the whole utterance.\nIt should be noted that the semantic masking strategy is easy\nto combine with the previous SpecAugment masking strategy.\nTherefore, we adopt a time warp, frequency masking and time\nmasking strategy in our masking strategy.\n3.2. Why Semantic Mask Works?\nSpectrum augmentation [4] is similar to our method, since both\npropose to mask spectrum for E2E model training. However,\nthe intuitions behind these two methods are different. SpecAug-\nment randomly masks spectrum in order to add noise to the\nsource input, making the E2E ASR problem harder and prevents\nthe over-ﬁtting problem in a large E2E model.\n1https://github.com/MontrealCorpusTools/\nMontreal-Forced-Aligner\nFigure 2: CNN layer architecture.\nIn contrast, our model aims to force the decoder to learn\na better language model. Suppose that if a few words’ speech\nfeatures are masked, the E2E model has to predict the token\nbased on other signals, such as tokens that have generated or\nother unmasked speech features. In this way, we might alleviate\nthe over-ﬁtting issue that generating words only considering its\ncorresponding speech features while ignoring other useful fea-\ntures. We believe our model is more effective when the input\nis noisy, because a model may generate correct tokens without\nconsidering previous generated tokens in a noise-free setting but\nit has to consider other signals when inputs are noisy, which is\nconﬁrmed in our experiment.\nMoreover, our method slightly reduces the hyper-parameter\ntuning workload of SpecAugment and is more robust when the\nvariance of input audio length is large. 1) Time mask multi-\nplicity and size of each mask are two hyper-parameters, which\nis required to be tuned based on different averaged utterance\nlengths and speaker speaking speeds. We believe that the hyper-\nparameter in our method, word masking ratio, is more stable\nwith thre pre-computation alignment information. 2) A ﬁxed\nmasking strategy may be too large for short utterances, and too\nsmall for long utterances. Our method masks a utterance based\non its total word count, addressing the length variance prob-\nlem in traditional SpecAugment. Concurrently, [12] proposes\nan adaptive SpecAug method to handle input audio with large\nlength variance.\n4. Model\nFollowing [11], we add convolution layers before Transformer\nblocks and discard the widely used positional encoding compo-\nnent. According to our preliminary experiments, the convolu-\ntion layers slightly improve the performance of the E2E model.\nIn the following, we will describe the CNN layers and Trans-\nformer block respectively.\n4.1. CNN Layer\nWe represent input signals as a sequence of log-Mel ﬁlter bank\nfeatures, denoted as X = (x0 ...,x n), where xi is a 83-dim\nvector. Since the length of spectrum is much longer than text,\nwe use VGG-like convolution block [13] with layer normal-\nization and max-pooling function. The speciﬁc architecture is\nshown in Figure 2 . We hope the convolution block is able to\nlearn local relationships within a small context and relative po-\nsitional information. According to our experiments, the spe-\nciﬁc architecture outperforms Convolutional 2D subsampling\nmethod [7]. We also use 1D-CNN in the decoder to extract\nlocal features replacing the position embedding 2.\n4.2. Transformer Block\nOur Transformer architecture is implemented as [7], depicting\nin Figure 3. The transformer module consumes the outputs\nof CNN and extracts features with a self-attention mechanism.\nSuppose that Q, Kand V are inputs of a transformer block, its\noutputs are calculated by the following equation\nSelfAttention(Q,K,V) =softmax( QK√dk\n)V, (1)\nwhere dk is the dimension of the feature vector. To enable deal-\ning with multiple attentions, multi-head attention is proposed,\nwhich is formulated as\nMultihead(Q,K,V) = [H1 ... Hdhead ]Whead (2)\nwhere Hi = SelfAttention(Qi,Ki,Vi)\nwhere dhead is the number of attention heads. Moreover, resid-\nual connection [14], feed-forward layer and layer normalization\n[15] are indispensable parts in Transformer, and their combina-\ntions are shown in Figure 3.\n4.3. ASR Training and Decoding\nFollowing previous work [7], we employ a multi-task learning\nstrategy to train the E2E model. Formally speaking, both the\nE2E model decoder and the CTC module predict the frame-wise\ndistribution of Y given corresponding source X, denoted as\nPs2s(Y|X) and Pctc(Y|X). We weighted averaged two nega-\ntive log likelihoods to train our model\nL= −αlog Ps2s(Y|X) −(1 −α) logPctc(Y|X). (3)\nwhere αis set to 0.7 in our experiment.\nWe combine scores of E2E model Ps2s, CTC score Pctc\nand a RNN based language model Prnn in the decoding pro-\ncess, which is formulated as\nP(yi|X,y<i) =Pctc(yi|X,y<i) +β1Ps2s(yi|X,y<i) (4)\n+ β2Prnn(yi|X,y<i),\nwhere β1 and β2 are tuned on the development set.\nWe rescore our beam outputs based on another right-to-\nleft language model Pr2l(Y) and the sentence length penalty\nWordcount(Y). The right-to-left language model is formulated\nas\nPr2l(Y) =\n1∏\ni=n\nP(yi|yn...yi+1). (5)\nIt is a common practice to rerank outputs of a left-to-right s2s\nmodel with a right-to-left language model in the NLP commu-\nnity [16], since the right-to-left model is more sensitive to the\nerrors existing in the right part of a sentence. We ﬁnd its perfor-\nmance is better than a left-to-right Transformer language model\nconsisting of more parameters.\nScore(Y) = logPs2s(Y|X) +γ1Wordcount(Y) (6)\n+ γ2 log Pr2l(Y).\nwhere Ptrans lm denotes the sentence generative probability\ngiven by a Transformer language model.\n2Experiment results show the Encoder CNN is more powerful than\nthe decoder CNN.\nFigure 3: E2E ASR model architecture.\n5. EXPERIMENT\nIn this section, we describe our experiments on LibriSpeech [2]\nand TedLium2 [17]. We compare our results with state-of-the-\nart hybrid and E2E systems. We implemented our approach\nbased on ESPnet [7], and the speciﬁc settings on two datasets\nare the same with [7], except the decoding setting. We use the\nbeam size 20, β1 = 0.5, and β2 = 0.7 in our experiment.\n5.1. Librispeech 960h\nWe represent input signals as a sequence of 80-dim log-Mel\nﬁlter bank with 3-dim pitch features [22]. SentencePiece is em-\nployed as the tokenizer, and the vocabulary size is 5000. We\ntrain a base model with 12 encoder layers, 6 decoders, and the\nattention vector size is 512 with 8 heads, containing 75M pa-\nrameters. To explore the effect of a large model, we enlarge\nthe model to 24 encoder layers and 12 decoder layer, obtaining\na 138M parameter model. The hyper-parameters of SpecAug-\nment follow [7] for a fair comparison. We use Adam algorithm\nto update the model, and the warmup step is 25000. The learn-\ning rate decreases proportionally to the inverse square root of\nthe step number after the 25000-th step. We train our model 40\nepochs (decrease epochs due to speed perturbation) on 4 P40\nGPUs, which approximately costs 5 days to coverage. We also\napply speed perturbation by changing the audio speed to 0.9, 1.0\nand 1.1. Following [7], we average the last 5 checkpoints as the\nﬁnal model. Unlike [18] and [20], we use the same checkpoint\nfor test-clean and test-other dataset.\nThe RNN language model uses the released LSTM lan-\nDev Test\nclean other clean other\nE2E Model\nRWTH (E2E) [18] 2.9 8.4 2.8 9.3\nLAS [4] - - 3.2 9.8\nLAS+SpecAugment [4] - - 2.5 5.8\nESPNET Transformer [7] 2.2 5.6 2.6 5.7\nWav2letter Transformers [19] 2.56 6.65 3.05 7.01\n+ LM Fusion [19] 2.11 5.25 2.30 5.64\n+ Rescore [19] 2.17 4.67 2.31 5.18\nBaseline Transformer 3.51 9.10 3.69 8.95\n+ LM Fusion 2.40 6.02 2.66 6.15\nBase Model with SpecAugment 3.33 9.05 3.57 9.00\n+ LM Fusion 2.20 5.73 2.39 5.94\nBase Model with Semantic Mask 2.93 7.75 3.04 7.43\n+ LM Fusion 2.09 5.31 2.32 5.55\n+ Speed Perturbation 2.07 5.06 2.31 5.21\n+ Rescore 2.05 5.01 2.24 5.12\nLarge Model with Semantic Mask2.64 6.90 2.74 6.65\n+ LM Fusion & Speed 2.02 4.91 2.19 5.19\n+ Rescore 1.98 4.78 2.08 4.95\nHybrid Model\nRWTH (HMM) [18] 1.9 4.5 2.3 5.0\nWang et al. [20] - - 2.60 5.59\n+ Rescore - - 2.26 4.85\nMulti-stream self-attention [21] 1.8 5.8 2.2 5.7\nTable 1: Comparison of the Librispeech ASR benchmark\nguage model provided by ESPnet 3. The architecture of right-\nto-left language model is the same as the RNN model provided\nby ESPnet, but it models a sentence from a different direction.\nWe evaluate our model in different settings. The base-\nline Transformer represents the model with position embedding.\nThe comparison between baseline Transformer and our archi-\ntecture (Model with SpecAugment) indicates the improvements\nattributed to the architecture. Model with semantic mask is we\nuse the semantic mask strategy on top of SpecAugment, which\noutperforms model with SpecAugment with a large margin in a\nno external language model fusion setting, demonstrating that\nour masking strategy helps the E2E model to learn a better lan-\nguage model. The gap becomes smaller when equipped with\na language model fusion component, which further conﬁrms\nour motivation in Section 1. Speed Perturbation does not help\nmodel performance on the clean dataset, but it is effective on\nthe test-other dataset. Rescore is beneﬁcial to both test-clean\nand test-other datasets. Larger model size (138M v.s. 75M)\nprovides over 10% performance gain when LM model is absent.\nAs far as we know, our model is the best E2E ASR system\non the Librispeech testset. Our model is built upon the code\nbase of ESPnet, and achieves relative 10% gains in a fair com-\nparison (75M base model), due to the better architecture and\nmasking strategy. The parameter number of the large model is\nsmaller than Wav2letter Transformer but it shows better perfor-\nmance. Comparing with hybrid methods, our model obtains a\nsimilar performance on the test-clean set, but is still worse than\nthe best hybrid model on the test-other dataset. Furthermore,\nthe improvement on test-other dataset is more signiﬁcant. The\nreason might be that our semantic masking is more suitable on\na noisy setting, because the input features are not reliable and\nthe model has to predict the next token relying on previous ones\nand the whole context of the input.\n3https://github.com/espnet/espnet/tree/\nmaster/egs/librispeech/asr1\nDev Test\nTime\nwrap\nTime\nmask\nFreq\nmask\nWord\nmask clean other clean other\n\u0017 \u0017 \u0017 \u0017 3.30 9.97 3.62 10.20\n\u0017 \u0013 \u0017 \u0017 2.37 6.94 2.63 7.02\n\u0017 \u0017 \u0017 \u0013 2.38 6.54 2.52 6.91\n\u0013 \u0013 \u0013 \u0017 2.20 5.73 2.39 5.94\n\u0013 \u0017 \u0013 \u0013 2.18 5.45 2.36 5.65\n\u0013 \u0013 \u0013 \u0013 2.09 5.31 2.32 5.55\nTable 2: Ablation test of different masking methods. The fourth\nline is a default setting of SpecAugment. The ﬁfth line uses word\nmask to replace random time mask, and the last line combines\nboth methods on the time axis.\nWe also analyze the performance of different masking\nstrategies, showing in Table 2, where all models are in model\nbased setting and shallow fused with the RNN language model.\nAccording to the comparison between the second line and the\nthird line, we ﬁnd that the word masking is slightly better than\nthe time masking if other SpecAugment operations are absent.\nWhen it combined with other SpecAugment methods, the word\nmasking is still better than the time masking, indicating the\nsuperior of the semantic masking. We also attempt to mask\nphrases and add white-noise to masked regions, but there is no\nimprovement according to several experiment comparisons.\nTest\nKaldi (Chain + TDNN + Large LM) 9.0\nESPnet RNN 11.0\nESPnet Transformer 10.4\n+SpecAugment 8.9\n+ LM Fusion 8.1\n+Semantic Mask 8.5\n+ LM Fusion 7.7\nTable 3: Experiment results on TEDLIUM2.\n5.2. TedLium2\nTo verify the generalization of the semantic mask, we further\nconduct experiments on TedLium2 [23] dataset, which is ex-\ntracted from TED talks. The corpus consists of 207 hours of\nspeech data accompanying 90k transcripts. For a fair compari-\nson, we use the same data-preprocessing method, Transformer\narchitecture and hyperparameter settings as in [7]. Our acoustic\nfeatures are 80-dim log-Mel ﬁlter bank and 3-dim pitch features,\nwhich is normalized by the mean and the standard deviation for\ntraining set. The utterances with more than 3000 frames or more\nthan 400 characters are discarded. The vocabulary size is set to\n1000.\nThe experiment results are listed in Table 3, showing a sim-\nilar trend as the results in Librispeech dataset. Semantic mask is\ncomplementary to specagumentation, which enables better S2S\nlanguage modeling training in an E2E model, resulting in a rel-\native 4.5% gain. The experiment proves the effectiveness of\nsemantic mask on a different and smaller dataset.\n6. Conclusion\nThis paper presents a semantic mask method for E2E speech\nrecognition, which is able to train a model to better consider\nthe whole audio context for the disambiguation. Moreover, we\nelaborate a new architecture for E2E model, achieving state-of-\nthe-art performance on the Librispeech test set in the scope of\nE2E models.\n7. References\n[1] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation\nby jointly learning to align and translate,” in 3rd International\nConference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.\n[Online]. Available: http://arxiv.org/abs/1409.0473\n[2] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur,\n“Librispeech: An ASR corpus based on public domain audio\nbooks,” in 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, ICASSP 2015, South Brisbane,\nQueensland, Australia, April 19-24, 2015 , 2015, pp. 5206–\n5210. [Online]. Available: https://doi.org/10.1109/ICASSP.2015.\n7178964\n[3] S. Toshniwal, A. Kannan, C. Chiu, Y . Wu, T. N. Sainath, and\nK. Livescu, “A comparison of techniques for language model\nintegration in encoder-decoder speech recognition,” in2018 IEEE\nSpoken Language Technology Workshop, SLT 2018, Athens,\nGreece, December 18-21, 2018 , 2018, pp. 369–375. [Online].\nAvailable: https://doi.org/10.1109/SLT.2018.8639038\n[4] D. S. Park, W. Chan, Y . Zhang, C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “Specaugment: A simple data augmentation method\nfor automatic speech recognition,” CoRR, vol. abs/1904.08779,\n2019. [Online]. Available: http://arxiv.org/abs/1904.08779\n[5] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for language\nunderstanding,” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers) , 2019, pp. 4171–4186. [Online]. Available:\nhttps://www.aclweb.org/anthology/N19-1423/\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is\nall you need,” in Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information\nProcessing Systems 2017, 4-9 December 2017, Long Beach,\nCA, USA , 2017, pp. 5998–6008. [Online]. Available: http:\n//papers.nips.cc/paper/7181-attention-is-all-you-need\n[7] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma,\nZ. Jiang, M. Someki, N. E. Y . Soplin, R. Yamamoto,\nX. Wang, S. Watanabe, T. Yoshimura, and W. Zhang,\n“A comparative study on transformer vs RNN in speech\napplications,” CoRR, vol. abs/1909.06317, 2019. [Online].\nAvailable: http://arxiv.org/abs/1909.06317\n[8] L. Dong, S. Xu, and B. Xu, “Speech-transformer: A no-\nrecurrence sequence-to-sequence model for speech recognition,”\nin 2018 IEEE International Conference on Acoustics, Speech\nand Signal Processing, ICASSP 2018, Calgary, AB, Canada,\nApril 15-20, 2018 , 2018, pp. 5884–5888. [Online]. Available:\nhttps://doi.org/10.1109/ICASSP.2018.8462506\n[9] A. Graves, “Sequence transduction with recurrent neural\nnetworks,” CoRR, vol. abs/1211.3711, 2012. [Online]. Available:\nhttp://arxiv.org/abs/1211.3711\n[10] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, “Transformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,” arXiv\npreprint arXiv:2002.02562, 2020.\n[11] A. Mohamed, D. Okhonko, and L. Zettlemoyer, “Transformers\nwith convolutional context for ASR,”CoRR, vol. abs/1904.11660,\n2019. [Online]. Available: http://arxiv.org/abs/1904.11660\n[12] D. S. Park, Y . Zhang, C.-C. Chiu, Y . Chen, B. Li, W. Chan, Q. V .\nLe, and Y . Wu, “Specaugment on large scale datasets,” arXiv\npreprint arXiv:1912.05533, 2019.\n[13] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” in 3rd International\nConference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.\n[Online]. Available: http://arxiv.org/abs/1409.1556\n[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV ,\nUSA, June 27-30, 2016 , 2016, pp. 770–778. [Online]. Available:\nhttps://doi.org/10.1109/CVPR.2016.90\n[15] L. J. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\nCoRR, vol. abs/1607.06450, 2016. [Online]. Available: http:\n//arxiv.org/abs/1607.06450\n[16] R. Sennrich, A. Birch, A. Currey, U. Germann, B. Haddow,\nK. Heaﬁeld, A. V . M. Barone, and P. Williams, “The univer-\nsity of edinburgh’s neural mt systems for wmt17,”arXiv preprint\narXiv:1708.00726, 2017.\n[17] A. Rousseau, P. Del ´eglise, and Y . Est `eve, “TED-LIUM: an\nautomatic speech recognition dedicated corpus,” in Proceedings\nof the Eighth International Conference on Language Resources\nand Evaluation, LREC 2012, Istanbul, Turkey, May 23-\n25, 2012 , 2012, pp. 125–129. [Online]. Available: http:\n//www.lrec-conf.org/proceedings/lrec2012/summaries/698.html\n[18] C. L ¨uscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer,\nR. Schl ¨uter, and H. Ney, “Rwth asr systems for librispeech: Hy-\nbrid vs attention,”Interspeech, Graz, Austria, pp. 231–235, 2019.\n[19] G. Synnaeve, Q. Xu, J. Kahn, E. Grave, T. Likhomanenko,\nV . Pratap, A. Sriram, V . Liptchinsky, and R. Collobert, “End-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,” arXiv preprint arXiv:1911.08460, 2019.\n[20] Y . Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., “Transformer-\nbased acoustic modeling for hybrid speech recognition,” arXiv\npreprint arXiv:1910.09799, 2019.\n[21] K. J. Han, R. Prieto, K. Wu, and T. Ma, “State-of-the-art\nspeech recognition using multi-stream self-attention with dilated\n1d convolutions,” CoRR, vol. abs/1910.00716, 2019. [Online].\nAvailable: http://arxiv.org/abs/1910.00716\n[22] P. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Trmal,\nand S. Khudanpur, “A pitch extraction algorithm tuned for\nautomatic speech recognition,” in IEEE International Conference\non Acoustics, Speech and Signal Processing, ICASSP 2014,\nFlorence, Italy, May 4-9, 2014 , 2014, pp. 2494–2498. [Online].\nAvailable: https://doi.org/10.1109/ICASSP.2014.6854049\n[23] A. Rousseau, P. Del ´eglise, and Y . Est `eve, “Enhancing the\nTED-LIUM corpus with selected data for language modeling\nand more TED talks,” in Proceedings of the Ninth International\nConference on Language Resources and Evaluation, LREC\n2014, Reykjavik, Iceland, May 26-31, 2014 , 2014, pp. 3935–\n3939. [Online]. Available: http://www.lrec-conf.org/proceedings/\nlrec2014/summaries/1104.html",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8100728988647461
    },
    {
      "name": "Transformer",
      "score": 0.6963012218475342
    },
    {
      "name": "Security token",
      "score": 0.6406211256980896
    },
    {
      "name": "End-to-end principle",
      "score": 0.59135502576828
    },
    {
      "name": "Language model",
      "score": 0.5886406898498535
    },
    {
      "name": "Speech recognition",
      "score": 0.5733007192611694
    },
    {
      "name": "Encoder",
      "score": 0.5602596998214722
    },
    {
      "name": "Overfitting",
      "score": 0.4772762656211853
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45028266310691833
    },
    {
      "name": "Artificial neural network",
      "score": 0.4476461112499237
    },
    {
      "name": "MNIST database",
      "score": 0.41890838742256165
    },
    {
      "name": "Voltage",
      "score": 0.10431772470474243
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 24
}