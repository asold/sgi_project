{
    "title": "Interpretable Embeddings for Next Point-of-Interest Recommendation via Large Language Model Question–Answering",
    "url": "https://openalex.org/W4404483747",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2110513954",
            "name": "Jiubing Chen",
            "affiliations": [
                "Jilin University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2117646672",
            "name": "Haoyu Wang",
            "affiliations": [
                "Jilin University"
            ]
        },
        {
            "id": "https://openalex.org/A2309043662",
            "name": "Jianxin Shang",
            "affiliations": [
                "Northeast Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2230486812",
            "name": "Chaomurilige",
            "affiliations": [
                "Minzu University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205780198",
        "https://openalex.org/W4390494325",
        "https://openalex.org/W4390905130",
        "https://openalex.org/W4226056947",
        "https://openalex.org/W4226330269",
        "https://openalex.org/W4380628120",
        "https://openalex.org/W4384895066",
        "https://openalex.org/W4290943973",
        "https://openalex.org/W4284696020",
        "https://openalex.org/W4284668299",
        "https://openalex.org/W4386746103",
        "https://openalex.org/W4396818449",
        "https://openalex.org/W4214823533",
        "https://openalex.org/W4323345594",
        "https://openalex.org/W2604411573",
        "https://openalex.org/W3128267727",
        "https://openalex.org/W4399657602",
        "https://openalex.org/W4386729952",
        "https://openalex.org/W4401110399",
        "https://openalex.org/W2573719245",
        "https://openalex.org/W1981886741",
        "https://openalex.org/W2808143564",
        "https://openalex.org/W1546409232",
        "https://openalex.org/W2205235818",
        "https://openalex.org/W1972243012",
        "https://openalex.org/W2788114581",
        "https://openalex.org/W3040157551",
        "https://openalex.org/W4289533859",
        "https://openalex.org/W4389076429",
        "https://openalex.org/W4382239876",
        "https://openalex.org/W4400525641",
        "https://openalex.org/W4393161195",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2539781657",
        "https://openalex.org/W2998167534",
        "https://openalex.org/W3080292238",
        "https://openalex.org/W3034912136",
        "https://openalex.org/W4393026959",
        "https://openalex.org/W3164797320"
    ],
    "abstract": "Next point-of-interest (POI) recommendation provides users with location suggestions that they may be interested in, allowing them to explore their surroundings. Existing sequence-based or graph-based POI recommendation methods have matured in capturing spatiotemporal information; however, POI recommendation methods based on large language models (LLMs) focus more on capturing sequential transition relationships. This raises an unexplored challenge: how to leverage LLMs to better capture geographic contextual information. To address this, we propose interpretable embeddings for next point-of-interest recommendation via large language model question–answering, named QA-POI, which transforms the POI recommendation task into obtaining interpretable embeddings via LLM prompts, followed by lightweight MLP fine-tuning. We introduce question–answer embeddings, which are generated by asking LLMs yes/no questions about the user’s trajectory sequence. By asking spatiotemporal questions about the trajectory sequence, we aim to extract as much spatiotemporal information from the LLM as possible. During training, QA-POI iteratively selects the most valuable subset of potential questions from a set of questions to prompt the LLM for the next POI recommendation. It is then fine-tuned for the next POI recommendation task using a lightweight Multi-Layer Perceptron (MLP). Extensive experiments on two datasets demonstrate the effectiveness of our approach.",
    "full_text": null
}