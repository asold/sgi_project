{
    "title": "Local Large Language Models for Complex Structured Tasks.",
    "url": "https://openalex.org/W4399317495",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2489397549",
            "name": "V. K. Cody Bumgardner",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A5034023878",
            "name": "Aaron Mullen",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A5065476157",
            "name": "Samuel E Armstrong",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A2564437643",
            "name": "Caylin Hickey",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A2477966631",
            "name": "Victor Marek",
            "affiliations": [
                "University of Kentucky"
            ]
        },
        {
            "id": "https://openalex.org/A2689396314",
            "name": "Jeff Talbert",
            "affiliations": [
                "University of Kentucky"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4367668587",
        "https://openalex.org/W2122653375",
        "https://openalex.org/W4362716434",
        "https://openalex.org/W4385330587",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4368367885",
        "https://openalex.org/W4322718832",
        "https://openalex.org/W4365503932",
        "https://openalex.org/W4321459182",
        "https://openalex.org/W4387356888"
    ],
    "abstract": "This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex language tasks. The authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local, fine-tuned LLMs to respond to specific generative instructions and provide structured outputs. Over 150k uncurated surgical pathology reports containing gross descriptions, final diagnoses, and condition codes were used. Different model architectures were trained and evaluated, including LLaMA, BERT, and LongFormer. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics. LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform structured generative tasks on domain-specific language in the medical domain.",
    "full_text": null
}