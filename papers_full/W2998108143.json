{
  "title": "Axial Attention in Multidimensional Transformers",
  "url": "https://openalex.org/W2998108143",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2978107110",
      "name": "Ho, Jonathan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221877697",
      "name": "Kalchbrenner, Nal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281193570",
      "name": "Weissenborn, Dirk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221615782",
      "name": "Salimans, Tim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2413794162",
    "https://openalex.org/W3005844337",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2097039814",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W2918222882",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2962990490",
    "https://openalex.org/W2963406904",
    "https://openalex.org/W2962750131",
    "https://openalex.org/W2963629403",
    "https://openalex.org/W2963428348",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2950946978",
    "https://openalex.org/W2948412951"
  ],
  "abstract": "We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.",
  "full_text": "AXIAL ATTENTION\nIN MULTIDIMENSIONAL TRANSFORMERS\nJonathan Ho∗\nUC Berkeley\nNal Kalchbrenner∗\nGoogle Brain\nDirk Weissenborn\nGoogle Brain\nTim Salimans\nGoogle Brain\nABSTRACT\nWe propose Axial Transformers, a self-attention-based autoregressive model for\nimages and other data organized as high dimensional tensors. Existing autoregres-\nsive models either suffer from excessively large computational resource require-\nments for high dimensional data, or make compromises in terms of distribution\nexpressiveness or ease of implementation in order to decrease resource require-\nments. Our architecture, by contrast, maintains both full expressiveness over joint\ndistributions over data and ease of implementation with standard deep learning\nframeworks, while requiring reasonable memory and computation and achieving\nstate-of-the-art results on standard generative modeling benchmarks. Our models\nare based on axial attention, a simple generalization of self-attention that natu-\nrally aligns with the multiple dimensions of the tensors in both the encoding and\nthe decoding settings. Notably the proposed structure of the layers allows for the\nvast majority of the context to be computed in parallel during decoding without\nintroducing any independence assumptions. This semi-parallel structure goes a\nlong way to making decoding from even a very large Axial Transformer broadly\napplicable. We demonstrate state-of-the-art results for the Axial Transformer on\nthe ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR\nRobotic Pushing video benchmark. We open source the implementation of Axial\nTransformers.\n1 I NTRODUCTION\nAutoregressive models are a family of exact likelihood-based generative models that represent the\njoint distribution of data x = (x1, . . . , xN) as a product of conditionals pθ(x) =∏N\ni=1 pθ(xi|x<i).\nNeural network models in this family have achieved state-of-the-art log likelihoods on high-\ndimensional image and video datasets (van den Oord et al., 2016a; Chen et al., 2018; Menick &\nKalchbrenner, 2018; Parmar et al., 2018; Child et al., 2019; Weissenborn et al., 2019; Salimans\net al., 2017; Kalchbrenner et al., 2017; Uria et al., 2016; Parikh et al., 2016; Theis & Bethge, 2015;\nvan den Oord et al., 2016b) due to architectural innovations that enable the following capabilities:\n1. Large, high information bandwidth receptive ﬁelds for each pixel xi, capable of expressing\nlong-range dependencies over previous pixels x<i, and\n2. Computationally efﬁcient, vectorizable computation of the log likelihood and its gradient.\nAutoregressive model architectures that can read long-range dependencies over large receptive ﬁelds\nare able to express all joint distributions over the data. Meanwhile, architectures that admit fast log\nlikelihood gradient computation are suitable for training using a stochastic gradient method on a\nmaximum likelihood objective—a straightforward, stable training procedure for generative models.\nThese desiderata make self-attention a compelling building block for autoregressive model architec-\ntures. Self-attention is a neural network operation that is able to transform a sequence y1, . . . , yN\ninto a sequence y′\n1, . . . , y′\nN, where each y′\ni depends on all yi by way of a single vectorizable com-\nputation (Vaswani et al., 2017). Self-attention is remarkably effective at learning long-range depen-\ndencies between data dimensions and neural networks that incorporate self-attention in their designs\nare state-of-the-art on many tasks from language modelling and machine translation to image and\nvideo modelling (Parmar et al., 2018; Child et al., 2019).\n1\narXiv:1912.12180v1  [cs.CV]  20 Dec 2019\nDURING SAMPLING\nG\nG\n[x4] [x4]\nB\nB\n[x4]\nShift down \nby one and \npad on top\nRun once per channel Run once per locationRun once per row\nFigure 1: The Axial Transformer model for 2-dimensional tensors. Before sampling a channel we\nencode all previous channels and frames with 8 blocks of unmasked row and unmasked column at-\ntention (left). Then, for each row, we apply 4 blocks of unmasked row and masked column attention\nto integrate the previously sampled rows for the active channels into our encoded representation\n(middle). Finally, we shift the encoded representation up to make sure the conditioning information\nsatisﬁes causality, and we run the inner decoder consisting of 4 blocks of masked row attention to\nsample a new row in the image (right).\nBut the power of self-attention comes at the price of computational complexity. The memory and\ncomputation it consumes grow quadratically with the sequence length N making it prohibitively\nexpensive to directly apply self-attention to long sequences. In the case of autoregressive models\nof multidimensional tensors such as images or videos, the aim to capture large receptive ﬁelds in\nmultiple dimensions further exacerbates the problem as even a modest number of receptive ﬁeld\nsteps in each dimension can encompass a large total number of locations. Various approaches have\nbeen proposed to alleviate this difﬁculty at the cost of either limiting the receptive ﬁeld or requiring\noperations that may not be broadly available on GPUs or TPUs.\nWe propose the Axial Transformer, a simple yet effective self-attention-based autoregressive model\nfor data organized as multidimensional tensors. Rather than applying attention to a ﬂattened string\nof tensor elements, our model instead applies attention along a single axis of the tensor without\nﬂattening—we refer to this as “axial attention.” Since the length of any single axis (that is, the height\nor width of an image) is typically much smaller than the total number of elements, an axial attention\noperation enjoys a signiﬁcant saving in computation and memory over standard self-attention: for\na d-dimensional tensor with shape N = N1/d ×···× N1/d, axial attention saves a O(N(d−1)/d)\nfactor of resources over standard self-attention.\nOur Axial Transformer architecture allows for the majority of the contextx<ito be embedded with a\nhigh degree of parallelism without introducing conditional independence assumptions among any of\nthe locations, but has an interesting property that it is amenable to a simple-to-implement fast sam-\npling procedure. To sample one row of an image, the Axial Transformer only runs an autoregressive\nTransformer over that one row only, without re-embedding pixels from previous rows. We structure\nthe Axial Transformer, however, so that it always deﬁnes a fully expressive joint distribution. No\ndependencies on previous pixels are ever lost.\nWe evaluate Axial Transformers on image and video modelling benchmarks. We show that Axial\nTransformer achieves state-of-the-art results on ImageNet-32 and on ImageNet-64. We also show\nthat, simply by stacking a video along the channel dimension, the Axial Transformer can be directly\napplied to the channel-stacked video without nearly any modiﬁcation. On the BAIR Robot Pushing\nbenchmark, the Axial Transformer signiﬁcantly outperforms previous results without using an archi-\ntecture specially designed for videos. The generated samples on these datasets are of the expected\nhigh quality.\nAxial Transformers do not require subroutines for GPUs or TPUs that may exhibit unfavorable\nmemory bandwidth and computation trade-offs. Axial Transformers are simple to implement using\n2\nFigure 2: Types of axial attention layers that are the building blocks of the Axial Transformer. The\nblue locations correspond to the receptive ﬁeld of the output red location.\nefﬁcient operations that are widely available in deep learning frameworks (primarily dense-dense\nMatMuls). An open source implementation of our models is available at anonymized URL.\n2 B ACKGROUND\nTo set the stage for our discussion, we ﬁrst review self-attention and its computational resource\nrequirements in the context of autoregressive modeling. A self-attention layer takes as input a length\nN sequence of D-dimensional embeddings X (a N ×D matrix) and produces an output sequence\nY (also a N ×D matrix) via:\nQ = XWQ, K = XWK, V = XWV\nA = softmax\n(\nQK⊤/\n√\nD\n)\n, Y = AV\nWQ, WK, and WV are D ×D parameter matrices responsible for projecting the entries of the\nsequence X into keys, queries, and values, respectively. Each entry of the output sequence Y is a\nlinear combination of values inV weighted by the attention matrixA, which itself is computed from\nsimilarities between all pairs of query and key vectors. Both the expressive power and the resource\ncost of self-attention come from computingA and Y : it takes O(N2) time and space to compute the\npairwise similarities between Q and K and to compute the linear combination of V vectors.\nThis quadratic complexity makes it impractical to apply self-attention to images and videos directly\nas ﬂattened vectors: a small 32×32×3 image has 3072 dimensions. Sequences such as these are too\nlong for self-attention, so attempts to scale self-attention to these modalities generally involve re-\nstricting these sequence lengths in a modality-aware manner while attempting to preserve modeling\nperformance.\nOne strategy is to restrict the conditioning context x<i to a carefully designed small subset of the\ndata dimensions. While this reduces the cost of attention, which is only performed over these small\nsubsets instead of the full data, the model can no longer express all joint distributions over the\ndata. Parmar et al. (2018) propose image models with conditioning context x<i restricted to a\nsmall window of the full image, but the implementation requires redundant data copies to extract\nand process these windows. Weissenborn et al. (2019) similarly scale video autoregressive models\nby restricting the context, again preventing their model from expressing all joint distributions over\npixels. Our models do not restrict context and hence we obtain better log likelihoods, as we will see\nin section 4.\nA different strategy is to stack multiple sparse attention layers, each with restricted context for\ncomputational efﬁciency, but in a manner that overlapping these layers yields a full-context model.\nChild et al. (2019) propose two sparse attention patterns with this property. However, the architecture\nthey propose that works best for images (the Strided Sparse Transformer) requires custom sparse\nattention GPU kernels to implement a speciﬁc block-sparse variant of matrix-matrix-multiply. The\nmodel cannot be easily implemented on other hardware such as TPUs.\nSee table 1 for a summary of these architecture design tradeoffs. Our goal in this paper is to de-\nsign attention-based autoregressive models that attain the best of all worlds. Our Axial Transformer,\ndescribed in subsequent sections, has a full conditioning context, so its ability to express joint distri-\nbutions is never limited. The Axial Transformer also does not require any redundant data copies or\ncustom kernels to implement in an efﬁcient way. Indeed, we designed, and will make open source,\nan efﬁcient implementation that uses only standard operations in deep learning libraries.\n3\nTable 1: Trade-offs of recently proposed multidimensional Transformer architectures.\nModel Full receptive\nﬁeld\nAttention faster\nthan O(N2)\nNeeds no\ncustom kernels\nSemi-parallel\ncontext aggregation\nTransformer (Vaswani et al., 2017) yes no yes no\nImage Transformer (Parmar et al., 2018) no yes yes no\nBlock Transformer (Weissenborn et al., 2019) no yes yes no\nStrided Sparse Transformer (Child et al., 2019) yes yes no no\nAxial Transformer (ours) yes yes yes yes\n3 A XIAL TRANSFORMERS\nWe now describe Axial Transformers, our self-attention-based autoregressive models for high-\ndimensional data tensors. We describe its basic building block in section 3.1 and then we complete\nthe description into a full autoregressive model in section 3.2.\n3.1 A XIAL ATTENTION\nWe ﬁrst introduce our basic building block for developing self-attention-based autoregressive models\nfor high-dimensional data tensors.\nThe proposed approach does not change the original shape of the multidimensional data tensor and\nperforms a masked or unmasked attention over a single axis of the tensor at a time. We call this\noperation axial attention, denoted by Attentionk(x). It performs attention over axis k of the tensor\nx, mixing information along axis k while keeping information along other axes independent. It is\nstraightforward to implement: axial attention over axisk can be implemented by transposing all axes\nexcept k to the batch axis, calling standard attention as a subroutine, then undoing the transpose (an\nalternative is to use the einsum operation available in most deep learning libraries).\nWhen the data is an image, we call Attention 1 column attention, as it mixes information within\ncolumns while keeping separate columns independent. We call Attention 2 row attentionfor anal-\nogous reasons. Axial attention on a square image of size N = S ×S performs attention on S\nsequences of length S—this is a total of O(S ·S2) =O(N\n√\nN) computation—an O(\n√\nN) savings\nin computation over standard self-attention. In general, for a d-dimensional tensor with N = Sd,\naxial attention saves O(N(d−1)/d) computation over standard attention. Of course, a single layer of\naxial attention along some axis k does not have the full receptive ﬁeld since it covers a single axis,\nbut we will see in section 3.2 that stacking two axial attention layers allows the model to obtain a\nglobal receptive ﬁeld.\nIt will be important for us to also deﬁne MaskedAttention k to be the causally masked variant of\nAttentionk: component i of the result of MaskedAttentionk(x) along axis k depends on only com-\nponents 1, . . . , iof x along axis k. The receptive ﬁelds of these attention patterns, both unmasked\nand masked, are illustrated in ﬁg. 2. We will use these masked blocks to build our autoregressive\nmodel in section 3.2.\nAxial attention can be used within standard Transformer layers in a straightforward manner to pro-\nduce Axial Transformer layers. The basic building blocks are the same as those found in the standard\nTransformer architecture:\n• LayerNorm(x): layer normalization (Ba et al., 2016), and\n• DenseD(x): a dense layer operating over the last axis of the input x. The letter D denotes\nthe dimension of the output activations. If the input has shape H ×W ×C, then this\noperation is identical to a 1 ×1 convolution, and the output has shape H ×W ×D.\nWe use these to deﬁne ResNet axial attention blocks operating on tensors ofD-dimensional embed-\ndings (Vaswani et al., 2017; Child et al., 2019):\n• FeedforwardBlock(x) =x + DenseD(Nonlinearity(DenseD′ (LayerNorm(x))))\n• AttentionBlockk(x) =x + DenseD(Attentionk(LayerNorm(x)))\n4\n•TransformerBlockk(x) =FeedforwardBlock(AttentionBlockk(x))\nD′is chosen to be some constant factor larger than D, from 1 to 4 (Vaswani et al., 2017). We also\ndeﬁne a MaskedTransformerBlockk using MaskedAttentionk in place of Attentionk.\nOperations similar to unmasked axial attention have been proposed in other contexts in computer\nvision (Huang et al., 2019). Our focus in forthcoming sections is the use of masked axial attention\nand its utility in autoregressive image modeling, which is not explored in these works.\n3.2 A XIAL TRANSFORMERS\nWe now describe Axial Transformers, our axial attention-based autoregressive models for images\nand videos. We will use the axial attention operations described in section 3.1 as building blocks in\na multi-layer autoregressive model of the form pθ(x) =∏N\ni=1 pθ(xi|x<i) following the raster scan\nordering of pixels. We will accomplish this by building an autoregressive model over rows (sec-\ntion 3.2.1), then conditioning each row on previous rows (section 3.2.1), then further conditioning\non previous channels and frames (section 3.2.2). Decomposing the model in this manner also leads\nto a simple fast and partly parallel sampling procedure (section 3.2.1).\n3.2.1 A MODEL FOR SINGLE -CHANNEL IMAGES\nWe begin with an autoregressive model for a single-channel image x with shape H ×W, with\neach pixel taking an integer value in [0, 255] representing its intensity. As is standard practice\nwith Transformers, pixel intensities are ﬁrst embedded into a H ×W ×D tensor of D-dimensional\nembeddings, which we callh. The architecture’s responsibility is to transformh into a H ×W ×256\ntensor of logits suitable for classiﬁcation or sampling. These logits must depend only on previous\npixels in the input x along the raster scan ordering to ensure that the architecture deﬁnes a valid\nautoregressive model.\nInner Decoder: a row-wise modelOur idea is to begin with masked row attention layers to create\na “row-wise” model:\nh ←Embed(x)\nh ←ShiftRight(h) +PositionEmbeddings\nh ←MaskedTransformerBlock2(h) ×Lrow\nHere, Lrow is the number of masked row attention blocks applied toh. PositionEmbeddings is a H ×\nW ×D tensor of position embeddings that inform the attention layers of the position. For parameter\nefﬁciency we use “additively factorized” position embeddings, meaning that we parameterize them\nas a broadcasted sum of H ×1 ×D embeddings for rows and 1 ×W ×D embeddings for columns.\nThe operation ShiftRight shifts the input right by one pixel, which has the effect of shifting the\nreceptive ﬁeld left by one pixel. This ensures that the masked row attention layers exclude the current\npixel from their receptive ﬁeld, which is crucial for architecture to deﬁne a correct autoregressive\nmodel.\nAs this model employs row attention only, it enjoys the computational efﬁciency beneﬁts described\nin section 3.1. However, it clearly does not deﬁne a full-context model because each location in\nthe output does not depend on input pixels in previous rows. If we were to use the resulting h as\nlogits for pixel intensity prediction, we would obtain a set of H independent autoregressive models\np(xi,j|xi,1, . . . , xi,j−1) for each row i ∈[1, H], not a single autoregressive model with full context.\nWe address this issue next.\nOuter Decoder: capturing the rows aboveEach pixel xi,j in the aforementioned model already\ndepends on previous pixels in its own row xi,<j. We just need to make it depend on all previous\nrows x<i,: too. So, we insert unmasked row and masked column layers in the beginning of the model\n5\nFigure 3: Arrangement of inputs to the encoding network of the Axial Transformer. Previously\navailable or generated channels of an image or video are sequentially stacked in the input. A variable\nnumber of padding planes are used as placeholders for future generated channels. A ﬁnal integer\nplane signals to the Axial Transformer the channel that is being generated at that step.\nas follows (newly inserted operations are underlined):\nh ←Embed(x)\nu ←h + PositionEmbeddings\nu ←MaskedTransformerBlock1(TransformerBlock2(u)) ×Lupper/2\nh ←ShiftDown(u) + ShiftRight(h) +PositionEmbeddings\nh ←MaskedTransformerBlock2(h) ×Lrow\nThe tensor u represents context captured above the current pixel. It is computed by unmasked row\nand masked column attention layers, repeated to a total of Lupper layers to increase model capacity,\nwhich make u cover the receptive ﬁeld at all rows above and including the current pixel. The\nShiftDown operation shifts u down one pixel, which shifts its receptive ﬁeld up one pixel. Thus we\nhave a context which captures all pixels above whileexcluding the current row, which we add toh as\ninput to the masked row layers. We have thus converted the row-wise model into a fully expressive\nautoregressive model that captures not only pixels in the current row but also those above.\nFollowing standard practice, we pass the ﬁnal h through layer normalization and a ﬁnal dense layer\nto produce logits with shape H ×W ×256. The logits at each location depend on all previous pixel\nlocations in the raster scan ordering.\nSemi-Parallel Sampling Naive implementations of sampling from sequential models are notori-\nously slow because they require re-evaluating the entire network to sample each location. In the\ncase of our model for a\n√\nN ×\n√\nN square image, each network evaluation takesO(N\n√\nN(Lupper +\nLrow)) time, so sampling the whole image would take O(N2√\nN(Lupper + Lrow)), which is far too\nlarge.\nFortunately, our architecture is amenable to a particularly simple implementation of a faster sampling\nthat is able to compute large sections of the model in parallel (see Figure 1). Pseudocode is as\nfollows:\n1. For each row i ∈[1, H]:\n(a) Compute the upper context u including information about all x<i,∗using the upper\nlayers\n(b) For each column j ∈[1, W]:\ni. Sample xi,j conditioned on u and prior elements of row i (xi,<j).\nBecause the Lrow row-wise layers are independent over rows (they depend on other rows only\nthrough the upper context, as explained in section 3.2.1), sampling one row can be accomplished\nby evaluating the row-wise layers for that one row only, completely ignoring other rows. Thus,\nin one row of\n√\nN pixels, each pixel can be sampled in O(NLrow), so all pixels can be sampled\nin O(N2Lrow). Before each of the\n√\nN rows can be sampled, the upper context must be com-\nputed in O(N\n√\nNLupper), for a total of O(N2Lupper) over the course of all rows. Thus we arrive at\nO(N2(Lupper+Lrow)) in total, which is\n√\nN faster than the naive implementation. To our knowledge,\nsampling speedups of this type are not possible with contemporary work on scaling Transformers to\nimages and videos (Child et al., 2019; Weissenborn et al., 2019).\n6\nTable 2: Unconditional and class-conditional image modeling results (bits/dim)\nModel ImageNet 32x32 ImageNet 64x64\nMultiscale PixelCNN (Reed et al., 2017) 3.95 3.70\nPixelCNN/RNN (van den Oord et al., 2016a) 3.86 3.63\nGated PixelCNN (van den Oord et al., 2016b) 3.83 3.57\nPixelSNAIL (Chen et al., 2018) 3.80 3.52\nSPN (Menick & Kalchbrenner, 2018) 3.79 3.52\nImage Transformer (Parmar et al., 2018) 3.77 –\nStrided Sparse Transformer (Child et al., 2019) – 3.44\nAxial Transformer + LSTM inner decoder 3.77 3.46\nAxial Transformer 3.76 (3.758) 3.44 (3.439)\nTable 3: Video modeling results (bits/dim) on the BAIR Robotic Pushingdataset (Ebert et al.,\n2017). We condition on a single video frame and model the next 15 frames, similar to Weissenborn\net al. (2019). Kumar et al. (2019) instead condition on the 3 prior frames of the video.\nModel bits/dim next 15 frames\nVideoFlow (Kumar et al., 2019) 1.87\nVideo Transformer (Weissenborn et al., 2019) 1.35\nAxial Transformer (ours) 1.29\n3.2.2 C HANNEL ENCODER FOR MULTI -CHANNEL IMAGES AND VIDEOS\nWe have just described an architecture for a single-channel image of shapeH ×W. Here, we show\nhow to extend the architecture to multi-channel images or videos of shape H ×W ×C (here C is\neither the number of channels in a multi-channel image, or the product of the number of channels\nand timesteps in a video). One way to model such data of shape H ×W ×C is to simply stack the\nchannels on top of each other into a single-channel image of shape(H ·C)×W or H×(W ·C). This\nis simple to implement, but does increase the sequence length for column attention or row attention,\nwhich can be undesirable for large C. We instead opt to model one channel at a time as a single-\nchannel image, but now conditioned on previous channels using an extra set of unmasked row and\nunmasked column attention layers. This means that we have a model of the form p(x:,:,c|x:,:,<c),\nwhere previous channels x:,:,<c are processed into a H ×W ×D tensor of context information,\nwhich is then added into the ﬁrst encoding blocks of the model in section 3.2.1 (Figure 3).\nWe do not share any parameters among any of these layers. At training time, we train on a random\nchannel slice of each image: we process the previous slices using these unmasked attention layers to\nproduce a context tensor, and maximize the likelihood of the randomly chosen slice conditioned on\nthis context. This amounts to training on an unbiased estimate of log likelihood for the whole data\ntensor. See ﬁg. 1 for an illustration of this complete model.\n4 E XPERIMENTS\nWe benchmarked our models on standard datasets for generative image and video models: down-\nsampled ImageNet (van den Oord et al., 2016a) and BAIR Robot Pushing (Ebert et al., 2017). All\nAxial Transformers have 8 total layers in the encoder, 8 layers in the outer decoder and 4 layers\nin the inner decoder. We use a hidden size of 2048 neurons throughout and for all setups and 16\nheads with 128 neurons each for the attention component. We train for approximately 200k steps on\nImageNet32 and ImageNet64 and for 200k steps on BAIR Robot Pushing. Our models can overﬁt\non ImageNet32, but on the other datasets the models keep on gradually improving with more steps.\nSee table 2 and table 3 for our results.\n7\n4.1 A BLATION STUDY\nTo push the limits of the semi-parallel sampling by making the inner decoder as small as possible,\nwe train an Axial Transformer with the inner decoder replaced by a single LSTM layer of 2048 units.\nThis slows down training time by about 20% on ImageNet32 and about 80% on ImageNet64 when\nmaintaining the number of steps and all else ﬁxed. We ﬁnd that the Axial Transformer + LSTM inner\ndecoder performs rather well on the ImageNet32 and ImageNet64 benchmarks (table 2), thereby also\nshowing the effectiveness of the remaining parts of the Axial Transformer that capture the context\nof the rows above. We also ﬁnd however that the full four layers of the inner decoder of the Axial\nTransformer provide an additional boost in performance as well as signiﬁcantly faster training. The\nAxial Transformer + LSTM inner decoder has the advantage of requiring only a couple of matrix-\nvector products to compute the layers at each autogressive step, comparing favourably with about\nthe 12 matrix-vector products required by the Axial Transformer, but the slower training time would\nmake the LSTM inner decoder quickly impractical for larger tensors.\n4.2 S AMPLES\nIn ﬁg. 4 and ﬁg. 5, we show samples from our 64 ×64 and 32 ×32 ImageNet models. The samples\nare globally coherent and show visibly recognizable scenes, meaning that our Axial Transformer\narchitecture successfully captures long-range dependencies across thousands of data dimensions in\nthese image datasets. The samples also don’t show any architecture-correlated artefacts. In addition,\nin ﬁg. 6 we show samples from the BAIR Robotic Pushing dataset. The ﬁrst frame is each row is\ngiven by the dataset and the rest are continuation. We note the high quality exactness of details and\nthe very large diversity (at temperature 1.0).\n5 C ONCLUSION\nWe proposed the Axial Transformer, an self-attention-based autoregressive model for data organized\nas high dimensional tensors. It is based on axial attention, a simple generalization of self-attention\nthat scales better with the dimension of input data, achieving a O(N(d−1)/d) savings in compu-\ntation and memory for a d-dimensional input tensor with N elements. Axial attention is easy to\nimplement and does not require custom kernels to run efﬁciently on modern accelerators. Axial\nTransformers use axial self-attention layers and a shift operation to naturally and efﬁciently build\nfull receptive ﬁelds of multidimensional tensors. Our model matches or outperforms the state-of-the-\nart on ImageNet-32 and ImageNet-64 image benchmarks and sets a signiﬁcant new state-of-the-art\non the BAIR Robot Pushing video benchmark.\n8\nFigure 4: 64 ×64 ImageNet samples at temperature 1.0\nFigure 5: 32 ×32 ImageNet samples at temperature 0.99\n9\nFigure 6: 15 ×64 ×64 BAIR Robot Pushing samples at temperature 1.0\n10\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nXi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved au-\ntoregressive generative model. In International Conference on Machine Learning, pp. 863–871,\n2018.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nFrederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with\ntemporal skip connections. In Conference on Robot Learning, pp. 344–356, 2017.\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Cc-\nnet: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE International\nConference on Computer Vision, pp. 603–612, 2019.\nNal Kalchbrenner, A¨aron Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and\nKoray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning,\npp. 1771–1779, 2017.\nManoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent\nDinh, and Durk Kingma. Videoﬂow: A ﬂow-based generative model for video. arXiv preprint\narXiv:1903.01434, 2019.\nJacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel networks\nand multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018.\nAnkur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4052–\n4061, 2018.\nScott Reed, A¨aron van den Oord, Nal Kalchbrenner, Sergio G´omez Colmenarejo, Ziyu Wang, Yutian\nChen, Dan Belov, and Nando de Freitas. Parallel multiscale autoregressive density estimation. In\nProceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2912–\n2921. JMLR. org, 2017.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the\nPixelCNN with discretized logistic mixture likelihood and other modiﬁcations. In International\nConference on Learning Representations (ICLR), 2017.\nLucas Theis and Matthias Bethge. Generative image modeling using spatial lstms. In Advances in\nNeural Information Processing Systems, pp. 1927–1935, 2015.\nBenigno Uria, Marc-Alexandre C ˆot´e, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural\nautoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):7184–\n7220, 2016.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nInternational Conference on Machine Learning (ICML), 2016a.\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-\nray Kavukcuoglu. Conditional image generation with PixelCNN decoders. arXiv preprint\narXiv:1606.05328, 2016b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nDirk Weissenborn, Oscar T ¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models.\narXiv preprint arXiv:1906.02634, 2019.\n11",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.790558934211731
    },
    {
      "name": "Computer science",
      "score": 0.7232909202575684
    },
    {
      "name": "Autoregressive model",
      "score": 0.6266013383865356
    },
    {
      "name": "Decoding methods",
      "score": 0.5574691295623779
    },
    {
      "name": "Computation",
      "score": 0.4787434935569763
    },
    {
      "name": "Computer engineering",
      "score": 0.47040969133377075
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42546606063842773
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4247332811355591
    },
    {
      "name": "Algorithm",
      "score": 0.3374941349029541
    },
    {
      "name": "Mathematics",
      "score": 0.10435619950294495
    },
    {
      "name": "Engineering",
      "score": 0.1034092903137207
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}