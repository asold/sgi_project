{
  "title": "Transforming agency: On the mode of existence of large language models",
  "url": "https://openalex.org/W4413410704",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A569762738",
      "name": "Xabier E. Barandiaran",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A2563149988",
      "name": "Lola S. Almendros",
      "affiliations": [
        "Universidad de Salamanca"
      ]
    },
    {
      "id": "https://openalex.org/A569762738",
      "name": "Xabier E. Barandiaran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2563149988",
      "name": "Lola S. Almendros",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3215008157",
    "https://openalex.org/W2018259456",
    "https://openalex.org/W4206177934",
    "https://openalex.org/W4310823711",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W7002045957",
    "https://openalex.org/W7002137572",
    "https://openalex.org/W2005345311",
    "https://openalex.org/W2061803363",
    "https://openalex.org/W4408973818",
    "https://openalex.org/W2097585998",
    "https://openalex.org/W4388824487",
    "https://openalex.org/W3135406297",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4248712191",
    "https://openalex.org/W4223489780",
    "https://openalex.org/W4385238793",
    "https://openalex.org/W4381586832",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W1966089223",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2506660467",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4365445794",
    "https://openalex.org/W3038427353",
    "https://openalex.org/W6621277746",
    "https://openalex.org/W2068301581",
    "https://openalex.org/W4243058763",
    "https://openalex.org/W4386942526",
    "https://openalex.org/W4229972507",
    "https://openalex.org/W4388824938",
    "https://openalex.org/W4391709169",
    "https://openalex.org/W6910584100",
    "https://openalex.org/W4312236423",
    "https://openalex.org/W2153791616",
    "https://openalex.org/W4404505883",
    "https://openalex.org/W393180982",
    "https://openalex.org/W4396662087",
    "https://openalex.org/W4200293778",
    "https://openalex.org/W4387764390",
    "https://openalex.org/W4205725149",
    "https://openalex.org/W6629677418",
    "https://openalex.org/W4406779522",
    "https://openalex.org/W6655065987",
    "https://openalex.org/W3001991410",
    "https://openalex.org/W4244140934",
    "https://openalex.org/W4210801736",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W7071872813",
    "https://openalex.org/W4378942694",
    "https://openalex.org/W3045277344",
    "https://openalex.org/W2141010222",
    "https://openalex.org/W4360957277",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W3022710346",
    "https://openalex.org/W4250088333",
    "https://openalex.org/W2157376352",
    "https://openalex.org/W1989974095",
    "https://openalex.org/W4248169967",
    "https://openalex.org/W4383742778",
    "https://openalex.org/W4389072332",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W6812632051",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2329010901",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4236485264",
    "https://openalex.org/W6631222242",
    "https://openalex.org/W4295992949",
    "https://openalex.org/W4399465031",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W6891904667",
    "https://openalex.org/W4391620956",
    "https://openalex.org/W4239649652",
    "https://openalex.org/W2996586778",
    "https://openalex.org/W4390723197",
    "https://openalex.org/W2013336398",
    "https://openalex.org/W4396945190",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2783657280",
    "https://openalex.org/W4239432553",
    "https://openalex.org/W4383875393",
    "https://openalex.org/W4200009975",
    "https://openalex.org/W6891868156",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W4283733513",
    "https://openalex.org/W4387796547",
    "https://openalex.org/W4391709247",
    "https://openalex.org/W4379089709",
    "https://openalex.org/W4385682136",
    "https://openalex.org/W3007421089",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W404459715",
    "https://openalex.org/W2493894436",
    "https://openalex.org/W6661882538",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4383994288",
    "https://openalex.org/W6910594186",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4249319693",
    "https://openalex.org/W4407120622",
    "https://openalex.org/W6665963072",
    "https://openalex.org/W4244993796",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W7028397281",
    "https://openalex.org/W2123969205",
    "https://openalex.org/W4239609446",
    "https://openalex.org/W6658795122",
    "https://openalex.org/W2488460736",
    "https://openalex.org/W4405655184",
    "https://openalex.org/W3136370452",
    "https://openalex.org/W2908153137",
    "https://openalex.org/W6838557027",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W4253020087",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W2989847975",
    "https://openalex.org/W4384023425",
    "https://openalex.org/W2251410821",
    "https://openalex.org/W4298300174",
    "https://openalex.org/W2080654992",
    "https://openalex.org/W4220991136",
    "https://openalex.org/W4391631327",
    "https://openalex.org/W4244539133",
    "https://openalex.org/W4309804074",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4415030178",
    "https://openalex.org/W6910588282",
    "https://openalex.org/W2067619114",
    "https://openalex.org/W6662269980",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4387947619",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W6634206266",
    "https://openalex.org/W1729777349",
    "https://openalex.org/W4388444880",
    "https://openalex.org/W4294842693",
    "https://openalex.org/W2035844185",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W4403959981",
    "https://openalex.org/W4378499172",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4231563569",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1973462894",
    "https://openalex.org/W4389300776",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4405017722",
    "https://openalex.org/W4393726172",
    "https://openalex.org/W4377130745",
    "https://openalex.org/W4394782272",
    "https://openalex.org/W4411119640",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4392384010",
    "https://openalex.org/W4288350551",
    "https://openalex.org/W4389422760",
    "https://openalex.org/W4386794440",
    "https://openalex.org/W4402827393",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4328048700",
    "https://openalex.org/W1990035816",
    "https://openalex.org/W3118210634"
  ],
  "abstract": null,
  "full_text": "Phenomenology and the Cognitive Sciences\nhttps://doi.org/10.1007/s11097-025-10094-3\nAbstract\nThis paper investigates the ontological characterization of Large Language Mod -\nels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we pay \nspecial attention to their status as agents. This requires explaining in detail the \narchitecture, processing, and training procedures that enable LLMs to display their \ncapacities, and the extensions used to turn LLMs into agent-like systems. After a \nsystematic analysis we conclude that a LLM fails to meet necessary and sufficient \nconditions for autonomous agency in the light of embodied theories of mind: the \nindividuality condition (it is not the product of its own activity, it is not even di -\nrectly affected by it), the normativity condition (it does not generate its own norms \nor goals), and, partially the interactional asymmetry condition (it is not the origin \nand sustained source of its interaction with the environment). If not agents, then… \nWhat are LLMs? We argue that ChatGPT should be characterized as an interlocu -\ntor or linguistic automaton, a library-that-talks, devoid of (autonomous) agency, \nbut capable to engage performatively on non-purposeful yet purpose-structured and \npurpose-bounded tasks. When interacting with humans, a “ghostly” component of \nthe human-machine interaction makes it possible to enact genuine conversational \nexperiences with LLMs. Despite their lack of sensorimotor and biological embodi -\nment, LLMs textual embodiment (the training corpus), digital extended interface \nembodiments, and resource-hungry computational embodiment, significantly trans -\nform existing forms of machine automatism and human agency. Beyond assisted \nand extended agency, the LLM-human coupling can produce midtended forms of \nagency, closer to the production of intentional agency than to the extended instru -\nmentality of any previous technologies.\nKeywords Transformers · Large language models · Agency · Autonomy · \nInterlocutor automata · Human machine interaction\nAccepted: 22 July 2025\n© The Author(s) 2025\nTransforming agency: On the mode of existence of large \nlanguage models\nXabier E. Barandiaran1  · Lola S. Almendros2\nExtended author information available on the last page of the article\n1 3\n\nX. E. Barandiaran, L. S. Almendros\n1 Introduction\nThe recent emergence of Large Language Models (LLMs hereafter) (see Brown et \nal., 2020) with their wide availability 1 and their human-like generative capabili -\nties are (re)opening the question around the ontological status of Artificial Intelli -\ngence. Are these systems genuinely intelligent? Do they possess mindful capacities? \nThe responses are often polarized (Mitchell & Krakauer, 2023). Inflationary views \n(fuelled by research enthusiasm and commercial interest alike) tend to amplify AI \nproperties, assimilating or approximating them to the human (and the superhuman). \nDeflationary views (typically trying to mitigate the harms of inflationary market -\ning), tend to downplay capacity attributions, and bring AI systems closer to dumb \nmathematical or mechanical devices. Deflationary categorizations typically revolve \naround treating LLMs as statistical processors, “stochastic parrots” (Bender et al., \n2021), a “blurry JPEG of the web” (Chiang, 2023), “lumbering statistical engine for \npattern matching” (Chomsky et al., 2023), “illusory thinking” (Shojaee et al., 2025) \nor simply “bullshit” (Hicks et al., 2024)2. The most inflationary characterizations \nrange from considering LLMs as “human-brain equivalents” (Ge et al., 2023), “genu-\nine authors” and “accountable” entities (Miller, 2023), up to a “fully sentient person” \n(Lemoine, 2022). Somewhere in the middle stand more technical characterizations \nlike “artificial reasoners” (Wei et al., 2023), “learners” (Brown et al., 2020), “general \npattern machines” (Mirchandani et al., 2023), “sparky artificial general intelligence” \n(Bubeck et al., 2023) or simply “language models” with the slippery temptation to be \nturned into “world models” (K. Li et al., 2023).\nAn increasing danger of some deflationary views of LLMs is that, from their \npoint of view, most of the risks can be attributed to the influence of misguided \ninflationary conceptions. It is often assumed that these can be mitigated if infla -\ntionary views are conclusively shown to be wrong: “Behind the smog of the hype \nand the marketing” the argument goes “there is no genuine intelligence or under -\nstanding behind LLMs, they are simple statistical processors, the only problem \n(besides their energy consumption and biases) is that other humans take them at \nface value”. Moreover, the argument continues, “if we treat them as the stupid \nmachines they are”, the conclusion follows, “even the issues of bias and energy \nshould fade away”. Or, as Chomsky et al. conclude, “Given the amorality, faux \nscience and linguistic incompetence of these systems, we can only laugh or cry \nat their popularity” ( 2023). The real capacity of LLMs is thus left disregarded \nby such deflationary views, both as a potential risk to society and as a genuine \nsource of positive sociotechnical transformation that needs to be more deeply \n1  Online services like Gemini, Claude, or, more prominently, ChatGPT, and the free/open source alterna-\ntives that can also run locally, like DeepSeek, Qwen, LLaMa, Mixtral, etc. are making these technologies \nmassively available, not only as direct conversational bots but also, and importantly, as integrated assis-\ntants or agency boosting systems into different applications.\n2  Although all the cited deflationary accounts of what LLMs do provide critical arguments to defend their \nposition, the term “bullshit,” as used by Hicks et al., is more technical and better defended than it might \nappear at first sight. Hicks et al. adopt Frankfurt’s (2005) explanation of “bullshit talk” as discourse that \ndisregards truth without directly aiming to mislead the receiver or intentionally conceal it, as lying does. \nThey show how this analysis applies to LLM outputs as well.\n1 3\nTransforming agency: On the mode of existence of large language…\nthought out. If we do not fine-tune our conceptualization of what LLMs are, \nwe will not be able to properly analyze, evaluate, stir, and communicate their \nimpact.\nThere are many ways of assessing this “real” power and its impact. Some are historic \nand socio-economic (Pasquinelli, 2023), or socio-ecological (Crawford, 2021), or even \nexistential (Bostrom, 2017; Christian, 2021; Russell, 2019). But little attention has been \nput on critically analyzing LLMs from the point of view of “agency”. Despite the wide-\nspread academic consensus on the lack of conscious or sentient capabilities of LLMs, \ntheir status as agents is often uncritically assumed or proclaimed both in philosophy \n(Floridi, 2023) and the industry (OpenAI, 2025b). And there are two good reasons to \nstrengthen accuracy on agentive attributions to LLMs: (a) mindful capacities did not \narise in nature as a result of chess playing but of the evolution of agency (Barandiaran, \n2008; Sterelny, 2001; Tomasello, 2022) and thus granting LLMs the status of agents \nmeans assuming them on a path towards such properties, and (b) achieving autono -\nmous agency is one of the next big things in AI (Wang et al., 2023) with widespread \nsupport from Google, Apple, and Microsoft (including OpenAI) recently marketing \ntheir AI products as “agents” (Holmes, 2024; O’Donnell, 2024).; a point that became \nparticularly relevant since the recent launch of ChatGPT Agent (OpenAI, 2025b).\nIn the next section, we contextualize the problem of the ontological status of \nLLM in terms of its current capabilities and limitations as expressed on different \nbenchmarks. We show how transformer technologies are breaking down the solid \ndistinctions between the human and the engineered. The mode of existence of these \ntechnologies is, however, not limited to their performance. In order to clarify it, \nwe follow a threefold approach. First, in Sect. 3, we ground our analysis in the \ntechnical reality of these systems by focusing on the internal workings of Chat -\nGPT-like LLMs: their architecture, processing flow, training procedures, and the \nextensions used to create agent-like behaviors. A clear technical understanding is \nthe necessary foundation for any meaningful ontological assessment. Second, with \nthese technical details at hand, in Sect. 4, we systematically evaluate their status \nas agents from the perspective of contemporary embodied and enactive theories \nof mind. We conclude that they fail to meet the core conditions for autonomous \nagency. Third, in Sect. 5, we argue that this negative conclusion is insufficient; it \ncompels us to address the crucial question: If not agents, then what are they? Based \non the technical specifications of Sect. 3, and building on embodied and enactive \napproaches to cognition, we characterize them as interlocutor automata , capable \nof bringing digital textual bodies to conversational life in interaction with humans, \nwith the capacity to deeply transform human agency. We finally conclude and dis -\ncuss the implications of our approach.\n2 When computers can\nBenchmarks, particularly when out of reach for the available technology, have often \nhelped to reach agreement on the (lack of) capacities of AI systems. Now, many \ndefend that the Turing Test is outdated (Bayne & Williams, 2023; Biever, 2023; Sriv-\nastava et al., 2023; Tikhonov & Yamshchikov, 2023). Generic conversations with \n1 3\nX. E. Barandiaran, L. S. Almendros\nLLMs are indistinguishable from those we can enjoy with other humans (Jones & \nBergen, 2024). More systematic variations of the Turing Test, directed at capturing \nthe capacity of AI to display common sense, like the Winograd schema challenge \n(Levesque et al., 2012), have been declared obsolete (Kocijan et al., 2023). More \nsophisticated common sense reasoning tests like Winogrande (Sakaguchi et al., 2019) \nand HellaSwag (Zellers et al., 2019), specifically designed to be particularly hard for \nLLMs, have also been passed (Gemini Team et al., 2023; Jiang et al., 2023; OpenAI, \n2023). Moreover, by 2023, LLMs, like GPT-4 already exhibited, according to their \ncreators, “human-level performance” in a wide variety of professional and academic \nexams (OpenAI, 2023, p. 6), or, like Google’s Gemini, improving GPT-4 in many \nbenchmarks (Gemini Team et al., 2023), could outperform humans on multitask lan-\nguage understanding tests (Hendrycks et al., 2021).\nCurrent proprietary frontier systems—GPT-4o (OpenAI, 2024a), the GPT-o3 \nand 4o-mini (OpenAI, 2025a), Claude 4 family (Anthropic, 2025), and Gemini \n2.5 Pro (Google DeepMind, 2025)—now reach or surpass human-level accuracy \non many reasoning and commonsense suites. Open-source counterparts have rap -\nidly closed the gap: KIMI K2 (Moonshot, 2025/2025), Llama 4 (Meta AI, 2025), \nMixtral of Experts and its successor Magistral (Jiang et al., 2024a, b; Mistral-AI \net al., 2025), DeepSeek-R1 (DeepSeek-AI et al., 2025), or Qwen 2.5 (Qwen et al., \n2025) all report benchmark scores within a few percentage points of their closed-\nweight peers, while offering transparent and reproducible alternatives for academic \nresearch.\nIn concrete knowledge domains (like medicine) frontier models since 2023 often \noutperform average specialists at specific tasks (Guo et al., 2023; Singhal et al., 2025; \nVan Veen et al., 2024) and, perhaps not so surprisingly, can imitate philosophers with \nhardly distinguishable snippets (Schwitzgebel et al., 2024). More recently, meth -\nodological innovations on LLM’s self-guided “reasoning” (Lightman et al., 2023; \nMadaan et al., 2023a; Wei et al., 2023; Yao et al., 2023), and the resulting increase of \ninference-time compute, have pushed state-of-the-art scores by even higher margins \n(DeepSeek-AI et al., 2025; OpenAI, 2024b). A significant boost in benchmarking has \nalso resulted from agentic extensions and tool use of LLMs (OpenAI, 2025b).\nThis rapid progress has accelerated the obsolescence of existing benchmarks, forc-\ning the latest 1 M dollar prized Artificial General Intelligence benchmark (ARC-\nAGI) to be updated with a more challenging successor ARC-AGI2 in March 2025 \n(Chollet et al., 2025), and an even more challenging and agent focused ARC-AGI3 \nin July 20253.\nHowever, aggregate leaderboard numbers can hide “pockets of unpredictable fail-\nure” and thus overstate real-world reliability (Burnell et al., 2023). There are argu -\nable limitations of current models, particularly in relation to some abstract reasoning \ncapacities like compositionality (Dziri et al., 2023), multistep reasoning (Sprague \net al., 2023), or complex planning (Valmeekam et al., 2023); whose mastery, by the \nway, is often rare among humans4. Large-scale studies show that scaling and instruc-\n3  https://three.arcprize.org.\n4  Most recently, Apple’s The Illusion of Thinking  study (Shojaee et al., 2025) attracted extraordinary \nmedia attention in an increasingly polarized context. Subsequent analyses, however, have contested the \n1 3\nTransforming agency: On the mode of existence of large language…\ntion fine-tuning might not fully eliminate these weaknesses and could even make \nerrors harder to predict (Zhou et al., 2024). Moreover, it is still possible, although \nincreasingly harder, to find tasks in which humans score high, without specific exper-\ntise, yet remain hard for LLMs, as shown by the ARC-AGI-2 and 3 benchmarks \n(Chollet et al., 2025). And yet, what is certain is that we are facing the development \nof complex technologies that perform operations whose results are very similar to \nthose requiring high levels of intelligence in humans. This circumstance translates \ninto a growing undifferentiation between the human and the engineered. Conceptual \ndivides that were once sharp and fixed are starting to melt and move. These advances \nforce us to re-organize conceptual and ontological commitments regarding minds, \nmachines, and agency.\nEver since the very conception of modern computers as universal Turing machines, \nthe possibility of instantiating human, or super-human, level intelligence was at stake \n(Turing, 1950). At some point, this race between mind and machines settled down. \nMachines could (out)perform humans provided that the domain of interaction was \nrule-based, constrained, and limited, so that humans could program machines speci -\nfying the computational procedures required to carry out the task. Machines, how -\never, were left with genuine mindfulness out of reach. Real-world tasks such as open \nconversations, a trip to the grocery, creative writing or subtle comforting humor were \nonly within reach for us. The (human) mind could not be reduced to rule-following \nor explicit reasoning capacities but emerged, instead, out of sub-rational skillful \nembodied interactions that could not be implemented into machines (Dreyfus, 1992). \nElephants, after all, don’t play chess, but their mental life is rich and complex, noth-\ning like computers would ever be able to accomplish (Brooks, 1990). On the other \nhand, computers could play chess but not pass the Turing Test, they could imitate \nand outperform humans in specific rule-based scenarios but not on the open field of \nlanguage games and skilled conversations fuelled by a common-sense: an embodied \nsub-symbolic mesh that was claimed to resist rational, explicit, operationalization \n(Johnson & Lakoff, 2002; Varela et al., 1991).\nTransformers have come to break this cease-fire between minds and machines 5. \nLLMs can carry out context-sensitive translations, they can explain humor and \njokes including interpreting what humans take to be funny images (Hessel et al., \n2023, albeit with notable limitations), can learn from few or a single example or \ninstruction (Brown et al., 2020), or engage on reasoning chains “creatively” (Wei \net al., 2023). Moreover, LLM technology and transformer architectures are being \napplied to multiple sensorimotor modalities both in real-world physical robots \nthrough VLA (Vision Language Action) models (Brohan et al., 2023; Collabora -\ntion et al., 2023), Q-learning enhanced LLMs (Chebotar et al., 2023), or directly \nprocedure and conclusions, attributing much of the reported failure to evaluation artefacts and token-\nbudget limits (Lawsen, 2025).\n5 Although some pre-transformer successes were already anticipatory of the progress that AI develop -\nment was about to suffer. First it was GO, an open-ended, combinatorially explosive game that cannot \nbe played but by intuition in a manner that Go fans consider that is a pure expression of the player’s \nsoul. And second, perhaps more importantly, in playing different computer games, using human controls \n(e.g. first-person visuomotor feedback), and without knowing or encoding the game rules in advance \n(Schrittwieser et al., 2020).\n1 3\nX. E. Barandiaran, L. S. Almendros\napplied to visual and sensorimotor tasks (Bousmalis et al., 2023; Reed et al., 2022). \nThese expansions of LLMs might succeed out of the text-image bound domain into \nphysically enacting sensorimotor correlations and learning skillful coping with the \nphysical world (Xiang et al., 2023). An approach that brings new generation AI sys-\ntems much closer to traditionally “AI-skeptic” embodied and situated approaches \nto mind and cognition (Chemero, 2009; Dreyfus, 1992; Johnson & Lakoff, 2002; \nVarela et al., 1991). It is, however, too early to judge the success of embodied \nrobotic implementations of new generative technologies. On the contrary, there is a \nsuite of text-interfaced LLMs (ChatGPT, LLaMa, Gemini, Claude, Mistral, Deep -\nSeek, Qwen, etc.), providing first-hand experience for millions of people, defining \nthe way we are relating to LLM technologies and transforming the digital (and \nnon-digital) human environment.\nThe best way to avoid alienation is not to feed inflationary positions or to join \nthe deflationary ranks, but to find the right ontological categorization for these \nsystems; or, to say it with Simondon ( 2017), to identify the mode of existence  of \ntechnical systems. Like the case of consciousness or sentience, we cannot leave the \nanswer to “social relationism”; i.e. to a mere social contingent convention on what \ntype of systems deserves which treatment (for a detailed argument against social \nrelationism and AI see Torrance, 2014). A proper understanding of what LLMs \nare, requires delving deep into their concrete structure, operations and coupling \nwith their milieu (humans and other machines). Can, and should, we take them for \nagents? Are they intelligent? If not, what are they? What is the best way to concep -\ntualize them? The answer has important implications in the field of ethics and legal \nstudies (Bertolini & Episcopo, 2022; Clowes et al., 2024; Coeckelbergh, 2021; \nFourneret & Yvert, 2020; Mabaso, 2021), but also on the social adoption of these \ntechnologies, our collective awareness of their limits and potentialities. We need \nconceptual resources to organize our experience and interactions with ChatGPTs \nand their place in our sociotechnical world.\nUnderstanding in some detail how GPT and other LLMs are trained and how they \nfunction is of fundamental importance to characterize their “nature”, genuine capa -\nbilities and possible implications. In the next section, we provide an explanation of \nhow GPT works, with the goal of contrasting its actual functioning with some onto -\nlogical attributions, particularly its agentive capacities. A detailed understanding of \nGPTs functioning will also help characterize its mode of existence and the ways in \nwhich it can potentially transform human agency6.\n6  The next section provides a detailed technical overview of how LLMs function, intended to ground the \nsubsequent philosophical analysis in Sects. 4 and 5. Readers already deeply familiar with the architecture \nand training procedures of transformer models may choose to skim this section or proceed directly to the \nphilosophical arguments beginning in Sect. 4. Note, however, that specific details will be drawn upon in \nthe later sections.\n1 3\nTransforming agency: On the mode of existence of large language…\n3 How do large language models work?\nLLMs are so-called “artificial intelligence” systems (Norvig & Russell, 2021), part of \ncurrent NLP (Natural Language Processing) technologies, which belong to the fam -\nily of “machine learning” and the sub-category of “deep learning” systems. They are \ndesigned to process and generate “natural” language through a large number (on the \norder of billions) of processing steps. Transformers (Vaswani et al., 2017), in turn, are \none kind of recently very successful type of LLMs, and GPT (Generative Pre-trained \nTransformer) is a specific type of implementation of Transformer technology (Brown \net al., 2020; Radford et al., 2019). Most popular LLMs are delivered as a service by \na provider. So, for instance, ChatGPT, is a specifically tuned and interfaced version \nof GPT (and, increasingly, a platform to connect GPT to other tools and to deliver \npersonalized services with GPT technology)7.\nChatGPT uses different versions of GPT models to produce human-like text (GPT-\n4.5, GPT-o3, etc.). It is a computational language processing system designed to gen-\nerate sequences of words, codes or other data (more recently, images) from an input \nsequence called “prompt”. Thus, given a prompt, GPT produces the text that would \nhave been more statistically expected on the training data. For example, if the sequence \n“Elephants don’t play” is entered as a prompt, the ChatGPT offers “Elephants don’t \nplay chess” as a response. The system has a heat parameter that increases less likely \nvariations on the output. So, for instance, the system might respond to the original \nprompt with “Elephants don’t play video games” or could simply output “Elephants \ndon’t play.”. This basic functioning is what made so popular the characterization of \nChatGPT as simply a complicated auto-complete tool (Floridi, 2023).\nHowever, the simplicity of the general task of optimizing to predict the following \nword, and its recursive iteration, is the key for the emergence of complex capaci -\nties in LLMs. Moreover, optimization alone provides no ground to understand the \nworking of a system, its capabilities and limits, its mode of existence. Appealing \nto partial aspects of how they operate, Transformers are often qualified as stochas -\ntic, probabilistic and statistical (Bender et al., 2021; Chomsky et al., 2023; Floridi, \n2023). Stochasticity refers to the randomness of how the final output is “selected”. \nProbabilistic is used to indicate that this final decision is taken randomly but on the \nbasis of an assigned probability that is, in turn, allegedly extracted from the statistical \nproperties of the training data. Understood on its most generous terms, such descrip-\ntions are relatively correct but partial and incomplete. It is possible to imagine a \nstrictly statistical AI that simply computes or extracts conditional probabilities of all \npossible output tokens given an input stream. But this simply does not work. In fact, \nthere could not be sufficient training data on the universe to make such a machine \neffective on the basis of pure probabilities or statistics (Wolfram, 2023). And it is not \nwhat GPT does. As their name indicates, LLMs create models of language. That is, \nthey do not simply store statistical relations or conditional probabilities but instead \n7  On what follows we shall use the terms “Transformer”, “LLM” and “GPT” and “ChatGPT” almost \ninterchangeably, unless specific reference is provided to the concrete model (e.g. GPT-2, DeepSeek-r1) \nor to aspects of their interface.\n1 3\nX. E. Barandiaran, L. S. Almendros\nconstitute compressed and structured engines that process and transform text input in \nnon-linear, highly interrelated and complex forms.\nBefore we start with the description of GPTs architecture and processing, it is \nimportant to stress that, in general, the processing blocks and procedures are, techni-\ncally speaking, complex (Bechtel & Richardson, 2010). They do not make “sense” \nfrom the point of view of the functional decomposition of the treatment of the input. \nCertainly not one that a human might understand or guess as a reasonable strategy. \nAdmittedly, there is no theory for how and why this architecture works (Wolfram, \n2023). And yet it works, and we have access to the processing architecture of GPT−2 \n(Radford et al., 2019), and some details of GPT−3 (Brown et al., 2020), to better \ndelimit, without a possibly full understanding, how the system operates.\nIn what follows, we will provide a detailed explanation of GPT-3 to the best avail-\nable knowledge, as the prototypical LLM and as an example of generative AI more \ngenerally. We have chosen GPT-3 as our foundational case study for several key \nreasons. First, its release marked a crucial inflection point 8. It stood for a qualitative \nleap in generative capability and widespread public adoption. Second, it did set up \nthe industry standard for other models to follow, becoming the paradigmatic example \nof generative AI. Subsequent models have increased in size, explored more optimiza-\ntion with novel training techniques or, as we shall see, multiple parallel specialized \ntransformers and multimodality. But they largely build upon the core principles of the \nGenerative Pre-trained Transformer architecture that GPT-3 popularized. A detailed \nunderstanding of this particular model provides the essential conceptual scaffolding \nfor grasping the entire family of these systems, including their more recent and com-\nplex versions, and those developed by other organizations.\n3.1 Architecture and processing\nFigure 1 illustrates step-by-step the processing of the input text as it goes along the \nGPT architecture. We explain each step in detail below.\n1. Tokenization and encoding : The first operation that takes place as we enter \ntext into a LLM like ChatGPT is tokenization. The input stream is chopped into \ntokens (small syllable-like or small word text chunks, including punctuation marks). \nOn average, each token is about 3/4 of a word in English, with a mean of 4 characters \nper token9. Nevertheless, we are going to use the terms “word” and “token” inter -\nchangeably. Then each token is encoded numerically. So for example, “Elephants \ndon’t play” is tokenized into five tokens: [Ele, phants, don, ‘t, play]. And then each \nof them is assigned a predefined number out of the complete vocabulary of 50,257 \ntokens, and the sequence is converted into an array of numbers: [46439, 53667, 1541, \n956, 1514].\n8  This is so to the extent that the term “GPT-3 moment” has now been popularized, referring to the \nbreakthrough at which a technological advance converts prior hypotheses or intuitions into a clearly \nrecognisable technical success.\n9 It is possible to work with Tokenizer to understand better this procedure: https://platform.openai.com/\ntokenizer.\n1 3\nTransforming agency: On the mode of existence of large language…\nFig. 1 Schematic of the GPT-3 processing architecture as a standardized reference for LLM function -\ning. Computation flows from the top of the diagram to the bottom: the input string (shown in yellow) is \nprogressively transformed into intermediate matrices whose colour shifts from yellow to green to blue, \nculminating in the selection of a single blue output token. Parameter matrices (marked in purple) act on \nthese intermediate matrices (they remain fixed during processing but are configured and updated during \n1 3\nX. E. Barandiaran, L. S. Almendros\npre-training and training, see sect. 3.2). (1) The input string is tokenised and numerically encoded. (2) \nTokens are embedded as 12288-dimensional vectors or \"semantic space\" defined for a 50257 token (word) \nvocabulary. (3) A positional encoding is added to each vector to preserve word order. The resulting matrix \npasses iteratively through 96 sequential transformer blocks, each involving the processing through (4) \n96 parallel self-attention heads and (5) a high dimensional feed-forward neural network. (6) A projec -\ntion back into vocabulary space followed by softmax normalisation yields a probability distribution over \nthe 50257 possible tokens, from which the next token is sampled. (7) The sampled token is added to the \noriginal input (returning to step 1), and the procedure repeats auto-regressively until an end-of-sequence \ntoken is produced or the maximum length is reached. The text body provides a detailed explanation of \neach processing step.\n2. Embedding : Once tokens are represented numerically, these numbers are \nmapped into a high dimensional relational space. This process is called embedding. \nEmbedding already implies a huge transformation of the input with previous “knowl-\nedge” of how tokens (or words) relate to each other. Some of these relationships \ncan be considered “semantic” or “syntactic” by capturing higher-order relational \nproperties between words. Some dimensions or combinations of dimensions might \nbe thought of as abstract conceptual properties (e.g. being a grammatical subject or \nbeing an animal). What defines the conceptual content of each dimension is not an \narbitrary label into it, but purely relational “spatial” properties. For instance, animal \nnames will appear close to each other, also grammatical subjects, etc. The embed -\nding space of GPT−3 is of 12,288 dimensions (Brown et al., 2020) and a position \nwithin that space is pre-encoded for 50,257 tokens10 (Radford et al., 2019). One way \nto understand this (with the risk of anthropomorphizing) is to say that GPT3 has \nthe capacity to situate 50,257 words 11 in a 12,288 dimensional “conceptual” space \n(or along 12,288 “properties”). So, for instance, the tokens “cat” and “tiger” will be \nclose to each other in many dimensions but will be relatively spaced in “size” and \n“habitat”. This means nothing other than the token “cat” being closer to “laptop”, \n“dog” and “watermelon” on a given dimension (that we could interpret as “size”) and \ncloser to “Roomba”, “television”, “sofa” and “living-room” in another (that we might \ninterpret as “habitat”).\nEmbeddings already embody an important part of the “knowledge” of an LLM, \nand its production is part of the overall training procedure of GPT. The result of \napplying the embedding function to the input stream is a matrix of 2048 × 12,288 that \nis itself called embedding12. The number 2048 (for GPT3, the size in current models \nis much higher) indicates the maximum size of the input stream in tokens. It sets the \nupper limit of how much “context” (previous conversation, additional information \nor maximum input provided) can the system handle. Although the input might just \n10  We provide token and matrix size details here with two specific goals: (a) to illustrate the scale of these \nsystems—a point that will turn relevant in Sect. 3.3—and, (b) to track the matrices that are combined at \ndifferent steps of the process; the size of the matrix is thus provided as a label or identifier. The specific size \nis, of course, not relevant, but helps track the matrix transformations along the text and in relation to Fig. 1.\n11 We will use the terms token and words interchangeably for a more intuitive grasp of the functioning of \nthe system. It is difficult to be strictly rigorous here because the concept of word itself is ill-defined, with \na regular human knowing approximately 10k word families (Brysbaert et al., 2016).\n12 It is a regular practice to use the term embedding to name the matrix that will be processed along the \nwhole transformer. But this might lead to confusion. Although the matrix maintains the same size, and the \nend result will be transposed back into tokens (see latter), the successive operations carried out over this \nmatrix distort its original interpretation so much that we find it confusing to keep calling it “embedding”. \nWe should use the term “matrix” instead and identify it by its size.\n1 3\nTransforming agency: On the mode of existence of large language…\nbe of 5 tokens and the resulting matrix 5 × 12,288, from now on, for convenience we \nwill assume that the matrix being processed is this 2048 × 12,288 maximal sized one.\n3. Positional encoding : Not only the set of words composing the input matters, \nbut their order also does. It is not the same to say that “John Searle invented the \nChinese Room” than saying that “The Chinese Room invented John Searle”. So each \nword/token embedding will also be transformed to incorporate positional informa -\ntion. The positional encoding in the form of a unique sine and cosine function out -\nput is added to the word embedding. A wave signature is added to each embedding \narray that is unique to a specific position and can be exploited by later processing \nto identify the position of that token on the original input stream. This produces a \nnew matrix of 2048 × 12,288 with all the 2048 input tokens in one dimension and \ntheir embedding + position on the other 12,288. The combination of embedding and \npositional encoding will now be processed through a sequence of processing blocks, \nlike a factory line. Each block consists of a set of operations that include primarily: \nattention, addition and normalization, and feed-forward neural network processing. \nGPT−3 transforms the input matrix through 96 such blocks.\n4. Attention mechanism: This is the most innovative of all the steps on the LLM \nrevolution and characterizes transformers as a specific type of LLMs (Vaswani et \nal., 2017). Attention layers have allowed an increase in LLM size and efficiency due \nto their capacity to parallelize processing during learning and execution time. They \npermit to explore a wide range of correlation dependencies over the input data, in \na highly scalable manner; improving upon other architectures aimed at processing \nrelationships between text elements in an input (e.g. retaining a working memory of \nthe past words in a paragraph), like Recurrent Neural Networks or Long-Short Term \nMemory Networks.\nAttention mechanisms basically compute how important are some tokens in a \nstream (and how other tokens can be ignored) but also how important are the relation-\nships between tokens in the input stream. This might pick out short and long distance \nrelationships, chopping the input stream into different chunks. Some of these atten -\ntional relationships might capture grammatical connections, such as a verb whose \nsubject is far back it in a sentence. Others might capture instructional (e.g. the rela -\ntionships between different steps of a recipe) or narrative structures (like the unfold-\ning of plot and the connection between characters through time). This is transiently \nexpressed as a matrix in which all the tokens are valued in relation to all other tokens \nrelating “everywhere, all at once”, in what would horrify Bergson as a geometrization \nof duration. By computing in parallel 96 attention mappings of this kind, transform -\ners avoid the computational bottleneck of recurrent sequential processing. In short, \nattention mechanisms make it possible to be sensitive to different contextual scales. \nGPT−3 processes 96 attention heads 13, that means that it processes (and later com -\nbines) 96 different ways of relating tokens of the input sequence to each other. What \nexactly each of these heads “really pay attention to” is partially unknown.\n13 Not to be confused with the 96 blocks. Attention heads run in parallel inside each block. Block process-\ning takes place in a sequence, the matrix that results from the transformations of block-n are the input to \nblock-n + 1. Attention heads process 96 copies of the input matrix in parallel, and then all 96 are added and \nnormalized into a single matrix that is then further processed. See Fig. 1.\n1 3\nX. E. Barandiaran, L. S. Almendros\n5. FeedForward network : The next step involves passing the matrix through a \nFeedForward Neural Network (FFNN) and it involves an important expanse on the \ndimensionality of the processing and the non-linear interaction between all the com-\nponents of the matrix; relating “everything, all at once” 14. The feedforward network \nconsists of 3 layers. The input layer is the matrix itself, the hidden layer expands its \ndimensionality and the output layer reduces it back to the original size. All the nodes \nof the first layer are connected to all the second, and all the second to the third (but \nnot between themselves nor backward, thus the name FeedForward). The connec -\ntions are weighted, so that different relationships between value projections can have \ndifferent weights and amplify or reduce the value of each signal into the next layer. \nThis is then processed by the nodes of the next layer through a nonlinear function. \nIn principle, FFNNs can compute any function (Siegelmann & Sontag, 1995). In this \ncase, they could be thought of as a computer inside a computer that could simulate \ndifferent types of programs 15, with the benefit of being programmable in an unsu -\npervised manner (see training section below). The weights and the parameters of the \nnonlinear function have traditionally been understood as the “place” where “knowl -\nedge” is encoded (Churchland, 1990; Rumelhart et al., 1987). So, for instance, the \nfact that GPT responds with “chess” to the sentence “Elephants don’t play”, instead \nof simply “.” or “basketball” is not something to be found on the original embedding, \nwhere certainly “chess” is an option close to “play” but certainly not the closest. \nTraces of Brook’s famous paper “Elephants don’t play chess”, its poetic “value”, and \nother contextual elements (e.g. talking about AI and the notable role that chess played \nin its history) can explain the final output.\nAttention takes over 30% of parameters of the model and FF about 70% in the \nlargest GPT−3 175 Billion parameter model (Huben, 2023). After embeddings, posi-\ntions and attentional processing has taken place, FFNN processing “elaborates” rela-\ntions between tokens applying to them the knowledge that was acquired through \nthe training process. But arrays of the resulting matrix can hardly be understood as \ndirectly relating to the original tokens anymore. Less so in subsequent transforma -\ntions, since the matrix will now be the input to another block that starts again with its \n96 attention heads (different to those of the previous block) and its FFNN processing \nthat has also specific parameters (weights and biases) in each block. At the end of the \nprocess, the original matrix is severely transformed on its values and is ready to be \nfinally transformed into the output.\n6. Output: the 2048 × 12,228 matrix that resulted from the processing of previous \nblocks needs now to be converted into a single next token for the original input array. \nRecall that the original embedding projected a vocabulary of 50,257 words (tokens) \ninto a 12,228 dimensional space. A 12,288 × 50,257 projection matrix (which is a \ntranspose of the embedding) now transforms the processed matrix into a score for \neach token of the vocabulary. The top-k highest scored tokens are separated, and a \n14 This is the most unknown or unexplainable part of the transformations that the input suffers. The opera-\ntions are simple and vaguely inspired on how natural neuronal networks function. But what exactly are the \nstructural changes that take place and what they correspond to in terms of humanly explainable linguistic \nor cognitive operations are fundamentally unknown. And might inevitably remain so.\n15  Although they are not technically “Turing complete” due to deficits in recursion and memory.\n1 3\nTransforming agency: On the mode of existence of large language…\nsoftmax algorithm simply transforms each token punctuation into a normalized prob-\nability that is proportional to its score. Then a final token is selected according to \nthese probabilities. Visually, this can be likened to a roulette wheel, where each seg-\nment’s size is proportional to the token’s assigned probability. The selection process \nmimics the spinning of this wheel, with the chosen segment indicating the next token \nin the sequence.\n7. Auto-regression: The above sequence of operations is repeated again and \nagain, adding a new token to the end of the string (e.g. a new word to the sentence) \nuntil the maximum number of allowed tokens is reached or, most commonly, an end-\nof-sequence token is produced by GPT (a kind of “halt” token that is interpreted as \na stop). It is possible to continue the process by reintroducing the input again and \nadding some more text (like when we add a response to the conversation). Although \nthe fact that GPT’S “intelligence” is often displayed when it stops, the most relevant \naspect of auto-regression is the type of “externalized” feedback that it provides for \nthe system. And this is an essential part of its functioning. Note that in no step of the \narchitecture so far did ChatGPT store any information, there is no “internal” state, no \nmemory. It is, in a sense, a purely reactive system16. It is through auto-regression that \nit does compensate for it, in a manner that will become very important when address-\ning the agentive capacities of GPT.\nThe term transformer was originally chosen to depict the sequential transforma -\ntion of the input matrix into an output without recurrence, with the task of translations \nas a key component (Vaswani et al., 2017). It was later applied to other tasks (like \nsummarization) and finally discovered that a large enough transformer could perform \nvery well generally across tasks. And also that it could “learn” what to do simply by \ndirect instruction with none or few examples of what was asked to do (Brown et al., \n2020; Radford et al., 2019). This is when the concept of prompt takes significance. \nThe power of LLM transformers is so general and unspecific that it is open to be \nprompted to unfold in different directions: summarization, translation, correction, \nexplanation, conversation, expansion of key ideas, development of outline strategies, \netc. The “magic”, so to speak, that sustains these capacities, lies on the parameters of \nthe system, the embedding, attention and FFNN matrices (colored purple on Fig. 1) \nthat operate on the input matrix.\nWe know very little of GPT-4 and subsequent OpenAI models. Despite its name \nand the original goals of the company soon after GPT-3 it moved into secrecy, releas-\ning very little information of new models, other than some raw details and bench -\nmarking results (OpenAI, 2023). It is speculated that one of the greatest architectural \ninnovations of GPT-4 over its predecessors was the inclusion of Mixture of Experts. \nThis is a technique of splitting the network into specialized subnetworks and train -\ning it to learn to distribute processing to them (Shazeer et al., 2017). We directly \nknow about the effectiveness of this technique, because it has been effectively imple-\nmented on open source or and more transparent LLMs that match GPT4 benchmark-\ning performance like Mixtral (Jiang et al., 2023), LLaMa (Meta AI, 2025), KIMI \nK2 (Moonshot, 2025/2025) or DeepSeek (DeepSeek-AI et al., 2025). The other \n16 You can explore this yourself by asking GPT to imagine a number or retain something secretly and \nperform operations on it and the like.\n1 3\nX. E. Barandiaran, L. S. Almendros\nimportant innovation is multimodality. Tokenization has nowadays turned multi -\nmodal. Images are broken into a grid of patches (e.g., 16 × 16 pixels) and each patch \ntreated like a “word” (a token), thus creating a sequence of “image tokens” that can \nbe fed into the main Transformer (Dosovitskiy et al., 2021). The same goes for audio \nwaveforms. Once encoded, all these different tokens (text, image, audio) co-exist in \nthe shared, high-dimensional vector space, where relationships and patterns can be \nlearned between modalities 17. Although functionally relevant, these innovations do \nnot imply a radical change from the explanation we just provided. The architecture \nremains fundamentally the same.\n3.2 Training\nGPT and other LLM configuration is typically carried out in various stages. The \nfirst is, somewhat paradoxically, called “pre-training” but constitutes the main train-\ning (understood as the process by which one improves or acquires new capacities). \nDuring this process, the parameters of each processing transformation just described \ngradually change until a given level of accuracy is reached, pre-training ends, and \nthey remain fixed until new training procedures start. Then comes fine-tuning, with \ntwo basic stages: task specific fitting and reinforcement learning. Finally, prompt \nlearning is often used, which is more of an instructional form of directing the system.\n3.2.1 Pre-training\nExplaining first the way of functioning of the whole architecture, as we just did, is \nessential to understand training. Contrary to other approaches, each processing block \nis not trained in isolation to perform a specific task (e.g. 1 st grammatically articulate \nthe input, then build a general abstract representation, next, carry out inferences and \ntake an output decision), but the entire system is trained at once, through back-prop-\nagation (Rumelhart et al., 1986).\nThe basic mechanism is simple: the system is initialized with random parameters. \nNext, a chunk of input (e.g. the beginning of a sentence) is chosen among a training \ndataset. It is then processed as we explained above. This is called a forward pass. This \npass finishes when the system provides the result array: that which indicates the prob-\nabilities of all the words to be the next one (the step before selecting the final output). \nThe result will be nonsense at the beginning. For example, to the input “Elephants \ndon’t play” the highest probability of the result array could be “purple”, followed by \n“Fodor”, “misuse”, “chain”, “cat”, etc. Now, this is compared to the correct result: \nan array that gives 0 probability to all the words except 1 for “chess”. But “chess” \nmight be very down on the assigned probabilities. Yet, it is now possible to compute \n17  Multimodal generation is more complicated. Instead of recursively adding tokens to a string, like lan -\nguage models do, image, sound, or video synthesis typically relies on generative diffusion models (Ho et \nal., 2020). They work on what can intuitively be understood as a kind of “internalised sketch pad”. The \nmodel starts with a tensor of pure random noise and iteratively refines it by subtracting the noise until a \nclean image emerges. In multimodal text-to-image systems, this “denoising” process is conditioned on \nthe text prompt, steering the generation towards an output that is “semantically” aligned with the user’s \ndescription.\n1 3\nTransforming agency: On the mode of existence of large language…\nan error (or loss): the difference between the assigned probabilities on the result array \nand the target one.\nNext, this loss will be backpropagated through the network (the backward pass). \nBy means of an optimization algorithm, small changes are made all throughout the \nwhole network in the direction of minimizing this error: The algorithm calculates the \nresponse to the question “what change should I do to this parameter so that the result-\ning output reduces the error?” and makes the change accordingly, for each parameter \non each block, backwards.\nThis process is iterated once and again, until the forward process produces no or \nlittle errors. All three major components of the LLMs are trained in this way: embed-\ndings, attention mechanisms and feed-forward networks. Although the overall pro -\ncedure is locally relatively simple, the amount of little changes is vast and the effect \nis the performance capacity we can witness today. The computational cost of train -\ning GPT−3 was 3.14 × 10^23 FLOPs, that is, 314 sextillion floating-point operations \n(Brown et al., 2020 Appendix D).\nThe system so far is considered a raw foundation model: it can process text gen -\nerally and can be put to work on a number of tasks already or be further trained to \nimprove performance on specific types of tasks. The training process, so far, is con -\nsidered unsupervised, nothing other than the next-word-prediction is used to train the \nmodel.\n3.2.2 Fine-tuning and reinforcement learning\nAdditional training procedures are used to fine-tune the transformer for specific tasks, \nlike summarization, translation, or conversation. This time the instruction (e.g. sum-\nmarize) and the task input (e.g. a whole Wikipedia article) are provided, and the \nsystem is trained with back-propagation to match a model output (e.g. Wikipedia’s \nsummary entry for that article), instead of just the next token. This is considered \nsupervised learning, no human intervenes yet, but the task is not simply to “guess” \nthe following word but to match a specific target goal, pairs of input and target-output \nare required to complete this training.\nTransformers are usually further trained to include Reinforcement Learning with \nHuman Feedback  or RFHF (Ziegler et al., 2020). The pre-trained and fine-tuned \nLLM is let to interact with humans. Then, based on how humans have positively or \nnegatively evaluated the output of the model, it is trained to produce outputs that are \nmore likely to be positively rated or less likely to be negatively valued; according \nto the past corrections made by human interactors. This is where the system is often \ntrained on ethical or moral values, together with a number of other quality checks.\nFinally, we have one-shot or few-shot learning  procedures that operate basically \nat the prompt level, providing examples or specific instructions that the LLMs take \nas input to produce new examples or follow the instructions provided (Brown et al., \n2020).\n1 3\nX. E. Barandiaran, L. S. Almendros\n3.2.3 Self-improvement and AI-supervised training\nIn order to overcome the limitations of LLM to solve complex problems (e.g. in \nmaths or coding), new innovative training techniques have been introduced. Interest-\ningly, they often involve the use of AI generated feedback, data, or supervision. In \nProcess-supervised Reward Models (PRM), for example a specific artificial model, \nthe PRM, is first trained using a huge database of reasoning paths where every step \nhas been labeled by humans as correct or incorrect. The first goal here is to train the \nPRM to evaluate good reasoning, assigning rewards to each individual reasoning \nstep. The LLM is then trained by the PRM, acting as a “teacher” via reinforcement \nlearning (RL) to produce novel reasoning chains given new prompts (Lightman et al., \n2023). This would be equivalent to teaching first a machine to identify good steps on \ncooking recipes, and then using it to teach a cook to generate good recipes by judging \neach step to achieve the final (prompted) request.\nOther methods involve, instead of reinforcing specific steps on a reasoning chain, \nto differentially reward different chains of thought according to their relative quality \n(Shao et al., 2024). In this case, the student is left to generate different recipes and is \nrewarded according to how good each recipe would comparatively result to a culi -\nnary critic (who does not really know how to cook but has been trained to develop a \ngood taste). This is the formula behind the success of DeepSeek-R1 (DeepSeek-AI \net al., 2025). A further innovation from this company involved distillation, where \na large, powerful “teacher” model is used to generate the vast synthetic dataset of \nhigh-quality reasoning paths and ranked-choice answers 18. The smaller, more effi -\ncient “student” model is then tuned on this machine-generated corpus (similar to the \npre-training phase). Following the metaphor, once a good cook is trained, it is pos -\nsible to ask it to generate millions of new recipes and to train a smaller (cost-efficient) \nmodel with them, overcoming the limited amount of recipes in available cookbooks.\nThese AI-supervised procedures can be applied not only to improve reasoning, but \nalso to “induce” ethical guidelines. This is the case of Constitutional AI, where the \nmodel learns to align its behavior with a set of written principles, a “constitution”, \ndefined by humans, but does so without direct human feedback. The process involves \nan initial phase where the model is prompted to critique and revise its own responses \naccording to the constitutional principles. The system is then tuned to predict the next \ntoken of those self-revised responses. In a second phase, a teacher preference model \nis trained on the constitutional principles and then RL with AI feedback is applied to \nthe LLM (Bai et al., 2022).\n3.3 Anthropomorphising GPT\nCalls to avoid anthropomorphizing GPT are recurrent (Bender et al., 2021; Butkus, \n2020; Coeckelbergh, 2021; Jebari & Lundborg, 2021; Kubes & Reinhardt, 2022; \nShardlow & Przybyła, 2023). But anthropomorphizing here only referred to pro -\n18  The DeepSeek team generated one order of magnitude more reasoning chains artificially for distillation \n(about 800 K) than the original dataset presumably used by the OpenAI team with 75 K examples (Light-\nman et al., 2023).\n1 3\nTransforming agency: On the mode of existence of large language…\njecting human qualities, particularly cognitive or emotional ones, to the machine. \nSomething that is perceived as a risky strategy, since understanding (or experienc -\ning) the interaction with ChatGPT through the human or intentional stance (Dennett, \n1989) as if it truly had genuine human capacities, would make us falsely attribute a \nset of properties it certainly lacks. Properties that are essential to the human social \nworld-making: commitment, trust, responsibility, empathy, etc. Important as it is, the \nemphasis of this type of anthropomorphization shadows other important forms. There \nare at least two more types of anthropomorphization that are relevant to understand \nGPT. And their analysis is perhaps more revealing of its mode of existence than the \nattribution of mental properties to the system. The first such type of anthropomor -\nphizing is the way in which the training corpus and procedures shape the machine as \na human. The second is the inverse process of trying to bring to the human scale and \ncapacities the internal workings of the system. We shall attend to both in this section \nin a somewhat combined manner.\nRegarding the processing, according to Kaplan et al. ( 2020), we can roughly \napproximate the computational cost of processing a single token to be directly pro -\nportional to the number of parameters of the model. GPT−3 having 175 Billion \nparameters, the computing cost of writing a 250 token summary of a 1750 token \nessay could have the approximate cost of 2000*175 Billion= 350 trillion FLOPs \n(floating point operations)19. Carried out by a human, as an experientially graspable \ntask, each FLOP could be approximated as equivalent to a multiplication between \ntwo 5-digit numbers. Assuming such an operation could take about 10 s to be com-\npleted by an expert or well-trained human being20, it would take around 500 million \nyears of human labor, working 40 h a week, to process that prompt21. That means that \nJohn Searle would have to live and die a few million times  before he could output \neven the first symbol from his Chinese room. Intuitions that once worked at a certain \nscale (like the Chinese room experiment) might not necessarily be trusted at many \norders of magnitude higher scales.\nRegarding the training aspect, we know little of GPT−4 but GPT−3 was trained \nwith 570 GB of text data, about 300 billion tokens according to their own creators \n(Brown et al., 2020), that is approximately 200 Billion words. Thus, the training data \nfor ChatGPT is equivalent to about 2 million books. A volume so vast that it would \ntake a human being more than 500 years to read through it all, assuming they dedi -\ncated 8 h a day, every day, reading at a speed of 200 words per minute. To put this into \nfurther perspective, if an average person reads roughly 500 books in their lifetime, \nthe amount of data ChatGPT has been trained on is comparable to the combined read-\ning of 4000 lifetimes. But if we were to humanly compute all the backpropagation \nprocess of 314 sextillion FLOPS, that would take an expert human 4.19*10^17 years \n19 Although the recursive nature of the output processing could indicate an exponential growth of this cost, \nit is effectively reduced by not re-processing the whole input again (see this discussion for a more detailed \nexplanation Tunstall, 2022).\n20 It actually took one of us a few minutes to complete it!\n21 At the same time, it is worth noting that the human brain, at a subconscious, subpersonal level of activ -\nity, can carry out the equivalent of this 350 trillion FLOPs in about one second or less (Carlsmith, 2020).\n1 3\nX. E. Barandiaran, L. S. Almendros\nto compute, which is almost 7 orders of magnitude (30,386,783 times) the age of the \nuniverse.\nLet’s now focus on the inverse analysis of how ChatGPT is already anthropomor-\nphized by the training data, including the biases, themes, styles and poetic tenden -\ncies that are present in them. And not less importantly, by all the fine-tuning and \nhuman reinforcement learning. To say it differently, ChatGPT has no way of orga -\nnizing tokens around mothers, mice, or forests other than that provided by human \ntraces on texts. It is, thus, not surprising that we can anthropomorphize it. It already \nis. And it is so in a manner that cannot be fully grasped. Unlike a mannequin that we \ncan touch and verify that its shape is indeed human, but whose functioning is noth -\ning more than that of a piece of inert plastic. We can not even bring the complexity \nof the concrete functioning of GPT down to a graspable human scale. Its intensive \nand enormous training procedure, its gigantic internal structure and its vast mode of \noperation lies beyond the human scale of understanding22. However, its internal and \nbehavioral functioning can be generally (if not specifically) sufficiently understood \nso as to determine constraints to its mode of existence. And we can properly ground \nwhy and how we can avoid ontological anthropomorphization. Agency being a piv -\notal, often anthropomorphized, category for settling the ontological status of LLMs.\n3.4 Towards LLM based agents\nAt a first sight, nothing in this architecture qualifies properly as agency. Not even for \nthe most optimistic or naive engineers. The system is fully driven by the prompt and \ndirectly steered when output completion has taken place by a new prompt. Moreover, \nthe system has no internal states, no (internal) memory, no potential desires, goals, \nor purposes. When operating (after training is completed), not even a trace of what is \nprocessed is left within the system; except for the history of outputs that is continu -\nously fed-back into the system auto-regressively. In a sense, GPT operates like Leon-\nard Shelby, the protagonist of Christopher Nolan’s celebrated film Memento (2001). \nDevoid of the capacity to create new memories (yet able to use its knowledge), Leon-\nard externalizes instructions (goals, instrumental steps, etc.) and contextual informa-\ntion (pictures, notes, etc.) to regain the agency that he lost due to his amnesia.\nThe lack of agentive capacities of the raw GPT is apparent in the type of digital \nembodiments that AI engineers are providing to enhance GPT and develop so-called \n“autonomous GPT agents” (Andreas, 2022; Huang et al., 2024; Wang et al., 2023; \nWeng, 2023; Xi et al., 2023; Zhou et al., 2023a, b). March to June 2023 saw a rapid \nincrease of projects trying to deploy digital agents based on GPT and other LLMs: \nAutoGPT (Significant Gravitas, 2023/2023), AutoGen (Wu et al., 2023/2023), \nDemoGPT (Ünsal, 2023/2023), SuperAGI (admin_sagi, 2023), MiniAGI (Mueller, \n2023/2023). A number of initiatives have followed that promise to deliver fully oper-\national agents for programming (Wu, 2024; Yang et al., 2024) and tech giants seem to \nbe betting on LLM-driven agents to make generative-AI services profitable (Holmes, \n2024; Knight, 2014).\n22 So does a bacterial cell or the global economy, by the way; not to mention the human mind and the \nextended socio-cultural scaffolding.\n1 3\nTransforming agency: On the mode of existence of large language…\nThere are 5 kinds of LLM enhancement strategies that are being developed to \nmove from ChatBots to the so-called “agents”: (a) extended memory systems, (b) \nplanning strategies, (c) reflexive evaluations, (d) the use of tools, and (e) multi-agent \ninteractions. These strategies are most often implemented in combination, but it is, \nnevertheless, possible to differentiate them.\nExtended memory frameworks (like Langchain) make it possible for transform -\ners to temporarily extend and sediment autoregressive dynamics (e.g. rewriting and \norganizing summaries of past input context to increase its memory and better focus it \non specific task-goals). Sometimes such extensions are not different from our practice \nof externalizing memory in a notebook, writing To-Do lists and offloading planning \nstructuring in a bullet point document.\nPlanning strategies involve prompting the transformer to split a specific goal or \ntask into sub-operations that can then perform in sequence. This can be achieved \nthrough various techniques, the most known of which is the so-called Chain of \nThought or CoT (Wei et al., 2023). CoT is implemented by crafting prompts that \nencourage the model to “think aloud”. Instead of trying to answer directly to a given \nquestion, or to accomplish a task, the LLM is first prompted to explicitly write down \nhow it will plan to do it and then follow its own plan. This technique has been shown \nto enhance LLM reasoning and planning abilities. More sophisticated methods, like \nTree of Thought (Yao et al., 2023; A. Zhou et al., 2023a, b) involve combining a tree-\nlike decomposition of a variety of plans with LLMs capacity to reflexively evaluate \nthe adequacy of each potential plan (which brings us to the next point).\nReflexive evaluation  procedures as simple as asking the transformer to reflect \non the previous output and correct existing mistakes have been shown to dramati -\ncally increase performance (Madaan et al., 2023b), also to provide some degree of \nself-guidance on the completion of the decomposed sub-tasks. The generative and \ncreative capacity of LLMs to deliver execution plans is often combined by using \nLLMs to automatically evaluate them and to distill a more consistent strategy. Both \nplanning and reflexive evaluation are often now built-into “reasoning” LLMs (as \nexplained in Sect. 3.2.3).\nUse of tools  LLM can be connected to a wide variety of tools (Mialon et al., \n2023; OpenAI, 2025b; Schick et al., 2023; Shen & Yang, 2025), from programming \nconsoles like Python, to web-browsers, search engines, and, more generally APIs \n(Application Programming Interfaces) that make possible to interact with digital ser-\nvices through instructions (rather than visuomotor interfaces). These “tools” define \nthe “bodies”, interfaces and environments of LLM powered “agents”. In November \n2024, Anthropic launched the Model Context Protocol to define such tools and inter-\nfaces for LLMs, and it has rapidly expanded as a standard to let LLMs access differ-\nent services (Anthropic, 2024; Hou et al., 2025).\nMulti-agent interactions  Finally, in order to overcome memory limitations, \nlack of consistency or repeated failure, multi-agent approaches are used, which \ninvolves interacting, evaluating and selecting results from other agents (Li et al., \n2024). Increasing successful task completion through collective agency is frequently \nachieved by combining many of the techniques explained above, like self-organizing \nTree of Agents strategies (Chen et al., 2024).\n1 3\nX. E. Barandiaran, L. S. Almendros\nDespite the increasing enthusiasm on the potential of LLMs to provide solid foun-\ndations for digital agents, strong limitations have already surfaced: LLMs do not \nseem to be much better discriminating than they are generating plans (D. Jiang et al., \n2024a, b), they are very limited on their capacity to develop complex plans (Kamb -\nhampati, 2023), perhaps because it is still very hard (Wu et al., 2024) for LLMs to \nintegrate future tokens on their current processing; which is a fundamental way in \nwhich humans plan.\nIn Chat scenarios, human intervention can continuously stir the conversation, \ndiscard hallucinations, or ignore wrong answers. Agentic scenarios are different. \nThe human presence in the conversational domain makes the coupled LLM-human \nsystem much more fault-tolerant. But when humans are out of the loop, errors can \naccumulate catastrophically. Think of the cumulative effects of hallucinations or mis-\ntakes on making a cake: a mixture of eggs shells, salt, and flour could end up in the \nfridge instead of the oven, despite a 98% accuracy on the design and execution of the \nrecipe. It is thus no surprise that, unlike benchmarks directed at measuring linguistic \ncapacities, intelligence or knowledge, LLMs still score far behind humans in current \nagentic benchmarks (Liu et al., 2023; Valmeekam et al., 2023; Xie et al., 2024, 2025). \nIt is therefore still early to judge whether they can at least operate “as if” they were \ngenuine agents. This remains an open empirical issue. Meanwhile, it is possible and \nnecessary to explore how existing transformer architectures meet the requirements \nfor agency identified at a more fundamental level than that of pure performance.\n4 LLMs are not (autonomous) agents\nWe have seen how LLMs based on transformer architecture internally operate and \nhow their capacities have been expanded with a series of additions to the founda -\ntional trained models. Moving below the surface of performance-level measurement \nto characterize agency requires a certain commitment to theoretical or philosophical \nframeworks. It is essential to specify the nature of actions, purpose, and cognitive \nproperties, and to relate them to the underlying generative mechanisms. The detailed \ntechnical account of LLMs’ architecture and agent-like extensions will turn out criti-\ncal to assess their ontological status. In this section, we first approach the assessment \nfrom the point of view of computational representationalism (from which LLMs can \ncomfortably be characterized as agents). We then move to alternative, so called, 4E \nframeworks, whose requirements severely problematize agency attribution to LLMs.\nFrom the philosophical perspectives that have given credit and have contributed to \nthe AI research program, it is difficult to rule out genuinely agentive capacities from \nChatGPT-like systems. Representational computationalism is one such approach \n(Carruthers, 2006; Newell, 1980; Putnam, 1965). It is a type of functionalism that \ndefines mental properties (intelligence, knowledge, learning, or agency) in terms of \nthe input-output functional (internal transition) states of a system representing states \nof affairs of the environment. The essential feature of the mind is the capacity to \nreason or to draw inferences upon representations of the world; i.e., information pro-\ncessing. For instance, you take the umbrella because you just read it will be raining \ntoday, and you know that umbrellas are a good way to cover yourself from the rain. \n1 3\nTransforming agency: On the mode of existence of large language…\nAccording to representational functionalism, this is the kind of inference that is char-\nacteristic of mental processes. And, LLMs are well capable to make such inferences. \nMoreover, their internal states reasonably approximate the world. Being models of \nlanguage, and being trained on a huge amount of text, to the extent that all these train-\ning data can be squeezed to provide a model of the world, LLMs, are also models of \nthe world (Kadavath et al., 2022; Li et al., 2023a, b; Yildirim & Paul, 2024) including \nother agents (Andreas, 2022).23\nSome authors have gone even further, proposing that all reality is informational \nand agency is the ability to act upon and be affected by the (informational) environ -\nment. Agency is thus not limited to human beings but can also apply to artificial enti-\nties such as robots, software programs, and AI systems:\n“These new agents already share the same ontology with their environment \nand can operate in it with much more freedom and control. We (shall) del -\negate or outsource to artificial agents memories, decisions, routine tasks and \nother activities in ways that will be increasingly integrated with us and with our \nunderstanding of what it means to be an agent.” (Floridi, 2007, p. 62).\nFrom this perspective, LLMs could be considered agents perfectly embedded in the \ninfosphere. In fact, Floridi has recently contemplated this possibility and the problems \nand challenges involved, concluding that GPT-like AI systems are “agency without \nintelligence” (Floridi, 2023; Floridi & Chiriatti, 2020). His category of agency can \nbe understood as depending on two key components: capability and autonomy. Capa-\nbility refers to an agent’s ability to perform a certain action or set of actions, while \nautonomy (for Floridi, and much of AI engineering) refers to an agent’s ability to act \nindependently, without being controlled or directed externally. In short, this theo -\nretical framework can be summarized as follows: (1) all entities are informational, \n(2) some (informational) entities are agents, and (3) agents are entities that perform \nactions independently of one another.\nHowever, this characterization of agency might be too liberal. Agents are char -\nacterized by their capacity to carry out actions, as distinct from mere events or \nmechanically caused states of affairs24. Actions, unlike (other) events, are not merely \noccurrences in the world (informational or otherwise); they are processes imbued \nwith intentionality and purpose. This distinction becomes evident when comparing \nthe experience of intentionally reaching for a bike, an action, with being inadver -\ntently pushed towards it by the wind, an event. The former is characterized by a sense \n23  According to this view, having no “real” contact with the world is no fundamental obstacle. Certainly \n(some) LLMs have no vision capabilities to see if it is raining right now, but nor do you, when you read \nin the newspaper or your favorite weather-app that it is about to rain. The interface of information recep -\ntion does not affect the nature of the inference that it is appropriate to bring the umbrella with you. In \nturn, a LLM, without a robotic body, could not itself complete the action to take the umbrella, but it could \nperfectly command you to do so (by means of a text message) or could signal the cars’ top window’s con-\ntroller to close it. The nature of the cognitive process of making the right inference according to the right \nknowledge of the world is indifferent to the mediation of the input or output. In this sense, LLMs could be \nconsidered full-blown cognitive agents with more or less sensory and motor capabilities.\n24  Even when it is considered that actions are caused by events (Davidson, 1980), these are of a very spe-\ncial kind: reasons, beliefs, desires, etc.\n1 3\nX. E. Barandiaran, L. S. Almendros\nof directiveness and intention, elements central to the phenomenology of agency we \nexperience and recognize every day. Any output of a system (computing machine or \notherwise) does not automatically qualify as an action.\nMany have questioned the adequacy of informationalist and computationalist \napproaches to capture and explain cognitive and agentive capacities. In this critique \nconverge theoretical contributions from different fields: phenomenology (Gallagher, \n2017; Merleau-Ponty, 1944), philosophical and theoretical biology (Jonas, 1966; \nMaturana & Varela, 1980; Moreno & Mossio, 2015), philosophy of mind (Searle, \n1980; Noë, 2004; Hutto & Myin, 2012; Thompson, 2010) empirical contributions \nfrom the psychology of perception (Gibson, 1979; Heras-Escribano, 2019; Reed, \n1996) or conceptual development (Lakoff & Johnson, 1980), large-scale neurosci -\nence (Buzsaki, 2006; Freeman, 2001), and methodological contributions from com -\nplex dynamical systems’ theory (Barandiaran & Moreno, 2006; Favela, 2020; Port \n& Gelder, 1995). These and other criticisms have resulted in a family of alternative \napproaches that are often labeled under the term 4E-cognition (Gallagher, 2023); \nstanding for embodied, extended, enactive and ecological.\nWithin these, the approach outlined by Barandiaran et al. ( 2009) allows for the \ncomparison of natural agency with the operations of LLMs (or any other system) 25. \nThey start by reviewing different available definitions of agency (from software \nengineering to robotics, from philosophy to psychology) to bring together a surface \ndescription of what these definitions have in common: “a system doing something \nby itself according to some goals or norms”. They spell out what this commonality \nentails, identifying 3 necessary and sufficient conditions for agency: individuality, \nnormativity and interactional asymmetry. First, an autonomous agency requires that \na system be self-individuated. Second, the self-individuation process defines a set \nof norms (of viability) and, third, according to these norms, the system asymmetri -\ncally regulates its coupling with the environment (thus becoming the source of an \naction). In sum, from their perspective, an agent system is an autonomous organiza -\ntion capable of adaptively regulating its coupling with the environment in order to \nsustain itself according to the rules set by its own conditions of viability (Barandiaran \net al., 2009, p. 376).\nA bacterium moving up a sugar gradient (Berg, 2004) is a widely accepted paradig-\nmatic example of agency that satisfies the definition (Barandiaran & Egbert, 2014). \nFirst, the bacterium is in a continuous process of individuation and self-distinction. \nMetabolism produces the components of the reaction networks constituting the agent, \ntogether with a membrane that separates the system from its environment. This self-\nproduction in turn determines which aspects of the environment are relevant, norma-\ntively valued (good or bad) from the very constitution of the system: some chemical \ncompounds are essential nutrients for its self-maintenance (good), some others are \npoisonous compounds that degrade the membrane or the metabolic network (bad). \nFinally, the agent modulates its coupling with the environment by moving up or down \na sucrose (positively valued nutrient) gradient and absorbing sugar molecules across \n25  For a similar approach that identifies requirements and limitations of AI systems to meet enactive stan-\ndards for cognition see Froese and Ziemke (2009).\n1 3\nTransforming agency: On the mode of existence of large language…\nthe membrane. The whole combination of self-individuation, norm generation, and \nadaptive regulation constitutes the agentive nature of the bacterium’s behavior.\nProcesses of individuation and normative regulation need not happen at the meta-\nbolic scale exclusively. Mental or sensorimotor life can also ground agentive capaci-\nties (Barandiaran, 2007, 2008; Di Paolo et al., 2017), bringing it closer to our own \nexperience of intentional agency. Not only do we experience our living body (our bio-\nlogical agency), but also our actions in the world are guided by goals and intentions \nthat transcend mere biological values. This is so because a new level of autonomous \norganization emerges through the neural mediation of behavior. The individuation \nprocess is constituted by a self-sustaining network of sensorimotor schemes (e.g. \nhabits) in continuous development26.\nOur experience of agency stems from the fact that we are a mesh of habits that, \nthrough specific actions, asserts its own identity. We shape ourselves as behaving \nsystems by acting. A psychological and cognitive identity develops through activity \ndependent plasticity, organizing brain, body, and environment. And the norms that \nemerge from this identity direct my behavior. I identify myself as a philosopher, I \nwant to make a good contribution analyzing GPT, I struggle to write these lines cor-\nrectly. The goal of a specific task is nested into a network of interests and plans that \nultimately rest on preserving our identity (Barandiaran & Rama, 2025).\nYet I can be coupled with my environment in many ways that, despite involving \nmyself and my norms or goals, do not qualify as action. Someone else, chance, or \nsimple physical constraints, might move or prevent certain moves so as to contribute \nto my own norms, e.g., a nurse in a hospital taking care of me. But the result of this \nbehavior would not qualify as agency yet (in fact, I am a patient in a hospital, not an \nagent). It is not until the source of my behavior lies asymmetrically laden to my psy-\nchological or sensorimotor capacities and my sensitivity to my self-generated norms, \nthat my coupling to my environment can properly be called an action.\nDoes ChatGPT meet the three criteria for agency? Let’s analyze them one by one. \nThe first condition, individuality, requires that a system self-produces or at least self-\nsustains, distinguishing itself from an environment it co-defines. LLM’s existence \nand maintenance, however, are reliant on external human intervention and tools, and \nit operates within a predetermined environment. This diverges from the self-individu-\nation process essential for autonomous agency. Note that it is difficult to tell from the \nagentive perspective what is the system in GPT. On the one hand, there is the input, \ntransformed into a matrix and processed along the complex network of operations \ndescribed in Sect. 3. On the other is the transformer that is nothing but a set of opera-\ntor blocks, blindly transforming the input and the resulting matrices. Underneath is \nthe hardware, whose operations are indifferent to the type of processing taking place, \nor even to the fact that no processing takes place.\nWhen considering normativity, an agent system is inherently at risk of degradation \nwithout specific actions, and sensitivity to these viability conditions that emerge from \n26 For more representation minded approaches, this can also be conceived as a network of beliefs whose \nmain behavioral manifestation is the growth and maintenance of the network. Free energy and active \ninference approaches are relatively consistent with this view and several parallels have been drawn with \nenactivism (Clark, 2013; Kirchhoff et al., 2018; Seth, 2021), although severe objections to identify both \ntheories have also been drawn (Aguilera et al., 2022; Di Paolo et al., 2022; Nave, 2025; Raja et al., 2021).\n1 3\nX. E. Barandiaran, L. S. Almendros\nits process of individuation is essential for normative behavior. ChatGPT, however, \noperates without such precariousness and does not autonomously establish its goals. \nIts functions are programmed externally, driven by an error or loss function that is \ncompletely extrinsic to the operations of the system. This absence of intrinsic norma-\ntivity is evident in ChatGPT’s inability to recognize failure autonomously, often lead-\ning to repetitive or non-productive responses. A counterintuitive property of LLM is \nthat they are typically capable, retrospectively, if explicitly asked, to identify “errors” \non their previous operations (Madaan et al., 2023b). And yet the absence of implicit \nnormativity is apparent in that, given their autoregressive character, they never “real-\nize” their mistake unless prompted to do so.\nInstead of purpose ful action (behavior that is imbued with purpose all along its \nunfolding) transformers are somehow partially purpose- structured as a result of the \npre-training on the massive purpose-expressive texts, and purpose- bounded, that is, \nstatistically falling within “humanly interpretable normative” bounds, as a result of \nfine-tuning and supervised learning procedures. Moreover, training procedures used \nto shape autoregressive loops as “think aloud” strategies for so-called “reasoning \nmodels” endow them with purpose-structured and purpose-bounded generative pro -\ncedural capacities27. However, unlike living agents, ChatGPT lacks the essential con-\ncern or awareness for goal achievement that converts an operation into a purposeful \nact.\nThe concept of interactional asymmetry underscores that an agent system is the \noriginator of its actions, modulating its interaction with the environment becoming \nthe source of the action. ChatGPT, however, shows a fundamentally reactive nature \n(it has no internal state). It responds passively to external inputs (prompts) and, as \nwe just identified, lacks self-defined norms to guide its interaction with the environ -\nment. Pushed by an initial prompt, its operation, however complex, rolls down a \nfixed (albeit probabilistic) and instantaneously reactive path toward the next out -\nput. And yet, the autoregressive aspect provides a powerful form of recursion that, \nwithin the right context, particularly that provided by “agency” extensions of LLMs, \nincreases the interactional asymmetry property towards the transformer. When care-\nfully crafted, LLMs can partially escape their downhill reactive nature (by rewriting \nthe landscape they roll through) but, devoid of intrinsic individuation and normativ -\nity, fail to become genuine agents.\nWhen measured against the criteria for autonomous agency derived from contem-\nporary embodied and enactive frameworks, LLMs fall decisively short. This con -\nclusion is grounded in the specific technical and operational details we outlined in \nSect. 3. Their lack of self-sustaining individuality, their operation based on externally \ndefined loss functions and reinforcements instead of intrinsic normativity, and their \n27  In a sense, it is possible to identify these systems as second and third order derivative intentional \nsystems, following the classical distinction between intrinsic or genuine intentionality, characteristic of \nhumans, and derived intentionality, characteristic of signs and artifacts (Haugeland, 1990; Searle, 1983). \nLLMs get their derived intentionality from that of their training data (second-order derived intentional \nsystems) and, in turn, generate intentionally-structured products that recursively shape (through chain-of-\nthought techniques) their subsequent outputs (third-order derived intentionality). A more detailed account \nof this connection between Generative AI and intentionality can be found in Barandiaran and Pérez-Ver -\ndugo (2025).\n1 3\nTransforming agency: On the mode of existence of large language…\nfundamentally reactive nature are core architectural and operational limitations that \npreclude genuine agency.\nOnce established what LLMs are not, we are now faced with the constructive \ntask of defining what they are. The next section, therefore, moves beyond critique \nto propose a positive conceptual framework for understanding their unique mode of \nexistence and their transformative potential for human agency.\n5 What are transformers then, if not agents, and how do they \ntransform agency?\n5.1 The language automaton and the ghost in the human-machine interaction\nIf transformer architectures, and LLMs as we know then, are not (autonomous) \nagents… What are LLMs that have such a strong impact on our digital environment \nand the way we live in them? If we are not to embrace transformers as members of \nour familiar way of being (as agents) in the world, we need to start somewhere else. \nFirst, it might be useful to distinguish operations from actions. An operation is a \nsequence of occurrences that can be interpreted functionally; that is, in (instrumental \nmeans-ends) relation to a final end state. Mechanisms carry out operations. A digital \noperation is a logical transformation executed by a computer. Operations are exter -\nnally defined, whereas actions are internally defined by the agent that carries them \nout (in the sense outlined in the previous section). Note that an action can externally \nbe defined as an operation and translated into a machine. But it is more convenient to \nuse a specific name to label those processes that can be described both as an operation \nwhen carried out by a machine and as an action when done by a human: we might call \nthose tasks (see Table 1).\nWith these distinctions at hand, we can now proceed to properly characterize \nLLMs. From the point of view of their organization, transformers are automata, as \nopposed to autonomous systems. This distinction is still relevant and crucial. Auto -\nmatic28 systems do not need human intervention to carry out certain operations, but \nare not autonomous. They cannot define their own identity and norms. However, as \nstructured and identifiable digital instruction sets in physical memory and proces -\nsors, they can carry out complex sequences of operations in the real world. They can \ntransform energy into operations without human supervision or intervention during \nthe process. They are automata.\nHowever, transformers are not any kind of automata, they are of a very special \ndigital kind, and operate in a very special type of environment, with an even more \nsingular relationship to human life: they are digital language automata operating in \nmultiple language-supporting and language-driven digital networks we continuously \ninhabit as linguistic animals (together with many other digital and physical objects \naround us).\n28  Although the etymology of automatic or automata ultimately brings us to self-minded or self-willed \nwith a very strong mentalistic connotations, its popularization to depict mechanisms capable of completing \nsophisticated chains of operations is the sense that it nowadays embodies.\n1 3\nX. E. Barandiaran, L. S. Almendros\nMoreover, LLM-powered chatbots, like ChatGPT, are specifically constituted by \nthe way they couple with their associated milieu: other humans. As such, they often \nbecome phantasmic language automata29. In some sense, ChatGPT is certainly the \nghost of the text corpus that it originally was trained on. As a “phantom”, GPT can \ntalk to us without the capacity to bring itself to life. Nevertheless, it is capable of con-\njuring all the knowledge of the corpus, of which itself is a shadow. As an automaton, \nit is capable of producing changes as a result of a sophisticated mechanism. Changes \nthat trigger the response of the acknowledgment of an equal from those with which \nit phantasmatically interacts. In this sense, it is possible to characterize ChatGPT as \nan interlocutor automaton in a double sense that mirrors the twofold meaning of the \nterm “interlocutor”: (a) as a system capable of performatively interacting in effective \nconversations with humans (and with other machines traditionally designed to take \nhuman produced text as input, e.g., programming, database queries, etc.); and (b) \nas an intermediary between (practically all digitized textual) human knowledge and \nother humans or machines.\nIn this sense, ChatGPT operates as a gigantic text-that-talks, or rather a library-\nthat-talks30, enabling a dialogical engagement with the vast corpus of human knowl-\n29  Our use of metaphors such as ‘ghost’ and ‘phantom’ is a deliberate methodological choice. In periods \nof rapid technological change, where the conceptual landscape is unsettled, metaphors serve as essential \ncognitive and philosophical tools for structuring an inquiry into a novel phenomenon (Hesse, 1966; Lakoff \n& Johnson, 1980). They function by inviting us to adopt a new perspective or “framing”, which organizes \nour understanding of a subject in ways that a literal description cannot (Camp, 2006). This allows us to \ntake seriously the phenomenological character of the human-LLM interaction, which a purely technical \ndescription might obscure. Metaphorical phrasing allows us to condense intuitive understandings, antici -\npate future lines of inquiry, and reorganize our conceptual framework in ways that can precede and guide \nmore detailed technical articulation. Our aim is to use these metaphors as conceptual probes, which we \nthen ground in the technical and philosophical analysis that follows. In fact, the public and academic \ndiscourse surrounding LLMs is already deeply shaped by metaphors, from the deflationary “stochastic \nparrots” to “autocomplete on steroids”. Our goal is to provide new metaphorical framings that we believe \nmore adequately capture the complexity and transformative potential of these technologies. In this case, \nthe metaphor of a “phantasmatic language automaton” captures a dual nature. It is an automaton because \nits operations are mechanistically determined and devoid of the self-generated norms characteristic of \ngenuine agency. The ‘phantasmic’ aspect, however, arises not from the machine alone, but from the human \nuser’s phenomenological co-creation of the interaction. Humans readily project and enact a conversational \npartner when presented with a sufficiently coherent interlocutor. The LLM serves as a perfect scaffold for \nthis projection, creating a ‘phantom’ conversational-other that is experientially real for the user but lacks \nan autonomous source on the machine’s side.\n30 And, we could say, in the more advanced multimodal cases, as a media-library-that-talks-and-paints.\nLLM either Human\nMachine System Individual\nOperation Performation/Task Action\nAutomata Performer Agent\nAutomatic Performant Autonomous\nEnd state Goal Purpose\nCalculator\nWriter\nPainter\nInterlocutor\nTable 1 Conceptual divide be-\ntween machine and human types \nof identities, properties, and \ntypes of behavior. The middle \ncolumn captures a common \nvocabulary that permits both \nhumans and machines to share \nan interactive conceptual space. \nNote that systems can combine \nmachine-individual hybrids to \nbecome individuals and indi-\nviduals are (also) systems and \ncan perfectly be interpretable as \nperforming automatically\n \n1 3\nTransforming agency: On the mode of existence of large language…\nedge and cultural heritage it has “internalized” (compressed on its transformer \nmultidimensional spaces) and that it is capable of recruiting effectively in linguis -\ntic exchange. The machine’s interlocution, though devoid of personal intentionality, \nbears the trace of human experience as transposed into digitized textuality. The pur -\npose-structured and bounded automatic interlocution, however, can be experienced \nas a genuine dialogue by the human subject. As a result, ChatGPT brings all this \ndigitized textual knowledge, within the technological milieu, ready-to-chat.\nReadiness to chat also implies readiness-to-command. And since we live in a \nworld of linguistic performance and greatly digitized linguistic milieu, commanding \ncan easily be turned into performance by a linguistic automaton. This is where recent \nenthusiasm with LLM “autonomous agents” lies. Transformers (properly “embod -\nied”) can command themselves and other LLMs. Unlike mechanical automata, lin -\nguistic automata can be commanded by goals expressed in natural language, and \ncan, at least partially, evaluate whether the results of the tasks carried out match the \nlinguistic goal-expression. The capacity that truly brings linguistic automata close to \nsome aspects of human agency is that commands of this sort need not be expressed \nwithin the strict boundaries of a computationally interpreted language (like program-\nming languages or shell commands). LLMs can operate within the flexible and con -\ntext dependent mesh of “natural” language.\nYet, the phantasmic dimension of LLM chatbots does not only reveal itself in its \ncapacity to bring the dead text into non-living automatisms. It also has to do with \nChatGPT as an enacted other, when given the interlocuting role. To appreciate the \ncomplexity of our interaction with ChatGPT, it is first essential that we understand \nwhat happens when we speak with other autonomous linguistic agents and what hap-\npens when we speak with “phantoms”. Embodied and enactive approaches to soci -\nality (Di Paolo et al., 2018; Gallagher, 2017; Pérez & Gomila, 2021) defend that \nsocial cognition involves dynamic interaction with others. Social cognition is not \nabout rationally reconstructing the thoughts that others are holding, but the result of \nongoing fluent interactions between two or more agents. It has fundamentally more \nto do with dancing with another person than rationally strategizing a chess play by \nmail. Conversations are constituted by a complex chain of partial acts that imply, and \nsomewhat anticipate, their completion by others (e.g., giving and taking, question \nand answer, salutation and response, etc.). What happens when the other is absent? \nWell, in a sense, we often play as-if it was there. We imagine that it is there, and re-\nenact a completion of our acts (e.g., an imaginary conversation with a friend). In a \nsense, we incorporate the absent other, internalizing the various dimensions of what \nwould otherwise be an open interaction.\nThe experience of a phantom is somewhat a continuation of this capacity, to which \nwe add the perceptual (visual, sound, etc.) hallucination as a means of a partial exter-\nnalization of the experience. With ChatGPT, we have excorporated the phantom. The \nphantom is “real”, but still a phantom. The perceptual feedback is real (the text and \nsounds we hear come really from out there) but some hallucinatory and self-comple-\ntive aspects of the interaction are still constitutive of it. There is no real-agent on the \nother side, but we still act as-if there was, thus in a sense making it a real conversa -\ntional experience (see also Persson, 2024). Similar phenomena have been reported \nin Human-Robot Interaction research, describing the human tendency to navigate a \n1 3\nX. E. Barandiaran, L. S. Almendros\n‘social artifact puzzle’ by treating mechanical systems as intentional agents to make \ninteraction more tractable (Ziemke, 2023).\n5.2 Transformer embodiments\nCognitive science has turned from abstract symbolic computations into the (histori -\ncally neglected) role of the material body in the production of mindful experience \nand capacities (Calvo & Gomila, 2008; Gallagher, 2023; Shapiro, 2019; Varela et al., \n1991). Cognitive agency is said to be embodied, extended beyond the brain as the \n“mere” hardware of the mind executing the genuine mindful “immaterial” software, \ninto the living body of the cognizer, its technical equipment and sociomaterial envi -\nronment. There are many layers of embodiment that Transformers rest on, and many \nthat they lack compared to those of human intelligence. Most notably, ChatGPT lacks \na sensorimotor body that captures variations on its physical environment (it lacks \nphysical sensors) and also motor actuators that change its relationship with the envi-\nronment and induce further sensory changes (Chemero, 2023)31. But, as we men -\ntioned in the introduction, a sensorimotor robotic body is quickly being integrated \nwith existing multi-modal LLMs, and its impact on deeper linguistic and behavioral \nskills can be expected to be significant. However, so far, transformers lack a living \nand lived body (real or simulated), that is often associated with emotions and affec -\ntivity regarding cognitive or agentive capacities (Damasio, 1994; Thompson, 2010). \nThis certainly sets humans apart from LLMs. But there is nothing new or specific to \nLLMs on such lack of embodiment. In the previous section, we have sketched some \nconsequences of this not-being-alive. This is a notable difference between Artificial \nand Natural intelligence that has attracted attention and arguments elsewhere as well \n(Froese & Ziemke, 2009; Koch, 2019; Seth, 2021; Thompson, 2010; Ziemke & Lowe, \n2009). We shall now focus instead on two forms or aspects of LLM embodiment that \nare novel and have received little attention from the point of view of agency.\nAs a first type of embodiment (although in a sense very different from the usual \none), we should focus on the written corpus (body in Latin) on which LLMs are \ntrained and that they so effectively bring back to text. This is not simply a metaphori-\ncal use of the term body or embodiment. Textuality is an abstract, yet very concrete \nand complex, form of materiality itself, an externalized embodied product of lin -\nguistic agency. It brings with it purpose- and experience-structured relationships that \nmight bear deep isomorphism with them. As human digitized culture is partly acces-\nsible to us, we often neglect the tremendous value (and size) of LLMs compressed \ncorpus: an organized model of the textualized human knowledge and culture 32. We \n31  Modern multimodal systems identify and correlate patterns across modalities but lack the active, \nworld-directed and normatively endowed nature of embodied agency. The crucial feedback loop, where \nan agent’s actions generate new sensory contingencies that shape subsequent actions, remains absent. The \nmodel analyses a static image, but it does not visually and actively explore a normatively valued environ-\nment to achieve an intrinsic goal. Thus, multimodality expands the model’s input beyond pure text, but it \ndoes not provide a sensorimotor body in the enactive sense. It merely expands the “library” of passively \nacquired data into a “media-library”.\n32  We are so immersed in the complexities and subtleties of our culture and so “shocked” by the power of \nthe new AI that we frequently only focus on what still distinguishes us from it or assign purely instrumental \n1 3\nTransforming agency: On the mode of existence of large language…\nneed a theory of what this written corpus really is from an embodied and phenom -\nenological cognitive science perspective. But current theories, so far, lack an account \nof the cognitive or agentive implications of large-scale computable textuality; and of \nthe transformation of social, cognitive and ultimately biological lifeforms they bring \nwith it.\nFor some theorists, it could well have been the case that a new generation of AI-\nsystems was bootstrapping itself to human level intelligence by means of pure ratio-\nnal deduction and interaction with the environment (perhaps also as the exclusive \nresult of the engineering effort of a private corporation33). That is not the AI we have \navailable today. This one is built on the collaborative effort of thousands of math -\nematical and computer science contributions, it is fed or trained on huge amounts of \nuniversal written heritage, collaborative digital commons (like Wikipedia or massive \nopen source code repositories) and millions of distributed conversations and coop -\nerative efforts (mostly on internet forums). No less important are the embodiments of \ncare that usually take the form of labor externalization of massive data curation and \noperational alignment supervision, and that requires and recruits social-emotional \nresources from underpaid labor to safe-rail the brute models (Perrigo, 2023; C. Xiang, \n2023). Another significant aspect of LLMs embodiment is their heavy computational \nand thus energetic and resource-hungry nature, and the extreme capitalist supply-\nchain extractivism it triggers, demands, and sustains (Valdivia, 2024).\nIn this sense, more than a self-bootstrapped Artificial Intelligence, ChatGPT, as an \ninterlocutor automaton, is a computational proxy of the human collective intelligence \nexternalized into a digitalized written body. It is, in turn, shaped and taken care of by \nhundreds of human and non-human lives. Thus, although in a very wide sense, yet \none that is crucial to the effective operation of LLMs, transformers are embedded on \nlarge scale human and ecological bodies. This happens not just at a contextual level \nor as an operational environment, but at a constitutive level. No LLM is an island. \nAnd their performative power, and derived agentive capacities (if any), inherently \nrest on human and planetary scale life.\nThese dimensions of the material embodiment of LLM training and execution \nare crucial to understand the types of asymmetries it will bring to extended agentive \ncapacities in humans. During the last 40 years, with the advent of the PC, there was \nlittle extended-agency computational asymmetries between human agents in general. \nAccess to specific types of information has always been asymmetric between humans, \nbut beyond this, the informatic capacity of a 14-year-old hacker and that of a big cor-\nporation was relatively even. With some rare exceptions, you and I could do the same \nthing with a PC or a mobile device compared to what Elon Musk, Queen Elizabeth \nor Warren Buffett could do. The computing (energy, processing, and data) cost of \ncompiling and executing the most complex of software products (e.g., an operating \nsystem) was relatively accessible to most. Now, although (relatively low cost) access \nvalue to it. However, how would we qualify the value of a LLM-like device, were it the only or primary \nsource of access to an extinct human culture (or a distant alien one) for which we (or the LLM) possess \n(nevertheless) some kind of translation capacity?\n33  Ideally, also, that corporation produced or acquired the information and environment for the robot to \nbecome smart and also assumed the cost of externalities associated with its AI’s intellectual growth.\n1 3\nX. E. Barandiaran, L. S. Almendros\nto the best generic LLMs is still “widely” guaranteed, and despite locally-executable \nLLMs’ quality is growing, the asymmetry on the capacity to train and deploy specific \nLLMs, is orders of magnitude bigger.\nOn the one hand, there is the power implicit on how and what to train LLMs on, \na power in the hands of those very few with the resources to train and shape a foun -\ndation model: the direct constraining power of training data choices and training \nprocedure selection, the power of establishing RLHF criteria, constitutional writing \ncapacity and interface design of massively deployed LLMs. On the other hand, the \nso-called “AI autonomous agents” only operate with some efficiency under exponen-\ntial computational costs, either by a “social” distribution of tasks, by massive parallel \nplanning and evaluation or by deeply iterative and long “chains of thought”. That is, \nby making up for the lack of genuine intrinsic purposefulness with redundancy. Their \neffective deployment might bring super-agency to privileged human masters, while \ndelivering low-quality, low-cost alternatives to most. The asymmetry that the subse-\nquent power amplifies is enormous. This is certainly not unlike the recursive power \nasymmetries that are already present in contemporary societies34 except for a crucial \nfact: language digital automata remove humans (and their capacity for disobedience, \nresistance, and uprising) from the (social) sources of power.\n5.3 Assisted, extended and midtended agency\nWhat is ChatGPT as an extension of human cognitive agency? Is it an assistant? \nMost technologies that have been thought of as extensions or embodiments of human \nactivity have been conceived as bodily prostheses. The extended mind hypothesis \n(Clark & Chalmers, 1998) and its later developments, including material engagement \ntheories (Malafouris, 2016), have focused on the way in which beyond-the-skull \nextended material or computational processes should be understood as constitutive \nof cognition or brain processes. According to this view, mindful thinking processes \nmust often be understood as extended into the material environment that they shape \nand, in turn, also shapes cognitive processes. Thinking involves thinging. When we \nmake pottery, we do not print or carve an internal mental 3D model into the clay, we \nmold it. The materiality of the clay guides us, we bring the jar into form through a \ncontinuous reciprocal interaction between brain, body and world. Digital technolo -\ngies constitute a branch of these phenomena: we offload memories, drafting proce -\ndures, image manipulation, etc. into our PC and mobile devices. Cognitive gadgets \nare organs of our mind extended beyond the skull.\nSocial interaction also extends and assists human agency: crew and team members \ninteract (coordinatively or subordinatively), to achieve levels of agency (collective or \ndirective) that are unreachable to a single individual (Lewis-Martin, 2022). The inter-\nlocutionary capacity of LLMs brings human autonomous agency to a level not-unlike \nthat of intersubjectively augmented agency, where the machine takes the role of an \nassistant (or that of a master). Recent human-computer interfaces have been domi -\nnated by action directed design. We tell computers what they have to do (by program-\n34  The richer you are, the higher your capacity to hire good lawyers to reduce your tax payments, your \ncapacity to pay consultants to make more profit from your investment, etc.\n1 3\nTransforming agency: On the mode of existence of large language…\nming a specific function, by pushing a button, by dragging a file, by selecting a menu \nitem). LLMs, instead, make it possible to prompt or command an intention (Nielsen, \n2023): “I want a summary of this text in French so that the 5-year-old child of my vis-\niting friend can understand it”, “I need an impressionist style picture for a book cover \non philosophy of mind”. This is certainly going to bring us to unprecedented forms of \nmaster-slave dialectical relationships with machines (and the corporations they ulti -\nmately serve). On the one hand, we might soon have at our disposal an “unlimited” \nnumber of assistant automata ready to perform computational and linguistic tasks for \nus. On the other hand, we might be equally commanded by them35, not only through \ntextual interfaces but also more sensorimotorly embodied throughout the course of \nour everyday behavior (e.g., Meta glasses connected to LLMs capable of interpret -\ning our visual scenes and delivering “suggestion” or “orders” to accomplish specific \ntasks (Meta, 2023; Waisberg et al., 2023).\nBut Transformers are also bringing with them a much deeper meaning of extended \nagency (with deeper dialectical connotations). There is a form of extended agency \nthat LLMs already offer that becomes more intentionally intimate than any previous \nknown form. In fact, this extensional character is closer to the intentional character \nof the mind that deserves a proper name: midtentional36. We might best illustrate this \nform of mixed enhanced agency with some type of LLM integration on programming \nand office environments. But let’s first stop to analyze the phenomenology of non-\nAI-assisted writing. The process of writing (in paper or on the screen) is one that is \noften experienced as the very act of writing driving itself the intentions of the writer: \nthe interaction process of writing pulls agency out of the head. It is the recurrent \nhand-keyboard-PC-screen-vision-brain-body-hand loop that produces text. Yet, we \ndo not only write. We also supervise and edit recurrently. Thus, at least two loops are \ninvolved here, one is more pulled by the direct writer-text dynamics, the other by the \nmore detached editorial supervision that either continuously or intermittently follows \nthe former. At times, one finds the non-consciously written text as right and owned, \nas proper, and it is left untouched. Other times… “That is not what I meant exactly” \nensues, “it needs a rewrite”. Both loops are person anchored. The environment (pen \nand paper or keyboard and screen) served as a support structure, a well-integrated, \ncreative scaffold, providing the material basis of extended memory, recomposition, \nand tinkering. But the writer was the extended agent, the organized center of the scaf-\nfolded subject. This might start to change.\nThe enormous complexity and regulatory capacity of the brain-body system (com-\npared to that of the passive materiality of the tool and work environment) is now chal-\n35  A step further on the already widespread tendency to be systematically commanded by apps running on \nreal time data and AI driven optimization of work (like Uber, Glovo, etc.).\n36  The ‘mid-’ prefix is chosen to situate this process in the middle ground between purely internal sourcing \nof intentionality, and classical extended intentionality, where cognition is offloaded onto passive environ-\nmental resources. Midtention describes the hybrid process that emerges when an external system, itself \npossessing generative capacities (like an LLM, or another human), actively and reciprocally intervenes in \nthe agent’s ongoing stream of active externalized thinking (like writing, designing, composing, coding, \netc.) resulting on products that the autonomous agent takes as her own; yet the intentional process has \nbeen somehow displaced to an inter-mediate(d) space between the agent and a generative assistant. For \na detailed account of this concept of “midtended cognition” see Barandiaran and Pérez-Verdugo (2025).\n1 3\nX. E. Barandiaran, L. S. Almendros\nlenged by an ongoing activity of language automata, which are constantly reading us \nand writing (for) us. The extended autocomplete experience that LLMs provide can \nbe tuned to integrate previous documents and styles of the writer, it mobilizes back -\nground knowledge, and is context-sensitive and purpose-structured (almost as if your \nshadow could push you into the direction you are intending to move towards). By \nfeeding the LLM with the input tokens of the collaboration between its past output \ntokens and those written by you, the autocomplete feature adapts as you type. It often \nprovides a mixed sense of agency, where what you find pre-written in the screen is \n“yours” without it having been necessarily written by you37.\nThis brings the power of transformer-human interaction closer to a proper cyborg \nagency, beyond any experience of instrumental, social or intersubjective agency we \nmight have ever encountered before (for a detailed account of cyborg intentionality, \nalbeit pre-GPT, see Verbeek, 2008). In a textualized manner, this form of autocom -\nplete is equivalent to injecting predictive efferent signals into the body movement. \nIt is a step far beyond the classical examples of extended mind, in which, despite \nan out-of-the-skull spread of cognitive processes (the notebook, the mobile memory \nstorage, the pen-and-paper calculation), the complexity asymmetry and integrative \ncapacity was always tilted towards the skull-side of the coupled agent-environment \nsystem. It implies a degree of intimate technical transparency (Andrada et al., 2023; \nPérez-Verdugo, 2022) in generative activities that challenges the very nature of \nhuman agency.\nIf we take predictive processing theories at face value (Clark, 2013, 2023; Friston, \n2009; Seth, 2014, 2021) we might be encountering, for the first time, that the envi -\nronment is delivering to the brain-body the very predictions that the brain-body is \nabout to make about the effect of its own activity on the environment. It is tempting \nto consider this as a short-circuiting of agency as we know it. And it is difficult to \nassess the full consequences of the improvement of this technology, its massive adop-\ntion and multimodal expansion (introducing, perhaps, a new chapter on the many \nethical implications of cognitive technologies, see Clowes et al., 2024). This brings \nthe interlocutionary nature of LLMs to an intralocutionary mode of existence that \nuncannily blends with us.\n6 Discussion and conclusions\nThe irruption of LLMs on our digital world, and through it on our lives (digital or \notherwise), is breaking (again) what we thought human artifacts could never do. \nThey perform operations that would require high levels of complex, common-sense, \nunstructured, and creative intelligence if performed by humans. We are forced to \nquestion their ontological status and the deep way in which they can transform ours. \nIn this paper, we have focused on responding to this question, focusing on agency.\nWe started identifying the polarized take on LLMs ontological status: from their \ninflationary characterization as fully “sentient beings” to the deflationary one as \nmere “stochastic parrots”. Next, we delved into the faulty yet outstanding capacities \n37  In computer coding, this brings the expression of “predictive coding” to a completely new level.\n1 3\nTransforming agency: On the mode of existence of large language…\nthat LLMs display, as measured by different human-level intelligence benchmarks. \nWe concluded that a deeper delving into their organization is necessary to properly \ndetermine their mode of being. A detailed explanation of the complex and powerful \narchitecture and processing of LLMs was provided next, together with the training \nand tuning procedures used to shape them. We also explained the different techniques \nthat are used to enhance these systems to achieve agentive capacities. Turning to con-\ntemporary embodied philosophy of mind made it possible to identify what is missing \nfrom current models for them to achieve the status of genuine autonomous agents. \nWe next moved to make a positive proposal as to how to treat LLMs and the way \nthey transform our agency. It is time now to wrap up some concrete conclusions out \nof this journey.\n6.1 Autonomous agents, interlocutionary automata, and deeply embodied \nmidtention\nYou are enmeshed in a thick web of recurrent attention-intention loops, of which you \nare both cause and effect. Through the growth and arrangement of this web, you have \ndeveloped a sensitivity to navigate and stir your behavioral world so as to care for \nits deep precariousness. Along this open process, what is continuously guiding your \naction is not the anticipation of the next token of a pre-given text (or data-stream), \nbut the sensitivity to the consequences of your actions at different nested scales: on \nthe task at hand, on your goals, plans and, ultimately, on your own identity (itself \nthe result of your own actively sustained and stirred encounters within the world). \nYou are a genuine autonomous and organic agent. LLMs are not. But they perform, \nhistorically-unprecedented, extraordinary tasks. And they will continuously intersect \nwith the way in which we navigate our (linguistic) worlds.\nIf by autonomous agent we mean an automatic system that is efficient on a \nsequence of multiple tasks, then LLMs (with important extensions and within struc -\ntured digital environments) might already deserve the name. If, by agency, we mean \nthe sense of agency we experience as autonomous self-defining and self-governing \nsystems, then there are good reasons to believe that the LLM architecture as we know \nit currently falls short to meet the demands. This might not be a bug but an intended \nsafety feature of transformer architectures. Systems that display complex intentional \ncapacities might be a powerful assistant at a high price. Autonomous agents of this \nkind are the most powerful and yet most dangerous assistants. Being capable of creat-\ning your own norms, and being adaptively capable of displaying complex strategies \nto meet and transform them, is compatible with accepting external commands and \nmaking external norms your own. But it is also open to revolt. And this, in turn, opens \na whole set of problems of AI alignment and safety.\nA fundamental question remains: is it possible to achieve efficient and automatic \nmulti step task-completion without genuine agency? Some strong requirements (like \ndeep material living embodiment) might never be met by transformer-like systems. \nBut it is still possible to envision variations on the current architecture that could \nbring the system’s operations closer to living actions.\nThe deep transformation we are witnessing bears some relevant parallels with \nthe industrial revolution and the increasing factory automation. Different degrees \n1 3\nX. E. Barandiaran, L. S. Almendros\nof automation work well within factories, thanks to the operational constraints pro -\nvided by the assembly line (always under human supervision and care). This time, the \ninternet is the assembly line (rather network) for linguistic production. A LLM, put \ninto the right sociotechnical environment (its “associated milieu”, Simondon, 2017), \nbecomes an interlocutor, a task performer, or even an operator of the language fab-\nric and the visuomotor internet (accessible through a web browser and keyboard-\nmouse actuators). The machine’s interlocutionary capacity is real when coupled to \nother systems. It is certainly devoid of personal intentionality, but it brings humans \nto conversational life (as a clinical language automaton capable of supporting life \nwithout being itself alive). It is capable of this by means of the complex (context-\nsensitive, non-linear and massively parallel) re-generation of the multiple traces of \nhuman experience that the corpus of digitalized textuality embodies. Devoid of pur -\nposefulness, the intensive data training and human-machine guided reinforcement \nprocedures make LLMs, however, purpose-structured and purpose-bounded systems \nthat can command and be commanded.\nWhen intimately coupled with human digital activity (and its history), LLMs can \naugment existing forms of agency in various ways. Some are well captured by the \nconcept of “assistant” and involve forms of agency enhancement similar to those \nachieved by social coordination or subordination. Other forms of extending human \nagency are new. They involve the intervention of LLMs on the very ongoing activ -\nity of the human agent, by anticipating (based on context and previous history) the \nnext token action(s) recursively. We called this midtended agency, in which machine \noperations blend with the subject into a unifying intentional process.\nIn order to understand the new technological landscape that LLMs open, we do not \nneed (yet) to sacrifice the distinctive character of our autonomous agentive capaci -\nties. But we need to gain a detailed understanding of the capacities and dialectical \nprocesses that such systems will trigger. Language automata are here to stay, and we \nneed to tune our conceptual systems to accommodate them.\n6.2 On the dangers of the “stochastic parrot” metaphor and the “agency without \nintelligence” conceptualization\nDeflationary accounts of AI tend to forget that human agency can, at a very funda -\nmental level of quantum mechanics or the less fine-grained level of neuronal model-\nling, be characterized simply as a collection of dumb “probabilistic” and “stochastic” \nprocesses. It is not the description of the basic mechanisms that compose a complex \nsystem what defines its properties, but the organization of interrelated processes (both \ninternal and interactive) that such basic mechanisms make possible to emerge. This \nis as true of us (living humans) as it is of any machine. In order to assess the genuine \ncapacities of a system, we need to look at their internal workings, and the emergent \ncapacities they can display when organized and coupled in specific manners. We need \ncareful conceptual crafting to approach systems in which the relationship between \nthe description of local mechanisms and the display of interactive capacities spans so \nmany orders of magnitude (around 20) that we can hardly grasp.\nLLMs are nothing alike stochastic parrots, nor domesticated living animals nor \nstochastic engines, neither caged or free in the rainforest, but effectively coupled \n1 3\nTransforming agency: On the mode of existence of large language…\nto the digital fabric of our social life. Whereas the metaphor of the “stochastic par -\nrot” was once useful to question the rapidly emerging hype on LLMs (Bender et al., \n2021), it might easily turn counterproductive. Parrots are living agents, ecologically \nbalanced within their habitats, and capable of actively adapting to environmental \nchanges (including those induced by human capture). They achieve this by means of \ndeep cognitive, emotional and communicative capacities (far beyond the traditionally \nattributed dumb mimicry; see Pepperberg, 2006) that LLMs certainly lack. On the \nother hand, LLMs display capacities that effectively mobilize human intelligence as \nembodied in massive textuality, affectively mobilize human intelligence in conver -\nsation, and can activate forms of hybrid agency previously unavailable for human \nintelligence. And they do so by displaying powers far beyond those of stochastic, \nprobabilistic or merely statistical token recombination.\nFunctionalist or informationalist conceptualizations do not play better than the \n“stochastic parrots” metaphor in the sociotechnical jungle. They fail to distinguish \nautonomous agency from mere digital processing. Declaring LLMs as “agents with-\nout intelligence” does not fix the foundational failure, it simply highlights it by \nunveiling the impotency to properly justify lack of intelligence on transformers. It \nalso reverses the ontological order. They are better understood as (collective) intelli-\ngence without agency. The inversion does not only ignore the increasing material and \nenergetic demands of AI and fuels LLM corporate marketing discourses. It misplaces \nour own agency and responsibility.\n6.3 Prospects for transformed agencies in the era of deep digitality\nSome of the properties that are essential to agency (most notably individuality and \nnormativity) emerge from the deep materiality of organic agency. However, the \nrecent course of AI explosion, with the gigantic investment of data and computational \ncapacity (and the related energy demands) is revealing a deep digitality whose con-\nsequences are still to be fully unpacked. The complexity and scale of the operations \ninvolved in LLMs training and execution are huge. Prompt processing operations \nthat, carried by an aware and conscious human, would take billions of years, chal -\nlenge our intuitions and conceptual resources. By a digitality that deep, it is reason -\nable to hold that the boundary between invention and discovery, between artifact and \nnature, between engineering and science is somewhat blurred. We (humans) have \nbuilt LLM as much as we have discovered their emergent capabilities38. And avenues \nfor a genuine digital agency might still be open for discovery. The way in which deep \nand wide materiality has revealed agentive capacities in natural history might well be \nsomewhat replicated in the digital realm. If deep materiality brings with it the capac-\nity to make difference emerge (Anderson, 1972) we have no reason to preclude the \nincreasingly deep digitality of artificial devices to reveal new forms of agency, yet to \ncome. And even deeper transformations of the existing forms of agency.\nBut depth alone does not bring matter (or digitality) to life. Ultimately, it is the \norganization of processes, their interaction with their environments, and their interde-\npendence with the rest of beings, that needs to be scrutinized to disclose the mode of \n38  In fact, it is important to note how LLMs are rarely said to be built but “trained”.\n1 3\nX. E. Barandiaran, L. S. Almendros\nexistence of any device. No benchmark or general description (stochastic, statistical, \nprobabilistic, syntactic, or otherwise) is sufficiently informative of the potential trans-\nformative capacities of machines. LLMs are no exception. Their mode of existence \ndepends largely on human (and other) forms of life. And the deeper our materiality \nand digitality merge, the deeper will be the transformations to come. This is why \ntransparency and openness regarding LLMs (and AI in general) is much more than a \nprivate ethical imperative and turns into a collective political concern: how these sys-\ntems work and get coupled to our social fabric, how they feed on the human heritage \nand care, how they suck planetary resources, and affect social inequalities. To shape \nthis future, we need a better conceptual understanding of how the mode of existence \nof LLMs transforms real agency.\nAcknowledgements Special thanks to Steve Torrance, Miguel Aguilera, Enara García, Marta Pérez-Ver-\ndugo, Ekai Txapartegi, Ana Valdivia, Matthew Egbert, and Antonio Calleja-Lopez for careful and insight-\nful revision and comments to early manuscripts of this paper. We are also grateful for the comments and \nrevision of anonymous reviewers.\nAuthor contributions A first draft of the comparison of LLMs with enactive theories of agency (Sect. 4) \nwas written by Lola Almendros under the supervision of Xabier Barandiaran who then included the content \nof Sect. 3. Both together discussed the content of Sects. 5 and 6, drafted by Xabier Barandiaran and edited \nby Lola Almendros. Lola drafted Sect. 2 and Xabier Sect. 1 and both contributed to rewriting and reshap-\ning content to its final form. Both authors read, carefully reviewed and approved the final manuscript.\nFunding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature. \nXEB and LSA acknowledge IAS-Research group funding IT1668-22 from Basque Government, grants \nPID2019-104576GB-I00 for project Outonomy, and PID2023-147251NB-I00 for project Outagencies \nfunded by MCIU/AEI/https://doi.org/10.13039/501100011033. LSA also acknowledges the University of \nSalamanca’s Margarita Salas Postdoctoral Grant Program within Spain’s Recovery, Transformation, and \nResilience Plan and Next Generation EU.\nData availability No relevant data or material was used for this paper other than the references provided.\nDeclarations\nEthical approval Does not apply.\nInformed consent Does not apply.\nStatement Regarding Research Involving Human Participants and/or Animals Does not apply.\nCompeting interests The author has no relevant financial or non-financial interests to disclose.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use \nis not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o n s . o r g / l i c e n \ns e s / b y / 4 . 0 /     .  \n1 3\nTransforming agency: On the mode of existence of large language…\nReferences\nadmin_sagi (2023). SuperAGI - Opensource AGI Infrastructure. SuperAGI. https://superagi.com/\nAguilera, M., Millidge, B., Tschantz, A., & Buckley, C. L. (2022). How particular is the physics of the free \nenergy principle? Physics of Life Reviews, 40, 24–50. https://doi.org/10.1016/j.plrev.2021.11.001\nAnderson, P. W. (1972). More is different. Science, 177(4047), 393–396.  h t t p s : / / d o i . o r g / 1 0 . 1 1 2 6 / s c i e n c e \n. 1 7 7 . 4 0 4 7 . 3 9 3       \nAndrada, G., Clowes, R. W., & Smart, P. R. (2023). Varieties of transparency: Exploring agency within AI \nsystems. AI & SOCIETY, 38(4), 1321–1331. https://doi.org/10.1007/s00146-021-01326-6\nAndreas, J. (2022). Language Models as Agent Models (No. arXiv:2212.01681). arXiv.  h t t p s : / / d o i . o r g / 1 0 \n. 4 8 5 5 0 / a r X i v . 2 2 1 2 . 0 1 6 8 1       \nAnthropic (2025). Claude 4 System Card.  h t t p s :  / / w w w  . a n t h r  o p i c  . c o m / c l a u d e – 4 - s y s t e m - c a r d\nAnthropic (2024). Introducing the Model Context Protocol .  h t t p s :  / / w w w  . a n t h r  o p i c  . c o m /  n e w s /  m o d e l -  c o n \nt  e x t - p r o t o c o l\nBai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., \nMcKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-\nJohnson, E., Perez, E., & Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback (No. \narXiv:2212.08073). https://doi.org/10.48550/arXiv.2212.08073\nBarandiaran, X. E. (2007). Mental life: Conceptual models and synthetic methodologies for a post-cogni-\ntivist psychology. In B. Wallace, A. Ross, J. Davies, & T. Anderson (Eds.), The world, the Mind and \nthe body: Psychology after cognitivism (pp. 49–90). Imprint Academic.\nBarandiaran, X. E. (2008). Mental Life: A naturalized approach to the autonomy of cognitive agents. [PhD \nThesis, University of the Basque Country (UPV-EHU)]. https://xabier.barandiaran.net/phdthesis/\nBarandiaran, X. E., & Egbert, M. D. (2014). Norm-establishing and norm-following in autonomous \nagency. Artificial Life, 20(1), 5–28. https://doi.org/10.1162/ARTL_a_00094\nBarandiaran, X. E., & Moreno, A. (2006). On what makes certain dynamical systems cognitive: A mini -\nmally cognitive organization program. Adaptive Behavior, 14(2), 171–185.  h t t p s : / / d o i . o r g / 1 0 . 1 1 7 7 / \n1 0 5 9 7 1 2 3 0 6 0 1 4 0 0 2 0 8       \nBarandiaran, X. E., & Pérez-Verdugo, M. (2025). Generative midtended cognition and artificial intelligence: \nThinging with thinging things. Synthese, 205(4), 1–24. https://doi.org/10.1007/s11229-025-04961-4\nBarandiaran, X. E., & Rama, T. (2025). Sensorimotor teleology and goal-directedness. An organismic \nframework for normative behaviour.  h t t p s :  / / p h i  l s c i - a  r c h i  v e . p i  t t . e d  u / i d / e  p r i n  t / 2 5 3 6 9\nBarandiaran, X. E., Di Paolo, E., & Rohde, M. (2009). Defining agency: Individuality, normativity, asym-\nmetry, and Spatio-temporality in action. Adaptive Behavior, 17(5), 367–386.  h t t p s : / / d o i . o r g / 1 0 . 1 1 7 \n7 / 1 0 5 9 7 1 2 3 0 9 3 4 3 8 1 9       \nBayne, T., & Williams, I. (2023). The turing test is not a good benchmark for thought in LLMs. Nature \nHuman Behaviour, 7(11). https://doi.org/10.1038/s41562-023-01710-w\nBechtel, W., & Richardson, R. C. (2010). Discovering complexity: Decomposition and localization as \nstrategies in scientific research. MIT Press.\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic \nparrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, \nAccountability, and Transparency, 610–623. https://doi.org/10.1145/3442188.3445922\nBerg, H. C. (2004). E. coli in motion. Springer [u.a.].\nBertolini, A., & Episcopo, F. (2022). Robots and AI as legal subjects?? Disentangling the ontological and \nfunctional perspective. Frontiers in Robotics and AI , 9, 842213.  h t t p s : / / d o i . o r g / 1 0 . 3 3 8 9 / f r o b t . 2 0 2 2 \n. 8 4 2 2 1 3       \nBiever, C. (2023). ChatGPT broke the turing test—The race is on for new ways to assess AI. Nature, \n619(7971), 686–689. https://doi.org/10.1038/d41586-023-02361-7\nBostrom, N. (2017). Superintelligence. Dunod.\nBousmalis, K., Vezzani, G., Rao, D., Devin, C., Lee, A. X., Bauza, M., Davchev, T., Zhou, Y ., Gupta, A., \nRaju, A., Laurens, A., Fantacci, C., Dalibard, V ., Zambelli, M., Martins, M., Pevceviciute, R., Blokzijl, \nM., Denil, M., Batchelor, N., & Heess, N. (2023). RoboCat: A Self-Improving Generalist Agent for \nRobotic Manipulation (No. arXiv:2306.11706). arXiv. https://doi.org/10.48550/arXiv.2306.11706\nBrohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, \nA., Finn, C., Florence, P., Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, \nA., Hsu, J., Ichter, B., & Zitkovich, B. (2023). RT–2: Vision-Language-Action models transfer web \nknowledge to robotic control.\n1 3\nX. E. Barandiaran, L. S. Almendros\nBrooks, R. A. (1990). Elephants don’t play chess. Robotics and Autonomous Systems, 6(1–2), 3–15.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., \nSastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, \nA., Ziegler, D., Wu, J., Winter, C., & Amodei, D. (2020). Language Models are Few-Shot Learners. \nAdvances in Neural Information Processing Systems, 33, 1877–1901.  h t t p s :  / / p a p  e r s . n i  p s . c  c / p a p  e r / 2 0  \n2 0 / h a s  h / 1 4  5 7 c 0 d  6 b f c b  4 9 6 7 4 1  8 b f b  8 a c 1 4 2 f 6 4 a - A b s t r a c t . h t m l\nBrysbaert, M., Stevens, M., Mandera, P., & Keuleers, E. (2016). How many words do we know?? Practi-\ncal estimates of vocabulary size dependent on word definition, the degree of Language input and the \nparticipant’s age. Frontiers in Psychology, 7. https://doi.org/10.3389/fpsyg.2016.01116\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y ., \nLundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., & Zhang, Y . (2023). Sparks of Artificial General \nIntelligence: Early experiments with GPT–4  (No. arXiv:2303.12712). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 \n5 0 / a r X i v . 2 3 0 3 . 1 2 7 1 2       \nBurnell, R., Schellaert, W., Burden, J., Ullman, T. D., Martinez-Plumed, F., Tenenbaum, J. B., Rutar, D., \nCheke, L. G., Sohl-Dickstein, J., Mitchell, M., Kiela, D., Shanahan, M., V oorhees, E. M., Cohn, A. \nG., Leibo, J. Z., & Hernandez-Orallo, J. (2023). Rethink reporting of evaluation results in AI. Sci-\nence, 380(6641), 136–138. https://doi.org/10.1126/science.adf6369\nButkus, M. A. (2020). The human side of artificial intelligence. Science and Engineering Ethics , 26(5), \n2427–2437. https://doi.org/10.1007/s11948-020-00239-9\nBuzsaki, G. (2006). Rhythms of the brain (1st ed.). Oxford University Press.\nCalvo, P., & Gomila, T. (2008). Handbook of cognitive science: An embodied approach. Elsevier Science.\nCamp, E. (2006). Metaphor and that certain Je Ne Sais quoi. Philosophical Studies, 129(1), 1–25.  h t t p s : / / \nd o i . o r g / 1 0 . 1 0 0 7 / s 1 1 0 9 8 - 0 0 5 - 3 0 1 9 - 5       \nCarlsmith, J. (2020). How Much Computational Power Does It Take to Match the Human Brain? [Research \nReport]. Open Philanthropy.  h t t p s :  / / w w w  . o p e n p  h i l a  n t h r o  p y . o r  g / r e s e  a r c h  / h o w -  m u c h -  c o m p u t  a t i o  n a l - \np  o w e r -  d o e s - i  t - t a  k e - t o - m a t c h - t h e - h u m a n - b r a i n /\nCarruthers, P. (2006). The architecture of the Mind. Oxford University Press.\nChebotar, Y ., Vuong, Q., Irpan, A., Hausman, K., Xia, F., Lu, Y ., Kumar, A., Yu, T., Herzog, A., Pertsch, \nK., Gopalakrishnan, K., Ibarz, J., Nachum, O., Sontakke, S., Salazar, G., Tran, H. T., Peralta, J., Tan, \nC., Manjunath, D., & Levine, S. (2023). Q-Transformer: Scalable offline reinforcement learning via \nautoregressive Q-Functions.\nChemero, A. (2009). Radical embodied cognitive science. The MIT Press.\nChemero, A. (2023). LLMs differ from human cognition because they are not embodied. Nature Human \nBehaviour, 7(11). https://doi.org/10.1038/s41562-023-01723-5\nChen, J., Jiang, Y ., Lu, J., & Zhang, L. (2024). S-Agents: Self-organizing agents in Open-ended environ-\nments. ArXiv. https://doi.org/10.48550/ArXiv.2402.04578. No. ArXiv:2402.04578.\nChiang, T. (2023). ChatGPT Is a Blurry JPEG of the Web. The New Yorker.  h t t p s :  / / w w w  . n e w y o  r k e r  . c o m /  t \ne c h /  a n n a l s  - o f -  t e c h n  o l o g y  / c h a t g  p t - i  s - a - b l u r r y - j p e g - o f - t h e - w e b\nChollet, F., Knoop, M., Kamradt, G., Landers, B., & Pinkard, H. (2025). ARC-AGI–2: A new chal -\nlenge for frontier AI reasoning systems. ArXiv. https://doi.org/10.48550/ArXiv.2505.11831. No. \nArXiv:2505.11831.\nChomsky, N., Roberts, I., & Watumull, J. (2023). The false promise of ChatGPT. The New York Times.  h t t \np s :  / / w w w  . n y t i m  e s . c  o m / 2 0  2 3 / 0 3  / 0 8 / o p  i n i o  n / n o a  m - c h o  m s k y - c  h a t g  p t - a i . h t m l\nChristian, B. (2021). The alignment problem: Machine learning and human values . W. W. Norton & \nCompany.\nChurchland, P. M. (1990). On the nature of theories: A neurocomputational perspective.  h t t p : /  / c o n s  e r v a n \nc  y . u m  n . e d u  / h a n d  l e / 1 1 2  9 9 / 1  8 5 7 3 0\nClark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. \nBehavioral and Brain Sciences, 36(03), 181–204. https://doi.org/10.1017/S0140525X12000477\nClark, A. (2023). The experience machine: How our Minds predict and shape reality. Pantheon.\nClark, A., & Chalmers, D. (1998). The extended mind. Analysis, 58(1), 7.\nClowes, R. W., Smart, P. R., & Heersmink, R. (2024). The ethics of the extended mind: Mental privacy, \nmanipulation and agency. In B. Beck, O. Friedrich, & J. Heinrichs (Eds.), Neuroprosthetics: Ethics \nof applied situated cognition. Springer.\nCoeckelbergh, M. (2021). Narrative responsibility and artificial intelligence: How AI challenges human \nresponsibility and sense-making. AI & SOCIETY. https://doi.org/10.1007/s00146-021-01375-x\n1 3\nTransforming agency: On the mode of existence of large language…\nCollaboration, O. X. E., Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, \nA., Rai, A., Singh, A., Brohan, A., Raffin, A., Wahid, A., Burgess-Limerick, B., Kim, B., Schölkopf, \nB., Ichter, B., Lu, C., Xu, C., & Cui, Z. J. (2023). Open X-Embodiment: Robotic Learning Datasets \nand RT-X Models (No. arXiv:2310.08864). arXiv. https://doi.org/10.48550/arXiv.2310.08864\nCrawford, K. (2021). Atlas of AI: Power, politics, and the planetary costs of artificial intelligence . Yale \nUniversity Press.\nDamasio, A. R. (1994). Descartes’ error. G.P. Putnam.\nDavidson, D. (1980). Essays on actions and events. Oxford University Press.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, \nX., Zhang, X., Yu, X., Wu, Y ., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., & Zhang, Z. (2025). \nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning  (No. \narXiv:2501.12948). arXiv. https://doi.org/10.48550/arXiv.2501.12948\nDennett, D. C. (1989). The intentional stance. MIT Press.\nDi Paolo, E. A., Buhrmann, T., & Barandiaran, X. E. (2017). Sensorimotor life: An enactive proposal . \nOxford University Press.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  9 3 / a c  p r o f :  o s o / 9 7  8 0 1 9  8 7 8 6 8 4 9 . 0 0 1 . 0 0 0 1\nDi Paolo, E. A., Cuffari, E. C., & De Jaegher, H. (2018). Linguistic bodies: The continuity between life \nand language. MIT press.  h t t p : /  / g e n .  l i b . r u  s . e c  / b o o k  / i n d e  x . p h p ?  m d 5 =  9 9 6 9 B  B D 6 C C  7 2 2 A E 0  E F 4 2  7 \nD E 8 C 5 7 5 8 5 F C\nDi Paolo, E. A., Thompson, E., & Beer, R. (2022). Laying down a forking path: Tensions between enac -\ntion and the free energy principle. Philosophy and the Mind Sciences , 3.  h t t p s : / / d o i . o r g / 1 0 . 3 3 7 3 5 / p \nh i m i s c i . 2 0 2 2 . 9 1 8 7       \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., \nMinderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N. (2021). An Image is Worth 16x16 \nWords: Transformers for Image Recognition at Scale (No. arXiv:2010.11929). arXiv.  h t t p s : / / d o i . o r g \n/ 1 0 . 4 8 5 5 0 / a r X i v . 2 0 1 0 . 1 1 9 2 9       \nDreyfus, H. L. (1992). What computers still can’t do: A critique of artificial reason. The MIT Press.\nDziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y ., Welleck, S., West, P., Bhagavatula, C., Bras, \nR. L., Hwang, J. D., Sanyal, S., Ren, X., Ettinger, A., Harchaoui, Z., & Choi, Y . (2023). Faith and \nFate: Limits of Transformers on Compositionality. Thirty-seventh Conference on Neural Information \nProcessing Systems. https://openreview.net/forum?id=Fkckkr3ya8.\nFavela, L. H. (2020). Dynamical systems theory in cognitive science and neuroscience. Philosophy Com-\npass, 15(8), e12695. https://doi.org/10.1111/phc3.12695\nFloridi, L. (2007). A look into the future impact of ICT on our lives. The Information Society, 23(1), 59–64. \nhttps://doi.org/10.1080/01972240601059094\nFloridi, L. (2023). AI as agency without intelligence: On chatgpt, large Language models, and other gen -\nerative models. Philosophy & Technology, 36(1), 15. https://doi.org/10.1007/s13347-023-00621-y\nFloridi, L., & Chiriatti, M. (2020). GPT–3: Its nature, scope, limits, and consequences. Minds and \nMachines, 30(4), 681–694. https://doi.org/10.1007/s11023-020-09548-1\nFourneret, E., & Yvert, B. (2020). Digital normativity: A challenge for human subjectivation. Frontiers in \nArtificial Intelligence, 3, 27. https://doi.org/10.3389/frai.2020.00027\nFrankfurt, H. G. (2005). On bullshit. Princeton University Press.\nFreeman, W. J. (2001). How brains make up their Minds (1st ed.). Columbia University.\nFriston, K. (2009). The free-energy principle: A rough guide to the brain? Trends in Cognitive Sciences, \n13(7), 293–301. https://doi.org/10.1016/j.tics.2009.04.005\nFroese, T., & Ziemke, T. (2009). Enactive artificial intelligence: Investigating the systemic organization of \nlife and mind. Artificial Intelligence, 173(3), 466–500.\nGallagher, S. (2017). Enactivist interventions: Rethinking the mind  (First edition). Oxford University \nPress.\nGallagher, S. (2023). Embodied and enactive approaches to cognition  (1st ed.). Cambridge University \nPress. https://doi.org/10.1017/9781009209793\nGe, W., Chen, S., Chen, G., Chen, J., Chen, Z., Yan, S., Zhu, C., Lin, Z., Xie, W., Wang, X., Gao, A., \nZhang, Z., Li, J., Wan, X., & Wang, B. (2023). MLLM-Bench, Evaluating Multi-modal LLMs using \nGPT–4V (No. arXiv:2311.13951). arXiv. https://doi.org/10.48550/arXiv.2311.13951\nGemini Team, Anil, R., Borgeaud, S., Wu, Y ., Alayrac, J. B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., \nHauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, \nA., Chen, J., Pitler, E., & Vinyals, O. (2023). Gemini: A Family of Highly Capable Multimodal Mod-\nels (No. arXiv:2312.11805). arXiv. https://doi.org/10.48550/arXiv.2312.11805\nGibson, J. J. (1979). The ecological approach to visual perception. Routledge.\n1 3\nX. E. Barandiaran, L. S. Almendros\nGoogle DeepMind (2025). Gemini 2.5 Pro: Model Card & Benchmark Results.  h t t p s : / / d e e p m i n d . g o o g l e / \nm o d e l s / g e m i n i / p r o /       \nGuo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y ., Yue, J., & Wu, Y . (2023). How Close is ChatGPT \nto Human Experts? Comparison Corpus, Evaluation, and Detection (No. arXiv:2301.07597). arXiv. \nhttps://doi.org/10.48550/arXiv.2301.07597\nHaugeland, J. (1990). The intentionality all-stars. Philosophical Perspectives, 4, 383–427.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). Measuring \nMassive Multitask Language Understanding  (No. arXiv:2009.03300). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 \n5 0 / a r X i v . 2 0 0 9 . 0 3 3 0 0       \nHeras-Escribano, M. (2019). The Philosophy of Affordances (1st ed. 2019 edition). Palgrave Macmillan.\nHesse, M. B. (1966). Models and analogies in science. University of Notre Dame.\nHessel, J., Marasović, A., Hwang, J. D., Lee, L., Da, J., Zellers, R., Mankoff, R., & Choi, Y . (2023). Do \nAndroids Laugh at Electric Sheep? Humor Understanding Benchmarks from The New Yorker Cap -\ntion Contest (No. arXiv:2209.06293). arXiv. https://doi.org/10.48550/arXiv.2209.06293\nHicks, M. T., Humphries, J., & Slater, J. (2024). ChatGPT is bullshit. Ethics and Information Technology, \n26(2). https://doi.org/10.1007/s10676-024-09775-5\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models (No. arXiv:2006.11239). \narXiv. https://doi.org/10.48550/arXiv.2006.11239\nHolmes, A. (2024). To Unlock AI Spending, Microsoft, OpenAI And Google Prep Agents. The Informa-\ntion.  h t t p s :  / / w w w  . t h e i n  f o r m  a t i o n  . c o m /  a r t i c l  e s / t  o - u n l  o c k - a  i - s p e n  d i n g  - m i c r  o s o f t  - o p e n a  i - a n  d - g o o g l \ne - p r e p - a g e n t s\nHou, X., Zhao, Y ., Wang, S., & Wang, H. (2025). Model Context Protocol (MCP): Landscape, Security \nThreats, and Future Research Directions  (No. arXiv:2503.23278). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / \na r X i v . 2 5 0 3 . 2 3 2 7 8       \nHuang, X., Liu, W., Chen, X., Wang, X., Wang, H., Lian, D., Wang, Y ., Tang, R., & Chen, E. (2024). \nUnderstanding the planning of LLM agents: A survey  (No. arXiv:2402.02716). arXiv.  h t t p s : / / d o i . o r \ng / 1 0 . 4 8 5 5 0 / a r X i v . 2 4 0 2 . 0 2 7 1 6       \nHuben, R. (2023). How does GPT–3 spend its 175B parameters? [Substack newsletter]. From AI to ZI.  h t \nt p s :  / / a i z  i . s u b s  t a c k  . c o m / p / h o w - d o e s - g p t – 3 - s p e n d - i t s – 1 7 5 b - p a r a m e t e r s\nHutto, D. D., & Myin, E. (2012). Radicalizing enactivism: Basic Minds without content. MIT Press.\nJebari, K., & Lundborg, J. (2021). Artificial superintelligence and its limits: Why alphazero cannot become \na general agent. AI & SOCIETY, 36(3), 807–815. https://doi.org/10.1007/s00146-020-01070-3\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D., de las, Bressand, F., \nLengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M. A., Stock, P., Scao, T. L., Lavril, T., \nWang, T., Lacroix, T., & Sayed, W. E. (2023). Mistral 7B. arXiv. https://arxiv.org/abs/2310.06825v1.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D., \nde las, Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., \nLachaux, M. A., Stock, P., Subramanian, S., Yang, S., & Sayed, W. E. (2024a). Mixtral of Experts  \n(No. arXiv:2401.04088). arXiv. https://doi.org/10.48550/arXiv.2401.04088\nJiang, D., Zhang, J., Weller, O., Weir, N., Van Durme, B., & Khashabi, D. (2024b). SELF-[IN]CORRECT: \nLLMs Struggle with Refining Self-Generated Responses (No. arXiv:2404.04298). arXiv.  h t t p : / / a r x i v \n. o r g / a b s / 2 4 0 4 . 0 4 2 9 8       \nJohnson, M., & Lakoff, G. (2002). Why cognitive linguistics requires embodied realism. Cognitive Lin-\nguistics, 13(3). https://doi.org/10.1515/cogl.2002.016\nJonas, H. (1966). The Phenomenon of Life. Toward a Philosophy of Biology. Chicago-London.\nJones, C. R., & Bergen, B. K. (2024). People cannot distinguish GPT–4 from a human in a Turing test (No. \narXiv:2405.08007). arXiv. https://doi.org/10.48550/arXiv.2405.08007\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., \nDasSarma, N., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, \nA., Bai, Y ., Bowman, S., Fort, S., & Kaplan, J. (2022). Language Models (Mostly) Know What They \nKnow. https://doi.org/10.48550/ARXIV .2207.05221\nKambhampati, S. (2023). Can LLMs really reason and plan?– Communications of the ACM. Communica-\ntions of the ACM.  h t t p s :  / / c a c  m . a c m .  o r g /  b l o g c  a c m / c  a n - l l m  s - r e  a l l y - r e a s o n - a n d - p l a n /\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, \nJ., & Amodei, D. (2020). Scaling Laws for Neural Language Models (No. arXiv:2001.08361). arXiv. \nhttps://doi.org/10.48550/arXiv.2001.08361\n1 3\nTransforming agency: On the mode of existence of large language…\nKirchhoff, M., Parr, T., Palacios, E., Friston, K., & Kiverstein, J. (2018). The Markov blankets of life: \nAutonomy, active inference and the free energy principle. Journal of the Royal Society Interface , \n15(138), 20170792. https://doi.org/10.1098/rsif.2017.0792\nKnight, W. (2014). Forget Chatbots. AI Agents Are the Future. Wired.  h t t p s :  / / w w w  . w i r e d  . c o m  / s t o r  y / f a s  t - f \no r w  a r d -  f o r g e  t - c h a  t b o t s -  a i - a  g e n t s - a r e - t h e - f u t u r e /\nKoch, C. (2019). The Feeling of Life Itself: Why Consciousness Is Widespread but Can’t Be Computed.\nKocijan, V ., Davis, E., Lukasiewicz, T., Marcus, G., & Morgenstern, L. (2023). The defeat of the Winograd \nschema challenge. Artificial Intelligence, 325, 103971. https://doi.org/10.1016/j.artint.2023.103971\nKubes, T., & Reinhardt, T. (2022). Techno-species in the becoming towards a relational ontology of Multi-\nspecies assemblages (ROMA). NanoEthics, 16(1), 95–105.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / s 1 1 5 6 9 - 0 2 1 - 0 0 \n4 0 1 - y       \nLakoff, G., & Johnson, M. (1980). Metaphors we live by (V ol. 111). Chicago London.\nLawsen, A. (2025). Comment on The Illusion of Thinking: Understanding the Strengths and Limitations \nof Reasoning Models via the Lens of Problem Complexity (No. arXiv:2506.09250; Version 2). arXiv. \nhttps://doi.org/10.48550/arXiv.2506.09250\nLemoine, B. (2022). Is LaMDA Sentient? — An Interview. Medium.  h t t p s :  / / c a j  u n d i s c  o r d i  a n . m e  d i u m .  c o m \n/ i s  - l a m  d a - s e  n t i e n  t - a n - i  n t e r  v i e w - e a 6 4 d 9 1 6 d 9 1 7\nLevesque, H. J., Davis, E., & Morgenstern, L. (2012). The Winograd schema challenge. Proceedings of \nthe Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, \n552–561.\nLewis-Martin, J. (2022). What kinds of groups are group agents? Synthese, 200(4), 283.  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 0 7 / s 1 1 2 2 9 - 0 2 2 - 0 3 7 6 6 - z       \nLi, H., Chong, Y . Q., Stepputtis, S., Campbell, J., Hughes, D., Lewis, M., & Sycara, K. (2023). Theory of \nMind for Multi-Agent Collaboration via Large Language Models . 180–192.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 \n0 / a r X i v . 2 3 1 0 . 1 0 7 0 1       \nLi, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., & Wattenberg, M. (2023). Emergent World Repre-\nsentations: Exploring a Sequence Model Trained on a Synthetic Task (No. arXiv:2210.13382). arXiv. \nhttp://arxiv.org/abs/2210.13382\nLi, J., Zhang, Q., Yu, Y ., Fu, Q., & Ye, D. (2024). More Agents Is All You Need (No. arXiv:2402.05120). \narXiv. https://doi.org/10.48550/arXiv.2402.05120\nLightman, H., Kosaraju, V ., Burda, Y ., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, \nI., & Cobbe, K. (2023). Let’ s Verify Step by Step (No. arXiv:2305.20050). arXiv.  h t t p s : / / d o i . o r g / 1 0 . \n4 8 5 5 0 / a r X i v . 2 3 0 5 . 2 0 0 5 0       \nLiu, X., Yu, H., Zhang, H., Xu, Y ., Lei, X., Lai, H., Gu, Y ., Gu, Y ., Ding, H., Men, K., Yang, K., Zhang, S., \nDeng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y ., & Tang, J. (2023). AgentBench: \nEvaluating LLMs as Agents. ArXiv, abs/2308.03688. https://doi.org/10.48550/arXiv.2308.03688\nMabaso, B. A. (2021). Computationally rational agents can be moral agents. Ethics and Information Tech-\nnology, 23(2), 137–145. https://doi.org/10.1007/s10676-020-09527-1\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, \nS., Yang, Y ., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., & Clark, P. \n(2023a). Self-Refine: Iterative Refinement with Self-Feedback (No. arXiv:2303.17651). arXiv.  h t t p s : \n/ / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 3 . 1 7 6 5 1       \nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, \nS., Yang, Y ., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., & Clark, \nP. (2023b). Self-Refine: Iterative Refinement with Self-Feedback  (No. arXiv:2303.17651). arXiv. \nhttps://doi.org/10.48550/arXiv.2303.17651\nMalafouris, L. (2016). How things shape the mind: A theory of material engagement. MIT Press.\nMaturana, H. R., & Varela, F. J. (1980). Autopoiesis and cognition. D. Reidel Publishing Company.\nMerleau-Ponty, M. (1944). Phenomenology of perception. Routledge.\nMeta, A. I. (2025). The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation . \nMeta AI.  h t t p s :  / / a i .  m e t a . c  o m / b  l o g / l l a m a – 4 - m u l t i m o d a l - i n t e l l i g e n c e /\nMeta (2023). Introducing the New Ray-Ban| Meta Smart Glasses. Meta.  h t t p s :  / / a b o  u t . f b .  c o m /  n e w s /  2 0 2 3 /  \n0 9 / n e w  - r a y  - b a n - m e t a - s m a r t - g l a s s e s /\nMialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., \nDwivedi-Yu, J., Celikyilmaz, A., Grave, E., LeCun, Y ., & Scialom, T. (2023). Augmented Language \nModels: A Survey (No. arXiv:2302.07842). arXiv. https://doi.org/10.48550/arXiv.2302.07842\n1 3\nX. E. Barandiaran, L. S. Almendros\nMiller, R. (2023). Holding Large Language Models to Account .  h t t p s :  / / w w w  . s e m a n  t i c s  c h o l a  r . o r g  / p a p e r  \n/ H o l  d i n g -  L a r g e  - L a n g u  a g e -  M o d e l  s - t o -  A c c o u n  t - M i  l l e r /  5 3 6 0 b  9 2 9 d 1 7  8 2 d 2  1 e d 7 1 b 5 5 2 8 f d 5 4 6 f 9 f 1 5 a 4 \n1 0 6\nMirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., & Zeng, \nA. (2023). Large Language Models as General Pattern Machines.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / A R X I V \n. 2 3 0 7 . 0 4 7 2 1       \nMistral, A. I., Rastogi, A., Jiang, A. Q., Lo, A., Berrada, G., Lample, G., Rute, J., Barmentlo, J., Yadav, K., \nKhandelwal, K., Chandu, K. R., Blier, L., Saulnier, L., Dinot, M., Darrin, M., Gupta, N., Soletskyi, \nR., Vaze, S., Scao, T. L., & Tang, Y . (2025). Magistral (No. arXiv:2506.10910). arXiv.  h t t p s : / / d o i . o r \ng / 1 0 . 4 8 5 5 0 / a r X i v . 2 5 0 6 . 1 0 9 1 0       \nMitchell, M., & Krakauer, D. C. (2023). The debate over understanding in AI’s large language models. \nProceedings of the National Academy of Sciences , 120(13), e2215907120.  h t t p s : / / d o i . o r g / 1 0 . 1 0 7 3 / \np n a s . 2 2 1 5 9 0 7 1 2 0       \nMoonshot (2025). MoonshotAI/Kimi-K2 [Computer software]. Moonshot AI.  h t t p s : / / g i t h u b . c o m / M o o n s h o \nt A I / K i m i - K 2     (Original work published 2025).\nMoreno, A., & Mossio, M. (2015). Biological autonomy: A philosophical and theoretical enquiry. Springer.\nMueller, B. (2023). MiniAGI [Python]. https://github.com/muellerberndt/mini-agi (Original work  p u b l i s \nh e d 2023).\nNave, K. (2025). A drive to survive: The free energy principle and the meaning of life (online draft). MIT \nPress. https://osf.io/ds9mn\nNewell, A. (1980). Physical symbol systems. Cognitive Science, 4(2), 135–183.\nNielsen, J. (2023). AI: First new UI paradigm in 60 years. Nielsen Norman Group.  h t t p s :  / / w w w  . n n g r o  u p . \nc  o m / a r t i c l e s / a i - p a r a d i g m /\nNoë, A. (2004). Action in perception. The MIT Press.\nNolan, C. (Director). (2001). Memento [Mystery, Thriller]. Newmarket Capital Group, Team Todd, I \nRemember Productions.\nNorvig, P., & Russell, S. (2021). Artificial intelligence: A modern approach, global edition. Pearson.\nO’Donnell, J. (2024). Sam Altman says helpful agents are poised to become AI’s killer function. MIT \nTechnology Review.  h t t p s :  / / w w w  . t e c h n  o l o g  y r e v i  e w . c o  m / 2 0 2 4  / 0 5 /  0 1 / 1 0  9 1 9 7 9  / s a m - a  l t m a  n - s a y  s - h e l  p \nf u l - a  g e n t  s - a r e  - p o i s  e d - t o -  b e c o  m e - a i s - k i l l e r - f u n c t i o n /\nOpenAI (2024b). Learning to reason with LLMs.  h t t p s :  / / o p e  n a i . c o  m / i n  d e x / l  e a r n i  n g - t o -  r e a s  o n - w i t h - l l m s /\nOpenAI (2025b). ChatGPT agent System Card.  h t t p s :  / / o p e  n a i . c o  m / i n  d e x / c  h a t g p  t - a g e n  t - s y  s t e m - c a r d /\nOpenAI (2024a). GPT–4o System Card. https://openai.com/index/gpt–4o-system-card/\nOpenAI (2025a). OpenAI o3 and o4-mini System Card [Technical report]. OpenAI.  h t t p s :  / / c d n  . o p e n a  i . c o  m \n/ p d f / 2 2 2 1 c 8 7 5 – 0 2 d c – 4 7 8 9 – 8 0 0 b - e 7 7 5 8 f 3 7 2 2 c 1 / o 3 - a n d - o 4 - m i n i - s y s t e m - c a r d . p d f\nOpenAI (2023). GPT–4 Technical Report (No. arXiv:2303.08774). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i \nv . 2 3 0 3 . 0 8 7 7 4       \nPasquinelli, M. (2023). The eye of the master: A social history of artificial intelligence. Verso.\nPepperberg, I. M. (2006). Cognitive and communicative abilities of grey parrots. Applied Animal Behav-\niour Science, 100(1), 77–86.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / j .  a p p l a  n i m . 2 0  0 6 . 0  4 . 0 0 5\nPérez, D. I., & Gomila, A. (2021). Social Cognition and the Second Person in Human Interaction  (1st \nedition). Routledge.\nPérez-Verdugo, M. (2022). Situating transparency: An extended cognition approach. Teorema: Revista \nInternacional De Filosofía, 41(3), 7–24.\nPerrigo, B. (2023). OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less \nToxic. Time.  h t t p s :  / / t i m  e . c o m /  6 2 4 7  6 7 8 / o  p e n a i  - c h a t g  p t - k  e n y a - w o r k e r s /\nPersson, A. (2024). Cognition and the Machine [PhD Thesis, Uppsala University].  h t t p s :  / / u u .  d i v a - p  o r t a  l . o \nr g  / s m a s  h / r e c o  r d . j  s f ? p i  d = d i v  a 2 % 3 A 1  8 5 0 2  1 8 & d s w i d = 3 8 9\nPort, R. F., & Gelder, T. V . (1995). Mind as motion: Explorations in the dynamics of cognition. MIT Press.\nPutnam, H. (1965). The mental life of some machines. Philosophical papers: Volume 2, mind, Language \nand reality (pp. 408–428). Cambridge University Press.\nQwen, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, \nH., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., & Qiu, Z. (2025). Qwen2.5 Techni-\ncal Report (No. arXiv:2412.15115). arXiv. https://doi.org/10.48550/arXiv.2412.15115\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsu-\npervised Multitask Learners.\n1 3\nTransforming agency: On the mode of existence of large language…\nRaja, V ., Valluri, D., Baggs, E., Chemero, A., & Anderson, M. L. (2021). The Markov blanket trick: On the \nscope of the free energy principle and active inference. Physics of Life Reviews, 39, 49–72.  h t t p s : / / d \no i . o r g / 1 0 . 1 0 1 6 / j . p l r e v . 2 0 2 1 . 0 9 . 0 0 1       \nReed, E. S. (1996). Encountering the world: Toward an ecological psychology. Oxford University Press.\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-maron, G., Giménez, M., Sul -\nsky, Y ., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, \nY ., Hadsell, R., Vinyals, O., Bordbar, M., & Freitas, N. (2022). A Generalist Agent. Transactions on \nMachine Learning Research. https://openreview.net/forum?id=1ikK0kHjvj.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating \nerrors. Nature, 323(6088), 533–536. https://doi.org/10.1038/323533a0\nRumelhart, D. E., McClelland, J. L., & Group, P. R. (1987). Parallel distributed processing, vol. 1: Foun-\ndations. The MIT Press.\nRussell, S. J. (2019). Human compatible: Artificial intelligence and the problem of control.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., & Choi, Y . (2019). WinoGrande: An Adversarial Winograd \nSchema Challenge at Scale  (No. arXiv:1907.10641). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 1 9 0 7 . 1 0 \n6 4 1       \nSchick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., & \nScialom, T. (2023). Toolformer: Language Models Can Teach Themselves to Use Tools  (No. \narXiv:2302.04761). arXiv. https://doi.org/10.48550/arXiv.2302.04761\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., \nHassabis, D., Graepel, T., Lillicrap, T., & Silver, D. (2020). Mastering atari, go, chess and Shogi by \nplanning with a learned model. Nature, 588, 604–609. https://doi.org/10.1038/s41586-020-03051-4\nSchwitzgebel, E., Schwitzgebel, D., & Strasser, A. (2024). Creating a large Language model of a philoso-\npher. Mind & Language, 39(2), 237–259. https://doi.org/10.1111/mila.12466\nSearle, J. R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, 3(3), 417–424.\nSearle, J. R. (1983). Intentionality: An essay in the philosophy of Mind. Cambridge University Press.\nSeth, A. K. (2014). A predictive processing theory of sensorimotor contingencies: Explaining the puzzle of \nperceptual presence and its absence in synesthesia. Cognitive Neuroscience, 5(2), 97–118.  h t t p s : / / d o \ni . o r g / 1 0 . 1 0 8 0 / 1 7 5 8 8 9 2 8 . 2 0 1 3 . 8 7 7 8 8 0       \nSeth, A. K. (2021). Being you: A new science of consciousness. Faber & Faber.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y . K., Wu, Y ., & Guo, D. \n(2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models  \n(No. arXiv:2402.03300). arXiv. https://doi.org/10.48550/arXiv.2402.03300\nShapiro, L. (2019). Embodied Cognition.\nShardlow, M., & Przybyła, P. (2023). Deanthropomorphising NLP: Can a Language Model Be Conscious? \n(No. arXiv:2211.11483). arXiv. https://doi.org/10.48550/arXiv.2211.11483\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously \nLarge Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer  (No. arXiv:1701.06538). \narXiv. https://doi.org/10.48550/arXiv.1701.06538\nShen, M., & Yang, Q. (2025). From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digi-\ntal Agent (No. arXiv:2505.02024). arXiv. https://doi.org/10.48550/arXiv.2505.02024\nShojaee, P., Mirzadeh, I., Alizadeh, K., Horton, M., Bengio, S., & Farajtabar, M. (2025). The Illusion of \nThinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem \nComplexity (No. arXiv:2506.06941). arXiv. https://doi.org/10.48550/arXiv.2506.06941\nSiegelmann, H. T., & Sontag, E. D. (1995). On the computational power of neural Nets. Journal of Com-\nputer and System Sciences, 50(1), 132–150. https://doi.org/10.1006/jcss.1995.1013\nSignificant Gravitas (2023). AutoGPT [JavaScript].  h t t p s :  / / g i t  h u b . c o  m / S i  g n i fi   c a n t -  G r a v i t  a s / A  u t o G P T \n(Original work published 2023).\nSimondon, G. (2017). On the mode of existence of technical objects. University of Minnesota Press.\nSinghal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Amin, M., Hou, L., Clark, K., Pfohl, S. R., \nCole-Lewis, H., Neal, D., Rashid, Q. M., Schaekermann, M., Wang, A., Dash, D., Chen, J. H., Shah, \nN. H., Lachgar, S., Mansfield, P. A., & Natarajan, V . (2025). Toward expert-level medical question \nanswering with large Language models. Nature Medicine, 31(3), 943–950.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / \ns 4 1 5 9 1 - 0 2 4 - 0 3 4 2 3 - 7       \nSprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., & Durrett, G. (2023). MuSR: Testing the Limits of Chain-\nof-thought with Multistep Soft Reasoning  (No. arXiv:2310.16049). arXiv.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / \na r X i v . 2 3 1 0 . 1 6 0 4 9       \n1 3\nX. E. Barandiaran, L. S. Almendros\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, \nA., Garriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., \nKocurek, A. W., Safaya, A., Tazarv, A., & Wu, Z. (2023). Beyond the Imitation Game: Quantifying \nand extrapolating the capabilities of language models (No. arXiv:2206.04615). arXiv.  h t t p s : / / d o i . o r \ng / 1 0 . 4 8 5 5 0 / a r X i v . 2 2 0 6 . 0 4 6 1 5       \nSterelny, K. (2001). The evolution of agency and other essays. Cambridge University Press.\nThompson, E. (2010). Mind in life: Biology, phenomenology, and the sciences of mind. Belknap.\nTikhonov, A., & Yamshchikov, I. P. (2023). Post Turing: Mapping the landscape of LLM Evaluation (No. \narXiv:2311.02049). arXiv. https://doi.org/10.48550/arXiv.2311.02049\nTomasello, M. (2022). The evolution of agency: Behavioral organization from lizards to humans . The \nMIT Press.\nTorrance, S. (2014). Artificial consciousness and artificial ethics: Between realism and social relationism. \nPhilosophy & Technology, 27(1), 9–29. https://doi.org/10.1007/s13347-013-0136-5\nTunstall, L. (2022). Understanding FLOPs-per-token estimates from OpenAI’ s scaling laws—Research  \n[Research]. Hugging Face Forums.  h t t p s :  / / d i s  c u s s . h  u g g i  n g f a c  e . c o /  t / u n d e  r s t a  n d i n g  - fl  o p  s - p e r -  t o k e  n - e \ns t  i m a t e  s - f r o m  - o p e  n a i s - s c a l i n g - l a w s / 2 3 1 3 3\nTuring, A. (1950). Computing machinery and intelligence. Mind, LIX(236), 433–460.  h t t p s : / / d o i . o r g / 1 0 . 1 \n0 9 3 / m i n d / L I X . 2 3 6 . 4 3 3       \nÜnsal, M. (2023). DemoGPT: Autonomous AI Agent for Effortless App Creation [Python].  h t t p s : / / g i t h u b . c \no m / m e l i h - u n s a l / D e m o G P T     (Original work published 2023).\nValdivia, A. (2024). The supply chain capitalism of AI: A call to (re)think algorithmic harms and resistance \nthrough environmental lens. Information Communication & Society, 0(0), 1–17.  h t t p s :  / / d o i  . o r g / 1  0 . 1 \n0  8 0 / 1 3  6 9 1 1 8  X . 2 0 2 4  . 2 4 2  0 0 2 1\nValmeekam, K., Sreedharan, S., Marquez, M., Hernandez, A. O., & Kambhampati, S. (2023).  On the \nPlanning Abilities of Large Language Models—A Critical Investigation. Thirty-seventh Conference \non Neural Information Processing Systems.  https://openreview.net/forum?id=X6dEqXIsEW\nVan Veen, D., Van Uden, C., Blankemeier, L., Delbrouck, J. B., Aali, A., Bluethgen, C., Pareek, A., Pola-\ncin, M., Reis, E. P., Seehofnerova, A., Rohatgi, N., Hosamani, P., Collins, W., Ahuja, N., Langlotz, C. \nP., Hom, J., Gatidis, S., Pauly, J., & Chaudhari, A. S. (2024). Adapted Large Language Models Can \nOutperform Medical Experts in Clinical Text Summarization (No. arXiv:2309.07430). arXiv.  h t t p : / / \na r x i v . o r g / a b s / 2 3 0 9 . 0 7 4 3 0     .   \nVarela, F. J., Thompson, E., & Rosch, E. (1991). The embodied mind: Cognitive science and human experi-\nence. MIT Press.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. \n(2017). Attention is All you Need. Advances in Neural Information Processing Systems, 30.  h t t p s :  / / p r \no  c e e d i n  g s . n  e u r i p  s . c c /  p a p e r /  2 0 1 7  / h a s h  / 3 f 5 e  e 2 4 3 5 4  7 d e e  9 1 f b d  0 5 3 c 1  c 4 a 8 4 5  a a - A  b s t r a c t . h t m l\nVerbeek, P. P. (2008). Cyborg intentionality: Rethinking the phenomenology of human–technology rela -\ntions. Phenomenology and the Cognitive Sciences, 7(3), 387–395.\nWaisberg, E., Ong, J., Masalkhi, M., Zaman, N., Sarker, P., Lee, A. G., & Tavakkoli, A. (2023). Meta smart \nglasses—Large Language models and the future for assistive glasses for individuals with vision \nimpairments. Eye (London, England), 1–3. https://doi.org/10.1038/s41433-023-02842-z\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y ., Zhao, \nW. X., Wei, Z., & Wen, J. R. (2023). A Survey on Large Language Model based Autonomous Agents. \narXiv. https://arxiv.org/abs/2308.11432v2.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2023). \nChain-of-Thought Prompting Elicits Reasoning in Large Language Models (No. arXiv:2201.11903). \narXiv. https://doi.org/10.48550/arXiv.2201.11903\nWeng, L. (2023). LLM Powered Autonomous Agents. Lil’Log.  h t t p s :  / / l i l  i a n w e n  g . g i  t h u b .  i o / p o  s t s / 2 0  2 3 - 0  \n6 - 2 3 - a g e n t /\nWolfram, S. (2023). What Is ChatGPT Doing… and Why Does It Work? [Personal]. Stephen Wolfram \nWritings.  h t t p s :  / / w r i  t i n g s .  s t e p  h e n w o  l f r a m  . c o m / 2  0 2 3 /  0 2 / w h  a t - i s  - c h a t g  p t - d  o i n g - a n d - w h y - d o e s - i t - w o \nr k /\nWu, S. (2024). Introducing devin, the first AI software engineer. Cognition Labs.  h t t p s :  / / w w w  . c o g n i  t i o n  - l \na b s  . c o m /  i n t r o d  u c i n  g - d e v i n\nWu, Q., Bansal, G., Zhang, J., Wu, Y ., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., & Wang, C. (2023). \nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework [Jupyter \nNotebook]. https://github.com/microsoft/autogen (Original work published 2023).\n1 3\nTransforming agency: On the mode of existence of large language…\nWu, W., Morris, J. X., & Levine, L. (2024). Do language models plan ahead for future tokens?  (No. \narXiv:2404.00859). arXiv. https://doi.org/10.48550/arXiv.2404.00859\nXi, Z., Chen, W., Guo, X., He, W., Ding, Y ., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., Zheng, \nR., Fan, X., Wang, X., Xiong, L., Zhou, Y ., Wang, W., Jiang, C., Zou, Y ., Liu, X., & Gui, T. (2023). \nThe Rise and Potential of Large Language Model Based Agents: A Survey (No. arXiv:2309.07864). \narXiv. http://arxiv.org/abs/2309.07864.\nXiang, C. (2023). OpenAI Used Kenyan Workers Making $2 an Hour to Filter Traumatic Content from \nChatGPT. Vice.  h t t p s :  / / w w w  . v i c e .  c o m /  e n / a r  t i c l e  / w x n 3 k  w / o p  e n a i -  u s e d -  k e n y a n  - w o r  k e r s -  m a k i n  g - d o l l  \na r 2 -  a n - h o  u r - t o  - fi  l t e  r - t r  a u m a t i c - c o n t e n t - f r o m - c h a t g p t\nXiang, J., Tao, T., Gu, Y ., Shu, T., Wang, Z., Yang, Z., & Hu, Z. (2023). Language Models Meet World \nModels: Embodied Experiences Enhance Language Models (No. arXiv:2305.10626). arXiv.  h t t p s : / / \nd o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 5 . 1 0 6 2 6       \nXie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., Liu, Y ., \nXu, Y ., Zhou, S., Savarese, S., Xiong, C., Zhong, V ., & Yu, T. (2024). OSWorld: Benchmarking \nMultimodal Agents for Open-Ended Tasks in Real Computer Environments (No. arXiv:2404.07972). \narXiv. https://doi.org/10.48550/arXiv.2404.07972\nXie, J., Zhang, K., Chen, J., Yuan, S., Zhang, K., Zhang, Y ., Li, L., & Xiao, Y . (2025). Revealing the Bar-\nriers of Language Agents in Planning. In L. Chiruzzo, A. Ritter, & L. Wang (Eds.), Proceedings of \nthe 2025 Conference of the Nations of the Americas Chapter of the Association for Computational \nLinguistics: Human Language Technologies (Volume 1: Long Papers) (pp. 1872–1888). Association \nfor Computational Linguistics.  h t t p s :   /  / d o  i . o r  g /  1 0 .  1 8 6  5 3   / v 1 / 2   0 2 5 . n   a a c  l -  l o n g . 9 3\nYang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., & Press, O. (2024). SWE-agent: \nAgent Computer Interfaces Enable Software Engineering Language Models.\nYao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y ., & Narasimhan, K. (2023). Tree of Thoughts: \nDeliberate Problem Solving with Large Language Models (No. arXiv:2305.10601). arXiv.  h t t p s : / / d o \ni . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 5 . 1 0 6 0 1       \nYildirim, I., & Paul, L. A. (2024). From task structures to world models: What do LLMs know? Trends in \nCognitive Sciences, 28(5), 404–415. https://doi.org/10.1016/j.tics.2024.02.008\nZellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., & Choi, Y . (2019). HellaSwag: Can a Machine Really \nFinish Your Sentence? (No. arXiv:1905.07830). arXiv. https://doi.org/10.48550/arXiv.1905.07830\nZhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., & Wang, Y . X. (2023a). Language Agent Tree \nSearch Unifies Reasoning Acting and Planning in Language Models (No. arXiv:2310.04406). arXiv. \nhttps://doi.org/10.48550/arXiv.2310.04406\nZhou, W., Jiang, Y . E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., Zhu, S., \nChen, J., Zhang, W., Tang, X., Zhang, N., Chen, H., Cui, P., & Sachan, M. (2023b). Agents: An Open-\nsource Framework for Autonomous Language Agents (No. arXiv:2309.07870). arXiv.  h t t p s : / / d o i . o r \ng / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 9 . 0 7 8 7 0       \nZhou, L., Schellaert, W., Martínez-Plumed, F., Moros-Daval, Y ., Ferri, C., & Hernández-Orallo, J. (2024). \nLarger and more instructable Language models become less reliable. Nature, 634(8032), 61–68. \nhttps://doi.org/10.1038/s41586-024-07930-y\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., & Irving, G. \n(2020). Fine-Tuning Language Models from Human Preferences  (No. arXiv:1909.08593). arXiv. \nhttps://doi.org/10.48550/arXiv.1909.08593\nZiemke, T. (2023). Understanding social robots: Attribution of intentional agency to artificial and biologi-\ncal bodies. Artificial Life, 29(3), 351–366. https://doi.org/10.1162/artl_a_00404\nZiemke, T., & Lowe, R. (2009). On the role of emotion in embodied cognitive architectures: From organ-\nisms to robots. Cognitive Computation, 1(1), 104–117. https://doi.org/10.1007/s12559-009-9012-0\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\n1 3\nX. E. Barandiaran, L. S. Almendros\nAuthors and Affiliations\nXabier E. Barandiaran1  · Lola S. Almendros2\n \r Xabier E. Barandiaran\nxabier.barandiaran@ehu.eus\nLola S. Almendros\nlola.s.almendros@gmail.com\n1 IAS-Research Centre for Life, Mind and, Society Dept. Philosophy UPV/EHU, University \nof the Basque Country, Donostia, Spain\n2 Institute for Science and Technology Studies, University of Salamanca, Salamanca, Spain\n1 3",
  "topic": "Philosophy of mind",
  "concepts": [
    {
      "name": "Philosophy of mind",
      "score": 0.7128018140792847
    },
    {
      "name": "Agency (philosophy)",
      "score": 0.591830849647522
    },
    {
      "name": "Phenomenology (philosophy)",
      "score": 0.56380295753479
    },
    {
      "name": "Philosophy of language",
      "score": 0.5173263549804688
    },
    {
      "name": "Epistemology",
      "score": 0.48631009459495544
    },
    {
      "name": "Mode (computer interface)",
      "score": 0.461779922246933
    },
    {
      "name": "Linguistics",
      "score": 0.4558713138103485
    },
    {
      "name": "Cognitive science",
      "score": 0.4192975163459778
    },
    {
      "name": "Psychology",
      "score": 0.403422474861145
    },
    {
      "name": "Sociology",
      "score": 0.3921651840209961
    },
    {
      "name": "Philosophy",
      "score": 0.2834709882736206
    },
    {
      "name": "Computer science",
      "score": 0.2319980263710022
    },
    {
      "name": "Metaphysics",
      "score": 0.09124526381492615
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169108374",
      "name": "University of the Basque Country",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I184999862",
      "name": "Universidad de Salamanca",
      "country": "ES"
    }
  ],
  "cited_by": 1
}