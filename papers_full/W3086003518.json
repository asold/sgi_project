{
  "title": "Optimising and comparing source-extraction tools using objective segmentation quality criteria",
  "url": "https://openalex.org/W3086003518",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3023419717",
      "name": "Caroline Haigh",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A2996334934",
      "name": "Nushkia Chamba",
      "affiliations": [
        "Stockholm University",
        "Instituto de Astrofísica de Canarias",
        "AlbaNova",
        "Universidad de La Laguna"
      ]
    },
    {
      "id": "https://openalex.org/A4284141327",
      "name": "Aku Venhola",
      "affiliations": [
        "University of Oulu",
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A2590575495",
      "name": "Reynier Peletier",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A3074217573",
      "name": "Lars Doorenbos",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A2047182514",
      "name": "Matthew Watkins",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A3188675235",
      "name": "Michael H. F. Wilkinson",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A3023419717",
      "name": "Caroline Haigh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2996334934",
      "name": "Nushkia Chamba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284141327",
      "name": "Aku Venhola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2590575495",
      "name": "Reynier Peletier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3074217573",
      "name": "Lars Doorenbos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047182514",
      "name": "Matthew Watkins",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3188675235",
      "name": "Michael H. F. Wilkinson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2782971999",
    "https://openalex.org/W2127193339",
    "https://openalex.org/W1607094988",
    "https://openalex.org/W2135625048",
    "https://openalex.org/W2888532099",
    "https://openalex.org/W2067741302",
    "https://openalex.org/W2146103050",
    "https://openalex.org/W2142238829",
    "https://openalex.org/W2188041026",
    "https://openalex.org/W2894421858",
    "https://openalex.org/W2953475150",
    "https://openalex.org/W1983192361",
    "https://openalex.org/W2094181926",
    "https://openalex.org/W1966828720",
    "https://openalex.org/W2298287474",
    "https://openalex.org/W1969296052",
    "https://openalex.org/W1965508489",
    "https://openalex.org/W2257344440",
    "https://openalex.org/W4288562380",
    "https://openalex.org/W2159714423",
    "https://openalex.org/W1510052597",
    "https://openalex.org/W2129217551",
    "https://openalex.org/W2788595045",
    "https://openalex.org/W2601359817",
    "https://openalex.org/W1572126587",
    "https://openalex.org/W2088682428",
    "https://openalex.org/W1972544340",
    "https://openalex.org/W2087496273",
    "https://openalex.org/W2798304462",
    "https://openalex.org/W2897914751",
    "https://openalex.org/W2066202006",
    "https://openalex.org/W2786879317",
    "https://openalex.org/W1652775531",
    "https://openalex.org/W2293501768",
    "https://openalex.org/W2541426243",
    "https://openalex.org/W2885073701",
    "https://openalex.org/W2955923615",
    "https://openalex.org/W2139370468",
    "https://openalex.org/W2751754953",
    "https://openalex.org/W2146842130",
    "https://openalex.org/W1998468445",
    "https://openalex.org/W2322601319",
    "https://openalex.org/W2158055978",
    "https://openalex.org/W4295973587",
    "https://openalex.org/W4289217525",
    "https://openalex.org/W4288360392"
  ],
  "abstract": "Context. With the growth of the scale, depth, and resolution of astronomical imaging surveys, there is increased need for highly accurate automated detection and extraction of astronomical sources from images. This also means there is a need for objective quality criteria, and automated methods to optimise parameter settings for these software tools. Aims. We present a comparison of several tools developed to perform this task: namely SExtractor, ProFound, NoiseChisel, and MTObjects. In particular, we focus on evaluating performance in situations that present challenges for detection. For example, faint and diffuse galaxies; extended structures, such as streams; and objects close to bright sources. Furthermore, we develop an automated method to optimise the parameters for the above tools. Methods. We present four different objective segmentation quality measures, based on precision, recall, and a new measure for the correctly identified area of sources. Bayesian optimisation is used to find optimal parameter settings for each of the four tools when applied to simulated data, for which a ground truth is known. After training, the tools are tested on similar simulated data in order to provide a performance baseline. We then qualitatively assess tool performance on real astronomical images from two different surveys. Results. We determine that when area is disregarded, all four tools are capable of broadly similar levels of detection completeness, while only NoiseChisel and MTObjects are capable of locating the faint outskirts of objects. MTObjects achieves the highest scores on all tests for all four quality measures, whilst SExtractor obtains the highest speeds. No tool has sufficient speed and accuracy to be well suited to large-scale automated segmentation in its current form.",
  "full_text": "Astronomy & Astrophysics manuscript no. ms ©ESO 2020\nNovember 18, 2020\nOptimising and comparing source extraction tools using objective\nsegmentation quality criteria\nCaroline Haigh1, Nushkia Chamba2,3, Aku Venhola4,5, Reynier Peletier4, Lars Doorenbos1, Matthew Watkins1, and\nMichael H. F. Wilkinson1,\n1 Bernoulli Institute for Mathematics, Computer Science and Artiﬁcial Intelligence,\nUniversity of Groningen, P.O. Box 407, 9700 AK Groningen, The Netherlands\ne-mail: (c.haigh,m.h.f.wilkinson)@rug.nl\n2 Instituto de Astrofísica de Canarias, Calle Vía Láctea,\ns/n, 38205 San Cristóbal de La Laguna, Santa Cruz de Tenerife, Spain\ne-mail: chamba@iac.es\n3 Departamento de Astrofísica, Universidad de La Laguna,\n38205, La Laguna, Tenerife, Spain\n4 Kapteyn Astronomical Institute, University of Groningen,\nP.O. Box 800, 9700 A V Groningen, The Netherlands\ne-mail: peletier@astro.rug.nl\n5 Space Physics and Astronomy Research Unit, University of Oulu,\nPentti Kaiteran katu 1, 90014 Oulu, Finland\ne-mail: aku.venhola@oulu.fi\nJune 2020\nABSTRACT\nContext. With the growth of the scale, depth, and resolution of astronomical imaging surveys, there is an increased need for highly\naccurate automated detection and extraction of astronomical sources from images. This also means there is a need for objective quality\ncriteria, and automated methods to optimise parameter settings for these software tools.\nAims. We present a comparison of several tools which have been developed to perform this task: namely SExtractor, ProFound,\nNoiseChisel, and MTObjects. In particular, we focus on evaluating performance in situations which present challenges for detection\n– for example, faint and di ﬀuse galaxies; extended structures, such as streams; and objects close to bright sources. Furthermore, we\ndevelop an automated method to optimise the parameters for the above tools.\nMethods. We present four di ﬀerent objective segmentation quality measures, based on precision, recall, and a new measure for\nthe correctly identiﬁed area of sources. Bayesian optimisation is used to ﬁnd optimal parameter settings for each of the four tools\non simulated data, for which a ground truth is known. After training, the tools are tested on similar simulated data, to provide a\nperformance baseline. We then qualitatively assess tool performance on real astronomical images from two diﬀerent surveys.\nResults. We determine that when area is disregarded, all four tools are capable of broadly similar levels of detection completeness,\nwhile only NoiseChisel and MTObjects are capable of locating the faint outskirts of objects. MTObjects produces the highest scores\non all tests on all four quality measures, whilst SExtractor obtains the highest speeds. No tool has suﬃcient speed and accuracy to be\nwell-suited to large-scale automated segmentation in its current form.\nKey words. techniques: image processing – surveys – methods: data analysis\n1. Introduction\nSegmentation maps – images showing which pixels in an image\nbelong to which source – are used extensively to preprocess ob-\nservational data for analysis. They are used for masking sources,\nestimating sky backgrounds, and creating catalogues, amongst\nother applications. It is therefore essential that the tools used to\ncreate these maps are accurate and reliable. Otherwise, the later\nscientiﬁc process may be invalidated by errors in the measure-\nments of sources.\nUnfortunately, astronomical images have many properties\nwhich cause problems for traditional image segmentation algo-\nrithms. Images may be highly noisy, and have an extremely large\ndynamic range. Objects generally have no clear boundaries, and\ntheir outer regions may extend below the level of background\nnoise. As many generic segmentation algorithms are edge-based\n(Pal & Pal 1993; Wilkinson 1998), they are unable to accurately\nprocess these images.\nIn addition, with the growth of the scale of astronomical sur-\nveys, there is an increased need for a fast and accurate tool for\nsegmentation. This is illustrated by current projects such as the\nLegacy Survey of Space and Time (LSST), which aims to pro-\nduce around 15TB of raw data per night (Ivezi´c et al. 2008). With\nsurveys of this scale, human intervention will no longer be feasi-\nble, meaning that the tools should ideally be robust to variations\nin images without manual tuning.\nBecause of these unique challenges, a number of tools have\nbeen developed for the sole purpose of accurately detecting\nsources in astronomical images. The most well-known of these\nfor optical data is SExtractor (Bertin & Arnouts 1996). In re-\ncent years, however, a number of alternatives have been pro-\nposed, including ProFound (Robotham et al. 2018), NoiseChisel\nArticle number, page 1 of 34\narXiv:2009.07586v2  [astro-ph.GA]  16 Nov 2020\nA&A proofs:manuscript no. ms\nFig. 1: A gri-composite image of IAC Stripe82 ﬁeld\nf0363_g.rec.fits showing a large structure of galactic cirri.\nSuch complex, overlapping structures are challenging for source\ndetection tools.\n(Akhlaghi & Ichikawa 2015), and MTObjects (Teeninga et al.\n2013, 2016).\nIn this paper, we evaluate and compare these segmentation\ntools, in order to study their strengths and weaknesses. A thor-\nough comparison provides a means for astronomers to choose\nwhich algorithm is best suited for their scientiﬁc goals. In ad-\ndition, several of these tools are still under active development,\nand such an analysis can help to direct future advancements.\nFor this comparison, we develop numerical measures for seg-\nmentation quality (Sect. 3.3), and propose a method for auto-\nmatic conﬁguration of tool parameters (Sect. 3.2). This approach\nto evaluating segmentation maps aims to provide an objective\nmeasure of quality. To do this, we use simulated images, with\na known ground truth (Sect. 3.1), to provide evaluations which\nare not dependent on visual biases and preconceptions. We sup-\nplement our results by demonstrating the performance of our au-\ntomatically conﬁgured parameters on real survey images (Sect.\n5).\nThroughout this paper we use the terms ‘segmentation’,\n‘source detection’, and ‘source extraction’ interchangeably, to\nrefer to the process of identifying unique sources in astronom-\nical images and marking the pixels of the image in which each\nsource is the dominant contributor.\n2. Source extraction methods\n2.1. Previous methods\nFor as long astronomical images have been produced, it has been\nnecessary for their contents to be catalogued and measured, in\norder that they may be used for scientiﬁc applications. As man-\nually locating and outlining objects is a slow and subjective pro-\ncess, particularly when considering the faint outskirts of objects,\nmany attempts have been made at automating this process.\nEarly automatic tools directly scanned photographic plates to\nlocate sources and produce measurements. Notable is COSMOS\n(Pratt 1977), which used a process of repeated thresholding to\nproduce ‘coarse measurements’ of images – essentially quantis-\ning the image over an estimated local background level. It then\nused ‘ﬁne measurements’ to produce more accurate measure-\nments of objects’ proﬁles. Later additions included improved de-\nblending of adjacent sources (Beard et al. 1990).\nWhilst modern tools no longer use digitised photographic\nplates, instead working directly with data captured by CCDs, the\noverall process used in recent tools are fundamentally very sim-\nilar to those used in their predecessors. Almost all tools follow\nthe same four main steps:\n1. Identify and measure the background level.\n2. Threshold the image relative to the background.\n3. Locate (and deblend) sources appearing above the threshold.\n4. Produce a catalogue of sources and their measured proper-\nties.\nSExtractor, described in more detail below, uses a very sim-\nilar method of repeated thresholding to COSMOS. In contrast,\nseveral other tools make use of dendrograms – hierarchical rep-\nresentations of images, in which nodes representing local max-\nima are connected at the highest brightness level where thresh-\nolding would show a single, unbroken object. Users may sub-\nsequently ‘prune’ the dendrogram by removing nodes connect-\ning very small or faint regions, and may automatically or manu-\nally mark objects meeting some criteria. Dendrograms have been\nused to visualise and analyse hierarchical structure in both in-\nfrared images (Houlahan & Scalo 1992) and radio data cubes\n(Rosolowsky et al. 2008; Goodman et al. 2009).\nOther tools have deviated from a thresholding-based ap-\nproach. Many of these tools and their methods are described in\nMasias et al. (2012).\n2.2. Deblending\nDeblending – the process of separating overlapping or nested\nsources – is closely linked to source extraction; all of the tools\nwe discuss in this paper make some attempt at deblending. How-\never, for some scientiﬁc purposes, the tools do not produce suﬃ-\nciently accurate separation of sources, leading to problems such\nas poor photometry (Abbott et al. 2018; Huang et al. 2018), and\nsystematic measurements of physical properties such as redshift\n(Boucaud et al. 2020) and cluster mass (Simet & Mandelbaum\n2015). Consequently, several tools also exist to perform deblend-\ning as a separate process.\nAs these tools are predominantly either designed to use\nthe results of another source extraction tool (such as SCAR-\nLET(Melchior et al. 2018), which uses SExtractor for initial\nsource detection), or are predominantly designed for smaller im-\nages with only a few galaxies (such as the machine learning-\nbased methods proposed in Reiman & Göhre (2019)), we chose\nto not include them in the comparisons in this paper. However,\nthe evaluation process we deﬁne in section 3 could equally be\napplied to compare deblending-speciﬁc tools in future work.\n2.3. Compared tools\nWe chose to focus our comparison on four tools – SExtrac-\ntor (Bertin & Arnouts 1996), which is in common use, and\nthree recent alternatives: ProFound (Robotham et al. 2018),\nArticle number, page 2 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nNoiseChisel + Segment (Akhlaghi & Ichikawa 2015), and MTO-\nbjects (Teeninga et al. 2013, 2016). We chose to exclude several\nother source extraction tools from this comparison for various\nreasons – notably DeepScan (Prole et al. 2018), which is depen-\ndent on the use of another tool (such as SExtractor) to produce\nan initial mask; and AstroDendro (Robitaille et al. 2013a), which\nwas prohibitively slow to run on large images.\n2.3.1. SExtractor\nSExtractor (Bertin & Arnouts 1996) is a widely-used tool for the\ncreation of segmentation maps. It was developed with the goal\nof producing catalogues of astronomical sources from large scale\nsky surveys.\nThe ﬁrst step in the SExtractor pipeline is the estimation\nand subtraction of the background. The image is divided into\ntiles, and a histogram is produced for each. Values more than\n3 standard deviations from the median are removed. Tiles are\nthen classiﬁed into crowded and uncrowded ﬁelds based on the\nchange in histogram distribution, and a background value is es-\ntimated based on each tile’s median and mode.\nThe image is then thresholded at an ﬁxed number of expo-\nnentially spaced levels above a user-deﬁned threshold. This con-\nverts the light in the image into trees, with branches represent-\ning bright areas within larger, fainter objects. Pixels in branches\nwhich contain at least a given proportion of the light of their\nparent objects are marked as individual objects, whilst branches\ncontaining a lower amount of light are regarded as part of the\nparent object. Pixels in the outskirts of objects are allocated la-\nbels based on the probability that a pixel of that value is present\nat that point, using proﬁles ﬁtted to the detected sources.\nIn practice, SExtractor may be used in multiple passes,\nparticularly when detecting extended sources. For example, a\nhot/cold method may be used, wherein a sensitive pass captures\nthe outskirts of objects, and a less sensitive pass identiﬁes which\nobjects are not false positive detections (Rix et al. 2004). It may\nalso be used to identify candidate objects, which are then manu-\nally veriﬁed.\nSExtractor version 2.19.5 was used for this comparison, us-\ning the default ﬁlter – a convolution with a 3×3 pyramidal func-\ntion, which approximates Gaussian smoothing. We found in sub-\nsequent testing, described in Appendix A, that using a 9x9 Gaus-\nsian PSF with a full width at half maximum of 5 pixels produced\nmarginally better results, although this diﬀerence was not signif-\nicant, and did not aﬀect the general conclusions of this paper.\n2.3.2. ProFound\nProFound (Robotham et al. 2018), like SExtractor, was designed\nas a general purpose package for detecting and extracting astro-\nnomical sources; however, it aims to produce a more accurate\nsegmentation, which may be used for galaxy proﬁling.\nInstead of using multiple thresholds, ProFound uses a sin-\ngle threshold after the background estimation stage in order\nto demarcate pixels containing sources. These pixels are then\nprocessed in descending order of brightness, with a watershed\nprocess being used to allocate less bright neighbouring pixels\n(within some tolerance) to the object of the brightest pixel in a\nregion, until all pixels bordering the object are either allocated\nto other objects, are marked as background, or have higher ﬂux\nthan neighbouring pixels within the object.\nFollowing this process, the background is re-estimated, and\nan iterative process of calculating photometric properties of the\nsegments and repeatedly dilating them is performed, to produce\na ﬁnal segmentation map. ProFound version 1.1.0 was used for\nthis comparison.\n2.3.3. NoiseChisel + Segment\nNoiseChisel (Akhlaghi & Ichikawa 2015) was designed with\nthe goal of ﬁnding ‘nebulous objects’, such as irregular or faint\ngalaxies, accurately. NoiseChisel is intended to be hand-tuned\nfor individual images – the tutorial states that that conﬁgurations\nare ‘not generic’ (GNU Astronomy Utilities 2019).\nNoiseChisel separates the image into areas containing light\nfrom objects, and areas containing only background. To do this,\nit uses a threshold below the estimated background level, and\nperforms a series of binary morphological operations to create\nan initial detection map. Further morphological operations are\nthen performed on the ‘objects’ and ‘background’ separately,\nand area and signal-to-noise thresholds are used to remove false\ndetections. Segment then produces a map of ‘clumps’ by locat-\ning connected regions around local maxima in the image with a\nwatershed-like process. It then discards those that do not meet\na signal-to-noise threshold, and grows the remaining clumps to\ncreate a ﬁnal segmentation map.\nSince the publication of the original paper, the program has\nbeen split into two separate tools within the GNU Astronomy\nUtilities package: NoiseChisel, and Segment. For the purposes\nof this comparison, the tools are treated as a single pipeline, and\nevaluated together – we examine only this ﬁnal ‘objects’ output.\nThe latest version as of the start of the comparison was used –\nversion 0.7.42a. Several new versions have since been released,\nwhich may contain di ﬀerent parameters and produce di ﬀerent\nresults.\n2.3.4. MTObjects\nMTObjects (Teeninga et al. 2013, 2016) takes a similar approach\nto SExtractor; both operate on the principle that after a back-\nground subtraction step, objects can be detected by a threshold-\ning process. However, where SExtractor uses a small number of\nﬁxed thresholds, MTObjects uses tree-based morphological op-\nerators.\nA max-tree (Salembier et al. 1998) is constructed from the\nsmoothed and background-subtracted image. The max-tree is a\ntree of the image: the leaves represent local maximum pixels,\nnodes represent increasingly large connected areas of the image,\nwith decreasing minimum pixel values, and the root represents\nthe entire image. This tree is then ﬁltered, by using tests to de-\ntermine which nodes of the tree – or areas of the image – contain\nan amount of ﬂux, given their area, that is statistically signiﬁcant\nrelative to their background. If a node has no signiﬁcant par-\nent, or its parent has another child with greater ﬂux, it is marked\nas an object. Despite representing all connected components at\nall grey levels in the image, building the max-tree is very e ﬃ-\ncient (O(N log N) typically for ﬂoating point images (Carlinet &\nGéraud 2014)).\nThe max-tree structure used in MTObjects is very similar\nto the dendrogram, used in several astronomical applications as\ndescribed above (Houlahan & Scalo 1992; Rosolowsky et al.\n2008). There are two main di ﬀerences. Firstly, the dendrogram\nonly contains nodes where areas connect, whereas the max-tree\ncontains a node for every di ﬀerence in brightness value. Sec-\nondly, MTObjects uses a single statistical signiﬁcance test to de-\ntect objects, combining multiple attributes of the node, whilst the\nArticle number, page 3 of 34\nA&A proofs:manuscript no. ms\ndendrogram methods frequently ﬁlter small and faint objects at\nﬁxed thresholds.\nThere have been no oﬃcial software releases of MTObjects\n– we used a Python and C implementation, 1 which we adapted\nfrom the software used in the original paper. We used signiﬁ-\ncance test 4 as recommended by Teeninga et al. (2016).\n3. Methodology\n3.1. Data\nIn this section we describe the data upon which we tested the\ntools. Simulated data (Sects. 3.1.1, 3.1.2) allows us to accurately\nquantify performance on a simpliﬁed version of the problem,\nwhilst survey images (Sect. 3.1.3) allow us to qualitatively ex-\nplore behaviour in a range of diﬀerent real situations.\n3.1.1. Simulated data\nTesting source detection algorithms on real observational data\nhas several limitations. Firstly, the ground truth is not known –\neven if objects have been manually labelled, it is possible that\nobjects have been missed, or incorrectly measured. In particu-\nlar, it is di ﬃcult to establish the true extent of objects at a low\nbrightness level, as their outer regions may not be clearly visu-\nally distinguishable from background.\nSecondly, many features of interest – such as ultra-di ﬀuse\ngalaxies – are not thought to be very common. This means that\nit is diﬃcult to establish a statistical measure of how accurately\nthey can be detected. As these objects are also more diﬃcult for\nalgorithms to detect, a larger sample is required to determine the\naccuracy of the algorithms.\nBy using simulated data, we gain the ability to test the algo-\nrithms on large datasets with a known ground truth. This means\nthat we can make accurate measures of precision and accuracy\nfor faint features, as well as taking into account the true extent of\nobjects. We can also measure accuracy of algorithms in diﬀerent\ncontrolled conditions, such as with high noise, background vari-\nation, and overlapping sources.\nWe created ten frames of data emulating images in the r’-\nband of data in the Fornax Deep Survey (FDS). This is a deep,\nmedium-sized ground based survey of the nearby Fornax cluster,\nwhich is located at the distance of 20 Mpc (Iodice et al. 2016;\nVenhola et al. 2018). Each simulated image contained approxi-\nmately 1500 ‘stars’, 4000 ‘cluster galaxies’ and 50 ‘background\ngalaxies’. Stars were simulated as point sources, and galaxies as\nSérsic models (Sersic 1968). The number and structural parame-\nters of the stars and galaxies were drawn from distributions simi-\nlar to those found in the FDS. In the simulated images, stars have\nmagnitudes between 10 and 23 mag, and galaxies have mean\neﬀective surface brightnesses between 21 mag arcsec −2 and 31\nmag arcsec−2. Background galaxies have eﬀective radii between\n0.5 and 3.5 arcsec and Sérsic indices between 2 and 4. Cluster\ngalaxies have eﬀective radii between 2.5 and 40 arcsec, and Sér-\nsic indices between 0.5 and 2. Axis ratios varied from 0.3 to 1.0.\nTo replicate observation conditions, images were convolved with\nthe r-band point spread function of the OmegaCAM, and Pois-\nsonian and Gaussian noise were added (Venhola et al. 2018). For\nfurther details of the process, see Venhola (2019, Chapter 5).\n1 https://github.com/CarolineHaigh/mtobjects\nFig. 2: Simulated survey images\n3.1.2. Choosing a ground truth\nAstronomical sources have no clear boundary; their light merely\nbecomes insigniﬁcant in relation to noise and background light\nat some point in their outskirts. This means that when we create\na ground truth for simulated images – a ‘correct’ segmentation\nmap – we need to choose a threshold, t, below which we judge\nlight from sources to be undetectable. Assuming a ﬂat back-\nground, this threshold can be expressed as a sum of the back-\nground level, bg, and some multiple, n, of the standard deviation\nof the noise, σ:\nt = bg+ (n ∗σ)\nSources may also overlap, meaning that each pixel contains\nlight from multiple sources. In segmentation maps, each pixel is\nallocated to a single source; therefore, it is necessary to deter-\nmine which source has the strongest relationship with a given\npixel. It should be noted that whilst segmentation maps are the\ntraditional method of demarcating sources within an image, they\nare limited by their inability to represent the reality that pixels\ncontain light belonging to multiple sources2. Consequently, tree-\nbased methods, which inherently model nested objects, are un-\nable to capture this structure within segmentation maps. As such,\ninformation contained in the models is lost, and not measured in\nthe evaluation.\nWe initially considered allocating each pixel to the source\nwhich contributed the most ﬂux to it. However, this meant that\nfainter sources in the vicinity of bright sources were entirely\nerased, as they had a lower raw ﬂux contribution.\nInstead, we chose to allocate labels based on a combination\nof the importance of the pixel to the source and the importance of\nthe source to the pixel. For a source with total ﬂuxFs, contribut-\ning a ﬂux fs,p to a pixel with total ﬂux Fp, the pixel contains fs,p\nFs\nof the source’s ﬂux. Conversely, the source contributesfs,p\nFp\nof the\nlight contained within the pixel.\n2 A new data format would be required to clearly represent this nested\ndata. This could prove to be a challenging problem, due to the complex-\nity of allocating multiple labels and proportional brightnesses to each\npixel\nArticle number, page 4 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nThese measures may be combined to give a single measure:\nfs,p\nFs\n× fs,p\nFp\nWhen allocating a pixel to a source, Fp will be constant for\nall sources contributing to the pixel. Therefore, the pixel may be\nallocated to the source with the highest value for\n( fs,p)2\nFs\nand fs,p ≥t\nThe value of n has a substantial eﬀect on the areas allocated\nto objects, as shown in Fig. 3. Consequently, it has an large im-\npact on the evaluation of the segmentation maps produced by the\ntools.\nAs we wished to evaluate the performance of the tools at\nlevels of low surface brightness, we chose to use a value of n =\n0.1 for our ground truths. This pushes the tools to optimise their\nparameters to capture and correctly allocate as much of the light\nin the images as possible.\n3.1.3. Real-world data\nWhilst testing algorithms on real-world data has several limita-\ntions, as discussed above, it is nevertheless essential. It allows\nus to subjectively evaluate performance on structures and condi-\ntions which cannot be easily simulated, such as streams, spiral\ngalaxies, and unusual artefacts.\nWith this in mind, we selected a number of images in the\noptical which contained examples of these features. We chose\nimages from the Fornax Deep Survey (FDS; Iodice et al. 2016;\nVenhola et al. 2018), IAC Stripe 82 Legacy Project 3 (hereafter\nIAC Stripe82; Fliri & Trujillo 2016; Román & Trujillo 2018)\nwhich is a 2.5 degree stripe ( −50◦ <R.A. < 60◦,−1.25◦ <Dec\n< 1.25◦) with a total area of 275 square degrees in all the ﬁve\nSloan Digital Sky Survey (SDSS) bands and the Hubble Ultra\nDeep Field (HUDF; Beckwith et al. 2006), a 11 arcmin 2 region\nin the Southern Sky. As the simulated images were designed to\nmimic the FDS, using real images from this survey allowed us to\ntest the optimised parameters with similar imaging conditions,\nwhere they would be expected to perform well. The additional\nuse of IAC Stripe82 and HUDF images allows us to examine the\nconsistency of parameters on images with very diﬀerent imaging\nconditions.\nWhile the FDS and IAC Stripe82 are deep surveys using\nground-based telescopes, the VLT Survey Telescope (VST) and\nthe SDSS Telescope respectively, the well-studied HUDF ex-\ntends our analysis to the higher resolution, space-based data\nfrom the Hubble Space Telescope. In terms of depth, the HUDF\nis the deepest with a point source depth of∼29 mag which corre-\nsponds to a surface brightness limit of µV606 ∼32.5 mag/arcsec2\nin the V606-band, computed as a 3 σ ﬂuctuation with respect to\nthe background of the image in 10 ×10 arcsec2 boxes (3σ; 10 ×\n10 arcsec2). The FDS images in the SDSS r-band have a limit-\ning depth of µr ∼29.8 mag/arcsec2 (3σ; 10×10 arcsec2) and the\nIAC Stripe82 survey is ∼1 mag shallower than FDS with a lim-\niting surface brightness depth ofµr ∼28.6 mag/arcsec2 (3σ; 10×\n10 arcsec2) and µg ∼29.1 mag/arcsec2 (3σ; 10 ×10 arcsec2). In\norder to select the deepest imaging from all these surveys in the\noptical regime, we use the V606-band images in the HUDF and\n3 http://research.iac.es/proyecto/stripe82/\nthe SDSS r- and g-band images from FDS and IAC Stripe82 re-\nspectively.\nAdditionally, the FDS, IAC Stripe 82 and HUDF datasets\ncollectively represent deep data with di ﬀerent surface bright-\nness depths and spatial resolutions: FDS is > 1 mag deeper\nand two times higher in spatial resolution than IAC Stripe82\n(0.2 arcsec/pixel resolution in FDS (rebinned from the 0.21 ar-\nsec/pixel of the VST) compared to 0.396 arcsec /pixel in SDSS)\nand the HUDF is > 2 mag deeper than FDS, with the best reso-\nlution currently possible from space ∼0.05 arcsec/pixel. There-\nfore, the optimised parameters of each algorithm are tested on\nreal images with varying depth and resolution. However, in this\nwork we speciﬁcally chose images in the optical wavelengths\nto test the limits of current detection algorithms for upcoming\ndeeper and wider surveys such as LSST. In future work, a sim-\nilar analysis to that presented herein can readily be extended to\nother wavelengths.\n3.2. Parameter optimisation\nTo produce a fair comparison of the algorithms’ capabilities,\nthey should be tested with parameters that are as close to op-\ntimal as possible. Due to the extremely large parameter spaces\nof some of the tools, it was not feasible to manually optimise the\ntools, or to test every possible combination of parameters.\nWe therefore chose to use an automatic method to select\ngood parameters for each tool. We initially considered use of\na genetic algorithm for this purpose; however, this proved to be\nprohibitively slow, as a high number of time-consuming runs of\neach tool was required. Instead, we used Bayesian optimisation.\n3.2.1. Bayesian optimisation\nBayesian optimisation is a method of black-box optimisation\nwhich is well-suited for functions which take a long time to eval-\nuate (Jones et al. 1998). It operates by creating a model of how\nthe function behaves, identifying the regions in parameter space\nwhere it may perform well or where it may not be well-ﬁtted, and\nchoosing points in these regions to evaluate, in order to improve\nthe model.\nIn the context of source extraction tools, the input takes the\nform of a set of relevant parameters, as dictated by each tool’s\ndocumentation. The parameters are evaluated by running the tool\non a training image, comparing the output to a known ground\ntruth, and choosing a metric (as detailed in Sect. 3.3) as the out-\nput score to optimise.\nWe used the GPyOpt optimisation library (The GPyOpt au-\nthors 2016) to perform the optimisations. For each metric, each\ntool was optimised on every image individually, and the found\nparameters were then applied to all of the remaining images,\nto assess their performance. The tools’ default parameters were\nused as a starting point. 120 evaluations were performed on each\nimage in batches of four, using the local penalisation method,\nand the best set of parameters was chosen.\n3.3. Metrics\nThe quality of a segmentation can be measured both in terms of\nthe presence and absence of ground truth objects, and the simi-\nlarity between the true objects and segmented shapes.\nArticle number, page 5 of 34\nA&A proofs:manuscript no. ms\n(a) Simulated image\n (b) Ground truth (1.0 σ)\n(c) Ground truth (0.5 σ)\n (d) Ground truth (0.1 σ)\nFig. 3: Ground truth segmentations of a simulated image, with a varying threshold ( n ∗σ). The coloured regions label distinct\nobjects, and the black regions make up the background.\n3.3.1. Matching detections\nWhen measuring detection rates, it is necessary to match de-\ntected objects with ground truth objects. It may be the case that\na detected object covers the area of multiple true objects, or con-\nversely that multiple detected objects are found within the area\nof a single true object. Therefore, a one-to-one mapping is re-\nquired, in order to prevent algorithms from being rewarded for\nfailing to correctly distinguish between sources.\nWe chose to use the brightest pixel in each object as an iden-\ntiﬁer – the detected object containing the brightest pixel in a\nground truth object was matched to it. In the event that a detected\nobject contained the brightest pixel of multiple ground truth ob-\njects, the object containing the pixel with the highest ﬂux was\nchosen as a unique match.\nThree measures made use of this matching procedure:\n– Detection recall (completeness) – the proportion of objects\nwhich are detected.\n– Detection precision (purity) – the proportion of segments\nwhich can be matched to real objects.\n– F-score – the harmonic mean of precision and recall:\nF-score = 2 ×precision ×recall\nprecision + recall\n3.3.2. Evaluating areas\nIn order to quantify the accuracy of the areas of segmented ob-\njects, we used a modiﬁed version of over-merging and under-\nmerging scores (Levine & Nazif 1981). The under-merging score\nmeasures the extent to which objects which should be a sin-\ngle segment are broken into multiple pieces by the segmentation\ntool. The over-merging score measures the opposite – the extent\nto which multiple objects are incorrectly combined into a single\nsegment by the tool. Combining these scores gives a measure of\nthe overall quality of the segmentation.\nIn the original method, the ground truth segmentation is di-\nvided into N segments, R1...RN , with areas A1...AN , and the test\nsegmentation is divided into M segments, T1...TM, with areas\na1...aM. The original metrics are calculated by ﬁndingRk to max-\nimise T j ∩Rk,for each test segment, T j:\n– Under-merging error (UM):\nUM =\nM∑\nj=1\n(Ak −(T j ∩Rk))(T j ∩Rk)\nAk\n– Over-merging error (OM):\nOM =\nM∑\nj=1\n(aj −(T j ∩Rk))\nArticle number, page 6 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nIn these original deﬁnitions, we found that the over-merging\nscore did not penalise segmentations which divided large ob-\njects into many small pieces. This meant that tools could ﬁnd\nenormous numbers of false positives, fragmenting the ‘back-\nground’ segment, without penalty. Consequently, we chose to re-\ndeﬁne the over-merging score to become symmetric to the under-\nmerging score, which better takes into account the number and\nsize of segments. We also deﬁned an area score, which combined\nthe two measures to give an overall score.\n– Over-merging error (OM) – for each reference segment, Rk,\nﬁnd T j to maximise T j ∩Rk\nOM =\nN∑\nk=1\n(aj −(T j ∩Rk))(T j ∩Rk)\naj\n– Area score –\nArea score= 1 −\n√\nOM2 + UM 2\nAs the area score alone does not take into account precision and\nrecall, we also deﬁned two combined scores. These give us the\nability to optimise for a balanced F-score and area score.\n– Combined score A –\n√\nArea score2 + F-score2\n– Combined score B –\n3√\n(1 −OM) ×(1 −UM ) ×F-score\nWe additionally measure speed – the rate at which images can\nbe processed, measured in megapixels per second.\n4. Results\nWhilst the original intent was to compare all four programs on\nall metrics, ProFound proved to be very slow to optimise and\nrun, making it impractical for use on large images and surveys.\nAs such, it was optimised only on F-score and area score. Pro-\ncessing speeds are discussed in more detail in Sect. 4.6.\n4.1. Detection accuracy\nFigure 4 shows the range of F-scores produced when each tool is\noptimised for F-score. Two plots are shown for each tool: one in\nwhich the scores are grouped by the image being evaluated, and\none in which the scores are grouped by the training image used\nto optimise the parameters. The scores of the training image are\nexcluded from both graphs.\nFor both MTObjects and SExtractor, it is notable that the\nscores have smaller interquartile ranges and more varied medi-\nans when grouped by test image. This suggests that for these\ntools, the factor limiting the performance is the structure of each\nindividual test image, rather than the particular parameter set\nchosen. In contrast, ProFound has a smaller interquartile range\nwhen scores were grouped by optimisation image, suggesting\nthat in this case, performance is limited by the image used in the\noptimisation process.\nOverall, we see the strongest performance from MTObjects,\nwith median scores of over 0.80 for the majority of images. The\nweakest performance was produced by SExtractor, with scores\nof under 0.78 in most cases.\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nMT\n1 2 3 4 5 6 7 8 9 10\nTest ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nPF\n1 2 3 4 5 6 7 8 9 10\nTest ID\nSE\n(a) F-scores grouped by image evaluated\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nMT\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nPF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nSE\n(b) F-scores grouped by image used to optimise parameters\nFig. 4: F-score test distributions. Each tool’s parameters were\noptimised for F-score on each of the ten images, and evaluated\non the remaining nine images. Boxes extend from ﬁrst (Q1) to\nthird (Q3) quartiles of the results, with median values marked;\nwhiskers extend to the furthest F-score less than 1.5 ∗(Q3 −Q1)\nfrom each end of the box.\nExamining the precision and recall scores that make up the\nF-scores shows that all programs are capable of broadly simi-\nlar performance, with recall between 0.61 and 0.7 and precision\ngreater than 0.93. Whilst the recall scores appear low, many of\nthe faintest objects in the image are not even visible to the hu-\nman eye, and may in fact be impossible to detect by any tool;\nArticle number, page 7 of 34\nA&A proofs:manuscript no. ms\n0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\nPrecision\n0.62\n0.64\n0.66\n0.68Recall\nSE\nNC\nMT\nPF\nFig. 5: Precision vs recall – each tool’s parameters were opti-\nmised for F-score on each of the ten images, and evaluated on\nthe remaining nine images.\nthese objects are included in order to fully explore the limits\nof the tools’ capabilities. It is therefore useful to regard recall\nscores primarily as a relative measure, to compare the tools’ per-\nformances.\nDiﬀerences between the programs become apparent when\nthe scores are plotted against each other, as shown in Fig. 5. All\nthe tools have a moderate spread of recall scores, which may\nbe caused by diﬀerences in diﬃculty between the individual im-\nages.\nMTObjects and NoiseChisel both produce generally higher\nlevels of precision than SExtractor; with MTObjects giving a\nslightly higher maximum value, and a lower spread. ProFound\nachieves the greatest values for both precision and recall, but has\na very wide spread.\nWhen optimised for area score, SExtractor performed with\na substantially lower precision – it found an enormous number\nof false positives, as shown in Fig. 6. Here, we clearly see that\noptimising for area score is detrimental to the F-score results.\nThis appears to be a result of a very low threshold being selected\nin order to maximise the area of large shapes, meaning that a\nlarge number of small areas of noise are incorrectly marked as\nobjects.\nIn contrast, NoiseChisel and MTObjects were capable of in-\ncreasing their area scores without compromising their F-scores\nsubstantially. ProFound performed inconsistently, covering the\nfull range of precision scores across the ten optimisations.\n4.2. Area measures\nUnsurprisingly, all tools were capable of reaching higher area\nscores when optimised for area score than F-score, as can be\nseen in Fig. 9.\nWhen optimised for area score, NoiseChisel and MTOb-\njects both performed well, with area scores substantially higher\nthan the other two tools, and MTObjects having a slight edge\nover NoiseChisel. Both tools also showed lower variation when\n0.0 0.2 0.4 0.6 0.8 1.0\nPrecision\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Recall\nSE\nNC\nMT\nPF\nFig. 6: Precision vs recall – each tool’s parameters were opti-\nmised for area score on each of the ten images, and evaluated on\nthe remaining nine images.\nscores were grouped by test image, as shown in Fig. 7, suggest-\ning that the tools’ performance is being limited by the content of\nthe test images, rather than the parameters found in the optimi-\nsation.\nIn contrast, ProFound had much greater variability in area\nscores when grouped by test image, and indeed, substantial vari-\nation between the parameter sets. It also produced the weakest\narea scores overall. SExtractor was capable of producing higher\narea scores than ProFound, but at substantial cost to precision,\nas discussed previously.\n4.3. Combined scores\nThe two combined metrics oﬀered a way of optimising for both\narea and F-score, diﬀering in the balance between the two mea-\nsures. As such, optimising for these metrics gives an indication\nof the overall peak performance of the tools.\nIn practice, both metrics produced broadly similar results in\nterms of both area and F-score, as shown in Fig. 8. MTObjects\nproduced the highest values for both F-score and area score,\nwith NoiseChisel producing slightly lower values in both met-\nrics. SExtractor produced lower F-scores, with a large degree of\nvariability, and substantially lower area scores, as would be ex-\npected from its limited success when optimising purely for area.\nThese results indicate that optimisation for combined scores pre-\nvents a large number of spurious detections being found by SEx-\ntractor, when compared to area score alone.\n4.4. Overview of optimisation metrics\nFigure 9 shows an overview of the results of the optimisation, in\nthe form of scatter plots of F-score and area score. Points rep-\nresent the result of evaluating the four tools on each image, us-\ning the parameters found by optimising for each metric on every\nother image individually. From this, we can make several obser-\nvations about the tools’ performance.\nArticle number, page 8 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nMT\n1 2 3 4 5 6 7 8 9 10\nTest ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nPF\n1 2 3 4 5 6 7 8 9 10\nTest ID\nSE\n(a) Area scores grouped by image evaluated\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nMT\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nPF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nSE\n(b) Area scores grouped by image used to optimise parameters\nFig. 7: Area score test distributions. Each tool’s parameters were\noptimised for area score on each of the ten images, and evaluated\non the remaining nine images.\nFirstly, the tools designed speciﬁcally for locating low\nsurface-brightness structures (NoiseChisel and MTObjects) are,\nunsurprisingly, capable of achieving higher area scores than the\ngeneral-purpose tools. Secondly, all the tools must to some de-\ngree compromise F-score to obtain a higher area score, but this\ntrade-oﬀ is much greater for the general-purpose tools. Thirdly,\nMTObjects has less spread than the other tools; indeed, it ﬁnds\nidentical parameters and consequently produces identical results\nfor nearly all optimisations over area or combined scores.\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nCombined A\nSE\nNC\nMT\n(a) Optimised for Combined A\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nCombined B\nSE\nNC\nMT\n(b) Optimised for Combined B\nFig. 8: F-score vs area score – each tool’s parameters were opti-\nmised for the combined measures on each of the ten images, and\nevaluated on the remaining nine images.\nExamining Figs. 10 and 11 gives further insight into the be-\nhaviour behind these scores. We see that both NoiseChisel and\nMTObjects capture regions of light with visually similar bound-\naries, but that MTObjects marks many small, fractured sections\nin the outer regions as background. Meanwhile, NoiseChisel\ncaptures an area of light with fewer holes, but segments it into\nobjects rather arbitrarily. In contrast, SExtractor and ProFound,\nwhich both have generally lower area scores, capture the com-\npact centres of objects and only limited areas of the outskirts.\nArticle number, page 9 of 34\nA&A proofs:manuscript no. ms\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nF-score\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nArea\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nComb. A\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nComb. B\nSE\nNC\nMT\nPF\nFig. 9: A summary of test scores for each program using each optimisation method. Each point represents the evaluation of the\nsegmentation of one image using parameters found by optimising on a di ﬀerent image. Each plot shows results for a di ﬀerent\noptimisation metric. Note that ProFound was only optimised on F-score and area score.\n4.5. Background values\nEach program makes internal estimations of background, which\nmay be global or localised. We may also examine the pixels in\nthe image which are not allocated to any segment in the ﬁnal\nmap. As the simulated images have a ﬂat background with a\nmean of 0, we can use the mean value of these unallocated pix-\nels as an indication of whether pixels containing no source light\nare being incorrectly allocated to sources, or conversely whether\npixels are incorrectly regarded as belonging to sources.\nProFound and SExtractor both consistently overestimated\nthe background, giving values in the order of 10 −1σ, where σ\nis the standard deviation of the background noise (1 .1 ×10−12\nfor the simulated images). This suggests that they are not detect-\ning some parts of the sources, which visual inspection of Figs.\n10 and 11 conﬁrm to be the case. There was one exception to this\nbehaviour: SExtractor generally underestimated the background\nwhen optimised for area, with values in the order of −10−2σ.\nThis corresponds to the large number of small false positive de-\ntections made under this optimisation, thanks to the low back-\nground threshold used (see Table B.1).\nMTObjects also underestimated the background, with values\nof around −10−1σ when optimised for metrics including area\nArticle number, page 10 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Original simulated image\n (b) Ground truth (0.1 σ)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(c) Segmentation maps\nFig. 10: Segmentations of a full simulated image, using the parameters which gave the highest median score for each combination\nof optimisation measure and tool on the simulated images: SExtractor (SE), NoiseChisel + Segment (NC), MTObjects (MT) and\nProFound (PF). The coloured regions label distinct objects, and the black regions make up the background. Due to speed, PF was\nnot optimised for Combined A and B.\nArticle number, page 11 of 34\nA&A proofs:manuscript no. ms\n(a) Original simulated image\n (b) Ground truth (0.1 σ)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(c) Segmentation maps\nFig. 11: Segmentations of a section of a simulated image, using the parameters which gave the highest median score for each\ncombination of optimisation measure and tool. For more information, see Fig. 10.\nmeasures; it underestimated to a lesser degree ( −10−2σ) when\noptimised for F-score. This behaviour may be a consequence of\nthe holes in the outskirts of objects causing the optimisation pro-\ncess to select parameters which overestimate the size of objects,\nthereby increasing the solid area within objects but also the num-\nber of incorrectly labelled background pixels.\nThe strongest background estimation performance was pro-\nduced by NoiseChisel. Whilst optimising for F-score gave an\nArticle number, page 12 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nMT NC PF SE\nTool\n1.5\n1.0\n0.5\n0.0\n0.5\nF-score\nMT NC PF SE\nTool\nArea\nMT NC SE\nTool\n1.5\n1.0\n0.5\n0.0\n0.5\nComb. A\nMT NC SE\nTool\nComb. B\nlog10(Speed (Mpx/second))\nFig. 12: Distributions of processing speed across all combina-\ntions of images and optimised parameters for each tool and opti-\nmisation metric.\noverestimation in a similar range to SExtractor and ProFound,\nit produced mean backgrounds in the order ±10−3σwhen opti-\nmised for a metric including area measures. Not only were the\nvalues closer to the goal of 0, there was also no evidence of sys-\ntematic over- or under-estimation.\n4.6. Speed\nThe speed at which an image can be processed is very important\nwhen we consider the size and quantity of images produced by\nmodern surveys.\nAt its best, SExtractor was the fastest of all the tools by a\nconsiderable margin, as shown in Fig. 12. When optimised for\narea, this advantage vanished completely – potentially due to\nthe vast increase in the number of false positives and large ob-\njects. When optimised for combined metrics, processing speed\ndepended heavily on the individual set of parameters, producing\na wide spread of speeds.\nMTObjects had the most consistent speed across optimisa-\ntions. Neither SExtractor or MTObjects used parallel process-\ning, which potentially reduced their speed. It should be noted\nthat the original C implementation of MTObjects is faster than\nour current Python and C++ implementation, as Teeninga et al.\n(2016) reported SExtractor was only 2.5 times faster than the C\nversion of MTObjects in terms of median performance, and only\n1.3 times faster on average. Some code optimisation and using\na parallel max-tree algorithm Moschini et al. (2018) should be\nable to improve the performance in terms of speed.\nNoiseChisel showed fast performance when optimised for\nF-Score alone, but was much slower when area score was in-\ncluded in the optimisation criterion. This appears to be due to\na combination of factors; predominantly a lower value for ‘det-\ngrowquant’, which aﬀects how much objects are grown after de-\ntection.4\n4 We found that some non-optimal parameter combinations also\ncaused substantial slowdown, due to the program requiring large\nAs mentioned previously, ProFound consistently had a very\nlong processing time, which greatly reduced its viability as a\ntool for processing large images from surveys with many images.\nThis is due in part to it writing temporary data to disk, which is\ndiscussed in the original ProFound publication (Robotham et al.\n2018) – ProFound o ﬀers a low-memory mode which reduces\nthe amount of data stored, allowing the processing of larger im-\nages without a drastic slowdown; however, as noted, the method\nis fundamentally rather slow. The use of R as the implementa-\ntion language may be further reducing the tool’s potential speed.\nThe authors of ProFound are rewriting parts of the code in C++,\nwhich should signiﬁcantly improve its performance.\n4.7. Parameter consistency\nMTObjects was by far the most consistent of the tools – having\nonly two relevant parameters, it had a much smaller parameter\nspace to explore. While its optimised parameters varied slightly\nwhen optimising only over F-score, all other metrics gave the\nsame optimal parameters for all cases but one, as shown in Table\nB.4.\nSExtractor and NoiseChisel, optimised over 6 and 20 pa-\nrameters respectively, displayed far less consistency in the pa-\nrameters that were found (Tables B.1, B.2, B.3). This could po-\ntentially have been reduced by increasing the number of itera-\ntions of the optimisation process. However, the similar scores\nproduced using very di ﬀerent parameters suggest that there is\nno single best choice – many combinations of settings perform\nequally well overall, but are better and worse in certain contexts.\n4.8. Inserted galaxies\nAs a ﬁnal step, we evaluated the performance of the tools on\na sample of real galaxies, inserted into a frame of the Fornax\nDeep Survey (FDS), which the simulated data was designed to\nemulate. Testing the tools on real galaxies allows us to verify\nthat the behaviour of the tools generalises to galaxies which are\nnot perfect ellipticals.\nWe selected a sample of 22 galaxies from the EFIGI cat-\nalogue (Baillard et al. 2011), which contains images from the\nfourth data release of the Sloan Digital Sky Survey (SDSS)\n(Adelman-McCarthy et al. 2006). Galaxies were selected with\nD25 (diameter of the 25.0 mag arcsec −2 isophote, in units of\nlog 0.1 arcmin) between 1.7 and 1.999, a heliocentric velocity <\n2000 km/s, and a galactic latitude between 60 ◦and 70◦. This is\na representative sample of galaxies in the nearby Universe, with\nhigh-quality SDSS images and detailed morphological types. We\nisolated the galaxy at the centre of each image using k-ﬂat ﬁlter-\ning (Ouzounis & Wilkinson 2010), which removed areas of light\nnot connected to the central pixel, whilst preserving the galaxy’s\ninternal detail. We then convolved each galaxy with the r-band\npoint spread function of the OmegaCAM, and added Poissonian\nnoise.\nIn order to examine the performance of the algorithms on\ngalaxies of diﬀerent brightnesses, we scaled the images to four\ndiﬀerent brightness levels, as shown in Fig. 13. At the bright-\nest level, the brightest pixel in each galaxy had a value in the\nsame order as the brightest pixels in the image, (around 10 −10,\ncorresponding to a surface brightness of 21.5 mag arcsec −2). At\nthe faintest, the brightest pixels were barely visible to the hu-\nman eye (around 10−13, corresponding to a surface brightness of\namounts of memory and consequently writing some data structures to\ndisk.\nArticle number, page 13 of 34\nA&A proofs:manuscript no. ms\n(a) 10−10\n (b) 10−11\n(c) 10−12\n (d) 10−13\nFig. 13: A galaxy from the EFIGI sample, inserted into the FDS\nframe at the four given brightness scalings.\n29 mag arcsec −2)). We selected 22 locations in the FDS frame\nwhere there were very few objects present, in order to minimise\ninterference with the inserted galaxies. We then created four im-\nages, with the 22 galaxies inserted into the same locations in the\nFDS frame, using a di ﬀerent brightness scaling for each image.\nWe then ran all four tools on each image with the four sets of\noptimised parameters obtained on the simulated images.\nWhilst using inserted galaxies meant that we had a ground\ntruth for those galaxies, there may still have been other objects\npresent around them in the FDS frame, which would also be de-\ntected by the tools. This means that we are unable to rely on the\npreviously deﬁned metrics, as other detected objects would be\nmarked as false positives and raise the under-merging error.\nInstead, we use a modiﬁed process to determine whether an\ninserted galaxy has been detected. If the brightest pixel in an\nobject is contained within a non-background segment of the seg-\nmentation map, and it is also the brightest pixel in that segment,\nwe determine that the object has been detected.\nAdditionally, we classify detections into two types – those\nwhere the galaxy has been mostly detected as a single object,\nand those where the algorithm has substantially fragmented the\ngalaxy. To do this, we check for other detected segments whose\nbrightest pixel is contained within the area of the inserted galaxy,\nsuggesting that they are not primarily detecting some other back-\nground object. If there are multiple segments which meet this cri-\nteria, we check that the segment containing the most light from\nthe inserted galaxy has at least 10×the amount of light contained\nin the segment containing the second most light – if it does, we\nmark the detection as ‘whole’; otherwise as fragmented. Whilst\nlacking the numerical accuracy of the previously deﬁned area\nscore, this provides an indication of the quality of detections.\nExamples of the two types of detection are shown in Fig. 14.\nThe results of this process are summarised in Fig. 15.\nAt higher brightnesses, most tools perform well, with only\nNoiseChisel failing to detect any objects at the two highest\nbrightness levels.\n(a) A ‘whole’ detected galaxy.\n (b) A fragmented galaxy.\nFig. 14: Segmentation maps showing the two deﬁned types of\ndetection.\nAt fainter levels, the tools show more variation. At the 10−12\nbrightness level, ProFound shows the strongest performance,\nfully detecting nearly over 90% of the objects under an area\nscore optimisation. SExtractor shows high levels of fragmen-\ntation at this level, consistent with its low area score found on\nthe simulated data. NoiseChisel maintains a roughly consistent\nrate of fragmented detections, but with fewer detections over-\nall, whilst MTObjects begins to show some fragmentation and a\nlower detection rate at this level.\nAt the faintest brightness, very few of the inserted galax-\nies are visible to the human eye, and this is reﬂected in the\nresults. Again, ProFound has a stronger performance than the\nother tools, with up to 40% of galaxies detected, but a higher\nrate of fragmentation than at higher brightness levels. SExtrac-\ntor reaches a similar detection rate under an area score optimisa-\ntion, but only produces fragmented detections; visual inspection\nshows that this is due to the tool ﬁnding many tiny objects, as\non the simulated images. Both NoiseChisel and MTObjects ﬁnd\nvery few objects at this low brightness level.\nThese results are generally consistent with the results shown\nin the preceding sections – all tools were capable of similar F-\nscores, and this is reﬂected in the similar detection rates found on\nthe inserted galaxies. Similarly, variations in area score roughly\ncorrespond to the fraction of the inserted galaxies with substan-\ntial fragmentation for each tool, particularly at the 10 −12 bright-\nness level.\nIt is notable that when the inserted galaxies are fainter, op-\ntimisations for F-score appear to be less e ﬀective than optimi-\nsations for area score. This may be due to the higher sensitivity\nto noise and lower thresholds generally found in area-based op-\ntimisations causing the fainter objects to be detected, whilst the\nF-score-based optimisations ignore these objects in order to min-\nimise false detections.\n5. Qualitative evaluation\nIn this section, we evaluate how the optimised parameters for\neach tool transfer to di ﬀerent surveys and instruments. We se-\nlected three surveys to apply the tools to, using the parameters\nwith the highest median test score following the optimisation\nprocess: the Fornax Deep Survey (FDS) (Iodice et al. 2016; Ven-\nhola et al. 2018); the IAC Stripe82 Legacy Project (Fliri & Tru-\njillo 2016; Román & Trujillo 2018); and the Hubble Ultra Deep\nField (HUDF) (Beckwith et al. 2006). All of these datasets are\ndeep surveys, with surface brightness limits fainter than µ ∼28\nmag/arcsec2, and have been used in several studies of galaxies of\nArticle number, page 14 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B\n(a) MTObjects\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B (b) NoiseChisel\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\n(c) ProFound\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B (d) SExtractor\nFig. 15: Percentage of inserted objects found, grouped by tool, brightness scaling, and optimisation metric, using the parameters\nwhich gave the highest median score for each combination of optimisation measure and tool. Lighter, stacked bars represent galaxies\nwhich are detected but substantially fragmented.\nlow surface brightness – for example, Venhola et al. (2017); Ven-\nhola et al. (2019); Iodice et al. (2019) for FDS, Román & Trujillo\n(2017a,b) for IAC Stripe82, and Oesch et al. (2009); Bouwens\net al. (2008) for HUDF. As far as the authors are aware, all these\nworks have used SExtractor for masking sources of light and\nprocessing observational data. Therefore, evaluating the quality\nof segmentation for these deep datasets using the other available\nsource extraction tools is an added value to ongoing research on\nfaint structures of galaxies.\nMoreover, using the source extraction tools to derive seg-\nmentation maps of a completely new dataset with the ‘best’ op-\ntimised parameters allows us to assess whether or not the pa-\nrameters perform in a consistent manner across diﬀerent datasets\nwhich have been acquired in very diﬀerent conditions. It is also\na test of the tool’s practical applicability to large astronomical\nsurveys of the future, such as those produced by Euclid (Amiaux\net al. 2012) and the LSST (Ivezi´c et al. 2008)).\nThe ‘best’ parameters derived from our optimisation scheme\nfor each test score that are used for the tools are highlighted with\nan asterisk in Appendix B.\n5.1. Fornax Deep Survey (FDS)\nAs the simulated images were created using the characteristics\nof the Fornax Deep Survey, using images from the real survey\nallows us to check that the parameters found on simulated data\nperform similarly on data which contains more unusual struc-\ntures. The limiting surface brightness for r-band images of FDS\nis 29.8 mag/arcsec2 (3σ; 10 ×10 arcsec2) (Venhola et al. 2017).\nWe here show a complete frame of the survey, and two\nsmaller areas of the same frame, containing faint and challeng-\ning objects. For each combination of training image and optimi-\nsation method, the parameters with the highest median test score\non the simulated dataset was used.\nIt is clear from Fig. 16 that the parameters produce very sim-\nilar behaviour on the real images to on the simulated images.\nMTObjects and NoiseChisel both capture similar areas of light,\nbut segment them very di ﬀerently; whilst ProFound and SEx-\ntractor capture only the centres of objects.\nExamining smaller details of the images gives more insight\ninto behaviour on challenging sources. Fig. 17 shows the seg-\nmentation of a faint, elongated galaxy. SExtractor only detects\na small area of the galaxy when optimised for area, and incor-\nrectly merges it with other surrounding objects; in all other op-\ntimisations it fails to detect the galaxy at all, perhaps due to a\ntoo high detection threshold. ProFound detects small blobs cov-\nering the area of the galaxy, but does not identify an underlying\nstructure. Similarly, NoiseChisel, whilst locating a larger area of\nlight, breaks it into chunks appearing to correspond to smaller\nobjects, losing the large structure. MTObjects was the only tool\nto capture the entire structure as one object, but incorrectly la-\nbelled it as the same object as the bright source in the bottom\nArticle number, page 15 of 34\nA&A proofs:manuscript no. ms\nTable 1: Summary of Qualitative Evaluation\nMTObjects NoiseChisel ProFound SExtractor\nOptimised parameters 2 20 8 6\nLanguage Python /C C R C\nClean edges of detected objects - ✓ ✓ Sometimes\nDetects elongated galaxy (FDS - Fig. 17) ✓ Fragmented - Fragmented\nDetects galaxy close to star (FDS - Fig. 18) ✓ Fragmented - Fragmented\nDetects cirrus (Stripe82 - Fig. 19) ✓ ✓ - Sometimes\nIsolates spiral substructures (HUDF - Fig. 24) ✓ - - -\nright corner, as well as connecting it to the outskirts of the object\nin the bottom left.\nFigure 18 contains another faint structure, located near a\nbright star, which is extremely di ﬃcult to visually detect. All\nfour tools struggle to produce ideal results in this situation. As\nbefore, ProFound and SExtractor do not detect the faintest parts\nof objects, which here gives the advantage of allowing both tools\nto distinguish between smaller sources. SExtractor again pro-\nduces a high number of false positives when optimised for area,\nbut does begin to detect areas of structure in Combined A and\nB. In contrast, ProFound produces a blobby segmentation, with\nless visual similarity to the input image, but again covering a\ngood deal of the smaller structures. NoiseChisel and MTObjects\nmark almost all of the image section as containing sources, but\nwith a very diﬀerent segmentation. NoiseChisel’s area optimisa-\ntion fails to detect any substructures in this part of the image,\nmarking all objects as a single large structure. In the other opti-\nmisations, it shows very little visual similarity to the input image.\nMTObjects correctly detects many of the sources in the area, al-\nthough it again joins the outskirts of some objects, and produces\na ragged appearance.\n5.2. IAC Stripe 82 Legacy Project\nAs an added layer of generalisation, we test the parameters\nwith the highest median test score found for each combina-\ntion of training image and optimisation method on deep g-band\nIAC Stripe82 images. The limiting surface brightness is 29 .1\nmag/arcsec2 (3σ, 10 ×10 arcsec2) (Román & Trujillo 2018).\nThese images consist of faint and di ﬀuse structures such as\ngalactic cirri, tidal streams, interacting galaxies as well as scat-\ntered light from point sources.\nSimilarly to the segmentation of the simulated images seen\nFigs. 10 and 11, we ﬁnd that SExtractor detects the least area\nof regions of light compared to the other tools. In particular, it\nmisses large portions of the galactic cirrus structure in Fig. 19,\neven when optimised for the area score. As in the case of the\nsimulated images, the fact that many smaller objects (including\nmany false positives) are detected in the background when op-\ntimised for the area score is most likely a consequence of the\nvery low threshold used to ﬁnd larger areas. However, the galac-\ntic cirrus structure is highly extended and di ﬀuse with low and\nhigh density regions, so the tool is unable to segment the struc-\nture as a single object, and fragments it into several pieces. This\n‘failure of detection’ may, however, be taken advantage of (with\nsome manual intervention) in studying the properties of galactic\ncirrus (Román et al. 2019).\nRemarkably, the performance of SExtractor on the image of\nthe interacting galaxies connected with a tidal stream in Fig. 20\nis much better on the main objects with parameters optimised for\narea score and both combined scores, whilst performing poorly\nin the background. Similar observations can be made in the case\nof the elliptical galaxy with a large stream in Fig. 21, but this\nstream is much fainter than in the interacting galaxies case, and\nSExtractor detects the stream in fragments (similar to the galac-\ntic cirrus).\nIn contrast, for all the IAC Stripe82 images, both\nNoiseChisel and MTObjects detect the largest amount of light\nas distinct objects or diﬀuse regions (reﬂected in the highest op-\ntimisation scores). Visually, NoiseChisel’s performance seems\nbetter when optimised for F-Score compared to the other scores,\nbut there is still di ﬀuse light around the objects which have\ngone undetected. When optimised for area score or the combined\nscores, this missing light is recovered; but as mentioned previ-\nously, the algorithm seems to segment structures within larger\nobjects rather arbitrarily. When comparing the outputs from each\noptimisation method, we can see that the substructure is seg-\nmented quite diﬀerently in all the IAC Stripe82 examples. This\nis probably a consequence of growing the ‘clumps’ (as detected\nin the CLUMPS output of Segment) to cover the full detected area\n– if the detected area is di ﬀerent, then the growth of the clumps\nseems to vary. This eﬀect is visible in comparing the tool’s out-\nput when optimised for all four measures in all the IAC Stripe\n82 examples. The fact that the substructure over the detected re-\ngions seems visually arbitrary may not be an issue in some cases\n– such as when segmentation maps are used for reducing datasets\nwhere all pixels with a signiﬁcant amount of signal above the\nbackground needs to be masked for processing (see for example\nBorlaﬀ et al. (2019)), or when the user is simply not concerned\nwith the substructure of astronomical sources.5\nHowever, for studies where more accurate segmentation of\ntidal streams and nested objects (or substructure) is required for\nphotometric calculations, it is not possible to automatically allo-\ncate which of these fragmented regions belong to the host struc-\nture, and the user may need to manually select regions of inter-\nest. This is especially visible for the large galactic cirrus in Fig.\n19 and the faint stream in Fig. 21 where the structures are seg-\nmented into separate objects of all kinds of shapes.\nFor the same IAC Stripe82 examples, a similar observation\ncan be made for MTObjects, but the partitioning better follows\nthe visual shape of all objects (background and nested). This be-\nhaviour means that the user is able to make a visual mapping\nbetween the input image and segmentation map much more eas-\nily, if they need to manually select regions of interest. In com-\nparing, the outputs of the tool when optimised for the di ﬀerent\nscores are very similar – the outputs for the area and combined\nscores are the same, and the only visible diﬀerence with F-Score\nis the extent to which the edges are fractured outwards. Com-\npared to the other tools, the existence of these highly fractured\nedges of the segmented regions in MTObjects may not be an ap-\n5 The NoiseChisel manual ( https://www.gnu.org/software/\ngnuastro/manual/html_node/NoiseChisel.html) states that the\nuser may choose to run Segment after NoiseChisel depending on\nwhether they want to analyse the substructure of sources.\nArticle number, page 16 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Input image – the r-band of ﬁeld 11 of the FDS.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld, using the parameters which gave the highest median score for each combination of optimi-\nsation measure and tool. For more information, see Fig. 10.\nFig. 16: Segmentations of a complete FDS ﬁeld (ﬁeld 11).\npealing characteristic for the user if smoother edges are required – for instance to make photometric calculations, such as the total\nmagnitude of objects.6\n6 Of the tools, this e ﬀect in the segmentation maps can only be con-\ntrolled in NoiseChisel without compromising the extent to which ob-\njects are detected.\nArticle number, page 17 of 34\nA&A proofs:manuscript no. ms\n(a) Input image – the r-band of ﬁeld\n11 of the FDS.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld section, using the parameters which gave the highest median score for each combination\nof optimisation measure and tool. For more information, see Fig. 10.\nFig. 17: Segmentations of a section of an FDS ﬁeld (ﬁeld 11), showing a low-surface brightness galaxy.\nArticle number, page 18 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Input image – the r-band of ﬁeld 11 of the FDS.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld, using the parameters which gave the highest median score for each combination of\noptimisation measure and tool.\nFig. 18: Segmentations of a section of a FDS ﬁeld (ﬁeld 11), showing a very faint structure in the lower centre.\nAnother characteristic of MTObjects can be seen in the ﬁeld\ncontaminated by a cluster of bright stars to the right of an ellipti-\ncal galaxy in Fig. 21. MTObjects allocates the diﬀuse stream and\nfaint halo around the core of the galaxy to the cluster of stars.\nThis is clearly a problem with how the detected regions are rep-\nresented – MTObjects is ﬁnding the diﬀuse regions in the image\n(at least those that could be visually identiﬁed in this example),\nbut allocating it to the wrong object. This means that the user\nwill need to once again manually select which regions belong\nto the galaxy, and this may not always be possible to identify in\nadvance when dealing with deep datasets. Apart from these ex-\nceptions, MTObjects performs fairly similarly and consistently\nacross the IAC Stripe82 images tested in this work.\nArticle number, page 19 of 34\nA&A proofs:manuscript no. ms\nDue to speed, at the time of writing we are only able to com-\nplete the optimisation of parameters for ProFound using the F-\nScore and area score. In this work, we ﬁnd that when optimised\nfor these scores, only in the merging galaxies case in Fig. 20 does\nthe tool segment the galaxy shape and its companion (though\nalso fragmented into several arbitrary pieces, as in NoiseChisel’s\nsegmentation). In all of the other images, the large galaxies or\nstructures are barely visible, and only because our eye is able to\nconnect the smaller fragments into one connected region.\n5.3. The Hubble Ultra Deep Field\nIn order to examine the behaviour of the tools on space-based\nobservations, we ran the tools on the V606-band of the Hub-\nble Ultra Deep Field (HUDF). As mentioned previously in\nSect. 3.1, the HUDF is the deepest data used in this work, a\npoint source depth of 29.3 mag (Beckwith et al. 2006) which\nis equivalent to a limiting surface brightness depth of µV606 ∼\n32.5 mag/arcsec2 (3σ; 10 ×10 arcsec2).\nAs the original drizzled image contained wide, zero-valued\nborders, we rotated and cropped it to contain as much of the ﬁeld\nas possible, while excluding the borders. We then ran the tools on\nthe image, using the same optimised parameters as in the previ-\nous sections. We show here the complete image, and two smaller\nareas of interest containing a type of feature not common in the\nother surveys: face-on spiral galaxies with visible substructures.\nBesides these artefacts, the behaviour of all four tools on the\nHUDF image appears to be generally similar to behaviour on the\nimages from other surveys, despite having a higher depth and a\ndiﬀerent telescope type.\nThis is further corroborated by Fig. 23, which shows a face-\non spiral galaxy, as well as several smaller elliptical galaxies. As\nbefore, SExtractor ﬁnds only the bright centres of objects, ex-\ncept when optimised for area; however, it noticeably divides the\nspiral galaxy into chunks where there is substructure. Somewhat\narbitrary division of the galaxy is also visible in the results of\nNoiseChisel and ProFound; with NoiseChisel capturing more of\nthe outskirts, as before. MTObjects appears to be the most suc-\ncessful at segmenting the spiral, with the majority of the galaxy\ncaptured as a single object, with smaller structures nested within\nit, although, as in previous instances, the outskirts are fractured.\nIn Fig. 24, which shows a larger spiral galaxy displayed at\nthe same scale, the tools have even greater diﬃculty segmenting\nthe galaxy in a meaningful way. As before, MTObjects has the\nmost success in separating nested structures without fragmenting\nthe overall structure of the object. NoiseChisel is also consistent\nwith previous behaviour. In contrast, ProFound produces quite\ndiﬀerent segmentations, with a far less blobby appearance. SEx-\ntractor produces quite poor segmentations when area is included\nin the optimisation; with elongated ovals being found in both of\nthe combined score images.\nIt must be remembered that the parameters were optimised\nfor images in quite diﬀerent conditions, so it is diﬃcult to quan-\ntify how much these inaccurate segmentations are caused by pa-\nrameters ill-suited for this context. However, as the behaviour is\nvery similar to that shown in the images from di ﬀerent surveys,\nit is reasonable to expect that it is largely caused by inherent\nlimitations of the tools.\n5.4. Usability\nAs shown in the parameter tables in Appendix B, only MTO-\nbjects reliably found the same set of ‘optimal’ parameters over\nmultiple optimisations. All of the other tools appeared to have\nmultiple locally optimum parameter combinations. This has a\nnegative impact on ease of use – users manually conﬁguring a\ntool through trial and error may fail to ﬁnd globally optimum pa-\nrameters, and be unaware of this fact. The best parameters may\nalso be dependent on the image used for optimisation – the pa-\nrameters found for one image or survey may not produce optimal\nresults when applied to others.\nAll four programs deﬁne parameters in terms of the individ-\nual steps of the method (e.g. use n thresholds), rather than in\nterms of how they a ﬀect the overall detection (e.g. detect ob-\njects to a given degree of certainty). Without using an optimisa-\ntion framework, users have no choice but to manually select set-\ntings that visually produce a good result, but which do not nec-\nessarily have any scientiﬁc justiﬁcation for being chosen. This\nis further exacerbated by large parameter spaces in the cases\nof NoiseChisel and ProFound allowing the user to inﬁnitely ad-\njust the behaviour of the tools without the implications of their\nchoices being clear. The ability to deﬁne performance in terms\nof the result rather than the process would greatly improve the\nease of use of the tools, as well as reducing the opacity of their\nbehaviour.\nThese are not major problems in the case of a user process-\ning a small number of images, but are compounded when large\nsurveys, requiring automatic segmentation for many images, are\nconsidered. The user must select a set of parameters that pro-\nduces good results for all images in their survey – an impossible\ntask if the tool requires manual tuning on individual images.\n6. Conclusions\nAll the compared methods were capable of a reasonable level of\nobject detection, as measured by F-score. However, ProFound\nand SExtractor were incapable of detecting the outskirts of ob-\njects with any degree of accuracy. NoiseChisel and MTObjects\nwere both much more accurate at ﬁnding these fainter regions,\nbut both had other di ﬃculties – the ‘Segment’ tool used in\nNoiseChisel divided detected light into apparently arbitrary re-\ngions, whilst MTObjects produced extremely ragged edges and\nhad a tendency to over-allocate faint regions to the brightest ob-\njects. NoiseChisel also produced the most accurate background\nvalues.\nWe found that there appears to be a trade-o ﬀ between\nspeed and accurate detection of objects’ outskirts. SExtractor\nwas capable of the highest speeds by a substantial margin, but\nwas unable to accurately detect faint regions. MTObjects and\nNoiseChisel were both able to detect these regions, but at the cost\nof processing speed. There may potentially be improvements to\nbe made on both tools by increased parallelisation and optimisa-\ntion of the code.\nA common weakness in the tools was in accurately deblend-\ning nested objects. The MTObjects approach, using tree-based\nconnected morphological ﬁlters (Salembier & Wilkinson 2009)\ndeals relatively well when small, faint objects are nested within\nlarger, brighter ones, but performs poorly when more similar ob-\njects overlap. In the latter case, the other methods, which are\ngenerally based on a form of watershed segmentation (Beucher\n1982; Roerdink & Meijster 2000) might give a better result. This\nis a non-trivial problem, which merits further investigation.\nMTObjects was the only tool to ﬁnd stable parameters across\nmultiple optimisations, suggesting that it requires the least ad-\njustment for individual images, and may be the best-suited for\nuse in automatic pipelines. Furthermore, in the test on simu-\nlated data, it consistently outperformed the other methods, re-\nArticle number, page 20 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Left: gri-composite image. Right: g-band input image in log scale.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentation maps, using the parameters which gave the highest median score for each combination of optimi-\nsation measure and tool. For more information, see Fig. 10.\nFig. 19: Results for IAC Stripe82 ﬁeld f0363_g.rec.fits showing a large structure of galactic cirri.\ngardless of the quality measure used. The likelihood of ranking\nin ﬁrst place out of four, in the case of F-score and area score,\nin ten tests, is about 10−6 under the null hypothesis that all tools\nhave equal performance. Despite the modest performance mar-\ngin with respect to the others, the result is statistically signiﬁcant.\nWe found that the optimisation criteria must be chosen care-\nfully in order to produce useful parameters. In particular, we\nfound that optimising for area alone caused a substantial drop\nin accuracy for SExtractor and ProFound, whereas combining\nmultiple criteria yielded more meaningful results.\nAs discussed in the introduction, the growth of the scale of\nmodern surveys means that there is a need for segmentation tools\nwhich are fast, automatic, and accurate. We found that of the\ntools tested, MTObjects was capable of the highest scores on\nboth area and detection measures, and had the most consistent\nparameters, whilst SExtractor obtained the highest speeds, but\nwith much lower accuracy. As noted earlier, a faster implemen-\ntation of MTObjects already exists, and the developers of Pro-\nFound are rewriting parts of their tool to improve its speed.\nArticle number, page 21 of 34\nA&A proofs:manuscript no. ms\n(a) Left: gri-composite image. Right: g-\nband input image in log scale.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentation maps, using the parameters which gave the highest median score for each combination of optimi-\nsation measure and tool. For more information, see Fig. 10.\nFig. 20: Results for IAC Stripe82 ﬁeld cropped to show two interacting galaxies (SDSS J031943.04+003355.64 and SDSS\nJ031947.01+003504.44).\nArticle number, page 22 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Left: gri-composite image. Right: g-band input image in log scale.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentation maps, using the parameters which gave the highest median score for each combination of optimi-\nsation measure and tool. For more information, see Fig. 10.\nFig. 21: Results for IAC Stripe82 ﬁeld zoomed in on elliptical galaxy with an extended, very faint tidal stream (SDSS\nJ235618.80-001820.17) and a bright collection of stars with a signiﬁcant amount of scattered light contaminating the galaxy\nfrom the right.\nIn addition to this analysis, we have presented a framework\nfor automated parameter setting and evaluation of astronomical\nsource detection tools, which is generic, and can be used with\nany other quality measure or model ground truth. This procedure\ncan in the future be used to analyse improvements to existing\ntools, as well as evaluating the capabilities of future techniques.\n7. Acknowledgements\nWe acknowledge ﬁnancial support from the European Union’s\nHorizon 2020 research and innovation programme under Marie\nSkłodowska-Curie grant agreement No 721463 to the SUNDIAL\nITN network.\nNC acknowledges support from the State Research Agency\n(AEI) of the Spanish Ministry of Science, Innovation and Uni-\nversities (MCIU) and the European Regional Development Fund\n(FEDER) under the grant with reference AY A2016-76219-P, and\nfrom the Fundación BBV A under its 2017 programme of as-\nsistance to scientiﬁc research groups, for the project \"Using\nmachine-learning techniques to drag galaxies from the noise in\ndeep imaging\". The IAC project P /300724 is ﬁnanced by the\nMinistry of Science, Innovation and Universities, through the\nState Budget and by the Canary Islands Department of Econ-\nomy, Knowledge and Employment, through the Regional Budget\nof the Autonomous Community.\nA V acknowledges ﬁnancial support from the Emil Aaltonen\nFoundation.\nArticle number, page 23 of 34\nA&A proofs:manuscript no. ms\n(a) Input image – the v-band of the\nﬁeld.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld, using the parameters which gave the highest median score for each combination of\noptimisation measure and tool. For more information, see Fig. 10.\nFig. 22: Segmentations of the rotated and cropped Hubble Ultra Deep Field.\nThe Dell R815 Opteron server was obtained through fund-\ning from the Netherlands Organisation for Scientiﬁc Research\n(NWO) under project number 612.001.110.\nThis research made use of Astropy,7 a community-developed\ncore Python package for Astronomy (Robitaille et al. 2013b)\n(Price-Whelan et al. 2018).\nThis work was partly done using GNU Astronomy Utili-\nties (Gnuastro, ascl.net /1801.009) version 0.7.42-22d2. Gnuas-\ntro is a generic package for astronomical data manipulation and\n7 http://www.astropy.org\nanalysis which was initially created and developed for research\nfunded by the Monbukagakusho (Japanese government) schol-\narship and European Research Council (ERC) advanced grant\n339659-MUSICOS.\nReferences\nAbbott, T., Abdalla, F., Allam, S., et al. 2018, The Astrophysical Journal Sup-\nplement Series, 239, 18\nAdelman-McCarthy, J. K., Agüeros, M. A., Allam, S. S., et al. 2006, The Astro-\nphysical Journal Supplement Series, 162, 38\nArticle number, page 24 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\n(a) Input image – the v-band of the\nﬁeld.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld, using the parameters which gave the highest median score for each combination of\noptimisation measure and tool. For more information, see Fig. 10.\nFig. 23: Segmentations of a section of the Hubble Ultra Deep Field.\nAkhlaghi, M. & Ichikawa, T. 2015, The Astrophysical Journal Supplement Se-\nries, 220, 1\nAmiaux, J., Scaramella, R., Mellier, Y ., et al. 2012, in Space Telescopes and\nInstrumentation 2012: Optical, Infrared, and Millimeter Wave, V ol. 8442, In-\nternational Society for Optics and Photonics, 84420Z\nBaillard, A., Bertin, E., De Lapparent, V ., et al. 2011, Astronomy & Astro-\nphysics, 532, A74\nBeard, S., MacGillivray, H., & Thanisch, P. 1990, Monthly Notices of the Royal\nAstronomical Society, 247, 311\nBeckwith, S. V ., Stiavelli, M., Koekemoer, A. M., et al. 2006, The Astronomical\nJournal, 132, 1729\nBertin, E. 2006, Institut d’Astrophysique & Observatoire de Paris\nBertin, E. & Arnouts, S. 1996, Astronomy and Astrophysics Supplement Series,\n117, 393\nBeucher, S. 1982, in ICASSP ’82. IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, V ol. 7, 1928–1931\nBorlaﬀ, A., Trujillo, I., Román, J., et al. 2019, Astronomy & Astrophysics, 621,\nA133\nBoucaud, A., Heneka, C., Ishida, E. E., et al. 2020, Monthly Notices of the Royal\nAstronomical Society, 491, 2481\nBouwens, R. J., Illingworth, G. D., Franx, M., & Ford, H. 2008, The astrophysi-\ncal journal, 686, 230\nCarlinet, E. & Géraud, T. 2014, IEEE Transactions on Image Processing, 23,\n3885\nFliri, J. & Trujillo, I. 2016, Monthly Notices of the Royal Astronomical Society,\n456, 1359\nArticle number, page 25 of 34\nA&A proofs:manuscript no. ms\n(a) Input image – the v-band of the\nﬁeld.\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b) Segmentations of the ﬁeld, using the parameters which gave the highest median score for each combination of\noptimisation measure and tool. For more information, see Fig. 10.\nFig. 24: Segmentations of a face-on spiral in the Hubble Ultra Deep Field.\nGNU Astronomy Utilities. 2019, NoiseChisel optimization,\nhttps://www.gnu.org/software/gnuastro/manual/html_node/NoiseChisel-\noptimization.html, accessed: 2019-08-15\nGoodman, A. A., Rosolowsky, E. W., Borkin, M. A., et al. 2009, Nature, 457, 63\nHoulahan, P. & Scalo, J. 1992, The Astrophysical Journal, 393, 172\nHuang, S., Leauthaud, A., Murata, R., et al. 2018, Publications of the Astronom-\nical Society of Japan, 70, S6\nIodice, E., Capaccioli, M., Grado, A., et al. 2016, Astrophysical Journal, 820, 42\nIodice, E., Spavone, M., Capaccioli, M., et al. 2019, Astronomy and Astro-\nphysics, 623, A1\nIvezi´c, Z., Tyson, J., Abel, B., et al. 2008, arXiv preprint arXiv:0805.2366\nJones, D. R., Schonlau, M., & Welch, W. J. 1998, Journal of Global optimization,\n13, 455\nLevine, M. D. & Nazif, A. 1981, An experimental rule-based system for testing\nlow level segmentation strategies (McGill Univ.)\nMasias, M., Freixenet, J., Lladó, X., & Peracaula, M. 2012, Monthly Notices of\nthe Royal Astronomical Society, 422, 1674\nMelchior, P., Moolekamp, F., Jerdee, M., et al. 2018, Astronomy and Computing,\n24, 129\nMoschini, U., Meijster, A., & Wilkinson, M. H. F. 2018, IEEE Transactions on\nPattern Analysis Machine Intelligence, 40, 513\nOesch, P., Bouwens, R. J., Carollo, C. M., et al. 2009, The Astrophysical Journal\nLetters, 709, L21\nOuzounis, G. K. & Wilkinson, M. H. 2010, IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 33, 224\nPal, N. R. & Pal, S. K. 1993, Pattern recognition, 26, 1277\nPratt, N. 1977, Vistas in Astronomy, 21, 1\nArticle number, page 26 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nPrice-Whelan, A., Sip ˝ocz, B., Günther, H., et al. 2018, The Astronomical Jour-\nnal, 156, 123\nProle, D. J., Davies, J. I., Keenan, O. C., & Davies, L. J. 2018, Monthly Notices\nof the Royal Astronomical Society, 478, 667\nReiman, D. M. & Göhre, B. E. 2019, Monthly Notices of the Royal Astronomical\nSociety, 485, 2617\nRix, H.-W., Barden, M., Beckwith, S. V ., et al. 2004, The Astrophysical Journal\nSupplement Series, 152, 163\nRobitaille, T., Beaumont, C., McDonald, B., & Rosolowsky, E. 2013a, Astro-\ndendro, a Python package to compute dendrograms of astronomical data.,\nhttp://www.dendrograms.org/, accessed: 2019-08-15\nRobitaille, T. P., Tollerud, E. J., Greenﬁeld, P., et al. 2013b, Astronomy & Astro-\nphysics, 558, A33\nRobotham, A., Davies, L., Driver, S., et al. 2018, Monthly Notices of the Royal\nAstronomical Society, 476, 3137\nRoerdink, J. B. T. M. & Meijster, A. 2000, Fundamenta Informaticae, 41, 187\nRomán, J. & Trujillo, I. 2017a, Monthly Notices of the Royal Astronomical So-\nciety, 468, 703\nRomán, J. & Trujillo, I. 2017b, Monthly Notices of the Royal Astronomical So-\nciety, 468, 4039\nRomán, J. & Trujillo, I. 2018, Research Notes of the American Astronomical\nSociety, 2, 144\nRomán, J., Trujillo, I., & Montes, M. 2019, arXiv preprint arXiv:1907.00978\nRosolowsky, E., Pineda, J., Kau ﬀmann, J., & Goodman, A. 2008, The Astro-\nphysical Journal, 679, 1338\nSalembier, P., Oliveras, A., & Garrido, L. 1998, IEEE Transactions on Image\nProcessing, 7, 555\nSalembier, P. & Wilkinson, M. H. F. 2009, IEEE Signal Processing Magazine,\n26, 136\nSersic, J. L. 1968, Cordoba, Argentina: Observatorio Astronomico, 1968\nSimet, M. & Mandelbaum, R. 2015, Monthly Notices of the Royal Astronomical\nSociety, 449, 1259\nTeeninga, P., Moschini, U., Trager, S. C., & Wilkinson, M. H. F. 2013, in 11th\nInternational Conference \"Pattern Recognition and Image Analysis: New In-\nformation Technologies\" (PRIA-11-2013), IPSI RAS, 746–749\nTeeninga, P., Moschini, U., Trager, S. C., & Wilkinson, M. H. F. 2016, Mathe-\nmatical Morphology - Theory and Applications, 1, 100\nThe GPyOpt authors. 2016, GPyOpt: A Bayesian Optimization framework in\nPython, http://github.com/SheﬃeldML/GPyOpt\nVenhola, A. 2019, PhD thesis, University of Groningen,\nhttp://hdl.handle.net/11370/1bcc02c2-2c78-4cﬀ-b801-147c31b000a8\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2018, Astronomy & Astro-\nphysics, 620, A165\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2019, Astronomy & Astro-\nphysics, 625, A143\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2017, Astronomy and Astro-\nphysics, 608, A142\nWilkinson, M. H. F. 1998, in Digital Image Analysis of Microbes, ed. M. H. F.\nWilkinson & F. Schut (Chichester, UK: John Wiley and Sons, Ltd), 135–171\nArticle number, page 27 of 34\nA&A proofs:manuscript no. ms\nAppendix A: SExtractor ﬁlters\nSExtractor uses a ﬁlter to pre-process the input image. A number\nof ﬁlters are provided with the tool, but custom ﬁlters may also\nbe used. The SExtractor manual suggests that the symmetrical\nPSF of the data is an optimal ﬁlter for detecting stars (Bertin\n2006), whilst documentation provided with the ﬁlters suggests\nthat gaussian or top-hat ﬁlters are eﬀective in detecting extended,\nlow-surface brightness objects.\nAs the range of valid ﬁlters is inﬁnite, it would not be fea-\nsible to optimise the ﬁlter in addition to the other parameters.\nAccordingly, we used the default ﬁlter throughout the main ex-\nperiments of the paper. We subsequently tested a subset of the\navailable ﬁlters to determine if they had a signiﬁcant e ﬀect on\nthe tool’s performance:\n– Default – 3 ×3 pyramidal function (approximating gaussian\nsmoothing).\n– Gaussian – 9 ×9 gaussian PSF with a full width at half max-\nimum of 5 pixels.\n– PSF – 9×9 symmetrical window of the PSF of the simulated\nimages.\n– Top-hat – 5 ×5 top-hat PSF.\nWe optimised SExtractor’s parameters for Combined A score\nfor as described in Sect. 3.2. Fig. A.1 shows the distribution of\nF-scores and area scores for each of the four ﬁlters.\nWe found that the di ﬀerent ﬁlters had very little di ﬀerence\non F-score, but that there was a slightly higher area-score on av-\nerage when using the gaussian ﬁlter as compared to the default.\nWhilst the gaussian ﬁlter could therefore be recommended in this\nsituation, the choice of ﬁlter had no eﬀect on the overall conclu-\nsions – as shown in Fig. 8, both MTObjects and NoiseChisel\nachieved substantially higher area scores of 0.4-0.6 compared to\nSExtractor’s scores of 0.1-0.25. Plotting the four SExtractor ﬁl-\nters on the same axes as Fig. 8 shows the relative similarity of\nthe scores, as in Fig. A.2\nAppendix B: Optimised parameter tables\nParameter sets marked with an asterisk produced the highest me-\ndian test score for their optimisation metric and tool.\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775F-score\nDefault\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nGaussian\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775F-score\nPSF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nTop-hat\n(a) F-scores grouped by image used to optimise parameters\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24Area score\nDefault\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nGaussian\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24Area score\nPSF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nTop-hat\n(b) Area scores grouped by image used to optimise parameters\nFig. A.1: Optimised test distributions. Each tool’s parameters\nwere optimised for Combined A score on each of the ten im-\nages, and evaluated on the remaining nine images. Boxes extend\nfrom ﬁrst (Q1) to third (Q3) quartiles of the results, with median\nvalues marked; whiskers extend to the furthest F-score less than\n1.5 ∗(Q3 −Q1) from each end of the box.\nArticle number, page 28 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nTable B.1: Optimised parameters – Source Extractor\nMode Image BACK SIZE BACK\nFILTERSIZE\nDEBLEND MIN-\nCONT\nDEBLEND\nNTHRESH\nDETECT\nMINAREA\nDETECT\nTHRESH\n0 1 65 2 1.00E-03 33 6 1.45E +00\n0 2 64 2 1.00E-03 34 9 1.35E +00\n0 3 65 2 1.00E-03 35 5 1.59E +00\n0 4 64 3 1.00E-03 33 5 1.48E +00\n0 5 64 2 1.00E-03 31 5 1.51E +00\n0 6 37 5 1.00E-03 23 7 1.46E +00\n0 7 65 2 1.00E-03 33 5 1.78E +00\n0* 8 63 3 1.00E-03 30 5 1.43E +00\n0 9 63 3 1.00E-03 32 6 1.69E +00\n0 10 17 7 1.73E-02 62 8 1.28E +00\n1 1 88 5 1.00E-03 21 8 1.00E-01\n1 2 36 9 1.00E-03 8 17 1.00E-01\n1 3 63 5 9.82E-02 32 4 1.00E-01\n1 4 22 7 9.81E-02 3 6 1.00E-01\n1* 5 107 5 1.00E-03 41 30 1.00E-01\n1 6 24 11 1.00E-01 8 49 1.00E-01\n1 7 111 6 2.12E-02 6 14 1.05E-01\n1 8 108 3 1.00E-03 26 34 1.00E-01\n1 9 35 5 4.40E-02 48 28 1.13E-01\n1 10 80 4 3.67E-02 23 2 1.06E-01\n2 1 84 7 6.96E-02 44 36 6.56E-01\n2 2 29 7 1.82E-02 3 49 5.96E-01\n2 3 105 6 1.00E-03 63 37 6.03E-01\n2 4 110 11 1.00E-03 60 32 7.28E-01\n2 5 98 9 1.00E-03 40 28 7.07E-01\n2 6 33 8 1.00E-03 47 29 6.19E-01\n2 7 124 10 1.00E-03 50 29 1.24E +00\n2* 8 98 7 1.00E-03 29 46 5.70E-01\n2 9 85 4 1.00E-03 44 22 8.94E-01\n2 10 118 3 6.87E-03 41 9 1.07E +00\n3 1 123 7 1.00E-03 60 43 3.93E-01\n3 2 71 5 2.53E-02 23 41 4.91E-01\n3 3 119 2 2.33E-02 23 44 4.94E-01\n3 4 72 11 1.00E-03 55 47 5.23E-01\n3 5 61 7 9.62E-02 53 43 5.35E-01\n3 6 101 3 1.00E-03 38 41 5.30E-01\n3* 7 119 6 1.00E-03 20 49 4.37E-01\n3 8 104 7 1.00E-03 49 37 5.11E-01\n3 9 25 8 1.07E-02 39 25 6.56E-01\n3 10 88 4 1.00E-03 49 16 8.23E-01\nArticle number, page 29 of 34\nA&A proofs:manuscript no. ms\nTable B.2: Optimised parameters – Noise Chisel (Noise Chisel)\nMode Image tilesize qthresh snquant detgrow-\nquant\ndthresh erode opening detgrow-\nmaxholesize\nmeanmed-\nqdiﬀ\nerode-\nngb\nopening-\nngb\nminsky- frac noerode-\nquant\n0 1 27 4.99E-01 9.99E-01 9.99E-01 0.00E +00 3 4 100 5.00E-03 4 4 8.00E-01 7.00E-01\n0 2 90 4.99E-01 9.71E-01 9.99E-01 6.34E-01 4 3 52 2.00E-02 4 4 7.54E-01 1.00E +00\n0 3 50 4.00E-01 9.27E-01 9.98E-01 8.26E-01 5 4 19 2.00E-02 4 4 6.62E-01 9.73E-01\n0* 4 26 4.99E-01 9.99E-01 9.86E-01 4.03E-01 3 1 59 2.00E-02 4 8 8.00E-01 1.00E +00\n0 5 49 4.99E-01 6.12E-01 9.46E-01 6.87E-01 1 5 77 1.98E-02 8 8 8.00E-01 7.02E-01\n0 6 40 4.99E-01 9.99E-01 9.99E-01 3.19E-01 10 1 86 2.00E-02 4 4 8.00E-01 1.00E +00\n0 7 87 4.33E-01 9.71E-01 9.99E-01 5.20E-01 6 2 14 6.30E-03 8 8 8.00E-01 9.79E-01\n0 8 26 2.00E-01 9.99E-01 9.99E-01 0.00E +00 2 5 48 2.00E-02 4 4 8.00E-01 9.48E-01\n0 9 98 4.99E-01 6.00E-01 9.99E-01 5.39E-01 8 4 88 5.00E-03 8 8 4.00E-01 7.00E-01\n0 10 87 4.99E-01 9.99E-01 8.28E-01 1.67E-01 5 2 26 2.00E-02 4 8 6.03E-01 7.00E-01\n1* 1 75 3.69E-01 9.99E-01 6.00E-01 1.00E +00 6 5 79 5.00E-03 4 8 4.00E-01 7.75E-01\n1 2 40 4.99E-01 9.99E-01 6.00E-01 1.00E +00 1 5 25 2.00E-02 4 8 4.00E-01 1.00E +00\n1 3 27 4.99E-01 6.00E-01 6.00E-01 0.00E +00 1 5 85 1.60E-02 8 8 4.56E-01 1.00E +00\n1 4 86 4.99E-01 9.99E-01 6.00E-01 1.00E +00 6 3 32 2.00E-02 4 8 8.00E-01 1.00E +00\n1 5 34 4.99E-01 6.00E-01 6.00E-01 0.00E +00 5 2 97 8.07E-03 4 4 4.00E-01 1.00E +00\n1 6 28 2.00E-01 6.00E-01 6.19E-01 6.86E-01 2 1 94 1.97E-02 4 8 4.00E-01 1.00E +00\n1 7 57 2.00E-01 9.99E-01 6.00E-01 0.00E +00 10 5 58 5.00E-03 8 4 4.00E-01 7.00E-01\n1 8 33 4.99E-01 6.15E-01 6.15E-01 1.69E-13 5 1 65 2.00E-02 8 4 8.00E-01 1.00E +00\n1 9 96 3.64E-01 9.28E-01 6.03E-01 7.15E-01 1 4 63 1.03E-02 8 8 6.87E-01 9.27E-01\n1 10 43 3.74E-01 8.28E-01 6.54E-01 8.98E-01 1 5 16 1.14E-02 4 8 4.98E-01 1.00E +00\n2* 1 68 3.26E-01 6.00E-01 6.11E-01 6.85E-01 8 5 68 2.00E-02 4 8 5.16E-01 8.05E-01\n2 2 30 4.99E-01 6.00E-01 6.00E-01 6.36E-01 2 2 67 1.06E-02 8 8 8.00E-01 1.00E +00\n2 3 27 3.46E-01 9.99E-01 6.09E-01 0.00E +00 9 2 55 1.98E-02 4 4 4.94E-01 8.84E-01\n2 4 28 4.99E-01 6.00E-01 6.16E-01 0.00E +00 4 3 80 2.00E-02 8 4 4.00E-01 9.03E-01\n2 5 89 2.00E-01 6.58E-01 6.18E-01 7.90E-01 5 2 8 2.00E-02 8 8 8.00E-01 7.09E-01\n2 6 29 2.91E-01 9.99E-01 6.10E-01 1.19E-02 3 1 68 2.00E-02 4 4 5.01E-01 1.00E +00\n2 7 30 3.50E-01 6.00E-01 6.00E-01 0.00E +00 8 5 28 7.24E-03 8 8 6.97E-01 7.00E-01\n2 8 26 2.00E-01 6.00E-01 6.00E-01 2.73E-02 9 5 23 2.00E-02 8 8 5.20E-01 7.00E-01\n2 9 45 2.00E-01 8.07E-01 7.21E-01 7.51E-01 7 2 76 8.42E-03 8 8 7.46E-01 7.88E-01\n2 10 82 4.42E-01 6.87E-01 6.82E-01 5.27E-01 6 2 38 2.00E-02 4 8 7.44E-01 9.14E-01\n3 1 27 2.04E-01 9.08E-01 6.03E-01 0.00E +00 4 4 23 2.00E-02 8 4 7.20E-01 7.68E-01\n3 2 64 4.99E-01 6.00E-01 6.00E-01 7.87E-01 4 2 41 7.97E-03 8 8 4.00E-01 7.00E-01\n3 3 22 2.00E-01 9.98E-01 6.02E-01 1.57E-03 1 5 48 2.00E-02 8 8 4.02E-01 7.01E-01\n3 4 20 2.00E-01 9.99E-01 6.00E-01 0.00E +00 1 5 76 5.00E-03 8 8 4.00E-01 7.00E-01\n3 5 41 4.99E-01 9.50E-01 6.00E-01 2.16E-01 8 1 90 5.00E-03 4 8 4.00E-01 1.00E +00\n3 6 82 4.67E-01 9.96E-01 6.37E-01 1.07E-02 8 1 38 1.09E-02 8 4 8.00E-01 9.56E-01\n3 7 63 4.82E-01 6.93E-01 6.38E-01 5.90E-01 7 2 85 1.89E-02 8 8 4.80E-01 8.70E-01\n3 8 31 4.99E-01 7.88E-01 6.29E-01 5.51E-01 9 5 29 2.00E-02 4 4 5.33E-01 9.86E-01\n3* 9 71 2.00E-01 9.99E-01 6.00E-01 0.00E +00 1 5 11 2.00E-02 4 8 4.00E-01 7.00E-01\n3 10 100 3.44E-01 9.99E-01 6.00E-01 0.00E +00 9 5 5 2.00E-02 4 4 4.00E-01 7.28E-01\nArticle number, page 30 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nTable B.3: Optimised parameters – Noise Chisel (Segment)\nMode Image tilesize snquant gthresh snminarea minriver-\nlength\nobjbordersn minskyfrac\n0 1 72 9.99E-01 1.00E +00 25 23 1.28E +01 4.00E-01\n0 2 90 9.99E-01 1.00E +00 18 12 1.86E +01 5.75E-01\n0 3 93 9.94E-01 8.65E-01 23 20 3.18E +01 6.67E-01\n0* 4 60 9.99E-01 6.69E-01 25 9 2.56E +01 7.52E-01\n0 5 20 9.99E-01 2.44E-01 20 40 2.00E +01 8.00E-01\n0 6 20 9.99E-01 1.00E +00 19 5 1.12E +01 8.00E-01\n0 7 29 9.99E-01 1.00E +00 21 37 1.49E +01 7.28E-01\n0 8 72 9.99E-01 1.00E +00 20 40 3.16E +01 8.00E-01\n0 9 45 9.99E-01 0.00E +00 20 14 2.26E +01 8.00E-01\n0 10 91 9.99E-01 3.77E-01 25 25 1.51E +01 4.00E-01\n1* 1 65 9.99E-01 5.41E-01 10 34 5.00E-01 4.00E-01\n1 2 85 9.99E-01 1.00E +00 25 35 1.73E +01 8.00E-01\n1 3 35 9.99E-01 0.00E +00 15 21 1.88E +01 4.00E-01\n1 4 65 9.99E-01 1.00E +00 23 8 1.80E +01 7.43E-01\n1 5 32 9.99E-01 6.34E-01 15 14 1.34E +00 8.00E-01\n1 6 41 9.99E-01 4.41E-01 21 36 1.15E +01 5.99E-01\n1 7 52 9.99E-01 0.00E +00 25 40 1.81E +01 4.00E-01\n1 8 46 9.99E-01 2.85E-02 25 40 2.12E +01 8.00E-01\n1 9 82 9.97E-01 5.72E-01 20 23 3.45E +01 7.81E-01\n1 10 61 9.99E-01 2.14E-01 17 31 8.88E +00 5.64E-01\n2* 1 25 9.99E-01 3.28E-01 25 24 3.11E +01 4.90E-01\n2 2 45 9.99E-01 4.89E-01 24 21 1.77E +01 4.00E-01\n2 3 80 9.99E-01 6.60E-03 22 30 5.88E +00 4.09E-01\n2 4 81 9.99E-01 0.00E +00 22 8 1.03E +01 8.00E-01\n2 5 31 9.99E-01 2.48E-01 22 16 7.32E +00 4.00E-01\n2 6 74 9.99E-01 9.37E-03 25 22 6.71E +00 4.10E-01\n2 7 38 9.99E-01 2.11E-01 20 31 1.29E +01 4.00E-01\n2 8 20 9.99E-01 0.00E +00 25 33 7.35E +00 4.00E-01\n2 9 90 9.99E-01 8.11E-01 14 27 3.96E +01 4.00E-01\n2 10 48 9.99E-01 6.99E-01 18 15 2.60E +01 6.55E-01\n3 1 72 9.99E-01 5.36E-16 24 13 1.29E +01 4.03E-01\n3 2 94 9.99E-01 8.28E-01 25 30 2.42E +01 4.00E-01\n3 3 31 9.97E-01 3.17E-02 25 5 2.21E +01 4.02E-01\n3 4 20 9.99E-01 1.00E +00 25 11 1.63E +01 4.00E-01\n3 5 32 9.99E-01 0.00E +00 25 25 1.81E +01 8.00E-01\n3 6 20 9.99E-01 3.93E-01 22 26 9.46E +00 6.79E-01\n3 7 78 9.99E-01 5.50E-02 24 21 2.89E +01 4.52E-01\n3 8 47 9.99E-01 6.88E-01 16 16 3.86E +01 8.00E-01\n3* 9 74 9.99E-01 0.00E +00 25 11 2.38E +01 8.00E-01\n3 10 32 9.99E-01 2.18E-01 23 18 1.11E +01 5.88E-01\nArticle number, page 31 of 34\nA&A proofs:manuscript no. ms\nTable B.4: Optimised parameters – MTObjects\nMode Image move_factor min_dist\n0 1 4.96E-02 1.49E-01\n0 2 6.14E-02 1.15E-01\n0 3 5.73E-02 1.57E-01\n0 4 8.24E-03 1.32E-01\n0 5 1.21E-01 1.45E-01\n0 6 0 1.50E-01\n0* 7 0 1.13E-01\n0 8 9.34E-02 1.55E-01\n0 9 1.13E-01 1.33E-01\n0 10 3.07E-02 1.17E-01\n1* 1 0 0\n1 2 0 0\n1 3 0 0\n1 4 0 0\n1 5 0 0\n1 6 0 0\n1 7 0 0\n1 8 0 0\n1 9 0 0\n1 10 0 0\n2* 1 0 0\n2 2 0 0\n2 3 0 0\n2 4 0 0\n2 5 0 0\n2 6 0 0\n2 7 0 0\n2 8 0 0\n2 9 0 0\n2 10 0 0\n3* 1 0 0\n3 2 0 0\n3 3 0 0\n3 4 0 0\n3 5 0 0\n3 6 0 0\n3 7 0 0\n3 8 9.36E-03 0\n3 9 0 0\n3 10 0 0\nArticle number, page 32 of 34\nCaroline Haigh et al.: Optimising and comparing source extraction tools using objective segmentation quality criteria\nTable B.5: Optimised parameters – ProFound\nMode Image skycut tolerance ext sigma pixcut size iters threshold\n0 1 7.22E-01 3.71E +00 1.92E +00 2.22E +00 4 5 7 7.50E-01\n0 2 9.34E-01 2.06E +00 2.29E +00 1.35E +00 10 7 2 8.26E-01\n0 3 1.00E +00 1.00E +00 7.03E +00 1.19E +00 7 7 4 7.50E-01\n0 4 8.23E-01 1.58E +00 8.03E +00 2.23E +00 6 7 3 1.20E +00\n0 5 1.22E +00 3.52E +00 3.89E +00 9.17E-01 6 5 6 1.78E +00\n0 6 6.54E-01 1.00E +00 3.78E +00 3.00E +00 15 7 9 7.50E-01\n0 7 9.34E-01 1.00E +00 4.55E +00 1.43E +00 16 5 0 1.96E +00\n0* 8 5.56E-01 2.03E +00 3.28E +00 2.11E +00 15 7 6 9.49E-01\n0 9 1.79E +00 3.90E +00 3.29E +00 8.76E-01 4 5 7 1.35E +00\n0 10 6.25E-01 2.93E +00 4.33E +00 1.36E +00 14 9 8 2.00E +00\n1 1 5.26E-01 5.29E +00 2.22E +00 1.98E +00 7 9 7 8.68E-01\n1 2 2.38E-01 2.62E +00 9.01E +00 2.21E +00 11 7 6 1.12E +00\n1 3 7.65E-01 5.71E +00 2.70E +00 1.45E +00 5 5 8 8.42E-01\n1 4 6.13E-01 4.44E +00 4.77E +00 2.83E +00 5 7 7 8.00E-01\n1 5 1.57E-01 3.72E +00 4.23E +00 2.74E +00 10 9 6 1.24E +00\n1 6 2.77E-01 2.49E +00 8.57E +00 2.26E +00 15 7 9 8.31E-01\n1 7 6.23E-01 3.68E +00 5.18E +00 1.01E +00 1 5 4 1.04E +00\n1* 8 6.45E-01 6.00E +00 8.97E +00 1.13E +00 14 9 9 9.10E-01\n1 9 2.82E-01 4.48E +00 6.28E +00 2.07E +00 3 7 4 7.50E-01\n1 10 1.33E +00 3.23E +00 3.63E +00 1.82E +00 9 7 9 8.23E-01\nArticle number, page 33 of 34\nA&A proofs:manuscript no. ms\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nCombined A\nSE+Default\nSE+Gaussian\nSE+PSF\nSE+Top-hat\nNC\nMT\nFig. A.2: F-score vs area score – each tool’s parameters were\noptimised for Combined A score on each of the ten images, and\nevaluated on the remaining nine images.\nArticle number, page 34 of 34",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I169381384",
      "name": "University of Groningen",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I161593684",
      "name": "Stockholm University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210135511",
      "name": "AlbaNova",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I158438070",
      "name": "Universidad de La Laguna",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210130807",
      "name": "Instituto de Astrofísica de Canarias",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I98381234",
      "name": "University of Oulu",
      "country": "FI"
    }
  ],
  "cited_by": 30
}