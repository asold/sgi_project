{
  "title": "Dual Use Concerns of Generative AI and Large Language Models",
  "url": "https://openalex.org/W4376653359",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2963914530",
      "name": "Grinbaum, Alexei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298392487",
      "name": "Adomaitis, Laurynas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1545223934",
    "https://openalex.org/W4287019820",
    "https://openalex.org/W4287109193",
    "https://openalex.org/W3187573115",
    "https://openalex.org/W2144551261",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2001590476",
    "https://openalex.org/W4376312605",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W581181895",
    "https://openalex.org/W2095818564",
    "https://openalex.org/W2081538851",
    "https://openalex.org/W2158112812",
    "https://openalex.org/W4394645559",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W576501181",
    "https://openalex.org/W4288091255",
    "https://openalex.org/W1908686216",
    "https://openalex.org/W4308411238",
    "https://openalex.org/W2800520656",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4200167541",
    "https://openalex.org/W4378627669",
    "https://openalex.org/W3025614179",
    "https://openalex.org/W4309633992",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W4221159674",
    "https://openalex.org/W3175765954",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4297677265",
    "https://openalex.org/W2171230567",
    "https://openalex.org/W1987602805",
    "https://openalex.org/W3209475483",
    "https://openalex.org/W4244988984",
    "https://openalex.org/W4308243057",
    "https://openalex.org/W2994592900",
    "https://openalex.org/W2104148197",
    "https://openalex.org/W2560400559",
    "https://openalex.org/W4388555465",
    "https://openalex.org/W4385573981",
    "https://openalex.org/W2124632436",
    "https://openalex.org/W4288057743",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2575653999",
    "https://openalex.org/W2112170460",
    "https://openalex.org/W2506282302",
    "https://openalex.org/W3089298226",
    "https://openalex.org/W4321437806",
    "https://openalex.org/W4361229539"
  ],
  "abstract": "We suggest the implementation of the Dual Use Research of Concern (DURC) framework, originally designed for life sciences, to the domain of generative AI, with a specific focus on Large Language Models (LLMs). With its demonstrated advantages and drawbacks in biological research, we believe the DURC criteria can be effectively redefined for LLMs, potentially contributing to improved AI governance. Acknowledging the balance that must be struck when employing the DURC framework, we highlight its crucial political role in enhancing societal awareness of the impact of generative AI. As a final point, we offer a series of specific recommendations for applying the DURC approach to LLM research.",
  "full_text": "1 \nDual Use Concerns of Generative AI and Large Language Models \n \nAlexei Grinbaum \nalexei.grinbaum@cea.fr  \nCEA-Saclay/Larsim \nGif-sur-Yvette 91191 \n \nLaurynas Adomaitis \nlaurynas.adomaitis@cea.fr  \nCEA-Saclay/Larsim \nGif-sur-Yvette 91191  \n \nAbstract \nWe suggest the implementation of the Dual Use Research of Concern (DURC) framework, \noriginally designed for life sciences, to the domain of generative AI, with a specific focus on \nLarge Language Models (LLMs). With its demonstrated advantages and drawbacks in \nbiological research, we believe the DURC criteria can be effectively redefined for LLMs, \npotentially contributing to improved AI governance. Acknowledging the balance that must be \nstruck when employing the DURC framework, we highlight its crucial political role in \nenhancing societal awareness of the impact of generative AI.\n As a final point, we offer a \nseries of specific recommendations for applying the DURC approach to LLM research. \n \nKeywords: Dual Use Research of Concern (DURC), Generative AI, Large Language \nModels (LLMs), AI Ethics \n \nConflict of interest \nNo conflict of interest to report. \nFunding \nThis research was supported through projects TechEthos (grant number 101006249)  and \nMultiRATE (grant number 101073929) funded by the European Commission Horizon program. \nEthics approval \nNo human subjects were involved in the study. \nConsent \nNo data needing consent has been used. \nData availability statement \nIn this article, we do not analyze or generate any datasets. Our work proceeds within a \ntheoretical and philosophical approach. \nAuthor Contribution \nAll authors contributed to the study conception and design. Sections 1 and 4 were written with \nequal contribution. Sections 2 and 3 were conceived by Adomaitis and later edited by \nGrinbaum. Sections 5 and 6 were conceived by Grinbaum and later edited by Adomaitis. All \nauthors read and approved the final manuscript. \n  \n2 \n1. Introduction \n \nHigh-ranking authorities in Europe and the United States have recently expressed their \nconcern regarding the societal implications of generative artificial intelligence  (Coulter and \nMukherjee 2023; White House 2023a) . These concerns have quickly found their way into \nregulatory documents, e.g. the Executive Order signed by the United States President J. Biden \n(The White House 2023)  or the recently amended and agreed -upon European legislative \nproposal for the AI Act (Council of Europe 2023; European P arliament 2023b). In academic \ncircles, such worries have also been echoed by distinguished scientists, e.g. computer scientist \nGeoffrey Hinton compared the risks associated with generative AI to the regulation of \nbiological and chemical weapons by emphasizing that, although not “foolproof”, this regulation \ngenerally prevents their use (Heaven 2023). Perhaps the best examples  of publicly declared \nconcerns were, in early 2023, the call for a moratorium initiated by the Future of Life Institute \nand backed by figures such as Elon Musk; in late 2023, the “Bletchley Declaration” signed at \nthe UK summit on AI (AI Safety Summit 2023). A temporary halt on the development of large \nlanguage models (LLMs) “more powerful than GPT-4” (Future of Life Institute 2023) is not an \nisolated call, but part of a historical lineage of similar a ppeals in the face of emerging \ntechnologies, many falling under the concept of Dual Use Research of Concern (DURC). In \nthis paper, we argue that DURC is relevant and applicable to the domain of generative AI, \nespecially in relation to LLMs. \nIn Section 2, we introduce the concept of dual -use research, originally conceived in \nrelation to chemical and biological weapons . Despite its roots, we illustrate that DURC \nfundamentally differs from this early interpretation of dual -use. Between 2005 and 2021, \nDURC has been employed in areas such as gain -of-function biological research and gene \nediting, maintaining its relevance as a utilitarian governance approach for potentially high-risk \ninnovative biotechnological research. In Section 3, we propose the  application of the DURC \nframework to generative AI, including Large Language Models (LLMs). We redefine the \ntraditional DURC categories, typically used in biological research, to address dual-use concerns \nspecific to generative AI and LLMs. In Section 4, we examine how DURC frameworks could \nplay a role in establishing the shared responsibility of complex stakeholder networks that are \noften involved in the development and deployment of AI technologies. In Section 5, we explore \nthe advantages and potential l imitations of the DURC framework. Applying DURC to \ngenerative AI could heighten political awareness of its significant role  as a societal \ntransformation force. This symbolic value of DURC should not be underestimated, knowing \nthat scientific research and ambitious technological innovation are not going to be put to a halt. \nWe conclude in Section 6 by providing a series of specific recommendations for LLM research \nas an application of the DURC framework. \n \n2. What is Dual Use Research of Concern? \n \nIn some regulatory frameworks (e.g. Export Administration Regulations  2013; \nRegulation (EU) 2021/821 2021), the notions of “dual-use” and “misuse” refer to the interplay \nbetween civil and military research. Dual use concerns initially came to the public eye at  the \ntime of the Manhattan Project, which led both to nuclear energy production and to the atomic \n3 \nbomb. Already at the time, the dual nature of the implications of their research plagued scientists \nand raised questions about what we now call “open scienc e” (Schweber 2013) . In \nbiotechnology, the awareness of the dual use situation began with the recombinant DNA \ntechnology in the 1970s. In 1975, the Asilomar conference proposed a moratorium on genetic \nengineering (Berg et al. 1975) . As we show below, this short -lived moratorium in biological \nresearch is similar to the current proposals for generative AI. In the later years, e.g. during the \n2001 anthrax attacks in the United States, the dual-use debate went on to address concerns about \npotential use of research for bioterrorism (Atlas 2002). Once more, this is similar to the current \ndebate on the dual use of AI (Urbina 2022). \nThere exists a more general sense in which technologies have been called “dual use”, \nthe widest scope being “technologies that could be used for either good or bad purposes ” \n(Koplin 2023). This very broad definition seems to include numerous techniques and devices \nwith multiple uses. For example, a kitchen knife is a necessary tool in everyday life but it also \naccounts for at least half of knife crimes (Hern et al.  2005). The mere possibility of using a \ntechnology for bad purposes is not enough for ethical analysis ; one needs to consider  the \nresponsibility of the innovator and the imputability of the outcome . In the case of knife crime, \nit would be very strange to assign the responsibility to the manufacturer of the knife. Thus, the \nconcept of dual use needs to be defined more precisely.  \nOne possible criterion for classifying a technology as dual -use would be to require a \ndemonstrable potential for large-scale harm via malevolent or negligent use of this technology. \nThis introduces the aspects of scale and risk, as opposed to a mere possibility of one-shot effect. \nFor instance, prior to World War I, the deployment of biological agents in warfare was rare and \nsporadic, typically involving immediate  tactical measures such as well poisoning, launching \ndiseased cadavers into besieged cities, or distributing infected blankets  (Geissler and Moon \n1999). The specific worry about the military u se of toxins and pathogens emerged la ter, with \nresearch and application of typhoid and anthrax bacilli, cholera vibriones, dysentery bacteria, \nparatyphoid bacteria, and botulinus toxin, demonstrating the potential for large -scale harm . \nThese biological and toxin weapons have led to the Biological and Toxin Weapons Convention \n(BTWC) and various verification projects (e.g. VEREX). \nThe considerations of scope and risk make the notion of dual-use more precise, and they \npoint to specific frameworks  of evaluating and addressing the risks of ha rm as well as \nresponsibilities involved. In this paper, we choose to pursue an analogy between LLMs and an \nexisting dual-use technology with a considerable history of DURC governance.  \nThe current phase  of the dual-use debate in biology was initiated in 2005 with the \npublication of a study reconstructing the 1918 Spanish influenza virus. In a note added in a late-\nstage revision of this publication, the authors inserted a new statement clarifying the purpose \nof their research. They stated that their work was driven by “historical curiosity” but also added \na pragmatic goal: “The fundamental purpose of this work was to provide information critical to \nprotect public health and to develop measures effective against future influenza pandemics” \n(Tumpey et al. 2005). The belated addition of such a utilitarian objective gave rise to a lively \ndebate and a new wave of the DURC debate in biological research in the early 2000s. \nA few years later, a similar scenario unfolded regarding gain- of-function (GOF) \nresearch on the H5N1 bird influenza virus. Searching for  the genetic signature of pandemic \ntransmission capabilities, scientists altered the H5N1 virus to render it airborne transmissible \n4 \namong mammals (Herfst et al. 2012; Imai et al. 2012). Once more, they justified their creation \nof an entirely new and potentially pandemic virus as essential for public health protection – a \nutilitarian argument that sparked intense debate  in academic circles , with many scholars \nquestioning its validity  (Evans 2013; Lipsitch and Bloom 2012; Lipsitch and Galvani 2014) . \nCritics voiced concerns about the creation of high and unprecedented risks in the pursuit of \nscientific knowledge, leading to suggestions and implementation of a research moratorium and \npublishing restrictions  (Collins and Fauci 2012). Nonetheless, these publication restrictions \nonly lasted a few months and were lifted after the original article had been revised. The federal \nfunding freeze endured until 2017 and was lifted following the publication of the US regulatory \nframework on DURC (US HHS 2017). \nThis debate reveals a conflict between two paramount values  in responsible research \nand innovation: the pursuit of knowledge and the safeguarding of public safety. At its core, the \nconcept of DURC encapsulates a utilitarian dilemma, wherein a single scientific endeavor can \nsimultaneously pose significant security threats and yield vital societal benefits. This dilemma \nis not exclusive to the fields of chemistry or biology; it can extend to other research areas, \nincluding artificial intelligence. \nNowadays, t he most commonly accepted definition of DURC comes from the Fink \nReport: “Research that, on the basis of ‘state of the art and knowledge,’ could reasonably lead \nto knowledge, products or technologies that c ould be directly diverted and/ or pose a threat to \npublic health; agriculture, wildlife, flora, the environment and/ or national security” (National \nResearch Council 2004) . This definition has been adopted in 2007 by the National Security \nAdvisory Board for Biosecurity (NSABB), a body under the National Institutes of Health \ngoverning high risk in biological research (NSABB 2007) . While the Fink Report and the \nNSABB guidelines are primarily oriented towards life sciences, there is no inherent aspect in \nthe definition that would preclude its application to information technology and AI. \n \nTable 1. Distinction between the civil-military duality and dual use concerns in research. \nCivil-Military Applications Fink Report Dual Use \n§ 730.3 of US Export Administration Regulations \n(EAR): “A ‘dual-use’ item is one that has civil \napplications as well as terrorism and military or \nweapons of mass destruction (WMD)-related \napplications.” \n \n“The transfer from civil to military application \ninvolves a process in which social actors reinterpret \nthe purpose of a technology from a peaceful to a \nhostile context” (Tucker 2012, p. 30). \n“Research that, on the basis of ‘state of the art and \nknowledge,’ could reasonably lead to knowledge, \nproducts or technologies that could be directly \ndiverted and / or pose a threat to public health; \nagriculture, wildlife, flora, the environment and / or \nnational security” (National Research Council 2004). \n \nBoth the “civil  – military” dilemma and the “high risk – high benefit” utilitarian \ndilemma are called “dual use” in the literature. Here, we use the latter concept in line with recent \nscientific literature on DURC (Korn et al. 2019). There is no shortage of discussion of military \napplications of the life sciences, e.g. for making chemical weapons  or bioterrorism. Digital \ntechnologies, including AI, have also been widely discussed in the context of civil vs. military \napplication (Sanger 2023) . There are concerns that AI -fueled cyberspace skirmishes could \n5 \n“escalate into conventional warfare” (Taddeo and Floridi 2018), as well as numerous scenarios \nexplored for autonomous weapons and their role in warfare (Christie et al. 2023; Scharre 2018). \nWhile the military applications debate is broad and rich, there is little analysis of DURC aspects \nof generative AI research, and in particular of LLMs. However, in 2018, the Future of Humanity \nInstitute have found that then-current AI technologies expand existing threats, introduces new \nthreats, and changes the type of threats to the digital, physical, and political security of \nindividuals and nations (Brundage et al. 2018) . Five years later, OpenAI published AI \ngovernance commitments that heavily resound with concerns typical for dual -use research. \nTheir first commitment is to safety “ in areas including misuse, societal risks, and national \nsecurity concerns, such as bio, cyber, and other safety areas”, while they mention examples of \nways in which systems “ can lower barriers to entry for  [bio, chemical, and radiological ] \nweapons development, design, acquisition, or use ”  (OpenAI 2023a) . Similarly, Anthropic \nmade a dedicated study on the possibility of using LLMs for the design of biological weapons, \nhighlighting growing biosecurity risks (Anthropic 2023). This discussion of risks can be seen \nas setting the stage for considering generative AI as dual use. \n \n3. Applying DURC framework to generative AI and LLMs \n \nIn 2022 and 2023, the DURC approach is getting increasingly relevant to generative AI \nas LLMs are getting easier to build using standardized tools. This is similar to what happened \nin biology in the years following the publication of the Registry of Standard Biological Parts \n(Galdzicki et al. 2011) . This modular approach promoted do- it-yourself construction of \nbiological systems and gave easy access to the tools of synthetic biology. In generative AI, a \ngroup of researchers at Stanford recently published a method of obtaining a highly functioning \nmodel, known as Stanford Alpaca, which is “ surprisingly small and easy/cheap to reproduce” \n(Taori et al. 2023). Another group of researchers managed to bring down the price of fine tuning \nLLMs even further (R. Zhang et al. 2023) . Alphabet (formerly Google) has publ ished a small \nLLM, called Gemini Nano (Pichai and Hassabis 2023) , with the goal of running it on EDG E \ndevices with little computational power. \nThere is a clear trend indicating that high -performance, finely -tuned LLMs are \nbecoming increasingly available.\n Similar to the case of modular biological parts , LLMs can \nnow be created inexpensively by a growing number of researchers. This increased accessibility \namplifies the potential for misuse, necessitating the regulation of foundation models \n(Bommasani et al. 2022) due to their potential to spread misinformation, manipulate, influence, \nperpetrate scams, or generate toxic language. Specific social risks have been identified by LLM \ndevelopers (Weidinger et al. 2021) . While some of the identified risks can be emergent, i.e. \nunintended by the initial designers and arising from training, others can stem from poor design, \ne.g. insufficient mitigation against bias or poor elimination of toxic outputs. Further, output \ntoxicity may be highly context-dependent, e.g. generating a phrase calling someone a dog might \nbe an offense, or not, depending on the circumstances. This dependency on context is an \nintrinsic characteristic of several types of harmful language, including insults, medical advice, \netc. In this sense, LLMs present risks that are highly contextual. \nSome areas for potential misuse of LLMs carry a malevolent intention by design, e.g. \nthe user’s intention to make disinformation cheaper and more effective, or an intention to \n6 \nfacilitate fraud, scams and more tar geted manipulation, or to assist code generation  for cyber-\nattacks, creating weapons, or illegitimate surveillance and censorship (Weidinger et al. 2021). \nThese intentions can only come true in virtue of the LLM’s capacity to empower such types of \nuses, which underscores the LLM potential for misuse. This potential is always present in the \nmodel itself, even if the user is unaware of it. Furthermore, an even more concerning aspect of \nLLMs lies with the ‘unknown unknowns’, i.e. potential misuses that are not foreseen by current \nforesight and risk evaluation benchmarks (Tamkin et al. 2021) . This means that w hile recent \nefforts to mitigate the risks associated with LLMs have concentrated on specific types of harm, \nmore harmful behaviour may emerge from future use of LLMs. Thus we believe that powerful \ngenerative AI research, including on LLMs, should be classified under DURC. \nBefore exploring the parallels between DURC in biological research and generative AI, \nwe address an obvious difference between the two. Harmful biological agents usually exist in \nnature, whereas AI systems are works of human ingenuity. The former exist aut onomously, \nindependent of human influence, whereas the latter can be understood as agents only by \nprojection. One might be tempted to infer that the risk s stem from different sources : human \nintent versus naturally occurring life. However, the case of gain -of-function (GOF) research \nshows that this distinction is not necessarily valid: the variants of H5N1 virus manufactured for \nincreasing scientific knowledge did not previously exist in nature. These artificially engineered \nforms of bird flu were intentiona lly created  for research purposes , yet their creation has \nescalated pandemic risks. This example suggests that the boundary between life and technique \ndoes not imply an insurmountable difference between DURC categories in biology and in \ncomputer science. \nMoreover, LLMs possess unique characteristics that distinguish them from other digital \nsystems in terms of risk. First of all, they are considered a “generalist technology” (Vannuccini \nand Prytkova 2021), so instead of serving one purpose, they can generate an extensive range of \noutputs, which span across contexts  of application. This generalization, combined with their \nautonomy, can create risks that are far less predictable and more pervasive compared to \ntraditional digital systems. Secondly, LLMs have an exceptional ability to mimic human -like \nbehavior (speech and appearances, in case of multimodal systems)  and cognition. This poses \nunique risks for impersonation and individual exploitation. Lastly, LLMs accrue increasingly \nimportant roles in decision-making processes (logistics, healthcare, finance, etc.), which puts \nimmense pressure on their reliability. \nResearch on artificial agents, be they biological or digital, could be subject to DURC.   \nAs part of the definition and regulation of DURC, NSABB identified a set of seven research \ncategories as criteria for DURC (NSABB 2007). Although these categories have been devised \nfor the life sciences, we adapt and rephrase them for the use with generative AI. \n \nTable 2. NSABB dual use categories applied to generative AI, including LLMs. \nNo. NSABB Categories Digital Dual Use Research of Concern \n(1) Enhances the harmful \nconsequences of a \nbiological agent or toxin. \nWhile agency can be projected on AI systems by users, digital agents do \nnot preexist in nature and do not possess ontological harmful properties \nlike toxins. However, LLMs can be used for malicious activities, e.g. \ngenerating highly persuasive disinformation, creating deepfakes, or \n7 \nenhancing cyberattacks (C and J 2023; Gregory 2022; Ropek 2023). \nUnlike biological agents, LLMs can both give rise to such activities and \nbe used to improve the efficacy of human-designed activities with an \nexplicit malicious intention. \n(2) Disrupts immunity or the \neffectiveness of an \nimmunization without \nclinical and/or \nagricultural justification. \nRapid evolution of LLMs has drastically outpaced the development of \ncountermeasures, such as content verification tools, watermarks, or fact-\nchecking algorithms (Clark et al. 2021; Grinbaum and Adomaitis 2022b; \nHeikkilä 2022). It is increasingly challenging to distinguish between \ngenuine and artificial content, rendering existing content moderation and \nrecommendation systems ineffective (cf. “spin” attacks (Bagdasaryan \nand Shmatikov 2022)). LLMs can degrade the flow of language, \nincluding in important settings like computer code, legal texts, or medical \nstatements, by inserting erroneous but difficult-to-detect flaws. This is \nnot necessarily an intended purpose of LLM generation but an emergent \nproperty that is hard to control and thereby poses a significant threat. \n(3) Confers to a biological \nagent or toxin resistance \nto clinically and/or \nagriculturally useful \nprophylactic or \ntherapeutic interventions \nagainst that agent or toxin \nor facilitates its ability to \nevade detection \nmethodologies. \nLLMs facilitate unpredictable and/or undetectable behaviors of digital \nsystems. Transformer-based LLMs exhibit emergent behaviors without \nany obvious robust control mechanism (Wei et al. 2022). Models are \nbeing released without sufficient measures against model replication and \npotential inference attacks (Mireshghallah et al. 2022; Moradi and \nSamwald 2021). \n(4) Increases the stability of, \ntransmissibility of, or \nability to disseminate a \nbiological agent or toxin. \nLLMs can alter or modify computer code or human language to \nobfuscate malicious activity or intent. LLMs can be utilized to develop \nsophisticated obfuscation, cryptographic, or evasion techniques, making \nit difficult for security systems to identify or interpret attack vectors or \nactions of malicious agents (Oak 2022). The speed of generation exceeds \nhuman capacity to maintain conscious control of the proliferation of toxic \nor erroneous language. \n(5) Alters the host range or \ntropism of a biological \nagent or toxin. \nThe cost of deployment enhances the risks of dual use. in contrast with \nother mass-destruction weapons, “the materials and equipment required \nto create and propagate a biological attack using naturally occurring or \ngenetically manipulated pathogens remain decidedly “low-tech,” \ninexpensive, and widely available” (National Research Council 2007). \nThe case of LLMs is even more severe since replicating a foundation \nmodel is accessible to individuals and the smallest of organizations \n(Taori et al. 2023; R. Zhang et al. 2023). LLMs already have the potential \nto revolutionize spear phishing and other types of attacks due to drastic \nreductions in cost and time (Hazell 2023). This availability drastically \nlowers the barriers to entry, and thus increases the range of actors that \ncan engage in malicious uses.  \n8 \n(6) Enhances the \nsusceptibility of a host \npopulation. \nLLMs are quickly becoming more accessible and widespread to all \npeople speaking a language, as well as to programmers writing computer \ncode. Professional groups and societies as a whole will increasingly \nbecome more reliant on LLMs. This dependence on AI-generated content \nand the erosion of trust in information sources can make abuses of AI \nsystems more critical and consequential (Weidinger et al. 2021). \n(7) Generates a novel \npathogenic agent or toxin \nor reconstitutes an \neradicated or extinct \nbiological agent. \nLLMs can “invent” emerging capacities that lead to novel types of harms \nor toxic language. They can also reinforce known harms or attach vectors \nand apply them in novel applications. For example, LLMs can be used to \nautomate cyberattacks, including phishing, mass-scale social engineering, \nand producing malicious code. By generating convincing content tailored \nto specific targets, LLMs make it easier for malicious actors to \nweaponize language (EUROPOL 2023). \nBased on the application of these criteria , LLMs and generative AI research  can be \nconsidered as DURC. This categorization, however, does not imply that LLM research should \nbe prohibited or that the benefits of the technology  should not be exploited. Like in biology, \nwhere GOF research continues to this day, DURC raises  awareness of risks and provide s \nguidelines on how to encourage safety when applying LLM research.  \nNuclear energy is another technological dual -use area that can be compared to LLMs \n(Koplin 2023). Apocalyptic projections apply in both cases: some estimate that nuclear risks \ncould “set civilization back centuries” (Scouras 2019), while others claim that AI brings about \nexistential risks (Bostrom and Yudkowsky 2014) . However, the parallel between LLMs and \nnuclear science is more difficult to establish  than the parallel with biological research . The \nbiggest gap belongs with the problem of scarcity and accessibility  of resources. A motivated \nand complex government effort is needed for nuclear science to be exploited with potential \nharm. Materials, such as uranium and plutonium, are rare, heavily regulated, and monitored by \ninternational bodies. Meanwhile, LLMs are either open sou rce or can be developed with \ngrowing ease by private companies or even by individual researchers, as recent LLaMA -2 \nmodels show well (Touvron et al. 2023) . While nuclear research requires political will at the \nlevel of countries, LLMs present a multi-stakeholder dilemma involving individuals, research \nlabs, and business. This makes AI governance a more complex task requiring specific measures \nnot seen in the nuclear sphere. \n \n4. Dual-use and Shared Responsibility \n \nThe DURC framework is not a panacea but also not a pharmakon: it is a necessary pragmatic \nstep in the governance of LLMs, similarly to what occurred in GOF research. Even if one agrees \nthat LLM research is DURC, it does not by itself require setting a rigid regulatory framework; \nrather, it may suggest self -regulation measures like voluntary commitments (White House \n2023b) or industry-wide self-governance bodies (OpenAI 2023b). In bioethics, there is a debate \non whether a well -developed system of self -regulation could strike a better balance between \nrespecting scientific openness and protecting society from harm . Some argue that it will work  \nprovided that scientists engage with the system in good faith (Resnik 2010); others oppose it, \n9 \nsaying that it might be an overestim ation of the scientists’ competences  in assessing security \nrisks (Selgelid 2007). It remains to be seen if self -regulation can be efficient in the AI sector, \nnot least because manufacturers are private companies which also need to promote their risky \nproducts. \nOne of the crucial functions of a DURC framework is to distribute shared responsibility \nalong a complex chain of stakeholders in the case of malfunctioning or damage. In the case of \nLLMs, stakeholders include research and innovation actors  such as individual researchers and \nengineering teams, industrial actors such as manufacturers or deployers of AI systems, \necosystem members such as open-source platforms and independent model evaluation groups, \ngovernance bodies including regulators and standardization agencies, and final users. To arrive \nat a working notion of responsibility, criteria need to be established for how it should be shared \namong the different actors in the value chain, e.g., the programmers who design a foundation \nmodel and those who design control layers, the trainers w ho select training data, the \nmanufacturer of the AI system and that of possible plugins, an intermediary entity using the \nAPI supplied by the manufacturer, and the final user  (Grinbaum et al. 2017). The HHS P3CO \nframework in biological or chemical research introduced a set of criteria and norms governing \nshared responsibility (US HHS 2017) . Similarly, s hared responsibility needs to be a ctively \npromoted in the AI sector. For example, the proposal for an “AI Liability Directive” from the \nEuropean Commission establishes the responsibility of the manufacturer for each individual \noutput of an LLM, even if the manufacturer could not possibly i ntend the system to produce \nsuch an outcome  (European Commission 2022) . A careful consideration of DURC should \nremove the burden of duality from the manufacturer and distribute it along the chain of actors \ninvolved in building up the dual or damaging aspect of a particular outcome, rather than the \nmodel itself. Similarly to GOF in biotechnology, researchers should not be liable for anything \nand everything that the system they have built might do in the future. Such indefinite precaution \nmay lead to the “infantilization of technology” (Grinbaum and Groves 2013), which continues \nto treat technologies as direct ‘organs’ or ‘extensions’ of their creators, imputing them with \nunlimited responsibility. Instead, responsibility should be limited, much like the responsibility \nof parents for their children: LLMs should not be fixed in a position of eternal childspeak. This \nwould set the stage for more equitable and accountable research from the societal perspective . \nWith an accepted DURC framework, emergent issues of responsibility for the use, misuse, or \noutcomes of generative AI could be addressed systematically rather than ad hoc. \n \n \n5. Benefits and limitations of the DURC framework \n \nThere are potential pitfalls to categorizing a research domain as DURC. One such risk \nis the erosion of open science benefits. Specifically, the implementation of DURC could \nobstruct or even preclude the online publication of AI models, disrupting the principles of open \nsource and open data (LAION e.V 2023). If DURC constraints on the advancement of science \nbecome excessively restrictive, then less establ ished or unverified researchers and research \nteams may struggle to pursue their work  in a fully compliant way . Therefore, any proposed \nDURC framework for AI systems should strive to balance regulatory measures with the \npromotion of open source and open dat a in AI model development. While the application of \n10 \nDURC may inevitably curtail certain benefits, e.g. reproducibility, such limitations should be \nkept to a minimum. \nThe need to introduce the right amount of limitations and constraints leads to a known \nproblem in DURC, namely excessive formalization and bureaucratization. When the NSABB \npublished the final recommendations, they were adopted in policy documents, e.g. in the US \nDepartment of Health and Human Services “Framework for Guiding Funding Decisions about \nProposed Research Involving Enhanced Potential Pandemic Pathogens” (Evans 2020; US HHS \n2017), but overall they failed to make a lasting impact on science. One reason for this belongs \nwith the use of language that can hardly, if at all, be implemented on the operational level: t he \nguidelines suggested that scientists should consider scenarios that are “credible”, “realistic”, or \n“plausible”, while also being “highly unlikely but still credible”, based on fourteen  different \ncategories of possible damage. These requirements proved to be  more suited for regulatory \npurposes or legal proceedings, than scientific  work of biolog ists. As Evans puts it, \n“…mandating scientists [to] conduct research only [on] certain issues would be an unjustifiable \nburden on their freedom (in addition to any utilitarian assertions about the role of scientific \nfreedom in promoting health outcom es)” (Evans 2020) . Freedom of research, which is \nenshrined, e.g., in the German (Art. 5 Absatz 3 GG) or Austrian (Art. 17 StGG) constitutions, \ncan be seen as a permanent counterweight to the utilitarian DURC arguments. \nMoreover, as debates on GOF showed, the utilitarian analysis of risks and benefits is \nnot clear-cut and can be manipulated. Some scientists think that “gain-of-function research can \ncome in han dy,” while others admit that “their practical importance wasn’t  […] very \nextraordinary” (Dance 2021) . In GOF research, the risks and benefits are subject to expert \ncontroversy and cannot be agreed upon consensually. The same applies to LLMs and generative \nAI models. An established analysis provides twenty-one identified types of harm (Weidinger et \nal. 2021), while the benefits of LLMs are difficult to quantify. \nThe fact that generative AI models are not designed with a specific purpose further \ncomplicates their benefit assessment within the DURC framework. As an example, Sam \nAltman, the CEO of OpenAI, has admitted to overlooking the 'problem-solving' criterion when \nbuilding a business around a powerful technology that was not developed to provide a specific \nbenefit or solve a particular problem (Mollman 2023). However, such limitations of the \nutilitarian approach are not unprecedented in the context of  emerging technologies (Grinbaum \nand Groves 2013) . It is important to consider the individual desire and the ambition of \ngroundbreaking scientists. These qualities form an integral part of the virtue ethics analysis, \nwhich complements the utilitarian approach by considering the values of a scientist on an \nindividual level. \nAnother type of limitation of the DURC framework is its focus on short - or, at most, \nmedium-term horizon. Making realistic risk evaluations long into the future is not feasible due \nto uncertainty. LLMs, however, will have long-term effects on language (Grinbaum et al. 2021) \nwhich can hardly, if ever, be address ed via governance frameworks. For example, a language \nalways carries, implicitly or explicitly, a particular set of cultural values that express \ncivilizational choices and a particular mode of life. Over time, these values will influence the \nusers of LLMs. Many such effects will remain invisible to the user, but their longer -term \ninfluence on language and culture as a whole will eventually emerge and therefore should not \nbe ignored. \n11 \nMoreover, DURC frameworks usually  focus on state-funded research that can be \nmisused to threaten public health or national security . DURC largely relies on policy levers \ndirected towards government -funded research . These levers are, however, not in place for \nLLMs. Generative AI d evelopers are typically funded independently, often through private \ncorporations, and do not rely heavily on government support. T he usual funding -related \nincentives and policy measures are not applicable to major LLM developers. Furthermore, the \nglobal and diffuse nature of LLM development means that regulatory efforts in one jurisdiction \nmight not prevent misuse in another, not to mention the potential for clandestine development \nthat evade law enforcement . On the whole , the implementation of DURC meets here new \nchallenges that indicat e a lack of “teeth” in any governance framework due to low entrance \nbarriers and highly international character of generative AI. \nDespite all these limitations, DURC has a positive, and sometimes necessary, role to \nplay in research. One major role of the DURC framework is to facilitate the relationship \nbetween science and politics . The application of DURC to generative AI would reflect broad \npolitical awareness that LLMs are playing a major role in the life of society as a whole. LLMs \nare a powerful tool t hat can influence all aspects of life, from private to professional and \npolitical, and therefore create risks with far -reaching implications. The symbolic and political \nvalue of explicitly treating LLMs as DURC should not be overlooked. The DURC framework \nwould set the stage and rules for reflecting upon, and anticipating, the influence of technology \non society while addressing the inevitable conflicts that will arise. \n \n6. Specific DURC recommendations for generative AI and LLMs \n \nThe benefits and limitations of DURC outlined in the previous section demonstrate the \nneed to adapt this framework to generative AI and LLMs. Principles of risk governance should \nnot be too general for DURC to be impactful and operational in generative AI. We suggest the \nfollowing recommendations. \n \n1a) Develop standardized benchmarks to evaluate foundation models and generative \nAI systems for intentional abuse  \n1b) Evaluate foundation models for unintentional harm via ‘ red-teaming’ by \nindependent human testers \n \nFoundation models or general-purpose AI (GPAI) models , including openly accessible ones, \nhave recently been included in the European AI Act (European Parliament 2023a) as requiring \nspecific compliance measures . This makes them subject to regulation by public authorities . \nOther provisions of the AI Act only apply to AI systems understood as marketable products. \nAlthough the debates continue on which GPAI models should be regulated directly, involved \nparties agree on the importance of evaluating the  AI systems that are accessible to the public  \n(e.g., ChatGPT using GPT -3 or GPT-4). Evaluation is a cornerstone of DURC. For LLMs, i t \nincludes a variety of testing techniques (automatically computed benchmarks on standardized \ndatasets, penetration testing, human ‘red teaming’, etc.). A DURC framework should begin with \ntesting and evaluation requirements ; the results of such testing should be published alongside \nthe model. \n12 \n \n2) Evaluate datasets and use high-quality data for training \n \nMany emergent risks of LLMs stem from existing bias in training datasets, often replicating or \namplifying harmful statistical associations in their training data  (Caliskan et al. 2017; Qian et \nal. 2022) . It is a  problem that  is particularly striking for historically marginalized  groups, \nlanguages, and cultures (Field et al. 2021) . Beyond bias, the epistemic  quality of the training \ndata is also an important factor, e.g. training on books vs. online forums leads to the outputs of \nvastly different quality. LLMs may also have the ability to “ know what they don’t know ” \n(Osband et al. 2022) . Depending on specific applications of the LLMs, they may be given \nadditional requirements of both unfair bias mitigation, and adversarial quality control.  \n \n3) Enforce the human-machine distinction via watermarks \n \nAt a societal level, the use of nudging and deception can lend it self to political manipulation \n(Reisach 2021). LLMs can be used to create disinformation at scale (Xu 2020) . A scalable \nproduction of fake content has the potential to create “filter bubbles” or “echo chambers”, \nwhereby media consumers r ely only on unverified content (Colleoni et al. 2014) . Moreover, \nchatbots can be designed to achieve optimal rankings in recommendation algorithms that supply \nthe content to the end users, emphasizing specific political views. Risks arising from fraud or \nmanipulation, which comprise a large spectrum of societal risks, imply that users should not \nmistake an output produced by a machine for an output created by a human author (Aaronson \n2022; Grinbaum and Adomaitis 2022b; Kirchenbauer et al. 2023) . The purpose of \nalgorithmically designed watermark s is to maintain the possibility of distinguishing between  \nmachine and human authorship. Yet, the use of watermarking techniques for LLM s should \nremain unintrusive. The user’s experience, e.g. in obtaining medical or legal advice, should not \nbe perturbed by irrelevant disclaimers in the outputs . We recommend  that watermark s be \nsufficiently hidden from the user but detectable only with a minor effort, as well as sufficiently \nrobust to resist adversarial attempts to blur the origin of the text by editing. Even if watermark \nefficiency cannot be absolutely guaranteed  (H. Zhang et al. 2023) , the introduction of \nwatermarks is a necessary regulatory step from the societal point of view. \n \n4) Avoid overpolicing and sterilization of language \n \nAny implemented controls must be proportional to the risks, while unnecessary limitations can \ndistort or impoverish the generated language. The current iterations of LLMs encourage a stale \nand repetitive use of literary devices (Smith -Ruiu 2023) as well as an arbitrary avoidance of \ncritical topics (Weidinger et al. 2021) . Since humans imitate language abilities of their \ninterlocutors, be they machines or other humans  (Grinbaum 2023), this creates risks for the \ncognitive and cultural developments of  the individual and of society as a whole. In general, \noutput filtering is useful, for example generalist chatbots should not provide medical or legal \nadvice. However, this has collateral effects, namely, excluding all types of “toxic” language  \nresults in a sterilized language. H uman users may find that their own language gets less \nesthetically pleasing and more banal as a result of their interaction with chatbots.  \n13 \n \n5) Introduce a set of criteria and norms governing shared responsibility \n \nLLMs and LLM -based chatbots are machines, yet they acquire qualities by projection \n(Grinbaum and Adomaitis 2022a) . Their emergent “conduct” may lead to morally significant \nconsequences. For example, chatbots  can be perceived as lying , mislead ing, hurt ing, \nmanipulating or insulting human beings (Davis 2016). Such effects usually induce respective  \nprojections of moral responsibility on machines. However, digital agents are not moral agents \nand cannot assume  responsibility. A chatbot should never be perceived by the user as a \nresponsible person, even by projection (Grinbaum 2019) . To remove such projections, a  \ncomprehensive set of criteria and norms that clearly outline the shared responsibilities between \nAI developers, users, and other stakeholders should be created (Adomaitis et al. 2022; Dignum \n2019). The DURC framework can help to phrase such criteria in a language understandable to \nlegal professionals and non- experts in general . Shared responsibility should emphasize the \ncollective dimension of design and deployment of AI systems. \n \nIn conclusion, the application of the Dual Use Research of Concern (DURC) framework to the \nfield of generative AI and Large Language Models (LLMs) brings a new perspective on the \nrapidly expanding influence of these technologies. It serves as a call for re flective governance \nallowing researchers, policymakers, and the public to conscientiously address the broad -\nreaching implications of LLMs. The recommendations provided in this article are meant to offer \ntangible starting points for shaping the ethical trajectory of generative AI research, thereby \nensuring that the balance between innovation and security is maintained. It is our hope that this \nperspective will stimulate further dialogue, leading to the emergence of robust strategies that \nuphold the integrity and potential of AI, while safeguarding societal interests. \n  \n14 \nReferences \nAaronson, S. (2022, November 29). My AI Safety Lecture for UT Effective Altruism. Shtetl- Optimized. \nhttps://scottaaronson.blog/?p=6823. Accessed 11 January 2023 \nAdomaitis, L., Grinbaum, A., & Lenzi, D. (2022). TechEthos D2.2: Identification and specification of potential \nethical issues and impacts and analysis of ethical issues of digital extended realit y, neurotechnologies, \nand climate engineering. CEA Paris Saclay. https://hal-cea.archives-ouvertes.fr/cea-03710862. \nAccessed 25 October 2022 \nAI Safety Summit. (2023, November 1). The Bletchley Declaration by Countries Attending the AI Safety \nSummit, 1-2 November 2023. GOV.UK. https://www.gov.uk/government/publications/ai-safety-\nsummit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-\nsummit-1-2-november-2023. Accessed 23 December 2023 \nAnthropic. (2023, July 26). Frontier Threats Red Teaming for AI Safety. \nhttps://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety. Accessed 31 July 2023 \nAtlas, R. M. (2002). National Security and the Biological Research Community. Science, 298(5594), 753–754. \nhttps://doi.org/10.1126/science.1078329 \nBagdasaryan, E., & Shmatikov, V. (2022). Spinning Language Models: Risks of Propaganda -As-A-Service and \nCountermeasures. In 2022 IEEE Symposium on Security and Privacy (SP) (pp. 769–786). \nhttps://doi.org/10.1109/SP46214.2022.9833572 \nBerg, P., Baltimore, D., Brenner, S., Roblin, R. O., & Singer, M. F. (1975). Asilomar Conference on \nRecombinant DNA Molecules. Science, 188(4192), 991–994. https://doi.org/10.1126/science.1056638 \nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., et al. (2022, July 12). On the \nOpportunities and Risks of Foundation Models. arXiv. https://doi.org/10.48550/arXiv.2108.07258  \nBostrom, N., & Yudkowsky, E. (2014). The ethics of artificial intelligence. The Cambridge handbook of \nartificial intelligence, 1, 316–334. \nBrundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., et al. (2018). The Malicious Use of \nArtificial Intelligence: Forecasting, Prevention, and Mitigation. \nhttps://www.repository.cam.ac.uk/handle/1810/275332. Accessed 4 May 2023 \nC, D., & J, P. (2023, March 14). ChatGPT and large language models: what’s the risk? National Cyber Security \nCenter. https://www.ncsc.gov.uk/blog-post/chatgpt-and-large-language-models-whats-the-risk. \nAccessed 5 May 2023 \nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora \ncontain human-like biases. Science (New York, N.Y.), 356(6334), 183–186. \nhttps://doi.org/10.1126/science.aal4230 \nChristie, E. H., Ertan, A., Adomaitis, L., & Klaus, M. (2023). Regulating lethal autonomous weapon systems: \nexploring the challenges of explainability and traceability. AI and Ethics, 1–17. \nClark, E., August, T., Serrano, S., Haduong, N., Gururangan, S., & Smith, N. A. (2021, July 7). All That’s \n“Human” Is Not Gold: Evaluating Human Evaluation of Generated Text. arXiv. \nhttps://doi.org/10.48550/arXiv.2107.00061 \nColleoni, E., Rozza, A., & Arvidsson, A. (2014). Echo chamber or public sphere? Predicting politic al orientation \nand measuring political homophily in Twitter using big data. Journal of communication, 64(2), 317–\n332. \nCoulter, M., & Mukherjee, S. (2023, April 17). EU lawmakers call for summit to control “very powerful” AI. \nReuters. https://www.reuters.com/technology/eu-lawmakers-call-political-attention-powerful-ai-2023-\n04-17/. Accessed 10 May 2023 \nCouncil of Europe. (2023, December 9). Artificial intelligence act: Council and Parliament strike a deal on the \nfirst rules for AI in the world. Press Release. https://www.consilium.europa.eu/en/press/press-\nreleases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-\nworldwide-rules-for-ai/. Accessed 23 December 2023 \nDance, A. (2021). The shifting sands of ‘gain-of-function’ research. Nature, 598(7882), 554–557. \nhttps://doi.org/10.1038/d41586-021-02903-x \nDavis, E. (2016). AI amusements: the tragic tale of Tay the chatbot. AI Matters, 2(4), 20–24. \nhttps://doi.org/10.1145/3008665.3008674 \n15 \nDignum, V. (2019). Taking Responsibility. In V. Dignum (Ed.), Responsible Artificial Intelligence: How to \nDevelop and Use AI in a Responsible Way (pp. 47–69). Cham: Springer International Publishing. \nhttps://doi.org/10.1007/978-3-030-30371-6_4 \nEuropean Commission (2022). Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF \nTHE COUNCIL on adapting non-contractual civil liability rules to artificial intelligence (AI Liability \nDirective) (2022). https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52022PC0496. \nAccessed 31 July 2023 \nEuropean Parliament (2023a). Artificial Intelligence Act. Amendments adopted by the European Parliament on \n14 June 2023. , Pub. L. No. P9_TA(2023)0236 (2023). \nEuropean Parliament (2023b). Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI. Press \nrelease on 09 December 2023. https://www.europarl.europa.eu/news/en/press-\nroom/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai  \nEUROPOL. (2023). ChatGPT - the impact of Large Language Models on Law Enforcement. Publications Office \nof the European Union. https://www.europol.europa.eu/publications-events/publications/chatgpt-\nimpact-of-large-language-models-law-enforcement. Accessed 5 May 2023 \nEvans, N. G. (2013). “But Nature Started It”: Examining Taubenberger and Morens’ View on Influenza A Virus \nand Dual-Use Research of Concern. mBio, 4(4), e00547-13. https://doi.org/10.1128/mBio.00547-13 \nEvans, N. G. (2020). Dual-Use and Infectious Disease Research. In M. Eccleston-Turner & I. Brassington (Eds.), \nInfectious Diseases in the New Millennium: Legal and Ethical Challenges (pp. 193–215). Cham: \nSpringer International Publishing. https://doi.org/10.1007/978-3-030-39819-4_9 \nExport Administration Regulations (2013). https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=celex%3A32021R0821 \nField, A., Blodgett, S. L., Waseem, Z., & Tsvetkov, Y. (2021). A Survey of Race, Racism, and  Anti-Racism in \nNLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and \nthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)  (pp. \n1905–1925). Presented at the ACL-IJCNLP 2021, Online: Association for Computational Linguistics. \nhttps://doi.org/10.18653/v1/2021.acl-long.149 \nFuture of Life Institute. (2023, March 22). Pause Giant AI Experiments: An Open Letter. Future of Life Institute . \nhttps://futureoflife.org/open-letter/pause-giant-ai-experiments/. Accessed 3 April 2023 \nGaldzicki, M., Rodriguez, C., Chandran, D., Sauro, H. M., & Gennari, J. H. (2011). Standard Biological Parts \nKnowledgebase. PLOS ONE, 6(2), e17005. https://doi.org/10.1371/journal.pone.0017005 \nGeissler, E., & Moon, J. E. van C. (1999). Biological and Toxin Weapons: Research, Development, and Use \nfrom the Middle Ages to 1945. Oxford University Press. \nGregory, S. (2022). Deepfakes, misinformation and disinformation and authenticity infrastructure responses: \nImpacts on frontline witnessing, distant witnessing, and civic journalism. Journalism, 23(3), 708–729. \nhttps://doi.org/10.1177/14648849211060644 \nGrinbaum, A. (2019). Les robots et le mal. Paris: Desclée de Brouwer. \nGrinbaum, A. (2023). Parole de machines. HUMENSCIENCES. \nGrinbaum, A., & Adomaitis, L. (2022a). Moral Equivalence in the Metaverse. NanoEthics, 16(3), 257–270. \nhttps://doi.org/10.1007/s11569-022-00426-x \nGrinbaum, A., & Adomaitis, L. (2022b, September 7). The Ethical Need for Watermarks in Machine -Generated \nLanguage. arXiv. https://doi.org/10.48550/arXiv.2209.03118 \nGrinbaum, A., Chatila, R., Devillers, L., Ganascia, J.-G., Tessier, C., & Dauchet, M. (2017). Ethics in Robotics \nResearch: CERNA Mission and Context. IEEE Robotics & Automation Magazine , 24(3), 139–145. \nPresented at the IEEE Robotics & Automation Magazine. https://doi.org/10.1109/MRA.2016.2611586  \nGrinbaum, A., Devillers, L., Adda, G., Chatila, R., Martin, C., Zolynski, C., & Villata, S. (2021). Agents \nconversationnels: Enjeux d’éthique (Report). Comité national pilote d’éthique du numérique; CCNE. \nGrinbaum, A., & Groves, C. (2013). What is “responsible” about responsible innovation? Understanding the \nethical issues. Responsible innovation: Managing the responsible emergence of science and innovation \nin society, 119–142. \nHazell, J. (2023, May 12). Large Language Models Can Be Used To Effectively Scale Spear Phishing \nCampaigns. arXiv. https://doi.org/10.48550/arXiv.2305.06972 \n16 \nHeaven, W. D. (2023, May 2). Geoffrey Hinton tells us why he’s now scared of the tech he helped build. MIT \nTechnology Review. https://www.technologyreview.com/2023/05/02/1072528/geoffrey-hinton-google-\nwhy-scared-ai/. Accessed 3 May 2023 \nHeikkilä, M. (2022, December 19). How to spot AI-generated text. MIT Technology Review. \nhttps://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/. Accessed 5 \nMay 2023 \nHerfst, S., Schrauwen, E. J. A., Linster, M., Chutinimitkul, S., de Wit, E., Munster, V. J., et al. (2012). Airborne \nTransmission of Influenza A/H5N1 Virus Between Ferrets. Science, 336(6088), 1534–1541. \nhttps://doi.org/10.1126/science.1213362 \nHern, E., Glazebrook, W., & Beckett, M. (2005). Reducing knife crime. BMJ : British Medical Journal, \n330(7502), 1221–1222. \nImai, M., Watanabe, T., Hatta, M., Das, S. C., Ozawa, M., Shinya, K., et al. (2012). Experimental adaptation of \nan influenza H5 HA confers respiratory droplet transmission to a reassortant H5 HA/H1N1 v irus in \nferrets. Nature, 486(7403), 420–428. https://doi.org/10.1038/nature10831 \nKirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., & Goldstein, T. (2023, January 27). A Watermark for \nLarge Language Models. arXiv. https://doi.org/10.48550/arXiv.2301.10226 \nKoplin, J. J. (2023). Dual-use implications of AI text generation. Ethics and Information Technology , 25(2), 32. \nhttps://doi.org/10.1007/s10676-023-09703-z \nKorn, H., Pironneau, O., Fagot-Largeault, A., d’Artemare, B., Becard, N., Zini, S., et al. (2019). RECHERCHES \nDUALES A RISQUE-Recommandations pour leur pris en compte dans les processus de conduite de \nrecherche en biologie (PhD Thesis). Académie des Sciences. \nLAION e.V. (2023, May 4). An Open Letter to the European Parliament. \nLipsitch, M., & Bloom, B. R. (2012). Rethinking Biosafety in Research on Potential Pandemic Pathogens. mBio , \n3(5), e00360-12. https://doi.org/10.1128/mBio.00360-12 \nLipsitch, M., & Galvani, A. P. (2014). Ethical Alternatives to Experiments with Novel Potential Pand emic \nPathogens. PLOS Medicine, 11(5), e1001646. https://doi.org/10.1371/journal.pmed.1001646 \nMireshghallah, F., Goyal, K., Uniyal, A., Berg-Kirkpatrick, T., & Shokri, R. (2022, November 3). Quantifying \nPrivacy Risks of Masked Language Models Using Membership Inference Attacks. arXiv. \nhttps://doi.org/10.48550/arXiv.2203.03929 \nMoradi, M., & Samwald, M. (2021, August 27). Evaluating the Robustness of Neural Language Models to Input \nPerturbations. arXiv. https://doi.org/10.48550/arXiv.2108.12237 \nNational Research Council. (2004). Biotechnology Research in an Age of Terrorism. Washington, D.C.: National \nAcademies Press. https://doi.org/10.17226/10827 \nNational Research Council. (2007). Biosecurity and Dual-Use Research in the Life Sciences. In Science and \nSecurity in a Post 9/11 World: A Report Based on Regional Discussions Between the Science and \nSecurity Communities. National Academies Press (US). \nhttps://www.ncbi.nlm.nih.gov/books/NBK11496/. Accessed 11 May 2023 \nNSABB. (2007). Proposed framework for the oversight of dual use life sciences research: strategies for \nminimizing the potential misuse of research information. A report of the National Science Advisory \nBoard for Biosecurity (NSABB). National Science Advisory Board for Biosecurity, Office of \nBiotechnology …. \nOak, R. (2022). Poster – Towards Authorship Obfuscation with Language Models. In Proceedings of the 2022 \nACM SIGSAC Conference on Computer and Communications Security (pp. 3435–3437). New York, \nNY, USA: Association for Computing Machinery. https://doi.org/10.1145/3548606.3563512 \nOpenAI. (2023a, July 21). Moving AI governance forward. https://openai.com/blog/moving- ai-governance-\nforward. Accessed 27 July 2023 \nOpenAI. (2023b, July 26). Frontier Model Forum. https://openai.com/blog/frontier -model-forum. Accessed 31 \nJuly 2023 \nOsband, I., Asghari, S. M., Van Roy, B., McAleese, N., Aslanides, J., & Irving, G. (2022, November 2). Fine -\nTuning Language Models via Epistemic Neural Networks. arXiv. \nhttps://doi.org/10.48550/arXiv.2211.01568 \nPichai, S., & Hassabis, D. (2023, December 6). Introducing Gemini: our largest and most capable AI model. \nGoogle. https://blog.google/technology/ai/google-gemini-ai/. Accessed 23 December 2023 \n17 \nQian, R., Ross, C., Fernandes, J., Smith, E., Kiela, D., & Williams, A. (2022). Perturbation augmentation for \nfairer nlp. arXiv preprint arXiv:2205.12586. \nRegulation (EU) 2021/821. , 206 OJ L (2021). http://data.europa.eu/eli/reg/2021/821/oj/eng. Accessed 10 May \n2023 \nReisach, U. (2021). The responsibility of social media in times of societal and political manipulation. European \nJournal of Operational Research, 291(3), 906–917. https://doi.org/10.1016/j.ejor.2020.09.020 \nResnik, D. B. (2010). Can Scientists Regulate the Publication of Dual Use Research? Studies in Ethics, Law, and \nTechnology, 4(1). https://doi.org/10.2202/1941-6008.1124 \nRopek, L. (2023, January 20). ChatGPT Is Pretty Good at Writing Malware, It Turns Out. Gizmodo. \nhttps://gizmodo.com/chatgpt-ai-polymorphic-malware-computer-virus-cyber-1850012195. Accessed 5 \nMay 2023 \nSanger, D. E. (2023, May 5). The Next Fear on A.I.: Hollywood’s Killer Robots Become the Military’s Tools. \nThe New York Times. https://www.nytimes.com/2023/05/05/us/politics/ai-military-war-nuclear-\nweapons-russia-china.html. Accessed 10 May 2023 \nScharre, P. (2018). Army of None: Autonomous Weapons and the Future of War (First Edition, Stated, First \nPrinting.). New York ; London: W. W. Norton & Company. \nSchweber, S. S. (2013). In the Shadow of the Bomb: Oppenheimer, Bethe, and the Moral Responsibility of the \nScientist. In In the Shadow of the Bomb. Princeton University Press. \nhttps://doi.org/10.1515/9781400849499 \nScouras, J. (2019). Nuclear War as a Global Catastrophic Risk. Journal of Benefit -Cost Analysis, 10(2), 274–\n295. https://doi.org/10.1017/bca.2019.16 \nSelgelid, M. J. (2007). A tale of two studies; ethics, bioterrorism, and the censorship of science. The Hastings \nCenter Report, 37(3), 35–43. https://doi.org/10.1353/hcr.2007.0046 \nSmith-Ruiu, J. (2023, April 5). My Dinners with GPT-4. Justin Smith-Ruiu’s Hinternet. Substack newsletter. \nhttps://justinehsmith.substack.com/p/my-dinners-with-gpt-4. Accessed 10 May 2023 \nTaddeo, M., & Floridi, L. (2018). Regulate artificial intelligence to avert cyber arms race. Nature, 556(7701), \n296–298. https://doi.org/10.1038/d41586-018-04602-6 \nTamkin, A., Brundage, M., Clark, J., & Ganguli, D. (2021, February 4). Understanding the Capabilities, \nLimitations, and Societal Impact of Large Language Models. arXiv. \nhttps://doi.org/10.48550/arXiv.2102.02503 \nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., et al. (2023, April 3). Stanford Alpaca: An \nInstruction-following LLaMA Model. Python, Tatsu’s shared repositories. https://github.com/tatsu -\nlab/stanford_alpaca. Accessed 3 April 2023 \nThe White House. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial \nIntelligence (2023). https://www.whitehouse.gov/briefing-room/presidential-\nactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-\nartificial-intelligence/. Accessed 23 December 2023 \nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., et al. (2023, July 19). Llama 2: Open \nFoundation and Fine-Tuned Chat Models. arXiv. https://doi.org/10.48550/arXiv.2307.09288 \nTucker, J. B. (2012). Innovation, Dual Use, and Security: Managing the Risks of Emerging Biological and \nChemical Technologies. MIT Press. \nTumpey, T. M., Basler, C. F., Aguilar, P. V., Zeng, H., Solórzano, A., Swayne, D. E., et al. (2005). \nCharacterization of the Reconstructed 1918 Spanish Influenza Pandemic Virus. Science, 310(5745), 77–\n80. https://doi.org/10.1126/science.1119392 \nUS HHS. (2017). Framework for Guiding Funding Decisions about Proposed Research Involving Enhanced \nPotential Pandemic Pathogens. US Department of Health and Human Services. \nVannuccini, S., & Prytkova, E. (2021, April 7). Artificial Intelligence’s New Clothes? From General Purpose \nTechnology to Large Technical System. SSRN Scholarly Paper, Rochester, NY. \nhttps://doi.org/10.2139/ssrn.3860041 \nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., et al. (2022, October 26). Emergent \nAbilities of Large Language Models. arXiv. https://doi.org/10.48550/arXiv.2206.07682 \nWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., et al. (2021). Ethical and social risks of \nharm from Language Models. arXiv preprint arXiv:2112.04359. \n18 \nWhite House. (2023a, May 4). Readout of White House Meeting with CEOs on Advancing Responsible \nArtificial Intelligence Innovation. The White House. https://www.whitehouse.gov/briefing-\nroom/statements-releases/2023/05/04/readout-of-white-house-meeting-with-ceos-on-advancing-\nresponsible-artificial-intelligence-innovation/. Accessed 10 May 2023 \nWhite House. (2023b, July 21). FACT SHEET: Biden-Harris Administration Secures Voluntary Commitments \nfrom Leading Artificial Intelligence Companies to Manage the Risks Posed by AI. The White House. \nhttps://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-\nadministration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-\nmanage-the-risks-posed-by-ai/. Accessed 31 July 2023 \nXu, A. Y. (2020, June 22). Creating Fake News with OpenAI’s Language Models. Medium . \nhttps://towardsdatascience.com/creating-fake-news-with-openais-language-models-368e01a698a3. \nAccessed 30 August 2022 \nZhang, H., Edelman, B. L., Francati, D., Venturi, D., Ateniese, G., & Barak, B. (2023, November 14). \nWatermarks in the Sand: Impossibility of Strong Watermarking for Generative Models. arXiv. \nhttps://doi.org/10.48550/arXiv.2311.04378 \nZhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., et al. (2023, March 28). LLaMA-Adapter: Efficient Fine-\ntuning of Language Models with Zero-init Attention. arXiv. https://doi.org/10.48550/arXiv.2303.16199  \n ",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.8404754400253296
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.6905452013015747
    },
    {
      "name": "Corporate governance",
      "score": 0.565478503704071
    },
    {
      "name": "Dual language",
      "score": 0.5147088170051575
    },
    {
      "name": "Politics",
      "score": 0.506138026714325
    },
    {
      "name": "Dual purpose",
      "score": 0.49824953079223633
    },
    {
      "name": "Focus (optics)",
      "score": 0.45294803380966187
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.44265320897102356
    },
    {
      "name": "Generative model",
      "score": 0.4372391104698181
    },
    {
      "name": "Computer science",
      "score": 0.375662624835968
    },
    {
      "name": "Management science",
      "score": 0.34514665603637695
    },
    {
      "name": "Cognitive science",
      "score": 0.32451504468917847
    },
    {
      "name": "Political science",
      "score": 0.3064141571521759
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2526349425315857
    },
    {
      "name": "Psychology",
      "score": 0.2486661970615387
    },
    {
      "name": "Linguistics",
      "score": 0.15765276551246643
    },
    {
      "name": "Engineering",
      "score": 0.14744848012924194
    },
    {
      "name": "Management",
      "score": 0.1098686158657074
    },
    {
      "name": "Economics",
      "score": 0.09035235643386841
    },
    {
      "name": "Mathematics education",
      "score": 0.09022629261016846
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128565",
      "name": "CEA Paris-Saclay",
      "country": "FR"
    }
  ],
  "cited_by": 1
}