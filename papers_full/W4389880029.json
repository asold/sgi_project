{
    "title": "Pansharpening via Multiscale Embedding and Dual Attention Transformers",
    "url": "https://openalex.org/W4389880029",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2170089029",
            "name": "Wen-Sheng Fan",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2101170733",
            "name": "Fan Liu",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2115013002",
            "name": "Jingzhi Li",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2923136550",
        "https://openalex.org/W2114161542",
        "https://openalex.org/W4382793509",
        "https://openalex.org/W2587456632",
        "https://openalex.org/W6604338981",
        "https://openalex.org/W4285205078",
        "https://openalex.org/W4309347615",
        "https://openalex.org/W2171211028",
        "https://openalex.org/W3097824737",
        "https://openalex.org/W3168631257",
        "https://openalex.org/W6631230401",
        "https://openalex.org/W2172185514",
        "https://openalex.org/W2152254169",
        "https://openalex.org/W2001800591",
        "https://openalex.org/W2124743705",
        "https://openalex.org/W2119077559",
        "https://openalex.org/W2117146861",
        "https://openalex.org/W2022075948",
        "https://openalex.org/W2054440797",
        "https://openalex.org/W2163677711",
        "https://openalex.org/W1511001221",
        "https://openalex.org/W2963129413",
        "https://openalex.org/W6856691458",
        "https://openalex.org/W6857839881",
        "https://openalex.org/W1885185971",
        "https://openalex.org/W2462592242",
        "https://openalex.org/W2777033955",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963183385",
        "https://openalex.org/W3115223653",
        "https://openalex.org/W6777200146",
        "https://openalex.org/W3176195847",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W4387831734",
        "https://openalex.org/W6839606922",
        "https://openalex.org/W4213256221",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4379795970",
        "https://openalex.org/W6669509095",
        "https://openalex.org/W4288391574",
        "https://openalex.org/W2171108951",
        "https://openalex.org/W2963007295",
        "https://openalex.org/W3096904276",
        "https://openalex.org/W3047443805",
        "https://openalex.org/W6802112106",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W4312340363",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3035421056",
        "https://openalex.org/W6685078114",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2792142731",
        "https://openalex.org/W3204305289",
        "https://openalex.org/W6623043969",
        "https://openalex.org/W1991460509",
        "https://openalex.org/W2163334907",
        "https://openalex.org/W2142843085",
        "https://openalex.org/W2038580618",
        "https://openalex.org/W4388685310",
        "https://openalex.org/W106317687",
        "https://openalex.org/W3103695279",
        "https://openalex.org/W3099347747",
        "https://openalex.org/W1522046140",
        "https://openalex.org/W2528170672"
    ],
    "abstract": "Pansharpening is a fundamental and crucial image processing task for many remote sensing applications, which generates a high-resolution multispectral image by fusing a low-resolution multispectral image and a high-resolution panchromatic image. Recently, vision transformers have been introduced into the pansharpening task for utilizing global contextual information. However, long-range and local dependencies modeling and multiscale feature learning are all essential to the pansharpening task. Learning and exploiting these various information raises a big challenge and limits the performance and efficiency of existing pansharpening methods. To solve this issue, we propose a pansharpening network based on multiscale embedding and dual attention transformers (MDPNet). Specifically, a multiscale embedding block is proposed to embed multiscale information of the images into vectors. Thus, transformers only need to process a multispectral embedding sequence and a panchromatic embedding sequence to efficiently use multiscale information. Furthermore, an additive hybrid attention transformer is proposed to fuse the embedding sequences in an additive injection manner. Finally, a channel self-attention transformer is proposed to utilize channel correlations for high-quality detail generation. Experiments over QuickBird and WorldView-3 datasets demonstrate the proposed MDPNet outperforms state-of-the-art methods visually and quantitatively with low running time. Ablation studies further verify the effectiveness of the proposed multiscale embedding and transformers in pansharpening.",
    "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024 2705\nPansharpening via Multiscale Embedding and\nDual Attention Transformers\nWensheng Fan , Fan Liu , Member , IEEE, and Jingzhi Li\nAbstract—Pansharpening is a fundamental and crucial image\nprocessing task for many remote sensing applications, which gen-\nerates a high-resolution multispectral image by fusing a low-\nresolution multispectral image and a high-resolution panchromatic\nimage. Recently, vision transformers have been introduced into\nthe pansharpening task for utilizing global contextual informa-\ntion. However, long-range and local dependencies modeling and\nmultiscale feature learning are all essential to the pansharpening\ntask. Learning and exploiting these various information raises a\nbig challenge and limits the performance and efﬁciency of ex-\nisting pansharpening methods. To solve this issue, we propose\na pansharpening network based on multiscale embedding and\ndual attention transformers (MDPNet). Speciﬁcally, a multiscale\nembedding block is proposed to embed multiscale information of\nthe images into vectors. Thus, transformers only need to process a\nmultispectral embedding sequence and a panchromatic embedding\nsequence to efﬁciently use multiscale information. Furthermore,\nan additive hybrid attention transformer is proposed to fuse the\nembedding sequences in an additive injection manner. Finally, a\nchannel self-attention transformer is proposed to utilize channel\ncorrelations for high-quality detail generation. Experiments over\nQuickBird and WorldView-3 datasets demonstrate the proposed\nMDPNet outperforms state-of-the-art methods visually and quan-\ntitatively with low running time. Ablation studies further verify the\neffectiveness of the proposed multiscale embedding and transform-\ners in pansharpening.\nIndex Terms—Attention mechanism, image fusion, multiscale\nembedding, pansharpening, remote sensing, vision transformer\n(ViT).\nI. INTRODUCTION\nM\nULTISPECTRAL (MS) images are widely used for\nvarious remote sensing applications such as land cover\nclassiﬁcation [1], environmental change detection[2], [3], and\nagriculture monitoring[4]. Due to physical constraints, there is a\ntradeoff between spatial and spectral resolutions during satellite\nimaging. The satellite can only provide low-spatial-resolution\n(LR) MS images and corresponding high-spatial-resolution\nManuscript received 12 May 2023; revised 23 July 2023, 24 November 2023,\nand 12 December 2023; accepted 14 December 2023. Date of publication\n18 December 2023; date of current version 10 January 2024. This work was\nsupported in part by the National Natural Science Foundation of China under\nGrant 61703299 and in part by the Basic Research Project Foundation of Shanxi\nProvince under Grant 202203021221094.(Corresponding author: Fan Liu.)\nWensheng Fan is with the College of Electrical and Power Engineering,\nTaiyuan University of Technology, Taiyuan 030024, China (e-mail: fanwen-\nsheng9603@163.com).\nFan Liu and Jingzhi Li are with the College of Computer Science and Tech-\nnology (College of Data Science), Taiyuan University of Technology, Jinzhong\n030600, China (e-mail: liufan@tyut.edu.cn; 916381552@qq.com).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3344215\n(HR) PAN images[5]. To obtain HRMS images, image pro-\ncessing is needed. Image processing applies procedures to an\nimage to enhance it or derive valuable information from it[6].\nAs a remote sensing image processing technique, pansharpening\nsharpens LRMS images using their corresponding PAN images\nto produce HRMS images. Therefore, pansharpening can im-\nprove the performance of remote sensing applications such as\nland-use classiﬁcation[7].\nIn the past several decades, many pansharpening algorithms\nhave been developed. They can be roughly grouped into four\nmain categories: component substitution (CS), multiresolution\nanalysis (MRA), variational optimization (VO), and deep learn-\ning (DL) [8], [9], [10]. The ﬁrst three classes are traditional\nalgorithms that emerged decades ago. The DL-based methods\nhave arisen recently and achieved promising outcomes.\nCS-based algorithms usually transform the up-sampled MS\nimage into another space to separate out its spatial component,\nand then, replace it with the PAN image to enrich spatial details.\nWell-known CS algorithms include those exploiting intensity-\nhue-saturation (IHS) transform[11], Gram–Schmidt (GS) trans-\nform [12], and band-dependent spatial detail (BDSD) [13].\nMRA-based methods typically use multiscale decomposition\nor high-pass ﬁltering to extract spatial details from the PAN\nimage and obtain the HRMS image via detail injection. Rep-\nresentative MRA approaches include additive wavelet lumi-\nnance (AWL)[14], smoothing ﬁlter-based intensity modulation\n(SFIM) [15], and generalized Laplacian pyramids with modula-\ntion transfer function (MTF-GLP)[16]. VO-based approaches\nbuild a model with suitable regularization terms based on certain\npriors or assumptions and utilize an effective algorithm to opti-\nmize the model. Typical VO-based methods include Bayesian-\nbased fusion methods [17], [18], sparse representation-based\ndetail injection [19], and total variation (TV)[20]. There are\nalso hybrid methods that combine different kinds of traditional\napproaches and even combine them with DL techniques to\ncomplement each other[21], [22], [23].\nDL techniques are also widely applied to the remote sensing\nﬁeld and have shown great potential continuously no matter in\nspeciﬁc tasks such as cross-city semantic segmentation [24]\nor in universal foundation model development [25].A sf o r\nthe pansharpening task, inspired by the image super-resolution\nmethod based on convolutional neural network (CNN)[26],\nMasi et al.[27] proposed an efﬁcient three-layer CNN for pan-\nsharpening (PNN), which produced promising outcomes in the\npansharpening task. Introducing more domain knowledge, Yang\net al.[28] proposed PanNet, which learns the spatial details to be\n© 2023 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see\nhttps://creativecommons.org/licenses/by/4.0/\n2706 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\ninjected into the MS image in the high-pass ﬁltering domain via\nresidual learning[29], which successfully protects both spatial\nand spectral information in the deep network. Based on the\nobservation that the scale of features varies among different\nground objects, Yuan et al.[30] proposed a multiscale and mul-\ntidepth CNN (MSDCNN) and achieved superior pansharpening\nperformance. The exploration and utilization of multiscale fea-\ntures have since become a key concern in developing DL-based\nmethods. To enhance the ﬁdelity of pan-sharpened images,\ngenerative adversarial network techniques are applied to the pan-\nsharpening task[31], [32], [33]. These methods typically design\none or several discriminators to judge the ﬁdelity of pansharp-\nening outcomes. On the other hand, to model the long-range\ndependencies in the images, the transformer techniques[34],\n[35] are recently applied to the pansharpening task[36]. Zhou\net al. [37] designed a customized transformer for pansharpen-\ning, which enhances the spatial details of the pan-sharpened\nimage via both hard-attention and soft-attention mechanisms.\nDR-NET [38] inserts Swin transformer[39] blocks into a hier-\narchical u-shaped architecture to combine the transformer with\na typical multiscale pansharpening scheme, which reduces the\ndetail loss in the down-sampling process. To mutually rein-\nforce spatial and spectral features, Zhang et al.[40] propose a\ncross-interaction kernel attention network for the improvement\nof dynamic convolution-based pansharpening.\nThe aforementioned methods are designed by following\nobservations. On the one hand, remote sensing images contain a\nlarge number of repetitive ground objects. Ground objects with\nsimilar spatial and spectral information can be close or far apart.\nOn the other hand, ground objects in remote sensing images are\nin many sizes. Thus, there are several potential drawbacks in\ntransformer-based methods regarding the following aspects.\n1) Long-range and local dependencies modeling and mul-\ntiscale feature learning are all essential to pansharpen-\ning. It is difﬁcult for transformer-based method to fully\nand efﬁciently exploit these various information, which\nmay cause performance limitation and high computational\ncomplexity.\n2) The traditional self-attention transformers can only play\nthe role of long-range feature extraction. They have little\nto do with feature fusion. This prevents transformer from\nmodeling the dependencies between the original LRMS\nimage and the fusion product and is unfavorable to gener-\nating details complement to the LRMS image.\n3) The dependencies along spatial and channel dimensions\nare not fully utilized in existing transformer-based meth-\nods. Correlations among both dimensions are crucial to the\nspatial and spectral quality of pansharpening outcomes.\nTo solve these problems, we propose a pansharpening network\nbased on multiscale embedding and dual attention transformers\n(MDPNet) is proposed in this article. Speciﬁcally, a multiscale\nembedding block is proposed to embed the multiscale infor-\nmation of the LRMS and PAN images into two sequences of\nvectors. Thus, only processing this pair of multiscale embed-\nding sequences is enough to efﬁciently realize the utilization\nof multiscale features. To fuse the two sequences, a feature\nfusion module based on the additive hybrid attention transformer\n(AHAT) is proposed considering the long-range dependencies in\nthe spatial dimension. Finally, a detail generation module based\non channel self-attention transformer (CSAT) is proposed to\ngenerate details for detail injection considering the correlations\namong feature channels. Experiments over datasets collected by\nQuickBird (QB) and WorldView-3 (WV3) satellites demonstrate\nthat our MDPNet outperforms state-of-the-art methods, and has\nlower running time than other transformer-based approaches.\nThe main contributions of this article are as follows.\n1) We propose a multiscale embedding block to embed the\nmultiscale information of the LRMS and PAN images\ninto two sequences of embedding vectors. Then, the trans-\nformer only needs to process these two sequences without\nany down-sampling operations and separate treatments for\nefﬁciency.\n2) A feature fusion module based on the AHAT is proposed to\nfuse the LRMS and PAN embedding sequences. Consid-\nering domain-speciﬁc knowledge, we use additive spatial\ninformation injection in the AHAT to transfer the texture\nand structure features from the PAN embedding sequence\nto the LRMS one.\n3) A detail generation module based on the CSAT is proposed\nto generate details considering the correlations among\nfeature channels. The module enhances the fused feature\nmaps via interaction along the channel dimension, and\nthus improves the quality of resulting details.\nThe rest of this article is organized as follows. SectionII re-\nviews related works. SectionIII elaborates the proposed method.\nSection IV analyses the experimental results. Finally, SectionV\nconcludes this article.\nII. RELATED WORK\nA. Additive Detail Injection-Based Methods\nAdditive detail injection is a uniﬁed framework for traditional\nCS-based and MRA-based pansharpening methods [41].C S\napproaches typically use linear transformations and only sub-\nstitute the spatial component. Thus, the transformation and\nsubstitution process can be recast into an additive detail injection\nmodel as follows[42]:\nHk = ˜Lk + Gk ·(P −IL) (1)\nwhere Hk denotes the kth band of the desired HRMS image.\n˜L ∈ RH×W×B is the interpolated LRMS image at the PAN\nscale, where W and H are the width and height of the PAN\nimage. B is the number of MS bands.Gk denotes the injection\ngain matrix.P ∈ RH×W×1 is the PAN image.IL is the intensity\ncomponent of˜L. The calculation approaches ofIL and Gk dis-\ntinguish different CS-based methods. The GSA[43] algorithm\ndetermines the optimal weights to obtainIL via multivariate\nregression at the reduced resolution. The BDSD[13] calculates\nIL for each MS band separately with different weights.\nMRA-based methods typically rely on an iterative decompo-\nsition process to obtain the low-pass PAN imagePL [9].A n d\nthe difference betweenP and PL is the detail to be added to˜L.\nThe general additive detail injection model for MRA approaches\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2707\ncan be formulated as\nHk = ˜Lk + Gk ·(P −PL) . (2)\nThe ways to computePL and Gk are the main differences among\nMRA-based methods. AWL[14] method uses shift-invariant “à\ntrous” wavelet decomposition to extract the PAN details and\nadds them to the intensity component of˜L. MTF-GLP[16] uses\nGaussian ﬁlters that match the MS sensor’s MTF to extractPL,\nwhich makes the obtained detail just complement˜L.\nThe additive detail injection model has also inspired many\nDL-based methods and gives them an explicit physical interpre-\ntation. Detail injection-based CNN (DiCNN)[44] learns details\nin an end-to-end manner, and thus, they can be directly added\nto ˜L to avoid separately predictingGk and PL. FusionNet[45]\nuses the difference between the duplicated PAN image and˜L as\nthe network input to avoid the calculation ofIL or PL, and\nthus, overcomes the limitations of traditional CS and MRA\nschemes. In remote sensing, addition is also a popular strategy\nto fuse distinctive image representations extracted by different\nnetworks. For example, Hong et al.[46] fuse the features ex-\ntracted by miniGCNs and CNNs via additive strategy to improve\nhyperspectral image classiﬁcation performance.\nB. Multiscale Deep Neural Networks for Pansharpening\nMultiscale information contained in the LRMS and PAN\nimages has proven to be quite useful in the pansharpening\ntask. MSDCNN [30] extracts such information using parallel\nconvolutional layers of different kernel sizes, which comple-\nments a fundamental serial CNN and signiﬁcantly improves\nits performance. In addition to using convolutional kernels of\ndifferent sizes, image pyramids can also be used to introduce\nthe multiscale property into DL methods. Laplacian pyramid\npansharpening petwork (LPPN)[47] applies pyramidal decom-\nposition to both PAN and LRMS images. Then, multiple sub-\nnetworks are utilized to process the pyramid layers separately,\nwhich facilitates the full use of multiscale information.\nC. Vision Transformers (ViTs) for Pansharpening\nThe ViT [35] partitions an image into patches and embeds\nthem into a sequence of vectors. Thus, the long-range depen-\ndencies among patches can be modeled by transformer blocks\nvia attention mechanisms. With the success of the ViT in image\nrecognition tasks, many attempts have been made to apply\nViT to high-resolution image processing[39], [48]. In remote\nsensing, a group-wise spectral embedding approach is proposed\nin SpectralFormer[49] to focus on the spectral characteristics for\naccurate hyperspectral image classiﬁcation. This demonstrates\nthat a ﬂexible embedding strategy can deeply affect the function\nof transformers.\nIn the pansharpening ﬁeld, ViTs have also achieved promising\noutcomes. Zhou et al.[37] proposed a customized transformer\nfor pansharpening and put it into a detail injection-based frame-\nwork to extract long-range features. HyperTransformer [50]\ncaptures multiscale long-range features by using transformer\nblocks at different scales of a backbone network. DR-NET[38]\nincorporates transformer blocks in the encoder of a Unet-like\nCNN [51] to reduce the loss of details during down sampling.\nIn this article, the proposed method avoids down sampling to\nimprove spatial detail preservation and efﬁciency. The utiliza-\ntion of multiscale information is carried out by the embedding\nprocess. Furthermore, similarities in both spatial and channel\ndimensions are captured by the proposed AHAT and CSAT\nblocks to generate high-quality details for detail injection.\nIII. METHODOLOGY\nA. Overall Network Architecture\nThe overall architecture of the proposed MDPNet is depicted\nin Fig. 1, which consists of two multiscale embedding blocks,\na feature fusion module based on the AHAT and a detail gen-\neration module based on the CSAT.˜L and P are embedded\ninto multiscale embedding sequencesE0\nL and EP through two\nmultiscale embedding blocks, respectively. The embedding se-\nquences are fused via two stacked AHAT blocks, one with a\nregular window partitioning strategy (W-AHAT) and the other\nwith a shifted window partitioning strategy (SW-AHAT)[39].\nLong-range features along the spatial dimension are captured\nthrough these two blocks. The two stacked AHAT blocks output\nfused vector sequenceE2\nL as follows:\nE2\nL =S W-AHAT\n(\nW-AHAT\n(\nE0\nL,EP\n)\n,EP\n)\n. (3)\nSubsequently, the fused sequenceE2\nL is reshaped back to feature\nmaps F and fed into the CSAT-based detail generation module\nto produce residual detailsD ∈ RH×W×B. The HRMS image\nH ∈ RH×W×B is obtained by adding D to ˜L following the\nwidely used detail injection framework. The detail generation\nand injection process can be summarized as follows:\nH = ˜L +C S A T(F) . (4)\nThe speciﬁc structures of the multiscale embedding block, the\nAHAT-based feature fusion module, and the CSAT-based detail\ngeneration module will be elaborated in the following.\nB. Multiscale Embedding Block\nA ss h o w ni nF i g .2, for each pixel in the input image, the\nmultiscale embedding module split out s different sizes of\npatches centered on the pixel and ﬂattens them into vectors.\nIt is noteworthy that the 1×1 pixel itself is also retained as a\nvector to preserve details at the ﬁnest scale. All the vectors are\nprojected tol-dimensional embedding vectors via corresponding\nlinear layers, respectively. Then, these embedding vectors are\nconcatenated to form ansl-dimensional multiscale embedding\nvector, which represents the multiscale information around the\npixel. Finally, The multiscale embedding vectors for all the\npixels in˜Land Pcomprise the multiscale embedding sequences\nE0\nL ∈ RHW ×sl and EP ∈ RHW ×sl, respectively. Both the high-\nresolution property and multiscale information of ˜L and P\nare maintained in the two sequences for long-range feature\nextracting and merging.\n2708 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 1. Architecture of the proposed MDPNet.↑ 4 denotes up sampling the LRMS image by a factor of 4 using bicubic interpolation.\nFig. 2. Schematic diagram of the multiscale embedding block.\nC. AHAT-Based Feature Fusion Module\nThe vanilla transformer block[34] consists of a self-attention\nmechanism and a feed-forward network. The self-attention\nmechanism can capture self-similarity and extract long-range\nfeatures from an embedding sequence. However, in the pan-\nsharpening task, there are LRMS and PAN embedding sequences\nthat need long-range feature extraction and fusion. Besides, in\nthe additive detail injection framework, both spectral and spatial\ninformation in˜L and P are useful for detail generation since the\ndetails have a spectral dimension, which has already been proven\nin [45]. This inspires us to design a hybrid attention mechanism\nthat can extract useful spectral and spatial features related to the\nup-sampled LRMS image˜L to prepare for generating comple-\nmentary details.\nThus, in the proposed additive hybrid attention (AHA), the\nLRMS multiscale embedding sequenceEi\nL (E0\nL for W-AHAT,\nE1\nL for SW-AHAT) is linearly projected to the query matrixQ.\nBoth Ei\nL and EP should serve as keys and values to extract\nspectral and spatial information related to Q. In the vanilla\nFig. 3. Structure of the (S)W-AHAT blocks. The W-AHAT block uses regular\nwindow partitioning and merging. The SW-AHAT block uses shifted window\npartitioning and merging[39].\nViT [35], the positional information of the image patches is\nembedded in a position embedding sequence and added to the\npatch embedding sequence. Similarly, the information of PAN\npatches, i.e.,EP, is linearly transformed to PAN keysKP and\nPAN valuesVP, and they are separately added to the LRMS keys\nKL and LRMS valuesVL for the injection of PAN information,\nas shown in Fig.3. It is noteworthy thatEP mainly contains\nspatial information andEi\nL is rich of spectral information. Thus,\nthe linear layers that transform them into keys and values have\ndifferent weights. The calculation of the proposed AHA can be\nsummarized as follows:\nAHA (Q,KL,VL,KP,VP)\n= softmax\n(\nQ(KL + KP)T\n√\nd\n)\n(VL + VP) (5)\nwhere d is the dimension of query vectors inQ.\nThe goal of the AHA is to extract long-range fused features\nfrom Ei\nL and EP. However, the local ﬁne-grained detail features\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2709\nFig. 4. Structure of the CSAT block.\nare also vital to the pansharpening task. The convolution layer\nis naturally good at local reception. Thus, a convolutional detail\nenhancement short cut (DESC) is designed to add local detail\nfeatures. Speciﬁcally,EP is reshaped back to feature maps and\npropagated to the DESC. It has a 3×3 depth-wise convolution\nlayer between two point-wise convolution layers, which can save\nparameters and makes it lightweight[52]. The output of the\nDESC is ﬂattened to a vector sequence and added to the output of\nAHA. Finally, the output of the AHATEi+1\nL is obtained through\na standard multilayer perceptron (MLP).\nIn addition, the computation complexity of the attention is\nquadratic to the length of embedding sequences, i.e.,HW.T o\nreduce the computational burden, the shifted window partition-\ning approach in[39] is employed. Speciﬁcally, in the W-AHAT\nblock, regular window partitioning and merging conﬁgurations\nare used. In the SW-AHAT block, the window partitioning\nand merging conﬁgurations are shifted half the window size\nof the W-AHAT. The rest of the two blocks are exactly the\nsame.\nD. CSAT-Based Detail Generation Module\nThe AHAT-based feature fusion module only captures\nthe dependencies along the spatial dimension. However, the\ncorrelations along the channel dimension are not captured,\nwhich will affect the generation of detailsD.T h u s ,w ep r o -\npose a channel self-attention (CSA) in our detail generation\nmodule.\nBefore input to the CSA,E2\nL is reshaped to feature maps\nF ∈ RH×W×sl and preconvoluted via a 1×1 convolution. The\nCSA learns residuals to enhance the preconvoluted feature maps\nFP. Speciﬁcally, as shown in Fig.4, three 3×3 convolution\nlayers with a stride of 2 are used to squeeze the information\nof FP into smaller feature maps. Then, each channel of the\nsqueezed feature maps is ﬂattened to a vector, which is a channel\ndescriptor containing the global information of the channel. The\nchannel descriptors are linearly projected to queries and keys.\nEach channel ofFP is directly ﬂattened to a value to avoid spatial\ninformation loss in squeezing and projecting. Subsequently,\nthrough dot-product attention, the self-similarity among FP\nchannels is captured and the residuals are obtained by a 3×3\nconvolution layer to enhance FP. Finally, the details D are\ngenerated from the enhancedFP via a 1×1 convolution layer.\nIV . EXPERIMENTAL RESULTS\nA. Datasets\nThe experiments are conducted on two datasets collected by\nQB and WV3 satellites. The QB data have four MS bands, while\nthe WV3 data have eight MS bands. Since the real HRMS images\nare unavailable, we follow Wald’s protocol[53] to spatially\ndegrade the LRMS and PAN images by a factor of 4 (the spatial-\nresolution ratio between them), and the original LRMS image\ncan be used as the reference image for supervised learning and\nreduced-resolution evaluation. We also perform full-resolution\nevaluation with the original LRMS and PAN images, but there\nare no reference images for assessment.\nAll the reduced-resolution and full-resolution images are\nrandomly cropped into LRMS patches with a size of 32×32\nand PAN patches with a size of 128×128. We crop the images\nfrom the top left to the bottom right with a ﬁxed stride that is\ngreater than the patch size. As a result, 11 216 QB and 11 160\nWV3 reduced-resolution patch pairs are extracted. To generate\nthe training, validation and reduced-resolution testing sets, the\nQB and WV3 patch pairs are divided into 8974/1121/1121 and\n8928/1116/1116 patch pairs in a ratio of 8:1:1, respectively. For\nfull-resolution testing, we also crop 1121 QB and 1116 WV3\nfull-resolution patch pairs.\nB. Implementation Details\nThe proposed method is implemented using the PyTorch\nframework and trained with an NVIDIA GeForce RTX 3090\nGPU. Our model is trained for 500 epochs by optimizing theℓ1\nloss between the fused image and the reference image. To im-\nplement the optimization, we employ an AdamW[54] optimizer\nwith an initial learning rate of 0.0005, a momentum of 0.9,β1\n= 0.9, β2 = 0.999, and a weight decay coefﬁcient of 0.05. The\nminibatch size is set to 16.\nConsidering the tradeoff between computational complexity\nand performance, the number of scales is set tos = 5 and the\nembedding dimension of each scale is set tol = 12 by default.\nThus, in the AHAT blocks, the dimension of embedding vectors,\nqueries, keys, and values issl = 60. The window size in the\nshifted window partitioning approach of the AHAT blocks is\nset to 8. Since the dimension of embedding vectorssl = 60 is\nrelatively low, the number of heads of the AHA is set to 2. In\nthe CSAT, the channel number of feature maps is also set to 60.\nC. Compared Methods and Quantitative Metrics\nThe MDPNet is compared with nine representative methods,\nincluding two CS algorithms: GSA[43],B D S D[13]; one MRA\nmethod: MTF-GLP-FS [55]; one VO-based method: TV[20],\nthree CNN-based methods: PNN [27],M S D C N N[30], and\nPanCSC-Net [56]; and two transformer-based methods: Zhou\net al.[37] and DR-NET[38].\nFive widely used metrics are adopted for the quantitative\nassessment of the methods. The metrics can be grouped into\nfour full-reference indicators and one no-reference indicator\naccording to whether they require a reference HRMS im-\nage in their calculations. For reduced-resolution assessment,\n2710 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nFig. 5. Visual results on the QB reduced-resolution testing set. (a) Degraded LRMS image. (b) Degraded PAN image. (c) Reference image. (d) GSA. (e) BDSD.\n(f) MTF-GLP-FS. (g) TV . (h) PNN. (i) MSDCNN. (j) PanCSC-Net. (k) Zhou et al.[37] (l) DR-NET. (m) MDPNet.\nFig. 6. Residual maps between the results and the reference in Fig.5. (a) GSA. (b) BDSD. (c) MTF-GLP-FS. (d) TV . (e) PNN. (f) MSDCNN. (g) PanCSC-Net.\n(h) Zhou et al.[37] (i) DR-NET. (j) MDPNet.\nwe measure the four full-reference indexes including spectral\nangle mapper (SAM)[57], Erreur Relative Globale Adimension-\nnelle de Synthese (ERGAS)[58], spatial correlation coefﬁcient\n(sCC) [59], and theQ2n [60], [61] index (i.e., Q4 for four-band\ndata and Q8 for eight-band data). ERGAS andQ2n evaluate the\noverall quality of pansharpening results, SAM estimates spectral\ndistortions, and sCC measures the quality of spatial details. For\nfull-resolution assessment, we employ the no-reference index\nhybrid quality with no reference (HQNR)[62] with its spectral\ndistortion component Dλ and spatial distortion componentDS\nto measure the pansharpening quality in the absence of the\nreference HRMS image.\nD. Reduced Resolution Assessment\nFig. 5 shows the visual results of the compared algorithms\non a QB reduced-resolution testing patch pair. The red box\narea is enlarged in the corner of the image to provide clearer\nvisual comparison. To highlight the differences, residual maps\nbetween the pansharpening results and the reference image are\nvisualized in Fig.6. A pixel with a small mean absolute error\n(MAE) is shown in blue and a pixel with a big MAE is displayed\nin yellow. From the red buildings in the enlarged view, it can be\nobserved that the results of GSA and MTF-GLP-FS are lighter\nin color than the reference image. In the results of BDSD and\nTV , the red buildings appear a little redder and a little pinker,\nrespectively. The fusion image of the PNN shows an apparent\nyellow tint. In the result of the MSDCNN, the reﬂection of the\nsun on the red rooftop is missing and slight blurring effects\nappear on the building edges. For the outcomes of PanCSC-Net,\nZhou et al.[37] and DR-NET, the color of the little red building\nin the enlarged view is lighter than that of the reference image.\nAccording to the enlarged view in the residual maps, it can also\nbe found that the residuals of PanCSC-Net, DR-NET, and Zhou\net al.[37] are larger than those of the proposed MDPNet. These\nﬁndings prove that our method possesses a better visual effect\nand fewer errors than the compared methods.\nFig. 7 displays the visual results of the compared methods\non a WV3 reduced-resolution testing patch pair. Fig.8 shows\nthe corresponding residual maps. In the enlarged view of GSA,\nBDSD, and MTF-GLP-FS fusion results, the color of the rooftop\nis apparently whiter than that in the reference image, which\nis an obvious spectral distortion. Also suffering from evident\nspectral distortion, the rooftop in the enlarged view of the TV\nresult appears darker in color than that in the reference image.\nAccording to the enlarged view in the residual maps, it can be\nfound that the residual maps of PNN and MSDCNN have larger\nyellow areas than the transformer-based methods. In the result\nof PanCSC-Net, the color near building borders is lighter than\nthe reference image. Among the transformer-based approaches,\nthe residual maps of Zhou et al.[37] and DR-NET have more\nyellow points with a large MAE than the proposed MDPNet in\nthe enlarged view, which demonstrates that our method achieves\nmore accurate prediction.\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2711\nFig. 7. Visual results on the WV3 reduced-resolution testing set. (a) Degraded LRMS image. (b) Degraded PAN image. (c) Reference image. (d) GSA. (e) BDSD.\n(f) MTF-GLP-FS. (g) TV . (h) PNN. (i) MSDCNN. (j) PanCSC-Net. (k) Zhou et al.[37] (l) DR-NET. (m) MDPNet.\nFig. 8. Residual maps between the results and the reference in Fig.7. (a) GSA. (b) BDSD. (c) MTF-GLP-FS. (d) TV . (e) PNN. (f) MSDCNN. (g) PanCSC-Net.\n(h) Zhou et al.[37] (i) DR-NET. (j) MDPNet.\nTABLE I\nAVERAGE VALUES AND STANDARD DEVIATIONS (STD) OF THE QUANTITATIVECOMPARISON ON1121 QB REDUCED-RESOLUTION TESTING PATCHES\nTableI lists the quantitative assessment results of all the com-\npared methods across the 1121 pairs of QB reduced-resolution\ntesting patches, including the mean value and standard deviation\n(STD) on each evaluation index. The best result of each index\nis shown in boldface, and the second-best result is underlined.\nFrom the full-reference indicators, it can be found that the\ntraditional methods generally fall behind the DL-based methods\nover the QB reduced-resolution data. Among the DL-based\nmethods, using deep multiscale features, the MSDCNN yields\nmuch better quantitative results than the simple three-layer PNN.\nPanCSC-Net shows better performance than PNN and MSD-\nCNN, but is slightly inferior to Zhou et al.[37] and DR-NET. The\ntransformer-based Zhou et al.[37] and DR-NET slightly surpass\nthe CNN-based methods on all the metrics, while the proposed\nMDPNet achieves better quantitative results than the compared\ntransformer-based approaches on the QB reduced-resolution\ntesting data.\nTable II reports the quantitative assessment results of all the\ncompared methods across the 1116 pairs of WV3 reduced-\nresolution testing patches. Compared to the QB data with\nfour MS bands, the WV3 data contain eight MS bands and\nare more challenging, especially in spectral preservation. For\nthe spectral distortion indicator SAM, the DL-based methods\nyield obviously better values than the traditional algorithms.\nOn the other metrics, the DL-based approaches also show\nsuperior performance. The CNN-based PNN and MSDCNN\nhave close quantitative results in terms of all the indicators\non the WV3 reduced-resolution testing data, and MSDCNN is\nslightly better. PanCSC-Net surpasses the classical CNN-based\nmethods PNN and MSDCNN, and yields results close to Zhou\n2712 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE II\nAVERAGE VALUES AND STD OF THE QUANTITATIVECOMPARISON ON1116 WV3 REDUCED-RESOLUTION TESTING PATCHES\nFig. 9. Visual results on the QB full-resolution testing set. (a) LRMS image. (b) PAN image. (c) GSA. (d) BDSD. (e) MTF-GLP-FS. (f) TV . (g) PNN.\n(h) MSDCNN. (i) PanCSC-Net. (j) Zhou et al.[37] (k) DR-NET. (l) MDPNet.\net al.[37] and DR-NET. The transformer-based methods slightly\noutperform PanCSC-Net on the WV3 reduced-resolution test-\ning data. The proposed MDPNet yields slightly better quan-\ntitative results than Zhou et al.[37] and DR-NET on all the\nmetrics.\nE. Full-Resolution Assessment\nFig. 9 shows the visual results of the compared approaches on\na QB full-resolution testing patch pair. It can be observed that\nthe red rooftop in the enlarged view of the GSA fusion result is\nobviously lighter in color. The edges of the red rooftop in the\nfusion image of the BDSD are oversaturated. The outcome of\nMTF-GLP-FS suffers from both lighter colors and oversaturated\nedges. In the enlarged view of the TV fusion result, obvious\ncolor artifacts can be found on the rooftops. The red rooftop\nin the result of the PNN has a yellowish hue, and that in the\nresult of the MSDCNN suffers from blurring effects. The shadow\nover the red rooftop in the result of PanCSC-Net is so dark that\nit looks unnatural. In the outcome of Zhou et al.[37] the red\nrooftops present slight color distortion and artifacts. The result\nof DR-NET has an oversaturation problem on the edges of the\nrooftop. By comparison, it can be found that the fusion result\nof the proposed MDPNet presents the best spatial and spectral\nﬁdelity.\nFig. 10 displays the visual results of the compared approaches\non a WV3 full-resolution testing patch pair. Through the en-\nlarged view, it can be observed that the building in the fusion\nresults of GSA, MTF-GLP-FS, and MSDCNN has a lighter\ncolor, while that in the fusion image of BDSD is too dark.\nThe fusion results of TV , PanCSC-Net and DR-NET suffer from\nsome color artifacts. In the enlarged view of the fusion results\nof the PNN and Zhou et al.[37] a small number of pixels with\nabnormal colors also appear on the rooftop. The fusion image\nof the proposed MDPNet has clear spatial details and higher\nspectral ﬁdelity.\nTable III lists the quantitative results across the 1121 pairs\nof QB full-resolution testing patches. The GSA and TV have\na higher spatial distortion indexDS and a higher spectral dis-\ntortion indexDλ, respectively. The BDSD shows a balance on\nthe metrics, and MTF-GLP-FS yields a much betterDλ value\nthan other traditional methods. On the whole, the traditional\nmethods fall behind the DL-based methods. The MSDCNN\nperforms much better on the spectral indexDλ than the PNN.\nThe transformer-based Zhou et al.[37] and DR-NET slightly\noutperform the CNN-based methods on all the metrics and Zhou\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2713\nFig. 10. Visual results on the WV3 full-resolution testing set. (a) LRMS image. (b) PAN image. (c) GSA. (d) BDSD. (e) MTF-GLP-FS. (f) TV . (g) PNN.\n(h) MSDCNN. (i) PanCSC-Net. (j) Zhou et al.[37] (k) DR-NET. (l) MDPNet.\nTABLE III\nAVERAGE VALUES AND STD OF THE QUANTITATIVECOMPARISON ON1121 QB\nFULL-RESOLUTION TESTING PATCHES\nTABLE IV\nAVERAGE VALUES AND STD OF THE QUANTITATIVECOMPARISON ON1116\nWV3 FULL-RESOLUTION TESTING PATCHES\net al.[37] performs better than DR-NET. Although PanCSC-Net\nis inferior to the transformer-based methods on the reduced-\nresolution data, it shows better results on the full-resolution QB\ndata. The proposed MDPNet achieves even better quantitative\nresults than PanCSC-Net on all three metrics.\nTable IV reports the quantitative results across the 1116 pairs\nof WV3 full-resolution testing patches. From the no-reference\nindicators, it can be found that GSA has unsatisfactory overall\nfusion quality. BDSD performs poorly on theDλ index. MTF-\nGLP-FS and TV have much better quantitative results than other\ntraditional methods and even surpass the CNN-based methods.\nThe Dλ values of PNN and MSDCNN are relatively poor.\nThe transformer-based Zhou et al.[37] yields the second-best\nspatial distortion indexDS and is slightly superior to DR-NET.\nPanCSC-Net performs satisfactorily onDλ and DS indexes, and\nhas the second-best HQNR value. The proposed MDPNet yields\nthe second-best Dλ value, while itsDS and HQNR values are\nbetter than all the compared methods.\nF . Ablation Study\nSince the multiscale embedding blocks, AHAT-based feature\nfusion module and CSAT-based detail generation module are the\ncore of the proposed MDPNet, a series of ablation experiments\nare conducted to verify their effectiveness. The results of the\nablation experiments will be presented and analyzed in the\nfollowing.\n1) Multiscale Embedding Blocks:The multiscale embedding\nblock embeds s = 5 different sizes of image patches centered\non each pixel into a multiscale embedding vector. To test the\neffect of each scale and the total number of scaless,av a r i e t yo f\nmodule settings are tested under the condition that the dimension\nof a multiscale embedding vector is kept assl = 60 for the\nfairness of comparison. Speciﬁc module settings are reported in\nTableV. MDPNet represents the full proposed method. To study\nthe effect of only using the information at one scale, model\nvariants with sufﬁxes o1–o5 are tested. To study the effect of\nscale numbers, model variants with sufﬁxes s4–s2 reduce one\nscale per step. Note that MDPNet-o1 is equivalent to MDPNet-\ns1, and the proposed MDPNet is equivalent to MDPNet-s5.\nTable VI reports the quantitative evaluation results corre-\nsponding to the experimental settings in TableV. The SAM,\nERGAS, sCC, and Q4 indexes are measured on 1121 QB\nreduced-resolution testing patch pairs. TheDλ, DS, and HQNR\n2714 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nTABLE V\nEXPERIMENTAL SETTINGS ON THEABLATION STUDY OF THEMULTISCALE\nEMBEDDING BLOCKS\nTABLE VI\nAVERAGE ABLATION STUDY RESULTS OF THEMULTISCALE EMBEDDING\nBLOCKS ON THE1121 QB REDUCED-RESOLUTION AND 1121 QB\nFULL-RESOLUTION TESTING PATCHES\nindexes are measured on 1121 QB full-resolution testing patch\npairs. From MDPNet-o1 to MDPNet-o5, the fusion performance\non the reduced-resolution data improves slightly in general\nas the size of the embedded patch grows larger (i.e., scale\nis increased). But the performance on the full-resolution data\nﬂuctuates greatly. This indicates that adopting a larger embedded\npatch size makes the embedding vector contain more useful in-\nformation in the reduced-resolution case, while the single-scale\nembedding tends to be less robust on the full-resolution data.\nAs for the inﬂuence of the number of scaless, compared with\nMDPNet-o1, MDPNet-s2 adds the embedding of 3×3 patches,\nand the performance is signiﬁcantly improved at both reduced\nand full resolutions. From MDPNet-s2 to MDPNet-s4, the fusion\neffect is constantly promoted on the reduced-resolution data as\nthe embedding of larger patches is added, and the increments are\nlarger than those from MDPNet-o1 to MDPNet-o5. The results\nof MDPNet-s2 are slightly inferior to those of ACPMT-o5.\nHowever, the fusion results of MDPNet-s3, MDPNet-s4, and\nMDPNet are far superior to those of MDPNet-o5, which indi-\ncates that the performance enhancement brought by multiscale\ninformation is far greater than that by only increasing a single\nscale. On the full-resolution data, MDPNet-s2 to MDPNet-s4\nshow smaller performance ﬂuctuations. Furthermore, the gen-\neral performance of MDPNet-s2 to MDPNet-s4 is slightly better\nthan that of MDPNet-o1 to MDPNet-o5. The proposed MDPNet\nuses ﬁve scales to obtain the best fusion results, which proves the\nsuperiority of the proposed multiscale embedding blocks over\nstandard single-scale patch embedding.\n2) AHAT-Based Feature Fusion Module:To verify the ef-\nfectiveness of the AHAT-based feature fusion module, a series\nof model variants are designed and tested. Fig.11 shows the\nTABLE VII\nAVERAGE ABLATION STUDY RESULTS OF THEFEATURE FUSION AND DETAIL\nGENERATION MODULES ON THE1121 QB REDUCED-RESOLUTION AND 1121\nQB FULL-RESOLUTION TESTING PATCHES\nstructural changes that we made in the model variants. As\nshown in Fig.11(a), the model w/o AHAT removes the entire\nAHAT-based feature fusion module and directly addsEP to E0\nL\nfor fusion. TableVII lists the quantitative results of the ablation\nexperiments for the feature fusion module. Among them, the\nresults of w/o AHAT are far worse than those of the full MDPNet,\nwhich proves the key role that the AHAT-based feature fusion\nmodule plays in our MDPNet. The AHA and the DESC are\ntwo core operations of the AHAT. As shown in Fig.11(b),t h e\nvariant Self-Attention+DESC (SA+DESC) replaces the AHA\nwith the standard self-attention to verify the superiority of the\nAHA. On the other hand, as shown in Fig.11(c), the model Only\nAHA removes the DESC and retains only the AHA to verify the\neffectiveness of the DESC. It can be seen from TableVII that\nthe performances of SA+DESC and Only AHA are signiﬁcantly\ninferior to the full MDPNet, which conﬁrms the importance of\nthe AHA and the DESC. Adding PAN keys and values to MS\nkeys and values apparently improves the fusion performance,\nand a CNN-based short cut is also helpful for transformers in\nfeature fusion.\n3) CSAT-Based Detail Generation Module:To validate the\nCSAT-based detail generation module, two ablation experiments\nare conducted. As shown in Fig.11(d), the variant w/o CSAT\nreplaces the entire CSAT-based detail generation module with\nas i m p l e3×3 convolution layer to prove the necessity of the\nmodule. Furthermore, as shown in Fig.11(e), the variant w/o\nCSA veriﬁes the signiﬁcance of the CSA by removing the\nCSA from the detail generation module. Table VII lists the\nquantitative results of the ablation experiments. It can be found\nthat the results of w/o CSA are slightly better than those of w/o\nCSAT, which proves the positive effect of the CSAT-based detail\ngeneration module. Besides, the performance of w/o CSAT and\nw/o CSA on the reduced-resolution data is signiﬁcantly inferior\nto that of the full MDPNet, but on the full-resolution data, w/o\nCSAT and w/o CSA yield remarkable performance close to the\nMDPNet, especially on the spectral distortion indexDλ.T h i s\nindicates that the CSAT tends to improve the spatial quality of\npansharpening outcomes at full resolution. An underlying cause\nmay be that the spectral preservation is mainly guaranteed by\nthe skip connection that inject the detailsD to the up-sampled\nLRMS image ˜L, while the CSAT is primarily responsible for\nreﬁning the detail of features to generate high-qualityD.\nG. Computational Efﬁciency\nTo further evaluate the computational efﬁciency and model\ncomplexity of the proposed method, the network parameters\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2715\nFig. 11. Structural change diagrams of the model variants in the ablation study for the feature fusion and detail generation modules. (a) W/o AHAT. (b)SA+DESC.\n(c) Only AHA. (d) W/o CSAT. (e) W/o CSA.\nTABLE VIII\nNUMBER OF PARAMETERS (#PARAM.) AND AVERAGE RUNNING TIME OF\nDIFFERENT METHODS ON THE1121 QB REDUCED-RESOLUTION\nTESTING PATCHES\nand the average running time of all the compared methods on\nthe 1121 QB reduced-resolution testing images are measured in\nTableVIII. The traditional methods are tested on a 2.6-GHz Intel\nCore i7-10750H CPU, while the DL-based methods are tested\non an NVIDIA GeForce RTX 2060 GPU. By comparison, it\ncan be found that the running time of TV is signiﬁcantly longer\nthan those of other methods, while the running times of PNN and\nMSDCNN are signiﬁcantly shorter than those of other DL-based\nmethods due to their simple network architecture. Compared to\nother transformer-based methods, the proposed MDPNet has\na much shorter average running time, which demonstrates the\ngreater efﬁciency of our MDPNet. As for the number of pa-\nrameters, DR-NET has signiﬁcantly more parameters than other\nmethods. This is because the quantity of feature maps throughout\nthe DR-NET is large, which inevitably leads to a large number\nof parameters in the network layers. PanCSC-Net has the fewest\nparameters, but its running time is relatively long. Note that\nthe number of parameters is not directly related to the running\ntime. There could be a lot of time-consuming operations without\nparameters in a DL-based method. Although DR-NET has a\nlarge number of parameters, it shows a relatively short running\ntime. The number of parameters in our MDPNet is acceptable,\nmuch lower than that of DR-NET but higher than those of\nMSDCNN and Zhou et al.[37] Although the proposed MDPNet\nhas more parameters, its running time is relatively short.\nV. CONCLUSION\nIn this article, we propose a pansharpening network based on\nmultiscale embedding and dual attention transformers, termed\nMDPNet. To avoid the inefﬁciency caused by directly com-\nbining the transformer with the classical multiscale network\narchitecture, we propose the multiscale embedding block to\nembed multiscale information of the images into two embedding\nsequences. Then, the transformers only need to process these two\nembedding sequences to make full use of multiscale information.\n2716 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 17, 2024\nMoreover, considering domain-speciﬁc knowledge, we propose\nthe AHAT, in which the PAN spatial information is added to\nthe MS keys and values for long-range feature extraction and\ninformation fusion. Finally, the CSAT is proposed to capture the\ncorrelations along the channel dimension and further enhance\nthe fused feature maps. Experimental results on QB and WV3\ndatasets demonstrate that the proposed MDPNet outperforms\nthe different kinds of pansharpening methods in terms of both\nvisual effects and quantitative metrics, and its running time is\nshorter than the compared transformer-based methods. More-\nover, ablation studies veriﬁed the effectiveness of the multiscale\nembedding block, AHAT, and CSAT.\nIn the future, we will make efforts to reduce the number of\nvalues and keys in the proposed AHAT while maintaining its\neffectiveness on the pansharpening task, which might result in\nfewer network parameters and higher efﬁciency. Moreover, there\nis bound to be some redundant information among the multiscale\nembedding vectors. Efforts will be made to remove redundancy\nwithin the embedding process.\nREFERENCES\n[1] P. Ghamisi et al., “Multisource and multitemporal data fusion in remote\nsensing: A comprehensive review of the state of the art,”IEEE Geosci.\nRemote Sens. Mag., vol. 7, no. 1, pp. 6–39, Mar. 2019.\n[2] F. Bovolo, L. Bruzzone, L. Capobianco, A. Garzelli, S. Marchesi, and F.\nNencini, “Analysis of the effects of pansharpening in change detection on\nVHR images,”IEEE Geosci. Remote Sens. Lett., vol. 7, no. 1, pp. 53–57,\nJan. 2010.\n[3] M. T. Amare, S. T. Demissie, S. A. Beza, and S. H. Erena, “Land cover\nchange detection and prediction in the Fafan catchment of Ethiopia,”J.\nGeovisualization Spatial Anal., vol. 7, no. 2, 2023, Art. no. 19.\n[4] J. K. Gilbertson, J. Kemp, and A. Van Niekerk, “Effect of pan-sharpening\nmulti-temporal landsat 8 imagery for crop type differentiation using dif-\nferent classiﬁcation techniques,”Comput. Electron. Agriculture, vol. 134,\npp. 151–159, 2017.\n[5] Y . Zhang, “Understanding image fusion,”Photogramm. Eng. Remote\nSens., vol. 70, no. 6, pp. 657–661, 2004.\n[6] V . Arora, E. Yin-Kwee Ng, and A. Singh, “Machine learning and its\napplications,” in Smart Electrical and Mechanical Systems, R. Sehgal,\nN. Gupta, A. Tomar, M. D. Sharma, and V . Kumaran, Eds. New York, NY ,\nUSA: Academic Press, 2022, ch. 1, pp. 1–37. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/B9780323907897000026\n[7] Y . Bouslihim, M. H. Kharrou, A. Miftah, T. Attou, L. Bouchaou, and A.\nChehbouni, “Comparing pan-sharpened Landsat-9 and Sentinel-2 for land-\nuse classiﬁcation using machine learning classiﬁers,”J. Geovisualization\nSpatial Anal., vol. 6, no. 2, Art. no. 35, 2022.\n[8] G. Vivone et al., “A critical comparison among pansharpening algo-\nrithms,”IEEE Trans. Geosci. Remote Sens., vol. 53, no. 5, pp. 2565–2586,\nMay 2015.\n[9] G. Vivone et al., “A new benchmark based on recent advances in multispec-\ntral pansharpening: Revisiting pansharpening with classical and emerging\npansharpening methods,”IEEE Geosci. Remote Sens. Mag., vol. 9, no. 1,\npp. 53–81, Mar. 2021.\n[10] G. Vivone, M. Dalla Mura, A. Garzelli, and F. Paciﬁci, “A benchmarking\nprotocol for pansharpening: Dataset, preprocessing, and quality assess-\nment,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14,\npp. 6102–6118, Jun. 2021.\n[11] W. Carper, T. Lillesand, and R. Kiefer, “The use of intensity-hue-saturation\ntransformations for merging spot panchromatic and multispectral image\ndata,” Photogrammetric Eng. remote Sens., vol. 56, no. 4, pp. 459–467,\n1990.\n[12] C. A. Laben and B. V . Brower, “Process for enhancing the spatial resolution\nof multispectral imagery using pan-sharpening,” U.S. Patent 6 011 875,\nJan. 4, 2000.\n[13] A. Garzelli, F. Nencini, and L. Capobianco, “Optimal MMSE pan sharp-\nening of very high resolution multispectral images,”IEEE Trans. Geosci.\nRemote Sens., vol. 46, no. 1, pp. 228–236, Jan. 2008.\n[14] J. Nunez, X. Otazu, O. Fors, A. Prades, V . Pala, and R. Arbiol,\n“Multiresolution-based image fusion with additive wavelet decomposi-\ntion,” IEEE Trans. Geosci. Remote Sens., vol. 37, no. 3, pp. 1204–1211,\nMay 1999.\n[15] J. G. Liu, “Smoothing ﬁlter-based intensity modulation: A spec-\ntral preserve image fusion technique for improving spatial de-\ntails,” Int. J. Remote Sens. , vol. 21, no. 18, pp. 3461–3472, 2000,\ndoi: 10.1080/014311600750037499.\n[16] B. Aiazzi, L. Alparone, S. Baronti, A. Garzelli, and M. Selva, “MTF-\ntailored multiscale fusion of high-resolution ms and pan imagery,”Pho-\ntogrammetric Eng. Remote Sens., vol. 72, no. 5, pp. 591–596, 2006.\n[17] D. Fasbender, J. Radoux, and P. Bogaert, “Bayesian data fusion for adapt-\nable image pansharpening,”IEEE Trans. Geosci. Remote Sens., vol. 46,\nno. 6, pp. 1847–1857, Jun. 2008.\n[18] Y . Zhang, S. De Backer, and P. Scheunders, “Noise-resistant wavelet-based\nBayesian fusion of multispectral and hyperspectral images,”IEEE Trans.\nGeosci. Remote Sens., vol. 47, no. 11, pp. 3834–3843, Nov. 2009.\n[19] M. R. Vicinanza, R. Restaino, G. Vivone, M. Dalla Mura, and J. Chanussot,\n“A pansharpening method based on the sparse representation of injected\ndetails,” IEEE Geosci. Remote Sens. Lett., vol. 12, no. 1, pp. 180–184,\nJan. 2015.\n[20] F. Palsson, J. R. Sveinsson, and M. O. Ulfarsson, “A new pansharpening\nalgorithm based on total variation,”IEEE Geosci. Remote Sens. Lett.,\nvol. 11, no. 1, pp. 318–322, Jan. 2014.\n[21] X. Otazu, M. Gonzalez-Audicana, O. Fors, and J. Nunez, “Introduction\nof sensor spectral response into image fusion methods. application to\nwavelet-based methods,” IEEE Trans. Geosci. Remote Sens., vol. 43,\nno. 10, pp. 2376–2385, Oct. 2005.\n[22] M. Ghahremani and H. Ghassemian, “Remote-sensing image fusion based\non curvelets and ICA,”Int. J. Remote Sens., vol. 36, no. 16, pp. 4131–4143,\n2015.\n[23] H. Shen, M. Jiang, J. Li, Q. Yuan, Y . Wei, and L. Zhang, “Spatial–spectral\nfusion by combining deep learning and variational model,”IEEE Trans.\nGeosci. Remote Sens., vol. 57, no. 8, pp. 6169–6181, Aug. 2019.\n[24] D. Hong et al., “Cross-city matters: A multimodal remote sensing\nbenchmark dataset for cross-city semantic segmentation using high-\nresolution domain adaptation networks,”Remote Sens. Environ., vol. 299,\nArt. no. 113856, 2023. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0034425723004078\n[25] D. Hong et al., “SpectralGPT: Spectral foundation model,” 2023,\narXiv:2311.07113.\n[26] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using\ndeep convolutional networks,”IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 38, no. 2, pp. 295–307, Feb. 2016.\n[27] G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa, “Pansharpening\nby convolutional neural networks,”Remote Sens., vol. 8, no. 7, 2016,\nArt. no. 594.\n[28] J. Yang, X. Fu, Y . Hu, Y . Huang, X. Ding, and J. Paisley, “PanNet: A\ndeep network architecture for pan-sharpening,” inProc. IEEE Int. Conf.\nComput. Vis., 2017, pp. 1753–1761.\n[29] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[30] Q. Yuan, Y . Wei, X. Meng, H. Shen, and L. Zhang, “A multiscale and\nmultidepth convolutional neural network for remote sensing imagery pan-\nsharpening,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 11,\nno. 3, pp. 978–989, Mar. 2018.\n[31] Q. Liu, H. Zhou, Q. Xu, X. Liu, and Y . Wang, “PSGAN: A generative\nadversarial network for remote sensing image pan-sharpening,” IEEE\nTrans. Geosci. Remote Sens., vol. 59, no. 12, pp. 10227–10242, Dec. 2021.\n[32] J. Ma, W. Yu, C. Chen, P. Liang, X. Guo, and J. Jiang, “Pan-GAN: An\nunsupervised pan-sharpening method for remote sensing image fusion,”\nInf. Fusion, vol. 62, pp. 110–120, 2020. [Online]. Available: https://www.\nsciencedirect.com/science/article/pii/S1566253520302591\n[33] H. Zhou, Q. Liu, and Y . Wang, “PGMAN: An unsupervised generative\nmultiadversarial network for pansharpening,”IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 6316–6327, 2021.\n[34] A. Vaswani et al., “Attention is all you need,” inAdv. Neural Inf. Process.\nSyst., 2017, pp. 5998–6008.\n[35] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” inProc. Int. Conf. Learn. Representations,\n2020.\n[36] J. Li, W. Fan, T. Lian, and F. Liu, “Cross-attention-based common and\nunique feature extraction for pansharpening,”IEEE Geosci. Remote Sens.\nLett., vol. 20, pp. 1–5, Oct. 2023.\nFAN et al.: PANSHARPENING VIA MULTISCALE EMBEDDING AND DUAL ATTENTION TRANSFORMERS 2717\n[37] M. Zhou, J. Huang, Y . Fang, X. Fu, and A. Liu, “Pan-sharpening with\ncustomized transformer and invertible neural network,” inProc. AAAI\nConf. Art. Intell., 2022, pp. 3553–3561. [Online]. Available: https://ojs.\naaai.org/index.php/AAAI/article/view/20267\n[38] X. Su, J. Li, and Z. Hua, “Transformer-based regression network for pan-\nsharpening remote sensing images,”IEEE Trans. Geosci. Remote Sens.,\nvol. 60, Feb. 2022, Art. no. 5407423.\n[39] Z. Liu et al., “Swin Transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[40] P. Zhang, Y . Mei, P. Gao, and B. Zhao, “Cross-interaction kernel attention\nnetwork for pansharpening,”IEEE Geosci. Remote Sens. Lett., vol. 20,\nJun. 2023, Art. no. 5001505.\n[41] W. Dou, Y . Chen, X. Li, and D. Z. Sui, “A general framework for\ncomponent substitution image fusion: An implementation using the fast\nimage fusion method,” Comput. Geosci., vol. 33, no. 2, pp. 219–228,\n2007. [Online]. Available: https://www.sciencedirect.com/science/article/\npii/S0098300406001245\n[42] L.-J. Deng et al., “Machine learning in pansharpening: A benchmark, from\nshallow to deep networks,”IEEE Geosci. Remote Sens. Mag., vol. 10, no. 3,\npp. 279–315, Sep. 2022.\n[43] B. Aiazzi, S. Baronti, and M. Selva, “Improving component substitution\npansharpening through multivariate regression of MS+pan data,”IEEE\nTrans. Geosci. Remote Sens., vol. 45, no. 10, pp. 3230–3239, Oct. 2007.\n[44] L. He et al., “Pansharpening via detail injection based convolutional neural\nnetworks,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 12,\nno. 4, pp. 1188–1204, Apr. 2019.\n[45] L.-J. Deng, G. Vivone, C. Jin, and J. Chanussot, “Detail injection-based\ndeep convolutional neural networks for pansharpening,” IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 8, pp. 6995–7010, Aug. 2021.\n[46] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph con-\nvolutional networks for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 7, pp. 5966–5978, Jul. 2021.\n[47] C. Jin, L.-J. Deng, T.-Z. Huang, and G. Vivone, “Laplacian pyra-\nmid networks: A new approach for multispectral pansharpening,”Inf.\nFusion, vol. 78, pp. 158–170, 2022. [Online]. Available: https://www.\nsciencedirect.com/science/article/pii/S1566253521001809\n[48] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” inProc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 568–578.\n[49] D. Hong et al., “Spectralformer: Rethinking hyperspectral image classi-\nﬁcation with transformers,”IEEE Trans. Geosci. Remote Sens., vol. 60,\nNov. 2022, Art. no. 5518615.\n[50] W. G. C. Bandara and V . M. Patel, “Hypertransformer: A textural and\nspectral feature fusion transformer for pansharpening,” inProc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit., 2022, pp. 1767–1777.\n[51] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks\nfor biomedical image segmentation,” in Proc. Int. Conf. Med. Image\nComput. Comput.- Assist. Interv., 2015, pp. 234–241.\n[52] D. Haase and M. Amthor, “Rethinking depthwise separable convo-\nlutions: How intra-kernel correlations lead to improved mobilenets,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2020,\npp. 14600–14609.\n[53] L. Wald, T. Ranchin, and M. Mangolini, “Fusion of satellite images of\ndifferent spatial resolutions: Assessing the quality of resulting images,”\nPhotogrammetric Eng. Remote Sens., vol. 63, no. 6, pp. 691–699, 1997.\n[Online]. Available: https://hal.science/hal-00365304\n[54] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in\nProc. Int. Conf. Learn. Representations, 2019.\n[55] G. Vivone, R. Restaino, and J. Chanussot, “Full scale regression-based\ninjection coefﬁcients for panchromatic sharpening,”IEEE Trans. Image\nProcess., vol. 27, no. 7, pp. 3418–3431, Jul. 2018.\n[56] X. Cao, X. Fu, D. Hong, Z. Xu, and D. Meng, “PanCSC-Net: A model-\ndriven deep unfolding method for pansharpening,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Oct. 2022, Art. no. 5404713.\n[57] R. H. Yuhas, A. F. Goetz, and J. W. Boardman, “Discrimination among\nsemi-arid landscape endmembers using the spectral angle mapper (SAM)\nalgorithm,” in Proc. 3rd Annu. JPL Airborne Geosci. Workshop, 1992,\npp. 147–149.\n[58] L. Wald, Data Fusion: Deﬁnitions and Architectures: Fusion of Images\nof Different Spatial Resolutions. Paris, France: Les Presses de l’École des\nMines, 2002.\n[59] J. Zhou, D. L. Civco, and J. Silander, “A wavelet transform method to\nmerge landsat TM and spot panchromatic data,”Int. J. Remote Sens.,\nvol. 19, no. 4, pp. 743–757, 1998.\n[60] L. Alparone, S. Baronti, A. Garzelli, and F. Nencini, “A global quality\nmeasurement of pan-sharpened multispectral imagery,” IEEE Geosci.\nRemote Sens. Lett., vol. 1, no. 4, pp. 313–317, Oct. 2004.\n[61] A. Garzelli and F. Nencini, “Hypercomplex quality assessment of\nmulti/hyperspectral images,” IEEE Geosci. Remote Sens. Lett.,v o l .6 ,\nno. 4, pp. 662–665, Oct. 2009.\n[62] B. Aiazzi, L. Alparone, S. Baronti, R. Carlá, A. Garzelli, and L. Santurri,\n“Full-scale assessment of pansharpening methods and data products,” in\nProc. Image Signal Process. Remote Sens., 2014, pp. 1–12.\nWensheng Fan received the B.S. degree in network\nengineering from the North University of China,\nTaiyuan, China, in 2018, and the M.S. degree in\nsoftware engineering from the Taiyuan University of\nTechnology, Jinzhong, China, in 2023. He is cur-\nrently working toward the Ph.D. degree in electri-\ncal engineering with the College of Electrical and\nPower Engineering, Taiyuan University of Technol-\nogy, Taiyuan.\nHis research interests include deep learning, in-\ntelligent control theory and application, and remote\nsensing image processing.\nFan Liu (Member, IEEE) received the Ph.D. degree\nin pattern recognition and intelligent systems from\nXidian University, Xian, China, in 2014.\nShe is currently an Associate Professor with the\nCollege of Computer Science and Technology (Col-\nlege of Data Science), Taiyuan University of Technol-\nogy, Jinzhong, China. Her main research interests in-\nclude remote sensing image processing and machine\nlearning.\nJingzhi Li received the B.S. degree in software en-\ngineering in 2020 from the Taiyuan University of\nTechnology, Jinzhong, China, where she is currently\nworking toward the M.S. degree in software engi-\nneering with the College of Computer Science and\nTechnology (College of Data Science).\nHer research interests include deep learning and\nremote sensing image processing."
}