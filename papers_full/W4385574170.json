{
  "title": "EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation",
  "url": "https://openalex.org/W4385574170",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100826716",
      "name": "Tao Ge",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5107977475",
      "name": "Siqing Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014662947",
      "name": "Furu Wei",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970868759",
    "https://openalex.org/W3092521391",
    "https://openalex.org/W2153013403",
    "https://openalex.org/W2610986956",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W2144950812",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W3212480971",
    "https://openalex.org/W3168991979",
    "https://openalex.org/W3155622660",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W3005444338",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W3119821914",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3120074043",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385570274",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W3174851730",
    "https://openalex.org/W3119303959",
    "https://openalex.org/W2983872911",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2967827612",
    "https://openalex.org/W4221146568",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W3207622241",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3105306115",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3122154272",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3135335819"
  ],
  "abstract": "We introduce EdgeFormer – a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to perform better given the same parameter budget; moreover, EdgeFormer is further enhanced by layer adaptation innovation that is proposed for improving the network with shared layers.Extensive experiments show EdgeFormer can effectively outperform previous parameter-efficient Transformer baselines and achieve competitive results under both the computation and memory constraints. Given the promising results, we release EdgeLM – the pretrained version of EdgeFormer, which is the first publicly available pretrained on-device seq2seq model that can be easily fine-tuned for seq2seq tasks with strong results, facilitating on-device seq2seq generation in practice.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10786–10798\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEdgeFormer: A Parameter-Efficient Transformer for\nOn-Device Seq2seq Generation\nTao Ge Si-Qing Chen Furu Wei\nMicrosoft\n{tage,sqchen,fuwei}@microsoft.com\nAbstract\nWe introduce EDGE FORMER – a parameter-\nefficient Transformer for on-device seq2seq\ngeneration under the strict computation and\nmemory constraints. Compared with the pre-\nvious parameter-efficient Transformers, EDGE -\nFORMER applies two novel principles for cost-\neffective parameterization, allowing it to per-\nform better given the same parameter budget;\nmoreover, EDGE FORMER is further enhanced\nby layer adaptation innovation that is proposed\nfor improving the network with shared layers.\nExtensive experiments show EDGE FORMER\ncan effectively outperform previous parameter-\nefficient Transformer baselines and achieve\ncompetitive results under both the computation\nand memory constraints. Given the promising\nresults, we release EDGE LM1 – the pretrained\nversion of EDGE FORMER , which is the first\npublicly available pretrained on-device seq2seq\nmodel that can be easily fine-tuned for seq2seq\ntasks with strong results, facilitating on-device\nseq2seq generation in practice.\n1 Introduction\nOn-device modeling draws increasing attention for\nits unique advantages (Dhar et al., 2019). On the\nother hand, strict resource constraints prevent many\nneural networks performing well in the on-device\nsetting. In Natural Language Processing (NLP),\non-device sequence-to-sequence (seq2seq) genera-\ntion remains challenging, especially for the Trans-\nformer (Vaswani et al., 2017) under strict resource\nconstraints in both computation and memory.\nTo customize the Transformer for seq2seq\ntasks in the on-device setting, we propose EDGE -\nFORMER – a novel parameter-efficient Trans-\nformer of the encoder-decoder architecture. EDGE -\nFORMER is structurally similar to the standard\nTransformer with a deep encoder and shallow de-\ncoder, but with a modification that it uses an in-\n1https://github.com/microsoft/unilm/tree/\nmaster/edgelm\nself-a'n\ncross-a'n\nFFN\nReLU\nWf2\nWf1\ndffn\nd\ndffn\nd\nself-a'n\ncross-a'n ReLU\nWf2\nWf1\ndffn\nd\ndffn\nd\nFFN\nFFN\n(a) (b)\nFigure 1: (a) Vanilla Transformer decoder layer in\nwhich dffn > d; (b) Interleaved Transformer decoder\nlayer with shared lightweight FFNs in which dffn < d.\nterleaved decoder with shared lightweight feed-\nforward networks, as shown in Figure 1. The mod-\nified decoder architecture allows EDGE FORMER\nto apply two novel principles that we propose for\ncost-effective parameterization: 1) encoder-favored\nparameterization that suggests we parameterize the\nencoder using as many parameters as possible; 2)\nload-balanced parameterization that suggests we\nbalance the load of model parameters to avoid them\nbeing either underused or overused in a neural net-\nwork with shared parameterization.\nIn addition to cost-effective parameterization,\nEDGE FORMER proposes and applies layer adap-\ntation to further improve the model with tied lay-\ners, as Figure 2 shows. Inspired by parameter-\nefficient task transfer, we investigate 3 efficient\nlayer adaptation approaches for improving the per-\nformance with negligible cost. We show EDGE -\nFORMER (with fewer than 10 million model param-\neters) largely outperforms the strong UNIVERSAL\nTRANSFORMER baselines in the on-device setting\nwith competitive results, and the int8-quantized\nEDGE FORMER can perform high-quality on-device\nseq2seq generation within around 100ms latency\n(20-30 sequence length on average) using two mid-\nto-high-end CPU cores and less than 50MB RAM.\nThe contributions of this work are three-fold:\n• This paper is one of the earliest work that\nformally studies on-device seq2seq genera-\ntion by discussing its challenges and defining\n10786\nShared\tWeights\n\tW /uni2208/uni211Dd×d\nA /uni2208/uni211Dd×r\nB /uni2208/uni211Dr×d\nd\nd\nr\nd\nself-a'n\n pm\n1  pm\nL\n /uni22EF hm\n1  hm\n2\n /uni22EF hm\nT−1  hm\nT\nFFN\n /uni22EF\n /uni22EF\n hm+1\n1  hm+1\n2  hm+1\nT−1  hm+1\nT\n(c) (d)\nencoder\t\t\nlayer\t1\nencoder\t\t\nlayer\t2\nencoder\t\t\nlayer\t3\n(a)\nencoder\t\t\nlayer\t1\nencoder\t\t\nlayer\t2\nencoder\t\t\nlayer\t3\n(b)\nBias\n-LA3\nBias\n-LA2\nBias\n-LA1\nFigure 2: (a) Encoder layers with shared weights (the same color) without layer adaptation: the tied weights\nundermine the specialities of encoder layers to process their specific inputs; (b) Bias-based Layer Adaptation\n(Bias-LA) employs free bias terms to adapt layers with tied weights to fit their specific roles well; (c) Adapter-LA\nuses a layer-specific LoRA adaptation block with rankr < dfor layer adaptation; (d) Prefix-LA uses L layer-specific\ntokens (i.e., learnable parameters) as the prefix (dotted square) to adapt the mth layer.\na practical setting with appropriate resource\nconstraints for the evaluation.\n• We propose EDGE FORMER , a parameter-\nefficient Transformer with novel cost-effective\nparameterization and layer adaptation, achiev-\ning the state-of-the-art result in the on-device\nseq2seq generation setting under strict com-\nputing and memory resource constraints.\n• We introduce and release EDGE LM (the pre-\ntrained EDGE FORMER ) – the first publicly\navailable pretrained on-device seq2seq model\nthat can be easily fine-tuned for seq2seq tasks\nwith strong results, which can largely reduce\nthe effort for delivering a powerful on-device\nseq2seq model in practice.\n2 Background: Transformer\n2.1 Architecture\nThe Transformer follows the encoder-decoder ar-\nchitecture. The Transformer encoder consists of a\nstack of encoder layers, each of which has a self-\nattention module parameterized by projection ma-\ntrices for the query, key, value and output: [ WQ,\nWK, WV , WO] whose shapes are all d ×d, fol-\nlowed2 by a feed-forward network (FFN) parame-\nterized by Wf1 ∈Rd×dffn and Wf2 ∈Rdffn ×d.\nThe Transformer decoder consists of a stack of\ndecoder layers whose architecture is similar to an\nencoder layer except for an additional cross atten-\ntion module between self-attention and FFN.\n2For simplicity, we omit discussing the layer normalization\nand residual connection that are not related with this work.\nIn summary, we understand that the main param-\neters in an encoder layer i are:\nΦei = [W{Q,K,V,O}\nei ,Wf1\nei ,Wf2\nei ]\n|Φei|= 4d2 + 2d ×dencffn. For a decoder layer j,\nits main parameters are:\nΦdj = [W{Q,K,V,O}\ndj ,W{Q,K,V,O}\ndj ,Wf1\ndj ,Wf2\ndj ]\nwhere W{Q,K,V,O}\ndj is the cross-attention module.\n|Φdj |= 8d2 + 2d ×ddecffn.\n2.2 Parameterization: Full vs Shared\nFull Parameterization Full parameterization is\na common parameterization approach for Trans-\nformer, meaning that each model parameter (ex-\ncluding embedding) is independent without being\nshared by multiple modules in the network. In a for-\nward pass, each parameter is used only once. Full\nparameterization allows parameters to be flexible\nto fit their roles well during model training.\nShared Parameterization Despite the advan-\ntages of full parameterization, it is criticized to use\nlarge numbers of parameters inefficiently, motivat-\ning shared parameterization where multiple mod-\nules in a network share parameters. For a model\nwith shared parameterization (e.g., ALBERT ), each\nmodel parameter is exploited more than once in a\nforward pass. The efficient parameterization can\nlead to a better result given the same parameteriza-\ntion budget, despite slowing down inference. On\nthe other hand, given a fixed architecture (i.e., the\nsame network depth and width), shared parameteri-\nzation usually underperforms full parameterization\nbecause it has much fewer free model parameters.\n10787\nLayer Module #params d= 512 d= 384 d= 768\n#params / FLOPS#params / FLOPS#params / FLOPS\nencoder layer self-attn 4d2\n3.15M / 95.4M 1.77M / 53.9M 7.08M / 214MFFN 8d2\nvanilla decoder layer\nself-attn 4d2\n4.20M / 128M 2.37M / 72.3M 9.45M / 286Mcross-attn 4d2\nFFN 8d2\ninterleaved decoder layer\nself-attn 4d2\n2.23M / 72.9M 1.25M / 41.3M 5.01M / 162Mcross-attn 4d2\n2 shared lightweight FFNsd2/2\nModel d= 512 d= 384 d= 768\n#params / FLOPS#params / FLOPS#params / FLOPS\n6+6 Transformer (full parameterization) 44M / 1.84G 25M / 1.13G 99M / 3.76G\n12+2 Transformer (full parameterization) 46M / 1.90G 26M / 1.17G 104M / 3.89G\n12+2 UNIVERSALTRANSFORMER(shared parameterization) 7.4M / 1.90G 4.1M / 1.17G 16.5M / 3.89G\nEdgeFormer (Ours) 8.6M / 1.79G 4.8M / 1.11G 19.2M / 3.65G\nTable 1: Top: #params and FLOPS for Transformer layers. For the encoder and vanilla decoder layer, dffn = 4d;\nwhile for the interleaved decoder layer, dffn = d/4. Bottom: #params and FLOPS of whole models, where #params\nexcludes embedding lookup, and FLOPS is measured on a sample with src/tgt length of 30 and 32K vocabulary.\n3 Constraints for On-device Seq2seq\nComputation On-device computer vision (CV)\nmodels tend to use 1G FLOPS (0.5G MACS) as a\nconstraint, which is directly followed by previous\nwork on on-device translation (Wu et al., 2020). In\nour work, however, we propose to relax the FLOPS\nconstraint for typical seq2seq tasks to 2G FLOPS\n(1G MACS) because the latency requirement for\non-device seq2seq generation is not so rigid as CV\ntasks and it is uncommon for an on-device seq2seq\nmodel to handle too many concurrent requests in\npractice. The relaxed constraint allows better pre-\ndiction quality that strongly correlates with user\nexperience for seq2seq tasks, but still ensure the\nCPU on edge devices to process tens of sentences\nper second, which is more than sufficient for an on-\ndevice seq2seq model. In addition to FLOPS that is\na theoretical hardware-independent measurement\nfor computational cost, we also require the runtime\nlatency for an input sentence (typically 20 ∼30 to-\nkens on average) to be within around 100ms using\ntwo mid-to-high-end CPU cores.\nMemory In contrast to deploying a model on a\ncloud server without caring about memory cost\nmuch, there is a very strict memory constraint for\nan on-device model in practice, because a user’s\nedge device (e.g., PC) is not only for model hosting;\ninstead, it usually runs many other (background)\napps and programs at the same time besides the\nmodel. To ensure moderate memory cost, we limit\nthe number of model parameters (excluding word\nembedding lookup table) up to 10 million, follow-\ning previous work (Wu et al., 2020), and require the\nruntime memory footprint to be less than 50MB.\n4 EdgeFormer\n4.1 Architecture\nThe biggest challenge for an on-device seq2seq\nmodel is regarding the model size and memory\ncost. As shown in Table 1, the number of parame-\nters of a standard Transformer-base model (d=512)\nis about 45 million (excluding the embedding pa-\nrameters), which is far beyond the parameterization\nbudget (10 million) and unavoidably leads to mas-\nsive memory cost despite acceptable FLOPS.\nEDGE FORMER is proposed to address the chal-\nlenge. Instead of disruptive architectural changes3\nas previous research (Wu et al., 2020; Mehta et al.,\n2020; Panahi et al., 2021), EDGE FORMER ’s archi-\ntecture basically follows the standard Transformer\nconsisting of a 12-layer encoder and 2-layer 4 de-\ncoder, which is efficient in decoding. We mainly\ndiscuss the model with d=512 in this paper since\nit can achieve good performance in the on-device\nsetting as long as it can be appropriately parame-\nterized. The minor architectural modification we\npropose for EDGE FORMER is using an interleaved\n3We basically follow the standard Transformer without\nmajor architectural changes because a standard Transformer\nshould be more widely compatible and supported than a cus-\ntomized model architecture in user devices with various envi-\nronments (e.g., hardware and runtime libraries).\n4We do not use 1-layer decoder because it does not consis-\ntently perform well (Sun et al., 2021).\n10788\n(a)\n (b)\nFigure 3: (a) Performance of 6+6 Transformer (d = 512) on the newstest2013 English-German (En-De) translation\ndataset (dev set): densely parameterizing the decoder is uneconomical and much less beneficial than parameterizing\nthe encoder; (b) Comparison of x+2 Transformer with full-/shared-parameterized x encoder layers on newstest2013\nEn-De dataset: when x > 6, the performance of the Transformer with shared parameterization only improves\nmarginally even if x continues to increase.\ndecoder where attention modules are interleaved\nwith shared lightweight5 FFNs (ddecffn < d; in this\nwork, ddecffn = d/4) in each decoder layer (shown\nin Figure 1). The modification is helpful for cost-\neffective parameterization (Section 4.2):\n• The interleaved structure makes the architec-\nture of encoder and decoder layers consis-\ntent (Ma et al., 2021), facilitating shared pa-\nrameterization of attention modules through-\nout the encoder and decoder.\n• As shown in Table 1, the lightweight FFNs\nthat interleave attention modules in the de-\ncoder reduce FLOPS and save a large number\nof parameters for decoder FFNs’ parameteri-\nzation that is very uneconomical.\n4.2 Cost-effective Parameterization\nDue to the tight parameterization budget (i.e., 10\nmillion), EDGE FORMER cannot be fully parameter-\nized as in the standard way; instead, it has to adopt\nshared parameterization.\nAs a strong baseline for shared parameteriza-\ntion, UNIVERSAL TRANSFORMER lets all its M\nencoder layers share 1 group of encoder layer pa-\nrameters and all its N decoder layers share 1 group\nof decoder layer parameters:\nΦe1\ntied\n= Φe2\ntied\n= · · ·\ntied\n= ΦeM\nΦd1\ntied\n= Φd2\ntied\n= · · ·\ntied\n= ΦdN\n5As observed by Kasai et al. (2020), reducing ddecffn does\nnot hurt the result much, as shown in Table 8 in Appendix A.\nAlthough UNIVERSAL TRANSFORMER is a pop-\nular solution to shared parameterization, it is still\nnot cost-effective for two reasons:\nFirst, UNIVERSAL TRANSFORMER uses (over)\nhalf of total parameters to parameterize the de-\ncoder, which is uneconomical. As shown in Figure\n3a, given a fixed architecture (6+6 Transformer,\nd = 512), densely parameterizing the decoder re-\nsults in much less performance gain than parame-\nterizing the encoder. This suggests we use as many\nparameters as possible to parameterize the encoder\nfor the performance.\nSecond, UNIVERSAL TRANSFORMER does not\nconsider load balance of model parameters, which\nwas a rarely discussed problem until the recent\nemergence of Mixture-of-Expert models (Fedus\net al., 2021). For the Transformers with a deep en-\ncoder and shallow decoder, UNIVERSAL TRANS -\nFORMER ’s parameterization method will overbur-\nden parameters in the encoder but underutilize pa-\nrameters in the decoder. For example, for a 12+2\nUNIVERSAL TRANSFORMER , a parameter in the\nencoder is used 12 times, while a parameter in the\ndecoder is used only twice in a forward pass. As\nshown in Figure 3b, moderately reusing parame-\nters (e.g., when x ≤4) helps better utilize the pa-\nrameters, resulting in significant performance gain\nwithout increasing parameters. However, as the\nshared parameters are overused (when x >6), the\nperformance gain will become marginal, which is\nintuitive because a parameter’s capability is limited.\nThis suggests we balance the load of parameters to\n10789\navoid them being either overused or underused.\nBased on the above insights, we parameterize\nEDGE FORMER in the following two novel princi-\nples for cost-effective parameterization:\nEncoder-favored Parameterization For EDGE -\nFORMER , we parameterize its encoder using as\nmany parameters as possible: except a small num-\nber of parameters (d2/2) for all lightweight FFNs\nin the decoder, we use almost all parameters in our\nbudget to parameterize the encoder. For attention\nmodules in the decoder, we let them reuse (i.e.,\nshare) parameters with the attention modules in\nthe encoder since attention modules in both the en-\ncoder and decoder work in the same mechanism\nand can be effectively shared (Dong et al., 2019).\nThanks to the interleaved decoder architecture that\nmakes the structure of encoder and decoder layers\nconsistent, we let the self-attention module in a de-\ncoder layer share parameters with its corresponding\nodd layer in the encoder, and let its cross-attention\nmodule share with the corresponding even layer in\nthe encoder, inspired by Ma et al. (2021):\nW[Q,K,V,O]\ndj\ntied= W[Q,K,V,O]\ne2j−1 (1 ≤j ≤2)\nW[Q,K,V,O]\ndj\ntied= W[Q,K,V,O]\ne2j (1 ≤j ≤2)\nLoad-balanced Parameterization We try pa-\nrameterizing EDGE FORMER with a balanced load\nfor each model parameter so that each parameter\ncould be as equally exploited as possible in a for-\nward pass. Given the parameterization budget and\nthe load balance principle, we create 2 groups of\nencoder FFN parameters equally shared by all en-\ncoder layers, 1 group of decoder FFN parameters is\nshared by light FFNs in the decoder, and 4 groups\nof attention parameters are shared throughout the\nencoder and decoder. Except for parameters in the\nencoder FFNs that are used 6 times, other parame-\nters are all used 4 times in a forward pass, resulting\nin a load balanced parameterization:\nW[Q,K,V,O]\nei\ntied= W[Q,K,V,O]\nei+4 (1 ≤i <9)\nW[f1,f2]\nei\ntied= W[f1,f2]\nei+2 (1 ≤i <11)\nW[f1,f2]\ndj\ntied= W[f1,f2]\nd1 (1 ≤j ≤2)\n4.3 Layer Adaptation\nShared parameterization causes layers with tied\nweights to become less specialized, as discussed in\nSection 1. To allow tied layers to be better adapted\nto their corresponding roles, we propose layer adap-\ntation to further enhance EDGE FORMER . Inspired\nby parameter-efficient task transfer methods, we in-\nvestigate three efficient layer adaption approaches:\nBias-based Layer Adaptation (Bias-LA) In-\nspired by BitFit (Ben Zaken et al., 2021) fine-\ntuning with only bias terms, we untie all bias terms\nof each layer and use them to specialize the layers\nwith tied weights, as shown in Figure 2(b). As Bit-\nFit, bias-based layer adaptation introduces very few\nadditional parameters without inference overhead.\nAdapter-based Layer Adaptation (Adapter-LA)\nAdapter-based approaches (Houlsby et al., 2019) in-\ntroduce adapter modules for NLP task transfer with-\nout full fune-tuning. We borrow this idea for layer\nadaptation by introducing an independent adapter\nmodule for each layer. Specifically, we adopt the\nrecently proposed LoRA (Hu et al., 2021) as our\nlayer adapter, as Figure 2(c) shows. In our experi-\nments, we apply the layer adapter to WQ and WV ,\nas the original paper of LoRA suggests.\nPrefix-based Layer Adaptation (Prefix-LA) In-\nspired by recent work (Li & Liang, 2021; Lester\net al., 2021) using a prefix/prompt for task transfer,\nwe introduce L tokens with learnable parameters\nas a specific prefix for each layer to adapt layers\nwith tied weights, as shown in Figure 2(d). The pre-\nfixs are only used for keys and values in attention\nmodules, which will not introduce much inference\noverhead as long as L is moderately set.\nFollowing the encoder-favored principle in Sec-\ntion 4.2, we only apply LA to encoder layers.\n5 Experiments\n5.1 Experimental Setting\nWe mainly evaluate our approach in Machine Trans-\nlation (MT). We select the most popular MT bench-\nmark – WMT14 English-German (En-De) transla-\ntion task, which is also a touchstone for seq2seq\nevaluation, as our main test bed. To compare with\nprevious work, we also evaluate WMT14 English-\nFrench (En-Fr) translation. We follow the standard\nway to train and evaluate evaluate WMT14 En-De\nand En-Fr. As Ott et al. (2018), we use a joint\nsource-target dictionary of 32K Byte Pair Encod-\ning (BPE) for En-De, and 40K BPE for En-Fr. We\nmainly use sacreBLEU (Post, 2018) for evaluation.\nWe selectUNIVERSAL TRANSFORMER which is\nthe most popular and a strong baseline of parameter-\n10790\nModel #Params FLOPS sacreBLEU\nTeacher 176M 6.7G 29.3\n6+6 Transformer (full enc, full dec) 44M 1.8G 28.5\n6+6 Transformer (full enc, shared dec) 23M 1.8G 28.2\n6+6 Transformer (full dec, shared enc) 28M 1.8G 27.3\n12+2 Transformer (full enc, full dec) 46M 1.9G 28.5\n12+2 Transformer (full enc, shared dec) 42M 1.9G 28.4\n12+2 Transformer (full dec, shared enc) 12M 1.9G 27.2\n12+2 UT 7.4M 1.9G 27.0\n12+2 UT (dffn = 2560) 8.5M 2.1G 27.2\n12+2 UT (dencffn = 3072)1 8.5M 2.3G 27.4\n12+2 UT (ddecffn = 3072) 8.5M 2.0G 27.0\nEDGE FORMER w/o LA2 8.6M 1.8G 27.7†(1)\nEDGE FORMER (Bias-LA) 8.6M 1.8G 27.8\nEDGE FORMER (Adapter-LA) (r= 32) 9.4M 1.8G 28.0†(2)\nEDGE FORMER (Prefix-LA) (L= 8) 8.6M 1.9G 28.0†(2)\nTable 2: WMT14 En-De results. To fairly compare with UNIVERSAL TRANSFORMER (UT) that is originally\nsmaller than EDGE FORMER , we also test UT with larger FFNs to make its model size comparable toEDGE FORMER .\n†(i) denotes p <0.05 in significance test compared with the model marked with i.\nFFNs Load #Params FLOPS sacreBLEU\n2 FFNs (dffn = 2048)1 6-6 8.6M 1.8G 27.7\n3 FFNs (dffn = 1536) 4-4-4 9.1M 1.6G 27.4†(1)\n4 FFNs (dffn = 1024) 3-3-3-3 8.6M 1.4G 27.2†(1)\n2 FFNs (dffn = 2048) 1-11 8.6M 1.8G 27.5†(1)\n2 FFNs (dffn = 2048) 11-1 8.6M 1.8G 27.4†(1)\nTable 3: Performance of EDGE FORMER with various encoder FFN parameterization on WMT14 En-De. Load 6-6\nmeans the 2 groups of FFN parameters are used 6 times each, while Load 1-11 means 1 group of FFN is used once,\nand the other is used 11 times.\nefficient Transformer for fair comparison. By de-\nfault, we apply Seq-KD (Kim & Rush, 2016) to\ntrain models and use the full-parameterized 6+6\nTransformer-big ( d = 1 , 024) model (Vaswani\net al., 2017; Ott et al., 2018) as the teacher.\nBy default, for each experiment, we train 5 mod-\nels with different initializations and report their\naverage evaluation results for Table 2, 3 and 6\nwith significance test. For inference, we use beam\nsearch (beam=5).\n5.2 Offline Evaluation\nWe evaluate EDGE FORMER and compare it with\nUNIVERSAL TRANSFORMER (UT) on WMT14\nEn-De. According to Table 2, the EDGE FORMER\nwithout layer adaptation (LA) largely outperforms\nUTs. Among the LA approaches, both Adapter-LA\nand Prefix-LA are clear to benefit the result with\nmarginal computational or parameterization cost,\nwhile Bias-LA does not show significant perfor-\nmance gain though it is the cheapest.\nAs discussed in Section 4.2, the advantage\nof EDGE FORMER over UT comes from its cost-\neffective parameterization. The encoder-favored\nprinciple is again supported by comparing 6+6\nTransformers’ results in Table 2, which is consis-\ntent with the observation on the dev set in Figure\n3a. To further understand the effectiveness of load-\nbalanced parameterization principles, we conduct\nan ablation study by adjusting encoder FFNs in\nEDGE FORMER . Table 3 shows the results ofEDGE -\nFORMER with various FFN parameterization. As\nwe reduce dffn (e.g., to 1536 or 1024), we can in-\ncrease the group of encoder FFN parameters and\nreduce their load given a fixed parameterization\nbudget. However, such a strategy leads to a clear\ndegradation of sacreBLEU. One reason is that the\nFFN parameters of a reduced load (3-4 times) are\nnot so fully utilized as the baseline (6 times) despite\nother reasons such as the differences of network\nshape (e.g., dffn). To minimize the effects of other\nfactors, we compare the first group with a balanced\n10791\n(a) (b)\nFigure 4: The effects of (a) rank r in Adapter-LA, and (b) prefix length L in Prefix-LA on the performance in\nWMT14 En-De. Note that r = 64will lead to exceed our parameterization budget despite better performance.\nModel #params FLOPS En-De En-Fr\n6+6 Transformer 44M 1.8G 28.3 41.0\n12+2 Transformer 46M 1.9G 28.4 41.4\n12+2 UT 7.4M 1.9G 26.2 39.2\nDeLighT 31.4M - 27.6 39.6\nShapeshifter 8.2M - 26.6 40.8\nLite Transformer (small) 2.9M 0.2G 22.5 35.3\nLite Transformer (medium) 11.7M 0.7G 25.6 39.1\nLite Transformer (big) 17.3M 1.0G 26.5 39.6\nEdgeFormer w/o LA 8.6M 1.8G 26.5 39.8\nEdgeFormer (Adapter-LA) 9.4M 1.8G 26.9 40.5\nEdgeFormer (Prefix-LA) 8.6M 1.9G 26.8 40.3\nTable 4: Result comparison to previous parameter-efficient Transformers that have fewer parameters than the\nbaseline Transformer (around 45M parameters). “-” means that the metrics are unavailable or not comparable in\nthe original paper. The underlines denote that the metrics cannot meet the on-device requirement. Note that all the\nmodels in this table do not apply Seq-KD.\nparameter load (i.e., 6-6) and the last group with a\nimbalanced parameter load (1-11 or 11-1), show-\ning load-balanced parameterization is consistently\nbetter than the imbalanced counterparts.\nAfter discussing parameterization, we then an-\nalyze the effects of layer adaptation on the results\nby mainly focusing on Adapter-LA and Prefix-LA\nthat both show performance gain. Figure 4 shows\nthe effects of the rank r in Adapter-LA and prefix\nlength L in Prefix-LA. As r increases, the model\nperformance will gradually improve. However,\nwhen r becomes large (e.g., r ≥64), it will ex-\nceed our parameterization budget and thus the gain\nwill become meaningless. As for prefix length\nL in Prefix-LA, it is different from r that it will\nnot keep improving the results as it increases: the\ngain can hardly be observed after some length (e.g.,\nL = 8), which is similar to the observation in\nprefix-tuning (Li & Liang, 2021). Therefore, we\nuse r = 32 and L = 8 as the default setting to\nreport the results of Adapter-LA and Prefix-LA.\nFinally, we compare EDGE FORMER with re-\ncent work on parameter-efficient Transformer mod-\neling. To keep consistency of the training and\nevaluation protocols with previous work, we here\ngive up using Seq-KD to train the models, and re-\nport BLEU (Papineni et al., 2002) for comparison.\nSpecifically, we compare with DeLighT (Mehta\net al., 2020), Shapeshifter (Panahi et al., 2021)\nand Lite Transformer (Wu et al., 2020), and show\nthe results in Table 4. However, it is notable that\nthe results are not strictly comparable because the\nprevious studies have their own focus and setting,\nwhich are different from ours. For example, De-\nLighT and Lite Transformer focus much more on\nFLOPS than the model size, thus they do not a\ndesirable tradeoff between the model quality and\nsize; while Shapeshifter’s goal is minimizing the\nmodel size despite an additional 10% ∼20% in-\nference overhead. Regardless of these factors that\n10792\nWMT14 En-De\nModel Disk size (7zip) Peak Memory Latency 1 Latency 2 sacreBLEU\nEdgeFormer (Adapter-LA, 32k vocab) 28MB 60MB 65ms 114ms 27.2\nEdgeFormer (Adapter-LA, 8k vocab) 15MB 47MB 59ms 101ms 27.1\nCoNLL-14\nModel Disk size (7zip) Peak Memory Latency 1 Latency 2 F0.5\nEdgeFormer (Adapter-LA, 2k vocab) 11MB 42MB 51ms 98ms 50.8\nTable 5: Runtime results for int8-quantized EDGE FORMER , in which Latency1 and Latency 2 denote the average\nlatency per sentence measured on the Intel® Xeon® E-2288G CPU and Qualcomm SM8150 Snapdragon 855 CPU,\nrespectively. We run through the test set with batch size=1, and use greedy decoding instead of beam search.\nModel #Param FLOPS CoNLL14 XSum SQuAD-NQG\nF0.5 RG-1 RG-2 RG-L B4 MTR RG-L\nTransformer-base 44M 1.8G 50.1 31.2 10.7 24.9 2.6∗ 9.0∗ 26.0∗\nPretrained 12+2 UT (dffn = 2048) 7.4M 1.4G 50.8 36.0 14.5 29.2 19.8 22.2 46.9\nPretrained 12+2 UT (dffn = 3072)1 9.4M 1.9G 51.1 36.7 14.9 29.7 20.1 22.4 47.1\nEDGELM 9.4M 1.3G 52.0(1) 37.2(1) 15.4(1) 30.3(1) 20.6(1) 23.0(1) 47.4(1)\nTable 6: The performance of EDGE LM in comparison with the baselines. ∗denotes that the results are from Chen\net al. (2019).\nprevent fair comparison, EDGE FORMER achieves\n26.9 BLEU in En-De under the strict on-device\nresource constraints, which outperforms the state-\nof-the-art Shapeshifter with the similar model size\ndespite. It is notable that EDGE FORMER here uses\nthe same model architecture and training configura-\ntion for both En-De and En-Fr, while Shapeshifter\nuses different model architecture configurations\nspecific for En-De and En-Fr, which may account\nfor its better performance in En-Fr.\n5.3 Runtime Evaluation\nWe conduct experiments in WMT14 En-De trans-\nlation and CoNLL-14 Grammatical Error Correc-\ntion6 (GEC) benchmark for runtime latency and\nmemory evaluation using onnxruntime7 that sup-\nports efficient seq2seq decoding. We apply int8-\nquantization to EDGE FORMER and test latency on\n2 devices: a 2-core Intel® Xeon® E-2288G CPU\n(in PC), and a 2-core Qualcomm SM8150 Snap-\ndragon 855 CPU (in Pixel 4), which are both cur-\nrent mid-to-high end CPUs launched 2-3 years ago.\nTable 5 shows runtime evaluation results. With\nint8-quantization and smaller vocabulary, EDGE -\nFORMER can not only meet the on-device seq2seq\nrequirements but also maintain its good perfor-\nmance, demonstrating its practical values.\n6We include experiments details of GEC in Appendix A.\n7https://github.com/microsoft/onnxruntime\n6 EdgeLM – The Pretrained EdgeFormer\nGiven the promising results, we introduce\nEDGE LM – the pretrained 8 EDGE FORMER\n(Adapter-LA) with 8K sentenpiece vocabulary with\nfactorized embedding (dembed = 128) through the\nsame self-supervised task (i.e., masked span infill-\ning) as T5 (Raffel et al., 2019) and make it publicly\navailable for downstream on-device seq2seq task\nfine-tuning.\nWe evaluate EDGE LM in the benchmarks of\nthree popular seq2seq tasks: CoNLL-14 for Gram-\nmatical Error Correction (GEC), XSum (Narayan\net al., 2018) for Abstractive Summarization, and\nSQuAD-NQG (Du et al., 2017) for Question Gen-\neration. According to Table 6, EDGE LM achieves\nsignificantly better performance than the pretrained\nUT models as well as the Transformer-base model\ntrained from scratch. We believe that EDGE LM,\nas the first publicly released on-device seq2seq\npretrained model, can largely facilitate on-device\nseq2seq generation in practice.\n7 Related Work\nOn-device seq2seq generation in NLP is a research\narea that has been less explored than on-device CV\nand NLU (Tambe et al., 2021). Besides the general\ntechniques like pruning, compression, quantiza-\ntion and knowledge distillation (Fan et al., 2019;\n8We include pretraining details in the Appendix B.\n10793\nXu et al., 2020; Li et al., 2022) that are orthogo-\nnal to our effort, parameter-efficient Transformer-\nbased seq2seq modeling is the most related re-\nsearch branch to ours. In this branch, UNIVERSAL\nTRANSFORMER (Dehghani et al., 2018) uses cross-\nlayer sharing method, which is the most popular\nsolution to parameter efficiency. Takase & Kiyono\n(2021) extends UNIVERSAL TRANSFORMER by\nstudying different ways for layer sharing, and Reid\net al. (2021) proposes to free the first and last en-\ncoder layer and widen the intermediate layers for\nbetter performance. However, both the approaches\nconsider parameter-efficiency only without caring\nabout latency becoming worse.\nIn addition to work improving parameter effi-\nciency by weight sharing, there is research that\nstudies lightweight model architecture for seq2seq\nlearning where early work (Gehring et al., 2017;\nWu et al., 2019) mainly focuses on CNNs, while\nrecent efforts have tended to switch to attention-\nbased models such as Mehta et al. (2020). Also,\nlow-rank factorization has been studied intensively\nto make the model tiny (Zhang et al., 2021; Panahi\net al., 2021); and hardware-aware network archi-\ntecture search with elastic modeling (Wang et al.,\n2020) has been proposed recently for facilitating\ndeployment of seq2seq models on various devices.\nAmong previous studies, the work of Wu et al.\n(2020) is the most related to ours, which studies\nseq2seq generation in an on-device setting. How-\never, it sets the computational constraint for on-\ndevice seq2seq to be the same with the CV tasks,\nwhich is too strict and unnecessary, as discussed\nin Section 3. As a result, their models focus on\nFLOPS optimization much more than memory,\nleading to an undesirable tradeoff between the\nquality and model size for the practical on-device\nseq2seq setting which should care about memory\nmuch more than latency. In contrast, our work care-\nfully evaluates bottleneck constraints, and proposes\nappropriate models with parameterization and layer\nadaptation innovations, largely improving the re-\nsults for practical on-device seq2seq generation.\n8 Conclusion and Future Work\nWe formally study on-device seq2seq generation,\nincluding defining its practical resource constraint\nsetting and proposing an appropriate modeling\ntechnology EDGE FORMER . The cost-effective pa-\nrameterization and layer adaptation innovations in\nEDGE FORMER both prove effective to improve\nthe results with negligible computation and mem-\nory cost, achieving state-of-the-art results in the\non-device seq2seq generation setting. Our re-\nleased pretrained EDGE FORMER – EDGE LM can\nbe easily fine-tuned for downstream seq2seq tasks,\nlargely facilitating on-device seq2seq generation in\npractice.\nFor future work, we plan to further study load-\nbalanced parameterization for parameter-efficient\nmodels, which is an interesting and new but seem-\ningly profound machine learning research problem:\ninstead of naively assuming that all the parameters\nare equal in this preliminary study, we suspect that\nparameters in different modules (e.g., parameters\nin the self-attn and FFN; or parameters in differ-\nent layers) should be under different amounts of\nload. We look forward to in-depth research on this\nproblem, which might be helpful to deepen our\nunderstanding of neural networks.\n9 Limitations\nEDGE FORMER is a preliminary model proposed\nfor on-device seq2seq generation setting, which\nstill has much room for improvement. For exam-\nple, as mentioned in Section 8, the current load\nbalance mechanism naively assumes that the num-\nber of times that a parameter is used in a forward\npass is equal to its load, which may not be always\ntrue because parameters in different moduels are\ndifferent: some parameters may be effectively used\nmore times than others, which requires deeper un-\nderstanding of neural network and the Transformer.\nAcknowledgments\nWe thank all the anonymous reviewers for their\nvaluable comments. We thank Xiaohu Tang,\nFucheng Jia, Yifan Yang and Huiqiang Jiang for\ntheir help in runtime evaluation, and thank Shum-\ning Ma, Ting Cao, Fan Yang, Qiufeng Yin, Yuqing\nYang and Lidong Zhou in Microsoft Research\nAsia for the discussion and helpful comments.\nWe also appreciate the support from Wenbing Li,\nYufeng Li, Bowen Bao, Ye Wang, Sunghoon Choi,\nScott McKay and Emma Ning in Microsoft AI\nFrameworks for onnxruntime, and appreciate the\nfeedback and valuable suggestions from Joshua\nBurkholder, Xun Wang, Weixin Cai and Zhang\nLi in Microsoft Office Intelligence regarding de-\ntailed constraints for on-device seq2seq generation\nin real-word applications.\n10794\nReferences\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.\nBitfit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. arXiv\ne-prints, pp. arXiv–2106, 2021.\nChristopher Bryant, Mariano Felice, Øistein E Ander-\nsen, and Ted Briscoe. The bea-2019 shared task on\ngrammatical error correction. In Proceedings of the\nFourteenth Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pp. 52–75, 2019.\nMengyun Chen, Tao Ge, Xingxing Zhang, Furu Wei,\nand Ming Zhou. Improving the efficiency of gram-\nmatical error correction with erroneous span detec-\ntion and correction. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 7162–7169. Association\nfor Computational Linguistics, 2020.\nYu Chen, Lingfei Wu, and Mohammed J Zaki. Rein-\nforcement learning based graph-to-sequence model\nfor natural question generation. arXiv preprint\narXiv:1908.04942, 2019.\nDaniel Dahlmeier and Hwee Tou Ng. Better evaluation\nfor grammatical error correction. In Proceedings of\nthe 2012 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 568–572, 2012.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Łukasz Kaiser. Universal trans-\nformers. arXiv preprint arXiv:1807.03819, 2018.\nSauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi,\nUnmesh Kurup, and Mohak Shah. On-device ma-\nchine learning: An algorithms and learning theory\nperspective. arXiv preprint arXiv:1911.00623, 2019.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong\nLiu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-\nWuen Hon. Unified language model pre-training\nfor natural language understanding and generation.\narXiv preprint arXiv:1905.03197, 2019.\nXinya Du, Junru Shao, and Claire Cardie. Learning to\nask: Neural question generation for reading compre-\nhension. In Association for Computational Linguis-\ntics (ACL), 2017.\nAngela Fan, Edouard Grave, and Armand Joulin. Re-\nducing transformer depth on demand with structured\ndropout. In International Conference on Learning\nRepresentations, 2019.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch\ntransformers: Scaling to trillion parameter models\nwith simple and efficient sparsity, 2021.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. Convolutional se-\nquence to sequence learning. In International Confer-\nence on Machine Learning, pp. 1243–1252. PMLR,\n2017.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pp.\n2790–2799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685,\n2021.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah A Smith. Deep encoder, shallow decoder:\nReevaluating non-autoregressive machine translation.\narXiv preprint arXiv:2006.10369, 2020.\nYoon Kim and Alexander M Rush. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947, 2016.\nTaku Kudo and John Richardson. Sentencepiece: A\nsimple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv\npreprint arXiv:1808.06226, 2018.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The\npower of scale for parameter-efficient prompt tuning.\narXiv preprint arXiv:2104.08691, 2021.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Opti-\nmizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\nZheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati,\nParminder Bhatia, Andrew Arnold, Bing Xiang, and\nDan Roth. Dq-bart: Efficient sequence-to-sequence\nmodel via joint distillation and quantization. arXiv\npreprint arXiv:2203.11239, 2022.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Sing-\nhal, Hany Hassan Awadalla, Xia Song, and Furu\nWei. Deltalm: Encoder-decoder pre-training for\nlanguage generation and translation by augmenting\npretrained multilingual encoders. arXiv preprint\narXiv:2106.13736, 2021.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Delight:\nDeep and light-weight transformer. arXiv preprint\narXiv:2008.00623, 2020.\nTomoya Mizumoto, Mamoru Komachi, Masaaki Na-\ngata, and Yuji Matsumoto. Mining revision log of\nlanguage learning sns for automated japanese error\ncorrection of second language learners. In Proceed-\nings of 5th International Joint Conference on Natural\nLanguage Processing, pp. 147–155, 2011.\n10795\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\nDon’t give me the details, just the summary! topic-\naware convolutional neural networks for extreme\nsummarization. arXiv preprint arXiv:1808.08745,\n2018.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian\nHadiwinoto, and Joel Tetreault. The CoNLL-2013\nshared task on grammatical error correction. In Pro-\nceedings of the Seventeenth Conference on Compu-\ntational Natural Language Learning: Shared Task,\npp. 1–12, Sofia, Bulgaria, August 2013. Associa-\ntion for Computational Linguistics. URL https:\n//aclanthology.org/W13-3601.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. The conll-2014 shared task on gram-\nmatical error correction. In Proceedings of the Eigh-\nteenth Conference on Computational Natural Lan-\nguage Learning: Shared Task, pp. 1–14, 2014.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. Scaling neural machine translation. arXiv\npreprint arXiv:1806.00187, 2018.\nAliakbar Panahi, Seyran Saeedi, and Tom Arodz.\nShapeshifter: a parameter-efficient transformer using\nfactorized reshaped matrices. Advances in Neural\nInformation Processing Systems, 34, 2021.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th\nannual meeting of the Association for Computational\nLinguistics, pp. 311–318, 2002.\nMatt Post. A call for clarity in reporting bleu scores.\narXiv preprint arXiv:1804.08771, 2018.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of trans-\nfer learning with a unified text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019.\nMachel Reid, Edison Marrese-Taylor, and Yutaka Mat-\nsuo. Subformer: Exploring weight sharing for pa-\nrameter efficiency in generative transformers. arXiv\npreprint arXiv:2101.00234, 2021.\nXin Sun, Tao Ge, Furu Wei, and Houfeng Wang. Instan-\ntaneous grammatical error correction with shallow\naggressive decoding. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pp. 5937–5947, 2021.\nSho Takase and Shun Kiyono. Lessons on parameter\nsharing across layers in transformers. arXiv preprint\narXiv:2104.06022, 2021.\nThierry Tambe, Coleman Hooper, Lillian Pentecost,\nTianyu Jia, En-Yu Yang, Marco Donato, Victor Sanh,\nPaul Whatmough, Alexander M Rush, David Brooks,\net al. Edgebert: Sentence-level energy optimiza-\ntions for latency-aware multi-task nlp inference. In\nMICRO-54: 54th Annual IEEE/ACM International\nSymposium on Microarchitecture, pp. 830–844, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing\nsystems, pp. 5998–6008, 2017.\nHanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai,\nLigeng Zhu, Chuang Gan, and Song Han. Hat:\nHardware-aware transformers for efficient natural lan-\nguage processing. arXiv preprint arXiv:2005.14187,\n2020.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. Pay less attention with lightweight\nand dynamic convolutions. In International Con-\nference on Learning Representations, 2019. URL\nhttps://arxiv.org/abs/1901.10430.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention.\narXiv preprint arXiv:2004.11886, 2020.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. Bert-of-theseus: Compressing bert\nby progressive module replacing. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 7859–\n7869, 2020.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\nA new dataset and method for automatically grading\nesol texts. In Proceedings of the 49th annual meet-\ning of the association for computational linguistics:\nhuman language technologies, pp. 180–189, 2011.\nAston Zhang, Yi Tay, Shuai Zhang, Alvin Chan,\nAnh Tuan Luu, Siu Cheung Hui, and Jie Fu. Beyond\nfully-connected layers with quaternions: Parameter-\nization of hypercomplex multiplications with 1/n\nparameters. arXiv preprint arXiv:2102.08597, 2021.\nWangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, and\nFuru Wei. Improving sequence-to-sequence pre-\ntraining via sequence span rewriting. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 571–582, 2021.\n10796\nA Details of Evaluations for MT and GEC\nFor MT, we follow the setting of Ott et al. (2018)\nto train the model on the WMT14 datasets. For En-\nDe, the training set contains 4.5M parallel sentence\npairs. We use newstest2013 as our dev set. For\nEn-Fr, there are 36M parallel sentence pairs for\ntraining, and we use newstest2012+2013 as the dev\nset.\nIn GEC evaluation, we follow previous\nwork (Chen et al., 2020; Zhou et al., 2021) to use\nthe BEA-19 restricted setting, training with Lang-\n8 (Mizumoto et al., 2011), FCE (Yannakoudakis\net al., 2011) and WI+LOCNESS (Bryant et al.,\n2019). We validate on CoNLL-13 shared task\ndataset (Ng et al., 2013), and test on CoNLL-14\nshared task dataset (Ng et al., 2014). After de-\nduplicating, we have around 900K sentence pairs\nfor training. Both the dev (CoNLL-13) and test\n(CoNLL-14) have 1.3K sampales. We train a sen-\ntencepiece (Kudo & Richardson, 2018) model of\n2K vocabulary for tokenization, and evaluate with\nthe metric of Max-Match (Dahlmeier & Ng, 2012)\nF0.5.\nFor model training configuration, we show in\nTable 7; The ablation study into the reduction of\nddecffn is presented in Table 8.\nConfigurations Values\nNumber of epochs 1000\nDevices 8 Nvidia V100 GPU\nMax tokens per GPU 20,000\nUpdate Frequency 4\nOptimizer Adam\n(β1=0.9,β2=0.99,ϵ=1×10−8)\nLearning rate 1×10−3\nLearning rate scheduler inverse sqrt\nWarmup 4000\nWeight decay 0.00001\nLoss Function label smoothed cross entropy\n(label-smoothing=0.1)\nDropout [0.1, 0.2] for MT, [0.3, 0.4, 0.5] for GEC\nTable 7: Training details for EDGE FORMER for NMT\nand GEC.\nddecffn #Param sacreBLEU\n2048 46M 26.5\n512 37M 26.4\n256 35M 26.4\n128 34M 26.4\nTable 8: The ablation study into the reduction of ddecffn\non a standard 6+6 Transformer on the dev set.\nConfigurations Values\nTotal updates 250,000\nDevices 8 × 8 Nvidia V100 GPU\nBatch size per GPU 128\nSample length 512\nOptimizer Adam\n(β1=0.9, β2=0.98 ϵ=1 × 10−6)\nLearning rate 5 × 10−4\nLearning rate scheduler polynomial\nclip norm 2.0\nWarmup 10,000\nLoss Function cross entropy\nWeight decay 0.0\nDropout 0.1\nTable 9: Pretraining details for EDGE LM.\nConfigurations Values\nTotal updates 100,000\nDevices 8 Nvidia V100 GPU\nMax tokens per GPU 20,000\nOptimizer Adam\n(β1=0.9,β2=0.98ϵ=1 × 10−6)\nLearning rate Vary for different downstream tasks\nLearning rate scheduler polynomial\nclip norm 1.0\nWarmup 8,000\nLoss Function cross entropy\nWeight decay 0.0\nDropout 0.1\nTable 10: Fine-tuning details for EDGE LM.\nB Configurations of Pretraining and\nFine-tuning\nWe pretrain EDGE FORMER with the same pretrain\ndata as RoBERTa (Liu et al., 2019), through the\nsame pretrain task as T5 (Raffel et al., 2019). The\ndetailed configuration of pretraining is shown in\nTable 9.\nFor downstream task fine-tuning, we present the\nconfiguration details in Table 10.\nC Detailed Evaluation of EdgeLM\nIn addition to the evaluation results presented in\nTable 6, we present more detailed results in Table\n11 and 12 to show the performance of EDGE LM\nin different sizes and with different vocabularies\nwith factorized embedding parameterization respec-\ntively. According to Table 11, EDGE LMs consis-\ntently outperform the pretrained UTs in the down-\nstream tasks under various model sizes with only\n70% computation cost of UTs.\nAccording to Table 12, we show that enlarging\nthe vocabulary size with factorized embedding is\neffective to improve abstractive summarization and\nquestion generation tasks, while it appears to have\nan adverse effect for GEC. One reason for the per-\nformance degradation is that GEC is more sensitive\n10797\nModel #Param FLOPS CoNLL-14 Xsum QG\nF0.5 RG-1 RG-2 RG-L B4 MTR RG-L\nTransformer 44M 1.8G 50.1 31.2 10.7 24.9 2.6 9.0 26.0\nPretrained UT (d= 512, dffn = 3072) 9.4M 1.9G 51.1 36.7 14.9 29.7 20.1 22.4 47.1\nEDGELM (d= 512) 9.4M 1.3G 52.0 37.2 15.4 30.3 20.6 23.1 47.4\nPretrained UT (d= 384, dffn = 2432) 5.5M 1.1G 50.1 31.9 11.5 25.7 17.9 21.0 45.9\nEDGELM (d= 384) 5.5M 0.8G 50.1 32.3 11.7 26.0 18.9 21.2 45.9\nPretrained UT (d= 768, dffn = 4608) 21.2M 4.2G 52.8 37.3 15.6 30.4 20.9 23.5 47.5\nEDGELM (d= 768) 21.4M 2.9G 53.1 37.9 15.9 30.8 21.0 23.6 47.8\nTable 11: The comparison between pretrained Universal Transformer (UT) and EdgeLM with Adapter-based layer\nadaptation (r = d/8). We enlarge UT’s dffn to let its model size comparable with its EdgeFormer counterpart with\nthe same d for fair comparison given the same parameter budget though introducing additional cost.\nVocab dembed #Param (including embedding) CoNLL-14 Xsum QG\nF0.5 RG-1 RG-2 RG-L B4 MTR RG-L\nspm2k 512 10.4M 52.7 36.3 14.8 29.5 19.0 21.7 46.3\nspm8k 128 10.5M 52.0 37.2 15.4 30.3 20.6 23.1 47.4\nspm16k 64 10.5M 51.6 37.1 15.1 30.0 19.6 22.3 46.7\nTable 12: EDGE LM (d = 512) with Adapter-LA (r = 32) using different vocabulary and dembed. Except the model\nwith spm2k whose dembed = d, the models with spm8k and spm16k use factorized embedding to prevent increasing\nthe total parameters.\nto morphological and syntactic information of a\ntoken; when dembed becomes small with factorized\nembedding, it may not accurately capture the mor-\nphological and syntactic information of the token.\n10798",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8441556096076965
    },
    {
      "name": "Computation",
      "score": 0.8045186996459961
    },
    {
      "name": "Computer science",
      "score": 0.7707599401473999
    },
    {
      "name": "Computer engineering",
      "score": 0.4240337014198303
    },
    {
      "name": "Algorithm",
      "score": 0.28009551763534546
    },
    {
      "name": "Electrical engineering",
      "score": 0.1646929383277893
    },
    {
      "name": "Engineering",
      "score": 0.08503040671348572
    },
    {
      "name": "Voltage",
      "score": 0.072397381067276
    }
  ],
  "institutions": []
}