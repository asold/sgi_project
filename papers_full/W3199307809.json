{
    "title": "Exploring a Unified Sequence-To-Sequence Transformer for Medical Product Safety Monitoring in Social Media",
    "url": "https://openalex.org/W3199307809",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2944691927",
            "name": "Shivam Raval",
            "affiliations": [
                "Bayer (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2782551028",
            "name": "Hooman Sedghamiz",
            "affiliations": [
                "Bayer (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2483174281",
            "name": "Enrico Santus",
            "affiliations": [
                "Bayer (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2588980223",
            "name": "Tuka Alhanai",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2234397692",
            "name": "Mohammad Ghassemi",
            "affiliations": [
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A791988515",
            "name": "Emmanuele Chersoni",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2908840510",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W1481281556",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2071478164",
        "https://openalex.org/W1507711477",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2131546905",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3162285866",
        "https://openalex.org/W2520278632",
        "https://openalex.org/W2171469118",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2573840943",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3023337184",
        "https://openalex.org/W3153955816",
        "https://openalex.org/W4311782530",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W133394232",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3004025777",
        "https://openalex.org/W2895296143",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2129767020",
        "https://openalex.org/W2608364534",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3103031657",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W1509228788"
    ],
    "abstract": "Findings of the Association for Computational Linguistics: EMNLP 2021, Punta Cana, Dominican Republic, November 7–11, 2021",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3534–3546\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3534\nExploring a Uniﬁed Sequence-To-Sequence Transformer for\nMedical Product Safety Monitoring in Social Media\nShivam Raval1 ∗ Hooman Sedghamiz1 ∗ Enrico Santus1 Tuka Alhanai2\nMohammad Ghassemi3 Emmanuele Chersoni4\n1 DSIG - Bayer Pharmaceuticals, New Jersey, USA\n2 New York University, Abu Dhabi, UAE\n3 Michigan State University, Michigan, USA\n4 The Hong Kong Polytechnic University, Hong Kong\nsbraval@asu.edu, {hooman.sedghamiz,enrico.santus}@bayer.com\ntuka.alhanai@nyu.edu, ghassem3@msu.edu\nemmanuele.chersoni@polyu.edu.hk\nAbstract\nAdverse Events (AE) are harmful events re-\nsulting from the use of medical products. Al-\nthough social media may be crucial for early\nAE detection, the sheer scale of this data\nmakes it logistically intractable to analyze us-\ning human agents, with NLP representing the\nonly low-cost and scalable alternative.\nIn this paper, we frame AE Detection and\nExtraction as a sequence-to-sequence problem\nusing the T5 model architecture and achieve\nstrong performance improvements over com-\npetitive baselines on several English bench-\nmarks (F1 = 0.71, 12.7% relative improvement\nfor AE Detection; Strict F1 = 0.713, 12.4%\nrelative improvement for AE Extraction). Mo-\ntivated by the strong commonalities between\nAE-related tasks, the class imbalance in AE\nbenchmarks and the linguistic and structural\nvariety typical of social media posts, we pro-\npose a new strategy for multi-task training that\naccounts, at the same time, for task and dataset\ncharacteristics. Our multi-task approach in-\ncreases model robustness, leading to further\nperformance gains. Finally, our framework\nshows some language transfer capabilities, ob-\ntaining higher performance than Multilingual\nBERT in zero-shot learning on French data.\n1 Introduction\nBefore market release, drugs are regularly tested for\nsafety and effectiveness in clinical trials. However,\nsince no clinical trial is large enough to ﬁnd all po-\ntential Adverse Events (AEs) on a wide and diverse\nrange of population, Pharmacovigilance continu-\nously monitors the market to timely intervene, in\ncase unexpected AEs are discovered.\n∗equal contribution\nAccording to multiple sources (Sen, 2016;\nAlatawi and Hansen, 2017), AEs are systemati-\ncally under-reported in ofﬁcial channels. A grow-\ning number of patients, though, talk about them\non social platforms like Twitter and health forums,\nsharing medical conditions, treatment reviews, side\neffect descriptions and so on. These outlets contain\ncrucial information for Pharmacovigilance, but the\nsheer scale of this data – velocity, volume, variety\n– makes manual exploration prohibitively expen-\nsive. For this reason, Natural Language Processing\n(NLP) technologies represent the only low-cost and\nscalable alternative.\nIn recent years, the research community ap-\nproached this problem by promoting thematic\nworkshops and shared tasks, such as the Social\nMedia Mining For Health Applications (SMM4H)\n(Weissenbacher et al., 2019; Klein et al., 2020),\nas well as by creating resources, such as CADEC\n(Karimi et al., 2015). Despite these efforts, the\nautomatic detection of AEs from social outlets has\nstill to face major challenges: i) posts containing\nAEs are rare compared to other posts (i.e. rare\nsignal and imbalanced data); ii) text typologies\nlargely differ across media (i.e. text length and\nstructure); iii) informal and ﬁgurative language\nis dominant, often containing slang, idioms, sar-\ncasm and metaphors; iv) datasets contain broaddif-\nferences in the annotations, sometimes focusing\nonly on the symptom mentions and others times\nincluding temporal, locative and intensity modi-\nﬁers; v) annotated resources for model ﬁne-tuning\nare only available for a small set of languages (i.e.\ncross-lingualism).\nMost of these challenges have led the research\ncommunity to develop end-to-end solutions for\neach task, missing the beneﬁt of performing multi-\n3535\nple related tasks with the same model (i.e. transfer\nlearning). In this paper, we aim to tackle the above-\nmentioned challenges all at once by framing the\nAE detection and AE extraction tasks as genera-\ntive sequence-to-sequence (seq-to-seq) problems,\nto be addressed with a single architecture, namely\nT5 (Raffel et al., 2019). In previous studies, the\nT5 architecture showed high ﬂexibility in dealing\nwith text from different domains and typologies,\neven in knowledge-intensive tasks (Petroni et al.,\n2021). Furthermore, the T5 architecture is capable\nof incrementally learning new tasks with few or no\nlabels (Xue et al., 2020).\nIn the following paragraphs, not only we show\nthat T5 outperforms strong baselines on multiple\nEnglish benchmarks (F1 = 0.71, 10.94% relative\nimprovement for AE detection; Strict F1 = 0.713,\n12.46% relative improvement for AE extraction),\nbut, to fully unleash its potential and thereby ad-\ndress the above-mentioned challenges (i.e., small,\nvaried and imbalanced data), we introduce a novel\nmulti-task/data training framework that efﬁciently\nhandles task complexity, data imbalance and tex-\ntual differences, further improving over the state-of-\nthe-art results. Assessed in multiple cross-textual\nand (zero-shot learning) cross-lingual AE detection\nand AE extraction settings, T5 shows robustness\nand improves over all the competitive baselines, de-\nspite being simpler – in terms of number of layers\nand parameters – than the competitors.\nTo summarize, our contributions are: i) we use\nT5 for framing AE detection and extraction as a\nsequence-to-sequence problem, obtaining strong\nperformance on multiple tasks and datasets; ii) we\ndescribe a new approach for balancing data across\ntasks and datasets in a multi-task setting, which\nleads to F1-score improvements on all benchmarks;\niii) we test our model in a crosslingual transfer\n(English to French) scenario, showing that it out-\nperforms Multilingual BERT in zero-shot learning.\n2 Related Work\nEarly efforts in automated Pharmacovigilance have\ntargeted Electronic Health Records (EHR) to detect\nevidence of AEs (Uzuner et al., 2011; Jagannatha\net al., 2019). However, not all AEs lead to clinical\nvisitations: many users prefer to discuss their expe-\nriences with drugs on the Internet, and this fact led\nto a growing interest in the automatic detection of\nadverse events from social media platforms.\nSome of the early machine learning systems for\nAE detection from social media data used a com-\nbination of various classiﬁers along with word em-\nbeddings as features (Sarker and Gonzalez, 2015;\nNikfarjam et al., 2015; Daniulaityte et al., 2016;\nMetke-Jimenez and Karimi, 2016).\nAfter the introduction of challenges such as\nSocial Media Mining for Health Applications\n(SMM4H) (Weissenbacher et al., 2018, 2019) and\nCADEC (Karimi et al., 2015), most works focused\non neural networks (Sarker et al., 2018; Minard\net al., 2018). With the development of attention\nmechanism (Vaswani et al., 2017), Transformer-\nbased language models such as BERT (Devlin\net al., 2019) and its biomedical (e.g., BioBERT\n(Lee et al., 2020), ClinicalBERT (Alsentzer et al.,\n2019) and PubMedBERT (Gu et al., 2020)) and\nnon-biomedical variants (e.g., SpanBERT (Joshi\net al., 2020)) obtained state-of-the-art performance\nin AE detection (Weissenbacher et al., 2019; Klein\net al., 2020; Portelli et al., 2021a,b).\nModels like BERT and its variants can be de-\nscribed as encoder-only: in order to carry out a\nspeciﬁc task, a decoder has to be followed by task-\nspeciﬁc trainable network, most often in the form\nof a linear layer. Recent developments in NLP led\nto the introduction of models such as T5 (Raffel\net al., 2019), which is an encoder-decoder Trans-\nformer architecture. In a series of studies, T5 and\nits variants have shown performance gain on vari-\nous datasets and applications (Raffel et al., 2019;\nXue et al., 2020), despite being smaller in terms of\nparameters. The preﬁx training approach adopted\nby T5 allows users to ﬁne-tune on various tasks\nconcurrently, creating a single model that can incre-\nmentally learn while being capable of performing\ndifferent tasks simultaneously. To our knowledge,\nthe present contribution is the ﬁrst to frame AE\ndetection and extraction as generative problems.\n3 Methods\n3.1 The T5 Model\nWe employ T5, a pre-trained encoder-decoder trans-\nformer proposed by Raffel et al. (2019). This model\nmaps a vector sequence of n input words repre-\nsented by X1:n = x1,··· ,xn to an output se-\nquence of Y1:m = y1,··· ,ym with an a-priori\nunknown length of m, with a conditional probabil-\nity deﬁned as:\npθmodel (Y1:m|X1:n) (1)\nThe architecture of the model is very similar to\n3536\nT5\n\"assert ade: I had wild dreams after taking\n10/325 mg of Norco in the morning\"\n \"adverse event problem\"\n\"ner ade: I had wild dreams after taking\n10/325 mg of Norco in the morning\"\n \"wild dreams; \"\n\"ner drug: I had wild dreams after taking\n10/325 mg of Norco in the morning\"\n \"Norco;\"\n\"ner dosage: I had wild dreams after taking\n10/325 mg of Norco in the morning\"\n\"10/325\nmg\"\n\"ner ade: I feel a bit drowsy & have a little blurred\nvision after taking Arthrotec when I need it.\"\n \"bit drowsy; blurred vision; \"\nFigure 1: Diagram of our sequence-to-sequence framework, which is a ﬁne-tuned T5 model for four preﬁx: “assert\nade:” (yellow box) for detection task, “ner ade:” (pink box) for the task of extracting AE’s , “ner drug:” (blue box)\nfor extracting drug mentions and “ner dosage:” (green box) for extracting drug dosage information from the input.\nthe original Transformer proposed by Vaswani et al.\n(2017). An input sequence is ﬁrst passed to the en-\ncoder which consists of self-attention followed by\nfeed-forward layers. The encoder maps the input to\na sequence of embeddings that go through normal-\nization and drop-out layers. The decoder attends\nto the output of the encoder using several attention\nlayers. The self-attention layers, instead, employ\nmasking to make the decoder only attend to the\npast tokens, in an auto-regressive manner:\npθdecoder (Y1:m) =\nm∏\ni=1\npθdecoder (yi|Y0:i−1) (2)\nwhere pθdecoder (yi|Y0:i−1) is the probability distri-\nbution of the next token yi. Finally, the output of\nthe decoder passes through a SoftMax layer over\nthe vocabulary. Raffel et al. (2019) proposed to add\na preﬁx in front of the input sequence to inform\nthe model about which task to perform (e.g. sum-\nmarization, question answering, classiﬁcation etc.;\nsee Figure 1).The model was trained on the Colos-\nsal Clean Crawled Corpus (C4), a massive corpus\n(about 750 GB) of web-extracted and cleaned text.\n3.1.1 Pre-Training and Pre-Finetuning\nRaffel et al. (2019) explored a wide range of\narchitectures and pre-training objectives, ﬁnding\nthat encoder-decoder models generally outperform\ndecoder-only language models, and that a BERT-\nstyle denoising objective – where the model is\ntrained to recover masked words in the input –\nworks best. Moreover, the best variant of their sys-\ntem made use of an objective that corrupts contigu-\nous spans of tokens, similarly to the span corrup-\ntion strategy introduced for the SpanBERT model\n(Joshi et al., 2020).\nThe resulting model was then pre-ﬁnetuned on\na variety of tasks from the following sources: the\nGLUE (Wang et al., 2018) and the SuperGLUE\n(Wang et al., 2019) benchmarks for natural lan-\nguage understanding, the abstractive summariza-\ntion data by Hermann et al. (2015) and Nallap-\nati et al. (2016), the SQUAD question answering\ndataset (Rajpurkar et al., 2016) and the WMT trans-\nlation benchmarks for translation from English to\nFrench, from English to German and from English\nto Romanian. The tasks were all treated as a single\ntask in the sequence-to-sequence format, by con-\ncatenating all the datasets together and appending\nthe task-speciﬁc preﬁxes to the instances.\nT5 comes in versions, small (60 million param-\neters), base (220 million parameters), large (770\nmillion parametrs), 3B (3 billion parameters) and\n11B (11 billion parameters). In the paper we will\nuse the term T5 to either refer to the architecture\nor to the T5-Base, as opposed to T5-Small, which\nwill always be mentioned as such.\n3.2 Seq-to-Seq AE-related Tasks\nGiven an input sequence of words X1:n =\nx1,··· ,xn that potentially contains drug, dosage\nand AE mentions, we frame the AE detection (i.e.\nbinary classiﬁcation) and extraction (i.e. span de-\ntection) tasks as seq-to-seq problems, further ﬁne-\ntuning T5 to generate Y1:m = y1,··· ,ym, where\nY is either the classiﬁcation label or the text span\nwith the AE. By selecting the preﬁxes (see Table\n1), we train T5 on all these tasks (see Figure 1).\nPreﬁx Task Deﬁnition Task Type\nassert ade Contains AE or not CLS (binary)\nner ade Extract AE span NER (span)\nner drug Extract drug span NER (span)\nner dosage Extract drug dosage span NER (span)\nTable 1: Preﬁx and task deﬁnition. AE assertion is bi-\nnary classiﬁcation (CLS), while the remaining tasks are\nName Entity Recognition (NER).\n3537\nFor the AE detection, as it is a binary classi-\nﬁcation task, we have chosen the preﬁx “assert\nade:” and the labels i) “adverse event problem”\n(i.e., positive) and ii) “health ok” (i.e., negative).\nUsually, to train Named Entity Recognition (NER)\nsystems, the input data is transformed into stan-\ndard Inside–Outside–Beginning (IOB) format and\nindividual tokens are classiﬁed in one of the IOB\ntags. However, the T5 model can utilize the direct\nspan as a generation target. If multiple spans can\nbe extracted, they can be provided to the system\nseparated by a semicolon or other special charac-\nters. For our experiments on language transfer, we\nsimply apply the “assert ade: ” to data in a differ-\nent language (i.e., French). The model will auto-\nmatically leverage the knowledge acquired during\nthe pre-ﬁnetuning in the machine translation task.\nTasks and deﬁnitions are summarized in Table 1.\n3.3 Multi-Task and Multi-Dataset\nFine-Tuning\nGenerative models like T5 can be easily trained\non multiple tasks. However, multi-task learning\nposes challenges as models may overﬁt or underﬁt,\ndepending on the task difﬁculty, the label distribu-\ntion and the variability across tasks and datasets\n(Arivazhagan et al., 2019). In Raffel et al. (2019),\nproportional mixing and temperature scaling train-\ning strategies were adopted to address the data bal-\nance across tasks. In this work, we extend these\nstrategies to a multi-dataset scenario, in which\ntasks are trained on multiple datasets containing\nheterogeneous data. This scenario is typical in AE\ndetection, where data comes from medical blogs,\nforums, tweets and other social media outlets, each\nof which carries speciﬁc writing styles as well as\ndifferent textual structures and lengths. The an-\nnotation scheme may differ too across datasets,\nwith some schemes focused on the symptoms only,\nwhile others including also the temporal, manner\nand intensity modiﬁers.\nWe assume a multinomial probability distribu-\ntion θt over the ﬁne-tuning task t, given that the\nﬁne-tuning task titself is comprised of dataset(s) d.\nWe deﬁne Md as the number of samples of dataset\nd and ρd the probability of drawing an example\nfrom dduring training.\nIn proportional mixing, we intuitively sample in\nproportion to the dataset size. Therefore, the proba-\nbility of drawing a sample from task tis computed\nas θt = min(γt,Nt)∑\nt min(γt,Nt) , where Nt corresponds to\nthe number of samples available for task tacross\nall datasets, computed as Nt = ∑\ndMd. The prob-\nability of drawing from dataset dis similarly es-\ntimated as ρd = min(γd,Md)∑\nt min(γd,Md) . For the sake of\nalgorithm re-utilization, these parameters γd and\nγt are introduced because, even with proportional\nmixing, large datasets may still dominate the train-\ning. These parameters are meant to limit the impact\nof such large datasets and they have been set to 214\nas in the original paper (Raffel et al., 2019).\nTemperature scaling has also been shown to\nboost multi-task training performance (Raffel et al.,\n2019; Goodwin et al., 2020). It was used for Mul-\ntilingual BERT, to make sure that the model had\nsufﬁcient training on low-resource languages (De-\nvlin et al., 2019). To implement scaling with a\ntemperature T, the mixing rate for each task and\ndataset is raised to the power of 1/T, and then the\nrates are re-normalized so that they sum to 1. There-\nfore, initially, the probabilities are computed with\ntemperature scaling, respectively, as θt =\nT√θt∑\nt\nT√θt\n(for the probability of drawing from task t) and as\nρd =\nT√ρd∑\nd T√ρd\n(for the probability of drawing from\ndataset d). We set T as 2 as it is the best reported\nvalue for temperature scaling strategy demonstrated\nin Raffel et al. (2019) and Goodwin et al. (2020).\nTo assess the value of using multi-dataset sam-\npling, in our experiments we will compare the orig-\ninal proportional mixing and temperature scaling\nby Raffel et al. (2019) with our approach.\n4 Experimental Settings\nGeneral ﬁgures for all the datasets are reported in\nTable 2, while more detailed textual statistics are\navailable in Appendix A. More details about the\ntraining can be found in Appendix B.\n4.1 Datasets\nSMM4H This dataset was introduced for the\nShared Tasks on AE in the Workshop on Social\nMedia Mining for Health Applications (SMM4H)\n(Weissenbacher et al., 2018). The dataset is com-\nposed of Twitter posts, typically short, informal\ntexts with non-standard ortography, and it contains\nannotations for both detection (i.e., Task 1, classiﬁ-\ncation) and extraction (i.e., Task 2, NER) of AEs.\nThe number of samples differs from the original\ndataset as many tweets vanished, due to deletion or\naccess restriction in the platform. Splits are strat-\niﬁed, to maintain an equal ratio of positive and\nnegative examples (see Table 2).\n3538\nCADEC CADEC contains 1,250 medical forum\nposts annotated with patient-reported AEs. In this\ndataset, texts are long and informal, often deviating\nfrom English syntax and punctuation rules. Forum\nposts may contain more than one AE. For our goals,\nwe adopted the training, validation, and test splits\nproposed by Dai et al. (2020) (see Table 2).\nADE corpus v2 This dataset (Gurulingappa\net al., 2012) contains case reports extracted from\nMEDLINE and it was used for multi-task training,\nas it contains annotations for all tasks in Table 1, i.e.\ndrugs, dosage, AE detection and extraction. Splits\nare stratiﬁed, to maintain an equal ratio of positive\nand negative examples (see Table 2).\nWEB-RADR This dataset is a manually curated\nbenchmark based on tweets. We used it exclusively\nto test the performance of the multi-task models,\nas it was originally introduced only for testing pur-\nposes (Dietrich et al., 2020) (see Table 2).\nDataset Total Positive Negative\nSMM4H Task 1\n(AE Detection) 15,482 1,339 14,143\nTrain (80%) 12,386 1,071 11,315\nValidation (10%) 1,548 134 1,414\nTest (10%) 1,548 134 1,414\nSMM4H Task 2\n(AE Det., AE & Drug Extr.)2,276 1300 976\nTrain (60%) 1,365 780 585\nValidation (20%) 455 260 195\nTest (20%) 456 260 196\nCADEC\n(AE Det., AE & Drug Extr.)1,250 1,105 145\nTrain (70%) 875 779 96\nValidation (15%) 187 163 24\nTest (15%) 188 163 25\nADE Corpus v2\n(AE Detection) 23,516 6,821 16,695\nTrain (60%) 14,109 4,091 10,018\nValidation (20%) 4,703 1,365 3,338\nTest (20%) 4,704 1,365 3,339\nADE Corpus v2\n(AE Extraction) 6,821 6,821 0\nTrain (60%) 4,091 4,091 0\nValidation (20%) 1,365 1,365 0\nTest (20%) 1,365 1,365 0\nADE Corpus v2\n(Drug Extraction) 7,100 7,100 0\nTrain (60%) 4,260 4,260 0\nValidation (20%) 1,420 1,420 0\nTest (20%) 1,420 1,420 0\nADE Corpus v2\n(Drug Dosage Extraction) 279 0 0\nTrain (60%) 167 0 0\nValidation (20%) 56 0 0\nTest (20%) 56 0 0\nWEB-RADR\n(AE Detection & Extraction)\nTest 57,481 1,056 56,425\nSMM4H-French\n(AE Detection)\nTest 1,941 31 1,910\nTable 2: Dataset Statistics and Splits.\nSMM4H-French The SMM4H French Dataset\ncontains a total of 1,941 samples out of which 31\nsamples belong to AE (positive) class and 1,910\nsamples have the label Non-AE (negative class).\nThis dataset is only used for testing the zero-shot\ntransfer (see Table 2).\n4.2 Settings\nAE Detection We train and test T5 and the base-\nlines (see 4.3.1) on the SMM4H Task 1 dataset. We\nthen assess the robustness of T5 and the best per-\nforming baseline on the test sets of CADEC, ADE\nCorpus v2 and WEB-RADR.\nAE Extraction We train and test T5 and the base-\nlines (see 4.3.2) on the SMM4H Task 2 dataset. We\nthen assess the robustness of T5 and the best per-\nforming baseline by testing them (trained on either\nSMM4H Task 2 or CADEC) on the test sets of\nSMM4H Task 2, CADEC, ADE Corpus v2 and\nWEB-RADR.\nMulti-Task Learning We train T5-Base on all\nthe training sets for all tasks, using proportional\nmixing or temperature scaling both with the orig-\ninal multi-task (see 4.3.3) and with our proposed\nmulti-task and multi-dataset approach, and we eval-\nuate the resulting models on the available test sets.\nLanguage Transfer We train T5 and the Multi-\nlingual BERT (see 4.3.4) on the SMM4H Task 1\nEnglish dataset, and then we test it in a zero-shot\nlearning setting on the SMM4H-French dataset.\n4.3 Baselines\n4.3.1 AE Detection\nOur baselines are ﬁve pre-trained BERT variants\nwith a classiﬁcation head ﬁne-tuned for AE detec-\ntion. A weighted cross-entropy loss function is\nused for all of them to adjust for class imbalance.\nBioBERT (Lee et al., 2020) was built upon the\noriginal BERT and further pre-trained on PubMed\nabstracts. We used BioBERT v1.1, which was re-\nported to perform better in biomedical tasks.\nBioClinicalBERT (Alsentzer et al., 2019) was pre-\ntrained on MIMIC III dataset containing Electronic\nHealth Records (EHR) of ICU patients.\nSciBERT (Beltagy et al., 2019) was pre-trained on\n1.14 million papers, randomly selected from seman-\ntic scholar, with an 18-82 ratio between computer\nscience and biomedical papers.\nPubMedBERT (Gu et al., 2020) was pre-trained\n3539\nfrom scratch on PubMed abstracts, without build-\ning upon the vocabulary of the original BERT.\nSpanBERT (Joshi et al., 2020) adopts a different\npre-training objective from BERT. This model is\ntrained by masking full contiguous spans instead\nof single words or subwords, which allows it to\nencode span-level information.\n4.3.2 AE Extraction Task Baselines\nFor the AE EXTRACTION task, we use the\nfour models described in Portelli et al. (2021a),\nnamely BERT, BERT+CRF, SpanBERT, and\nSpanBERT+CRF. The authors reported state-of-\nthe-art performance with the SpanBERT mod-\nels on SMM4H, and their implementation\nis publicly available at https://github.com/\nailabUdineGit/ADE.\n4.3.3 Multi-Task Learning\nFor Multi-Task Learning, we use as baseline the T5\nmodel ﬁne-tuned with the original training strate-\ngies by Raffel et al. (2019), which balance across\ntasks (TB, task balancing) but do not account for\nmulti-dataset learning (DB, dataset balancing). We\nrefer to them as T5TB-PM for proportional mixing\nand T5TB-TS for temperature scaling. We refer to\nour approach, which accounts also for the multi-\ndataset learning, as T5TDB-PM for proportional mix-\ning and T5TDB-TS for temperature scaling.\n4.3.4 Language Transfer\nAs a baseline for Language Transfer, we use Multi-\nlingual BERT (the uncased version), which was pre-\ntrained on monolingual corpora in 102 languages\n(Devlin et al., 2019). The model was ﬁne-tuned by\nadding a classiﬁcation head on the top to perform\nAE Detection in a zero-shot setting.\n4.4 Metrics\nWe adopt the same metrics of the SMM4H com-\npetition. 1 For the AE Detection (i.e., the assert\nade preﬁx) we use precision, recall, and F1-score\nfor the positive (AE) class. For the AE Extraction\n(i.e., the ner ade, ner drug, ner dosage preﬁxes) we\nuse both Strict and Partial Match F1-Score (Weis-\nsenbacher et al., 2019; Klein et al., 2020). The\nsame AE Detection and AE Extraction metrics have\nalso been used in the Multi-Task setting and in the\nLanguage Transfer settings.\n1https://competitions.codalab.org/\ncompetitions/20798\n5 Results and Analysis\n5.1 AE Detection\nTable 3 summarizes precision, recall and F1 score\nobtained by T5-Small, T5-Base and the baselines\non the SMM4H Task 1 test set.\nModel Precision Recall F1\nBioBERT 55.5 63.1 59.0\nBioClinicalBERT 68.3 59.7 63.7\nSciBERT 68.8 55.9 61.7\nPubMedBERT 59.7 61.9 60.8\nSpanBERT 55.0 73.1 62.8\nT5-Small 58.1 65.0 61.3\nT5-Base 68.8 73.7 71.1\nTable 3: Precision, Recall and F1-Score for the positive\nAE class in the SMM4H Task 1 test set\nT5-Small obtains competitive performance, lag-\nging slightly behind the performance of some\nBERT variants, while T5-Base outperforms all the\nother approaches, with a 12.7% relative F1-score\nimprovement over the best baseline, BioClinical-\nBERT (the improvement for the McNemar test is\nsigniﬁcant at p <0.001). It should also be no-\nticed that the two versions of T5, together with\nSpanBERT, improve over the Recall of the other\nBERT variants. The result seems to comply with\nthe report by Portelli et al. (2021a,b), who found\nthat models relying on span-based objectives had\nincreased recall in the task, probably because they\nare better at identifying longer AE spans that would\notherwise go undetected.\nModel\\\nTest set\nSMM4H\nTask 2 CADEC\nADE\nCorpus\nv2\nWEB-\nRADR\nBioClinical BERT 82.5 90.1 28.6 32.3\nT5-Base 88.0 93.7 31.7 35.8\nTable 4: F1-Score for T5-Base and BioClinicalBERT\ntrained on SMM4H Task 1 and tested on all datasets.\nTable 4 provides the results for the model gener-\nalization evaluation that we run for T5 and the best\nbaseline. In this evaluation, we train the systems on\nSMM4H Task 1 and test on the other datasets (i.e.\nSMM4H Task 2, CADEC, ADE Corpus V2 and\nWEB-RADR), which differ from the training set\nin terms of linguistic features, text structures, text\nlengths and even annotation schemes. Both mod-\nels obtain high performance on SMM4H Task 2\nand CADEC, despite their textual differences. The\nlarge linguistic difference of the ADE Corpus v2\n3540\nSMM4H Task 2 CADEC\nArchitecture Partial F1 Strict F1 Partial F1 Strict F1\nBERT 66.1 55.9 77.7 65.2\nBERT+CRF 68.1 59.5 77.2 64.4\nSpanBERT 66.7 59.2 79.2 67.2\nSpanBERT + CRF 70.1 63.4 79.4 67.6\nT5-Small 70.7 66.1 75.6 65.7\nT5-Base 75.1 71.3 79.1 69.8\nDai et al. (2020) - - - 69.0\nTable 5: Partial and Strict F1 score for the AE Extraction task on SMM4H Task 2 and CADEC. For CADEC, we\nalso report the current SOTA model by Dai et al. (2020).\n(i.e., MEDLINE case reports) explains instead the\ndrastic drop in performance for both systems in this\ndataset, in which T5-Base still performs better than\nthe baseline. WEB-RADR also proves to be a chal-\nlenging benchmark for its extreme class imbalance,\nbut our system still achieves an F1-score around\n0.36 for the positive class, while BioClinicalBERT\nis performs than the T5-Base.\n5.1.1 Qualitative Analysis on SMM4H\nIn order to better understand the model perfor-\nmance, we picked some samples from the SMM4H\nTask 1 test dataset to compare between captured\nand non-captured AE and analyze the reason be-\nhind the miss-classiﬁcation. In few cases, the\nmodel has problems identifying non-standardized\nacronyms, for example the input “really bad RLS\nfrom <drug name>”, is classiﬁed as non-AE by the\nmodel compared to its original label as an AE. The\nmodel is not able to understand the meaning be-\nhind RLS, which denotes Restless Leg Syndrome\nin this scenario. We observed that if the RLS is\nchanged to nightmares, headache or restless leg\nsyndrome, the model recognizes the input as an\nAE. The model is able to capture most of the AE\nFigure 2: Performance in the AE Extraction task, with\nthe number of layers and parameters for each system.\nunusual references such as “<drug name> burns\nlike thousand suns”, “<drug name> was a joke”,\n“<drug name> tastes like battery acid”. Yet we\nfound some cases in which the model failed. For\nexample, the inputs “stomach feels like a cement\nmixer after taking <drug name>” was classiﬁed as\nnon-AE. In this case, “cement mixer” is used in a\nﬁgurative way to refer to the fact that the stomach\nis not well or it is churning. Once we replace this\nﬁgurative image with a term such as churning, the\nmodel correctly classiﬁes the sample as AE.\n5.2 AE Extraction\nTable 5 summarizes the results for the AE Ex-\ntraction task for T5 and the baselines trained on\nSMM4H Task 2, including the scores for a recent\nSOTA system on CADEC (Dai et al., 2020). It\ncan be seen that both T5 models outperform all\nthe baselines on the SMM4H data, while on the\nlonger and more structured CADEC texts the Span-\nBERT architectures are more competitive for the\npartial F1-score. On the other hand, our best model\nstill retains a better performance for the Strict F1\nmetric, suggesting that it is more accurate in detect-\ning the boundaries of the AE span. T5-Base also\noutperforms the system by Dai et al. (2020).\nModel SMM4H\nTask 2 CADEC\nADE\nCorpus\nv2\nWEB-\nRADR\nTrained on SMM4H Task 2\nSpanBERT\n+ CRF 70.1 (63.4) 15.7 (2.8) 24.6 (15.1) 18.9 (7.3)\nT5-Base 75.1 (71.3) 24.4 (20.5) 38.9 (29.5) 36.2 (13.9)\nTrained on CADEC\nSpanBERT\n+ CRF 35.4 (28.6) 79.4 (67.6) 31.2 (24.8) 20.1 (7.9)\nT5-Base 57.9 (51.6) 79.1 (69.8) 50.3 (43.7) 30.4 (18.8)\nTable 6: Partial (strict) F1-scores for T5-Base and Span-\nBERT+CRF trained on SMM4H Task 2 and CADEC\nand evaluated on all datasets.\nIn order to further evaluate the system general-\n3541\nText Statistics BERT BERT+CRF SpanBERT SpanBERT+CRF T5-Base\nDale Chall Readability + 8.34 8.15 8.20 8.32 9.44\nAutomated Readability + 8.42 8.37 8.35 8.51 10.37\nFlesch Reading Ease − 62.73 63.57 62.60 62.92 53.18\nTable 7: Text Statistics metric to evalute the quality of span generated by models trained on SMM4H Task 2 dataset\n( + represents higher score is better and − means lower score is better)\nTask Model/Dataset SMM4H\nTask 1\nSMM4H\nTask 2 CADEC ADE\nCorpus v2\nWEB-\nRADR Avg. Score\nassert ade\nT5TB -PM 55.2 91.5 92.7 91.7 31.9 72.6\nT5TDB - PM 67.9 88.5 98.7 91.7 37.4 76.8\nT5TB -TS 65.3 83.5 91.1 90.9 36.1 73.3\nT5TDB - TS 69.4 89.4 98.7 91.5 37.3 77.2\nner ade\nT5TB -PM - 75.7 (71.8) 46.5 (39.9) 58.4 (53.4) 38.6 (15.1) 54.8 (45.0)\nT5TDB - PM - 75.7 (71.8) 74.4 (64.0) 59.7 (55.9) 38.7 (15.8) 62.1 (51.8)\nT5TB -TS - 75.3 (70.2) 45.2 (38.4) 59.7 (56.1) 38.9 (15.6) 54.7 (45.0)\nT5TDB - TS - 75.7 (71.1) 75.3 (66.0) 60.3 (56.7) 39.1 (15.8) 62.6 (52.4)\nner drug\nT5TB -PM - 92.3 (92.3) 88.7 (88.7) 79.4 (79.0) - 86.8 (86.6)\nT5TDB - PM - 90.3 (90.3) 92.4 (91.8) 82.2 (82.0) - 88.3 (88.0)\nT5TB -TS - 88.2 (88.1) 88.4 (88.1) 80.2 (79.8) - 85.6 (85.3)\nT5TDB - TS - 91.8 (91.8) 94.1 (93.4) 83.1 (82.8) - 89.6 (89.3)\nner dosage\nT5TB -PM - - - 73.2 (67.8) - 73.2 (67.8)\nT5TDB - PM - - - 78.5 (71.4) - 78.5 (71.4)\nT5TB -TS - - - 76.7 (71.4) - 76.7 (71.4)\nT5TDB - TS - - - 78.5 (71.4) - 78.5 (71.4)\nTable 8: F1-scores for the multi-task setting. Task Balancing (TB) is compared to our Task and Dataset Balancing\n(TDB) approach, with PM = Proportional Mixing and TS = Temperature Scaling. F1 of the positive class is\nreported for AE Detection (the assert ade row), while partial (strict) F1 is reported for the Extraction tasks.\nization capability, we test on all the AE Extraction\ndatasets both T5-Base and SpanBERT+CRF (best\nbaseline), after training them on SMM4H Task 2\nand on CADEC. In Table 6, it can be seen that\nT5-Base has better generalization than the baseline\non all datasets, with F1-scores that are 10 points\nhigher or more. Training on CADEC generalizes\nbetter (with the only exception of the partial met-\nric for WEB-RADR), while systems trained on the\nSMM4H perform poorly on the other benchmarks.\nFig. 2 compares the baselines and the T5 per-\nformance in AE extraction, in terms of number\nof layers/parameters. The plot suggests that the\nmodel parameters and the number of layers are not\nthe factors for the T5 models performance gain, e.g.\nT5-Small has almost half the number of parame-\nters (60 million) and half of the layers (6 layers) of\nBERT and its variants and it still performs better.\n5.2.1 Analysis of Extracted Spans on\nSMM4H\nWe employ some commonly used text statistics to\nassess the spans extracted by the T5 model. Table 7\ncompares three text statistics metrics for the model\ntrained on the SMM4H Task 2 dataset. The higher\nscores obtained by T5 in the Dale Chall Readability\n(Chall and Dale, 1995) and Automated Readability\nindex (Smith and Senter, 1967) suggest this model\nis able to generate a higher percentage of AE spans\nwith rare terms. The lower Flesch Reading score\n(Kincaid et al., 1975), instead, indicates that the\nmodel generates spans that are more readable.\n5.3 Multi-Task Learning\nTable 8 includes the scores on all the test sets for\nthe multi-task T5 models, trained either with the\noriginal or with our proposed strategy (see 3.3).\nIn AE Detection, our T5 TDB approach always\noutperforms the original T5 TB by a large margin\n(5.8% relative improvement for PM and 5.3% for\nTS), except for the Proportional Mixing case in\nSMM4H Task 2.Margins are smaller in ADE Cor-\npus V2 and WEB-RADR. Looking at the compari-\nson between TS and PM for T5, the former is better\nin the SMM4H subsets and comparable in all the\nothers, globally obtaining a higher average score.\nOur training approach improves both partial and\n3542\nstrict F1-scores on the AE Extraction task, where\nthe models are tested on all datasets except for\nSMM4H Task 1, which does not have AE Extrac-\ntion annotations (13.3% relative improvement for\nPM and 14.4% for TS). In all datasets, our training\nstrategies obtain equal or superior performance for\nboth partial and strict F1 scores, with large gains on\nCADEC and more marginal gains on SMM4H Task\n2, ADE Corpus v2 and WEB-RADR. TS is again\npreferable to PM, obtaining a higher average score.\nThe results for the Drug and Dosage tasks are simi-\nlar: in Drug Extraction, SMM4H Task 2 conﬁrms\nto be more challenging for T5 TDB-PM (T5TB-PM\noutperforms it by 2 points), while T5 TDB-TS out-\nperforms its counterpart. In all the other settings,\nthe Task and Dataset Balancing approaches score\nhigher than Task Balancing-only ones.\nOverall, our approaches consistently achieve\ngains in the multi-task setting, independently from\ntask type (i.e. Detection or Extraction) and annota-\ntion scheme. TS proves to be superior to PM in all\ntasks, even though it may lag slightly behind PM\nin some datasets.\n5.4 Cross-lingual Transfer\nAs a ﬁnal evaluation, we tested the ability of T5-\nBase and Multilingual BERT to generalize the AE\nDetection task to a new language, i.e. French. No-\ntice that the SMM4H French data proved to be chal-\nlenging, due to the extreme class imbalance (Klein\net al., 2020). It can be seen in Table 9 that T5-\nBase obtains higher F1-score, speciﬁcally thanks\nto a higher precision. Multilingual BERT, instead,\nshows higher recall. Overall, the T5-Base perfor-\nmance in zero-shot learning is encouraging, and\nfurther improvements are likely to come with few\nshot learning or with more targeted strategies for\nmultilingual training.\nArchitecture Zero-Shot\nPrecision Recall F1\nMultilingual BERT 10.2 32.2 15.5\nT5-Base 17.9 22.6 20.0\nTable 9: Metrics for Multilingual BERT and T5-Base\non zero-shot learning on SMM4H-French.\n6 Conclusions\nIn order to address several typical challenges of the\nhealthcare domain (small, imbalanced and highly\nvariable datasets, cross-lingual data), we proposed\nto treat AE Detection and AE/Drug/Dosage Ex-\ntraction tasks as sequence-to-sequence problems,\nadapting the T5 architecture and improving over all\nthe baselines in both the Detection and the Extrac-\ntion tasks. To maximize the beneﬁt of multi-task\nand multi-dataset learning, we introduced a new\ntraining strategy that extends Raffel et al. (2019),\nshowing that our approach accounts for multiple\nand diverse datasets and leads to consistent im-\nprovements over the original T5 proposal. Finally,\nthe model also shows some language transfer abili-\nties in the zero shot setting, leaving the door open\nfor future experiments to extend our training frame-\nwork towards multilinguality (Xue et al., 2020).\nAcknowledgments\nWe would like to thank the reviewers and the chairs\nfor their insightful reviews and suggestions.\nReferences\nYasser M Alatawi and Richard A Hansen. 2017. Empir-\nical Estimation of Under-reporting in the US Food\nand Drug Administration Adverse Event Reporting\nSystem (FAERS). Expert Opinion on Drug Safety ,\n16(7):761–767.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly Available Clinical\nBERT Embeddings. In Proceedings of the NAACL\nWorkshop on Clinical Natural Language Processing.\nNaveen Arivazhagan, Ankur Bapna, Orhan Fi-\nrat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Fos-\nter, Colin Cherry, Wolfgang Macherey, Zhifeng\nChen, Yonghui Wu, and Google AI. 2019. Mas-\nsively Multilingual Neural Machine Translation in\nthe Wild: Findings and Challenges. arXiv preprint\narXiv:1907.05019.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. arXiv preprint arXiv:1903.10676.\nJeanne Sternlicht Chall and Edgar Dale. 1995. Read-\nability Revisited: The New Dale-Chall Readability\nFormula. Brookline Books.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile\nParis. 2020. An Effective Transition-based Model\nfor Discontinuous NER. In Proceedings of ACL.\nRaminta Daniulaityte, Lu Chen, Francois R Lamy,\nRobert G Carlson, Krishnaprasad Thirunarayan, and\nAmit Sheth. 2016. “When ‘Bad’ Is ‘Good’”: Pden-\ntifying Personal Communication and Sentiment in\nDrug-related Tweets. JMIR Public Health and\nSurveillance, 2(2):e162.\n3543\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nJuergen Dietrich, Lucie M Gattepaille, Britta Anne\nGrum, Letitia Jiri, Magnus Lerch, Daniele Sartori,\nand Antoni Wisniewski. 2020. Adverse Events\nin Twitter-development of a Benchmark Reference\nDataset: Results from IMI WEB-RADR. Drug\nSafety, pages 1–12.\nRudolf Flesch and Alan J Gould. 1949. The Art of\nReadable Writing, volume 8. Harper New York.\nTravis R Goodwin, Max E Savery, and Dina Demner-\nFushman. 2020. Towards Zero-Shot Conditional\nSummarization with Adaptive Multi-Task Fine-\nTuning. In Proceedings of EMNLP.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc Language Model Pretraining for Biomedi-\ncal Natural Language Processing. arXiv preprint\narXiv:2007.15779.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius,\nand Luca Toldo. 2012. Development of a Bench-\nmark Corpus to Support the Automatic Extrac-\ntion of Drug-related Adverse Effects from Medical\nCase Reports. Journal of Biomedical Informatics ,\n45(5):885–892.\nKarl Moritz Hermann, Tomáš Koˇcisk`y, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suley-\nman, and Phil Blunsom. 2015. Teaching Ma-\nchines to Read and Comprehend. arXiv preprint\narXiv:1506.03340.\nAbhyuday Jagannatha, Feifan Liu, Weisong Liu, and\nHong Yu. 2019. Overview of the First Natural Lan-\nguage Processing Challenge for Extracting Medi-\ncation, Indication, and Adverse Drug Events from\nElectronic Health Record Notes (MADE 1.0). Drug\nSafety, 42(1):99–111.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving Pre-training by Representing and Predict-\ning Spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nSarvnaz Karimi, Alejandro Metke-Jimenez, Madonna\nKemp, and Chen Wang. 2015. CADEC: A Cor-\npus of Adverse Drug Event Annotations. Journal\nof Biomedical Informatics, 55:73–81.\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L\nRogers, and Brad S Chissom. 1975. Derivation\nof New Readability Formulas (Automated Readabil-\nity Index, Fog count and Flesch reading ease For-\nmula) for Navy Enlisted Personnel. Technical re-\nport, Naval Technical Training Command Milling-\nton TN Research Branch.\nAri Klein, Ilseyar Alimova, Ivan Flores, Arjun Magge,\nZulfat Miftahutdinov, Anne-Lyse Minard, Karen\nO’connor, Abeed Sarker, Elena Tutubalina, Davy\nWeissenbacher, et al. 2020. Overview of the Social\nMedia Mining for Health Applications (# SMM4H)\nShared Tasks at COLING 2020. In Proceedings of\nthe COLING Social Media Mining for Health Appli-\ncations Workshop & Shared Task.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: a Pre-trained\nBiomedical Language Representation Model\nfor Biomedical Text Mining. Bioinformatics,\n36(4):1234–1240.\nAlejandro Metke-Jimenez and Sarvnaz Karimi. 2016.\nConcept Identiﬁcation and Normalisation for Ad-\nverse Drug Event Discovery in Medical Forums. In\nBMDID@ ISWC. Citeseer.\nAnne-Lyse Minard, Christian Raymond, and Vincent\nClaveau. 2018. IRISA at SMM4H 2018: Neural\nNetwork and Bagging for Tweet Classiﬁcation. In\nProceedings of the EMNLP Workshop on Social Me-\ndia Mining for Health Applications.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive Text Summa-\nrization Using Sequence-to-sequence RNNs and Be-\nyond. In Proceedings of CONLL.\nAzadeh Nikfarjam, Abeed Sarker, Karen O’connor,\nRachel Ginn, and Graciela Gonzalez. 2015. Phar-\nmacovigilance from Social Media: Mining Adverse\nDrug Reaction Mentions Using Sequence Labeling\nwith Word Embedding Cluster Features. Journal\nof the American Medical Informatics Association ,\n22(3):671–681.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, et al. 2021. KILT: A Benchmark for\nKnowledge Intensive Language Tasks. In Proceed-\nings of NAACL.\nBeatrice Portelli, Edoardo Lenzi, Emmanuele Cher-\nsoni, Giuseppe Serra, and Enrico Santus. 2021a.\nBERT Prescriptions to Avoid Unwanted Headaches:\nA Comparison of Transformer Architectures for Ad-\nverse Drug Event Detection. In Proceedings of\nEACL.\nBeatrice Portelli, Daniele Passabi, Edoardo Lenzi,\nGiuseppe Serra, Enrico Santus, and Emmanuele\nChersoni. 2021b. Improving Adverse Drug Event\nExtraction with SpanBERT on Different Text Ty-\npologies. In Proceedings of the AAAI International\nWorkshop on Health Intelligence (W3PHIAI).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-text\nTransformer. arXiv preprint arXiv:1910.10683.\n3544\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of EMNLP.\nAbeed Sarker, Maksim Belousov, Jasper Friedrichs,\nKai Hakala, Svetlana Kiritchenko, Farrokh\nMehryary, Sifei Han, Tung Tran, Anthony Rios,\nRamakanth Kavuluru, et al. 2018. Data and Systems\nfor Medication-related Text Classiﬁcation and Con-\ncept Normalization from Twitter: Insights from the\nSocial Media Mining for Health (SMM4H)-2017\nShared Task. Journal of the American Medical\nInformatics Association, 25(10):1274–1283.\nAbeed Sarker and Graciela Gonzalez. 2015. Portable\nAutomatic Text Classiﬁcation for Adverse Drug Re-\naction Detection via Multi-corpus Training. Journal\nof Biomedical Informatics, 53:196–207.\nSukanta Sen. 2016. Consumer Reporting of Adverse\nDrug Reactions: A Current Perspective. Interna-\ntional Journal of Green Pharmacy (IJGP), 10(03).\nEdgar A Smith and RJ Senter. 1967. Automated Read-\nability Index. AMRL-TR. Aerospace Medical Re-\nsearch Laboratories (US), pages 1–14.\nÖzlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/V A Challenge on\nConcepts, Assertions, and Relations in Clinical Text.\nJournal of the American Medical Informatics Asso-\nciation, 18(5):552–556.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019. Super-\nGLUE: A Stickier Benchmark for General-purpose\nLanguage Understanding Systems. arXiv preprint\narXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGLUE: A Multi-task Benchmark and Analysis Plat-\nform for Natural Language Understanding. arXiv\npreprint arXiv:1804.07461.\nDavy Weissenbacher, Abeed Sarker, Arjun Magge,\nAshlynn Daughton, Karen O’Connor, Michael Paul,\nand Graciela Gonzalez. 2019. Overview of the ACL\nSocial Media Mining for Health (SMM4H) Shared\nTasks at ACL 2019. In Proceedings of the ACL\nSocial Media Mining for Health Applications (#\nSMM4H) Workshop & Shared Task.\nDavy Weissenbacher, Abeed Sarker, Michael Paul, and\nGraciela Gonzalez. 2018. Overview of the Social\nMedia Mining for health (SMM4H) Shared Tasks\nat EMNLP 2018. In Proceedings of the EMNLP\nWorkshop on Social media Mining for Health Appli-\ncations Workshop & Shared Task.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. HuggingFace’s Transformers:\nState-of-the-art Natural Language Processing. arXiv\npreprint arXiv:1910.03771.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mT5: A Mas-\nsively Multilingual Pre-trained Text-to-text Trans-\nformer. arXiv preprint arXiv:2010.11934.\n3545\nA Dataset Textual Statistics\nTable 10 presents textual statistics to show the dif-\nference in type of datasets with respect to their\ninput sequence length, target (extraction) span\nsequence length and other parameters. It can\nobserved that the input sequence length is rela-\ntively short for the SMM4H and for WEB-RADR\ndatasets, while CADEC and ADE corpus datasets\ntend to include longer texts. The Flesch reading\nease score (Flesch and Gould, 1949) indicates the\nreadability of the sentence, with lower values repre-\nsenting that the text is difﬁcult to understand for the\naverage reader. The ADE corpus datasets have the\nlowest Flesch reading score, as the text is adopted\nfrom MEDLINE and contains more medical terms,\nwhile Twitter data (SMM4H, WEB-RADR) and the\nhealth forum (CADEC) datasets contain a lower\namount of scientiﬁc terminology and are typically\nmade of shorter texts, with a lower degree of syn-\ntactic complexity.\nB Training Details\nAll the experiments have been performed on the\ntop of Hugging-face’s Python package (Wolf et al.,\n2019). 2 The code for the models implemented\nin the paper is available at https://github.com/\nshivamraval98/MultiTask-T5_AE\nB.1 AE Detection\nThe baseline BERT models for AE detection were\ntrained on one NVIDIA Tesla V100 16 GB GPU\n2https://github.com/huggingface/transformers\nand it takes the model approximately 30 minutes to\nexecute for all epochs. The hyperparameters used\nfor baseline models are detailed in Table 11.\nModel Epoch Batch Size Warm-up Steps\nBioBERT 3 32 400\nBioClinicalBERT5 40 500\nSciBERT 5 40 400\nPubMedBERT 5 40 300\nSpanBERT 3 40 400\nTable 11: Hyperparameters for AE Detection baselines.\nThe learning rate and weight decay was kept constant\nwith values 5e−05 and 0.01 respectively\nThe T5 models were trained using a cluster of\nfour NVIDIA Tesla V100 16 GB GPU, with 80\nbatch size per GPU and 10 epochs for T5-Small,\nand 16 batch size per GPU and 7 epochs for T5-\nBase. The learning rate for the both the t5 models\nwas set to 1e−04. The input and the generated\nsequence length were set to 130 and 20, respec-\ntively, with exponential length penalty set to 2 for\nthe generated sequence. For the rest of the hyperpa-\nrameters, we used the default values in the library.\nThe T5-Small model approximately takes 3-5\nminutes per epoch while T5-Base executes for 7-\n10 minutes per epoch in the aforementioned cluster\nenvironment setting.\nB.2 AE Extraction\nThe hyperparameters for the baseline mod-\nels (BERT, BERT+CRF, SpanBERT and Span-\nBERT+CRF) of AE extraction were set as de-\nscribed in Portelli et al. (2021a). The hyperparame-\nDataset Avg. Seq\nLength\nAvg.\nSpan Length\n(AE, Drug or\nDosage)\nAvg.\nStopwords\nin span\nAvg. Freq.\nof AE per\nsample\nUnique AE\nwords\n% of AE\nSamples\nUnique Drug\nMentions\nFlesch Reading\nEase Score\nSMM4H Task 1\n(AE Detection) 98.9 - - - - 8.6 - 64.7\nSMM4H Task 2\n(AE Det., AE & Drug Extr.)108.8 9.1 0.2 1 1108 57.1 69 62.1\nCADEC\n(AE Det., AE & Drug Extr.)459.4 16.1 2.4 6 2303 89.0 320 69.1\nADE Corpus v2\n(AE Detection) 132.5 - - - - 28.9 - 23.2\nADE Corpus v2\n(AE Extraction) 152.1 18.5 0.1 1 2662 100 - 13.6\nADE Corpus v2\n(Drug Extraction) 152.3 10.8 0 - - 100 1251 14.3\nADE Corpus v2\n(Drug Dosage Extraction)163.4 8.5 0 - - 100 - 23.6\nWEB-RADR\n(AE Detection & Extraction)106.3 16.5 1.1 2 2037 1.8 - 61.3\nSMM4H French\n(AE Detection) 142.4 - - - - 1.6 - -\nTable 10: Comparison of the AE datasets according to different textual statistics.\n3546\nter setting for the T5-Small and T5-Base for both\nSMM4H Task 2 and CADEC dataset is presented\nin Table 12 and the default values were utilized for\nthe rest of the hyperparameters.\nModel ISL OSL BS EP LR Time\nSMM4H Task 2 AE Extraction\nT5-Small 130 20 80 10 1e-4 5\nT5-Base 130 20 64 7 1e-4 7\nCADEC AE Extraction\nT5-Small 512 150 64 25 1e-3 10\nT5-Base 512 150 32 20 1e-3 20\nTable 12: Hyperparameters for T5-Small and T5-Base\nwhen trained on SMM4H and CADEC AE Extraction\nTask (ISL = Input Sequence Length, OSL = Output Se-\nquence Length, BS = Batch Size (over all GPU’s), EP\n= Epoch, LR = Learning Rate, Time = Training Time\nin mins per epoch).\nB.3 Multi-Task Training\nThe Multi-Task Training was performed on T5-\nBase by combining all the training sets and experi-\nmenting for the originally proposed Task Balanc-\ning (TB) approach, and for our proposed task plus\nmulti-dataset balancing (TDB) strategy for propor-\ntional mixing (PM) and temperature scaling (TS).\nThe same hyperparameters were utilized for all set-\ntings with batch size 8, learning rate 1e−04, input\nsequence length 512 and output sequence length\n150. Temperature value was kept to be 2 for the\ntemperature scaling method. For every multi-task\nsetting, it took the model approximately 60 min-\nutes to train for one epoch in the 4 GPU cluster\ncomputing environment setting.\nB.4 Cross-Lingual Transfer\nMultilingual BERT was trained using the four GPU\ncluster setting with batch size 256 over all GPU’s\nfor 7 epochs. The learning rate was set as 5e−05\nwith 0 warmup steps and 0.01 weight decay. The\nT5-Base model trained on English SMM4H Task\n1 AE Detection dataset was utilized to perform\nzero-shot on SMM4H French Dataset."
}