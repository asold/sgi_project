{
    "title": "GOHSP: A Unified Framework of Graph and Optimization-Based Heterogeneous Structured Pruning for Vision Transformer",
    "url": "https://openalex.org/W4382317631",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2136060477",
            "name": "Miao Yin",
            "affiliations": [
                "Rutgers Sexual and Reproductive Health and Rights"
            ]
        },
        {
            "id": "https://openalex.org/A1272528903",
            "name": "Burak Uzkent",
            "affiliations": [
                "Research!America (United States)",
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2109533396",
            "name": "Yilin Shen",
            "affiliations": [
                "Research!America (United States)",
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2058645848",
            "name": "Hongxia Jin",
            "affiliations": [
                "Research!America (United States)",
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2030840999",
            "name": "Bo Yuan",
            "affiliations": [
                "Rutgers Sexual and Reproductive Health and Rights"
            ]
        },
        {
            "id": "https://openalex.org/A2030840999",
            "name": "Bo Yuan",
            "affiliations": [
                "Rutgers Sexual and Reproductive Health and Rights"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2276892413",
        "https://openalex.org/W2164278908",
        "https://openalex.org/W3169769781",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W2058532290",
        "https://openalex.org/W3101913037",
        "https://openalex.org/W2928560789",
        "https://openalex.org/W2737121650",
        "https://openalex.org/W2177847924",
        "https://openalex.org/W6774506737",
        "https://openalex.org/W2953797043",
        "https://openalex.org/W2924888702",
        "https://openalex.org/W6848045988",
        "https://openalex.org/W3215986121",
        "https://openalex.org/W6631501603",
        "https://openalex.org/W3101584733",
        "https://openalex.org/W6639055396",
        "https://openalex.org/W2901014608",
        "https://openalex.org/W6684411828",
        "https://openalex.org/W3128289811",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3008091349",
        "https://openalex.org/W2991818340",
        "https://openalex.org/W6725543821",
        "https://openalex.org/W3188427387",
        "https://openalex.org/W2769044225",
        "https://openalex.org/W6810818354",
        "https://openalex.org/W2754084392",
        "https://openalex.org/W2891967877",
        "https://openalex.org/W2963094099",
        "https://openalex.org/W1854214752",
        "https://openalex.org/W1525595230",
        "https://openalex.org/W3009447447",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2984618279",
        "https://openalex.org/W4226484461",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W3170188883",
        "https://openalex.org/W3160949036",
        "https://openalex.org/W2997768846",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2764043458",
        "https://openalex.org/W2966234499",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3168124404",
        "https://openalex.org/W4312290555",
        "https://openalex.org/W2963000224",
        "https://openalex.org/W2964001144",
        "https://openalex.org/W2896409484",
        "https://openalex.org/W2802074788",
        "https://openalex.org/W4292363360",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963145730",
        "https://openalex.org/W2962988160",
        "https://openalex.org/W2963363373",
        "https://openalex.org/W3137963805",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3035296770",
        "https://openalex.org/W4308536459",
        "https://openalex.org/W4297813615",
        "https://openalex.org/W4312444958",
        "https://openalex.org/W3034513523"
    ],
    "abstract": "The recently proposed Vision transformers (ViTs) have shown very impressive empirical performance in various computer vision tasks, and they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then severely hinder their potential deployment in many practical resources constrained applications. To mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable practical efficiency. However, unlike its current popularity for CNNs and RNNs, structured pruning for ViT models is little explored. In this paper, we propose GOHSP, a unified framework of Graph and Optimization-based Structured Pruning for ViT models. We first develop a graph-based ranking for measuring the importance of attention heads, and the extracted importance information is further integrated to an optimization-based procedure to impose the heterogeneous structured sparsity patterns on the ViT models. Experimental results show that our proposed GOHSP demonstrates excellent compression performance. On CIFAR-10 dataset, our approach can bring 40% parameters reduction with no accuracy loss for ViT-Small model. On ImageNet dataset, with 30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our approach achieves 1.65% and 0.76% accuracy increase over the existing structured pruning methods, respectively.",
    "full_text": "GOHSP: A Uniﬁed Framework of Graph and Optimization-Based Heterogeneous\nStructured Pruning for Vision Transformer\nMiao Yin1* Burak Uzkent2, Yilin Shen2, Hongxia Jin2, Bo Yuan1\n1 Rutgers University\n2 Samsung Research America\nAbstract\nThe recently proposed Vision transformers (ViTs) have\nshown very impressive empirical performance in various\ncomputer vision tasks, and they are viewed as an impor-\ntant type of foundation model. However, ViTs are typically\nconstructed with large-scale sizes, which then severely hin-\nder their potential deployment in many practical resources-\nconstrained applications. To mitigate this challenging prob-\nlem, structured pruning is a promising solution to compress\nmodel size and enable practical efﬁciency. However, unlike\nits current popularity for CNNs and RNNs, structured prun-\ning for ViT models is little explored.\nIn this paper, we propose GOHSP, a uniﬁed framework of\nGraph and Optimization-based Structured Pruning for ViT\nmodels. We ﬁrst develop a graph-based ranking for measur-\ning the importance of attention heads, and the extracted im-\nportance information is further integrated to an optimization-\nbased procedure to impose the heterogeneous structured spar-\nsity patterns on the ViT models. Experimental results show\nthat our proposed GOHSP demonstrates excellent compres-\nsion performance. On CIFAR-10 dataset, our approach can\nbring 40% parameters reduction with no accuracy loss for\nViT-Small model. On ImageNet dataset, with 30% and 35%\nsparsity ratio for DeiT-Tiny and DeiT-Small models, our ap-\nproach achieves1:65% and 0:76% accuracy increase over the\nexisting structured pruning methods, respectively.\nIntroduction\nRecently applying transformer architecture to computer vi-\nsion has emerged as an important forefront of foundation\nmodel design (Dosovitskiy et al. 2020). Thanks to the del-\nicate vision-speciﬁc self-attention, inherent minimal induc-\ntive biases and high scalability and parallelism,vision trans-\nformers (ViTs) (Dosovitskiy et al. 2020; Touvron et al.\n2021; Zhou et al. 2021) have shown very outstanding and\neven state-of-the-art performance in many fundamental and\ndownstream image and video processing tasks, such as im-\nage classiﬁcation, object detection, super-resolution, video\nclassiﬁcation etc.\nMotivated by the scaling success of the giant natural lan-\nguage processing (NLP) transformers (e.g., BERT (Devlin\n*This work was done during Miao Yin’s internship at Samsung\nResearch America.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\net al. 2018) and GPT-3 (Brown et al. 2020)), the existing\nViTs are also constructed with large model sizes to adapt for\nmassive data training (Zhai et al. 2021). Consequently, they\nare suffering from huge memory footprints and extensive\ncomputational costs. These limitations, if not being properly\naddressed, could severely hinder the widespread adoption of\nViTs in many practical scenarios, especially on the resource-\nconstrained mobile platforms and Internet-of-things (IoT)\ndevices.\nTo mitigate this challenging problem, one attractive solu-\ntion is to perform model compression (Yu et al. 2017; Kim\net al. 2016; Pan et al. 2019) to reduce the network costs with-\nout affecting task performance. However, unlike the current\npopularity of compressing convolutional and recurrent neu-\nral networks (CNNs and RNNs), ViT-oriented model com-\npression has not been systematically studied yet. In partic-\nular, structured pruning, as an important hardware-friendly\ncompression strategy that can bring practical efﬁciency on\nthe off-the-shelf hardware, is little explored for ViT models.\nTo date, a rich set of structured pruning approaches have\nbeen proposed and investigated in the existing literatures,\nand most of them focus on sparsifying the CNNs at the chan-\nnel level (He, Zhang, and Sun 2017; Ye et al. 2018). On the\nother hand, as will be analyzed and elaborated in Section ,\nbecause of the difference of the underlying architecture, the\nstructured sparse ViT models can exhibit multi-granularity\nsparsity (i.e., head-level and column-level) in the different\ncomponent modules (i.e., attention head and multi-layer per-\nception (MLP)). The co-existence of such heterogeneous\nsparse patterns raises a series of new research challenges and\nquestions when we consider the efﬁcient structured pruning\nstrategy for ViT models. For instance, for each component\nmodule what is the corresponding suitable pruning criterion\nto obtain its speciﬁc sparse pattern? Also, how should we\nperform the entire pruning process across different modules\nwith different levels of granularity sparsity to optimize the\noverall compression and task performance?\nTechnical Preview & Contributions. To answer these\nquestions, in this paper we propose GOHSP, a uniﬁed frame-\nwork of Graph and Optimization-based Structure Prun-\ning for vision transformer. To be speciﬁc, we ﬁrst de-\nvelop a graph-based ranking approach to measure the im-\nportance of attention heads. As a soft-pruning guideline,\nsuch importance information is then integrated to the overall\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n10954\noptimization-based procedure to impose the different types\nof structured sparsity in a joint and global way. Overall, the\ncontributions of this paper are summarized as follows:\n• We propose a graph-based ranking algorithm to mea-\nsure and determine the importance of attention heads.\nBy modeling the inter-head correlation as a converged\nMarkov chain, the head importance can be interpreted\nand calculated as the stationary distribution, which is fur-\nther used as a soft guideline for the overall pruning pro-\ncedure.\n• We propose a uniﬁed framework to jointly optimize dif-\nferent types of structured sparsity in the different mod-\nules. The complicated coordination for different sparse\npatterns are automatically learned and optimized in a sys-\ntematic way.\n• We evaluate the performance of our structured pruning\napproach of different ViT models. Particularly, On Ima-\ngeNet dataset, with 30% and 40% sparsity ratio for DeiT-\nTiny and DeiT-Small models, our approach achieves\n1:65% and 0:76% accuracy increase than the existing\nstructured pruning methods, respectively.\nRelated Work\nVision Transformer.Inspired by the grand success of trans-\nformer architecture in NLP domains, deep learning re-\nsearchers have actively explored the efﬁcient transformer-\nbased neural networks for computer vision. Most recently,\nseveral vision transformers (ViTs) and their variants have\nalready shown very impressive performance in several im-\nage and video processing tasks (Dosovitskiy et al. 2020;\nTouvron et al. 2021; Zhou et al. 2021). However, in order\nto achieve competitive performance with the state-of-the-art\nCNNs, ViTs typically have to scale up their model sizes and\ntherefore they suffer from costly computation and storage.\nDynamic Inference with ViTs. To reduce the deploy-\nment costs of ViTs, several works (Wang et al. 2021;\nBakhtiarnia, Zhang, and Iosiﬁdis 2021; Rao et al. 2021;\nMeng et al. 2022; Xu et al. 2022; Uzkent, Yeh, and Er-\nmon 2020; Uzkent and Ermon 2020) have been proposed to\nimprove the processing speed via dynamically pruning the\ntokens/patches or skipping transformer components adap-\ntively. Essentially as dynamic inference approaches, this set\nof works do not pursue to reduce the model sizes but focus\non input-aware inference to obtain practical speedup. Our\nstructured pruning-based solution is orthogonal to them, and\nthese two different strategies can be potentially combined\ntogether to achieve higher speed and smaller memory foot-\nprint.\nStructured Pruning. Model compression is a promising\nstrategy to reduce the deployment costs of neural networks.\nAmong various model compression techniques, structured\npruning is a very popular choice because its hardware-\nfriendly nature can bring practical efﬁciency on the real-\nworld devices. Based on different pruning criterias, various\nstructured pruning approaches have been extensively studied\nin the existing literature (Yu et al. 2018; Zhuang et al. 2018;\nLiu et al. 2019; He et al. 2019; Lin et al. 2020; Tiwari et al.\n2021; Lou et al. 2022), and most of them focus on pruning\nHead-based Sparsity\n(Multi-Head Attention)\n(a) Conventional Unstructured Sparsity\n(b) Unified Structured Sparsity (Ours)\nColumn-based Sparsity \n(Multi-Head Attention)\nColumn-based Sparsity \n(MLP)\nUnstructured Sparsity \n(MLP)\nUnstructured Sparsity\n(Multi-Head Attention)\nFigure 1: (a) Sparsity pattern of ViT models after un-\nstructured pruning. Only part of the Multi-Head Attention\nand MLP columns are pruned which are not hardware-\nfriendly.(b) Heterogeneous sparsity patterns of ViT models\nafter structured pruning. Certain MLP and Multi-Head At-\ntention columns are removed which is hardware-friendly.\nCNN models; while the efﬁcient structured pruning of ViTs\nis little explored. One of these studies, (Chen et al. 2021),\nprunes the vision transformers using structured pruning. (Yu\net al. 2022), on the other hand, focuses on FLOPs reduction\nwith the vision transformers using pruning, layer skipping,\nand knowledge distillation whereas in our study we focus on\nstructured pruning to mainly reduce the number of parame-\nters for building hardware-friendly compressed models. For\nthis reason, we compare our method to (Chen et al. 2021).\nStructured Pruning of ViTs: Analysis\nNotation. Considering an L-block vision transformer,\nW(l)\nattn = fW(l)\nqkv;W(l)\nprojgand W(l)\nmlp = fW(l)\nfc1;W(l)\nfc2grep-\nresent the weights of the attention layer and the MLP layer\nat l-th block, respectively. For each attention layer, there are\nH self-attention heads, namely W(l)\nqkv = fW(l;h)\nqkv gH\nh=1 and\nW(l)\nproj = fW(l;h)\nproj gH\nh=1. To simplify the notation, in the fol-\nlowing content we take one block as the example and omit\nthe superscript (layer index).\nHeterogeneity of structured sparsity. Because of the\ndifference of the network architecture, the meaning of\n‘structured sparsity’ varies with different model types. As\ndescribed and performed in (Wen et al. 2016; Anwar,\nHwang, and Sung 2017; Liu et al. 2018, 2020), the struc-\ntured pruning of CNN and RNN typically indicates the re-\nmoval of the entire channels of the weight tensors and the\nentire columns of the weight matrices, respectively. Notice\nthat here for either of these two cases, only one type of the\nstructured sparse pattern exist because of the architectural\nhomogeneity of the CNN and RNN.\nOn the other hand, a ViT model exhibits inherent archi-\ntectural heterogeneity. Within the same block, the front-end\nmulti-head attention module and the back-end MLP mod-\nule represent two types of design philosophy for information\nprocessing, and thereby leading to huge difference on both\n10955\ncomputing procedures and the available structured sparse\npatterns.\nTo be speciﬁc, when we consider performing structured\npruning of ViT model, three types of structured sparse pat-\nterns can co-exist with different levels of granularity across\ndifferent modules. For the multi-head attention module, be-\ncause each attention head is processing the information in-\ndividually in a parallel way, the pruning can be performed\nat the head-level to sparsify this component. In addition,\nconsider the weights in the heads are represented in the ma-\ntrix format; the column-level sparsity can also be introduced\ntowards structured pruning. Meanwhile, because the MLP\nconsists of multiple weight matrices as well, the column-\nlevel of granularity sparsity can be imposed on this back-end\nmodule at the same time. Consequently, a structured pruned\nViT model can exhibit heterogeneous structured sparsity\n(see Fig. 1).\nProblem Deﬁnition. Based on the above analysis, the\nstructured pruning of a vision transformer model with loss\nfunction `(\u0001) can be formulated as the following general op-\ntimization problem:\nmin\nWattn;Wmlp\n`(Wattn;Wmlp);\ns.t. kWattnkh\n0 \u0014\u0014h\nattn;\nkWattnkc\n0 \u0014\u0014c\nattn;\nkWmlpkc\n0 \u0014\u0014c\nmlp;\n(1)\nwhere \u0014c and \u0014h are the desired number of columns and the\ndesired number of heads after pruning, respectively. k\u0001k c\n0\nand k\u0001k h\n0 are the column-based and head-based group L0-\nnorm, which denote the number of non-zero columns and\nthe number of non-zero heads, respectively.\nQuestions to be Answered. Solving the above opti-\nmization problem is non-trivial since it contains the con-\nstraints involved with multi-granularity sparsity for different\nmodel components. More speciﬁcally, two important ques-\ntions need to be answered.Question #1:\nWhat is the suitable\npruning criterion to obtain head-level sparsity?\nAnalysis: From the perspective of information process-\ning, multi-head attention shares some interesting similarity\nwith convolutional layer. Both of them use multiple indi-\nvidual computing units, i.e., attention heads and convolu-\ntional ﬁlters, to perform parallel computations. Therefore, a\nnaive way to perform head-level pruning is to leverage the\nexisting criteria developed in the channel pruning of CNNs.\nHowever, such straightforward solution, in principle, may\nnot be the best choice because of two reasons. First, the re-\nceptive ﬁelds and the focused locality of the attention head\nand ﬁlters are different, and hence simply using the crite-\nrion for pruning channels is not a suitable strategy. Second\nand more importantly, most of the existing channel pruning\ncriterias are built on the information of each individual chan-\nnel (the corresponding ﬁlter weight and/or its feature map).\nWhen adopting this philosophy in the head pruning, the in-\nsufﬁcient utilization of inter-head information will probably\ncause non-negligible performance loss. Overall, the unique\ncharacteristics of multi-head attention mechanism calls for\nattention-speciﬁc pruning criterion.\nQuestion #2:\nHow should we coordinate the pruning\nacross different modules with different levels of granularity?\nAnalysis: As indicated before, three types of structured\nsparse pattern can co-exist in the different modules of the\npruned ViT models. A key component of the to-be-explored\nstructured pruning strategy is to develop a good coordination\nscheme that can properly impose these different structured\nsparse patterns in a joint and global way. Consider the com-\nplicated interaction among different types of structured spar-\nsity, the expected pruning strategy should be able to solve\nthis problem in a systematic and global way.\nStructured Pruning of ViTs: Method\nGraph-based Head Ranking\nTo answer Question #1, we propose a graph-based approach\nto measure and determine the importance of different at-\ntention heads, which can be further used for the follow-up\npruning. Our key idea is to model the inter-head correla-\ntion as a graph, and then leverage the graph-based ranking,\na methodology that has been successfully used in many web\nsearch and NLP algorithms, such as PageRank (Page et al.\n1999), TextRank (Mihalcea and Tarau 2004) and LexRank\n(Erkan and Radev 2004), to select important attention heads.\nGraph Construction of Markov Chain. To be speciﬁc,\nwe ﬁrst construct a graphG= (A;E) to represent the atten-\ntion heads and their similarities in the block of a ViT model.\nThe set of nodes A denote all the attention heads fAhgH\nh=1,\nand E is the set of connected edges. For edge E(Ai;Aj),\nits weight is deﬁned as the expected cosine similarity be-\ntween Ai and Aj. According to (Mihalcea and Tarau 2004),\nthe graph deﬁned with such cosine similarity can be inter-\npreted as a Markov chain, where each node is a state, and\nthe transition probability P(i;j) between two states is the\nedge weight. In such scenario, P(i;j) can be calculated as:\nP(i;j) =EX\u0018D[CosineSim(Ai(X);Aj(X))] ; (2)\nwhere Ai(X) is the output of i-th attention head with sam-\npled input X and Dis the data set. Built upon this calcula-\ntion, the entire transition matrix P of a Markov chain. No-\ntice that as indicated in (Erkan and Radev 2004), each col-\numn of P should be further normalized.\nBatch estimation.\nCalculating the transition probability\ncan be very costly since it needs to be performed across the\nentire training dataset D(see Eq. 2). To solve this problem,\nwe adopt a batch-based estimation strategy to improve com-\nputation efﬁciency without sacriﬁcing ranking performance.\nTo be speciﬁc, as described in Eq. 3, only a batch of training\ndata is sampled and used to to estimate the transition prob-\nability. As our ablation study in Section will show, using\ndifferent batch sizes (B ) bring very stable ranking results\nfor the attention heads, thereby empirically verifying the ef-\nfectiveness of this estimation strategy.\nP(i;j) =CosineSim\n BX\nb=1\nAi(Xb);\nBX\nb=1\nAj(Xb)\n!\n:\n(3)\nImportance Ranking. Mathematically, an irreducible\nand aperiodic Markov chain is guaranteed to converge to a\n10956\nMulti-Head Attention\nEmbedded \nPatches\nMLP\nBlock 0.1\nGraph-based Heads Ranking\nMulti-Head Attention\nEmbedded \nPatches\nMLP\nBlock\nData Optimization-based Soft Pruning\nMulti-Head Attention\nEmbedded \nPatches\nMLP\nBlock\nFine-Tuning\nX\nScore Mask\n0.7\n0.5\n0.3\nNormalize\nFigure 2: Procedure of the proposed multi-stage structured pruning approach.\nstationary distribution (Seneta 2006). As indicated in (Erkan\nand Radev 2004), once converged, the probability of a ran-\ndom walker stays in one state can reﬂect the state impor-\ntance. Motivated by this observation, we propose to quantify\nthe importance of each attention head via calculating the sta-\ntionary distribution in our constructed Markov chain. To that\nend, the iterativepower method (Erkan and Radev 2004) can\nbe used via setting a uniform distribution for the states as the\ninitialization. Overall, the entire graph-based head ranking\nprocedure is described in Algorithm 2.\nSoft-Pruning Mask. Once the importance score for each\nstate is obtained via calculating the stationary distribution,\nthe corresponding attention heads can be ranked. Here we\nuse a binary mark matrix Mattn = fMqkv;Mprojgto in-\ndicate the weight entries associated with the least important\nheads that should be removed. Notice that at this stage the\nhead pruning is not performed yet. Instead such binary mask\nserves as the guideline for the next-stage optimization, and\nit is essentially integrated into Eq. 1 as follows:\nmin\nWattn;Wmlp\n`(Wattn;Wmlp)\ns.t. k(1 \u0000Mattn) \fWattnk0 = 0;\nkWmlpk0 \u0014\u0014c\nmlp;\nkMattn \fWattnkc\n0 \u0014\u0014c\nattn;\n(4)\nwhere \fis element-wise product. In general, because the\noverall optimization phase coordinates and adjusts the dif-\nferent types of structured sparse pattern from a global per-\nspective, this ranking-only ”soft” pruning strategy, instead\nof directly pruning the least important heads, can provide\nmore ﬂexibility and possibility for the next-stage optimiza-\ntion procedure to identify better structured sparse models.\nOptimization-based Structured Pruning\nAs pointed out by Question #2, the co-existence of multi-\ngranularity and multi-location of the sparsity of ViT models\nmake the entire structured pruning procedure become very\nchallenging. To solve this, we propose to use advanced op-\ntimization technique to perform systematic structured prun-\ning. To be speciﬁc, considering the complicated interactions\namong different types of structured sparsity, we do not prune\nthe heads or columns immediately, since any direct hard\npruning at the early stage may cause severe accuracy loss.\nInstead, we adopt ”soft-pruning” strategy via optimizing the\nentire ViT models towards the desired structured sparse for-\nmats. In other words, the three types of sparsity pattern are\ngradually imposed onto the attention heads and MLPs.\nTo that end, we ﬁrst relax the constraints of Eq. 4 and\nrewrite it as follows:\nmin\nWattn;Wmlp\n`(Wattn;Wmlp) +\u0015\n2 k(1 \u0000Mattn) \fWattnk2\nF;\ns.t. kWmlpkc\n0 \u0014\u0014c\nmlp;\nkMattn \fWattnkc\n0 \u0014\u0014c\nattn;\n(5)\nwhere \u0015 is the coefﬁcient that controls the inﬂuence of\nquadratic term.\nOptimization-based Soft Pruning. As indicated in\n(Boyd, Parikh, and Chu 2011), when the constraints of con-\ntinuous non-convex problem are sparsity related (as Eq.\n5 shows), Douglas—Rachford splitting method (Eckstein\nand Bertsekas 1992) can be a suitable optimization solution\nfor such types of problem. Following this philosophy, we\nﬁrst introduce auxiliary variables Zattn;Zmlp and indicator\nfunctions as:\ng(Zattn) =\n\u001a0 kMattn \fZattnkc\n0 \u0014\u0014c\nattn;\n+1 otherwise; (6)\nh(Zmlp) =\n\u001a0 kZmlpkc\n0 \u0014\u0014c\nmlp;\n+1 otherwise: (7)\nThen, we can rewrite Eq. 5 as the following equivalent form:\nmin\nW;Z\n`(Wattn;Wmlp) +g(Zattn) +h(Zmlp)+\n\u0015\n2 k(1 \u0000Mattn) \fWattnk2\nF;\ns.t. Wmlp = Zmlp;\nWattn = Zattn:\n(8)\n10957\nIn such scenario, the corresponding augmented Lagrangian\nfunction of the above optimization objective is:\nL\u001a(Wattn;Wmlp;Zmlp) =`(Wattn;Wmlp) +g(Zattn)+\nh(Zmlp) +\u0015\n2 k(1 \u0000Mattn) \fWattnk2\nF+\n\u001a\n2kWattn \u0000Zattn + Uattnk2\nF+\n\u001a\n2kUattnk2\nF + \u001a\n2kWmlp \u0000Zmlp + Umlpk2\nF + \u001a\n2kUmlpk2\nF;\n(9)\nwhere \u001a >0 is the penalty parameter, and Uattn;Umlp are\nthe Lagrangian multipliers. Then the variables at step tcan\nbe iteratively updated as:\nWt\nattn = Wt\u00001\nattn\u0000\u0011\n`(Wattn;Wt\u00001\nmlp )\nWattn\n\u0000\n\u0015\n\u0002\n(1 \u0000Mattn) \fWt\u00001\nattn\n\u0003\n\u0000\u001a(Wt\u00001\nattn \u0000Zt\u00001\nattn + Ut\u00001\nattn);\n(10)\nWt\nmlp = Wt\u00001\nmlp \u0000\u0011`(Wt\nattn;Wmlp)\nWmlp\n\u0000\n\u001a(Wt\u00001\nmlp \u0000Zt\u00001\nmlp + Ut\u00001\nmlp );\n(11)\nZt\nattn = P(Wt\nattn + Ut\u00001\nattn); (12)\nZt\nmlp = P(Wt\nmlp + Ut\u00001\nmlp ); (13)\nUt\nattn = Ut\u00001\nattn + Wt\nattn \u0000Zt\nattn; (14)\nUt\nmlp = Ut\u00001\nmlp + Wt\nmlp \u0000Zt\nmlp: (15)\nHere \u0011is the optimizer learning rate for training the ViT, and\nP is the Euclidean projection for the sparse constraint.\nFinal Hard-Pruning and Fine-Tuning. After the above\ndescribed optimization procedure, the structured sparse pat-\nterns have been gradually imposed onto the ViT model.\nIn other words, the weight values of the masked attention\nheads, as well as some columns of MLPs and attention\nheads, become extremely small. At this stage, we can now\nprune those small weights and then perform a few rounds of\nﬁne-tuning to achieve higher performance.\nAlgorithm 1: Overall Procedure of GOHSP Framework\nInput: Dense weight fWattn;Wmlpg, desired model size\nf\u0014attn;\u0014mlpg, training data D, number of epochs E;\nOutput: Structured sparse weight f~Wattn; ~Wmlpg;\n1: Sample a batch of data fXbgB\nb=1 from D;\n2: Calculate importance score s via Alg. 2;\n3: Obtain structured mask Mattn according to s;\n4: Zattn := Wattn, Zmlp := Wmlp; // Initialize auxiliary\nvariables\n5: Uattn := 0, Umlp := 0; // Initialize Lagrangian multi-\npliers\n6: for e= 1to Edo\n7: Update Wattn;Wattn via Eq. 10 and Eq. 11;\n8: Update Zattn;Zmlp via Eq. 12 and Eq. 13;\n9: Update Uattn;Umlp via Eq. 14 and Eq. 15;\n10: Fine-tune pruned weight f~Wattn; ~Wmlpg.\nAlgorithm 2: Graph-based Attention Head Ranking\nInput: Sampled batch fXbgB\nb=1, attention heads fAhgH\nh=1;\nOutput: Importance score s = [s1;\u0001\u0001\u0001 ;sH].\n1: Initialize transition matrix: P := zeros(H;H);\n2: for i= 1to H do\n3: for j = 1to H do\n4: Calculate P(i;j) via Eq. 3;\n5: Normalize each column of P;\n6: Initialize s := ones(H)=H;\n7: repeat\n8: s0:= s;\n9: s := Ps;\n10: \u000e:= ks \u0000s0k;\n11: until \u000e\u0014\u000f\nMethod Sparsity\n# Paramters Top-1 (%)\nBaseline -\n48.0M 97.85\nSOMP 40%\n28.8M 96.07\nSGMP 40% 28.8M 96.93\nGOHSP (Ours) 40% 28.8M 97.89\nGOHSP (Ours) 80% 9.6M 97.40\nTable 1: Performance comparison between our GOHSP\nand structured one-shot/gradually magnitude-based pruning\n(SOMP/SGMP) of ViT-Small model on CIFAR-10 dataset.\nOverall, by using graph-based head ranking and\noptimization-based structured pruning, the previously raised\nQuestion #1 and #2 can be properly addressed. The overall\nGOHSP framework is summarized in Fig. 2.\nExperiments\nExperimental Settings\nDataset and Baseline. We evaluate the performance of\nour proposed GOHSP approach on CIFAR-10 and Ima-\ngeNet datasets (Deng et al. 2009). For experiments on\nthe CIFAR-10 dataset, the original dense model is ViT-\nSmall1(Dosovitskiy et al. 2020) with 48M parameters. For\nexperiments on the ImageNet dataset, the original dense\nmodels are DeiT-Tiny and DeiT-Small (Touvron et al. 2021)\nwith 5.7M and 22.1M parameters, respectively.\nHyper-parameters and Sparsity Ratio. For our experi-\nments on the CIFAR-10 dataset, the batch size, learning rate\nand \u001a are set as 256, 0.1 and 0.001, respectively. For Ima-\ngeNet dataset, the batch size, learning rate and \u001aare set as\n256, 0.01 and 0.001, respectively. For both of these two ex-\nperiments, SGD is selected as the training optimizer with-\nout using weight decay, and we apply Erd ˝os-R´enyi (Mo-\ncanu et al. 2018) to determine the sparsity distribution of\neach layer given an overall sparsity ratio. In particular, soft-\npruning maintains high accuracy at the high sparsity ratios.\n1We take this model from open source library timm.\n10958\nModel Method Sparsity\n# Parameters FLOPs # Run-time # Top-1 (%)\nDeiT-T\niny\nBaseline - 5.7M - - 72.20\nOMP (Unstructured)\n30% 4.02M 25.56% - 68.35\nGMP (Unstructured) 30% 4.02M 25.56% - 69.56\nTP (Unstructured) 30% 4.02M 25.56% - 68.38\nSSP (Structured) 30% 4.2M 23.69% - 68.59\nS2ViTE (Structured) 30% 4.2M 23.69% 10.57 % 70.12\nGOHSP (Structured) 30% 4.0M 30% 13.41% 70.24\nDeiT-Small\nBaseline -\n22.1M - - 79.90\nSSP (Structured)\n40% 14.6M 31.63% - 77.74\nS2ViTE (Structured) 40% 14.6M 31.63% 22.65% 79.22\nGOHSP (Structured) 40% 14.4M 35% 24.61% 79.98\nGOHSP (Structured) 50% 11.1M 39% 26.57% 79.86\nTable 2: Comparison results of our method, GOHSP, with other structured and unstructured pruning methods on ImageNet.\nPerformance Evaluation\nCIFAR-10 Dataset. Table 1 shows performance compari-\nson on CIFAR-10 dataset between our proposed GOHSP\nand other structured pruning method (structured one-shot\nmagnitude pruning (SOMP) (Han, Mao, and Dally 2015)\nand structured gradually magnitude pruning (SGMP) (Zhu\nand Gupta 2017)) for ViT-Small model. It is seen that\nwith the same sparsity ratio, our approach brings signiﬁcant\nperformance improvement. Compared to SGMP approach,\nGOHSP achieves 0:96% accuracy increase with the same\npruned model size. Even compared with the baseline, the\nstructured sparse model pruned by GOHSP can outperform\nthe uncompressed model with 40% fewer parameters while\n80% compressed model achieves only0:45% worse than the\nfull ViT-Small model.\nImageNet Dataset. Table 2 summarizes the performance\non ImageNet dataset between GOHSP and other structured\npruning approaches (SOMP, SGMP, Talyer pruning (TP),\nSalience-based Structured Pruning (SSP) and S2ViTE(Chen\net al. 2021)) for DeiT-Tiny and DeiT-Small models. It is seen\nthat due to the limited redundancy in such small-size model,\nthe existing pruning approaches suffer from more than2:5%\naccuracy loss when compressing DeiT-Tiny. Instead, with\nthe even fewer parameters and more FLOPs reduction, our\nGOHSP approach can achieve at least 0:68% accuracy in-\ncrease over the unstructured pruning approaches. Compared\nto the structured pruning approach (SSP), our method enjoys\n1:65% accuracy improvement with lower storage cost and\ncomputational cost. In addition, when compressing DeiT-\nSmall model, with fewer parameters and more FLOPs re-\nduction, our GOHSP approach can achieve 0:76% accuracy\nincrease as compared to the state-of-the-art structured prun-\ning method S2ViTE (Chen et al. 2021) and can even outper-\nform the original DeiT-Small. With50% pruned DeiT-Small\nwe achieve similar accuracy to the full DeiT-Small. Finally,\nwe report 26:57% improvement in run-time efﬁciency with\nour 50% pruned DeiT-Small.\nAblation Study, Visualization and Discussion\nTo obtain the deep understanding of the effect of our pro-\nposed approach, we perform several ablation studies and a\n0.2 0.3 0.4 0.5 0.6\nSparsity\n90.0\n92.0\n94.0\n96.0\n98.0Top-1 Accuracy (%)\nOurs Hard Pruning\nFigure 3: Results on the effect of soft-pruning (ours) and\nhard-pruning for ViT-Small model on CIFAR-10 dataset.\ndetailed analysis. Here the experiments conducted in the ab-\nlation study focus on compressing ViT-Small on CIFAR-10.\nSoft Pruning vs Hard Pruning. As described in Opti-\nmization section, after ranking the attention heads, we use\nthe ranking information as a soft-pruning mask to guide\nthe next-phase optimization. The optimization itself is also\n1 2 3 4 5 6 7 8\nHead Index\n1\n2\n3\n4\n5\n6\n7\n8 Block Index\nBatch Size=256\n0\n2\n4\n6\n1 2 3 4 5 6 7 8\nHead Index\n1\n2\n3\n4\n5\n6\n7\n8 Block Index\nBatch Size=512\n0\n2\n4\n6\n1 2 3 4 5 6 7 8\nHead Index\n1\n2\n3\n4\n5\n6\n7\n8 Block Index\nBatch Size=1024\n0\n2\n4\n6\n1 2 3 4 5 6 7 8\nHead Index\n1\n2\n3\n4\n5\n6\n7\n8 Block Index\nBatch Size=1536\n0\n2\n4\n6\nFigure 4: The effect of batch sizes for ranking results. Dif-\nferent colors represent different ranking scores. We can see\nthat our head ranking algorithm is not sensitive to batch size.\n10959\na soft-pruning procedure that does not directly zero the\nweights but gradually impose the structured sparsity. To ana-\nlyze the effect of this strategy, we conduct an ablation exper-\niment via performing the direct hard pruning. In this ablation\nstudy, the least important attention heads are removed ac-\ncording to their ranks, and the columns of MLPs with least\ngroup L1 norm are also pruned. Such hard pruned models\nare still trained with the same hyper-parameters settings that\nare used for soft pruning method. Fig. 3 shows the curves\nof top-1 test accuracy with different target sparsity settings.\nThe soft-pruning strategy brings very signiﬁcant accuracy\nimprovement over the direct hard pruning with the same\nsparsity ratio.\nEffect of Batch Size on Head Ranking.As shown in Eq.\n3, the importance scores of attention head is calculated on\na batch of data. To investigate the potential impact of batch\nsizes for the ranking results, we observe the change of rank-\ning with different batch sizes. As shown in Fig. 4, the rank-\ning results are very stable (almost the same) when the batch\nsize varies. Therefore we can conclude that using batches of\ndata can already achieve very good estimation of head rank-\ning. In other words, our ranking approach has low sensitivity\nto the distribution of input data.\nSensitivity of Penalty Parameter \u001a. We also explore the\neffect of hyperparameter \u001aon the structured pruning proce-\ndure. Fig. 5 (a) shows the convergence of training process\nwith respect to different \u001a. It is seen that the convergence\nspeed is always fast, and hence it demonstrates the promis-\ning convergence property of our approach in practice. Fig. 5\n0 10 20 30 40 50 60\nEpoch\n0\n250\n500Loss\n(a) Curves of training loss\n=0.001\n=0.002\n=0.0005\n0 10 20 30 40 50 60\nEpoch\n0\n50\n100\nL2-Norm\n(b) Curves of sparsity strength\n=0.001\n=0.002\n=0.0005\n0 10 20 30 40 50 60\nEpoch\n50\n75\n100Top-1 (%)\n(c) Curves of test accuracy\n=0.001\n=0.002\n=0.0005\n40 50 60\n0\n3\n6\n40 50 60\n96\n97\n98\nFigure 5: Effect of \u001aon the structured pruning procedure. \u001a\ncontrols the trade-off between the speed of imposing sparsity\nand task performance.\n(b) illustrates the L2-norm of the masked entries. It is seen\nthat the larger\u001amakes the model exhibit more sparsity at the\nearlier stage, thereby indicating that larger\u001acan bring fewer\nepochs in the ﬁnal ﬁne-tuning stage. However, as shown in\nFig. 5 (c), too large \u001abrings accuracy degradation, so \u001acan\nbe considered as a parameter that controls the trade-off be-\ntween the speed of imposing sparsity and task performance.\nVisualization. Fig. 6 illustrates the sparsity patterns in\nthe pruned ViT models after performing our GOHSP ap-\nproach. It is seen that three types of structured sparsity pat-\nterns (head-level sparsity, column-level sparsity in the head\nand column-level sparsity in the MLP) are imposed on the\npruned models. Such pruning can be more effective on hard-\nware than the unstructured pruning methods.\nBlock9Block10Block11\nMulti-Head Attention Layer MLP Layer\nBlock9Block10Block11\nMulti-Head Attention Layer MLP Layer\nBlock9Block10Block11\nMulti-Head Attention Layer MLP Layer\nFigure 6: Visualization of the imposed structured sparsity on\nthe DeiT-Small model. The columns and heads with lighter\ncolor are pruned. Our method can prune columns (Block9,\nBlock10, and Block11), and heads (Block10, Block11) of\nthe Multi-Head Attention layer. On the other hand, we can\nprune columns of MLP layers in all the blocks.\nWhy Douglas—Rachford splitting method?\n1) Convergence:\nAccording to (Boyd, Parikh, and Chu\n2011), within a few iterations it can provide satisﬁed\nsolution for large-scale problems – particularly attractive\nfor DNN applications. More speciﬁcally for this work,\nthe fast convergence of Douglas—Rachford splitting\nmethod can avoid gradient explosion problem introduced\nby the additional sparsity loss in Eq. 9. 2) Flexibility:\nDouglas—Rachford splitting method, by its nature, divides\nthe original difﬁcult optimization problem into several\nless complicated sub-problems, each of which can be then\naddressed independently.\nConclusion\nIn this paper we propose GOHSP, a uniﬁed framework to\nperform graph and optimization-based heterogeneous struc-\ntured pruning for vision transformers. By using graph-based\nranking and leveraging the advanced optimization tech-\nnique, our approach can efﬁciently impose different types\nof structured sparse patterns on the vision transformers with\nhigh compression rate and task performance.\n10960\nReferences\nAnwar, S.; Hwang, K.; and Sung, W. 2017. Structured prun-\ning of deep convolutional neural networks. ACM Journal\non Emerging Technologies in Computing Systems (JETC) ,\n13(3): 1–18.\nBakhtiarnia, A.; Zhang, Q.; and Iosiﬁdis, A. 2021. Single-\nLayer Vision Transformers for More Accurate Early Exits\nwith Less Overhead. arXiv preprint arXiv:2105.09121.\nBoyd, S.; Parikh, N.; and Chu, E. 2011. Distributed opti-\nmization and statistical learning via the alternating direc-\ntion method of multipliers. Now Publishers Inc.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nChen, T.; Cheng, Y .; Gan, Z.; Yuan, L.; Zhang, L.; and\nWang, Z. 2021. Chasing sparsity in vision transformers:\nAn end-to-end exploration. Advances in Neural Information\nProcessing Systems, 34.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEckstein, J.; and Bertsekas, D. P. 1992. On the Dou-\nglas—Rachford splitting method and the proximal point al-\ngorithm for maximal monotone operators. Mathematical\nProgramming, 55(1): 293–318.\nErkan, G.; and Radev, D. R. 2004. Lexrank: Graph-based\nlexical centrality as salience in text summarization. Journal\nof artiﬁcial intelligence research, 22: 457–479.\nHan, S.; Mao, H.; and Dally, W. J. 2015. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149.\nHe, Y .; Liu, P.; Wang, Z.; Hu, Z.; and Yang, Y . 2019. Filter\npruning via geometric median for deep convolutional neu-\nral networks acceleration. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n4340–4349.\nHe, Y .; Zhang, X.; and Sun, J. 2017. Channel pruning for ac-\ncelerating very deep neural networks. In Proceedings of the\nIEEE international conference on computer vision, 1389–\n1397.\nKim, Y .-D.; Park, E.; Yoo, S.; Choi, T.; Yang, L.; and Shin,\nD. 2016. Compression of deep convolutional neural net-\nworks for fast and low power mobile applications. In Inter-\nnational Conference on Learning Representations.\nLin, M.; Ji, R.; Wang, Y .; Zhang, Y .; Zhang, B.; Tian, Y .;\nand Shao, L. 2020. Hrank: Filter pruning using high-rank\nfeature map. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 1529–1538.\nLiu, N.; Ma, X.; Xu, Z.; Wang, Y .; Tang, J.; and Ye, J.\n2020. AutoCompress: An automatic DNN structured prun-\ning framework for ultra-high compression rates. InProceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 34, 4876–4883.\nLiu, Z.; Mu, H.; Zhang, X.; Guo, Z.; Yang, X.; Cheng, K.-T.;\nand Sun, J. 2019. Metapruning: Meta learning for automatic\nneural network channel pruning. InProceedings of the IEEE\nInternational Conference on Computer Vision, 3296–3305.\nLiu, Z.; Sun, M.; Zhou, T.; Huang, G.; and Darrell, T. 2018.\nRethinking the value of network pruning. arXiv preprint\narXiv:1810.05270.\nLou, Q.; Hsu, Y .-C.; Uzkent, B.; Hua, T.; Shen, Y .; and Jin,\nH. 2022. Lite-MDETR: A Lightweight Multi-Modal Detec-\ntor. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 12206–12215.\nMeng, L.; Li, H.; Chen, B.-C.; Lan, S.; Wu, Z.; Jiang, Y .-\nG.; and Lim, S.-N. 2022. AdaViT: Adaptive Vision Trans-\nformers for Efﬁcient Image Recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 12309–12318.\nMihalcea, R.; and Tarau, P. 2004. Textrank: Bringing order\ninto text. In Proceedings of the 2004 Conference on Empir-\nical Methods in Natural Language Processing, 404–411.\nMocanu, D. C.; Mocanu, E.; Stone, P.; Nguyen, P. H.;\nGibescu, M.; and Liotta, A. 2018. Scalable training of ar-\ntiﬁcial neural networks with adaptive sparse connectivity in-\nspired by network science. Nature Communications, 9(1):\n1–12.\nPage, L.; Brin, S.; Motwani, R.; and Winograd, T. 1999. The\nPageRank citation ranking: Bringing order to the web. Tech-\nnical report, Stanford InfoLab.\nPan, Y .; Xu, J.; Wang, M.; Ye, J.; Wang, F.; Bai, K.; and Xu,\nZ. 2019. Compressing recurrent neural networks with tensor\nring for action recognition. InProceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 33, 4683–4690.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efﬁcient Vision Transform-\ners with Dynamic Token Sparsiﬁcation. arXiv preprint\narXiv:2106.02034.\nSeneta, E. 2006. Non-negative matrices and Markov chains.\nSpringer Science & Business Media.\nTiwari, R.; Bamba, U.; Chavan, A.; and Gupta, D. 2021.\nChipNet: Budget-Aware Pruning with Heaviside Continuous\nApproximations. In International Conference on Learning\nRepresentations.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nUzkent, B.; and Ermon, S. 2020. Learning when and where\nto zoom with deep reinforcement learning. In Proceedings\n10961\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 12345–12354.\nUzkent, B.; Yeh, C.; and Ermon, S. 2020. Efﬁcient object\ndetection in large images using deep reinforcement learn-\ning. In Proceedings of the IEEE/CVF winter conference on\napplications of computer vision, 1824–1833.\nWang, Y .; Huang, R.; Song, S.; Huang, Z.; and Huang, G.\n2021. Not All Images are Worth 16x16 Words: Dynamic\nVision Transformers with Adaptive Sequence Length.arXiv\npreprint arXiv:2105.15075.\nWen, W.; Wu, C.; Wang, Y .; Chen, Y .; and Li, H. 2016.\nLearning structured sparsity in deep neural networks. Ad-\nvances in neural information processing systems, 29: 2074–\n2082.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2022. Evo-vit: Slow-fast\ntoken evolution for dynamic vision transformer. InProceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 36, 2964–2972.\nYe, J.; Lu, X.; Lin, Z.; and Wang, J. Z. 2018. Rethinking the\nsmaller-norm-less-informative assumption in channel prun-\ning of convolution layers. arXiv preprint arXiv:1802.00124.\nYu, R.; Li, A.; Chen, C.-F.; Lai, J.-H.; Morariu, V . I.; Han,\nX.; Gao, M.; Lin, C.-Y .; and Davis, L. S. 2018. Nisp: Prun-\ning networks using neuron importance score propagation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 9194–9203.\nYu, S.; Chen, T.; Shen, J.; Yuan, H.; Tan, J.; Yang, S.; Liu, J.;\nand Wang, Z. 2022. Uniﬁed Visual Transformer Compres-\nsion. In International Conference on Learning Representa-\ntions.\nYu, X.; Liu, T.; Wang, X.; and Tao, D. 2017. On compress-\ning deep models by low rank and sparse decomposition. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 7370–7379.\nZhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L.\n2021. Scaling vision transformers. arXiv preprint\narXiv:2106.04560.\nZhou, D.; Kang, B.; Jin, X.; Yang, L.; Lian, X.; Jiang, Z.;\nHou, Q.; and Feng, J. 2021. Deepvit: Towards deeper vision\ntransformer. arXiv preprint arXiv:2103.11886.\nZhu, M.; and Gupta, S. 2017. To prune, or not to prune: ex-\nploring the efﬁcacy of pruning for model compression.arXiv\npreprint arXiv:1710.01878.\nZhuang, Z.; Tan, M.; Zhuang, B.; Liu, J.; Guo, Y .; Wu, Q.;\nHuang, J.; and Zhu, J. 2018. Discrimination-aware channel\npruning for deep neural networks. In Advances in Neural\nInformation Processing Systems, 875–886.\n10962"
}