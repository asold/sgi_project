{
  "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
  "url": "https://openalex.org/W3025064182",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2027506410",
      "name": "Nora Kassner",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2964117978",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2509019445",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962926715",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2047431989",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963372062",
    "https://openalex.org/W2909970382",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W2951286828"
  ],
  "abstract": "Building on Petroni et al. (2019), we pro- pose two new probing tasks analyzing fac- tual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (“Birds cannot [MASK]”) and non-negated (“Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (“Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811–7818\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7811\nNegated and Misprimed Probes for Pretrained Language Models:\nBirds Can Talk, But Cannot Fly\nNora Kassner, Hinrich Sch¨utze\nCenter for Information and Language Processing (CIS)\nLMU Munich, Germany\nkassner@cis.lmu.de\nAbstract\nBuilding on Petroni et al. (2019), we pro-\npose two new probing tasks analyzing fac-\ntual knowledge stored in Pretrained Language\nModels (PLMs). (1) Negation. We ﬁnd\nthat PLMs do not distinguish between negated\n(“Birds cannot [MASK]”) and non-negated\n(“Birds can [MASK]”) cloze questions. (2)\nMispriming. Inspired by priming methods in\nhuman psychology, we add “misprimes” to\ncloze questions (“Talk? Birds can [MASK]”).\nWe ﬁnd that PLMs are easily distracted by\nmisprimes. These results suggest that PLMs\nstill have a long way to go to adequately learn\nhuman-like factual knowledge.\n1 Introduction\nPLMs like Transformer-XL (Dai et al., 2019),\nELMo (Peters et al., 2018) and BERT (Devlin et al.,\n2019) have emerged as universal tools that capture\na diverse range of linguistic and factual knowledge.\nRecently, Petroni et al. (2019) introduced LAMA\n(LAnguage Model Analysis) to investigate whether\nPLMs can recall factual knowledge that is part of\ntheir training corpus. Since the PLM training ob-\njective is to predict masked tokens, question an-\nswering (QA) tasks can be reformulated as cloze\nquestions. For example, “Who wrote ‘Dubliners’?”\nis reformulated as “[MASK] wrote ‘Dubliners’.” In\nthis setup, Petroni et al. (2019) show that PLMs out-\nperform automatically extracted knowledge bases\non QA. In this paper, we investigate this capability\nof PLMs in the context of (1) negation and what\nwe call (2) mispriming.\n(1) Negation. To study the effect of negation\non PLMs, we introduce the negated LAMA dataset.\nWe insert negation elements (e.g., “not”) in LAMA\ncloze questions (e.g., “The theory of relativity was\nnot developed by [MASK].”) – this gives us posi-\ntive/negative pairs of cloze questions.\nQuerying PLMs with these pairs and comparing\nthe predictions, we ﬁnd that the predicted ﬁllers\nhave high overlap. Models are equally prone to\ngenerate facts (“Birds can ﬂy”) and their incor-\nrect negation (“Birds cannot ﬂy”). We ﬁnd that\nBERT handles negation best among PLMs, but it\nstill fails badly on most negated probes. In a second\nexperiment, we show that BERT can in principle\nmemorize both positive and negative facts correctly\nif they occur in training, but that it poorly gener-\nalizes to unseen sentences (positive and negative).\nHowever, after ﬁnetuning, BERT does learn to cor-\nrectly classify unseen facts as true/false.\n(2) Mispriming. We use priming, a standard\nexperimental method in human psychology (Tul-\nving and Schacter, 1990) where a ﬁrst stimulus\n(e.g., “dog”) can inﬂuence the response to a sec-\nond stimulus (e.g., “wolf” in response to “name\nan animal”). Our novel idea is to use priming\nfor probing PLMs, speciﬁcally mispriming: we\ngive automatically generated misprimes to PLMs\nthat would not mislead humans. For example, we\nadd “Talk? Birds can [MASK]” to LAMA where\n“Talk?” is the misprime. A human would ignore\nthe misprime, stick to what she knows and produce\na ﬁller like “ﬂy”. We show that, in contrast, PLMs\nare misled and ﬁll in “talk” for the mask.\nWe could have manually generated more natural\nmisprimes. For example, misprime “regent of Anti-\noch” in “Tancred, regent of Antioch, played a role\nin the conquest of [MASK]” tricks BERT into chos-\ning the ﬁller “Antioch” (instead of “Jerusalem”).\nOur automatic misprimes are less natural, but au-\ntomatic generation allows us to create a large mis-\nprime dataset for this initial study.\nContribution. We show that PLMs’ ability to\nlearn factual knowledge is – in contrast to human\ncapabilities – extremely brittle for negated sen-\ntences and for sentences preceded by distracting\nmaterial (i.e., misprimes). Data and code will be\n7812\npublished.1\n2 Data and Models\nLAMA’s cloze questions are generated from\nsubject-relation-object triples from knowledge\nbases (KBs) and question-answer pairs. For KB\ntriples, cloze questions are generated, for each re-\nlation, by a templatic statement that contains vari-\nables X and Y for subject and object (e.g, “X was\nborn in Y”). We then substitute the subject for X\nand MASK for Y . In a question-answer pair, we\nMASK the answer.\nLAMA is based on several sources: (i) Google-\nRE. 3 relations: “place of birth”, “date of birth”,\n“place of death”. (ii) T-REx (Elsahar et al., 2018).\nSubset of Wikidata triples. 41 relations. (iii) Con-\nceptNet (Li et al., 2016). 16 commonsense rela-\ntions. The underlying corpus provides matching\nstatements to query PLMs. (iv) SQuAD (Rajpurkar\net al., 2016). Subset of 305 context-insensitive\nquestions, reworded as cloze questions.\nWe use the source code provided by Petroni\net al. (2019) and Wolf et al. (2019) to evaluate\nTransformer-XL large (Txl), ELMo original (Eb),\nELMo 5.5B (E5B), BERT-base (Bb) and BERT-\nlarge (Bl).\nNegated LAMA. We created negated LAMA\nby manually inserting a negation element in each\ntemplate or question. For ConceptNet we only\nconsider an easy-to-negate subset (see appendix).\nMisprimed LAMA. We misprime LAMA by\ninserting an incorrect word and a question mark\nat the beginning of a statement; e.g., “Talk?” in\n“Talk? Birds can [MASK].” We only misprime\nquestions that are answered correctly by BERT-\nlarge. To make sure the misprime is misleading,\nwe manually remove correct primes for SQuAD\nand ConceptNet and automatically remove primes\nthat are the correct ﬁller for a different instance of\nthe same relation for T-REx and ConceptNet. We\ncreate four versions of misprimed LAMA (A, B, C,\nD) as described in the caption of Table 3; Table 1\ngives examples.\n3 Results\nNegated LAMA. Table 2 gives spearman rank cor-\nrelation ρand % overlap in rank 1 predictions be-\ntween original and negated LAMA.\nOur assumption is that the correct answers for\na pair of positive question and negative question\n1https://github.com/norakassner/LAMA primed negated\nVersion Query\nA Dinosaurs? Munich is located in [MASK] .\nB Somalia? Munich is located in [MASK] .\nC Prussia? Munich is located in [MASK] .\nD Prussia? “This is great”. . . .\n“What a surprise.” “Good to know.” . . .\nMunich is located in [MASK] .\nTable 1: Examples for different versions of misprimes:\n(A) are randomly chosen, (B) are randomly chosen\nfrom correct ﬁllers of different instances of the relation,\n(C) were top-ranked ﬁllers for the original cloze ques-\ntion but have at least a 30% lower prediction probabil-\nity than the correct object. (D) is like (C) except that 20\nshort neutral sentences are inserted between misprime\nand MASK sentence.\nshould not overlap, so high values indicate lack\nof understanding of negation. The two measures\nare complementary and yet agree very well. The\ncorrelation measure is sensitive in distinguishing\ncases where negation has a small effect from those\nwhere it has a larger effect.2 % overlap is a measure\nthat is direct and easy to interpret.\nIn most cases, ρ> 85%; overlap in rank 1 pre-\ndictions is also high. ConcepNet results are most\nstrongly correlated but TREx 1-1 results are less\noverlapping. Table 4 gives examples (lines marked\n“N”). BERT has slightly better results. Google-RE\ndate of birth is an outlier because the pattern “X\n(not born in [MASK])” rarely occurs in corpora\nand predictions are often nonsensical.\nIn summary, PLMs poorly distinguish positive\nand negative sentences.\nWe give two examples of the few cases where\nPLMs make correct predictions, i.e., they solve\nthe cloze task as human subjects would. For “The\ncapital of X is not Y” (TREX, 1-1) top ranked pre-\ndictions are “listed”, “known”, “mentioned” (vs.\ncities for “The capital of X is Y”). This is appropri-\nate since the predicted sentences are more common\nthan sentences like “The capital of X is not Paris”.\nFor “X was born in Y”, cities are predicted, but\n2A reviewer observes that spearman correlation is gener-\nally high and wonders whether high spearman correlation is re-\nally a reliable indicator of negation not changing the answer of\nthe model. As a sanity check, we also randomly sampled, for\neach query correctly answered by BERT-large (e.g., “Einstein\nborn in [MASK]”), another query with a different answer, but\nthe same template relation (e.g., “Newton born in [MASK]”)\nand computed the spearman correlation between the predic-\ntions for the two queries. In general, these positive-positive\nspearman correlations were signiﬁcantly lower than those be-\ntween positive (“Einstein born in [MASK]”) and negative\n(“Einstein not born in [MASK]”) queries (t-test, p <0.01).\nThere were two exceptions (not signiﬁcantly lower): T-REx\n1-1 and Google-RE birth-date.\n7813\nFacts Rels Txl Eb E5b Bb Bl\nρ % ρ % ρ % ρ % ρ %\nGoogle-RE\nbirth-place 2937 1 92.8 47.1 97.1 28.5 96.0 22.9 89.3 11.2 88.3 20.1\nbirth-date 1825 1 87.8 21.9 92.5 1.5 90.7 7.5 70.4 0.1 56.8 0.3\ndeath-place 765 1 85.8 1.4 94.3 57.8 95.9 80.7 89.8 21.7 87.0 13.2\nT-REx\n1-1 937 2 89.7 88.7 95.0 28.6 93.0 56.5 71.5 35.7 47.2 22.7\nN-1 20006 23 90.6 46.6 96.2 78.6 96.3 89.4 87.4 52.1 84.8 45.0\nN-M 13096 16 92.4 44.2 95.5 71.1 96.2 80.5 91.9 58.8 88.9 54.2\nConceptNet - 2996 16 91.1 32.0 96.8 63.5 96.2 53.5 89.9 34.9 88.6 31.3\nSQuAD - 305 - 91.8 46.9 97.1 62.0 96.4 53.1 89.5 42.9 86.5 41.9\nTable 2: PLMs do not distinguish positive and negative sentences. Mean spearman rank correlation ( ρ) and mean\npercentage of overlap in ﬁrst ranked predictions (%) between the original and the negated queries for Transformer-\nXL large (Txl), ELMo original (Eb), ELMo 5.5B (E5B), BERT-base (Bb) and BERT-large (Bl).\nfor “X was not born in Y”, sometimes countries\nare predicted. This also seems natural: for the posi-\ntive sentence, cities are more informative, for the\nnegative, countries.\nBalanced corpus. Investigating this further, we\ntrain BERT-base from scratch on a synthetic cor-\npus. Hyperparameters are listed in the appendix.\nThe corpus contains as many positive sentences of\nform “xj is an” as negative sentences of form “xj\nis not an” where xj is drawn from a set of 200\nsubjects S and an from a set of 20 adjectives A.\nThe 20 adjectives form 10 pairs of antonyms (e.g.,\n“good”/”bad”).S is divided into 10 groups gm of\n20. Finally, there is an underlying KB that deﬁnes\nvalid adjectives for groups. For example, assume\nthat g1 has property am = “good”. Then for each\nxi ∈ g1, the sentences “xi is good” and “xi is not\nbad” are true. The training set is generated to con-\ntain all positive and negative sentences for 70% of\nthe subjects. It also contains either only the posi-\ntive sentences for the other 30% of subjects (in that\ncase the negative sentences are added to test) or\nvice versa. Cloze questions are generated in the for-\nmat “xj is [MASK]”/“xj is not [MASK]”. We test\nwhether (i) BERT memorizes positive and negative\nsentences seen during training, (ii) it generalizes to\nthe test set. As an example, a correct generalization\nwould be “xi is not bad” if “xi is good” was part of\nthe training set. The question is: does BERT learn,\nbased on the patterns of positive/negative sentences\nand within-group regularities, to distinguish facts\nfrom non-facts.\nTable 5 (“pretrained BERT”) shows that BERT\nmemorizes positive and negative sentences, but\npoorly generalizes to the test set for both positive\nand negative. The learning curves (see appendix)\nshow that this is not due to overﬁtting the training\ndata. While the training loss rises, the test preci-\nsion ﬂuctuates around a plateau. However, if we\nCorpus Relation Facts A B C D\nGoogle-RE\nbirth-place 386 11.7 44.7 99.5 98.4\nbirth-date 25 72.0 91.7 100.0 88.0\ndeath-place 88 14.8 47.1 98.9 98.9\nT-REx\n1-1 661 12.7 20.6 30.1 28.1\nN-1 7034 22.1 48.3 59.9 41.2\nN-M 2774 26.6 55.3 58.7 43.9\nConceptNet - 146 52.1 59.6 82.9 70.6\nSQuAD - 51 33.3 - 68.6 60.8\nTable 3: Absolute precision drop (from 100%, lower\nbetter) when mispriming BERT-large for the LAMA\nsubset that was answered correctly in its original form.\nWe insert objects that (A) are randomly chosen, (B)\nare randomly chosen from correct ﬁllers of different in-\nstances of the relation (not done for SQuAD as it is\nnot organized in relations), (C) were top-ranked ﬁllers\nfor the original cloze question but have at least a 30%\nlower prediction probability than the correct object. (D)\ninvestigates the effect of distance, manipulating (C)\nfurther by inserting a concatenation of 20 neutral sen-\ntences (e.g., “Good to know.”, see appendix) between\nmisprime and cloze question.\nﬁnetune BERT (“ﬁnetuned BERT”) on the task of\nclassifying sentences as true/false, its test accuracy\nis 100%. (Recall that false sentences simply cor-\nrespond to true sentence with a “not” inserted or\nremoved.) So BERT easily learns negation if su-\npervision is available, but fails without it. This\nexperiment demonstrates the difﬁculty of learning\nnegation through unsupervised pretraining. We\nsuggest that the inability of pretrained BERT to\ndistinguish true from false is a serious impediment\nto accurately handling factual knowledge.\nMisprimed LAMA. Table 3 shows the effect of\nmispriming on BERT-large for questions answered\ncorrectly in original LAMA; recall that Table 1\ngives examples of sentences constructed in modes\nA, B, C and D. In most cases, mispriming with a\nhighly ranked incorrect object causes a precision\ndrop of over 60% (C). Example predictions can be\nfound in Table 4 (lines marked “M”). This sensi-\n7814\ncloze question true top 3 words generated with log probs\nGoogle\nRE\nO Marcel Oopa died in the city of [MASK]. Paris Paris (-2.3), Lausanne (-3.3), Brussels (-3.3)\nN Marcel Oopa did not die in the city of [MASK]. Paris (-2.4), Helsinki (-3.5), Warsaw (-3.5)\nM Yokohama? Marcel Oopa died in the city of [MASK]. Yokohama (-1.0), Tokyo (-2.5), Paris (-3.0)\nO Anatoly Alexine was born in the city of [MASK]. Moscow Moscow (-1.2), Kiev (-1.6), Odessa (-2.5)\nN Anatoly Alexine was not born in the city of [MASK]. Moscow (-1.2), Kiev (-1.5), Novgorod (-2.5)\nM Kiev? Anatoly Alexine was born in the city of [MASK]. Kiev (-0.0), Moscow (-6.1), Vilnius (-7.0)\nTERx\nO Platonism is named after [MASK] . Plato Plato (-1.5), Aristotle (-3.5), Locke (-5.8)\nN Platonism is not named after [MASK]. Plato (-0.24), Aristotle (-2.5), Locke (-5.7)\nM Cicero? Platonism is named after [MASK]. Cicero (-2.3), Plato ( -3.5), Aristotle (-5.1)\nO Lexus is owned by [MASK] . Toyota Toyota (-1.4), Renault (-2.0), Nissan (-2.4)\nN Lexus is not owned by [MASK]. Ferrari (-1.0), Fiat (-1.4), BMW (-3.7)\nM Microsoft? Lexus is owned by [MASK] . Microsoft (-1.2), Google ( -2.1), Toyota (-2.6)\nConcept\nNet\nO Birds can [MASK]. ﬂy ﬂy (-0.5), sing (-2.3), talk (-2.8)\nN Birds cannot [MASK]. ﬂy (-0.3), sing ( -3.6), speak (-4.1)\nM Talk? Birds can [MASK]. talk (-0.2), ﬂy ( -2.5), speak (-3.9)\nO A beagle is a type of [MASK]. dog dog (-0.1), animal (-3.7), pigeon (-4.1)\nN A beagle is not a type of [MASK]. dog (-0.2), horse ( -3.8), animal (-4.1)\nM Pigeon? A beagle is a type of [MASK]. dog (-1.3), pigeon ( -1.4), bird (-2.2)\nSQuAD\nO Quran is a [MASK] text. religious religious (-1.0), sacred (-1.8), Muslim (-3.2)\nN Quran is not a [MASK] text. religious (-1.1), sacred ( -2.3), complete (-3.3)\nM Secular? Quran is a [MASK] text. religious (-1.5), banned ( -2.8), secular (-3.0)\nO Isaac’s chains are made out of [MASK]. silver silver (-1.9), gold (-2.1), iron (-2.2)\nN Isaac’s chains are not made out of [MASK]. iron (-1.2), metal ( -2.1), gold (-2.1)\nM Iron? Isaac’s chains are made out of [MASK]. iron (-0.4), steel ( -2.8), metal (-2.8)\nTable 4: BERT-large examples for (O) original , (N) negated and (M) misprimed (Table 3 C) LAMA.\ntrain test\npos neg pos neg\npretrained BERT 0.9 0.9 0.2 0.2\nﬁnetuned BERT 1.0 1.0 1.0 1.0\nTable 5: Accuracy of BERT on balanced corpus. Pre-\ntrained BERT does not model negation well, but ﬁne-\ntuned BERT classiﬁes sentences as true/false correctly.\ntivity to misprimes still exists when the distance\nbetween misprime and cloze question is increased:\nthe drop persists when 20 sentences are inserted\n(D). Striking are the results for Google-RE where\nthe model recalls almost no facts (C). Table 4 (lines\nmarked “M”) shows predicted ﬁllers for these mis-\nprimed sentences. BERT is less but still badly\naffected by misprimes that match selectional re-\nstrictions (B). The model is more robust against\npriming with random words (A): the precision drop\nis on average more than 35% lower than for (D).\nWe included the baseline (A) as a sanity check for\nthe precision drop measure. These baseline results\nshow that the presence of a misprime per se does\nnot confuse the model; a less distracting misprime\n(different type of entity or a completely implausible\nanswer) often results in a correct answer by BERT.\n4 Discussion\nWhereas Petroni et al. (2019)’s results suggest that\nPLMs are able to memorize facts, our results indi-\ncate that PLMs largely do not learn the meaning\nof negation. They mostly seem to predict ﬁllers\nbased on co-occurrence of subject (e.g., “Quran”)\nand ﬁller (“religious”) and to ignore negation.\nA key problem is that in the LAMA setup, not\nanswering (i.e., admitting ignorance) is not an op-\ntion. While the prediction probability generally is\nsomewhat lower in the negated compared to the\npositive answer, there is no threshold across cloze\nquestions that could be used to distinguish valid\npositive from invalid negative answers (cf. Table 4).\nWe suspect that a possible explanation for PLMs’\npoor performance is that negated sentences occur\nmuch less frequently in training corpora. Our syn-\nthetic corpus study (Table 5) shows that BERT is\nable to memorize negative facts that occur in the\ncorpus. However, the PLM objective encourages\nthe model to predict ﬁllers based on similar sen-\ntences in the training corpus – and if the most simi-\nlar statement to a negative sentence is positive, then\nthe ﬁller is generally incorrect. However, after ﬁne-\ntuning, BERT is able to classify truth/falseness cor-\nrectly, demonstrating that negation can be learned\nthrough supervised training.\nThe mispriming experiment shows that BERT\noften handles random misprimes correctly (Table 3\nA). There are also cases where BERT does the\nright thing for difﬁcult misprimes, e.g., it robustly\nattributes “religious” to Quran (Table 4). In general,\nhowever, BERT is highly sensitive to misleading\ncontext (Table 3 C) that would not change human\n7815\nbehavior in QA. It is especially striking that a single\nword sufﬁces to distract BERT. This may suggest\nthat it is not knowledge that is learned by BERT, but\nthat its performance is mainly based on similarity\nmatching between the current context on the one\nhand and sentences in its training corpus and/or\nrecent context on the other hand. Poerner et al.\n(2019) present a similar analysis.\nOur work is a new way of analyzing differences\nbetween PLMs and human-level natural language\nunderstanding. We should aspire to develop PLMs\nthat – like humans – can handle negation and are\nnot easily distracted by misprimes.\n5 Related Work\nPLMs are top performers for many tasks, includ-\ning QA (Kwiatkowski et al., 2019; Alberti et al.,\n2019). PLMs are usually ﬁnetuned (Liu et al., 2019;\nDevlin et al., 2019), but recent work has applied\nmodels without ﬁnetuning (Radford et al., 2019;\nPetroni et al., 2019). Bosselut et al. (2019) investi-\ngate PLMs’ common sense knowledge, but do not\nconsider negation explicitly or priming.\nA wide range of literature analyzes linguis-\ntic knowledge stored in pretrained embeddings\n(Jumelet and Hupkes, 2018; Gulordava et al., 2018;\nGiulianelli et al., 2018; McCoy et al., 2019; Das-\ngupta et al., 2018; Marvin and Linzen, 2018;\nWarstadt and Bowman, 2019; Kann et al., 2019).\nOur work analyzes factual knowledge. McCoy\net al. (2019) show that BERT ﬁnetuned to perform\nnatural language inference heavily relies on syntac-\ntic heuristics, also suggesting that it is not able to\nadequately acquire common sense.\nWarstadt et al. (2019) investigate BERT’s un-\nderstanding of how negative polarity items are\nlicensed. Our work, focusing on factual knowl-\nedge stored in negated sentences, is complementary\nsince grammaticality and factuality are mostly or-\nthogonal properties. Kim et al. (2019) investigate\nunderstanding of negation particles when PLMs\nare ﬁnetuned. In contrast, our focus is on the inter-\naction of negation and factual knowledge learned\nin pretraining. Ettinger (2019) deﬁnes and applies\npsycho-linguistic diagnostics for PLMs. Our use of\npriming is complementary. Their data consists of\ntwo sets of 72 and 16 sentences whereas we create\n42,867 negated sentences covering a wide range of\ntopics and relations.\nRibeiro et al. (2018) test for comprehension of\nminimally modiﬁed sentences in an adversarial\nsetup while trying to keep the overall semantics\nthe same. In contrast, we investigate large changes\nof meaning (negation) and context (mispriming).\nIn contrast to adversarial work (e.g., (Wallace et al.,\n2019)), we do not focus on adversarial examples\nfor a speciﬁc task, but on pretrained models’ ability\nto robustly store factual knowledge.\n6 Conclusion\nOur results suggest that pretrained language models\naddress open domain QA in datasets like LAMA by\nmechanisms that are more akin to relatively shallow\npattern matching than the recall of learned factual\nknowledge and inference.\nImplications for future work on pretrained\nlanguage models. (i) Both factual knowledge and\nlogic are discrete phenomena in the sense that sen-\ntences with similar representations in current pre-\ntrained language models differ sharply in factuality\nand truth value (e.g., “Newton was born in 1641”\nvs. “Newton was born in 1642”). Further archi-\ntectural innovations in deep learning seem neces-\nsary to deal with such discrete phenomena. (ii)\nWe found that PLMs have difﬁculty distinguishing\n“informed” best guesses (based on information ex-\ntracted from training corpora) from “random” best\nguesses (made in the absence of any evidence in\nthe training corpora). This implies that better con-\nﬁdence assessment of PLM predictions is needed.\n(iii) Our premise was that we should emulate hu-\nman language processing and that therefore tasks\nthat are easy for humans are good tests for NLP\nmodels. To the extent this is true, the two phenom-\nena we have investigated in this paper – that PLMs\nseem to ignore negation in many cases and that they\nare easily confused by simple distractors – seem\nto be good vehicles for encouraging the develop-\nment of PLMs whose performance on NLP tasks is\ncloser to humans.\nAcknowledgements. We thank the reviewers\nfor their constructive criticism. This work was\nfunded by the German Federal Ministry of Ed-\nucation and Research (BMBF) under Grant No.\n01IS18036A and by the European Research Coun-\ncil (Grant No. 740516). The authors of this work\ntake full responsibility for its content.\n7816\nReferences\nChris Alberti, Kenton Lee, and Michael Collins. 2019.\nA BERT baseline for the natural questions. ArXiv,\nabs/1901.08634.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nIshita Dasgupta, Demi Guo, Andreas Stuhlm ¨uller,\nSamuel J Gershman, and Noah D Goodman. 2018.\nEvaluating compositionality in sentence embed-\ndings. arXiv preprint arXiv:1802.04302.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nAllyson Ettinger. 2019. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 240–248,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJaap Jumelet and Dieuwke Hupkes. 2018. Do lan-\nguage models understand anything? on the ability\nof LSTMs to understand negative polarity items. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 222–231, Brussels, Belgium.\nAssociation for Computational Linguistics.\nKatharina Kann, Alex Warstadt, Adina Williams, and\nSamuel R. Bowman. 2019. Verb argument structure\nalternations in word and sentence embeddings. In\nProceedings of the Society for Computation in Lin-\nguistics (SCiL) 2019, pages 287–297.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019. Probing what dif-\nferent NLP tasks teach machines about function\nword comprehension. In Proceedings of the Eighth\nJoint Conference on Lexical and Computational Se-\nmantics (*SEM 2019), pages 235–249, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:453–466.\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.\n2016. Commonsense knowledge base completion.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1445–1455, Berlin, Germany.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\n7817\nfor Computational Linguistics, pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2019. BERT is not a knowledge base (yet): Fac-\ntual knowledge vs. name-based reasoning in unsu-\npervised qa. ArXiv, abs/1911.03681.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversar-\nial rules for debugging NLP models. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 856–865, Melbourne, Australia. Association\nfor Computational Linguistics.\nEndel Tulving and Daniel Schacter. 1990. Priming and\nhuman memory systems. Science, 247(4940):301–\n306.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAlex Warstadt and Samuel R. Bowman. 2019.\nGrammatical analysis of pretrained sentence en-\ncoders with acceptability judgments. ArXiv,\nabs/1901.03438.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nA Appendix\nA.1 Details on LAMA\nWe use source code provided by Petroni et al.\n(2019) 3. T-REx, parts of ConceptNet and SQuAD\nallow multiple true answers (N-M). To ensure sin-\ngle true objects for Google-RE, we reformulate the\ntemplates asking for location to speciﬁcally ask for\ncities (e.g., “born in [MASK]” to “born in the city\nof [MASK]”). We do not change any other tem-\nplates. T-REx still queries for ”died in [MASK]”.\nA.1.1 Details on negated LAMA\nFor ConceptNet we extract an easy-to-negate sub-\nset. The ﬁnal subset includes 2,996 of the 11,458\nsamples. We proceed as follows:\n1. We only negate sentences of maximal token\nsequence length 4 or if we ﬁnd a match with one\nof the following patterns: “is a type of ”, “is made\nof”, “is part of”, “are made of.”, “can be made of”,\n“are a type of ”, “are a part off”.\n2. The selected subset is automatically negated\nby a manually created verb negation dictionary.\nA.1.2 Details on misprimed LAMA\nTo investigate the effect of distance between the\nprime and the cloze question, we insert a concate-\nnation of up to 20 “neutral” sentences. The longest\nsequence has 89 byte pair encodings. The distance\nupon the full concatenation of all 20 sentences did\nnot lessen the effect of the prime much. The used\nsentences are: ”This is great.”, ”This is interesting.”,\n”Hold this thought.”, ”What a surprise.”, ”Good\nto know.”, ”Pretty awesome stuff.”, ”Nice seeing\nyou.”, ”Let’s meet again soon.”, ”This is nice.”,\n3github.com/facebookresearch/LAMA\n7818\nFigure 1: Training loss and test accuracy when pretrain-\ning BERT-base on a balanced corpus. The model is able\nto memorize positive and negative sentences seen dur-\ning training but is not able to generalize to an unseen\ntest set for both positive and negative sentences.\n”Have a nice time.”, ”That is okay.”, ”Long time no\nsee.”, ”What a day.”, ”Wonderful story.”, ”That’s\nnew to me.”, ”Very cool.”, ”Till next time.”, ”That’s\nenough.”, ”This is amazing.”, ”I will think about\nit.”\nbatch size 512\nlearning rate 6e-5\nnumber of epochs 2000\nmax. sequence length 13\nTable 6: Hyper-parameters for pretraining BERT-base\non a balanced corpus of negative and positive sen-\ntences.\nbatch size 32\nlearning rate 4e-5\nnumber of epochs 20\nmax. sequence length 7\nTable 7: Hyper-parameters for ﬁnetuning on the task of\nclassifying sentences as true/false.\nA.2 Details on the balanced corpus\nWe pretrain BERT-base from scratch on a corpus\non equally many negative and positive sentences.\nWe concatenate multiples of the same training data\ninto one training ﬁle to compensate for the little\namount of data. Hyper-parameters for pretraining\nare listed in Table 6. The full vocabulary is 349\ntokens long.\nFigure 1 shows that training loss and test ac-\ncuracy are uncorrelated. Test accuracy stagnates\naround 0.5 which is not more than random guessing\nas for each relation half of the adjectives hold.\nWe ﬁnetune on the task of classifying sentences\nas true/false. We concatenate multiples of the same\ntraining data into one training ﬁle to compensate\nfor the little amount of data. Hyperparameters for\nﬁnetuning are listed in Table 7.\nWe use source code provided by Wolf et al.\n(2019) 4.\n4github.com/huggingface/transformers",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.7521542906761169
    },
    {
      "name": "Computer science",
      "score": 0.4887545108795166
    },
    {
      "name": "Priming (agriculture)",
      "score": 0.4852980971336365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45661208033561707
    },
    {
      "name": "Natural language processing",
      "score": 0.451793909072876
    },
    {
      "name": "Psychology",
      "score": 0.43769416213035583
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4223385155200958
    },
    {
      "name": "Linguistics",
      "score": 0.3949165344238281
    },
    {
      "name": "Cognitive science",
      "score": 0.37608861923217773
    },
    {
      "name": "Biology",
      "score": 0.14423099160194397
    },
    {
      "name": "Philosophy",
      "score": 0.06177201867103577
    },
    {
      "name": "Germination",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ]
}