{
  "title": "Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change",
  "url": "https://openalex.org/W4285225407",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013040086",
      "name": "Mario Giulianelli",
      "affiliations": [
        "University of Helsinki",
        "University of Amsterdam",
        "University of Oslo"
      ]
    },
    {
      "id": "https://openalex.org/A5071409817",
      "name": "Andrey Kutuzov",
      "affiliations": [
        "University of Helsinki",
        "University of Amsterdam",
        "University of Oslo"
      ]
    },
    {
      "id": "https://openalex.org/A5047339175",
      "name": "Lidia Pivovarova",
      "affiliations": [
        "University of Helsinki",
        "University of Amsterdam",
        "University of Oslo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035582413",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W1973942085",
    "https://openalex.org/W4205950360",
    "https://openalex.org/W2130337399",
    "https://openalex.org/W2948599618",
    "https://openalex.org/W3205336478",
    "https://openalex.org/W3115289550",
    "https://openalex.org/W2931922640",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W3002481786",
    "https://openalex.org/W2490986620",
    "https://openalex.org/W3186276845",
    "https://openalex.org/W2296560646",
    "https://openalex.org/W2909693411",
    "https://openalex.org/W2165232124",
    "https://openalex.org/W2992121790",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W3116639614",
    "https://openalex.org/W3115844705",
    "https://openalex.org/W2024801944",
    "https://openalex.org/W2950863844",
    "https://openalex.org/W3103365499",
    "https://openalex.org/W2963308321",
    "https://openalex.org/W2519672670",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4221156342",
    "https://openalex.org/W3115228514",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W2741029840",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3167421168",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Morphological and syntactic changes in word usage — as captured, e.g., by grammatical profiles — have been shown to be good predictors of a word’s meaning change. In this work, we explore whether large pre-trained contextualised language models, a common tool for lexical semantic change detection, are sensitive to such morphosyntactic changes. To this end, we first compare the performance of grammatical profiles against that of a multilingual neural language model (XLM-R) on 10 datasets, covering 7 languages, and then combine the two approaches in ensembles to assess their complementarity. Our results show that ensembling grammatical profiles with XLM-R improves semantic change detection performance for most datasets and languages. This indicates that language models do not fully cover the fine-grained morphological and syntactic signals that are explicitly represented in grammatical profiles. An interesting exception are the test sets where the time spans under analysis are much longer than the time gap between them (for example, century-long spans with a one-year gap between them). Morphosyntactic change is slow so grammatical profiles do not detect in such cases. In contrast, language models, thanks to their access to lexical information, are able to detect fast topical changes.",
  "full_text": "Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change, pages 54 - 67\nMay 26-27, 2022 ©2022 Association for Computational Linguistics\nDo Not Fire the Linguist:\nGrammatical Profiles Help Language Models Detect Semantic Change\nMario Giulianelli∗\nILLC, University of Amsterdam\nm.giulianelli@uva.nl\nAndrey Kutuzov∗\nUniversity of Oslo\nandreku@ifi.uio.no\nLidia Pivovarova\nUniversity of Helsinki\nfirst.last@helsinki.fi\nAbstract\nMorphological and syntactic changes in word\nusage—as captured, e.g., by grammatical\nprofiles—have been shown to be good predic-\ntors of a word’s meaning change. In this work,\nwe explore whether large pre-trained contex-\ntualised language models, a common tool for\nlexical semantic change detection, are sensitive\nto such morphosyntactic changes. To this end,\nwe first compare the performance of grammati-\ncal profiles against that of a multilingual neural\nlanguage model (XLM-R) on 10 datasets, cov-\nering 7 languages, and then combine the two\napproaches in ensembles to assess their com-\nplementarity. Our results show that ensembling\ngrammatical profiles with XLM-R improves se-\nmantic change detection performance for most\ndatasets and languages. This indicates that\nlanguage models do not fully cover the fine-\ngrained morphological and syntactic signals\nthat are explicitly represented in grammatical\nprofiles.\nAn interesting exception are the test sets where\nthe time spans under analysis are much longer\nthan the time gap between them (for exam-\nple, century-long spans with a one-year gap be-\ntween them). Morphosyntactic change is slow\nso grammatical profiles do not detect in such\ncases. In contrast, language models, thanks to\ntheir access to lexical information, are able to\ndetect fast topical changes.\n1 Introduction\nHuman language is in continuous evolution. New\nword senses arise, and existing senses can change\nor disappear over time as a result of social and cul-\ntural dynamics or technological advances. NLP\npractitioners have become increasingly interested\nin this diachronic perspective of semantics. Some\nworks focus on constructing, testing and improv-\ning psycholinguistic and sociolinguistic theories of\nmeaning change (Xu and Kemp, 2015; Hamilton\n∗Equal contribution, the authors are listed alphabetically.\net al., 2016; Goel et al., 2016; Noble et al., 2021);\nothers are concerned with surveying how the mean-\ning of words has evolved historically (Garg et al.,\n2018; Kozlowski et al., 2019) or how it is currently\ntransforming in public discourse (Azarbonyad et al.,\n2017; Del Tredici et al., 2019). Recently, we also\nsee increased interest in more application-oriented\nwork, with efforts to develop adaptive learning sys-\ntems that can remain up-to-date with humans’ con-\ntinuously evolving language use (temporal gener-\nalization; Lazaridou et al., 2021).\nAn increasingly popular way to determine\nwhether and to what degree the meaning of words\nhas changed over time is to use ‘contextualised’\n(or ‘token-based’) word embeddings extracted\nfrom large pre-trained language models (Giulianelli\net al., 2020; Montariol et al., 2021) as they encode\nrich, context-sensitive semantic information. How-\never, it has also been shown recently that changes\nin the frequency distribution of morphological and\nsyntactic features of words, as captured by gram-\nmatical profiles, can also be employed for lexi-\ncal semantic change detection (Giulianelli et al.,\n2021), with competitive performance. These are,\nto some extent, two opposing approaches: while\nlanguage models (LMs) are largely based on word\nco-occurrence statistics, grammatical profiles are\nde-lexicalised and rely on explicit linguistic infor-\nmation.\nAlthough they are superficially unaware of mor-\nphology and syntax, LMs have been shown to cap-\nture approximations of grammatical information in\ntheir deep representations (Warstadt et al., 2020).\nYet are these sufficient to detect meaning shifts\nthat are accompanied by morphosyntactic changes\nin word usage? We hypothesise that this is not\nthe case, and to test this hypothesis, we combine\nLM-based methods and grammatical profiles into\nensemble models of lexical semantic change detec-\ntion.1 If adding grammatical profiles to LMs re-\n1Throughout the paper, we refer to the systems that com-\n54\nFigure 1: Performance of an XLM-R based method\n(PRT) and an ensemble method (PRT-MORPHSYNT)\non the ranking task; see Section 3 for method descrip-\ntions. The scores for the three Russian datasets are\naveraged as they exhibit similar trends.\nsults in a boost in performance, then this means that\nLMs do not capture morphosyntactic change as ac-\ncurately as explicit morphological tagging and syn-\ntactic parsing (or at the very least that it is difficult\nto extract this type of information from the mod-\nels). If we do not observe any boost, this suggests\nthat LMs already represent all the necessary gram-\nmatical information and explicit linguistic annota-\ntion is not required. We conduct our experiments\nwith 10 datasets, covering 7 languages. For com-\nparability, we use the same model for all the lan-\nguages. We choose XLM-R (Conneau et al., 2020),\na multilingual Transformer-based masked language\nmodel which has already been successfully applied\nto the semantic change detection task (Arefyev and\nZhikov, 2020; Arefyev et al., 2021). Although it\ncovers the full linguistic diversity of our data, we\nadditionally fine-tune XLM-R on monolingual di-\nachronic corpora.\nOur quantitative and qualitative evaluation of\nthe resulting ensembles on the graded and binary\nsemantic change detection tasks largely confirm\nour hypothesis. Ensembling XLM-R and gram-\nmatical profiles improves the results for 4 out of 6\nlanguages in graded change detection (as well as\nfor 1 of the 2 Norwegian datasets) and for 5 out of\n6 languages in binary change detection. Figure 1\nillustrates these improvements. The reasons why\nbine LMs with grammatical profiles as ‘ensembles’. These\nare not statistical methods of ensemble learning but systems\nthat combine the predictions of different models.\nensembles do not outperform the XLM-R baseline\non some datasets are linked to the size of the gaps\nbetween the historical time periods represented in\nthe diachronic corpora; we analyse and discuss\nthese reasons in Section 4.3. Overall, we show\nthat providing large language models with explicit\nmorphosyntactic information helps them quantify\nsemantic change.\n2 Tasks and Data\nThe goal of lexical semantic change detection is\nto determine whether and to what extent a word’s\nmeaning has changed over a certain period of time.\nThe performance of automatic systems that ad-\ndress this problem is typically assessed in two\ntasks (Schlechtweg et al., 2020). Task 1 is a bi-\nnary classification task: given a diachronic corpus\nand a set of target words, a system must deter-\nmine whether the words lost or gained any senses\nbetween two time periods. We refer to it as theclas-\nsification task and use accuracy as an evaluation\nmetric. Task 2 is a ranking task: a system must\nrank the target words according to the degree of\ntheir semantic change. We refer to it as the ranking\ntask and use the Spearman rank-correlation with\nthe gold rankings as an evaluation metric.\nWe rely on a collection of diachronic corpora\nand annotated target word lists covering seven lan-\nguages from three Indo-European language fami-\nlies. Target words are annotated with binary and\ngraded scores of semantic change, corresponding\nrespectively to Task 1 and 2 (Schlechtweg et al.,\n2018). English (EN), German (DE), Latin (LA),\nand Swedish (SW) data are available from the Sem-\nEval 2020 Unsupervised Lexical Semantic Change\nDetection shared task (Schlechtweg et al., 2020).\nFor Italian (IT), we use the data released for the\nEvaLita competition (Basile et al., 2020). For Nor-\nwegian (NO), we use the NorDiaChange dataset\nrecently released by Kutuzov et al. (2022), consist-\ning of two subsets with different target word lists\nand time spans. Finally, for Russian (RU), we draw\nfrom the RuShiftEval shared task (Kutuzov and\nPivovarova, 2021), consisting of three subsets with\ndifferent time spans and a shared target word list.\nTable 1 summarises the most important properties\nof the datasets and indicates what types of anno-\ntations are available for each language. Note that\nthe subset splitting in Norwegian and Russian is\nnot introduced by us, but is provided by the corre-\nsponding dataset creators.\n55\nEN DE IT LA NO-1 NO-2 RU-1 RU-2 RU-3 SW\nPeriod 1 1810-1860 1800-1899 1945-1970 -200-0 1929-1965 1980-1990 1700-1916 1918-1990 1700-1916 1790-1830\nPeriod 2 1960-2010 1946-1990 1990-2014 0-2000 1970-2013 2012-2019 1918-1990 1992-2016 1992-2016 1895-1903\nTokens(mln) 7+7 70+72 52+197 2+9 57+175 43+649 93+122 122+107 93+107 71+110\nTargets 37 48 18 40 80 80 99 99 99 32\nRanking ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓\nClassification ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓\nTable 1: Statistics for our collection of diachronic corpora and of the corresponding semantic change annotations.\n3 Methods\n3.1 Grammatical Profiles\nGrammatical profiling is a corpus linguistic tech-\nnique which allows to distinguish subtle semantic\ndifferences by measuring the distance between dis-\ntributions of grammatical parameters (Gries and\nDivjak, 2009; Janda and Lyashevskaya, 2011). It\nhas been shown recently that diachronic changes\nin grammatical profiles can serve as a strong indi-\ncation of semantic change (Giulianelli et al., 2021).\nFor our experiments we adopt this method in its\nbest performing configuration.\nFirst, the diachronic corpus of interest is tagged\nand parsed using UDPipe (Straka and Straková,\n2017). We then find all occurrences of a target\nword in the corpus and create a count vector for\neach detected morphological feature. For example,\na morphological profile for an English verb could\nlook as follows:\nTense : {Past 42, Pres 51}\nVerbForm : {Part 68, Fin 25, Inf 9}\nMood : {Ind 25}\nVoice : {Pass : 17}\nIn this way, count vectors are constructed for each\ntarget word in each time period of the corpus; these\nare a word’s grammatical profiles. The cosine dis-\ntance between count vectors is computed separately\nfor every morphological category, and the degree\nof semantic change between periods is measured\nas the maximum among the computed cosine dis-\ntances. We refer to this type of grammatical profile\nas MORPH.\nIn addition to morphological features, a sepa-\nrate vector of syntactic features is created, which\ncontains counts of dependency arc labels from a\ntarget word to its syntactic head. We refer to these\ngrammatical profiles as SYNT. Semantic change is\nmeasured as the cosine distance between two syn-\ntactic vectors. Morphological and syntactic profiles\ncan also be combined. We do this by concatenat-\ning syntactic features to the array of morphological\nfeatures, and then using the maximum cosine dis-\ntance as our third profile-based measure of seman-\ntic change, MORPHSYNT.\n3.2 Static Embeddings\nStatic embeddings (e.g., Mikolov et al., 2013) are\nknown to perform very well at detecting lexical\nsemantic change (Schlechtweg et al., 2020). There-\nfore, although they are not directly relevant to our\nresearch question, we include them in our experi-\nments as a point of comparison, following the com-\nmon approach proposed by Hamilton et al. (2016).\nFurther details can be found in Appendix B.\n3.3 Contextualised Embeddings\nMany have argued that static representations are not\ntheoretically appropriate as a model of word mean-\ning because they conflate all the usages of a word\ninto a single context-independent embedding, and\nthat contextualised representations should be used\ninstead (e.g., Schütze, 1998; Erk and Padó, 2008;\nPilehvar and Collier, 2016). This has motivated\nthe development of semantic change detection al-\ngorithms that rely on context-dependent representa-\ntions, where every usage of a word corresponds to\na unique token embedding (Giulianelli et al., 2020;\nMartinc et al., 2020a). Language models produce\nvery competitive results across languages (Kutu-\nzov and Giulianelli, 2020), and they lead to more\ninterpretable systems (Montariol et al., 2021).\nWe choose XLM-R (Conneau et al., 2020)\nas our pre-trained language model, since it was\nshown to perform well in semantic change shared\ntasks (Schlechtweg et al., 2020; Kutuzov and Pivo-\nvarova, 2021) and because, being multilingual, it\ncan be applied to all languages under analysis, mak-\ning evaluation more consistent. First, we finetune\nXLM-R on the monolingual diachronic corpora of\ninterest. Then, we deploy it to produce token em-\nbeddings for the target words in the diachronic cor-\npus (in both time periods, T1 and T2). Further de-\ntails on these two steps can be found in Appendix A.\nWe compute graded semantic change scores based\non the extracted XLM-R embeddings and we use\nthe scores to compile an ordered list of target words\n56\nfor the ranking task. Change scores are computed\nin four ways: 1) measuring the average pairwise\ncosine distance (APD) between embeddings col-\nlected in T1 and those in T2 (Giulianelli et al.,\n2020); 2) measuring the cosine distance between\nprototype embeddings ( PRT)—i.e., the average\ncontextualised word embeddings of T1 and T2 (Ku-\ntuzov and Giulianelli, 2020); 3) clustering the em-\nbeddings and then calculating the Jensen-Shannon\nDivergence (JSD) between the putative sense dis-\ntributions of T1 and T2 (Martinc et al., 2020b; Giu-\nlianelli et al., 2020); 4) by taking a simple average\n(APD-PRT) of the predictions made by PRT and\nAPD. The mathematical definitions of the metrics\nare given in Appendix A.4.\n3.4 Change Point Detection\nTo solve the classification task, we transform the\ncontinuous scores produced by our three metrics\ninto binary semantic change predictions. Follow-\ning Giulianelli et al. (2021), we rank target words\naccording to their continuous scores and classify\nthe top n words in the ranking as ‘changed’ (1) and\nthe rest of the list as ’stable’ (0). To determine the\nchange point n, we apply an offline change point\ndetection algorithm (Truong et al., 2020) with the\ndefault settings.2\n3.5 Ensembling\nTo find out whether grammatical profiles can im-\nprove the performance of embedding-based detec-\ntion methods, we test all possible combinations of\ngrammatical profile types and embedding-based\nmetrics. Grammatical profiles come in three vari-\nants: MORPH, SYNT, and MORPHSYNT. Our\nembedding-based measures include APD, PRT,\nAPD-PRT, and JSD. We compute the geometric\nmean √cgce between the change score cg obtained\nusing grammatical profiles and the score ce output\nby an embedding-based metric, and use the result-\ning value as the ensemble semantic change score\n(e.g., PRT-MORPHSYNT).\n4 Results\nWe assess the performance of all methods presented\nin Section 3 on both semantic change detection\ntasks using our multilingual collection of semantic\nchange datasets (see Section 2).\n2https://pypi.org/project/ruptures/\n4.1 Ranking Task\nFor all methods, the Spearman rank-correlation\nbetween predicted scores and human annotations\nvaries across languages and test sets; no method\nis a silver bullet for the ranking task (see Table 2).\nXLM-R obtains higher correlation scores in En-\nglish, Swedish, Norwegian-1, and Russian (1, 2,\nand 3); whereas grammatical profiles outperform it\nin German, Latin, Swedish, and Norwegian-2. To\nbetter understand the strengths of all methods, we\nnow first present the results of each of them individ-\nually; then we report the performance of ensembles,\nwhere each method is combined with every other\nto generate semantic change predictions.\nGrammatical Profiles Whether morphological\nfeatures, syntactic features, or a combination of\nboth are the most effective depends on the dataset;\nthis also varies across test sets of the same language,\nas can be seen in the first three rows of Table 2.\nThe performance of the different features diverges\nmostly for English and Norwegian-1, where SYNT\nis the best approach, as well as for Norwegian-2\nand Russian, where MORPH works best. Com-\nbining morphological and syntactic features helps\ncreating better rankings for German, Latin, and\nSwedish.\nContextualised Embeddings The correlation\nscores of average pairwise distance (APD) and pro-\ntotype distance (PRT) differ substantially for all\ndatasets, with the exception of Norwegian-1 (see\nrows 4-7 of Table 2). APD outperforms PRT on\nEnglish, Swedish, Norwegian, and Russian; PRT\nis better on German and Latin. Combining the\ntwo metrics in an ensemble (APD-PRT) marginally\nimproves correlation scores for Norwegian-1 and\nRussian-1. Clustering contextualised embeddings\n(JSD) yields unstable results across datasets; it is\nthe best contextualised method only for German.\nEnsembles Whenever grammatical profiles pro-\nduce better rankings than XLM-R, i.e., for German,\nLatin, Swedish, and Norwegian-2, combining the\npredictions of the two methods yields higher corre-\nlation scores than either method in isolation. The\nmost effective contextualised method in combina-\ntion with grammatical profiles is PRT, regardless of\nthe profile type. The PRT-MORPHSYNT combina-\ntion produces the overall best ranking for German\nand Latin, two languages with rich syntax and mor-\nphology. Which type of grammatical profile is\nthe most complementary to XLM-R varies across\n57\ndatasets and it mostly corresponds to the profile\ntype that obtains the best performance in isolation.\nEnsembles with JSD are outperformed by other\nmethods, so we do not report them in Tables 2 and\n3.\nStatic embeddings, despite their good perfor-\nmance in isolation, do not combine well with gram-\nmatical profiles: this type of ensemble improves\ncorrelation scores only for Latin (Table 4). As\na final note, ensembles of grammatical profiles\nand XLM-R achieve the new best performance\non the Latin ranking task of SemEval 2020 (PRT-\nMORPHSYNT), and establish a new SOTA for\nthe recently released Norwegian-2 dataset (APD-\nMORPH).\n4.2 Classification Task\nAlthough our binary predictions for the classifica-\ntion task are dependent on the rankings discussed\nin the previous section, the overall trends of classi-\nfication accuracy partly differ from the correlation\ntrends of the ranking task. The classification results\nare shown in Table 3. Compared to the ranking\ntask, ensemble methods more often produce a per-\nformance improvement with respect to grammati-\ncal profiles and contextualised embeddings used in\nisolation. They do so for English, German, Latin,\nand both Norwegian datasets. It is also more often\nthe case that the best standalone profile and con-\ntextualised approach yield the best ensemble when\ncombined. Another notable difference is that, when\nused in isolation, profiles outperform XLM-R; the\nopposite is true in the ranking task.\nFollowing the structure of Section 4.1, we first\npresent the results of each approach individually\nand then we report the performance of ensembles.\nGrammatical Profiles At least one of the three\nprofile types is substantially above chance perfor-\nmance for each language; as in the ranking task,\ndifferent profile types fit different datasets. Never-\ntheless, SYNT is the best profile type for 4 out of\n7 datasets: German, Swedish, Norwegian-1, and\nItalian. For the first two, it achieves the best over-\nall scores (see Table 3). Combining morphology\nand syntax helps only in the case of Latin, where\nprofiles obtain the best overall accuracy.\nContextualised Embeddings The accuracy of\nAPD and PRT is relatively similar across test sets,\nwith the exception of Italian, where APD has the\nbest overall accuracy and PRT is slightly below\nchance. Combining APD and PRT improves results\nfor English, German, and Norwegian. The accuracy\nof clustering-based JSD is either close to or below\nchance level for all languages.\nEnsembles Ensembles of grammatical profiles\nand contextualised embeddings are the best per-\nforming method for Norwegian. For German and\nLatin they are on par with pure profiles, and for\nEnglish on par with pure XLM-R. The comple-\nmentarity of different profile types and contextu-\nalised metrics varies across datasets yet it is over-\nall stronger than that between profiles and static\nembeddings (combining the latter two improves\nperformance only for Latin and for Norwegian-2,\nsee Appendix B). A more fine-grained analysis of\nthe classification results of the ensembles reveals\nthat 1) ensemble predictions are virtually always\ncorrect when the two standalone predictions also\nare, 2) ensembling tends to have positive effects\non precision with respect to both standalone meth-\nods, and 3) it tends to improve the precision of\ncontextualised methods.\n4.3 Why ensembles fail\nTables 2 and 3 show that grammatical profiles are\nconsistently worse than XLM-R on all Russian\ndatasets, Norwegian-1 and English. This naturally\nextends to their ensembles, so for all these datasets,\ncontextualised embeddings in isolation are the best\napproach. The explanation may seem simple for\nEnglish: its poor morphology does not provide\nenough signal for semantic change detection. Yet\nthis does not hold for Russian (a synthetic language\nwith rich morphology) and, arguably, for Nor-\nwegian. Moreover, ensembles with morphology-\nbased grammatical profiles outperform pure XLM-\nR on Norwegian-2, but not on Norwegian-1. Thus,\nthe explanation is likely not language-specific.\nWe believe that the different nature of the di-\nachronic corpora can be a better explaining factor.\nSemEval-2020 datasets feature time periods sepa-\nrated by at least several decades, and the same is\ntrue for Norwegian-2 (more than 20 years gap). In\ncontrast, the gaps are much shorter for Norwegian-\n1 (5 years gap), Russian-1 and Russian-2 (2 years\ngap). We observe that when two time periods with\na very short gap between them are compared, the\ndistributions of morphosyntactic features largely\noverlap, negatively affecting the performance of\ngrammatical profiles. In these cases, LM-based\nmethods can still detect semantic change as they\nhave access to lexical information: changes at the\n58\nMethod EN DE LA SW NO-1 NO-2 RU-1 RU-2 RU-3 A VG\nPROFILES\nMORPH 0.218 0.120 0.519 0.303 0.106 0.409 0.028 0.241 0.293 0.248\nSYNT 0.331 0.146 0.265 0.184 0.179 0.006 0.056 0.111 0.279 0.173\nMORPHSYNT 0.320 0.298 0.525 0.334 0.064 0.265 0.000 0.149 0.242 0.244\nCONTEXTUALISED (XLM-R)\nAPD 0.514 0.073 0.162 0.310 0.389 0.387 0.372 0.480 0.457 0.349\nPRT 0.320 0.210 0.394 0.212 0.378 0.270 0.294 0.313 0.313 0.300\nAPD-PRT 0.457 0.202 0.370 0.220 0.394 0.325 0.376 0.374 0.384 0.345\nClustering/JSD 0.127 0.287 0.318 -0.108 0.160 -0.137 0.247 0.267 0.362 0.169\nENSEMBLES\nAPD-MORPH 0.262 0.140 0.506 0.350 0.151 0.503 0.062 0.288 0.340 0.289\nAPD-SYNT 0.384 0.159 0.264 0.255 0.262 0.119 0.093 0.181 0.354 0.230\nAPD-MORPHSYNT 0.390 0.290 0.513 0.397 0.180 0.364 0.036 0.216 0.299 0.298\nPRT-MORPH 0.278 0.204 0.528 0.305 0.236 0.478 0.112 0.309 0.336 0.309\nPRT-SYNT 0.448 0.213 0.401 0.280 0.351 0.146 0.186 0.246 0.351 0.291\nPRT-MORPHSYNT 0.451 0.354 0.572 0.356 0.273 0.360 0.117 0.269 0.326 0.342\nAPD-PRT-MORPH 0.277 0.188 0.518 0.338 0.189 0.497 0.092 0.310 0.340 0.305\nAPD-PRT-SYNT 0.405 0.189 0.376 0.295 0.330 0.121 0.147 0.235 0.367 0.274\nAPD-PRT-MORPHSYNT0.418 0.337 0.554 0.377 0.236 0.359 0.092 0.255 0.328 0.328\nTable 2: Spearman rank-correlation scores in the ranking task (‘Task 2’). Bold indicates the best method overall (for\neach language); italic indicates the best results for a group of methods.\nMethod EN DE LA SW NO-1 NO-2 IT A VG\nPROFILES\nMORPH 0.622 0.479 0.625 0.581 0.486 0.703 0.500 0.571\nSYNT 0.514 0.625 0.514 0.677 0.622 0.514 0.611 0.582\nMORPHSYNT 0.541 0.521 0.675 0.581 0.486 0.432 0.444 0.526\nCONTEXTUALISED (XLM-R)\nAPD 0.568 0.500 0.500 0.613 0.486 0.595 0.667 0.561\nPRT 0.595 0.500 0.550 0.548 0.541 0.541 0.444 0.531\nAPD-PRT 0.676 0.542 0.550 0.613 0.568 0.459 0.500 0.558\nClustering/JSD 0.459 0.521 0.500 0.516 0.541 0.486 0.389 0.487\nENSEMBLES\nAPD-MORPH 0.622 0.500 0.575 0.613 0.541 0.730 0.500 0.583\nAPD-SYNT 0.568 0.479 0.550 0.581 0.622 0.622 0.611 0.576\nAPD-MORPHSYNT 0.622 0.625 0.600 0.613 0.514 0.703 0.611 0.613\nPRT-MORPH 0.676 0.458 0.525 0.581 0.541 0.486 0.500 0.538\nPRT-SYNT 0.541 0.521 0.575 0.613 0.703 0.568 0.500 0.574\nPRT-MORPHSYNT 0.541 0.479 0.525 0.581 0.676 0.486 0.444 0.533\nAPD-PRT-MORPH 0.649 0.458 0.650 0.581 0.541 0.676 0.611 0.595\nAPD-PRT-SYNT 0.514 0.542 0.550 0.548 0.676 0.595 0.500 0.561\nPRT-MORPHSYNT 0.541 0.479 0.525 0.581 0.676 0.486 0.444 0.533\nTable 3: Binary accuracy scores in the classification task (‘Task 1’). Bold indicates the best method overall (for\neach language); italic indicates the best results for a group of methods.\n59\nreferential and topical level can happen much faster\n(consider, e.g., the words ‘computer’ or ‘mouse’ in\nEnglish). On the other hand, when the gap between\ntime periods is more substantial, changes in mor-\nphological and syntactic behavior of words also\nemerge. In these cases grammatical profiles help\ndetect semantic shifts which LMs overlook. It is\npossible that adding the length of the time gap as\na feature in our ensemble systems can make them\nless sensitive to the nature of the datasets .\nExceptions to this pattern are Latin (no gap be-\ntween the time periods, but great performance of\ngrammatical profiles) and Russian-3 (80 years gap,\nbut profiles still lag behind XLM-R). For Latin, its\nextremely rich morphology can compensate for the\nsmall gap between time periods. Moreover, the\nsecond time period spans two millennia, making\nthe short gap less problematic. Rich morphology\ndoes not help surpass XLM-R for Russian-3, but\nprofiles do work much better for this dataset than\nfor Russian-1 and Russian-2, where the gaps are\nonly two years long.\n5 Analysis\nIn this section, we analyse the predictions of all\nmethods beyond task performance. We quantita-\ntively evaluate their complementarity (Section 5.1),\nand investigate whether and how predictions made\nwith grammatical profiles improve the performance\nof embedding-based metrics (Section 5.2).\n5.1 Correlations between methods\nTo investigate whether various methods use differ-\nent types of linguistic information, we compute\nSpearman rank-correlations between the predic-\ntions of standalone methods. The correlations, aver-\naged over all datasets, are presented in Figure 2 (we\nshow averaged correlations since they are highly\nconsistent across corpora). We include the corre-\nlations of static embeddings as well (SGNS-raw\nand SGNS-lemma). More details about their im-\nplementations and performance can be found in\nAppendix B.\nThe two methods with the highest correlation are\nSGNS-raw and SGNS-lemma. This is expected, as\nthe two methods differ only in the lemmatisation of\ntarget words. Profile-based methods (MORPH and\nSYNT) do not correlate with each other. Slight sig-\nnificant correlations are only observed for Russian-\n2 (0.32) and Russian-3 (0.45). Interestingly, for\nRussian, SYNT significantly correlates with static\nFigure 2: Averaged Spearman correlations between\nmodel predictions.\nembeddings: the correlation with SGNS-raw is\n0.48 for Russian-1, 0.52 for Russian-2 and 0.49\nfor Russian-3. Significant correlations between\nMORPH and static embeddings are observed for\nLatin (0.46) and Russian-3 (0.38).\nContextualised methods correlate weakly with\ngrammatical profiles. Although we once again\nobserve exceptional behaviour for the Russian\ndatasets, the correlation between profiles and con-\ntextualized embeddings is on average weaker than\nbetween profiles and static embeddings, which\nmight explain why combining contextualized em-\nbeddings with profiles yields notable performance\nimprovements.\n5.2 Qualitative analysis\nIn this section, we inspect the error patterns of our\nmethods to find out when grammatical profiles help\ncorrect the predictions of embedding-based metrics.\nWe frame this analysis in terms of false positives\nand false negatives. The definition of false positives\nand negatives is straightforward in the classification\ntask. For the ranking task, we look at the signed\ndistance between gold and predicted rankings of\neach word, considering a word as a false positive\nwhen the positive distance is in the highest 20% bin\nof the distance distribution (i.e., when the predicted\nrank is much higher than the true rank), and as\na false negative if the negative distance is in the\nlowest 20% bin (i.e., when the predicted rank is\nmuch lower than the true rank). For each language,\nwe focus on the best grammatical profile, the best\ncontextualised method, and the best ensemble of\nthese two.\nIn the English ranking task, we observe that four\n60\nof APD’s five false positives are corrected by the en-\nsemble: for example, the ranking of ‘tree’ improves\nby 20 positions, that of ‘ part’ by 19, and that of\n‘bag’ by 17. As a result, ‘tree’ and ‘bag’ are only 1\nposition away from their respective gold ranks. For\nboth words, the distribution of morphological fea-\ntures hardly vary between time period (e.g., 43.73%\nof the usages of ‘tree’ are singular, 56.27% plural\nin the first time period; and in the second time pe-\nriod the percentages become 43.67% and 56.33%).\nSyntactic features vary only slightly; the most dras-\ntic change among these three words is the increase\nof direct object usages of ‘ bag’ from 33.16% to\n41.40%, with all the other features remaining rela-\ntively stable—overall, a negligible change. Among\nAPD’s five false negatives, four are corrected in the\nbest ensemble (PRT-MORPHSYNT): the strongest\nranking improvements concern ‘graft’ and ‘plane’,\nwhose rankings improve respectively by 18 and 15\npositions. The syntactic profiles of these words\nvary substantially across time periods, with mul-\ntiple syntactic categories increasing or decreasing\ntheir frequency of usage (e.g., usages of ‘plane’ in\nsubject and object position increase from 12.85%\nto 24.13% and from 13.25% to 19.67% respec-\ntively; while usages as a noun modifier decrease\nfrom 35.34% to 20.36%). The two targets that do\nnot benefit from the ensemble are ‘gas’ and ‘risk’:\nboth are false negatives for the best grammatical\nprofile (SYNT) and they remain for the ensemble.\nIn the Norwegian ranking task, the best ensemble\n(APD-MORPH) helps pure APD mostly by fixing\nextreme false positives and false negatives. As an\nexample of a fixed false positive, APD ranked ‘test’\n(‘TEST ’) very high, although in fact it did not expe-\nrience any semantic change at all (change score of\n0). APD-MORPH decreased the change score as-\nsigned to ‘test’ from 0.216 down to 0.013, returning\nit to its proper place at the bottom of the ranking.\nOn the other hand, ‘ stryk’ changed its dominant\nmeaning sharply in the 21st century from ‘RIVER\nRAPIDS ’ to ‘FAILURE ’, but APD failed to capture it.\nAPD-MORPH fixed this false negative by moving\n‘stryk’ significantly upwards in the ranking, only 8\nposition away from its gold rank.\nIn the Latin predicted rankings, it is somewhat\nlikely for a word to be a false positive — e.g., ‘itero’\n(‘TO REPEAT ’), ‘jus’ (a ‘RIGHT ’, the ‘LAW’) ‘an-\ncilla’ (‘HANDMAID ’) — or a false negative — ‘vir-\ntus’ (‘STRENGTH ’; ‘COURAGE ’; ‘MANLINESS ’),\n‘humanitas’ (‘ HUMAN NATURE ’; ‘ KINDNESS ’;\n‘CIVISILATION ’), ‘pontifex’ (‘BISHOP ’; but also,\nthe ‘ POPE ’) — for both contextualised embed-\ndings (PRT) and profiles (MORPHSYNT). As\nin the case of English, the ranking of these\nwords does not improve with ensembling (PRT-\nMORPHSYNT). Overall, 7 out of 10 false nega-\ntives and 2 out of 7 false positives are corrected by\nthe ensemble.\nFor German, too, ensembling (PRT-\nMORPHSYNT) is most effective for false\nnegatives, 5 out of 8 are corrected. The words\nwith the greatest improvements are ‘ abdecken’\n(‘TO UNCOVER ’; but also, in financial jargon,\n‘TO COVER ’, as in Risiko abdecken, to cover a\nrisk), gaining 16 positions, and ‘ Eintagsfliege’\n(‘MAYFLY ’, the insect; but also, metaphorically,\n‘FLEETING STAR ’) gaining 17 positions and\nthereby obtaining the exact gold rank. Never-\ntheless, out of 8 false positives, 4 are corrected;\nwith, e.g., ‘ aufrechterhalten’ (‘ TO SUSTAIN ’)\nlosing 31 rankings and ‘ Festspiel’ (‘FESTIVAL ’)\nlosing 18. As we observed for English and Latin,\nsome words are simply difficult to rank for both\nmethods (here, JSD and PRT-MORPHSYNT):\nfor example, the degree of semantic change\nof ‘ Truppenteil’ (‘ TROOP UNIT ’) and ‘ Lyzeum’\n(‘LYCEUM ’) is overestimated whereas the change\nof ‘packen’ (‘TO PACK ’; to seize) and ‘vorliegen’\n(‘TO BE AVAILABLE ’; ‘ TO BE EXISTENT ’); is\nunderestimated.\nFor Italian, we analyse the classification task.\nAmong APD’s 4 false positives, 2 are cor-\nrected in the ensemble’s ranking (APD-SYNT),\n‘processare’ (‘ PROCESS ’; ‘ TAKE TO TRIAL ’)\nand ‘ unico’ (‘ UNIQUE ’); two nouns, ‘ brama’\n(‘YEARNING ’) and ‘ cappuccio’ (‘ HOOD ’), re-\nmain misclassified. APD’s false negatives are\n‘pilotato’ (‘ DRIVEN ’; but also, metaphorically,\n‘PREMEDITATED ’) and ‘rampante’ (‘UNBRIDLED ’;\nmetaphorically, ‘EXUBERANT ’); ‘pilotato’ is cor-\nrectly classified by the ensemble while ‘rampante’\nremains undetected by all methods. Overall, the\ncontribution of SYNT is not always helpful: it also\nleads to one changing word being labelled as stable,\nand three stable words being classified as changing.\n6 Conclusion\nWe showed that providing large pre-trained lan-\nguage models with explicit morphosyntactic infor-\nmation can in many cases help detect and quantify\nlexical semantic change. Such ‘ensemble’ predic-\n61\ntions are produced in a very straightforward way—\ni.e., by computing the geometric mean between\nsemantic change scores predicted by grammatical\nprofiles and by language models (via their contextu-\nalized embeddings). In the majority of the datasets\nunder analysis (treating the three Russian datasets\nas one), the ensemble predictions outperformed\nsingle grammatical profiles or contextualised em-\nbeddings in the task of ranking words by the degree\nof their semantic change. The datasets where this\nwas not true are characterized by specific proper-\nties: either languages with poor morphology or\nlong time spans separated by narrow gaps.\nWe believe this means that although\nTransformer-based language models (like\nXLM-R, which we used here) are able to track\nmorphological and syntactic properties to some\nextent (Warstadt et al., 2020), their encoding of\ngrammatical features is only approximate and can\ntherefore be improved by explicit linguistic pre-\nprocessing (morphological tagging and syntactic\nparsing). At any rate, we showed that this is true\nfor the semantic change detection task, when a\nmodel has to take into account diachronic changes\nin morphosyntactic properties of words. The signal\nprovided by these changes is complementary to\nthe changes in typical lexical contexts more easily\ncaptured by distributional language models. Thus,\nit is still too early to fire the linguist, even if the\n‘linguist’ is in fact an automated tagger.\nAs has already been said, an important limitation\nof grammatical profiles is their low performance\nwhen measuring semantic change across long time\nperiods separated by very narrow gaps. This makes\nsense from a linguistic point of view: grammar\nchanges slowly and gradually, sharp bursts are\nrare. In contrast, lexical contexts can change very\nquickly: for example, due to social and political\nevents or technical progress, which is why language\nmodels excel with these datasets. The main practi-\ncal take-away is therefore that diachronic grammat-\nical profiles should be used in combination with\nlanguage models especially when the gap between\nthe compared time periods is large enough for sig-\nnificant grammatical changes to occur.\nIn the future, we plan to experiment with more\nsophisticated ensembling methods that go beyond\nsimple averaging (including the usage of the in-\nformation about gaps between time spans), and to\nperform a deeper analysis of ensemble predictions,\nespecially in relation to distinct word senses. Fi-\nnally, we also plan to evaluate ensembles formed\nwith monolingual language models, instead of the\nmultilingual XLM-R, as they have the potential\nto better capture the idiosyncrasies of specific lan-\nguages.\nAcknowledgements\nThis work has been partly supported by the Eu-\nropean Union’s Horizon 2020 research and inno-\nvation programme under grant 819455 (DREAM)\nand by Academy of Finland under grant numbers\n333716 and 333717 (RiCEP). Our experiments\nwere run on resources provided by UNINETT\nSigma2 - the National Infrastructure for High Per-\nformance Computing and Data Storage in Norway.\nReferences\nNikolay Arefyev, Daniil Homskiy, Maksim Fedoseev,\nAdis Davletov, Vitaly Protasov, and Alexander\nPanchenko. 2021. Deepmistake: Which senses are\nhard to distinguish for a wordincontext model. Com-\nputational linguistics and intellectual technologies:\nPapers from the annual conference Dialogue.\nNikolay Arefyev and Vasily Zhikov. 2020. BOS at\nSemEval-2020 task 1: Word sense induction via lex-\nical substitution for lexical semantic change detec-\ntion. In Proceedings of the Fourteenth Workshop\non Semantic Evaluation, pages 171–179, Barcelona\n(online). International Committee for Computational\nLinguistics.\nHosein Azarbonyad, Mostafa Dehghani, Kaspar Beelen,\nAlexandra Arkut, Maarten Marx, and Jaap Kamps.\n2017. Words are malleable: Computing semantic\nshifts in political and media discourse. In Proceed-\nings of the 2017 ACM on Conference on Information\nand Knowledge Management, pages 1509–1518.\nPierpaolo Basile, Annalina Caputo, Tommaso Caselli,\nPierluigi Cassotti, and Rossella Varvara. 2020.\nDIACR-Ita @ EV ALITA2020: Overview of\nthe EV ALITA2020 Diachronic Lexical Semantics\n(DIACR-Ita) Task. Proceedings of the Seventh\nEvaluation Campaign of Natural Language Pro-\ncessing and Speech Tools for Italian. Final Work-\nshop (EVALITA 2020) CEUR Workshop Proceedings\n(CEUR-WS.org).\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\n62\nMarco Del Tredici, Raquel Fernández, and Gemma\nBoleda. 2019. Short-term meaning shift: A distri-\nbutional exploration. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2069–2075, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nKatrin Erk and Sebastian Padó. 2008. A structured vec-\ntor space model for word meaning in context. In\nProceedings of the 2008 Conference on Empirical\nMethods in Natural Language Processing, pages 897–\n906, Honolulu, Hawaii. Association for Computa-\ntional Linguistics.\nBrendan Frey and Delbert Dueck. 2007. Clustering\nby passing messages between data points. Science,\n315(5814):972–976.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify 100\nyears of gender and ethnic stereotypes. Proceedings\nof the National Academy of Sciences, 115(16):E3635–\nE3644.\nMario Giulianelli, Marco Del Tredici, and Raquel Fer-\nnández. 2020. Analysing lexical semantic change\nwith contextualised word representations. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3960–\n3973, Online. Association for Computational Lin-\nguistics.\nMario Giulianelli, Andrey Kutuzov, and Lidia Pivo-\nvarova. 2021. Grammatical profiling for semantic\nchange detection. In Proceedings of the 25th Confer-\nence on Computational Natural Language Learning,\npages 423–434, Online. Association for Computa-\ntional Linguistics.\nRahul Goel, Sandeep Soni, Naman Goyal, John Paparri-\nzos, Hanna Wallach, Fernando Diaz, and Jacob Eisen-\nstein. 2016. The social dynamics of language change\nin online networks. In International Conference on\nSocial Informatics, pages 41–57. Springer.\nStefan Th Gries and Dagmar Divjak. 2009. Behavioral\nprofiles: a corpus-based approach to cognitive seman-\ntic analysis. New directions in cognitive linguistics,\n57:75.\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1489–1501, Berlin, Germany. Association for Com-\nputational Linguistics.\nLaura A Janda and Olga Lyashevskaya. 2011. Gram-\nmatical profiles and the interaction of the lexicon\nwith aspect, tense, and mood in Russian. Cognitive\nlinguistics, 22(4):719–763.\nAustin C Kozlowski, Matt Taddy, and James A Evans.\n2019. The geometry of culture: Analyzing the mean-\nings of class through word embeddings. American\nSociological Review, 84(5):905–949.\nAndrey Kutuzov and Mario Giulianelli. 2020. UiO-\nUvA at SemEval-2020 task 1: Contextualised em-\nbeddings for lexical semantic change detection. In\nProceedings of the Fourteenth Workshop on Semantic\nEvaluation, pages 126–134, Barcelona (online). Inter-\nnational Committee for Computational Linguistics.\nAndrey Kutuzov and Lidia Pivovarova. 2021. RuShiftE-\nval: a shared task on semantic shift detection for\nRussian. Computational linguistics and intellectual\ntechnologies: Papers from the annual conference Di-\nalogue.\nAndrey Kutuzov, Samia Touileb, Petter Mæhlum,\nTita Ranveig Enstad, and Alexandra Wittemann.\n2022. NorDiaChange: Diachronic semantic change\ndataset for Norwegian. In Proceedings of the 13th\nLanguage Resources and Evaluation Conference\n(LREC). European Language Resources Association.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\nmodels. Advances in Neural Information Processing\nSystems, 34.\nJianhua Lin. 1991. Divergence measures based on the\nShannon entropy. IEEE Transactions on Information\ntheory, 37(1):145–151.\nMatej Martinc, Petra Kralj Novak, and Senja Pollak.\n2020a. Leveraging contextual embeddings for de-\ntecting diachronic semantic shift. In Proceedings of\nthe 12th Language Resources and Evaluation Confer-\nence, pages 4811–4819, Marseille, France. European\nLanguage Resources Association.\nMatej Martinc, Syrielle Montariol, Elaine Zosa, and\nLidia Pivovarova. 2020b. Capturing evolution in\nword usage: Just add more clusters? In Companion\nProceedings of the Web Conference 2020, pages 343–\n349.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in Neural Information Processing Systems\n26.\nSyrielle Montariol, Matej Martinc, and Lidia Pivovarova.\n2021. Scalable and interpretable semantic change\ndetection. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4642–4652, Online. Association\nfor Computational Linguistics.\n63\nBill Noble, Asad Sayeed, Raquel Fernández, and Staffan\nLarsson. 2021. Semantic shift in social networks. In\nProceedings of *SEM 2021: The Tenth Joint Con-\nference on Lexical and Computational Semantics ,\npages 26–37, Online. Association for Computational\nLinguistics.\nMohammad Taher Pilehvar and Nigel Collier. 2016. De-\nconflated semantic representations. In Proceedings\nof the 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1680–1690, Austin,\nTexas. Association for Computational Linguistics.\nDominik Schlechtweg, Anna Hätty, Marco Del Tredici,\nand Sabine Schulte im Walde. 2019. A wind of\nchange: Detecting and evaluating lexical semantic\nchange across times and domains. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 732–746, Florence,\nItaly. Association for Computational Linguistics.\nDominik Schlechtweg, Barbara McGillivray, Simon\nHengchen, Haim Dubossarsky, and Nina Tahmasebi.\n2020. SemEval-2020 task 1: Unsupervised lexical\nsemantic change detection. In Proceedings of the\nFourteenth Workshop on Semantic Evaluation, pages\n1–23, Barcelona (online). International Committee\nfor Computational Linguistics.\nDominik Schlechtweg, Sabine Schulte im Walde, and\nStefanie Eckmann. 2018. Diachronic usage related-\nness (DURel): A framework for the annotation of\nlexical semantic change. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 169–174, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nHinrich Schütze. 1998. Automatic word sense discrimi-\nnation. Computational Linguistics, 24(1):97–123.\nMilan Straka and Jana Straková. 2017. Tokenizing,\nPOS tagging, lemmatizing and parsing UD 2.0 with\nUDPipe. In Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 88–99, Vancouver, Canada.\nAssociation for Computational Linguistics.\nCharles Truong, Laurent Oudre, and Nicolas Vayatis.\n2020. Selective review of offline change point detec-\ntion methods. Signal Processing, 167:107299.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nYang Xu and Charles Kemp. 2015. A computational\nevaluation of two laws of semantic change. In\nCogSci.\n64\nAppendix\nA Contextualised embeddings\nGiven two time periods t1, t2 , two corpora C1, C2,\nand a set of target words, we use a neural language\nmodel to obtain token embeddings of each occur-\nrence of the target words in C1 and C2 and use\nthem to compute a continuous change score. This\nscore indicates the degree of semantic change un-\ndergone by a word between t1 and t2. As a lan-\nguage model, we choose XLM-R (Conneau et al.,\n2020), pre-trained multilingual transformer, in the\nHuggingface implementation (Wolf et al., 2020).\nA.1 Target Lemmas and Word Forms\nThe lists of target words that we rely on contain\nannotations for lemmas. However, only extracting\nembeddings for exact matches of the lemmas would\nresult in discarding a large number of word usages,\nthose where the target lemma takes another form\n(e.g., as a result of grammatical inflection). To take\nall of a lemma’s possible word forms into account,\nwe parse the corpora using UDPipe (Straka and\nStraková, 2017) and collect a set of word forms for\neach target word from the UDPipe output. Further-\nmore, because some word forms are not present\nin the vocabulary of XLM-R, we add them to the\nvocabulary before fine-tuning.3\nA.2 Finetuning the Language Model\nAs a first step, to adapt the model to the charac-\nteristics of the diachronic corpora, we finetune it,\nseparately, on each language-specific corpus. We\nlimit the maximum sequence length of the trans-\nformer to 256 and train the model with a batch size\nof 16 for an amount of epochs dependent on the\ncorpus size: 5 epochs for English and Latin, 3 for\nGerman and Swedish, 2 for Russian, Italian and\nNorwegian.\nA.3 Extracting contextualised embeddings\nGiven a target word w and its sentential context\ns = (v1, ..., vi, ..., vm) with w = vi, we extract the\nactivations of the language model’s hidden layers\nfor sentence position i. We then average over the\nlayers (12 for XLM-R) and obtain a single vectorial\n3Even after adding the word forms to the vocabulary, the\nHuggingface tokenizer still fails to recognise about a dozen\nof the target word forms and splits them into sub-tokens. For\nthese exceptional cases, we extract the average contextualised\nembedding over the sub-tokens.\nrepresentation (for XLM-R, the vector dimension-\nality is 768). In our experiments, the maximum\ncontext length m is set to 256 and sentences are\nprocessed in batches of size 32. The Nw contex-\ntualised embeddings collected for w can be repre-\nsented as the usage matrix Uw = (w1, . . . ,wNw ).\nThe time-specific usage matrices U1\nw, U2\nw for time\nperiods t1 and t2 are used as input to a metric of\nsemantic change.\nA.4 Metrics of Semantic Change\nAs explained in Section 3.3, semantic change\nscores are computed using three metrics: 1) av-\nerage pairwise distance or APD (Giulianelli et al.,\n2020), 2) prototype distance or PRT (Kutuzov and\nGiulianelli, 2020), and 3) Jensen-Shannon Diver-\ngence between embedding cluster distributions or\nJSD (Martinc et al., 2020b; Giulianelli et al., 2020):\nAPD Given two usage matrices Ut1\nw , Ut2\nw , the de-\ngree of change of w is calculated as the average\ncosine distance between any two embeddings from\ndifferent time periods:\nAPD(Ut1\nw,Ut2\nw\n)= 1\nNt1w ·Nt2w\n∑\nxi∈Ut1w , xj∈Ut2w\ncos(xi,xj)\n(1)\nwhere Nt1\nw and Nt2\nw are the number of occurrences\nof w in time periods t1 and t2.\nPRT Here, the degree of change ofw is measured\nas the cosine distance between the average token\nembeddings (‘prototypes’) of all occurrences of w\nin the two time periods:\nPRT(Ut1\nw,Ut2\nw\n)= 1−cos\n(∑\nxi∈Ut1w\nxi\nNt1w\n,\n∑\nxj∈Ut2w\nxj\nNt2w\n)\n(2)\nJSD To compute this measure, we form a single\nusage matrix [Ut1\nw ; Ut2\nw ] with occurrences from two\ncorpora. We standardise it and then clustered its\nentries using Affinity Propagation (Frey and Dueck,\n2007), a clustering algorithm which automatically\nselects a number of clusters for each word.4 Finally,\nwe define probability distributions ut1\nw , ut2\nw based\non the normalised counts of word embeddings in\neach cluster and compute a the Jensen-Shannon\nDivergence (Lin, 1991) between the distributions:\nJSD(ut1w,ut2w) = H(1\n2\n(ut1w +ut2w\n))−1\n2\n(H(ut1w\n)−H(ut2w\n))\n(3)\n4We use the scikit-learn implementation of Affinity Propa-\ngation with default hyperparameters.\n65\nB Static Embeddings\nWe follow the common approach proposed\nby Hamilton et al. (2016), SGNS+OP, to train\nskip-gram negative sampling embeddings (SGNS;\nMikolov et al., 2013) from scratch for each time pe-\nriod of the diachronic corpus, and then to align the\nseparate vector spaces using the Orthogonal Pro-\ncrustes method (OP). Semantic change is measured\nas the cosine distance between the embeddings of a\ntarget word in the aligned spaces (for more details,\nsee Schlechtweg et al., 2019).\nWe decided not to lemmatize our corpora for\nthese experiments to preserve as much grammat-\nical information as it is possible but we use two\npreprocessing strategies for target words. In the\nfirst strategy (SGNS-raw) we use a raw, unlemma-\ntized, corpus and learn embeddings for target words\nonly in their dictionary form. All other inflected\nforms of the target words are ignored. In the sec-\nond strategy (SGNS-lemma), we lemmatize target\nword occurrences (but not other words) and thus\nuse all target word forms to train their embeddings.\nIn the ranking task, SGNS+OP confirms itself as\na very competitive approach, achieving the best cor-\nrelation scores on German, Swedish, and Russian\n3 (see Table 4). Our results show that lemmatizing\ntarget word forms, so that they all contribute to\nthe same static embedding, brings substantial per-\nformance improvements as well as more stability\nacross test sets.\nOur classification results again confirm the\nstrength of static embeddings, which outperform\nother approaches for German, Norwegian-1, and\nItalian (for English and Swedish, they perform on\npar with the profile-contextualised ensembles). Tar-\nget word form lemmatization is important but less\ndecisive than in the ranking task (see Table 5).\n66\nMethod EN DE LA SW NO-1 NO-2 RU-1 RU-2 RU-3\nSTATIC\nRaw text (SNGS-raw) 0.378 0.226 0.250 -0.036 0.320 0.181 0.101 0.148 0.255\nTarget words lemmatized (SGNS-lemma)0.498 0.369 0.106 0.494 0.238 0.392 0.256 0.292 0.538\nENSEMBLES\nSGNS-raw-MORPH 0.253 0.105 0.436 0.204 0.116 0.368 0.020 0.222 0.275\nSGNS-raw-SYNT 0.341 0.159 0.234 0.158 0.250 0.024 0.019 0.113 0.248\nSGNS-raw-MORPHSYNT 0.354 0.258 0.454 0.297 0.142 0.218 0.013 0.148 0.229\nSGNS-lemma-MORPH 0.255 0.157 0.409 0.386 0.106 0.440 0.057 0.259 0.332\nSGNS-lemma-SYNT 0.364 0.173 0.224 0.242 0.212 0.156 0.071 0.129 0.315\nSGNS-lemma-MORPHSYNT 0.367 0.269 0.415 0.461 0.128 0.341 0.023 0.163 0.286\nTable 4: Spearman correlation scores in the ranking task (‘Task 2’) with type-based static embeddings (SGNS-OP).\nBold values are cases when SGNS-OP outperforms all other methods (XLM-R and grammatical profiles).\nMethod EN DE LA SW NO-1 NO-2 IT\nRaw (SGNS-raw) 0.514 0.542 0.400 0.548 0.757 0.649 0.722\nTarget words lemmatized (SGNS-lemma) 0.676 0.646 0.375 0.742 0.676 0.676 0.778\nENSEMBLES\nSGNS-raw-MORPH 0.622 0.562 0.600 0.484 0.486 0.622 0.500\nSGNS-raw-SYNT 0.541 0.583 0.550 0.581 0.649 0.486 0.500\nSGNS-raw-MORPHSYNT 0.649 0.625 0.500 0.677 0.595 0.486 0.611\nSGNS-lemma-MORPH 0.622 0.438 0.625 0.484 0.514 0.703 0.500\nSGNS-lemma-SYNT 0.541 0.479 0.525 0.581 0.649 0.568 0.611\nSGNS-lemma-MORPHSYNT 0.649 0.604 0.600 0.742 0.514 0.676 0.389\nTable 5: Binary accuracy scores in the classification task (‘Task 1’) with type-based static embeddings (SGNS-OP).\nBold values are cases when SGNS-OP outperforms all other methods (XLM-R and grammatical profiles).\n67",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8242965340614319
    },
    {
      "name": "Natural language processing",
      "score": 0.7064333558082581
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6575311422348022
    },
    {
      "name": "Semantic role labeling",
      "score": 0.5059916377067566
    },
    {
      "name": "Word (group theory)",
      "score": 0.47076845169067383
    },
    {
      "name": "Semantic change",
      "score": 0.42037951946258545
    },
    {
      "name": "Linguistics",
      "score": 0.2848960757255554
    },
    {
      "name": "Sentence",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133731052",
      "name": "University of Helsinki",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I184942183",
      "name": "University of Oslo",
      "country": "NO"
    }
  ],
  "cited_by": 9
}