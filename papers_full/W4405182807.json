{
  "title": "Automatic item generation in various STEM subjects using large language model prompting",
  "url": "https://openalex.org/W4405182807",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2342395186",
      "name": "Kuang Wen Chan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132219760",
      "name": "FARHAN ALI",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101136922",
      "name": "Joon-Hyeong Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115061080",
      "name": "Kah Shen Brandon Sham",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Erdalyn Yeh Thong Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115061081",
      "name": "Francis Woon Chien Chong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995668734",
      "name": "Kun Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2622325465",
      "name": "Guan Kheng Sze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4377692699",
    "https://openalex.org/W6857600051",
    "https://openalex.org/W4393318738",
    "https://openalex.org/W4286561208",
    "https://openalex.org/W4392296327",
    "https://openalex.org/W2947796013",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4386249608",
    "https://openalex.org/W6849025862",
    "https://openalex.org/W4381573643",
    "https://openalex.org/W6863052277",
    "https://openalex.org/W6634390793",
    "https://openalex.org/W6849081288",
    "https://openalex.org/W4366986986",
    "https://openalex.org/W6863624196",
    "https://openalex.org/W2128489726",
    "https://openalex.org/W4225425474",
    "https://openalex.org/W4383818129",
    "https://openalex.org/W4323359867",
    "https://openalex.org/W6859553100",
    "https://openalex.org/W4398185616",
    "https://openalex.org/W4399387113",
    "https://openalex.org/W4391723926",
    "https://openalex.org/W4366733179",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W6713910010",
    "https://openalex.org/W2099155179",
    "https://openalex.org/W6857180886",
    "https://openalex.org/W6859621498",
    "https://openalex.org/W2023913065",
    "https://openalex.org/W6852857644",
    "https://openalex.org/W4396988713",
    "https://openalex.org/W6848670031",
    "https://openalex.org/W4366420437",
    "https://openalex.org/W2145896244",
    "https://openalex.org/W2088526819",
    "https://openalex.org/W6860227977",
    "https://openalex.org/W6750074417",
    "https://openalex.org/W4386069852",
    "https://openalex.org/W6868609447",
    "https://openalex.org/W6863088484",
    "https://openalex.org/W4391227797",
    "https://openalex.org/W6865461179",
    "https://openalex.org/W2018591760",
    "https://openalex.org/W6864691335",
    "https://openalex.org/W4399500059",
    "https://openalex.org/W4313366754",
    "https://openalex.org/W4375854861",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W1996445490",
    "https://openalex.org/W6865368185",
    "https://openalex.org/W6863228043",
    "https://openalex.org/W4389485341",
    "https://openalex.org/W4401043079",
    "https://openalex.org/W592559112",
    "https://openalex.org/W4388046560",
    "https://openalex.org/W4313425238",
    "https://openalex.org/W4378223984",
    "https://openalex.org/W4394580931",
    "https://openalex.org/W4393161681",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W282787389",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W958651444",
    "https://openalex.org/W2912794432",
    "https://openalex.org/W4390307319",
    "https://openalex.org/W4391912473",
    "https://openalex.org/W4395483820",
    "https://openalex.org/W4401043108",
    "https://openalex.org/W4387024875",
    "https://openalex.org/W4390229778",
    "https://openalex.org/W4398859084",
    "https://openalex.org/W4400530533",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390414972",
    "https://openalex.org/W4392202731",
    "https://openalex.org/W2795767316",
    "https://openalex.org/W4324046518"
  ],
  "abstract": "Large language models (LLMs) that power chatbots such as ChatGPT have capabilities across numerous domains. Teachers and students have been increasingly using chatbots in science, technology, engineering, and mathematics (STEM) subjects in various ways, including for assessment purposes. However, there has been a lack of systematic investigation into LLMsâ€™ capabilities and limitations in automatically generating items for STEM subject assessments, especially given that LLMs can hallucinate and may risk promoting misconceptions and hindering conceptual understanding. To address this, we systematically investigated LLMs' conceptual understanding and quality of working in generating question and answer pairs across various STEM subjects. We used prompt engineering on GPT-3.5 and GPT-4 with three different approaches: standard prompting, standard prompting with added chain-of-thought prompting using worked examples with steps, and the chain-of-thought prompting with coding language. The questions and answer pairs were generated at the pre-university level in the three STEM subjects of chemistry, physics, and mathematics and evaluated by subject-matter experts. We found that LLMs generated quality questions when using the chain-of-thought prompting for both GPT-3.5 and GPT-4 and when using the chain-of-thought prompting with coding language for GPT-4 overall. However, there were varying patterns in generating multistep answers, with differences in final answer and intermediate step accuracy. An interesting finding was that the chain-of-thought prompting with coding language for GPT-4 significantly outperformed the other approaches in generating correct final answers while demonstrating moderate accuracy in generating multistep answers correctly. In addition, through qualitative analysis, we identified domain-specific prompting patterns across the three STEM subjects. We then discussed how our findings aligned with, contradicted, and contributed to the current body of knowledge on automatic item generation research using LLMs, and the implications for teachers using LLMs to generate STEM assessment items.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.45715659856796265
    },
    {
      "name": "Natural language processing",
      "score": 0.4567890763282776
    },
    {
      "name": "Psychology",
      "score": 0.3634743392467499
    }
  ],
  "institutions": [],
  "cited_by": 8
}