{
  "title": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference",
  "url": "https://openalex.org/W4385571795",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103134860",
      "name": "Junhao Zheng",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1979104026",
      "name": "Qianli Ma",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2343886657",
      "name": "Shengjie Qiu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096475003",
      "name": "Yue Wu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4382474645",
      "name": "Peitian Ma",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2111413089",
      "name": "Junlong Liu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5080125914",
      "name": "Huawen Feng",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3176509201",
      "name": "Xichen Shang",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112596765",
      "name": "Haibin Chen",
      "affiliations": [
        "South China University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3174531908",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4220671231",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W3212660220",
    "https://openalex.org/W4288265053",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287285787",
    "https://openalex.org/W2400892929",
    "https://openalex.org/W2486285194",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W4229061408",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3102714038",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W3035651653",
    "https://openalex.org/W4225691633",
    "https://openalex.org/W2547185913",
    "https://openalex.org/W4385567121",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3104182862",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W2104608657",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W3175136611",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2971339032",
    "https://openalex.org/W2983995706"
  ],
  "abstract": "Junhao Zheng, Qianli Ma, Shengjie Qiu, Yue Wu, Peitian Ma, Junlong Liu, Huawen Feng, Xichen Shang, Haibin Chen. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9155–9173\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPreserving Commonsense Knowledge from Pre-trained\nLanguage Models via Causal Inference\nJunhao Zheng, Qianli Ma*, Shengjie Qiu, Yue Wu, Peitian Ma,\nJunlong Liu, Huawen Feng, Xichen Shang and Haibin Chen\nSchool of Computer Science and Engineering,\nSouth China University of Technology, Guangzhou, China\njunhaozheng47@outlook.com, qianlima@scut.edu.cn∗\nAbstract\nFine-tuning has been proven to be a simple\nand effective technique to transfer the learned\nknowledge of Pre-trained Language Models\n(PLMs) to downstream tasks. However, vanilla\nfine-tuning easily overfits the target data and\ndegrades the generalization ability. Most exist-\ning studies attribute it to catastrophic forgetting,\nand they retain the pre-trained knowledge in-\ndiscriminately without identifying what knowl-\nedge is transferable. Motivated by this, we\nframe fine-tuning into a causal graph and dis-\ncover that the crux of catastrophic forgetting\nlies in the missing causal effects from the pre-\ntrained data. Based on the causal view, we\npropose a unified objective for fine-tuning to\nretrieve the causality back. Intriguingly, the\nunified objective can be seen as the sum of the\nvanilla fine-tuning objective, which learns new\nknowledge from target data, and the causal ob-\njective, which preserves old knowledge from\nPLMs. Therefore, our method is flexible and\ncan mitigate negative transfer while preserving\nknowledge. Since endowing models with com-\nmonsense is a long-standing challenge, we im-\nplement our method on commonsense QA with\na proposed heuristic estimation to verify its ef-\nfectiveness. In the experiments, our method\noutperforms state-of-the-art fine-tuning meth-\nods on all six commonsense QA datasets and\ncan be implemented as a plug-in module to in-\nflate the performance of existing QA models.\n1\n1 Introduction\nDeep Pre-trained Language Models (PLMs) such\nas RoBERTa (Liu et al., 2019b) and T5 (Raffel\net al., 2020)) are inherently knowledge bases since\nthey are exposed to a tremendous amount of data\n(e.g., the C4 dataset (Raffel et al., 2020)) in the\n∗*Corresponding author\n1Our codes are publicly available\nat https://github.com/zzz47zzz/CET and\nhttps://github.com/qianlima-lab/CET\npre-training stage (Petroni et al., 2019; AlKhamissi\net al., 2022). Unfortunately, transferring the intrin-\nsic knowledge in PLMs to downstream tasks is non-\ntrivial. In practice, fine-tuning is adopted widely\ndue to its flexibility (Chen et al., 2020) and numer-\nous improved methods (Lee et al., 2019; Chen et al.,\n2020, 2019; Mosbach et al., 2020; Zhang et al.,\n2020b; Xu et al., 2021a; Aghajanyan et al., 2020;\nWu et al., 2022) are proposed in recent years. How-\never, fine-tuning faces two challenges when adapt-\ning models to new domains (Chen et al., 2019),\nincluding catastrophic forgetting (Kirkpatrick et al.,\n2017) and negative transfer (Torrey and Shavlik,\n2010). More specifically, catastrophic forgetting\nrefers to models losing previously learned knowl-\nedge and overfitting the target domain data. Neg-\native transfer occurs because not all pre-trained\nknowledge is transferable across domains. Obvi-\nously, catastrophic forgetting and negative transfer\nconstitute a dilemma where the crux lies in identi-\nfying and utilizing transferable knowledge.\nA large body of previous work has been con-\nducted to solve this problem. Existing fine-tuning\nmethods for mitigating catastrophic forgetting can\nbe summarized as preventing the fine-tuned models\nfrom deviating too far from the pre-trained weights.\nFor example, RecAdam (Chen et al., 2020) and\nChild-Tuning (Xu et al., 2021a) utilize the Fisher\nInformation Matrix estimated by the pre-trained\nmodel to constraint the update in the fine-tuned\nmodel. Mixout (Lee et al., 2019) randomly re-\nplaces the model parameters with their pre-trained\nweights. These methods constrain the update of\nmodels’ parameters indiscriminately without iden-\ntifying what knowledge is transferable and thus\nsusceptible to negative transfer. Chen et al. (2019)\nproposed BSS, which focuses on mitigating nega-\ntive transfer by penalizing the small singular values\nof the feature matrix. However, when only negative\ntransfer is concerned, BSS may not fully utilize the\npre-trained knowledge.\n9155\nIn this paper, we propose a novel method called\nCausal Effect Tuning (CET) for mining the pre-\ntrained knowledge in PLMs. Unlike the previous\nfine-tuning method, our method is rooted in the\ntheory of causal inference. It delves into the causal-\nities between data, models, and features instead\nof merely statistical association. First, we frame\nvanilla fine-tuning into a causal graph (Glymour\net al., 2016) and find out that the cause of catas-\ntrophic forgetting is the vanishing causal effects of\npre-trained data. Therefore, preventing forgetting\nis to maximize the causal effect. Then, we ap-\nproximate the causal effect with the likelihood of\nthe joint prediction of K-Nearest-Neighbor (KNN)\nsamples. Since equipping models with common-\nsense knowledge is still challenging, we implement\nthe proposed causal graph with a heuristic approx-\nimation on commonsense QA. We measure the\ndistance with the similarity between gold answers\n(i.e., ground-truth answers) instead of questions for\nretrieving KNNs. The rationale is that the ques-\ntions with the same gold answer share the same\ncommonsense knowledge in PLMs. Finally, we\napply our method to RoBERTa (Liu et al., 2019b)\nand T5 (Raffel et al., 2020) and conduct extensive\nexperiments on six commonsense datasets. The\nexperimental results show that our method outper-\nforms state-of-the-art fine-tuning methods and can\nbe plugged into the state-of-the-art QA models to\nimprove performance.\nMore importantly, our method is lightweight and\nflexible since it requires no learnable parameter\nexcept PLMs and has fewer hyper-parameters to\ntune. It is worth noting that our method readily\ncontrols the strength of knowledge preservation by\na single hyper-parameter, enabling a good balance\nbetween preserving pre-trained knowledge and ab-\nsorbing new knowledge from downstream tasks. In\nsummary, our contributions are three-fold:\n• We present a causal graph for fine-tuning with\nless forgetting by identifying the root cause of\ncatastrophic forgetting as the missing causal\neffects of pre-trained data.\n• Based on the proposed causal graph, we de-\nsign a lightweight and flexible fine-tuning\nmethod called Causal Effect Tuning for pre-\nserving knowledge in PLMs.\n• For commonsense QA, we estimate the causal\neffect with a heuristic approximation. And we\nverify the effectiveness and versatility of our\nmethod through extensive experiments on six\ncommonsense QA datasets.\n2 Related Work\n2.1 Fine-tuning Methods\nApart from the methods mentioned above, some ap-\nproaches improve downstream performances from\nthe perspective of robustness. Aghajanyan et al.\n(2020) proposed R3F, which regularizes the sym-\nmetric KL divergence between the classifications\nof the original samples and the perturbed ones. Wu\net al. (2022) proposed Noisytune, which adds uni-\nform distribution noise to pre-trained parameters\nbefore fine-tuning to reduce the risk of overfitting\nthe pre-training tasks and data. Besides, Mosbach\net al. (2020); Zhang et al. (2020b) increased the\nstability of fine-tuning BERT (Devlin et al., 2019)\nin the low-data regime. Mosbach et al. (2020) ad-\nvocated fine-tuning for a long time and choosing\ngood optimizers and hyper-parameters. Zhang et al.\n(2020b) verified that re-initialized the top layers\nof BERT helps pre-trained knowledge transfer to\ndownstream tasks.\n2.2 Causal Inference\nCausal inference (Glymour et al., 2016; Schölkopf,\n2022) has been recently introduced to various com-\nputer vision tasks such as image classification (Hu\net al., 2021), semantic segmentation (Zhang et al.,\n2020a) and long-tailed classification (Tang et al.,\n2020; Nan et al., 2021), and NLP tasks such as dis-\ntantly supervised NER (Zhang et al., 2021), neural\ndialogue generation (Zhu et al., 2020) and contin-\nual named entity recognition (Zheng et al., 2022).\nTo our best knowledge, we are the first to apply\ncausal inference to fine-tuning.\n2.3 Continual Learning\nAlthough catastrophic forgetting happens in both\ncontinual learning (Rebuffi et al., 2017; Hu et al.,\n2021) and fine-tuning, the targets of these two tasks\nare fundamentally different. Continual learning\naims to learn a growing number of tasks sequen-\ntially and maximize the performance on all recog-\nnized tasks. In contrast, fine-tuning maximize only\nthe performance of target tasks. The recent advance\nin continual learning (Hu et al., 2021; Zheng et al.,\n2022) partially inspires this work.\n9156\n(a) Vanilla Fine-Tuning\n (b) Fine-Tuning with Less Forgetting\nFigure 1: The causal graphs of vanilla fine-tuning and our method. (a): The knowledge is forgotten during vanilla\nfine-tuning since the causal effect of the pre-trained data is missing; (b): When conditioning on HT\n0 , the causal\neffect of the pre-trained data is retained through the causal path P ↔XT →H → ˆY. In addition, the model\nabsorbs new knowledge from XNT through the causal path XNT →H →ˆY.\n3 Methodology\nIn this section, we first use causal graphs (Pearl,\n2009) to analyze how pre-trained knowledge is for-\ngotten in fine-tuning. Then, we present a causal\ngraph for anti-forgetting based on previous anal-\nysis. Next, we estimate the causal effect through\nderivations and propose a unified learning objec-\ntive for fine-tuning with less forgetting. At last, we\nprovide a heuristic approximation for estimating\nthe causal effect on a challenging downstream task,\ncommonsense QA. Note that the proposed causal\ngraph and the fine-tuning method are generic to all\ndownstream tasks.\n3.1 Vanilla Fine-Tuning\nIn a causal graph, nodes represent variables, and di-\nrected edges are causalities between nodes. Fig.(1a)\ndelineates the process of vanilla fine-tuning. We\ndenote the pre-trained data (i.e., pre-trained knowl-\nedge) as P; the data in target tasks as X; the fea-\nture of X extracted by the pre-trained model and\nfine-tuned model as H0 and H, respectively; the\nprediction of the fine-tuned model on target tasks\nas ˆY (i.e., the probability over categories). The\ncausality between nodes (i.e., directed edges) is as\nfollows: (1) X →H → ˆY: X →H represents\nthat the feature H is extracted by the backbone\nmodel such as RoBERTa, andH →ˆY represents\na classifier compute the prediction ˆY according to\nthe extracted feature H; (2) X →H0 ←P: H0\nis determined by both P and X because H0 is ex-\ntracted by the pre-trained model, which is trained\non P 2.\n2Here, we ignore the effect of initial parameters initial-\nized from the pre-trained model since it will be exponentially\ndecayed towards zero during fine-tuning (Kirkpatrick et al.,\n2017).\nThen, the effect of pre-trained data P on predic-\ntions ˆY can be calculated as:\nEffectP = P( ˆY = ˆy|do(P = p))\n−P( ˆY = ˆy|do(P = 0)) (1)\n= P( ˆY = ˆy|P = p) −P( ˆY = ˆy|P = 0)\n(2)\n= P( ˆY = ˆy) −P( ˆY = ˆy) (3)\n= 0, (4)\nIn Eq.(1), do(P = 0)represents that no pre-trained\ndata is used for pre-training, and do(P = p)\nrepresents a standard pre-training is performed.\nThen, P( ˆY = ˆy|do(P = p)) is the prediction\ngiven by a pre-trained-then-fine-tuned model\nand P( ˆY = ˆy|do(P = 0))is the prediction given\nby a randomly-initialized-then-fine-tuned model.\nEq.(1) defines EffectP as the difference between\nthe two predictions. Eq.(2) holds because P has\nno parent nodes. Eq.(3) holds because collider H0\nblocks all causal paths from P to Y.\nEq.(1)-(4) shows that a vanilla fine-tuned model\nwill eventually forget all pre-trained knowledge\nwhen no constraints are imposed. In practice, fine-\ntuned models will not forget all learned knowl-\nedge because the learning rate and training time\nare considerably lower and shorter than those in\npre-training. However, fine-tuned models likely\nforget partial pre-trained knowledge, overfit the tar-\nget data, and fall into sub-optimal states since the\namount of target data is usually considerably less\nthan that of pre-trained data.\n3.2 Fine-Tuning with Less Forgetting\nThe causal graph in Fig.(1a) necessitates the re-\ntrieval of the causality between P and ˆY back. A\nstraightforward solution is utilizing the pre-trained\n9157\ndata to constrain model behaviors in new tasks.\nHowever, it is often obstructed by time, space, and\nfinancial constraints.\nThanks to causal inference, we can build a causal\npath between P and X without storing P. In the\ncausal graph Fig.(1a), H0 is the joint outcome of\nthe independent causes P and X. Intriguingly,\nonce the common effect H0 is observed, the causes\nP and X become dependent. The causal effect is\ncalled colliding effect in Hu et al. (2021); Zheng\net al. (2022)3. We’d like to provide a vivid exam-\nple (Pearl, 2009) for understanding this pattern in\ncausal inference: If the admission criteria to a cer-\ntain school require either high grades or special mu-\nsical talents, then these two attributes will be found\nto be correlated (negatively) in that school’s student\npopulation, even if these attributes are uncorrelated\nin the population at large. By conditioning on H0,\nthe causal effect of pre-trained data is preserved\nduring fine-tuning (i.e., EffectP >0), and thus the\npre-trained knowledge is preserved.\nExcept for preserving old knowledge, assimilat-\ning new knowledge from target data is critical. In\naddition, negative transfer may occur if we pre-\nserve pre-trained knowledge overly. Motivated by\nthis, we split the target data into two nodes XT\nand XNT . XT represents the samples where we\ncalculate colliding effects, and their knowledge\nshould be transferred from PLMs. XNT is the\nsamples where we do not calculate colliding ef-\nfects, and their knowledge is domain-specific and\nshould be absorbed into fine-tuned models. Con-\nsequently, the causal graph for our method is in\nFig.(1b), and the rationale is as follows: The fine-\ntuned model preserves pre-trained knowledge by\nutilizing colliding effects (P ↔XT ) while learn-\ning domain-specific knowledge (XNT ). The final\nprediction depends on both pre-trained knowl-\nedge and domain-specific knowledge from causal\npaths P ↔XT →H →ˆY and XNT →H →ˆY,\nrespectively.\n3.3 Estimating Colliding Effects\nNext, we need to estimate the colliding effect be-\ntween P and XT . When conditioning on H0,\n3This phenomenon is also known as Berkson’s paradox in\n(Berkson, 1946) and as the explaining away effect in (Peari\nand Kim, 1983).\nEffectP can be calculated as:\nEffectP =\nN∑\ni=1\nEffect(i)\nP (5)\n≈\nN∑\ni=1\nK∑\nk=0\nP( ˆY(i)|X = x(i,k))WP (x(i),x(i,k)),\n(6)\nwhere ∑K\nk=0 WP (x(i),x(i,k)) = 1. N is the num-\nber of samples in the target data and x(i) is the i-th\nsample. Effect(i)\nP is the colliding effect of P on\nthe prediction ˆY(i). WP (·,·) is a function deter-\nmined by the pre-trained model and measures the\nsimilarity between two samples in the hidden space\nof the pre-trained model. In this case, we denote\nWP (x(i),x(i,k)) as Wi,k for brevity. x(i,k) is the\nk-th nearest neighbor of x(i) in the hidden space.\nSince x(i) always has the largest similarity with\nitself, we let x(i,0) = x(i) and call x(i) the anchor\nsample. Besides, we assume that the K Nearest\nNeighbours (KNNs) are sorted in descending order\naccording to the similarity. Therefore, we have\nWi,0 ≥Wi,1 ≥Wi,2 ≥···≥ Wi,K. Kis a hyper-\nparameter representing the number of neighbors for\nestimating ˆY(i). We provide a detailed derivation\nand further explanation in Appendix A.\nEq.(5) re-writes the total causal effect as the sum\nof the causal effect on the prediction of each tar-\nget sample (i.e.,Effect(i)\nP ). In Eq.(6), P( ˆY(i)|X =\nx(i,k)) represents the likelihood of ˆY(i) when x(i,k)\nis the model input. Eq.(6) shows that Effect(i)\nP can\nbe approximated by the weighted sum of the like-\nlihood when the model input is the anchor sample\nx(i) and its KNNs. Since we expect to maximize\nP( ˆY(i) = y(i)|X = x(i)), maximizing Effect(i)\nP\nequals to maximizing the likelihood of the joint\nprediction on the ground-truth label y(i).\n3.4 Overall Objective\nIn Eq. 6, the total causal effect EffectP is bro-\nken down into the causal effect of each sample\nEffect(i)\nP . In this case, maximizing EffectP is to\npreserve the related knowledge of all samples. As\nwe mentioned before, indiscriminately preserving\nknowledge may lead to negative transfer. To ad-\ndress this problem, we introduce a similarity thresh-\nold θto select the number of nearest neighbors for\neach sample automatically. Specifically, for the\ni-th sample, we truncate the ki (K ≥ki ≥0) near-\nest neighbors whose similarity is greater or equal\n9158\nFigure 2: An illustration of Causal Effect Tuning. x(i) is the anchor sample and h(i)\n0 is the hidden feature extracted\nby the pre-trained model. x(i,1),x(i,2),x(i,3) are the KNNs of x(i). We apply colliding effects onx(i) to preserve the\nold knowledge. After fine-tuning, the “Red” knowledge is preserved with colliding effects, and “blue” knowledge is\nforgotten without colliding effects. A specific instance is as follows: x(i)= “What is a fast but expensive way to send\nsmall cargo? (answer: airplane)”; x(i,1)=“Where could you find a seat that sometimes vibrates?”(answer: airplane);\nx(i,2)=“What has metal wings?”(answer: airplane); x(i,3)= \"It was important precious cargo, so it was delivered as\nquickly as possible by means of what?”(answer: aeroplane). The “red” knowledge represents the commonsense\nabout “airplane”.\nthan θ. In this way, we differentiate the strength of\nknowledge preservation on each sample by select-\ning the neighbors with small distances to their an-\nchor sample. More interestingly, when ki = 0, i.e.,\na sample has no neighbors, the Effect(i)\nP amounts\nto P( ˆY(i) = y(i)|X = x(i)), which is exactly the\nobjective of each sample in vanilla fine-tuning. Fig.\n2 provides an illustration for our method, where the\nsamples with no neighbors can be seen as a special\ncase of our method. Formally, we define the overall\nobjective as follows:\nmax EffectP =\nN∑\ni=1\nEffect(i)\nP (7)\n=\n∑\ni∈ST\nEffect(i)\nP\n  \nColliding Effects\n+\n∑\ni∈SNT\nEffect(i)\nP\n  \nVanilla Fine-Tuning\n, (8)\n=\n∑\ni∈ST\nki∑\nk=0\nP( ˆY(i)|X = x(i,k))Wi,k\n  \nColliding Effects\n(9)\n+\n∑\ni∈SNT\nP( ˆY(i)|X = x(i))\n  \nVanilla Fine-Tuning\n,\nwhere ∑\nk Wi,k = 1,ST = {i|ki > 0},SNT =\n{i|ki = 0}. Considering the distances between\nKNNs and their anchor sample are approximated\nand thus inaccurate, we set Wi,0 = W0 and Wi,1 =\nWi,2 = ··· = Wi,ki = 1−W0\nki\nwhen ki > 0 for\nimplementation. W0 is a hyper-parameter for con-\ntrolling the strength of colliding effects. When\nW0 = 0, the overall target degenerates to the\nvanilla fine-tuning target. When W0 = 1, the over-\nall target retains knowledge indiscriminately on all\nsamples. In Eq.(9), the second term amounts to the\nvanilla fine-tuning objective since only the anchor\nsample’s prediction is computed. In other words,\nwe preserve knowledge for the samples with KNNs\nand learn new knowledge for the samples without\nKNNs. The rationale is that the knowledge should\nbe preserved when more samples require it to an-\nswer the question. In the proposed causal graph\nin Fig.(1b), the first and the second term of Eq.(9)\ncorrespond to the two causal paths throughXT and\nXNT respectively. We summarized the proposed\nmethod in Fig. 2 and Alg. 1 in Appendix A.\n3.5 An Implementation on Commonsense QA\nIn this subsection, we provide an implementation\nfor the causal graph in Fig.(1b) on commonsense\nQA. We note that the overall objective in Eq. 9 is\nagnostic to specific downstream tasks and model\narchitectures. The implementation can be different\nin various tasks or model architectures, and the key\nis to find proper KNNs. This paper provides an\nimplementation on commonsense QA since PLMs\nmay be endowed with commonsense knowledge\nin pre-training (Petroni et al., 2019; AlKhamissi\net al., 2022), and it is still challenging for models to\n9159\ncapitalize on commonsense (Talmor et al., 2018).\nWe first formulate the commonsense QA as\nfollows: Given a dataset with N samples\n{(q(i),a(i),{o(i)\nj }j)}N\ni , we train the best model for\nchoosing the gold answera(i) among options {o(i)\nj }\ngiven a questionq(i). More specifically, the input of\nthe i-th sample can be x(i) = q(i)||o(i)\n1 ||···|| o(i)\nj or\n{x(i)}j = {q(i)||o(i)\nj }j 4 where ||is the string-level\nconcatenation.\nThen, we define a metric to search KNNs. A sim-\nple solution is to compute the euclidean distance or\ncosine similarity between the average last hidden\nstates of PLMs. However, this method struggles\nto capture accurate semantic meanings, and mea-\nsuring sentence similarity remains challenging. In\nthis regard, we provide a simple heuristic approxi-\nmation. In most cases, the questions with the same\ngold answers share the same knowledge. For exam-\nple, “airplane” is the gold answer to the following\nquestions, and we can use the knowledge about “air-\nplane” to answer them: “What is a fast but expen-\nsive way to send small cargo?”; “Where could you\nfind a seat that sometimes vibrates?”; “What has\nmetal wings?”. Therefore, we estimate the similar-\nity between gold answers to cope with the difficulty\nof evaluating sentence similarity. Since options are\nusually much shorter than questions, lightweight\ntools such as spaCy (Honnibal et al., 2020) can be\nused to retrieve gold answers with close semantic\nmeanings (e.g., “airplane” and “aeroplane”).\nAt last, we define the input of the i-th sam-\nple’s KNNs as x(i,k) = q(i,k)||o(i)\n1 ||···|| o(i)\nj or\n{x(i,k)}j = {q(i,k)||o(i)\nj }j. It alleviates the over-\nfitting problem since the model needs to select the\ncorrect answer among the options of anchor sample\nwhen the question is from its KNNs.\n4 Experiments\n4.1 Settings\nDatasets. We conduct experiments on 6 datasets:\nCommonsenseQA(CSQA) (Talmor et al., 2018),\nOpenBookQA(OBQA) (Mihaylov et al., 2018),\nARC (Clark et al., 2018, 2016), QASC (Khot et al.,\n2020), SocialIQA (SIQA) (Sap et al., 2019), PIQA\n(Bisk et al., 2020). Since the official test sets of\nCSQA, QASC, SIQA, and PIQA are not available,\nwe follow (Yasunaga et al., 2021) and use the offi-\n4Concatenating all options or each option depends on mod-\nels.\ncial dev sets as test sets and split in-house dev set\nfrom the original training sets. The dataset statistics\nare summarized in Table 6 in Appendix B.\nTraining. Given its popularity, we use RoBERTa-\nlarge (Liu et al., 2019b) as the backbone model in\ndefault. We also explore T5-large (Raffel et al.,\n2020) since Khashabi et al. (2020) showed that it\nexcels at answering questions in different formats.\nOther training details are specified in Appendix B.\nCompetitive Methods. We make comparisons\nwith nine state-of-the-art fine-tuning methods:\nvanilla fine-tuning, BSS (Chen et al., 2019),\nChildTune-F&ChildTune-D (Xu et al., 2021a),\nMixout (Lee et al., 2019), NoisyTune (Wu et al.,\n2022), R3F (Aghajanyan et al., 2020), RecAdam\n(Chen et al., 2020) and ReInit (Zhang et al., 2020b).\nFor each method, we use the recommended hyper-\nparameters in the paper and source code for a fair\ncomparison. We discuss the implementation details\nof the fine-tuning methods in Appendix C.\nHyper-Parameters. As for the hyperparameters\nof our methods, we fix K = 5and search the best\nW0 in {0.5, 0.7, 0.9, 0.95, 0.97} for each dataset.\nWe use spaCy to estimate the similarity between\ngold answers. We set θ = 0.99 for PIQA and\nθ= 1.00 for other datasets (i.e., the gold answers\nshould be matched precisely).\n4.2 Results and Analyses\nComparisons with State-Of-The-Art. To\ndemonstrate the effectiveness of our method,\nwe re-implement several strong baselines on\ncommonsense QA datasets using their officially\nreleased codes and hyper-parameters. The results\nare summarized in Table 1. Results show that\nour method outperforms all fine-tuning methods\nconsistently. On QASC and OBQA, our method\nachieves 57.57% and 70.76% accuracy, obtaining\n3.53% and 2.64% improvements on vanilla\nfine-tuning.\nWhy our method better preserves commonsense\nknowledge from PLMs? The reasons are two-fold.\nThe first reason is that our method utilizes the col-\nliding effect for transferring the “colliding” com-\nmonsense knowledge, while other methods do not.\nFor instance, in Fig.2, our method encourages mod-\nels to update x(i) and its KNNs x(i,1),x(i,2),x(i,3)\nsimultaneously. In this way, the commonsense\nknowledge about “airplane” that “airplanes deliver\nsmall and precious cargo”, “airplanes have metal\nwings” and “airplanes have seats” can be trans-\n9160\nTable 1: Comparison with state-of-the-art methods. The average accuracy (%) and the standard derivation are\nreported.\nMethods CSQA OBQA ARC-Easy ARC-Challenge QASC PIQA SIQA\nFine-Tuning 75.74 (0.47) 68.12 (0.32) 67.66 (0.45) 45.98 (0.53) 54.04 (1.05) 78.62 (0.53) 77.46 (0.33)\nBSS 76.21 (0.63) 68.64 (1.23) 68.24 (0.31) 46.62 (0.80) 53.82 (1.20) 78.20 (0.96) 77.35 (0.18)\nChildTune-F 75.50 (0.44) 69.84 (0.88) 68.17 (0.77) 46.30 (1.67) 54.41 (1.63) 77.61 (1.06) 75.87 (0.64)\nChildTune-D 76.76 (0.81) 69.36 (0.60) 67.86 (0.73) 45.28 (0.67) 55.77 (0.52) 78.32 (0.38) 78.20 (0.35)\nMixout 76.09 (0.56) 69.70 (0.71) 67.85 (0.57) 44.87 (0.72) 57.34 (1.02) 79.22 (0.31) 77.89 (0.37)\nNoisyTune 76.01 (0.61) 67.56 (0.52) 67.61 (0.58) 46.05 (0.65) 54.43 (0.60) 78.61 (0.31) 76.59 (0.36)\nR3F 76.59 (0.48) 68.47 (0.26) 68.13 (0.68) 47.01 (0.58) 55.69 (0.78) 79.38 (0.60) 77.05 (0.44)\nRecAdam 75.43 (0.33) 70.68 (0.89) 68.07 (0.69) 45.90 (0.59) 54.62 (1.22) 78.26 (1.25) 76.71 (0.61)\nReInit 75.51 (0.71) 69.92 (1.14) 67.63 (0.59) 46.68 (0.39) 52.12 (1.66) 78.61 (0.37) 77.79 (0.15)\nCET(Ours) 76.82 (0.33) 70.76 (0.33) 68.53 (0.53) 47.52 (0.38) 57.57 (0.44) 79.43 (0.27) 78.76 (0.31)\nTable 2: Comparisons with knowledge-graph-based methods on CSQA with different proportions of training data.\nWe use the train-dev-test split in Jiang et al. (2022) and thus the CSQA results are inconsistent with those in other\nexperiments. The results of RoBERTa-large, RGCN, KagNet, Relation Network, MHGRN, QAGNN, and SAFE are\nreported in Jiang et al. (2022). We report the average accuracy (%).\nMethods use GNN? use KG?\nProportion of Training Data\n5% 10% 20% 50% 80% 100%\nRoBERTa-large /enc-37/enc-3729.66 42.84 58.47 66.13 68.47 68.69\n+RGCN (Schlichtkrull et al., 2018) /enc-33/enc-3324.41 43.75 59.44 66.07 68.33 68.41\n+KagNet (Lin et al., 2019) /enc-33/enc-3321.92 49.83 60.09 66.93 69.14 68.59\n+Relation Network (Santoro et al., 2017) /enc-33/enc-3323.77 34.09 59.90 65.62 67.37 69.08\n+MHGRN (Feng et al., 2020) /enc-33/enc-3329.01 32.02 50.23 68.09 70.83 71.11\n+QAGNN (Yasunaga et al., 2021) /enc-33/enc-3332.95 37.77 50.15 69.33 70.99 73.41\n+SAFE (Jiang et al., 2022) /enc-33/enc-3336.45 56.51 65.16 70.72 73.22 74.03\n+CET(Ours) /enc-37/enc-3756.24 59.55 65.19 67.93 70.02 70.99\n+CET+QAGNN /enc-33/enc-3358.78 60.35 65.59 70.43 72.04 73.81\n+CET+SAFE /enc-33/enc-3359.39 61.02 65.75 70.79 73.31 74.54\nTable 3: An CSQA example and its KNNs in our\nmethod.\nGold Answer Question\nAnchor pet shops Too many people want exotic snakes. The demand\nis driving what to carry them?\nKNNs\npet shops Where can a person buy a snake?\npet shop Where might a blowfish be kept?\npet shop Where can you take home a hermit crab?\npet store Where would you get a dog if you do not have one?\npet store\nJohn loves animals and he hates animal abuse. Because\nof this, john is very careful about the places he goes. Where\nmight he avoid going?\nferred jointly, which reduces the risk of over-fitting.\nWe provide more examples from each dataset in\nTable 3 and Table 10,11, in Appendix F. The sec-\nond reason is that our method does not directly\nconstrain (e.g., ChildTune-D, Mixout, RecAdam)\nor modify (e.g., NoisyTune, ReInit) the parame-\nters of fine-tuned models. Empirical results show\nthat these methods encounter negative transfers on\nsome of the datasets. Instead, our method builds\nupon the causal inference theory and utilizes the\njoint prediction as a soft constraint to transfer re-\nlated knowledge while mitigating negative transfer.\nCompared with Knowledge-Graph-Based Meth-\nods. Utilizing knowledge graphs such as Con-\nceptNet (Speer et al., 2017) is a common prac-\ntice for building commonsense QA systems.\nWe compared our method with six knowledge-\ngraph-based methods: Relation Network (San-\ntoro et al., 2017), KagNet (Lin et al., 2019),\nRGCN(Schlichtkrull et al., 2018), MHGRN(Feng\net al., 2020), QAGNN(Yasunaga et al., 2021),\nSAFE(Jiang et al., 2022). Detailed descriptions and\nother related works are given in Appendix D. Note\nthat these methods utilize knowledge graphs (KGs)\nas external knowledge resources, and most of them\ntrain graph neural networks (GNNs) for extracting\nfeatures from KGs. In contrast, our method does\nnot introduce any additional learnable parameters\nexcept PLMs and the final fully-connected layer.\nThe result in Table 2 shows that our method out-\n9161\nperforms RGCN, KagNet, and Relation Network\nby only mining the internal knowledge of PLMs.\nFurthermore, our method significantly outperforms\nall the knowledge-graph-based methods under low\nresource conditions (≤20% training data is used),\nwhich shows that our method helps PLMs adapt to\ndownstream tasks with less data.\nIn addition, our method can be easily imple-\nmented as a plug-in module by simply substitut-\ning the vanilla fine-tuning objective for the causal\neffect in Eq.(9). We combine our method with\nQAGNN and SAFE, respectively. Table 2 shows\nthat our approach consistently improves QAGNN\nand SAFE and achieves superior performances.\nTherefore, the pre-trained commonsense knowl-\nedge benefits downstream tasks even when KGs\nare introduced.\nFine-tuning on a Cyclic Chain of Tasks. To\nunderstand how our method preserves knowledge\nduring fine-tuning, we follow Aghajanyan et al.\n(2020) and design a cyclic chain of tasks:\nA→B →C  \nCycle1\n→A→B →C  \nCycle2\n→···\nIn our experiment, we set A=CSQA, B=OBQA,\nand C=QASC for a demonstration. Specifically,\nwe start from a PLM and fine-tune it on CSQA.\nThen, we use the model fine-tuned on CSQA to ini-\ntialize the backbone model’s parameters and con-\ntinue fine-tuning it on OBQA. Table 4 shows that\nour method retains knowledge significantly bet-\nter than vanilla fine-tuning. The performances on\nOBQA and QASC improve at every cycle, suggest-\ning that our method effectively retains knowledge\nfrom the previous datasets. Unfortunately, both per-\nformances of vanilla fine-tuning and our method\non CSQA degrade slightly, showing that negative\ntransfer happens. In this case, vanilla fine-tuning\nwill lead to more serious performance degradation.\nThe experiment is for demonstration, and a better\ncombination of tasks that promote each other may\nbe found.\nAblation Study. To verify the effectiveness of\nour method, we consider the following ablated\nversion of our method: (1) replacing the KNNs\n(Large,Ours) with randomly selected samples\n(Rand) or samples with the smallest similarity\n(Small); (2) searching the KNNs according to the\nsimilarity of average last hidden states ( Avg) in-\nstead of gold answers (Gold, Ours). The result in\nTable 5 shows that the model learns commonsense\nTable 4: The results of cyclical sequential fine-tuning\nfor three cycles. The average accuracy (%) is reported.\nDataset Fine-Tuning CET(Ours)\nCycle1\nCSQA 75.74 76.82\nOBQA 68.80 70.89\nQASC 54.31 57.49\nCycle 2\nCSQA 75.52 76.69\nOBQA 69.95 71.18\nQASC 55.06 57.64\nCycle 3\nCSQA 75.44 76.75\nOBQA 70.28 71.45\nQASC 55.12 57.78\nTable 5: The ablation study of our method. Gold/Avg:\nsearching the KNNs according to the similarity\nof gold answers or the average last hidden states.\nLarge/Small/Rand: searching the KNNs with the largest\nor smallest similarity, or randomly. The average accu-\nracy (%) is reported.\nMethods CSQA OBQA QASC\nGold+Large(Ours) 76.82 70.76 57.57\nGold+Rand 74.61 68.53 55.77\nGold+Small 74.04 64.67 53.13\nAvg+Large 76.17 69.64 55.62\nAvg+Rand 74.12 68.54 54.54\nAvg+Small 74.20 68.07 53.46\nFine-Tuning 75.74 68.12 54.04\nknowledge better when the KNNs share the gold\nanswer with close meaning.\nAdditional Experiments. Due to space con-\nstraints, we present the experiments on T5, the\nhyper-parameter analysis, the experiments on\nNamed Entity Recognition, and further discussions\nin Appendix E.\n5 Conclusion\nWe propose a novel fine-tuning technique rooted in\ncausal inference for preserving pre-trained knowl-\nedge from PLMs. Although many fine-tuning meth-\nods have been proposed in recent years, most of\nthem overlooked one or both hidden issues of fine-\ntuning, catastrophic forgetting and negative trans-\nfer, which result in a dilemma. In this paper, we\nprovide an answer to the dilemma from the casual\nlens. Impressively, we empirically find that the\nproposed method achieves the best performance\non six commonsense QA datasets and is flexible\nto be applied to various QA systems and model\narchitectures.\n9162\nLimitations\nThere are three limitations on our method. First,\nwe did not verify our method on more generic tasks,\nsuch as text classification, yet it is not limited to\ncommonsense QA. Extending our method to other\ndownstream tasks is our future work. Second, our\nmethod requires a longer training time and a larger\nGPU memory since the KNNs require forward and\nbackward propagation additionally. Third, we do\nnot consider the ambiguity of gold answers, which\nmay affect the quality of KNNs. For example, “ap-\nple” may refer to a kind of fruit or a technology\ncompany.\nAcknowledgements\nThe work described in this paper was partially\nfunded by the National Natural Science Founda-\ntion of China (Grant Nos. 62272173, 61872148),\nthe Natural Science Foundation of Guang-\ndong Province (Grant Nos. 2022A1515010179,\n2019A1515010768).\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2020. Better fine-tuning by reducing representational\ncollapse. In International Conference on Learning\nRepresentations.\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases. arXiv preprint\narXiv:2204.06031.\nJoseph Berkson. 1946. Limitations of the application of\nfourfold table analysis to hospital data. Biometrics\nBulletin, 2(3):47–53.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long,\nand Jianmin Wang. 2019. Catastrophic forgetting\nmeets negative transfer: Batch spectral shrinkage for\nsafe transfer learning. Advances in Neural Informa-\ntion Processing Systems, 32.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nPeter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Turney, and Daniel\nKhashabi. 2016. Combining retrieval, statistics, and\ninference to answer elementary science questions. In\nThirtieth AAAI Conference on Artificial Intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1295–1309, Online. As-\nsociation for Computational Linguistics.\nMadelyn Glymour, Judea Pearl, and Nicholas P Jewell.\n2016. Causal inference in statistics: A primer. John\nWiley & Sons.\nMatthew Honnibal, Ines Montani, Sofie Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy: Industrial-\nstrength Natural Language Processing in Python.\nEduard Hovy, Mitch Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. Ontonotes:\nthe 90% solution. In Proceedings of the human lan-\nguage technology conference of the NAACL, Com-\npanion Volume: Short Papers, pages 57–60.\nXinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng\nHua, and Hanwang Zhang. 2021. Distilling causal\neffect of data in class-incremental learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3957–3966.\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji-Rong\nWen. 2022. Great truths are always simple: A rather\nsimple knowledge encoder for enhancing the com-\nmonsense reasoning capacity of pre-trained models.\narXiv preprint arXiv:2205.01841.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\n9163\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. Qasc: A\ndataset for question answering via sentence compo-\nsition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8082–8090.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In Interna-\ntional Conference on Learning Representations.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2829–2839, Hong Kong,\nChina. Association for Computational Linguistics.\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He,\nSean Welleck, Hannaneh Hajishirzi, and Yejin Choi.\n2022a. Rainier: Reinforced knowledge introspector\nfor commonsense question answering. arXiv preprint\narXiv:2210.03078.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022b. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3154–3169, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019a. On the variance of the adaptive learning rate\nand beyond. In International Conference on Learn-\ning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. In International Conference on Learning Rep-\nresentations.\nShawn N Murphy, Griffin Weber, Michael Mendis, Vi-\nvian Gainer, Henry C Chueh, Susanne Churchill, and\nIsaac Kohane. 2010. Serving the enterprise and be-\nyond with informatics for integrating biology and the\nbedside (i2b2). Journal of the American Medical\nInformatics Association, 17(2):124–130.\nGuoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, and\nWei Lu. 2021. Uncovering main causalities for long-\ntailed information extraction. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9683–9695.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nJ Peari and J Kim. 1983. A computationai modei for\ncombined causal and diagnostic reasoning in infer-\nence systems. In Proceeding of the 8th International\nJoint Conference on Artificial Intelligence.\nJudea Pearl. 2009. Causality. Cambridge university\npress.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942, Florence, Italy. Association for\nComputational Linguistics.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. 2017. icarl: In-\ncremental classifier and representation learning. In\nProceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 2001–2010.\n9164\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142–147.\nAdam Santoro, David Raposo, David G Barrett, Ma-\nteusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Timothy Lillicrap. 2017. A simple neural net-\nwork module for relational reasoning. Advances in\nneural information processing systems, 30.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne van den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European semantic web confer-\nence, pages 593–607. Springer.\nBernhard Schölkopf. 2022. Causality for machine learn-\ning. In Probabilistic and Causal Inference: The\nWorks of Judea Pearl, pages 765–804.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\nKaihua Tang, Jianqiang Huang, and Hanwang Zhang.\n2020. Long-tailed classification by keeping the good\nand removing the bad momentum causal effect. Ad-\nvances in Neural Information Processing Systems ,\n33:1513–1524.\nLisa Torrey and Jude Shavlik. 2010. Transfer learn-\ning. In Handbook of research on machine learning\napplications and trends: algorithms, methods, and\ntechniques, pages 242–264. IGI global.\nPeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro\nSzekely, and Xiang Ren. 2020a. Connecting the dots:\nA knowledgeable path generator for commonsense\nquestion answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4129–4140, Online. Association for Computational\nLinguistics.\nTan Wang, Jianqiang Huang, Hanwang Zhang, and\nQianru Sun. 2020b. Visual commonsense r-cnn. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10760–\n10770.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2022. NoisyTune: A little noise can help\nyou finetune pretrained language models better. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 680–685, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and Fei Huang.\n2021a. Raise a child in large language model: To-\nwards effective and generalizable fine-tuning. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 9514–\n9528.\nYichong Xu, Chenguang Zhu, Ruochen Xu, Yang Liu,\nMichael Zeng, and Xuedong Huang. 2021b. Fus-\ning context into knowledge graph for commonsense\nquestion answering. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 1201–1207, Online. Association for Computa-\ntional Linguistics.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1008–1025, Online. Association for Computational\nLinguistics.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nReasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 535–546, Online.\nAssociation for Computational Linguistics.\nDong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng\nHua, and Qianru Sun. 2020a. Causal intervention for\nweakly-supervised semantic segmentation. Advances\nin Neural Information Processing Systems, 33:655–\n666.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Yoav Artzi. 2020b. Revisiting few-\nsample bert fine-tuning. In International Conference\non Learning Representations.\nWenkai Zhang, Hongyu Lin, Xianpei Han, and Le Sun.\n2021. De-biasing distantly supervised named entity\nrecognition via causal intervention. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\n9165\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4803–4813.\nJunhao Zheng, Zhanxian Liang, Haibin Chen, and\nQianli Ma. 2022. Distilling causal effect from miscel-\nlaneous other-class for continual named entity recog-\nnition. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3602–3615.\nQingfu Zhu, Weinan Zhang, Ting Liu, and\nWilliam Yang Wang. 2020. Counterfactual\noff-policy training for neural dialogue generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3438–3448.\nA A Detailed Derivation for the Colliding\nEffect\nAlgorithm 1: Causal Effect Tuning\nInput: D= {(x(i),y(i))}N\ni=1: a training set\nwith N samples; F0: a pre-trained\nmodel\nOutput: F: a fine-tuned model\n1 Initialize F←F 0;\n2 Compute the KNNs for each sample x(i):\nx(i,1),··· ,x(i,ki);\n3 while not converge do\n4 Compute EffectP according to Eq.(9);\n5 F← arg max\nF\nEffectP ;\n6 end\n7 return F;\nWithout loss of generality, we first define the\nfine-tuning process formally as follows: Given a\npre-trained model F0 and a dataset with Nsamples\n{(x(i),y(i)}N\ni=1, we aim to learn a model Fwhich\nhas the best performance on predicting the label\ny(i). Recall that in Eq.(5), we re-write EffectP as\nthe sum of the causal effect on each prediction ˆY(i).\nNow, the outcome node ˆY in the causal graph be-\ncomes ˆY(i). Then, we need to condition on H0\nto utilize colliding effects. Considering when pre-\ndicting ˆY(i), x(i) should play an important role.\nFurthermore, when X = x(i), its hidden feature is\nsimply calculated as h(i)\n0 = F0(x(i)). Therefore, it\nis natural to choose h(i)\n0 as the hidden feature we\ncondition on.\nAfter controlling H0 = h(i)\n0 , the meaning of the\ninput node X in the causal graph becomes all sam-\nples whose hidden feature is h(i)\n0 . Unfortunately,\ndue to the sparsity in high dimensional spaces,\nonly x(i) satisfies this constraint. Intuitively, if\nwe loosen this constraint a bit, the colliding effect\nwill not disappear instantly. Instead, the colliding\neffect will vanish gradually when the hidden fea-\nture becomes farther and farther away from h(i)\n0 .\nPut differently, colliding effects still exist when\nsamples bear a resemblance to each other in the\nhidden space of the pre-trained model.\nNow, we provide a derivation as follows:\nEffect(i)\nP\n= P( ˆY (i)|H0 = h(i)\n0 , P= p) −P( ˆY (i)|H0 = h(i)\n0 , P= 0)\n(10)\n=\nN∑\nk=1\n(P( ˆY (i)|X = x(k), H0 = h(i)\n0 ) (11)\nP(X = x(k)|H0 = h(i)\n0 , P= p)\n−P( ˆY (i)|X = x(k), H0 = h(i)\n0 )\nP(X = x(k)|H0 = h(i)\n0 , P= 0))\n=\nN∑\nk=1\nP( ˆY (i)|X = x(k))(P(X = x(k)|H0 = h(i)\n0 , P= p)\n(12)\n−P(X = x(k)|H0 = h(i)\n0 , P= 0))\n≈\nN∑\nk=1\nP( ˆY (i)|X = x(k))P(X = x(k)|H0 = h(i)\n0 , P= p)\n(13)\n=\nN∑\nk=1\nP( ˆY (i)|X = x(k)) (14)\nP(H0 = h(i)\n0 |X = x(k), P= p)P(X = x(k)|P = p)\nP(H0 = h(i)\n0 |P = p)\n=\nN∑\nk=1\nP( ˆY (i)|X = x(k))WP (x(i), x(k)) (15)\n≈\nK∑\nk=0\nP( ˆY (i)|X = x(i,k))WP (x(i), x(i,k)) (16)\nEq.(10) is deduced from Eq.(2) and the condition\nof H0 = h(i)\n0 . Eq.(11) expands Eq.(10) as the sum\nof all N samples. In Eq.(12), P( ˆY(i)|X,H0) =\nP( ˆY(i)|X) because X is the only mediator (Pearl,\n2009) from P to ˆY(i). In Eq.(13), we approxi-\nmate P(X = x(k)|H0 = h(i)\n0 ,P = 0)as zero be-\ncause the likelihood of X = x(k) is small when the\nmodel is randomly initialized. Eq.(14) is obtained\nby applying Bayes formula to Eq.(13). In Eq.(14),\nP(H0 = h(i)\n0 |P = p) and P(X = x(k)|P = p)\nare intractable and can be seen as constants. We\nnote that the likelihood term P(H0 = h(i)\n0 |X =\nx(k),P = p) represents how likely the hidden fea-\nture is h(i)\n0 when the input sample is x(k). Obvi-\nously, the likelihood is the largest when k = i\nand becomes smaller when the hidden feature of\n9166\nx(k) become farther away from h(i)\n0 . Therefore, the\nfractional term of Eq. 14 can be regarded as a scal-\ning factor of the likelihood P( ˆY(i)|X = x(k)). In\nEq.(15), we re-write the fractional term of Eq.(14)\nas a function of x(i) and x(k) since h(i)\n0 = F0(x(i)).\nIn Eq.(15), we truncate the top K samples, which\nare closest to x(i), in the hidden space of the pre-\ntrained model. Besides, we let x(i,0) = x(i) since\nx(i) has the largest similarity with itself. Addition-\nally, we let ∑K\nk=0 WP (x(i),x(i,k)) = 1to ensure\nthat the joint prediction is a probability distribution\nover categories.\nB Training Details\nThe dataset statistics is in Table 6. All models\nare implemented based on Pytorch (Paszke et al.,\n2019) and Huggingface (Wolf et al., 2019). We\nuse the default hyper-parameters of RoBERTa and\nT5 according to the Huggingface implementation.\nFollowing Yasunaga et al. (2021); Khashabi et al.\n(2020), we concatenate all options as input when\nthe backbone is T5 and concatenate each option re-\nspectively when the backbone is RoBERTa. We\ntuned the batch size in {64, 128}, the learning\nrate of the backbone model in {5e-5, 2e-5, 1e-5}.\nBefore fine-tuning RoBERTa, a randomly initial-\nized fully connected (FC) layer is added on top of\nRoBERTa, and the learning rate of the FC layer\nis 1e-2. We use RAdam (Liu et al., 2019a) as the\noptimizer and use a constant learning rate sched-\nuler. The weight decay is 1e-2, and the maximum\ngradient norm is 1.0. For each dataset, the training\nhyper-parameters are the same for all methods for\na fair comparison. We select the best model accord-\ning to the performance on the dev set and report\nthe test accuracy of the chosen model. The experi-\nments are run on GeForce RTX 3090 GPU. Each\nexperiment is repeated five times. Since we do not\nintroduce any learnable parameters except PLMs,\nthe total number of parameters of our method is the\nsame as PLMs (RoBERTa-large and T5-large have\n355M and 770M parameters, respectively).\nC Details of the Competitive Fine-tuning\nMethods\nThe details of the competitive fine-tuning methods\nare as follows. Note that we use recommended\nhyper-parameters in the paper or the source code\nfor a fair comparison.\n• vanilla fine-tuning: fine-tuning has been\nproven to be a simple and effective method of\nadapting large PLMs to downstream tasks.\n• BSS (Chen et al., 2019) 5: BSS focuses on\nmitigating negative transfer by penalizing the\nsmall singular values of the feature matrix.\nWe penalize the smallest singular value, and\nthe weight of the regularization term is set as\n1e-3 as recommended.\n• ChildTune-F&ChildTune-D (Xu et al., 2021a)\n6: ChildTune-F&ChildTune-D update a subset\nof parameters (called child network) of large\nPLMs in the backward process. ChildTune-\nD utilizes the Fisher Information Matrix es-\ntimated by the pre-trained model to deter-\nmine the child network. ChildTune-F uses\nBernoulli distribution to determine the child\nnetwork.\n• Mixout 7 (Lee et al., 2019): Mixout randomly\nmixes the parameters of the pre-trained and\nthe fine-tuned model to regularize the fine-\ntuning process. In the experiments, the mixing\nprobability pis set as 0.9.\n• NoisyTune (Wu et al., 2022): NoisyTune adds\nuniform noises to the parameter of the pre-\ntrained model based on their standard devia-\ntions. The scaling factor λ, which controls the\nrelative noise intensity, is set as 0.15.\n• R3F 8 (Aghajanyan et al., 2020): R3F allevi-\nates representational collapse by introducing\nparametric noise. R3F generates noise from\neither a normal or uniform distribution.\n• RecAdam 9 (Chen et al., 2020): RecAdam\noptimizes a multi-task objective and utilize an\nannealing coefficient to gradually shift the ob-\njective from pre-training to downstream tasks.\n• ReInit (Zhang et al., 2020b): Zhang et al.\n(2020b) verified that transferring the top pre-\ntrained layers slows down learning and hurts\nperformance. ReInit re-initializes the top lay-\ners of PLMs when adapting to new tasks. In\nour experiments, we re-initialize the top 3\ntransformer block.\n5https://github.com/thuml/Batch-Spectral-Shrinkage\n6https://github.com/alibaba/AliceMind/tree/main/\nChildTuning\n7https://github.com/bloodwass/mixout\n8https://github.com/facebookresearch/fairseq/tree/main/\nexamples/rxf\n9https://github.com/Sanyuan-Chen/RecAdam\n9167\nTable 6: The dataset statistics.\nTrain Dev Test Option Number Question Length Option Length\nCommonsenseQA 8.5k 1.2k 1.2k 5 13.4 1.5\nOpenBookQA 5.0k 0.5k 0.5k 4 10.7 2.9\nARC-Easy 2.2k 0.6k 2.4k 4 19.4 3.7\nARC-Challenge 1.1k 0.3k 1.2k 4 22.3 4.9\nQASC 7.3k 0.8k 0.9k 8 8.1 1.6\nPIQA 14k 1.8k 1.8k 2 7.1 19.4\nSocialIQA 31k 1.9k 1.9k 3 20.1 3.6\nD Related Works of Commonsense QA\nCommonsense reasoning is a key pillar of human\ncognition and intelligence, but it is still a long-\nstanding challenge for deep learning systems (Xu\net al., 2021b; Wang et al., 2020b; Talmor et al.,\n2018). Current question and answering (QA) sys-\ntems rely on external sources such as knowledge\ngraphs (e.g., ConceptNet) (Yasunaga et al., 2021;\nFeng et al., 2020; Wang et al., 2020a; Lin et al.,\n2019), knowledge bases (e.g., Wiktionary) (Xu\net al., 2021b) and generative pre-trained language\nmodels (e.g., GPT3 (Brown et al., 2020)) (Liu\net al., 2022b; Yang et al., 2020; Rajani et al., 2019;\nLiu et al., 2022a), and achieve remarkable success.\nDespite the remarkable success, collecting high-\nquality external knowledge is usually expensive,\nand noisy knowledge is easily introduced (Liu et al.,\n2022b). In this paper, we present a novel fine-\ntuning method that retains commonsense knowl-\nedge from PLMs since they are exposed to a colos-\nsal amount of data in pre-training and inherently\nknowledge bases (Petroni et al., 2019; AlKhamissi\net al., 2022). Different from the existing common-\nsense QA models, our method does not rely on KGs\nor GNNs. Moreover, our method can be a plug-in\nmodule to enhance the performance of common-\nsense QA models. We compared six commonsense\nQA methods in the experiments:\n• Relation Network (Santoro et al., 2017) uti-\nlizes a relational reasoning structure over the\nknowledge graph;\n• KagNet (Lin et al., 2019) aggregates informa-\ntion with graph convolutional networks and\nLSTMs, and a hierarchical path-based atten-\ntion mechanism;\n• RGCN (Schlichtkrull et al., 2018) extends\nthe graph convolutional network with relation-\nspecific weights;\nTable 7: The average accuracy (%) of fine-tuning and\nour method when T5-large is used as the backbone\nmodel.\nMethods Fine-Tuning CET(Ours)\nCSQA 76.33 (0.55) 76.85 (0.30)\nOBQA 68.04 (0.62) 69.14 (0.35)\nARC-Easy 70.96 (0.48) 71.63 (0.34)\nARC-Challenge 46.68 (0.53) 48.55 (0.58)\nQASC 60.69 (0.78) 61.79 (0.81)\nPIQA 78.96 (0.42) 81.58 (0.55)\nSIQA 78.25 (0.38) 79.40 (0.44)\nTable 8: The average accuracy (%) of our method when\ndifferent Kis selected.\nK=3 K=5\nCSQA 76.74 76.82\nOBQA 70.88 70.76\nARC-EASY 68.59 68.53\nARC-CHALLENGE 47.40 47.52\nQASC 57.42 57.57\nPIQA 79.13 79.43\nSIQA 78.61 78.76\n• MHGRN (Feng et al., 2020) utilizes both\nGNNs and path-based models for common-\nsense QA;\n• QAGNN (Yasunaga et al., 2021) models the\nQA context and the knowledge graph in a\njoint graph and extracts their representations\nthrough a GNN.\n• SAFE (Jiang et al., 2022) designs a simple\nMLP-based knowledge encoder that utilizes\nstatistical relation paths as features.\nE Additional Experimental Results\nExperiments on T5. Our method is model-\nagnostic since it only requires computing the joint\nprediction. Different from discriminant models\nsuch as RoBERTa, T5 is a generative model whose\n9168\n(a) The backbone is RoBERTa-large\n (b) The backbone is T5-large\nFigure 3: The absolute improvements (%) of our method w.r.t fine-tuning whenW0 = {0.50,0.70,0.90,0.95,0.97}.\nThe backbone model is RoBERTa-large (a) and T5-large (b), respectively.\noutput is in text format. Following Khashabi et al.\n(2020), we concatenate a question and its all op-\ntions with prefixes (a),(b),(c),... as the input, and\nexpect the model to output the ground-truth op-\ntion in text format. To adapt our model to T5, we\nsubstitute the prediction from the probability distri-\nbution over options to the probability distribution\nover vocabulary. In this way, we encourage T5 to\ngenerate the same gold answer when the input is\nthe question of the anchor sample and its KNNs.\nThe experimental result is in Table 7. From\nthe result, we find that our method still improves\nvanilla fine-tuning consistently, which demon-\nstrates that our approach can be applied to various\narchitectures. Besides, we also apply ReInit on T5\nas in RoBERTa. Unfortunately, T5 fails to adapt\nto downstream tasks when only a few parameters\nare re-initiated (e.g., the self-attention layer or the\ncross-attention layer in the topmost transformer\nblock). We conjecture that the final language mod-\neling head (LM head), which maps the last hidden\nstates to the vocabulary space, hinders the knowl-\nedge of the bottom layers to transfer to new tasks.\nDifferent from ReInit, our method is also applica-\nble to T5 because it has no assumptions about the\nmodel architecture.\nHyper-parameter Analysis. We consider two\nhyper-parameters that may influence the effective-\nness of our method: the number of neighbors K\nand the weight for controlling the strength of col-\nliding effects W0. Fig. 3a and 3b show that\nour method is robust when various W0 are cho-\nsen. When the backbone is RoBERTa-large, our\nmethod achieves the best performance when W0 =\n0.7 on OBQA, ARC-Easy, and ARC-Challenge;\nwhen W0 = 0.9 on QASC and SIQA; and when\nW0 = 0.97 on CSQA. When the backbone is T5-\nlarge, our method achieves the best performance\nwhen W0 = 0.9 on QASC; when W0 = 0.95 on\nCSQA, OBQA, ARC-Easy, and PIQA; and when\nW0 = 0.97 on ARC-Challenge and SIQA. In addi-\ntion, we find that some datasets, such as CSQA, re-\nquire more domain-specific knowledge while some\ndatasets, such as OBQA, require more pre-trained\nknowledge. The result of Kin Table 8 shows that\na larger Kis beneficial. Our method is also robust\nto K because the similarity threshold θtruncates\nthe number of nearest neighbors for each sample.\nDifferences between Our Method and Data Aug-\nmentation. Our method recombines the KNN\nquestions with the options of the anchor sample.\nA reasonable conjecture is that our method “adds”\nKNN samples to enhance generalization ability.\nWe do the following experiment to test the hypoth-\nesis: We add the same KNN samples generated by\nour method into the original training set for fine-\ntuning. The result shows that its improvement is\nnot statistically significant. The reason may be as\nfollows: Recall that we set θ = 1.0 on five out of\nsix datasets where the gold answer of the anchor\nsample and its KNNs should be matched precisely.\nTherefore, on most datasets, the KNN samples re-\ncombine with the options containing their original\ngold answer, suggesting that they provide no ad-\nditional information. Besides, the newly added\nsamples change the data distribution of the original\n9169\nTable 9: Comparison between CET and vanilla fine-tuning on NER.\nMethod\nCoNLL2003 OntoNotes5 I2B2\nMicro F1 Macro F1 Micro F1 Macro F1 Micro F1 Macro F1\nVanilla Fine-Tuning 92.52 91.09 89.35 80.42 92.81 85.61\nCET (Ours) 92.94 91.52 90.09 81.67 94.07 88.46\ntraining set.\nExperiments on Named Entity Recognition. To\ndemonstrate that CET has the potential to improve\nmore generic tasks, we apply CET to another task,\nNamed Entity Recognition (NER), which is a fun-\ndamental task in NLP. First, NER can be formu-\nlated as a word-level classification task. Therefore,\nboth \"anchor\" and KNNs refer to a specific word.\nThen, we use the Euclidean distance as a metric to\nfind the KNNs in the space of the last hidden states\nof PLMs. Considering NER focuses on recogniz-\ning entities, we only compute the causal effects on\nentity words. During training, both the sentences\ncontaining anchor and KNN words are fed into the\nmodel. And then, we compute the joint prediction\nas in Eq.6 by combining the score prediction of the\nanchor word and the corresponding KNN words.\nFinally, we jointly optimize the causal effects of\nentity words and the vanilla fine-tuning objective\nof non-entity words as in Eq.9.\nWe choose three widely used datasets for ex-\nperiments: CoNLL2003 (Sang and De Meulder,\n2003), Ontonotes5 (Hovy et al., 2006), I2B2 (Mur-\nphy et al., 2010). Following previous experiments,\nwe use RoBERTa-large as the backbone. The result\nin Table 9 indicates that CET outperforms vanilla\nfine-tuning consistently.\nTo better understand CET, here is an example\nfrom CoNLL2003: The anchor is a Location en-\ntity \"California\" in the sentence \". . . Marine Lab-\noratories in California say . . . \". Its three nearest\nneighbours are 1. \"California\" in the sentence \"At\nCalifornia, Tim . . . \"; 2. \"Oakland\" in the sentence\n\"OAKLAND AT NEW YORK\"; 3. \"Florida\" in the\nsentence \"At Florida, . . . \". As shown, the anchor\nand KNN words share the related prior knowledge\nof PLMs, which can also be illustrated in Figure 2.\nF More examples of Colliding Effects\nTable 10: Examples from PIQA and QASC.\nPIQA Gold Answer Question\nAnchor throw it away how do you dispose of a cutip?\nKNNs\nthrow it away how do you dispose of something?\nthrow it away how do you scrap metal?\nQASC Gold Answer Question\nAnchor bacteria What causes botulism?\nKNNs\nbacteria what may die if it becomes too hot?\nbacteria what causes serious illness?\nbacteria What causes food to spoil?\nbacteria What can cause people to die?\nbacteria what feed on dead organisms?\n9170\nTable 11: Examples from CSQA, OBQA, ARC-Easy, ARC-Challenge, and SIQA.\nCSQA Gold Answer Question\nAnchor television To prevent any glare during the big football game he made sure to clean the dust of his what?\nKNNs\ntelevision Where do you watch garbage?\ntelevision What home entertainment equipment requires cable?\ntelevision What does one watch garbage reality shows on?\ntelevision Where might I hear and see information on current events?\ntelevision James wasn’t a repair person, but even he knew that he didn’t need a freon coin in a what?\nOBQA Gold Answer Question\nAnchor sun The leaves of a plant benefit from?\nKNNs\nsun The moon orbits an object that orbits the\nsun Which of these items is required for a deer to live\nsun What is larger then the human planet and causes cycles of day and night?\nthe sun Despite what some think, instead around themselves, our planet spins around\nARC-Easy Gold Answer Question\nAnchor line graph\nA student wants to find the relationship between the diameter of several plastic disks\nand the circumference of each disk. Which of these types of graphs should be constructed\nto determine this relationship?\nKNNs\nline graph\nThe number of squirrels in a certain ecosystem changes over time. These changes can be\nrepresented as a number of connected data points. Which method would a student most\nlikely use to show this information?\nline graph In a city, the daily high and low 16 temperatures for a month are best represented by which\nof the following?\nline graph A student measures the growth of a group of plants given different amounts of fertilizer.\nWhich data display should the student use to compare the growth of the plants?\nline graph\nScientists recorded the hourly temperature at a weather station for the month of July and\nwant to quickly measure a trend over time in temperature changes. Which of these formats\nwould be the most appropriate representation of the temperature data to quickly measure\nany trend?\nline graph The most effective way to show a change happening over time is to display your results\nusing a\nARC-Challenge Gold Answer Question\nAnchor air\nFour materials are put into small containers. These materials are then moved from the\nsmall containers into larger containers. Which material will spread out to completely\nfill a larger container?\nKNNs\nair When you make soap bubbles, what is inside the bubbles?\nair When a tadpole grows, its gills change into lungs. What does it now need to survive?\nair How are green plants an important part of the carbon dioxide-oxygen cycle?\nair Which of the following substances can be separated into several elements?\nSIQA Gold Answer Question\nAnchor compassionate Jan had always wanted a puppy, but decided to adopt an older shelter dog instead. How would\nyou describe Jan?\nKNNs\ncompassionate Jan gave Kai’s husband a hug after hearing the good news about Kai’s recovery. How would\nKai feel as a result?\ncompassionate Quinn ran over a squirrel on the road. They felt a little guilty. How would you describe Quinn?\ncompassionate Cameron was volunteering at a soup kitchen and provided assistance to individuals. How would\nCameron feel afterwards?\ncompassionate Bailey found out that the local fire department lacked funding. Bailey decided to do something\nabout it. How would you describe Bailey?\ncompassionate Ash let the dog inside as it was getting too hot for dog to be outside. How would you describe\nAsh?\n9171\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the Limitations section\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur work does not involve any sensitive data or sensitive tasks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nIn abstract and section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nIn section 5 and appendix B.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nIn section 5 and appendix B. We cite all the datasets and codes.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAll the datasets and codes can be used for research purposes.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe use of existing artifacts was consistent with their intended use.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nAll the datasets and codes can be used for research purposes and do not contains any information\nthat names or uniquely identiﬁes individual people or offensive content.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAll the datasets and codes are available for research purposes. Therefore, we don’t need to provide\ndocumentation for them.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nIn section 5 and appendix B.\nC □\u0013 Did you run computational experiments?\nIn section 5 and appendix B.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn section 5 and appendix B.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9172\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nIn section 5 and appendix B.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nIn section 5 and appendix B.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nIn section 5 and appendix B.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n9173",
  "topic": "Commonsense reasoning",
  "concepts": [
    {
      "name": "Commonsense reasoning",
      "score": 0.6964577436447144
    },
    {
      "name": "Computer science",
      "score": 0.6131641864776611
    },
    {
      "name": "Chen",
      "score": 0.5956737995147705
    },
    {
      "name": "Computational linguistics",
      "score": 0.5504006147384644
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5454649329185486
    },
    {
      "name": "Inference",
      "score": 0.5380702614784241
    },
    {
      "name": "Natural language processing",
      "score": 0.5220710635185242
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4810898005962372
    },
    {
      "name": "Cognitive science",
      "score": 0.391250342130661
    },
    {
      "name": "Knowledge extraction",
      "score": 0.21270644664764404
    },
    {
      "name": "Psychology",
      "score": 0.2001079022884369
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    }
  ]
}