{
  "title": "Multi-cell LSTM Based Neural Language Model",
  "url": "https://openalex.org/W2901701848",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5082499287",
      "name": "Thomas Cherian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076144190",
      "name": "Akshay Badola",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032847771",
      "name": "Vineet Padmanabhan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1408639475",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2152511329",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W1597533204"
  ],
  "abstract": "Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.",
  "full_text": "Multi-cell LSTM Based Neural Language Model\nThomas Cherian, Akshay Badola and Vineet Padmanabhan\nSchool of Computer & Information Sciences, University of Hyderabad, India\nthoma@uohyd.ac.in, badola@uohyd.ac.in, vineetcs@uohyd.ernet.in\nAbstract\nLanguage models, being at the heart of many\nNLP problems, are always of great interest to re-\nsearchers. Neural language models come with the\nadvantage of distributed representations and long\nrange contexts. With its particular dynamics that\nallow the cycling of information within the net-\nwork, ‘Recurrent neural network’ (RNN) becomes\nan ideal paradigm for neural language modeling.\nLong Short-Term Memory (LSTM) architecture\nsolves the inadequacies of the standard RNN in\nmodeling long-range contexts. In spite of a plethora\nof RNN variants, possibility to add multiple mem-\nory cells in LSTM nodes was seldom explored.\nHere we propose a multi-cell node architecture for\nLSTMs and study its applicability for neural lan-\nguage modeling. The proposed multi-cell LSTM\nlanguage models outperform the state-of-the-art re-\nsults on well-known Penn Treebank (PTB) setup.\n1 Introduction\nA language model is a function, or an algorithm to learn\nsuch a function that grasps the salient characteristics of the\ndistribution of sequences of words in a natural language,\nallowing one to make probabilistic predictions for the\nnext word given preceding ones. Language models are\nat the heart of many NLP tasks like Speech Recognition,\nMachine Translation, Handwriting Recognition, Parsing,\nInformation Retrieval and Part of Speech tagging. N-\ngram based approaches are the most popular techniques\nfor language modeling [Manning and Sch ¨utze, 1999;\nJelinek and Mercer, 1980]. They are based on the Markov’s\nassumptions that the probability of occurrence of a particular\nword in the sequence depends only on the occurrence of\nprevious n− 1 words. Though successful, they are unable\nto make use of the longer contexts and the word similarities.\nN-gram based approaches can not look into the context\nbeyond a smaller ‘n’, say 3 in case of the trigram approach.\nAlso they treat each word as a stand-alone entity and hence\nare unable to identify and utilize the similarity of words.\nThe language models based on neural networks are termed\nas neural language models. These models exploit neural\nnetwork’s ability to learn distributed representations to ﬁght\nthe ‘curse of dimensionality’ [Bengio and Bengio, 2000;\nBengio et al., 2003]. Distributed representations also equip\nthe neural models with the ability to utilize the word similar-\nities. In contrast to the n-gram models; because of the use\nof recurrent neural networks; neural models do not require\nto predeﬁne the context length. This allows the models\nto leverage larger contexts to make the predictions. These\nadvantages make neural language models a popular choice\ndespite the high computational complexity of the model.\n“Recurrent neural networks form a very powerful and\nexpressive family for sequential tasks. This is mainly\nbecause of the high dimensional hidden state with non-linear\ndynamics that enable them to remember and process past\ninformation.” [Sutskever et al., 2011 ]. Cheap and easily\ncomputable gradients by means of back-propagation through\ntime (BPTT) made them even more attractive. Even then\nRNNs failed to make their way into the mainstream research\nand applications for a long time due to the difﬁculty in\neffectively training them. The ’Vanishing and Exploding\ngradient’ problems; discussed by Bengio et al. [Bengio et\nal., 1994 ] led to the complete negligence of the model for\ndecades until recently. In 1997, Hochreiter et al. [Hochreiter\nand Schmidhuber, 1997 ] came up with the LSTM architec-\nture to deal with the inadequacies of the standard RNN in\nmodeling long-range contexts. Gers et al. [Gers F. et al.,\n1999] further enhanced the standard LSTM model with the\naddition of the forget gate. LSTMs prevent the ‘vanishing\ngradient problem’ with the use of ‘Constant Error Carousels’\n(CECs) [Hochreiter and Schmidhuber, 1997]; also known as\n‘memory cells’. Currently LSTMs are at the core of RNN\nresearch. Many variants of basic LSTM have been proposed.\nHowever, to the best of our knowledge, these studies were\ncentered around the efﬁcient use of the gates in LSTM cell\nand none of them studied the effects of incorporating multiple\nmemory cells into the single LSTM node. Here we move in\nthis direction, propose a multi-cell LSTM architecture and\nstudies its application on the neural language modeling.\nWe propose a multi-cell node architecture for the LSTMs\nwith the hypothesis that the availability of more information\nand the efﬁcient selection of the right information will in-turn\nresult in a better model. The use of multiple memory cells\nis concealed within the nodes so as to keep the network\narXiv:1811.06477v1  [cs.NE]  15 Nov 2018\ndynamics same as that of standard LSTM networks. This\nintroduces the need to have an efﬁcient selection mechanism\nthat selects a single value from the multi-cells such that a\nsingle output is transmitted from the node.\nConcretely, our contributions are two-fold:\n• We propose a multi-cell node architecture for LSTM net-\nwork and investigate the optimum strategies for select-\ning a particular cell value or combining all the memory\ncell values so as to output a single value from the node.\n• Further, we apply the multi-cell LSTM for neural lan-\nguage modeling and compare its performance with\nthe state-of-the-art Zaremba’s models [Zaremba et al.,\n2014].\n2 Neural Language Modeling\nNeural Language Modeling came into limelight with the\n‘Neural Probabilistic Language Model’ (hereafter referred\nas NPLM) proposed by Bengio et al. [Bengio et al., 2003].\nIt deals with the challenges of n-gram language models and\nthe ‘Curse of dimensionality’ [Bengio and Bengio, 2000 ]\nby simultaneously learning the distributed representation\nof words and the probability distribution for the sequences\nof words expressed in terms of these representations. The\nfeed-forward neural network based NPLM architecture\nmakes use of a single hidden layer with hyperbolic tangent\nactivation to calculate the probability distribution. The\nsoftmax output layer gives the probabilities. Generalization\nis obtained with the use of distributed word representations.\nArisoy et al. [Arisoy et al., 2012] extended Bengio’s model\nby adding more hidden layers (upto 4). They observe that the\ndeeper architectures have the potential to improve over the\nmodels with single hidden layer.\nNPLM was exceptionally successful and the further\ninvestigations [Goodman, 2001] show that this single model\noutperforms the mixture of several other models which are\nbased on other techniques. Even though Bengio et al. were\nsuccessful in incorporating the idea of word similarities\ninto the model [Bengio et al., 2003 ]; owing to the use of\nﬁxed length context that needs to be speciﬁed prior to the\ntraining; NPLM was unable to make use of larger contexts.\nIn contrast to the feed forward neural networks, the recurrent\nneural networks do not limit the context size. With recurrent\nconnections, theoretically, information can cycle in the\nnetwork for arbitrarily long time. In [Mikolov et al., 2010]\nMikolov et al. study the ‘Recurrent Neural Network based\nLanguage Model’ (hereafter referred as RNNLM). RNNLM\nmakes use of ‘simple recurrent neural network’ or the ‘Elman\nnetwork’ architecture [Elman, 1990]. The network consists\nof an input layer, a hidden layer (also known as the context\nlayer) with sigmoid activation and the output softmax layer.\nInput to the network is the concatenation of one-hot-vector\nencoding of the current word and the output from neurons in\ncontext layer at previous time-step. Learning rate annealing\nand early stopping techniques are used for efﬁcient training.\nEven though the preliminary results on toy tasks were\npromising, Mikolov et al. [Mikolov et al., 2010 ] conclude\nthat it is impossible for simple RNNs trained with gradient\ndescent to truly capture long context information, mainly\nbecause of the ‘Vanishing gradient’ and ‘Exploding gradient’\nproblems. One way to deal with this inefﬁcacy of gradient\ndescent approach to learn long-range context information in\nthe simple RNN is to use an enhanced learning technique.\nMartens et al. [Martens and Sutskever, 2011 ] have shown\nthat the use of HF optimization technique can solve the\nvanishing gradient problem in RNNs. In [Sutskever et al.,\n2011] Sutskever et al. propose a character-level language\nmodel, making use of the Hessian-free optimization to\novercome the difﬁculties associated with RNN training.\nAnother popular technique to solve the training difﬁculties\nwith the simple RNN is to modify the network to include\nthe ‘memory units’ that are specially structured to store\ninformation over longer periods. Such networks are termed\nas Long Short Term Memory (LSTM) networks. In[Zaremba\net al., 2014] Zaremba et al. makes use of a LSTM network\nfor language modeling (hereafter referred as LSTMLM).\nLSTMLM uses two LSTM layers as the hidden network.\nAs in the case of [Bengio et al., 2003], the network simul-\ntaneously learns the distributed word embeddings and the\nprobability function of the sequences of words represented in\nterms of these embeddings. Softmax output layer gives the\nprobability distribution. The network is trained by means of\nback-propagation and stochastic gradient descent. Learning\nrate annealing is used for the efﬁcient training. ‘Gradient\nClipping’ [Pascanu et al., 2013] is employed to prevent the\n‘exploding gradient’ problem. Zaremba et al. were also\nsuccessful in identifying and employing an efﬁcient way to\napply dropout regularization to the RNNs. Because of this,\nthe dropout-regularized LSTMLM was able to perform much\nbetter than the non-regularized RNN based language models.\nOur model is similar to the neural language model by\nZaremba et al. [Zaremba et al., 2014]. Instead of the stan-\ndard single cell LSTMs used by Zaremba et al., we use our\nmulti-cell LSTMs.\n3 Node structure in LSTM and the proposed\nmulti-cell LSTM node\nFigure 1 shows the structure of a LSTM node. Core of the\nLSTM node is the memory cell which stores its state. Infor-\nmation stored in the memory cell is updated with the help of\ntwo gates. The ‘Input gate’ regulates the amount of infor-\nmation that gets added into the cell state, whereas the ‘For-\nget gate’ controls the amount of information that is removed\nfrom the current cell state. The input, forget and output gates\nuse the sigmoid activation. The hyperbolic tangent of the cell\nstate, controlled by the ‘Output gate’ is given as the output of\nthe node.\nat = tanh (Wcxt + Ucht−1 + bc) (1)\nit = σ(Wixt + Uiht−1 + bi) (2)\nFigure 1: LSTM cell structural diagram\nft = σ(Wf xt + Uf ht−1 + bf ) (3)\not = σ(Woxt + Uoht−1 + bo) (4)\nct = it.at + ft.ct−1 (5)\nht = ot.tanh (ct) (6)\nThe equations (1) to (6) describe how a LSTM cell is\nupdated at every time-step t. In these equations, xt is the\ninput to the memory cell at time t. it, ft and ot are the input,\nforget and output gate values at time t. at is the modulated\ninput to the cell at time t. ct denotes the cell state value at\ntime t, where as ct−1 is the cell state value in the previous\ntime step. ht is the output of the cell at time t.\nBecause of this particular node structure, LSTMs are able\nto store the information for longer time-steps and hence can\nsolve the vanishing gradient problem. This makes LSTMs\na popular and powerful choice for langauge modeling\napplications, where there is a need to store long contextual\ninformation. However, the ‘exploding gradients’ problem\nstill prevails in the network. ’Gradient clipping’ [Pascanu et\nal., 2013] can be used to prevent this.\nFigure 2 gives the architecture of a multi-cell LSTM node.\nInstead of a single memory cell in a standard LSTM node,\neach multi-cell LSTM node holds mmemory cells. The gate\nvalues and the input to the cells are calculated as in the case of\nstandard LSTM node. These are then broadcast to the mem-\nory cells and each cell updates its values using equation (7).\nct\ni = it.at + ft.ct−1\ni (7)\nWhere ct\ni denotes the content of memory-cell i at time t.\nct−1\ni is the content of the memory-cell at time t− 1.\nUpdated cell values ﬂow into the selection module, where\nthey are combined or a single value is selected based on\nthe underlying strategy. Regulated by the output gate, the\nhyperbolic tangent of the selected/combined cell value moves\ninto output. The number of memory cells in each node,mis a\nhyper-parameter of the network that can be tuned while train-\ning. In the next section, we discuss various strategies that can\nbe used for the selection of a single value from the multi-cells.\nFigure 2: Multi-cell LSTM node\n4 Strategies for combining multiple memory\ncell values in a multi-cell LSTM node\nAs in the case of standard LSTM node, mutli-cell LSTM node\ntakes two inputs (xt, ht−1) and provides a single output (ht).\nTo get the single output from multiple memory cells, we need\nto have a selection mechanism. Here we discuss the different\nstrategies applied to select a particular memory cell/combine\nall the cell values, so as to pass a single value to the output.\nFrom the selection module, we obtain the effective cell value\n(ceff ). Output of the node is then computed using Equation\n8.\nht = ot.tanh (ct\neff ) (8)\n4.1 Simple Mean\nThe effective cell value, ceff is calculated as the mean value\nof the multiple memory cells in the node. Hyperbolic tangent\nof ceff regulated by the output gate forms the output of the\nnode.\nct\neff = 1\nm\nm∑\ni=1\nct\ni (9)\nwhere mis the number of memory cells in the node. ct\ni de-\nnotes the content of memory-cell iat time t.\n4.2 Weighted Sum\nEach cell is given a static weight. First cell is given the max-\nimum weighting and the other cells are weighted in decreas-\ning order with a constant decay from the previous cell weight.\nceff is calculated as the sum of these weighted cells.\nct\neff =\nm∑\ni=1\nct\ni.wi (10)\nwhere wi is the static weight for the ith memory cell.\n4.3 Random Selection\nFrom the m memory cells available, we randomly pick a\nmemory cell. This is our effective cell value (ceff ).\nct\neff = Random(Ct) (11)\nwhere Ct is a one dimensional vector holding the values of\nall memory cells in the node at time t.\n4.4 Max Pooling\nSimilar to the ‘max pooling’ layer used in Convolutional Neu-\nral Networks (CNN), selection mechanism of the node picks\nthe maximum value stored in its memory cells. This is the\neffective cell value of the node.\nct\neff = Max(Ct) (12)\n4.5 Min-Max Pooling\nWe deﬁne a threshold value for output gate. If the value is less\nthan the threshold, we take the minimum of the cell values.\nWe go with the maximum of cell values, if the gate value is\ngreater than the threshold.\nct\neff =\n{Min(Ct), if ot <othr\nMax(Ct), otherwise (13)\nwhere othr is the threshold value for the output gate. It is a\nhyper parameter that can be tuned by training.\n4.6 Learnable Weights for the cells\nWe associate each memory cell with a trainable weight that\ngets updated in the backward pass. Now we can combine all\nthe weighted cells or select a particular cell by applying any\nof the above strategies. Here we demonstrate it with the ‘Max\nPooling’ technique. Maximum of the weighted cell values is\ntaken as the effective cell value.\nct\neff = Max(Ct.Wt\ncell) (14)\nwhere Wt\ncell is the learned weights for the memory cells at\ntime t.\n5 Network Architecture\nArchitecture of our network is same as the one used by\nZaremba et al. [Zaremba et al., 2014] for language model-\ning. Input is given through an embedding layer having the\nsame size as that of the hidden layers. It is followed by two\nhidden layers comprising of LSTM nodes. Fully connected\nsoftmax layer gives the output probability distribution.\n6 Results and Observations\nHere we discuss in detail about the datasets, experimental\nsetup, results obtained and the comparisons with the results\nof Zaremba et al. [Zaremba et al., 2014].\n6.1 Dataset\nPenn Tree Bank, popularly known as the PTB dataset [Mar-\ncus et al., 1994] was used for the experiments. Originally a\ncorpus of English sentences with linguistic structure annota-\ntions, it is a collection of 2,499 stories sampled from the three\nyear Wall Street Journal (WSJ) collection of 98,732 stories.\nA variant of the original dataset, distributed with the Chainer\npackage [Tokui et al., 2015] for Python was used for our ex-\nperiments. The corpus with a vocabulary of 10000 words\nis divided into training, test and validation sets consisting of\n929k, 82k and 73k words respectively.\n6.2 Evaluation Metric\nPerplexity measure was used as the performance evaluation\nmetric. Most popular performance metric for the language\nmodeling systems, Perplexity is a measure of how well the\nprobability model or the probability distribution predicts a\nsample. Lower the Perplexity, better the model is. A low\nperplexity indicates that the model is good at predicting the\nsample. The perplexity of a probability distribution is deﬁned\nas follows:\nPerplexity = 2H(p) = 2−∑\nx p(x)log2p(x) (15)\nwhere, H(p) is the entropy of the distribution and the x\nranges over the events.\n6.3 Experimental Setup\nMulti-cell LSTM based language models (hereafter referred\nas MLSTM-LM) of three different sizes were trained. These\nare denoted as the small MLSTM-LM, medium MLSTM-LM\nand the large MLSTM-LM. All of these have two layers of\nmulti-cell LSTMs and are unrolled for 35 steps. We follow\nthe conventions used by Zaremba et al. in [Zaremba et al.,\n2014]. Hidden states are initialized to 0. We also ensure the\nstatefulness of the model by which the ﬁnal hidden state of\nthe current mini-batch is used as the initial hidden state of\nthe subsequent mini-batch. Mini-batch size is kept as 20. As\nin [Zaremba et al., 2014], we apply dropout on all the con-\nnections other than the recurrent connections in LSTM layers.\nThe small MLSTM-LM has 200 nodes per hidden layer.\nDropout rate of 0.4 is used for these networks. Initial\nlearning rate of 1 is used. The medium MLSTM-LM has 650\nnodes in the hidden layers. Dropout rate of 0.5 and initial\nlearning rate of 1.2 were used. For the large MLSTM-LMs\nhaving 1500 nodes per hidden layer, initial learning rate of\n1.2 and dropout rate of 0.65 were used. All the models use\nthe Algorithm 1 for learning rate annealing.\nAt the end of each epoch, perplexity is computed for the\nvalidation set. This is used to anneal the learning rate. Learn-\ning rate annealing algorithm is run after each epoch. We de-\nﬁne ‘epochs to wait’as the number of epochs to wait for an\nimprovement in the perplexity, before annealing the learning\nrate. ‘minimum reduction’is the threshold improvement in\nperplexity from the previously recorded perplexity value to\nconsider an improvement. ‘minimum learning rate’deﬁnes\nthe lower bound for learning rate annealing. ‘given chances’\nholds the number of epochs since last improvement in the\nmetric. If there is no considerable improvement for perplex-\nity from the previous epoch, ‘given chances’is incremented.\nIf ‘given chances’crosses the ‘epochs to wait’, we reduce the\nlearning rate by multiplying it with the decay factor.\nWe deﬁne ’Learning rate decay’as 0.5, ’epochs to wait’as\n2, ’minimum reduction’as 2 and ’minimum learning rate’as\n0.0001.\n6.4 Results and Discussion\nExperiments were conducted with all three models, choos-\ning different selection strategies listed in Section 4. Hyper\nAlgorithm 1:Procedure: Learning rate annealing\nInput: Learning rate decay, minimum learning rate,\nminimum reduction, epochs to wait\nOutput: updated learning rate\nbegin\nif current perplexity is greater than (previous\nperplexity- minimum reduction)then\nif given-chances less than the epochs to wait\nthen\nIncrement given-chances.\nelse\nnew learning rate = max(minimum learning\nrate, learning rate * learning rate decay)\ngiven-chances =0\nend\nelse\ngiven-chances = 0\nend\nReturn updated learning rate\nend\nparameters; including the number of memory cells per node;\nwere ﬁne-tuned with the help of repeated runs. Table 1 lists\nthe performance of Large MLSTM-LM under different selec-\ntion strategies. All these models have 10 memory cells per\nLSTM node. As we can see, the selection strategies namely\nSimple Mean, Random Selection, Max Pooling and the Min-\nMax Pooling gives almost same results, whereas the other\ntwo strategies; Weighted Sum and Learnable Weights for the\ncells; do not give comparable results. We obtain the top per-\nforming model when the ‘Max Pooling’ strategy is used.\nSelection Strategy Validation\nPerplexity\nTest\nPerplexity\nSimple Mean 81.27 77.6\nRandom Selection 81.52 77.46\nMax Pooling 80.62 77.12\nMin-Max Pooling 81.12 77.53\nTable 1: Comparison of the multi-cell selection strategies\nTable 2 presents the experiments to choose the optimum\nnumber of memory cells. All these experiments were carried\nout with the Large MLSTM-LM network, using ‘Max\nPooling’ strategy. Models perform more or less in a similar\nfashion despite the change in number of memory cells. We\nrecorded the lowest perplexity with the model having 10\nmemory cells per LSTM node.\nTable 3 presents the performance of top models. As\nevident from the table, MLSTM-LM models outperform\nthe models from [Zaremba et al., 2014 ] on both validation\nand test sets. While replicating the test of Zaremba et al.\nusing our learning rate annealing algorithm; instead of their\noriginal strategy; we obtain better results as compared to the\nresults reported by them in [Zaremba et al., 2014].\nFigure 3 gives the Epoch v/s Validation set perplexity plot\nfor the top MLSTM-LM and replicated result of Large reg-\nularized LSTM model by Zaremba et al. [Zaremba et al.,\n2014]. Figure 4 is the Epoch v/s Validation set perplexity plot\nfor small, medium and large MLSTM-LM models. While\nZaremba et al. [Zaremba et al., 2014 ] reports the satura-\ntion of large model at Epoch 55 and the medium model at\nEpoch 39, all of our models attain saturation well before that.\nAll of our models were able to outperform the models from\n[Zaremba et al., 2014] with a training time of approximately\n30 epochs. Interestingly the replication of Zaremba’s experi-\nment on the large regularized LSTM model using our learning\nrate annealing algorithm also saturates early. This also shows\nthe effectiveness of our annealing algorithm.\nNumber of\nMemory-Cells\nValidation\nPerplexity\nTest\nPerplexity\n2 81.41 77.7\n3 81.18 77.32\n4 80.89 77.47\n5 81.42 77.9\n6 81.17 77.17\n7 80.99 77.43\n8 81.16 77.43\n9 81.72 77.62\n10 80.62 77.12\n15 81.71 77.56\n30 81.61 77.3\n50 81.49 77.69\nTable 2: Model performance on varying the number of memory cells\nin the nodes\nModel Number\nof nodes\nin LSTM\nlayers\nValidation\nPer-\nplex-\nity\nTest\nPer-\nplex-\nity\nResults by Zaremba et al. [Zaremba et al., 2014]\nNon-regularized LSTM 200 120.7 114.5\nMedium-regularized\nLSTM\n650 86.2 82.7\nLarge regularized LSTM 1500 82.2 78.4\nTop models from experiments\nSmall MLSTM-LM 200 94.5 89.39\nMedium MLSTM-LM 650 83.88 79.95\nLarge MLSTM-LM 1500 80.62 77.12\nLarge regularized\nLSTM(Replicated)\n1500 81.87 77.85\nRegularized\nLSTM(Replicated)\n200 103.216 99.19\nTable 3: Comparison of the top models with the results of Zaremba\net.al.\n5 10 15 20 25 30 35 40 45\n80\n90\n100\n110\n120\nEpochs\nValidation Perplexity\nEpoch v/s Validation Perplexity Plot\nMLSTM-LMLSTM-LM\nFigure 3: Epoch v/s Validation Perplexity plot for LSTM-LM and\nMLSTM-LM\n0 10 20 30 40 50 60 70\n80\n90\n100\n110\n120\n130\nEpochs\nValidation Perplexity\nEpoch v/s Validation Perplexity Plot\nLarge MLSTM-LMMedium MLSTM-LMSmall MLSTM-LM\nFigure 4: Epoch v/s Validation Perplexity plot for Small, Medium\nand Large MLSTM-LMs\n6.5 Miscellaneous Experiments\nExperiments were also conducted to check the performance\nof the models on varying number of hidden layers and em-\nbedding size. Table 4 summarizes the experiments with dif-\nferent number of hidden layers. Figure 5 gives the Epoch v/s\nValidation perplexity plot for the models. Even though we\nhave a fairly good result with a single hidden layer model,\nwe obtain the optimum result with the network having two\nhidden layers. Performance deteriorates when more hidden\nlayers are added. It might be an indication of the requirement\nfor better regularization techniques as we add more hidden\nlayers. More research is required on this and we leave it for\nfuture work.\nTable 5 describes the experiments conducted to study\nthe effect of embedding size on the model performance.\nSmall and Large MLSTM-LMs were used for the study. As\nevident from the table, both the models give best results\nwhen the embedding size equals the number of nodes in\nthe hidden layers. Performance of the models deteriorates\nas we use embedding size different from the hidden layer size.\n8 10 12 14 16 18 20 22 24 26 28 30 32\n80\n100\n120\n140\n160\n180\n200\nEpochs\nValidation Perplexity\nEpoch v/s Validation Perplexity Plot\nHidden Layer = 1Hidden Layer = 2Hidden Layer = 4\nFigure 5: Epoch v/s Validation Perplexity plot for MLSTM-LMs\nwith different number of hidden layers\nExperiments were also conducted to study the effects of\nweight-decay and max-norm regularization techniques. Sri-\nvastava et al. [Srivastava et al., 2014] suggest that the max-\nnorm regularization which thresholds the L2 norm of the\nweight vectors between 3 and 4 enhances the performance\nof dropout-regularized networks. But we were unable to ob-\ntain any considerable improvement in the model performance\nwith the max-norm regularization. As reported by Mikolov\net al. [Mikolov et al., 2010 ], regularization of network to\npenalize large weights by weight-decay did not provide any\nsigniﬁcant improvements.\n7 Conclusion\nIn this paper, we have proposed the multi-cell LSTM archi-\ntecture and discussed the various selection strategies to select\na particular memory cell or combine all the memory cells of\na node. We have applied the model successfully for language\nmodeling. Effectiveness of different selection strategies and\nHidden Layers Validation\nPerplexity\nTest\nPerplexity\n1 84.50 80.91\n2 80.62 77.12\n4 91 87.2\nTable 4: Model performance on varying the number of hidden layers\nModel Units Embedding\nsize\nValidation\nPerplexity\nTest Per-\nplexity\nSmall 200 300 127.21 123\nSmall 200 600 124.51 121.4\nSmall 200 200 94.5 89.39\nLarge 1500 300 102.92 99\nLarge 1500 600 95.07 92.38\nLarge 1500 1500 80.62 77.12\nTable 5: Perplexity of the models on varying the embedding size\nthe effect of varying the number of memory cells on model\nperformance were also studied. Our MLSTM-LM models\nwere able to outperform the state-of-the-art Zaremba’s\nmodels and attain saturation early as compared to the results\nreported by Zaremba et al. [Zaremba et al., 2014].\nMulti-cell LSTMs need to be investigated more for its per-\nformance on the other applications of standard LSTMs. We\nhave explored only the ‘maximum’ and ‘average’ functions\nfor selecting a particular value out of the multi-cells. Fur-\nther ideas from signal theory can also be used with the use of\nlinear ﬁlters, as average is also a linear ﬁlter. MLSTM-LMs\nshould be applied on the various language modeling applica-\ntions like speech recognition in order to measure its practical-\nity.\nReferences\n[Arisoy et al., 2012] Ebru Arisoy, Tara N. Sainath, Brian\nKingsbury, and Bhuvana Ramabhadran. Deep neural net-\nwork language models. InProceedings of the NAACL-HLT\n2012 Workshop: Will We Ever Really Replace the N-gram\nModel? On the Future of Language Modeling for HLT,\nWLM ’12, pages 20–28, Stroudsburg, PA, USA, 2012. As-\nsociation for Computational Linguistics.\n[Bengio and Bengio, 2000] S. Bengio and Y . Bengio. Taking\non the curse of dimensionality in joint distributions using\nneural networks. Trans. Neur. Netw., 11(3):550–557, May\n2000.\n[Bengio et al., 1994] Y . Bengio, P. Simard, and P. Frasconi.\nLearning long-term dependencies with gradient descent is\ndifﬁcult. Trans. Neur. Netw., 5(2):157–166, March 1994.\n[Bengio et al., 2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Janvin. A neural probabilis-\ntic language model. J. Mach. Learn. Res., 3:1137–1155,\nMarch 2003.\n[Elman, 1990] Jeffrey L. Elman. Finding structure in time.\nCognitive Science, 14(2):179–211, 1990.\n[Gers F. et al., 1999] A. Gers F., J. Schmidhuber, and\nF. Cummins. Learning to forget: Continual prediction with\nlstm. Technical report, 1999.\n[Goodman, 2001] Joshua Goodman. A bit of progress in lan-\nguage modeling. CoRR, cs.CL/0108005, 2001.\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. Long short-term memory. Neural\nComput., 9(8):1735–1780, November 1997.\n[Jelinek and Mercer, 1980] Fred Jelinek and Robert L. Mer-\ncer. Interpolated estimation of Markov source parameters\nfrom sparse data. In Edzard S. Gelsema and Laveen N.\nKanal, editors, Proceedings, Workshop on Pattern Recog-\nnition in Practice, pages 381–397. North Holland, Ams-\nterdam, 1980.\n[Manning and Sch¨utze, 1999] Christopher D. Manning and\nHinrich Sch ¨utze. Foundations of Statistical Natural Lan-\nguage Processing. MIT Press, Cambridge, MA, USA,\n1999.\n[Marcus et al., 1994] Mitchell Marcus, Grace Kim,\nMary Ann Marcinkiewicz, Robert MacIntyre, Ann\nBies, Mark Ferguson, Karen Katz, and Britta Schasberger.\nThe penn treebank: Annotating predicate argument struc-\nture. In Proceedings of the Workshop on Human Language\nTechnology, HLT ’94, pages 114–119, Stroudsburg, PA,\nUSA, 1994. Association for Computational Linguistics.\n[Martens and Sutskever, 2011] James Martens and Ilya\nSutskever. Learning recurrent neural networks with\nhessian-free optimization. In ICML, 2011.\n[Mikolov et al., 2010] Tomas Mikolov, Martin Karaﬁat,\nLukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Re-\ncurrent neural network based language model. In INTER-\nSPEECH, pages 1045–1048. ISCA, 2010.\n[Pascanu et al., 2013] Razvan Pascanu, Tomas Mikolov, and\nYoshua Bengio. On the difﬁculty of training recurrent\nneural networks. In Proceedings of the 30th Interna-\ntional Conference on International Conference on Ma-\nchine Learning - Volume 28, ICML’13, pages III–1310–\nIII–1318. JMLR.org, 2013.\n[Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton,\nAlex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: A simple way to prevent neural networks\nfrom overﬁtting. J. Mach. Learn. Res., 15(1):1929–1958,\nJanuary 2014.\n[Sutskever et al., 2011] Ilya Sutskever, James Martens, and\nGeoffrey Hinton. Generating text with recurrent neural\nnetworks. In Lise Getoor and Tobias Scheffer, editors,\nProceedings of the 28th International Conference on Ma-\nchine Learning (ICML-11), ICML ’11, pages 1017–1024,\nNew York, NY , USA, June 2011. ACM.\n[Tokui et al., 2015] Seiya Tokui, Kenta Oono, Shohei Hido,\nand Justin Clayton. Chainer: a next-generation open\nsource framework for deep learning. In Proceedings of\nWorkshop on Machine Learning Systems (LearningSys) in\nThe Twenty-ninth Annual Conference on Neural Informa-\ntion Processing Systems (NIPS), 2015.\n[Zaremba et al., 2014] Wojciech Zaremba, Ilya Sutskever,\nand Oriol Vinyals. Recurrent neural network regulariza-\ntion. CoRR, abs/1409.2329, 2014.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9478728771209717
    },
    {
      "name": "Computer science",
      "score": 0.8633463382720947
    },
    {
      "name": "Language model",
      "score": 0.8123894333839417
    },
    {
      "name": "Recurrent neural network",
      "score": 0.8106827735900879
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6369922161102295
    },
    {
      "name": "Node (physics)",
      "score": 0.4815102517604828
    },
    {
      "name": "Artificial neural network",
      "score": 0.4810114800930023
    },
    {
      "name": "Natural language processing",
      "score": 0.45269519090652466
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4521430432796478
    },
    {
      "name": "Language understanding",
      "score": 0.4376221001148224
    },
    {
      "name": "Parsing",
      "score": 0.07844176888465881
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 4
}