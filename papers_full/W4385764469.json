{
  "title": "Sign Language-to-Text Dictionary with Lightweight Transformer Models",
  "url": "https://openalex.org/W4385764469",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2990194847",
      "name": "Jérôme Fink",
      "affiliations": [
        "Namur Institute of Language, Text and Transmediality",
        "University of Namur",
        "Namur Digital Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4298611473",
      "name": "Pierre Poitier",
      "affiliations": [
        "University of Namur",
        "Namur Digital Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2344062840",
      "name": "Maxime André",
      "affiliations": [
        "University of Namur",
        "Namur Digital Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2122351284",
      "name": "Loup Meurice",
      "affiliations": [
        "University of Namur",
        "Namur Digital Institute"
      ]
    },
    {
      "id": "https://openalex.org/A297991091",
      "name": "Benoît Frénay",
      "affiliations": [
        "Namur Digital Institute",
        "University of Namur"
      ]
    },
    {
      "id": "https://openalex.org/A2133957942",
      "name": "Anthony Cleve",
      "affiliations": [
        "Namur Digital Institute",
        "University of Namur"
      ]
    },
    {
      "id": "https://openalex.org/A2095850004",
      "name": "Bruno Dumas",
      "affiliations": [
        "University of Namur",
        "Namur Digital Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1986727092",
      "name": "Laurence Meurant",
      "affiliations": [
        "University of Namur",
        "Namur Institute of Language, Text and Transmediality"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2336188181",
    "https://openalex.org/W3031252323",
    "https://openalex.org/W1643761381",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W1975275129",
    "https://openalex.org/W3108425892",
    "https://openalex.org/W2399378618",
    "https://openalex.org/W4297803663",
    "https://openalex.org/W2016940542",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W4288327876",
    "https://openalex.org/W3152762260",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2903314716",
    "https://openalex.org/W2921243003",
    "https://openalex.org/W2936273214",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3080888993",
    "https://openalex.org/W4287815957",
    "https://openalex.org/W2109606373"
  ],
  "abstract": "The recent advances in deep learning have been beneficial to automatic sign language recognition (SLR). However, free-to-access, usable, and accessible tools are still not widely available to the deaf community. The need for a sign language-to-text dictionary was raised by a bilingual deaf school in Belgium and linguist experts in sign languages (SL) in order to improve the autonomy of students. To meet that need, an efficient SLR system was built based on a specific transformer model. The proposed system is able to recognize 700 different signs, with a top-10 accuracy of 83%. Those results are competitive with other systems in the literature while using 10 times less parameters than existing solutions. The integration of this model into a usable and accessible web application for the dictionary is also introduced. A user-centered human-computer interaction (HCI) methodology was followed to design and implement the user interface. To the best of our knowledge, this is the first publicly released sign language-to-text dictionary using video captured by a standard camera.",
  "full_text": "Sign Language-to-Text Dictionary with Lightweight Transformer Models\nJ´erˆome Fink1,2,3 , Pierre Poitier1,3 , Maxime Andr´e1,3 , Loup Meurice1,3 , Benoˆıt\nFr´enay1,3 , Anthony Cleve1,3 , Bruno Dumas1,3 , Laurence Meurant2,3\n1Namur Digital Institute (NaDI)\n2Namur Institute of Language, Text and Transmediality (NaLTT)\n3University of Namur\n{jerome.fink, pierre.poitier, maxime.andre, loup.meurice, benoit.frenay, anthony.cleve, bruno.dumas,\nlaurence.meurant}@unamur.be\nAbstract\nThe recent advances in deep learning have been\nbeneficial to automatic sign language recognition\n(SLR). However, free-to-access, usable, and acces-\nsible tools are still not widely available to the deaf\ncommunity. The need for a sign language-to-text\ndictionary was raised by a bilingual deaf school\nin Belgium and linguist experts in sign languages\n(SL) in order to improve the autonomy of students.\nTo meet that need, an efficient SLR system was\nbuilt based on a specific transformer model. The\nproposed system is able to recognize 700 different\nsigns, with a top-10 accuracy of 83%. Those results\nare competitive with other systems in the literature\nwhile using 10 times less parameters than existing\nsolutions. The integration of this model into a us-\nable and accessible web application for the dictio-\nnary is also introduced. A user-centered human-\ncomputer interaction (HCI) methodology was fol-\nlowed to design and implement the user interface.\nTo the best of our knowledge, this is the first pub-\nlicly released sign language-to-text dictionary us-\ning video captured by a standard camera.\n1 Introduction\nThe rise of deep learning [LeCun et al., 2015] led to the cre-\nation of successful methods to process unstructured data such\nas images, videos or texts. These achievements are reflected\nin sign language recognition (SLR). The field has gained in\npopularity [Koller, 2020] as it provides a challenging bench-\nmark for gesture or poses recognition. Indeed, to correctly\nclassify signs, a model should be able to grasp facial expres-\nsions and precise hand gestures [Stokoe, 1972]. Moreover,\nthere is a clear societal dimension for such technologies, such\nas the sign language-to-text dictionary which is proposed here\nto help the deaf community.\nTechnological advances alone cannot explain the success\nof SLR. In the past decades, linguists began to have access\nto affordable storage and recording devices. It facilitated the\nstudy of sign languages (SL) and has encouraged several re-\nsearch teams to create digital sign language corpora. In the\nmeantime, the expansion of smartphones and social networks\nled to the creation of groups on social media platforms in\nwhich deaf users can share SL vocabulary or communicate\nonline. The increasing availability of sign language (SL) data\nallows machine learning (ML) researchers to exploit those\ncorpus [Fink et al., 2021] or crowdsource [Vaezi Joze and\nKoller, 2019] social media platforms to build large-scale SL\ndatasets suitable for deep learning.\nDespite those advances, few tools are available to the deaf\ncommunity. Initiatives led to the creation of lexicons for sign\nlanguage enabling to search for a sign corresponding to a\nwritten word1. However, the opposite is not possible as those\ntool does not offer a search from a sign to a written word.\nThis work proposes to enhance those tools by providing a dic-\ntionary searchable via a webcam recording. This dictionary\nis, to the best of our knowledge, the first publicly available\nsign language-to-text dictionary 2 using only video informa-\ntion from a simple webcam to identify the sign.\nThe overall process leading to the creation and use of our\ndictionary is summarized in Figure 1. A corpus of French\nBelgian Sign Language (LSFB) built by a team of linguists\nfrom the LSFB laboratory (LSFB Lab) of Namur [Meurant,\n2015] is used as a database for the system. A cleaned version\nof the corpus [Fink et al., 2021] is used as a dataset for the\nmachine learning pipeline. This paper focuses on the creation\nof a lightweight model for SLR using an architecture similar\nto the one introduced by Vision Transformer (ViT) [Dosovit-\nskiy et al., 2021]. In addition, the integration of the result-\ning model into a web application is also presented. A user-\ncentered approach is followed for ensuring the stakeholder’s\nrequirements meeting on the resulting dictionary. This en-\nsures that our tool will actually be useful to the deaf commu-\nnity, as confirmed by its quick adoption after its public release\nin October 2022.\nThis paper is organized as follows. Section 2 introduces\nthe stakeholders of the SLR system along with its require-\nments. Then, Section 3 discusses the research in SLR. Sec-\ntion 4 gives more information about the dataset used in this\nwork and its specificities. Section 5 describes the architec-\nture developed for the dictionary and reports results for var-\nious architectural choices. A quantitative evaluation of the\nbest-performing model is reported. Section 6 explains how\n1auslan.org.au\n2dico.corpus-lsfb.be\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5968\nUser\nWeb\nApp\n(4) Sign capture\n(5) Querying\n(6) Translation\nThis work\nLSFB Corpus\nLinguists\n(1) Recording & Annotating\nMeurant et al., 2015\n(2) Cleaning Cleaned SL \nDataset\n(3) Model training\nFink et al., 2021\nSL\nRecognition\nModel\nFigure 1: The high-level processes that lead to the creation and manipulation of the bidirectional sign language dictionary. (1)\nThe LSFB Lab collected and annotated a large corpus of French Belgian Sign Language (LSFB) [Meurant, 2015]. (2) The\ncorpus was preprocessed and cleaned to create a sign language dataset [Fink et al., 2021]. (3) The dataset is used to train our\nSLR model. (4) An interface was built to capture the user’s signs and use them to query the dictionary (5). The dictionary\nproposes possible translations to the user along with definitions and usage examples in text and in video (6).\nthe web application integrating the model was designed, im-\nplemented and evaluated using a user-centered approach. Fi-\nnally, Section 7 concludes and discusses future works.\n2 Stakeholders and Requirements\nIt is important to notice that sign languages are not universal\nand may vary depending on the country or region. The system\npresented in this paper focuses on the French Belgian Sign\nLanguage (LSFB). Nevertheless, the overall process followed\nto build the system is transferable to any sign language (SL),\nprovided that the amount of available data is sufficient.\nOur project was initiated by the French Belgian Sign Lan-\nguage Laboratory (LSFB Lab) of Namur, where linguists\nhave been working on the LSFB since early 2000. They col-\nlected videos of SL conversations to better study and char-\nacterize the language. They also released a text-to-sign lan-\nguage lexicon. The LSFB Lab collaborates with Sainte-\nMarie, a bilingual French and LSFB school located in Na-\nmur. The creation of a sign language-to-text dictionary could\nimprove the autonomy of deaf students. Thus, the school was\ninterested and involved in the creation of the interface.\nDiscussions with the stakeholders allowed us to gather re-\nquirements for the application. First, the system should be\nrobust to variations. The users are not expected to stand in a\ncontrolled environment with uniform background and light-\nning or to wear specific clothing. Also, skin color and any\nother physical characteristics should have no influence.\nThe system should not rely on expensive, impractical or\nhard-to-find hardware. Thus, the dictionary should only rely\non video captured by a standard webcam that can be found on\nlaptops or smartphones. The association hosting the system\ncannot afford a server with GPUs. Thus, the algorithm must\nrun efficiently on CPU only. Finally, the system should an-\nswer in less than 10 seconds to a query. This ensures that the\ninterface is fluid and not frustrating to use.\n3 Related Work\nSign language recognition is gaining in popularity in machine\nlearning [Koller, 2020]. Continuous SLR aims to translate\nSL sentences directly into text, while isolated SLR focuses\non classifying a single sign. This section focuses on isolated\nsign language recognition using RGB data, as our system can\nonly rely on raw videos for its predictions and its aim is not\nto recognize and translate entire sentences.\nThe first vision-based SLR systems relied on handcrafted\nfeatures like the work of [Huang and Huang, 1998 ] us-\ning Otsu thresholding to isolate the hands. Those methods\nwere only capable of recognizing a limited number of signs\n(< 100) from a few signers (<5). The use of sequential mod-\nels such as Hidden Markov Models led to the first system able\nto recognize larger sign vocabulary like in the work of[Kadir\net al., 2004] that achieved 92% accuracy for 164 signs. By\nusing dynamic time warping, [Wang et al., 2012 ] achieve\nimpressive results with 78% top-10 accuracy on 1,113 signs\nusing 20 frames and meta-information about the number of\nhands used to perform the sign and the handedness of the\nsigners. However, those systems are sensitive to changes in\nlighting, background and signer variations.\nThe success of convolutional neural networks (CNN) for\ncomputer vision along with the development of large pub-\nlic datasets for sign language allowed the creation of algo-\nrithms robust to variability in the input data. A CNN-based\nmethod [Pigou et al., 2016] was able to classify a vocabulary\nof 100 signs performed by 78 different signers with a top-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5969\n1 accuracy of 60% and a top-10 of 90%. The development\nof sequential models allows leveraging the temporal infor-\nmation in sign language videos. The MS-ASL dataset was\nbenchmarked [Vaezi Joze and Koller, 2019] on several archi-\ntectures such as CNN+LSTM and I3D networks with a top-1\naccuracy of 81% for 1,000 signs and 222 signers. Recently,\ntransformer networks proved to be efficient in sign language\nrecognition. A transformer-based architecture achieved 73%\naccuracy on a vocabulary of 100 signs performed by 67 sign-\ners by mixing frame information with skeleton metadata ex-\ntracted from the videos [De Coster et al., 2020].\nIn parallel, advances in pose estimation led to the creation\nof valuable tools for preprocessing sign language videos.\nOpenPose [Cao et al., 2019] and MediaPipe [Lugaresi et al.,\n2019] provide easy-to-use models to extract skeletons land-\nmarks from raw RGB videos. Those skeletons are often used\nas a preprocessing step in SLR [Konstantinidis et al., 2018].\nThis work follows this trend by leveraging landmarks.\nSince their creation, transformer-based architec-\ntures [Vaswani et al., 2017 ] have proven successful on\ntasks such as image classification with the vision transformer\n(ViT) [Dosovitskiy et al., 2021]. This work investigates the\nadaptation of such architectures for isolated SLR.\n4 Dataset\nOur SLR algorithm is trained on one of the largest sign lan-\nguage datasets in the world: the French Belgian Sign Lan-\nguage (LSFB) dataset [Fink et al., 2021]. It is made of 50\nhours of video, including37 hours manually annotated by lin-\nguists from the LSFB Lab. Those videos depict natural dis-\ncussions in LSFB between two individuals. In total,100 sign-\ners participated in the recording sessions. Videos are recorded\nin a studio with controlled lighting and camera position. For\neach discussion, two videos are recorded, each focusing on\none of the two signers.\nLSFB-ISOL. The dataset exists in two versions: (i) LSFB-\nCONT which contains continuous videos of the whole LSFB\ndiscussions and (ii) LSFB-ISOL in which all the signs are iso-\nlated in shorter videos extracted from the continuous videos.\nOnly LSFB-ISOL is used here as this paper does not focus\non continuous SLR but rather on the recognition of isolated\nsigns. Resulting videos only contain a single sign with an as-\nsociated label. In total, LSFB-ISOL contains 4,181 different\nsigns that are performed by the 100 signers. In this work,\nthose labels are filtered to only keep the ones associated with\nFrench translations in the LSFB dictionary and having more\nthan 20 examples. This leads to a filtered dataset with 700\nlabels and 77,900 instances.\nThe LSFB dataset is challenging as signers are free to dis-\ncuss without vocabulary or rhythm constraints. In this con-\ntext, signers tend to sign more quickly and signs overlap.\nThus, the start position of each sign depends on the previous\none.\nPose Features. The dictionary uses pose data extracted\nfrom frames with MediaPipe [Lugaresi et al., 2019 ]. As\nshown in Figure 2, a pose contains65 landmarks for the body\npose (23) and the hands (2 × 21). As each landmark is made\nof an x and y component, each pose contains 130 features in\ntotal.\nFigure 2: A frame sampled from the LSFB dataset along with\nits corresponding pose extracted using MediaPipe.\nMultiple reasons motivate the use of poses instead of di-\nrectly using the RGB frames:\n(i) Less information is contained in a pose. An RGB frame\nof size 224x224 contains 150k values while a pose of\n65 2D coordinates only contains 130 values. This rep-\nresents a significantly smaller feature space that is easier\nto work with.\n(ii) Some biases appear in the LSFB datasets, e.g., the uni-\nform background and controlled lightning. This can\ncause bias if the training is performed directly on the\nframes. However, the poses are extracted with Medi-\naPipe which is trained with respect to guidelines that\nprevent issues such as physical biases (background,\nlight condition, etc.) and ethical biases (morphology,\ngender, skin color, etc.) [Lugaresi et al., 2019]. There-\nfore, this paper “delegates” some potential biases to Me-\ndiaPipe by using poses.\n(iii) Poses only contains information about the joints of the\nsigner. Therefore, irrelevant information, e.g., the color\nof the clothes, is not used to make the prediction. This\nprevents overfitting by filtering information. It also\nmakes the model robust to those variations by design.\nFeatures are processed to avoid a discontinuity in pose se-\nquences and to mitigate vibrations caused by a lack of preci-\nsion in the pose estimation. Linear interpolation is used to fill\nin missing values. Then, a filter[Savitzky and Golay, 1964] is\nused with a moving window of size 7 and a polynomial order\nof 2 to smooth values and thus mitigate vibrations.\n5 Model Design\nThis section introduces the SLR model integrated to the dic-\ntionary. First, the overall architecture is described and re-\nsults are reported for various meta-parameters. The best-\nperforming model is discussed and other results found in the\nliterature are reported.\n5.1 Model Architecture\nThe success of transformer-based architectures in computer\nvision motivates their use for the challenging task of SLR.\nAs the target is a specific class (i.e., type of sign) for a se-\nquence of frames constituting a sign, the decoder part of the\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5970\ntransformer architecture [Vaswani et al., 2017] is not useful\nin our case. Instead, the architecture is inspired by the vi-\nsion transformer (ViT) [Dosovitskiy et al., 2021] for image\nclassification. Figure 3 shows the high-level architecture of\nour sign language classifier. The linear embedding reduces\nthe dimensionality of the input data before applying a posi-\ntional encoding on each token. The positional encoding is\na 1D trainable vector added to each input token. A classi-\nfication token is added to the sequence as introduced in the\nViT paper. This token is then passed as input to the multi-\nlayer perceptron (MLP) containing a normalization layer [Ba\net al., 2016] followed by a linear layer in order to predict a\nlabel for the sequence. The detailed architecture for the two\nother components is discussed in the following sections.\n5.2 Training Setup\nThis section presents the training setup used to create our\nmodels. The filtered LSFB-Isol dataset presented in Section 4\nis used, with a total of 77,900 instances and a vocabulary of\n700 signs. The dataset is split into a training set containing\n70% of the data and a test set containing the remaining. The\nsigners appearing in the training set are not in the test set, to\nassess the ability of the model to deal with new signers. The\nMediaPipe landmarks are extracted from each clip. Only the\nlandmarks are provided as input to our model, i.e., there are\n130 input features. The raw video frames are not used.\nAll the models are trained using the same training scheme.\nThe optimizer is a SGD with a learning rate of 2 × 10−3 and\na momentum of 0.9. The loss function is the classical cross-\nentropy loss. The models are trained for 600 epochs. As\nrecommended by [Vaswani et al., 2017], a warmup phase is\nperformed. A linear warmup is applied during the first 200\nepochs. The batch size is set to 128. The metric used to com-\npare each model is the standard accuracy. The clip sequences\nexceeding the maximal sequence length are cropped and the\nones that are shorter are masked.\n5.3 Transformer Encoder Architecture\nA transformer encoder is made of one or several encoder lay-\ners containing a multi-head attention layer and a feed-forward\nnetwork [Vaswani et al., 2017]. The number of encoder lay-\ners and attention heads has an influence on the performance\nand complexity of the model. To determine the transformer\nencoder architecture for our SLR model, a grid search on sev-\neral meta-parameters was performed (see Table 1). The max-\nimal length of signs sequences is set to50 and the embedding\nsize of the tokens is set to96. In total, 16 configurations were\nconsidered and the results are reported in Table 2.\nNumber of attention heads 2, 4, 8, 16\nNumber of encoder layers 1, 2, 4, 6\nTable 1: The meta-parameters considered during the grid\nsearch for the transformer encoder architecture (see Table 2).\nOn the training set, the accuracy score rises as the model\ncomplexity increases, but it is not the case with the test accu-\nracy. It can be observed that models quickly overfit when they\nare more complex. The best performances are obtained with\nNb. layers Nb. heads Train acc. Test acc.\n2 61.2% 50.7%\n4 67.2% 51.3%\n8 66.4% 44.9%1\n16 68.0% 45.3%\n2 79.4% 51.6%\n4 80.7% 51.9%\n8 81.3% 47.3%2\n16 79.8% 41.9%\n2 93.7% 48.5%\n4 93.8% 45.0%\n8 94.0% 42.1%4\n16 94.4% 37.2%\n2 98.0% 41.1%\n4 98.8% 33.8%\n8 99.1% 35.5%6\n16 99.0% 26.3%\nTable 2: Training and test accuracy for the 16 models trained\nto find the best meta-parameters for the transformer encoder.\nThe best training and test accuracy are highlighted.\na transformer encoder with 2 layers and 4 attention heads.\nThus, those meta-parameters were chosen for our model.\n5.4 Embedding Block Architecture\nThe linear embedding and position encoding block reduce the\ndimensions of the input and add position information to each\ntoken before passing them to the transformer encoder. To find\nthe best sequence length and token size, several architectures\nare considered for the embedding block. Table 3 summarizes\nthe combinations of meta-parameters. The transformer en-\ncoder block is the one selected in the previous section. Once\nagain, a grid search was applied to test all the combinations of\nthose two meta-parameters. Table 4 summarizes the results.\nTokens size 64, 80, 96, 112\nMax sequence length 30, 50, 60\nTable 3: Summary of the meta-parameters considered during\nthe grid search for the embedding block (see Table 4).\nAugmenting the maximal size of the sequence seems to\nbe damageable to the performance, and the embedding size\nshould remain moderate. As in Table 2, too complex models\ntend to overfit. The best model is obtained with a maximal\nsequence length of 30 and an embedding size of 80.\n5.5 Results and Discussions\nOur best-performing architecture uses a transformer encoder\nwith 2 layers and 4 attention heads with a maximal sequence\nlength of 30 frames and a token size of 80. It reaches a top-1\naccuracy of 54% and a top-10 accuracy of 83% on the test\nset. The top-10 accuracy is relevant in our use case as the\nuser of the dictionary could choose the correct sign out of\nthe 10 proposed by the system. The average recall and pre-\ncision obtained by the model are respectively 43% and 51%.\nThe per-class accuracy shows that classes with more exam-\nples are better identified by the model. Due to the unbalanced\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5971\nLinear Embedding + position encoding\nTransformer Encoder\nclf token\n token 1\n token 2\n token 3\n token 4\n token 5\n token 6\nMLP classifier\nFigure 3: Summary of the architecture used for LSFB recognition. The input is a sequence of skeletons extracted using\nMediaPipe [Lugaresi et al., 2019]. Each skeleton is embedded using a linear layer and a positional encoding is added to the\nresulting vector. A classification token is added at the start of the sequence as introduced by ViT. Then, the sequence of resulting\ntokens is sent to a transformer encoder. The classification token is then used to predict the label for the sign.\nMax. seq.\nlength\nEmbedding\nsize Training acc. Test acc.\n64 70.7% 52%\n80 76.7% 54.4%\n96 81.2% 53.6%30\n112 84.2% 50.5%\n64 69.9% 48.6%\n80 75.9% 47.9%\n96 79.7% 46.7%50\n112 84.0% 49.4%\n64 68.9% 42.8%\n80 75.3% 44.2%\n96 80.1% 47.0%60\n112 83.2% 46.7%\nTable 4: Training and test accuracy for the 12 models trained\nusing various sequence lengths and embedding sizes. The\nbest training and test accuracy are highlighted.\nnature of the data, the most common signs have hundreds of\nexamples while the least represented appears only 20 times\nleading to a great disparity in per-sign accuracy. The model\nalso frequently mistakes signs presenting the same hand con-\nfiguration and gestures.\nTo better assess the performances of our model regarding\nprevious works, Table 5 reports results obtained by models\nusing RGB video for isolated sign recognition. Only mod-\nels trained on datasets with a similar number of signers and\nvocabulary are reported.\nNotice that those results should be taken with caution as\nthey are obtained on different datasets captured in different\nconditions and using distinct sign languages. For instance,\nthe LSFB dataset and the BSL-1K [Albanie et al., 2020] are\nthe only reported datasets containing signs extracted from\nsentences. Thus signs from those datasets are performed\nfaster and might overlap with the previous sign. It may not\nbe relevant to compare the accuracy obtained on datasets that\nare so different. It is done here to give an indicative assess-\nment of our system. Actually, the performances in real-world\nconditions may be radically different and the only relevant in-\ndicator of performance is the adoption of the system by users.\nA key advantage of our LSFB classifier is that it proposes\nthe lightest architecture for SLR currently available with, at\nleast, 10 times fewer parameters than other methods. It is\nalso lighter than a MobileNet [Sandler et al., 2018] network\ndesigned to run on embedded devices. Despite that, the accu-\nracy of our method is in the same range as the performance\nobtained by other models in the literature. The LSFB classi-\nfier is light enough to run on CPU efficiently, which is key for\nits adoption by non-profit stakeholders that have not enough\nresources and technical knowledge to maintain a GPU server.\nOur overarching goal is to maximize its societal impact.\n6 System Integration\nTo achieve tangible societal impact, according to United Na-\ntions’ Sustainable Development Goals [UN, 2015] and par-\nticularly the goal 4 “Quality Education“ and the goal 10 “Re-\nduced Inequalities“, the model is integrated into a free and\naccessible system: the sign language-to-text dictionary which\nhas been publicly released and is already used by the deaf\ncommunity.\nAs illustrated in Figure 4, the system takes the form of a\nweb application combining the features and appearance in-\nspired by well-established online textual dictionaries such as\nGoogle Translate3 or Linguee4. The dictionary allows users\nto sign in front of their camera to search for the literal trans-\nlation of a sign in French. Users are invited to sign during a\n3translate.google.com\n4www.linguee.com\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5972\nAuthors Vocabulary Signers Parameters Top-1 Top-10 Dataset Base architecture\n[Izutov, 2020] 500 222 8.3M 63.36 - MS-ASL S3D\n[Izutov, 2020] 1000 222 8.3M 45.65 - MS-ASL S3D\n[Li et al., 2020] 1000 116 12M 47.33 84.33 WLASL I3D\n[Albanie et al., 2020] 1000 40 12M 65.57 - BSL-1K I3D\n[Liao et al., 2019] 500 8 11.4M 89.8 - DEVISIGN-D Resnet + LSTM\nLSFB classifier (ours) 700 100 782k 54.4 83.4 LSFB-ISOL ViT\nTable 5: This table reports the score obtained by other researchers on various datasets for isolated SLR using only RGB video.\nThe number of parameters for each architecture is reported. Our solution has, at least, 10 times fewer parameters than other\nmethods.\nfixed time window. Then, they are able to browse the propo-\nsitions made by the model to find the corresponding sign in\nthe dictionary. For the selected predicted sign, all the possible\nFrench translations are displayed. Moreover, for each trans-\nlation, the application displays bilingual examples showing\nhow the sign is used in a real SL video sentence alongside\nits French translation. This allows users to understand the\nuse of the sign in different contexts. The dictionary drasti-\ncally increases the autonomy of deaf people. It is also a use-\nful tool for French-speaking people learning sign language or\nsign language interpreters who can perfect their knowledge\nby browsing contextual examples of signs.\nThe remaining of this section discusses the design and im-\nplementation of the dictionary. The compliance with the re-\nquirements elicited by the stakeholders is also assessed.\n6.1 Design and Implementation\nIn order to put the user in the center of the process, the de-\nsign phase started with requirements engineering activities\nwith the stakeholders. First, based on semi-conducted discus-\nsions, four personas [Lallemand, 2018 ] were created (deaf\nuser, deaf student, bilingual teacher, and sign language ex-\npert). This HCI good practice helped to identify the tar-\nget users for the dictionary and the scope of their require-\nments. Moreover, a comparison of famous online dictionar-\nies or translators (e.g., Google Translate, DeepL, Microsoft\nBing) was conducted to confront their features with the needs\nof the personas. This then initiated the design of low and\nhigh-fidelity prototypes [Lallemand, 2018] for the dictionary.\nThose artifacts were evaluated in a continuous collaboration\nand validation with the four users representing each persona\n(2 deaf students, 1 bilingual teacher, 1 sign language expert),\nstakeholders (2 project leaders), and experts in HCI (1 UX\nexpert and 1 inclusive UX expert). Finally, as the website\nis used by deaf people, great care has been taken to ensure\naccessibility. Guidelines for the design of interfaces suited\nfor deaf people were searched. The web content accessibil-\nity guidelines (WCAG2) [Caldwell et al., 2008] proposed by\nthe W3C provide some general recommendations to design\ninclusive websites but nothing specific to the context of deaf-\nness. Therefore, the rest of the literature was explored and\nexamined. Among the identified works, the guidelines were\nsometimes not the primary focus of the study or were too gen-\neral for our purpose. There was a need for precision, com-\npleteness and cohesion. The work by [Andr´e, 2022 ] gath-\nered, classified, and completed the recommendations found\nin the literature to establish a checklist for the creation of UX\nadapted to deafness (e.g., transforming all sound signals to\nvisual ones, using icons instead of texts). Those recommen-\ndations were applied to the creation of our dictionary.\nTo transform the prototype into a working web applica-\ntion, all the components were implemented and connected\ntogether. The frontend of the application uses MediaPipe to\nextract the poses on the client side. Thus, only the landmarks\nextracted on the devices of the users are sent to the server\nto reduce the bandwidth needs and to preserve the privacy\nof users. A RESTful API provides endpoints to retrieve the\npossible translation for a given sign and the video example\nfrom the corpus LSFB. The API rely on our model to predict\nthe label of a sign given MediaPipe landmarks. The global\narchitecture is depicted in Figure 5.\n6.2 Requirements Assessment\nTo assess the conformity of the user requirements, a usabil-\nity testing [Lallemand, 2018 ] approach was followed. The\nmain goal was to collect qualitative data to improve the sys-\ntem following a feedback loop mechanism. Six realistic us-\nage scenarios mixing success and failure cases were proposed\nto the four users. It should be noted that tester users were not\ninvolved in the dataset creation, few years earlier. Those sce-\nnarios forced them to go through all the application function-\nalities, allowing us to observe their reactions and spot their\ndifficulties. The tests were followed by a survey and a semi-\nconducted discussion [Lallemand, 2018] to assess the feeling\nof users about the web application. Each test session was\nrecorded by two cameras and two microphones. An observer\ntook notes on an observation grid to spot all the hesitations or\nissues encountered by the user during the scenarios. A briefed\nsign language interpreter assisted the test conductor when the\nuser was deaf. All the materials used during the tests were\ntranslated into sign language by the interpreter.\nThe observations and remarks collected during those tests\nshowed that the users were able to execute all scenarios with-\nout major difficulties. The success rate for the scenarios\nranges from 87% to 98%. The gap is explained by the va-\nriety of users. Indeed, it has been noticed that children took a\nlittle more time, due to their distraction. In general, the first\nscenario also lasted longer, since users were new to the appli-\ncation. Finally, users reported that they appreciated the ease\nof use, simplicity, guidance, and the contextualized exam-\nples. However, they also asked for a better tolerance to their\nown inaccuracy while signing. Those insights were compiled\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5973\nFigure 4: Screenshot of the dictionary 6 after a successful search. The top of the interface shows the sign performed by the user\nalong with the possible translation in French. The bottom of the interface gives contextual examples of the selected translation\nin sign language (video) and in French (text). Signers can hence improve themselves based on those examples.\nLSFB \nCorpus\nWeb App\nSign Language\nRecognition \nREST API\nModel\nUser\nFigure 5: The system is made of three artifacts: (i) the web\napplication that provides an interface for the user and uses\nMediaPipe JS to preprocess locally the captured video, (ii)\nan API hosting the SLR model and that is linked to (iii) the\ncorpus database containing lexicon and contextual examples.\nto serve as the starting point of the next development itera-\ntion [Andr´e, 2022], as the dictionary will continue to evolve,\nso as to better meet the deaf community’s needs.\nRegarding the requirements elicited by the stakeholders in\nSection 2, the system is compliant as it has been successfully\ndeployed on the server of the LSFB Lab while responding in\nless than 10 seconds to a query. Users can use the website in\nvarious environments and lighting conditions.\n7 Conclusion and Future Work\nThis work introduces the first dictionary searchable from sign\nlanguage to text, publicly available through a web interface7.\nIt relies on a lightweight sign language recognition model, in-\nspired by the recent advances in transformer networks such as\nthe Vision Transformer architecture introduced by [Dosovit-\nskiy et al., 2021]. This work leverages the progress made in\n7dico.corpus-lsfb.be\npose estimation to achieve SLR on landmarks extracted from\nvideos instead of the raw frames. This further reduced the\ncomplexity of the model and it removes several challenges\nsuch as the robustness to changes in the recording environ-\nment. Those challenges are delegated to pose estimation li-\nbraries such as MediaPipe. Our model is able to classify 700\nsigns with a top-10 accuracy of 83%, and is light enough to\nbe run on embedded devices if needed. The model achieves\ncompetitive results while being 10 times lighter than alter-\nnative solutions. The model is integrated into a web dictio-\nnary allowing the user to search for the meaning of a sign in\nFrench. The dictionary is continuously populated by a team\nof linguists, the LSFB Lab. A user-centered HCI methodol-\nogy was followed to design the interface with insights from\nthe stakeholders and future users of the system. An evalu-\nation of the tool was performed with the users to assess its\ncompliance with the requirements identified.\nIn future work, metrics-based methods will be explored to\ntrain models that recognize more signs by predicting the dis-\ntance between two signs instead of predicting a label directly.\nThus, the model might be able to recognize new signs with-\nout being retrained. New architectures will be investigated to\nimprove the SLR performance and classification robustness.\nOnline learning methods will be investigated to leverage the\ninput of the users of our website to retrain the model.\nA new design iteration for the interface will also be con-\nducted. A survey will be sent to the users to collect their opin-\nions on the UI after a few months of use. Those insights will\nbe considered to upgrade the interface if needed. A browser\nplugin will also be developed to provide better integration of\nthe tool for the users. The developed dictionary is meant to\nbecome a long-lasting tool for the deaf community.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5974\nEthical Statement\nOur work has no ethical or societal risk. All subjects involved\nin the dataset agreed to have their video publicly published.\nMoreover, the developed application does not collect any pri-\nvate data and relies on pose estimation only. Above all, the\ndictionary improves the autonomy of deaf people and con-\ntributes to a more inclusive education system. More gener-\nally, it supports a better inclusion of the deaf community in\nsociety, according to SDGs 4 and 10 from United Nations.\nAcknowledgments\nWe would like to thank the members of the LSFB Lab for\ntheir major contribution and collaboration. Moreover, we ex-\npress our gratitude to the Baillet Latour Fund, the Walloon\nregion for the Ph.D. grant from FRIA (F.R.S.-FNRS) and\nthe project ARIAC piloted by Trail, an initiative of the Digi-\ntal4Wallonia for their funding. This work was also funded by\nthe FWO and F.R.S.-FNRS under the Excellence of Science\n(EOS) program.\nReferences\n[Albanie et al., 2020] Samuel Albanie, G ¨ul Varol, Liliane\nMomeni, Triantafyllos Afouras, Joon Son Chung, Neil\nFox, and Andrew Zisserman. Bsl-1k: Scaling up co-\narticulated sign language recognition using mouthing cues.\nIn Computer Vision–ECCV 2020: 16th European Con-\nference, Glasgow, UK, August 23–28, 2020, Proceedings,\nPart XI 16, pages 35–53. Springer, 2020.\n[Andr´e, 2022] Maxime Andr´e. Recommandations pour des\ninterfaces utilisateurs adapt´ees `a la surdit´e. Master’s thesis,\nUniversit´e de Namur, 2022.\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Ge-\noffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[Caldwell et al., 2008] Ben Caldwell, Michael Cooper,\nLoretta Guarino Reid, Gregg Vanderheiden, Wendy\nChisholm, John Slatin, and Jason White. Web content\naccessibility guidelines (wcag) 2.0. WWW Consortium\n(W3C), 290:1–34, 2008.\n[Cao et al., 2019] Zhe Cao, Tomas Simon, Shih-En Wei, and\nYaser Sheikh. Openpose: Realtime multi-person 2d pose\nestimation using part affinity fields. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2019.\n[De Coster et al., 2020] Mathieu De Coster, Mieke Van Her-\nreweghe, and Joni Dambre. Sign language recogni-\ntion with transformer networks. In 12th international\nconference on language resources and evaluation, pages\n6018–6024. European Language Resources Association\n(ELRA), 2020.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In International\nConference on Learning Representations, 2021.\n[Fink et al., 2021] J´erˆome Fink, Beno ˆıt Fr ´enay, Laurence\nMeurant, and Anthony Cleve. Lsfb-cont and lsfb-isol:\nTwo new datasets for vision-based sign language recog-\nnition. In 2021 International Joint Conference on Neural\nNetworks (IJCNN), pages 1–8, 2021.\n[Huang and Huang, 1998] Chung-Lin Huang and Wen-Yi\nHuang. Sign language recognition using model-based\ntracking and a 3d hopfield neural network. Machine Vi-\nsion and Applications, 10(5-6):292–307, April 1998.\n[Izutov, 2020] Evgeny Izutov. Asl recognition with metric-\nlearning based lightweight network. arXiv preprint\narXiv:2004.05054, 2020.\n[Kadir et al., 2004] Timor Kadir, Richard Bowden, Eng-Jon\nOng, and Andrew Zisserman. Minimal training, large lex-\nicon, unconstrained sign language recognition. In BMVC,\npages 1–10, 2004.\n[Koller, 2020] Oscar Koller. Quantitative survey of the state\nof the art in sign language recognition. arXiv preprint\narXiv:2008.09918, 2020.\n[Konstantinidis et al., 2018] Dimitrios Konstantinidis, Kos-\nmas Dimitropoulos, and Petros Daras. Sign language\nrecognition based on hand and body skeletal data. In2018-\n3DTV-Conference: The True Vision-Capture, Transmis-\nsion and Display of 3D Video (3DTV-CON), pages 1–4.\nIEEE, 2018.\n[Lallemand, 2018] Carine Lallemand. M´ethodes de De-\nsign UX. 30 m´ethodes fondamentales pour concevoir des\nexp´eriences optimales. (2e edition). 09 2018.\n[LeCun et al., 2015] Yann LeCun, Yoshua Bengio, and Ge-\noffrey Hinton. Deep learning. Nature, 521(7553):436–\n444, May 2015.\n[Li et al., 2020] Dongxu Li, Cristian Rodriguez, Xin Yu, and\nHongdong Li. Word-level deep sign language recognition\nfrom video: A new large-scale dataset and methods com-\nparison. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision (WACV), March\n2020.\n[Liao et al., 2019] Yanqiu Liao, Pengwen Xiong, Weidong\nMin, Weiqiong Min, and Jiahao Lu. Dynamic sign lan-\nguage recognition based on video sequence with blstm-3d\nresidual networks. IEEE Access, 7:38044–38054, 2019.\n[Lugaresi et al., 2019] Camillo Lugaresi, Jiuqiang Tang,\nHadon Nash, Chris McClanahan, Esha Uboweja, Michael\nHays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong,\nJuhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg,\nand Matthias Grundmann. Mediapipe: A framework for\nbuilding perception pipelines. 2019.\n[Meurant, 2015] Laurence Meurant. Corpus LSFB. Corpus\ninformatis´e en libre acces de vid ´eo et d’annotations de\nlangue des signes de Belgique francophone. Namur: Lab-\noratoire de langue des signes de Belgique francophone\n(LSFB Lab), FRS-FNRS, Universit´e de Namur, 2015.\n[Pigou et al., 2016] Lionel Pigou, Mieke Van Herreweghe,\nand Joni Dambre. Sign classification in sign language\ncorpora with deep neural networks. In Eleni Efthimiou,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5975\nStavroula-Evita Fotinea, Thomas Hanke, Julie A. Hochge-\nsang, Jette Kristoffersen, and Johanna Mesch, editors,Pro-\nceedings of the LREC2016 7th Workshop on the Represen-\ntation and Processing of Sign Languages: Corpus Mining,\npages 175–178, Portoro ˇz, Slovenia, May 2016. European\nLanguage Resources Association (ELRA).\n[Sandler et al., 2018] Mark Sandler, Andrew Howard, Men-\nglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4510–4520, 2018.\n[Savitzky and Golay, 1964] Abraham. Savitzky and M. J. E.\nGolay. Smoothing and differentiation of data by sim-\nplified least squares procedures. Analytical Chemistry,\n36(8):1627–1639, July 1964.\n[Stokoe, 1972] William C Stokoe. Classification and de-\nscription of sign languages. Current trends in linguistics,\n12:345–371, 1972.\n[UN, 2015] UN. The 17 goals — sustainable develop-\nment. https://sdgs.un.org/goals, 2015. (Accessed on\n05/15/2023).\n[Vaezi Joze and Koller, 2019] Hamid Vaezi Joze and Oscar\nKoller. Ms-asl: A large-scale data set and benchmark for\nunderstanding american sign language. In The British Ma-\nchine Vision Conference (BMVC), September 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, volume 30, 2017.\n[Wang et al., 2012] Haijing Wang, Alexandra Stefan, Saj-\njad Moradi, Vassilis Athitsos, Carol Neidle, and Farhad\nKamangar. A system for large vocabulary sign search.\nIn Trends and Topics in Computer Vision: ECCV 2010\nWorkshops, Heraklion, Crete, Greece, September 10-11,\n2010, Revised Selected Papers, Part I 11, pages 342–353.\nSpringer, 2012.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n5976",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8483811616897583
    },
    {
      "name": "USable",
      "score": 0.7703772187232971
    },
    {
      "name": "Sign language",
      "score": 0.7474953532218933
    },
    {
      "name": "Transformer",
      "score": 0.6281294226646423
    },
    {
      "name": "American Sign Language",
      "score": 0.4935990571975708
    },
    {
      "name": "Natural language processing",
      "score": 0.4725712835788727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47135868668556213
    },
    {
      "name": "User interface",
      "score": 0.46194037795066833
    },
    {
      "name": "Sign (mathematics)",
      "score": 0.41049709916114807
    },
    {
      "name": "World Wide Web",
      "score": 0.21683526039123535
    },
    {
      "name": "Programming language",
      "score": 0.1695249378681183
    },
    {
      "name": "Linguistics",
      "score": 0.12730637192726135
    },
    {
      "name": "Engineering",
      "score": 0.07520654797554016
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}