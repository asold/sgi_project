{
  "title": "Enhance Multimodal Transformer With External Label And In-Domain Pretrain: Hateful Meme Challenge Winning Solution",
  "url": "https://openalex.org/W3111930335",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2350976852",
      "name": "Zhu Ron",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W3023989664",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2967013449",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3038476992",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3021649351"
  ],
  "abstract": "Hateful meme detection is a new research area recently brought out that requires both visual, linguistic understanding of the meme and some background knowledge to performing well on the task. This technical report summarises the first place solution of the Hateful Meme Detection Challenge 2020, which extending state-of-the-art visual-linguistic transformers to tackle this problem. At the end of the report, we also point out the shortcomings and possible directions for improving the current methodology.",
  "full_text": "Enhance Multimodal Transformer With External\nLabel And In-Domain Pretrain: Hateful Meme\nChallenge Winning Solution\nRon Zhu\nALFRED SYSTEM\nDecember 2020\nAbstract\nHateful meme detection is a new research area recently brought out\nthat requires both visual, linguistic understanding of the meme and some\nbackground knowledge to performing well on the task. This technical re-\nport summarises the ﬁrst place solution of the Hateful Meme Detection\nChallenge 2020, which extending state-of-the-art visual-linguistic trans-\nformers to tackle this problem. At the end of the report, we also point\nout the shortcomings and possible directions for improving the current\nmethodology.\n1 Introduction\nThe Hateful Memes Challenge[8] introduces the dataset designed to be truly\nmultimodal by including confounders that prevent the model from exploiting\nunimodal prior. Despite the recent advances in multimodal reasoning, we only\nget 0.71 AUROC when applying state of the art multimodal models to the chal-\nlenge, which is far behind from non-expert human performance. In this paper,\nwe discuss the diﬀerence between the commonly seen multimodal reasoning task\nand the meme classiﬁcation. And the approaches we build on top of the exist-\ning visual-linguistic models to improve the performance and achieving 0.845\nAUROC on the hateful memes detection dataset.\n1\narXiv:2012.08290v1  [cs.CL]  15 Dec 2020\nFigure 1: Complete data processing pipeline\n2 Problem Description\nIn the typical visual-linguistic multimodal dataset, there is a direct relation-\nship between the image and textual input. Hateful memes have their unique\ncharacteristic that is much diﬀerent from the usual multimodal dataset. Com-\npare to the multimodal dataset for a task like VCR[16] and VQA[1], only some\nof them have a direct relationship that appears in common multimodal down-\nstream tasks and pre-train datasets. Most of the time, the meme’s hateful part\nis linked by subtle wording or external knowledge about the real-world event.\nIn that sense, hateful memes challenge is more of a visual-linguistic-knowledge\nmultimodal classiﬁcation problem.\nMoreover, hateful memes are built on diverse topics and often contain novel\nobjects that no oﬀ-the-shelf classiﬁer will recognize, which is much diﬀerent\nfrom the everyday object presented in typical multimodal datasets. Visual hints\nlike injury, traditional costume wear by the subject, and historical scenes are\nhard to recognize and low on the sample number. Those factors make visual\nunderstanding part of the challenge tough to solve even without the lingual\npart. The context of the visual modality context has an enormous impact on\n2\nthe meme’s polarity. Because visual hint is hard to recognize even for the state\nof the art image classiﬁer or object detector, it becomes essential to incorporate\ninformation from a diﬀerent source and format.\nIn some cases, the position of the meme caption will also aﬀect the meaning\nof the meme. For example, intentionally place the caption of “wishing machine”\non top of the female subject make the meme hateful, but if we place the caption\non anything else instead of human will not. And this not commonly see and\naccounted for in the general visual-linguistic model.\n3 Approach\nThe solution comprises four diﬀerent VL transformer architecture ensemble,\nnamely: VL-BERT[13], UNITER[3], VILLA[6], and ERNIE-Vil[15]. All the\nmodels besides ERNIE-Vil were modiﬁed for better performance of hateful\nmeme detection. Furthermore, we conduct extra steps to extract more informa-\ntion from the hateful memes dataset for both training and inference.\n3.1 Background\nIn the recent trend of applying the pre-trained transformer model to VL mul-\ntimodal task have greatly improved the SOTA of multiple benchmarks. State\nof the art VL transformers like Oscar[10], VILLA, ERNIE-Vil are utilize large\nscale dataset with paired image and text for pretraining. Although they achieve\nenormous improvement on the common sense VL tasks, pretrained VL trans-\nformer still performs poorly on hateful meme detection. The task requires the\nmodel to have a deep understanding of image, text, real-world events, and the\nability to recognize inter-modal interaction between three modalities.\n3.2 Additional Data Source\nAs forward-mentioned, we will need more than the image and text description\nof memes to do well on hateful meme detection. So at the ﬁrst step of the data\npreprocessing, we will try to extract a few critical information from the data\nsource using Web Entity Detection and FairFace[7] classiﬁer.\nMemes often contain a reference to famous historical events, news, celebri-\nties. And memes reference to news topic and diﬀerent sub-culture trend are\nchanging rapidly. It not uncommon to see new memes popping up every few\ndays or weeks. So using static knowledge sources(ex: Wikipedia) alone may not\nbe adequate for this problem. To deal with this challenge, we use Google Vision\nWeb Entity Detection to capture the image’s context. Despite this, without\ntaking the entity’s background knowledge and the relation between entities into\naccount, we only solve half of the problem.\nAccording to the Hateful Meme Challenge paper, amount all types of hateful\nmemes, hateful meme related to race or ethnicity is the most prominent. Almost\nhalf of the hateful memes can be categorized to it. Furthermore, it is unrealistic\n3\nto expect a pretrained VL transformer to learn to identify race with just a few\nthousand samples from the hateful memes dataset. Thus the inclusion of race\nand gender labels created from the FairFace classiﬁer. We apply the classiﬁer\nby ﬁrst detecting and classifying every face or head in the image, then mapping\nthe label back to the person’s bounding box with the largest overlapped area\nwith the face. In the end, we should get the race and gender of most of the\nperson to appear in the image unless the person’s head is obscured.\n3.3 Extend VL-BERT Visual-Linguistic Framework\nTo jointly train multiple types of labels with the original meme image and\ntext. We need a way to combine information from diﬀerent sources and formats\nseamlessly. Also, the ability to explicitly link images and text to the external\nlabels will help deal with the unique characteristic of memes and remove the\nneed for learning how to use external labels from scratch.\nInspire by Oscar, we represented all external labels as a special type of text\ntoken and linked it to the general area in the image or a special image region\nusing visual feature embedding. By doing so, we also create the implicit link\nbetween text tokens that have related image features. For example, text token\n“bowler hat” with the image region of a hat and “man dressed in a three-piece\nsuit” with the image region covers the entire person wearing the hat/suit. With\na versertail framework like this, we can easily add more types of labels to handle\ndiﬀerent types of memes in the future.\nFigure 2: Architecture of extened VL-BERT\n4\n3.4 UNITER-ITM\nMany pretrained VL transformer inspires by BERT[4] also include the VL adap-\ntation of the NSP(next sentence prediction) task - ITM(image text matching).\nAlthough some research shows that the inclusion of ITM tasks gives no beneﬁt\nor even reduces the performance on the downstream task, we can still ﬁnd it\nhelpful depending on the task on hand. In the case of hateful memes detection,\nthere is an apparent relationship between it and ITM; namely, the meme with\nthe description(text) aligning the image’s content is most probably benign, but\nmeme may not necessarily being hateful when they are not aligning. It may\njust the meme’s description and image being unrecognizable or hard to under-\nstant by the model, or actually being hateful and comparing people to object\nor animals.\nWith the UNITER pre-trained transformer, we keep the ITM head pre-\ntrained on ITM and WRA(word region alignment) tasks instead of randomly\ninitializing the classiﬁer head weight. Then swap the weights of class 0 of clas-\nsiﬁer head with class 1, so now class “image-text-match” will become “benign”\nand “image-text-not-match” become “hateful”.\nSuggest by recent research[2] both single-stream or dual-stream VL trans-\nformers are biasing toward linguistic modal. By reusing ITM head as the start-\ning point, we should get an extra beneﬁt of reducing linguistic bias due to the\nface that ITM and WRA require both visual and linguistic feature to work,\nwhich is not always true for another pre-train task like MRC(masked region\nclassiﬁcation) or MLM(masked language modeling).\nFigure 3: UNITER-ITM\n5\n3.5 ERNIE-Vil\nERNIE-Vil is the VL model pretrained on information extracted from the scene\ngraph to teach the model about ﬁne-grained object attribute and relationship\nmodel. Being the STOA model on the VCR leaderboard, ERNIE-Vil can achieve\ncompetitive performance to extended VL-BERT and UNITER-ITM with some\nhyper-parameter tuning. Including ERNIE-Vil into the ﬁnal ensemble add di-\nversity to the prediction and improve generalization.\n4 Experiment\n4.1 Apply entity/race/gender tags\nBy applying the new labels obtained from the external model to extended VL-\nBERT, our accuracy get a signiﬁcant boost on the dev-seen set of hateful memes\ndataset. Nevertheless, when we apply the same entity tags to other single stream\nmodels like UNITER and Oscar, they give worse or almost no performance\ndiﬀerence. Then apply the same trick on ERNIE-Vil give a noticeable jump in\nperformance. While the diﬀerence in architecture and training target is small\nbetween single-stream models we tested, entity tags’ eﬀect diﬀers. There are a\nfew potential reasons that may cause this phenomenon:\n1) Lack of explicitly visual-linguistic fusion mechanism (VL-BERT’s visual\nembedding for every token, and ERNIE-Vil’s cross-modality layers) make it\nhard to learn how to utilize a new type of input format with a small dataset\nlike the one we are using.\n2) Lower lingual representation quality(VL-BERT has been pre-trained with\ntext only MLM task, ERNIE-Vil have the independent text modality stream)\nlet the entity tags that are represented in the text become less eﬀective.\nFigure 4: dev-seen AUROC of extended VL-BERT with entity/race/gender tags\n6\n(a) UNITER-ITM with and w/o entity tags\n (b) ERNIE-Vil with and w/o entity tags\nFigure 5: Hateful Memes dev-seen set AUROC\n4.2 Reuse of UNITER ITM head\nAs shown in the ﬁgure.6, UNITER-ITM compare to randomly initialized clas-\nsiﬁcation head:\n1) seen not as easy to overﬁt.\n2) getting better performance. (0.778 vs. 0.765 AUROC).\n3) may suﬀer less bias toward text modality, thanks to ITM and WRA pre-\ntrain task require both modal to work.\nThe high accuracy of UNITER-ITM at the very start of the training may\nalso indicate a correlation between ITM pre-train task and hateful memes clas-\nsiﬁcation. This leaves an interesting question of if there is another pre-train\ntask for the VL model that also has the same high correlation to hateful memes\nclassiﬁcation.\nFigure 6: dev-seen AUROC of UNITER w/ and w/o reuse ITM head\n7\n5 Feature Direction\nHateful meme detection is currently one of the few tasks that require the model\nto identify entities and relationships from image, text, and external knowledge\nbase. In the Hateful Memes dataset, hateful metaphor hidden between rela-\ntion visual, linguistic, knowledge base entities are common but extremely hard\nto spot for the machine. Most of the pre-trained models currently apply to\nhateful meme detection are only take image and text into account, and the pre-\ntrain dataset only contained everyday object and simple description that gives\na shallow explanation of the image. Although the pre-trained language model\nused in some VL models can be treated as a knowledge base and have decent\nperformance in the task like open-domain QA. However, this kind of implicit\nknowledge base is hard to update and apply to a diﬀerent domain. Several\nresearch attempts to resolve this problem by letting the model learn to use the\nexternal knowledge base instead of remembering everything inside the model’s\nparameters, for example, RAG[9].\nA better and more ﬂexible source of knowledge that provides in-depth in-\nformation about the entity that appears in the meme should help reduce the\nperformance gap between machine and human. One of the promising directions\nalso comes from the ﬁeld of open-domain QA: knowledge graph enhanced pre-\ntrain transformers(language model). There are many ways we can inject external\nknowledge into pre-trained transformers like K-Adapter[14], K-BERT[11]. One\nof them, MHGRN[5], is especially interesting to me. Due to its plug-and-play\nnature, MHGRN can place after any pre-trained transformer; in our case, any\none of the VL transformers will work. This gave us many options and ﬂexibility\non how we want to handle the visual-linguistic part of the problem. Moreover,\nthe attention GNN used in MHGRN makes it easier to scale the model to a\nlarger graph, which may be a critical part of the problem when dealing with the\nsubtle relationship between key entities inside a meme. Despite the advantage\nmentioned above, using a knowledge-graph-based method to solve hateful meme\ndetection still come with a few challenges. To cover the diverse topic meme is\nreferencing from, we will also need a sizeable knowledge graph like wiki-data or\nConceptNet[12] to compensate for it. For every meme we are dealing with, we\nneed to extract a subgraph that contains the entity that appears in the meme’s\nimage or text and an entity that is potentially related to the context of the\nmeme. Entities in a large scale knowledge like wiki-data usually have dozens of\ntype of relation per entity. Thus building a subgraph that both includes enough\ninformation and has a reasonable size will be challenging.\n6 Conclusion\nIn this paper, we propose two methods to improve pre-trained visual-linguistic\ntransformers’ performance on the task of hateful meme detection. The ﬁrst\napproach focuses on building a visual-linguistic framework on top of VL-BERT\nto incorporate pieces of information that are almost impossible to learn from the\n8\nhateful meme challenge dataset directly. The second approach tries to utilize\nthe entire pre-trained model to get a better score on meme classiﬁcation. In\nspite of the improvement brought by those two approaches, there is still much\nspace to improve. Furthermore, we also point out some of the promising research\ndirections that may give the model a better understanding of the memes.\nReferences\n[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell,\nC. Lawrence Zitnick, Dhruv Batra, and Devi Parikh. Vqa: Visual question\nanswering, 2016.\n[2] Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing\nLiu. Behind the scene: Revealing the secrets of pre-trained vision-and-\nlanguage models, 2020.\n[3] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed,\nZhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text rep-\nresentation learning. In ECCV, 2020.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\nPre-training of deep bidirectional transformers for language understanding,\n2019.\n[5] Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and\nXiang Ren. Scalable multi-hop relational reasoning for knowledge-aware\nquestion answering, 2020.\n[6] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing\nLiu. Large-scale adversarial training for vision-and-language representation\nlearning, 2020.\n[7] Kimmo K”arkk”ainen and Jungseock Joo. Fairface: Face attribute dataset\nfor balanced race, gender, and age. arXiv preprint arXiv:1908.04913, 2019.\n[8] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Aman-\npreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes\nchallenge: Detecting hate speech in multimodal memes, 2020.\n[9] Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen tau Yih,\nTim Rockt¨ aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks, 2020.\n[10] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang,\nLijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jian-\nfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language\ntasks, 2020.\n9\n[11] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng,\nand Ping Wang. K-bert: Enabling language representation with knowledge\ngraph, 2019.\n[12] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An\nopen multilingual graph of general knowledge, 2018.\n[13] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng\nDai. Vl-bert: Pre-training of generic visual-linguistic representations, 2020.\n[14] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jian-\nshu ji, Guihong Cao, Daxin Jiang, and Ming Zhou. K-adapter: Infusing\nknowledge into pre-trained models with adapters, 2020.\n[15] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng\nWang. Ernie-vil: Knowledge enhanced vision-language representations\nthrough scene graph, 2020.\n[16] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recogni-\ntion to cognition: Visual commonsense reasoning, 2019.\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5794596672058105
    },
    {
      "name": "Computer science",
      "score": 0.5411587953567505
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4359409213066101
    },
    {
      "name": "Engineering",
      "score": 0.21616709232330322
    },
    {
      "name": "Electrical engineering",
      "score": 0.0844452977180481
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}