{
  "title": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer",
  "url": "https://openalex.org/W4393148727",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101799777",
      "name": "Hui Lin",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5008366979",
      "name": "Zhiheng Ma",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5026880795",
      "name": "Xiaopeng Hong",
      "affiliations": [
        "Harbin Institute of Technology",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5013116996",
      "name": "Qinnan Shangguan",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5091017287",
      "name": "Deyu Meng",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2895051362",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6777600214",
    "https://openalex.org/W6792881045",
    "https://openalex.org/W3205705101",
    "https://openalex.org/W6848935878",
    "https://openalex.org/W4386081456",
    "https://openalex.org/W2072232009",
    "https://openalex.org/W2886443245",
    "https://openalex.org/W3169575312",
    "https://openalex.org/W2788040570",
    "https://openalex.org/W3154723007",
    "https://openalex.org/W6810086659",
    "https://openalex.org/W6798518378",
    "https://openalex.org/W6843304446",
    "https://openalex.org/W4221142499",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W2969835258",
    "https://openalex.org/W2808519136",
    "https://openalex.org/W6756774705",
    "https://openalex.org/W3039523392",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W6772347466",
    "https://openalex.org/W2967161970",
    "https://openalex.org/W6797794648",
    "https://openalex.org/W1483870316",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2911004101",
    "https://openalex.org/W6848579055",
    "https://openalex.org/W3097305524",
    "https://openalex.org/W2743112477",
    "https://openalex.org/W3186104459",
    "https://openalex.org/W3176458063",
    "https://openalex.org/W6783507135",
    "https://openalex.org/W3027606690",
    "https://openalex.org/W2097324787",
    "https://openalex.org/W6839065725",
    "https://openalex.org/W2973151385",
    "https://openalex.org/W3113251869",
    "https://openalex.org/W3035307763",
    "https://openalex.org/W3097407159",
    "https://openalex.org/W2586716774",
    "https://openalex.org/W4292262994",
    "https://openalex.org/W2463631526",
    "https://openalex.org/W2966271765",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W4286902310",
    "https://openalex.org/W3004672782",
    "https://openalex.org/W3190723141",
    "https://openalex.org/W3203845557",
    "https://openalex.org/W4313182800",
    "https://openalex.org/W3026197937",
    "https://openalex.org/W2997403460",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2964209782",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2982007926",
    "https://openalex.org/W2952145882",
    "https://openalex.org/W4225264236",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W4285212493",
    "https://openalex.org/W4281706128",
    "https://openalex.org/W3167812602",
    "https://openalex.org/W3087861058",
    "https://openalex.org/W3109242411",
    "https://openalex.org/W4304098313",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3174519905",
    "https://openalex.org/W2963035940",
    "https://openalex.org/W2967069910",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4312613051",
    "https://openalex.org/W4320167334",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4389363851",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2967776630",
    "https://openalex.org/W4312791242",
    "https://openalex.org/W2981436300"
  ],
  "abstract": "Transformer has been popular in recent crowd counting work since it breaks the limited receptive field of traditional CNNs. However, since crowd images always contain a large number of similar patches, the self-attention mechanism in Transformer tends to find a homogenized solution where the attention maps of almost all patches are identical. In this paper, we address this problem by proposing Gramformer: a graph-modulated transformer to enhance the network by adjusting the attention and input node features respectively on the basis of two different types of graphs. Firstly, an attention graph is proposed to diverse attention maps to attend to complementary information. The graph is building upon the dissimilarities between patches, modulating the attention in an anti-similarity fashion. Secondly, a feature-based centrality encoding is proposed to discover the centrality positions or importance of nodes. We encode them with a proposed centrality indices scheme to modulate the node features and similarity relationships. Extensive experiments on four challenging crowd counting datasets have validated the competitiveness of the proposed method. Code is available at https://github.com/LoraLinH/Gramformer.",
  "full_text": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer\nHui Lin1, Zhiheng Ma2, Xiaopeng Hong3,4*, Qinnan Shangguan3, Deyu Meng1\n1School of Mathematics and Statistics, Xi’an Jiaotong University\n2Shenzhen Institute of Advanced Technology, Chinese Academy of Science\n3Harbin Institute of Technology\n4Peng Cheng Laboratory\nlinhuixjtu@gmail.com; zh.ma@siat.ac.cn; hongxiaopeng@ieee.org;\nsg12qt@gmail.com; dymeng@mail.xjtu.edu.cn\nAbstract\nTransformer has been popular in recent crowd counting work\nsince it breaks the limited receptive field of traditional CNNs.\nHowever, since crowd images always contain a large num-\nber of similar patches, the self-attention mechanism in Trans-\nformer tends to find a homogenized solution where the at-\ntention maps of almost all patches are identical. In this pa-\nper, we address this problem by proposing Gramformer: a\ngraph-modulated transformer to enhance the network by ad-\njusting the attention and input node features respectively on\nthe basis of two different types of graphs. Firstly, an atten-\ntion graph is proposed to diverse attention maps to attend\nto complementary information. The graph is building upon\nthe dissimilarities between patches, modulating the attention\nin an anti-similarity fashion. Secondly, a feature-based cen-\ntrality encoding is proposed to discover the centrality posi-\ntions or importance of nodes. We encode them with a pro-\nposed centrality indices scheme to modulate the node fea-\ntures and similarity relationships. Extensive experiments on\nfour challenging crowd counting datasets have validated the\ncompetitiveness of the proposed method. Code is available at\nhttps://github.com/LoraLinH/Gramformer.\nIntroduction\nCrowd counting, which aims to estimate the number of peo-\nple in crowded scenes, is a core and challenging task in\ncomputer vision. It has a wide range of real-world appli-\ncations, including congestion estimation, traffic monitoring,\nand other crowd management scenarios, especially after the\noutbreak of COVID-19. The increasing demand drives the\nprosperous development of counting methods.\nOver the last few decades, Convolution Neural Net-\nwork (CNN) has been utilized as the mainstream struc-\nture to regress the density map and predict the total crowd\ncount (Zhang et al. 2016; Li, Zhang, and Chen 2018; Ma\net al. 2019). However, the accuracy of these methods will\nsometimes be constrained by the fixed-size convolution ker-\nnel, where each patch only receives information from its spa-\ntial neighbours. This makes traditional CNNs to be difficult\nto deal with different variations in crowd images. Recently,\nwith the blossom of transformers, some approaches (Liang\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nVanilla Attention\nGraph-guided Attention\nFigure 1: Visualization comparison of attention maps in dif-\nferent patches between vanilla attention and the proposed\ngraph-guided attention. Each white cross mark indicates the\nlocation of a patch which the attention map is corresponding\nto. The vanilla attention finds a homogenized solution where\nthe attention maps of most patches are similar (to the fi-\nnal density map), regardless of whether they are background\n(the first column) or foreground (the last two columns).\net al. 2022; Lin et al. 2022b,a) have leveraged on the self-\nattention mechanisms to further boost the accuracy. The\ntransformer treats the image as a sequence of patches and\neach patch receives information from all other ones. With a\ndot-product operation, the intensity of information received\nis based on the feature similarity of different nodes.\nWhile the transformer architecture is effective at comput-\ning crowd density in an image, it may produce homogenized\nattention maps as there are numerous regions that appear\nsimilar in a crowdy image. In our experiments, we are sur-\nprised to find that the attention maps of most patches pro-\nduced by the vanilla transformer are almost identical 1, ir-\nrespective of whether they are foreground or background.\nAs depicted in Figure 1, these attention maps are predomi-\nnantly focused on head positions and similar to the crowd\ndensity map (homogenized attention). It can be also seen\n1The recent study (Wang, Mei, and Yuille 2023) also reveals\nthat the transformer in CLIP (Radford et al. 2021) tends to pro-\nduce highly similar attention maps across various source points in\nnatural images.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3395\nModel Dataset ANVar\nVanilla Transformer ImageNet val 5028.2\nVanilla Transformer QNRF test 368.6\nOur model QNRF test 4777.6\nTable 1: The comparison of the average normalized variance\n(ANVar) of attention maps generated by an Imagenet pre-\ntrained transformer on natural images and crowded images\n(resized to a uniform size of 384×384). The ANVar of the\nattention maps learned by the transformer on crowd images\n(QNRF) is significantly lower (<10%) than those on natural\nimages (ImageNet). Our Gramformer reduces homogeniza-\ntion and diversifies the attention maps.\nthat in Table 1, the variance of the attention maps learned by\nthe vanilla transformer on crowd images (QNRF) is below\none-tenth of those on natural images (ImageNet). The con-\ngruence observed between the homogenized attention maps\nand the density maps accelerates the production of the tar-\nget density map by the counting model. However, it comes\nat the cost of overlooking other details that may be relevant.\nAs a result, this scheme only focuses on the current attention\nregion, ignoring its relationships with other patches, poten-\ntially leading to performance loss.\nIn this paper, we address this problem by proposing a\nnovel solution called graph-modulated transformer(Gram-\nformer for crowd counting). We introduce two types of\ngraphs, the attention graph and the feature-based centrality\nencoding graph, to modulate the attention and input node\nfeatures of the transformer blocks, respectively. Building\nupon these two representations, we enhance the transformer\nnetwork for crowd counting from two primary perspectives.\nFirst, we propose a graph-guided attention modulation\nmethod to diversify the generated attention maps. We sup-\npose that the key to diverse attention maps lies in attending\nto complementary information. Based on this understanding,\nwe design an Edge Weight Regression (EWR) network to\nencode the dissimilarity between patchesinto edge weights.\nConsidering the perspective geomery in crowd images, we\nfurther introduce an edge regularization term to limit the dis-\nsimilarity encoded by EWR within the same horizontal line\nto be as little as possible. The graph edges form an attention\nmask, which is incorporated into the transformer block to\nmodulate the attention maps in an anti-similarity fashion.\nSecondly, we propose a graph-guided node feature mod-\nulation method that encodes the centrality positions or im-\nportance of nodes in the graph. To achieve this, we devise a\nCentrality Indicesscheme, which constructs a feature-based\nneighboring graph and explores the overall similarity of all\nnodes. Then we encode the graph structural information by\nthe centrality embedding vectors to modulate input node fea-\ntures and thus further enhances the self-attention relations.\nWe propose a graph-modulated transformer architecture\nfor crowd counting to modulate the attention and input node\nfeatures of the transformer blocks, respectively. The contri-\nbutions in detail can be summarized as follows:\n• We propose a graph-guided attention modulation method\nto diversify the generated attention maps in an anti-\nsimilarity manner.\n• We propose an edge weight regression network with an\nedge regularization training term, to determine and regu-\nlarize the edge weights for the attention graph.\n• We propose a graph-guided node feature modulation\nmethod to capture centrality information and modulate\nthe input feature node.\n• Our approach consistently achieves promising counting\nperformance on popular benchmarks.\nRelated Works\nGraph Transformer\nRecently, with the blossom of Transformer (Vaswani et al.\n2017), researchers gradually focus on the generalization and\ncombination between Transformers and GNNs (Han et al.\n2022). GraphTransformer (Dwivedi and Bresson 2020) in-\ntroduces a generalization of transformer networks to homo-\ngeneous graphs. It incorporates the attention mechanism into\ngraph structure and proposes to use Laplacian eigenvector as\npositional encoding. GraphTrans (Wu et al. 2021) applies\na permutation-invariant Transformer module after a stan-\ndard GNN module. SAN (Kreuzer et al. 2021) learns po-\nsitional encodings by proposing the Laplace spectrum of a\ngiven graph. LSPE (Dwivedi et al. 2021) presents a random-\nwalk diffusion-based positional encoding scheme to initial-\nize the positional representations of the nodes. It enables\nGNNs to learn both structural and positional representa-\ntions. Graphi (Mialon et al. 2021) encodes structural and\npositional information by positive definite kernels and local\nsub-structures such as paths of short length. GPS (Ramp´aˇsek\net al. 2022) focuses and categorizes three main ingredients\nin unified transformer and graph neural networks: position-\nal/structural encoding, local message-passing mechanism,\nand global attention mechanism. Graphormer (Ying et al.\n2021) proposes several structural encodings to incorporate\nstructural information reflected on nodes and the relation be-\ntween node pairs. The proposed graph-guided transformer\ndiffers from existing approaches by modulating both the at-\ntention mechanism and node features respectively based on\ndifferent types of constructed graph structures.\nCrowd Counting\nEarly crowd counting methods adopt state-of-the-art detec-\ntors to perform the task, i.e., Faster R-CNN (Ren et al. 2015),\nYOLO (Redmon et al. 2016), RetinaNet (Lin et al. 2017),\netc. However, the detection-based methods (Wu and Nevatia\n2005; Idrees et al. 2013) appear to perform poorly in images\nwith heavy occlusion. To improve this, density-based meth-\nods are introduced and gain promising counting accuracy.\nCNN is popularly used to generate predicted density maps\nsince it possesses translation equivariance and is effective in\nextracting local details (Liu et al. 2022), which has driven\nrapid developments in crowd countings (Zhang et al. 2016;\nLi, Zhang, and Chen 2018; Ma et al. 2019). Further improve-\nments such as multi-scale mechanisms (Zeng et al. 2017;\nMa et al. 2020; Sindagi and Patel 2017; Cao et al. 2018; Liu\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3396\nFigure 2: The framework of Gramformer, which contains two main parts: an attention graph to modulate the attention mecha-\nnism by its edge weight, and a feature-based centrality encoding graph to encode the centrality or importance of each node. In\nthe attention graph, different colors represent different semantic values predicted by EWR, and the color difference corresponds\nto the strength of connecting edges. In centrality encoding, each node is assigned a centrality index, which is linked to its\nin-degree. The centrality index is used to find the corresponding embedding from a learnable bank to modulate node features.\net al. 2019), perspective estimation (Yang et al. 2020b; Yan\net al. 2019; Yang et al. 2020a), auxiliary task (Liu et al. 2020;\nZhao et al. 2019; Yang et al. 2020c), density refinement (Liu\net al. 2018), optimal transport (Ma et al. 2021; Lin et al.\n2021; Wang et al. 2020a; Wan, Liu, and Chan 2021) also\nshow their effectiveness based on CNN.\nThe Vision Transformer (ViT) (Dosovitskiy et al. 2020)\nhas demonstrated its outstanding performances in a vari-\nety of vision tasks (Carion et al. 2020; Zheng et al. 2021;\nChen et al. 2021). Lately, some works adopt transformer\nto boost the counting performance. TransCrowd (Liang\net al. 2022) uses transformer to reformulate the weakly-\nsupervised crowd counting problem from the perspective\nof sequence-to-count. MAN (Lin et al. 2022b)incorporates\nglobal attention, learnable local attention, and instance atten-\ntion into a counting model. CLTR (Liang, Xu, and Bai 2022)\nintroduces an end-to-end transformer framework targeting\non the crowd localization task. DACount (Lin et al. 2022a)\nproposes an agency-guided semi-supervised approach and\nuses a foreground transformer to refine crowd information.\nRecently, HyGnn (Luo et al. 2020) uses a Hybrid GNN\nto formulate information from different scales and domains.\nRRP (Chen et al. 2020) employs a vanilla GCN to propa-\ngate information between different regions. STGN (Wu et al.\n2022) and CoCo-GCN (Zhai et al. 2022) target on video\ncrowd counting and multi-view crowd counting respectively\nfrom graph perspective. Our method is distinct from them in\ntwo ways. First, we construct two different types of graphs\nbased on the dissimilarity between each two patches and\nthe overall similarity of all nodes. Second, these graphs will\nbe leveraged to modulate the attention mechanisms with an\nanti-similarity fashion, and node features with a centrality\nindices scheme, respectively.\nThe Proposed Method\nGiven a crowd imageI, we denote the feature map extracted\nby the backbone as F ∈ RC×W×H, where C, W, and\nH are the channel, width, and height, respectively. These\nfeatures can be represented as a set of nodes, denoted as\nV = {v1, v2, ..., vN }, where2 N = W × H. We then build\ntwo graphs with two different ways of edge construction,\ni.e., an attention graph with edges Ea and a feature-based\ncentrality encoding graph with edges Ec. Guided by these\ntwo graphs, we improve the transformer by modulating the\nattention mechanism using the edge weights produced by the\nattention graph and biasing the input node features based on\nthe in-degree of the node in the neighboring graph, which is\ntermed by the graph-modulated transformer (Gramformer).\nStructure of Graph-Modulated Transformer\nThe structure of Gramformer is shown in Figure 2. Gram-\nformer’s main features include an attention modulation\nscheme and a node feature modulation scheme. In details,\ngiven a single attention head s ∈ {0,1, ..., S} and corre-\nsponding learnable matrices Ws\nQ, Ws\nK ∈ RC×C\nS , we define\nthe graph-modulated multi-head attention as:\nRs(vl\ni, vl\nj) =Es\nij · S(\n(ˆvl\niWs\nQ)(ˆvl\njWs\nK)T\n√\nC\n). (1)\nS is the softmax function and l is the transformer layer. ˆvl\ni is\nthe modulated node features, which is defined by\nˆvl\ni = vl\ni + P(vl\ni). (2)\n2Without being ambiguous, the symbol v is also used to denote\nnode features for simplicity.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3397\nEs is a matrix of modulation factors, which is produced by\nthe attention graph. P(vl\ni) is a biased vector, which is from\na learnable embedding bank corresponding to the centrality\nindex produced by the neighboring graph. Note that Es is\nset in the first layer of the transformer and kept constant for\nthe remaining layers, while P(vl\ni) is re-computed in every\nlayers. In the following subsections, we will elaborate on\nhow to calculate Es and P(vl\ni) using two tailored graphs.\nAttention Graph\nThe key to diverse attention maps lies in attending to com-\nplementary information. Based on this understanding, we\npropose an Edge Weight Regression (EWR) network to con-\nstruct edges of the attention graph while encoding the dis-\nsimilarity between patches into edge weights.\nSpecifically, EWR estimates the semantic content value\ncontained in each node and then determines the edges based\non the degree of difference between the semantic values. For\nexample, if the semantic content of nodes vA and vB have\na large gap, a strong edge will be formed between vA and\nvB to encourage the model to explore the relationship be-\ntween distinctive elements. On the other hand, if the nodevC\nhas a similar information content to the node vD, the model\nwill create a weaker or no connection betweenvC and vD to\nreduce excessive attention in the subsequent attention mod-\nule. We also extend the EWR to multiple heads to construct\nheterogeneous graphs with different edge connections to en-\ncoder different kinds of semantics, which is consistent with\nmulti-head attention. For head s, the weight and intensity of\nedge Es\nij ∈ Ea between nodes vi and vj can be obtained by\nEs\nij = |Fs\nE(v0\ni ) − Fs\nE(v0\nj )|. (3)\nFs\nE denotes a head-specific EWR module, consisting of a\nfeed-forward network (FFN) followed by an activating layer.\nWe use a sigmoid function to normalize the output within a\nrange of values from0 to 1. In particular,0 prohibits an edge\nconstruction. We directly apply the edges to modulate the\nattention mechanism based on their weights and intensities,\ni.e., E = Ea. EWR is in an anti-similarity fashion, diver-\nsifying the attention by appropriately reducing the intensity\nbetween weakly connected nodes. In this way, the attention\nis guided to be diversified, which can effectively discover the\nmissing context and residual details. In our model, the atten-\ntion graph will be generated by EWR after the backbone and\nremain constant throughout subsequent transformer layers.\nWe further introduce an Edge Regularization term to en-\ncourage the dissimilarity encoded by EWR within the same\nhorizontal line to be as little as possible. The idea is moti-\nvated by the existence of the perspective geometry of pin-\nhole cameras in crowd images, which usually generates per-\nspective or scale variations only along the vertical axis (Shi\net al. 2019). And within the same horizontal axis, these se-\nmantics remain consistent. Based on this observation, we\npropose a regularization to minimize the variance of the esti-\nmated semantic content value among nodes within the same\nhorizontal axis. The regularization can be written as\nQ =\nX\nh\nX\ni\ns.t. yi=h\n(FE(v0\ni ) −\nFh), (4)\nwhere yi is the horizontal coordinate of each node and Fh\nis the mean semantic value of the hth row.\nThe Rationality of the Edge Regularization. Due to the\nimaging process of perspective geometry, crowd images of-\nten exhibit similar scales along the horizontal axis, resulting\nin similar density values in the density map. As the vanilla\ntransformer tends to produce attention maps highly similar\nto the predicted density map, as observed in the introduc-\ntion, the attention map will also exhibit similar values along\nthe horizontal axis, leading to excessive focus between dif-\nferent locations on the same horizontal axis. Overly homo-\ngeneous attention maps are detrimental to counting perfor-\nmance. Therefore, to diversify the attention maps, the pro-\nposed edge regularization penalty suppresses cases where\nsimilar attention values are produced along the same hori-\nzontal axis and breaks the downside of excessive attention.\nFeature-based Centrality Encoding\nWe then go on to modulate the node features by looking at\nthe overall structure and discovering the role or importance\nof each node. To achieve this, we devise a Centrality Indices\nscheme and a feature-based neighboring graph, which ex-\nplores the overall similarity of all nodes and modulate the\nnode features by encoding the centrality representation P\ninto the node features.\nFor each node vi, we select its similarity-based top q-NN,\ndenoted as N(vi) where q is the percentage. We then con-\nstruct directed graph edges E′\nij ∈ Ec as follows:\nE′\nij =\n\u001a1, if vj ∈ N(vi)\n0, otherwise . (5)\nHere we adopt the L2 norm Euclidean distance to calculate\nthe similarity. Then, we gather all the neighbour sets of the\nentire graph as N = {N(v1), N(v2), ...,N(vN )}. To iden-\ntify the centrality, we count the number of occurrences of\neach node in N. Nodes with high occurrences (in-degree in\nthe graph) represent central positions in the feature space\nand we consider the occurrence as the centrality index. Sup-\npose that {0, 1, ..., m} are the set of centrality indices, where\nm is the upper bound. All occurrences will be normalized to\nensure that the maximum value does not exceed the bound.\nWe then introduce a learnable bank consisting of a series\nof centrality embedding vectors, denoted as{p1, p2, ..., pm},\np ∈ RC. Each node is encoded using the corresponding rep-\nresentation obtained through embedding matching. Specifi-\ncally, we match the node vl\nj with its centrality index i from\nthe bank, denoted as P(vl\nj) =pi, where i corresponds to the\noccurrences of node vl\nj within N. In this way, we modulate\nfeatures by encoding with distinctive vectors to incorporate\nthe feature structural information into representations.\nFurther Processing\nBased on the attention map, the features will be further pro-\ncessed by:\n˜vl\ni = LN(vl\ni + Concats\n\n\nNX\nj=1\nRs(ˆvl\ni, ˆvl\nj) · (vl\njWs\nv )\n\nWo),\n(6)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3398\nwhere Ws\nv ∈ RC×C\nS is the projection weight matrix and\nRs is the graph-modulated attention map obtained by Equa-\ntion 1. The layer normalization (LN) function is used to bal-\nance the scales of the values. All information fromS graphs\nwill be concatenated and finally integrated by the weight ma-\ntrix Wo ∈ RC×C to update the node features. With an ad-\nditional FFN (feedforward network) K to fuse the features,\nthe updated nodes can be obtained by:\nVl+1 = LN(˜Vl + K(˜Vl)). (7)\nTo be general, we denote the initial nodes extracted by the\nbackbone as V0, and each transformer layer updates the node\nfeatures to V1, ...,VL, where L is the number of layers.\nIt is important to note that while the attention graphEa re-\nmains constant throughout the L layers, the neighboring and\nsimilarity relationships will change in each layer after the\nupdate of features. This means that the feature-based neigh-\nboring graph is dynamic and the centrality indices in each\nlayer will adjust to represent these changes. It allows the\nmodel more effectively and timely to capture the dynamic\ncentrality feature positions of nodes.\nFinally, we transmit graph nodes VL into the density\nregression head to obtain the predicted density map. For\ncounting supervision, we utilize the Instance Attention\nLoss (IAL) to mitigate the negative impact of annotation\nnoise (Lin et al. 2022b). IAL dynamically focuses on the\nmost important annotations and ignores supervisions with\nhigh uncertainty. Thus the final loss is expressed by\nL = LIA + λQ, (8)\nwhere λ is the weight of the regularization.\nExperimental Results\nDatasets\nWe evaluate our crowd counting method and compare it with\nother state-of-the-art methods on four largest crowd count-\ning benchmarks. They are widely used in recent papers and\nare described as follows.\n• ShanghaiTech A (Zhang et al. 2016) includes 482 images\nwith 244,167 annotated points. 300 images are divided\nfor the training set and the remaining 182 images are for\nthe test set. The numbers of people vary from 33 to 3,139.\n• UCF-QNRF (Idrees et al. 2018) contains 1,535 crowded\nimages with about 1.25 million annotated points. It has a\nwide range of people count and images with an average\nresolution of 2,013 × 2,902. The training and test sets\ninclude 1,201 and 334 images, respectively.\n• NWPU-CROWD (Wang et al. 2020b) contains 5,109 im-\nages and 2.13 million annotated instances with point and\nbox labels. 3,109 images are used in the training set; 500\nimages are in the validation set; and the remaining 1,500\nimages are in the test set. Images in NWPU-CROWD are\nin largely various density and illumination scenes. It is\nthe largest dataset in which test labels are not released.\n• JHU-CROWD++ (Sindagi, Yasarla, and Patel 2020) has\na more complex context with 4,372 images and 1.51 mil-\nlion annotated points. 2,272 images are chosen for the\ntraining set; 500 images are for the validation set; and\nthe rest 1,600 images for the test set. The dataset con-\ntains diverse scenarios and is collected under different\nenvironmental conditions of weather and illumination.\nImplement Details\nNetwork Structure: VGG-19 (Pretrained on Ima-\ngeNet) (Simonyan and Zisserman 2014) is adopted as the\nbackbone network to extract features which will be for-\nmulated into graphs. The Edge Weight Regression (EWR)\nadopts two 3 × 3 convolution layers with a ReLU activation\nfunction in the middle and a sigmoid function at the end.\nThe density regression head consists of an upsampling layer\nand three convolution layers with two ReLUs in middle.\nThe kernel sizes of the first two layers are 3 × 3 and that of\nthe last is 1 × 1.\nTraining Details: We set the training batch size as 1 and\ncrop images with a size of 512 × 512. As some images in\nShanghaiTech A contain smaller resolution, the crop size\nfor this dataset changes to 256 × 256. Random scaling of\n[0.75, 1.25] and horizontal flipping are also adopted to aug-\nment each training image. We use Adam algorithm (Kingma\nand Ba 2014) with a learning rate 10−5 to optimize the pa-\nrameters. We set the percentage of nearest neighbors q as\n30%, and the maximum in-degree boundm as 18. The num-\nber of transformer layers L is 2 and the loss weight λ is 0.1.\nComparison with State-of-the-art Methods\nWe evaluate our model on the above four datasets and list\nrecent state-of-the-arts methods for comparison. The quan-\ntitative results, measured by MAE (Mean Absolute Error),\nMSE (Mean Square Error) and NAE (Normalized Absolute\nError), are listed in Tables 2 and 3.\nOur method achieves great counting accuracy on all four\nbenchmark datasets. Generally, by comparing Gramformer\nagainst other state-of-the-art methods, Gramformer outper-\nforms or at least is on par with these competitors on all\ndatasets. Specifically, on dense datasets NWPU and UCF-\nQNRF, our method achieves the best MAE and MSE. On\nJHU++, Gramformer performs best on one of the evaluation\nmetrics. And on the sparse dataset, ShanghaiTech A, Gram-\nformer ranks second and shows a competitive performance.\nFurthermore, we observe that Gramformer outperforms\nother approaches by large margins and is on par with the\nrecent methods MAN and CLTR, which are built based on\ntransformer. We believe the characteristic of the attention\nmodule in the transformer works effectively on crowd count-\ning task. And especially, it will help gain great accuracy\nimprovements on dense datasets, i.e., UCF-QNRF, JHU++\nand NWPU. Therefore, our graph-guided improvement in\nthe application of attention to the counting task is an inspir-\ning direction. We also present a visualization comparison in\nthe appendix.\nKey Issues and Discussion\nAblation Studies: We perform the ablation experiments\non UCF-QNRF to study the effect of each proposed term in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3399\nDataset ShanghaiTech A UCF-QNRF JHU++\nMethod MAE MSE MAE MSE MAE MSE\nMCNN (Zhang et al. 2016) (CVPR 16) 110.2 173.2 277 426 188.9 483.4\nCA-Net (Liu, Salzmann, and Fua 2019) (CVPR 19) 61.3 100.0 107.0 183.0 100.1 314.0\nCG-DRCN-CC (Sindagi, Yasarla, and Patel 2020) (PAMI 20) 60.2 94.0 95.5 164.3 71.0 278.6\nDPN-IPSM (Ma et al. 2020) (ACMMM 20) 58.1 91.7 84.7 147.2 - -\nBL (Ma et al. 2019) (ICCV 19) 62.8 101.8 88.7 154.8 75.0 299.9\nDM-Count (Wang et al. 2020a) (NIPS 20) 59.7 95.7 85.6 148.3 - -\nUOT (Ma et al. 2021) (AAAI 21) 58.1 95.9 83.3 142.3 60.5 252.7\nGL (Wan, Liu, and Chan 2021) (CVPR 21) 61.3 95.4 84.3 147.5 59.9 259.5\nS3 (Lin et al. 2021) (IJCAI 21) 57.0 96.0 80.6 139.8 59.4 244.0\nP2PNet (Song et al. 2021) (ICCV 21) 52.7 85.1 85.3 154.5 - -\nChfL (Shu et al. 2022) (CVPR 22) 57.5 94.3 80.3 137.6 57.0 235.7\nSTEERER (Han et al. 2023) 55.6 87.3 76.7 135.1 55.4 221.4\nMAN (Lin et al. 2021) (CVPR 22) 56.8 90.3 77.3 131.5 53.4 209.9\nCLTR (Liang, Xu, and Bai 2022) (ECCV 22) 56.9 95.2 85.8 141.3 59.5 240.6\nGramformer 54.7 87.1 76.7 129.5 53.1 228.1\nTable 2: Comparisons with the state of the arts on ShanghaiTech A, UCF-QNRF and JHU-Crowd++. The best performance is\nshown in bold and the second best is shown in underlined. Note that the results of STEERER are based on the VGG19, which\nshares the same backbone as our method.\nDataset Overall Scene Level (MAE)\nMethod MAE MSE NAE Avg. S0 ∽ S4\nMCNN (Zhang et al. 2016) 232.5 714.6 1.063 1171.9 356.0 / 72.1 / 103.5 / 509.5 / 4818.2\nSANet (Cao et al. 2018) 190.6 491.4 0.991 716.3 432.0 / 65.0 / 104.2 / 385.1 / 2595.4\nCSRNet (Li, Zhang, and Chen 2018) 121.3 387.8 0.604 522.7 176.0 / 35.8 / 59.8 / 285.8 /2055.8\nBL (Ma et al. 2019) 105.4 454.2 0.203 750.5 66.5 / 8.7 / 41.2 / 249.9 / 3386.4\nSFCN+ (Wang et al. 2020b) 105.7 424.1 0.254 712.7 54.2 / 14.8 / 44.4 / 249.6 / 3200.5\nUOT (Ma et al. 2021) 87.8 387.5 0.185 566.5 80.7 / 7.9 / 36.3 / 212.0 / 2495.4\nDMCount (Wang et al. 2020a) 88.4 388.6 0.169 498.0 146.7 / 7.6 / 31.2 / 228.7 / 2075.8\nGL (Wan, Liu, and Chan 2021) 79.3 346.1 0.180 508.5 92.4 / 8.2 / 35.4 / 179.2 / 2228.3\nS3 (Lin et al. 2021) 81.7 349.8 0.222 466.5 78.2 / 10.5 / 35.3 / 206.2 / 2002.4\nChfL (Shu et al. 2022) 76.8 343.0 0.170 470.1 56.7 / 8.4 / 32.1 / 195.1 / 2058.0\nMAN (Lin et al. 2022b) 76.5 323.0 0.170 464.6 43.3 / 8.5 / 35.3 / 190.9 / 2044.9\nGramformer 72.5 316.4 0.160 441.9 26.9 / 7.4 / 33.4 / 186.8 / 1955.2\nTable 3: Comparisons on NWPU-CROWD, which is further divided into five refined subsetsS0 ∽ S4 according to crowd size.\nComponents MAE MSE\nBaseline (VGG+IAL) 84.7 150.9\nB + EWR 79.3 135.4\nB + Centrality Indices 79.8 138.1\nB + EWR + CI 77.9 130.2\nB + EWR + Edge Regularization 78.1 132.8\nB + EWR + CI + ER 76.7 129.5\nTable 4: Ablation study on UCF-QNRF. After combining all\nproposed components, the model achieves its best accuracy.\nGramformer and provide quantitative results in Table 4. We\nstart with the baseline of VGG+IAL, which details can be\nreferred to MAN (Lin et al. 2022b). We first test the con-\ntribution of graph-modulated attention by EWR. The per-\nformance from baseline is improved by 5.4 and 15.5, for\nMAE and MSE, respectively. Then, we incorporate the cen-\ntral indices scheme into our model, the counting accuracy\nin terms of MAE is further improved by 3.9 without EWR\nand 1.4 with EWR. And finally, we adopt Edge Regulariza-\ntion to insert the prior knowledge of perspective geometry\nin crowd images. The model then achieves its best with76.7\nand 129.5 for MAE and MSE.\nm MAE MSE\n36 80.2 140.1\n24 78.4 136.9\n18 76.7 129.5\n12 80.2 138.7\nTable 5: The influence of the number of centrality embed-\nding vectors. Experiments are conducted on UCF-QNRF.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3400\nGraph-guided Attention and Graph Transformer: We\nstudy the effectiveness of the proposed graph-guided atten-\ntion. We choose Graphormer (Ying et al. 2021), one of the\nmost popular models among graph transformers, as our com-\nparison model. We extract its attention module to replace our\nproposed graph-guided attention. Specifically, its edge con-\nnections are constructed according to the nearest neighbor\nsimilarities. The feature of each edge is obtained by con-\ncatenating the features of two endpoint nodes and smooth-\ning with an MLP (Multilayer Perceptron). The feature of its\nedge is then encoded into a single value and added to the\nattention map, which is R in Equation 1. More detailed in-\nformation about the model can be found in the appendix.\nThe results of this baseline on UCF-QNRF is 80.9 for MAE\nand 142.5 for MSE, with a worse of 4.2 for MAE and 13.0\nfor MSE respectively. The comparison shows the efficacy of\nbuilding edges by diversifying the attention maps for more\nabundant information.\nThe Influence of Centrality Indices: We hold experi-\nments on UCF-QNRF to study the influence of the num-\nber of centrality embedding vectors. m is the upper bound\nof in-degree ranking values, determining the number of dis-\ntinguished embedding vectors. The results are shown in Ta-\nble 5. When m = 76.7, our model performs best. When m\nis reduced or elevated, the accuracy drops accordingly. We\nposit that an ill-suited value of m, resulting in either a lim-\nited number of encodings or an excessive set of embedding\nchoices, has the potential to undermine the performance.\nThe Influence of The Percentage of Nearest Neighborsq:\nq determines how many nodes are selected as neighbors of\na node when calculating its in-degree value. A lower q al-\nlows each node to select only the most similar ones, which\nreduces the number of nodes with high occurrences and cor-\nresponding centrality indices. And when q grows larger, the\nneighbor selection process becomes more relaxed. At this\npoint, nodes with high in-degree values will increase, influ-\nencing the learning ability of graph structure via embedding\nvectors. We hold experiments to study these influences. The\nresults on UCF-QNRF are shown in Table 6. Gramformer\nachieves its best when q = 0.3. And the accuracy drops\nwhen q gradually increases or decreases. It justifies the im-\nportance of the nearest neighbor selection process in the up-\ndating of graph structure and subsequent count predictions.\nAn appropriate number of selected neighbors will aid in im-\nproving the counting accuracy.\nWe also study the combined influence of m and q. Nine\nmodels are trained with different settings: a cross-product\nof m ∈ {12, 18, 24} and q ∈ {0.2, 0.3, 0.4}, to show a\nclear comparison. Table 7 reports MAE of counting results\non UCF-QNRF. The counting accuracy of the model is af-\nfected by different settings. And Gramformer performs best\nunder the setting in the center with m = 18and q = 0.3.\nRunning Cost Evaluation: In Table 8, we compare the\nmodel size, the floating point operations (FLOPs) computed\non one 384 × 384 input and the inference time for 100 im-\nages between Gramformer and other models including of\nViT-B (Dosovitskiy et al. 2020), Bayesian (Ma et al. 2019),\nq MAE MSE\n0.06 81.6 140.1\n0.08 78.3 136.0\n0.1 78.8 135.4\n0.2 77.9 132.3\n0.3 76.7 129.5\n0.4 78.2 133.1\nTable 6: The influence of the nearest neighbor selection.\nWhen calculating the in-degree value, the decision of each\nnode to connect to too many or too few (q ) neighbors will\nreduce the counting accuracy.\nCentrality Nearest Neighbors\nBank Size 0.2 0.3 0.4\n12 78.8 80.2 81.5\n18 77.9 76.7 78.2\n24 81.4 78.4 80.6\nTable 7: Performances of Gramformer under different set-\ntings of centrality embedding and nearest neighbors on\nUCF-QNRF (measured as MAE).\nModel Model Size (M) GFLOPs Inference time\nViT-B 86.0 55.4 21.3\nBayesian 21.5 56.9 10.3\nVGG19+Trans 29.9 58.0 10.8\nMAN 30.9 58.2 11.3\nGramformer 29.0 60.9 12.6\nTable 8: Running Cost Evaluation.\nthe combination of VGG19 and Transformer (Vaswani et al.\n2017) and MAN (Lin et al. 2022b). All experiments are con-\nducted with a single RTX 3080 GPU. Gramformer has a\nslight increase in FLOPs and inference time, for the com-\nputation of nearest neighbours in forward predictions. How-\never, our model is smaller than VGG19+Trans and MAN,\nand significantly smaller than ViT-B. This means that the\nproposed components are lightweight.\nConclusion\nThis paper aims to enhance the ability of transformers to\nmodulate the attention mechanism and input node features\nrespectively on the basis of two different types of graphs. We\ncontribute to diversifying the attention map to attend to more\ncomplementary information by proposing a graph-guided at-\ntention modulation. We also encode the centrality or impor-\ntance of nodes by designing a centrality indices scheme to\nadjust the input node features. The proposed Gramformer\nachieves high counting accuracy on popular crowd counting\ndatasets. Improving the transformer network by graph mod-\nulation is an inspiring direction, and we will apply it to a\nwider range of vision tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3401\nAcknowledgements\nThis work is funded by the National Natural Sci-\nence Foundation of China (62076195, 62206271,\n62376070, 12226004 and 61721002), the Funda-\nmental Research Funds for the Central Universities\n(AUGA5710011522), the Guangdong Basic and Ap-\nplied Basic Research Foundation (2020B1515130004),\nand the Shenzhen Key Technical Projects under Grant\n(JSGG20220831105801004, CJGJZD2022051714160501,\nand JCYJ20220818101406014).\nReferences\nCao, X.; Wang, Z.; Zhao, Y .; and Su, F. 2018. Scale aggregation\nnetwork for accurate and efficient crowd counting. In ECCV.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov, A.; and\nZagoruyko, S. 2020. End-to-end object detection with transform-\ners. In ECCV.\nChen, X.; Bin, Y .; Gao, C.; Sang, N.; and Tang, H. 2020. Relevant\nregion prediction for crowd counting. Neurocomputing.\nChen, X.; Yan, B.; Zhu, J.; Wang, D.; Yang, X.; and Lu, H. 2021.\nTransformer tracking. In CVPR.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai,\nX.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;\nGelly, S.; et al. 2020. An Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale. In ICLR.\nDwivedi, V . P.; and Bresson, X. 2020. A generalization of trans-\nformer networks to graphs. arXiv preprint.\nDwivedi, V . P.; Luu, A. T.; Laurent, T.; Bengio, Y .; and Bresson,\nX. 2021. Graph Neural Networks with Learnable Structural and\nPositional Representations. In ICLR.\nHan, K.; Wang, Y .; Guo, J.; Tang, Y .; and Wu, E. 2022. Vision\nGNN: An Image is Worth Graph of Nodes. In NIPS.\nHan, T.; Bai, L.; Liu, L.; and Ouyang, W. 2023. Steerer: Resolving\nscale variations for counting and localization via selective inheri-\ntance learning. In ICCV, 21848–21859.\nIdrees, H.; Saleemi, I.; Seibert, C.; and Shah, M. 2013. Multi-\nsource multi-scale counting in extremely dense crowd images. In\nICCV.\nIdrees, H.; Tayyab, M.; Athrey, K.; Zhang, D.; Al-Maadeed, S.;\nRajpoot, N.; and Shah, M. 2018. Composition loss for counting,\ndensity map estimation and localization in dense crowds. InECCV.\nKingma, D. P.; and Ba, J. L. 2014. ADAM: AMETHOD FOR\nSTOCHASTIC OPTIMIZATION. arXiv preprint.\nKreuzer, D.; Beaini, D.; Hamilton, W.; L´etourneau, V .; and Tossou,\nP. 2021. Rethinking graph transformers with spectral attention.\nNIPS.\nLi, Y .; Zhang, X.; and Chen, D. 2018. Csrnet: Dilated convolutional\nneural networks for understanding the highly congested scenes. In\nCVPR.\nLiang, D.; Chen, X.; Xu, W.; Zhou, Y .; and Bai, X. 2022. Tran-\nscrowd: weakly-supervised crowd counting with transformers.Sci-\nence China Information Sciences.\nLiang, D.; Xu, W.; and Bai, X. 2022. An end-to-end transformer\nmodel for crowd localization. In ECCV.\nLin, H.; Hong, X.; Ma, Z.; Wei, X.; Qiu, Y .; Wang, Y .; and Gong,\nY . 2021. Direct Measure Matching for Crowd Counting. InIJCAI.\nLin, H.; Ma, Z.; Hong, X.; Wang, Y .; and Su, Z. 2022a. Semi-\nsupervised Crowd Counting via Density Agency. In ACM MM.\nLin, H.; Ma, Z.; Ji, R.; Wang, Y .; and Hong, X. 2022b. Boosting\nCrowd Counting via Multifaceted Attention. In CVPR.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P. 2017.\nFocal loss for dense object detection. In ICCV.\nLiu, L.; Qiu, Z.; Li, G.; Liu, S.; Ouyang, W.; and Lin, L. 2019.\nCrowd counting with deep structured scale integration network. In\nICCV, 1774–1783.\nLiu, L.; Wang, H.; Li, G.; Ouyang, W.; and Lin, L. 2018. Crowd\nCounting using Deep Recurrent Spatial-Aware Network. In IJCAI.\nLiu, W.; Salzmann, M.; and Fua, P. 2019. Context-aware crowd\ncounting. In CVPR.\nLiu, Y .; Liu, L.; Wang, P.; Zhang, P.; and Lei, Y . 2020. Semi-\nsupervised crowd counting via self-training on surrogate tasks. In\nECCV.\nLiu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.; and Xie,\nS. 2022. A convnet for the 2020s. In CVPR.\nLuo, A.; Yang, F.; Li, X.; Nie, D.; Jiao, Z.; Zhou, S.; and Cheng, H.\n2020. Hybrid graph neural networks for crowd counting. In AAAI.\nMa, Z.; Wei, X.; Hong, X.; and Gong, Y . 2019. Bayesian loss for\ncrowd count estimation with point supervision. In ICCV.\nMa, Z.; Wei, X.; Hong, X.; and Gong, Y . 2020. Learning scales\nfrom points: A scale-aware probabilistic model for crowd counting.\nIn ACMMM.\nMa, Z.; Wei, X.; Hong, X.; Lin, H.; Qiu, Y .; and Gong, Y . 2021.\nLearning to count via unbalanced optimal transport. In AAAI.\nMialon, G.; Chen, D.; Selosse, M.; and Mairal, J. 2021. Graphit:\nEncoding graph structure in transformers. arXiv preprint.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agar-\nwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.\nLearning transferable visual models from natural language super-\nvision. In ICML. PMLR.\nRamp´aˇsek, L.; Galkin, M.; Dwivedi, V . P.; Luu, A. T.; Wolf, G.;\nand Beaini, D. 2022. Recipe for a general, powerful, scalable graph\ntransformer. arXiv preprint.\nRedmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016. You\nonly look once: Unified, real-time object detection. In CVPR.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To-\nwards real-time object detection with region proposal networks.\nNIPS.\nShi, M.; Yang, Z.; Xu, C.; and Chen, Q. 2019. Revisiting perspec-\ntive information for efficient crowd counting. In CVPR.\nShu, W.; Wan, J.; Tan, K. C.; Kwong, S.; and Chan, A. B. 2022.\nCrowd counting in the frequency domain. In CVPR.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint.\nSindagi, V .; Yasarla, R.; and Patel, V . M. 2020. Jhu-crowd++:\nLarge-scale crowd counting dataset and a benchmark method.\nPAMI.\nSindagi, V . A.; and Patel, V . M. 2017. Generating high-quality\ncrowd density maps using contextual pyramid cnns. In ICCV.\nSong, Q.; Wang, C.; Jiang, Z.; Wang, Y .; Tai, Y .; Wang, C.; Li, J.;\nHuang, F.; and Wu, Y . 2021. Rethinking Counting and Localization\nin Crowds: A Purely Point-Based Framework. In ICCV.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. In Advances in neural information processing sys-\ntems.\nWan, J.; Liu, Z.; and Chan, A. B. 2021. A Generalized Loss Func-\ntion for Crowd Counting and Localization. In CVPR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3402\nWang, B.; Liu, H.; Samaras, D.; and Nguyen, M. H. 2020a. Distri-\nbution Matching for Crowd Counting. NIPS.\nWang, F.; Mei, J.; and Yuille, A. 2023. SCLIP: Rethinking Self-\nAttention for Dense Vision-Language Inference. arXiv preprint.\nWang, Q.; Gao, J.; Lin, W.; and Li, X. 2020b. NWPU-crowd: A\nlarge-scale benchmark for crowd counting and localization. PAMI.\nWu, B.; and Nevatia, R. 2005. Detection of multiple, partially\noccluded humans in a single image by bayesian combination of\nedgelet part detectors. In ICCV.\nWu, Z.; Jain, P.; Wright, M.; Mirhoseini, A.; Gonzalez, J. E.; and\nStoica, I. 2021. Representing long-range context for graph neural\nnetworks with global attention. NIPS.\nWu, Z.; Zhang, X.; Tian, G.; Wang, Y .; and Huang, Q. 2022.\nSpatial-Temporal Graph Network for Video Crowd Counting.\nIEEE Transactions on Circuits and Systems for Video Technology.\nYan, Z.; Yuan, Y .; Zuo, W.; Tan, X.; Wang, Y .; Wen, S.; and\nDing, E. 2019. Perspective-guided convolution networks for crowd\ncounting. In ICCV.\nYang, Y .; Li, G.; Du, D.; Huang, Q.; and Sebe, N. 2020a. Embed-\nding perspective analysis into multi-column convolutional neural\nnetwork for crowd counting. IEEE Transactions on Image Pro-\ncessing.\nYang, Y .; Li, G.; Wu, Z.; Su, L.; Huang, Q.; and Sebe, N. 2020b.\nReverse perspective network for perspective-aware object count-\ning. In CVPR.\nYang, Y .; Li, G.; Wu, Z.; Su, L.; Huang, Q.; and Sebe, N. 2020c.\nWeakly-supervised crowd counting learns from sorting rather than\nlocations. In ECCV.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen, Y .; and\nLiu, T.-Y . 2021. Do transformers really perform badly for graph\nrepresentation? NIPS.\nZeng, L.; Xu, X.; Cai, B.; Qiu, S.; and Zhang, T. 2017. Multi-scale\nconvolutional neural networks for crowd counting. In ICIP.\nZhai, Q.; Yang, F.; Li, X.; Xie, G.-S.; Cheng, H.; and Liu, Z. 2022.\nCo-Communication Graph Convolutional Network for Multi-View\nCrowd Counting. IEEE Transactions on Multimedia.\nZhang, Y .; Zhou, D.; Chen, S.; Gao, S.; and Ma, Y . 2016. Single-\nimage crowd counting via multi-column convolutional neural net-\nwork. In CVPR.\nZhao, M.; Zhang, J.; Zhang, C.; and Zhang, W. 2019. Leveraging\nheterogeneous auxiliary tasks to assist crowd counting. In CVPR.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu, Y .;\nFeng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with trans-\nformers. In CVPR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n3403",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5779425501823425
    },
    {
      "name": "Transformer",
      "score": 0.4555034637451172
    },
    {
      "name": "Graph",
      "score": 0.4300227761268616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3483999967575073
    },
    {
      "name": "Theoretical computer science",
      "score": 0.30905774235725403
    },
    {
      "name": "Engineering",
      "score": 0.1213890016078949
    },
    {
      "name": "Electrical engineering",
      "score": 0.11410805583000183
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ],
  "cited_by": 24
}