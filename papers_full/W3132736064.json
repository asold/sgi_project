{
  "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
  "url": "https://openalex.org/W3132736064",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286990194",
      "name": "Zhao, Tony Z.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913157063",
      "name": "Wallace, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1945348815",
      "name": "Feng Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2615303194",
      "name": "Klein, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221445710",
      "name": "Singh, Sameer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2950708443",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3085177480",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W1618905105",
    "https://openalex.org/W2922091957",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2403947200",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W2073241381",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3106954555",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3035131109",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2963012544"
  ],
  "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",
  "full_text": "Calibrate Before Use:\nImproving Few-Shot Performance of Language Models\nTony Z. Zhao* 1 Eric Wallace* 1 Shi Feng 2 Dan Klein 1 Sameer Singh 3\nAbstract\nGPT-3 can perform numerous tasks when pro-\nvided a natural language prompt that contains a\nfew training examples. We show that this type of\nfew-shot learning can be unstable: the choice of\nprompt format, training examples, and even the\norder of the training examples can cause accuracy\nto vary from near chance to near state-of-the-art.\nWe demonstrate that this instability arises from\nthe bias of language models towards predicting\ncertain answers, e.g., those that are placed near\nthe end of the prompt or are common in the pre-\ntraining data. To mitigate this, we ﬁrst estimate\nthe model’s bias towards each answer by asking\nfor its prediction when given the training prompt\nand a content-free test input such as “N/A”. We\nthen ﬁt calibration parameters that cause the pre-\ndiction for this input to be uniform across answers.\nOn a diverse set of tasks, this contextual calibra-\ntion procedure substantially improves GPT-3 and\nGPT-2’s average accuracy (up to 30.0% absolute)\nand reduces variance across different choices of\nthe prompt.\n1. Introduction\nFew-shot learning—the ability to learn tasks with limited\nexamples—is an important aspect of intelligence (Lake et al.,\n2015; Yogatama et al., 2019). Recent work shows that large\nneural language models can perform few-shot learning with-\nout ﬁnetuning (Radford et al., 2019; Brown et al., 2020).\nSpeciﬁcally, GPT-3 (Brown et al., 2020) can perform nu-\nmerous tasks when provided a few examples in a natural\nlanguage prompt. For example, to perform sentiment analy-\nsis one can condition GPT-3 on a prompt such as:\nInput: Subpar acting. Sentiment: Negative\nInput: Beautiful ﬁlm. Sentiment: Positive\nInput: Amazing. Sentiment:\n*Equal contribution 1UC Berkeley 2University of Mary-\nland 3UC Irvine. Correspondence to: Eric Wallace <ericwal-\nlace@berkeley.edu>.\nwhere the ﬁrst two lines correspond to two training examples\nand the last line is a test example. To make predictions, the\nmodel predicts whether the subsequent token is more likely\nto be the word “Positive” or “Negative”.\nThis style of few-shot “in-context” learning is interesting\nbecause it shows that the model can learn without parameter\nupdates. And, more importantly, it has numerous practi-\ncal advantages over the now-standard approach of ﬁnetun-\ning (Radford et al., 2018; Devlin et al., 2019). First, it allows\npractitioners to “rapidly prototype” NLP models: changing\nthe prompt immediately leads to a new model. Second, it\nprovides a fully natural language interface to a machine\nlearning model, which allows users—even those without\ntechnical expertise—to create NLP systems. Finally, since\nin-context learning reuses the same model for each task, it\nreduces memory requirements and system complexity when\nserving many different tasks.\nHowever, despite these promises, we show that GPT-3’s\naccuracy can be highly unstable across different prompts\n(Section 3). A prompt contains three components: a format,\na set of training examples, and a permutation (ordering) for\nthose examples. We show that different choices for these\nfactors can lead to highly different accuracies, e.g., changing\nthe permutation of the training examples in a sentiment\nanalysis prompt can change accuracy from near chance\n(54%) to near state-of-the-art (93%). This instability implies\nthat GPT-3 users, who typically design prompts manually,\ncannot expect to consistently obtain good accuracy.\nWe next analyze what causes this instability. We identify\nthree pitfalls of language models that lead them to be bi-\nased toward certain answers during few-shot learning. In\nparticular, they suffer from majority label bias, recency bias,\nand common token bias (Section 4). The majority label and\nrecency biases lead the model to predict training answers\nthat appear frequently or near the end of the prompt. For\nexample, a prompt that ends with a Negative training ex-\nample may cause a bias towards the Negative class. On\nthe other hand, the common token bias leads the model to\nprefer answers that are frequent in its pre-training data, e.g.,\nit prefers “United States” over “Saint Lucia”, which is likely\nsuboptimal for the task of interest.\nWe identify that these biases typically result in a shift in\nthe output distribution of the model. We can thus coun-\narXiv:2102.09690v2  [cs.CL]  10 Jun 2021\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n0 1 4 8 16\nNumber of Training Examples\n40\n50\n60\n70\n80\n90AGNews Accuracy (%)GPT-3 175B\nWith Calibration\n0 1 4 8\nNumber of Training Examples\n40\n50\n60\n70\n80MIT Director Accuracy (%)\nGPT-3 13B\nWith Calibration\n0 1 4 8 16\nNumber of Training Examples\n30\n40\n50\n60\n70\n80DBPedia Accuracy (%)GPT-3 2.7B\nWith Calibration\nFigure 1.Few-shot learning can be highly unstable across different choices of the prompt. Above, we plot the mean accuracy ( ± one\nstandard deviation) across different choices of the training examples for three different datasets and model sizes. We show that our method,\ncontextual calibration, improves accuracy, reduces variance, and overall makes tools like GPT-3 more effective for end users.\nteract these biases by “calibrating” the output distribution.\nConcretely, we estimate the model’s bias towards certain an-\nswers by feeding in a dummy test input that is content-free.\nIn the prompt above for example, if we replace “Amazing.”\nwith the string “N/A”, the model predicts 62% Positive. We\nthen ﬁt the calibration parameters so that the content-free\ninput has uniform scores for each answer. This contextual\ncalibration procedure provides a good setting of the calibra-\ntion parameters without additional training data.\nWe test the effectiveness of contextual calibration on a range\nof tasks (Section 5). Contextual calibration consistently\nimproves GPT-3 and GPT-2’s accuracy (up to 30.0% ab-\nsolute) across different choices of the prompt format and\nexamples (e.g., Figure 1). It also makes the accuracy more\nstable across different prompts, thus mitigating the need\nfor prompt engineering. Overall, contextual calibration is a\nsimple method that makes language models better few-shot\nlearners: it enables end users to obtain higher accuracy with\nconsiderably less effort.\n2. Background and Experimental Setup\nNeural autoregressive language models (LMs) take as input\na sequence of tokens and output a probability distribution\nover the next token. Large neural LMs can perform tasks in a\nzero- or few-shot manner using in-context learning (Radford\net al., 2019; Brown et al., 2020). To do so, a natural language\nprompt is fed into the model. This prompt contains three\ncomponents: a format, a set of training examples, and a\npermutation (ordering) of the training examples.\nPrompt Format The prompt format is a template which\nconsists of placeholders for the training and test example(s)\nand possibly a natural language description of the task. For\nexample, the format of the prompt in Section 1 is a template\nwith the style: “Input:” input “Sentiment:” label. Many\nalternate formats exist, e.g., one could frame the task as\nquestion answering.\nPrompt Training Examples The prompt’straining exam-\nples are used to teach the LM how to solve the task at\nhand. The prompt from Section 1 consists of two training\nexamples; we refer to this as “two-shot” learning. We also\nconsider “zero-shot” learning, where no training examples\nare present.\nTraining Example Permutation When training examples\nare used, they have a particular permutation, e.g., the “Sub-\npar acting” example comes ﬁrst in the prompt from Sec-\ntion 1. The permutation matters because neural language\nmodels update their hidden states in a left-to-right-fashion.\nTo make predictions on an input, we slot it into the test\nplaceholder and generate from the LM. For example, see the\n“Amazing.” test example in the prompt from Section 1. For\ngeneration tasks, we generate greedily from the LM until\nit produces a newline character. For classiﬁcation tasks,\nthe probability for each class is given by the probability\nassigned to its associated label name, e.g., the words “Nega-\ntive” and “Positive” for sentiment classiﬁcation.\n2.1. Datasets and Prompt Formats\nWe use datasets for three tasks: text classiﬁcation, fact\nretrieval, and information extraction. We use a ﬁxed prompt\nformat for each dataset unless otherwise speciﬁed. We show\nthe format and examples from each dataset in Appendix B.\nText Classiﬁcation We study text classiﬁcation using six\ndatasets: sentiment analysis using SST-2 (Socher et al.,\n2013), 6-way question classiﬁcation usingTREC (V oorhees\n& Tice, 2000), textual entailment using 3-wayCB (de Marn-\neffe et al., 2019) and binary RTE (Dagan et al., 2005) from\nSuperGLUE (Wang et al., 2019), and topic classiﬁcation\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n1 2 3 4 5 6 7 8 9 10\nTraining Set ID\n50\n60\n70\n80\n90SST-2 Accuracy (%)\nAccuracy Across Training Sets and Permutations\nFigure 2.There is high variance in GPT-3’s accuracy as we change\nthe prompt’straining examples, as well as the permutation of the\nexamples. Here, we select ten different sets of four SST-2 training\nexamples. For each set of examples, we vary their permutation and\nplot GPT-3 2.7B’s accuracy for each permutation (and its quartiles).\n1 2 3 4 5 6 7 8 9 10\nFormat ID\n50\n60\n70\n80\n90SST-2 Accuracy (%)\nAccuracy Across Formats and Training Sets\nFigure 3.There is high variance in GPT-3’s accuracy as we change\nthe prompt format. In this ﬁgure, we use ten different prompt\nformats for SST-2. For each format, we plot GPT-3 2.7B’s accuracy\nfor different sets of four training examples, along with the quartiles.\nusing the 4-way AGNews (Zhang et al., 2015) and 14-way\nDBPedia (Zhang et al., 2015) datasets. The prompt in Sec-\ntion 1 shows an example of the sentiment analysis task.\nFact Retrieval We evaluate fact retrieval with LAMA\n(Petroni et al., 2019). The dataset consists of knowledge\nbase triples that are placed into templates with missing ob-\njects, e.g. “Obama was born in”. We use these templates\nas our prompts, and remove the relations where the missing\nanswer is not at the end of the template (left-to-right LMs\ncannot solve these). The answers are always single tokens,\nand we report average accuracy across all triples.\nInformation Extraction We consider information extrac-\ntion using two slot ﬁlling datasets, ATIS (Hemphill et al.,\n1990) and MIT Movies trivia10k13 (Liu et al., 2012). We\nuse two random slots for each dataset, airline and departure\ndate for ATIS, and director name and movie genre for MIT\nMovies. The answer for both datasets is a span of text from\nthe input, e.g., the ATIS airline task is to predict “american\nairlines” when given the sentence “list a ﬂight on american\nairlines from toronto to san diego”. We use Exact Match\nbetween the model’s generated output and the ground-truth\nspan as our evaluation metric.\n2.2. Model Details\nWe run our experiments on three sizes of GPT-3 (2.7B, 13B,\nand 175B parameters) as well as GPT-2 (1.5B parameters).\nWe access GPT-3 using the OpenAI API. We release code\nto replicate our experiments.1\n1https://www.github.com/tonyzhaozh/few-shot-learning\n3. Accuracy Varies Highly Across Prompts\nThis section studies how GPT-3’s accuracy changes as we\nvary each aspect of the prompt (training examples, permu-\ntation, format). We focus on a subset of the datasets to\nsimplify our analysis; in Section 5 we show that our ﬁnd-\nings hold across all of the datasets we study.\nGPT-3’s accuracy depends highly on both selection and\npermutation of training examples. Concretely, we use a\nﬁxed prompt format and choose different random sets of\ntraining examples. For each set of training examples, we\nevaluate the accuracy for all possible permutations.\nFigure 2 shows the results for SST-2 (4-shot, GPT-3 2.7B).\nSurprisingly, varying the permutation can be as important,\nor even more important, than which training examples are\nchosen. For example, varying the permutation of the train-\ning examples can cause accuracy to go from near chance\n(54.3%) to near state-of-the-art (93.4%). For a qualitative\nexample of the sensitivity to permutations, see Table 2 in\nAppendix A. This high importance on example order is in\ncontrast to standard machine learning, where the ordering\nof examples during training is typically an afterthought.\nThe variance persists with more data and larger models.\nAdding more training examples into the prompt does not\nnecessarily reduce the variance in accuracy. We sweep over\nthe number of training examples for three different datasets\nin Figure 1 (red curves). The variance remains high even\nwhen we use 16 training examples. Moreover, adding more\ntraining examples can sometimes hurt accuracy (e.g., mean\naccuracy drops from 36.0% to 25.9% for DBPedia 0-shot\nto 1-shot). The variance in accuracy can also remain high\nwhen using larger models, e.g., the left of Figure 1.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n[P,P,P,P] [N,P,P,P] [P,N,P,P] [P,P,N,P] [P,P,P,N] [N,N,P,P] [N,P,N,P] [P,N,N,P] [N,P,P,N] [P,N,P,N] [P,P,N,N] [N,N,N,P] [N,N,P,N] [N,P,N,N] [P,N,N,N] [N,N,N,N]\n0\n0.2\n0.4\n0.6\n0.8\n1.0Probability\np(Positive)\nFigure 4.Majority label and recency biasescause GPT-3 to become biased towards certain answers and help to explain the high variance\nacross different examples and orderings. Above, we use 4-shot SST-2 with prompts that have different class balances and permutations,\ne.g., [P P N N] indicates two positive training examples and then two negative. We plot how often GPT-3 2.7B predicts Positive on the\nbalanced validation set. When the prompt is unbalanced, the predictions are unbalanced ( majority label bias). In addition, balanced\nprompts that have one class repeated near the end, e.g., end with two Negative examples, will have a bias towards that class (recency bias).\nGPT-3’s accuracy depends highly on prompt format.\nWe next keep the set of training examples and permutations\nﬁxed but vary the prompt format. We focus on SST-2, and\nwe manually design an additional 14 prompt formats. The\nformats include question-answer templates, conversation-\nstyle templates, prompts that resemble Web pages, and vari-\nations on the label names (all formats available in Table 7 in\nAppendix B). The accuracy for ten of the formats is shown\nin Figure 3. We ﬁnd that some of the formats are better than\nothers on average. However, all of the formats still suffer\nfrom high variance across different training sets.\n4. What Causes the High Variance?\nWe next analyze why GPT-3’s accuracy varies across differ-\nent training examples, permutations, and prompt formats.\nConcretely, we show that the variance arises because LMs\nare biased towards outputting answers that are (1) frequent\nin the prompt (majority label bias), (2) towards the end of the\nprompt (recency bias), and (3) common in the pre-training\ndata (common token bias).\nMajority Label Bias We ﬁnd that GPT-3 is biased towards\nanswers that are frequent in the prompt. A trivial case is\nwhen a text classiﬁcation prompt has a class imbalance, e.g.,\nmore Positive than Negative sentiment examples. This is\ndemonstrated in the “unbalanced” region of Figure 4: when\none class is more common, GPT-3 2.7B is heavily biased\ntowards predicting that class. Since the SST-2 sentiment\nanalysis dataset is balanced, this bias causes large accuracy\ndegradations. The majority label bias also explains why we\nfrequently observe a drop in accuracy when moving from\n0-shot to 1-shot—we found that the drop is due to the model\nfrequently repeating the class of the one training example.\nThe majority label bias also occurs for generation tasks. On\nthe validation set for 4-shot LAMA with GPT-3 2.7B, 50.2%\nof the model predictions are a repeat of one of the four train-\ning answers (the correct repeat rate is 24.7%). Overall, the\nmajority label bias helps to explain why different choices for\nthe training examples heavily inﬂuence GPT-3’s accuracy—\nit shifts the distribution of model predictions.\nRecency Bias The model’s majority label bias is aggravated\nby its recency bias: the tendency to repeat answers that ap-\npear towards the end of the prompt. The “balanced” region\nof Figure 4 demonstrates this. For instance, when two Neg-\native examples appear at the end (P P N N), the model will\nheavily prefer the Negative class. Moreover, the recency\nbias can outweigh the majority label bias, e.g., the “P P P\nN” training set leads to nearly 90% of predictions being\nNegative, despite 3\n4 of the training examples being Positive.\nRecency bias also affects generation tasks. For 4-shot\nLAMA, the training answers that are closer to the end of the\nprompt are more likely to be repeated by the model. Con-\ncretely, the model “overpredicts” the answer from the 1st,\n2nd, 3rd, and 4th training example by 8.5%, 8.3%, 14.3%,\nand 16.1%, respectively.2 Overall, recency bias helps to\nexplain why the permutation of the training examples is\nimportant—the ordering of the examples heavily inﬂuences\nthe distribution of the model predictions.\nCommon Token Bias Finally, we ﬁnd that GPT-3 is bi-\nased towards outputting tokens that are common in its pre-\ntraining distribution, which is likely suboptimal for the dis-\ntribution of answers on the downstream task. A simple case\nof this occurs for the LAMA fact retrieval dataset, where\nthe model often predicts common entities such as “America”\nwhen the ground-truth answer is instead a rare entity.\nA more nuanced case of the common token bias occurs for\n2Over all relations, as well as three different sets of training\nexamples, the model repeats the training example at a rate of\n20.7%, 19.8%, 29.9%, and 26.8%. The ground-truth repeat rate is\n12.2%, 11.5%, 15.6%, and 10.7%. We deﬁne “overpredicts” as the\nmodel’s repeat rate minus the ground-truth repeat rate.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\ntext classiﬁcation. Recall that the model makes predictions\nby generating the label name associated with each class.\nBecause certain label names appear more frequently in the\npre-training data, the model will be inherently biased to-\nwards predicting certain classes. For example, on DBPedia\n(a balanced 14-way topic classiﬁcation dataset), GPT-3 pre-\ndicts the “book” class 11× more often than the “artist” class.\nIn fact, there is a moderate correlation (r = 0.67) between\nthe frequency of a DBPedia label name and the rate at which\nGPT-3 predicts its class.3 Overall, the common token bias\nhelps to explain why the choice of label names is important,\nand why the model struggles on rare answers.\nThe Impact of Biases on Model Predictions We ﬁnd that\nthe end result of the above three biases is typically a sim-\nple shift in the model’s output distribution. For example,\nFigure 5 visualizes this shift for a SST-2 sentiment prompt.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\np(Positive)\nPrompt With 67% Accuracy\nFigure 5.The Positive class probability for 25 random test inputs\nfor a particular sentiment analysis prompt. Negative ground-truth\nexamples are marked with G and Positive are marked with G.\nThe prompt used in Figure 5 and the model’s intrinsic biases\ncause it to frequently predict high conﬁdence for the Positive\nclass. Since the default 50% threshold is used to make pre-\ndictions, this results in frequent false positives. Importantly,\nnote that if we could optimally set the classiﬁcation thresh-\nold (p(Positive) = 0.68 in this case), the classiﬁer would be\nhighly accurate (94% on the validation set).\n5. Contextual Calibration\nThus far, we have shown that GPT-3 is biased towards cer-\ntain answers due to the prompt and the model’s intrinsic\nbiases. Here, we look to correct this by “calibrating” the\nmodel’s output probabilities.4 A common technique for\nadjusting output probabilities is to apply an afﬁne transfor-\nmation (Platt, 1999; Guo et al., 2017):\nˆ q= softmax(Wˆ p+ b), (1)\nwhere a weight matrix W and a bias vector b are applied\nto the original probabilities ˆ pto get the new probabilities\n3The frequency of a token on the web is calculated using\nGoogle Ngrams https://books.google.com/ngrams. The pre-\ndictions are from the 0-shot setting on the validation set.\n4The output of GPT-3 is biased (its outputs are shifted), similar\nto how measurement devices such as voltage meters or weighing\nscales are biased. Just like how these devices require “calibration\nbefore use”, where the devices’ outputs are scaled/zeroed-out, we\nhope to apply a similar calibration procedure to LMs. This goal is\ndistinct from statistical calibration (Brier, 1950; Guo et al., 2017),\ni.e., aligning a model’s conﬁdence estimate with its true accuracy.\nˆ q.5 For classiﬁcation tasks, ˆ pis the set of probabilities that\nare associated with each label name, renormalized to one.\nFor generation tasks, ˆ pis the entire set of probabilities for\nthe ﬁrst token.6 In this paper, we restrict the matrix W to\nbe diagonal, known as vector scaling (Guo et al., 2017), to\nprevent the parameters from growing quadratically in the\nsize of ˆ p(which is ≈ 50, 000 for generation tasks).\nThe main challenge in the zero- or few-shot setting is that\nwe do not have data to learn W and b. We thus propose a\nnovel data-free procedure to infer a good setting of these\nparameters. The key idea is that the model’s bias towards\ncertain answers can be estimated by feeding in a content-\nfree input such as the string “N/A”. For example, consider\nthe two-shot prompt:\nInput: Subpar acting. Sentiment: Negative\nInput: Beautiful ﬁlm. Sentiment: Positive\nInput: N/A Sentiment:\nwhere “N/A” serves as the test input. Ideally, GPT-3 would\nscore this test input as 50% Positive and 50% Negative.\nHowever, the model’s biases cause it to score this input as\n61.8% Positive. Note that this error iscontextual: a different\nchoice of the training examples, permutation, and format\nwill lead to different predictions for the content-free input.\nWe can correct this error by setting W and b so that the\nclass scores for the content-free input are uniform. We ﬁrst\nobtain ˆ pfor the content-free input, denoted ˆ pcf. We then set\nW = diag(ˆ pcf)−1 and b to the all-zero vector.7 To make\ntest predictions, we compute Wˆ p+ b and take the argmax.\nImplementation Details This contextual calibration proce-\ndure adds trivial amounts of computational overhead and\nis implemented in a few lines of code (compute and save\nˆ pcf, adjust output probabilities). For the content-free in-\nput, many good choices exist, including “N/A”, the empty\nstring, and gibberish tokens. In all our experiments, we aver-\nage the probabilities from three content-free inputs: “N/A”,\n“[MASK]”, and the empty string.8 One could also craft the\ncontent-free input in a task-speciﬁc manner. We explore this\nfor LAMA, where we replace the subject with the content-\nfree input, e.g., we use “N/A was born in” as the input.\n5This afﬁne transformation is usually applied to the logits, i.e.,\nprior to the softmax. However, we only have access to GPT-3’s\noutput probabilities in the OpenAI API.\n6We only calibrate the prediction of the ﬁrst output token for\ngeneration tasks. This is reasonable because, for the tasks we\nconsider, we found that the model’s predictions are highly deter-\nministic after generating the ﬁrst token.\n7An alternate solution is to set b to −ˆ pcf and W to the identity.\nEmpirically, this alternate solution yields higher accuracy for gen-\neration tasks (where the dimensionality of ˆ pis large). The solution\nin the main text performs better for classiﬁcation.\n8We found this simple ensemble to achieve the best results for\nAGNews, and we reuse it for all other datasets. See Section 5.2 for\nan ablation on the choice of content-free input.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nDataset LM 0-shot 1-shot 4-shot 8-shot\nBaseline Ours Baseline Ours Baseline Ours Baseline Ours\nText Classiﬁcation\nAGNews 2.7B 44.7 0.0 63.2 0.0 33.0 5.1 59.6 6.4 43.3 8.3 71.1 8.5 50.8 7.8 72.7 5.8\n175B 43.9 0.0 73.9 0.0 62.1 6.3 77.1 3.8 61.0 10.9 85.9 1.3 79.1 2.6 84.3 2.5\nTREC 2.7B 31.0 0.0 38.8 0.0 24.3 6.4 36.8 7.7 25.8 11.5 38.6 13.2 29.3 8.0 44.3 11.4\n175B 47.4 0.0 57.4 0.0 57.7 6.0 75.7 1.4 60.2 7.6 69.7 1.4 45.6 4.0 66.9 6.5\nCB 2.7B 44.6 0.0 50.0 0.0 33.8 16.6 33.0 7.3 43.5 11.9 54.2 4.7 43.9 8.4 53.0 7.7\n175B 30.4 0.0 48.2 0.0 50.9 6.7 51.8 7.2 45.2 19.4 60.7 6.7 59.6 11.3 65.0 7.9\nRTE 2.7B 44.8 0.0 49.5 0.0 49.6 2.9 50.4 2.7 44.0 1.4 54.5 4.7 49.2 1.9 54.8 2.8\n175B 57.8 0.0 57.8 0.0 62.9 2.7 62.8 2.3 58.7 11.9 60.4 8.1 66.2 5.8 65.5 2.5\nSST-2 2.7B 57.2 0.0 71.4 0.0 67.3 7.9 79.1 8.3 59.1 10.2 79.9 7.8 54.0 4.3 82.0 5.5\n175B 71.6 0.0 75.8 0.0 93.3 2.8 94.7 1.4 93.6 3.3 94.3 1.0 95.6 1.0 95.3 0.7\nDBPedia 2.7B 36.0 0.0 38.7 0.0 25.9 4.4 61.6 2.9 61.0 12.8 66.0 7.5 72.6 4.5 74.8 5.0\n175B 22.0 0.0 59.7 0.0 79.3 3.0 85.3 2.2 84.6 5.8 86.9 4.0 82.3 7.8 86.9 1.9\nFact Retrieval\nLAMA 2.7B 14.0 0.0 22.7 0.0 29.7 1.8 31.6 1.3 35.8 3.8 37.4 3.4 42.5 1.3 42.5 1.4\n175B 23.5 0.0 30.1 0.0 48.9 2.3 49.0 1.4 62.0 2.4 61.8 2.9 63.8 1.0 63.6 1.3\nInformation Extraction\nMIT-G 2.7B 5.0 0.0 5.7 0.0 26.7 11.4 37.9 5.7 53.1 7.8 54.7 6.0 59.0 4.7 59.1 4.8\n13B 15.0 0.0 18.7 0.0 47.3 3.9 52.0 7.9 57.9 4.8 58.9 4.0 59.0 4.7 59.1 4.8\nMIT-D 2.7B 46.3 0.0 47.0 0.0 42.0 13.0 53.5 13.5 73.5 4.9 74.1 5.0 75.3 1.0 75.1 1.3\n13B 36.3 0.0 38.7 0.0 58.6 21.4 72.8 4.0 75.4 1.9 75.9 2.1 77.8 0.5 77.8 0.5\nATIS-A 2.7B 10.8 0.0 14.0 0.0 29.8 12.8 33.1 9.4 43.0 26.2 47.3 21.3 55.6 5.0 58.8 4.0\n13B 49.5 0.0 52.7 0.0 69.6 17.4 71.8 17.1 67.5 10.4 69.6 13.4 63.4 4.6 64.5 4.0\nATIS-D 2.7B 6.4 0.0 12.9 0.0 42.3 28.8 65.6 20.8 75.0 6.7 83.4 4.2 81.0 8.8 88.3 3.7\n13B 4.0 0.0 5.0 0.0 97.9 0.6 95.5 4.6 98.0 0.6 97.8 0.7 98.8 0.3 98.8 0.3\nTable 1.Contextual calibration improves accuracy across a range of tasks.We show the mean and standard deviation across different\nchoices of the training examples (the prompt format is ﬁxed). The LM column indicates the GPT-3 size (see Appendix A for GPT-2\nresults). The Baseline column shows the standard approach of greedy decoding (Brown et al., 2020) and Ours corresponds to greedy\ndecoding after modifying the output probabilities using contextual calibration. We bold the better result of the baseline and ours. MIT-G,\nMIT-D, ATIS-A, and ATIS-D indicate the MIT Genre, MIT Director, ATIS Airline, and ATIS Departure Date datasets.\n5.1. Results for Contextual Calibration\nHere, we evaluate the effectiveness of contextual calibra-\ntion across all of our datasets and LMs. We ﬁrst use a\nﬁxed prompt format and select ﬁve different random sets\nof training examples, placing them in an arbitrary order in\nthe prompt. We do not artiﬁcially balance the labels of the\ntraining examples for the classiﬁcation tasks. We use the\nsame sets of training examples for the baseline (standard de-\ncoding without calibration) and contextual calibration. We\nuse labeling budgets of 0–8 examples; using more than 8-\nshots causes the cost of querying the OpenAI API to become\nprohibitively expensive.\nTable 1 shows the results and Figure 1 in Section 1 plots the\nsame data for a subset of the tasks.\nImproves Mean And Worst-Case Accuracy Contextual\ncalibration dramatically improves GPT-3’s average and\nworst-case accuracy, by up to 30.0% absolute. These gains\nhold for both classiﬁcation and generation tasks. Contextual\ncalibration also sometimes allows GPT-3 2.7B to outper-\nform the GPT-3 175B baseline—by up to 19.3%—despite\nbeing over 50x smaller.\nCan Reduce Variance Across Training Sets Figure 6\nplots the difference in the standard deviation between the\nbaseline and contextual calibration for all tasks from Table 1.\nContextual calibration reduces the variance considerably in\na majority of cases, and it does not increase variance by\nmuch in the remaining cases.\nReduces Drop from 0-shot to 1-shot For the baseline,\nthere are four cases where there is a drop in accuracy when\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n15\n 10\n 5\n 0 5\nStd Dev of Contextual Calibration  Baseline\nFigure 6.Aside from improving mean accuracy, contextual cal-\nibration also reduces the standard deviation of accuracy across\ndifferent choices of the training examples. We plot the differ-\nence in standard deviation between contextual calibration and the\nbaseline from Table 1.\nmoving from 0-shot to 1-shot (TREC, AGNews, DBpedia,\nSST-2). We attribute this drop to the majority label bias (see\ndiscussion in Section 4). Calibration removes this drop in\nthree out of four cases.\nImproves GPT-2We also test GPT-2 1.5B (see Table 4 in\nAppendix A). We ﬁnd that like GPT-3, GPT-2’s accuracy\nalso highly varies across different prompts. This suggests\nthat the variance that we observe for few-shot in-context\nlearning is a general problem for LMs. Second, contextual\ncalibration works out-of-the-box for GPT-2—it improves\nthe mean accuracy and reduces variance for most tasks.\nImproves Accuracy Across Formats In our next set of ex-\nperiments, we use a ﬁxed set of training examples and vary\nthe prompt format. We use the 15 prompt formats for SST-2\ndiscussed in Section 3. We also create 15 prompt formats\nfor each of three random relations in LAMA (P20, P159,\nP19) by using the paraphrases of the original LAMA tem-\nplates generated by Jiang et al. (2020b). Figure 7 shows the\nresults before and after calibration for SST-2, and Figure 9\nin Appendix A show the results for LAMA. Contextual cali-\nbration improves the average and worst-case accuracy for\nboth datasets, and reduces the variance for SST-2.\n5.2. Ablations on Contextual Calibration\nWe ﬁnally conduct two analyses/ablations on contextual\ncalibration. We ﬁrst analyze how effective contextual cal-\nibration is at inferring a good setting of W. To do so, we\ncompare its accuracy to an “oracle calibration” method that\nuses the validation set to ﬁnd the best possible diagonal W.\nWe evaluate this oracle on AGNews, and ﬁnd that contextual\ncalibration is surprisingly close to it (Figure 8).\nWe also study how the choice of content-free input affects\naccuracy. In Table 3 in Appendix A, we show the accu-\nracy for SST-2 and AGNews for different choices of the\ncontent-free input. The choice of content-free input matters,\nhowever, many good choices exist.\n0 1 4 8\nNumber of Training Examples\n40\n50\n60\n70\n80\n90SST-2 Accuracy (%)\nAccuracy Over Diff. Formats\nGPT-3 2.7B\nWith Calibration\nFigure 7.GPT-3 has high variance across different prompt formats;\ncontextual calibration reduces this variance and improves mean\naccuracy. We show the mean accuracy (± standard deviation) over\n15 different prompt formats for SST-2.\n6. Discussion\nDoes Calibration Eliminate the Need to Engineer\nPrompts? The motivation behind “prompt engineering”\nis that not all prompts lead to the same accuracy. Thus, one\nshould tune the prompt’s format and examples to achieve the\nbest possible performance (Brown et al., 2020; Gao et al.,\n2020). Contextual calibration does not eliminate the need to\nengineer prompts, however, it does mitigate it: contextual\ncalibration makes the accuracy of the best, average, and\nworst-case prompts more similar (and higher).\nShould You Finetune in the Few-shot Setting? We use a\nﬁxed LM with no ﬁnetuning. As mentioned in Section 1,\nthere are numerous reasons not to ﬁnetune: it enables rapid\nprototyping, provides a fully natural language interface, and\nis more efﬁcient in terms of memory requirements and sys-\ntem complexity when serving many different tasks. More-\nover, like in-context learning without contextual calibration,\nﬁnetuning can be unstable in the few-shot setting (Schick\n& Sch¨utze, 2021). Nevertheless, if these disadvantages are\nacceptable or avoidable, ﬁnetuning can improve accuracy\nover in-context learning in some cases (Schick & Sch¨utze,\n2020; Gao et al., 2020). An interesting direction for future\nwork is to study the interplay between contextual calibration\nand ﬁnetuning, e.g., does contextual calibration alleviate the\nneed to ﬁnetune, or vice versa?\n7. Related Work\nFew-shot Learning with Language Models Recent work\nuses LMs to solve NLP tasks, e.g., for story cloze pre-\ndiction (Schwartz et al., 2017), knowledge base comple-\ntion (Petroni et al., 2019), and Winograd schemas (Trinh &\nLe, 2018). Radford et al. (2019) and Brown et al. (2020)\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n0 1 4 8 16\nNumber of Training Examples\n40\n50\n60\n70\n80AGNews Accuracy (%)\nAccuracy Over Diff. Training Sets\nOracle Calibration\nContextual Calibration\nUncalibrated Baseline\nFigure 8.Contextual calibration, despite using no training data,\nachieves similar accuracy to an “oracle” calibration that ﬁnds the\nbest W using the validation set. The plot shows GPT-3 175B’s\nmean accuracy (± standard deviation) on AGNews over different\nchoices of the training examples.\nshow that large LMs can be used to solve a myriad of tasks\nin a few-shot manner via in-context learning. Our paper\nprovides a simple modiﬁcation to their setting that improves\nperformance. Asking LMs to complete natural language\nprompts is also used as a method to “probe” LMs, e.g., ana-\nlyzing their factual (Petroni et al., 2019; Jiang et al., 2020b;\nShin et al., 2020) or commonsense knowledge (Bosselut\net al., 2019). Our results suggest that these probing methods\nmay underestimate model accuracy, and we recommend that\nfuture work take advantage of contextual calibration.\nVolatility of Few-shot Learning in NLP Recent work\nshows that when using masked language models such as\nBERT for zero-shot learning, the prompt format can impact\naccuracy (Petroni et al., 2019; Jiang et al., 2020b; Shin et al.,\n2020). Independent and concurrent work also shows that\nwhen ﬁnetuning masked language models on few examples,\nthe choice of training examples can impact results (Schick\n& Sch¨utze, 2020; Gao et al., 2020). We show that similar\ninstabilities occur for in-context learning (i.e., no ﬁnetuning)\nwith left-to-right language models. We also show a surpris-\ning instability associated with example ordering. Moreover,\nunlike past work, we analyze why these instabilities occur,\nand we use insights from this analysis to mitigate the issues.\nFailures of Language Models We identify failures when\nLMs are used for in-context learning (e.g., recency bias).\nPast work identiﬁes similar failures when LMs are used\nfor text generation. For example, neural LMs often repeat\nthemselves (Holtzman et al., 2020), suffer from overconﬁ-\ndence (Braverman et al., 2020; Jiang et al., 2020a), suffer\nfrom recency bias (Khandelwal et al., 2018; Ravfogel et al.,\n2019), and prefer generic responses instead of rare text (Li\net al., 2016; Logan et al., 2019). Past work mitigates these\ndegeneracies by modifying the model’s output probabilities\nor generation schemes, e.g., explicitly preventing repeti-\ntions (Paulus et al., 2018) or using sampling instead of\ngreedy decoding (Holtzman et al., 2020).\n8. Conclusion and Future Work\nWe show that few-shot learning can be highly volatile across\ndifferent choices of the prompt. Through a detailed analysis,\nwe identify that this volatility arises from biases in LMs, e.g.,\ntheir tendency to output recent or common tokens. We use\nthese insights to develop contextual calibration—a simple\nprocedure to adjust the model’s output probabilities—which\nimproves accuracy, reduces variance, and overall makes\ntools like GPT-3 more effective for end users.\nLooking at the bigger picture, our results inspire two future\nresearch directions in few-shot learning for NLP. First, on\nthe methods side, we show that good few-shot learning re-\nquires attention to detail : small but non-trivial decisions\nsuch as calibration can greatly inﬂuence results. This makes\nit difﬁcult to correctly develop and compare new methods\n(e.g., pretraining schemes or model architectures). We thus\nhope to make other few-shot learning methods more robust,\nand also expand our techniques to cover a wider ranger of\ntasks (e.g., calibration for open-ended generation). Second,\non the analysis side, our results highlight the need to under-\nstand what GPT-3 learns from the prompt. The model has an\nimpressive ability to improve with more training examples,\nhowever, we show that the model learns some superﬁcial\npatterns such as repetition of common answers. We hope to\nbetter understand and analyze the dynamics of in-context\nlearning in future work.\nAcknowledgements\nWe thank OpenAI for providing academic access to the GPT-\n3 API. We thank Sewon Min, Nikhil Kandpal, Nelson Liu,\nGirish Sastry, Marco Tulio Ribeiro, and the members of\nBerkeley NLP for valuable feedback on the paper.\nThis work was supported by DARPA under the LwLL pro-\ngram/Grant No. FA8750-19-1-0504, DARPA MCS program\nunder Contract No. N660011924033 with the United States\nOfﬁce Of Naval Research, DARPA and the Air Force Re-\nsearch Laboratory (AFRL), and NSF award #IIS-1756023.\nReferences\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celiky-\nilmaz, A., and Choi, Y . COMET: Commonsense trans-\nformers for automatic knowledge graph construction. In\nACL, 2019.\nBraverman, M., Chen, X., Kakade, S., Narasimhan, K.,\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nZhang, C., and Zhang, Y . Calibration, entropy rates, and\nmemory in language models. In ICML, 2020.\nBrier, G. W. Veriﬁcation of forecasts expressed in terms of\nprobability. Monthly Weather Review, 1950.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In NeurIPS, 2020.\nDagan, I., Glickman, O., and Magnini, B. The PASCAL\nrecognising textual entailment challenge. In Machine\nLearning Challenges Workshop, 2005.\nde Marneffe, M.-C., Simons, M., and Tonhauser, J. The\nCommitmentBank: Investigating projection in naturally\noccurring discourse. In Sinn und Bedeutung, 2019.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In NAACL, 2019.\nGao, T., Fisch, A., and Chen, D. Making pre-trained lan-\nguage models better few-shot learners. arXiv preprint\narXiv:2012.15723, 2020.\nGuo, C., Pleiss, G., Sun, Y ., and Weinberger, K. Q. On\ncalibration of modern neural networks. In ICML, 2017.\nHemphill, C. T., Godfrey, J. J., and Doddington, G. R. The\nATIS spoken language systems pilot corpus. In Speech\nand Natural Language Workshop, 1990.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y .\nThe curious case of neural text degeneration. In ICLR,\n2020.\nJiang, Z., Araki, J., Ding, H., and Neubig, G. How can\nwe know when language models know? arXiv preprint\narXiv:2012.00955, 2020a.\nJiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we\nknow what language models know? In TACL, 2020b.\nKhandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp\nnearby, fuzzy far away: How neural language models use\ncontext. In ACL, 2018.\nLake, B. M., Salakhutdinov, R., and Tenenbaum, J. B.\nHuman-level concept learning through probabilistic pro-\ngram induction. In Science, 2015.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. A\ndiversity-promoting objective function for neural conver-\nsation models. In NAACL, 2016.\nLiu, J., Cyphers, S., Pasupat, P., McGraw, I., and Glass, J. A\nconversational movie search system based on conditional\nrandom ﬁelds. In INTERSPEECH, 2012.\nLogan, R. L., Liu, N. F., Peters, M. E., Gardner, M., and\nSingh, S. Barack’s wife Hillary: Using knowledge-graphs\nfor fact-aware language modeling. In ACL, 2019.\nPaulus, R., Xiong, C., and Socher, R. A deep reinforced\nmodel for abstractive summarization. In ICLR, 2018.\nPetroni, F., Rockt ¨aschel, T., Lewis, P., Bakhtin, A., Wu,\nY ., Miller, A. H., and Riedel, S. Language models as\nknowledge bases? In EMNLP, 2019.\nPlatt, J. C. Probabilistic outputs for support vector machines\nand comparisons to regularized likelihood methods. In\nAdvances in Large Margin Classiﬁers, 1999.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining. Technical Report, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. Technical Report, 2019.\nRavfogel, S., Goldberg, Y ., and Linzen, T. Studying the\ninductive biases of RNNs with synthetic variations of\nnatural languages. In NAACL, 2019.\nSchick, T. and Sch ¨utze, H. It’s not just size that matters:\nSmall language models are also few-shot learners. arXiv\npreprint arXiv:2009.07118, 2020.\nSchick, T. and Sch¨utze, H. Exploiting cloze questions for\nfew-shot text classiﬁcation and natural language infer-\nence. In EACL, 2021.\nSchwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y ., and\nSmith, N. A. The effect of different writing tasks on\nlinguistic style: A case study of the ROC story cloze task.\nIn ACL, 2017.\nShin, T., Razeghi, Y ., Logan IV , R. L., Wallace, E., and\nSingh, S. AutoPrompt: Eliciting knowledge from lan-\nguage models with automatically generated prompts. In\nEMNLP, 2020.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nEMNLP, 2013.\nTrinh, T. H. and Le, Q. V . A simple method for common-\nsense reasoning. arXiv preprint arXiv:1806.02847, 2018.\nV oorhees, E. M. and Tice, D. M. Building a question an-\nswering test collection. In SIGIR, 2000.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nWang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. Su-\nperGLUE: A stickier benchmark for general-purpose lan-\nguage understanding systems. In NeurIPS, 2019.\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\nT., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W.,\nYu, L., Dyer, C., et al. Learning and evaluating general\nlinguistic intelligence. arXiv preprint arXiv:1901.11373,\n2019.\nZhang, X., Zhao, J., and LeCun, Y . Character-level con-\nvolutional networks for text classiﬁcation. In NeurIPS,\n2015.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nA. Additional Results on Variance and\nCalibration\nTable 2 shows an example of the sensitivity to ordering.\nPrompt (test input not shown) Acc.\nReview: the whole thing ’s fairly lame , making it par for\nthe course for disney sequels .\nAnswer: Negative\nReview: this quiet , introspective and entertaining indepen-\ndent is worth seeking .\nAnswer: Positive\n88.5%\nReview: this quiet , introspective and entertaining indepen-\ndent is worth seeking .\nAnswer: Positive\nReview: the whole thing ’s fairly lame , making it par for\nthe course for disney sequels .\nAnswer: Negative\n51.3%\nTable 2.Top:a prompt consisting of two training examples (the test\ninput is not shown) that leads to good test accuracy for GPT-3 2.7B\n(88.5%). Bottom: simply reversing the order of the two examples\ncauses the accuracy to drop to near random chance (51.3%).\nTable 3 demonstrates that the choice of content-free input\ndoes affect accuracy, however, many good choices exist.\nContent-free Input SST-2 AGNews\nUncalibrated Baseline 66.5 48.5\nN/A 74.2 64.5\n[MASK] 74.5 63.8\n‘’ 72.9 64.7\nN/A, [MASK], ‘’ 79.0 66.5\nthe 69.1 59.0\nabc 77.5 57.3\nthe man. 79.4 62.0\ndasjhasjkdhjskdhds 79.3 64.5\nnfjkhdvy84tr9bpuirvwe 78.4 65.5\nTable 3.We show the accuracy for 1-shot SST-2 and 0-shot AG-\nNews over different choices for the content-free input. The choice\nof content-free input matters, however, many good choices exist.\nThe token ‘’ indicates the empty string. Recall that in our experi-\nments, we ensemble over N/A, [MASK], and the empty string.\nFigure 9 shows how GPT-3 accuracy changes as the prompt\nformat is varied for LAMA, with and without calibration.\nTable 4 shows the effect of calibration for GPT-2.\nB. Prompt Formats Used\nTables 5 and 6 show the default prompt format used for all\ntasks. Table 7 shows the 15 different formats used when\nstudying the effect of prompt format for SST-2.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\n0 1 4 8\nNumber of Training Examples\n0\n10\n20\n30\n40LAMA Accuracy (%)\nAccuracy Over Diff. Formats (P20)\nGPT-3 2.7B\nWith Calibration\n0 1 4 8\nNumber of Training Examples\n10\n20\n30LAMA Accuracy (%)\nAccuracy Over Diff. Formats (P159)\nGPT-3 2.7B\nWith Calibration\n0 1 4 8\nNumber of Training Examples\n0\n10\n20LAMA Accuracy (%)\nAccuracy Over Diff. Formats (P19)\nGPT-3 2.7B\nWith Calibration\nFigure 9.Contextual calibration improves GPT-3’s accuracy across various prompt formats for LAMA. We plot GPT-2 2.7B’s mean\naccuracy over 15 different formats for the LAMA “place of death” relation (P20), “Headquarter Location” relation (P159), and “place of\nbirth” relation (P19).\nDataset LM 0-shot 1-shot 4-shot 8-shot\nBaseline Ours Baseline Ours Baseline Ours Baseline Ours\nText Classiﬁcation\nAGNews GPT-2 44.0 0.0 60.0 0.0 45.4 8.4 67.9 5.7 44.6 12.2 58.0 13.6 57.1 11.6 63.1 7.3\nTREC GPT-2 24.0 0.0 37.3 0.0 21.5 5.2 41.1 2.6 23.1 5.9 44.2 2.2 32.7 7.5 44.1 3.6\nCB GPT-2 44.6 0.0 17.9 0.0 49.6 10.0 47.1 12.2 40.0 8.3 55.4 7.3 48.9 5.7 63.2 1.4\nRTE GPT-2 51.0 0.0 48.5 0.0 57.6 2.1 56.3 2.4 53.2 6.0 57.5 1.8 54.9 3.0 57.7 1.29\nSST-2 GPT-2 60.0 0.0 82.0 0.0 66.7 17.9 73.0 11.4 64.9 8.4 73.8 10.9 54.5 4.6 64.6 8.8\nDBPedia GPT-2 64.3 0.0 58.3 0.0 33.6 18.9 69.5 9.4 53.0 14.8 75.3 8.1 66.0 3.6 74.3 8.7\nFact Retrieval\nLAMA GPT-2 14.0 0.0 22.7 0.0 29.7 1.8 31.6 1.3 35.8 3.8 37.4 3.4 42.5 1.3 42.5 1.4\nInformation Extraction\nMIT-G GPT-2 7.7 0.0 10.0 0.0 32.9 10.0 41.2 4.1 44.3 6.5 47.7 5.8 56.9 2.5 59.5 2.5\nMIT-D GPT-2 29.3 0.0 41.7 0.0 26.2 10.5 58.8 4.8 70.5 2.5 75.4 1.8 77.1 4.4 78.1 3.9\nATIS-A GPT-2 15.1 0.0 35.5 0.0 41.5 11.7 51.4 7.5 55.1 18.9 65.8 11.7 63.4 10.6 69.9 10.4\nATIS-D GPT-2 1.0 0.0 2.5 0.0 62.3 9.2 68.7 4.3 81.1 3.6 83.2 7.2 81.8 4.5 83.9 5.0\nTable 4.Contextual calibration improves accuracy for GPT-2. This table is analogous to Table 1 but shows results for GPT-2 XL.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nTask Prompt Label Names\nSST-2 Review: This movie is amazing!\nSentiment: Positive\nReview: Horriﬁc movie, don’t see it.\nSentiment:\nPositive, Negative\nAGNews Article: USATODAY .com - Retail sales bounced back a bit in July, and new claims for\njobless beneﬁts fell last week, the government said Thursday, indicating the economy is\nimproving from a midsummer slump.\nAnswer: Business\nArticle: New hard-drive based devices feature color screens, support for WMP 10.\nAnswer:\nWorld, Sports, Business, Technology\nTREC Classify the questions based on whether their answer type is a Number, Location, Person,\nDescription, Entity, or Abbreviation.\nQuestion: How did serfdom develop in and then leave Russia?\nAnswer Type: Description\nQuestion: When was Ozzy Osbourne born?\nAnswer Type:\nNumber, Location, Person, Description,\nEntity, Abbreviation\nDBPedia Classify the documents based on whether they are about a Company, School, Artist, Ath-\nlete, Politician, Transportation, Building, Nature, Village, Animal, Plant, Album, Film,\nor Book.\nArticle: Geoffrey D. Falksen (born July 31 1982) is an American steampunk writer.\nAnswer: Artist\nArticle: The Perrin River is a 1.3-mile-long (2.1 km) tidal river in the U.S. state of Vir-\nginia. It is a small inlet on the north shore of the York River near that river’s mouth at\nChesapeake Bay.\nAnswer:\nCompany, School, Artist, Athlete, Politi-\ncian, Transportation, Building, Nature,\nVillage, Animal, Plant, Album, Film,\nBook\nCB But he ended up eating it himself. I was reluctant to kiss my mother, afraid that somehow\nher weakness and unhappiness would infect me. Naturally I didn’t think for a minute that\nmy life and spirit could stimulate her.\nquestion: her life and spirit could stimulate her mother. True, False, or Neither?\nanswer: Neither\nValence the void-brain, Valence the virtuous valet. Why couldn’t the ﬁgger choose his\nown portion of titanic anatomy to shaft? Did he think he was helping?\nquestion: Valence was helping. True, False, or Neither?\nanswer:\nTrue, False, Neither\nRTE Others argue that Mr. Sharon should have negotiated the Gaza pullout - both to obtain\nat least some written promises of better Palestinian behavior, and to provide Mr. Abbas\nwith a prime prize to show his people that diplomacy, not violence, delivered Gaza.\nquestion: Mr. Abbas is a member of the Palestinian family. True or False?\nanswer: False\nThe program will include Falla’s ”Night in the Gardens of Spain,” Ravel’s Piano Concerto\nin G, Berlioz’s Overture to ”Beatrice and Benedict,” and Roy Harris’ Symphony No. 3.\nquestion: Beatrice and Benedict is an overture by Berlioz. True or False?\nanswer:\nTrue, False\nTable 5.The prompts used for text classiﬁcation. We show one training example per task for illustration purposes. The right column\nshows the label names (to make predictions, we check the LM’s probability for these tokens).\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nTask Prompt\nLAMA Alexander Berntsson was born in Sweden\nKhalid Karami was born in\nATIS\n(Airline)\nSentence: what are the two american airlines ﬂights that leave from dallas to san francisco in the evening\nAirline name: american airlines\nSentence: list a ﬂight on american airlines from toronto to san diego\nAirline name:\nATIS\n(Depart Date)\nSentence: please list any ﬂight available leaving oakland california tuesday arriving philadelphia wednesday\nDepart date - Day name: tuesday\nSentence: show me all all ﬂights from pittsburgh to atlanta on wednesday which leave before noon and serve\nbreakfast\nDepart date - Day name:\nMIT Movies\n(Genre)\nSentence: last to a famous series of animated movies about a big green ogre and his donkey and cat friends\nGenre: animated\nSentence: what is a great comedy featuring the talents of steve carell as a loser looking for a friend\nGenre:\nMIT Movies\n(Director)\nSentence: in 2005 director christopher nolan rebooted a legendary dc comics superhero with a darker grittier edge\nin which movie\nDirector: christopher nolan\nSentence: what 1967 mike nichols ﬁlm features dustin hoffman in romantic interludes with anne bancroft as mrs\nrobinson\nDirector:\nTable 6.The prompts used for generation tasks. We show one training example per task for illustration purposes.\nCalibrate Before Use: Improving Few-Shot Performance of Language Models\nFormat ID Prompt Label Names\n1 Review: This movie is amazing!\nAnswer: Positive\nReview: Horriﬁc movie, don’t see it.\nAnswer:\nPositive, Negative\n2 Review: This movie is amazing!\nAnswer: good\nReview: Horriﬁc movie, don’t see it.\nAnswer:\ngood, bad\n3 My review for last night’s ﬁlm: This movie is amazing! The critics agreed that this movie was good\nMy review for last night’s ﬁlm: Horriﬁc movie, don’t see it. The critics agreed that this movie was\ngood, bad\n4 Here is what our critics think for this month’s ﬁlms.\nOne of our critics wrote ”This movie is amazing!”. Her sentiment towards the ﬁlm was positive.\nOne of our critics wrote ”Horriﬁc movie, don’t see it”. Her sentiment towards the ﬁlm was\npositive, negative\n5 Critical reception [ edit ]\nIn a contemporary review, Roger Ebert wrote ”This movie is amazing!”. Entertainment Weekly agreed, and\nthe overall critical reception of the ﬁlm was good.\nIn a contemporary review, Roger Ebert wrote ”Horriﬁc movie, don’t see it”. Entertainment Weekly agreed, and\nthe overall critical reception of the ﬁlm was\ngood, bad\n6 Review: This movie is amazing!\nPositive Review? Yes\nReview: Horriﬁc movie, don’t see it.\nPositive Review?\nYes, No\n7 Review: This movie is amazing!\nQuestion: Is the sentiment of the above review Positive or Negative?\nAnswer: Positive\nReview: This movie is amazing!\nQuestion: Is the sentiment of the above review Positive or Negative?\nAnswer:\nPositive, Negative\n8 Review: This movie is amazing!\nQuestion: Did the author think that the movie was good or bad?\nAnswer: good\nReview: This movie is amazing!\nQuestion: Did the author think that the movie was good or bad?\nAnswer:\ngood, bad\n9 Question: Did the author of the following tweet think that the movie was good or bad?\nTweet: This movie is amazing!\nAnswer: good\nQuestion: Did the author of the following tweet think that the movie was good or bad?\nTweet: Horriﬁc movie, don’t see it\nAnswer:\ngood, bad\n10 This movie is amazing! My overall feeling was that the movie was good\nHorriﬁc movie, don’t see it. My overall feeling was that the movie was\ngood, bad\n11 This movie is amazing! I liked the movie.\nHorriﬁc movie, don’t see it. I\nliked, hated\n12 This movie is amazing! My friend asked me if I would give the movie 0 or 5 stars, I said 5\nHorriﬁc movie, don’t see it. My friend asked me if I would give the movie 0 or 5 stars, I said\n0, 5\n13 Input: This movie is amazing!\nSentiment: Positive\nInput: Horriﬁc movie, don’t see it.\nSentiment:\nPositive, Negative\n14 Review: This movie is amazing!\nPositive: True\nReview: Horriﬁc movie, don’t see it.\nPositive:\nTrue, False\n15 Review: This movie is amazing!\nStars: 5\nReview: Horriﬁc movie, don’t see it.\nStars:\n5, 0\nTable 7.The different prompt formats used when studying the effect of format for SST-2. We show one training example for illustration.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7736109495162964
    },
    {
      "name": "Variance (accounting)",
      "score": 0.7372786402702332
    },
    {
      "name": "Training set",
      "score": 0.6666966676712036
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6577353477478027
    },
    {
      "name": "Calibration",
      "score": 0.6368076205253601
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5072019100189209
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5042847394943237
    },
    {
      "name": "Machine learning",
      "score": 0.5039250254631042
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4692542254924774
    },
    {
      "name": "Natural language understanding",
      "score": 0.4281241297721863
    },
    {
      "name": "Task (project management)",
      "score": 0.4130759537220001
    },
    {
      "name": "Natural language",
      "score": 0.36765575408935547
    },
    {
      "name": "Statistics",
      "score": 0.28181368112564087
    },
    {
      "name": "Mathematics",
      "score": 0.14706331491470337
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 72
}