{
    "title": "An Empirical Study of Efficient ASR Rescoring with Transformers",
    "url": "https://openalex.org/W2981710752",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5028860597",
            "name": "Hongzhao Huang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5047400593",
            "name": "Fuchun Peng",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962753370",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2176797192",
        "https://openalex.org/W2972320704",
        "https://openalex.org/W2916979304",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2963932686",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2964084166",
        "https://openalex.org/W1520465330",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963660642",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2981436362",
        "https://openalex.org/W2886490473",
        "https://openalex.org/W2932970397",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2120861206",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3008525923",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W179875071"
    ],
    "abstract": "Neural language models (LMs) have been proved to significantly outperform classical n-gram LMs for language modeling due to their superior abilities to model long-range dependencies in text and handle data sparsity problems. And recently, well configured deep Transformers have exhibited superior performance over shallow stack of recurrent neural network layers for language modeling. However, these state-of-the-art deep Transformer models were mostly engineered to be deep with high model capacity, which makes it computationally inefficient and challenging to be deployed into large-scale real-world applications. Therefore, it is important to develop Transformer LMs that have relatively small model sizes, while still retaining good performance of those much larger models. In this paper, we aim to conduct empirical study on training Transformers with small parameter sizes in the context of ASR rescoring. By combining techniques including subword units, adaptive softmax, large-scale model pre-training, and knowledge distillation, we show that we are able to successfully train small Transformer LMs with significant relative word error rate reductions (WERR) through n-best rescoring. In particular, our experiments on a video speech recognition dataset show that we are able to achieve WERRs ranging from 6.46% to 7.17% while only with 5.5% to 11.9% parameter sizes of the well-known large GPT model [1], whose WERR with rescoring on the same dataset is 7.58%.",
    "full_text": "arXiv:1910.11450v1  [cs.CL]  24 Oct 2019\nAN EMPIRICAL STUDY OF EFFICIENT ASR RESCORING WITH TRANSFORMERS\nHongzhao Huang, Fuchun P eng\nFacebook AI, Menlo Park, CA, USA\nABSTRACT\nNeural language models (LMs) have been proved to signiﬁcant ly\noutperform classical n-gram LMs for language modeling due to\ntheir superior abilities to model long-range dependencies in text\nand handle data sparsity problems. And recently , well conﬁg -\nured deep Transformers have exhibited superior performanc e over\nshallow stack of recurrent neural network layers for langua ge mod-\neling. However, these state-of-the-art deep Transformer m odels\nwere mostly engineered to be deep with high model capacity , w hich\nmakes it computationally inefﬁcient and challenging to be d eployed\ninto large-scale real-world applications. Therefore, it i s important\nto develop Transformer LMs that have relatively small model sizes,\nwhile still retaining good performance of those much larger models.\nIn this paper, we aim to conduct empirical study on training T rans-\nformers with small parameter sizes in the context of ASR resc oring.\nBy combining techniques including subword units, adaptive soft-\nmax, large-scale model pre-training, and knowledge distil lation, we\nshow that we are able to successfully train small Transforme r LMs\nwith signiﬁcant relative word error rate reductions (WERR) through\nn-best rescoring. In particular, our experiments on a video s peech\nrecognition dataset show that we are able to achieve WERRs ra nging\nfrom 6. 46% to 7. 17% while only with 5. 5% to 11. 9% parameter\nsizes of the well-known large GPT model [1], whose WERR with\nrescoring on the same dataset is 7. 58%.\nIndex T erms— neural language modeling, transformer, pre-\ntraining, knowledge distillation, adaptive softmax\n1. INTRODUCTION\nNeural networks have been proven to outperform traditional n-gram\nlanguage models (LMs) and have achieved state-of-the-art ( SOT A)\nperformance in language modeling [2, 3, 4, 5]. This is mainly be-\ncause n-gram LMs suffer from data sparsity problems, which makes\nit difﬁcult to capture large contexts and model long-range d ependen-\ncies in text. In contrast, neural models overcome these issu es with\ndistributed representation learning in a latent semantic s pace, thus\nwith superior abilities in modeling long-range dependenci es and bet-\nter model performance. However, compared to n-gram LMs, neural\nmodels are computationally expensive and slow , which makes it dif-\nﬁcult to be used in ﬁrst-pass automatic speech recognition ( ASR)\nsystems, where search space could be very large. Thus, neura l LMs\nhave been mostly used in second-pass rescoring, either thro ugh the\nn-best lists or lattices generated by the ﬁrst-pass systems w ith n-\ngram LMs [6, 7, 8, 9, 10].\nTransformer, which was originally invented in an encoder-\ndecoder framework for machine translation [11], has been po pular in\nnatural language processing (NLP). With the usage of self-a ttention\nmechanism and residual connections, it allows for successf ul train-\ning of very deep and high capacity networks, resulting in SOT A\nperformance in many NLP tasks [1, 12, 13, 14]. A number of rece nt\nworks [15, 13, 16, 17] on language modeling also demonstrate the\nsuperior ability of deep Transformers over shallow stack of recurrent\nneural networks such as LSTM [18]. However, these SOT A Trans -\nformer models were mostly engineered to have very high capac ity\nwith great depth. For example, even the smallest model of Ope nAI\nGPT2 [13] has 24 decoder layers with 345M model parameters.\nAnd Irie et al. [17] uses up to 42 and 96 decoder layers for ASR\nrescoring. Such a large model size makes it unrealistic to di rectly\ndeploy these models into large-scale applications due to la tency and\ncomputation resource restrictions, even for second-pass A SR rescor-\ning where the scoring space has been greatly pruned. In addit ion,\nsmaller model size is important for on-device applications where\nmachine capacity such as memory is usually limited.\nIn this work, we aim to conduct empirical study on efﬁcient AS R\nrescoring with Transformers, which is important to put thes e su-\nperior Transformer models into large-scale real-world app lications.\nFirst of all, we know that a neural LM trained with the standar d\ncross entropy loss contains a softmax layer that involves a s umma-\ntion over the entire output vocabulary . Thus the model size o f the\nsoftmax layer is proportional to the size of output vocabula ry , and\nlarger vocabulary could signiﬁcantly increase the model si ze. In or-\nder to handle this issue, we propose to combine subword unit m od-\nels with adaptive softmax. Subword units such as byte pair en cod-\ning (BPE) [19] can represent an open vocabulary through a ﬁxe d-\nsize vocabulary of character sequences, which is an effecti ve way to\nreduce model sizes and handle out-of-vocabulary issues. Ad aptive\nsoftmax [20] is a technique to speed up the softmax layer by as sign-\ning larger capacity to more frequent vocabulary units, whil e smaller\ncapacity to less frequent ones. Thus it can further reduce mo del sizes\nfrom the softmax layer.\nFor language modeling, it has been observed that higher capa c-\nity and depth tends to lead to better metrics with regarding t o per-\nplexity (PPL) [17]. Thus existing work mostly focused on tra ining\nvery large models to achieve SOT A performance. In contrast, in this\nwork we switch our focus to train Transformers with small par ame-\nter sizes to make them applicable to large-scale applicatio ns. In our\nempirical study , we observe that small Transformer LMs also per-\nform reasonably well with n-best rescoring. W e further propose to\nleverage a simple yet effective strategy with large-scale m odel pre-\ntraining and ﬁne-tuning to ﬁrst train powerful teacher mode ls. W e\nthen adopt knowledge distillation [21] to transfer knowled ge from\nthese teacher models into small student models to further im prove\ntheir performance.\nThe main contributions of this paper are summarized as follo ws:\n• W e show that subword unit models with different vocabulary\nsizes can achieve similar performance for ASR rescoring. By\ncombining with adaptive softmax, we can signiﬁcantly reduc e\nmodel sizes of Transformer LMs.\n• W e experiment Transformer LMs with small parameter\nsizes, and achieve signiﬁcant word error rate reductions\nwith second-pass n-best rescoring. Compared to those much\nlarger models, only slight performance degradation is ob-\nserved.\n• W e propose to improve small Transformer LMs with large-\nscale model pre-training and knowledge distillation, whic h\nfurther reduce PPLs and WERs over models that are trained\nwithout using these techniques.\n• By combining all of these techniques, we successfully train\nsmall Transformer LMs that achieve relative WERRs ranging\nfrom 6. 46% to 7. 17% while only with 5. 5% to 11. 9% pa-\nrameter sizes of the well-known large GPT model [1], whose\nWERR with rescoring on the same dataset is 7. 58%.\n2. OUR APPROACH\nIn this section, we introduce the details of our explored tec hniques to\ntrain small Transformer LMs with the goal of retraining perf ormance\nof those large models.\n2.1. Preliminaries\nGiven a text corpus D = {S1, . . . , S N } with vocabulary V, where\neach Si is a sequence of text with k word or subword units Si =\n{w(i)\n1 , . . . , w (i)\nk }, we can train a standard left-to-right neural lan-\nguage model Θ by maximizing the following objective function:\nLCE (Θ) =\n∑\ni\n∑\nj\nlogP (w(i)\nj |h(i)\nj ; Θ) (1)\nwhere the conditional probability P of w(i)\nj given its context\nhistory h(i)\nj and the unnormalized logit z(i)\nj is computed as:\nP (w(i)\nj |h(i)\nj ; Θ) =\nexp(z(i)\nj )\n∑ |V|\nv exp(zv)\n(2)\nFrom Equation 2, we can see that computation of the normalize d\nprobability for each w(i)\nj needs to go through a softmax layer that in-\nvolves a summation over all units in the vocabulary . This cou ld be\nvery computationally inefﬁcient and is a major performance bottle-\nneck for neural LMs with large output vocabularies. In this w ork,\nwe choose to train neural LMs based on the standard deep Trans -\nformer decoder [11], which consists of a stack of N transformer\nblocks. Each block contains a self-attention layer for mode ling con-\ntextual information, and a position-wise feed-forward lay er for fea-\nture transformation. Residual connection and layer normal ization\nare added between each layer so that lower layer information can be\npassed to upper layers, which allows for successful trainin g of very\ndeep Transformer networks.\n2.2. Subword Unit Models\nLarge word-level vocabularies are often used in large-scal e neural\nlanguage model training, resulting in signiﬁcant increase of model\nsize from the softmax layer. Thus an effective way to reduce m odel\nsize is to directly reduce the size of the output vocabulary . A straight-\nforward method to reduce vocabulary size is to simply group t hose\nwords with low frequencies into one cluster and replace them by\na speciﬁc symbol. However, this approach has shown poor perf or-\nmance in handling rare and unknown words [19, 22].\nIn order to better handle this challenge, subword unit repre -\nsentations such as byte pair encoding (BPE) [19] and wordpie ce\nmodel [22] have been proposed with improved performance in m any\nNLP tasks. This approach chooses to divide words into a limit ed set\nof subword units, and it can effectively interpolate betwee n word-\nlevel inputs for frequent words and character-level inputs for rare\nwords. Thus it is able to achieve a good balance between chara cter-\nlevel and word-level models. In this work, we adopt BPE 1 for input\nrepresentations. Different from previous work that normal ly used a\nrelatively large BPE vocabulary , we also conduct empirical study on\nthe choice of BPE unit sizes and their impact on ASR rescoring .\n2.3. Adaptive Softmax\nEven though with subword units, it is still computationally inefﬁ-\ncient to obtain normalized model predictions through the so ftmax\nlayer. Extensive study has been conducted to reduce the comp uta-\ntional costs from the softmax layer. Existing approaches ca n roughly\nbe grouped into two categories: (i) modifying the softmax ar chitec-\nture such as through hierarchical softmax [23] to make it mor e efﬁ-\ncient, and (ii) completely removing the softmax layer and ut ilizing\nother auxiliary loss such as self-normalization [24, 25] an d noise\ncontrastive estimation (NCE) [26, 27]. In this work, we choo se to\nexploit adaptive softmax [20], an improved approach over hi erarchi-\ncal softmax. It assigns larger capacity to more frequent voc ab units\nand smaller capacity to less frequent ones. Thus it can reduc e model\nsize and speed up both model training and inference. By combi ng\nwith subword unit models, we ﬁnd that it works effectively to reduce\nparameter sizes while maintaining model performance.\n2.4. Knowledge Distillation\nKnowledge distillation (KD) is a model compression techniq ue that\nis also known as teacher student training, where a small mode l (stu-\ndent) is trained to match the output of larger models (teache rs) [21].\nMore speciﬁcally , the student model is learned to minimize a new\nloss function based on the weighted linear combination of cr oss-\nentropy loss with hard labels from training data and Kullbac k-\nLeibler (KL) divergence to predicted distributions (soft l abels) of\nteacher models. Formally , we need to modify the objective fu nction\nas deﬁned in Equation 1 as follows:\nL(Θ) = α LCE (Θ) + (1 − α )LKLD(Θ) (3)\nwhere LKLD(Θ) is KL divergence loss computed from student\nand teacher model outputs, α is used to control the balance of the\ntwo loss. W e optimize the values of alpha and temperature on the\ndevelopment set and ﬁnd that the optimal values for alpha and tem-\nperature is 0. 1 and 1. 0, respectively . W e also completely remove\ndropouts for student models following the existing study on KD for\nlanguage modeling [28] as it gives the best performance.\n2.5. Pre-training and Fine-tuning\nIn order to fully leverage the power of knowledge distillati on, we\nneed to ﬁrst successfully train teacher models with superio r perfor-\nmance. And existence of high-quality in-domain data is impo rtant\nfor this step. However, in many cases it is challenging to obt ain\nadequate in-domain data in a timely fashion due to emergence of\nnew domains or extra annotation costs. Fortunately , there e xists\nabundant general domain text data from diverse sources, inc luding\nNews articles, Wikipedia, and social media posts etc. These gen-\neral corpuses have played an important role in the successfu l ap-\nplications of pre-trained models in natural language under standing\n1 https://github.com/glample/fastBPE\n(NLU) tasks [1, 12, 29]. But different from these existing wo rk on\nimproving NLU with pre-trained Transformers, we study the e ffec-\ntiveness of the pre-training strategy with deep Transforme rs for ASR\nrescoring, together with knowledge distillation.\nIn this work, we ﬁrst construct a large pre-training corpus t hat is\nnot domain speciﬁc, then we pre-train deep Transformer LMs w ith\nhigh capacity on this corpus. These pre-trained models are t hen fur-\nther optimized on the target domain data, and used to guide th e learn-\ning of small student models.\n3. EXPERIMENT AL SETUP\nIn all experiments of this work, we target to build a strong AS R\nsystem for automatic video transcription, which has many do wn-\nstreaming applications such as auto-captioning of videos. T o eval-\nuate the effectiveness of our proposed approaches, we ﬁrst g ather\nn-best candidates from the ﬁst-pass decoding with our in-hou se hy-\nbrid ASR system, which has achieved state-of-the-art perfo rmance\non multiple speech recognition datasets [30]. For acoustic model-\ning, we utilize a multi-layer Latency Controlled Bidirecti onal LSTM\n(LC-BLSTM) [31] with grapheme representations. In the ﬁrst -pass\ndecoding, we use our in-house dynamic decoder [32] with a pru ned\n5-gram LM. For Transformer LMs, we leverage the PyT orch im-\nplementation of Transformer 2 with Adam as optimizer. The n-best\ncandidates are further reranked with additional evidence g enerated\nby neural LMs. W e optimize all model hyper-parameters in the de-\nvelopment set, and use word error rate as the evaluation metr ic.\nSpeech Recognition Dataset. W e evaluate the effectiveness of our\nproposed approaches on an in-house English video dataset. It is ran-\ndomly sampled from the pool of publicly shared videos by user s on\nFacebook platform. This data is completely anonymized, and no\nuser-identiﬁable information (UII) is access to both trans cribers and\nresearchers. W e use a total of 943, 346 videos as training data, 4, 309\nvideos as development data, and 8, 189 videos as testing data. The\ntotal duration of this dataset is 13. 9K hours, and the total number of\ntokens in the transcriptions is 144M. It is a challenging dataset as it\ncontains videos from diverse speakers, content topics, and acoustic\nconditions.\nPre-training Corpus. W e construct a large-scale background text\ncorpus for neural LM pre-training from public Facebook user posts,\nwhere we randomly sample 105 million posts that users publicly\nshared on the Facebook platform. W e do not have access to any u ser\nUII information, and we directly converted the text into BPE and\nmachine reading format for model training.\nN-best Rescoring. After we obtain n-best (i.e., n = 50 is used\nin this work) candidates for each video from the ﬁst-pass ASR sys-\ntem. W eighted linear combination is then performed to re-es timate\nthe ﬁnal ranking score of each n-best candidate ci through s(ci) =\nsam(ci) + αs n\ngram (ci) + (1 − α )snlm(ci), where sam(ci) is the\nacoustic score from acoustic model, sn gram (ci) is the estimated\nprobability from the 5-gram LM, and snlm(ci) is the neural lan-\nguage modeling score. Finally we choose the top ranked candi dates\nas the ﬁnal ASR output and measure new word error rates on them .\nApproaches for Comparison. T o empirically study the impact of\nour strategies, we compare the following approaches:\n• n-gram: this is the ﬁrst-pass ASR system with n-gram LM.\nBy comparing to this baseline, we can understand the impact\nof ASR n-best rescoring with Transformer LMs.\n2 https://github.com/pytorch/fairseq\nT able 1. The overall WER and relative WERR of each approach on\nthe video dataset. “#BPE” denotes the size of BPE output vocabu-\nlary , “#Param” represents the number of model parameters of each\nTransformer LM.\nApproach #BPE #Param WER WERR\nn-gram - - 16. 88 -\nLarge 25K 123. 4M 15. 60 7 . 58%\nSmall one 10K 14. 7M 15. 67 7 . 17%\nSmall one 5K 11. 8M 15. 73 6 . 81%\nSmall two 10K 8. 9M 15. 78 6 . 52%\nSmall two 5K 6. 8M 15. 79 6 . 46%\n• Large: this is a rescoring model with a high capacity Trans-\nformer LM. Here we follow the popular GPT conﬁgura-\ntion [1], where the numbers of decoder layers and attention\nheads are both set as 12. And the dimension of input embed-\ndings, hidden states and feed-forward layers is set as 768, 768\nand 3072, respectively . And we choose 25K BPE units as the\nvocabulary , which is similar to previous work on large-scal e\nTransformer pre-training [12].\n• Small one: this is a rescoring model with a small Transformer\nLM, where the number of decoder layers and attention heads\nis set as 6 and 8, respectively . The dimension of input em-\nbeddings, hidden states and feed-forward layers set is as 352,\n352 and 1408, respectively .\n• Small two : this is another rescoring model with a smaller\nTransformer LM than Small one. It uses the same numbers of\ndecoder layers and attention heads as Small one, but the di-\nmension of input embeddings, hidden states and feed-forwar d\nlayers is further reduced to 256, 256 and 1024. For both small\nTransformers, we experiment with different BPE vocabular-\nies with 10K and 5K units to understand the impact of small\nvocabularies on ASR rescoring.\n4. RESUL TS AND DISCUSSION\n4.1. Overall Performance\nT able 1 shows the overall performance of various approaches on the\nvideo dataset. Here we train Transformer Large only on in-domain\nvideo transcriptions without adaptive softmax to study the impact\nof various strategies we explore to train models with small p aram-\neter sizes. And both small Transformer LMs are trained with a ll\nof our explored strategies, including smaller BPE vocabula ry sizes,\nadaptive softmax, and knowledge distillation from high cap acity pre-\ntrained and ﬁne-tuned models.\nW e can see that n-best rescoring with Transformer LMs is effec-\ntive to improve speech recognition accuracy . Speciﬁcally , rescoring\nwith the Large model achieves 7. 58% WERR, showing the effec-\ntiveness of rescoring with Transformer LMs. Additionally , the ﬁrst\nsmall model Small one obtains 7. 17% and 6. 81% WERR with 10K\nand 5K BPE vocabularies, while they only have 11. 9% and 9. 6%\nmodel sizes of the large model. Furthermore, we can see that t he\neven smaller model Small two still achieves similar speech recogni-\ntion accuracy , while only with 7. 2% and 5. 5% parameter sizes of the\nlarge model.\nW e further conduct latency study on a random sample of 5, 000\nn-best candidates generated from the ﬁrst-pass ASR system. F or\neach Transformer LM, we run it on the sampled set for 10 times on\nT able 2. Effect of Sub-word Unit Models and Adaptive Softmax on\nTransformer Large. “ AdaSoft” indicates whether we use adaptive\nsoftmax or not.\n#BPE AdaSoft #Param WER\n25K No 123. 4M 15. 60\n25K Y es 110. 0M 15. 58\n10K No 100. 4M 15. 60\n10K Y es 97. 7M 15. 60\n5K No 92. 7M 15. 58\n5K Y es 91. 4M 15. 64\nT able 3. Effect of Sub-word Unit Models and Adaptive Softmax on\nTransformer Small two.\n#BPE AdaSoft #Param WER\n25K No 17. 5M 15. 84\n25K Y es 13. 0M 15. 85\n10K No 9. 9M 15. 87\n10K Y es 8. 9M 15. 91\n5K No 7. 3M 15. 92\n5K Y es 6. 8M 15. 97\nthe same CPU machine and compute the average duration of infe r-\nence time. Our study shows that both small models with 5K or 10K\nBPE vocabularies can achieve speedup from 7. 6x to 8. 4x over the\nlarge Transformer LM with 25K vocabulary . These results demon-\nstrate that we can successfully train much smaller Transfor mer LMs\nthat not only signiﬁcantly improve speech recognition accu racy , but\nalso greatly reduce model inference latency and computatio nal costs.\n4.2. Effect of Sub-word Unit Models and Adaptive Softmax\nIn this section, we aim to study the effect of BPE vocabulary s izes\nand adaptive softmax on both large and small models. Thus we t rain\nboth Transformer Large and Small two on in-domain video data,\nand compare the system performance after rescoring. T able 2 and\n3 demonstrate the impact of these two techniques. By compari ng\nthe rows with the same BPE sizes from these two tables, we can\nsee that adaptive softmax further reduces model sizes while retain-\ning the gains from rescoring, demonstrating its effectiven ess to re-\nduce model size from the softmax layer. In addition, by reduc ing\nthe BPE vocabulary sizes from 25K to 10K or 5K, we can still see\nthat similar speech recognition accuracy is achieved for bo th mod-\nels, showing reducing BPE vocabulary sizes is another effec tive way\nto reduce model sizes. By combining both techniques, we can r e-\nduce the model sizes by 26% for Transformer Large, and 61% for\nTransformer Small two.\n4.3. Effect of Model Pre-training and Knowledge Distillati on\nT o study the joint impact of model pre-training and knowledg e distil-\nlation, we compare the rescoring performance of Small two models\ntrained on in-domain data with and without knowledge distil lation.\nT able 4 shows the perplexities and word error rates achieved by these\nmodels with 10K and 5K BPE vocabularies. W e can see that by dis-\ntilling the knowledge from the pre-trained then ﬁne-tuned t eacher\nmodels, we can achieve 11. 8% and 12. 7% perplexity reductions for\n10K and 5K vocabularies respectively , and also further reductions\non WERs.\nT able 4. Effect of Model Pre-training and Knowledge Distillation\nwith Transformer Small two.\nT eacher #BPE Perplexity WER\n- 10K 61. 59 15 . 91\nLarge (pre-trained) 10K 54. 35 15 . 78\n- 5K 50. 09 15 . 97\nLarge (pre-trained) 5K 43. 75 15 . 79\nT able 5. Effect of Model Pre-training with Transformer Large.\nPre-trained #BPE Perplexity WER\nNo 10K 46. 68 15 . 60\nY es 10K 36. 85 15 . 45\nNo 5K 36. 42 15 . 64\nY es 5K 31. 78 15 . 44\nW e then further compare perplexity and rescoring performan ce\nof Transformer Large with and without large-scale model pre-\ntraining to understand the impact of pre-training on ASR res coring.\nThe results are shown in T able 5 for both 10K and 5K vocabularies.\nEven though we already have a relative large in-domain datas et with\n144M tokens for neural LM training, we can easily see that the\nsimple pre-training then ﬁne-tuning strategy is still very effective\nin reducing perplexities (i.e., 20. 7% and 12. 7% PPL reductions for\nboth 10K and 5K vocabulary sizes, respectively). The models with\npre-training also obtain better rescoring performance, de monstrating\nthe effectiveness of large-scale model pre-training.\n5. CONCLUSION AND FUTURE WORK\nIn this paper, we have studied several techniques including subword\nunits, adaptive softmax, knowledge distillation with larg e-scale\nmodel pre-training to train Transformer LMs with small para meter\nsizes for efﬁcient ASR rescoring. Our empirical study shows that\nwe can signiﬁcantly reduce model parameter sizes and improv e\nspeech recognition accuracy with n-best rescoring by combining all\nthese explored techniques together. In the future, we plan t o explore\nknowledge distillation with bi-directional teachers mode ls, as well\nas two-stage distillation in both pre-training and ﬁne-tun ing stages.\n6. REFERENCES\n[1] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ily a\nSutskever, “Improving language understanding by generati ve\npre-training, ” T echnical report, OpenAI, 2018.\n[2] Y oshua Bengio, R´ ejean Ducharme, Pascal Vincent, and Ch ris-\ntian Jauvin, “ A neural probabilistic language model, ” Journal\nof machine learning research, vol. 3, no. Feb, pp. 1137–1155,\n2003.\n[3] T om´ aˇ s Mikolov , Martin Karaﬁ ´ at, Luk´ aˇ s Burget, Jan ˇCernock ` y,\nand Sanjeev Khudanpur, “Recurrent neural network based lan -\nguage model, ” in Eleventh annual conference of the interna-\ntional speech communication association, 2010.\n[4] T om´ aˇ s Mikolov , Stefan Kombrink, Luk´ aˇ s Burget, Jan\nˇCernock ` y, and Sanjeev Khudanpur, “Extensions of recur-\nrent neural network language model, ” in 2011 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2011, pp. 5528–5531.\n[5] Martin Sundermeyer, Ralf Schl ¨ uter, and Hermann Ney , “L stm\nneural networks for language modeling, ” in Thirteenth annual\nconference of the international speech communication asso ci-\nation, 2012.\n[6] Anoop Deoras, T om´ aˇ s Mikolov , and Kenneth Church, “ A fa st\nre-scoring strategy to capture long-distance dependencie s, ” in\nProceedings of the Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computational Lin-\nguistics, 2011, pp. 1116–1127.\n[7] William Chan, Navdeep Jaitly , Quoc Le, and Oriol Vinyals ,\n“Listen, attend and spell: A neural network for large vocabu -\nlary conversational speech recognition, ” in 2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2016, pp. 4960–4964.\n[8] Shankar Kumar, Michael Alexander Nirschl, Dan Holtmann -\nRice, Hank Liao, Ananda Theertha Suresh, and Felix Y u, “Lat-\ntice rescoring strategies for long short term memory langua ge\nmodels in speech recognition, ” in IEEE Automatic Speech\nRecognition and Understanding W orkshop, 2017.\n[9] W ayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xue-\ndong Huang, and Andreas Stolcke, “The microsoft 2017 con-\nversational speech recognition system, ” in 2018 IEEE interna-\ntional conference on acoustics, speech and signal processi ng\n(ICASSP). IEEE, 2018, pp. 5934–5938.\n[10] Anirudh Raju, Denis Filimonov , Gautam Tiwari, Guitang Lan,\nand Ariya Rastrow , “Scalable multi corpora neural language\nmodels for ASR, ” in Interspeech, 2019.\n[11] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko -\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin, “ Attention is all you need, ” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[12] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina\nT outanova, “Bert: Pre-training of deep bidirectional tran sform-\ners for language understanding, ” in Proceedings of the 2019\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language T echnolo-\ngies, V olume 1 (Long and Short P apers), 2019, pp. 4171–4186.\n[13] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dari o\nAmodei, and Ilya Sutskever, “Language models are unsuper-\nvised multitask learners, ” OpenAI Blog, vol. 1, no. 8, 2019.\n[14] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. W eld, Lu ke\nZettlemoyer, and Omer Levy , “SpanBERT: Improving pre-\ntraining by representing and predicting spans, ” arXiv preprint\narXiv:1907.10529, 2019.\n[15] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo,\nand Llion Jones, “Character-level language modeling with\ndeeper self-attention, ” arXivpreprint arXiv:1808.04444, 2018.\n[16] Zihang Dai, Zhilin Y ang, Yiming Y ang, Jaime Carbonell, Quoc\nLe, and Ruslan Salakhutdinov , “Transformer-XL: Attentive\nlanguage models beyond a ﬁxed-length context, ” in Proceed-\nings of the 57th Annual Meeting of the Association for Compu-\ntational Linguistics, 2019.\n[17] Kazuki Irie, Albert Zeyer, Ralf Schl ¨ uter, and Hermann Ney ,\n“Language modeling with deep transformers, ” in INTER-\nSPEECH, 2019.\n[18] Sepp Hochreiter and J ¨ urgen Schmidhuber, “Long short- term\nmemory , ” Neural Comput., 1997.\n[19] Rico Sennrich, Barry Haddow , and Alexandra Birch, “Neu -\nral machine translation of rare words with subword units, ” i n\nProceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long P apers), 2016, pp.\n1715–1725.\n[20] ´Edouard Grave, Armand Joulin, Moustapha Ciss´ e,\nDavid Grangier Facebook AI Research, and Herv´ e J´ egou,\n“Efﬁcient softmax approximation for gpus, ” in Proceedings\nof the 34th International Conference on Machine Learning -\nV olume 70, 2017.\n[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, “Distil l-\ning the knowledge in a neural network, ” arXiv preprint\narXiv:1503.02531, 2015.\n[22] Y onghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mo-\nhammad Norouzi, and W olfgang Macherey etc, “Google’s\nneural machine translation system: Bridging the gap be-\ntween human and machine translation, ” arXiv preprint\narXiv:1609.08144, 2016.\n[23] Frederic Morin and Y oshua Bengio, “Hierarchical proba bilistic\nneural network language model, ” in AISTATS05, 2005.\n[24] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lam ar,\nRichard Schwartz, and John Makhoul, “Fast and robust neural\nnetwork joint models for statistical machine translation, ” in\nProceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long P apers), 2014.\n[25] W enlin Chen, David Grangier, and Michael Auli, “Strate gies\nfor training large vocabulary neural language models, ” in Pro-\nceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long P apers), 2016.\n[26] Andriy Mnih and Y ee Whye T eh, “ A fast and simple algorith m\nfor training neural probabilistic language models, ” in Proceed-\nings of the 29th International Coference on International Con-\nference on Machine Learning, 2012.\n[27] Xie Chen, Xunying Liu, Mark J. F . Gales, and Philip C. W oo d-\nland, “Recurrent neural network language model training\nwith noise contrastive estimation for speech recognition. , ” in\nICASSP, 2015.\n[28] Y angyang Shi, Mei-Y uh Hwang, Xin Lei, and Haoyu Sheng,\n“Knowledge distillation for recurrent neural network lang uage\nmodeling with trust regularization., ” in ICASSP, 2019.\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy , Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov , “Roberta: A robustly\noptimized BERT pretraining approach, ” arXiv preprint\narXiv:1907.11692, 2019.\n[30] Duc Le, Xiaohui Zhang, W eiyi Zheng, Christian F ¨ ugen, G eof-\nfrey Zweig, and Michael Seltzer, “From senones to chenones:\nTied context-dependent graphemes for hybrid speech recogn i-\ntion, ” in IEEE Automatic Speech Recognition and Understand-\ning W orkshop, 2019.\n[31] Y u Zhang, Guoguo Chen, Dong Y u, Kaisheng Y ao, Sanjeev\nKhudanpur, and James R. Glass, “Highway long short-term\nmemory rnns for distant speech recognition., ” in ICASSP,\n2016.\n[32] Jun Liu, Jiedan Zhu, Vishal Kathuria, and Fuchun Peng, “ Ef-\nﬁcient dynamic wfst decoding for personalized language mod -\nels, ” arXiv preprint arXiv:1910.10670, 2019."
}