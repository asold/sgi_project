{
  "title": "The Effect of Using E-Learning Tools in Online and Campus-based Classrooms on Student Performance",
  "url": "https://openalex.org/W2101170729",
  "year": 2011,
  "authors": [
    {
      "id": "https://openalex.org/A5062184258",
      "name": "Édith Galy",
      "affiliations": [
        "Brownsville Public Library"
      ]
    },
    {
      "id": "https://openalex.org/A5022470028",
      "name": "Clara Downey",
      "affiliations": [
        "Brownsville Public Library"
      ]
    },
    {
      "id": "https://openalex.org/A5000349831",
      "name": "Jennie Johnson",
      "affiliations": [
        "Brownsville Public Library"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1515402129",
    "https://openalex.org/W1578907434",
    "https://openalex.org/W6675028442",
    "https://openalex.org/W1981018178",
    "https://openalex.org/W1491644571",
    "https://openalex.org/W2139766064",
    "https://openalex.org/W6604931823",
    "https://openalex.org/W2166939925",
    "https://openalex.org/W6893254818",
    "https://openalex.org/W6629425436",
    "https://openalex.org/W2160861367",
    "https://openalex.org/W6677196538",
    "https://openalex.org/W1485865314",
    "https://openalex.org/W329276638",
    "https://openalex.org/W1539976456",
    "https://openalex.org/W2116515016",
    "https://openalex.org/W44127630",
    "https://openalex.org/W2126512988",
    "https://openalex.org/W1593544550",
    "https://openalex.org/W1557992034",
    "https://openalex.org/W1791587663",
    "https://openalex.org/W2033943395",
    "https://openalex.org/W2023107699",
    "https://openalex.org/W1598235329",
    "https://openalex.org/W2036389121",
    "https://openalex.org/W1981516854",
    "https://openalex.org/W2602257709",
    "https://openalex.org/W2030306182",
    "https://openalex.org/W2126310609",
    "https://openalex.org/W2108617106",
    "https://openalex.org/W2159533189",
    "https://openalex.org/W1533217779",
    "https://openalex.org/W2067007689",
    "https://openalex.org/W2891491867",
    "https://openalex.org/W1502752481",
    "https://openalex.org/W1556686995",
    "https://openalex.org/W1495844725",
    "https://openalex.org/W1811675263",
    "https://openalex.org/W2102447168",
    "https://openalex.org/W2087836072",
    "https://openalex.org/W2163548881",
    "https://openalex.org/W1597911883",
    "https://openalex.org/W2146733310",
    "https://openalex.org/W2052781747",
    "https://openalex.org/W2077256301",
    "https://openalex.org/W2070940979",
    "https://openalex.org/W2127184601",
    "https://openalex.org/W2033730096",
    "https://openalex.org/W1988885200",
    "https://openalex.org/W2135921556",
    "https://openalex.org/W325327638",
    "https://openalex.org/W2004181325",
    "https://openalex.org/W1925905589",
    "https://openalex.org/W2135099384",
    "https://openalex.org/W1593070952",
    "https://openalex.org/W2023627536",
    "https://openalex.org/W2112530076",
    "https://openalex.org/W2128185258",
    "https://openalex.org/W6606920931",
    "https://openalex.org/W2119679964",
    "https://openalex.org/W1608305941",
    "https://openalex.org/W1555769448",
    "https://openalex.org/W1480415088",
    "https://openalex.org/W2168569455",
    "https://openalex.org/W1976243401",
    "https://openalex.org/W2125468779",
    "https://openalex.org/W3208765826",
    "https://openalex.org/W2099697766",
    "https://openalex.org/W2110331776",
    "https://openalex.org/W2030376410",
    "https://openalex.org/W120039895",
    "https://openalex.org/W278548843",
    "https://openalex.org/W1885860673",
    "https://openalex.org/W169614790",
    "https://openalex.org/W2563410413",
    "https://openalex.org/W2077545220",
    "https://openalex.org/W2766732954",
    "https://openalex.org/W2015852493",
    "https://openalex.org/W2166982098",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2102256625",
    "https://openalex.org/W1979755600",
    "https://openalex.org/W1990587784",
    "https://openalex.org/W2116178727",
    "https://openalex.org/W1492970745",
    "https://openalex.org/W1550621568",
    "https://openalex.org/W4243846760",
    "https://openalex.org/W4300414598",
    "https://openalex.org/W2050779701",
    "https://openalex.org/W2087812817",
    "https://openalex.org/W4241569321",
    "https://openalex.org/W4241720604",
    "https://openalex.org/W2133871187",
    "https://openalex.org/W1991272408"
  ],
  "abstract": "An international association advancing the multidisciplinary study of informing systems. Founded in 1998, the Informing Science Institute (ISI) is a global community of academics shaping the future of informing science.",
  "full_text": "Journal of Information Technology Education Volume 10, 2011 \nThe Effect of Using E-Learning Tools  \nin Online and Campus-based Classrooms  \non Student Performance \nEdith Galy, Clara Downey, and Jennie Johnson \nUniversity of Texas at Brownsville, College of Business,  \nBrownsville, TX, USA \nedith.galy@utb.edu; claradowney@gmail.com; jennie.johnson@utb.edu  \nExecutive Summary \nCreating an integrative research framework that extends a model frequently used in the Information \nSystems field, the Technology Acceptance Model, together with variables used in the Education field, \nthis empirical study investigates the factors influencing student performance as reflected by their final \ncourse grade.  The Technology Acceptance Model explains computer acceptance in general terms. \nThe model measures the impact of external variables on internal beliefs, attitudes, and intentions. Per-\nceived Usefulness and Perceived Ease of Use, two main constructs in the model, refer to an individ-\nual’s perception of how the adoption of a new technology will increase their efficiency, and the indi-\nvidual’s perception of how easy the technology will be to use. The lower the perceived effort is, the \neasier the technology will be to adopt. Thus, Perceived Usefulness, Perceived Ease of Use, Computer \nSelf-Efficacy, and Computer Anxiety were measured to determine their effect on student perform-\nance.  \nThe proliferation of the personal computer was possible because of the applications written for it. The \ncontinuous creation of new applications has created ample ground to test the Technology Acceptance \nModel to determine how a user will decide to adopt such applications. The recent escalation of deliv-\nering online education via the Internet has again sparked a new dimension of information systems. \nThis has given rise to research using the Technology Acceptance Model for applications in the Educa-\ntion field.  \nToday’s modern classroom, whether online or campus-based, uses e-learning tools and Learning \nManagement Systems that capture student cognition and engages them in the learning process via \ntechnology, while increasing their need for self-directedness. In view of this, the present study also \nconsiders the students’ ability to work independently. \nThe results of the statistical analysis used in this study revealed marked differences in student percep-\ntions of e-learning tools between students who chose to take an online course and students who pre-\nferred to take the campus-based section. Additionally, Perceived Usefulness, Perceived Ease of Use, \nand the students’ ability to work independ-\nently were all statistically significant factors \nin predicting students’ final grades. \nMaterial published as part of this publication, either on-line or \nin print, is copyrighted by the Informing Science Institute. \nPermission to make digital or paper copy of part or all of these \nworks for personal or classroom use is granted without fee \nprovided that the copies are not made or distributed for profit \nor commercial advantage AND that copies 1) bear this notice \nin full and 2) give the full citation on the first page. It is per-\nmissible to abstract these works so long as credit is given. To \ncopy in all other cases or to republish or to post on a server or \nto redistribute to lists requires specific permission and payment \nof a fee. Contact HPublisher@InformingScience.orgH  to \nrequest redistribution permi\nKeywords:  E-Learning Tools, Technology \nAcceptance Model (TAM), Perceived Use-\nfulness, Perceived Ease of Use, , Computer \nSelf-Efficacy, Computer Anxiety, Ability to \nWork Independently, Distance Education \nssion.  \nEditor:  Peter Blakey \nE-Learning Tools and Student Performance \nIntroduction \nIn the 2006–2007 academic year, 2-year and 4-year institutions reported an estimated 12.2 million \nenrollments in college-level, credit-granting Distance Education (DE) courses.  Of these DE enroll-\nments, 77 percent were reported in online courses, 12 percent were reported in hybrid/blended online \ncourses, and 10 percent were reported in other types of DE courses including correspondence courses \n(Parsad & Lewis, 2008). The U.S. Department of Education (Parsad & Lewis, 2008) reported that in \n2006-2007 universities augmented online courses mainly because students required flexibility in their \nschedules, and to reach students who would not have had access to a college education. Doyle (2009) \nreports that 3.9 million students registered for classes in the Fall 2007 semester were taking at least \none online course. Allen & Seaman (2010b) report that there was a 17% increase in the Fall of 2008, \nand that one of four higher education students have at least taken one online course. In the Fall of \n2009, the number of students taking at least one online course rose to 5.6 million, a 21% increase \nfrom the previous year (Allen & Seaman, 2010a). However, DE has evolved as students choose to \nadd online courses onto their existing, campus-based curriculum. In other words, students are seeking \nto complement, not substitute, campus-based education.  Adding to the validity of this phenomenon is \nthat students take online courses in institutions close to their homes with three-fourths of online stu-\ndents living less than 40 miles from their school.   \nCurrent trends in education confirm that instructors are shifting away from authoritarian and non-\ninteractive courses.  Students have gained control in their courses with the aid of two-way communi-\ncation, as well as group-oriented activities (Lou, Bernard, & Abrami, 2006). Student self-directed \ncourses use audio, videotapes, and other web-based resources. E-learning, defined as computer net-\nworked technology to provide course material and content (Welsh, Wanberg, Brown, & Simmering, \n2003), is increasing. Beginning in 2003, the Sloan Consortium (Sloan-C) has conducted annual re-\nports concerning DE trends in the United States. Sloan-C provides the following definitions that are \nuseful in distinguishing classroom types: \n• Traditional – Course without online technology used; therefore, 0% of content is de-\nlivered online, \n• Web Facilitated – Courses that use web-based technology to facilitate what is essen-\ntially a face-to-face course and may use a Learning Management System or web pag-\nes to post syllabus and assignments. Less than 30% of the content is delivered online, \n• Blended/Hybrid – Substantial proportion of content is delivered online, typically uses \nonline discussions and has a reduced number of face-to-face meetings. 30-79% of the \ncontent is delivered online,  \n• Online – More than 80% of the content is delivered online. Typically there are no \nface-to-face meetings. \nThe advent of e-learning as part of the collaborative learning paradigm includes Web 2.0 technolo-\ngies, which are widely used by our students and are now making their way into the classroom. In-\nstructors are finding these new technologies very useful additions to their DE classrooms as they can \nenhance learning among our tech savvy students, mirroring the use of these technologies in their daily \nlives. Web 2.0 technologies include wikis, blogs, podcasts, social networks, video-sharing sites such \nas YouTube, and virtual worlds such as Second Life. Educators and researchers can anticipate that \nnew technologies will continue to be introduced, which will require adaptation by both students and \ninstructors, supported by investigation by researchers as to their effectiveness. It is important to look \nfor “clues as to how e-learning technologies can become powerful catalysts for change as well as \ntools for redesigning our learning and instructional systems” (Shroff & Vogel, 2009, p.60). \nThe maturing of instructional platforms, also referred to as Learning Management Systems (LMS), is \nanother development in recent DE history. Saadé and Kira (2009) describe LMS as a framework that \n210 \n Galy, Downey, & Johnson \nincludes instructor tools, learning process tools, and a repository of data. Examples of LMS platforms \ninclude WebCT, Blackboard, and DesireToLearn, which have emerged as the top three LMS and are \npervasive in today’s DE environment. Most recently Moodle has emerged as a new LMS open-source \nsystem, a free alternative to the aforementioned platforms (Unal & Unal, 2011).  It is essential that the \ntools implemented support the course assignments, activities, and content (Singh, Mangalaraj, & Ta-\nneja, 2010; Smart & Cappel, 2006).  “Clearly, technology-supported learning environments have the \npotential to provide tools and structure to transform education” (Shroff & Vogel, 2009, p. 60). The \nstudents used in this study have been exposed to Blackboard LMS through which students participate \nin discussion forums, online journals, Wikis, online testing and practice quizzes, virtual teams, You-\nTube, and other interactive tools. \nIn spite of all the technological advances evidenced in recent DE courses and the emphasis universi-\nties are now placing on integrating online students into the student community, attrition rates among \nonline education remain high (Haigh, 2007; McLaren, 2004). So who are the students most likely to \nsucceed? In search for this answer, Haigh (2007) investigated student characteristics including gen-\nder, study habits, learning styles, learning environment, access to resources, experiences with distance \nlearning, and technology proficiency.  Learning style can be defined “as a person’s preferred ap-\nproach to information processing, idea formation, and decision making” (Kalsbeek, 1989, p. 2). In \naddition to student learning styles, other student characteristics such as age, prior familiarity with \nhigher education, years of computer use, and student preference between individual rather than group \nwork may also be factors in explaining student success in DE. According to Doyle (2009), there are \nno statistically significant differences between online students and campus-based students in regard to \nrace, ethnicity, or income.  However, there are various demographic differences to consider. For ex-\nample, online students tend to be older than their peers in regular classrooms. An online student is \nmost likely to be over 30, while a regular student will most likely be between 15-23 years of age, and \nthere are more female online students than male students. As students continue to pursue both cam-\npus-based and online courses, it is imperative to investigate learning outcomes between these differ-\ning platforms (Allen & Seaman, 2010). \nIn the current climate, many formal education institutions have integrated DE into their curriculum \nformat.  While campus-based courses continue, students are increasingly given the option to pursue \ndegrees entirely offered online or supplement the traditional format with online classes (Allen & \nSeaman, 2010a, 2010b).  Considering the proliferation of students seeking courses online, it is essen-\ntial to investigate student success in online and campus-based platforms. In their comprehensive re-\nview, Bekele and Menchaca (2008) suggested relevant research should utilize strong theoretical \nframework, employ compatible implementation and assessment, and adhere to a clear methodological \nstructure.  Although constrained to only online students, Seok, DaCosta, Kinsell, and Tung (2010) \nsuggest that instructional design features require further investigation regarding student learning pref-\nerences, along with offering an array of evaluation methods (Kılıç-Çakmak, Karataş, & Ocak, 2009). \nThe purpose of this study is to explore differences in student perceptions of e-learning tools in online \nand campus-based courses and to measure the explanatory power of these perceptions in predicting \ntheir course grades. The intent of this study is to contribute to the existing literature by including and \ncomparing online and campus-based student performance. In addition, the following discussion of the \nliterature will suggest further exploration among variables from the field of Information Systems: \nPerceived Usefulness, Perceived Ease of Use, Computer Self-Efficacy, and Computer Anxiety in \ncombination with variables in the Education literature: the ability to work independently variable, and \ncourse performance. To begin, a review of the literature on course performance follows. \nCourse Performance \nStudent performance in online versus campus-based courses has been the subject of an increasing \nnumber of studies.  The relevant literature has employed a student’s grade as an indicator of course \n 211 \nE-Learning Tools and Student Performance \nperformance (Brecht & Ogilby, 2008; Buche, Davis, & Vician, 2007; Caspi & Gorsky, 2006; \nChyung, 2007; Hiltz & Wellman, 1997; Irani, Telg, Scherler, & Harrington, 2003; Kock, Verville, & \nGarza, 2007; Loomis, 2000; Manochehri, & Young, 2006; McLaren, 2004; Newlin, Lavooy, & \nWang, 2005; Puzziferro, 2008; Rabe-Hemp , Woollen, & Humiston, 2009; Syler, Cegielski, Oswald, \n& Rainer, 2006). Interestingly, among the studies comparing final course grade differences between \nonline and campus-based students, many have not found the difference to be statistically significant \n(Benson, Johnson, Taylor, Treat, Shinkareva, & Duncan, 2005; Hiltz & Wellman, 1997; Kock et al., \n2007; Manochehri, & Young, 2006; McLaren, 2004; Newlin, et al., 2005; Rabe-Hemp et al., 2009). \nNotwithstanding this fact, the course final grade is still an important reference in the literature. Kock \net al. (2007) conducted an experiment with an introduction to information systems (IS) course that \nwas taught both online and on-campus. Their experiment was longitudinal, including assessments in \nthe midterm point and at the end of the semester. Their instrument included both quantitative meas-\nures as indicated by test results and qualitative measures where students answered open-ended ques-\ntions. The quantitative results showed online students were performing at a significantly lower level \nthan their face-to-face counterparts in the midterm, but the results of the final exams did not show \nstatistically significant differences. The qualitative comparison, which measured the level of ambigu-\nity, cognitive effort, and excitement in the two types of courses, did not reveal any significant differ-\nences. \nMurthy and Kerr (2004) tested the performance differential of groups using one of three media to \nproblem solve an auditing problem in an accounting course. The three media were face-to-face, chat, \nand bulletin boards. There were a total of 35 teams, each consisting of four group members. The re-\nsults showed that the teams using the bulletin board out-performed the others by a wide margin.  Of \nthe teams using the bulletin boards, 77% answered both of the case questions correctly compared to \n42% of the teams using the chat and 44% using face-to-face communication. The difference in per-\nformance between the chat and face-to-face communication was not statistically significant. How-\never, the teams using the bulletin board communication method took longer to arrive at their answers. \nDobbs, Waid, and del Carmen (2009) included student perceptions as a factor that contributes to the \nexplanation of differences in student performance in online courses. Analysis of departmental evalua-\ntions indicated that around 30% of students felt that they had learned more in the online class, that it \nwas more intellectually challenging, and that it was more difficult. Student readiness for online cours-\nes has been shown to be a significant predictor of student course satisfaction.  The student’s readiness \nconstruct as measured by Dobbs et al. (2009) is composed of items such as a student’s general atti-\ntudes towards online courses, self-efficacy in prerequisite skills, independent initiative, and desire for \ninteraction.  \nWith the advent of incorporating web-based technology in the classroom, a videoconference format \nwas used to deliver course content.  Irani et al. (2003) found that personality type influenced student \nperformance, with introverts exhibiting a positive relationship between only instructional technique \nperception and grade attainment.  Loomis (2000) found that students reporting strong time manage-\nment skills earned higher grades, suggesting their ability to better direct their studies pursuant with \ntheir own schedules.  The use of web-based tutorials may indicate that students can achieve higher \ncourse grades (Syler et al., 2006). Comparing in-class and online sections, two separate studies found \nno statistically significant differences between students’ final grades (Kock et al., 2007; Newlin et al., \n2005).     \nStudent performance in this study was based on multiple assessment measures including exams, dis-\ncussion, and individual project-based work.  Other related research has employed a student’s grade as \nan indicator of course performance. In a meta-analysis of research concerning online education that \nsurvey business students, investigators found that early studies were not able to substantiate differ-\nences in test performance, and that subsequent studies revealed differences in attitudes toward a me-\ndium affected student performance as the semester progressed (Arbaugh, Godfrey, Johnson, Pollack, \n212 \n Galy, Downey, & Johnson \nNiendorf, & Wresch, 2009). Although many variables have been studied in relationship to student \nperformance, few have looked at it in relation to student acceptance of technology considering both \nonline and campus based students. \nTheoretical Background \nFishbein and Ajzen (1975) and Ajzen and Fishbein (1977) proposed that the immediate determinant \nof behavior is intention, or what they termed behavioral intention.  The construct captures motiva-\ntional factors such as how much effort a person is willing to exert to perform a specific behavior \n(Ajzen, 1991).  The Theory of Reasoned Action (TRA) model suggests that several factors precede \nbehavior, specifically attitudes and subjective norms (Ajzen & Fishbein, 1980; Fishbein & Ajzen, \n1975). Additionally, the model is based upon two basic assumptions: first, people are rational and, \nsecondly, their social actions are under volitional control.  Rational implies that humans use informa-\ntion available to them, and the model is based on the premise that social interactions are largely \nguided by reasoning and behavior (Ajzen, 1991).  Attitude toward a behavior is a favorable or posi-\ntive evaluation or unfavorable or negative evaluation of performing the behavior.  Subjective norm \nrefers to an individual’s perception of the social pressures felt either for or against performing a spe-\ncific behavior, and how one’s social circle or those who influence one’s decisions view the behavior.  \nIn essence, the model assists to explain a person’s behavior and is applicable in many fields.  \nThe Technology Acceptance Model (TAM), developed by F. Davis (1986) is a derivative of the TRA \nmodel.  Specifically, the model supports the investigation of an individual’s “determinants of com-\nputer usage behavior” (F. Davis, Bagozzi, & Warshaw, 1989, p. 983).  Based upon TRA, the TAM \nwas modified and “specifically tailored for modeling user acceptance of information systems” (F. \nDavis et al., 1989, p. 985).  The TAM model infers “that an individual’s behavioral intention to use a \nsystem is determined by two beliefs: perceived usefulness” and perceived ease of use (Venkatesh & \nDavis, 2000, p.186-187), the two main components of the model which has been widely used to \nmeasure the use of various information technologies and is now being used as a framework to meas-\nure the effectiveness of DE (Arbaugh, 2005; Martins & Kellermanns, 2004; Manochehri & Sharif, \n2010; Unal & Unal, 2011). Research into the cognitive processes a student undergoes when taking an \nonline class, also referred to as e-learning, is well served by using the models used in IS research. \nIn this study, e-learning tools are defined as the electronic delivery of learning materials and instruc-\ntion over the Internet (Welsh et al., 2003). The utilization of discussion forums and chats are exam-\nples of e-learning tools (Nam & Smith-Jackson, 2007; Saadé, 2007).  In the context of e-learning, a \nstudent’s likelihood of using a LMS such as Blackboard, for example, or any of the Web 2.0 tech-\nnologies can be determined by the student’s attitude toward using the technology and its perceived \nease of use.  \nConsidering this study’s focus on e-learning tools and their relationship to student performance in \nboth online and campus-based classes, TAM (F. Davis, 1989) provides insights into how the students’ \nperceptions of various e-learning tools influences students’ decisions to take an online or campus-\nbased course. In addition, these perceptions and attitudes play a predictive role on the course grade.  \nIn the model, perceptions that a particular technology is useful and easy to use influence students’ \nattitude toward the technology and, consequently, their decision to use it.   \nPerceived Usefulness and Perceived Ease of Use \nF. Davis (1989) argued that it is necessary to provide indicators for “predicting and explaining system \nuse” (p. 319). Perceived Usefulness (PU) is defined as “the extent to which a person believes that us-\ning the system will enhance his or her job performance,” and Perceived Ease of Use (PEU) is “the \nextent to which a person believes that using the system will be free of effort” (Venkatesh & Davis, \n2000, p.187). Previous research has investigated PU and PEU extensively (Adams, Nelson, & Todd, \n 213 \nE-Learning Tools and Student Performance \n1992; Agarwal & Karahanna, 2000; F. Davis, 1989; R. Davis & Wong, 2007; Gibson, Harris, &, Co-\nlaric, 2008; Igbaria, Guimaraes, & Davis, 1995; Saadé, 2007; Venkatesh, 2000; Venkatesh & Davis, \n2000).  F. Davis (1989) is credited with clarifying the variables and found quantifiable “relationships \nwith self-reported measures of usage behavior” (p. 333).  Interestingly, the study found users exhib-\nited a willingness to manage difficulty encountered with system usage.  Although the construct’s rela-\ntionships are well tested, Adams et al. (1992) found that user experience may influence the associa-\ntion between PEU and usage.  The construct was found to have a strong effect on PU and computer \nusage when considering user training, system quality, and computer experience (Igbaria et al., 1995). \nAgarwal and Karahanna (2000) reiterated the importance of investigating user response to technology \npractice, and stress the importance of user perceptions. Training has been found to directly influence \nuser perceptions of technology (Venkatesh, 1999). Furthermore, it is proposed that with the evolution \nof technology, the value of acknowledging the relevance of user experience on usage intention should \nbe explored.  Allowing that PEU is a strong indicator of user acceptance, Venkatesh (2000) highlights \nthe need to contribute to the research regarding the determinants of this construct.  The author also \nsuggests the need to enhance technology acceptance by further investigating PEU in light of condi-\ntions to improve positive perception, while recognizing the strength of user’s perception. For exam-\nple, students were found to assign a value of usefulness or utility to different course content. Content \nseen as beneficial, or of high utility, was visited more frequently than support material with no per-\nceived utility such as external references (Landry, Griffeth, & Hartman, 2006; Saadé & Bahli, 2005). \nThe following hypotheses were formulated related to student perceptions of e-learning tools and per-\nformance: \nH\n1:  Perceived Ease of Use of e-learning tools will have a statistically significant correla-\ntion to the course grade a student will achieve. \nH2:  Perceived Usefulness of e-learning tools will have a statistically significant correla-\ntion to the course grade a student will achieve. \nComputer Self-Efficacy \nSelf-efficacy is defined as a person’s perception regarding his or her ability to successfully execute a \nbehavior required in accomplishing a desired outcome (Bandura, 1977).  Researchers have shown the \nimportance of capturing both the magnitude (level of task difficulty) and strength (the confidence \nplaced on attaining the level of task difficulty) dimensions when measuring self-efficacy (Hartwick & \nBarki, 1994).  Bandura and Cervone (1986) further refined the definition as the belief in one’s capa-\nbilities to mobilize the motivation, cognitive resources, and courses of action needed to meet the situ-\national demands.  \nCompeau and Higgins (1995) review of research concerning the measurement of self-efficacy within \nthe computing context resulted in defining Computer Self-Efficacy (CSE) as “a judgment of one’s \ncapability to use a computer” (p. 192).  Interestingly, when measuring user support, a negative influ-\nence was found because users developed a dependency on the support and were not able to increase \ntheir CSE. Concerning PEU and usage, CSE did exhibit a strong relationship, which can be explained \nas a relationship between confidence and CSE (Gong, Xu, & Yu, 2004). This implies that the easier \nthe system is to use, the greater will be the perceived CSE. Saadé and Kira (2009) found evidence to \nsupport the important mediating role of CSE in student usage of LMS. They conclude that positive \nlearning usage increases with ease of use. Therefore, close attention must be paid to providing simple, \ninteractive, and familiar features in the course design in online courses. \nMartins & Kellermanns (2004) assessed business students’ self-efficacy of using the web and its rela-\ntionship to PEU. Furthermore, CSE is a dynamic construct and reflects more than ability assessment; \nit forms a critical influence on future intentions.  Students with greater feelings of preparedness report \na strong relationship with CSE (Mykytyn, Pearson, Paul, & Mykytyn, 2008).  When measuring stu-\n214 \n Galy, Downey, & Johnson \ndent learning, there was a strong relationship found with CSE (Simmering, Posey, & Piccoli, 2009), \nwhich strengthens the belief in confidence in personal abilities assists with knowledge acquisition. \nDue to the proliferation of technology usage, there is value in continued investigation of CSE’s role \namong computer users (Marakas, Yi, & Johnson, 1998).  Furthermore, as previous studies are mostly \nlimited to a single academic course, the inclusion of additional academic disciplines in a single study \nis suggested (Gong et al., 2004). Lastly, Puzziferro (2008) did not find a correlation between online \nself-efficacy and student performance, which necessitates further exploration into the relationship. \nThus, the following hypothesis was formulated for this study: \nH\n3:  Computer Self- Efficacy will have a statistically significant correlation to the course \ngrade a student will achieve. \nComputer Anxiety \nAccording to Bandura (1977), there are four primary antecedents for self-efficacy judgments, the \nmost important of which is emotional arousal.  Emotional arousal is tied to computer anxiety, and \ncomputer phobia and high levels of arousal are associated with reduced computer performance. There \nis a cycle of anxiety in a computer setting that increases resistance or even fear toward computers \n(Marakas et al., 1998). Anticipatory self-arousal (Bandura, 1977) needs to be controlled through di-\nrect anxiety-reducing mechanisms. \nComputer Anxiety (CA) is defined as the inclination of an individual to have a negative reaction \nwhile considering computer usage (Howard & Smith, 1986; Saadé & Kira, 2009).  The relevance of \ninvestigating the attitude toward computer usage is underpinned by Fishbein and Ajzen (1975), who \nsuggest that feelings for an object may impact response.  In fact, CA was found to negatively affect \nthe attitude toward using computer technology (Igbaria & Parasuraman, 1989).  From these findings, \nit was suggested that CA might be decreased with appropriate educational training.  An individual’s \nsystem experience can play an important role in computer usage.  Recent studies suggest that an indi-\nvidual’s CA could be lowered with positive system experience and not just mere exposure to technol-\nogy (Hackbarth, Grover, & Yi, 2003; Buche et al., 2007). Several studies have investigated the rela-\ntionship between self-efficacy and CA.  Compeau and Higgins (1995) suggested that self-efficacy \ninfluences an individual’s feelings. More specifically, Saade and Kira (2009) found that those users \nexhibiting high self-efficacy experienced less CA; therefore, it follows that as CA increases, PEU de-\ncreases (Saadé & Kira, 2009). Furthermore, CA was found to have a negative relationship to per-\nformance related outcomes (Compeau, Higgins, & Huff, 1999). Buche et al. (2007) wisely point to \nthe fact that even though our society uses technology ubiquitously, our “contemporary subjects” are \nstill faced with CA and that the need to study the relationship between CA and performance has be-\ncome even more important. Therefore, the following hypothesis was formulated to explore the rela-\ntionship between performance and CA:  \nH\n4: Computer Anxiety will have a statistically significant correlation to the course grade \na student will achieve. \nAbility to Work Independently (WORK) \nA substantial research stream within the education literature has sought to explain whether the differ-\nence in dispositions towards a particular instructional method can be considered a learning preference \nor style. Are there differences between students who chose the traditional lecture learning environ-\nment and the online environment?  Specifically, are there differences in student perceptions, skills, \nand aptitudes that lead them to choose an online environment over the campus-based environment? \nAre these differences in perceptions, skills, and aptitudes a contributing factor to explaining student \nperformance as the final course grade?  \n 215 \nE-Learning Tools and Student Performance \nThe ability for students to coordinate their study time has been investigated (Loomis, 2000; Rabe-\nHemp et al. 2009; Syler et al., 2006).  A fundamental component of student success is course prepara-\ntion and study.  A study conducted by Rabe-Hemp et al. (2009) found that online students reported \nmore time allocated to class preparation activities than campus-based students.  In addition, to assist \nstudent success online courses should communicate the necessity of time management (Calvin & \nFreeburg, 2010; Puzziferro, 2008). This suggests that successful students manage their course prepa-\nration according to their personal schedule, instead of attending physical class times (Loomis, 2000).   \nOnline courses and e-learning tools create learning situations where students are more responsible for \ntheir own learning.  Thus, online courses and the use of e-learning tools engage students in the learn-\ning process that Knowles (1975) described as participating in self-directed inquiry and Candy (1991) \ncharacterized as learning with predominant learner control. Generally recognized for revisiting the \nterm andragogy, Knowles (1980) began early to use the terms self-directed learning and self-directed \ninquiry in his writings related to adult learning.  A student’s ability to work independently is embod-\nied in this idea of self-directed or self-regulated learning (Manochehri & Sharif, 2010; Shroff & Vo-\ngel, 2009; Smart & Cappel, 2006).  Considered a pre-entry requirement for enrolling in an online \ncourse, a student characteristic associated with online success, or sometimes referred to a student suc-\ncess factor or a learning style preference, the ability to work independently has been studied in rela-\ntionship to student online learning adaptation and success (Artino, 2009; Hung, Chou, Chen & Own, \n2010; Shinkareva & Benson, 2007; Smith, Murphy & Mahoney, 2003).  The ability to work inde-\npendently (WORK), therefore, is defined as “self-management of studies by working on one’s own \ninitiative where a student takes more responsibility for his or her own learning” (Manochehri & Sha-\nrif, 2010, p. 36). The authors elaborate that technology has the opportunity to enhance WORK with \nits speed and ubiquitous accessibility, allowing for individualized instruction (Manochehri & Sharif, \n2010).  \nMany university DE departments provide students with the opportunity to self-assess their readiness \nfor online learning via survey instruments.  Student ability to work independently and other factors \nsuch as academic and life skills and course expectations are incorporated into such assessments. \nHaigh (2007) reviewed several universities’ DE student self-assessments to create questions to help \ncapture the notion of student self-sufficiency or ability to work independently and measured this in a \nstudy profiling online and face-to-face information science students. Thereby, the inclination towards \nour final hypothesis was formulated and worded as follows: \nH\n5:  The Ability to Work Independently will have  a statistically significant correlation to \nthe course grade a student will achieve. \nMethods \nBernard et al., (2009) conducted a meta-analysis of the DE literature and concluded that “studies in \nwhich instructional and other treatments are administered on a roughly equal footing” (p. 1245) pro-\nvide a better method of examining the effect of learning tools. Therefore, data for this study were col-\nlected from students pursuing a Bachelors of Business Administration, who were given the choice of \nenrolling in either the online section or campus-based classroom section of the same course during the \nFall Semester of 2009. The same instructor offered the pair of class sections simultaneously with al-\nmost identical materials and requirements. The paired classes included Information Systems in Or-\nganizations (IS), Human Resource Management (HRM), and International Management (INTL).  \nUniversity policy encourages the use of Blackboard as an instructional platform for all students, in-\ncluding campus-based students. As such, all students who participated in the study used Blackboard \nregularly during the semester. Surveys were administered through Blackboard to all students in the \nIS, HRM, and INTL sections.  Students were invited to participate with an offer to received extra \npoints for their participation, as done by Mykytyn et al. (2008). The questionnaire was made available \nto the students, who could answer it at their convenience, during the last two weeks of the semester.  \n216 \n Galy, Downey, & Johnson \nRepresenting a response rate of 71%, there were 141 completed surveys. There were 198 students \nenrolled when the invitations to participate were sent. The composition of responses was divided into \n50% on-line students, and 50% from campus-based students.  \nThe course syllabus, schedule, PowerPoint presentations, videos, and other reference materials are \navailable in Blackboard 24 hours a day for the 7 days of the week to all students. The e-learning tools \nused in these classes were tools associated with Blackboard, which include discussion board, videos, \nexaminations, quizzes, and Tegrity, as well as other web-based tools such as Calibrated Peer Review \n(CPR) and on-line ethics simulations.  Tegrity is a flexible software tool that can capture audio, video, \nand computer screen activity into one single “session\" which is automatically uploaded and then can \nbe linked to a Blackboard course. CPR is a web-based program that allows instructors to incorporate \nfrequent writing assignments into their courses, regardless of class size. The CPR system manages the \npeer-review process, including electronic paper submission, student training in reviewing, student \ninput analysis, and final performance report preparation.  In advance, the instructor creates the as-\nsignments and rubrics for reviewing which can be stored for current and future course use. \nInstrument Development \nThe questionnaire in the survey used scales found in both the IS and Education literature forming a \nset of 54 questions.  Using TAM, PU, and PEU are the first two constructs measured in this study. PU \nrefers to a person’s perception of how the adoption of a new technology will increase the efficiency of \nthe person’s performance in accomplishing a certain task. This is captured by the question, “I feel that \nonline learning is at least of equal quality to traditional classroom learning” (Haigh, 2007, p. 99). \nMeasured by a scale used by Saadé and Kira (2009), PEU is a person’s perception of how easy the \ntechnology will be to use (Venkatesh, 2000). The less perceived effort, the easier the technology will \nbe to adopt (F. Davis et al., 1989). Questions regarding the PEU of the course technologies and auto-\nmated learning tools were adapted from Saadé and Kira (2009). A third construct used in this study, \nand also used in the Haigh study, concerns the ability of the student to work independently (WORK).  \nThe fourth and fifth constructs, CSE and CA, use scales from Saadé and Kira (2009). \nInstrument Validation \nThe scaled constructs used in this study are presented in Table 1 along with their corresponding alpha \ncoefficients. \nVariable Cronbach Alpha Mean Standard Deviation\nComputer Self Efficacy (CSE) .905 14.25 5.827\nPerceived Ease of Use (PEU) .874 15.97 3.458\nAbility to Work Independently (WORK) .775 17.67 2.646\nComputer Anxiety (CA) .842 5.51 3.176\nTable 1: Reliability Assessment\n  \nTable 2 shows the factor loadings for the four latent variables used in the study, namely, CSE, PEU, \nWORK, and CA. The factor loadings were calculated using the principal component extraction meth-\nod. To decrease factor cross-loadings and increase discriminant validity, the factors were rotated us-\ning the varimax rotation method with Kaiser Normalization. All loadings below .30 were suppressed. \nConvergent validity was also achieved as evident by the high loadings on the questions corresponding \nconstructs. The total variance explained with the four factors was 71.203%. The Kaiser Sampling Ad-\nequacy Score was .769, and the factor scores were saved as variables using the Bartlett method. \n 217 \nE-Learning Tools and Student Performance \nItem CSE PEU WORK CA\nBefore the course started, I thought learning to navigate the on-line \ncourse components would be easy for me. a\n0.792\nBefore the course started, I thought I would find it easy to get the online \ncourse components to do what I wanted.\na \n0.857\nBefore the course started, I thought it would be easy for me to become \nskillful at  using the  on-line  course  com pone nt s.\na\n0.881\nBefore the course started, I thoughtI would find the on-line course easy to \nuse.\na\n0.853\nBefore the course started, I thought I could complete the required tasks \nusing learning tools only if I had seen someone else using it before trying \nit myse lf.a\n0.813\nBefore the course started, I thought I could complete the required tasks \nusing le arning tools only if  I could call some one  for he lp whe n I got \nstuck.a\n0.882\nBefore the course started, I thought I could complete the required tasks \nusing learning tools only if someone else had helped me get started. a\n0.863\nBefore the course started, I thought I could complete the required tasks \nusing learning tools if I had a lot of time to complete the task for which \nthe learning tool was provided. a\n0.763\nBefore the course started, I thought I could complete the required tasks \nusing learning tools only if  someone else showed me how to do it first.\na\n0.851\nIt scares me to thi nk that I could cause the computer to destroy a l arge \namount of information by hitting the wrong key.\na\n0.76\nI hesitate to use a computer for fear of making mistakes I cannot correct. a 0.906\nComputers are somewhat intimidating to me. a 0.881\nI am able to easily access the Internet as needed for my studies. a 0.739\nI am able to work independently. a 0.766\nI am self motivated and self disciplined. a 0.766\nI can usually figure out what to do in an assignment by reading and \nfollowing the  instructions.\na\n0.777\na  Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree\n  All scores below .30 were supressed.\nTable 2: Factor Analysis Rotated with Varimax with Kaiser Normalization\n \nFindings \nDifferences between Online and Campus-based Classes \nInitially, student performance between online and campus-based students was analyzed for statisti-\ncally significant differences with the use of independent sample T-tests. The literature points to main-\ntaining a longitudinal assessment of performance outcomes throughout the semester; therefore, vari-\nous performance measures were collected, as shown in Table 3.  \n218 \n Galy, Downey, & Johnson \nTable 3:   Performance Measures \nn Final Exam Midterm Discussion Project \nCourse \nGrade \nCourse \nCampus-\nbased On-line \nCampus-\nbased \nOn-\nline \nCampus-\nbased \nOn-\nline \nCampus-\nbased \nOn-\nline \nCampus-\nbased \nOn-\nline \nCampus-\nbased \nOn-\nline \nIS 38/44 25/37 72 89 78 74 91 79 79 64 81 80 \nHRM 16/25 24/40 71 81 79 80 88 97 79 86 84 84 \nINTL 16/17 22/35 88 90 85 82 94 88 90 84 87 85 \n \nThe course grade is, of course, the most important assessment indicator as it averages all other per-\nformance measures. The Final Exam and Project are end-of-semester measures, while the Midterm \nindicates performance approximately the eighth week of the semester. It is important to note that the \nProject grade is not a group project, but one completed individually. Furthermore, one discussion \ngrade was collected. This assignment was completed using Blackboard Discussion Forums where \nstudents were asked to provide their own posting as well as give feedback to at least one of their \nclassmates. The results, shown in Table 4, were surprising. The course grade averages as noted in Ta-\nble 4 were quite similar, notwithstanding the differences in other measures. Campus-based students \non average performed better on projects and discussion boards, while online students performed bet-\nter on the final. Contrary to findings in the literature review, the Midterm grade averages in this study \nwere very similar. The only statistically significant difference, however, was found in the Final Exam \nscores. \nPerformance Measure t-score Sig 2-tail\nMean \nDifference\nMean \nClassroom\nMean \nOnline\nCourse Grade -0.128 0.898 -0.201 82.70 82.90\nFinal Exam -5.685    0.000** -10.101 75.54 86.69\nMidterm 0.574 0.567 1.196 79.91 78.72\nDiscussion 0.729 0.467 2.743 90.76 88.01\nProject 1.039 0.301 3.740 81.56 77.82\n* *  Significant at .01 level\nTable 4: Differences between online and campus-based student performance\n \n \nSecondly, independent sample T-tests were performed on all questions comparing the mean scores of \nthe online students with the campus-based students. Using the case-by-case analysis for missing val-\nues, the questions that had statistically significant differences are shown in Table 5.  The t-score, sig-\nnificance, mean difference, and mean scores are shown.  Interestingly, the results indicate that online \nstudents tended to be more confident about their typing skills than in-classroom students. Online stu-\ndents were also more comfortable with written communication than their counterparts in the class-\nroom environment.  The campus-based students disagreed that online learning was of equal quality \nwith campus-based classroom learning.  Reaction to the statement of being apprehensive towards \ncomputers was also statistically significantly different. Online students tended to disagree with this \nstatement, while campus-based students tended to be neutral towards the statement.  Before the \ncourse started, online students thought that online courses would be easy to use to a higher degree \nthan campus-based students. \n 219 \nE-Learning Tools and Student Performance \n \nQuestion t-score Sig 2- tail\nMean \nDifference\nMean \nCampu s-\nba se d\nMean \nOnline\nI have good typing skills.\na\n-1.8 82 0. 062 -0. 292 4.1 4 4 .44\nI ne ed as s ista n ce to u nderstand text I read.a 1 .81 3 0. 072 0.4 19 2.7 1 2 .30\nI rarely ask fo r he lp when I need it. a 1 .84 1 0. 068 0.3 83 3.1 9 2 .80\nI pu t things of f un til the last minute. a 1 .66 8 0. 098 0.3 57 3.4 9 3 .13\nI fe el online learning is at least of equa l quality than traditional \nclassroo m learning. a -2.3 77 0. 019 -0. 523 2.8 6 3 .38\nI fe el apprehensive about using computers. a 2 .75 9 0. 007 0.6 59 2.8 1 2 .15\nBefore this course started, I thought I wo uld find on-line co u rses easy \nto us e. a -1.8 08 0. 073 -0. 326 3.8 9 4 .21\nBefore this course started, I thought I could co mp lete the required \ntasks using learning tools only if I had se en someone els e using it \nbefore trying it myself. a 1 .65 5 0. 100 0.3 82 3.1 1 2 .73\nDid you att en d tutorial se ssions? b 2 .24 6 0. 026 0.1 00 0.1 3 0 .03\nRate the course in level of dif ficu lty. C -2.0 99 0. 038 -0. 197 3.0 0 3 .20\na  Strongly  Dis agree, Slightly  Di sagree, Neu tral, Sl igh tly Agr ee, Strongly  Agree\n b No, Yes\nc  Ver y Easy, Easy, Mo derate, Diff icul t, Ver y Diff icul t\nTable 5: Differences between online and cam pus-b asedstudent percep tions\n \nParticipants Demographics \nThe respondent demographics are shown in Table 6 and coincide with the findings presented in the \nliterature. Parsad and Lewis (2008) indicated that there were more female online students than males \nstudents, and though this is true in this study, the difference is very slight: 52% of the 71 online stu-\ndent respondents were female and 48% were male. This statistic is comparable with the campus-based \nstatistic. With regard to age, there are a higher percentage of students older than 24 years old in the \nonline class compared to the traditional undergraduate age group of under 24 years old. Two-thirds of \nthe online students are over the typical age compared to 57% of the campus-based students being of \ntypical age in our university. Recent findings of the Department of Education have found that online \nstudents most typically live within the city-limits of the university (Parsad & Lewis 2008). This was \nalso the case in this study where 60% of the online students lived within the city limits, 37% within a \n60-mile range and only 2.8% live far from the university.  \n220 \n Galy, Downey, & Johnson \nTable 6: Student Demographics \n Online Campus-Based    Total \n Gender   \n       Female 52.1% 52.9%                     52.5% \n       Male 47.9% 47.1%                     47.5% \nAge   \n       Typical (under 24) 39.4% 57.1%                      48.2% \n       24-30 years old 42.3% 31.4%                      36.9%   \n       Older than 30 yrs old 18.3% 11.4%                      14.9% \nCity of Residence   \n        In-City 60.6% 67.1%                   63.8% \n        Near – within 60 36.6% 28.6%                   31.9% \n        Beyond 60 miles  2.8%      4.3%                     3.5% \nCorrelations \nBefore initiating the regression analysis, a correlation test was run among the variables. The results, \nshown in Table 7, give an initial indication of the strength of the variables relationships. There was no \ncorrelation between the predictor variables CSE, PEU, WORK, or CA and, as is demonstrated by fac-\ntor analysis, these are independent constructs. PU was slightly correlated with the rest of the vari-\nables, but none were found statistically significant, except for the Online Total variable, which indi-\ncates the number of online classes a student has taken. Noticeably, there are statistically significant \ncorrelations between course grade, PEU, and WORK. The age variable was found to be negatively \ncorrelated with course grade at a statically significant level of .05   \nV a r i a b l e 12345678\n1. CSE 1\n2. PEU .000 1\n3. PU -.072 .111 1\n4. WORK .000 .000 .031 1\n5. CA .000 .000 .062 .000 1\n6. Course Grade -.063      .309**     -.134      .286** .036 1\n7. Age -.055     -.096 .013     -.191* .095 -.241** 1\n8. Online Total -.135 .024      .184* -.044     -.078 -.008 .176* 1\n   *Correlation is significant at .05 level\n** Correlation is significant at .01 level\nTable 7: Correlations\n \nRegression \nRegression analysis was used to test the hypotheses, with PU, PUE, CSE, CA, and WORK as the in-\ndependent variables and Course Grade as the dependent variable. As shown in Table 8, the regression \nmodel revealed that there was evidence to support the hypotheses that PEU (H1), PU (H2) and the \nWORK (H5) were statistically significant contributors to the final course grade. However, we did not \nfind evidence to support the hypotheses that CSE (H3) or CA (H4) played a statistically significant \nrole in predicting the final course grade. \n 221 \nE-Learning Tools and Student Performance \nVariable B\nStandard \nError β t Stat p Value R 2 Adj R2 R2 change F\nPEU 3.115 0.713 0.334 4.371 0.000 0.101 0.095 0.101 15.354\nWORK 2.458 0.709 0.266 3.465 0.001 0.183 0.171 0.082 13.535\nPU -1.451 0.542 -0.203 -2.675 0.008 0.226 0.208 0.042 7.315\nAge -0.209 0.106 -0.152 -1.979 0.050 0.248 0.225 0.022 3.915\nCSE -0.072 -0.950 0.344\nCA 0.056 0.734 0.464\nRegression Model \nTable 8: Regression of Course Grade\n \nDiscussion and Practical Implications \nThe purpose of this study was to investigate the roles that PU, PEU, CSE, CA, and WORK, in the \ncontext of a LMS, play in predicting student performance in online and campus-based classes in three \nbusiness disciplines.  Regression analysis demonstrated that PU, PEU, and WORK were shown to be \nindeed related to final course grades, thus supporting H1, H2, and H5, suggesting that e-learning \ntools, as well as WORK, do play an important role in performance in both online and campus-based \ncourses. The statistically significant relationship between the PU and PEU variables from TAM in \ncourses delivered online and on-campus related to e-learning tools and WORK indicates that as these \nincrease, final grades increase.  The findings demonstrate the contribution value of PU and PEU for \ncampus-based classes using e-learning tools.  This has implications for campus-based courses in par-\nticular, suggesting that e-learning tools be considered in their design. This implies that this skill, often \nthought to be more important for students taking online courses, is equally important to campus-based \ncourses employing e-learning tools and that universities and colleges may want to provide self as-\nsessments of this skill to all students, not just students enrolling in online courses. \nOnline education has created an influx of students for universities and colleges that is both encourag-\ning and challenging as this method of instruction is subject to questions regarding the effectiveness of \nthe technological systems and practices of the institution and the instructors (Gibson et al., 2008).  \nConsidering these questions, the findings of this study, in addition to its primary finding related to the \nhypotheses tested, revealed that there are a statistically significant number of students who do not be-\nlieve that online courses are equal in value compared to those delivered on-campus.  This is evident \nby the negative sign seen in the PU variable in the Course Grade equation found in Table 8. This find-\ning coincides with the literature suggesting that student perceptions of the usefulness and effective-\nness of the online instructional format has a statistically significant effect on the emotional processes \nand learning the student experiences (Dobbs et al., 2009; Moneta & Kekkonen-Moneta, 2007).  Given \nthe perception that online courses do not have the same value as campus-based courses, it is incum-\nbent upon universities to reinforce to students that, although different, properly designed online \ncourses provide the same content and are subject to the same assurance of learning standards as cam-\npus-based courses and, at the same time, provide students with a better understanding of their learning \npreferences via self-assessments related to online learning. “In terms of teaching and learning, having \na usable course management system [or learning management system] means potentially reducing \nteacher time invested in setting up and managing the course and improving the student’s learning ex-\nperience” (Unal & Unal, 2011, p. 21). However, it is also true that “course designs that haphazardly \nintegrated a variety of features” (p. 21) hindered student learning because the design did not reinforce \ncourse objectives or satisfy student expectations (Unal & Unal, 2011). \n222 \n Galy, Downey, & Johnson \nAlthough CSE did not contribute with statistical significance to the regression equation when predict-\ning the final course grade, this study revealed statistically significant differences in CSE between \nonline and campus-based students. Online students perceived they had not only better typing skills \nbut also better written communication skills.  Online students reacted negatively to the statement “I \nfeel apprehensive about using computers,” in contrast to the neutral reaction of campus-based stu-\ndents. Ng, Yeung, and Hon (2006) state that it seems logical that students who are less skilled will \nhave more anxiety that will lessen their enjoyment of a DE course. Grant, Malloy, and Murphy (2009) \ncall attention to the fact the comprehension, level of skill, and self-efficacy a student possesses of a \nparticular computer software or e-learning tool is continuously changing, emphasizing the need for \ncontinuous measurement of these factors and subsequent adaptation or realignment of course content \nand methodologies. Similarly, Wan, Wang, and Haggerty (2008) conclude, “pretests could be de-\nsigned to help learners assess their own levels of virtual competence, and thus make the proper choice \n[online vs. campus-based courses]” (p. 519). \nAn important development in university education is that more often than not, the online student is \nalso a student attending campus-based courses. This rather recent development has led universities to \nconsider developing hybrid courses (Kock et al., 2007; Ng et al., 2006) to maximize university re-\nsources and provide flexible schedules for student convenience. Only a small percentage of students \nwould not be able to be on campus to attend a hybrid schedule. Hybrid courses, because they employ \ne-learning tools, have a higher potential to improve student PU and PEU and to exert less pressure on \na student to work independently. At the same time, hybrid courses may be more acceptable to stu-\ndents and instructors who believe that online instruction is not as valuable as campus-based instruc-\ntion or those that need encouragement to adapt or increase usage of technology. \nIn this study, as part of the data collection process, in addition to measuring student perceptions of e-\nlearning tools, students were given an opportunity to provide commentary and suggestions on how to \nimprove their courses. A content analysis of the comments section revealed that students expect the \ninstructor to provide a strict schedule for online courses and said they prefer weekly assignments and \nweekly quizzes rather than having an extended period of time, such as one or two months, to submit \nassignments. Students reported that they enjoy quick feedback and mentioned that they can tell when \nan instructor does not want to be bothered by online students. Also related to feedback, some online \nstudents suggested that a monthly meeting with the instructor be scheduled for each student. Accord-\ning to Kock et al. (2007) and Shroff &Vogel (2009), online learners may require more positive feed-\nback when they are doing well on assignments throughout the course of the online course, which may \nlead to higher self-efficacy and translate into high learning outcomes. “Feedback, building on assess-\nment, allows students to gauge their progress, consider alternate learning strategies, and project their \nown continued learning needs” (Bonnel, 2008, p. 290). Though generally it is understood that fre-\nquent feedback from the instructor encourages students, they also benefit from peer feedback and in-\nteraction with the course material (Bonnel, 2008; Kellog & Smith, 2009).  Kellogg and Smith (2009) \nfound that adult online students learned the most from interaction with course material and to a lesser \nextent from interactions with the instructor; yet our study found that adult online students wanted in-\nteraction with the instructor. \nNg et al. (2006) insist that peer interaction must be encouraged and suggest that online instructors \ncreate pairs of online buddies. A frequently used e-learning tool that facilitates peer interaction and \nstimulates student-to student exchange of ideas and information is the discussion board. In this study, \nstudents reported they enjoyed the use of video clips tied to discussion boards, which required them to \ninteract with other students. Kellog and Smith (2009) found, however, that adult learners were very \ncritical of group work in their online classes and seemed to indicate that peer interaction was the ac-\ntivity that provided the least learning opportunities.  Although students may enjoy discussion boards \nfor exchange of ideas, there is little data to support that students deepen their analytical skills through \n 223 \nE-Learning Tools and Student Performance \nthis process (Miers, Clarke, Pollard, Rickaby, Thomas & Turtle, 2007). Nevertheless, this does not \nmean that students’ cognitive skills are not supported by the discussion board exchange. \nAmong the e-learning tools mentioned specifically by the students in this study was the use of CPR, \nwhich they enjoyed. A few students asked for Tegrity, which is software that captures audio, video, \nand computer screen activity in one session, as well as the use of added online tutorials. There was \nagreement among online students in favor of increased time limits to complete tests, online quizzes \nand/or exams because they say there was simply not enough allotted time. Many online instructors \nrestrict the time a student has to finish these exams.  Authors have pointed toward better content de-\nvelopment and use of LMS tools that result in a better effort to fully engage the student. The in-\ncreased student engagement can be accomplished through “regular site updates, more online assess-\nment opportunities, and module consistency in terms of site content” and will most probably transfer \nto better student motivation (Love & Fry, 2006, p. 163). Love and Fry call for a university-wide pol-\nicy for consistent DE course templates. In this manner, a student can learn to navigate the LMS with \ngreat ease. \nNotwithstanding the differences in student perception, this study did not find any statistically signifi-\ncant difference between the final grades of the two groups, thereby indicating that students were able \nto attain the course learning objectives at an equivalent level in both online and campus-based sec-\ntions using similar e-learning tools.  This is consistent with the findings in the literature (Benson et \nal., 2005; Kock et al., 2007). Kock et al. suggest, “that the perceived differences in ambiguity and \ngrades throughout a course may become insignificant toward the end of the semester” (p. 350). Inter-\nestingly, the belief that online instruction is inferior to campus-based instruction was not supported by \nthe findings of this study. This could encourage students to change their perceptions that online in-\nstruction is inferior to campus-based instruction. This study was not concerned with finding a differ-\nence in performance between online and campus-based courses but aimed to explore the effects of e-\nlearning tools on performance.  It established that perceptions of the PU and PEU of e-learning tools \nin both types of classes predicted performance along with the students’ ability to work independently.  \nIt is important to reiterate that this study controlled for course instructor, course material, and semes-\nter by comparing different sections of the same class, answering the concern raised by Lou et al. \n(2006) that materials equivalence, instructor equivalence, and class size may moderate the results.  \nLimitations and Future Research \nThis study was limited to one campus where the student population is primarily Hispanic; therefore, \nresults may not be generalizable to a more culturally diverse population. Additional research that in-\ncludes other minority groups and/or more diverse groups of students may provide more insight into \nvarious e-learning tools and their relationship to performance. However, despite the sample being \ncomprised mainly of one cultural background, the sample included students with various majors in-\ncluding Accounting, Computer Science, Entrepreneurship, International Business, Management, and \nMarketing. There is evidence in the literature that suggests that older students are more self-directed \nand therefore more apt to succeed in DE courses (Dobbs et al., 2009); however, we found that age \nnegatively correlates with course grade.  \nParticular care was given to collect data from courses where students could select courses in which \nthe same instructor taught both online and on-campus, therefore, controlling for the instructor. In ad-\ndition, we limited the sample to instructors who kept both of these sections as similar as possible, us-\ning the same text and almost identical course content. We thought these elements were important to \ncontrol, although we realized this would limit the sample size and the small sample size could be con-\nsidered a limitation of this research study. \nAlthough grades were the performance measure used in this study, other outcome measures such as \ncourse satisfaction, as done by Artino (2009), could be studied separately or integrated with grades \n224 \n Galy, Downey, & Johnson \ninto future research, thus, providing a well-rounded, richer study.  Another area for future research, as \nNg et al. (2006) suggest, is implementing testing before and after a student participates in a course \nwith e-learning tools to measure the differential gains of student perception of e-learning tools. A fi-\nnal suggestion for future research is to investigate the effectiveness of hybrid courses to determine the \nbest combination of campus-based versus DE interactions. Several authors suggest that the campus-\nbased interactions should be more numerous at the beginning of the semester than towards the end of \nthe semester (Love & Fry, 2006; Ng et al., 2006).  \nResearchers recognize that empirical evidence is imperative in guiding our quest to find the best an-\nswers to questions related to DE, e-learning tools, and their relationship to student performance. We \nshould not, however, lose sight of the fact that we are researching questions that potentially have pro-\nfound effects on students’ lives and ultimately society.  Research related to student learning is pro-\nfound.  What needs to remain central to our quest for knowledge related to e-learning tools are our \nstudents and how learning environments provided by such tools improve student performance. Our \nstudy findings have contributed to the growing knowledge base related to DE and also support our \nteaching activities, providing potentially richer, more meaningful learning experiences that will im-\nprove learning outcomes for our students.   \nReferences \nAdams, D., Nelson, R., & Todd, P. (1992). Perceived usefulness, ease of use, and usage of information technol-\nogy: A replication. MIS Quarterly, 16(2), 227-247. \nAgarwal, R., & Karahanna, E. (2000). Time flies when you're having fun: Cognitive absorption and beliefs \nabout information technology usage. MIS Quarterly, 24(4), 665-694. \nAjzen, I. (1991).  The theory of planned behavior.  Organizational Behavior and Human Decision Processes, \n50, 179-211.   \nAjzen, I., & Fishbein, M. (1977).  Attitude-behavior relations:  A theoretical analysis and review of empirical \nresearch.  Psychological Bulletin, 84(5), 888-918. \nAjzen, I., & Fishbein, M. (1980).  Understanding attitudes and predicting social behavior.  Englewood Cliffs, \nNJ:  Prentice-Hall. \nAllen, I. E., & Seaman, J. (2010a). Class differences: Online education in the United States, 2010. Babson Sur-\nvey Research Group, Retrieved January 26, 2011, from \nhttp://sloanconsortium.org/sites/default/files/class_differences.pdf   \nAllen, I. E., & Seaman, J. (2010b). Learning on demand: Online education in the United States, 2009. Babson \nSurvey Research Group. Retrieved January 26, 2011, from \nhttp://sloanconsortium.org/publications/survey/pdf/learningondemand.pdf  \nArbaugh, J. B. (2005). Is there an optimal design for on-line MBA courses? Academy of Management Learning \n& Education, 4, 135-149. \nArbaugh, J. B., Godfrey, M. R., Johnson, M., Pollack, B. L., Niendorf, B., & Wresch, W. (2009). Research in \nonline and blended learning in the business disciplines: Key findings and possible future directions. Inter-\nnet and Higher Education, 12, 71-87. \nArtino, A. R. (2009). Online learning: Are subjective perceptions of instructional context related to academic \nsuccess? Internet and Higher Education, 12, 117-125. \nBandura, A. (1977).  Social learning theory.  Englewood Cliffs, NJ: Prentice Hall.  \nBandura A., & Cervone, D. (1986). Differential engagement of self-reactive mechanisms governing the motiva-\ntional effects of goal systems. Organizational Behavior and Human Decision Processes, 38(1), 92-113. \nBekele, T., & Menchaca, M. (2008). Research on internet-supported learning: A review. Quarterly Review of \nDistance Education, 9(4), 373-405. \n 225 \nE-Learning Tools and Student Performance \nBenson, A., Johnson, S., Taylor, G., Treat, T., Shinkareva, O., & Duncan, J. (2005). Achievement in online and \ncampus-based career and technical education courses. Community College Journal of Research & Practice, \n29(5), 369-394.  \nBernard, R. M. Abrami, P. C., Borokhovski, C., Wade, A. Tamim, R. M., Surkes, M. A., & Bethel, E. C. \n(2009). A meta-analysis of three types of interaction treatments in distance education. Review of Educa-\ntional Research, 79, 1243-1289. \nBonnel, W. (2008). Improving feedback to students in online courses. Nursing Education Perspectives, 29(5), \n290-294. \nBrecht, H., & Ogilby, S. M. (2008). Enabling a comprehensive teaching strategy: Video lectures.  Journal of \nInformation Technology Education, 7, IIP71-IIP86. Retrieved from \nhttp://www.jite.org/documents/Vol7/JITEV7IIP071-086Brecht371.pdf \nBuche, M., Davis, L., & Vician, C. (2007). A longitudinal investigation of the effects of computer anxiety on \nperformance in a computing-intensive environment. Journal of Information Systems Education, 18(4), 415-\n423. \nCalvin, J., & Freeburg, B. (2010). Exploring adult learners’ perceptions of technology competence and retention \nin web-based courses. Quarterly Review of Distance Education, 11(2), 63-72. \nCandy, P. C. (1991). Self-direction for lifeline learning: A comprehensive guide to theory and practice.  San \nFrancisco: Jossey-Bass. \nCaspi, A., & Gorsky, P. (2006). Instructional dialogue: Distance education students’ dialogic behaviour. Studies \nin Higher Education, 31(6), 735-752.  \nChyung, S. Y. (2007). Age and gender differences in online behavior, self-efficacy, and academic performance. \nQuarterly Review of Distance Education, 8(3), 213-222. \nCompeau, D., & Higgins, C. (1995). Computer self-efficacy: Development of a measure and initial test. MIS \nQuarterly, 19(2), 189-211. \nCompeau, D., Higgins, C., & Huff, S. (1999). Social cognitive theory and individual reactions to computing \ntechnology: A longitudinal study. MIS Quarterly, 23(2), 145-158. \nDavis, F. (1986). A technology acceptance model for empirically testing new end-user information systems: \nTheory and results. Doctoral dissertation, Sloan School of Management, Massachusetts Institute of Tech-\nnology. \nDavis, F. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. \nMIS Quarterly, 13(3), 319-340. \nDavis, F., Bagozzi, R., & Warshaw, P. (1989). User acceptance of computer technology: A comparison of two \ntheoretical models. Management Science, 35(8), 982-1003. \nDavis, R., & Wong, D. (2007). Conceptualizing and measuring the optimal experience of the elearning envi-\nronment. Decision Sciences Journal of Innovative Education, 5(1), 97-126. \nDobbs, R., Waid, C., & del Carmen, A. (2009). Students’ perceptions of online courses: The effect of online \ncourse experience. Quarterly Review of Distance Education, 10(1), 9-26. \nDoyle, W. R. (2009). Playing the numbers, online education: the revolution that wasn’t. The Magazine of High-\ner Learning, 41(3), 56-58. \nFishbein, M. & Ajzen, I. (1975). Belief, attitude, intention and behavior: An introduction to theory and re-\nsearch. Addison-Wesley, Reading, MA. \nGibson, S., Harris, M., & Colaric, S. (2008). Technology acceptance in an academic context: Faculty accep-\ntance of online education. Journal of Education for Business, 83(6), 355-359.  \nGong, M., Xu, Y., & Yu, Y. (2004). An enhanced technology acceptance model for web-based learn-\ning. Journal of Information Systems Education, 15(4), 365-374. \n226 \n Galy, Downey, & Johnson \nGrant, D. M., Malloy, A. D., & Murphy, M. C. (2009). A comparison of student perceptions of their computer \nskills and their actual abilities. Journal of Information Technology Education, 8, 141-160. Retrieved from \nhttp://www.jite.org/documents/Vol8/JITEv8p141-160Grant428.pdf \nHackbarth, G., Grover, V., & Yi, M. Y. (2003). Computer playfulness and anxiety: Positive and negative me-\ndiators of the system experience effect on perceived ease of use. Information & Management, 40(3), 221-\n232. \nHaigh, M. (2007). Divided by a common degree program? Profiling online and face-to-face information science \nstudents. Education for Information, 25(2), 93-110.  \nHartwick, J., & Barki, H. (1994). Explaining the role of user participation in information system use. Manage-\nment Science, 40(4), 440-465. \nHiltz, S. R., & Wellman, B. (1997). Asynchronous leaning networks as a virtual classroom. Communications of \nACM, 40(9), 44-49. \nHoward, G. S., & Smith, R. D. (1986). Computer anxiety in management: Myth or reality? Communications of \nthe ACM, 29(7), 611-615. \nHung, M., Chou, C., Chen, C., & Own, Z. (2010).  Learner readiness for online learning: Scale development \nand student perceptions.  Computers and Education, 55(3), 1080-1090. \nIgbaria, M., Guimaraes, T., & Davis, G. (1995). Testing the determinants of microcomputer usage via a struc-\ntural equation model. Journal of Management Information Systems, 1(4), 87-114. \nIgbaria, M., & Parasuraman, S. (1989). A path analytic study of individual characteristics, computer anxiety and \nattitudes toward microcomputers. Journal of Management, 15(3), 373-388. \nIrani, T., Telg, R., Scherler, C., & Harrington, M. (2003). Personality type and its relationship to distance edu-\ncation students’ course perceptions and performance. Quarterly Review of Distance Education, 4(4), 445-\n453. \nKalsbeek, D. (1989). Linking learning style theory with retention research: The TRAILS project. The Associa-\ntion for Institutional Research: AIR Professional File, 32, 2-9. \nKellog, D. L., & Smith, M. A. (2009). Student-to-student interaction revisited: A case study of working adult \nbusiness students in on-line courses. Decision Sciences Journal of Innovative Education, 7(2), 433-456. \nKılıç-Çakmak, E., Karataş, S., & Ocak, M. (2009). An analysis of factors affecting community college students' \nexpectations on e-learning. Quarterly Review of Distance Education, 10(4), 351-361. \nKnowles, M. S. (1975).  Self-directed learning:  A guide for learners and teachers. Englewood Cliffs, NJ:  \nPrentice Hall.  \nKnowles, M. S. (1980). The modern practice of adult education.  Chicago:  Association Press. \nKock, N., Verville, J., & Garza, V. (2007). Media naturalness and online learning: Findings supporting both the \nsignificant and no-significant-difference perspectives. Decisions Science Journal of Innovative Education, \n5(2), 333-355. \nLandry, B., Griffeth, R., & Hartman, S. (2006). Measuring student perceptions of Blackboard using the technol-\nogy acceptance model. Decision Sciences Journal of Innovative Education, 4(1), 87-99. \nLoomis, K. D. (2000). Learning styles and asynchronous learning: Comparing the LASSI model to class per-\nformance. Journal of Asynchronous Learning Networks, 4(1), 23-32. \nLou, Y., Bernard, R., & Abrami, P. (2006). Media and pedagogy in undergraduate distance education: A theory-\nbased meta-analysis of empirical literature. Educational Technology Research & Development, 54(2), 141-\n176.  \nLove, N. & Fry, N. (2006). Accounting students’ perceptions of a virtual learning environment: Springboard or \nsafety net? Accounting Education: An International Journal. 15(2), 151-166. \n 227 \nE-Learning Tools and Student Performance \nManochehri, N., & Sharif, K. (2010). A model-based investigation of learner attitude towards recently intro-\nduced classroom technology. Journal of Information Technology Education, 9, 31-52. Retrieved from \nhttp://www.jite.org/documents/Vol9/JITEv9p031-052Manochehri744.pdf  \nManochehri, N., & Young, J. I. (2006). The impact of student learning styles with web-based learning or in-\nstructor-based learning on student knowledge and satisfaction. Quarterly Review of Distance Education, \n7(3), 313-316. \nMarakas, G. M., Yi, M. Y., & Johnson, R.D. (1998). The multilevel and multifaceted character of computer \nself-efficacy: Toward clarification of the construct and an integrative framework for research. Information \nSystems Research, 9(2), 126-163. \nMartins, L. L., & Kellermanns, F. W. (2004). A model of business school students’ acceptance of a web-based \ncourse management system. Academy of Management Learning and Education, 3(1), 7-26. \nMcLaren, C. (2004). A comparison of student persistence and performance in online and classroom business \nstatistics experiences. Decision Sciences Journal of Innovative Education, 2(1), 1-10. \nMiers, M. E., Clarke, B. A., Pollard, K. C., Rickaby, C. E., Thomas, J., & Turtle, A. (2007). Online interprofes-\nsional learning: The student experience. Journal of Interprofessional Care, 21(5), 529-542. \nMoneta, G., & Kekkonen-Moneta, S. (2007). Affective learning in online multimedia and lecture versions of an \nintroductory computing course. Educational Psychology, 27(1), 51-74.  \nMurthy, U., & Kerr, D. (2004). Comparing audit team effectiveness via alternative modes of computer-\nmediated communication. Auditing, 23(1), 141-152.  \nMykytyn, K., Pearson, A., Paul, S., & Mykytyn, P. (2008). The use of problem-based learning to enhance MIS \neducation. Decision Sciences Journal of Innovative Education, 6(1), 89- 113. \nNam, C. S., & Smith-Jackson, T. L. (2007). Web-based learning environment: A theory-based design process \nfor development and evaluation. Journal of Information Technology Education, 6, 23-43. Retrieved from \nhttp://www.jite.org/documents/Vol6/JITEv6p023-043Nam145.pdf  \nNewlin, M., Lavooy, M., & Wang, A. (2005). An experimental comparison of conventional and web-based in-\nstructional formats. North American Journal of Psychology, 7(2), 327-335. \nNg, C., Yeung, A.S., & Hon, R.Y.H. (2006). Does online language learning diminish interaction between stu-\ndent and teacher? The Hong Kong Institute of Education, 43(3), 219-232. \nParsad, B., & Lewis, L. (2008). Distance education at degree-granting postsecondary institutions: 2006–07 \n(NCES 2009–044). National Center for Education Statistics, Institute of Education Sciences, U.S. Depart-\nment of Education. Washington, DC.  \nPuzziferro, M. (2008). Online technologies self-efficacy and self-regulated learning as predictors of final grade \nand satisfaction in college-level online courses. American Journal of Distance Education, 22(2), 72-89. \nRabe-Hemp, C., Woollen, S., & Humiston, G. (2009). A comparative analysis of student engagement, learning, \nand satisfaction in lecture hall and online learning settings. Quarterly Review of Distance Education, 10(2), \n207-218. \nSaadé, R. G., (2007). Dimensions of perceived usefulness: Toward enhanced assessment. Decision Sciences \nJournal of Innovative Education, 5(2), 289-310. \nSaadé, R. G., & Bahli, B. (2005). The impact of cognitive absorption on perceived usefulness and perceived \nease of use in online learning: An extension of the technology acceptance model. Information & Manage-\nment, 42(2), 317-327. \nSaadé, R. G., & Kira, D. (2009). Computer anxiety in e-learning: The effect of computer, self-efficacy. Journal \nof Information Technology Education, 8, 177-191. Retrieved from \nhttp://www.jite.org/documents/Vol8/JITEv8p177-191Saade724.pdf  \nSeok, S., DaCosta, B., Kinsell, C., & Tung, C. K. (2010). Comparison of instructors’ and students’ perceptions \nof the effectiveness of online courses. Quarterly Review of Distance Education, 11(1), 25-36. \n228 \n Galy, Downey, & Johnson \nShinkareva, O. N., & Benson, A. D. (2007). The relationship between adult students’ instructional technology \ncompetency and self-directed learning ability in an online course.  Human Resource Development Interna-\ntional, 10(4), 417-435.  \nShroff, R. H. & Vogel, D. R. (2009). Assessing the factors deemed to support individual student intrinsic moti-\nvation in technology supported online and face-to-face discussions. Journal of Information Technology Ed-\nucation, 8, 59-85. Retrieved from http://www.jite.org/documents/Vol8/JITEv8p059-085Shroff416.pdf  \nSimmering, M., Posey, C., & Piccoli, G. (2009). Computer self-efficacy and motivation to learn in a self-\ndirected online course. Decision Sciences Journal of Innovative Education, 7(1), 99-121. \nSingh, A., Mangalaraj, G., & Taneja, A. (2010). Bolstering teaching through online tools. Journal of Informa-\ntion Systems Education, 21(3), 299-311. \nSmart, K. L., & Cappel, J. J. (2006). Students' perceptions of online learning: A comparative study. Journal of \nInformation Technology Education, 5, 201-219. Retrieved from \nhttp://www.jite.org/documents/Vol5/v5p201-219Smart54.pdf  \nSmith, P. J., Murphy, K. L., & Moheney, S. E. (2003).  Towards identifying factors underlying readiness for \nonline learning:  An experimental study.  Distance Education, 24(1), 57-67. \nSyler, R., Cegielski, C., Oswald, S., & Rainer, R. (2006). Examining drivers of course performance: An ex-\nploratory examination of an introductory CIS applications course. Decision Sciences Journal of Innovative \nEducation, 4(1), 51-65. \nUnal, Z., & Unal, A. (2011). Evaluating and comparing the usability of web-based course management systems. \nJournal of Information Technology Education, 10, 19-38. Retrieved from \nhttp://www.jite.org/documents/Vol10/JITEv10p019-038Unal904.pdf  \nVenkatesh, V. (1999). Creation of favorable user perceptions: Exploring the role of intrinsic motivation. MIS \nQuarterly, 23(2), 239-260. \nVenkatesh, V. (2000) Determinants of perceived ease of use: Integrating control, intrinsic motivation, and emo-\ntion into the technology acceptance model. Information Systems Research, 11(4), 342-365. \nVenkatesh, V., & Davis, F. (2000). A theoretical extension of the technology acceptance model: Four longitudi-\nnal field studies. Management Science, 46(2), 186-204. \nWan, Z., Wang, Y., & Haggerty N. (2008). Why people benefit from e-learning differently: The effects of psy-\nchological processes on e-learning outcomes. Information & Management, 45, 513-521. \nWelsh, E., Wanberg, C., Brown, K., & Simmering, M. (2003). E-learning: Emerging uses, empirical results and \nfuture directions. International Journal of Training and Development, 7(4), 245–258.  \n 229 \nE-Learning Tools and Student Performance \n230 \nBiographies \nEdith Galy is an Associate Professor of Management Information Systems \nat the University of Texas at Brownsville. She serves as Chair of the As-\nsurance of Learning Committee for the College of Business in charge of \nstudent assessment for the college. Her current research interests are in stu-\ndent assessment, organizational learning, absorptive capacity of informa-\ntion technologies, cross-cultural influences of IT adoption, and change \nmanagement. She holds a Ph.D. in International Business with a support \nfield in Computer Information Systems from The University of Texas–Pan \nAmerican\n. \n \n \n \n \nClara Downey has over ten years private industry experience in the insur-\nance industry, and has worked in the Midwest, Silicon Valley, and Asia. \nShe completed a Ph.D. in Business Administration and holds a M.B.A. in \nInternational Management, from Thunderbird Graduate School in Arizona. \nHer undergraduate degree in East Asian Studies is from Indiana University, \nBloomington.  Research areas include the following: international strategic \nalliances, firm bankruptcy, and consumer behavior.  \n \n \n \n \nJennie Johnson joined the University of Texas at Brownsville in 2007 \nwhere she teaches organizational behavior and human resource manage-\nment. She holds a Ph.D. from Virginia Polytechnic Institute and State Uni-\nversity in human resource development and an M.S. in organizational be-\nhavior from Eastern Michigan University. Her research interests include \nstudent performance, organizational justice, ethical decision-making, and \nconflict management. Before an academic career, Dr. Johnson worked in \nbusiness and industry in human resource management, organizational de-\nvelopment, and workplace learning and performance for over 20 years. She \nhas worked in 25 countries as well as lived in Haiti, Uruguay, and Costa \nRica. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5347548723220825
    },
    {
      "name": "Mathematics education",
      "score": 0.5243061780929565
    },
    {
      "name": "Electronic learning",
      "score": 0.4835626780986786
    },
    {
      "name": "Multimedia",
      "score": 0.4753434658050537
    },
    {
      "name": "Educational technology",
      "score": 0.4614596962928772
    },
    {
      "name": "Instructional design",
      "score": 0.4216141700744629
    },
    {
      "name": "E learning",
      "score": 0.41761496663093567
    },
    {
      "name": "Psychology",
      "score": 0.3426555395126343
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210120822",
      "name": "Brownsville Public Library",
      "country": "US"
    }
  ]
}