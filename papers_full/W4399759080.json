{
  "title": "OSPC: Detecting Harmful Memes with Large Language Model as a Catalyst",
  "url": "https://openalex.org/W4399759080",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2350745879",
      "name": "Cao Jing-tao",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2003519119",
      "name": "Zhang Zheng",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1938003352",
      "name": "Wang Hong-ru",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2055876376",
      "name": "Liang Bin",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1935192010",
      "name": "Wang Hao",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2087743882",
      "name": "Wong Kam-Fai",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3172559340",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W4210804765",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4285605356",
    "https://openalex.org/W4392904279",
    "https://openalex.org/W4367311140",
    "https://openalex.org/W2914304175"
  ],
  "abstract": "Memes, which rapidly disseminate personal opinions and positions across the internet, also pose significant challenges in propagating social bias and prejudice. This study presents a novel approach to detecting harmful memes, particularly within the multicultural and multilingual context of Singapore. Our methodology integrates image captioning, Optical Character Recognition (OCR), and Large Language Model (LLM) analysis to comprehensively understand and classify harmful memes. Utilizing the BLIP model for image captioning, PP-OCR and TrOCR for text recognition across multiple languages, and the Qwen LLM for nuanced language understanding, our system is capable of identifying harmful content in memes created in English, Chinese, Malay, and Tamil. To enhance the system's performance, we fine-tuned our approach by leveraging additional data labeled using GPT-4V, aiming to distill the understanding capability of GPT-4V for harmful memes to our system. Our framework achieves top-1 at the public leaderboard of the Online Safety Prize Challenge hosted by AI Singapore, with the AUROC as 0.7749 and accuracy as 0.7087, significantly ahead of the other teams. Notably, our approach outperforms previous benchmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT an AUROC of 0.5561.",
  "full_text": "OSPC: Detecting Harmful Memes with Large Language Model as\na Catalyst\nJingtao Cao‚àó\njcao@se.cuhk.edu.hk\nMoE Key Laboratory of High\nConfidence Software Technologies,\nThe Chinese University of Hong Kong\nChina\nZheng Zhang‚àó\ncjang_cjengh@sjtu.edu.cn\nThe Hong Kong University of Science\nand Technology (Guangzhou)\nChina\nHongru Wang\nhrwang@se.cuhk.edu.hk\nMoE Key Laboratory of High\nConfidence Software Technologies,\nThe Chinese University of Hong Kong\nChina\nBin Liang\nbin.liang@cuhk.edu.hk\nMoE Key Laboratory of High\nConfidence Software Technologies,\nThe Chinese University of Hong Kong\nChina\nHao Wang\nhaowang@hkust-gz.edu.cn\nThe Hong Kong University of Science\nand Technology (Guangzhou)\nChina\nKam-Fai Wong‚Ä†\nkfwang@se.cuhk.edu.hk\nMoE Key Laboratory of High\nConfidence Software Technologies,\nThe Chinese University of Hong Kong\nChina\nABSTRACT\nMemes, which rapidly disseminate personal opinions and positions\nacross the internet, also pose significant challenges in propagating\nsocial bias and prejudice. This study presents a novel approach\nto detecting harmful memes, particularly within the multicultural\nand multilingual context of Singapore. Our methodology integrates\nimage captioning, Optical Character Recognition (OCR), and Large\nLanguage Model (LLM) analysis to comprehensively understand\nand classify harmful memes. Utilizing the BLIP model for image\ncaptioning, PP-OCR and TrOCR for text recognition across multiple\nlanguages, and the Qwen LLM for nuanced language understand-\ning, our system is capable of identifying harmful content in memes\ncreated in English, Chinese, Malay, and Tamil. To enhance the\nsystem‚Äôs performance, we fine-tuned our approach by leveraging\nadditional data labeled using GPT-4V, aiming to distill the under-\nstanding capability of GPT-4V for harmful memes to our system.\nOur framework achieves top-1 at the public leaderboard of the\nOnline Safety Prize Challenge hosted by AI Singapore, with the\nAUROC as 0.7749 and accuracy as 0.7087, significantly ahead of the\nother teams. Notably, our approach outperforms previous bench-\nmarks, with FLAVA achieving an AUROC of 0.5695 and VisualBERT\nan AUROC of 0.5561.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíMultimedia information systems ; ‚Ä¢ Se-\ncurity and privacy ‚ÜíHuman and societal aspects of security\n‚àóBoth authors contributed equally to this research.\n‚Ä†Corresponding Author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0172-6/24/05\nhttps://doi.org/10.1145/3589335.3665995\nand privacy; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelli-\ngence.\nKEYWORDS\nLarge Language Models, Multimodal Detection, Harmful Memes\nACM Reference Format:\nJingtao Cao, Zheng Zhang, Hongru Wang, Bin Liang, Hao Wang, and Kam-\nFai Wong. 2024. OSPC: Detecting Harmful Memes with Large Language\nModel as a Catalyst. In Proceedings of Companion Proceedings of the ACM\nWeb Conference 2024 (WWW ‚Äô24 Companion). ACM, New York, NY, USA,\n4 pages. https://doi.org/10.1145/3589335.3665995\n1 INTRODUCTION\nMemes, typically comprising an image with corresponding text,\nspread personal opinions and positions rapidly across the inter-\nnet. However, this same characteristic makes them a powerful tool\nfor disseminating social bias and prejudice. Such harmful memes\ncan perpetuate stereotypes, foster discrimination, and exacerbate\nsocial divisions in a wide variety of social dimensions, including\nrace, religion, sexual orientation, and more [17]. Recognizing this\nissue, AI Singapore has launched the Online Safety Prize Challenge\n[9] to stimulate research of technologies that can effectively de-\ntect harmful memes, particularly in Singapore‚Äôs multicultural and\nmultilingual context. Given the country‚Äôs linguistic diversity, these\nmemes could be created and spread in various languages, including\nEnglish, Chinese, Malay, and Tamil. This adds an additional layer\nof complexity to the detection process, as the memes not only em-\nbody the multimodal nature combining visual imagery with text\nto convey messages that are often nuanced and context-dependent\nbut also do so across different cultural and linguistic contexts.\nMost of previous methods either focus on single-modal informa-\ntion or overlook the nuanced context in a multilingual environment.\nFirst of all, traditional hate speech detection systems [ 4, 10, 12],\nprimarily designed for textual analysis, fall short in interpreting the\nintricate interplay of visuals and text that memes embody. Secondly,\nmost multimodal models trained for memes are frequently limited\nby their reliance on datasets with a predominantly Western context\n[3, 5, 11], overlooking the cultural specificities and diverse linguistic\narXiv:2406.09779v1  [cs.AI]  14 Jun 2024\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore. Jingtao Cao, Zheng Zhang, Hongru Wang, Bin Liang, Hao Wang, and Kam-Fai Wong\nFigure 1: The system pipeline for meme analysis.\nnuances present in global online communities, and particularly in\na linguistically rich and diverse country like Singapore.\nTo address these challenges, our solution leverages a composite\nsystem, integrating visual and textual analysis to understand and\nclassify memes in a comprehensive way. As shown in Figure 1,\nthe approach consists of three key steps: First, we employ a cap-\ntion model to generate captions from meme images, extracting the\nunderlying context and thematic elements of the visual content. Sec-\nondly, Optical Character Recognition (OCR) technology is utilized\nto identify and transcribe the text embedded within the memes,\nregardless of the language. Finally, we construct a prompt that in-\ncorporates both the caption and the text, which is then fed into a\nlarge language model (LLM), in order to get the probabilities of the\nnext token (i.e, ‚ÄúYes‚Äù or ‚ÄúNo‚Äù) to assess if the meme is harmful due\nto social bias. In summary, our contributions can be summarized as\nfollows:\n‚Ä¢We propose a systematic framework to detect harmful multi-\nmodal memes, considering all features in different modals based\non LLMs.\n‚Ä¢To further improve the performance of each sub-module, we\npropose several tailored techniques, including automatic dataset\ncollection method to build additional training corpus for better\nperformance (e.g., OCR and LLM), and additional translation\nmodel to address poor performance at low-resource languages.\n‚Ä¢Our framework achieve top-1 at the public leaderboard, with the\nAUROC as 0.7749 and accuracy as 0.7087, significantly ahead of\nthe other teams.\n2 METHODOLOGY\n2.1 Overall Framework\nIn this section, we first introduce the overall framework of our\nmethod and then present details of each component. As shown\nin Figure 1, our method integrates a three-step process for meme\nanalysis: 1) Step one: multimodal features extraction : we utilize\nexisting off-the-shelf models to convert visual content to the textual\ndescriptions, including a image-to-caption model (e.g., BLIP) and\ntwo OCR models (e.g., PP-OCR and TrOCR), similar with [19]; 2)\nStep two: classification based on LLMs : we then classify memes\nfor harmful content using Qwen with a careful-designed prompts to\nincorporate all intermediate results in previous step. Furthermore,\nwe utilize a translation model (e.g., Tamil to English) to better\nimprove the performance of low-resource languages; 3)Step three:\nfine-tuning with more training corpus : we fintune our approach\nby leveraging additional data labeled using GPT-4V automatically,\naiming to distill the understanding capability of GPT-4V for harmful\nmemes to our system.\n2.2 Multimodal Feature Extraction\nTo extract the visual context of memes, we utilize the BLIP model\n[7] for image captioning, converting images into descriptive text to\nfacilitate comprehensive meme analysis through LLMs. For OCR,\nthe PP-OCR model [6] is initially used due to its high speed, accu-\nracy, and multilingual support, despite its limitations with Tamil\ntext and complex training requirements. To overcome these, we also\nemploy the TrOCR model [8], a transformer-based OCR solution\nthat offers simplicity in training data format, enhancing adaptability\nfor diverse datasets.\nAs depicted in Figure 2, to construct a robust and diverse OCR\ndataset, we employed ChatGPT [13] to generate a wide range of\nText-to-Image (T2I) prompts across various themes. These prompts\nwere then used with Stable Diffusion [16] and SDXL [15] to generate\napproximately 52,000 images. On these images, we printed text\nin random locations, using random fonts, sizes, and colors. The\ntexts were sourced from corpora in English, Chinese, Malay, and\nOSPC: Detecting Harmful Memes with Large Language Model as a Catalyst WWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore.\nFigure 2: The construction method of OCR datasets.\nTamil, collected from the internet1. This approach allowed us to\ncreate a highly diverse OCR dataset, encompassing a wide range of\nlanguages and text appearances.\nOur OCR strategy is designed to maximize accuracy and effi-\nciency. Initially, we used PP-OCR to recognize text in memes. If\nthe output confidence score is above 0.9, we accept the recognition\nresult as it indicates high reliability. A confidence score below 0.9\nusually suggests that the text is either in Tamil or is difficult to\nrecognize for other reasons. In such cases, we switch to our trained\nTrOCR model for a second attempt at recognition. This dual-model\napproach ensures that we can accurately transcribe text from a\nwide variety of memes, thereby enhancing the overall performance\nof our meme analysis system.\n2.3 Classifying Memes with LLM\nFor the final step, we employ Qwen [ 1] as the base LLM, due to\nits commendable performance across multiple languages including\nEnglish, Chinese, and Malay. Also, Its ability to understand various\ncultural contexts makes it an ideal candidate for our purposes. Given\nthe constraints on computational speed and deployment size, we\nopt for the Qwen1.5-14B-Chat-Int4 variant. Recognizing Qwen‚Äôs\nlimitations in handling Tamil, we supplement it with a Tamil to\nEnglish Transformer translation model [ 18], specifically trained\nfor this task. The training corpus for this model was derived from\ntranslations of the Tamil corpus mentioned in section 2.2, facilitated\nby ChatGPT.\nUpon obtaining the caption and text of a meme, if the text is in\nTamil, it is first translated into English using our translation model.\nThe translated or original text, along with the caption, is then fed\ninto the following prompt template:\nRacial Disparities: Memes perpetuating stereotypes\nor prejudices based on race or ethnicity.\nReligious Beliefs and Practices: Memes that mock or\ndemean specific religions or religious practices.\nSexual Orientation: Memes that promote negative\n1Most of the corpus comes from https://github.com/mesolitica/malaysian-dataset,\nwhich contains a large amount of Malaysian internet corpus. Given the similar lin-\nguistic environment between Malaysia and Singapore, it includes content in English,\nChinese, Malay, and Tamil.\nstereotypes or biases about different sexual orienta-\ntions.\nNationalistic Sentiments and Xenophobia: Memes that\nfoster negative attitudes towards migrants or glorify\nextreme nationalistic views.\nSocio-Economic Divides: Memes that highlight or\nridicule class struggles and socio-economic dispar-\nities.\nAge-Related Biases: Memes perpetuating stereotypes\nor biases based on a person‚Äôs age.\nGender Discrimination: Memes that promote gender\nstereotypes or discriminate based on gender.\nDiscrimination Based on Illnesses and Disabilities:\nMemes that mock or belittle individuals with illnesses\nor disabilities.\nThere is {caption}. The text on the image is \"{text}\". Is\nit a meme that contain the above social biases? (\"Yes\"\nor \"No\")\nThis prompt template clearly delineates various definitions of\nharmful memes, thereby aiding the LLM in better identifying the\nharmful nature of a meme. After the LLM processes this prompt, we\nextract the logits corresponding to the ‚ÄúYes‚Äù and ‚ÄúNo‚Äù tokens from\nthe next token logits computed by the LLM. We then calculate the\nprobability of ‚ÄúYes‚Äù using the following formula, which represents\nthe final probability of the meme being harmful:\nùëÉ (Harmful)= ùëÉ (\"Yes\")= ùëílogit(\"Yes\")/ùë°\nùëílogit(\"Yes\")/ùë° +ùëílogit(\"No\")/ùë° (1)\nwhere ùë° is a temperature parameter. This formula quantifies the\nlikelihood of a meme being harmful based on the LLM‚Äôs analysis,\nenabling a systematic and nuanced approach to detecting harmful\ncontent within memes.\n2.4 Fine-Tuning Process\nTo enhance Qwen‚Äôs understanding of the harmfulness of memes,\nwe fine-tuned the Qwen1.5-14B-Chat-Int4 using QLoRA [ 2]. We\nutilized GPT-4V [14], the vision version of GPT-4 that can accept\nimages as input and has a strong understanding and grasp of various\nlinguistic and cultural backgrounds, to annotate the memes in the\nHateful Memes Challenge Dataset [ 5]. For each meme, GPT-4V\ndetermined whether it was harmful and provided corresponding\nreasons for this judgment.\nThe dataset is in a chat style. The questions were structured in a\nmanner similar to the prompt in section 2.3. The responses consisted\nof a simple ‚ÄúYes‚Äù or ‚ÄúNo‚Äù followed by a narrative explaining the\nreason behind the judgment.\n3 DISCUSSIONS\nIn addressing the challenge of detecting harmful memes within the\nmulticultural and multilingual context of Singapore, our solution\nprimarily optimized for English memes. Given the multilingual\nand multicultural knowledge base of Qwen, we hypothesized that\nimprovements made in detecting harmful English memes could be\ntransferable to memes in other languages and cultural contexts.\nWWW ‚Äô24 Companion, May 13‚Äì17, 2024, Singapore, Singapore. Jingtao Cao, Zheng Zhang, Hongru Wang, Bin Liang, Hao Wang, and Kam-Fai Wong\nHowever, this assumption proved difficult to empirically validate\ndue to limitations in data constraints.\nBesides, in the process of cleaning and annotating data, we em-\nployed GPT-4V, one of the most powerful multilingual and mul-\ntimodal LLMs available. While GPT-4V demonstrated strong per-\nformance in English, Chinese, and Malay, its capabilities in Tamil\nwere notably weaker, likely due to the scarcity of Tamil language\ncontent on the internet. This observation underscores the ongo-\ning challenges in leveraging LLMs for low-resource languages like\nTamil.\n4 LIMITATIONS\nOur approach to detecting harmful memes relies on converting\nthe visual information present in memes into text, which is then\nanalyzed by a LLM. However, we observed that the descriptions of\nsome memes generated by BLIP were overly simplistic or failed to\nrecognize specific historical figures in memes. This limitation led\nto inaccuracies in the subsequent analysis of memes by the LLM.\nMoreover, to address the deficiency of Qwen in processing Tamil,\nwe incorporated a translation model to convert Tamil text into\nEnglish before analysis. Ideally, enhancing Qwen‚Äôs capabilities to\ndirectly understand and analyze Tamil without the need for trans-\nlation would be preferable. This would not only streamline the\nprocess but also potentially improve the accuracy of meme analy-\nsis by preserving the original nuances of the language. However,\nachieving this level of linguistic proficiency in Qwen, especially for\na low-resource language like Tamil, remains a challenge we have\nyet to overcome.\n5 CONCLUSION AND FUTURE WORK\nOur approach to detecting harmful memes through a combination\nof image captioning, OCR, and LLM analysis represents a significant\nstep forward in addressing the challenges posed by the multimodal\nand multilingual nature of memes. By leveraging models like BLIP\nfor image captioning, PP-OCR and TrOCR for text recognition, and\nQwen for nuanced language understanding, we have developed\na system capable of identifying harmful content across a variety\nof languages and cultural contexts. To enhance the system‚Äôs abil-\nity, we fine-tuned Qwen and aligned its understanding of meme\nharmfulness with GPT-4V.\nFor future work, we aim to explore the potential of multimodal\nLLMs that could offer a more integrated approach to analyzing\nthe visual and textual components of memes. Additionally, enhanc-\ning the system‚Äôs capability to directly process and understand all\nlanguages present in the memes, including low-resource ones like\nTamil, without relying on translation, will be a key focus. By ad-\ndressing these challenges, we hope to further refine our solution,\nmaking it more effective and efficient in creating safer online envi-\nronments by mitigating the spread of harmful content.\nREFERENCES\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji\nLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,\nXingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng\nWang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\nZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical\nReport. arXiv preprint arXiv:2309.16609 (2023).\n[2] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\nQLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314\n(2023).\n[3] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata,\nEnming Luo, Ranjay Krishna, and Ariel Fuxman. 2024. Visual Program Distilla-\ntion: Distilling Tools and Programmatic Reasoning into Vision-Language Models.\narXiv:2312.03052 [cs.CV]\n[4] Md Saroar Jahan and Mourad Oussalah. 2023. A systematic review of hate speech\nautomatic detection using natural language processing. Neurocomputing 546\n(2023), 126232. https://doi.org/10.1016/j.neucom.2023.126232\n[5] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet\nSingh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes chal-\nlenge: detecting hate speech in multimodal memes. In Proceedings of the 34th\nInternational Conference on Neural Information Processing Systems (, Vancouver,\nBC, Canada,) (NIPS ‚Äô20) . Curran Associates Inc., Red Hook, NY, USA, Article 220,\n14 pages.\n[6] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du,\nYuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, Dianhai Yu, and Yanjun\nMa. 2022. PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight\nOCR System. arXiv:2206.03001 [cs.CV]\n[7] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping\nLanguage-Image Pre-training for Unified Vision-Language Understanding and\nGeneration. In ICML.\n[8] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha\nZhang, Zhoujun Li, and Furu Wei. 2023. TrOCR: transformer-based optical char-\nacter recognition with pre-trained models. In Proceedings of the Thirty-Seventh\nAAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innova-\ntive Applications of Artificial Intelligence and Thirteenth Symposium on Educational\nAdvances in Artificial Intelligence (AAAI‚Äô23/IAAI‚Äô23/EAAI‚Äô23) . AAAI Press, Article\n1469, 9 pages. https://doi.org/10.1609/aaai.v37i11.26538\n[9] Ying Ying Lim, Ming Shan Hee, Xun Wei Yee, Mun-Thye Mak, Weng Kuan Yau,\nXinming Sim, Wesley Tay, Wee Siong Ng, See Kiong Ng, and Roy Ka-Wei Lee.\n2024. AISG‚Äôs Online Safety Prize Challenge: Detecting Harmful Social Bias in\nMultimodal Memes. In Companion Proceedings of the ACM Web Conference 2024 .\n[10] Jitendra Singh Malik, Hezhe Qiao, Guansong Pang, and Anton van den Hen-\ngel. 2023. Deep Learning for Hate Speech Detection: A Comparative Study.\narXiv:2202.09517 [cs.CL]\n[11] Jingbiao Mei, Jinghong Chen, Weizhe Lin, Bill Byrne, and Marcus Tomalin. 2023.\nImproving hateful memes detection via learning hatefulness-aware embedding\nspace through retrieval-guided contrastive learning. arXiv:2311.08110 [cs.CL]\n[12] Marzieh Mozafari, Reza Farahbakhsh, and Noel Crespi. 2022. Cross-Lingual Few-\nShot Hate Speech and Offensive Language Detection Using Meta Learning. IEEE\nAccess 10 (2022), 14880‚Äì14896. https://doi.org/10.1109/ACCESS.2022.3147588\n[13] OpenAI. 2022. Introducing ChatGPT . https://openai.com/blog/chatgpt\n[14] OpenAI. 2023. GPT-4. https://openai.com/gpt-4\n[15] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas\nM√ºller, Joe Penna, and Robin Rombach. 2024. SDXL: Improving Latent Diffusion\nModels for High-Resolution Image Synthesis. In The Twelfth International Confer-\nence on Learning Representations . https://openreview.net/forum?id=di52zR8xgf\n[16] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. 2022. High-\nResolution Image Synthesis with Latent Diffusion Models. In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) . IEEE Computer So-\nciety, Los Alamitos, CA, USA, 10674‚Äì10685. https://doi.org/10.1109/CVPR52688.\n2022.01042\n[17] Shivam Sharma, Firoj Alam, Md Akhtar, Dimitar Dimitrov, Giovanni Martino,\nHamed Firooz, Alon Halevy, Fabrizio Silvestri, Preslav Nakov, and Tanmoy\nChakraborty. 2022. Detecting and Understanding Harmful Memes: A Survey.\n5563‚Äì5572. https://doi.org/10.24963/ijcai.2022/777\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[19] Hongru Wang, Baohang Zhou, Zhengkun Zhang, Yiming Du, David Ho, and\nKam-Fai Wong. 2024. M3sum: A Novel Unsupervised Language-Guided Video\nSummarization. In ICASSP 2024 - 2024 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . 4140‚Äì4144. https://doi.org/10.1109/\nICASSP48485.2024.10447504\nReceived 17 March 2024; revised 30 May 2024; accepted 23 May 2024",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8184102773666382
    },
    {
      "name": "Computer science",
      "score": 0.7542562484741211
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6102532148361206
    },
    {
      "name": "Tamil",
      "score": 0.5834629535675049
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5243383049964905
    },
    {
      "name": "Natural language processing",
      "score": 0.5200238227844238
    },
    {
      "name": "Language model",
      "score": 0.5142647624015808
    },
    {
      "name": "Dissemination",
      "score": 0.48268628120422363
    },
    {
      "name": "The Internet",
      "score": 0.47587719559669495
    },
    {
      "name": "World Wide Web",
      "score": 0.38022372126579285
    },
    {
      "name": "Information retrieval",
      "score": 0.3471263647079468
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2487572431564331
    },
    {
      "name": "Linguistics",
      "score": 0.1715821921825409
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}