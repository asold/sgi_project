{
  "title": "Large Language Models are Learnable Planners for Long-Term Recommendation",
  "url": "https://openalex.org/W4392490033",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1977227678",
      "name": "Shi Wentao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2329433866",
      "name": "He, Xiangnan",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2338957526",
      "name": "Gao Chong-ming",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2432706995",
      "name": "Li Xinyue",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2226268113",
      "name": "Zhang Jizhi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2351348477",
      "name": "Wang, Qifan",
      "affiliations": [
        "Menlo School"
      ]
    },
    {
      "id": "https://openalex.org/A3132652808",
      "name": "Feng, Fuli",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320342922",
    "https://openalex.org/W6702088316",
    "https://openalex.org/W4383993482",
    "https://openalex.org/W3185784178",
    "https://openalex.org/W4367319708",
    "https://openalex.org/W2604662567",
    "https://openalex.org/W6600473871",
    "https://openalex.org/W2948345531",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W6600263792",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W6600553734",
    "https://openalex.org/W4284675209",
    "https://openalex.org/W6603850445",
    "https://openalex.org/W2767807341",
    "https://openalex.org/W4293059865",
    "https://openalex.org/W6600120041",
    "https://openalex.org/W2799544270"
  ],
  "abstract": "Planning for both immediate and long-term benefits becomes increasingly important in recommendation. Existing methods apply Reinforcement Learning (RL) to learn planning capacity by maximizing cumulative reward for long-term recommendation. However, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch, resulting in sub-optimal performance. In this light, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key to achieving the target lies in formulating a guidance plan following principles of enhancing long-term engagement and grounding the plan to effective and executable actions in a personalized manner. To this end, we propose a Bi-level Learnable LLM Planner framework, which consists of a set of LLM instances and breaks down the learning process into macro-learning and micro-learning to learn macro-level guidance and micro-level personalized recommendation policies, respectively. Extensive experiments validate that the framework facilitates the planning ability of LLMs for long-term recommendation. Our code and data can be found at https://github.com/jizhi-zhang/BiLLP.",
  "full_text": "Large Language Models are Learnable Planners for Long-Term\nRecommendation\nWentao Shi\nUniversity of Science and\nTechnology of China\nHefei, China\nshiwentao123@mail.ustc.edu.cn\nXiangnan Heâˆ—\nUniversity of Science and\nTechnology of China\nHefei, China\nxiangnanhe@gmail.com\nYang Zhang\nUniversity of Science and\nTechnology of China\nHefei, China\nzy2015@mail.ustc.edu.cn\nChongming Gao\nUniversity of Science and\nTechnology of China\nHefei, China\nchongming.gao@gmail.com\nXinyue Li\nUniversity of Science and\nTechnology of China\nHefei, China\nlrel7@mail.ustc.edu.cn\nJizhi Zhang\nUniversity of Science and\nTechnology of China\nHefei, China\ncdzhangjizhi@mail.ustc.edu.cn\nQifan Wang\nMeta AI\nMenlo Park, USA\nwqfcr@fb.com\nFuli Fengâˆ—\nUniversity of Science and\nTechnology of China\nHefei, China\nfulifeng93@gmail.com\nABSTRACT\nPlanning for both immediate and long-term benefits becomes in-\ncreasingly important in recommendation. Existing methods apply\nReinforcement Learning (RL) to learn planning capacity by maxi-\nmizing cumulative reward for long-term recommendation. How-\never, the scarcity of recommendation data presents challenges such\nas instability and susceptibility to overfitting when training RL\nmodels from scratch, resulting in sub-optimal performance. In this\nlight, we propose to leverage the remarkable planning capabilities\nover sparse data of Large Language Models (LLMs) for long-term\nrecommendation. The key to achieving the target lies in formulat-\ning a guidance plan following principles of enhancing long-term\nengagement and grounding the plan to effective and executable\nactions in a personalized manner. To this end, we propose a Bi-\nlevel Learnable LLM Planner framework, which consists of a set of\nLLM instances and breaks down the learning process into macro-\nlearning and micro-learning to learn macro-level guidance and\nmicro-level personalized recommendation policies, respectively.\nExtensive experiments validate that the framework facilitates the\nplanning ability of LLMs for long-term recommendation. Our code\nand data can be found at https://github.com/jizhi-zhang/BiLLP.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\nLarge Language Model, LLM Planner, Long-term Engagement\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657683\nACM Reference Format:\nWentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi\nZhang, Qifan Wang, and Fuli Feng. 2024. Large Language Models are Learn-\nable Planners for Long-Term Recommendation. InProceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development in Infor-\nmation Retrieval (SIGIR â€™24), July 14â€“18, 2024, Washington, DC, USA. ACM,\nNew York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657683\n1 INTRODUCTION\nRecommendation systems have gained widespread adoption in\ncontemporary society to alleviate the overwhelming burden of in-\nformation overload [5]. Traditionally, researchers primarily focused\non optimizing usersâ€™ immediate responses (e.g. clicks) to maximize\ninstant benefits [ 51]. However, such a greedy recommendation\nstrategy tends to cater to usersâ€™ immediate interests excessively,\nneglecting long-term engagement [44] and even influencing the\necology negatively. For instance, some users will be confined within\nan echo chamber of preferred information and filter bubbles [17].\nTherefore, it is essential to investigate long-term recommendation.\nTo tackle this challenge, it is crucial to integrate planning capabil-\nities into the recommendation decision-making process to develop\npolicies that take into account not only immediate benefits but also\nlong-term consequences. Existing work primarily employs Rein-\nforcement Learning [7, 31, 48, 63] to acquire planning capabilities\nimplicitly through training models from scratch with the objective\nof maximizing cumulative rewards. However, these approaches are\nentirely data-driven, and their efficacy is significantly constrained\nby the quality and quantity of available data [ 15, 17, 38]. Unfor-\ntunately, recommendation data is typically sparse and naturally\nlong-tail distributed [6]. This poses a significant challenge for RL\nto acquire planning ability, particularly for sparse or long-tail items\nand users, resulting in sub-optimal performance.\nLLMs have emerged with powerful planning capabilities through\npre-training on massive and diverse textual data [1, 36, 42]. Previ-\nous studies have demonstrated that LLMs can break down complex\ntextual and agent tasks into subtasks and then execute them sequen-\ntially [20, 21, 37, 45]. By conceptualizing multi-round recommen-\ndations as analogous to such complex tasks, there is potential to\nharness the planning prowess of LLMs to devise a multi-round rec-\nommendation policy aimed at maximizing long-term engagement.\narXiv:2403.00843v2  [cs.IR]  26 Apr 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Trovato and Tobin, et al.\nUpon realization, benefitting from the inherent extensive world\nknowledge and robust reasoning capabilities in LLMs, it is antic-\nipated to obtain superior planning capabilities even in scenarios\nwith sparse recommendation data, especially for long-tail items.\nTo achieve the target, the key is to recall the task-solving princi-\nples to formulate a plan and make it effective and executable for\nindividual users. However, direct acquisition of such planning ca-\npabilities is non-trivial, due to the substantial scenario divergence\nbetween LLM pre-training and recommendation. In the realm of\nrecommendation tasks, the LLM itself may not naturally exhibit an\ninherent understanding (or commonsense) of the principles that\nenhance long-term engagement. Additionally, when tailoring rec-\nommendations for individual users, a personalized and item-specific\nstrategy becomes essential, far beyond the mere awareness of such\nguiding principles. It is necessary to inspire or teach the LLM to\nacquire the desired principles and make them personalized.\nWe propose a novel Bi-level Learnable LLM Planning (BiLLP)\nframework for long-term recommendation. BiLLP breaks down the\nlearning process into macro-learning and micro-learning through\na hierarchical mechanism. Macro-learning, aiming at acquiring\nhigh-level guiding principles, includes a Planner and Reflector, both\nimplemented as LLM instances. The Planner leverages memorized\nhigh-level experiences that imply guiding principles to formulate\nhigh-level plans for long-term goals, while the Reflector reflects\non the finished trajectory to gather new experiences for updating\nthe Planner. Micro-learning includes the LLM-based Actor-Critic\ncomponent to acquire planning personalization. The Actor person-\nalizes high-level plans into executable actions for users. The Critic\nfunctions similarly to the Reflector but operates on a more fine-\ngrained level. It can promptly evaluate the long-term advantage of\nan action given a state, facilitating the swift update of the Actor\npolicy and mitigating high-variance issues in Q-values [9].\nThe main contributions of this work are summarized as follows:\nâ€¢We introduce the idea of exploring the planning ability of LLMs\nwith a bi-level planning scheme to enhance long-term engage-\nment in recommendation.\nâ€¢We propose a new BiLLP framework with four modules, which\nlearns the planning ability at both macro and micro levels with\nlow variance estimations of Q-values.\nâ€¢We conduct extensive experiments, validating the capability of\nLLMs to plan for long-term recommendation and the superiority\nof the BiLLP framework.\n2 RELATED WORK\nâ€¢Interactive Recommendation. Interactive recommendation is a\ntypical setting to study long-term recommendation, where a model\nengages in online interactions with a user [16, 48]. In contrast to the\nstatic recommendation setting, where the focus is on identifying\nâ€œcorrectâ€ answers within a test set, interactive recommendation\nassesses the efficacy of results by accumulating rewards obtained\nthroughout the interaction trajectories. To improve the performance\nof interactive recommendation, extensive effort [ 10, 23, 62] has\nbeen made to model the recommendation environment as a Markov\ndecision process (MDP) and then utilize advanced RL algorithms to\ndeliver the optimal policy [7, 31, 48, 63]. CIRS [17] learns a causal\nuser model on historical data to capture the overexposure effect\nof items on user satisfaction, facilitating the planning of the RL\npolicy. DORL [15] alleviates Matthew Effect of Offline RL to improve\nlong-term engagement. However, these RL-based methods exhibit\nsuboptimal learning efficiency and poor planning performance\nwhen confronted with sparse recommendation data.\nâ€¢LLM for Recommendation. LLM-based Recommendation par-\nadigm has achieved remarkable advancements [ 11, 29, 50] ow-\ning to the extraordinary abilities of LLMs such as GPT4 [ 1] and\nLlama2 [42]. Distinguishing from existing LLM-based recommen-\ndation methods that are limited to directly using in-context learn-\ning [8, 19, 32, 47, 52, 58] or tuning [3, 26, 30, 49, 59, 60] for immediate\nresponse in the recommendation, our proposed BiLLP delves deeply\ninto how the powerful planning ability of LLMs can empower the\nlong-term engagement of recommendation systems. Some other\napproaches attempt to explore LLMsâ€™ planning capability in man-\naging API tools [12, 22, 43] for recommendation, but they are also\nrestricted to immediate responses and lack focus on usersâ€™ long-\nterm engagement. In contrast to the previous approach of LLM-\nbased recommendation, our proposed BiLLP deeply harnesses the\nplanning capabilities of LLM and utilizes it to enhance long-term\nengagement for users which is particularly challenging to optimize\nin traditional recommendations.\nâ€¢LLM Planner. After undergoing pre-training and instruction\ntuning, the LLMs have attained extensive world knowledge and\nproficient planning capabilities. Recent work [ 20, 21, 37, 45] ex-\nploit these powerful capabilities to generate better control plans for\nrobots and agents. ReAct [55] effectively integrates the action deci-\nsion with planning and results in promising performance. SwiftSage\n[28] integrates fast and slow thinking to solve complex tasks. How-\never, these methods lack the ability to learn from past experiences,\nwhich allows for better task planning. To enable self-improvement\nwithout fine-tuning, Reflexion [40] verbally reflects on task feed-\nback signals. ExpeL [61] utilize cross-task persistent memory to\nstore insights and trajectories. AdaPlanner [41] can learn from past\nfailure, past success, or both. In addition to these macro-level refine-\nments, some work [4, 39, 57] integrates LLMs with RL algorithm to\nlearn from the micro-level interaction experiences. However, they\nsuffer from the issues of high variance estimations of Q-values,\nwhich could be alleviated by our proposed Critic module .\n3 PRELIMINARY\nâ€¢Problem Definition. Following recent work on long-term recom-\nmendation [17], we adopt the interactive recommendation setting.\nThe target is to learn a recommendation model that recommends\nitems ğ‘– âˆˆI (i.e., makes an action 1 ğ‘ğ‘›) to a user ğ‘¢ âˆˆU at each\nstep ğ‘›based on the current state2 ğ‘ ğ‘›. As to applying LLMs for in-\nteractive recommendation, the recommendation process at each\nstep ğ‘›involves two main operations: generating a problem-solving\nplan, referred to as thought ğ‘¡ğ‘›, and subsequently providing an item\nrecommendation, denoted as action ğ‘ğ‘›. Based on this, the entire\ninteraction episode can be denoted as\nH1Â·Â·Â·ğ‘ = {ğ‘ 1,ğ‘¡1,ğ‘1,ğ‘Ÿ1,Â·Â·Â· ,ğ‘ ğ‘,ğ‘¡ğ‘,ğ‘ğ‘,ğ‘Ÿğ‘}, (1)\n1As an initial attempt, we constrain each action ğ‘ğ‘› to recommend only one item.\n2ğ‘ 1 is the initial state before interacting with the model.\nLarge Language Models are Learnable Planners for Long-Term Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nLLM Freeze\nMemory Update \nMem\nActor\nMem\nSimulated\nEnvironmentAdvantage\nThought  Action \nReward \n&\nState +\nTrajectory\n[, , , , . . , ]\nReflections â„“\nMicro \nLearning\nMacro \nLearning\nMem\nRetrieve Memory\nCall Tools\nTool\nCritic\nPlanner\nReflector\nForward Input\nBackward Update\nFigure 1: The overview of the proposed BiLLP framework. The black line indicates that the data serves as a prompt input for\nthe subsequent module. The red line denotes that the data is utilized to update the memory of the subsequent module.\nand the trajectory H1Â·Â·Â·ğ‘›,(1 â‰¤ğ‘› â‰¤ğ‘)can be thought of as a\nsubsequence of an episode.\nâ€¢Simulated Environment. The interactive recommendation set-\nting requires immediate user feedback for recommendation actions.\nCollecting online feedback from users can be financially burden-\nsome, we thus follow [15, 17] and construct â€œSimulated Environ-\nmentâ€ with offline data for both model learning and testing. This\nenvironment can mimic usersâ€™ behaviors in online scenarios, ac-\ncept recommendations (i.e. action ğ‘) from the model, and provide\nfeedback (i.e., reward ğ‘Ÿ) accordingly.\n4 METHOD\nIn this section, we present the proposed BiLLP framework for im-\nproving long-term engagement in the interactive recommendation.\nAs shown in Figure 1, the recommendation process of our frame-\nwork involves two main steps:\nâ€¢The Planner generates problem-solving plans (i.e., thoughts ğ‘¡),\nwhere the recommendation task is broken into sequential step-\nby-step sub-plans, striking a harmonious balance between explo-\nration and exploitation.\nâ€¢The Actor recommends items (i.e., takes actions ğ‘) to the user by\nincorporating both macro-level sub-plans (thoughts) and micro-\nlearning experiences.\nTo generate appropriate plans and personalized item recommenda-\ntions, the key lies in teaching LLMs to learn from past interaction\nepisodes. To enhance the learning process, the BiLLP framework\nemploys a hierarchical mechanism (See Figure 1):\nâ€¢Macro-learning involves the Planner and Reflector to generate\nmore appropriate plans, where the Reflector extracts high-level\nguiding principles from historical episodes and incorporates them\ninto the input of Planner to enhance the quality of plans.\nâ€¢Micro-learning involves the Actor and Critic to generate more\npersonalized recommendations, where Critic assesses the userâ€™s\ncurrent satisfaction level (action advantage value) and updates\nthe policy of Actor to enhance personalized recommendations.\n4.1 Macro-Learning\nThe macro-learning refers to a process in which the Reflector gen-\nerates reflections based on historical episodes and subsequently\nupdates them into the memory of the Planner. The Planner then\nretrieves the most relevant reflections from the memory and uti-\nlizes them as prompts to enhance the quality of plan generation.\nNext, we present the details of the Reflector and Planner, and the\nprocedure of the micro-learning process.\n4.1.1 Reflector. The Reflector is designed to extract guiding princi-\nples from historical episode data. When a user ends his interaction\nwith the model, we utilize this complete interaction episodeH1Â·Â·Â·ğ‘ğ‘\nas input, and then generate reflections â„“ as follows:\nâ„“ğ‘ = Reflector(H1Â·Â·Â·ğ‘\nğ‘ ). (2)\nWe implement the Reflector as an LLM instance. Based on the prede-\nfined instruction prompt and few-shot examples Pğ‘…, the reflection\nâ„“ generation process can be formulated as:\nâ„“ğ‘ = LLM(Pğ‘…,H1Â·Â·Â·ğ‘\nğ‘ ). (3)\nThe obtained reflections are then used to update the memoryMğ‘ƒ in\nthe Planner, denoted as â„“ğ‘ â†’Mğ‘ƒ. To facilitate understanding, we\nprovide an example of reflection in Table 1, which primarily covers\ntwo high-level aspects: analysis of withdrawal reasons and prospec-\ntive guidance. Specifically, in the example, the usersâ€™ disengagement\nis identified as stemming from the repetitive recommendation of\nidentical items, and the guiding principle for future recommenda-\ntions emphasizes prioritizing diversity. Both aspects do not involve\nspecific items.\n4.1.2 Planner. The Planner module is designed to generate forward-\nlooking plans and decompose the high-level plan into sub-plans,\nindicated in outputted thoughts, where thoughts facilitate the Actor\nto execute actions. The Planner is implemented as a frozen LLM in-\nstance equipped with a memory libraryMğ‘ƒ storing past reflections\nfor reference. At each step ğ‘›of a new episode, the Planner utilizes\nthe historical trajectory H1Â·Â·Â·ğ‘›âˆ’1 and the current state ğ‘ ğ‘› from the\nenvironment as input to generate the thought ğ‘¡ğ‘› with reflections\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Trovato and Tobin, et al.\nTable 1: Example of reflections.\nReflection Case 1\nThe user became dissatisfied with the final recommendation,\nwhich was a repeat of a previously recommended game. This\nsuggests that the user may have been looking for more variety\nin their recommendations. In the future, it would be beneficial\nto avoid repeating recommendations and instead focus on\nproviding a diverse range of games across different genres.\nobtained from the memory:\nğ‘¡ğ‘› = Planner(H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›; Mğ‘ƒ), (4)\nwhere the Mğ‘ƒ is a set of past episode reflections. Formally, we have\nMğ‘ƒ = {â„“ğ‘š|ğ‘š= 1,2,...}. When starting a new interaction process,\nmeaning a new episode, multiple relevant reflections â„“ğ¾\nMğ‘ƒ\nare re-\ntrieved from the memory library Mğ‘ƒ as guidance for generating\nnew thoughts. For the following steps in the episode, we utilize\nthe same reflections and other inputs to prompt LLM to generate\nthoughts. We next introduce these parts.\nâ€¢Reflection retrieval. To ensure that the retrieved reflections are\nhelpful for planning. We selectğ¾reflections with a minimal distance\nto this planning process. Taking the initial state ğ‘ 1 to represent the\nprocess for distance computation, we have\nâ„“ğ¾\nMğ‘ƒ\n= {â„“|ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘‘(â„“,ğ‘ 1))< ğ¾,â„“ âˆˆMğ‘ƒ}, (5)\nwhereğ‘‘(Â·,Â·)is defined as the Euclidean distance between the two en-\ncoded textsimplemented byFacebook AI Similarity Search (FAISS) [24],\na library that allows us to quickly search for similar documents.\nğ‘Ÿğ‘ğ‘›ğ‘˜(Â·)gets the rank of a value in ascending order.\nâ€¢Thought generation. We leverage the macro-level guidance from\nthe memory to generate a thought. For each input (H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›),\nwe can sample a thought from LLM policy as follows:\nğ‘¡ğ‘› âˆ¼LLM(Pğ‘ƒ,â„“ğ¾\nMğ‘ƒ\n,H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›). (6)\nHere, ğ‘¡ğ‘› is a sample from the Planner policy, the arguments in the\nfunction LLM(Â·)represent the prompt input to the LLM including\ntask instruction of the PlannerPğ‘ƒ (including few-shot examples), re-\ntrieved reflections â„“ğ¾\nMğ‘ƒ\n, state ğ‘ ğ‘›, and historical trajectory H1Â·Â·Â·ğ‘›âˆ’1\nin the current episode.\nTable 2 presents examples of our input prompt template and\ntwo representative thoughts, encapsulating common outputs of\ngenerated thoughts. In the input prompt, we integrate historical\ninteraction sequences and reflections, prompting the LLM to gener-\nate appropriate thoughts for guiding subsequent actions. In these\nexamples, we could find some interesting properties of the gener-\nated thoughts. In the case of thought example 1, we observe that\nLLM can analyze usersâ€™ interests and decompose multiple rounds\nof recommendation tasks into distinct sub-plans. By leveraging its\nplanning capabilities, the LLM can generate suggestions extending\nbeyond immediate choices, considering their potential long-term\nimpact on user satisfaction. This enables the method to take into\naccount various factors to optimize user long-term engagement and\nsatisfaction. In thought example 2, we note that LLMs can adhere\nto the previous plan, maintaining the consistency and continuity\nof the recommendation strategy.\n4.1.3 Update. The macro-learning involves updating the Planner\nduring the training. After an episode is completed, we update the\nPlanner module by injecting the new reflections for this episode\ninto its memory. The Planner memory update can be formulated as\nMğ‘ƒ â†â„“ğ‘, (7)\nwhere â„“ğ‘ denotes the reflections of the complete episode.\n4.2 Micro-Learning\nThe micro-learning refers to a process in which the Actor grounds\nthe thoughts into executable actions to environments and the Critic\nprovides evaluations of these actions. By updating the policy of Ac-\ntor based on the feedback of Critic and updating the policy of Critic\nbased on the feedback of the environment, Actor and Critic learn to\nprovide personalized recommendations in specific situations. The\nlearning mechanism is similar to the Planner-Reflector but operates\nin a more granular dimension, i.e., directly considering the recom-\nmendation of items. In essence, the micro-learning process bears\nanalogies to the Advantage Actor-Critic (A2C) algorithm [33]. In\nthe following, we first introduce the details of the Actor and Critic\nmodules and present the procedure of the micro-learning process.\n4.2.1 Actor. The Actor module aims to customize high-level plans\ninto executable actions for each users. As illustrated in Figure 1,\nsimilar to the Planner module, we implement it as an LLM instance\nequipped with a memory Mğ´ storing micro-level experiences. Ad-\nditionally, considering that some knowledge is valuable for per-\nsonalization but challenging for LLMs to handle [2], we add a tool\nlibrary denoted as ğ‘‡ğ‘™ to access such knowledge. At each step ğ‘›of\nan episode, the actor utilizes the historical trajectory H1Â·Â·Â·ğ‘›âˆ’1, the\ncurrent stateğ‘ ğ‘›, and the corresponding thoughtğ‘¡ğ‘› from the Planner\nmodule as inputs to generate an executable action ğ‘ğ‘› with knowl-\nedge obtained from the memory and the tool library as follows:\nğ‘ğ‘› = Actor(H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›,ğ‘¡ğ‘›; Mğ´,ğ‘‡ğ‘™).\nHere, the memory Mğ´ can be formulated as a set of micro-level\nexperiences, where the ğ‘š-th experience is a previous interaction\nrecord, including three factors: stateğ‘ ğ‘š, action ğ‘ğ‘š, and correspond-\ning value ğ‘£ğ‘š. Formally, we haveMğ´ = {(ğ‘ ğ‘š,ğ‘ğ‘š,ğ‘£ğ‘š)|ğ‘š= 1,2,... }.\nUpon receiving these inputs, each generation process comprises\nthree operations: 1) retrieving valuable experiences from the mem-\nory Mğ´, 2) utilizing the tools to gather valuable statistical informa-\ntion of the current state, and 3) integrating the results of the first\ntwo steps and other inputs to prompt LLM to generate an action.\nWe next elaborate on these operations.\nâ€¢Retrieval. Similar to the retrieval operation in the Planner mod-\nule, we rely on the similarity between the experience and input\nto select valuable experiences from the memory. Specifically, we\nleverage the distance between the state of an experience and the\ninput state to measure the similarity, and we select all experiences\nwith distances smaller than a threshold ğœğ´. The process can be\nformulated as follows:\nÎ¨ğ‘›\nğ´ = {(ğ‘ ğ‘š,ğ‘ğ‘š,ğ‘£ğ‘š)|ğ‘‘(ğ‘ ğ‘š,ğ‘ ğ‘›)< ğœğ´,ğ‘ ğ‘š âˆˆMğ´}, (8)\nwhere Î¨ğ‘›\nğ´ denotes the retrieved results, and ğ‘‘(Â·,Â·)is the same to\nthat in Equation (5).\nâ€¢Tool analysis. We utilize the tools in the tool libraryğ‘‡ğ‘™ to analyze\nusersâ€™ interaction history, extracting valuable information that is\nLarge Language Models are Learnable Planners for Long-Term Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 2: Example of the input and output for the Planner module.\nInstruction Input\nInstruction: Solve a recommendation task with interleaving Thought, Action, and Observation steps. Thought can reason\nabout the current situation and current user interest. Your goal is to meet the userâ€™s interest as much as possible\nand make recommendations to users as many times as possible. Note that if the user is not satisfied with your\nrecommendations, he will quit and not accept new recommendations. You may take as many steps as necessary.\nHere are some examples: <Few-shot Examples> (END OF EXAMPLES)\nReflection: {Reflections â„“ğ¾\nMğ‘ƒ\n}\n{Historical interaction sequence H1Â·Â·Â·ğ‘›âˆ’1}\nOutput: Thought\nCase 1: \"The user seems to enjoy a mix of Action and Independent video games. They also seem to appreciate Adventure\ngames. I would first recommend the user their favorite action games, and then recommend some other niche genre\ngames that they like. \"\nCase 2: The user seems to be satisfied with the recommendations so far. Following the previous plan, I should recommend\nsome other niche genre games that they like, such as RPG games..\nchallenging for the LLM to handle. In this study, we primarily focus\non leveraging the Category Analysis Tool . At the ğ‘›-th step, given\nthe state ğ‘ ğ‘›, the tool can identify a list of categories associated\nwith each legal action and conduct statistical analysis on the userâ€™s\nviewing history. Formally,\nOğ‘› = ğ‘‡ğ‘™(ğ‘ ğ‘›), (9)\nwhere Oğ‘› denotes the tool output in text format. Notably, the\nmethodology described here can be adapted and applied to var-\nious other tools.\nâ€¢Action generation. We leverage both the guidance from the\nPlanner module, micro-level knowledge obtained from the memory\nand tool to prompt the LLM of the Actor module to generate an\naction. For the input (H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›,ğ‘¡ğ‘›)with ğ‘¡ğ‘› representing the\nthought, once we obtain the corresponding retrieval resultsÎ¨ğ‘›\nğ´ and\ntool analysis result ğ‘‚ğ‘›, we can sample an action ğ‘â€²ğ‘› from the LLM\npolicy as follows:\nğ‘â€²\nğ‘› âˆ¼LLM(Pğ´,Î¨ğ‘›\nMğ´\n,Oğ‘›,H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›,ğ‘¡ğ‘›), (10)\nwhere Pğ´ represents the task instruction for the Actor. Note that\nthe temperature coefficient of the LLM should be set to a non-zero\nvalue, ensuring the generation of non-deterministic results.\nItem grounding. The final action should be specific to an item\nwithin the candidate pool. Note that the LLM may generate items\nğ‘â€²ğ‘› that are not necessarily included in the pool. To address this\nissue, we adopt the grounding strategy from [2] to map ğ‘â€²ğ‘› to an\nactual item with the highest similarity. Formally, the final action is\nobtained as follows:\nğ‘ğ‘› = arg min\nğ‘âˆˆI\nsim(eğ‘,eğ‘â€²ğ‘›), sim(eğ‘,eğ‘â€²ğ‘›):= âˆ¥eğ‘ âˆ’eğ‘â€²ğ‘›âˆ¥, (11)\nwhere eğ‘ represents the embedding of the action (item) ğ›¼ encoded\nby Llama2-7b [42], sim(Â·,Â·)denotes the embedding similarity mea-\nsured by the L2 distance, and âˆ¥Â·âˆ¥ signifies the ğ¿2 norm.\n4.2.2 Critic. The Critic module is an LLM-based evaluator, pro-\nviding evaluative feedback on the long-term goals for the actions\ngenerated by the Actor module to help update the policy of Actor.\nThe Critic module also contains a memory Mğ¶ to store previous\nexperiences. Inspired by the A2C algorithm, we utilize the advan-\ntage value ğ‘£ğ‘› of action ğ‘ğ‘› in the given state ğ‘ ğ‘› as the measurement.\nIn particular, Critic takes the state ğ‘ ğ‘›, action ğ‘ğ‘›, and the history\ntrajectory H1...ğ‘›âˆ’1 as inputs and then outputs the advantage ğ‘£ğ‘›\nwith the experiences in Mğ¶ as references, which can be abstracted\nas follows:\nğ‘£ğ‘› = Critic(ğ‘ ğ‘›,ğ‘ğ‘›; Mğ¶). (12)\nTo compute advantage values, similar to A2C, we first estimate\nthe state-value function ğ‘‰(ğ‘ ğ‘›), and then, based on it, we use the\nadvantage function [33] to determine the advantage value:\nâ€¢Estimating state-value. The function ğ‘‰(ğ‘ ğ‘›)provides an es-\ntimation of the value of being in state ğ‘ ğ‘› when following the\nActor policy. We directly model the function with the LLM of\nthe Critic module. In particular, we leverage in-context learning\nwith few-shot examples and previous estimations in the memory\nMğ¶ = {(ğ‘ ğ‘š,ğ‘‰(ğ‘ ğ‘š))|ğ‘š= 1,2,... }to predict the values of a given\nstate ğ‘ ğ‘›. Formally, we have:\nğ‘‰(ğ‘ ğ‘›)= LLM(Pğ¶,Î¦ğ‘›\nMğ¶\n,H1Â·Â·Â·ğ‘›âˆ’1,ğ‘ ğ‘›), (13)\nwhere Pğ¶ represents the used task prompt (including few-shot\nexamples), and Î¦ğ‘›\nMğ¶\ndenotes the selected experiences from Mğ¶,\nwhich is obtained as follows:\nÎ¦ğ‘›\nMğ¶\n= {(ğ‘ ğ‘š,ğ‘‰(ğ‘ ğ‘š))|ğ‘‘(ğ‘ ğ‘š,ğ‘ ğ‘›)< ğœğ¶,ğ‘ ğ‘š âˆˆMğ¶}, (14)\nwhere ğœğ¶ is a threshold, and ğ‘‘(Â·,Â·)denotes the same distance func-\ntion in Equation 5.\nâ€¢Computing advantage value. We next utilize the advantage\nfunction to determine the advantage value ğ‘£ğ‘› of action ğ‘ğ‘› given\nthe state ğ‘ ğ‘› at the ğ‘›-th step. The advantage value is:\nğ‘£ğ‘› = ğœ(ğ´(ğ‘ ğ‘›,ğ‘ğ‘›)), ğ´ (ğ‘ ğ‘›,ğ‘ğ‘›)= ğ‘Ÿğ‘› +ğ›¾âˆ—ğ‘‰(ğ‘ ğ‘›+1)âˆ’ğ‘‰(ğ‘ ğ‘›), (15)\nwhere ğ´(Â·,Â·)is the commonly used advantage function, ğ‘Ÿğ‘› denotes\nthe environmental reward at step ğ‘›, and ğ‘ ğ‘›+1 denotes the next-step\nstate if taking action ğ‘ğ‘› at state ğ‘ ğ‘›. Regarding the function ğœ, we\nhave ğœ(ğ‘¥)= 1 if ğ‘¥ â‰¥0 else 0. Note that this approach mitigates\nthe issue of high variance estimation of the Q-value in previous\nwork [33] (cf. Section 5.4).\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Trovato and Tobin, et al.\nThe Actor Policy\n(updated)\n  \nPolicy Gradient\n\nMemory-based Learning\n\n Mem\nstore\n retrieve\nFigure 2: The memory-based learning methods and policy\ngradient based methods have a comparable impact on the\nActor policy.\n4.2.3 Update. The micro-learning involves updating both the Ac-\ntor and the Critic during their iteration. At each step ğ‘›, after ob-\ntaining the advantage value ğ‘£ğ‘› for the action ğ‘ğ‘›, we update the two\nmodules by injecting the new experience into their memory.\nâ€¢The Critic memory update can be formulated as\nMğ¶ â†(ğ‘ ğ‘›,ğ‘Ÿğ‘› +ğ›¾âˆ—ğ‘‰(ğ‘ ğ‘›+1)), (16)\nwhere ğ‘Ÿğ‘› +ğ›¾ âˆ—ğ‘‰(ğ‘ ğ‘›+1)can be considered as a more accurate\nestimation of ğ‘‰(ğ‘ ğ‘›)[33].\nâ€¢The Actor memory update can be formulated as\nMğ´ â†(ğ‘ ğ‘›,ğ‘£ğ‘›). (17)\nThe updated memory incorporates new experiences, helping en-\nhance the next step of processing.\n4.3 Discussion\nNext, we compare the policy update of our BiLLP framework with\ntraditional gradient-based policy updates to illustrate why our ap-\nproach, based on in-context learning, can learn planning without\nthe need for gradient updates. As shown in Figure 2, for traditional\nmethods, when a favorable action is identified for a state (possibly\ndetermined based on Q-values in the REINFORCE algorithm [34] or\nthe Advantage Function in A2C [33]), the purpose of the gradient\nupdate is to adjust the policy to increase the probability of sampling\nthat specific action for the given state. In contrast, for our method,\nalthough no gradient updates are performed, the specific state and\naction are recorded in external memory. When encountering a\nsimilar state again, the retrieving probability of that specific state\nand action from the memory will increase, which would further\nenhance the probability of executing that specific action in that\nstate. This achieves a similar effect to gradient updates. This is the\nunderlying learning principle for our BiLLP framework.\n5 EXPERIMENTS\nIn this section, we evaluate the proposed BiLLP framework in the in-\nteractive recommendation settings. Our experiments aim to address\nthe following questions:\nâ€¢RQ1: How does BiLLP perform compared to state-of-the-art RL-\nbased methods and other LLM frameworks in the interactive\nrecommendation setting?\nâ€¢RQ2: To what extent can macro-learning and micro-learning\nmechanisms improve the LLMsâ€™ planning ability?\nâ€¢RQ3: Can the proposed Critic module effectively estimate the\nstate-value function to facilitate the update of the Actor module?\nâ€¢RQ4: Whether the proposed BiLLP framework is robust to dif-\nferent recommendation environments and base LLM models?\n5.1 Experiments Setup\nWe introduce the experimental settings with regard to simulated\nexperiments and baselines, which are implemented based on the\nEasyRL4Rec library3 [56].\n5.1.1 Recommendation Experiments. In the interactive recommen-\ndation setting, we are interested in examining the potential of\nmodels to mitigate the issue of filter bubbles and maximize usersâ€™\nlong-term engagement. Conducting direct online experiments for\nmodel learning and testing can be prohibitively costly. As a result,\nfollowing [15], we resort to creating simulated interactive environ-\nments using high-quality logs.\nâ€¢Steam [25] contains reviews and game information. The dataset\ncompiles titles and genres of games. we consider users who en-\ngage in gameplay for a duration exceeding 3 hours to have a\nrating of 5, while others are assigned a rating of 2. We filter out\nusers and items that interact less than 5 times in the log.\nâ€¢Amazon-Book [35] refers to a book recommendation dataset,\nthe â€œbookâ€ subset of the famous Amazon Product Review dataset4.\nThis dataset compiles titles and genres of books from Amazon,\ncollected between 1996 and 2018, with review scores ranging\nfrom 1 to 5. We filter out users and items that interact less than\n90 times in the log.\nTo better reflect the issue of filter bubbles and simulate real-world\nrecommendation scenarios, we follow [15, 17, 54] to introduce a quit\nmechanism. The interaction will terminate if any of the following\nconditions are met:\nâ€¢The similarity between a recommended item and the items in the\nrecent recommendation list (with a window size of ğ‘Š) is below\na predefined threshold ğ›½.\nâ€¢The online reward ğ‘Ÿ of a recommended item is less than 2.\nIn this sense, a model that effectively captures usersâ€™ interests and\nmitigates the risk of continuously recommending similar items that\nreinforce the filter bubble phenomenon is crucial for achieving a\nlonger interaction trajectory and maximizing cumulative rewards.\nTo estimate the online reward, we first split the dataset evenly into\ntraining and test sets in chronological order. For each set D âˆˆ\n{Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,Dğ‘¡ğ‘’ğ‘ ğ‘¡}, we utilize the DeepFM model [18] to fit the data\nand obtain vector representations for userseDğ‘¢ and items eD\nğ‘– . Then\nwe can calculate the online rewards:\nğ‘ŸD\nğ‘¢,ğ‘– = DeepFM(eD\nğ‘¢ ,eD\nğ‘– ), ğ‘¢ âˆˆU,ğ‘– âˆˆI, (18)\nand the similarity between the two items:\nsim(eD\nğ‘– ,eD\nğ‘— )= |eD\nğ‘– âˆ’eD\nğ‘— |2, ğ‘–,ğ‘— âˆˆI, (19)\nIt is noteworthy that we have established separate training and\ntest environments for each dataset in order to simulate real-world\nscenarios where the user interests may have evolved during online\ntraining and model deployment. For now, the simulated environ-\nments can play the same role as the online users. Therefore, we can\ntrain the model on the training simulated experiments and evaluate\nthe model on the test simulated experiments as the process shown\nin Figure 1. The statistics of the datasets are illustrated in Table 4.\n3https://github.com/chongminggao/easyrl4rec\n4https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/\nLarge Language Models are Learnable Planners for Long-Term Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 3: Average results of all methods in two environments (Bold: Best, Underline: Runner-up).\nMethods Steam Amazon\nLen Reach Rtraj Len Reach Rtraj\nSQN 2.183 Â±0.177 3.130 Â±0.050 6.837 Â±0.517 4.773 Â±0.059 4.303 Â±0.017 20.570 Â±0.245\nCRR 4.407 Â±0.088 3.263 Â±0.427 14.377 Â±1.658 3.923 Â±0.162 4.537 Â±0.103 17.833 Â±1.129\nBCQ 4.720 Â±0.343 3.997 Â±0.068 18.873 Â±1.092 4.847 Â±0.721 4.367 Â±0.053 21.150 Â±2.893\nCQL 5.853 Â±0.232 3.743 Â±0.147 21.907 Â±0.299 2.280 Â±0.185 4.497 Â±0.039 10.263 Â±0.882\nDQN 4.543 Â±0.693 4.500 Â±0.069 20.523 Â±3.618 4.647 Â±0.498 4.290 Â±0.083 19.923 Â±1.909\nA2C 9.647 Â±0.848 4.367 Â±0.069 42.180 Â±3.937 7.873 Â±0.310 4.497 Â±0.026 35.437 Â±1.453\nDORL 9.467 Â±0.862 4.033 Â±0.098 38.300 Â±4.173 7.507 Â±0.174 4.510 Â±0.014 33.887 Â±0.655\nActOnly 5.567 Â±0.160 4.537 Â±0.021 25.250 Â±0.637 6.383 Â±0.176 4.490 Â±0.008 28.660 Â±0.761\nReAct 11.630 Â±0.741 4.559 Â±0.047 52.990 Â±2.925 7.733 Â±0.450 4.603 Â±0.033 35.603 Â±1.806\nReflexion 12.690 Â±1.976 4.523 Â±0.026 57.423 Â±8.734 8.700 Â±0.535 4.670 Â±0.073 40.670 Â±2.954\nBiLLP 15.367 Â±0.119 4.503 Â±0.069 69.193 Â±1.590 9.413 Â±0.190 4.507 Â±0.012 42.443Â±0.817\nTable 4: Statistics of experiment datasets.\nDatasets #Users #Items #Train #Test\nSteam 6,012 190,365 1,654,303 958,452\nAmazon 3,109 13,864 339,701 137,948\n5.1.2 Evaluation Metrics. In this paper, we utilize three metrics:\nthe trajectory length (Len), the average single-round reward\n(Reach), and the cumulative reward of the whole trajectory (Rtraj)\nto evaluate the model performance in the interactive recommen-\ndation setting. Longer trajectory lengths and higher cumulative\nrewards demonstrate the modelâ€™s ability to maximize long-term\nengagement. However, it is important to note that a higher average\nreward is not necessarily better. Excessively high average rewards\nmay indicate a modelâ€™s overemphasis on immediate responses.\n5.1.3 Baselines. To comprehensively and fairly evaluate the su-\nperiority of our proposed BiLLP, we choose some representative\nRL-based methods and LLM-based methods as baselines. For the RL-\nbased methods, we choose seven representative baselines including\nthe State-Of-The-Art (SOTA) method for long-term engagement\noptimization to mitigate filter bubble problems:\nâ€¢DQN, or Deep Q-Networks [34], is a deep reinforcement learn-\ning algorithm that combines deep neural networks with the Q-\nlearning algorithm.\nâ€¢SQN, or Self-Supervised Q-learning [53], consists of two output\nlayers, namely the cross-entropy loss head and the RL head. The\nRL head is utilized to generate the final recommendations.\nâ€¢BCQ, or Batch-Constrained deep Q-learning [ 14], a modified\nversion of conventional deep Q-learning designed for batch re-\ninforcement learning. It utilizes the discrete-action variant [13],\nwhich focuses on discarding uncertain data and updating the\npolicy solely based on high-confidence data.\nâ€¢CQL, or Conservative Q-Learning [27], is a model-free RL method\nthat adds a Q-value regularizer on top of an actor-critic policy.\nâ€¢CRR, or Critic Regularized Regression [46], is a model-free RL\nmethod that learns the policy by avoiding OOD actions.\nâ€¢A2C, or Advantage Actor-Critic [33], improves the Actor-Critic\nalgorithm and stabilizes learning by using the Advantage func-\ntion as Critic instead of the Action value function.\nâ€¢DORL, or Debiased model-based Offline RL [15], add a penalty\nterm to relax the pessimism on states with high entropy to allevi-\nate the Matthew effect in offline RL-based recommendation. This\nis the SOTA method of maximizing usersâ€™ long-term engagement\nto alleviate filter bubble issues.\nEnsuring fair comparison, we also implement three LLM-based\nbaselines utilizing the same LLM backbone as BiLLP:\nâ€¢ActOnly, a baseline that recommends items to users according\nto instruction prompts without thought and planning.\nâ€¢ReAct, [55] utilizes LLMs to generate both reasoning traces\nand task-specific actions in an interleaved manner, allowing for\ngreater synergy between the reasoning and acting.\nâ€¢Reflexion, [40] verbally reflects on task feedback signals, then\nmaintains their own reflective text in an episodic memory buffer\nto induce better decision-making in subsequent trials.\n5.1.4 Implementation Details. For a fair comparison, all RL-based\nmethods are trained with 100,000 episode data, and all LLM-based\nmethods are trained with 100 episode data. For model-based RL\nmethods DORL, we use the same DeepFM model as [15]. For the\nReflection and BiLLP methods, we set the number of most similar\nreflections ğ¾ = 2. For the BiLLP method, we set the similarity\nthreshold ğœğ´ = 0.01 and ğœğ¶ = 0.1. The discount factor ğ›¾ is set\nto 0.5. All methods in two environments are evaluated with the\nquit parameters: ğ‘Š = 4, ğ›½ğ‘†ğ‘¡ğ‘’ğ‘ğ‘š = 50, and ğ›½ğ´ğ‘šğ‘ğ‘§ğ‘œğ‘› = 15. The\nmaximum round is set to 100. For all the RL-based methods, we\nleverage DeepFM [18] as the backbone following [15], and for all the\nLLM-based methods, we utilize the â€œgpt-3.5-turbo-16kâ€ provided by\nOpenAI as the LLM backbone for its strong long context modeling\nability. And the temperature is set to 0.5 for all experiments.\n5.2 Main Results Comparison (RQ1)\nAfter training, we evaluate all methods with 100 episodes (i.e., in-\nteraction trajectories) in two interactive environments. The results\nare shown in Table 3, where each result in the table is averaged\nover three random experiments with distinct seeds for robustness\nand reliability. From the results, we observe that:\nâ€¢BiLLP consistently achieves the best long-term performance (Len\nand Rtraj) over RL-based methods and LLM-based baselines across\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Trovato and Tobin, et al.\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018\n/uni0000002c/uni00000057/uni00000048/uni00000050/uni00000003/uni0000004a/uni00000055/uni00000052/uni00000058/uni00000053/uni00000056/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b\n/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004c/uni00000057/uni00000048/uni00000050/uni00000003/uni00000049/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000018\n/uni00000014/uni00000011/uni00000013/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000036/uni00000057/uni00000048/uni00000044/uni00000050\n/uni00000024/uni00000015/uni00000026\n/uni00000032/uni00000058/uni00000055/uni00000056\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018\n/uni0000002c/uni00000057/uni00000048/uni00000050/uni00000003/uni0000004a/uni00000055/uni00000052/uni00000058/uni00000053/uni00000056/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b\n/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000004c/uni00000057/uni00000048/uni00000050/uni00000003/uni00000049/uni00000055/uni00000048/uni00000054/uni00000058/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000024/uni00000050/uni00000044/uni0000005d/uni00000052/uni00000051\n/uni00000024/uni00000015/uni00000026\n/uni00000032/uni00000058/uni00000055/uni00000056\nFigure 3: The frequency distribution of items recommended\nby our method and A2C in the two environments.\ntwo datasets. This demonstrates the effectiveness of our proposed\nframework and its ability to stimulate and adapt the long-term\nplanning capacity of LLMs. For single-round rewardReach, BiLLP\nobtains a relatively higher score, which indicates it successfully\ncaptures user interests while avoiding excessive emphasis on\nimmediate responses.\nâ€¢The ActOnly method, which only utilizes LLMs to generate ac-\ntions (recommendations), exhibits inferior performance com-\npared to certain RL-based methods and LLM-based methods that\nincorporate planning. Drawing upon this, we can infer that an\nexplicit thinking and planning process is crucial for enhancing\nthe planning capabilities of LLMs.\nâ€¢The ReAct method, which integrates the thinking process and\naction process, still performs worse than Reflexion and BiLLP.\nThis underscores the significance of self-improvement in LLMs,\nin order to improve their planning abilities for long-term recom-\nmendation tasks.\nIn addition to the overall performance comparison, we con-\nduct an in-depth analysis of the recommended items for RL-based\nmethod A2C and LLM-based method BiLLP. We first calculate the\nitemsâ€™ popularity (occurrence frequencies) both in the training set\nand test set. Subsequently, we evenly divide the items into five\ngroups with decreasing popularity: 1, 2, 3, 4, 5. We analyze the pro-\nportion of items belonging to each group among the recommended\nitems generated by A2C and BiLLP, where the results are shown in\nFigure 3. From the figure, we observe that:\nâ€¢RL-based method A2C tends to overfit on popularity items and\nlack planning capabilities on long-tail items.\nâ€¢In contrast, BiLLP exhibits better planning capabilities on long-\ntail items, which could effectively alleviate the issue of filter\nbubbles and maximize long-term engagement.\n5.3 Ablation Study (RQ2)\nIn this subsection, we conduct ablation studies to evaluate the effect\nof the two learning mechanisms. Concretely, w/o Macro refers to\na variant of the BiLLP framework that does not use the reflective\ntext to enhance its Planner module, and w/o Micro refers to a\nvariant that does not use the micro-learning experience to enhance\nits Actor module. From Table 5, we can observe that:\nâ€¢The absence of either of the two learning mechanisms would\nresult in a decline in performance, thereby indicating that both\nlearning mechanisms have contributed to the enhancement of\nlong-term engagement.\nTable 5: Average results of all methods in the two environ-\nments (Bold: Best).\nMethods Steam\nLen Reach Rtraj\nw/o Macro 14.363 Â±0.467 4.523 Â±0.012 64.960 Â±2.011\nw/o Micro 14.270 Â±0.190 4.535 Â±0.005 64.720 Â±0.920\nBiLLP 15.367 Â±0.119 4.503 Â±0.069 69.193 Â±1.590\nMethods Amazon\nLen Reach Rtraj\nw/o Macro 8.947 Â±0.480 4.530 Â±0.057 40.547 Â±2.622\nw/o Micro 8.800 Â±0.432 4.707 Â±0.026 41.420 Â±2.003\nBiLLP 9.413 Â±0.190 4.507 Â±0.012 42.443 Â±0.817\n1 2 3 4 5 6 7 8 9 10\nState s\n2\n4\n6\n8\n10State-value function V (s)\nMonte Carlo Estimation\nLLM Estimation\nFigure 4: The memory-based in-context learning methods\nand policy gradient-based methods have a comparable impact\non the Actor policy.\nâ€¢Based on the experimental details presented in Section 5.1.4,\nthe aforementioned improvements are achieved using only 100\nepisodes of data for both learning mechanisms. This suggests the\nhigh efficiency of in-context learning compared to fine-tuning\nand training from scratch.\n5.4 Effects of Critic Module (RQ3)\nIn this subsection, our objective is to demonstrate the effective-\nness of the Critic module in estimating the state-value function,\ndenoted as ğ‘‰ğœ‹(ğ‘ ), which is crucial for facilitating the update pro-\ncess of the Actor module. The state-value function ğ‘‰ğœ‹(ğ‘ )gives the\nexpected cumulative discounted reward if we start from state ğ‘ and\nact according to the policy.\nTo get an accurate and unbiased estimation ofğ‘‰ğœ‹(ğ‘ ), for a spe-\ncific state ğ‘ , we sample 1000 complete trajectories according to the\nActor policy and calculate the cumulative discounted reward for\neach trajectory. Figure 4 illustrates the distribution of these reward\nsamples, along with their mean value. The mean value serves as\nan accurate and unbiased estimation of the state-value function\nğ‘‰ğœ‹(ğ‘ ). It is worth noting that prior studies [4, 57], have utilized a\nsingle trajectoryâ€™s cumulative discounted reward to estimate either\nthe state-value function ğ‘‰ğœ‹(ğ‘ )or the state-action value function\nğ‘„ğœ‹(ğ‘ ,ğ‘), which suffers from the issue of high variance estimation.\nLarge Language Models are Learnable Planners for Long-Term Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 6: Average Results of all methods in the two environ-\nments (Bold: Best).\nMethods Steam\nLen Reach Rtraj\nGPT-4-32k 25.400 Â±2.800 4.635 Â±0.115 118.235 Â±15.915\nGPT-3.5-16k 15.367 Â±0.119 4.503 Â±0.069 69.193 Â±1.590\nLlama-2-7B 13.800 Â±1.105 4.610 Â±0.065 63.767 Â±6.015\nMethods Amazon\nLen Reach Rtraj\nGPT-4-32k 12.450 Â±1.250 4.580 Â±0.070 57.180 Â±6.570\nGPT-3.5-16k 9.413 Â±0.190 4.507 Â±0.012 42.443 Â±0.817\nLlama-2-7B 8.100 Â±1.512 4.603 Â±0.054 37.300 Â±6.895\nIn contrast to these approaches, we leverage the Critic module\nto estimate the state-value function ğ‘‰ğœ‹(ğ‘ ). To evaluate our estima-\ntion, we repeat the estimation 100 times. The resulting estimations,\nas well as their distribution and mean value, are also depicted in\nFigure 4. Based on the analysis of ten different states, it can be\ninferred that the utilization of the Critic module effectively miti-\ngates estimation variance, despite the presence of a small bias in\nthe estimation.\n5.5 Robustness of the Framework (RQ4)\n5.5.1 Results with Different Environments. To validate that BiLLP\ncan work robustly in different environment settings, we vary the\nwindow size ğ‘Š in the exit mechanism and fix the similarity thresh-\nold ğ›½ to simulate different effects of filter bubbles on user disen-\ngagement. The evaluation results are shown in Figure 5, where all\nresults are averaged over three random experiments with distinct\nseeds. We visualize all three metrics and observe that:\nâ€¢As the window size ğ‘Š increases, the performance of the trajec-\ntory length and the cumulative reward metrics decrease for all\nmethods. This implies that when users are more susceptible to\nthe influence of filter bubbles, the model faces greater challenges\nin learning to improve usersâ€™ long-term engagement.\nâ€¢BiLLP outperforms all baselines in terms of both the trajectory\nlength and the cumulative reward metrics, which demonstrates\nthe robustness of BiLLP in different environments.\nâ€¢BiLLP obtains a relatively higher score in terms of the single-\nround in different environments, which indicates it successfully\ncaptures user interests while avoiding excessive emphasis on\nimmediate responses.\n5.5.2 Results with Different Base Models. To validate the robust-\nness of the BiLLP framework across various base models, we con-\nduct additional experiments with other different LLM backbones:\nâ€œgpt-4-32kâ€ and â€œLlama-2-7bâ€. The results are presented in Table 6.\nFrom the table, several noteworthy observations can be made:\nâ€¢BiLLP showcases superior performance compared to traditional\nRL-based methods with different base models, as demonstrated\nin Table 6. This indicates that our framework is robust across\ndifferent LLMs.\nâ€¢The performance of BiLLP based on â€œGPT-3.5-16kâ€ is superior\nto that based on â€œLlama-2-7Bâ€, while inferior to that based on\nâ€œGPT-4-32kâ€. This observation suggests a positive correlation\n123456789100\n25\n50\n75\n100\n125Cumulative reward\nSteam\nCRRA2CDQNDORLActOnlyOurs\n123456789100\n25\n50\n75\n100 Amazon\n12345678910051015202530Length of trajectory123456789100\n5\n10\n15\n20\n123456789103\n3.5\n4\n4.5\n5\nWindow sizeğ‘Š\nSingle-round reward123456789104\n4.25\n4.5\n4.75\n5\nWindow sizeğ‘Š\nFigure 5: Results under different simulated environments.\nbetween the strength of the LLM backbone and the performance\nenhancement of BiLLP.\n6 CONCLUSION\nIn this work, we explore the integration of planning capabilities\nfrom Large Language Models (LLMs) into the recommendation\nto optimize long-term engagement. To bridge the gap between\nthe pre-training scenarios and recommendation scenarios, we pro-\npose a bi-level learnable LLM planning framework called BiLLP,\nwhere the learning process can be divided into macro-learning and\nmicro-learning using a hierarchical mechanism. This hierarchical\napproach improves learning efficiency and adaptability. Extensive\nexperiments validate the capability of LLMs to plan for long-term\nrecommendation and the superiority of the BiLLP framework.\nA potential avenue for future research involves exploring tech-\nniques to enhance the planning capabilities of small-scale models\nin the context of recommendation tasks. Additionally, exploring\nthe integration of reinforcement learning algorithms within the\nplanning framework could provide further insights into optimizing\nlong-term engagement in recommendation systems.\nACKNOWLEDGMENTS\nThis work is supported by the National Key Research and Devel-\nopment Program of China (2022YFB3104701), the National Natural\nScience Foundation of China (62272437, 62121002), and the CCCD\nKey Lab of Ministry of Culture and Tourism.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Trovato and Tobin, et al.\nREFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng\nLuo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-Step Grounding Paradigm\nfor Large Language Models in Recommendation Systems. CoRR abs/2308.08434\n(2023).\n[3] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.\n2023. Tallrec: An effective and efficient tuning framework to align large language\nmodel with recommendation. arXiv preprint arXiv:2305.00447 (2023).\n[4] Ethan Brooks, Logan Walls, Richard L. Lewis, and Satinder Singh. 2023. Large\nLanguage Models can Implement Policy Iteration. arXiv:2210.03821 [cs.LG]\n[5] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He.\n2020. Bias and Debias in Recommender System: A Survey and Future Directions.\nCoRR abs/2010.03240 (2020).\n[6] Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, and Xiangnan\nHe. 2023. Adap-ğœ : Adaptively Modulating Embedding Magnitude for Recom-\nmendation. In WWW. ACM, 1085â€“1096.\n[7] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and\nEd H. Chi. 2019. Top-K Off-Policy Correction for a REINFORCE Recommender\nSystem. In WSDM. ACM, 456â€“464.\n[8] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxi-\nang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPTâ€™s Capabilities in\nRecommender Systems. arXiv preprint arXiv:2305.02182 (2023).\n[9] Thomas Degris, Martha White, and Richard S. Sutton. 2012. Off-Policy Actor-\nCritic. CoRR abs/1205.4839 (2012).\n[10] Gabriel Dulac-Arnold, Richard Evans, Peter Sunehag, and Ben Coppin. 2015.\nReinforcement Learning in Large Discrete Action Spaces. CoRR abs/1512.07679\n(2015).\n[11] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang\nTang, and Qing Li. 2023. Recommender systems in the era of large language\nmodels (llms). arXiv preprint arXiv:2307.02046 (2023).\n[12] Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang,\nKun Gai, and Fei Sun. 2023. A Large Language Model Enhanced Conversational\nRecommender System. arXiv preprint arXiv:2308.06212 (2023).\n[13] Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau.\n2019. Benchmarking Batch Deep Reinforcement Learning Algorithms. CoRR\nabs/1910.01708 (2019).\n[14] Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-Policy Deep Reinforce-\nment Learning without Exploration. In ICML (Proceedings of Machine Learning\nResearch, Vol. 97) . PMLR, 2052â€“2062.\n[15] Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang,\nShiqi Wang, Zhong Zhang, and Xiangnan He. 2023. Alleviating Matthew Effect of\nOffline Reinforcement Learning in Interactive Recommendation. In SIGIR. ACM,\n238â€“248.\n[16] Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng\nChua. 2021. Advances and Challenges in Conversational Recommender Systems:\nA Survey. AI Open 2 (2021), 100â€“126.\n[17] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang\nLei, Biao Li, Yuan Zhang, and Peng Jiang. 2024. CIRS: Bursting Filter Bubbles\nby Counterfactual Interactive Recommender System. ACM Trans. Inf. Syst. 42, 1\n(2024), 14:1â€“14:27.\n[18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\nIJCAI. ijcai.org, 1725â€“1731.\n[19] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\nand Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for\nrecommender systems. arXiv preprint arXiv:2305.08845 (2023).\n[20] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Lan-\nguage Models as Zero-Shot Planners: Extracting Actionable Knowledge for Em-\nbodied Agents. In ICML (Proceedings of Machine Learning Research, Vol. 162) .\nPMLR, 9118â€“9147.\n[21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy\nZeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and\nBrian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning with\nLanguage Models. In CoRL (Proceedings of Machine Learning Research, Vol. 205) .\nPMLR, 1769â€“1782.\n[22] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. 2023.\nRecommender ai agent: Integrating large language models for interactive recom-\nmendations. arXiv preprint arXiv:2308.16505 (2023).\n[23] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,\nHeng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A Tractable\nDecomposition for Reinforcement Learning with Recommendation Sets. InIJCAI.\nijcai.org, 2592â€“2599.\n[24] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535â€“547.\n[25] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-\nommendation. In ICDM. IEEE Computer Society, 197â€“206.\n[26] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy,\nLichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand\nUser Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint\narXiv:2305.06474 (2023).\n[27] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. Conserva-\ntive Q-Learning for Offline Reinforcement Learning. In NeurIPS.\n[28] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brah-\nman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023. Swift-\nSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive\nTasks. CoRR abs/2305.17390 (2023).\n[29] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu\nZhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al . 2023. How Can Recom-\nmender Systems Benefit from Large Language Models: A Survey. arXiv preprint\narXiv:2306.05817 (2023).\n[30] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan,\nRuiming Tang, Yong Yu, and Weinan Zhang. 2023. ReLLa: Retrieval-enhanced\nLarge Language Models for Lifelong Sequential Behavior Comprehension in\nRecommendation. arXiv preprint arXiv:2308.11131 (2023).\n[31] Feng Liu, Ruiming Tang, Xutao Li, Yunming Ye, Haokun Chen, Huifeng Guo,\nand Yuzhou Zhang. 2018. Deep Reinforcement Learning based Recommendation\nwith Explicit User-Item Interactions Modeling. CoRR abs/1810.12027 (2018).\n[32] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is chatgpt a\ngood recommender? a preliminary study. arXiv preprint arXiv:2304.10149 (2023).\n[33] Volodymyr Mnih, AdriÃ  PuigdomÃ¨nech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous Methods for Deep Reinforcement Learning. In ICML (JMLR Workshop\nand Conference Proceedings, Vol. 48) . JMLR.org, 1928â€“1937.\n[34] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\nAntonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari\nwith Deep Reinforcement Learning. CoRR abs/1312.5602 (2013).\n[35] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019. Justifying Recommendations\nusing Distantly-Labeled Reviews and Fine-Grained Aspects. In EMNLP/IJCNLP\n(1). Association for Computational Linguistics, 188â€“197.\n[36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730â€“27744.\n[37] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius,\nand Stefanie Tellex. 2022. Planning with Large Language Models via Corrective\nRe-prompting. CoRR abs/2211.09935 (2022).\n[38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017).\n[39] Junjie Sheng, Zixiao Huang, Chuyun Shen, Wenhao Li, Yun Hua, Bo Jin,\nHongyuan Zha, and Xiangfeng Wang. 2023. Can language agents be alternatives\nto PPO? A Preliminary Empirical Study On OpenAI Gym. CoRR abs/2312.03290\n(2023).\n[40] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik\nNarasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal\nReinforcement Learning. arXiv:2303.11366 [cs.AI]\n[41] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023.\nAdaPlanner: Adaptive Planning from Feedback with Language Models. CoRR\nabs/2305.16653 (2023).\n[42] Hugo Touvron et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 (2023).\n[43] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah\nCho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. 2023. Rec-\nmind: Large language model powered agent for recommendation. arXiv preprint\narXiv:2308.14296 (2023).\n[44] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson,\nLisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for Long-Term User\nExperience in Recommender Systems. In KDD. ACM, 4100â€“4109.\n[45] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023. Describe,\nExplain, Plan and Select: Interactive Planning with Large Language Models\nEnables Open-World Multi-Task Agents. CoRR abs/2302.01560 (2023).\n[46] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias Springen-\nberg, Scott E. Reed, Bobak Shahriari, Noah Y. Siegel, Ã‡aglar GÃ¼lÃ§ehre, Nicolas\nHeess, and Nando de Freitas. 2020. Critic Regularized Regression. In NeurIPS.\n[47] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng\nWang, Dawei Yin, and Chao Huang. 2023. Llmrec: Large language models with\ngraph augmentation for recommendation. arXiv preprint arXiv:2311.00423 (2023).\n[48] Junda Wu, Zhihui Xie, Tong Yu, Handong Zhao, Ruiyi Zhang, and Shuai Li. 2022.\nDynamics-Aware Adaptation for Reinforcement Learning Based Cross-Domain\nInteractive Recommendation. In SIGIR. ACM, 290â€“300.\nLarge Language Models are Learnable Planners for Long-Term Recommendation SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n[49] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2023.\nExploring large language model for graph data understanding in online job\nrecommendations. arXiv preprint arXiv:2307.05722 (2023).\n[50] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al . 2023. A Survey on Large\nLanguage Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).\n[51] Qingyun Wu, Hongning Wang, Liangjie Hong, and Yue Shi. 2017. Returning\nis believing: Optimizing long-term user engagement in recommender systems.\nIn Proceedings of the 2017 ACM on Conference on Information and Knowledge\nManagement. 1927â€“1936.\n[52] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,\nWeinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recom-\nmendation with Knowledge Augmentation from Large Language Models. arXiv\npreprint arXiv:2306.10933 (2023).\n[53] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M. Jose. 2020.\nSelf-Supervised Reinforcement Learning for Recommender Systems. In SIGIR.\nACM, 931â€“940.\n[54] Shuyuan Xu, Juntao Tan, Zuohui Fu, Jianchao Ji, Shelby Heinecke, and Yongfeng\nZhang. 2022. Dynamic Causal Collaborative Filtering. InCIKM. ACM, 2301â€“2310.\n[55] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\nModels. In ICLR. OpenReview.net.\n[56] Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian\nChen, Weizhi Ma, and Min Zhang. 2024. EasyRL4Rec: A User-Friendly Code\nLibrary for Reinforcement Learning Based Recommender Systems.arXiv preprint\narXiv:2402.15164 (2024).\n[57] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu.\n2023. Large Language Model Is Semi-Parametric Reinforcement Learning Agent.\nCoRR abs/2306.07929 (2023).\n[58] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.\n2023. Is chatgpt fair for recommendation? evaluating fairness in large language\nmodel recommendation. arXiv preprint arXiv:2305.07609 (2023).\n[59] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong\nWen. 2023. Recommendation as instruction following: A large language model\nempowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023).\n[60] Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He.\n2023. Collm: Integrating collaborative embeddings into large language models\nfor recommendation. arXiv preprint arXiv:2310.19488 (2023).\n[61] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao\nHuang. 2023. ExpeL: LLM Agents Are Experiential Learners.CoRR abs/2308.10144\n(2023).\n[62] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang\nTang. 2018. Deep reinforcement learning for page-wise recommendations. In\nRecSys. ACM, 95â€“103.\n[63] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,\nXing Xie, and Zhenhui Li. 2018. DRN: A Deep Reinforcement Learning Framework\nfor News Recommendation. In WWW. ACM, 167â€“176.",
  "topic": "Term (time)",
  "concepts": [
    {
      "name": "Term (time)",
      "score": 0.8171056509017944
    },
    {
      "name": "Computer science",
      "score": 0.5272535085678101
    },
    {
      "name": "Physics",
      "score": 0.062402576208114624
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 18
}