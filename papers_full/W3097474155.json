{
    "title": "Word Autobots: Using Transformers for Word Association in the Game Codenames",
    "url": "https://openalex.org/W3097474155",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2436462397",
            "name": "Catalina Jaramillo",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2647606224",
            "name": "Megan Charity",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2890498790",
            "name": "Rodrigo Canaan",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A319365700",
            "name": "Julian Togelius",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2436462397",
            "name": "Catalina Jaramillo",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2647606224",
            "name": "Megan Charity",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2890498790",
            "name": "Rodrigo Canaan",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A319365700",
            "name": "Julian Togelius",
            "affiliations": [
                "New York University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6805520290",
        "https://openalex.org/W2582685648",
        "https://openalex.org/W1579838312",
        "https://openalex.org/W2982491964",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6768244288",
        "https://openalex.org/W1965667542",
        "https://openalex.org/W6749215612",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2792919371",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4320341798",
        "https://openalex.org/W2913781869",
        "https://openalex.org/W2970252402"
    ],
    "abstract": "Winning the social game Codenames involves combining cooperative and language understanding capabilities. We developed six cooperative bots designed to play the Codemaster and Guesser roles in the Codenames AI Competition and tested them using the provided framework and a round-robin tournament set. The bots are based on term frequency - inverse document frequency (TF-IDF), Naive-Bayes and GPT-2 Transformer word embedding. Additionally, Transformer-based bots were assessed and compared with the concatenation of word2vec and GloVe baseline bot developed by Codenames AI Competition creators. Results from this Transformer implementation rivals the concatenated bot in terms of win rates and guess precision and outperforms it in terms of minimum and average turns taken to win the game and training data load time. Additionally, in an initial evaluation performed with 10 human players, the Transformer agent performed slightly better than the baseline as Codemaster, but worse as a Guesser.",
    "full_text": "Proceedings of the Sixteenth AAAI Conference on Artiﬁcial Intelligence and Interactive Digital Entertainment (AIIDE-20)\nWord Autobots:\nUsing Transformers for Word Association in the Game Codenames\nCatalina M. Jaramillo, Megan Charity, Rodrigo Canaan, Julian Togelius\nGame Innovation Lab, New Y ork University\n370 Jay St, Brooklyn, NY 11201, USA\n{cmj383, mlc761, rodrigo.canaan}@nyu.edu, julian@togelius.com\nAbstract\nWinning the social game Codenames involves combining co-\noperative and language understanding capabilities. We de-\nveloped six cooperative bots designed to play the Codemas-\nter and Guesser roles in the Codenames AI Competition and\ntested them using the provided framework and a round-robin\ntournament set. The bots are based on term frequency - in-\nverse document frequency (TF-IDF), Naive-Bayes and GPT-\n2 Transformer word embedding. Additionally, Transformer-\nbased bots were assessed and compared with the concatena-\ntion of word2vec and GloV e baseline bot developed by Co-\ndenames AI Competition creators. Results from this Trans-\nformer implementation rivals the concatenated bot in terms\nof win rates and guess precision and outperforms it in terms\nof minimum and average turns taken to win the game and\ntraining data load time. Additionally, in an initial evaluation\nperformed with 10 human players, the Transformer agent per-\nformed slightly better than the baseline as Codemaster, but\nworse as a Guesser.\nIntroduction\nMost AI research using games as testbeds use either classi-\ncal board games such as Chess or Go, or video games, typi-\ncally older ones (Y annakakis and Togelius 2018). But differ-\nent games pose different challenges for humans and agents.\nBuilding agents for games which are less explored is inter-\nesting for multiple reasons, for example as a way to better\nunderstand the design of these games and which cognitive\nchallenges they pose, to enable playtesting, balancing and\ncontent generation, or to create agents that are interesting to\nplay with.\nThis paper explores bots for the social board game Code-\nnames, where one of the main cognitive challenges is pre-\ndicting how players on your team make word associations.\nWhile we are not the very ﬁrst to build a bot for Codenames,\nwe are the ﬁrst to present a bot based on modern deep learn-\ning methods, we conduct a thorough empirical comparison,\nand we perform a user study with human players.\nCodenames has two characteristics that are individu-\nally appealing and which together distinguish it from other\ngame-based AI competitions and benchmarks:\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n• Natural language based. Many promising AI applica-\ntions also hinge on language models, such as translation,\nreading comprehension, and natural language inference\n(Brown et al. 2020). Different environments, like LIGHT\n(Learning in Interactive Games with Humans and Text)\nplatform (Urbanek et al. 2019) and AI Dungeon (Wal-\nton 2019), focus on AI language exploration and offer\nopportunities to further develop related applications. Co-\ndenames, as played between humans, uses language in\na very rich way (homonyms, antonyms, rhymes, popular\nculture references, etc.) and relies in making inferences\nand can also be used with this objective.\n• Cooperative in nature. Many real-world AI applications\ninvolve cooperation, not competition, with humans. Most\ngame-based AI benchmarks are competitive, and even\nsome of those that involve cooperation (e.g. GVGAI\nmulti-agent (Gaina, P ´erez-Li´ebana, and Lucas 2016),\nPommerman (Resnick et al. 2018)) have had success-\nful entries that largely ignore the partner agent and sim-\nply attempt to optimize one’s own behavior. In Code-\nnames, partner behavior is critical, as evidenced by the\nfact that codemaster-guesser pairs using the same mod-\neling achieve much higher scores and win rates than\nmixed pairs. This is similar to what is observed in Han-\nabi (Walton-Rivers, Williams, and Bartle 2019; Bard et\nal. 2020), where although agents with shared conventions\nachieve near-perfect scores, cooperation with unknown\nagents is still an open problem.\nCodenames, the Board Game\nFigure 1: An example Codenames game board (right) and\nkey (left). Each word is highlighted in the image (but not in\nthe game) with the key’s corresponding color.\n231\nCodenames (Chvatil 2015) is a board game where the ob-\njective is to identify a team’s secret agents (represented by\nwords on the board) in the minimum number of turns pos-\nsible. Also, it is important to avoid the words that represent\nbystanders (losing the turn), opposite team’s agents (losing\nthe turn and adding a guess to the other team’s count) and the\nassassin (losing the game). Each team consists of one Code-\nmaster and one or more Guessers. The role of the Codemas-\nter is to provide a clue word and number of secret agents re-\nlated with that word, based on the Codemaster’s knowledge\nof the board words’ assignment. The role of the Guessers\nis to pick the own team’s words from the board using the\nprovided clue. This is a cooperative game, where the Code-\nmaster of a team, using word association, picks a clue for the\nGuessers to identify as many of the own agents, from the 25\noptions in the board, while keeping the Guessers away from\nthe wrong key names.\nFor the Codenames AI Competition framework, the sub-\nset of words corresponding to our team are the red words.\nThe remaining words on the board are identiﬁed as bad\nwords and are split in a subset of blue words (correspond-\ning to the opposite team), civilian words (representing by-\nstanders) and one word labeled as assassin.\nCodenames, the AI Competition\nThe Codenames AI Competition Framework (Summerville\net al. ) tests AI’s understanding of human language and com-\nmunication capacity using the game Codenames as testbed.\nIt is a simpliﬁed version with a single team formed by\na Codemaster and a Guesser. Participant bots are paired at\nrandom trying to complete the game in the least moves, with\na 30 seconds limit per turn. For each turn, the Codemaster\ngives a clue -a word and a number of guesses (n); then the\nGuesser takes one or more guesses until (whichever comes\nﬁrst): the guesser passes the turn, n+1 guesses have been\nmade, or a mistake has been made (a non “red” word was\nguessed). The penalty for guessing a bad word is a lost turn.\nScoring is based on the number of turns (lower score is\nbetter), and a lost round gets 25 points; following the ap-\nproach used by Kim et al. (2019), the average number of\nturns is calculated using only won games. Competition base-\nline bots use word2vec (Mikolov et al. 2013), GloV e (Pen-\nnington, Socher, and Manning 2014), and WordNet models\n(Fellbaum 2012).\nRelated Work\nPrevious research with the artiﬁcial playing of Codenames\nby Kim et al. (2019) developed and compared performance\nfor both Codemaster and Guesser bots using different word\nembedding models (WordNet, word2vec and GloV e), and\ndifferent distance measures in a round-robin tournament.\nThe WordNet based bot was excluded from the original\nstudy because of its poor performance. For the remaining\nbots, results were mixed, showing better performance for\nCodemaster agents that were paired with similar approach\nGuesser, and for agents that used a concatenated word2vec\nand GloV e strategy. Their word2vec strategy looked to cre-\nate word vectors that predict both words and contexts, while\ntheir GloV e strategy looked to ﬁnd co-occurences between\nwords through weightings.\nOutside of the Codenames challenge, the concept of\nidentifying word relatedness and word contextualizing is a\nlargely explored area of natural language processing. The\nTF-IDF algorithm was used by Wu et al. (2008) for making\nword query relevancy decisions based on both document-\nwide and local relevance. They developed a context-based\nranking formula to sort probabilities accordingly. Jurafsky\nand Martin (2014) demonstrate that the use of a multi-\nlabel classiﬁcation Naive-Bayes method helps with word\nsense disambiguation (WSD) and determine word similar-\nities. OpenAI’s GPT-2 Transformer (Openai 2019) is in-\ntended for sequential word prediction based on unsupervised\nlearning: it provides a pretrained word embedding model\nthat comprises words’ meanings by considering the contex-\ntual relationships among words (Radford et al. 2019).\nMethods\nAll of the bots follow the baseline algorithm developed for\nthe Codenames AI framework that is shown in Kim et al.\n(2019). From there, the following word association algo-\nrithms are applied to enhance the bots’ performances within\nthe game.\nTerm Frequency - Inverse Document Frequency\nalgorithm\nThe TF-IDF algorithm determines the importance of a word\nbased on its frequency in a document compared to an entire\ncorpus of documents. For our experiment, our corpuses are\nWikipedia summaries retrieved from the Wikipedia Python\nlibrary and dictionary deﬁnitions retrieved from the PyDict\nlibrary for the words on the board. The bot preprocesses the\nembeddings for the words by retrieving these summaries,\nremoving stop words, and tokenizing the words within the\nsummaries and deﬁnitions. Using a bag-of-words model, it\ncalculates TF-IDF scores for each word in each summary.\nThe Codemaster bot uses the TF-IDF scores to select a\ncorpus document that contains the highest frequency of red\nwords. The bot selects a word with high term frequency\nscore from this document to use as clue word and the number\nof red words found within the document as the clue number.\nThe Guesser bot uses the same TF-IDF scores to do a re-\nverse search within the set of corpuses to ﬁnd the document\ncontaining the highest term frequency of the clue word and\nselects any board words found; ranking them based on their\nterm frequency times inverse document frequency score.\nNaive-Bayes Algorithm\nThe Naive-Bayes algorithm determines the probability of a\nword belonging to a particular class using Bayesian infer-\nence. It requires a corpus data set for the words, and class\nsets to categorize them into. Like the TF-IDF algorithm, we\nused tokenized Wikipedia summaries and dictionary deﬁni-\ntions as the corpus set for the board words. For the ”classes”\nrepresentation, we used a list of 118 generic word categories\n(i.e. ”animals”, ”countries”, ”food”, etc) and retrieved their\nsummaries and deﬁnitions to be used as training data.\n232\nThe Codemaster bot uses Laplace smoothing in order to\ndetermine the probabilities of each red word belonging to\nthese category ”classes.” When adding the probabilities of\nwords in each class, the category with the highest probabil-\nity is used as the clue word and the number of red words\nwith a probability higher than a preset threshold value - the\nminimum possible Laplace distance - are used as the guess\nnumber.\nThe Guesser bot uses the same Laplace smoothing algo-\nrithm and dataset to determine the probability of each board\nword belonging to the speciﬁed class clue word. It ranks\neach board word based on their probabilistic Bayesian score\ncalculated.\nTransformer Algorithm\nThe transformer-based bot uses the pretrained GPT-2 word\nembedding model for the representation of the words on the\nboard. GPT-2 uses stacks of transformer decoder blocks to\nbuild a highly contextualized Language Model designed to\npredict the next word in a sequence. (Radford et al. 2019).\nFor the Codenames (Chvatil 2015) setup, the input consists\nof a set of single words, hence a context based ﬁne tuning is\nnot considered since a pretrained embedding sufﬁces to re-\nﬂect a general meaning for each word. Also, because a static\nrepresentation was needed, only the ﬁrst layer of the em-\nbedding was used. In order to minimize the curse of dimen-\nsionality, the small OpenAI GPT-2 English model (with a\nvocabulary size of 50257 and 768 dimensions) was selected\n(Openai 2019). Cosine Similarity metric was used to calcu-\nlate distances between the words.\nIn the Codemaster bot, a subset of red words within a dis-\ntance threshold is selected and a centroid that represents it\nis calculated. In an initial approach, the set of bad words -\nwords with either a civilian label, an opponent team label, or\nan Assassin label - was also included in the centroid calcu-\nlation (using negative weights, varying with the importance\nof the risk); the resulting centroid was markedly displaced\nfrom the subset neighborhood, resulting in a meaningless\nword search. Using the centroid and the vocabulary in the\npretrained model, K nearest neighbors (KNN) was applied\nto ﬁnd a list of recommended words. These words are lo-\ncated in the subset vicinity, offering a spatial, representation\nand meaning closeness. For the weighted version of the bot,\nthis recommended list is sorted, using the relationship be-\ntween the similarity with the red words and the similarity\nwith the bad words to minimize the associated risk.\nThe Guesser bot uses KNN to search for K (number pro-\nvided by the Codemaster) words in the board that are closer\nto the clue based on the pretrained GPT-2 embedding.\nBad Word Weighting\nTo extend upon each of these base algorithms, we imple-\nmented a weighting system for each of the words found on\nthe board. Bad words are weighted based on their implicit\nrisk factor: -1 for civilians, -2 for blue words and -3 for the\nassassin. These weights are applied by the Codemaster bots\nto avoid selecting these words when looking for a clue word.\nExperiment Setup\nThe Codenames AI Competition Framework (Summerville\net al. ) is a single-team version of the game, where a Code-\nmaster and a Guesser play together to ﬁnd the red team’s\nwords in the minimum number of turns.\nTo assess the performance of the bots, a set of round-robin\ntournaments was used, matching pairs of Codemaster and\nGuessers bots and playing 30 games for each pair, with a\nﬁxed set of boards.\nIn a ﬁrst tournament the unweighted version of our three\nCodemasters were used. A second contest was run using the\nweighted for bad words versions.\nThe team wins the game when it ﬂips all red cards before\nﬂipping either the assassin or all the blue cards. Each time\nthat a bad word is ﬂipped, the red team losses the turn and the\nturn count in increased by one. The metrics used in the eval-\nuation are the number of turns to win (Turns), both minimum\nand average -calculated for wining cases; the percentage of\ngames won per each bot (Win Rate); and the number of good\n(red) and bad (either blue, civilian or assassin) words ﬂipped\nduring each game.\nIn a second stage of the experiment, the GPT-2 Trans-\nformer based bots were matched with the best perform-\ning bot found by Kim et al. (2019) built by concatenating\nword2vec and GloV e embeddings. The same metrics were\nobserved and compared to evaluate the agents.\nTo understand how human intuitive are the clues and\nguessing of the Transformer and concatenated w2v+GloV e\n(w2v+GloV e) bots, a study with 10 human players was per-\nformed. Each participant played a total of 4 games, one with\neach model as both Codemaster and Guesser, for a total of\n40 games.\nResults\nTable 1: Codenames Bots\nLabel Bot\nT GPT-2 Transformer\nTF TF-IDF\nNB Naive-Bayes\nWG word2vec+GloV e concatenation\nH Human player\nFor the graphs shown in this section, the label scheme de-\nscribed in Table 1 is used. For each pair of graph labels, the\nﬁrst listed algorithm represents the Codemaster used and the\nsecond listed algorithm represents the Guesser used. For ex-\nample, the label ”TF-T” represents a TF-IDF Codemaster\npaired with a GPT-2 Transformer Guesser.\nA link to the code repository for this project can be found\nin: https://github.com/MasterMilkX/codenames\nautobots.\nWin Rate\nFigure 2 shows the win rates for each bot pair - with both the\nunweighted Codemasters examining only the good words\nand the enhanced Codemasters applying weights to the bad\nwords. The Transformer self-pair had the highest win rate\n233\npercentage in both cases with 0.63 for the unweighted and\n0.9 for the weighted bots. When considering mixed pairs, the\nTF-IDF Codemaster paired with the Transformer Guesser\nreached the best rate, with 0.33 for the unweighted and 0.23\nfor the weighted bots. Naive-Bayes got the poorest results\nboth self paired and in mixed pairs.\n(a) Unweighted Codemasters\n(b) Weighted\nFigure 2: Win Rate comparison for a good word exclusive\nCodemaster and a weighted bad word Codemaster\nAverage vs. Minimum\nFigure 3 shows the minimum number of turns taken and the\naverage number of turns taken to win for each bot pair. The\nTransformer self-pair had the smallest average number of\nturns at 8.58 turns with unweighted Codemasters and 7.96\nturns with the weighted Codemasters. The TF-IDF self-pair\ntied with the Transformer self-pair for the minimum number\nof turns taken at 5 turns to win using the unweighted Code-\nmasters, and the Transformer self-pair had a minimum of 4\nturns with the weighted word Codemasters. Since there are\na total of 8 red words per game that must be found, these\nresults show that the bots are able to ﬁnd more red words us-\ning fewer turns and use clue words that associate to multiple\nred words on the board.\nAgain the Naive-Bayes bots perform poorly when paired\nwith the other bots - requiring twice as many turns to guess\nor not being able to win at all.\nBoard Words Left on Completion\nFigure 4 shows the average number of good and bad cards\nﬂipped on the board on completion (win or lose) for each\ngame.\n(a) Unweighted Codemasters\n(b) Weighted Codemasters\nFigure 3: Average and Minimum Turns to win comparison\nfor a good word exclusive Codemaster and a weighted bad\nword Codemaster\nThe Transformer self-pair is the only pair to have a higher\naverage of good words ﬂipped than bad words - meaning\nthe Transformer bots had a higher precision; the Codemaster\nfor selecting clue words closely related to the red words and\nthe Guesser for correctly interpreting these clue words to be\nassociated with red words more than bad words.\nThe TF-IDF self-pair had the closest ratio for average\ngood words to average bad words ﬂipped. Since there are\ntwice as many bad words on the board than good words, it\ncan be noted that this self-pair still performs well at select-\ning good words over bad words. The Naive-Bayes Codemas-\nter caused the highest averages for selecting bad words than\ngood words - resulting in the least amount of precision for all\nthe pairings and having only half of the good words revealed\nby the end of the game.\nWith the weighted Codemasters, bad words that were\nﬂipped were reduced for all pairings, meanwhile the number\nof good words that were ﬂipped remained the same except\nfor the Transformer self-paired bots.\nHuman Playability\nTransformer bot has a better performance as Codemaster\nwhen paired with a human Guesser: wining rate is 0.6 vs.0.4;\nthe average number of good words ﬂipped is 6.7 vs. 5.9,\nwhile the average number of bad words ﬂipped is 5.9 vs.\n6.1; additionally, the average number of turns is 11 vs.\n234\n(a) Unweighted Codemaster\n(b) Weighted Codemaster\nFigure 4: Average Word Flip comparison for a good word\nexclusive Codemaster and a weighted bad word Codemaster\n12.25, while the minimum number of turns to win is 6 for\nboth. In the case of human Codemaster, the concatenated\nw2v+GloV e Guesser bot outperformed the Transformer one:\nthe wining rate is 0.8 vs. 0.3; the average number of good\nwords ﬂipped is 7.1 vs. 6; and the average number of bad\nwords ﬂipped was 4.2 vs. 7.4. Both the minimum number\nof turns and the average number of turns to win were lower\nwith 7 vs. 11 and 10.75 vs. 12.67 respectively. Qualitative\nﬁndings from the human playability test included:\n• Bots provide a best clue based on the board space and can\nkeep repeating the same clue in cases where the person\nwas not able to guess properly.\n• Humans rely on abstract concepts and subtle relationships\nthat non-human agents have a harder time to identify.\nWord2vec+GloVe Comparison\nWin Rate Figure 5 shows the win rates for each bot\npairing. The self-paired w2v+GloV e has a 100% win rate\n(Kim et al. 2019). When paired with other bots, however,\nit reaches a win rate less than that of the Transformer\nself-paired bot. When the w2v+GloV e concatenation bot is\npaired with a Transformer bot, it performs equally as well as\nboth a Codemaster and a Guesser with a 60% win rate. When\npaired with the TF-IDF bots and the Naive-Bayes bots it has\n30% and below for a win rate.\nHowever, some of the win rates decrease when the W2V\nGuesser is paired with Codemasters using weighted bad\nwords compared to the Codemasters not examing the bad\nwords. The Weighted Transformer Codemaster results in a\n57% win rate, the Weighted TF-IDF Codemasters results in\na 23% win rate, and the Weighted Naive-Bayes Codemaster\nwin rate remains unchanged at 17%.\n(a) Unweighted Codemasters\n(b) Weighted Codemasters\nFigure 5: Win Rate comparison for the w2v+GloV e concate-\nnation bot with Unweighted and Weighted Codemasters\nMinimum and Average Turns to Win The unweighted\nTransformer self-pair bots performed better than the\nw2v+GloV e concatenated bots by winning the game in fewer\nturns - see Figure 6 with a lower minimum turn score of 5 vs.\n7. When paired with the weighted Codemasters, the average\nturns taken decreased for both the Transformer and TF-IDF\nbots. The average number of turns decreased to 7.6 turns\nfor the Transformer Codemaster - less than the w2v+GloV e\nself-pairing with 7.93 in average.\nIt can be observed that the weighted Transformer bot was\nable to give more clues within a single turn and search for the\nword association and similarities between more red words\nbetter than the w2v+GloV e bot self-pairing.\nAverage Words Flipped Figure 7 reveals that the\nw2v+GloV e self-pairing has perfect precision in determin-\ning good words from bad words, as mentioned by Kim et al.\n(2019). However, when paired with other bots its precision\nfalls, guessing more bad words on average than the Trans-\nformer self-pairing. It guesses the most good words when\npaired with the Transformer bot, either as Codemaster or\n235\n(a) Unweighted Codemasters\n(b) Weighted Codemasters\nFigure 6: Minimum and Average Turns comparison for\nthe w2v+GloV e concatenation bot with Unweighted and\nWeighted Codemasters\n(a) Unweighted Codemasters\n(b) Weighted Codemasters\nFigure 7: Average Words Flipped comparison for the\nw2v+GloV e concatenation bot with Unweighted and\nWeighted Codemasters\nGuesser. As Guesser, it is able to correctly guess at least half\nof the red words on average. When paired with the Trans-\nformer as a Guesser, it picks just as many red words as bad\nwords.\nLoad Time\nThe Transformer algorithm had the shortest average load\ntime (from command run to third clue output), 31.25 s, fol-\nlowed by Naive-Bayes with 59.75 s and TF-IDF with 60.25\ns. Although the w2v+GloV e algorithm has the highest win\nrate and creates the most precise clues and guesses it takes\nover 5 times as long to load (163.75).\nConclusion and Future Work\nIn this experiment, we challenged the Codenames frame-\nwork bots w2v+GloV e with our own bots using alternative\nalgorithms . We used two non-vector based approaches, TF-\nIDF and Naive-Bayes. Both failed to achieve good results.\nWe also created alternative word embedding search bots us-\ning GPT-2 and achieved competitive results in terms of win\nrate and average turns taken and lower minimum turns taken\nand load time. Key takeaways include:\n• When bots are self-paired (based on the same method),\nthe average performance is better.\n• Bots that consider the risk of choosing a bad word in the\nselection of the clue, provide better results compared with\nthe unweighted version of the bots.\n• The weighted transformer pair outperformed the other\nbots developed by the authors in all the metrics: it triples\nthe wining rate of the next pair, has a better minimum\nnumber of turns to win, and is the only one that reached\nan average number of good words higher than the number\nof bad words ﬂipped during the game.\n• The transformer bot is able to combine several red words\nin one single clue, reducing the number of turns needed to\nplay the game.\nWhen compared the w2v+GloV e bot, the transformer has:\n• Lower average and minimum number of turns to win.\n• Lower average good words ﬂipped.\n• A 90% wining rate (compared with 100% for the\nw2v+GloV e bot). But the weighted transformer pair has\na considerably lower computational cost.\nIn conclusion, the transformer bot offers a promising perfor-\nmance, and further work can be done aimed to keep improv-\ning it. Some suggested work includes:\n• Compare different threshold values for the selection of red\nwords subset used in searching for recommended words.\n• Compare results with different weights for the bad words\nand different threshold for the red words subset selection.\nAcknowledgments\nRodrigo Canaan grate-fully acknowledges the ﬁnancial sup-\nport from Honda Research Institute Europe (HRI-EU).\n236\nReferences\nBard, N.; Foerster, J. N.; Chandar, S.; Burch, N.; Lanctot, M.;\nSong, H. F.; Parisotto, E.; Dumoulin, V .; Moitra, S.; Hughes, E.;\net al. 2020. The hanabi challenge: A new frontier for ai research.\nArtiﬁcial Intelligence 280:103216.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P .; Neelakantan, A.; Shyam, P .; Sastry, G.; Askell, A.;\net al. 2020. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165.\nChvatil, V . 2015. Codenames. Board Game.\nFellbaum, C. 2012. Wordnet. The encyclopedia of applied lin-\nguistics.\nGaina, R. D.; P´erez-Li´ebana, D.; and Lucas, S. M. 2016. General\nvideo game for 2 players: Framework and competition. In2016\n8th Computer Science and Electronic Engineering (CEEC), 186–\n191. IEEE.\nJurafsky, D., and Martin, J. H. 2014.Speech and language pro-\ncessing: an introduction to natural language processing, compu-\ntational linguistics, and speech recognition. Dorling Kindersley\nPvt, Ltd.\nKim, A.; Ruzmaykin, M.; Truong, A.; and Summerville, A.\n2019. Cooperation and codenames: Understanding natural lan-\nguage processing via codenames. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence and Interactive Digital En-\ntertainment, volume 15, 160–166.\nMikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efﬁcient\nestimation of word representations in vector space.arXiv preprint\narXiv:1301.3781.\nOpenai. 2019. openai/gpt-2-output-dataset.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal vectors for word representation. InEmpirical Methods in\nNatural Language Processing (EMNLP), 1532–1543.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised multitask\nlearners. OpenAI Blog 1(8):9.\nResnick, C.; Eldridge, W.; Ha, D.; Britz, D.; Foerster, J.; Togelius,\nJ.; Cho, K.; and Bruna, J. 2018. Pommerman: A multi-agent\nplayground. arXiv preprint arXiv:1809.07124.\nSummerville, A.; Kim, A.; Ruzmaykin, M.; and Truong, A. The\ncodenames ai competition. https://sites.google.com/view/the-\ncodenames-ai-competition.\nUrbanek, J.; Fan, A.; Karamcheti, S.; Jain, S.; Humeau, S.; Dinan,\nE.; Rocktaschel, T.; Kiela, D.; Szlam, A.; and Weston, J. 2019.\nLearning to speak and act in a fantasy text adventure game.arXiv\npreprint arXiv:1903.03094.\nWalton-Rivers, J.; Williams, P . R.; and Bartle, R. 2019. The 2018\nhanabi competition. In2019 IEEE Conference on Games (CoG).\nIEEE.\nWalton, N. 2019. Ai dungeon. = https://aidungeon.io/.\nWu, H. C.; Luk, R. W. P .; Wong, K. F.; and Kwok, K. L. 2008.\nInterpreting tf-idf term weights as making relevance decisions.\nACM Transactions on Information Systems26(3):1–37.\nY annakakis, G. N., and Togelius, J. 2018.Artiﬁcial intelligence\nand games. Springer.\n237"
}