{
    "title": "Transformer transfer learning emotion detection model: synchronizing socially agreed and self-reported emotions in big data",
    "url": "https://openalex.org/W4318067823",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5066686272",
            "name": "Sanghyub John Lee",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5085026213",
            "name": "JongYoon Lim",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5091324165",
            "name": "Leonard J. Paas",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5057419366",
            "name": "Ho Seok Ahn",
            "affiliations": [
                "University of Auckland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2741447225",
        "https://openalex.org/W3022943557",
        "https://openalex.org/W1886441443",
        "https://openalex.org/W2536012551",
        "https://openalex.org/W2589711889",
        "https://openalex.org/W2955429306",
        "https://openalex.org/W3034323190",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W2158215225",
        "https://openalex.org/W2149820118",
        "https://openalex.org/W2182096631",
        "https://openalex.org/W2150403218",
        "https://openalex.org/W4213095704",
        "https://openalex.org/W3152850577",
        "https://openalex.org/W6600424091",
        "https://openalex.org/W2117967450",
        "https://openalex.org/W2805744755",
        "https://openalex.org/W2963177779",
        "https://openalex.org/W1569507287",
        "https://openalex.org/W2347127863",
        "https://openalex.org/W2796430037",
        "https://openalex.org/W2465157137",
        "https://openalex.org/W2891575196",
        "https://openalex.org/W4296976275",
        "https://openalex.org/W2758435862",
        "https://openalex.org/W2791393728",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3069727413",
        "https://openalex.org/W2135013680",
        "https://openalex.org/W2466778245",
        "https://openalex.org/W1971222444",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3117879109",
        "https://openalex.org/W3155638556",
        "https://openalex.org/W2952993453"
    ],
    "abstract": "Abstract Tactics to determine the emotions of authors of texts such as Twitter messages often rely on multiple annotators who label relatively small data sets of text passages. An alternative method gathers large text databases that contain the authors’ self-reported emotions, to which artificial intelligence, machine learning, and natural language processing tools can be applied. Both approaches have strength and weaknesses. Emotions evaluated by a few human annotators are susceptible to idiosyncratic biases that reflect the characteristics of the annotators. But models based on large, self-reported emotion data sets may overlook subtle, social emotions that human annotators can recognize. In seeking to establish a means to train emotion detection models so that they can achieve good performance in different contexts, the current study proposes a novel transformer transfer learning approach that parallels human development stages: (1) detect emotions reported by the texts’ authors and (2) synchronize the model with social emotions identified in annotator-rated emotion data sets. The analysis, based on a large, novel, self-reported emotion data set ( n = 3,654,544) and applied to 10 previously published data sets, shows that the transfer learning emotion model achieves relatively strong performance.",
    "full_text": "ORIGINAL ARTICLE\nTransformer transfer learning emotion detection model: synchronizing\nsocially agreed and self-reported emotions in big data\nSanghyub John Lee1 • JongYoon Lim2 • Leo Paas1 • Ho Seok Ahn2\nReceived: 11 May 2022 / Accepted: 6 January 2023 / Published online:26 January 2023\n/C211 The Author(s) 2023\nAbstract\nTactics to determine the emotions of authors of texts such as Twitter messages often rely on multiple annotators who label\nrelatively small data sets of text passages. An alternative method gathers large text databases that contain the authors’ self-\nreported emotions, to which artiﬁcial intelligence, machine learning, and natural language processing tools can be applied.\nBoth approaches have strength and weaknesses. Emotions evaluated by a few human annotators are susceptible to\nidiosyncratic biases that reﬂect the characteristics of the annotators. But models based on large, self-reported emotion data\nsets may overlook subtle, social emotions that human annotators can recognize. In seeking to establish a means to train\nemotion detection models so that they can achieve good performance in different contexts, the current study proposes a\nnovel transformer transfer learning approach that parallels human development stages: (1) detect emotions reported by the\ntexts’ authors and (2) synchronize the model with social emotions identiﬁed in annotator-rated emotion data sets. The\nanalysis, based on a large, novel, self-reported emotion data set ( n = 3,654,544) and applied to 10 previously published\ndata sets, shows that the transfer learning emotion model achieves relatively strong performance.\nKeywords Emotion data sets /C1 Emotion detection model /C1 Transformer-based language model /C1 Constructed emotion theory\n1 Introduction\nBillions of social networking service users engage in their\nareas of interest by sharing their opinions and information,\nmainly as text. Predicting the emotions they express in\nthese texts is critical for both researchers and businesses\n[31]; consumer emotions expressed in online restaurant\nreviews [37] or tweets about COVID-19 [ 18] for example\nmay provide valuable insights for commercial ﬁrms and\npublic policy makers. Yet detecting such expressions of\nhuman emotions is challenging, particularly when the\nanalyzed data only include text, not facial expressions or\nother nonverbal information [ 30].\nOne option is to task human annotators with generating\nlabels of emotions and categorizing text passages\n[2, 8, 9, 14, 26, 27, 29, 34]. This stream of research reﬂects\na classical view of emotion theory [ 35], which postulates\nthat categories such as fear and joy have universal bio-\nlogical ﬁngerprints, inherited by humans through evolution,\nthat get aroused by certain situations. Therefore, annotators\nshould be able to detect emotions expressed in texts\nauthored by others [ 8, 26, 27, 32]. Yet empirical studies\nalso reveal that the number and categories of emotions vary\nacross individuals, depending their age [ 11], gender [ 23],\nsocial contexts, and culture [ 16, 24]. Accordingly, anno-\ntators may not be able to judge every emotion expressed by\nothers in all situations, especially those that the annotators\nhave not experienced. In addition, personal experiences are\ninherently subjective, reﬂecting the effects of experiential\nblindness and a person’s current state of mind [ 5]. For\n& Sanghyub John Lee\nsanghyub.lee@auckland.ac.nz\nJongYoon Lim\njy.lim@auckland.ac.nz\nLeo Paas\nleo.paas@auckland.ac.nz\nHo Seok Ahn\nhs.ahn@auckland.ac.nz\n1 Marketing Department, University of Auckland Business\nSchool, Auckland 1142, New Zealand\n2 CARES, Department of Electrical, Computer and Software\nEngineering, University of Auckland, Auckland 1142, New\nZealand\n123\nNeural Computing and Applications (2023) 35:10945–10956\nhttps://doi.org/10.1007/s00521-023-08276-8(0123456789().,-volV)(0123456789().,- volV)\nexample, one and the same person can be evaluated dif-\nferently by others, depending on those evaluators’ states of\nmind. Men who have just crossed a high suspension bridge\nperceive women as more attractive than men who have not\ncrossed the bridge, because they confuse their fear with\nattraction [ 12]. Wang et al. [ 41] caution that annotators’\nassessments of emotions can be subjective and varied too.\nThus an alternative approach analyzes larger data sets,\nto which emotion labels have been added by the texts’\nauthors [25, 32, 33], which we refer to as self-rated labels.\nAccording to the theory of constructed emotions [ 6], the\nhuman brain uses past experiences, categorized as con-\ncepts, to guide people’s actions and give meaning to their\nobservations of their surroundings. Emotions are part of the\nmeaning attributed by and expressed in texts. Data sets\nwith self-reported emotions are relatively easy to collect\nand can train artiﬁcial intelligence (AI), machine learning\n(ML), and natural language processing (NLP) algorithms\n[25, 32, 33]. For example, tweets by authors who label\nthem with emotion hashtags, such as #joy, #anger, and\n#sadness, can be submitted to AI, ML, and NLP models,\nwhich produce general emotion detection rules. These rules\nthen can be applied to other tweets that contain emotional\ncontent but do not feature labels created by the text’s\nauthor or another annotator.\nSuch ML-based approaches suffer their own limitations;\nwhen authors report their own emotions, they might over-\nlook some socially constructed emotions that annotators\nmight be able to identify. Furthermore, models based on\nself-reported emotion data sets need sufﬁcient amounts of\ntext and labels for training, but what those precise amounts\nare remains unclear. In particular, ML models do not have\nprevious experience (as annotators might), so they may\nrequire more data. Considering these strengths, weak-\nnesses, and gaps, we ask,\nRQ1. Are models based on (1) annotator-labeled or (2)\nself-reported emotions more accurate in detecting emo-\ntions in texts with (a) self-reported or (b) annotated\nemotion labels?\nRQ2. Does the size of the data set featuring self-reported\nemotions affect the classiﬁcation accuracy of ML-based\nemotion detection models?\nRather than either self-reported emotions or annotated\nemotion labels, we propose a novel, combined approach,\nthe transformer transfer learning (TTL) model. In this\napproach, the transformer model gets trained in a series of\nstages that mimic human development. Over the course of\ntheir social-emotional development [ 22], children ﬁrst\nidentify their own emotions and then learn to synchronize\ntheir emotions with those of others, such as in response to\nrelationship issues [ 38]. The proposed TTL approach\nreplicates these two stages by ﬁrst training a transformer\nmodel, such as the RoBERTa-large model [ 21], on a large,\nself-reported emotion data set, and then on a relatively\nsmall, socially agreed emotion data set with annotator-\ngenerated labels.\nWith this novel two-step approach, we aim to improve\nforecasting accuracy in terms of predicting emotions across\ndifferent types of data sets, with self-reported or annotator-\nlabeled emotions. To the best of our knowledge, this study\nis the ﬁrst to train models sequentially on self-reported\nemotion data, followed by data gathered from annotator\nlabels. Previous transformer models trained on large\nemotion data sets achieve higher classiﬁcation accuracy\nthan those trained on small data sets only [ 9], though\nprevious research focuses on differences in size of the data\nsets, not the types of emotion data. We also note a previous\nstudy [ 43] that offers similar ﬁndings for chemical reac-\ntions, though unrelated to emotion detection.\nTo conﬁrm whether the newly introduced TTL approach\nachieves greater emotion detection accuracy than alterna-\ntive modeling approaches, including non-sequentially\ntrained models that also involve (1) annotator-rated emo-\ntion data, (2) self-reported emotion data, and (3) both\ntypes, we investigate the following question:\nRQ3. Does the TTL approach achieve higher classiﬁca-\ntion accuracy than models that have been non-sequen-\ntially trained on annotator-rated or self-reported emotion\ndata?\nIn search of answers for these questions, we gather a\nnovel data set with 3,654,544 tweets that include emotion\nhashtags, inserted by their authors. We analyze this large\ndata set, along with 10 previously published data sets that\ncontain text with emotion labels, whether provided by the\ntexts’ authors or annotators. The new data set can be\nleveraged for further research; it represents one of the\nlargest data sets of tweets containing self-reported emotion\nhashtags ( n = 3,654,544), posted on Twitter between\nOctober 2008 to October 2021. It is available as an open\ndata set for academic purposes ( https://github.com/Emo\ntionDetection/Self-Reported-SR-emotion-dataset.git\n).\nIn turn, the methodological contribution of this paper is\nthreefold. First, we offer the ﬁrst (to the best of our\nknowledge) assessment of the generalizability of the fore-\ncasting accuracy of emotion detection models developed\non the basis of either self-reported or annotator-labeled\nemotion data sets. To do so, we apply models trained on\neach self-reported emotion data set to predict the emotions\nthat annotators have used to label the texts and vice versa.\nSecond, regarding the relevance of large databases, we\nassess whether models developed on the newly collected\nself-reported data set achieve higher forecasting accuracy\nthan models developed on smaller, previously published\ndata sets that also contain self-reported emotions. Third, we\n10946 Neural Computing and Applications (2023) 35:10945–10956\n123\npropose and apply the TTL approach, which replicates the\nsocial-emotional development stages of children.\n2 Related work\nGrowing literature recognizes the importance of speciﬁc\n(ﬁne-grained) emotion detection models and data sets.\nExisting emotion detection algorithms rely on the concepts\nof (1) word-level affect lexicon, (2) phrase-level traditional\nML, (3) document-level deep learning, and (4) document-\nlevel pretrained transformer models.\nA word-level affect lexicon entails the identiﬁcation of a\ncorpus of words related to speciﬁc emotions. For example,\n‘‘stole’’ relates to the emotion of anger; ‘‘amazing’’ is\nrelated to joy. This vocabulary-based approach assigns the\nprimary emotion to a label in the text by searching for the\nfrequency of words associated with various, speciﬁc emo-\ntions (e.g., [ 4, 25]).\nPhrase-level traditional ML analyzes texts beyond sim-\nple counts of words. The algorithms learn the meaning of\nwords or phrases from the training data set, using an n-\ngram function (i.e., sequence of N words; [ 17]). For\nexample, the 2-g word tokens ‘‘ﬁne young’’ and ‘‘young\nman’’ can be extracted from the phrase ‘‘ﬁne young man’’\n[20, 28, 40].\nFor document-level deep learning, Goodfellow et al.\n[15] point out that as the amount of data used for NLP\nincreases, traditional ML algorithms suffer from insufﬁ-\ncient lexical feature extraction. Deep learning algorithms\ninstead use many hidden layers to ﬁnd complex document-\nlevel representations of large amounts of text, without\nlexical feature extraction [ 1, 26, 32, 41].\nIn 2018, Google introduced a pretrained transformer\nmodel, ‘‘Bidirectional Encoder Representations from\nTransformers’’ (BERT) [ 10], that outperformed other\nmodels in many NLP tasks, including speciﬁc emotion\nclassiﬁcations [ 3, 7, 9]. Previously published studies indi-\ncate that transformer models are optimal for emotion\ndetection, but classiﬁcation accuracy varies depending on\nthe emotion data set being analyzed [ 3, 7, 9]. As, Table 1\nshows, accuracy for classifying human emotions varies\nfrom 50 to 80%; it seems difﬁcult to surpass 80%.\nYet it is not clear whether models based on self-reported\nemotions generalize to data sets containing annotator labels\nor vice versa (RQ1). Previous research tends to compare\ndifferent algorithms used for emotion detection, not the\ntype of data being used to deﬁne the emotions expressed in\ntexts. According to this comparison, transformer models\noutperform alternative algorithms [ 3, 7, 9], so we integrate\nthem into our proposed TTL approach. Table 1 also shows\nthat emotion data sets exhibit a trend of increasing sizes\nover time (RQ2) but cannot indicate whether a larger data\nsets, containing authors’ self-reported emotions, enhance\nforecasting accuracy.\nFinally, recent studies [ 9, 43] propose that researchers\ncan increase model accuracy by sequentially training a\ntransformer model, ﬁrst on a large general data set and then\non a small, target data set (RQ3). For example, BERT [ 10]\nwas sequentially trained on a large emotion data set\n(GoEmotions [ 9]; n = 58 k) and then small data sets\n(sampled from various emotion data sets, [ 2\n][ 8][ 14][ 25]\n[27][ 32][ 33]; n \\ 1,000). It achieves greater classiﬁcation\naccuracy than models trained only on a small data set [ 9].\nPrior studies include various annotator-rated and self-re-\nported emotion data sets, but the focus is primarily on size\ndifferences, rather than types of labels, self-reported or\nhuman annotator labels. As noted in the introduction, we\naim to extend this approach to replicate the social-emo-\ntional development stages of humans and thereby achieve\nrobust performance across emotion data sets.\n3 Data sets\n3.1 Annotator-rated and self-reported emotion\ndata sets\nA total of 11 data sets were analyzed, each containing at\nleast four of Ekman’s [ 13] six commonly applied speciﬁc\nemotions (anger, disgust, fear, joy, sadness, and surprise).\nThe data sets consist of emotion labels and accompanying\ntext sentences, such as news headlines, tweets, and Reddit\ncomments. Table 2 summarizes seven previously collected\nannotator-rated emotion data sets:\n• Affective Text (D_A1): Six annotators labeled 1000\nnews headlines from Google News and CNN ( https://\nweb.eecs.umich.edu/*mihalcea/affectivetext/),\n• Emotion Cause data set (D_A2): Four annotators\nlabeled 2,000 automatically generated sentences\n(https://www.site.uottawa.ca/*diana/resources/emo\ntion_stimulus_data/),\n• CrowdFlower Sentiment Analysis (D_A3): 40,000\ntweets labeled by crowdsourcing ( https://data.world/\ncrowdﬂower/ sentiment-analysis-in-text),\n• Emotion Intensities 2017 in Tweets (D_A4): Annota-\ntors labeled 7,000 tweets ( https://saifmohammad.com/\nWebPages/EmotionIntensity-SharedTask.html),\n• GoEmotions (D_A5): Three to ﬁve annotators labeled\n58,000 Reddit comments ( https://github.com/google-\nresearch/google-research/tree/master/goemotions),\n• Stance Sentiment Emotion Corpus (SSEC) (D_A6):\nThree to six annotators labeled 4,800 tweets used in the\nSemEval 2016 competition ( http://www.romanklinger.\nde/ssec/), and\nNeural Computing and Applications (2023) 35:10945–10956 10947\n123\n• SemEval-2018 Affect in Tweets Data (D_A7): Seven\nannotators labeled 2,500 tweets ( http://saifmohammad.\ncom/ WebPages/SentimentEmotionLabeledData.html).\nTable 3 lists the three previously published self-reported\nemotion data sets:\n• CARER emotion data set (D_S1): 664,000 tweets with\nself-reported emotions ( https://github.com/dair-ai/emo\ntion_data set),\n• International Survey on Emotion Antecedents and\nReactions (ISEAR) (D_S2): 7,500 sentences in which\nTable 1 Key reading table for classifying speciﬁc emotions\nStudy Data Method Findings\nBalahur et al.\n[4]\n1081 cases Simple K-means EmotiNet derived from existing NLP resources, based on the\nseven most basic emotions (fear, anger, sadness, guilt, shame,\njoy, and disgust)\nMohammad\n[25]\n21,051 tweets labeled by\nsix emotions\nStrength of association (SoA) Twitter Emotion Corpus (TEC), developed by collecting tweets\nwith emotion hashtags, based on six basic emotions [ 13]\nMohammad\nand\nKiritchenko\n[28]\nTEC and 1250 newspaper\nheadlines data sets\nSupport vector machine\n(SVM)\nWhen using a combination of word-level affect lexicon and\n(18)-gram function with SVM, the micro F\n1 score was the\nhighest at .499\nLiu [20] TEC data set Various ML algorithms The weighted F 1 score was .579 with SVM, .478 with random\nforest, and highest at .605 with logistic regression\nVolkova and\nBachrach [40]\n52, 925 tweets Log-linear models with L2\nregularization\nTrained the model using lexical features extracted from tweets\nannotated with six basic emotions, for an overall accuracy\nscore of .78\nAbdul-Mageed\nand Ungar [ 1]\n682,082 tweets Gated Recurrent Neural Nets\n(GRNNs) model\nThe F\n1 score was .8012 on the six basic emotions [ 13]\nclassiﬁcation\nWang et al. [41] Approximately 2.5 million\ntweets with 131 hashtags\nMNB and LIBLINEAR The F 1 score was the highest at .6163, with a large-scale linear\nclassiﬁcation that classiﬁes the seven emotion labels (joy,\nsadness, anger, love, fear, thankfulness, and surprise)\nSaravia et al.\n[32]\n664,462 tweets with 339\nhashtags\nCNN architecture with a\nmatrix form of the enriched\npatterns\nThe average F\n1 score was .79 on eight emotions (sadness, joy,\nfear, anger, surprise, trust, disgust, and anticipation)\nMohammad\net al. [ 26]\n10.983 English tweets for\nthe SemEval-2018\ncompetition\nSVM/SVR, LSTMs, and Bi-\nLSTMs\nDeep learning algorithms were applied by the top-performing\nteams on a classiﬁcation with four emotions (anger, fear, joy,\nand sadness)\nChatterjee et al.\n[7]\n41,179 dialogues for the\nSemEval-2019\ncompetition\nBi-LSTMs with BERT and\nELMo\nThe highest-ranked team achieved a micro F\n1 score of .7959 on\nfour emotions (happy, sad, angry, and others)\nAl-Omari et al.\n[3]\nData set of SemEval-2019 Ensemble model (BERT,\nGloVe embeddings)\nThe F1 score was .748 on four emotions (happy, sad, angry, and\nothers)\nDemszky et al.\n[9]\n58 k Reddit comments Transformer model, BERT GoEmotions based on Reddit comments. The macro-average F 1\nscore was .64 on the six basic emotions [ 13] classiﬁcation\nTable 2 Annotator-rated emotion data sets\nID Data set Ekman emotion Size Tot. emotions Source\nD_A1 [2] Affective text All 6 1 k 16 News headlines\nD_A2 [14] The emotion cause All 6 2 k 6 FrameNet\nD_A3 [8] Crowd-ﬂower Anger surprise joy sadness 40 k 14 Tweets\nD_A4 [27] Emotion intensities 2017 Anger fear joy sadness 7 k 4 Tweets\nD_A5 [9] Go-emotions All 6 58 k 27 Reddit comments\nD_A6 [27, 32] SSEC All 6 4.8 k 8 Tweets\nD_A7 [26] SemEval-2018 All 6 2.5 k 11 Tweets\n10948 Neural Computing and Applications (2023) 35:10945–10956\n123\nparticipants reported emotions through a survey (https://\ngithub.com/sinmaniphel/py_isear_data set),\n• Twitter Emotion Corpus (TEC) (D_S3): Collection of\n21,000 tweets with emotion hashtags ( http://saifmoham\nmad.com/WebPages/SentimentEmotionLabeledData.\nhtml).\n• Collecting the self-reported emotion data set\nWe collected the new self-reported emotion data set\n(D_SR) by using the Twitter Application Programming\nInterface (API), with approval from Twitter. The collected\ndata set consists of publicly available information and\nexcludes personally identiﬁable information.\nWe collected tweets in English ( n = 5,367,357) posted\nbetween March 2008 and October 2021 that feature one of\nEkman’s six basic emotions with a hashtag [ 13]: #anger,\n#disgust, #fear, #joy, #sadness, and #surprise [ 25]. For\nexample, the text ‘‘Spring is coming!!!!’’ featured ‘‘#joy,’’\nas inserted by the tweet’s author. Emotion hashtags in the\ntweets (independent variables) affect the dependent vari-\nables and were used exclusively as label values. Website\naddresses and special characters were removed to obtain\nonly English words [ 25]. For example, exclamation marks\nwere removed from ‘‘Spring is coming!!!!,’’ resulting in\n‘‘Spring is coming.’’\nDuplicate tweets also were removed, retaining the ﬁrst\none only (11.32% of tweets in our data set). Tweets\nincluding multiple emotion hashtags were removed to\ncapture unique emotion, such as ‘‘Everything makes me\ncry … everything #sadness #angry #joy’’ [ 1] (3.95% of\ntweets). Tweets containing words that may not represent\neach emotion were also removed, such as #anger toge-\nther with ‘‘management’’ or ‘‘mentalhealth’’; #disgust\ncontaining ‘‘insideout’’; #fear in conjunction with ‘‘god’’;\n#joy containing ‘‘redvelvet’’; and #surprise with ‘‘birth-\nday’’ (4.83% of tweets). Also, tweets that had fewer than\nthree English words and re-tweets were excluded [ 25]\n(11.81% of tweets). This process resulted in\nn = 3,654,544.\nA ﬁrst descriptive analysis shows that the number of\nwords in a tweet is deﬁned by Mean = 15.59, SD = 8.85,\nMedian = 14, Min = 3, Max = 71, Q1 = 9, and Q3 = 20.\nIn Fig. 1, the word cloud of the most frequently occurring\nwords shows that ‘‘love’’ (n = 399,484) is the most men-\ntioned emotion word, which reﬂects general valence. The\nmost mentioned speciﬁc emotion is ‘‘joy.’’\n3.2 Integration of eleven emotion data sets\nWe included only cases reﬂecting one of Ekman’s [ 13] six\nemotions (anger, disgust, fear, joy, sadness, and surprise)\nfrom the 11 emotion data sets mentioned previously.\nTable 4 reports the ten previously collected emotion data\nsets, which contain a total of 108,317 cases that feature\nfour to six Ekman emotions. The new self-reported data set\nfeatures 3,654,544 cases. The annotator-rated emotion data\nsets, D_A1 to D_A7, contain a total of 63,516 cases. The\npreviously collected self-reported data sets, D_S1 to D_S3,\ninclude 44,801 cases, and D_SR contains 3,654,544 cases.\nThe occurrence of speciﬁc emotions across the 11 data sets\nranges, deﬁned by proportions from 1.42 to 38.81%.\n4 Methodology\n4.1 Transformer models\nAs discussed in Sect. 2, transformer models such as BERT\n[10] outperform alternative ML algorithms [ 3, 7] for\ndetecting speciﬁc emotions in texts. As the ﬁrst transformer\nmodel, BERT relies on text encoders trained on the\nBooksCorpus (with 800 million words) and the English-\nlanguage Wikipedia (with 2500 million words) [ 36]. Its\nbidirectional training references both left and right sides of\nsentences simultaneously. Masked language modeling\noptimizes the weights in BERT model, such that it can train\nTable 3 Self-reported emotion\ndata sets ID Data set Ekman emotion Size Total emotions Source\nD_S1 [32] CARER All 6 664 k 8 Tweets\nD_S2 [33] ISEAR Joy, fear, anger, sadness, disgust 7.5 k 7 Survey\nD_S3 [25] TEC All 6 21 k 6 Tweets\nFig. 1 Word cloud of most frequent words\nNeural Computing and Applications (2023) 35:10945–10956 10949\n123\na language model that calculates the probability distribu-\ntion of words appearing in a sentence on the basis of large,\nunlabeled texts with unsupervised learning (see Fig. 2). In\nturn, pretrained BERT models can be ﬁne-tuned with an\nadditional output layer (Fig. 3) to develop high-performing\nmodels for a wide range of NLP tasks, such as linguistic\nacceptability, natural language inferencing, similarity pre-\ndiction, and sentiment analysis [ 10]. The ﬁne-tuning pro-\ncess is introduced in more detail in Sect. 4.2.\nWe also note two versions of BERT: BERT-base\n(L = 12, H = 768, A = 12, total parameters = 110 M) and\nBERT-large ( L = 24, H = 1024, A = 16, total parame-\nters = 340 M), where L is the number of transformer\nblocks, H is the hidden size, and A is the number of\nattention blocks [ 10]. Because BERT-large contains more\ntraining parameters, its training time for a million sen-\ntences in our study is prolonged (approximately seven\nhours per epoch), compared with BERT-base (approxi-\nmately three hours per epoch), using the latest RTX3090\nGPU. Nevertheless, BERT-large achieves better perfor-\nmance than BERT-base.\nLiu et al. [ 21] point out that Robustly optimized BERT\napproaches (RoBERTa) outperformed BERT in various\nNLP tasks. This version uses the same architecture as\nBERT but pretrains ten times more data, including both\nBERT data and 63 million news articles, a web text corpus,\nand stories. Because it offers the highest level of fore-\ncasting accuracy to date, we apply RoBERTa-large to\ncompare emotion detection models [ 19].\n4.2 Fine-tuning process\nWith RoBERTa, a ﬁne-tuning process takes place for\nsequence-level classiﬁcation tasks, as in the BERT archi-\ntecture. We follow an existing process [ 19], in which\nemotion data sets get split into a training (80%) and a\ntesting (20%) data set for the transformer model training.\nAt the beginning of the sequence, a special class token\n[CLS] gets added for classiﬁcation tasks. The representa-\ntion values of dimensions are converted from token values.\nBecause the transformer blocks reﬂect the relationships of\nall pairs of words in a sentence, the [CLS] vector indicates\nthe contextual meaning of the entire sentence. In the output\nlayer of the transformer model, we add a fully connected\nlayer to categorize the class labels into speciﬁc emotions\n(Fig. 3). During training, we ﬁne-tuned all parameters of\nthe transformer model together, and the output label\nprobabilities were calculated using the Adam function [19].\nThis process appears as the middle area in Fig. 4.\nFigure 4 summarizes the new two-step TTL architecture\nthat we introduce. First, the transformer model, RoBERTa-\nlarge (upper area of Fig. 4), was trained on large self-re-\nported emotion data sets. In our study, the self-reported\nemotion transformer model (middle area in Fig. 4) was\ntrained on the large, integrated, self-reported emotion data\nsets, D_S1 to D_S3 and D_SR ( n = 3,699,345). Second,\nthe emotion model is synchronized to detect socially\nagreed emotions by retraining on relatively small, annota-\ntor-rated emotion data sets. For our study, the model\nundergoes further training (lower area in Fig. 4) on the\ncombined annotator-rated emotion data sets, D_A1 to\nD_A7 ( n = 63,516), through a ﬁne-tuning process, by re-\nusing the self-reported emotion model (middle area in\nFig. 4).\nThe purpose of RoBERTa and other ML models trained\nwith word vectors is to perform a classiﬁcation in which\nthe output indicates the likelihood of the input sentence\nbeing classiﬁed as one of the possible emotion labels, such\nas fear, anger, sadness, joy, surprise, and disgust. We use\nthe weighted F1 score [ 20] to assess forecasting accuracy;\nTable 4 Cases in emotion data sets that contain Ekman emotions\nID # of emotions Anger Disgust Fear Joy Sadness Surprise Total\nD_A1 6 79 41 185 433 262 217 1217 .03%\nD_A2 6 478 95 423 467 575 213 2251 .06%\nD_A3 4 110 0 0 5209 5165 2187 12,671 .34%\nD_A4 4 1701 0 2252 1616 1533 0 7102 .19%\nD_A5 6 3921 429 522 18,176 2283 3764 29,095 .77%\nD_A6 6 511 49 57 400 69 31 1117 .03%\nD_A7 4 2357 0 2820 2894 1992 0 10,063 .27%\nD_S1 5 2709 0 2373 6761 5797 719 18,359 .49%\nD_S2 5 1079 1066 1076 1092 1082 0 5395 .14%\nD_S3 6 1555 761 2814 8239 3830 3848 21,047 .56%\nD_SR 6 244,084 50,887 776,320 1,414,936 440,633 727,684 3,654,544 97.12%\nTotal 6 258,584 53,328 788,842 1,460,223 463,221 738,663 3,762,861 100.00%\n6.87% 1.42% 20.96% 38.81% 12.31% 19.63% 100.00%\n10950 Neural Computing and Applications (2023) 35:10945–10956\n123\nthis measure provides a suitable measure of unbalanced\ndata distributions [ 39], as exist for our analyses. The for-\nmula for the F1 score of each label (class) can be expressed\nas follows:\nF1 score lðÞ¼ 2 /C2 precisionðlÞ/C2 recallðlÞ\nprecision lðÞþ recallðlÞ ; ð1Þ\nwhere l is the label (anger, disgust, fear, joy, sadness, or\nsurprise), precision( l)i s truepositiveðlÞ\ntruepositiveðlÞþfalsepositiveðlÞ, and\nrecall(l)i s truepositiveðlÞ\ntruepositiveðlÞþfalsenegativeðlÞ. To calculate the weigh-\nted F1 score, we take the mean of all F1 scores of each label\nwhile weighting the data set size of each label.\n5 Evaluation\n5.1 Evaluation by models trained on separate\ndata sets\nTo address RQ1 and RQ2, we trained 11 RoBERTa-large\nmodels on the 11 separate emotion data sets deﬁned in\nTable 4. The 11 models take the labels M_A1 to M_A7,\nM_S1 to M_S3, and M_SR, depending on the speciﬁc data\nset on which they rely. We applied an 80/20 data set\nsplitting strategy to derive a training and a test set from\neach of the 11 data sets, so for example, the M_A1 model\nwas trained on 80% of the D_A1 data set, and the test set\nincluded 20% of D_A1.\nInput variables were stored as word vector tokens\n(pretrained embedding for different words), segment\nembeddings (sentence number encoded into a vector), and\nposition embeddings (encoded word within the sentence).\nOutput variables consisted of four to six labels, depending\non the Ekman emotions available in the data set (Tables 2\nand 3).\nFor the RoBERTa-large models, we used Hugging Face\n[42], one of the most frequently applied transformer\nlibraries for the Python programming language. The batch\nsize and learning rate are 16 and 0.00001, respectively, in\nall models. Devlin et al. [ 10] point out that overﬁtting may\noccur after approximately four epochs, due to the trans-\nformer model’s numerous parameters, up to 340 million.\nThus, training of the RoBERTa-large models was com-\npleted in three epochs.\nFor RQ1, we evaluated the classiﬁcation accuracy of the\nseven models based on annotator-labeled data (M_A1 to\nM_A7) in the seven annotator-rated emotion test sets\n(D_A1 to D_A7) and in the four self-reported emotion test\nsets (D_S1 to DS_3 and D_SR). Thus we obtain 77\nweighted F1 scores (upper part of Table 5). For example,\nthe ﬁrst two cells in the top row of Table 5 show that M_A1\nproduces F\n1 = 0.70 in the 20% test set derived from D_A1,\nFig. 2 Overview of pretraining BERT [ 10]\nFig. 3 BERT ﬁne-tuning network architecture [ 10]\nNeural Computing and Applications (2023) 35:10945–10956 10951\n123\nwhereas it indicates F 1 = 0.52 in test set derived from\nD_A2.\nThe annotator-rated models tend to perform best on test\nsets derived from the data set on which they were devel-\noped, i.e., the diagonal results in Table 5 are relatively\nhigh. For example, M_A1 was developed on the training\nset derived from D_A1, and it achieved an F\n1 score of 0.70\non the test set derived from D_A1—higher than the scores\nfor the ten other test sets (i.e., 0.38 to 0.62). Thus, data sets\nmay be rated by annotators with idiosyncratic rules for\nlabeling emotions.\nTable 5 also shows that annotator-rated models (M_A1\nto M_A7) resulted in a higher average F\n1 score in test sets\nof the seven annotator-rated emotion data sets (F 1 = 0.62;\nupper left quarter of Table 5) than in the four self-reported\nemotion test sets (F 1 = 0.53; upper right quarter of\nTable 5). For example, in the ﬁrst row of Table 5, the\naverage F 1 score of M_A1 is 0.55 for annotator-rated\nemotion test sets (D_A1 to D_A7) and 0.46 for self-re-\nported emotion test sets (D_S1 to D_S3 and D_SR). The\nunique rules that annotators apply when specifying emo-\ntions in each data set thus appear susceptible to bias in\nrelation to classifying self-reported emotions.\nNext, we evaluate the four models trained on the four\nself-reported emotion data sets, D_S1 to D_S3 and D_SR,\nin all 11 test sets. The resulting 44 weighted F\n1 scores\nappear in the lower part of Table 5. Here again, the self-\nreported models tend to perform best on the test set derived\nfrom the data set on which they were developed. We ﬁnd\nrelatively high diagonal results in Table 5 for the four\nmodels developed on the self-reported data sets. Further-\nmore, the four self-reported models achieve higher average\nF\n1 scores in self-reported emotion data sets (F 1 = 0.65;\nlower right quadrilateral, Table 5) than in annotator-rated\nemotion data sets (F 1 = 0.57; lower left quadrilateral,\nTable 5). Individual authors, across different data sets or in\ndifferent writing contexts, may exhibit biases similar to\nthose indicated by annotators when expressing their emo-\ntions. According to Table 5, this bias has a relatively strong\neffect when models based on self-reported emotions are\napplied to data sets with annotator-labeled emotions.\nFor RQ2, in the self-reported test sets (D_S1 to D_S3\nand D_SR), the M_SR model, based on the larger data set,\nresults in the highest average F\n1 score of 0.70, compared\nwith scores from 0.60 to 0.65 for M_S1 to M_S3 (lower\nright quadrilateral, Table 5). In contrast, the M_SR model\nachieved the lowest average F\n1 score of 0.49, compared\nwith scores ranging from 0.52 to 0.66 for M_S1 to M_S3,\nwhen testing the annotator-rated test sets (lower left\nquadrilateral, Table 5). Thus, a RoBERTa model trained on\nlarge data sets can achieve good performance in similar\ncontexts but not as much in different contexts.\n5.2 Evaluation by models trained on multiple\ndata sets\nTo address RQ3, we used the same input and output vari-\nables as in Sect. 5.1. To start, we trained the TTL emotion\nmodel on the large, self-reported training set, and then on\nthe smaller, annotator-rated training set. To test the pro-\nposed advantages of the TTL approach, we consider three\nalternative RoBERTa models as benchmarks: (1) the\nannotator-rated emotion RoBERTa model, trained on the\nseven annotator-rated training sets (containing 80% of\nD_A1 to D_A7, n = 63,516); (2) the self-reported emotion\nRoBERTa model, trained on the four self-reported training\nsets (containing 80% of D_S1 to D_SR, n = 3,699,345);\nand (3) the integration RoBERTa emotion model, trained\nsimultaneously on all 11 data sets, ( n = 3,762,861), instead\nof consecutively.\nTable 6 contains the 44 weighted F\n1 scores for the TTL\nemotion model and the three alternatives. The TTL emo-\ntion model achieved the highest average F\n1 score of 0.84\nacross the 11 analyzed data sets. The annotator-rated\nemotion model achieved the second highest average F 1\nscore (0.79).\nFigure 5 reports the plot of the loss, which reﬂects the\nclassiﬁcation error in the training and testing sets that\noccurs while training annotator-rated training sets. The loss\nassociated with the self-reported emotion model is greater\nthan that linked to the TTL emotion model; that is, the TTL\napproach can improve the performance of the transformer\nmodel during the model training stage. The TTL emotion\nmodel achieved the highest (D_A1, D_A5, and D_A7),\nsecond highest (D_A2, D_A3, and D_A6), or third highest\n(D_A4) F\n1 scores in the separate annotator-rated test sets.\nFig. 4 TTL architecture\n10952 Neural Computing and Applications (2023) 35:10945–10956\n123\nFurthermore, it achieved above-average F 1 scores, from\nﬁfth (D_S1, D_S2, and D_SR) to sixth (D_S3) highest\namong of 15 emotion models in the separate self-reported\ntest sets.\nFigure 6 plots the average F\n1 scores of the 15 emotion\nmodels from Sects. 5.1 and 5.2. The TTL emotion model\nachieves an average F 1 of 0.84, which is higher than the\nvalues for the 11 models trained on separate data sets\n(M_A1 to M_SR; F 1 between 0.52 and 0.68) and three\nmodels trained on multiple data sets (annotator-rated, self-\nreported, and integration emotion models; F\n1 between 0.62\nand 0.79).\n6 Discussion\nWe examined annotator-rated and self-reported emotion\ndata sets as sources for developing emotion detection\nmodels. Each data set has its own rules; any model tends to\ndo best when applied to the test set taken from the data set\non which the focal model was trained. This result provides\nfurther empirical support for the theory of constructed\nemotions [6], which argues that the concept of emotion can\nproduce different categories across different people,\ndepending on their personal experiences. This ﬁrst ﬁnding\ncontradicts the classical view of emotion theory [ 35] that\npeople possess inherent emotions, like universal biological\nﬁngerprints.\nIn relation to RQ1, we ﬁnd that models developed on\nannotator-rated emotion data sets perform less well on data\nsets with self-reported emotions (average F\n1 = 0.53) than\non those with annotator-rated emotions (average\nF1 = 0.62). Also relevant for RQ1 is our ﬁnding that people\nare biased in expressing their own emotions, similar to the\nbiases shown by annotators. That is, models developed on\nself-reported emotion data sets perform less well on data\nsets with annotator-rated emotions (average F\n1 = 0.57)\nthan on those with self-reported emotions (average\nF1 = 0.65).\nFor RQ2, the comparison of the ﬁndings with models\ntrained on self-reported emotion data sets conﬁrms that the\nM_SR model trained on a relatively large self-reported data\nset achieves better performance (average F 1 = 0.70) in the\nself-reported emotion test sets than the three models trained\non smaller, self-reported emotion data sets (M_S1 = 0.60;\nM_S2 = 0.65, M_S3 = 0.64). To the best of our knowl-\nedge, the D_SR emotion data set is the largest collection of\ntweets with emotion hashtag labels ( n = 3,654,544) ever\ncollected, spanning 13 years from October 2008 to October\n2021. Nevertheless, M_SR earns a relatively low score on\nannotator-rated emotion data sets (average F\n1 = 0.49)\ncompared with the other three models that are based on\nself-reported emotion labels (M_S1 = 0.60, M_S2 = 0.66,\nM_S3 = 0.64).\nTo answer RQ3, we offer the TTL emotion model, ini-\ntially trained on the four combined self-reported emotion\ndata sets ( n = 3,699,345) and then on the combined\nannotator-rated emotion data set ( n = 63,516). The model\ndisplays relatively strong performance, with the highest\naverage F\n1 score of 0.84; it achieves the highest average F 1\nscore of 0.87 on annotator-rated emotion test sets, but only\n0.79 on self-reported emotion test sets. Notably, the TTL\nemotion model reveals substantial improvements over the\nannotator-rated emotion models trained on corresponding\ntraining sets (D_A1, D_A3, D_A4, D_A5, and D_A7). The\naverage F\n1 score of the TTL emotion model also is higher\nthan those of the integration emotion model that trained all\nTable 5 Weighted F1 scores of emotion models (M_##) trained on separate data sets (D_##)\nModel D_A1 D_A2 D_A3 D_A4 D_A5 D _A6 D_A7 D_S1 D_S2 D_S3 D_SR Avg\nM_A1 .70 .52 .59 .38 .62 .65 .41 .51 .48 .41 .44 .52\nM_A2 .45 .98 .57 .70 .61 .56 .69 .69 .59 .38 .46 .61\nM_A3 .57 .52 .72 .40 .61 .38 .43 .57 .49 .47 .55 .52\nM_A4 .36 .87 .51 .86 .51 .50 .88 .60 .59 .40 .61 .61\nM_A5 .58 .75 .59 .46 .87 .74 .49 .63 .60 .42 .44 .60\nM_A6 .43 .62 .59 .44 .67 .83 .48 .57 .60 .38 .42 .55\nM_A7 .44 .94 .67 .90 .64 .64 .87 .65 .60 .49 .66 .68\nM_S1 .44 .78 .55 .56 .63 .63 .58 .94 .66 .38 .40 .60\nM_S2 .64 .77 .75 .52 .66 .72 .54 .64 .83 .50 .61 .65\nM_S3 .46 .72 .54 .53 .41 .46 .53 .54 .64 .70 .69 .57\nM_SR .36 .64 .54 .51 .39 .47 .53 .50 .68 .77 .84 .57\nThe delimiters (D and M) distinguish between data sets (e.g., D_A1) and models (e.g., M_A1). Avg = Average F 1 score of all test sets; for\nexample, the average value of M_A1 was calculated for D_A1 to D_SR.\nNeural Computing and Applications (2023) 35:10945–10956 10953\n123\ndata sets simultaneously, as well as the annotator-rated and\nself-reported emotion models (Fig. 6).\nFurther studies might apply the proposed TTL approach\nto other target domains with small annotator-rated emotion\ndata sets. For example, it might be useful for developing\nuniversally applicable emotion detection models that\nreﬂect other target domains, such as speciﬁc countries (e.g.,\nUSA and China), age groups (e.g., children and adults), and\ngenders, based on large, self-reported emotion data sets.\nA limitation of this study is that we only collected\nemotions expressed in tweets, which may not generalize to\nother text posted on various social media platforms. The\nTTL emotion model achieved the highest average F\n1 score\nof 0.84, which is only 0.05 higher than the annotator-rated\nemotion model value of 0.79. Considering that the classi-\nﬁcation accuracy of previous emotion detection studies\nfalls between 0.50 and 0.80, it may be difﬁcult to increase\nthe performance of human emotion detection dramatically.\nContinued research should integrate other types of social\nmedia data sets. Also, methods such as ensemble tech-\nniques can be used to investigate potential improvements to\nthe accuracy of transformer models.\nAcknowledgements This article was supported by Twitter, which\nprovided the academic research API that enabled us to collect tweets.\nAuthor’s contribution SJL drafted the manuscript and designed the\nstudy. All authors considered the results and approved the ﬁnal\nmanuscript.\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions. This work was not funded.\nData availability The analyzed emotion-labeled data set is shared as\nan open data set for academic purposes ( https://github.com/Emo\ntionDetection/Self-Reported-SR-emotion-dataset.git).\nTable 6 Weighted F1 scores of emotion models trained on multiple data sets\nModel D_A1 D_A2 D_A3 D_A4 D_A5 D_A6 D_A7 D_S1 D_S2 D_S3 D_SR Avg\nAnnotator .75 .96 .84 .88 .90 .85 .89 .67 .71 .59 .67 .79\nSelf .34 .63 .54 .52 .38 .51 .53 .94 .80 .80 .84 .62\nIntegration .49 .92 .58 .76 .70 .61 .75 .93 .81 .78 .84 .74\nTTL .77 .97 .83 .88 .90 .83 .90 .89 .79 .68 .79 .84\nAnnotator = annotator-rated emotion model trained on annotator-rated training sets (D_A1 to D_A7). Self = self-reported emotion model trained\non the self-reported training sets (D_S1 to D_SR). Integration = integration emotion model trained on the integrated data sets (annotator-rated\nand self-reported training sets). TTL = TTL emotion model sequentially trained with self-reported training sets, followed by annotator-rated\ntraining sets\nFig. 5 Plots of the losses of a annotator-rated emotion model and b TTL emotion model during training by annotator-rated training sets\nFig. 6 Average F1 scores of ﬁfteen emotion models\n10954 Neural Computing and Applications (2023) 35:10945–10956\n123\nCode availability Not applicable.\nDeclarations\nConflict of interest The authors declare that they have no conflict of\ninterest.\nEthical approval This article does not contain any studies with human\nparticipants or animals performed by any of the authors.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Abdul-Mageed M, Ungar L (2017) Emonet: Fine-grained emo-\ntion detection with gated recurrent neural networks. In: Pro-\nceedings of the 55th annual meeting of the association for\ncomputational linguistics (volume 1: Long papers), pp 718–728\n2. Agirre E, Ma`rquez L, Wicentowski R (2007) Proceedings of the\nFourth International Workshop on Semantic Evaluations\n(SemEval-2007) In: Proceedings of the fourth international\nworkshop on semantic evaluations (SemEval-2007)\n3. Al-Omari H, Abdullah MA, Shaikh S (2020) Emodet2: emotion\ndetection in English textual dialogue using BERT and BILSTM\nmodels. In: 2020 11th international conference on information\nand communication systems (ICICS), IEEE, pp 226–232\n4. Balahur A, Hermida JM, Montoyo A, Mun˜oz R (2011) Emotinet:\na knowledge base for emotion detection in text built on the\nappraisal theories. In: international conference on application of\nnatural language to information systems, Springer, Berlin, Hei-\ndelberg, pp 27–39\n5. Barrett LF (2017) The theory of constructed emotion: an active\ninference account of interoception and categorization. Soc Cogn\nAffect Neurosci 12(1):1–23\n6. Barrett LF (2017) Categories and their role in the science of\nemotion. Psychol Inq 28(1):20–26\n7. Chatterjee A, Narahari KN, Joshi M, Agrawal P (2019) SemEval-\n2019 task 3: EmoContext contextual emotion detection in text.\nIn: Proceedings of the 13th international workshop on semantic\nevaluation, pp 39–48\n8. Crowdﬂower (2017) Crowdﬂower’s data sets. https://data.world/\ncrowdﬂower. Accessed 19 Dec 2021\n9. Demszky D, Movshovitz-Attias D, Ko J, Cowen A, Nemade G,\nRavi S (2020) GoEmotions: a data set of ﬁne-grained emotions.\narXiv preprint arXiv:2005.00547.\n10. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-\ntraining of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\n11. Dunﬁeld K, Kuhlmeier VA, Connell L, Kelley E (2011) Exam-\nining the diversity of prosocial behavior: helping, sharing, and\ncomforting in infancy. Infancy 16(3):227–247\n12. Dutton DG, Aaron AP (1974) Some evidence for heightened\nsexual attraction under conditions of high anxiety. J Pers Soc\nPsychol 30(4):510–517\n13. Ekman P (1972) Universals and cultural differences in facial\nexpression of emotions, Nebraska. In: symposium on motivation,\nUniversity Nebraska Press, Lincoln, pp 83–207\n14. Ghazi D, Inkpen D, Szpakowicz S (2015) Detecting emotion\nstimuli in emotion-bearing sentences. In: international conference\non intelligent text processing and computational linguistics,\nSpringer, Cham, pp 152–165\n15. Goodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT\nPress, Cambridge\n16. Kitayama S, Mesquita B, Karasawa M (2006) Cultural affor-\ndances and emotional experience: socially engaging and disen-\ngaging emotions in Japan and the United States. J Pers Soc\nPsychol 91(5):890\n17. Kumar N, Dangeti P, Bhavsar K (2019) Natural language pro-\ncessing with Python cookbook. Packt Publishing, Birmingham\n18. Lee SJ, Kishore S, Lim J, Paas L, Ahn HS (2021) Overwhelmed\nby fear: emotion analysis of COVID-19 Vaccination Tweets. In:\nTENCON 2021–2021 IEEE Region 10 Conference (TENCON),\npp 429–434\n19. Lim J, Sa I, Ahn HS, Gasteiger N, Lee SJ, MacDonald B (2021)\nSubsentence extraction from text using coverage-based deep\nlearning language models. Sensors 21(8):2712\n20. Liu CH (2017) Applications of twitter emotion detection for stock\nmarket prediction. Doctoral dissertation, Massachusetts Institute\nof Technology\n21. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Stoyanov, V\n(2019) Roberta: A robustly optimised BERT pretraining\napproach. arXiv preprint arXiv:1907.11692\n22. Malik F, Marwaha R (2018) Developmental stages of social\nemotional development in children. StatPearls Publishing, Trea-\nsure Island\n23. Mcconatha JT, Lightner E, Deaner SL (1994) Culture, age, and\ngender as variables in the expression of emotions. J Soc Behav\nPers 9(3):481\n24. Mesquita B, Walker R (2003) Cultural differences in emotions: a\ncontext for interpreting emotional experiences. Behav Res Ther\n41(7):777–793\n25. Mohammad S (2012) # Emotional tweets. In: * SEM 2012: the\nﬁrst joint conference on lexical and computational semantics–\nVolume 1: proceedings of the main conference and the shared\ntask, and Volume 2: proceedings of the sixth international\nworkshop on semantic evaluation (SemEval 2012), pp 246–255\n26. Mohammad S, Bravo-Marquez F, Salameh M, Kiritchenko S\n(2018) Semeval-2018 task 1: affect in tweets. In: proceedings of\nthe 12th international workshop on semantic evaluation, pp 1–17\n27. Mohammad SM, Bravo-Marquez F (2017) Emotion intensities in\ntweets. arXiv preprint arXiv:1708.03696\n28. Mohammad SM, Kiritchenko S (2015) Using hashtags to capture\nﬁne emotion categories from tweets. Comput Intell\n31(2):301–326\n29. Mohammad SM, Sobhani P, Kiritchenko S (2017) Stance and\nsentiment in tweets. ACM Trans Internet Technol (TOIT)\n17(3):1–23\n30. Sailunaz K, Dhaliwal M, Rokne J, Alhajj R (2018) Emotion\ndetection from text and speech: a survey. Soc Netw Anal Min\n8(1):1–26\nNeural Computing and Applications (2023) 35:10945–10956 10955\n123\n31. Salehan M, Kim DJ (2016) Predicting the performance of online\nconsumer reviews: a sentiment mining approach to big data\nanalytics. Decis Support Syst 81:30–40\n32. Saravia E, Liu HCT, Huang YH, Wu J, Chen YS (2018) Carer:\nContextualized affect representations for emotion recognition. In:\nProceedings of the 2018 conference on empirical methods in\nnatural language processing, pp 3687–3697\n33. Scherer KR, Wallbott HG (1994) Evidence for universality and\ncultural variation of differential emotion response patterning.\nJ Pers Soc Psychol 66(2):310\n34. Schuff H, Barnes J, Mohme J, Pado ´ S, Klinger R (2017) Anno-\ntation, modelling and analysis of ﬁne-grained emotions on a\nstance and sentiment detection corpus. In: Proceedings of the 8th\nworkshop on computational approaches to subjectivity, sentiment\nand social media analysis, pp 13–23\n35. Siegel EH, Sands MK, Noortgate WVD, Condon P, Chang Y, Dy\nJ, Barrett FL (2018) Emotion ﬁngerprints or emotion popula-\ntions? A meta-analytic investigation of autonomic features of\nemotion categories. Psychol Bull 144(4):343\n36. Tenney I, Das D, Pavlick E (2019) BERT rediscovers the clas-\nsical NLP pipeline. arXiv preprint arXiv:1905.05950\n37. Tian GLUL, McIntosh C (2021) What factors affect consumers’\ndining sentiments and their ratings: evidence from restaurant\nonline review data. Food Qual Prefer 88:104060\n38. Uhls YT, Michikyan M, Morris J, Garcia D, Small GW, Zgourou\nE, Greenﬁeld PM (2014) Five days at outdoor education camp\nwithout screens improves preteen skills with nonverbal emotion\ncues. Comput Hum Behav 39:387–392\n39. Vinodhini G, Chandrasekaran RM (2012) Sentiment analysis and\nopinion mining: a survey. Int J 2(6):282–292\n40. Volkova S, Bachrach Y (2016) Inferring perceived demographics\nfrom user emotional tone and user-environment emotional con-\ntrast. In: Proceedings of the 54th annual meeting of the associa-\ntion for computational linguistics (Volume 1: Long Papers),\npp 1567–1578\n41. Wang W, Chen L, Thirunarayan K, Sheth AP (2012) Harnessing\ntwitter‘‘ big data’’ for automatic emotion identiﬁcation. In: 2012\ninternational conference on privacy, security, risk and trust and\n2012 international conference on social computing, IEEE,\npp 587–592\n42. Wolf T, Chaumond J, Debut L, Sanh V, Delangue C, Moi A,\nRush, M A (2020) Transformers: state-of-the-art natural language\nprocessing. In: Proceedings of the 2020 conference on empirical\nmethods in natural language processing: system demonstrations,\npp 38–45\n43. Zhang Y, Wang L, Wang X, Zhang C, Ge J, Tang J, Duan H\n(2021) Data augmentation and transfer learning strategies for\nreaction prediction in low chemical data regimes. Organ Chem\nFront 8(7):1415–1423\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n10956 Neural Computing and Applications (2023) 35:10945–10956\n123"
}