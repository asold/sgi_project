{
    "title": "Sharpness-Aware Minimization Improves Language Model Generalization",
    "url": "https://openalex.org/W3207523779",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2890704379",
            "name": "Dara Bahri",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2025229873",
            "name": "Hossein Mobahi",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2738935859",
            "name": "Yi Tay",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3120901154",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W4287865629",
        "https://openalex.org/W2948210185",
        "https://openalex.org/W2963959597",
        "https://openalex.org/W2964125128",
        "https://openalex.org/W3091401866",
        "https://openalex.org/W3088841408",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4286403637",
        "https://openalex.org/W3007685714",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3123802267",
        "https://openalex.org/W2962911098",
        "https://openalex.org/W3006861283",
        "https://openalex.org/W3006051380",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3168489096",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W3037492894",
        "https://openalex.org/W4287692509",
        "https://openalex.org/W3129626847",
        "https://openalex.org/W2788190072",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W4287646898",
        "https://openalex.org/W4244577065",
        "https://openalex.org/W2963023528",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W2155894447",
        "https://openalex.org/W2963317585",
        "https://openalex.org/W2963060032",
        "https://openalex.org/W4287827771",
        "https://openalex.org/W3007473808",
        "https://openalex.org/W4287126759",
        "https://openalex.org/W2523060838",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W4299300709",
        "https://openalex.org/W4288404646",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2139701068",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W4287324301",
        "https://openalex.org/W2963069632",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W3133629262",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2552194003",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W3128633047"
    ],
    "abstract": "The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size. Comparatively little work has been done to improve the generalization of these models through better optimization. In this work, we show that Sharpness-Aware Minimization (SAM), a recently proposed optimization procedure that encourages convergence to flatter minima, can substantially improve the generalization of language models without much computational overhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large gains when training data for these tasks is limited.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7360 - 7371\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nSharpness-Aware Minimization Improves Language Model\nGeneralization\nDara Bahri, Hossein Mobahi, Yi Tay\nGoogle Research\n{dbahri,hmobahi,yitay}@google.com\nAbstract\nThe allure of superhuman-level capabilities\nhas led to considerable interest in language\nmodels like GPT-3 and T5, wherein the re-\nsearch has, by and large, revolved around\nnew model architectures, training tasks, and\nloss objectives, along with substantial engi-\nneering efforts to scale up model capacity\nand dataset size. Comparatively little work\nhas been done to improve the generalization\nof these models through better optimization.\nIn this work, we show that Sharpness-Aware\nMinimization (SAM), a recently proposed op-\ntimization procedure that encourages conver-\ngence to ﬂatter minima, can substantially im-\nprove the generalization of language models\nwithout much computational overhead. We\nshow that SAM is able to boost performance\non SuperGLUE, GLUE, Web Questions, Nat-\nural Questions, Trivia QA, and TyDiQA, with\nparticularly large gains when training data for\nthese tasks is limited.\n1 Introduction\nOver the last several years, remarkable progress\nhas been made within the domain of natural lan-\nguage understanding, with machine-learned mod-\nels able to solve some tasks at near or above human-\nlevel performance. This progress has, by and large,\nbeen fueled by research centered around 1) bet-\nter inductive biases, such as the attention-enabled\nTransformer architecture (Vaswani et al., 2017),\n2) the clever leverage of massive corpora of tex-\ntual data that was historically disregarded as “un-\nlabeled,” usually in the form of pre-training objec-\ntives that strive to teach the model the structure\nof language (Radford et al., 2019; Devlin et al.,\n2018), 3) scaling up model capacity and the meth-\nods to support it (Shazeer and Stern, 2018), 4)\nmulti-task learning (Raffel et al., 2019), and lastly,\n5) larger and more diverse datasets along with ever-\nimproving benchmarks that attempt to test the true\ncapabilities of these models. Although these efforts\nall share the single goal of improving the model’s\ngeneralization, doing so by explicit changes to the\noptimization of the loss function has received less\nattention in comparison.\nRecently, motivated by the both empirical and\ntheoretical ﬁndings that ﬂatter minima lead to bet-\nter generalization (Kleinberg et al., 2018; Shirish\nKeskar et al., 2016; Chaudhari et al., 2019; Smith\nand Le, 2017), Foret et al. (2020) proposed a novel\nmodiﬁcation to vanilla stochastic gradient descent\nthey term “Sharpness-Aware Minimization,” or\nSAM. They show theoretically and empirically that\noptimizing with SAM encourages convergence to\nﬂatter points in the loss landscape and with it comes\nthe anticipated improvement in out-of-sample error.\nWhile their empirical ﬁndings are limited to com-\nputer vision tasks and datasets using convolutional\nneural networks (ResNets), follow-up work (Chen\net al., 2021) showed how SAM is particularly ef-\nfective on Vision transformers (ViTs) (Dosovitskiy\net al., 2020) and MLP-Mixers (Tolstikhin et al.,\n2021), architectures that are more prone than con-\nvolutional ones to land in sharp minima. Crucially,\nthey show that when equipped with SAM, ViTs\noutperform ResNets of similar size and throughput\nwithout the need for large-scale pre-training.\nEncouraged by wins in the vision domain, we\nask whether SAM can deliver similar gains in the\nlanguage domain. Our contributions are as follows:\n1. We show that blithely applying SAM when\nﬁne-tuning public pre-trained checkpoints\nof the text-to-text transformer (T5) (Raffel\net al., 2019) and its multilingual counterpart,\nmT5 (Xue et al., 2020) on SuperGLUE (Wang\net al., 2019), GLUE (Wang et al., 2018),\nTyDiQA-GoldP (Clark et al., 2020) and the\nClosed-Book Question Answering (CBQA)\ntasks from Roberts et al. (2020) – Web Ques-\ntions (Berant et al., 2013), Natural Ques-\ntions (Kwiatkowski et al., 2019), and Trivia\nQA (Joshi et al., 2017) – improves test perfor-\n7360\nmance quite markedly. Furthermore, by em-\nploying an approximation suggested by Brock\net al. (2021), these gains come only at the cost\nof about 25% extra compute.\n2. The improvement brought by SAM often in-\ncreases with less labeled training data, making\nSAM indispensable for data-limited tasks. We\ntest this by subsampling the training splits of\nCBQA and SuperGLUE datasets at rates rang-\ning from 2% to 80%.\n2 Related Works\nBetter Generalization. In light of ﬂatter minima\ngeneralizing better, Smith and Le (2017) showed\nthat the inherent noise in SGD serves as a form\nof implicit regularization, preventing the optimiza-\ntion from ever entering sharp valleys. Like SAM,\nentropy SGD (Chaudhari et al., 2019) explicitly\nencourages ﬂatter minima. Smith et al. (2021); Bar-\nrett and Dherin (2020) analyzed SGD’s generaliza-\ntion formally by way of continuous-time gradient\nﬂow. Optimization routines based on adversarial\nrisk (Zhu et al., 2019; He et al., 2020) and trust\nregions (Jiang et al., 2019; Aghajanyan et al., 2020)\nhave been proposed and shown to improve general-\nization across settings.\nWhile the number of methods which provide im-\nplicit or explicit regularization is overwhelmingly\nlarge, methods like early stopping, weight decay\n(or ℓ2-regularization), dropout (Srivastava et al.,\n2014), teacher-student or self-distillation (Hinton\net al., 2015; Mobahi et al., 2020), label smooth-\ning (Müller et al., 2019), batch normalization (Ioffe\nand Szegedy, 2015), mixup (Zhang et al., 2017),\nand data-augmentation more broadly are among\nthe most widely used in practice. Marginalization\nof Bayesian neural networks, though challenging,\nhas been shown to result in superior generaliza-\ntion in some settings (Wilson and Izmailov, 2020;\nMacKay, 1995).\nWhile ﬁrst-order optimization via SGD has been\nthe prevailing way of training neural networks\ndue to its efﬁciency and effectiveness, alternative\nsecond-order methods like K-FAC (Martens and\nGrosse, 2015) and Shampoo (Gupta et al., 2018)\nhave slowly gained traction, often enabled by clever\nengineering to make them feasible at scale. No-\ntably, Anil et al. (2020) presents a scalable imple-\nmentation of Shampoo that provides signiﬁcant\nconvergence and wall-clock time improvements\ncompared to ﬁrst-order methods. They demonstrate\nsuperior performance on machine translation and\nlanguage modeling.\nSAM. While this is, to the best of our knowledge,\nthe ﬁrst work detailing the beneﬁts of SAM for\nlanguage tasks, there have been successful applica-\ntions of SAM in the vision domain. Notably, Chen\net al. (2021) showed that convolution-free vision\nmodels like vision transformers (ViTs) (Dosovit-\nskiy et al., 2020) and MLP-Mixers (Tolstikhin et al.,\n2021) suffer from sharp minima and that SAM\nindeed smooths their loss landscapes. They cru-\ncially show that ViTs and MLP-Mixers outperfom\nResNets of similar and greater size on ImageNet\nwithout the use of pre-training or data augmenta-\ntions that would otherwise be necessary to achieve\nreasonable performance. They show that SAM in-\nduces sparsity in both architectures and leads to\nmore perceptive attention maps in ViTs. They ob-\nserve empirically that data augmentation and SAM\nare alike in that they both smooth the landscape on\naverage, but the latter does so by explicitly control-\nling the worst-case curvature, whereas the former\nsmooths over the directions induced by the aug-\nmentations. Furthermore, they observe that SAM\nencourages linearity with respect to the input, ex-\nhibiting an effect similar to that of mixup (Zhang\net al., 2017). Lastly, they show that SAM helps\ncontrastive learning and that it enables better ro-\nbustness on corrupted examples from ImageNet-\nC (Hendrycks and Dietterich, 2019) and ImageNet-\nR (Hendrycks et al., 2021).\nIn a similar spirit, Brock et al. (2021) proposed\nspeeding up SAM signiﬁcantly by using fewer ex-\namples when computing the ascent step, a strategy\nwhich we employ in this work, and they were able\nto apply it to ResNet model variants to advance the\nstate of the art on ImageNet without extra data.\nMeanwhile, in an attempt to make SAM’s radius\nρinvariant to the scale of the model parameters,\nKwon et al. (2021) proposed an adaptive version\nnamed Adaptive Sharpness-Aware Minimization\n(ASAM), which they then show empirically to out-\nperform normal SAM on a set of benchmark vision\ntasks.\n3 Review of Sharpness-Aware\nMinimization (SAM)\nWe begin by brieﬂy reviewing the SAM algorithm;\ninterested readers can see the original paper for a\nthorough treatment. In our presentation, we use\nthe ℓ2 norm (p= 2using notation from the origi-\n7361\nnal paper), assume a general optimizer (instead of\nvanilla SGD), and use the approximation proposed\nby Brock et al. (2021) to compute the ascent gra-\ndient (adversarial point) efﬁciently. Given a loss\nfunction L : W×X×Y→ R+, SAM seeks to\nﬁnd the parameter wwhose neighborhood has low\ntraining loss by optimizing the minimax objective:\nmin\nw\nmax\n||ϵ||2≤ρ\nLtrain(w+ ϵ).\nFinding the exact optima ϵ∗ of the inner-\nmaximization is challenging, so Foret et al. (2020)\nemploy a ﬁrst-order approximation, resulting in:\nˆϵ(w) = argmin\n||ϵ||2≤ρ\nLtrain(w) +ϵT∇wLtrain(w)\n= ρ∇wLtrain(w)/||∇wLtrain(w)||2.\nThat is, ˆϵ is just a scaling of the loss gradient\nat the current parameters. After computing ˆϵ(w),\nSAM performs gradient descent using the gradient\n∇wLtrain(w)|wadv at the nearby “adversarial” point\nwadv(w) ≜ w+ ˆϵ(w).\nPut another way, SAM plugs-and-plays with any\nﬁrst-order optimizer by simply replacing the gra-\ndient of the mini-batch Bat the current model\nweights wt ∈W with the gradient computed at\nwadv. wadv itself is computed by taking a gradient\nascent step of size ρalong the unit gradient vector\n∇wLM(w)/||∇wLM(w)||2|wt, where Mcan be\nthe mini-batch B, or a subset of it for enhanced\nefﬁciency. We found that setting Mto be 1/4-th\nof Bsped up the method signiﬁcantly with little\nloss in quality, in line with the recommendation of\nBrock et al. (2021). The end-to-end algorithm is\noutlined in Algorithm 1.\n4 Experiments\nWith SAM reviewed, we now discuss our exper-\niments. We evaluate SAM on a range of natural\nlanguage understanding tasks using the T5 (text-to-\ntext Transformer) framework (Raffel et al., 2019).\nT5 casts NLU tasks as sequence-to-sequence ones\nthat are learned using an encoder-decoder Trans-\nformer (Vaswani et al., 2017) architecture setup.\nThese Transformer models are typically pre-trained\non large corpora, like the Colossal Clean Crawled\nCorpus (C4) (Raffel et al., 2019), with, for exam-\nple, the objective of predicting a short contiguous\nspan of text that was intentionally corrupted in a\nsnippet of input text. The pre-trained model is typ-\nically ﬁne-tuned on a single task or a mixture of\nAlgorithm 1 Efﬁcient SAM Algorithm.\n1: input: training set S≜ ∪n\ni=1{(xi,yi)}, loss\nfunction L: W×X×Y→ R+, batch size b,\nneighborhood size ρ> 0 (default 0.15), ascent\nmicro-batch size a ≤ b (default b/4), ﬁrst-\norder optimizer update opt : W×W→W .\n2: initialize parameters w0, t= 0.\n3: while not converged do\n4: sample batch B= {(x1,y1),..., (xb,yb)}.\n5: sample ascent micro-batch M =\n{(x1,y1),..., (xa,ya)}.\n6: compute adversarial (ascent) point: wadv =\nwt + ρ ∇wLM(w)\n||∇wLM(w)||2\n|wt.\n7: compute gradient approximation for the\nSAM objective: gadv = ∇wLB(w)|wadv .\n8: update parameters: wt+1 = opt(wt,gadv).\n9: t= t+ 1.\n10: end while\n11: return wt\nmultiple tasks, the latter enabled by the fact that the\nframework treats all tasks as simple input-to-target\nsequence predictions.\nTo this end, we evaluate SAM in two ways:\n1. When publicly available pre-trained check-\npoints of the T5.1.1 model variant are ﬁne-\ntuned with and without SAM, on SuperGLUE,\nGLUE, TyDiQA, and the Closed-Book Ques-\ntion Answering benchmarks: Web Questions,\nNatural Questions, TriviaQA. We show SAM\nimproves generalization across benchmarks\nand four model sizes: Small (77M parame-\nters), Base (250M), Large (800M), and XL\n(3B).\n2. To show how it helps when task data is limited,\nwe report results when the training splits of\nthese benchmarks at various rates, ranging\nfrom 2% to 80%.\n4.1 Setup\nFramework. For all experiments, we train us-\ning Jax (Bradbury et al., 2018) and Google Cloud\nTPUs. To ensure fair comparisons, eliminate the\nimpact of exogenous factors, and reduce the possi-\nbility of software bugs, we train both standard and\nSAM-enabled models using the same codebase and\nsettings, so that the code paths are identical except\nfor the gradient calculation at each step, wherein\n7362\nModel SGlue BoolQ CB CoPA MultiRC ReCoRD RTE WiC WSC\nSmall 67.7 72.6 89.4 / 89.3 67.0 68.5 / 21.4 61.7 / 60.8 69.3 65.4 72.1\nSmall + SAM (0.05) 68.4 73.5 92.1 / 89.3 61.0 68.5 / 22.8 62.1 / 61.0 69.7 65.7 79.8\nBase 75.3 80.0 91.7 / 94.6 71.0 75.4 / 35.4 76.2 / 75.4 80.9 69.3 76.9\nBase + SAM (0.15) 78.5 82.2 93.7 / 94.6 78.0 77.5 / 39.1 78.2 / 77.2 85.9 70.4 81.7\nLarge 84.3 86.6 99.4 / 98.2 89.0 83.7 / 51.0 86.5 / 85.6 89.2 72.9 84.6\nLarge + SAM (0.15) 84.6 88.0 95.0 / 96.4 86.0 84.0 / 53.7 87.3 / 86.4 89.2 75.2 86.5\nXL 87.2 88.6 93.7 / 96.4 95.0 86.9 / 61.1 89.5 / 88.4 91.3 74.9 89.4\nXL + SAM (0.15) 89.1 89.4 100.0 / 100.0 95.0 87.9 / 63.7 90.9 / 90.0 92.1 75.5 94.2\nTable 1: Experimental results (dev scores) on the (full) SuperGLUE benchmark. Public checkpoints of various\nsizes are ﬁne-tuned with and without SAM for 250k steps. We see that SAM improves performance across all\nmodel sizes.\nModel Glue CoLA SST MRPC STSB QQP MNLI QNLI RTE\nSmall 79.8 39.9 92.3 90.1 / 85.8 85.4 / 84.9 87.3 / 90.5 80.9 / 81.4 87.6 74.0\nSmall + SAM (0.05) 79.9 42.9 92.1 90.9 / 87.3 85.5 / 85.3 87.6 / 90.7 81.0 / 81.4 87.7 70.4\nBase 84.7 50.9 94.2 91.5 / 88.2 88.5 / 88.3 88.5 / 91.4 87.3 / 87.6 92.1 81.6\nBase + SAM (0.15) 85.1 49.8 94.2 93.4 / 90.7 90.0 / 89.7 89.0 / 91.7 87.3 / 87.5 92.5 82.7\nLarge 86.2 57.8 95.3 89.5 / 85.3 88.3 / 88.5 87.8 / 90.9 88.9 / 89.0 93.4 85.9\nLarge + SAM (0.15) 88.3 65.6 95.6 93.6 / 91.2 90.2 / 89.8 89.7 / 92.2 89.9 / 89.7 94.5 85.9\nXL 89.9 70.7 96.3 92.8 / 90.0 91.2 / 91.0 89.6 / 92.1 91.1 / 91.3 95.5 90.6\nXL + SAM (0.15) 90.2 69.7 96.9 93.2 / 90.7 91.7 / 91.8 90.2 / 92.7 91.4 / 91.5 95.8 91.0\nTable 2: Experimental results (dev scores) on the (full) GLUE benchmark. Public checkpoints of various sizes are\nﬁne-tuned with and without SAM on the mixture of tasks for 250k steps. We see that SAM improves performance\nacross all model sizes.\nModel avg. (F1/EM)\nSmall 73.4 / 62.1\nSmall + SAM (0.02) 74.3 / 63.0\nBase 81.6 / 71.0\nBase + SAM (0.02) 82.0 / 71.3\nLarge 85.6 / 75.3\nLarge + SAM (0.02) 85.9 / 76.1\nXL 87.0 / 77.4\nXL + SAM (0.05) 87.3 / 77.7\nTable 3: Average results on TyDiQA-GoldP. Public\ncheckpoints of mT5 of various sizes are ﬁne-tuned on\nTyDiQA-GoldP for 20k steps. SAM boosts perfor-\nmance acros model sizes here as well. We found that\nsmaller values ofρthan those used for the English-only\nT5 model were necessary to achieve good performance\nhere. Full, per-language results are shown in the Ap-\npendix.\nSAM behaves differently. Our implementation of\nSAM is an adaptation of an existing open-source\nimplementation1 to ﬁt our framework for training\nlanguage models.\nEfﬁcient SAM. In Foret et al. (2020), the idea of\npartitioning the ascent mini-batch into mdisjoint\nmicro-batches and computing a distinct adversar-\nial point for each micro-batch and then averaging\nthe SAM-gradients at each of these points was pro-\nposed under the name m-sharpness. It was noted\nthere and in follow-up work (Chen et al., 2021)\nthat m> 1 can result in better performance. This\nmodiﬁcation incurs m-times more compute under\na naive sequential implementation (though it can be\nparallelized well if multiple devices are available).\nMeanwhile, Brock et al. (2021) suggests (in\nthe Appendix) using roughly 20% of the exam-\nples from the mini-batch for computing the adver-\nsarial point, observing little loss in model quality.\nWith m= 1, this approximation roughly reduces\nSAM’s relative runtime from 2x to 1.2x. Since\nwe understand how a 2m-x slow-down of model\ntraining may be prohibitive or signiﬁcantly deter\nSAM’s widespread adoption, we, at the possible\n1https://github.com/google-research/\nsam\n7363\nModel Natural Q. Web Q. TriviaQA\nSmall 16.7 / 12.4 22.8 / 16.5 10.2 / 7.3\nSmall + SAM (0.05) 17.5 / 13.1 23.5 / 16.9 11.0 / 7.8\nBase 23.2 / 18.1 29.7 / 22.5 19.3 / 15.3\nBase + SAM (0.15) 25.7 / 20.6 31.0 / 24.5 21.5 / 17.4\nLarge 27.4 / 22.3 34.3 / 27.6 25.2 / 20.9\nLarge + SAM (0.15) 30.6 / 25.0 36.4 / 29.6 28.5 / 24.2\nXL 33.5 / 27.5 39.3 / 31.6 36.5 / 31.1\nXL + SAM (0.15) 34.7 / 28.8 40.7 / 33.3 38.0 / 32.6\nTable 4: Experimental results (F1/EM) (test scores) on the (full) CBQA tasks. Public checkpoints of various\nsizes are ﬁne-tuned with and without SAM on the mixture of tasks for 20k steps. We see that SAM improves\nperformance across all model sizes.\nModel Natural Q. Web Q. TriviaQA\nSmall 19.2 / 15.0 23.8 / 17.7 10.9 / 8.1\nSmall + SAM (0.05) 20.8 / 16.5 25.9 / 20.4 12.3 / 9.4\nBase 26.3 / 21.1 31.6 / 26.0 20.6 / 16.8\nBase + SAM (0.15) 27.8 / 22.6 33.6 / 27.5 23.7 / 19.5\nLarge 28.1 / 23.0 32.7 / 25.8 25.1 / 20.8\nLarge + SAM (0.15) 30.8 / 25.3 34.4 / 28.0 28.8 / 24.2\nXL 33.4 / 27.3 37.1 / 30.6 35.5 / 30.2\nXL + SAM (0.15) 34.2 / 28.3 39.4 / 32.3 37.6 / 32.0\nTable 5: Experimental results (F1/EM) (test scores) on the (full) CBQA tasks, where the model is trained on each\nof the three tasks separately, rather than on a mixture. We see that SAM improves performance across all model\nsizes, as we observed when training on the mixtures. This suggests that SAM’s gains are not solely due to some\nability to better leverage multi-task learning.\nloss of larger improvements, set m = 1 and use\n1/4-th (25%) of the mini-batch, or the number of\navailable training devices (TPU cores in our case),\nwhichever is larger, to compute SAM’s adversarial\npoint. This is necessary because the mini-batch\ngradient computation is parallelized over devices\nand each device must receive at least one example.\nWe’ve observed from wall-clock times that with\nthese settings, SAM is all in all about 25% slower\nthan standard training.\nHyper-parameters. SAM has a single hyper-\nparameter ρ, which is size of the step taken along\nthe unit adversarial gradient vector. We search\nthe range [0.02,0.05,0.1,0.15,0.2,0.3] a single\ntime only when ﬁne-tuning on SuperGLUE. We\nfound that 0.05 is a reasonable choice for T5.1.1\nsmall models, and 0.15 for the Base, Large, and\nXL variants, and so for all subsequent experiments\nexcept for TyDiQA, we use these choices with-\nout additional tuning. For the mT5 model on Ty-\nDiQA, we found that a smaller ρwas necessary for\ngood performance. For this, we searched the range\n[0.01,0.02,0.05].\nFor all ﬁne-tuning, we use the AdaFactor opti-\nmizer with learning rate 1e-3, 128 batch size, and\nthe T5.1.1 settings. For SuperGLUE, we use 10%\ndropout rate, 512 input sequence length, 62 target\nsequence length, and ﬁne-tune for 250k steps. For\nNatural Questions, Web Questions, and TriviaQA,\nwe use 5% dropout, 38 input sequence length, 18\ntarget sequence length, and ﬁne-tune for 20k steps.\nFor TyDiQA, we use the ofﬁcial, public mT5 check-\npoints, 10% dropout, 1024 input sequence length,\n512 target sequence length, and ﬁne-tune for 20k\nsteps. We run each experiment once, due to re-\nsource constraints, and we take the best checkpoint\n(stored every 1k steps for SuperGLUE and GLUE\nand every 200 steps for all other datasets) across\n7364\nFigure 1: CBQA results at various training data sampling rates, for the Small ( top half) and Base ( bottom half)\nmodels. We see that SAM’s improvement is consistent across data size regimes and that the relative improvement\nis often largest in the ballpark of 20%.\ntraining steps. Following standard practice, we re-\nport the best checkpoint for each task-metric pair\n(e.g. SuperGLUE CB F1) individually.\n4.2 Full Data Results\nResults for SuperGLUE and GLUE are shown in\nTable 1 and Table 2 respectively. We observe that\nSAM improves the overall scores for both bench-\nmarks across all T5 model sizes. For Base and XL\nsizes on SuperGLUE, SAM brings 4.2% and 2.1%\nrelative gains in overall score respectively, while\nthe gain for Large on GLUE is 2.4%. As shown\nin Table 4, on Natural Questions, Web Questions,\nand Trivia QA tasks, we observe improvements\nfor each task, metric (F1 and EM), and model size.\nFor Base, we see a 13.8%, 8.8%, and 13.7% gain\non the exact match metric for Natural Questions,\nWeb Questions, and Trivia QA respectively. For\nLarge, these ﬁgures are 12.1%, 7.2%, and 15.7%.\nTable 3 shows the results for TyDiQA-GoldP. Here,\nwe observe more modest improvements in the 1-2%\nrange.\nSAM improves performance on all model sizes.\nIn light of the conventional wisdom that “larger\nmodels generalize better,” we suspected, a priori,\nthat SAM would be more helpful for the smaller\nmodels we consider, like Small and Base, and that\nwe should expect substantial diminishing returns\n7365\nModel SGlue BoolQ CB CoPA MultiRC ReCoRD RTE WiC WSC\nSmall 50.2 60.8 37.0 / 55.4 52.0 60.1 / 11.0 33.9 / 32.5 54.5 54.4 65.4\nSmall + SAM (0.05) 51.9 60.5 45.6 / 66.1 53.0 61.1 / 12.5 36.7 / 34.5 52.3 55.2 66.3\nBase 52.9 59.6 32.3 / 55.4 53.0 60.2 / 11.8 47.8 / 46.5 58.5 57.2 68.3\nBase + SAM (0.15) 56.7 61.4 41.8 / 64.3 55.0 62.4 / 15.7 59.7 / 57.9 62.5 55.3 68.3\nLarge 62.8 65.3 40.1 / 62.5 62.0 71.6 / 24.0 80.4 / 78.9 69.7 57.7 69.2\nLarge + SAM (0.15) 64.3 77.3 47.9 / 69.6 59.0 69.0 / 20.4 81.5 / 80.0 65.0 59.6 69.2\nXL 75.9 84.5 57.0 / 82.1 86.0 82.4 / 48.7 83.3 / 81.7 78.7 66.0 74.0\nXL + SAM (0.15) 77.0 82.5 58.9 / 83.9 85.0 79.9 / 45.3 86.8 / 85.6 80.5 64.4 83.7\nTable 6: SuperGLUE results when only 5% of the training data is available. We see again that SAM boosts\nperformance across the board, adding a whopping 7.2% relative improvement on the Base model.\nModel Natural Q. Web Q. TriviaQA\nSmall 4.9 / 2.9 4.8 / 1.8 3.0 / 1.3\nSmall + SAM (0.05) 6.0 / 3.7 7.1 / 2.2 3.3 / 1.6\nBase 7.9 / 4.8 13.7 / 4.6 7.6 / 4.2\nBase + SAM (0.15) 8.6 / 5.6 12.2 / 5.7 7.7 / 4.4\nLarge 8.7 / 5.2 14.0 / 7.0 9.8 / 6.0\nLarge + SAM (0.15) 10.5 / 6.6 14.9 / 7.7 10.6 / 7.1\nXL 13.1 / 8.0 20.6 / 11.9 19.6 / 15.3\nXL + SAM (0.15) 13.4 / 8.1 22.9 / 13.6 19.1 / 14.5\nTable 7: CBQA results when only 5% of the training data is available. We see SAM helps here as it did for sub-\nsampled SuperGLUE. For Natural Questions, SAM improves Base model performance by a relative8.86%/16.6%\n(F1/EM).\nas we scale up the model size. Surprisingly, we did\nnot observe any clear pattern with regards to size:\nindeed, sometimes the gains on XL were larger\nthan those on Small. Thus, we lean to recommend\nSAM to all practitioners regardless of the regime\nin model capacity they are working in.\nSAM improves single-task and multi-task\nlearning alike. Thus far, SAM has been trained\non a mixture of tasks, where the inﬂuence of a\nparticular task is proportional to the number of ex-\namples in its training split (i.e. no artiﬁcial up or\ndown-weighting). To rule out the possibility that\nthe gains observed are solely due to some ability of\nSAM’s to leverage multi-task learning and improve\ncross-task transfer, we conduct the following abla-\ntion. For each of the three CBQA tasks, we train\nonly on a single task and report the performance\non that task’s test set. Results are shown in Table 5.\nIndeed, we see similar gains when training and test-\ning on each single task individually. We conclude\nthat the mechanism driving SAM’s improvements\naffect single-task and multi-task learning alike.\n4.3 When training data is limited\nWe now switch gears and evaluate whether or not\nSAM helps when training data is scarce. Prior\nwork (Chen et al., 2021) showed that for vision\nmodels and tasks, SAM helps more when there is\nless training data to learn from. To test whether\nthis holds for language, we do as follows: we sub-\nsample the training splits for both SuperGLUE\nand CBQA datasets at rates ranging from 2% to\n80%, and observe test performance when the public\ncheckpoint is ﬁne-tuned with and without SAM. Su-\nperGLUE and CBQA results at a 5% sampling rate\nare shown in Tables 6 and 7 respectively. In both\ncases we see again that SAM boosts performance\nacross the board, adding, for example, a whopping\n7.2% relative improvement on the Base model on\n5% SuperGLUE and a relative 8.86%/16.6% to\nF1/EM on Natural Questions.\nFigure 1 plots the performance on the three\nCBQA tasks as a function of the sampling rate. We\nobserve consistent gains from SAM across the size\nof the subsampled training set, with the relative im-\nprovement appearing largest when the subsampling\n7366\nrate is around 20%.\n4.4 Sensitivity to hyper-parameters\nFigure 2 shows the impact of SAM’s hyper-\nparameters ρ, the ascent micro-batch size a, and\nthe sharpness factor mon the (full) SuperGLUE\nbenchmark for the Base model. For ρ, we see that\nall tested values perform better than ﬁne-tuning\nwithout SAM. However,0.15 is a “sweet spot,” per-\nforming better than values below or above it. Thus,\npractitioners with little computational budget for\nhyper-parameter tuning may still see large gains by\nusing a non-optimal ρ, while those with a gener-\nous budget should consider tuning. For the ascent\nmicro-batch size a, we see that when the normal\n(descent) batch size is 128, there is improvement\nas a is increased to 32 but little past this point.\nThus, setting ato be 1/4-th the descent batch size,\nas we do throughout our experiments, provides a\ngood trade-off between performance and computa-\ntional overhead. Increasing the sharpness m, where\neach of the mascent micro-batches has size 32/m,\ndoes not improve performance here. We thus rec-\nommend a default of 1, which is the setting used\nacross our experiments. Full results are shown in\nthe Appendix.\n5 Conclusion\nTo the best of our knowledge, this paper is the\nﬁrst to demonstrate how the recently-proposed\nSharpness-Aware Minimization can be applied for\nﬁne-tuning the ubiquitous text-to-text Transformer\n(T5) and its multilingual counterpart mT5 on lan-\nguage tasks of broad interest. We thereby corrob-\norate the already-documented success the method\nhas had in the vision domain. Furthermore, we\nreveal SAM’s beneﬁts when data is limited by ﬁne-\ntuning on subsamples of the original task training\nsplit. By approximating the ascent step of the algo-\nrithm via fewer samples, we show how large gains\ncan be had across benchmarks and model sizes\nwhile adding only around 25% additional compute\nand wall-clock training time. Our hope is that this\nwork will spur SAM’s adoption in the natural lan-\nguage processing community the way it is starting\nto in the vision one.\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\nFigure 2: Impact of SAM’s hyper-parametersρ, the as-\ncent micro-batch size a, and the sharpness m on the\nSuperGLUE benchmark for the Base model. Except for\nthe ablated hyper-parameter, we use ρ= 0.15, a= 32,\nm = 1. For ρ, we see that all tested values perform\nbetter than ﬁne-tuning without SAM. However, 0.15 is\na “sweet spot,” performing better than values below or\nabove it. For the ascent micro-batch size a, we see that\nwhen the descent batch size is 128, there is improve-\nment as a is increased to 32 but little past this point.\nThus, setting ato be 1/4-th the descent batch size pro-\nvides a good trade-off between performance and com-\nputational overhead. Increasing the sharpness mdoes\nnot help (in fact it hurts) here. Full results are shown in\nthe Appendix.\n7367\n2020. Better ﬁne-tuning by reducing representa-\ntional collapse. arXiv preprint arXiv:2008.03156.\nRohan Anil, Vineet Gupta, Tomer Koren, Kevin Re-\ngan, and Yoram Singer. 2020. Scalable second or-\nder optimization for deep learning. arXiv preprint\narXiv:2002.09018.\nDavid GT Barrett and Benoit Dherin. 2020. Im-\nplicit gradient regularization. arXiv preprint\narXiv:2009.11162.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural lan-\nguage processing, pages 1533–1544.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nAndrew Brock, Soham De, Samuel L Smith, and Karen\nSimonyan. 2021. High-performance large-scale\nimage recognition without normalization. arXiv\npreprint arXiv:2102.06171.\nPratik Chaudhari, Anna Choromanska, Stefano Soatto,\nYann LeCun, Carlo Baldassi, Christian Borgs, Jen-\nnifer Chayes, Levent Sagun, and Riccardo Zecchina.\n2019. Entropy-sgd: Biasing gradient descent into\nwide valleys. Journal of Statistical Mechanics: The-\nory and Experiment, 2019(12):124018.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong.\n2021. When vision transformers outperform resnets\nwithout pretraining or strong data augmentations.\narXiv preprint arXiv:2106.01548.\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and\nBehnam Neyshabur. 2020. Sharpness-aware min-\nimization for efﬁciently improving generalization.\narXiv preprint arXiv:2010.01412.\nVineet Gupta, Tomer Koren, and Yoram Singer. 2018.\nShampoo: Preconditioned stochastic tensor opti-\nmization. In International Conference on Machine\nLearning, pages 1842–1850. PMLR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav\nKadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021.\nThe many faces of robustness: A critical analysis of\nout-of-distribution generalization. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 8340–8349.\nDan Hendrycks and Thomas Dietterich. 2019. Bench-\nmarking neural network robustness to common\ncorruptions and perturbations. arXiv preprint\narXiv:1903.12261.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint\narXiv:1502.03167.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2019.\nSmart: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through princi-\npled regularized optimization. arXiv preprint\narXiv:1911.03437.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nBobby Kleinberg, Yuanzhi Li, and Yang Yuan. 2018.\nAn alternative view: When does sgd escape local\nminima? In International Conference on Machine\nLearning, pages 2698–2707. PMLR.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics,\n7:453–466.\nJungmin Kwon, Jeongseop Kim, Hyunseo Park,\nand In Kwon Choi. 2021. Asam: Adaptive\nsharpness-aware minimization for scale-invariant\nlearning of deep neural networks. arXiv preprint\narXiv:2102.11600.\nDavid JC MacKay. 1995. Probable networks and\nplausible predictions-a review of practical bayesian\nmethods for supervised neural networks. Network:\ncomputation in neural systems, 6(3):469.\n7368\nJames Martens and Roger Grosse. 2015. Optimiz-\ning neural networks with kronecker-factored approx-\nimate curvature. In International conference on ma-\nchine learning, pages 2408–2417. PMLR.\nHossein Mobahi, Mehrdad Farajtabar, and Peter L\nBartlett. 2020. Self-distillation ampliﬁes reg-\nularization in hilbert space. arXiv preprint\narXiv:2002.05715.\nRafael Müller, Simon Kornblith, and Geoffrey Hinton.\n2019. When does label smoothing help? arXiv\npreprint arXiv:1906.02629.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. 2016. On Large-Batch Training for Deep\nLearning: Generalization Gap and Sharp Minima.\narXiv e-prints, page arXiv:1609.04836.\nSamuel L Smith, Benoit Dherin, David GT Barrett, and\nSoham De. 2021. On the origin of implicit regular-\nization in stochastic gradient descent. arXiv preprint\narXiv:2101.12176.\nSamuel L Smith and Quoc V Le. 2017. A bayesian\nperspective on generalization and stochastic gradient\ndescent. arXiv preprint arXiv:1710.06451.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner,\nJessica Yung, Daniel Keysers, Jakob Uszkoreit,\nMario Lucic, et al. 2021. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint\narXiv:2105.01601.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019. Super-\nglue: A stickier benchmark for general-purpose\nlanguage understanding systems. arXiv preprint\narXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAndrew Gordon Wilson and Pavel Izmailov. 2020.\nBayesian deep learning and a probabilistic\nperspective of generalization. arXiv preprint\narXiv:2002.08791.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin,\nand David Lopez-Paz. 2017. mixup: Beyond\nempirical risk minimization. arXiv preprint\narXiv:1710.09412.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2019. Freelb: Enhanced ad-\nversarial training for natural language understanding.\narXiv preprint arXiv:1909.11764.\n7369\n6 Appendix\n6.1 TyDiQA-GoldP Results\nTable 8 shows the per-language TyDiQA-GoldP\nscores. We found that the multilingual mT5 model\nbeneﬁted from a smaller ρ than the vanilla T5\nmodel.\n6.2 Impact of hyper-parameters on\nSuperGLUE\nTable 9 shows the full (no subsampling) Super-\nGLUE results for the Base model for different\nhyper-parameter choices.\n7370\nModel avg (F1/EM) en ar bn ﬁ id ko ru sw te\nSmall 73.4 / 62.1 66.3 / 54.5 78.0 / 62.8 69.4 / 60.2 73.7 / 60.9 77.8 / 65.0 64.1 / 55.8 69.5 / 56.7 78.0 / 68.9 84.0 / 74.4\nSmall + SAM (0.02) 74.3 / 63.0 67.1 / 55.7 79.2 / 64.9 68.4 / 57.5 75.0 / 61.8 78.8 / 67.6 64.0 / 54.3 72.0 / 58.0 79.9 / 72.1 84.2 / 75.0\nBase 81.6 / 71.0 76.7 / 65.7 84.2 / 70.4 81.7 / 71.7 81.6 / 69.3 85.1 / 74.2 73.6 / 66.3 78.8 / 64.7 84.4 / 77.2 87.9 / 79.4\nBase + SAM (0.02) 82.0 / 71.3 76.1 / 65.0 83.9 / 70.2 84.4 / 75.2 81.1 / 69.6 85.4 / 74.0 74.5 / 66.7 79.5 / 65.3 84.8 / 76.4 88.8 / 79.1\nLarge 85.6 / 75.3 81.2 / 70.0 86.6 / 73.1 86.3 / 77.9 84.6 / 71.6 87.7 / 77.9 81.1 / 72.1 84.0 / 72.8 88.7 / 81.2 90.3 / 81.5\nLarge + SAM (0.02) 85.9 / 76.1 82.3 / 71.6 87.3 / 74.4 86.6 / 79.6 84.5 / 72.5 88.1 / 80.0 81.3 / 73.2 84.2 / 71.9 88.3 / 79.8 90.9 / 82.1\nXL 87.0 / 77.4 82.1 / 72.0 87.3 / 74.4 88.9 / 82.3 85.8 / 74.6 89.7 / 80.4 81.7 / 73.6 85.1 / 73.5 90.7 / 83.2 91.5 / 82.4\nXL + SAM (0.05) 87.3 / 77.7 82.8 / 73.4 87.6 / 75.0 89.2 / 81.4 86.4 / 74.2 89.6 / 80.2 82.6 / 75.0 85.5 / 74.8 90.7 / 83.0 91.4 / 82.5\nTable 8: Full Results for TyDiQA-GoldP.\nModel SGlue BoolQ CB CoPA MultiRC ReCoRD RTE WiC WSC\nBase + SAM (0.02) 76.6 80.5 92.4 / 92.9 73.0 76.2 / 36.8 77.3 / 76.4 81.9 70.8 80.8\nBase + SAM (0.05) 77.2 80.3 97.4 / 96.4 73.0 76.4 / 37.9 77.8 / 76.9 83.8 71.9 76.9\nBase + SAM (0.1) 77.4 81.7 94.8 / 94.6 72.0 76.8 / 38.3 79.3 / 78.3 83.8 72.7 77.9\nBase + SAM (0.15) 78.5 82.2 93.7 / 94.6 78.0 77.5 / 39.1 78.2 / 77.2 85.9 70.4 81.7\nBase + SAM (0.2) 77.7 82.3 95.0 / 96.4 75.0 77.2 / 39.5 77.9 / 76.8 84.1 71.8 76.9\nBase + SAM (0.3) 77.8 81.1 93.6 / 94.6 74.0 77.4 / 40.0 79.8 / 78.7 85.9 70.8 78.8\nBase + SAM (0.4) 77.6 80.6 94.3 / 96.4 79.0 76.3 / 37.5 77.9 / 76.9 81.9 71.0 78.8\nBase + SAM (8) 77.8 81.9 97.4 / 96.4 72.0 77.5 / 40.8 78.6 / 77.5 84.5 71.0 78.8\nBase + SAM (24) 78.1 82.5 97.4 / 96.4 74.0 77.2 / 40.2 79.2 / 78.4 83.8 69.4 80.8\nBase + SAM (32) 78.5 81.9 96.1 / 94.6 77.0 77.6 / 40.3 78.9 / 77.8 85.6 71.9 78.8\nBase + SAM (64) 78.5 82.3 96.1 / 94.6 79.0 78.4 / 40.8 79.3 / 78.2 84.5 71.3 76.9\nBase + SAM (128) 78.8 82.4 97.4 / 96.4 74.0 77.6 / 40.6 79.0 / 78.0 87.0 72.9 79.8\nBase + SAM (1) 78.6 81.5 96.1 / 96.4 73.0 77.2 / 40.0 79.0 / 77.9 87.0 72.6 81.7\nBase + SAM (2) 78.1 81.8 96.1 / 94.6 77.0 76.5 / 37.6 78.5 / 77.6 84.8 71.8 78.8\nBase + SAM (4) 78.3 82.3 97.4 / 96.4 75.0 77.3 / 38.5 78.2 / 77.1 85.2 72.1 79.8\nTable 9: SuperGLUE results for Base for different values of ρ (top), ascent micro-batch size a (middle), and\nsharpness m(bottom). Except for the ablated hyper-parameter, we use ρ= 0.15, a= 32, m= 1.\n7371"
}