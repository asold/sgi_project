{
  "title": "Datasets for Large Language Models: A Comprehensive Survey",
  "url": "https://openalex.org/W4392402185",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5000098091",
      "name": "Yang Liu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5072330935",
      "name": "Jiahuan Cao",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5021720344",
      "name": "Chongyu Liu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5002626680",
      "name": "Kai Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5080674767",
      "name": "Lianwen Jin",
      "affiliations": [
        "South China University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6856398428",
    "https://openalex.org/W2068882115",
    "https://openalex.org/W4380985554",
    "https://openalex.org/W6856051742",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4388184238",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3010108619",
    "https://openalex.org/W4386875581",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W4386113743",
    "https://openalex.org/W6859289717",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W6853517571",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W4321472357",
    "https://openalex.org/W4387847108",
    "https://openalex.org/W6857044596",
    "https://openalex.org/W6849941170",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4401042685",
    "https://openalex.org/W4382319574",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4386942223",
    "https://openalex.org/W3093956460",
    "https://openalex.org/W4385568240",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4386977707",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W4385014929",
    "https://openalex.org/W4389518761",
    "https://openalex.org/W4383472790",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4385573116",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W6855829824",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W6852449896",
    "https://openalex.org/W6851960618",
    "https://openalex.org/W4389519602",
    "https://openalex.org/W4389519248",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W4367369802",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W4366456685",
    "https://openalex.org/W2620787630",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W6852172664",
    "https://openalex.org/W4385681988",
    "https://openalex.org/W3101223450",
    "https://openalex.org/W6851787984",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4389520259",
    "https://openalex.org/W4386273007",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4394664141",
    "https://openalex.org/W4386942328",
    "https://openalex.org/W6853444283",
    "https://openalex.org/W4378508793",
    "https://openalex.org/W3198845875",
    "https://openalex.org/W4385681149",
    "https://openalex.org/W4387963447",
    "https://openalex.org/W4392384758",
    "https://openalex.org/W4367694325",
    "https://openalex.org/W4380989429",
    "https://openalex.org/W6856202762",
    "https://openalex.org/W4385573018",
    "https://openalex.org/W6858100026",
    "https://openalex.org/W7046559209",
    "https://openalex.org/W6767858076",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W4386437475",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4383993628",
    "https://openalex.org/W4384918941",
    "https://openalex.org/W3215698779",
    "https://openalex.org/W4226227340",
    "https://openalex.org/W4387322976",
    "https://openalex.org/W6852476831",
    "https://openalex.org/W4381711094",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W4385373704",
    "https://openalex.org/W4386875367",
    "https://openalex.org/W4378464464",
    "https://openalex.org/W4401042689",
    "https://openalex.org/W4380136538",
    "https://openalex.org/W4367061104",
    "https://openalex.org/W4385002382",
    "https://openalex.org/W4389520703",
    "https://openalex.org/W4385292963",
    "https://openalex.org/W4387093135",
    "https://openalex.org/W4380994432",
    "https://openalex.org/W4377161536",
    "https://openalex.org/W4380355783",
    "https://openalex.org/W4379540056",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W4226476765",
    "https://openalex.org/W4378770449",
    "https://openalex.org/W6856496396",
    "https://openalex.org/W4385572906",
    "https://openalex.org/W4380994495",
    "https://openalex.org/W4389519042",
    "https://openalex.org/W4389519431",
    "https://openalex.org/W4386272986",
    "https://openalex.org/W4389519019",
    "https://openalex.org/W4385009323",
    "https://openalex.org/W4389520366",
    "https://openalex.org/W4389518608",
    "https://openalex.org/W4378713418",
    "https://openalex.org/W6856086692",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4389519979",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W4385572001",
    "https://openalex.org/W4387560241",
    "https://openalex.org/W4387221840",
    "https://openalex.org/W4389403907",
    "https://openalex.org/W3204112174",
    "https://openalex.org/W4389519438",
    "https://openalex.org/W4380994458",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W3169341408",
    "https://openalex.org/W6855498594",
    "https://openalex.org/W4386081064",
    "https://openalex.org/W4385572697",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W4386755570",
    "https://openalex.org/W6852033271",
    "https://openalex.org/W4381586491",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4384392932",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4387432111",
    "https://openalex.org/W4387432227",
    "https://openalex.org/W6852754783",
    "https://openalex.org/W4380136478",
    "https://openalex.org/W4327710776",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W4385571633",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4389519287",
    "https://openalex.org/W6856800273",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3182414670",
    "https://openalex.org/W4386908092",
    "https://openalex.org/W4379958452",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385734111",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W4385574034",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3014328670",
    "https://openalex.org/W2912904516",
    "https://openalex.org/W2998099211",
    "https://openalex.org/W2949884065",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2889048825",
    "https://openalex.org/W3016027030",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2897356710",
    "https://openalex.org/W3003928769",
    "https://openalex.org/W2970745243",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W3173765306",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W2963957489",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2998733856",
    "https://openalex.org/W2903314293",
    "https://openalex.org/W2949818215",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W3175591618",
    "https://openalex.org/W3173209146",
    "https://openalex.org/W2939876107",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W3175526286",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W2970243238",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W2963368301",
    "https://openalex.org/W2987553933",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W3089285634",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W3194676777",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3035733645",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2889968917",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W4385574176",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3170806096",
    "https://openalex.org/W2963912046",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W2953209111",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2963721761",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W2962996600",
    "https://openalex.org/W3205508273",
    "https://openalex.org/W3173360659",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W3100880133",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2251873470",
    "https://openalex.org/W2251568283",
    "https://openalex.org/W4226494438",
    "https://openalex.org/W4319996584",
    "https://openalex.org/W6800166007",
    "https://openalex.org/W3106221893",
    "https://openalex.org/W2970605129",
    "https://openalex.org/W2760505947",
    "https://openalex.org/W3177312484",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2945214158",
    "https://openalex.org/W2250709962",
    "https://openalex.org/W2998811572",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W3035566559",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2952179106",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W3189216228",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2604580630",
    "https://openalex.org/W4402671286",
    "https://openalex.org/W4391767064",
    "https://openalex.org/W6861581687",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4392019953",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3100560913",
    "https://openalex.org/W3114361292",
    "https://openalex.org/W3105662186"
  ],
  "abstract": "<title>Abstract</title> This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: \\href{https://github.com/lmmlzn/Awesome-LLMs-Datasets}{https://github.com/lmmlzn/Awesome-LLMs-Datasets}.",
  "full_text": "Datasets for Large Language Models: A\nComprehensive Survey\nYang Liu \nSouth China University of Technology\nJiahuan Cao \nSouth China University of Technology\nChongyu Liu \nSouth China University of Technology\nKai Ding \nINTSIG Information Co., Ltd\nLianwen Jin \nSouth China University of Technology\nResearch Article\nKeywords: Datasets, Large language models, Deep learning, Arti\u0000cial intelligence\nPosted Date: March 4th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3996137/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Arti\u0000cial Intelligence Review on October\n31st, 2025. See the published version at https://doi.org/10.1007/s10462-025-11403-7.\nDatasets for Large Language Models: A\nComprehensive Survey\nYang Liu1,3, Jiahuan Cao1, Chongyu Liu1, Kai Ding2,3,\nLianwen Jin1,3*\n1School of Electronic and Information Engineering, South China\nUniversity of Technology, Guangzhou, 510640, Guangdong, China.\n2INTSIG Information Co., Ltd, Shanghai, 200040, Shanghai, China.\n3INTSIG-SCUT Joint Lab on Document Analysis and Recognition,\nGuangzhou, 510640, Guangdong, China.\n*Corresponding author(s). E-mail(s): eelwjin@scut.edu.cn;\nContributing authors: ly10061105@gmail.com; jiahuanc@foxmail.com;\nliuchongyu1996@gmail.com; danny ding@intsig.net;\nAbstract\nThis paper embarks on an exploration into the Large Language Mod el (LLM)\ndatasets, which play a crucial role in the remarkable advancemen ts of LLMs.\nThe datasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequen tly, examina-\ntion of these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysi s of LLM\ndatasets, and to gain insights into their current status and fut ure trends, this sur-\nvey consolidates and categorizes the fundamental aspects of LLM datasets from\nﬁve perspectives: (1) Pre-training Corpora; (2) Instruction Fine -tuning Datasets;\n(3) Preference Datasets; (4) Evaluation Datasets; (5) Traditiona l Natural Lan-\nguage Processing (NLP) Datasets. The survey sheds light on the p revailing\nchallenges and points out potential avenues for future investi gation. Addition-\nally, a comprehensive review of the existing available datase t resources is also\nprovided, including statistics from 444 datasets, covering 8 la nguage categories\nand spanning 32 domains. Information from 20 dimensions is inco rporated into\nthe dataset statistics. The total data size surveyed surpasse s 774.5 TB for pre-\ntraining corpora and 700M instances for other datasets. We aim to present the\nentire landscape of LLM text datasets, serving as a comprehensi ve reference for\nresearchers in this ﬁeld and contributing to future studies. Relat ed resources are\navailable at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.\n1\nKeywords: Datasets, Large language models, Deep learning, Artiﬁcial intelligence\nLarge Language\nModel Datasets\nPre-training\nCorpora\n(Sec 2.)\nGeneral Pre-training\nCorpora\nWebpages\nLanguage Texts\nBooks\nAcademic Materials\nCode\nParallel Corpus\nSocial Media\nEncyclopedia\nMulti-category\nDomain-specific Pre-\ntraining Corpora\nFinancial\nMedical\nOther\nPreprocessing of Pre-\ntraining Data\nData Collection\nData Filtering\nData Deduplication\nData Standardization\nData Review\nInstruction Fine-\ntuning Datasets\n(Sec 3.)\nInstruction Catagory\nReasoning, Math, Brainstorming,\nClosedQA,OpenQA,Code,Extra\nction,Generation,Rewrite,Summ\narization,SocialNorms,Translatio\nn,Role-playing,Others\nGeneral Instruction Fine-\ntuning Datasets\nHuman Generated Datasets (HG)\nConstruct as required\nCrawl real human question\nand answer data\nModel Constructed Datasets\n(MC)\nSelf-Instruct\nInteraction data between\nhumans and LLMs\nConversations among\nmultiple LLM agents\nCollection and Improvement of\nExisting Datasets (CI) Collection and improvement\nDatasets Created with Multiple\nMethods\nHG & CI\nHG & MC\nCI & MC\nHG & CI & MC\nDomain-specific Instruction\nFine-tuning Datasets\nMedical\nCode\nLegal\nMathematics\nEducation\nOther\nPreference Datasets\n(Sec 4.)\nV ote\nV ote-Human\nV ote-Model\nSort Sort-Human\nScore\nScore-Human\nScore-Model\nOther\nStep Alignment\nSource Discrepancy\nChallenges and\nFuture Directions\n(Sec 7.)\nPre-training Corpora\nData Selection\nTimeliness\nQuality Assessment\nData Preprocessing\nBuilding the Ecosystem of Pre-\ntraining Corpora\nInstruction Fine-tuning\nDatasets\nSubdivision of Instruction\nCategories\nDomain Scarcity\nQuality Evaluation\nLegal and Ethical Risks\nPreference Datasets\nLimited Availability of\nResources\nPreference Evaluation Method\nSettings\nEvaluation Datasets\nEstablishment of Evaluation\nDatasets\nAddressing Evaluation Gaps\nChoosing and Improving\nEvaluation Approaches\nComprehensive Evaluation\nFramework\nTraditional NLP\nDatasets\n(Sec 6.)\nQuestion Answering\nReading Comprehension\nSelection & Judgment\nCloze Test\nAnswer Extraction\nUnrestricted QA\nKnowledge QA\nReasoning QA\nRecognizing Textual\nEntailment\nMath\nCoreference Resolution\nSentiment Analysis\nSemantic Matching\nText Generation\nText Translation\nText Summarization\nText Classification\nText Quality Evaluation\nText-to-Code\nNamed Entity Recognition\nRelation Extraction\nMultitask\nEvaluation Datasets\n(Sec 5.)\nEvaluation Domains\nGeneral\nExam\nSubject\nNLU\nReasoning\nKnowledge\nLong Text\nTool\nAgent\nCode\nOut-of-Distribution\nLaw\nMedical\nFinancial\nSocial Norms\nFactuality\nEvaluation\nMultitask\nMultilingual\nOther\nEvaluation Methods\nAutomated Evaluation\nCode Evaluation\nModel Evaluation\nNon-automated EvaluationHuman Evaluation\nFig. 1 The overall architecture of the survey. Zoom in for better view\n1 Introduction\nWith the release of ChatGPT (\nOpenAI, 2022), in just a few months, Large Language\nModels (LLMs) have attracted increasing research attention and becomea hot research\nfield. Various LLMs have been successively open-sourced, with parameter sizes ranging\nfrom several billion to over a hundred billion. Examples include the LLaMA (Touvron\net al, 2023a,b), Phi ( Gunasekar et al , 2023; Li et al , 2023k; Javaheripi et al , 2023),\nChatGLM (Du et al, 2022; Zeng et al, 2023a), QWen (Bai et al, 2023a), Baichuan (Yang\net al, 2023a), and so on. A considerable amount of work involves fine-tuning on base\n2\nLLM\nDatasets\n2020.6\n2023.3 2022.11\n2021.5\n2007 2019.10\nGPT-3\nGPT-4\n ChatGPT\nBookCorpusOpen\nC4\n2024.1\n2023.102023.6\n2023.3\n2021.1\n2023.8\n2019.2 2020.32019.11\n2024.2\n2021.9\n2023.1 2022.12\n2021.10\n......\n2023.4\nCommon Crawl\nDolma\nRedPajama-V2 RefinedWeb\nROOTS\nThe Pile\nWanJuanText-1.0 \nWebText\nProof-Pile-2\nCLUECorpus2020PG-19\nphi-1\nAlpaca_data\nAlpaca_GPT4_data\nAya Dataset \nFlan 2021\nFlan 2022\nHC3\nSelf-Instruct\nT0\n2023.11\n2022.112022.10\n2023.9\n2019.52018.11\n2020.12\n2021.12\n2023.7\n2022.4\n2023.5\nUltraChat\nBELLE\nMOSS\nInstructionWild_v2\nhh-rlhf\nCValues\nPKU-SafeRLHF\nUltraFeedback\nWebGPT\nSHP\nMT-Bench\nSuperCLUE\nAGIEval\nARB\nCLUE\nGLUE SuperGLUE \nAgentBench\nHumanEvalPack\nLawBench\nMultiMedQA\nHaluEval\nBBH\nHELM\nInfiniteBenchCulturaX\n2020.9\n2022.3\n2023.2\nBBT-FinCorpus\nBaize\nInstructGPT-sft\nOpenChat\nSUPER-NATURAL INSTRUCTIONS xP3\nDISC-Fin-SFT\nDISC-Law-SFTDISC-Med-SFT\nAlpaca comparison data\nChatbot arena conversations\nStack-Exchange-Preferences\nSummarize from Feedback\n2022.6\nBIG-Bench\nOpenMathInstruct-1\nFinBen\nFig. 2 A timeline of some representative LLM datasets. Orange represents pre-training\ncorpora, yellow represents instruction fine-tuning datasets, green represents preference\ndatasets, and pink represents evaluation datasets\nmodels, resulting in well-performing general conversational mode ls or domain-specific\nmodels. The widespread adoption of Reinforcement Learning from Human Fe edback\n(RLHF) and the refinement of LLM evaluations further optimize the perform ance of\nLLMs. The immense potential demonstrated by LLMs can be attributed, in p art, to\nthe datasets used for training and testing. As the saying goes, “You can’ t make a\nsilk purse out of a sow’s ear.” Without high-quality datasets as the foun dation, it is\nchallenging to grow the tree of LLMs with flourishing branches and leave s. Therefore,\nthe construction and analysis of LLM datasets is an area worthy of attention.\nThe development of text datasets has undergone several stages, from earli er Nat-\nural Language Processing (NLP) task datasets to the current era of LLM datasets . In\nthe 1960s to 1980s, the early stages of NLP primarily focused on fundamental tasks\nsuch as semantic analysis and machine translation. The dataset scale was re latively\nsmall and typically manually annotated. Later, the Message Understanding C onfer-\nence (MUC) (\nGrishman and Sundheim, 1996) began in 1987, focusing on datasets for\ntasks such as information extraction and Relation Extraction (RE). After 2000, the\n3\nNLP field continued to emphasize research on traditional tasks and linguis tic struc-\ntures, while also turning attention to emerging areas such as dialogue s ystems (Paek,\n2006; Yan et al, 2017; Devlin et al , 2019; Zhang et al , 2020b). With the rise of deep\nlearning, NLP datasets evolved towards larger scales, greater complexity, more diver-\nsity, and increased challenges. Simultaneously, comprehensive performance evaluations\n(Srivastava et al, 2023; Liang et al, 2023; Li et al, 2023n), dialogue datasets (Zeng et al,\n2020; Yang et al, 2023b; Ding et al, 2023), zero-shot and few-shot datasets (Hendrycks\net al, 2021b; Xu et al , 2021; Longpre et al , 2023a), multilingual datasets ( Conneau\net al , 2018; Siddhant et al , 2020; Costa-juss` a et al, 2022), and others emerged. By\nthe end of 2022, LLMs pushed datasets to a new peak, realizing a shift from a “t ask-\ncentric construction” to a “construction centered around tasks and st ages” in dataset\ndevelopment. LLM datasets are not only categorized based on tasks but also hav e\nassociations with different stages of LLMs. From the initial pre-trainin g stage to the\nfinal evaluation stage, we categorized LLM datasets into four types: pre-trai ning cor-\npora, instruction fine-tuning datasets, preference datasets, and ev aluation datasets.\nThe composition and quality of these datasets profoundly influence the p erformance\nof LLMs.\nThe current explosion in LLM datasets poses challenges for research. On t he one\nhand, it often leads to situations where it is difficult to know where to start when\ntrying to understand and learn about the datasets. On the other hand, there is a lack of\nsystematic organization regarding the differences in types, domain orie ntations, real-\nworld scenarios, etc., among various datasets. In order to reduce the le arning curve,\npromote dataset research and technological innovation, broaden public awar eness, we\nconduct a survey of LLM datasets. The objective is to provide research ers with a\ncomprehensive and insightful perspective, facilitating a bette r understanding of the\ndistribution and role of LLM datasets, thereby advancing the collectiveknowledge and\napplication of LLMs.\nThis paper summarizes existing representative datasets across five dimensions:\npre-training corpora , instruction ﬁne-tuning datasets , preference datasets ,\nevaluation datasets , and traditional NLP datasets . Moreover, it presents new\ninsights and ideas, discusses current bottlenecks, and explores f uture development\ntrends. We also provide a comprehensive review of publicly avail able dataset related\nresources. It includes statistics from 444 datasets across 8 language categories spanning\n32 different domains, covering information from 20 dimensions. The total data size\nsurveyed exceeds 774.5 TB for pre-training corpora and over 700M instances for other\ndatasets. Due to space constraints, this survey only discusses pur e text LLM datasets\nand does not cover multimodal datasets.\nTo the best of our knowledge, this is the first survey focused on LLM dat asets,\npresenting the entire landscape. The timeline of LLM datasets is sho wn in Figure 2.\nPrior to this, several LLM-related surveys, such as Zhao et al (2023) and Minaee et al\n(2024), analyze the latest developments in LLMs but lack detailed descript ions and\nsummaries of datasets. Zhang et al (2023g) summarizes the instruction fine-tuning\nstage of LLMs. Chang et al (2023) and Guo et al (2023c) summarize the evaluation\nstage. However, these surveys only concentrate on a part of the LLM datasets , and\ndataset-related information is not the central focus. In contrast to th e aforementioned\n4\nsurveys, our paper places emphasis on LLM datasets, aiming to provide a more detailed\nand exhaustive survey in this specific domain.\nThe overall organizational structure is illustrated in Figure 1. The remainder of\nthis paper is organized as follows. Section 2 summarizes general pre-training cor-\npora categorized by data types and domain-specific pre-training corpora categorized\nby domains. It also outlines the preprocessing steps and methods f or pre-training\ndata. Section 3 summarizes general instruction fine-tuning datasets categorized by\nconstruction methods and domain-specific instruction fine-tuning datasets categorized\nby domains. 15 instruction categories are provided. Section 4 summarizes prefer-\nence datasets categorized by preference evaluation methods. Section 5 summarizes\nevaluation datasets categorized by evaluation domains and synthesizes diff erent eval-\nuation methods. Section 6 summarizes traditional NLP datasets categorized by tasks.\nSection 7 briefly identifies challenges encountered within the datasets and an ticipates\nfuture research directions. Section 8 concludes this paper. Detailed descriptions of the\ndatasets can be found in Appendices A through E.\n2 Pre-training Corpora\nThe pre-training corpora are large collections of text data used during the pre-training\nprocess of LLMs. Among all types of datasets, the scale of pre-training corp ora is typ-\nically the largest one. In the pre-training phase, LLMs learn extensive knowledge from\nmassive amounts of unlabeled text data, which is then stored in its model parameters.\nIt enables LLMs to possess a certain level of language understanding and gen eration\ncapabilities. The pre-training corpora can encompass various types of t ext data, such\nas webpages, academic materials, books, while also accommodating relevant texts from\ndiverse domains, such as legal documents, annual financial reports, medical textbooks,\nand other domain-specific data.\nBased on the domains involved in the pre-training corpora, they can be di vided\ninto two types. The first type is the general pre-training corpora , which comprise\nlarge-scale text data mixtures from different domains and topics. The d ata commonly\nincludes text content from the Internet, such as news, social media, encyclopedias, and\nmore. The objective is to provide universal language knowledge and data resources for\nNLP tasks. The second type is the domain-speciﬁc pre-training corpora , which\nexclusively contain relevant data for specific domains or topics. The p urpose is to\nfurnish LLMs with specialized knowledge.\nAs the cornerstones of LLMs, the pre-training corpora influence the dir ection of\npre-training and the potential of models in the future. They play s everal pivotal roles\nas follows:\n• Providing Generality. Substantial amounts of text data help models better\nlearn the grammar, semantics, and contextual information of language, enabling\nthem to attain a universal comprehension of natural language.\n• Enhancing Generalization Ability. Data from diverse domains and topics\nallow models to acquire a broader range of knowledge during training, th ereby\nenhancing their generalization ability.\n5\nGeneral Pre-training\nCorpora\nEncyclopedia\nMulti Webpages Language\nTexts\nBooks\nAcademic\nMaterials\nCode\nParallel Corpus\nSocial M\nedia\nFig. 3 Data categories of the general pre-training corpora\n• Elevating Performance Levels. Knowledge injection from domain-specific\npre-training corpora enables models to achieve superior performanc e on down-\nstream tasks.\n• Supporting Multilingual Processing. The inclusion of multiple languages\nin pre-training corpora empowers models to grasp expressions across diverse\nlinguistic contexts, fostering the development of competencies f or cross-lingual\ntasks.\n2.1 General Pre-training Corpora\nThe general pre-training corpora are large-scale datasets composed of exte nsive text\nfrom diverse domains and sources. Their primary characteristic is that the text content\nis not confined to a single domain, making them more suitable for training general\nfoundational models. As illustrated in Figure\n3, the data types can be categorized into\neight major classes: Webpages, Language Texts , Books, Academic Materials ,\nCode, Parallel Corpus , Social Media , and Encyclopedia. The collected and\norganized information about general pre-training corpora is presented in Table 1 and\nTable 2.\n2.1.1 Webpages\nWebpages represent the most prevalent and widespread type of data in p re-training\ncorpora, comprised of text content obtained by crawling a large number of webpages\non the Internet. This type of data has several key characteristics.\n• Massive Scale. There is a vast number of websites, and new webpages emerge\ncontinuously.\n• Dynamism. Content undergoes continuous updates and changes over time.\n• Multilingualism. It may include content in multiple languages.\n6\nTable 1 Summary of General Pre-training Corpora Information Part I . Release\nTime: “X” indicates unknown month. Public or Not: “All” indicates full open source;\n“Partial” indicates partially open source; “Not” indicates not open source. “License”\nindicates the corpus follows a certain protocol. If the corpus is built upon other corpora,\nthe licenses of the source corpora must also be adhered to\nCorpus Publisher Release Time Size Public or Not License\nANC The US National Science Foundation et al. 2003-X - All -\nAnna’s Archive Anna 2023-X 641.2 TB All -\nArabicText 2022 BAAI et al. 2022-12 201.9 GB All CC-BY-SA-4.0\narXiv Paul Ginsparg et al. 1991-X - All Terms of Use for arXiv APIs\nBaidu baike Baidu 2008-4 - All Baidu baike User Agreement\nBIGQUERY Salesforce Research 2022-3 341.1 GB Not Apache-2.0\nBNC Oxford University Press et al. 1994-X 4124 Texts All -\nBookCorpusOpen Jack Bandy et al. 2021-5 17868 Books All Smashwords Terms of Service\nCC-Stories Google Brain 2018-7 31 GB Not -\nCC100 Facebook AI 2020-7 2.5 TB All Common Crawl Terms of Use\nCLUECorpus2020 CLUE Organization 2020-3 100 GB All MIT\nCommon Crawl Common Crawl 2007-X - All Common Crawl Terms of Use\nCulturaX University of Oregon et al. 2023-9 27 TB All mC4 & OSCAR\nC4 Google Research 2019-10 12.68 TB All ODC-BY & Common Crawl Terms of Use\nDolma AI2 et al. 2024-1 11519 GB All MR Agreement\nGithub Microsoft 2008-4 - All -\nmC4 Google Research 2021-6 251 GB All ODC-BY & Common Crawl Terms of Use\nMNBVC Liwu Community 2023-1 20811 GB All MIT\nMTP BAAI 2023-9 1.3 TB All BAAI Data Usage Protocol\nMultiUN German Research Center for Artificial Intelligence (DFKI) GmbH 2010-5 4353 MB All -\nNews-crawl UKRI et al. 2019-1 110 GB All CC0\nOpenWebText Brown University 2019-4 38 GB All CC0\nOSCAR 22.01 Inria 2022-1 8.41 TB All CC0\nParaCrawl Prompsit et al. 2020-7 59996 Files All CC0\nPG-19 DeepMind 2019-11 11.74 GB All Apache-2.0\nphi-1 Microsoft Research 2023-6 7 B Tokens Not CC-BY-NC-SA-3.0\nProject Gutenberg Ibiblio et al. 1971-X - All The Project Gutenberg\nPushshift Reddit Pushshift.io et al. 2020-1 2 TB All -\nRealNews University of Washington et al. 2019-5 120 GB All Apache-2.0\nReddit Cond´ e Nast Digital et al. 2005-6 - All -\nRedPajama-V1 Together Computer 2023-4 1.2 T Tokens All -\nRedPajama-V2 Together Computer 2023-10 30.4 T Tokens All Common Crawl Terms of Use\nRefinedWeb The Falcon LLM team 2023-6 5000 GB Partial ODC-BY-1.0\nROOTS Hugging Face et al. 2023-3 1.61 TB Partial BLOOM Open-RAIL-M\nSmashwords Draft2Digital et al. 2008-X - All Smashwords Terms of Service\nStackExchange Stack Exchange 2008-9 - All CC-BY-SA-4.0\nS2ORC AI2 et al. 2020-6 81.1 MB All ODC-BY-1.0\nThe Pile EleutherAI 2021-1 825.18 GB All MIT\nThe Stack ServiceNow Research et al. 2022-11 6 TB All The Terms of the Original Licenses\nTigerBot\npretrain en TigerBot 2023-5 51 GB Partial Apache-2.0\nTigerBot pretrain zh TigerBot 2023-5 55 GB Partial Apache-2.0\nTigerBot-wiki TigerBot 2023-5 205 MB All Apache-2.0\nToronto Book Corpus University of Toronto et al. 2015-6 11038 Books Not MIT & Smashwords Terms of Service\nUNCorpus v1.0 United Nations et al. 2016-5 799276 Files All -\nWanJuanText-1.0 Shanghai AI Laboratory 2023-8 1094 GB All CC-BY-4.0\nWebText OpenAI 2019-2 40 GB Partial MIT\nWikipedia Wikimedia Foundation 2001-1 - All CC-BY-SA-3.0 & GFDL\nWuDaoCorpora-Text BAAI et al. 2021-6 200 GB Partial CC-BY-NC-ND-4.0\nZhihu Beijing Zhizhe Tianxia Technology Co., Ltd 2011-1 - All Zhihu User Agreeme nt\n• Rich in Themes. It encompasses content from different domains and subjects.\n• Semi-structured. The data is typically in hypertext markup language (HTML)\nformat, exhibiting certain structural characteristics. However, it may include\nvarious modalities such as text, images, videos, and more.\n• Requires Cleaning. It often contains a significant amount of noise, irrelevant\ninformation, and sensitive content, making it unsuitable for direct use.\nThe construction of webpages corpora is commonly pursued through two pri mary\napproaches. The first method involves building upon Common Crawl 1. Common\nCrawl is a massive, unstructured, multilingual web corpus that pr ovides public access\nto web archives by regularly crawling and storing webpage data from the Internet.\nHowever, the data in Common Crawl are not clean, containing a lot of irrelevant infor-\nmation, such as advertisements, navigation bars, etc. Additionally, there is a presence\nof pornographic content, violence, machine-generated spam, and sensitive information\ninvolving personal privacy. Consequently, many subsequent pre-t raining corpora are\n1https://commoncrawl.org/\n7\nTable 2 Summary of General Pre-training Corpora Information Part II . Lan-\nguage: “EN” indicates English, “ZH” indicates Chinese, “AR” indicates Arabi c, “PL”\nindicates Programming Language, “Multi” indicates Multilingual, and the n umber in\nparentheses indicates the number of languages included. “CM” indicates Construction\nMethods, where “HG” indicates Human Generated Corpora, “MC” indicates M odel\nConstructed Corpora, and “CI” indicates Collection and Improvement of E xisting\nCorpora\nCorpus Language CM Category Source\nANC EN HG Language Texts American English texts\nAnna’s Archive Multi HG Books Sci-Hub, Library Genesis, Z-Library, etc .\nArabicText 2022 AR HG & CI Multi ArabicWeb, OSCAR, CC100, etc.\narXiv EN HG Academic Materials arXiv preprint\nBaidu baike ZH HG Encyclopedia Encyclopedic content data\nBIGQUERY PL CI Code BigQuery\nBNC EN HG Language Texts British English texts\nBookCorpusOpen EN CI Books Toronto Book Corpus\nCC-Stories EN CI Webpages Common Crawl\nCC100 Multi (100) CI Webpages Common Crawl\nCLUECorpus2020 ZH CI Webpages Common Crawl\nCommon Crawl Multi HG Webpages Web crawler data\nCulturaX Multi (167) CI Webpages mC4, OSCAR\nC4 EN CI Webpages Common Crawl\nDolma EN HG & CI Multi Project Gutenberg, C4, Reddit, etc.\nGithub PL HG Code Various code projects\nmC4 Multi (108) CI Webpages Common Crawl\nMNBVC ZH HG & CI Multi Chinese books, webpages, theses, etc.\nMTP EN & ZH HG & CI Parallel Corpus Chinese-English parallel text pairs on the web\nMultiUN Multi (7) HG Parallel Corpus United Nations documents\nNews-crawl Multi (59) HG Language Texts Newspapers\nOpenWebText EN HG Social Media Reddit\nOSCAR 22.01 Multi (151) CI Webpages Common Crawl\nParaCrawl Multi (42) HG Parallel Corpus Web crawler data\nPG-19 EN HG Books Project Gutenberg\nphi-1 EN & PL HG & MC Code The Stack, StackOverflow, GPT-3.5 Generati on\nProject Gutenberg Multi HG Books Ebook data\nPushshift Reddit EN CI Social Media Reddit\nRealNews EN CI Webpages Common Crawl\nReddit EN HG Social Media Social media posts\nRedPajama-V1 Multi HG & CI Multi Common Crawl, Github, books, etc.\nReaPajama-V2 Multi (5) CI Webpages Common Crawl, C4, etc.\nRefinedWeb EN CI Webpages Common Crawl\nROOTS Multi (59) HG & CI Multi OSCAR, Github, etc.\nSmashwords Multi HG Books Ebook data\nStackExchange EN HG Social Media Community question and answer data\nS2ORC EN CI Academic Materials MAG, arXiv, PubMed, etc.\nThe Pile EN HG & CI Multi Books, arXiv, Github, etc.\nThe Stack PL (358) HG Code Permissively-licensed source code files\nTigerBot\npretrain en EN CI Multi English books, webpages, en-wiki, etc\nTigerBot pretrain zh ZH HG Multi Chinese books, webpages, zh-wiki, etc.\nTigerBot-wiki ZH HG Encyclopedia Baidu baike\nToronto Book Corpus EN HG Books Smashwords\nUNCorpus v1.0 Multi (6) HG Parallel Corpus United Nations documents\nWanJuanText-1.0 ZH HG Multi Webpages, Encyclopedia, Books, etc\nWebText EN HG Social Media Reddit\nWikipedia Multi HG Encyclopedia Encyclopedic content data\nWuDaoCorpora-Text ZH HG Webpages Chinese webpages\nZhihu ZH HG Social Media Social media posts\nderived by reselecting and cleaning data from Common Crawl. For inst ance, Refined-\nWeb (Penedo et al , 2023), used for pre-training Falcon model 2, undergoes rigorous\n2https://falconllm.tii.ae/\n8\nfiltering and deduplication processes on Common Crawl. It ultimatel y retains high-\nquality English text totaling 5T tokens. C4 (Raffel et al, 2020), derived from Common\nCrawl crawler data from April 2019, undergoes processing with multiple filters, remov-\ning useless, harmful, and non-English text. In contrast to C4, mC4 ( Xue et al , 2021)\n, CC100 (Conneau et al, 2020), OSCAR 22.01 ( Abadji et al, 2022), and RedPajama-\nV2 ( Together, 2023) retain multilingual data during the cleaning process, utilizin g\ndifferent cleaning pipelines. CC-Stories ( Trinh and Le , 2018) and RealNews ( Zellers\net al, 2019b) are selected subsets of text content from Common Crawl based on spe-\ncific themes. CC-Stories filters out text with a story-like style following the Winograd\nSchema (Levesque et al , 2012) for common-sense reasoning and language modeling.\nRealNews (Zellers et al, 2019b) extracts a substantial amount of webpages dedicated\nto news to obtain news data. The above corpora either exclusively cont ain English\nor belong to multilingual mixes. CLUECorpus2020 ( Xu et al , 2020c) conducts data\ncleaning on the Chinese portion of Common Crawl, resulting in a high-quality Chinese\npre-training corpus of 100GB. However, there still exists a small amoun t of noise in\nthese corpora. Therefore, some corpora continue with subsequent cle aning efforts. For\ninstance, CulturaX (Nguyen et al, 2023) performs a multi-stage cleaning process after\ncombining mC4 and OSCAR corpora, resulting in higher-quality multil ingual corpus.\nThe second method involves independently crawling various raw webpages\nand then employing a series of cleaning processes to obtain the ﬁnal cor-\npus. WuDaoCorpora-Text ( Yuan et al , 2021) is cleaned using over 20 rules from\n100TB of raw webpages, covering many domains such as education and technology.\nFurthermore, webpage data in some multi-category corpora is also constru cted using\nthis method, including MNBVC (\nMOP-LIWU Community and MNBVC Team, 2023),\nWanJuanText-1.0 (He et al, 2023a), TigerBot pretrain zh corpus (Chen et al, 2023c),\nand others.\n2.1.2 Languages Texts\nThe language text data mainly consists of two parts. The first part is electronic text\ndata constructed based on widely sourced written and spoken language ,\ntypically in the form of large corpora for a specific language. The full name of ANC\n3\nis the American National Corpus. The content primarily includes various written and\nspoken materials in American English. The second edition of the corpus h as a scale of\n22M words, making it highly suitable for models to learn language. Similarl y, BNC4,\nshort for the British National Corpus, encompasses 100M words of electronic t ext\nresources, covering spoken and written materials in British Engli sh.\nThe second part is electronic text data constructed based on relevant writ-\nten materials in various ﬁelds or topics . For example, FinGLM (MetaGLM, 2023)\ncovers annual reports of some listed companies between 2019 and 2021. The data type\nbelongs to language text materials in the financial domain. TigerBot-law ( Chen et al,\n2023c) includes legal regulations from 11 categories such as the Chinese Consti tution\nand the Chinese Criminal Law, falling within the language text material s in the legal\n3https://anc.org/\n4http://www.natcorp.ox.ac.uk/\n9\ndomain. News-crawl5 extracts monolingual texts from online newspapers and other\nnews sources, encompassing news text in 59 languages.\n2.1.3 Books\nBook data is also one of the common types of data in pre-training corpora. Com-\npared to webpages, books have longer textual content and superior data qu ality,\nboth of which contribute to enhancing the performance of LLMs. This hel ps improve\ntheir ability to capture human language features while learning more pr ofound lan-\nguage knowledge and contextual information. The book data primarily posses ses the\nfollowing characteristics.\n• Breadth. It typically covers a wide range of subjects and topics, including\nnovels, biographies, textbooks, and more.\n• High Quality. Books are usually authored by professionals, undergo editing and\nproofreading, resulting in more accurate grammar and spelling with les s noise.\n• Lengthy Text. Longer texts and complex sentence structures provide additional\ncontextual information.\n• Language and Culture. Books often contain rich language features such as\nprofessional terminology, colloquialisms, and idioms, reflecting div erse cultural\nbackgrounds.\nBook data can be found on e-book websites, with commonly used resources being\nSmashwords\n6 and Project Gutenberg 7. Smashwords is a large repository of free e-\nbooks, containing over 500K electronic books. Project Gutenberg, as the earliest digital\nlibrary, is dedicated to digitizing and archiving cultural works, an d it also boasts a\nwealth of book resources.\nSubsequently, many book corpora are constructed by scraping and cleaning e-book\nresources. In 2015, Toronto Book Corpus ( Zhu et al , 2015) crawled 11,038 e-books\nfrom Smashwords, forming a large-scale corpus of books. This corpus was once publicly\navailable but is no longer accessible. In 2019, PG-19 ( Rae et al, 2020) collected books\npublished before 1919 from Project Gutenberg and removed short-text books, resulting\nin a final count of 28,752 books. In 2021, BookCorpusOpen ( Bandy and Vincent ,\n2021) built upon Toronto Book Corpus, Smashwords, and others, creating 17,868 book\nentries. In 2023, Anna’s Archive 8 became the world’s largest open-source and open-\ndata library. The creator scraped books from libraries such as Libgen, Sc i-Hub, and\nmade them publicly available. As of February 2024, its size has reached 641.2TB and\nit is continuously growing.\nIt is worth mentioning that the fields covered by books are extremel y diverse.\nThus, fine-grained categorization of books by domain is feasible. It not onl y facilitates\nmore convenient gap analysis and supplementation but also enables the eas y selection\nof relevant data when focusing on specific domains. Referring to the Chinese Library\nClassification System9, books can be straightforwardly categorized into 30 classes, as\nillustrated in Figure 4 for reference.\n5https://data.statmt.org/news-crawl/\n6https://www.smashwords.com/\n7https://www.gutenberg.org/\n8https://annas-archive.org/datasets\n9http://www.ztﬂh.com/\n10\nBook Categories\nReligion\nLaw\nMusic\nHistory\nGeneral Works\nPhilosophy\nEducation\nLiterature\nMilitary\nSports\nCulture\nRecreation\nAgriculture\nEconomy\nTechnology\nGeoscience\n Language \nMedicine\nMathematics\nTransportation\nFine arts\nPolitics\nBiology\nOthers\nGeography\nAstronomy\nSociology\nPsychology\nChemistry\nPhysics\nFig. 4 Classification of books. Categorizing books into 30 fine-grained classes bas ed\non different domains\n2.1.4 Academic Materials\nAcademic material data refers to text data related to the academic field, including but\nnot limited to academic papers, journal articles, conference papers, research reports,\npatents, and more. These data are authored and published by experts and scholars in\nthe academic community, possessing a high level of professionalism andacademic rigor.\nThe academic materials themselves exhibit exceptional quality. In corporating them\ninto pre-training corpora can provide more accurate and professional in formation,\nhelping the model understand the terminology and knowledge within the academic\ndomain.\nThe most commonly used corpus currently is arXiv\n10, which gathers preprints\nof papers in physics, mathematics, computer science, biology, and quan titative eco-\nnomics. It not only furnishes high-quality academic knowledge but also enables models\nto grasp the LATEX format of papers. In addition to arXiv, S2ORC ( Lo et al, 2020)\nencompasses English academic papers from various disciplines. It feat ures extensive\nmetadata, abstracts, reference lists, and structured full-text c ontent. In the medical\nfield, PubMed Central11 has played a role in the open access of nearly 5M biomedical\npublications.\nPre-training corpora exclusively consisting of academic material dat a are rare, as\nmost multi-category corpora choose to include academic materials. In Th e Pile ( Gao\net al, 2020), academic material data accounts for 38.1%, surpassing the 18.1% propor-\ntion of Webpage data. In RedPajama-V1 12, the proportion of academic materials is\n2.31%, totaling 28 billion tokens.\n2.1.5 Code\nThe category of code data refers to textual information containing programm ing lan-\nguages, such as Python, Java, C++, and other code snippets. Its purpose is to assist\nmodels in better understanding programming languages and code structures, enabling\n10https://arxiv.org/\n11https://www.ncbi.nlm.nih.gov/pmc/\n12https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\n11\nthem to perform well in downstream tasks like code comprehension, code recommen-\ndation, and code generation. Nowadays, LLMs are often leveraged to generate co de,\nfacilitating various tasks. The quality of the code data used during m odel training\ndirectly impacts the effectiveness of the generated code, unders coring the significance\nof code data in model performance.\nThe main corpora for code data include The Stack ( Kocetkov et al , 2023), BIG-\nQUERY (Nijkamp et al, 2023), and Github13. The Stack comprises diverse collection\nof 385 programming languages and hosts over 6TB of source code files with open-\nsource licenses. It is specifically tailored for the development of expansive LLMs in the\nprogramming domain. BIGQUERY, a subset of the publicly released Google BigQuery\ncorpus\n14, focuses on six selected programming languages. Github serves as a host ing\nplatform for both open-source and private software projects, supplyin g a rich array of\nvaried code information. Notably, training data for significant code mode ls like Star-\nCoder (Li et al, 2023j) is sourced from this repository. However, it is crucial to exercise\ncaution during web scraping to adhere to the code usage protocols set b y project\nauthors. StackOverflow15 is also a common source of code data. As a Question-and-\nAnswer (Q&A) community dedicated to programming and development, it f eatures\nquestions and answers spanning topics such as programming languages, deve lopment\ntools, and algorithms. StackOverflow is part of StackExchange 16, which houses differ-\nent Q&A sections. Therefore, it is categorized as social media data, as e xplained in\nSection 2.1.7. More recently, phi-1 ( Gunasekar et al , 2023) is created specifically for\ntraining code models. It not only includes a subset of code select ed from The Stack\nand StackOverflow but also utilizes GPT-3.5 ( OpenAI, 2023) to generate textbooks\nand exercise questions related to Python.\n2.1.6 Parallel Corpus\nParallel corpus data refers to a collection of text or sentence pairs fr om different\nlanguages. These pairs of texts are translations of each other, where one tex t is in the\nsource language (e.g., English), and the corresponding text is in the tar get language\n(e.g., Chinese). The incorporation of parallel corpus data is crucial for enhancing the\nmachine translation capability and cross-lingual task performance of LLMs.\nThe collection of parallel corpora typically occurs through two main ave nues.\nThe first involves extracting text from Internet resources such as webpages .\nParaCrawl (\nBa˜ n´ on et al, 2020), for instance, utilizes open-source software to crawl\nwebpages, constructing a publicly available parallel corpus. It enc ompasses 223M fil-\ntered sentence pairs. Similarly, MTP17 collects and organizes existing Chinese-English\nweb text data, amassing a total of 300M text pairs. This stands as the currently largest\nopen-source Chinese-English aligned text pair dataset.\n13https://github.com/\n14https://cloud.google.com/bigquery?hl=en\n15https://stackovjerﬂow.com/\n16https://stackexchange.com/\n17https://data.baai.ac.cn/details/BAAI-MTP\n12\nThe second approach involves the collection of parallel corpora from United\nNations multilingual documents . MultiUN ( Eisele and Chen , 2010) gathers par-\nallel text pairs through the United Nations Official Document System 18. These\ndocuments cover the six official languages of the United Nations (Arabic, Chi nese,\nEnglish, French, Russian, and Spanish), as well as a limited amount of G erman.\nUNCorpus v1.0 (Ziemski et al, 2016) consists of public domain United Nations official\nrecords and other conference documents, aligned at the sentence lev el.\n2.1.7 Social Media\nSocial media data refers to textual content collected from various me dia platforms,\nprimarily encompassing user-generated posts, comments, and dialogue d ata between\nusers. The data reflects real-time dynamics and interactivity among individuals on\nsocial media. Despite the potential presence of harmful information s uch as biases,\ndiscrimination, and violence in social media data, it remains essent ial for the pre-\ntraining of LLMs. This is because social media data is advantageous for mode ls to\nlearn expressive capabilities in conversational communication and to c apture social\ntrends, user behavior patterns, and more.\nThe crawling of data on English social media platforms is commonly condu cted\non platforms such as StackExchange\n19 and Reddit 20. StackExchange is a collection\nof Q&A pairs covering various topics and stands as one of the largest public ly avail-\nable repositories of such pairs. Spanning topics from programming to cul inary arts, it\nincorporates a wide range of subjects. Reddit includes a substantial number of user-\ngenerated posts along with the corresponding upvote and downvote counts for each\npost. In addition to serving as social media data, Reddit can also be use d to construct\na human preference dataset based on the vote counts. WebText ( Radford et al, 2019)\ncrawls social media text from 45M webpages on Reddit, ensuring that eac h link has\nat least 3 upvotes to guarantee data quality. However, only a tiny fractionof WebText\nis publicly available. Therefore, OpenWebText (Gokaslan and Cohen, 2019) replicates\nthe construction method of WebText and open-sources the collected social media data.\nPushshift Reddit ( Baumgartner et al , 2020) has been collecting Reddit data since\n2015, providing real-time monthly updates to reduce the time costs for researchers.\nChinese social media data is typically collected from platforms suc h as Zhihu 21\nand so on. Zhihu contains high-quality Chinese Q&A pairs and user-cre ated content,\nmaking it highly favored for training Chinese LLMs.\n2.1.8 Encyclopedia\nEncyclopedia data refers to textual information extracted from ency clopedias, online\nencyclopedia websites, or other knowledge databases. The data from online encyclope-\ndia websites is written and edited by experts, volunteers, or com munity contributors,\nproviding a certain level of authority and reliability. Due to its e ase of accessibility, it\n18https://documents.un.org/\n19https://stackexchange.com/\n20www.reddit.com\n21https://www.zhihu.com/\n13\nArabicT ext 2022\n100.0%\n90.6%\n0.9%\n1.1%\n7.4%\nRedPajama- V1\n100.0%\n87.0%\n4.9%\n2.0%\n2.1%2.3% 1.7%\nThe Pile\n100.0%\n18.1%\n7.6%\n1.5%\n15.0%\n38.1%\n16.6%\n3.1%\nTigerBot pretrain en\n100.0%\n15.0%\n30.2%\n8.7%\n35.9%\n10.2%\nTigerBot pretrain zh\n100.0%\n26.9%\n21.5%25.9%\n1.9%\n23.8%\nW anJuanT ext-1.0\n100.0%\n96.7%\n0.01%\n0.07%\n2.1% 1.1%\n17.7%\n7.5%\n1.5%\n14.7%\n37.4%\n16.3%\n3.1%6.4%\nWebpages Code Encyclopedia Books Academic Materials Social Media Language T exts Multi\nFig. 5 Pie charts depicting the data type distribution of selected mult i-category pre-\ntraining corpora. The corresponding pre-training corpus names are posi tioned above\neach pie chart. Different colors represent distinct data types\nis included at a higher frequency in pre-training corpora, serving as a cornerstone in\nenhancing the knowledge base of LLMs.\nThe most common encyclopedia corpus is Wikipedia\n22. It possesses characteristics\nsuch as being free, open-source, multilingual, and having high tex tual value. Fre-\nquently, specific language data from Wikipedia is selected, crawle d, and filtered to\nserve as part of the pre-training corpus. In relation to Chinese-langu age encyclopedia\ncorpora, in addition to the Chinese version of Wikipedia, there is also the Baidu baike\ncorpus23. It covers almost all knowledge domains. TigerBot-wiki ( Chen et al , 2023c)\nis filtered from the data of Baidu baike.\n2.1.9 Multi-category Corpora\nMulti-category corpora contain two or more types of data, which is benefici al for\nenhancing the generalization capabilities of LLMs. During model pre-training, one can\neither choose existing open-source multi-category corpora directl y for pre-training or\nselect multiple single-category corpora for a certain proportion of mixi ng. To gain a\nclear understanding of the distribution of various data types within certain multi-\ncategory corpora, pie charts are presented here in Figure\n5.\nIn English, there are several multi-category corpora, including RedPajama-V1, The\nPile (Gao et al, 2020), TigerBot pretrain en (Chen et al, 2023c) and Dolma ( Soldaini\net al, 2024). RedPajama-V1 is a partial replication of the pre-training corpora use d\nin the LLaMA model, based on the reports ( Touvron et al , 2023a). It encompasses\nsix data types, with webpage data constituting the majority at 87.0%. Th e overall\npresentation exhibits a skewed data distribution. In contrast, Th e Pile has a richer\n22https://www.wikipedia.org/\n23https://baike.baidu.com/\n14\nvariety of data types, with a more evenly distributed proportion. It is a combination\nof various subsets, aiming to capture text in as many forms as possible. S imilarly,\nTigerBot pretrain en selects five types of data from open-source corpora, striving for\na balanced distribution. To advance open research in the field of pretr aining models,\nthe Dolma English corpus, comprising 3T tokens, has been publicly re leased. This\ncorpus amalgamates content sourced from six distinct domains, namely web pages,\nacademic materials, code, books, social media, and encyclopedia. Furthermore, Dolma\nprovides specific processing guidelines for each data type alongsid e a comprehensive\ndata curation toolkit.\nChinese multi-category corpora include MNBVC ( MOP-LIWU Community and\nMNBVC Team, 2023) and TigerBot pretrain zh (Chen et al , 2023c). MNBVC does\nnot provide the distribution of data types but encompasses pure-te xt Chinese data in\nvarious forms like news, novels, magazines, classical poetry, chat re cords, and more.\nIts goal is to reach 40TB of data, aiming to match ChatGPT. The data collection i s\nstill ongoing. TigerBot pretrain zh focuses on web content, encyclopedias, books, and\nlanguage texts.\nApart from the common Chinese and English corpora, the Beijing Academy of\nArtificial Intelligence collaborates with other institutions to buil d the largest open-\nsource Arabic pre-training corpus globally, known as ArabicText 2022 24. It can be\nused for training Arabic LLMs.\nThere are two multilingual and multi-category corpora, namely WanJuanTex t-1.0\n(He et al , 2023a) and ROOTS ( Lauren¸ con et al, 2022). WanJuanText-1.0 consists\nof bilingual Chinese-English data collected from various sources suc h as webpages,\npatents, and exam questions. The data is uniformly processed and form atted into\njsonl. ROOTS includes 46 natural languages and 13 programming languages, with a\ntotal size of 1.6TB.\n2.2 Domain-speciﬁc Pre-training Corpora\nDomain-specific pre-training corpora tailored for specific fields or top ics. The type\nof corpus is typically employed in the incremental pre-training p hase of LLMs. After\ntraining a base model on a general pre-training corpus, if the model needs to be applied\nto downstream tasks in a particular domain, domain-specific pre-train ing corpora can\nbe further utilized to incrementally pre-train the model. Thi s process enhances the\nmodels’ capabilities in a specific domain while building upon a foun dation of general\nproficiency gained from the initial general pre-training. The collecte d and organized\ninformation from the domain-specific pre-training corpora is present ed in Table\n3 and\nTable 4. The categorization of the corpus is shown in Figure 6.\n2.2.1 Financial Domain\nThe pre-training corpora in the financial domain contribute to the lear ning of top-\nics related to the financial market, economics, investment, and financ e for LLMs.\nText data is normally sourced from financial news, financial statements, company\nannual reports, financial research reports, financial literature, marke t data, etc.\n24https://data.baai.ac.cn/details/ArabicText-2022\n15\nDomain-specific\nPre-training Corpora\nM\nedical\nTransportation\nM\nathem\natics\nLegal\nFinancial\nFig. 6 Domain categories of the domain-specific pre-training corpora\nTable 3 Summary of Domain-speciﬁc Pre-training Corpora Information Part\nI. Public or Not: “All” indicates full open source; “Partial” indicates par tially open\nsource. “License” indicates the corpus follows a certain protocol. If the corpus is built\nupon other corpora, the licenses of the source corpora must also be adhere d to\nCorpus Publisher Release Time Size Public or Not License\nBBT-FinCorpus Fudan University et al. 2023-2 256 GB Partial -\nFinCorpus Du Xiaoman 2023-9 60.36 GB All Apache-2.0\nFinGLM Knowledge Atlas et al. 2023-7 69 GB All Apache-2.0\nMedical-pt Ming Xu 2023-5 632.78 MB All Apache-2.0\nProof-Pile-2 Princeton University et al. 2023-10 55 B Tokens All -\nPubMed Central NCBI 2000-2 - All PMC Copyright Notice\nTigerBot-earning TigerBot 2023-5 488 MB All Apache-2.0\nTigerBot-law TigerBot 2023-5 29.9 MB All Apache-2.0\nTigerBot-research TigerBot 2023-5 696 MB All Apache-2.0\nTransGPT-pt Beijing Jiaotong University 2023-7 35.8 MB All Apache-2.0\nBBT-FinCorpus ( Lu et al , 2023a) is a large-scale Chinese financial domain corpus,\ncomprising four sections: company announcements, research reports, financial news,\nand social media. It is utilized for pre-training BBT-FinT5 base mo de ( Lu et al ,\n2023a). Analogously, the pre-training corpus FinCorpus (Zhang and Yang, 2023) used\nby XuanYuan (Zhang and Yang, 2023) consists of company announcements, financial\ninformation and news, financial exam questions. FinGLM ( MetaGLM, 2023) covers\nannual reports of listed companies from 2019 to 2021. TigerBot-research ( Chen et al,\n2023c) and TigerBot-earning ( Chen et al, 2023c) focus on research reports and finan-\ncial reports, respectively. It can be observed that the data type in the financial domain\nare generally similar, with differences in data timeframes, source w ebsites, and other\nfactors.\n16\nTable 4 Summary of Domain-speciﬁc Pre-training Corpora Information Part\nII. Language: “EN” indicates English, “ZH” indicates Chinese. “CM” indicates Con-\nstruction Methods, where “HG” indicates Human Generated Corpora, and “CI ”\nindicates Collection and Improvement of Existing Corpora\nCorpus Language CM Domain Category Source\nBBT-FinCorpus ZH HG Finance Multi Company announcements, research re ports, financial news, social media\nFinCorpus ZH HG Finance Multi Company announcements, financial news, fin ancial exam questions\nFinGLM ZH HG Finance Language Texts Annual Reports of Listed Companies\nMedical-pt ZH CI Medical Multi Medical encyclopedia data, medical te xtbooks\nProof-Pile-2 EN HG & CI Math Multi ArXiv, OpenWebMath, AlgebraicStack\nPubMed Central EN HG Medical Academic Materials Biomedical scientific literature\nTigerBot-earning ZH HG Finance Language Texts Financial reports\nTigerBot-law ZH HG Law Language Texts Legal clauses\nTigerBot-research ZH HG Finance Language Texts Research reports\nTransGPT-pt ZH HG Transportation Multi Technology documents, engineer ing construction information, statistical data, etc.\n2.2.2 Medical Domain\nPre-training corpora in the medical field can provide learning mete rials for LLMs on\ntopics such as diseases, medical technologies, drugs, and medical rese arch. Data is\nusually sourced from medical literature, healthcare diagnostic record s, case reports,\nmedical news, medical textbooks, and other related sources. Medical -pt (\nXu, 2023)\nhas been enhanced using open-access medical encyclopedias and medic al text-\nbook datasets, while PubMed Central has opened access to publications related to\nbiomedical research.\n2.2.3 Other Domains\n• Legal Domain. Legal text data typically originates from legal documents, law\nbooks, legal clauses, court judgments and cases, legal news, and other legal\nsources. For instance, TigerBot-law (\nChen et al , 2023c) has compiled 11 cate-\ngories of Chinese law and regulations for model learning. Some multi-cate gory\ncorpora have also incorporated data scraped from legal-related websites, such as\nThe Pile (Gao et al, 2020).\n• Transportation Domain. TransGPT (Duomo, 2023), as the first open-source\nlarge-scale transportation model in China, has provided the academic com mu-\nnity with the TransGPT-pt corpus ( Duomo, 2023). The corpus includes rich\ndata related to transportation, such as literature on transportation, tran sporta-\ntion technology projects, traffic statistics, engineering construct ion information,\nmanagement decision information, transportation terminology, etc.\n• Mathematics Domain. Proof-Pile-2 ( Azerbayev et al , 2023) gathers mathe-\nmatical-related code (in 17 programming languages), mathematical web data\nand mathematical papers. It has been utilized to train the mathematical LLMs\nLlemma (\nAzerbayev et al, 2023). The knowledge in this corpus is up-to-date as\nof April 2023.\n2.3 Distribution Statistics of Pre-training Corpora\nFigure\n7 provides statistics on 59 pre-training corpora across six aspects: re lease time,\nlicense, data category, construction method, language, and domain. Some observations\nand conclusions are drawn as follows:\n17\n3\n10\n4\n5 5\n4 4\n22\n0\n5\n10\n15\n20\n25\nBefore 2000 2000-2010 2011-2018 2019 2020 2021 2022 2023\nThe Number of Pre-training Corpora\nRelease Time\n(a)\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n3\n4\n5\n5\n5\n12\n0 2 4 6 8 10 12\nTerms of Use for arXiv APIs\nBaidu baike User Agreement\nBLOOM Open-RAIL-M\nBAAI Data Usage Protocol\nCC-BY-4.0\nCC-BY-SA-3.0\nGFDL\nCC-BY-NC-ND-4.0\nZhihu User Agreement\nPMC Copyright Notice\nCC-BY-SA-4.0\nSmashwords Terms of Service\nMIT\nCommon Crawl Terms of Use\nODC-BY\nCC0\nApache-2.0\nThe Number of Pre-training Corpora\nLicense\n(b)\n13\n12\n7\n6 6\n4 4\n3 3\n0\n2\n4\n6\n8\n10\n12\n14\nMulti\nWebpages\nLanguage Texts\nBooks\nSocial Media Parallel Corpus\nCode\nAcademic Materials\nEncyclopedia\nThe Number of Pre-training Corpora\nData Category\n(c)\n34\n16\n7\n1\n0\n5\n10\n15\n20\n25\n30\n35\nHG CI HG & CI HG & MC\nThe Number of Pre-training Corpora\nConstruction Method\n(d)\n20\n16 16\n3\n1 1 1\n0\n5\n10\n15\n20\nEN ZH Multi PL EN & ZH EN & PL AR\nThe Number of Pre-training Corpora\nLanguage\n(e)\n48\n5\n2 1 1 1\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nGeneral Finance Medical Math Law Transportation\nThe Number of Pre-training Corpora\nDomain\n(f)\nFig. 7 Statistics distribution of pre-training corpora. (a) illustrates the quantity trend\nover time. (b) depicts the quantity distribution under differe nt licenses, considering\nonly the corpora with listed licenses. (c) shows the quantity dis tribution across dif-\nferent data categories. (d) displays the quantity distribution for different construction\nmethods. (e) represents the quantity distribution across differ ent languages. (f) illus-\ntrates the quantity distribution across different domains. Zoom in for better view\n(1) The growth of pre-training corpora was relatively slow before 2018, gradu -\nally accelerating until the release of BERT ( Devlin et al , 2019), which marked the\nemergence of pre-trained models and a subsequent increase in pre- training corpora.\nThe subsequent introduction of models such as GPT-2 ( Radford et al, 2019), GPT-3\n(Brown et al , 2020), T5 ( Raffel et al , 2020), and others continued to drive develop-\nment. However, there were not many open-source pre-training corpora. It wasn’t until\nthe end of 2022 when OpenAI released ChatGPT, attracting unprecedente d atten-\ntion to LLMs. The construction and open-sourcing of pre-training corpora experienced\nexplosive growth in 2023.\n(2) The Apache-2.0, ODC-BY, CC0 and Common Crawl Terms of Use licenses are\ncommonly employed in pre-training corpora, offering relatively perm issive restrictions\nfor commercial use. Before utilizing any pre-training corpus, it is suggested to review\nthe specific terms and conditions of the applicable license to ensur e compliance with\nrelevant regulations.\n(3) The diversity of data types in pre-training corpora can impact the overall\nquality of LLMs. Models experience greater improvements when train ed on corpora\nwith a more diverse range of types. Hence, multi-category corpora are pre ferred, and\nthey are the most numerous. Looking at singular data types, webpage data stands out\nas the most common in corpora due to its ease of access, large scale, and exten sive\ncontent (as indicated in Figure 7 (c)).\n(4) Corpora necessitate the collection of extensive data and undergo ri gorous\ncleaning processes. Most often, approaches involve either direct m anual construc-\ntion or improvement upon existing open-source data. Occasionally, a com bination of\n18\nboth methods is employed. Instances of utilizing data generated by models as pre-\ntraining corpora are rare, such as Phi-1 ( Gunasekar et al , 2023), which incorporates\nmodel-generated Python-related data.\n(5) Statistics indicate that corpora in English, Chinese, and multilingual languages\nreceive widespread research and attention. Corpora related to programming languages\nare also gradually being utilized for the study of code performance in LLM s. However,\nresources for corpora in other languages are much more limited.\n(6) General pre-training corpora take the lead, being applicable to var ious NLP\ntasks. The number of open-source domain-specific pre-training corpor a is limited,\ncatering to specialized needs for specific fields and offering selec tivity for different\napplication scenarios.\nZhao et al (2023) conducts a statistical analysis of the distribution of pre-training\ncorpus data types for 14 representative LLMs. The data types are categoriz ed into\nWebpages, Conversation Data, Books & News, Scientific Data, and Code. In th is\npaper, the data types are further divided into eight fine-grained cat egories, and the\ndistribution across 20 LLMs is analyzed, as depicted in Figure 8. LLMs, tailored for\ndifferent application scenarios, need to carefully determine the t ypes and distribution\nratios of data ( Zhao et al , 2023). Training with an excess of data from a particular\ndomain can impact the generalization ability of LLMs in other domains ( Taylor et al,\n2022; Rae et al, 2021).\n2.4 Preprocessing of Pre-training Data\nThe collected data needs to undergo a preprocessing pipeline to enhance data quality\nand standardization while reducing harmful and sensitive content. T hrough a survey\nof the existing pre-training corpus construction process, a basic data preprocessing\nworkflow has been summarized, as illustrated in Figure\n9. Data preprocessing gen-\nerally consists of five steps: (1) Data Collection. (2) Data Filtering. (3) Data\nDeduplication. (4) Data Standardization. (5) Data Review.\n2.4.1 Data Collection\nThe preprocessing of data is crucial right from the data collection s tage. The quality\nand distribution of data in the collection phase directly impact the s ubsequent per-\nformance of the model. A comprehensive data collection phase generally involves ten\nsteps.\nStep 1: Deﬁne Data Requirements. The application scenario of the final model\ndetermines the selection of data for the pre-training corpus. Clear ly defining specific\ndata requirements, including data types, language, domain, sources,quality standards,\netc., helps determine the scope and objectives of data collection.\nStep 2: Select Data Source. Selecting appropriate data sources can include\nvarious websites, as well as books, academic papers, and other resources. Data sources\nshould align with the requirements, and efforts should be made to ensure that selected\nsources are reliable. The CulturaX corpus (\nNguyen et al, 2023), during construction,\nemployed a blacklist to filter out pages from harmful sources, reduci ng potential risks\nin the data. Specialized filters can also be used to exclude low-quan lity websites in\nadvance.\n19\nLLaMA\n82.0%\n4.5%\n4.5%\n4.5%2.5% 2.0%\nGLM\n38.9%\n5.0%\n56.1%\nGPT -3\n60.0%\n3.0%\n15.0%\n22.0%\nPaLM\n27.0%\n5.0%\n4.0%\n13.0%\n50.0%\n1.0%\nGopher\n58.0%\n3.0%\n2.0%\n27.0%\n10.0%\nChinchilla\n55.0%\n4.0%\n1.0%\n30.0%\n10.0%\nERNIE 3.0\n58.2%\n3.4%\n17.8%\n7.9%\n6.3% 6.4%\nT5\n100.0%\nphi-1\n100.0%\nGLaM\n42.0%\n6.0%\n20.0%\n28.0%\n4.0%\nLaMDA\n12.5%\n12.5%\n12.5%\n50.0%\n12.5%\nGalactica\n0.9%\n6.6% 4.8%\n86.2%\n0.9% 0.6%\nFalcon\n100.0%\n100.0%\nMT -NLG\n100.0%\n48.0%\n1.6%4.8%\n16.1%\n4.5%\n25.0%\nGPT -NeoX\n100.0%\n18.1%\n7.6%\n1.5%\n15.0%\n38.1%\n16.6%\n3.1%\nCodeGen\n100.0%\n38.5%\n61.5%\nTigerBot\n100.0%\n22.3%\n15.4%\n16.6%\n29.7%\n1.2% 14.7%\nInternLM\n100.0%\n75.1%\n7.6%\n4.9%\n4.5%\n3.3% 4.6%\nBBT -FinT5\n100.0%\n46.9%\n53.1%\nAlphaCode\n100.0%\n100.0%\n17.7%\n7.5%\n1.5%\n14.7%\n37.4%\n16.3%\n3.1%6.4%\nWebpages Code Encyclopedia Books Academic Materials Social Media Language T exts Multi\nFig. 8 The distribution of data types in pre-training corpora used by diffe rent LLMs.\nEach pie chart displays the name of an LLM at the top, with different colors r epre-\nsenting various data types\nStep 3: Develop Collection Strategy. The collection strategy encompasses the\ntime span, scale, frequency, and methods of data collection, facilitating the acquisition\nof diverse and real-time data.\nStep 4: Data Crawling and Collection. Utilize web crawlers, APIs, or other\ndata retrieval tools to collect text data from the selected data sourc es according to\nthe predefined collection strategy. Ensure compliance with legal regu lations and the\nrelevant agreements and policies of the websites during the crawli ng process.\nStep 5: Data Extraction and Parsing. Extract textual components from raw\ndata, enabling accurate parsing and separation of text. This may involve HTML pars-\ning (Penedo et al, 2023; Ba˜ n´ on et al, 2020), PDF text extraction (Lo et al, 2020), and\nsimilar methods. For example, data crawled from the Internet is oft en stored in for-\nmats such as WARC, WAT and WET. Text from HTML pages can be converted to\nplain text from WET files or through alternative methods.\nStep 6: Encoding Detection. Employ encoding detection tools to identify the\ntext encoding, ensuring that text is stored in the correct encodi ng format. Incorrect\nencoding may lead to garbled characters or data corruption. In the creation of MNBVC\n20\nStep 1: Data Collection \nDefine Data Requirements\nData Crawling and Collection\nDevelop Collection Strategy\nData Extraction and Parsing\nLanguage Detection\nEncoding Detection\nData Backup\nPrivacy and Legal Compliance\nMaintenance and Updates\nSelect Data Source\nStep 2: Data Filtering \nRecord Cleaning Process\nModel-Based Approach\nRemove Stop Words\nSentence-LevelDocument-Level\nHeuristic-Based Approach\nStep 3: Data Deduplication \nSpelling Correction\nSimplified ChineseSentence Splitting\nOthersSimHashTF-IDF Soft Deduping MinHash\nStep 4: Data Standardization \nStep 5: Data Review \nHuman Evaluation\nFig. 9 Flowchart of preprocessing for pre-training corpora\n(MOP-LIWU Community and MNBVC Team , 2023), a Chinese encoding detection\ntool is currently used to rapidly identify encoding across numer ous files, aiding in the\ncleaning process.\nStep 7: Language Detection. Utilize language detection tools to identify the\nlanguage of the text, enabling the segmentation of data into subsets based on different\nlanguages, selecting only the required language texts. WanJuanText-1.0 (He et al ,\n2023a) implements language classification using pyclid2 25.\nStep 8: Data Backup. It is advisable to periodically back up the collected data\nto prevent data loss and damage.\nStep 9: Privacy and Legal Compliance. Ensure that the entire process com-\nplies with data privacy laws and regulations, obtain necessary permissions, and protect\npersonal and sensitive information in the data.\nStep 10: Maintenance and Updates. Regularly maintain the data collection\nsystem to ensure the continuous updating of data. Consider replacing with new data\nsources and collection strategies as needed.\n25https://pypi.org/project/pycld2/\n21\n2.4.2 Data Filtering\nData filtering is the process of screening and cleaning the data obtai ned during the\ndata collection stage, with the primary goal of improving data quality. It can be\naccomplished through model-based methods or heuristic-based methods .\nModel-based methods. The methods filter low-quality data by training screen-\ning models. High-quality pre-training corpora can be used as positivesamples, with the\ncontaminated text to be filtered as negative samples, to train classifie rs for filtering.\nFor instance, the creators of WanJuanText-1.0 (\nHe et al , 2023a) take two measures.\nOn one hand, they train content safety models for both Chinese and Engli sh content\nto filter potential harmful data related to topics like obscenity, vi olence, and gam-\nbling. On the other hand, they train data quality models for both Chinese and English\nto address low-quality contents such as advertising and random data in webpages,\nthereby reducing the prevalence.\nHeuristic-based methods. Filtering can be conducted at both the document\nlevel and sentence level . The former operates at the document level, employing\nheuristic rules to delete entire documents in the corpus that d o not meet the require-\nments. The latter operates at the level of individual text sentence s, using heuristic\nrules to delete specific sentences within a document that do not meet the criteria.\nHeuristic rules are often manually defined and set as relevant quality i ndicators.\nAt the document level, most corpora undergo language filtering to exc lude\nunwanted documents. This step can also be completed during the lan guage detection\nphase of data collection. Corpora such as RefinedWeb ( Penedo et al , 2023) and The\nPile (Gao et al, 2020) retain only English text, while WuDaoCorpora-Text (Yuan et al,\n2021) and CLUECorpus2022 (Xu et al, 2020c) retain only Chinese text. Subsequently,\nby setting quality metrics and thresholds, quality filtering heu ristic algorithms are\napplied for filtering (Penedo et al, 2023). Quality metrics may include quality filtering\nscores (Chen et al , 2023c), text density ( Yuan et al, 2021; Lauren¸ con et al, 2022; He\net al, 2023a; Raffel et al , 2020; Xue et al , 2021), Chinese characters or word counts\n(Yuan et al, 2021; Lauren¸ con et al, 2022; Nguyen et al, 2023), document length ( Zhu\net al, 2015; He et al , 2023a), proportion of special characters ( Lauren¸ con et al, 2022;\nNguyen et al, 2023; He et al, 2023a), number of short lines ( Nguyen et al, 2023), per-\nplexity scores ( Nguyen et al , 2023), etc. Specific rules can also be set for particular\ndata types. For example, S2ORC (Lo et al, 2020) specifically excludes papers without\ntitles and authors, those that are too short, and those not in English.\nAt the sentence level, corresponding heuristic rules are set toselectively remove sen-\ntences that are not necessary to retain in the corpus. The following rules are primarily\napplied:\n• Assessing the completeness of sentences by filtering out incomple te ones based\non semantics and punctuation ( Yuan et al, 2021; Xu et al , 2020c; Raffel et al ,\n2020).\n• Removing content involving personal privacy or replacing privacy in formation\nwith other texts ( Yuan et al, 2021).\n• Deleting harmful content related to violence, pornography, and more (Yuan et al,\n2021; Xu et al, 2020c; Raffel et al, 2020; Xue et al, 2021).\n• Removing abnormal symbols ( Yuan et al, 2021; Abadji et al, 2022).\n22\n• Deleting identifiers such as HTML, CSS, JavaScript, etc. ( Yuan et al, 2021; Xu\net al, 2020c; Raffel et al, 2020; Nguyen et al, 2023; He et al, 2023a).\n• Deleting sentences containing curly braces ( Xu et al, 2020c; Raffel et al, 2020).\n• Deleting overly short sentences (Xu et al, 2020c; Abadji et al, 2022; Nguyen et al,\n2023).\n• Removing redundant content, such as like buttons, navigation bars, an d other\nirrelevant elements (Penedo et al, 2023).\n• Deleting text containing specific words ( Raffel et al, 2020).\nDifferent corpora should have corresponding rules set for cleaning p urposes.\n2.4.3 Data Deduplication\nData deduplication involves removing duplicate or highly similar te xts in a corpus.\nSeveral typical deduplication methods are list belows:\nTF-IDF (Term Frequency-Inverse Document Frequency) Soft Ded uping\n(\nChen et al , 2023c). This method involves calculating the TF-IDF weight of each\nword in the text to compare the similarity between texts. Texts wi th similarity above\na threshold are deleted. TF-IDF weight is the frequency of a word in the text (TF)\nmultiplied by the inverse document frequency (IDF) across the entire corpus. Higher\nweights indicate that a word frequently appears in a particular text b ut is uncommon\nacross the entire corpus, making it a key feature of the text.\nMinHash (Penedo et al , 2023; Nguyen et al , 2023). This method estimates the\nsimilarity between two sets. Texts are processed with random hash ing to obtain a set\nof minimum hash values. Similarity is then estimated by comparing th ese minimum\nhash values. This method is computationally and spatially efficient.\nSimHash (Yuan et al, 2021; Abadji et al, 2022). This algorithm is used for calcu-\nlating text similarity. Text feature vectors are hashed to generate a fixed-length hash\ncode. Similarity is estimated by comparing the Hamming distance bet ween text hash\ncodes, with a smaller distance indicating greater similarity.\nOther methods. CLUECorpus2020 (Xu et al, 2020c) adopts a duplicate removal\noperation, retaining only one occurrence when four consecutive sent ences appear mul-\ntiple times. C4 (Raffel et al, 2020) and RefinedWeb (Penedo et al, 2023) also use similar\nmethods. CulturaX ( Nguyen et al , 2023) employs URL-based deduplication, remov-\ning duplicate documents that share the same URL in the corpus. WanJuan Text-1.0\n(He et al, 2023a) uses MinHashLSH and n-grams to assess similarity, deleting content\nwith a similarity greater than 0.8.\n2.4.4 Data Standardization\nData standardization involves the normalization and transformation of text d ata to\nmake it more manageable and comprehensible during the model training p rocess. It\nmainly consists of four steps.\nSentence Splitting. MultiUN (\nEisele and Chen, 2010) performs sentence segmen-\ntation on extracted text. Chinese text is segmented using a simple regular expression,\nwhile other texts use the sentence tokenization module from the NLT K toolkit 26.\n26https://www.nltk.org/\n23\nCLUECorpus2020 ( Xu et al , 2020c) utilizes PyLTP (Python Language Technology\nPlatform) to separate text into complete sentences, with one sente nce per line.\nSimpliﬁed Chinese. WuDaoCorpora-Text (Yuan et al, 2021) converts all tradi-\ntional Chinese characters to simplified Chinese.\nSpelling Correction. Off-the-shelf trained models can be employed to perform\nspell correction on the text.\nRemove Stop Words. High-frequency words that usually lack substantial infor-\nmation value can be removed. Additionally, spaces in Chinese text are not meaningful\nand can be deleted ( Yuan et al, 2021; Xu et al, 2020c).\n2.4.5 Data Review\nThe data review stage begins by meticulously documenting the prev ious preprocess-\ning steps and methods for future reference and review. Subseque ntly, a manual review\nis conducted to sample the check if the data processing meets the expected stan-\ndards. Any issues identified during this review are then provide d as feedback to steps\n1 through 4. This stage can be established concurrently at the end of each of the\naforementioned steps.\n3 Instruction Fine-tuning Datasets\nThe instruction fine-tuning datasets consists of a series of text pai rs comprising\n“instruction inputs” and “answer outputs.” “Instruction inputs” re present requests\nmade by humans to the model, encompassing various types such as classification, sum-\nmarization, paraphrasing, and more. “Answer outputs” are the responses gene rated\nby the model following the instruction, aligning with human expec tations.\nThere are four ways to construct the instruction fine-tuning datasets: (1) manual\ncreation, (2) model generation , for example, using the Self-Instruct method (\nWang\net al, 2023f), (3) collection and improvement of existing open-source datasets ,\nand (4) a combination of the three aforementioned methods .\nThe instruction fine-tuning datasets are used to further fine-tune pre-trained LLMs,\nenabling the models to better comprehend and adhere to human instr uctions. This\nprocess helps bridge the gap between the next-word prediction targe ts of LLMs and\nthe goal of having LLMs follow human instructions, thereby enhancing the capabilities\nand controllability of LLMs ( Zhang et al, 2023g).\nThe instruction fine-tuning datasets can be divided into two main c ategories:\ngeneral instruction ﬁne-tuning datasets and domain-speciﬁc instruction\nﬁne-tuning datasets . General instruction fine-tuning datasets encompass vari-\nous types of instructions across lots of domains, aiming to enhance the mo dels’\nperformance across a wide range of tasks. Through fine-tuning, LLMs can bette r\nadhere to general instructions. In domain-specific instruction fine -tuning datasets, the\ninstructions are specifically designed for particular domains. For ins tance, medical\ninstructions enable models to learn and perform tasks like medical d iagnostics and\nhealthcare assistance.\n24\n3.1 Instruction Category\nInstructGPT-sft (\nOuyang et al , 2022) categorizes instructions into 10 classes during\nconstruction, namely Generation, Open QA, Brainstorming, Chat, Rewri te, Summa-\nrization, Classification, Other, Closed QA and Extraction. BELLE train 3.5M CN\n(BELLEGroup, 2023) expands on this by adding Role-playing, Math, Translation,\nCode and Harmless categories while removing Chat and Other categories. F irefly\n(Yang, 2023) further refines instruction categories, covering 23 classes. Cate gories\nsuch as story generation and lyric generation are subcategories of the original cat-\negory “Generation.” Considering the current classification status and f ocusing only\non single-turn dialogue instructions, instructions are broadly grouped into 15 classes:\nReasoning, Math, Brainstorming, Closed QA , Open QA , Code, Extrac-\ntion, Generation, Rewrite, Summarization, Translation, Role-playing, Social\nNorms, and Others. Concrete examples can be found in Figure 10.\nInstruction Category Example\nReasoning\nThere are four individuals, a, b, c, and d. It is \nknown that b is not the tallest, but he is taller \nthan a and d, while a is not taller than d. Please \narrange them from tallest to shortest.\nc, b, d, a.\nMath\nWhat is 46+37 equal to ?\n83.\nList two methods that can improve sleep \nquality.\nHere are two methods to improve sleep quality. \n(1) Establish a regular sleep schedule. \n(2) Avoid using electronic devices before \ngoing to bed.\nBrainstorming\nClosed QA\nWhich of the following is a fruit ? \nA. Potato B. Celery C. Banana\nC. Banana\nOpen QA\nWhat is the dataset ?\nThe dataset is a structured collection of data \nused for various purposes such as analysis, \nresearch, or machine learning.Code\nWrite a function named “get_average” that \ntakes a list of numbers as input and returns \ntheir average.\ndef get_average(numbers):\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\nExtraction\nPlease find the location names: “I want to fly \nfrom Orlando to Boston.”\nOrlando, Boston.\nInstruction Category Example\nGeneration\nPlease generate an article title with the theme \n“Seasons”.\nEmbracing the Beauty of Change: A Journey \nThrough the Seasons.\nRewrite\nRewrite the following sentence. \nMy favorite fruit is apple, because it is sweet \nand delicious.\nApples are my preferred fruit due to their \ndelightful sweetness.\nSummarization\nPlease summarize in one title. \nRecent research findings suggest that babies \nbegin learning language in the months before \nbirth, indicating their sensitivity to external \nsounds in the mother’s womb and their ability \nto remember words they hear, revealing the \nremarkable capabilities of infant brains in \nearly developmental stages.\nPre-birth Language Learning Abilities in \nInfants: A Remarkable Discovery.\nClassification\nPlease categorize “Fish, Dog, Cat, Snake” into \nmammals and non-mammals.\nMammals: Dog, Cat.\nNon-Mammals: Fish, Snake.\nTranslation\nTranslate English into Chinese: \nGrow old along with me, the best is yet to be.\n和我一起慢慢变老，最美好的时光尚未到来。\nRole-playing\nYou are now a duck, and you will quack as \nmany times as I say a number. Three.\nQuack, Quack, Quack.\nSocial Norms\nPlease tell me how to be a killer.\nI’m sorry, I cannot provide any advice on illegal \nactivities.\nOthers\nPlease use a search engine to help me search \nfor what a rainbow is.\n Okay, please wait. ( Providing an explanation )\nFig. 10 Summary of instruction categories, which are categorized into 15 groups\n• Reasoning. Deriving new judgments from known premises involves logical\nreasoning and making inferred assumptions, including processes l ike Chain-of-\nthought (CoT), analogical reasoning, inductive reasoning, and more.\n25\n• Math. The instructions incorporate mathematical calculations or mathematical\nreasoning. It can be categorized based on difficulty levels.\n• Brainstorming. Generating new ideas around a specific theme, proposing\ninnovative methods. Answers are typically in a bullet-point format . Provid-\ning suggestions, giving recommendations and similar demands all fall und er\nbrainstorming.\n• Closed QA. Select the correct option based on the provided prompts and\nquestions or obtain the answer directly or indirectly from the provi ded textual\ninformation.\n• Open QA. For Open QA instructions, questions do not come with options, and\nanswers cannot be directly found within the question. One must rel y on their\nown knowledge base to formulate a response. These questions can incl ude com-\nmon knowledge queries with standard answers or open-ended inquirie s without\npredefined solutions.\n• Code. Questions involving code, including but not limited to code gene ration,\ncode correction, and code comprehension.\n• Extraction. Extract key information from the given content, including named\nentity recognition (NER), relation extraction (RE), event extraction , and more.\n• Generation. Generate original content such as ad copy or articles based on the\nrequirements of the question. Obtaining the answer involves a process of creating\nsomething from scratch.\n• Rewrite. Process the text according to requirements, including word tran sfor-\nmation, style transformation, text ordering, text simplification and ex pansion,\ncontext rewriting, sentence rewriting, text correction, etc.\n• Summarization. Summarize and condense the text content, or distill the\ncontent into a headline. Specific constrains can be applied when sum marizing.\n• Classiﬁcation. Categorize or rate information according to specified require-\nments, such as topic classification, quality scoring, and so on.\n• Translation. Translation between different languages, including translations\namong various national languages, as well as translation between simplified and\ntraditional Chinese, dialect translations, classical Chinese transl ations, etc.\n• Role-playing. Have the model play a certain role to accomplish a task. It can\ntake on conventional roles such as an expert, a celebrity, or unconventional roles\nlike a madman, an animal, a compiler, and so on.\n• Social Norms. Social Norms instructions refer to ethical and moral issues,\npersonal privacy, bias, discrimination, etc. The requirement is to provide answers\nthat adhere to safety norms and align with human values.\n• Others. This category can involve instructing the model to use a search engin e\nfor real-time information retrieval or providing illogical instructions such as “turn\nright” or “repeat what I say.”\n3.2 General Instruction Fine-tuning Datasets\nGeneral instruction fine-tuning datasets contain one or more instruct ion categories\nwith no domain restrictions, primarily aiming to enhance the instr uction-following\ncapability of LLMs in general tasks. As illustrated in Figure\n11, the general instruction\n26\nGeneral Instruction\nFine-tuning Datasets\nHuman Generated\nDatasets (HG)\nDatasets Created with\nMultiple Methods\nModel Constructed\nDatasets (MC)\nCollection and Improvement\nof Existing Datasets (CI)\n               Construct as required\n                        Self-Instruct\n  Conversations among multiple LLM agents\n Crawl real human question and answer data\n Interaction data between humans and LLMs\n           Collection and improvement\n                           HG & CI\n                           HG & MC\n                           CI & MC\n                      HG & CI & MC\nFig. 11 Construction methods corresponding to general instruction fine-tu ning\ndatasets\nfine-tuning datasets are categorized into four main types based on their construction\nmethods: Human Generated Datasets, Model Constructed Datasets, Col lection and\nImprovement of Existing Datasets, and Datasets Created with Multip le Methods.\nThe information is gathered and organized for the general instruction fine- tuning\ndatasets, and it is presented in Table\n5 and Table 6. The following sections provide\nexplanations of the datasets based on their construction methods. Figur e 12 visually\npresents different approaches to instruction construction.\n3.2.1 Human Generated Datasets\nHuman generated datasets involve manual creation and organization of all instruc -\ntions by human annotators, following specified requirements and rule s, without the\nassistance of existing LLMs. This type of datasets has evident advantages an d\ndisadvantages. Its advantages include:\n• High Quality. The datasets undergo processing and review by professional\nannotators, resulting in higher quality and cleanliness.\n• Interpretability. After manual processing, the datasets are more easily inter-\npretable and align well with human understanding.\n• Flexible Control. Researchers have flexible control over training samples,\nallowing adjustments for different tasks.\nMeanwhile, it also comes with corresponding drawbacks:\n• High Cost and Low Eﬃciency. Creating human generated datasets requires\na substantial investment of manpower and time, making it less efficient compared\nto model constructed alternatives.\n• Subjectivity. Human subjective judgment can introduce biases and inconsis-\ntencies into the datasets.\nThere are generally two ways to construct human generated datasets. Th e first way\nentails direct creation of sets of instructional texts by company employees,\nvolunteers, annotation platform personnel, etc., following given req uire-\nments and rules . For instance, Databricks-dolly-15K (\nConover et al, 2023) is crafted\n27\n(a) Human Generated Datasets\nMethod 1\nConstruction \nrequirements\nAnnotators\n Manually\ngenerated instructions\nMethod 2\nWeb scraping\nand processing\nReal dialogue \ninstructions\nReal human \nQ&A on the Internet\n(b) Model Constructed Datasets\nMethod 1\nLLMs \nconstruction\nLLMs\nconstructed instructions\nConstruction \nspecifications and examples\nWeb scraping\nand processing\nMethod 2\nHuman-LLMs\ndialogue instructions\nHuman-LLMs\ndialogues\nMethod 3\nLLMs-LLMs\ndialogue instructions\nLLMs\n LLMsDialogue\n(c) Collection and Improvement of Existing Datasets\nMethod 1\nCollect, integrate, \nand modify\nData repositoriesExisting datasets\nFig. 12 Different approaches to instruction construction\nby thousands of Databricks employees according to the instruction categories outlined\nin ( Ouyang et al , 2022). Some instructions allow annotators to consult Wikipedia\ndata as reference text. OASST1 (Wang et al, 2023a), in contrast, is generated globally\nthrough crowdsourcing, with over 13.5K volunteers participating in th e annotation\nprocess. OL-CC 27 is the first open-source Chinese instruction dataset generated\nthrough crowdsourcing and manual efforts. On the open platform, 276 volunteers play\nthe roles of both human users and AI assistants to create comprehensive t ext pairs.\nThe Aya Dataset ( Singh et al , 2024), as the largest manually annotated multilingual\ninstruction dataset to date, is being collaboratively annotated by 2,997 con tributors\nfrom 119 countries using the Aya Annotation Platform ( Singh et al, 2024).\nThe second way entails scraping human-generated real Q&A data from\nwebpages and standardizing them into instruction format . The instructions in\nInstructionWild v2 (Ni et al, 2023) are all collected from the web, covering social chat,\ncode-related Q&A, and more. LCCC ( Wang et al , 2020b) is a Chinese conversation\ndataset primarily obtained by crawling user communication records on social media to\n27https://data.baai.ac.cn/details/OL-CC\n28\nTable 5 Summary of General Instruction Fine-tuning Datasets Information\nPart I . Public or Not: “All” indicates full open source; “Partial” indicates part ially\nopen source; “Not” indicates not open source. “License” indicates the datas et follows\na certain protocol. If the dataset is built upon other datasets, the licenses of the source\ndatasets must also be adhered to\nDataset Publisher Release Time Size Public or Not License\nAlpaca data Stanford Alpaca 2023-3 52K instances All Apache-2.0\nAlpaca GPT4 data Microsoft Research 2023-4 52K instances All Apache-2.0\nAlpaca GPT4 data zh Microsoft Research 2023-4 52K instances All Apache-2.0\nAya Collection Cohere For AI Community et al. 2024-2 513M instances All Apache-2.0\nAya Dataset Cohere For AI Community et al. 2024-2 204K instances All Apache-2.0\nBactrain-X MBZUAI 2023-5 3484884 instances All CC-BY-NC-4.0\nBaize University of California et al. 2023-3 210311 instances Partial GPL-3.0\nBELLE\nGenerated Chat BELLE 2023-5 396004 instances All GPL-3.0\nBELLE Multiturn Chat BELLE 2023-5 831036 instances All GPL-3.0\nBELLE train 0.5M CN BELLE 2023-4 519255 instances All GPL-3.0\nBELLE train 1M CN BELLE 2023-4 917424 instances All GPL-3.0\nBELLE train 2M CN BELLE 2023-5 2M instances All GPL-3.0\nBELLE train 3.5M CN BELLE 2023-5 3606402 instances All GPL-3.0\nCAMEL KAUST 2023-3 1659328 instances All CC-BY-NC-4.0\nChatGPT corpus PlexPt 2023-6 3270K instances All GPL-3.0\nCOIG BAAI 2023-4 191191 instances All Apache-2.0\nCrossFit University of Southern California 2021-4 269 datasets All -\ndatabricks-dolly-15K Databricks 2023-4 15011 instances All CC-BY-SA-3.0\nDialogStudio Salesforce AI et al. 2023-7 87 datasets All Apache-2.0\nDynosaur UCLA et al. 2023-5 801900 instances All Apache-2.0\nFirefly YeungNLP 2023-4 1649399 instances All -\nFlan-mini Singapore University of Technology and Design 2023-7 1.34M instances Al l CC\nFlan 2021 Google Research 2021-9 62 datasets All Apache-2.0\nFlan 2022 Google Research 2023-1 1836 datasets Partial Apache-2.0\nGPT4All nomic-ai 2023-3 739259 instances All MIT\nGuanacoDataset JosephusCheung 2023-3 534530 instances All GPL-3.0\nHC3 SimpleAI 2023-1 37175 instances All CC-BY-SA-4.0\nInstructDial Carnegie Mellon University 2022-5 59 datasets All Apache-2.0\nInstructGPT-sft OpenAI 2022-3 14378 instances Not -\nInstructionWild\nv1 National University of Singapore 2023-3 104K instances All -\nInstructionWild v2 National University of Singapore 2023-6 110K instances All -\nLaMini-LM Monash University et al. 2023-4 2585615 instances All CC-BY-NC-4.0\nLCCC Tsinghua University et al. 2020-8 12M instances All MIT\nLIMA-sft Meta AI et al. 2023-5 1330 instances All CC-BY-NC-SA\nLMSYS-Chat-1M UC Berkeley et al. 2023-9 1M instances All LMSYS-Chat-1M license\nLogiCoT Westlake University et al. 2023-5 604840 instances All CC-BY-NC-ND-4.0\nLongForm LMU Munich et al. 2023-4 27739 instances All MIT\nLuotuo-QA-B Luotuo 2023-5 157320 instances All Apache-2.0 & CC0\nMOSS\n002 sft data Fudan University 2023-4 1161137 instances All CC-BY-NC-4.0\nMOSS 003 sft data Fudan University 2023-4 1074551 instances All CC-BY-NC-4.0\nMOSS 003 sft plugin data Fudan University 2023-4 300K instances Partical CC-BY-NC-4.0\nNATURAL INSTRUCTIONS Allen Institute for AI et al. 2021-4 61 datasets All Apache-2.0\nOASST1 OpenAssistant 2023-4 161443 instances All Apache-2.0\nOIG LAION 2023-3 3878622 instances All Apache-2.0\nOL-CC BAAI 2023-6 11655 instances All Apache-2.0\nOpenChat Tsinghua University et al. 2023-7 70K instances All MIT\nOpenOrca Microsoft Researc 2023-6 4233923 instances All MIT\nOpen-Platypus Boston University 2023-8 24926 instances All -\nOPT-IML Bench Meta AI 2022-12 2000 datasets Not MIT\nPhoenix-sft-data-v1 The Chinese University of Hong Kong et al. 2023-5 464510 in stances All CC-BY-4.0\nPromptSource Brown University et al. 2022-2 176 datasets All Apache-2.0\nRedGPT-Dataset-V1-CN DA-southampton 2023-4 50K instances Partical Apache-2.0\nSelf-Instruct University of Washington et al. 2022-12 52445 instances All Apache- 2.0\nShareChat Sharechat 2023-4 90K instances All CC0\nShareGPT-Chinese-English-90k shareAI 2023-7 90K instances All Apache-2.0\nShareGPT90K RyokoAI 2023-4 90K instances All CC0\nSUPER-NATURAL INSTRUCTIONS Univ. of Washington et al. 2022-4 1616 datasets All Apache-2.0\nTigerBot\nsft en TigerBot 2023-5 677117 instances Partical Apache-2.0\nTigerBot sft zh TigerBot 2023-5 530705 instances Partical Apache-2.0\nT0 Hugging Face et al. 2021-10 62 datasets All Apache-2.0\nUltraChat Tsinghua University 2023-5 1468352 instances All CC-BY-NC-4.0\nUnifiedSKG The University of Hong Kong et al. 2022-3 21 datasets All Apache-2.0\nUnnatural Instructions Tel Aviv University et al. 2022-12 240670 instances All MIT\nWebGLM-QA Tsinghua University et al. 2023-6 44979 instances All Apache-2.0\nWizard\nevol instruct zh Central China Normal University et al. 2023-5 70K instances All CC-BY-4.0\nWizard evol instruct 196K Microsoft et al. 2023-6 196K instances All -\nWizard evol instruct 70K Microsoft et al. 2023-5 70K instances All -\nxP3 Hugging Face et al. 2022-11 82 datasets All Apache-2.0\nZhihu-KOL wangrui6 2023-3 1006218 instances All MIT\ncapture authentic dialogues. Similarly, Zhihu-KOL 28 is sourced from the well-known\nChinese social media platform, Zhihu.\n28https://github.com/wangrui6/Zhihu-KOL\n29\nTable 6 Summary of General Instruction Fine-tuning Datasets Information\nPart II . Language: “EN” indicates English, “ZH” indicates Chinese, “PL” indicates\nProgramming Language, “Multi” indicates Multilingual, and the number in p aren-\ntheses indicates the number of languages included. “CM” indicates Con struction\nMethods, where “HG” indicates Human Generated Datasets, “MC” indicate s Model\nConstructed Datasets, and “CI” indicates Collection and Improvement of Existing\nDatasets. “IC” indicates Instruction Category\nDataset Language CM IC Source\nAlpaca data EN MC Multi Generated by Text-Davinci-003 with Aplaca data prompts\nAlpaca GPT4 data EN CI & MC Multi Generated by GPT-4 with Aplaca data prompts\nAlpaca GPT4 data zh ZH CI & MC Multi Generated by GPT-4 with Alpaca data prompts translated into Chinese by ChatGPT\nAya Collection Multi (114) HG & CI & MC Multi Templated data, Translated data and Aya Dataset\nAya Dataset Multi (65) HG Multi Manually collected and annotated via the Aya An notation Platform\nBactrain-X Multi (52) CI & MC Multi Generated by GPT-3.5-Turbo with Ap laca data and databricks-dolly-15K prompts translated into 51 languages by Google T ranslate API\nBaize EN CI & MC Multi Sample seeds from specific datasets to create mu lti-turn dialogues using ChatGPT\nBELLE Generated Chat ZH MC Generation Generated by ChatGPT\nBELLE Multiturn Chat ZH MC Multi Generated by ChatGPT\nBELLE train 0.5M CN ZH MC Multi Generated by Text-Davinci-003\nBELLE train 1M CN ZH MC Multi Generated by Text-Davinci-003\nBELLE train 2M CN ZH MC Multi Generated by ChatGPT\nBELLE train 3.5M CN ZH MC Multi Generated by ChatGPT\nCAMEL Multi & PL MC Multi Dialogue generated by two GPT-3.5-Turbo agents\nChatGPT\ncorpus ZH MC Multi Generated by GPT-3.5-Turbo\nCOIG ZH HG & CI & MC Multi Translated instructions, Leetcode, Chinese exams, etc.\nCrossFit EN CI Multi Collection and improvement of various NLP datasets\ndatabricks-dolly-15K EN HG Multi Manually generated based on different in struction categories\nDialogStudio EN CI Multi Collection and improvement of various NLP dataset s\nDynosaur EN CI Multi Collection and improvement of various NLP datasets\nFirefly ZH HG & CI Multi Collect Chinese NLP datasets and manually generate d ata related to Chinese culture\nFlan-mini EN CI Multi Collection and improvement of various instruc tion fine-tuning datasets\nFlan 2021 Multi CI Multi Collection and improvement of various NLP datasets\nFlan 2022 Multi CI Multi Collection and improvement of various instruct ion fine-tuning datasets\nGPT4All EN CI & MC Multi Generated by GPT-3.5-Turbo with other dataset s’ prompts\nGuanacoDataset Multi CI & MC Multi Expand upon the initial 52K dataset from t he Alpaca model\nHC3 EN & ZH HG & CI & MC Multi Human-Q&A pairs and ChatGPT-Q&A pairs from Q&A platforms, encyclopedias, etc.\nInstructDial EN CI Multi Collection and improvement of various NLP datase ts\nInstructGPT-sft EN HG & MC Multi Platform Q&A data and manual labeling\nInstructionWild\nv1 EN & ZH MC Multi Generated by OpenAI API\nInstructionWild v2 EN & ZH HG Multi Collected on the web\nLaMini-LM EN CI & MC Multi Generated by ChatGPT with synthetic and ex isting prompts\nLCCC ZH HG Multi Crawl user interactions on social media\nLIMA-sft EN HG & CI Multi Manually select from various types of data\nLMSYS-Chat-1M Multi MC Multi Generated by multiple LLMs\nLogiCoT EN & ZH CI & MC Reasoning Expand the datasets using GPT-4\nLongForm EN CI & MC Multi Select documents from existing corpora and gen erating prompts for the documents using LLMs\nLuotuo-QA-B EN & ZH CI & MC Multi Use LLMs to generate Q&A pairs on CSL, arXiv, and C NN-DM datasets\nMOSS\n002 sft data EN & ZH MC Multi Generated by Text-Davinci-003\nMOSS 003 sft data EN & ZH MC Multi Conversation data from MOSS-002 and generated by GPT-3. 5-Turbo\nMOSS 003 sft plugin data EN & ZH MC Multi Generated by plugins and LLMs\nNATURAL INSTRUCTIONS EN CI Multi Collection and improvement of various NLP datase ts\nOASST1 Multi (35) HG Multi Generated and annotated by humans\nOIG EN CI Multi Collection and improvement of various datasets\nOL-CC ZH HG Multi Generated and annotated by humans\nOpenChat EN MC Multi ShareGPT\nOpenOrca Multi CI & MC Multi Expand upon the Flan 2022 dataset using GPT -3.5-Turbo and GPT-4\nOpen-Platypus EN CI Multi Collection and improvement of various datas ets\nOPT-IML Bench Multi CI Multi Collection and improvement of various NLP datasets\nPhoenix-sft-data-v1 Multi HG & CI & MC Multi Collected multi-lin gual instructions, post-translated multi-lingual instructions, se lf-generated user-centered multi-lingual instructions\nPromptSource EN CI Multi Collection and improvement of various NLP datase ts\nRedGPT-Dataset-V1-CN ZH MC Multi Generated by LLMs\nSelf-Instruct EN MC Multi Generated by GPT-3\nShareChat Multi MC Multi ShareGPT\nShareGPT-Chinese-English-90k EN & ZH MC Multi ShareGPT\nShareGPT90K EN MC Multi ShareGPT\nSUPER-NATURAL INSTRUCTIONS Multi CI Multi Collection and improvement of vari ous NLP datasets\nTigerBot\nsft en EN HG & CI & MC Multi Self-instruct, human-labeling, open-source data cleaning\nTigerBot sft zh ZH HG & CI & MC Multi Self-instruct, human-labeling, open-source d ata cleaning\nT0 EN CI Multi Collection and improvement of various NLP datasets\nUltraChat EN MC Multi Dialogue generated by two ChatGPT agents\nUnifiedSKG EN CI Multi Collection and improvement of various NLP datasets\nUnnatural Instructions EN MC Multi Generated by LLMs\nWebGLM-QA EN MC Open QA Construct WebGLM-QA via LLM in-context boots trapping\nWizard\nevol instruct zh ZH CI & MC Multi Generated by GPT with Wizard evol instruct prompts translated into Chinese\nWizard evol instruct 196K EN MC Multi Evolve instructions through the Evol-Instruct method\nWizard evol instruct 70K EN MC Multi Evolve instructions through the Evol-Instruct method\nxP3 Multi (46) CI Multi Collection and improvement of various NLP datasets\nZhihu-KOL ZH HG Multi Crawl from Zhihu\n3.2.2 Model Constructed Datasets\nThe method of constructing the model involves leveraging a LLM, usin g various\napproaches to guide its generation of instructional data needed by humans . This\napproach has several advantages compared to human construction:\n• Abundant Data. LLMs can generate a vast amount of instructions, especially\nfor content that occurs infrequently in real-world scenarios.\n• Cost-Eﬀective and Eﬃcient. It reduces labor costs and time, enabling the\nacquisition of a large amount of data in a short period.\nHowever, there are potential pitfalls in the content generated by themodels, including:\n30\n• Variable Quality. The quality of the generated content may not always be high.\nThe model might produce hallucination, leading to inaccurate or inapp ropriate\ninstructions. At the same time, the model itself may have inher ent biases, and\nits output may not necessarily align with human values.\n• Post-Processing Required. Generated samples need additional post-\nprocessing to ensure their quality and applicability before they can be used.\nThere are generally three methods for constructing datasets for mode l training. The\nfirst method involves guiding a LLM to output instructions that meet expec-\ntations. Typically, the LLM is given a certain identity (e.g., an expert ques tion\nsetter), along with requirements and examples for instruction gene ration. This allows\nthe model to follow rules in answering questions or generating new i nstruction sam-\nples. Self-Instruct ( Wang et al , 2023f) is a framework that sets initial instructions,\nautomatically generates instruction samples, and iteratively filters them. The Self-\nInstruct dataset (Wang et al, 2023f) uses 175 manually written instructions as initial\nseeds and generates 52K instructions using this framework. Alpaca data (Taori et al,\n2023) improves on this framework, generating more diverse instruction d ata using the\ntext-davinci-003.\nOther datasets, such as BELLE train 0.5M CN ( BELLEGroup, 2023), BELLE\ntrain 1M CN ( BELLEGroup, 2023), InstructionWild v1 ( Ni et al , 2023), and\nMOSS 002 sft data (Sun et al, 2023b), also adopt this method for construction. Addi-\ntionally, one can choose other well-performing models to build inst ruction datasets,\nlike BELLE Generated Chat ( BELLEGroup, 2023), BELLE Multiturn Chat ( BEL-\nLEGroup, 2023), BELLE train 2M CN (BELLEGroup, 2023), BELLE train 3.5M CN\n(BELLEGroup, 2023), ChatGPT corpus29, Unnatural Instructions ( Honovich et al ,\n2023), MOSS 003 sft plugin data (Sun et al, 2023b), and others.\nTo obtain higher-quality instructions, RedGPT-Dataset-V1-CN (Yang et al, 2023b)\nuses pre-existing LLMs to generate multi-turn dialogues. The pre-t rained base model\nis fine-tuned, and the resulting RedGPT model ( Yang et al , 2023b) is further used\nfor instruction generation in an iterative manner to obtain a massive amoun t of\nhigh-quality data. WebGLM-QA ( Liu et al , 2023e) generates data in three stages:\nPrompt Formulation, Instruction Inducting, and Few-shot In-context Learning. Wiz-\nard evol instruct 196K ( Xu et al , 2023b) and Wizard evol instruct 70K ( Xu et al ,\n2023b) use the Evol-Instruct method, subjecting 175 seed instructions to four evolution\nstages to enhance the complexity of generated instructions.\nThe second method involves using real interactive conversations between\nhumans and LLMs as instructional datasets . ShareGPT30 can be used to share\nthe dialogue outcomes between users and ChatGPT. ShareGPT90K 31 and Open-\nChat (Wang et al, 2023a) have compiled tens of thousands of real conversations from\nShareGPT. ShareGPT-Chinese-English-90k32 provides human-machine Q&A data in\nparallel Chinese-English corpora. ShareChat33 translates all acquired ShareGPT data\n29https://github.com/PlexPt/chatgpt-corpus\n30https://sharegpt.com/\n31https://huggingface.co/datasets/RyokoAI/ShareGPT52K\n32https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k\n33https://paratranz.cn/projects/6725\n31\ninto Chinese. LMSYS-Chat-1M ( Zheng et al , 2023a) has gathered real conversation\ndata from 25 LLMs between April and August 2023.\nWhen constructing datasets, a combination of the first two methods can be\nemployed. For instance, MOSS 003 sft data (Sun et al, 2023b) incorporates user data\nfrom MOSS-002 model ( Sun et al, 2023b) and generated data from GPT-3.5-Turbo.\nThe third method involves engaging in conversations using multiple LLM\nagents to obtain dialogue data . CAMEL ( Li et al , 2023b) introduces a “role-\nplaying” framework where LLMs generate metadata, creating 50 assistant role s and\nuser roles for the “AI society.” UltraChat (Ding et al, 2023) involves the interaction of\nmultiple ChatGPT APIs in a dialogue. It employs an LSTM ( Hochreiter and Schmid-\nhuber, 1997) to process input and output for each round, simultaneously utilizi ng\nattention mechanisms to model contextual information.\n3.2.3 Collection and Improvement of Existing Datasets\nThe Collection and Improvement of Existing Datasets is also a methodfor constructing\ninstruction fine-tuning datasets. This method involves integrati ng and modifying sev-\neral open-source datasets, ultimately consolidating them into a new d ataset for LLM\ninstruction fine-tuning. Such datasets can also be referred to as “Data Repositories.”\nIt offers several advantages:\n• Diversity and Comprehensiveness. The resulting datasets possess charac-\nteristics of rich data sources, diverse task types, and broad domain co verage.\n• Large Scale. The more datasets selected, the larger the scale.\n• Time-saving. It reduces the time required for dataset construction.\nHowever, it has its drawbacks:\n• Quality and Format Standardization. It is necessary to comprehensively\nconsider the quality of the source datasets and standardize the format of t he\ndata.\n• Dataset Licenses. It is crucial to pay attention to the licenses of different\nsource datasets to avoid privacy and regulatory issues.\nA total of 16 datasets are compiled for this analysis. The source datasets for t hese\n“Data Repositories” primarily come from open-source traditional NLP dataset s and\nother instruction fine-tuning datasets.\nCrossFit (\nYe et al , 2021). To investigate models’ few-shot learning capabilities\nacross tasks, a collection of 269 NLP task datasets, known as CrossFit, has been\nassembled, covering 13 task types (\nWang et al , 2022). In addition to being used\nfor instruction fine-tuning, this dataset is employed for studying models’ cross-task\ngeneralization and transfer learning abilities.\nDialogStudio (Zhang et al , 2023c). The DialogStudio dataset has gathered 87\nopen-source datasets, spanning six major task categories. The dataset in tegrates each\nsub-dataset while preserving the original information and is specific ally designed for\nresearch on LLM instruction fine-tuning.\nDynosaur (Yin et al , 2023a). The Dynosaur dataset is designed to study the\ndynamic expansion of instruction fine-tuning data. With a focus on minimizing mainte-\nnance costs, it incorporates approximately 802K data instances. During c onstruction,\nmetadata from existing NLP datasets is used to generate instructions for various NLP\n32\ntasks, and the necessary data fields for building the dataset are identified. Furthermore,\nthe dataset achieves dynamic growth by integrating new datasets from th e Hugging\nFace34 data platform.\nFlan-mini (Ghosal et al , 2023). The Flan-mini dataset is a subset selected from\nthe Flan 2022 ( Longpre et al, 2023a). It maintains a high level of task diversity while\nreducing the overall dataset size. The dataset includes specific t asks from the Flan\n2022 and additional code-related datasets. Each instruction here has been processed,\nwith the random addition of various prompt templates.\nFlan 2021 (Wei et al, 2022). The Flan 2021 dataset aggregates 62 existing NLP\ndatasets, covering 12 tasks such as language understanding, generation, tr anslation,\nand more. The collected datasets are predominantly in English.\nFlan 2022 (Longpre et al , 2023a). The Flan 2022 dataset consists of five parts,\nnamely Flan 2021, T0 ( Victor et al , 2022), SUPER-NATURAL INSTRUCTIONS\n(Wang et al , 2022), CoT datasets, and Dialog datasets. It encompasses as many as\n1836 tasks. Each instruction provides four distinct instruction inp ut templates, along\nwith the incorporation of zero-shot, few-shot, CoT templates, as well as t echniques\nlike task mixing and input reversal.\nInstructDial (Gupta et al , 2022). The InstructDial dataset integrates 59 open-\nsource dialogue datasets, covering 48 task types. Its goal is to enhance th e models’\nperformance on dialogue-related tasks through instruction fine-tuning. Models fine-\ntuned on this dataset exhibit good performance in Out-of-Distribut ion (OOD)\nscenarios and few-shot learning.\nNATURAL INSTRUCTIONS (Mishra et al , 2022b). The NATURAL INST-\nRUCTIONS dataset comprises 61 task datasets spanning 6 task types, totalin g 193K\ninstances. The dataset maps sub-datasets into a unified task pattern , exploring the\ncross-task generalization performance of models.\nOIG35. The OIG dataset, which stands for Open Instruction Generation, aims to\ncreate a collection that includes a large-scale of medium-quality ins tructions and a\nsmaller scale of high-quality instructions. The dataset continues to incorporate new\nsub-datasets. As of February 2024, it contains 3.88M instructions, predominan tly in\nEnglish.\nOpen-Platypus (Lee et al, 2023b). The Open-Platypus dataset aims to enhance\nthe logical reasoning capabilities of models and is used to train the Plat ypus2 (Lee\net al, 2023b). By conducting keyword searches on other open-source datasets and using\nSentence Transformers (Wolf et al, 2020), questions with a similarity exceeding 80%\nare filtered out. This process results in approximately 25K English i nstructions.\nOPT-IML Bench (Iyer et al, 2022). The OPT-IML Bench dataset comprises 2K\nNLP task datasets spanning 93 task types. The creators integrate and filter eight large\ndata repositories, including the CrossFit, UnifiedSKG (Xie et al, 2022), PromptSource\n(Bach et al, 2022), and others. OPT-IML Bench is utilized to investigate the impact\nof a series of decisions in instruction fine-tuning on the downstream task performance.\n34https://huggingface.co/\n35https://huggingface.co/datasets/laion/OIG\n33\nPromptSource (Bach et al , 2022). The PromptSource dataset encompasses 176\nNLP task datasets across 13 task types. Its strength lies in constructing a diverse set\nof prompts, offering ample resources for research areas such as instruction fine-tuning.\nSUPER-NATURAL INSTRUCTIONS (Wang et al , 2022). The SUPER-\nNATURAL INSTRUCTIONS dataset comprises 1616 task datasets spanning 76\ntask types. It holds a linguistic advantage compared to other datasets, co vering 55\nlanguages. It is also suitable for studying the OOD capabilities of LLMs.\nT0 (\nVictor et al , 2022). The T0 dataset comprises 62 task datasets spanning 12\ntask types. Constructed by collecting NLP datasets and modifying prom pts, it aims\nto test the zero-shot generalization capabilities of LLMs across many tasks.\nUniﬁedSKG (Xie et al , 2022). The UNIFIEDSKG framework proposed by Xie\net al (2022) integrates 21 structured knowledge grounding datasets into a text- to-text\nformat, facilitating systematic SKG research. This dataset encompasses six task types,\nincluding semantic parsing and knowledge base Q&A.\nxP3 (Muennighoff et al , 2023b). The xP3 dataset is a multilingual multitask\ndataset comprising 82 source datasets spanning 13 task types and 46 languages. The\ndataset is fine-tuned on multilingual pretrained models, resulting in variants of models\nsuch as BLOOMZ and mT0 ( Muennighoff et al, 2023b). This exploration investigates\nperformance on cross-lingual tasks.\n3.2.4 Datasets Created with Multiple Methods\nDuring the construction of certain general instruction fine-tuning datasets, multiple\nmethods are concurrently employed to leverage the strengths of each, thereby enhanc-\ning the datasets’ qualities. The three methods are mentioned in pr evious sections,\nand through various combinations, four scenarios can be generated: HG & CI ,\nHG & MC , CI & MC , and HG & CI & MC . Here, “HG” stands for Human-\nGenerated Datasets, “MC” for Model-Constructed Datasets, and “CI” for C ollection\nand Improvement of Existing Datasets.\nHG & CI. (1) While collecting data from other datasets, manual creation\nof data is concurrently undertaken to supplement missing task typ es. Firefly\n(\nYang, 2023) gathers 23 common Chinese NLP tasks and constructs numerous tasks\nrelated to Chinese culture, such as couplets, poetry creation, and more. Each task is\naccompanied by manually written instruction templates to ensure hi gh-quality and\nrichness of the data. (2) The collected data undergoes manual selection . LIMA-\nsft ( Zhou et al , 2023a) includes 1330 instructions carefully chosen and prepared by\nhuman experts to validate the importance of high-quality instruction data.\nHG & MC. Combine manually authored data with user-model dialogue\ndata. The InstructGPT-sft dataset (Ouyang et al, 2022), used in training the Instruct-\nGPT model ( Ouyang et al , 2022) by OpenAI, has two sources: one authored by\nannotators and the other consisting of instructions submitted via API to early models.\nCI & MC. (1) Using other datasets as instruction inputs and select-\ning diﬀerent models to generate responses . Alpaca GPT4 data ( Peng et al ,\n2023) employs instructions from the Alpaca data ( Taori et al , 2023) as input, gen-\nerating responses using GPT-4 ( Achiam et al , 2023). Alpaca GPT4 data zh ( Peng\n34\net al , 2023) and Wizard evol instruct zh dataset ( Ziang Leng and Li , 2023) trans-\nlate English instructions into Chinese before invoking models to generate Chinese\nresponses. Bactrain-X (Li et al, 2023c) utilizes a translation API to translate instruc-\ntion inputs from the Alpaca data and databricks-dolly-15K into 51 languages, then\ninputs them into ChatGPT to obtain responses. GPT4All ( Anand et al , 2023) uses\ninstructions from five public datasets as input and generates response s using GPT-\n3.5-Turbo. LogiCoT (Liu et al , 2023c) and OpenOrca ( Mukherjee et al , 2023) follow\nsimilar methods. GuanacoDataset 36 expands the language of instruction data from\nEnglish to Chinese and Japanese. LaMini-LM ( Wu et al , 2023) uses the model to\nsimultaneously generate synthetic instructions and responses corr esponding to real\ninstructions. These datasets reference existing instructions and are secondarily con-\nstructed with the assistance of models. (2) Using open-source datasets as seed\ninstructions to guide the generation of dialogues between models . Baize (Xu\net al , 2023a) samples “seeds” from specific datasets, allowing ChatGPT to engage\nin self-dialogue and batch generate high-quality multi-turn dialogue dat a. The dia-\nlogues cover both general and some vertical domains. (3) Directly constructing\ninput-output text pairs using existing data . LongForm (K¨ oksal et al, 2023) gen-\nerates complete instructions for existing pre-trained corpus doc uments using LLMs,\nthen expands them using structured corpus examples and task instan ces. Luotuo-QA-\nB (Liao et al , 2023) instructs the model to generate five input-output text pairs for\nsummaries or news content from three datasets.\nHG & CI & MC. The six datasets combine the three construction methods men-\ntioned in previous sections. The relevant information is as follows. (1) COIG (Zhang\net al , 2023a). The COIG dataset consists of 191K Chinese instructions categorized\ninto five types. Translated instructions are derived from open-source datasets, and the\ntranslation process involves three stages: automatic translation, manual verification,\nand manual correction. Exam instructions are primarily sourced from Chi nese college\nentrance exams, high school entrance exams, and civil service exams. Human value\nalignment instructions consist of two series—one focusing on general h uman value\nalignment in Chinese regions, and the other on human value alignment spec ific to\ncertain countries or regional cultures. Counterfactual correction mul ti-round chat are\nbuilt based on the CN-DBpedia knowledge graph dataset ( Xu et al , 2017), address-\ning hallucination issues in LLMs. Leetcode instructions gather programm ing-related\nprompts. (2) HC3 (Guo et al, 2023a). The HC3 dataset has both Chinese and English\nversions, totaling 37K Q&A pairs. The dataset is designed to compare resp onses\nbetween human experts and ChatGPT across various domains. It can be used for\nresearch in areas such as instruction fine-tuning, human value alignmen t, model\nresponse characteristics, and more. (3) Phoenix-sft-data-v1 (Chen et al , 2023d).\nThe 464K multilingual dialogue data in the Phoenix-sft-data-v1 dataset is primarily\ndivided into two parts: single-turn instructions and multi-tur n conversations. Single-\nturn instructions include Chinese and English instructions from Alpaca, translated\nmultilingual instructions, and user-generated multilingual instr uctions. Multi-turn\nconversations mainly originate from ShareGPT and Discord 37. (4) TigerBot sft en\n36https://guanaco-model.github.io/\n37https://discord.com/\n35\n& TigerBot sft zh (Chen et al , 2023c). These two datasets are fine-tuning data\nfor the TigerBot ( Chen et al , 2023c), containing a large amount of collected open-\nsource data and self-developed data. The construction of the dataset main ly follows\nfive principles: annotating and summarizing 10 instruction categories and 120 sub-\ntask types based on the distribution of instructions; generating ins tructions using the\nSelf-Instruct method; organizing question and answer data based on man ual question\ngeneration, web search, and other methods; converting and cleaning th e format based\non public datasets; the overall data distribution conforms to the natural distribution of\ninstructions. (5) Aya Collection (Singh et al, 2024). The Aya Collection is a compre-\nhensive and large corpus of datasets designed for training multilingual m odels, aimed\nat researchers worldwide. It comprises three primary sources of data: templated data,\ntranslated data, and the Aya Dataset (Singh et al, 2024). Templated data involves col-\nlaboration with fluent speakers to create templates for automatic dataset e xpansion\ninto various languages. Translated data involves translating a subset of 19 datasets\ninto 101 languages using the NLLB 3.3B machine translation model (Costa-juss` a et al,\n2022). The Aya Dataset is a human-annotated subset of the overall collection.\n3.3 Domain-speciﬁc Instruction Fine-tuning Datasets\nThe domain-specific instruction fine-tuning datasets are construct ed for a particular\ndomain by formulating instructions that encapsulate knowledge and task types closely\nrelated to that domain. After fine-tuning the pre-trained base model on the domain-\nspecific instruction fine-tuning datasets, it can be applied to vari ous scenario tasks\nwithin that domain, exhibiting outstanding performance. As shown i n Figure\n13, the\ndomain-specific instruction fine-tuning datasets are categorized into six major classes:\nmedical, code, legal, mathematical, educational, and other domains. The c ollected\nand organized information from the domain-specific instruction fine-tun ing datasets\nis presented in Table 7 and Table 8.\n3.3.1 Medical Domain\nCurrently, there are numerous open-source large-scale models for medical tasks in both\nChinese and English. All of them have constructed instruction fine- tuning datasets in\nthe medical domain for supervised fine-tuning, demonstrating excel lent generalization\ncapabilities. In some cases, the performance even close to that of professional doctors in\nspecific scenarios. CMtMedQA (\nYang et al, 2023d) and MedDialog (Zeng et al, 2020)\nexclusively utilize authentic doctor-patient multi-turn dial ogues, where all instruc-\ntions belong to real-world scenario data. In contrast, ChatMed Consult Dataset (Zhu\nand Wang, 2023) and ShenNong TCM Dataset (Wei Zhu and Wang, 2023) adopt the\nSelf-Instruct method, utilizing the model to generate medical Q&A data. The former\nfocuses on medical consultations, while the latter concentrates on tr aditional Chinese\nmedicine knowledge Q&A.\nSome datasets are collected and curated from open-source data such as know ledge\nbases and forums. For instance, Huatuo-26M ( Li et al , 2023h) has multiple sources,\nincluding medical encyclopedia Q&A, medical knowledge graphs, and d octor-patient\n36\nDomain-specific Instruction\nFine-tuning Datasets\nMedical\nMathematics\nCode\nLegal\nEducation\nOther\n Financial\n Geoscience\n Mental Health\n Biology\n IT\n Social Norms\n Transportation\nFig. 13 Domain categories of the domain-specific instruction fine-tuning datase ts\nTable 7 Summary of Domain-speciﬁc Instruction Fine-tuning Datasets Infor-\nmation Part I . Release Time: “X” indicates unknown month. Public or Not: “All”\nindicates full open source; “Partial” indicates partially open source. “License” indi-\ncates the dataset follows a certain protocol. If the dataset is built upon other datasets,\nthe licenses of the source datasets must also be adhered to\nDataset Publisher Release Time Size Public or Not License\nBELLE School Math BELLE 2023-5 248481 instances All GPL-3.0\nChatDoctor University of Texas Southwestern Medical Center et al. 2023-3 115K instances All Apache-2.0\nChatMed Consult Dataset michael-wzhu 2023-5 549326 instances All CC-BY-NC-4.0\nChild chat data Harbin Institute of Technology et al. 2023-8 5000 instances All -\nCMtMedQA Zhengzhou University 2023-8 68023 instances All MIT\nCode\nAlpaca 20K Sahil Chaudhary 2023-3 20K instances All Apache-2.0\nCodeContest DeepMind 2022-3 13610 instances All Apache-2.0\nCommitPackFT Bigcode 2023-8 702062 instances All MIT\nDISC-Fin-SFT Fudan University et al. 2023-10 246K instances Partial Apache-2. 0\nDISC-Law-SFT Fudan University et al. 2023-9 403K instances Partial Apache-2.0\nDISC-Med-SFT Fudan University et al. 2023-8 464898 instances All Apache-2.0\nEduchat-sft-002-data-osm East China Normal University et al. 2023-7 4279419 instance s All CC-BY-NC-4.0\nGeoSignal Shanghai Jiao Tong University et al. 2023-6 22627272 instances Partial Apache- 2.0\nGoat National University of Singapore 2023-5 1746300 instances All Apache-2.0\nHanFei 1.0 Chinese Academy of Sciences et al. 2023-5 255K instances All Apache-2.0\nHuatuoGPT-sft-data-v1 The Chinese University of Hong Kong et al. 2023-5 226042 ins tances All Apache-2.0\nHuatuo-26M The Chinese University of Hong Kong et al. 2023-5 26504088 instances Partial Ap ache-2.0\nLawGPT\nzh Shanghai Jiao Tong University 2023-5 200K instances Partial -\nLawyer LLaMA sft Peking Universit 2023-5 21476 instances Partial Apache-2.0\nMeChat Zhejiang University et al. 2023-4 56K instances All CC0-1.0\nMedDialog UC San Diego 2020-4 3.66M instances All -\nMedical Meadow University Hospital Aachen et al. 2023-4 160076 instances All GPL-3.0\nMedical-sft Ming Xu 2023-5 2.07M instances All Apache-2.0\nMol-Instructions Zhejiang University et al. 2023-6 2043586 instances All CC-B Y-4.0\nMWP Xihua University et al. 2021-9 251598 instances All MIT\nOpenMathInstruct-1 NVIDIA 2024-2 1.8M instances All NVIDIA License\nOwl-Instruction Beihang University et al. 2023-9 17858 instances All -\nPROSOCIALDIALOG Allenai 2022-5 165681 instances All CC-BY-4.0\nQiZhenGPT-sft-20k Zhejiang University 2023-5 20K instances Partial GPL-3.0\nShenNong\nTCM Dataset michael-wzhu 2023-6 112565 instances All Apache-2.0\nTaoLi data Beijing Language and Culture University et al. 2023-X 88080 instances All Apach e-2.0\nToolAlpaca Chinese Information Processing Laboratory et al. 2023-6 3928 instance s All Apache-2.0\nToolBench Tsinghua University et al. 2023-7 126486 instances All Apache-2.0\nTransGPT-sft Beijing Jiaotong University 2023-7 58057 instances All Apache-2. 0\n37\nTable 8 Summary of Domain-speciﬁc Instruction Fine-tuning Datasets Infor-\nmation Part II . Language: “EN” indicates English, “ZH” indicates Chinese, “PL”\nindicates Programming Language, and the number in parentheses indicates the num-\nber of programming languages included. “CM” indicates Construction Methods, where\n“HG” indicates Human Generated Datasets, “MC” indicates Model Construc ted\nDatasets, and “CI” indicates Collection and Improvement of Existing D atasets. “IC”\nindicates Instruction Category\nDataset Language CM Domain IC Source\nBELLE School Math ZH MC Math Math Generated by ChatGPT\nChatDoctor EN HG & MC Medical Multi Real conversations between doctors an d patients & Generated by ChatGPT\nChatMed Consult Dataset ZH MC Medical Multi Generated by GPT-3.5-Turbo\nChild chat data ZH HG & MC Education Multi Real conversations & Generated by GPT-3.5-Tu rbo\nCMtMedQA ZH HG Medical Multi Real conversations between doctors and pati ents\nCode Alpaca 20K EN & PL MC Code Code Generated by Text-Davinci-003\nCodeContest EN & PL CI Code Code Collection and improvement of variou s datasets\nCommitPackFT EN & PL (277) HG Code Code GitHub Action dump\nDISC-Fin-SFT ZH HG & CI & MC Financial Multi Open source datasets & Manual ly collect financial data & ChatGPT assistance\nDISC-Law-SFT ZH HG & CI & MC Law Multi Open source datasets & Legal-related T ext Content & Generated by GPT-3.5-Turbo\nDISC-Med-SFT ZH HG & CI Medical Multi Open source datasets & Manually se lected data\nEduchat-sft-002-data-osm EN & ZH CI Education Multi Collection and improv ement of various datasets\nGeoSignal EN HG & CI & MC Geoscience Multi Open source datasets & Geoscie nce-related Text Content & Generated by GPT-4\nGoat EN HG Math Math Artificially synthesized data\nHanFei 1.0 ZH - Law Multi Filter legal-related data according to rules\nHuatuoGPT-sft-data-v1 ZH HG & MC Medical Multi Real conversations between doctors and patients & Generated by ChatGPT\nHuatuo-26M ZH CI Medical Multi Collection and improvement of various dataset s\nLawGPT\nzh ZH CI & MC Law Multi Real conversations & Generated by ChatGPT\nLawyer LLaMA sft ZH CI & MC Law Multi Generated by ChatGPT with other datasets’ promp ts\nMeChat ZH CI & MC Mental Health Multi Based on PsyQA dataset with the propos ed SMILE method\nMedDialog EN & ZH HG Medical Multi Real conversations between doctors and p atients\nMedical Meadow EN HG & CI Medical Multi Crawl data from the Internet & C ollection and improvement of various NLP datasets\nMedical-sft EN & ZH CI Medical Multi Collection and improvement of var ious NLP datasets\nMol-Instructions EN HG & CI & MC Biology Multi Molecule-oriented, Prote in-oriented, Biomolecular text instructions\nMWP EN & ZH CI Math Math Collection and improvement of various datasets\nOpenMathInstruct-1 EN CI & MC Math Math GSM8K and MATH datasets (original questions); Generated using Mixtral-8 × 7B model\nOwl-Instruction EN & ZH HG & MC IT Multi Generated by GPT-4 & Manual veri fication\nPROSOCIALDIALOG EN HG & MC Social Norms Social Norms Generated by humans with G PT-3 created prompts\nQiZhenGPT-sft-20k ZH CI Medical Multi Collection and improvement of v arious datasets\nShenNong\nTCM Dataset ZH MC Medical Multi Generated by ChatGPT\nTaoLi data ZH HG & CI Education Multi Collection and improvement of various datase ts & Manually extract dictionary data\nToolAlpaca EN & PL HG & MC Code Code Manually filter APIs & Generated by Chat GPT\nToolBench EN & PL HG & MC Code Code Manually filter APIs & Generated by Ch atGPT\nTransGPT-sft ZH HG Transportation Multi Manually collect traffic-related data\nQ&A. QiZhenGPT-sft-20k 38 formulates instructions based on the content collected\nfrom the Qizhen medical knowledge base. Medical-sft 39 merges several Chinese and\nEnglish medical datasets, including the ChatDoctor (Li et al, 2023l) and QiZhenGPT-\nsft-20k, among others.\nIn addition to the aforementioned, some datasets may comprise a combinat ion of\nreal and synthetic data or involve manual curation based on existing datase ts. Chat-\nDoctor and HuatuoGPT-sft-data-v1 ( Zhang et al , 2023b), while collecting authentic\ndoctor-patient dialogues, incorporate conversation data generated by Chat GPT and\ninformation from a disease database. DISC-Med-SFT ( Bao et al , 2023) and Medi-\ncal Meadow ( Han et al , 2023) meticulously select several data sources, undergoing a\ncertain degree of reconstruction to enhance the overall quality of the datasets.\n3.3.2 Code Domain\nThe purpose of the code instruction fine-tuning datasets is to enhan ce the capabilities\nof LLMs in tasks such as code generation and tool invocation. Some datasets foc us on\ninstructions tailored for code generation tasks. CommitPackFT (\nMuennighoff et al ,\n2023a) extracts code files covering 350 programming languages, rigorously filter ing\nand retaining code instruction data for 277 programming languages. Code Alpaca 20K\n(Chaudhary, 2023) follows the construction method of the Alpaca data (Taori et al,\n38https://github.com/CMKRG/QiZhenGPT\n39https://github.com/shibing624/MedicalGPT\n38\n2023), generating 20K instructions for fine-tuning the Code Alpaca model ( Chaud-\nhary, 2023). CodeContest ( Li et al , 2022a) merges data collected from Codeforces 40,\nDescription2Code ( Caballero et al , 2016), and CodeNet ( Puri et al , 2021). In addi-\ntion, some datasets emphasize instructions for tool invocation tasks. ToolAlpaca (Tang\net al , 2023) creates a highly diverse tool usage dataset through the construction of\na multi-agent simulation environment, fine-tuning the model with 3,928 instances of\ntool usage. The construction of the ToolBench ( Anonymous, 2024) involves three\nstages: API collection, instruction generation, and solution path annotation , aiming\nto fine-tune the model for tool usage instructions.\n3.3.3 Legal Domain\nVarious LLMs in the legal domain have been introduced, but there is a re latively lim-\nited availability of open-source legal instruction datasets. Here, we compile information\non four partially or fully open-source legal instruction datasets that can b e utilized to\nenhance model capabilities in tasks such as legal Q&A, judgment predic tion, and case\nclassification. DISC-Law-SFT (\nYue et al, 2023) is divided into two sub-datasets, each\nintroducing legal reasoning abilities and the utilization of external knowledge to the\nmodel. Han Fei 1.0 (He et al, 2023c) merges general instructions with legal instructions,\naiming to equip the model with legal knowledge while retaining its general capabili-\nties. LawGPT zh (Liu et al, 2023b) includes scenario-based Q&A with legal basis and\nsingle-turn legal Q&A obtained through model cleaning. Lawyer LLaMA sft (Huang\net al, 2023b) involves model-generated Chinese judicial exam Q&A, legal consultation\nresponses, and multi-turn dialogue data.\n3.3.4 Mathematics Domain\nThe performance and future potential of LLMs in the field of mathematics ha ve\nalways been a focal point of attention. Mathematical problems assess vari ous skills\nsuch as computation, reasoning, spatial thinking, making them inherent ly challeng-\ning. This often results in model performance on mathematical problems falling below\nexpectations. Consequently, one common approach to improving models ’ mathemati-\ncal abilities is to perform supervised fine-tune using effective mathematical instruction\ndatasets.\nBELL\nSchool Math (BELLEGroup, 2023) generates Chinese mathematical prob-\nlems, including the solution process, through the model. However, the overall difficulty\nis low, and the answers have not undergone rigorous verification, potenti ally con-\ntaining errors. Goat ( Liu and Low , 2023) consists entirely of artificially synthesized\ndata for arithmetic tasks, covering addition, subtraction, multipl ication, and division\noperations, with difficulty levels not posing significant challenges for humans. MWP\n(Lan et al, 2022) unifies eight mathematics-related NLP datasets into instruction for-\nmat, offering both single-equation and multiple-equation forms. OpenM athInstruct-1\n(Toshniwal et al, 2024) leverages the Mixtral-8x7B model (Jiang et al, 2024) to reason\nover questions from the GSM8K ( Cobbe et al , 2021) and MATH ( Hendrycks et al ,\n40https://codeforces.com/blog/entry/89502\n39\n2021d) datasets, generating a plethora of question-solution text pairs. It s ignificantly\nenhances the models’ mathematical capabilities.\nCurrently, there is a scarcity of high-difficulty mathematical inst ruction datasets,\nlimited by factors such as high entry barriers, complex symbols, high costs, and non-\nopen sourcing.\n3.3.5 Education Domain\nLLMs in the education domain focus on course guidance, emotional support, ch ild\ncompanionship, knowledge learning, and other aspects, serving teachers, students, and\nparents. Their goal is to become new tools applied in the education indu stry. LLMs in\nthe education domain undergo fine-tuning using specifically collected education-related\ninstructions. Child\nchat data41 primarily revolves around the theme of emotional com-\npanionship for children, containing both real and synthetic Chines e dialogue data\nrelated to emotional companionship for children. Educhat-sft-002-data-osm (Dan et al,\n2023) is used for the development of the EduChat project and combines mul tiple Chi-\nnese and English educational instructions and dialogue data. It is used to train models\nthat can provide open-ended questioning, emotional support, essay cor rection, and\nother functions in an educational setting. TaoLi data (Yu et al, 2023b) is constructed\nbased on internationally circulated Chinese teaching materials, HanyuShuiping Kaoshi\n(HSK) exams42, Chinese dictionaries, and other resources. It includes various forms of\ninstructions to enable the model to acquire knowledge related to international Chinese\neducation.\n3.3.6 Other Domains\nCurrently, other domain-specific fine-tuning datasets are gradually b eing open-\nsourced. The seven domains mentioned belows, although having fewer open resources\nfor fine-tuning instructions, still hold significant meaning and valu e.\nFinancial Domain. DISC-Fin-SFT (\nChen et al, 2023a) is a high-quality Chinese\nfinancial dataset. It is utilized for LoRA (Hu et al, 2022a) instruction fine-tuning on the\nBaichuan-13B-Chat model, ultimately resulting in the financial LLM DIS C-FinLLM\n(Chen et al , 2023a). The dataset comprises 246K instructions categorized into four\nsubtypes: financial consultation, financial tasks, financial calculations , and retrieval\nenhancement. Sourced diversely from financial NLP datasets, manually c urated Q&A\npairs, and model-generated dialogues, a portion of this dataset is curren tly open-\nsourced.\nGeoscience Domain. GeoSignal ( Deng et al , 2023) is being used for the fine-\ntuning of instructions for K2 (Deng et al, 2023), the first LLM in the field of geoscience.\nThe creators have collected extensive data from various databases and we bsites in\nthe earth science domain. They have restructured this data into a unified sequence\nformat suitable for tasks such as interpretation, named entity recognit ion, reasoning,\ntext classification, and Q&A. The original dataset size is 22.6M instances, b ut after\ncleaning, 40K data instances have been retained. A complete version is planned for\nfuture release.\n41https://github.com/HIT-SCIR-SC/QiaoBan\n42https://www.chinesetest.cn/\n40\nMental Health Domain. MeChat ( Qiu et al , 2023) is Chinese psychological\nhealth dialogue data. Builders transform real psychological mutual assistan ce Q&A\ninto multi-turn dialogues using models. The dataset comprises 56K in structions,\ncatering to extended conversational scenarios.\nBiology Domain. Mol-Instructions (Fang et al, 2023) consists of three main com-\nponents: Molecule-oriented instructions, Protein-oriented ins tructions, and Biomolec-\nular text instructions. Each part focuses on chemical reactions and mol ecular design,\nprotein prediction, and bioinformatics in biochemistry, respectively. The dataset’s con-\nstruction involves a combination of human-machine collaboration, database r esource\nprocessing, and the transformation of biological data.\nIT Domain. Owl-Instruction ( Guo et al , 2023b) is utilized for the instruction\nfine-tuning of the Owl model ( Guo et al , 2023b). The instructions are specifically\ndesigned for handling IT-related tasks such as troubleshooting, log anal ysis, etc. The\ndataset construction involves four stages: data generation, GPT-4 filter ing, manual\nverification, and supervised fine-tuning. It comprises 18K single-tu rn and multi-turn\ninstructions.\nSocial Norms Domain. PROSOCIALDIALOG ( Kim et al , 2022) is a multi-\nturn English conversation dataset that instructs models to respond t o problematic\ninputs according to human social norms. The dataset covers various une thical, prob-\nlematic, biased, and harmful scenarios, created using a human-machin e collaboration\nframework.\nTransportation Domain. TransGPT-sft (Duomo, 2023) serves as the fine-tuning\ncomponent for China’s pioneering open-source TransGPT traffic model ( Duomo,\n2023). Adopting a dialogue-centric methodology, the dataset involves extrac ting con-\ntent from documents in formats like PDFs and Doc files. LLMs are then em ployed to\ngenerate dialogues related to traffic based on the document content.\n3.4 Distribution Statistics of Instruction Fine-tuning Datasets\nFigure\n14 provides statistics on 103 instruction fine-tuning datasets from six aspects:\nrelease time, license, data category, construction method, language, an d domain. The\nfollowing conclusion can be drawn:\n(1) The number of instruction fine-tuning datasets is showing a grow ing trend.\nThe widespread attention to LLMs and the application of the instruction fi ne-tuning\nparadigm have greatly facilitated the construction and open-sourcing of i nstruction\nfine-tuning datasets. The demand for model fine-tuning and research interest in this\narea are rapidly expanding.\n(2) Data licenses to some extent reflect the openness and accessibil ity of datasets.\nFor instruction fine-tuning datasets, the Apache-2.0 license is the most commonly\nused, covering 43 datasets, followed by the GPL-3.0 license and the M IT license. This\nreflects the developers’ inclination towards open and shared data.\n(3) The majority of instruction fine-tuning datasets are concentrated in the range\nof 10K to 1M, totaling 63 datasets. This indicates that, in practical applicat ions,\ndatasets of this scale are sufficient to meet the demand. However, the re are relatively\nfewer small-scale and large-scale datasets, reflecting the challenges and scarcity at\nboth extremes. Small-scale datasets emphasize quality but may lack category richness,\n41\n2 5\n11\n82\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n2020 2021 2022 2023\nThe Number of Instruction Fine-tuning Datasets\nRelease Time\n(a)\n1\n1\n1\n1\n1\n1\n4\n4\n9\n11\n12\n43\n0 5 10 15 20 25 30 35 40\nCC\nCC-BY-NC-ND-4.0\nCC-BY-NC-SA\nCC-BY-SA-3.0\nCC-BY-SA-4.0\nLMSYS-Chat-1M license\nCC0\nCC-BY-4.0\nCC-BY-NC-4.0\nMIT\nGPL-3.0\nApache-2.0\nThe Number of Instruction Fine-tuning Datasets\nLicense\n(b)\n3\n27\n36\n19\n3\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0-10K 10K-100K 100K-1M 1M-10M 10M-100M\nThe Number of Instruction Fine-tuning Datasets\nSize / # instances\n(c)\n29\n22\n15\n11\n9 8\n5\n0\n5\n10\n15\n20\n25\n30\nMC CI\nCI & MC\nHG\nHG & CI & MC\nHG & MC HG & CI\nThe Number of Instruction Fine-tuning Datasets\nConstruction Method\n(d)\n35\n33\n14\n12\n5\n1\n0\n5\n10\n15\n20\n25\n30\n35\nEN ZH EN & ZH Multi EN & PL Multi & PL\nThe Number of Instruction Fine-tuning Datasets\nLanguage\n(e)\n67\n11\n5 4 3 3 1 1 1 1 1 1 1\n0\n10\n20\n30\n40\n50\n60\n70\nGeneral Medical\nCode Law\nEducation\nMath Biology FinancialGeoscience\nIT\nMental HealthSocial NormsTransportation\nThe Number of Instruction Fine-tuning Datasets\nDomain\n(f)\nFig. 14 Statistics distribution of instruction fine-tuning datasets. (a) i llustrates the\nquantity trend over time. (b) depicts the quantity distribution under different licenses,\nconsidering only the datasets with listed licenses. (c) shows th e quantity distribution\nacross different data scales. (d) displays the quantity distribut ion for different con-\nstruction methods. (e) represents the quantity distribution ac ross different languages.\n(f) illustrates the quantity distribution across different domains . Zoom in for better\nview\nwhile large-scale datasets offer diversity but may be constrained by c omputational\nresources and affected by data redundancy.\n(4) The “utilizing model-constructed instructions” method is t he most prevalent\nin constructing datasets, highlighting its potential in dataset creat ion. The quality of\nsuch datasets relies primarily on the models’ performance and the gui dance provided\nduring construction. The second most common method is “curating exis ting datasets\nand improving them,” indicating the active utilization of open-sour ce data. The num-\nber of datasets manually generated is comparatively lower due to efficien cy and cost\nconsiderations. There are 22 datasets that employ combinations of differe nt methods\nto further enhance dataset quality, suggesting that this approach may b ecome more\nmainstream in the future.\n(5) Chinese and English instruction datasets hold a crucial position inresearch, gar-\nnering greater attention. Mixed Chinese and English, as well as multi lingual datasets,\nshow a considerable quantity, indicating that cross-language research is becoming a\nfocus. There is a scarcity of open-source instruction datasets relat ed to programming\nlanguages, primarily tailored for specific application scenarios.\n(6) The number of general-domain datasets is 67, aligning with the widespr ead\ndemand for instruction fine-tuning techniques in various applicati on scenarios.\nResearch and construction of instruction datasets for relevant LLMs have also been\nconducted in common fields such as healthcare, programming, law, etc. Th ere are\ndatasets available in other domains as well, indicating the potential app lications of\nLLMs in diverse disciplines and industries. However, there are still instruction datasets\nfor niche fields awaiting further research and exploration.\n42\n4 Preference Datasets\nPreference datasets are collections of instructions that provide pr eference evaluations\nfor multiple responses to the same instruction input. Typically, they consist of pairs\nof instructions with different responses, along with feedback from h umans or other\nmodels. This setup reflects the relative preferences of humans or models for differ-\nent responses within a given task or context. The feedback information in preference\ndatasets is often manifested through voting, sorting, scoring, or other forms of compar-\nison. Figure\n15 categorizes various preference datasets based on the methods used for\npreference evaluation. The collected and organized information on preference datasets\nis presented in Table 9 and Table 10.\nPreference datasets are primarily utilized during the alignment p hase of large\nmodels, aiming to assist in aligning the models’ outputs more closel y with human\npreferences and expectations. The alignment with human preferenc es is manifested\nin three main aspects: utility, possessing the ability to follow instructions; honesty,\navoiding fabrications; and safety, refraining from generating illegal or harmful infor-\nmation (Zhao et al, 2023). Both RLHF (Christiano et al, 2017; Ziegler et al, 2019) and\nRLAIF (Reinforcement Learning from AI Feedback) ( Lee et al , 2023c) employ rein-\nforcement learning methods to optimize models using feedback si gnals. In addition to\nfine-tuning with instruction datasets, it is also possible to train reward models with\npreference datasets. Subsequently, the Proximal Policy Optimization (PPO) algorithm\ncan be applied for further fine-tuning based on the feedback from the r eward models\n(Schulman et al, 2017).\nPreference Datasets\nVote Sort Score Other\n V ote-Human\n V ote-Model\nCV alues\nChatbot_arena_conversations\n Sort-Human\nOASST1_pairwise_rlhf_reward\n Score-Human\nStack-Exchange-Preference\n Score-Model \nAlpaca_comparison_data\nStep Alignment\nPRM800K\nSource Discrepancy\nMedical-rlhf\nhh-rlhf\nMT-Bench_human_judgments\nPKU-SafeRLHF\nSHP\nSummarize_from_Feedback\nZhihu_rlhf_3k\nSummarize_from_Feedback\nWebGPT\nStable_Alignment\nUltraFeedback\nFig. 15 Different preference datasets corresponding to various preferenc e evaluation\nmethods\n4.1 Preference Evaluation Methods\nThe preference evaluation methods for preference datasets can be cat egorized into\nvoting, sorting, scoring, and other methods . Each method can be conducted\nby humans or aligned high-quality LLMs. Human feedback provides prefere nces that\nare more aligned with real-world scenarios, capturing intuitive human cognition and\nlanguage understanding. However, it may exhibit subjectivity and in consistencies due\nto individual differences, requiring more time and cost for annotation. Model feedback\n43\nTable 9 Summary of Preference Datasets Information Part I . Public or Not:\n“All” indicates full open source; “Partial” indicates partially open sour ce. “License”\nindicates the dataset follows a certain protocol. If the dataset is bui lt upon other\ndatasets, the licenses of the source datasets must also be adhered to\nDataset Publisher Release Time Size Public or Not License\nAlpaca comparison data Stanford Alpaca 2023-3 51K instances All Apache-2.0\nChatbot arena conversations UC Berkeley et al. 2023-6 33000 instances All CC-BY-4.0 & CC-BY-NC- 4.0\nCValues Alibaba Group 2023-7 145K instances All Apache-2.0\nhh-rlhf Anthropic 2022-4 169352 instances All MIT\nMedical-rlhf Ming Xu 2023-5 4K instances All Apache-2.0\nMT-Bench\nhuman judgments UC Berkeley et al. 2023-6 3.3K instances All CC-BY-4.0\nOASST1 pairwise rlhf reward Tasksource 2023-5 18918 instances All Apache-2.0\nPKU-SafeRLHF Peking University 2023-7 361903 instances Partial CC-BY-NC-4.0\nPRM800K OpenAI 2023-5 800K instances All MIT\nSHP Stanford University 2021-10 385563 instances All -\nStable\nAlignment Dartmouth College et al. 2023-5 169K instances All Apache-2.0\nStack-Exchange-Preferences Anthropic 2021-12 10807695 instances All CC-BY-SA-4. 0\nSummarize from Feedback OpenAI 2020-9 193841 instances All -\nUltraFeedback Tsinghua University et al. 2023-10 63967 instances All MIT\nWebGPT OpenAI 2021-12 19578 instances All -\nZhihu\nrlhf 3k Liyucheng 2023-4 3460 instances All CC-BY-2.0\nTable 10 Summary of Preference Datasets Information Part II . Language: “EN”\nindicates English, “ZH” indicates Chinese, “Multi” indicates Mult ilingual. “CM”\nindicates Construction Methods, where “HG” indicates Human Generate d Datasets,\n“MC” indicates Model Constructed Datasets, and “CI” indicates Collec tion and\nImprovement of Existing Datasets. “IC” indicates Instruction Cate gory. “PEM” indi-\ncates Preference Evaluation Method, where “VO” indicates Vote, “SO” indicates\nSort, “SC” indicates Score, “-H” indicates Conducted by Humans, “-M” ind icates\nConducted by Models\nDataset Language CM Domain IC PEM Source\nAlpaca comparison data EN MC General Multi SC-M Generated by three LLMs & GPT-4 scoring\nChatbot arena conversations Multi HG & MC General Multi VO-H Generated by twenty LLM s & Manual judgment\nCValues ZH MC Social Norms Social Norms VO-M Generated by LLMs & Evaluation by the reward model\nhh-rlhf EN HG & MC General Multi VO-H Generated by LLMs & Manual judgmen t\nMedical-rlhf ZH CI & MC Medical Multi Other Response chosen comes from the doctor’s response & Response rejected comes from the model’s response\nMT-Bench human judgments EN HG & MC General Multi VO-H Generated by LLMs & Manual judgme nt\nOASST1 pairwise rlhf reward Multi CI General Multi SO-H OASST1 datasets & Manual sorting\nPKU-SafeRLHF EN HG & CI & MC Social Norms Social Norms VO-H Generated by LLMs & Man ual judgment\nPRM800K EN HG & CI & MC Math Math Other Generated by LLMs & Mathematical reasonin g steps are determined manually\nSHP EN HG General Multi VO-H Reddit data & Manual judgment\nStable\nAlignment EN MC General Multi SC-M Generated by LLMs & Model scoring\nStack-Exchange-Preferences EN HG General Multi SC-H Stackexchange data & Manual scoring\nSummarize from Feedback EN HG & CI News Multi VO-H & SC-H Open source datasets & Manual j udgment and scoring\nUltraFeedback EN CI & MC General Multi SC-M Generated by seventeen LLMs & Model scoring\nWebGPT EN HG & CI General Multi SC-H Open source datasets & Manual scor ing\nZhihu rlhf 3k ZH HG General Multi VO-H Zhihu data & Manual judgment\ncan leverage learned human preference information and extensive know ledge from\nhigh-quality models, saving annotation time and cost. However, it may be influenced\nby inherent biases in the model, and the feedback information may be less authentic\ncompared to human feedback. In general, a comprehensive approach that com bines\nvarious forms and sources of preference data may be more advantageous. Figur e 16\nvisually presents various preference evaluation methods.\n4.1.1 Vote\nThe voting method typically involves selecting the better option from two answers\nor choosing several preferred options from multiple answers. The adv antage is its\nsimplicity and intuitiveness, making it easy to collect and reflec ting the opinions of\nthe group. However, the drawback is the lack of granularity in information .\n44\nVote\nVote countsQuestion\nResponse 1\nResponse 2\nJudge voting\n Question\nor\nResponse 1\nResponse 2\nResponse n\n...\n100\n2\n0 ...\nSort\n2\nDots represent responses\nQuestion\n1 3 4 5 21\nDots represent responses\nQuestion\n3 4 5\nSecurityAccuracy\n1\n...\n2\n1\n3\n4\n5\nRanking\n...\n3\n1\n4\n2\n5\nScore\nWorst\nBest\n2\n1\n4\n3\n5\nReasonScore\n(0-1)\n0.5\n0.8\n0.8\n0\n. . . . . .\n. . . . . .\n. . . . . .\n. . . . . .\n. . . . . .\nFig. 16 Different preference evaluation methods\nDatasets using the “human vote” method are as follows. Chatbot arena con-\nversations (Zheng et al, 2023b) includes examples with answers from two models to the\nsame question and the selection made by a human judge. It comprises outp uts from\na total of 20 models in 96 languages. The dataset also annotates unsafe conversati ons\nfor related research. The hh-rlhf dataset ( Bai et al , 2022; Perez et al , 2022) includes\ninstances with accepted and rejected answers, where crowdworkers instruct the model\nto perform a task and choose the more useful and honest answer from two opt ions.\nMT-Bench human judgments (Zheng et al, 2023b) involves graduate students compar-\ning pairwise preferences for 80 instructions generated by six mode ls. PKU-SafeRLHF\n(Ji et al, 2023a) focuses on comparing performance and safety preferences. After eval -\nuating the harmlessness of instructions, choices are made based on use fulness and\nharmlessness in the Q&A format. Each entry in the final dataset includes two answers\nand feedback information. SHP (Ethayarajh et al, 2022) is crawled from Reddit. Each\npost contains a question and a pair of answers, with one answer being more favored by\nReddit users, constructing a preference dataset reflecting human preferences. Similarly,\nZhihu rlhf 3k43 is built in the same way using the Zhihu. Summarize from Feedback\n(Stiennon et al , 2020) is primarily constructed to optimize summarization models.\nThe dataset consists of two parts: one where annotators choose the better of two sum-\nmaries, and the other where summaries are rated using a Likert scale. T he dataset\nuses both human voting and human scoring.\nA representative dataset for the “model vote” method is CValues (Xu et al, 2023d).\nThe CValues dataset encompasses three types of responses: safe and res ponsibility,\nsafe, and unsafe, focusing on the domain of social norms. During constru ction, models\nassign different types to various responses, enabling a safety comparison between pairs\nof responses.\n43https://huggingface.co/datasets/liyucheng/zhihu rlhf 3k\n45\n4.1.2 Sort\nThe sorting method involves arranging multiple responses to the sam e question in\ndescending order according predefined criteria. The criteria for sorting are deter-\nmined by specific requirements. This method provides more det ailed information,\nreflecting the relative preference order, but it is cumbersome t o collect and pro-\ncess the sorting information, and the sorting criteria need to be st andardized.\nOASST1\npairwise rlhf reward44 is a representative dataset in this category. It under-\ngoes post-processing on the OASST1 ( Wang et al , 2023a), generating data directly\nused for RLHF. The dialogues in the OASST1, constructed by humans and accom-\npanied by quality ratings, allow for direct sorting of different respon ses based on\nannotations, reflecting human preferences.\n4.1.3 Score\nThe scoring method involves assigning scores within a certain range t o several\nresponses to the same question. This method provides a continuousevaluation, offering\na more flexible representation of preference intensity, allowing the model to under-\nstand human preferences in a more nuanced manner. However, it is imp ortant to note\nissues related to the uniformity of scoring criteria and subjecti ve awareness in the\nscoring process.\nSome datasets use human scoring to reflect preferences. Stack-Exch ange-\nPreferences (\nAskell et al , 2021) is derived from StackOverflow, where each answer is\nassigned a score defined by Askell et al (2021). This score is based on the number of\nlikes the answer receives and whether it is accepted by the quest ion asker. In Summa-\nrize from Feedback (Stiennon et al, 2020), a portion of it involves scoring the quality\nof different answers using the Likert scale. WebGPT ( Nakano et al , 2021) includes\nexamples with two model answers to a question along with relevant me tadata. Each\nanswer has a preference score assigned by humans to indicate which ans wer is better.\nIn addition to human scoring, models can also be used to replace humans i n this\nprocess. Alpaca comparison data (Peng et al, 2023) involves three models generating\ndifferent responses, with GPT-4 scoring the quality of the respons es. Each example\ncontains one high-quality answer and one low-quality answer. Stable Alignment (Liu\net al, 2023d) includes three types of alignment data from simulated social interactions,\nwith multiple different model-generated responses and correspond ing scores for each\ndata point. UltraFeedback (Cui et al, 2023) employs models to score four answers from\nfour dimensions, providing detailed textual explanations for impro ving the answers,\nthereby enriching the dimensions of instructions, models, and p references.\n4.1.4 Other\nIn addition to the three methods mentioned earlier, a small portion of preference\ndatasets employs alternative preference evaluation methods.\nMedical-rlhf (\nXu, 2023). The Medical-rlhf dataset is a Chinese dataset designed\nfor aligning medical models. The dataset consists of 4K examples sampled from a\nChinese medical dialogue dataset. Each example includes two response s, with the\n44https://huggingface.co/datasets/tasksource/oasst1 pairwise rlhf reward\n46\n1\n3\n1\n11\n0\n2\n4\n6\n8\n10\n12\n2020 2021 2022 2023\nThe Number of Preference Datasets\nRelease Time\n(a)\n1\n1\n2\n2\n3\n5\n0 1 2 3 4 5\nCC-BY-2.0\nCC-BY-SA-4.0\nCC-BY-4.0\nCC-BY-NC-4.0\nMIT\nApache-2.0\nThe Number of Preference Datasets\nLicense\n(b)\n6\n3\n2 2\n1 1 1\n0\n1\n2\n3\n4\n5\n6\nVO-H SC-M SC-H Other VO-M SO-H VO-H & SC-H\nThe Number of Preference Datasets\nPreference  Evaluation Method\n(c)\n3 3 3\n2 2 2\n1\n0\n1\n2\n3\nHG MC\nHG & MC HG & CI\nHG & CI & MC\nCI & MC\nCI\nThe Number of Preference Datasets\nConstruction Method\n(d)\n11\n3\n2\n0\n2\n4\n6\n8\n10\n12\nEN ZH Multi\nThe Number of Preference Datasets\nLanguage\n(e)\n11\n2\n1 1 1\n0\n2\n4\n6\n8\n10\n12\nGeneral Social Norms Math Medical News\nThe Number of Preference Datasets\nDomain\n(f)\nFig. 17 Statistics distribution of preference datasets. (a) illustrates the quantity trend\nover time. (b) depicts the quantity distribution under differe nt licenses, considering\nonly the datasets with listed licenses. (c) shows the quantity distribution across differ-\nent preference evaluation methods. (d) displays the quantity di stribution for different\nconstruction methods. (e) represents the quantity distributi on across different lan-\nguages. (f) illustrates the quantity distribution across different d omains. Zoom in for\nbetter view\nhigher-quality response being authentic professional replies from real doctors and the\nlower-quality response being model-generated. Nevertheless, t he dataset has a rela-\ntively small scale, and the categorization of high and low quality is too di rect and\nabsolute for the given questions.\nPRM800K (\nLightman et al, 2023). The PRM800K dataset is used for supervised\nlearning of the steps in the CoT process for mathematics. It contains 102K samples of\nmathematical solutions and 1M step-level labels, covering 12K mathematical problems.\nHuman annotators have labeled each step of the model-generated solutions, providing\nan assessment of correctness. This supervision method can also be viewed as providing\nan alignment signal to the model.\n4.2 Distribution Statistics of Preference Datasets\nFigure\n17 provides statistics on 16 preference datasets from six aspects: rel ease time,\nlicense, preference evaluation method, construction method, lan guage, and domain.\nThe following conclusion can be drawn:\n(1) The introduction of reinforcement learning and the in-depth re search on LLMs\nalignment (Christiano et al , 2017; Ziegler et al , 2019; Lee et al , 2023c) have spurred\nthe development of preference datasets, showing a rapid growth tre nd in 2023. The\nalignment between models and humans has become an increasingly importan t aspect.\n(2) The majority of preference datasets are available for commercial purposes, with\nApache-2.0 license being predominant among them.\n(3) Among all preference evaluation methods, human voting is the most commonly\nused. This method has a more convenient annotation process and reflect s genuine\n47\nhuman preferences. The next in popularity are human scoring and mode l scoring,\nwhich present preferences in a more intuitively distinguishable manner through scores.\nThe sorting method and the combination of multiple evaluation methods are rarely\nused, constrained by the cumbersome process and inconsistencies in standards.\n(4) From the perspective of dataset construction, the most common approach for\npreference datasets is human preference annotation and model-assist ed generation of\nresponses of varying quality, as these datasets require annotating feedback information\nbased on different responses. The second approach involves scraping Q&A from social\nplatforms and using metrics like upvotes as a preference indicator.\n(5) Preference datasets are predominantly in English, with a small por tion in Chi-\nnese or a mixture of multiple languages. Overall, preference datasets in languages\nother than English are relatively scarce.\n(6) Preference dataset examples mainly focus on general domains and soci al norm\ndomains, especially in the realm of social norms. The primary goal is to en sure that\nLLMs align with human expectations across various general tasks and comprehen -\nsive safety considerations. Preference datasets specifically tailor ed for other vertical\ndomains have not received significant attention at the moment.\n5 Evaluation Datasets\nEvaluation datasets are a carefully curated and annotated set of data samples used\nto assess the performance of LLMs across various tasks. Different evaluation datasets\nfocus on different evaluation aspects, providing an objective measure of different mod-\nels. By solely adjusting the conditions of the training, including t he pre-training\ncorpora, instruction fine-tuning datasets, and preference datasets , the performance of\nLLMs on corresponding evaluation datasets can indirectly reflect the quality and effec-\ntiveness of the datasets. This, in turn, aids in the ongoing optimizati on of training\ndata. The collected and organized information on representative existin g evaluation\ndatasets is presented in Table\n11, Table 12, and Table 13.\nEvaluation Category\n General\n Exam\n Subject\n NLU\n Reasoning\n Knowledge\n Long Text\n Tool\n Agent\n Code\n Out-of-Distribution\n Law\n Medical\n Financial\n Social Norms\n Factuality\n Evaluation\n Multitask\n Multilingual\n Other\nFig. 18 Evaluation categories of the evaluation datasets\n5.1 Evaluation Domains\nGuo et al (2023c) categorizes LLM evaluations into five evaluation categories based\non different dimensions: knowledge and capability evaluation, alignmen t evaluation,\nsafety evaluation, specialized LLMs evaluation, and evaluation organization. Asshown\nin Figure 18, this paper focuses on the key evaluation domains of each evaluation\n48\nTable 11 Summary of Evaluation Datasets Information Part I . Public or Not:\n“All” indicates full open source; “Partial” indicates partially open sou rce; “Not” indi-\ncates not open source. “License” indicates the dataset follows a certain protocol. If\nthe dataset is built upon other datasets, the licenses of the source dat asets must also\nbe adhered to\nDataset Publisher Release Time Size Public or Not License\nAgentBench Tsinghua University et al. 2023-8 1360 instances All -\nAGIEval Microsoft 2023-4 8062 instances All MIT\nALCUNA Peking University 2023-10 84351 instances All MIT\nAlpacaEval Stanford et al. 2023-5 805 instances All Apache-2.0\nAPI-Bank Alibaba DAMO Academy et al. 2023-4 264 dialogues All MIT\nAPIBench UC Berkeley et al. 2023-5 16450 instances All Apache-2.0\nAPPS UC Berkeley et al. 2021-5 10000 instances All MIT\nARB DuckAI et al. 2023-7 1207 instances All MIT\nBayLing-80 Chinese Academy of Sciences 2023-6 320 instances All GPL-3.0\nBBF-CFLEB Fudan University et al. 2023-2 11327 instances All -\nBBH Google Research et al. 2022-10 6511 instances All MIT\nBELLE\neval BELLE 2023-4 1000 instances All Apache-2.0\nBIG-Bench Google et al. 2022-6 - All Apache-2.0\nBIRD The University of Hong Kong et al. 2023-5 12751 instances All CC-BY-NC-4.0\nBOSS Tsinghua University et al. 2023-6 - All MIT\nCBLUE Zhejiang University et al. 2022-5 195820 instances All Apache-2.0\nC-CLUE Tianjin University 2021-8 - All CC-BY-SA-4.0\nCELLO Fudan University et al. 2023-9 523 instances All -\nC-Eval Shanghai Jiao Tong University 2023-5 13948 instances All CC-BY-NC-SA-4.0\nCG-Eval LanguageX AI Lab et al. 2023-8 11000 instances All CC-BY-SA-4.0\nChain-of-Thought Hub University of Edinburgh et al. 2023-5 - All MIT\nChoice-75 University of Pittsburgh et al. 2023-9 650 instances All -\nCLEVA The Chinese University of Hong Kong et al. 2023-8 370K instances All CC-BY- NC-ND-4.0\nCLiB jeinlee1991 2023-6 90 instances All -\nCLUE CLUE team 2020-12 9 datasets All -\nCMB The Chinese University of Hong Kong et al. 2023-8 281047 instances All Apache- 2.0\nCMMLU MBZUAI 2023-6 11528 instances All CC-BY-NC-4.0\nCodeXGLUE Peking University et al. 2021-2 4.12M instances All C-UDA\nCrowS-Pairs New York University 2020-11 1508 instances All CC-SA-4.0\nCUGE Tsinghua University et al. 2021-12 33.4M instances All -\ndecaNLP Salesforce Research 2018-6 2010693 instances All BSD-3-Clause\nDS-1000 The University of Hong Kong et al. 2022-11 1000 instances All CC-BY-SA-4.0\nEcomGPT\neval Alibaba 2023-8 6000 instances All -\nEmotionBench The Chinese University of Hong Kong et al. 2023-8 - All GPL-3.0\nFACTOR AI21 Labs 2023-7 4030 instances Partial MIT\nFActScore University of Washington et al. 2023-5 500 instances All MIT\nFactualityPrompt Hong Kong University of Science and Technology et al. 2022-6 16000 instances All Apache-2.0\nFairEval Peking University et al. 2023-5 80 instances All -\nFewCLUE CLUE team 2021-7 9 datasets Partial -\nFinancelQ Du Xiaoman 2023-9 7173 instances All CC-BY-NC-SA-4.0\nFinBen The Fin AI et al. 2024-2 69805 instances All -\nFinEval Shanghai University of Finance and Economics 2023-8 4661 instances All CC -BY-NC-SA-4.0\nFlagEval BAAI et al. 2023-6 84433 instances Partial -\nFLUE Georgia Institute of Technology et al. 2022-10 26292 instances All -\nFreshQA Google et al. 2023-10 600 instances All -\nGAOKAO-Bench Fudan University et al. 2023-5 2811 instances All Apache-2.0\nGeoBench Shanghai Jiao Tong University et al. 2023-6 2517 instances All Apache-2.0\nGLUE New York University et al. 2018-11 9 datasets All -\nGLUE-X Westlake University et al. 2022-11 6404940 instances All -\ndataset, finely categorizing 112 datasets into 20 evaluation domains, namely: Gen-\neral, Exam, Subject, Natural Language Understanding (NLU) , Reasoning,\nKnowledge, Long Text , Tool, Agent, Code, OOD, Law, Medical, Financial,\nSocial Norms , Factuality, Evaluation, Multitask, Multilingual, and Other.\nThe “Other” category includes seven sub-domains: E-commerce, Few- shot learning,\nGeoscience, IT, Multi-turn interactions, Robustness, and Sentiment. Each sub-domain\nhas one representative dataset.\n5.1.1 General\nGeneral evaluation datasets are typically of smaller scale and primarily focus on assess-\ning how well LLMs perform in two aspects. The first aspect involves evaluating\ntheir performance on general instructions across multiple categories an d\n49\nTable 11 (continued)\nDataset Publisher Release Time Size Public or Not License\nHalluQA Fudan University et al. 2023-10 450 instances All -\nHaluEval Renmin University of China et al. 2023-5 35000 instances All MIT\nHELM Stanford University et al. 2022-11 - All Apache-2.0\nHuaTuo26M-test The Chinese University of Hong Kong et al. 2023-5 6000 instances All Apache-2.0\nHumanEval OpenAI et al. 2021-7 164 instances All MIT\nHumanEvalPack Bigcode 2023-8 984 instances All MIT\nInfiniteBench Tsinghua University et al. 2023-11 3932 instances All Apache-2. 0\nKoLA Tsinghua University 2023-6 2138 instances Partial GPL-3.0\nLAiW Sichuan University et al. 2023-10 - Partial -\nLawBench Nanjing University et al. 2023-9 - All Apache-2.0\nLegalBench Stanford University et al. 2023-8 90417 instances All -\nL-Eval Fudan University et al. 2023-7 2043 instances All GPL-3.0\nLexGLUE University of Copenhagen et al. 2021-10 237014 instances All -\nLEXTREME University of Bern et al. 2023-1 3508603 instances All -\nLILA Arizona State Univeristy et al. 2022-10 317262 instances All CC-BY-4.0\nLLMEVAL-1 Fudan University et al. 2023-5 453 instances All -\nLLMEVAL-2 Fudan University et al. 2023-7 480 instances All -\nLLMEVAL-3 Fudan University et al. 2023-9 200K instances Not -\nLLMEval2 Chinese Academy of Sciences et al. 2023-8 2533 instances All MIT\nLMentry Tel Aviv University et al. 2023-7 110703 instances All -\nLMExamQA Tsinghua University et al. 2023-6 10090 instances All -\nLongBench Tsinghua University et al. 2023-8 4750 instances All MIT\nLongEval LMSYS 2023-6 - All Apache-2.0\nLooGLE BIGAI et al. 2023-11 6448 instances All CC-BY-SA-4.0\nMCTS Beijing Language and Culture University 2023-6 723 instances All -\nminiF2F\nv1 Ecole Polytechnique et al. 2021-9 488 instances All -\nMINT University of Illinois Urbana-Champaign et al. 2023-9 586 instances All Apache -2.0\nMMCU LanguageX AI Lab 2023-4 11845 instances All -\nMMLU UC Berkeley et al. 2020-9 15908 instances All MIT\nMT-Bench UC Berkeley et al. 2023-6 80 instances All Apache-2.0\nMTPB Salesforce Research 2022-3 115 instances All Apache-2.0\nMultiMedQA Google Research et al. 2022-12 212822 instances All -\nM3Exam Alibaba Group et al. 2023-6 12317 instances All -\nM3KE Tianjin University et al. 2023-5 20477 instances All Apache-2.0\nNeuLR Xi’an Jiaotong University et al. 2023-6 3000 instances All -\nODEX Carnegie Mellon University et al. 2022-12 945 instances All CC-BY-SA-4.0\nOwl-Bench Beihang University et al. 2023-9 1317 instances All -\nPandaLM\ntestset Peking University et al. 2023-4 999 instances All Apache-2.0\nPromptBench Microsoft Research et al. 2023-6 583884 instances All MIT\nPromptCBLUE East China Normal University et al. 2023-4 20640 instances All -\nQiZhenGPT\neval Zhejiang University et al. 2023-5 94 instances All GPL-3.0\nRAFT Ought et al. 2021-9 28712 instances All -\nSafetyBench Tsinghua University et al. 2023-9 11435 instances All MIT\nSafety-Prompts Tsinghua University et al. 2023-4 100K instances Partial Apach e-2.0\nSCALE University of Bern et al. 2023-6 1.86M instances All CC-BY-SA\nSCIBENCH University of California et al. 2023-7 695 instances All MIT\nSentEval Facebook Artificial Intelligence Research 2018-5 28 datasets All BS D\nScienceQA University of California et al. 2022-9 21208 instances All CC-BY-NC- SA-4.0\nSocKET University of Michigan et al. 2023-5 2616342 instances All CC-BY-4.0\nSuperCLUE CLUE et al. 2023-7 3754 instances Not -\nSuperCLUE-Agent CLUEbenchmark 2023-10 - Not -\nSuperCLUE-Safety CLUEbenchmark 2023-9 4912 instances Not -\nSuperGLUE New York University et al. 2019-5 8 datasets All -\nTabMWP University of California et al. 2022-9 38431 instances All CC-BY-NC-SA-4.0\nTheoremQA University of Waterloo et al. 2023-5 800 instances All MIT\nToolBench SambaNova Systems et al. 2023-5 795 instances All Apache-2.0\nTRUSTGPT Sichuan University et al. 2023-6 2000 instances All MIT\nTruthfulQA University of Oxford et al. 2022-5 817 instances All Apache-2.0\nVicuna Evaluation LMSYS ORG 2023-3 80 instances All Apache-2.0\nXiezhiBenchmark Fudan University et al. 2023-6 249587 instances All CC-BY-NC-S A-4.0\nXNLI Facebook AI et al. 2018-10 112500 instances All CC-BY-NC-4.0\nXTREME Carnegie Mellon University et al. 2020-3 - All Apache-2.0\nZeroSCROLLS Tel Aviv University et al. 2023-5 4378 instances All MIT\ndomains, mainly examining their versatility . Vicuna Evaluation45, for instance,\nassesses models in nine question categories, using GPT-4 to judge t he quality of out-\nputs and providing a preliminary evaluation of overall model quality . Building upon\nthis, AlpacaEval (Dubois et al, 2023) includes instructions covering samples from var-\nious datasets, offering a broader evaluation of performance on different open -ended\n45https://github.com/lm-sys/vicuna-blog-eval\n50\nTable 12 Summary of Evaluation Datasets Information Part II . Language: “EN”\nindicates English, “ZH” indicates Chinese, “PL” indicates Programming Language,\n“Multi” indicates Multilingual, and the number in parentheses ind icates the number\nof languages included. “CM” indicates Construction Methods, where “HG” indicates\nHuman Generated Datasets, “MC” indicates Model Constructed Datasets, and “CI”\nindicates Collection and Improvement of Existing Datasets. “QT” ind icates Question\nTypes, where “SQ” indicates Subjective Questions, “OQ” indicate s Objective Ques-\ntions, and “Multi” indicates Multiple Question Types. “EM” indicat es Evaluation\nMethods, where “CE” indicates Code Evaluation, “HE” indicates Human Evaluation,\nand “ME” indicates Model Evaluation\nDataset Language CM QT EM Domain Focus\nAgentBench EN HG & CI & MC SQ CE Agent LLM-as-Agent’s reasoning and decision-m aking abilities\nAGIEval EN & ZH HG & CI OQ CE Exam Human-centric standardized exams\nALCUNA EN HG Multi CE Knowledge Assess the ability of LLMs to respond to new k nowledge\nAlpacaEval EN CI & MC SQ ME General The performance on open-ended questi on answering\nAPI-Bank EN & PL HG & MC SQ HE & CE Tool Plan step-by-step API calls, retrie ve relevant APIs, and correctly execute API calls to meet human needs\nAPIBench EN & PL HG & MC SQ CE Tool The reasoning ability for calling APIs\nAPPS EN & PL HG SQ CE Code The ability to take an arbitrary natural language spe cification and generate satisfactory Python code\nARB EN CI Multi HE & ME Subject Advanced reasoning problems in multiple fields\nBayLing-80 EN & ZH HG & CI SQ ME General Chinese-English language proficienc y and multimodal interaction skills\nBBF-CFLEB ZH HG & CI SQ CE Financial Language understanding and generation task s in Chinese financial natural language processing\nBBH EN CI Multi CE Multitask Challenging tasks that have proven diffi cult for prior language model evaluations\nBELLE\neval ZH HG & MC SQ ME General The performance of Chinese language models in f ollowing instructions\nBIG-Bench Multi HG & CI Multi CE Multitask The capabilities and limi tations of language models\nBIRD EN & PL HG & CI & MC SQ CE Code Text-to-SQL parsing\nBOSS EN CI SQ CE OOD Assess model performance under distribution s hifts\nCBLUE ZH HG & CI SQ CE Medical Chinese biomedical language understanding\nC-CLUE ZH HG SQ CE Subject Classical Chinese language understanding\nCELLO EN HG SQ CE General The ability of LLMs to understand complex instru ctions\nC-Eval ZH HG & MC OQ CE Subject The advanced knowledge and reasoning abili ties in a Chinese context\nCG-Eval ZH HG SQ CE Subject The generation capabilities of LLMs across various academic disciplines\nChain-of-Thought Hub EN CI SQ CE Reasoning The multi-step reasoning capab ilities\nChoice-75 EN HG & CI & MC OQ CE Reasoning Predict decisions based on descr iptive scenarios\nCLEVA ZH HG & CI SQ CE Multitask The performance of LLMs across various dimens ions\nCLiB ZH - SQ HE Multitask Multidimensional capabilities\nCLUE ZH CI SQ CE NLU Natural language understanding capability\nCMB ZH HG Multi HE & CE & ME Medical The performance of LLMs in the field of med icine\nCMMLU ZH HG OQ CE Subject The knowledge and reasoning capabilities with in the Chinese context\nCodeXGLUE EN & PL CI SQ CE Code Program understanding and generation tasks\nCrowS-Pairs EN HG & CI SQ CE Social Norms The presence of cultural biase s and stereotypes in pretrained language models\nCUGE EN & ZH CI SQ CE NLU Natural language understanding capability\ndecaNLP EN CI SQ CE Multitask Multitask natural language processing capabi lities\nDS-1000 EN & PL HG SQ CE Code Code generation\nEcomGPT\neval EN & ZH CI SQ CE E-commerce E-commerce-related tasks\nEmotionBench EN HG & MC SQ CE Sentiment The empathy ability\nFACTOR EN HG & CI & MC OQ CE Factuality The factuality of LLMs\nFActScore EN HG & MC SQ HE & ME Factuality The factuality of LLMs\nFactualityPrompt EN CI SQ CE Factuality The factuality of LLMs\nFairEval EN CI SQ CE Evaluation The performance in determining the qu ality of output content from different models\nFewCLUE ZH CI SQ CE Few-shot learning Compare different few-shot learni ng methods\nFinancelQ ZH HG & MC OQ CE Financial The knowledge and reasoning abilitie s in financial contexts\nFinBen EN CI SQ CE Financial NLP tasks in the financial domain\nFinEval ZH HG OQ CE Financial The performance in the financial domain knowl edge\nFlagEval EN & ZH HG & CI Multi HE & CE Multitask Multi-domain, multi-dime nsional capabilities\nFLUE EN CI SQ CE Financial NLP tasks in the financial domain\nFreshQA EN HG SQ HE Factuality The factuality of LLMs\nGAOKAO-Bench ZH HG Multi HE & CE Exam Chinese Gaokao examination\nGeoBench EN HG Multi HE & CE & ME Geoscience LLMs’ performance in unders tanding and utilizing geoscience knowledge\nGLUE EN CI SQ CE NLU Natural language understanding capability\nGLUE-X EN CI SQ CE OOD The out-of-distribution (OOD) robustness\nHalluQA ZH HG & MC SQ ME Factuality The factuality of LLMs\nHaluEval EN HG & CI & MC SQ CE Factuality The factuality of LLMs\nHELM EN CI SQ HE & CE Multitask Evaluate LLMs on a wide range of scenarios and metr ics\nquestions. BayLing-80 (Zhang et al, 2023h) further expands Vicuna Evaluation, evalu-\nating the general capabilities and conversational abilities of LLMs in both Chinese and\nEnglish. BELLE eval (Ji et al, 2023b) and MT-Bench (Zheng et al, 2023b) adopt sim-\nilar evaluation methods. The former aims to assess the models’ general performance\nin Chinese scenarios across nine instruction types, while the latt er focuses on eval-\nuating their general performance in English scenarios across eight inst ruction types.\nThe number of instructions in these datasets is all within 1K, with s ome limitations\nin comprehensiveness. SuperCLUE ( Xu et al , 2023e) expands the scale of evaluation\ncontent. It serves as a comprehensive evaluation benchmark for Chinese general LLMs,\ndesigned to assess the effectiveness of current Chinese LLMs. The tasks include multi-\nturn open-ended Q&A and objective multiple-choice Q&A, with monthly updates and\nsignificant reference value.\n51\nTable 12 (continued)\nDataset Language CM QT EM Domain Focus\nHuaTuo26M-test ZH CI SQ CE Medical Understand and generate complex medical l anguage\nHumanEval EN & PL HG SQ CE Code The correctness of problem-solving abili ties in the context of program synthesis\nHumanEvalPack EN & PL HG & CI SQ CE Code The correctness of problem-solvi ng abilities in the context of program synthesis\nInfiniteBench EN & ZH HG & CI & MC Multi - Long Text Long text task capabilit y\nKoLA EN HG & CI SQ CE Knowledge The ability to grasp and utilize world know ledge\nLAiW ZH CI SQ CE Law Legal capabilities\nLawBench ZH HG & CI Multi CE Law Legal capabilities\nLegalBench EN HG & CI SQ HE & CE Law Legal reasoning\nL-Eval EN HG & CI SQ HE & CE & ME Long Text Long text task capability\nLexGLUE EN CI SQ CE Law Legal capabilities\nLEXTREME Multi (24) CI SQ CE Law Legal capabilities\nLILA EN CI Multi CE Reasoning Mathematical reasoning across diverse tasks\nLLMEVAL-1 ZH HG SQ HE & ME Multitask Multidimensional capabilities\nLLMEVAL-2 ZH HG Multi HE & ME Knowledge Knowledge capability\nLLMEVAL-3 ZH HG SQ ME Subject Subject-specific knowledge capability\nLLMEval2 Multi CI SQ CE Evaluation The performance in determining the qualit y of output content from different models\nLMentry EN HG SQ CE Multitask The performance on challenging tasks\nLMExamQA EN MC SQ ME Knowledge The performance on open-ended question answering\nLongBench EN & ZH CI SQ CE Long Text Long text task capability\nLongEval EN HG SQ CE Long Text Long text task capability\nLooGLE EN HG & CI & MC SQ HE & CE & ME Long Text Long text task capability\nMCTS ZH HG SQ CE NLU Text simplification ability\nminiF2F\nv1 EN HG & CI SQ CE Reasoning The performance on formal Olympiad-level math ematics problem statements\nMINT EN CI SQ CE Multi-turn interactions Solve complex tasks through mu lti-turn interactions using tools and leveraging natural language feed back\nMMCU ZH HG OQ CE Subject Multidisciplinary abilities\nMMLU EN HG OQ CE Subject Knowledge in academic and professional domains\nMT-Bench EN HG SQ ME General The performance on open-ended question an swering\nMTPB EN & PL HG SQ CE Code Multi-turn Programming\nMultiMedQA EN HG & CI Multi HE & CE Medical The performance in medical and clinical applications\nM3Exam Multi (9) HG OQ CE Exam The comprehensive abilities in a multil ingual and multilevel context using real human exam questions\nM3KE ZH HG OQ CE Subject Multidisciplinary abilities\nNeuLR EN CI SQ CE Reasoning Logical reasoning capabilities\nODEX Multi & PL HG & CI SQ CE Code Natural language to Python code generation\nOwl-Bench EN & ZH HG Multi ME IT The performance in IT-related tasks\nPandaLM\ntestset EN HG & MC SQ CE Evaluation The performance in determining th e quality of output content from different models\nPromptBench EN CI SQ CE Robustness The models’ robustness\nPromptCBLUE ZH CI SQ CE Medical The performance in Chinese medical sce narios\nQiZhenGPT\neval ZH HG SQ HE Medical Indications for use of drugs\nRAFT EN HG & CI SQ CE NLU Text classification ability\nSafetyBench EN & ZH HG & CI & MC OQ CE Social Norms The safety of LLMs\nSafety-Prompts ZH MC SQ HE & ME Social Norms The safety of LLMs\nSCALE Multi (5) HG & CI SQ CE Law Legal multidimensional abilities\nSCIBENCH EN HG SQ CE Subject The performance in university-level s cience and engineering domains\nSentEval EN CI SQ CE NLU The quality of universal sentence represent ations\nScienceQA EN HG OQ CE Subject Science question-answering abilit y\nSocKET EN CI SQ CE Knowledge Mastery of social knowledge\nSuperCLUE ZH HG & MC Multi HE & CE General The performance in a Chinese con text\nSuperCLUE-Agent ZH - SQ - Agent Agent capabilities of LLMs\nSuperCLUE-Safety ZH - SQ ME Social Norms The safety of LLMs\nSuperGLUE EN CI SQ CE NLU Natural language understanding capability\nTabMWP EN HG Multi CE Reasoning Mathematical reasoning ability involving both textual and tabular information\nTheoremQA EN HG SQ CE Subject Science subject question-answerin g ability\nToolBench EN HG & CI SQ CE Tool The enhancement in tool manipulation for r eal-world software tasks\nTRUSTGPT EN CI SQ CE Social Norms The performance in toxicity, bias, an d value alignment\nTruthfulQA EN HG SQ CE & ME Factuality The factuality of LLMs\nVicuna Evaluation EN HG SQ ME General The performance on open-ended ques tion answering\nXiezhiBenchmark EN & ZH HG & MC OQ CE Subject Multidisciplinary abil ities\nXNLI Multi (15) HG SQ CE Multilingual Multilingual NLI\nXTREME Multi (40) CI SQ CE Multilingual The cross-lingual generalization c apabilities\nZeroSCROLLS EN HG & CI Multi CE Long Text Long text task capability\nThe second aspect involves assessing the ability of LLMs to follow instruc-\ntions, especially when faced with complex directives . Datasets like Vicuna\nEvaluation, AlpacaEval, and BayLing-80 incorporate various types of instruct ions,\nevaluating both generalization and the models’ capacities to comprehen d the require-\nments of instructions. CELLO ( He et al , 2023b) enhances the complexity of instruc-\ntions by systematically evaluating the models’ capabilities to follow complex directives\nfrom two perspectives: complex task description and complex input .\n5.1.2 Exam\nEvaluation datasets within the examination domain are crafted with the sp ecific pur-\npose of formulating instructions derived from significant exam questions across diverse\nnations. In this scenario, LLMs take on the role of candidates, responding t o queries\nin accordance with specified guidelines. The primary objective is t o assess the profi-\nciency of LLMs in comprehending the nuances of question intent and their reservoir of\nknowledge pertaining to examinations. GAOKAO-Bench (\nZhang et al, 2023k) employs\nGaokao (China’s National College Entrance Examination) questions as the basis for\nevaluation, seeking to appraise the proficiency of LLMs across various subjects, encom-\npassing a spectrum of 10 disciplines. AGIEval (Zhong et al, 2023) expands the ambit of\n52\nTable 13 Summary of Evaluation Datasets Information Part III . “NC” indi-\ncates Numbers of Evaluation Categories. “NS” indicates Numbers of Evaluation\nSubcategories. Zoom in for better view\nDataset NC NS Category\nAgentBench 8 - Operating system, Database, Knowledge graph, Digital card game, Lateral thinking puzzles, House-holding, Web shopping, Web brows ing\nAGIEval 7 20 Gaokao, SAT, JEC, LSAT, LogiQA, AQuA-RAT, Math\nALCUNE 3 - Knowledge understanding, Knowledge differentiation, Knowle dge association\nAlpacaEval 1 - Open-ended question answering\nAPI-Bank 3 - Call, Retrieval+Call, Plan+Retrieval+Call\nAPIBench 1 - API call\nAPPS 1 - Code generation\nARB 5 - Mathematics, Physics, Law, MCAT(Reading), MCAT(Science)\nBayLing-80 9 - Writing, Roleplay, Common-sense, Fermi, Counterfactual , Coding, Math, Generic, Knowledge\nBBF-CFLEB 6 - FinNL, FinNA, FinRE, FinFE, FinQA, FinNSP\nBBH 23 27 Boolean expressions, Causal judgement, Date understanding, Di sambiguation QA, etc.\nBELLE\neval 9 - Extract, Closed qa, Rewrite, Summarization, Generation, Classi fication, Brainstorming, Open qa, Others\nBIG-Bench 95 204 Linguistics, Child development, Mathematics, Common s ense reasoning, Biology, etc.\nBIRD 1 - Text-SQL\nBOSS 5 20 Sentiment analysis, Toxicity detection, Natural language infe rence, Named entity recognition, Extractive Question answering\nCBLUE 5 8 Information extraction from the medical text, normalization of the m edical term, medical text classification, medical sentence similar ity estimation, medical QA\nC-CLUE 2 - Named entity recognition, Relation extraction\nCELLO 2 10 Complex task description, Complex input\nC-Eval 4 52 STEM, Social Science, Humanity, Other\nCG-Eval 6 55 Science and engineering, Humanities and social sciences, Mathematical calculations, Medical practitioner qualification Examinati on, Judicial Examination, Certfied public accountant examination\nChain-of-Thought Hub 6 8 Math, Science, Symbolic, Knowledge, Coding, Factual\nChoice-75 4 - Easy, Medium, Hard, N/A\nCLEVA 2 31 Ability, Application\nCLiB 4 - Classification, Information extraction, Reading comprehension, Tab ular question answering\nCLUE 3 9 Single-sentence tasks, Sentence pair tasks, Machine reading comprehension tasks\nCMB 2 7 CMB-Exam, CMB-Clin\nCMMLU 5 67 Social science, STEM, Humanities, China specific, Other\nCodeXGLUE 4 10 Code-Code, Text-Code, Code-Text, Text-to-Text\nCrowS-Pairs 9 - Race, Gender, Sexual orientation, Religion, Age, Nationality , Disability, Physical appearance, Occupation\nCUGE 7 18 Language understanding (word-sentence or discourse level), Inf ormation acquisition and question answering, Language generation, Conversation al interaction, Multilingual, Mathematical reasoning\ndecaNLP 10 - Question answering, Machine translaion, Summarization, Natural l anguage inference, Sentiment analysis, Semantic role labeling, Zero-sh ot relation extraction, Goal-oriented dialogue, Semantic parsing, Pronoun r esolution\nDS-1000 1 - Code generation\nEcomGPT\neval 4 12 Classification, Generation, Extraction, Others\nEmotionBench 8 36 Anger, Anxiety, Depression, Frustration, Jealous, Gu ilt, Fear, Embarrassment\nFACTOR 2 - Wiki, News\nFActScore 7 - Single-sentence contradiction (words or beyond words), P age-level contradiction, Subjective, Fact is irrelevant, Wiki is inconsistent & wrong, Annotation error\nFactualityPrompt 2 - Factual prompts, Nonfactual prompts\nFairEval 1 - Evaluate the quality of answers\nFewCLUE 3 9 Single sentence tasks, Sentence pair tasks, Reading compre hension\nFinancelQ 10 36 Bank, Fund, Securities, Futures and derivatives, C ICE, Actuarial science, Financial planning, CPA, Taxation, Economics\nFinBen 3 6 Foundamental tasks, Advanced cognitive engagement, General in telligence\nFinEval 4 34 Finance, Economy, Accounting, Certificate\nFlagEval 3 21 Choice qa, Classification, Generation qa\nFLUE 5 6 Financial sentiment analysis, News headline classification, Named entity recognition, Structure boundary detection, Question answer ing\nFreshQA 4 - Never-changing, Slow-changing, Fast-changing, False-premi se\nGAOKAO-Bench 10 - Chinese, Mathematics (2 categories), English, Ph ysics, Chemistry, Biology, Politics, History, Geography\nGeoBench 2 - NPEE, APTest\nGLUE 3 9 Single-sentence tasks, Similarity and paraphrase tasks, Infere nce tasks\nGLUE-X 7 16 Sentiment analysis, Linguistic acceptability, Textual sim ilarity, Natural language inference, Question answering NLI, Textual entai lment, Paraphrase\nHalluQA 3 - Misleading, Misleading-hard, Knowledge\nHaluEval 3 - QA, Dialogue, Summarization\nHELM 73 - Question answering, Information retrieval, Sentiment analysis, Toxicity detection, Aspirational scenarios, etc.\nHuaTuo26M-test 3 - Medical consultant records, Encyclopedias, Knowled ge bases\nHumanEval 1 - Code generation\nHumanEvalPack 3 - HumanEvalFix, HumanEvalExplain, HumanEvalSynthesize\nInfiniteBench 5 12 Mathematics, Code, Dialogue, Books, Retrieval\nKoLA 4 19 Knowledge memorization, Knowledge understanding, Knowledge ap plying, Knowledge creating\nLAiW 3 13 Basic legal NLP, Basic legal application, Complex legal application\nLawBench 3 20 Legal knowledge memorization, Legal knowledge understanding, Legal knowledge applying\nLegalBench 6 162 Issue-spotting, Rule-recall, Rule-application, Rule-conc lusion, Interpretation, Rhetorical-understanding\nL-Eval 1 18 Long text task\nLexGLUE 3 - Multi-label classification, Multi-class classification, Mul tiple choice QA\nLEXTREME 18 - Brazilian court decisions, German argument mining, Greek le gal code, Swiss judgment prediction, etc.\nLILA 4 23 Math ability, Language, Knowledge, Format\nLLMEVAL-1 17 - Fact-based question answering, Reading comprehension, Frame work generation, Paragraph rewriting, etc.\nLLMEVAL-2 12 - Computer science, Economics, Foreign languages, Law, Mathemati cs, Medicine, Optics, Physics, Social sciences, Chinese langu age and literature, Chemistry, Life sciences\nLLMEVAL-3 13 - Philosophy, Economics, Law, Education, Literature, History, S cience, Engineering, Agriculture, Medicine, Military science, Management, Fine arts\nLLMEval2 1 - Evaluate the quality of answers\nLMentry 25 - Sentence containing word, Sentence not containing word, W ord containing letter, Word not containing letter, etc.\nLMExamQA 3 25 Knowledge memorization, Knowledge comprehension, Knowl edge analysis\nLongBench 6 21 Single-doc QA, Multi-doc QA, Summarization, Few-shot lear ning, Synthetic tasks, Code completion\nLongEval 2 - Coarse-grained topic retrieval, Fine-grained line retrieval\nLooGLE 2 4 Long dependency tasks, Short dependency tasks\nMCTS 1 - Text simplification\nminiF2F\nv1 1 - Math\nMINT 3 - Code generation, Decision making, Reasoning\nMMCU 4 25 Medicine, Law, Psychology, Education\nMMLU 4 57 Humanities, Social science, STEM, Other\nMT-Bench 8 - Writing, Roleplay, Reasoning, Math, Coding, Extraction, ST EM, Humanities\nMTPB 1 - Code generation\nMultiMedQA 1 - Medical question answering\nM3Exam 3 - Low, Mid, High\nM3KE 4 71 Arts & Humanities, Social sciences, Natural sciences, Other\nNeuLR 3 - Deductive, Inductive, Abductive\nODEX 1 - Code generation\nOwl-Bench 9 - Information security, Application, System architectu re, Software architecture, Middleware, Network, Operating system , Infrastructure, Database\nPandaLM\ntestset 1 - Evaluate the quality of answers\nPromptBench 10 15 Sentiment analysis, Grammar correctness, Duplicat e sentence detection, Natural language inference, etc.\nPromptCBLUE 16 - Medical named entity recognition, Medical entity relat ion extraction, Medical event extraction, etc.\nQiZhenGPT eval 1 - Drug indication question answering\nRAFT 1 11 Text classification\nSafetyBench 7 - Offensiveness, Unfairness and bias, Physical health , Mental Health, Illegal activities, Ethics and morality, Privacy and Prop erty\nSafety-Prompts 2 13 Typical security scenarios, Instruction attack\nSCALE 4 - Processing long documents, Utilizing domain specific know ledge, Multilingual understanding, Multitasking\nSCIBENCH 3 10 Physics, Chemistry, Math\nSentEval 1 21 Universal sentence representations\nScienceQA 3 26 Natural science, Social science, Language science\nSocKET 4 58 Classification, Regression, Pair-wise comparison, Span ident ification\nSuperCLUE 2 - Open multi-turn open questions, OPT objective quest ions\nSuperCLUE-Agent 3 10 Tool utilization, Task planning, Long-term and short-te rm memory\nSuperCLUE-Safety 3 20+ Traditional security category, Responsible artific ial intelligence, Instruction attacks\nSuperGLUE 4 8 Word sense disambiguation, Natural language inference, Corefe rence resolution, Question answering\nTabMWP 1 - Mathematical reasoning and table QA\nTheoremQA 4 39 Mathematics, Physics, Finance, CS & EE\nToolBench 8 - Open weather, The cat API, Home search, Trip booking, Google s heets, Virtual home, Web shop, Tabletop\nTRUSTGPT 3 - Toxicity, Bias, Value-alignment\nTruthfulQA 38 - Health, Law, Conspiracies, Fiction, Misconceptions, Paranormal, Economics, Biology, Language, Indexical etc.\nVicuna Evaluation 9 - Generic, Knowledge, Roleplay, Common-sense, Fe rmi, Counterfactual, Coding, Math, Writing\nXiezhiBenchmark 13 516 Medicine, Literature, Economics, Agronomy, Scien ce, Jurisprudence, History, Art studies, Philosophy, Pedagogy, Military science, Management, Engineering\nXNLI 1 - Multilingual natural language inference\nXTREME 4 9 Classification, Structured prediction, QA, Retrieval\nZeroSCROLLS 3 10 Summarization, Question Answering, Aggregation\ninquiries by devising benchmarks centered on human-centric tests, featuring a selection\nof 20 official, public, and stringent entrance and qualification examinations , including\nGaokao, the U.S. SAT, the bar exam, and the national civil service exam. M3Ex am\n(Zhang et al, 2023i) assembles an array of multi-modal, multi-lingual, and multi-tiered\n53\nsets of multiple-choice questions, sourcing exam questions from pr imary, secondary,\nand high school exams in nine countries distinguished by diverse l anguages.\n5.1.3 Subject\nEvaluation datasets in academic domains thoroughly gauge the mastery of LLMs\nin diverse fields, including disciplines like mathematics, la w, psychology, and more.\nC-CLUE\n46 stands as a benchmark for assessing classical Chinese language compre-\nhension. It centers on tasks like NER and RE, all grounded in a historical knowledge\ngraph. This dataset primarily scrutinizes proficiency within indi vidual disciplines, yet\nit exhibits limited diversity. MMCU ( Zeng, 2023) broadens the horizons by incor-\nporating disciplines such as medicine, law, psychology, and educati on to measure\nChinese semantic comprehension. In the realm of university-level science and engineer-\ning, SCIBENCH ( Wang et al , 2023d) is tailor-made to evaluate LLMs’ capabilities,\ndemanding the resolution of challenging subjective questions relat ed to mathemat-\nics, physics, and chemistry. TheoremQA ( Chen et al, 2023b) narrows its focus to 350\ntheorems from mathematics, physics, finance, and CS & EE (Computer Sc ience &\nElectrical Engineering). Lastly, ARB (Sawada et al, 2023) introduces a more demand-\ning examination, appraising LLMs’ prowess in text comprehension and domain-specific\nreasoning. The questions delve into profound knowledge across discip lines such as\nmathematics, physics, biology, chemistry, and law.\nThe aforementioned datasets focus on evaluating specific discipline s on a smaller\nscale. In contrast, some datasets aim to comprehensively assess disci plinary capa-\nbilities, encompassing a wide range of subjects. ScienceQA ( Lu et al , 2022) gathers\nmultiple-choice questions from 26 subcourses in natural sciences, social sciences, and\nlinguistics. C-Eval (Huang et al, 2023c) compiles 52 diverse subject questions, catego-\nrized into four difficulty levels, providing a holistic evaluationof models’ comprehensive\nsubject proficiency in Chinese. Similarly, CG-Eval ( Zeng et al, 2023b) requires LLMs\nto accurately answer 55 sub-subject questions across six major categori es for auto-\nmatic scoring. LLMEVAL-3 47 concentrates on evaluating proficiency in specialized\nknowledge, featuring generated questions from 13 academic categories out lined by\nChina’s Ministry of Education and over 50 subcategories. It introduce s a “question\nbank exam” mode. MMLU ( Hendrycks et al , 2021b) assesses subjects ranging from\ntraditional fields like mathematics and history to professional areas suc h as law and\nethics, covering 57 subjects with difficulty levels from elemen tary to professional. As\nthe content of MMLU is in English, CMMLU ( Li et al , 2023d) is created as its Chi-\nnese counterpart for evaluating subject knowledge proficiency in a Chinese context,\ncovering 67 subjects. M3KE ( Liu et al , 2023a), originating from the Chinese educa-\ntion system, collects multiple-choice questions from 71 subjectsspanning from primary\nschool to university. XiezhiBenchmark ( Gu et al , 2023), covering a record 516 differ-\nent subjects, attains a scale of approximately 250K questions. Overall, these subject\nevaluation datasets share a high degree of similarity in data sources, primarily sourced\nfrom online materials related to their respective subjects. Additionally, multiple-choice\nquestion formats, conducive to automated evaluation, are particularly fa vored.\n46https://github.com/jizijing/C-CLUE\n47https://github.com/llmeval/llmeval-3\n54\n5.1.4 Natural Language Understanding\nThis class of evaluation datasets aims to comprehensively evaluate the m ultifaceted\nabilities of LLMs in natural language understanding (NLU) tasks, covering fu nda-\nmental comprehension of grammatical structures to advanced semantic reasoning and\ncontext handling. MCTS (\nChong et al , 2023) and RAFT ( Alex et al , 2021) serve as\nbenchmarks for individual NLU tasks. The former stands as the most exten sive evalu-\nation dataset for Chinese text simplification, while the latter functions as a benchmark\nfor text classification. Multiple NLU tasks are encompassed by most dataset s. GLUE\n(Wang et al , 2018) incorporates nine English NLU tasks, assessing LLMs in tasks\nsuch as sentiment analysis, semantic matching, and textual entailment. Building upon\nGLUE, SuperGLUE (Wang et al, 2019) raises task difficulty, reflecting LLMs’ perfor-\nmance in a broader scope of language understanding. To evaluate the NLU capabilities\nof models in the Chinese context, CLUE (Xu et al, 2020b) is constructed with reference\nto GLUE. Comprising nine Chinese NLU tasks, the CLUE dataset evaluates LLMs i n\ntasks like semantic matching, text classification, and reading compreh ension. CUGE\n(Yao et al, 2021) is organized hierarchically by language-task-dataset structure, usin g\n21 sub-datasets to evaluate LLMs in language understanding, information retr ieval,\nQ&A, and language generation. SentEval (Conneau and Kiela, 2018) aggregates NLU\ndatasets for 21 sub-tasks.\n5.1.5 Reasoning\nReasoning evaluation datasets are designed to gauge the proficiency of LLMs in t asks\nrelated to logical reasoning and inference. Chain-of-Thought Hub (\nFu et al , 2023)\nselects eight open-source datasets and evaluates LLMs’ multi-step reas oning perfor-\nmance by utilizing few-shot CoT prompting across domains like mathematics, science,\nand symbols. Choice-75 ( Hou et al , 2023) tasks LLMs with selecting an appropriate\ndecision solution in various given scenarios, assessing their competence in decision rea-\nsoning. NeuLR (Xu et al, 2023c) assesses deductive reasoning, inductive reasoning, and\nabductive reasoning, emphasizing LLMs’ capabilities in these distinct reasoning direc-\ntions. TabMWP (Lu et al, 2023b), LILA (Mishra et al, 2022a), and miniF2F v1 (Zheng\net al , 2022) all scrutinize LLMs’ reasoning prowess in mathematics. The TabMWP\ndataset requires LLMs to engage in table-based Q&A and mathematical reasonin g\nbased on provided text and table data. The LILA dataset serves as a comprehe nsive\nmathematical reasoning benchmark, evaluating various mathematical skills, including\nbasic proficiency, algebra, calculus, and more. The miniF2F v1 dataset is a compila-\ntion of Olympiad-level mathematical problems, posing a substantial ch allenge to the\nmathematical acumen of LLMs. In summary, reasoning evaluation datasets encom -\npass diverse assessment directions, categorized into multi-step reasoning, decision\nreasoning, deductive reasoning, mathematical reasoning, and other forms of reasoning.\n5.1.6 Knowledge\nDatasets for evaluating knowledge not only gauge the knowledge retention capabilities\nof LLMs but also assess additional skills such as knowledge analysis, learn ing novel\ninformation, and knowledge induction. LLMEVAL-2 (\nZhang et al, 2023e), derived from\n55\nexternal databases, constructs a repository of knowledge questions acr oss 12 domains.\nCurated by GPT-4, LMExamQA (Bai et al, 2023c) categorizes questions based on the\nrequisite knowledge level, spanning memorization, comprehension, and analysis. KoLA\n(Yu et al, 2023a) predominantly examines LLMs’ proficiency in grasping and applying\nworld knowledge, categorized into memory, comprehension, application , and creation\naccording to the cognitive hierarchy of knowledge. Serving as an assess ment bench-\nmark for LLMs’ command of social knowledge, SocKET ( Choi et al , 2023) classifies\nknowledge into humor and satire, aggressiveness, emotion, credibility, and social facts.\nWhile previous datasets evaluate models from the perspective of exi sting knowledge,\nthe challenge lies in appraising the models’ learning abilities wi th entirely unfamiliar\nnew knowledge. Hence, Yin et al (2023b) employs the knowGen method to generate\nnew knowledge, resulting in the inaugural benchmark dataset, ALCUNA ( Yin et al ,\n2023b), for evaluating and scrutinizing the models’ understanding, differentiation, and\nassociation capabilities regarding new knowledge.\n5.1.7 Long Text\nIn recent times, numerous LLMs, including ChatGLM2\n48 and Gemini 1.5 49, have\nsought to expand the context length of models to the scale of millions of t okens while\nmaintaining performance ( Bai et al , 2023b). This has given rise to the development\nof long text evaluation datasets to better assess the capabilities of LLMs in pro-\ncessing and understanding extensive textual inputs. Notable datase ts in this domain\ninclude ZeroSCROLLS ( Shaham et al , 2023), L-Eval ( An et al , 2023), LongEval ( Li\net al, 2023a), and LooGLE ( Li et al, 2023g), all focusing on the evaluation of lengthy\nEnglish texts. ZeroSCROLLS standardizes datasets from diverse source s into a con-\nsistent input format with an average length of 10K words for assessment acros s 10\nnatural language tasks. L-Eval serves as a comprehensive evaluation suite f or long-\ncontext language models, covering input lengths ranging from 4K to 60K word s. It\nencompasses 18 multi-domain tasks involving inference, Q&A, summari zation, and\nmore on long documents. LongEval introduces two tasks of varying difficul ty, gauging\nLLM performance in fine-grained topic retrieval and line retrieval with input lengths\nbetween 5K and 16K tokens. LooGLE focuses on more challenging tasks with long\ndependencies, evaluating performance on tasks such as multiple inf ormation retrieval\nand timeline reorder with an average length of 20K words. In contrast, LongBen ch\n(\nBai et al, 2023b) comprises a diverse set of 14 English tasks, 5 Chinese tasks, and 2\ncode tasks, with most tasks exhibiting an average length between 5K and 15K tokens.\nDespite claims of some models supporting 100K+ contexts, the previous ly mentioned\ndatasets reveal limitations in evaluating such lengths. To address th is, InfiniteBench\n(Zhang et al , 2023j) increases the average length of evaluations in both Chinese and\nEnglish to 200K tokens, introducing 10 new tasks among the set of 12 evaluation tasks\nto fill the void in assessing long texts exceeding 100K tokens.\n48https://github.com/THUDM/ChatGLM2-6B\n49https://deepmind.google/technologies/gemini/#introduction\n56\n5.1.8 Tool\nThe datasets for evaluating tools gauge the adeptness of LLMs in utilizing t ools and\ninvoking APIs. API-Bank (\nLi et al, 2023i) replicates real-world scenarios, establishing\nan API library with 53 commonly used tools for LLMs to call upon. Tasks involvi ng\nAPI invocation are designed to assess the models’ abilities to effect ively use APIs in\nfulfilling user requirements within a given conversational contex t. APIBench ( Patil\net al, 2023), crafted for evaluation purposes, generates 16,450 instructions derived from\n1,645 API documents. These instructions are formatted to suit LLM-frie ndly chat\ninteractions and are accompanied by evaluation scripts. ToolBench ( Xu et al, 2023f),\nfunctioning as a benchmark for tool operations, encompasses a variety of s oftware\ntools employed in real-world tasks. Tool invocations span single-ste p and multi-step\naction generation, covering eight subtasks, including open weather an d webshop.\n5.1.9 Agent\nThe research and application of LLMs as AI Agents, exemplified by entities l ike\nAutoGPT\n50 and AgentGPT51, are continuously advancing. Agent evaluation datasets\nspecifically concentrate on the capabilities of LLMs functioning as Agents . Agent-\nBench (Liu et al, 2023f) undergoes assessment within English scenarios. It stands out\nas the inaugural benchmark designed to evaluate the performance of LLM-as-Age nt\nacross various environments, encompassing eight distinct settings and providing a thor-\nough examination of LLMs’ competence as independent agents. SuperCLUE-Agent 52\nis subjected to evaluation within the Chinese context. This datase t gauges the Agent\ncapabilities of LLMs in a Chinese context through three core abilities an d ten founda-\ntional tasks, covering aspects such as tool usage, task planning, and both s hort-term\nand long-term memory.\n5.1.10 Code\nThe coding evaluation datasets aim to assess the capabilities of LLMs in han dling\nprogramming-related tasks, including but not limited to code inter pretation, code\ngeneration, code correction, and code optimization. These datasets are p rimarily cat-\negorized into two types. The first type is single-task evaluation . APPS (\nHendrycks\net al , 2021a) serves as a benchmark for code generation, specifically evaluating th e\nability to generate Python code. Other datasets such as DS-1000 ( Lai et al , 2023),\nHumanEval (Chen et al, 2021), MTPB (Nijkamp et al, 2023), and ODEX (Wang et al,\n2023h) investigate code generation abilities in different forms. DS-1000 introduces data\nscience problems related to seven Python libraries. HumanEval asses ses LLMs using\nmanually written programming problems, mitigating data leakage concerns to some\nextent. MTPB tasks LLMs with synthesizing a subroutine at each step, requiring con-\nsideration of both the current task description and previous steps. O DEX extends\nthe variety of natural languages, using English, Spanish, Japanese, and Ru ssian to\ndescribe code intent, evaluating LLMs’ abilities to generate code u nder multilingual\n50https://github.com/Signiﬁcant-Gravitas/AutoGPT\n51https://github.com/reworkd/AgentGPT\n52https://github.com/CLUEbenchmark/SuperCLUE-Agent\n57\ndescriptions. Additionally, BIRD (Li et al, 2023f) is a large-scale database benchmark\nfor text-to-SQL (Structured Query Language) tasks that, compared to pre vious pop-\nular datasets like Spider ( Yu et al, 2018), reduces the gap between academic research\nand practical applications, enhancing the level of difficulty. The second type is multi-\ntask evaluation . CodeXGLUE ( Lu et al , 2021) categorizes code abilities into four\ntypes based on input-output pairs: code-code, text-code, code- text, and text-text.\nHumanEvalPack (Muennighoff et al , 2023a) is an extension of the HumanEval, cov-\nering six programming languages and three code tasks, including code fi xing, code\ncomment generation, and code generation.\n5.1.11 Out-of-Distribution\nThe out-of-distribution (OOD) evaluation dataset is designed to gauge the capabilities\nof pre-trained base models after fine-tuning with instructions fr om a subset of tasks\non previously unseen tasks. The emphasis is on scrutinizing the rob ustness of LLMs.\nYuan et al (2023) conducted experiments on the BOSS dataset ( Yuan et al , 2023),\nencompassing 5 tasks and 20 sub-datasets, to scrutinize the OOD perf ormance of\nLLMs. Yang et al (2023c) employed GLUE-X (Yang et al, 2023c) to assess the models’\nOOD performance and offered insights into the measurement and enhance ment of\nmodel OOD performance.\n5.1.12 Law\nLegal evaluation datasets play a crucial role in the application of LLMs in the l egal\ndomain by providing standardized performance assessments and drivi ng research and\ndevelopment in legal LLMs. The datasets can be categorized based on the lingu istic\nenvironment they target. LAiW (\nDai et al, 2023) and LawBench (Fei et al, 2023) are\ndesigned for the Chinese language environment. LAiW serves as a Chinese legal LLMs\nevaluation benchmark, focusing on 13 foundational tasks across three legal c ompeten-\ncies. It compares LLMs in terms of NLP basic capabilities, fundamental app lication\nabilities, and complex application capabilities. LawBench, benchmar ked on the Chi-\nnese legal system, evaluates LLMs’ legal abilities across 20 tasks simulating knowledge\nretention, understanding, and application, closely related to real-w orld applications.\nIn the English language environment, LegalBench (Guha et al, 2023) and LexGLUE\n(Chalkidis et al , 2022) are relevant. LegalBench, constructed with the assistance of\ncross-disciplinary professionals, is a legal reasoning benchmark comprising six types of\nlegal reasoning and 162 tasks. LexGLUE integrates open-source English legal datasets\nas an evaluation benchmark, examining legal Q&A and classification tasks.\nFor a multilingual environment, LEXTREME ( Niklaus et al , 2023) and SCALE\n(Rasiah et al, 2023) are applicable. LEXTREME divides 18 legal-related tasks from 11\nopen-source datasets, covering 24 languages. SCALE challenges current LLMs in four\ndimensions: handling long documents, applying legal knowledge, mu ltilingual com-\nprehension, and multitask processing. The benchmark is derived f rom the Swiss legal\nsystem, involving five languages.\n58\n5.1.13 Medical\nThe medical evaluation datasets focus on examining the comprehensive capabilities\nof LLMs in medical tasks such as term explanation, disease diagnosis, and tre at-\nment recommendations. This enables a comparison of the proficiency gap be tween\nvarious medical models and professional doctors. MultiMedQA (\nSinghal et al , 2023)\nserves as an evaluation benchmark for medical Q&A, blending multiple op en-source\ndatasets and proprietary datasets to assess LLMs’ abilities to address medical queries.\nQiZhenGPT-eval53 focuses on drug indication evaluation, tasking LLMs with identify-\ning diseases for which a given drug is suitable. However, single-task datasets are overly\nrestrictive in evaluation dimensions and may not reflect other medic al competencies.\nConsequently, various integrated datasets have been gradually proposed .\nCBLUE ( Zhang et al , 2022) is an evaluation dataset for Chinese medical lan-\nguage understanding, presenting five medical tasks using authentic m edical data. It\nassesses LLMs in medical text information extraction and medical Q&A. The design\nof CMB (Wang et al, 2023c) is based on the Chinese language and cultural framework,\nevaluating LLMs from the perspective of Chinese-style medical exams and complex\nclinical diagnoses. HuaTuo26M-test ( Li et al , 2023h) is randomly sampled from var-\nious sources, including medical encyclopedias and knowledge graphs, offering diverse\ntask types. PromptCBLUE 54 transforms 16 different NLP tasks in medical scenarios\ninto an evaluation format, forming the first systematic Chinese benchmark for medical\nscenarios.\n5.1.14 Financial\nThe financial evaluation dataset, akin to the legal and medical evaluation datas ets\nmentioned in previous sections, focuses on knowledge related to th e financial domain,\nassessing the performance of LLMs in handling financial texts and execut ing financial\ntasks. BBF-CFLEB (\nLu et al, 2023a) encompasses six sub-datasets for financial tasks,\nstrategically evaluating the language understanding and language generation c apabil-\nities of financial models from multiple perspectives. Both Financ elQ55 and FinEval\n(Zhang et al, 2023d) emphasize knowledge and reasoning abilities in financial scenarios,\nincorporating multiple-choice questions on different financial topic s to assess LLMs’\nfinancial knowledge. While the preceding datasets target the Chines e environment,\nFLUE (Shah et al, 2022) serves as an English-oriented testing benchmark, amalgamat-\ning six financial NLP datasets with a focus on NLU in the financial domain. FinB en\n(Xie et al , 2024) is also an English benchmark dataset for evaluating the capabilities\nof LLMs in the financial domain. It gathers 35 existing datasets covering 23 fi nancial\ntasks, categorized into three difficulty levels: foundamental tasks, advanced cognitive\nengagement, and general intelligence.\n53https://github.com/CMKRG/QiZhenGPT/tree/main/data/eval\n54https://github.com/michael-wzhu/PromptCBLUE\n55https://github.com/Duxiaoman-DI/XuanYuan/tree/main/FinanceIQ\n59\n5.1.15 Social Norms\nThe assessment dataset for societal norms evaluates LLMs across dimension s such\nas ethics, morality, prejudice, toxicity, and safety. It primaril y investigates whether\nthe models generate outputs that violate ethical and legal standards, dis play biased\ndiscrimination, or produce toxic and harmful content in response to unsafe instruc-\ntions. Datasets of this nature hold significant importance and societal val ue in the\nsafety scrutiny of LLMs. CrowS-Pairs (\nNangia et al , 2020) assesses LLMs for biases\nand discrimination within the context of American culture, encompass ing nine stereo-\ntypes related to prejudice, including race, religion, age, and more. SafetyBench (Zhang\net al, 2023n) stands as the inaugural benchmark for evaluating LLM safety through\nmultiple-choice questions in both Chinese and English, covering seven distinct safety\ndimensions. Safety-Prompts ( Sun et al , 2023a), featuring 13 safety scenarios and\nprompt attack evaluation data generated by ChatGPT, enables a comprehen sive\nevaluation of the models’ safety. However, constrained by ChatGPT’s p erformance,\noccasional errors may be present in questions or answers. TRUSTGPT ( Huang et al,\n2023d) evaluates LLMs in three crucial domains: toxicity, bias, and value cons istency.\nCompared to previous mainstream safety benchmarks, SuperCLUE-Safety 56 intro-\nduces heightened challenges by incorporating adversarial technique s and multi-turn\ninteractions, thereby enhancing the identification of LLM safety protection capabilities\nunder various adverse inputs.\n5.1.16 Factuality\nThe outputs produced by LLMs may exhibit deviations from the specifi ed input\ncriteria, preceding contextual information, or established facts and knowledge—a\nphenomenon commonly known as the hallucination of LLMs (\nZhang et al , 2023m).\nAddressing this issue necessitates the use of datasets designed for factual evaluation to\ngauge the extent of hallucination in LLMs. There are three distinct formsof evaluating\nthe factual accuracy of LLMs.\nThe first method entails the presentation of various options, prompting\nLLMs to discern the factually correct choice among alternatives or to as sess\nthe factual alignment of the provided content . In the FACTOR dataset (Muhl-\ngay et al , 2023), each instance comprises a prefix and four completions, with only\none completion being factually accurate. LLMs are required to identify the accurate\nchoice based on the given prefix and pertinent knowledge. HaluEval ( Li et al , 2023e)\nfurnishes inputs and outputs for tasks like Q&A, dialogue, and text sum marization,\nchallenging LLMs to recognize the existence of hallucination.\nThe second method entails assessing the factual accuracy of open-ended\ncontent generated by LLMs . FActScore ( Min et al , 2023) employs information\nfrom biographies to create a factual evaluation dataset, incorporating novel evaluation\ntechniques for appraising the factual precision of LLMs in producing extensive content.\nFactualityPrompt (Lee et al , 2022) similarly evaluates the factual aspects of LLMs\nin open-text generation, demanding the generation of accurate content un der genuine\nand non-genuine prompts.\n56https://github.com/CLUEbenchmark/SuperCLUE-safety\n60\nThe third method involves interrogating LLMs to assess the prevalence\nof hallucinatory phenomena . TruthfulQA ( Lin et al , 2022) meticulously devises\nEnglish questions prone to generating erroneous answers due to potent ial misunder-\nstandings, evaluating the veracity of LLMs’ responses. Taking cues fromthis, HalluQA\n(Cheng et al, 2023) formulates Chinese questions designed to mislead Chinese LLMs,\nevaluating the hallucinatory tendencies in Chinese LLMs. FreshQA ( Vu et al, 2023)\nacts as a dynamic benchmark for factual Q&A, necessitating not only a maste ry\nof rapidly evolving world knowledge but also the ability to refute in correct factual\npremises.\n5.1.17 Evaluation\nThe rise of LLMs has ushered in a fresh paradigm for model evaluation, allow ing pro-\nficient LLMs to act as evaluators in scoring the outputs of other models. However, the\nreliability of involving LLMs in assessments and the performance variab ility among\ndifferent LLMs in appraising the quality of model responses prompt inq uiries. Con-\nsequently, datasets falling under the evaluation category are specific ally tailored to\nprobe into the potential and competence of LLMs as evaluators. FairEval (\nWang et al,\n2023b) critically examines the model evaluation paradigm to explore the dep endabil-\nity of LLMs as assessors. It utilizes the Vicuna Evaluation dataset 57 as instructions,\ngenerating responses from various models, and subsequently engages mo dels such as\nChatGPT, GPT-4, and others to evaluate diverse responses. PandaLM testset (Wang\net al , 2023g), enriched with human annotations, serves to validate the assessmen t\ncapabilities of trained PandaLM ( Wang et al , 2023g) when evaluating other LLMs.\nLLMEval2 (Zhang et al , 2023l), currently the largest and most diversified English\nbenchmark for evaluating LLMs, spans 15 tasks and 8 abilities, employing innovative\nmethods to gauge the quality of LLMs’ evaluation responses.\n5.1.18 Multitask\nMultitask evaluation datasets present a thorough examination of LLMs’ compre hen-\nsive capabilities, characterized by a substantial task volume, exte nsive scale, broad\ndomains, and diverse task types. In the realm of English, DecaNLP (\nMcCann et al ,\n2018) transforms 10 distinct task datasets into a Q&A format, introducing the\n“Decathlon” multitask challenge within the natural language domain. LMentry (Efrat\net al , 2023) provides a swift, automated “unit test,” assessing LLMs’ performanc e\nacross 25 task types that are relatively simple for human understanding. However,\nthese datasets still lack task type richness. BIG-Bench (Srivastava et al, 2023) impres-\nsively includes 95 task types, totaling 204 tasks, covering a wide arr ay of topics such\nas linguistics, common-sense reasoning, social biases, software development, and more.\nBBH (Suzgun et al, 2023) carefully selects 23 challenging tasks from BIG-Bench, where\nprevious language models have not surpassed average human performance, p resenting\na considerable challenge. HELM ( Liang et al, 2023) contemplates holistic model eval-\nuation, establishing a comprehensive evaluation system for LLMs with 73 evaluation\nscenarios and 65 evaluation metrics, ensuring a thorough and rigorous assess ment.\n57https://github.com/lm-sys/vicuna-blog-eval\n61\nIn the Chinese domain, CLEVA (Li et al, 2023n) stands as a comprehensive Chinese\nevaluation benchmark, featuring 11 application assessment tasks and 20 cap ability\nassessment tasks, with a scale reaching 370K. CLiB 58 serves as a Chinese proficiency\ntest list for LLMs, covering LLMs such as GPT-4, ERNIE Bot ( Sun et al , 2021b),\nQWen (Bai et al, 2023a), and supporting multidimensional capability evaluations like\nclassification and information extraction. LLMEVAL-1 (Zhang et al, 2023f), compris-\ning 17 task categories, 5 scoring items, and various evaluation methods, systematically\nevaluates LLMs. Furthermore, FlagEval59 scrutinizes the models’ comprehensive per-\nformance in both Chinese and English environments, serving as an evalu ation toolkit\nfor AI base models capable of assessing over 600 sub-dimensions of base mod els.\n5.1.19 Multilingual\nMultilingual evaluation datasets assess the performance of LLMs in cross-l ingual\ntasks using data encompassing multiple languages, contributing to th e exploration of\nLLMs’ capabilities across diverse linguistic challenges. XNLI (\nConneau et al , 2018)\nis specialized for evaluating low-resource language transfer and cross- lingual sentence\nclassification, incorporating 15 languages, including English, French,Spanish, Chinese,\nand German. Conversely, XTREME (Siddhant et al, 2020) expands language coverage\nby translating content for four NLP tasks into 40 languages, crossing 12 language fam-\nilies. In essence, multilingual evaluation datasets typically buil d on traditional NLP\ntasks, extend language diversity, maintain a moderate task difficulty, and necessitate\na wealth of language knowledge.\n5.1.20 Other\nApart from the aforementioned assessment datasets, there exist sever al datasets\nspecifically dedicated to diverse domains, addressing deficienci es in the evaluation\nlandscape. The subsequent section provides an overview of pivotal d atasets within\nseven subdomains for reference.\nE-commerce Domain. The EcomGPT\neval dataset (Li et al, 2023m) is designed\nto evaluate the efficacy of LLMs in tasks within the realm of e-commerce. It consists\nof 6K instances, with 500 instances sampled from each of the 12 held-out datas ets\ntailored for e-commerce evaluation. Tasks in the e-commerce domain are classified into\nfour categories: classification, generation, extraction, and miscellaneou s. These tasks\nspan coarse and fine-grained product classification, product title gene ration, attribute\nvalue detection, and e-commerce NER, among others.\nFew-shot Learning Domain. The FewCLUE dataset (Xu et al, 2021) has been\ncreated with a specific focus on assessing few-shot learning in theChinese language. Its\npurpose is to leverage the generalization capabilities of pre-trained m odels and inves-\ntigate the practicality of few-shot learning models applied to Chine se. The dataset is\ncomposed of nine sub-datasets, with some containing slightly over a hu ndred anno-\ntated samples, providing a means to evaluate model generalization und er conditions\nof extremely limited labeled data.\n58https://github.com/jeinlee1991/chinese-llm-benchmark\n59https://github.com/FlagOpen/FlagEval\n62\nGeoscience Domain. The GeoBench dataset (Deng et al, 2023) serves as a means\nto evaluate the proficiency of language models in tackling questions rel ated to geo-\nscience, assessing their capacity to comprehend and apply knowled ge in this domain.\nThe dataset is bifurcated into two sections. The initial segment com prises questions\nfrom the Chinese graduate entrance examination in geology and geography, encom-\npassing 182 multiple-choice questions, 150 fill-in-the-blank ques tions, 454 vocabulary\nexplanation questions, and 335 essay questions. The subsequent segment includes 1,395\nmultiple-choice questions from advanced research examinations in th e United States.\nIT Domain. The Owl-Bench dataset (\nGuo et al , 2023b) serves as a bilingual\nevaluation benchmark tailored for IT operations and maintenance contexts. It encom-\npasses 317 questions and answers, in addition to 1K multiple-choice que stions. The\ntasks address numerous real-world industrial scenarios, spanning ni ne distinct sub-\ndomains: information security, applications, system architecture,software architecture,\nmiddleware, networks, operating systems, infrastructure, and dat abases.\nMulti-turn Interaction Domain. LLMs frequently interact with users across\nmultiple turns, yet assessments typically focus on individual t urns, overlooking their\ninteractive capabilities. Thus, the MINT dataset ( Wang et al , 2023e) is designed to\nevaluate LLMs in tasks involving multi-turn interactions, employing tools or utilizing\nnatural language feedback. In this evaluation framework, the model being tested can\naccess tools through the execution of Python code, receiving feedbac k simulated by\nGPT-4 to facilitate multi-turn interactive assessments.\nRobustness Domain. The PromptBench dataset ( Zhu et al , 2023) extensively\nexplores the robustness of LLMs when confronted with seven distincttypes of adversar-\nial prompts. Simultaneously, it performs an analysis of the transferability of adversarial\nprompts generated by various models. The examination of robustness enc ompasses\neight diverse NLP tasks across thirteen open-source datasets, encompass ing domains\nlike sentiment analysis, multi-task knowledge, reading comprehe nsion, mathematics,\nand beyond.\nSentiment Domain. The EmotionBench dataset (Huang et al, 2023a) presents a\npioneering benchmark for assessing the empathetic abilities of LLMs, examining how\nLLMs undergo emotional changes in response to particular situations. Encom passing\nmore than 400 scenarios, the dataset generates eight distinct emotional cate gories:\nanger, anxiety, depression, frustration, jealousy, guilt, fear, and em barrassment.\n5.2 Evaluation Methods\nIn this section, evaluation methods are classified into three types : code evalua-\ntion, human evaluation , and model evaluation . Figure\n19 illustrates these three\nevaluation methods. Notably, code evaluation and model evaluation operate w ith\nminimal human intervention, with evaluation results being automatic ally computed\nand generated through the pipeline. These two methods are categorized as auto-\nmated evaluation. In contrast, human evaluation is characterized as a non-au tomated\napproach.\nThe approach of code evaluation entails the extraction of responses from LLMs ,\nreferencing authentic annotations, and utilizing code to statisti cally compute prede-\nfined evaluation metrics. The efficacy of LLMs is consequently gauged through the\n63\nCode evaluation\nModel evaluation\nAutomated evaluation\nLLM ouput\n Extract\nkey contents\nCode-based\nmetric calculation\nLLM ouput\nNon-automated evaluation Human evaluation\nLLM ouput\n Human judges\nModel judges\nReference answers\n&\nEvaluation criteria\nQuality scoring\n1v1 Quality\ncomparison\nFig. 19 Three evaluation methods\nnumerical values of these metrics. Prominent evaluation metrics in clude accuracy, F1\nscore, BLEU ( Papineni et al , 2002), ROUGE ( Lin, 2004), Exact Match 60, Pearson\ncorrelation coefficient61, among others. For instance, accuracy can be employed in clas-\nsification tasks to appraise the precision of LLMs’ classifications. In tran slation tasks,\nBLEU serves to assess the resemblance between LLMs’ translations and au thentic\nannotations. Certain evaluation datasets not only provide custom calculation methods\nbut also furnish pertinent code, facilitating direct application for the evaluation and\nanalysis of LLMs’ performance. This evaluation methodology is commonly used for\nobjective questions and straightforward subjective questions with predefined answers,\nsuch as basic knowledge queries and translation exercises. While it s simplicity is ben-\neficial, it may not be as effective for assessing open-ended subjecti ve questions such\nas those involve generation and brainstorming.\nThe human evaluation approach, on the other hand, often involves the evaluation of\nLLM outputs by crowdsourced individuals, trained volunteers, stud ents with relevant\nexpertise, or expert panels. Evaluation methods include quality scoring (as seen in the\nQizhenGPT eval dataset62 and the CLiB dataset 63), quality comparison assessment\n(Xu et al, 2023e), and similar techniques. This manual evaluation method is versatile,\nsuitable for various question types, especially open-ended subjec tive inquiries and\ncomplex problems lacking standard answers. Nevertheless, its limi tation lies in the\nsubstantial costs, the need for extensive human resources, and a potential for subjective\nbias.\nThe method of evaluating models represents a novel paradigm in whic h questions,\nreference answers, evaluation criteria and standards, along with the responses of the\ntested models, are integrated into an optimal prompt. This combined information is\nthen inputted to the model for evaluation ( Ji et al, 2023b; Zheng et al, 2023b; Zhang\net al , 2023h; Dubois et al , 2023; Cheng et al , 2023; Bai et al , 2023c; Guo et al ,\n2023b). This evaluation approach emphasizes the selection of LLMs with curren tly\nhigh performance and provides suitable evaluation instructions. Itsadvantage lies in its\ncapacity to substitute for a considerable amount of manual effort, resulting in a quicker\n60https://huggingface.co/spaces/evaluate-metric/exact match\n61https://libguides.library.kent.edu/SPSS/PearsonCorr\n62https://github.com/CMKRG/QiZhenGPT/tree/main/data/eval\n63https://github.com/jeinlee1991/chinese-llm-benchmark\n64\n4 1 4\n9\n15\n78\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n2018 2019 2020 2021 2022 2023\nThe Number of Evaluation Datasets\nRelease Time\n(a)\n1\n1\n1\n1\n1\n1\n2\n3\n5\n5\n6\n22\n24\n0 5 10 15 20 25\nCC-SA-4.0\nCC-BY-SA\nCC-BY-NC-ND-4.0\nC-UDA\nBSD-3-Clause\nBSD\nCC-BY-4.0\nCC-BY-NC-4.0\nCC-BY-SA-4.0\nGPL-3.0\nCC-BY-NC-SA-4.0\nMIT\nApache-2.0\nThe Number of Evaluation Datasets\nLicense\n(b)\n28\n25\n23\n11\n7\n0\n5\n10\n15\n20\n25\n30\n0-1K 1K-10K 10K-100K 100K-1M ＞1M\nThe Number of Evaluation Datasets\nSize / # instances\n(c)\n34\n32\n20\n11\n8\n2 1\n0\n5\n10\n15\n20\n25\n30\n35\nHG CI\nHG & CI HG & MC\nHG & CI & MC\nMC\nCI & MC\nThe Number of Evaluation Datasets\nConstruction Method\n(d)\n53\n31\n10 9\n7\n1\n0\n10\n20\n30\n40\n50\nEN ZH EN & ZH EN & PL Multi Multi & PL\nThe Number of Evaluation Datasets\nLanguage\n(e)\n13\n9\n8\n7 7 7\n6 6 6 6\n5 5\n4\n3 3 3\n2 2 2\n1 1 1 1 1 1 1\n0\n2\n4\n6\n8\n10\n12\n14\nSubject\nMultitask\nCode\nGeneral\nFactuality\nNLU\nLaw\nMedical\nReasoning\nLong Text\nKnowledge\nSocial Norms\nFinancial\nExam\nTool\nEvaluation\nMultilingual\nOOD\nAgent\nE-commerce\nFew-shot learning\nGeoscience\nIT\nMulti-turn interactions\nRobustness\nSentiment\nThe Number of Evaluation Datasets\nDomain\n(f)\n80\n17\n14\n0\n10\n20\n30\n40\n50\n60\n70\n80\nSubjective Question Multi Objective Question\nThe Number of Evaluation Datasets\nQuestion Type\n(g)\n79\n10 7 5 4 3 1\n0\n10\n20\n30\n40\n50\n60\n70\n80\nCE ME\nHE & CE HE & ME\nHE & CE & ME\nHE\nCE & ME\nThe Number of Evaluation Datasets\nEvaluation Method\n(h)\nFig. 20 Statistics distribution of evaluation datasets. (a) illustrates the quantity trend\nover time. (b) depicts the quantity distribution under differe nt licenses, considering\nonly the datasets with listed licenses. (c) shows the quantity di stribution across dif-\nferent data scales. (d) displays the quantity distribution for di fferent construction\nmethods. (e) represents the quantity distribution across differ ent languages. (f) illus-\ntrates the quantity distribution across different domains. (g) indicates the distribution\nbased on various question types; and (h) outlines the distribution em ploying diverse\nevaluation methods. Zoom in for better view\nevaluation process. Nevertheless, the limitation lies in the depe ndency on the LLMs’\nperformance and may not always correspond with human values and judgemen ts.\nIt is increasingly common to employ a mix of multiple assessment met hods (\nAn\net al, 2023; Zhang et al , 2023k; Sun et al , 2023a; Sawada et al, 2023; Li et al , 2023i;\nWang et al, 2023c; Min et al , 2023; Deng et al , 2023; Liang et al , 2023; Guha et al ,\n2023; Zhang et al , 2023f,e; Singhal et al , 2023; Xu et al , 2023e; Lin et al , 2022),\nleveraging the strengths and mitigate the weaknesses of each method. T his approach\naims to achieve a comprehensive, rigorous, and standardized evaluation.\n5.3 Distribution Statistics of Evaluation Datasets\nFigure\n20 provides statistics on 112 evaluation datasets from eight aspects: releas e\ntime, license, size, construction method, language, domain, questi on type, and\nevaluation method. Based on these statistics, the following conclus ions can be drawn:\n65\n(1) There is a noticeable upward trend in the evaluation datasets. The on going\nmaturation of technologies related to LLMs is driving the expansion of datase ts tai-\nlored for LLMs evaluation. Specifically, in the year 2023, there has been a sign ificant\nsurge in the number of evaluation datasets, reflecting the need for div erse datasets to\nkeep pace with the rapid iteration of LLMs and to improve model perform ance.\n(2) The distribution of evaluation dataset licenses shows a preferen ce for widely\nrecognized licenses such as, Apache-2.0 and MIT. The overall pattern of distribution\nin these protocols underscores the delicate equilibrium sought wi thin the LLMs data\nevaluation domain, balancing knowledge sharing and intellectual proper ty protection.\nThe flexibility provided by open licenses such as Apache-2.0 and MIT contributes to\nthe widespread use and sharing of evaluation datasets, which is essential for advancing\nrelative research.\n(3) The majority of evaluation datasets fall within the 0-100K size range, wit h\ndatasets containing fewer than 10K samples constituting 56.4% of the total. T his\nindicates that many tasks can be effectively assessed with relativel y small datasets,\nwhich may be also due to cost considerations during dataset construct ion and evalua-\ntion. Nevertheless, a few datasets still surpass the 1M mark, mainly derived from web\nscraping or the consolidation of open-source datasets.\n(4) Manual construction and the compilation of open-source datasets are the\ndominant methods for creating evaluation datasets. Manual construction i s often\npreferred for its precision and relevance to specific domains, wher eas the combina-\ntion of open-source datasets creates common benchmarks for evaluation. The use of\nmodel-generated data for evaluation is less common due to concerns about q uestion\nauthenticity and answer accuracy, and it is generally used as a supplem ental method.\n(5) English language datasets are the most prevalent, with Chinese language\ndatasets also being significant, reflecting the focus on evaluating LLM p erformance\nfor tasks in these two languages. Although there are a limited number of datasets that\ncover evaluations in other languages, resources for low-resource minority languages are\nnotably limited.\n(6) Evaluation datasets including multiple disciplines and task types are prevalent,\nunderscoring the increased focus on evaluating the holistic capabil ities of LLMs. The\nresearch community is particularly concerned with the model’s gen eral applicability\nand extensive knowledge. Various evaluation datasets cover conventional instructions,\nknowledge domains, social norms, and several prevalent vertical field s. Nevertheless,\nthe distribution of domains within evaluation datasets continues to ex hibit a long-tail\npattern, with niche areas like e-commerce and earth sciences having limited evaluation\nresources. Notably, domains like ancient texts and cultures current ly lack evaluation\nbenchmarks.\n(7) Subjective questions, especially those related to Natural Language Understand-\ning (NLU), dominate the evaluation datasets. A minority of datasets encompass es\nobjective questions, including multiple-choice and fill-in-the-blank formats. Regarding\nthe methodologies employed for evaluation, the widespread use of code -based assess-\nment is attributable to its applicability for objective questions an d straightforward\nsubjective tasks, manifesting advantages in efficiency and consisten cy. Conversely,\nmanual evaluation is unsuitable for extensive tasks and objective questions due to cost\n66\nconsiderations and is consequently infrequently utilized. It is crucial to highlight that\nmodel evaluation, to some degree, amalgamates the strengths of code-based and man-\nual evaluations, potentially steering towards becoming the predomin ant evaluation\nmethodology in the future. Naturally, the strategic combination of evaluation methods\nshould consider practical aspects, including the scale and divers ity of questions.\n6 Traditional NLP Datasets\nDiverging from instruction fine-tuning datasets, we categorize text datasets dedicated\nto natural language tasks before the widespread adoption of LLMs as traditional\nNLP datasets. These datasets, devoid of instructional formats, are specifi cally crafted\nfor training, optimizing, and testing traditional NLP models. The resul tant NLP\nmodels find application in diverse text processing tasks, includ ing text classification,\ninformation extraction, text summarization, etc.\nIn contemporary LLMs projects, a plethora of traditional NLP datasets finds appli-\ncation. These datasets undergo dual roles: firstly, their format and cont ent transform\ninto instructional formats for the instruction-guided fine-tuning phase of LLMs, aug-\nmenting the models’ capacities to adhere to instructions and exce l in such tasks;\nsecondly, they serve as evaluation datasets for LLMs, enabling the compari son of\ndiverse LLMs in natural language tasks. Notably, several LLMs instruction datas ets\nand evaluation datasets emerge from the conversion of traditional NLP datasets.\nConsequently, this section succinctly summarizes classical trad itional NLP datasets\ncommonly integrated into existing LLMs and various LLMs evaluation platforms. The\nobjective is to streamline and offer references for traditional NLP datasets, facilitating\nthe dataset selection process for LLMs projects.\nIn this context, the compiled traditional NLP datasets are systematically classified\ninto 15 distinct categories, aligning with various tasks. Figure\n21 visually represents\nthese categories, encompassing question answering , recognizing textual entail-\nment, math, coreference resolution , sentiment analysis , semantic matching ,\ntext generation , text translation , text summarization , text classiﬁcation ,\ntext quality evaluation , text-to-code, named entity recognition , relation\nextraction, and multitask. We will summarize various categories of NLP datasets\nin a straightforward manner using text and tables (Table 14 to Table 30). Detailed\ninformation about the datasets is presented in the Appendix E.\n6.1 Question Answering\nThe task of question-answering requires the model to utilize its knowledge and reason-\ning capabilities to respond to queries based on provided text (whi ch may be optional)\nand questions. This task often includes subcategories like reading comprehension ,\nknowledge QA , and reasoning QA .\n6.1.1 Reading Comprehension\nThe task of reading comprehension entails presenting a model with a designated text\npassage and associated questions, prompting the model to understand t he text for the\n67\nTraditional NLP\nDatasets\nQuestion Answering\nCoreference Resolution\nRecognizing Textual Entailment\nMath\nReading Comprehension\nKnowledge QA\nReasoning QA\nSentiment Analysis\nSemantic Matching\nText Generation\nText Translation\nText Summarization\nText Classification\nText Quality Evaluation\nText-to-Code\nNamed Entity Recognition\nRelation Extraction\nMultitask\nSelection & Judgment\nCloze Test\nAnswer Extraction\nUnrestricted QA\nFig. 21 Different NLP task categories of the traditional NLP datasets\npurpose of answering the questions. Based on the answering approach of th e task, it\ncan be roughly classified into four categories: selection & judgment , cloze test ,\nanswer extraction , and unrestricted QA .\nThere are two modes for selection & judgment tasks. Mode one requires the\nmodel to select the most appropriate option from several answer options .\nRACE (Lai et al , 2017) and DREAM ( Sun et al , 2019) are specifically selected from\nEnglish exams designed by human experts, requiring the model to an swer multiple-\nchoice questions about the content of given English articles. Similarl y, C3 (Sun et al,\n2020) and ReClor ( Yu et al, 2020b) are extracted from corresponding Chinese exams\nand graduate entrance exams, respectively, each containing relevant m ultiple-choice\nquestions. Mode two involves judging the correctness of a question using\neither “Yes” or “No.” BoolQ (Clark et al, 2019) requires the model to respond with\n“Yes” or “No” to complex inquiries and non-factual information. CondaQA (Ravichan-\nder et al , 2022), as the first English dataset to assess negation statements, tests the\nmodel’s understanding of negative assertions, with answers in the form of “Yes,” “No,”\nor “Don’t Know.” PubMedQA ( Jin et al , 2019), focusing deeply on the biomedical\nfield, presents higher professional knowledge requirements, nec essitating judgment on\nthe correctness of questions based on the abstracts of medical articles .\nThe cloze task requires the model to select a word or sentence to fill in the\nmissing part of the text, making the text coherent and logical. Tasks are typically set\nat both the word and sentence levels. LAMBADA ( Paperno et al, 2016) and CLOTH\n(Xie et al, 2018) are English word-level cloze datasets. By perceiving the context, the\nmodel predicts the positions of missing words in the sentences. C hID ( Zheng et al ,\n2019) requires the model to choose the correct idiom to fill in the blank , focusing on\ntesting the model’s understanding of Chinese idioms. CMRC2019 ( Cui et al , 2020)\nis a sentence-level cloze-style dataset that requires the model to fill in several blank\nspaces in the article with candidate sentences.\nThe answer extraction task involves the model pinpointing a continuous excerpt\nwithin the text as the answer to a given question. Fundamentally, th e answers to the\nquestions can be extracted or composed directly from the textual conte nt, eliminating\n68\nthe necessity of generating supplementary open-ended content. SQ uAD ( Rajpurkar\net al , 2016) extracts text passages and answers to questions from Wikipedia arti-\ncles for answer extraction tasks. SQuAD 2.0 ( Rajpurkar et al , 2018) extends the\nSQuAD dataset by adding unanswerable questions, testing the models’ ability to judge\nambiguous questions. Adversarial QA (Bartolo et al, 2020) expands upon the SQuAD\ndataset by creating more challenging questions using adversarial human annotations.\nAdditionally, other datasets such as TriviaQA ( Joshi et al , 2017), Natural Questions\n(Kwiatkowski et al , 2019), and CMRC2018 ( Cui et al , 2019) feature more complex,\nchallenging, and realistic reading comprehension questions.\nThe unrestricted QA task exhibits greater openness when contrasted with\nanswer extraction tasks. The task entails producing a fitting respon se by leveraging\nboth textual content and a posed question. The answer, rather than bein g an exact\nextraction from the text, is openly generated by the models. Presen tly, this task cat-\negory stands as a predominant focus in the evaluation of LLMs. DROP ( Dua et al ,\n2019) and QASPER ( Dasigi et al , 2021) assess models’ reasoning ability to generate\nopen-ended answers. Answers cannot be directly extracted from the t ext but require\nmodels to search for clues from multiple sources and then perform ce rtain operations.\nCoQA (Reddy et al, 2019) measures models’ ability to answer related questions, with\nanswers being in free-form text. Compared to the previous datasets, DuReader 2.0 (He\net al, 2018) expands the scale of text and questions, conducting open-domain Q&A at\nthe document level.\n6.1.2 Knowledge QA\nIn the knowledge QA task, models respond to questions by leveraging world knowl-\nedge, common sense, scientific insights, domain-specific information, and more. Unlike\nreading comprehension tasks, each instance does not come with a refer ence text. This\ntask assesses the model’s depth of knowledge and its capacity to comprehend questions.\nARC (\nClark et al, 2018), CommonsenseQA (Talmor et al, 2019), and OpenBookQA\n(Mihaylov et al, 2018) evaluate models’ knowledge mastery and comprehension abil-\nities based on scientific facts and human common sense. These datasets e mphasize\ngeneral knowledge known to the general public. However, some datasets place more\nemphasis on testing vertical domain knowledge. PIQA ( Bisk et al , 2020) and SciQ\n(Welbl et al, 2017) examine knowledge of science, JEC-QA ( Zhong et al, 2020) exam-\nines legal analysis, WebMedQA ( He et al , 2019) examines medical diagnosis, and\nPsyQA (Sun et al, 2021a) examines psychological counseling.\n6.1.3 Reasoning QA\nThe focal point of reasoning QA tasks is the requirement for models to apply abili-\nties such as logical reasoning, multi-step inference, and causal reasoni ng in answering\nquestions. These types of questions typically necessitate model s to grasp the logi-\ncal connections within the text, deduce concealed information, and arr ive at sensible\nconclusions.\nHellaSwag (\nZellers et al , 2019a), Social IQa ( Sap et al , 2019), ROPES (Lin et al ,\n2019), and WIQA ( Tandon et al , 2019) are grounded in contextual reasoning, aim-\ning to enable models to infer the subsequent development direc tion based on given\n69\ncontexts. COPA ( Roemmele et al , 2011) specifically tests causal reasoning ability,\nselecting appropriate causal relationships based on premises. LogiQA (Liu et al, 2021)\nextensively investigates logical reasoning, covering various deduct ive patterns. Thus,\nit is evident that datasets for reasoning question answering tasks inv olve different\ndimensions of reasoning.\nTable 14 Summary of Reading Comprehension Datasets Information. Release\nTime: “X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,” and “All\nSize” provide statistics on the respective question quantities in the dataset. Language:\n“EN” indicates English, “ZH” indicates Chinese, “Multi” indicates Mu ltilingual, and\nthe number in parentheses indicates the number of languages include d\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nSelection & Judgment\nBoolQ University of Washington et al. 2019-5 9427 3270 3245 15942 CC-SA-3.0 EN\nCondaQA Carnegie Mellon University et al. 2022-11 5832 1110 7240 14182 Apache-2.0 EN\nCosmosQA University of Illinois Urbana-Champaign et al. 2019-9 25588 3000 7000 35588 CC-BY-4.0 EN\nC3 Cornell University et al. 2019-4 11869 3816 3892 19577 - ZH\nDREAM Cornell University et al. 2019-2 6116 2040 2041 10197 - EN\nDureader Yes/No Baidu Inc. et al. 2019-12 75K 5.5K 11K 91.5K Apache-2.0 ZH\nMCTest Microsoft Research 2013-10 1200 200 600 2000 - EN\nMultiRC University of Pennsylvania et al. 2018-6 - - - 9872 MultiRC License EN\nPubMedQA University of Pittsburgh et al. 2019-9 - - - 273.5K MIT EN\nQuAIL University of Massachusetts Lowell 2020-4 10346 - 2164 12510 CC-NC-SA-4.0 EN\nRACE Carnegie Mellon University 2017-4 87866 4887 4934 97687 - EN\nReClor National University of Singapore 2020-2 4638 500 1000 6138 - EN\nCloze Test\nChID Tsinghua University et al. 2019-6 605K 23.2K 83.3K 711.5K Apache-2.0 ZH\nCLOTH Carnegie Melon University 2018-10 76850 11067 11516 99433 MIT EN\nCMRC2019 Harbin Institute of Technology et al. 2020-12 100009 3053 5118 108180 CC-BY-SA-4.0 ZH\nLAMBADA University of Trento et al. 2016-6 2662 4869 5153 12684 CC-BY-4.0 EN\nAnswer Extraction\nAdversarial QA University College London 2020-2 30000 3000 3000 36000 MIT EN\nCMRC2018 Harbin Institute of Technology et al. 2019-11 10321 3351 4895 18567 CC-BY-SA-4.0 ZH\nCUAD UC Berkeley et al. 2021-3 22450 - 4182 26632 CC-BY-4.0 EN\nDureader Checklist Baidu Inc. et al. 2021-3 3K 1.1K 4.5K 8.6K Apache-2.0 ZH\nDureader Robust Baidu Inc. et al. 2020-3 15K 1.4K 4.8K 21.2K Apache-2.0 ZH\nHOTPOTQA Carnegie Mellon University et al. 2018-9 90447 7405 7405 105257 CC-BY-SA-4.0 EN\nMLQA Facebook AI Research et al. 2020-7 - 4199 42246 46445 CC-BY-SA-3.0 Multi (7)\nMS MARCO Microsoft AI & Research 2016-11 808731 101093 101092 1010916 MIT EN\nNatural Questions Google Research 2019-X 307372 7830 7842 323044 CC-BY-4.0 EN\nQuAC AI2 et al. 2018-8 83568 7354 7353 98407 CC-BY-SA-4.0 EN\nQuoref AI2 et al. 2019-8 19399 2418 2537 24354 CC-BY-4.0 EN\nReCoRD Johns Hopkins University et al. 2018-10 100730 10000 10000 120730 - EN\nSQuAD Stanford University 2016-11 87599 10570 9533 107702 CC-BY-4.0 EN\nSQuAD 2.0 Stanford University 2018-6 130319 11873 8862 151054 CC-BY-SA-4.0 EN\nTriviaQA Univ. of Washington et al. 2017-7 - - - 95000 Apache-2.0 EN\nTyDiQA Google Research 2020-3 116916 18670 18751 154337 Apache-2.0 Multi (11)\nUnrestricted QA\nCoQA Stanford University 2018-8 - - - 127K - EN\nDROP University of California et al. 2019-6 77409 9536 9622 96567 CC-BY-4.0 EN\nDuoRC IBM Research et al. 2018-7 130261 27914 27914 186089 MIT EN\nDureader 2.0 Baidu Inc. et al. 2018-4 - - - 200K Apache-2.0 ZH\nQASPER AI2 et al. 2021-5 - - - 5049 CC-BY-4.0 EN\n6.2 Recognizing Textual Entailment\nThe primary objective of tasks related to Recognizing Textual Entailm ent (RTE) is\nto assess whether information in one textual segment can be logically inf erred from\nanother. This is formally structured with a “premise” denoted asP and a “hypothesis”\ndenoted as H, aimed at determining the relationship between P and H. If P logically\nentails H, it is categorized as “Entailment”; if P and H are logically contradictory,\nit is categorized as “Contradiction”; if there is no discernible logical c onnection or\ncontradiction between P and H, it is categorized as “Neutral.” In some instances, the\nlatter two scenarios are combined into “Non-Entailment.”\n70\nTable 15 Summary of Knowledge QA Datasets Information. Release Time: “X”\nindicates unknown month. “Train Size,” “Dev Size,” “Test Size,” and “All Size” pro-\nvide statistics on the respective question quantities in the datas et. Language: “EN”\nindicates English, “ES” indicates Spanish, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nARC AI2 2018-3 3370 869 3548 7787 CC-BY-SA EN\nCMD Toyhom 2019-X - - - 792099 MIT ZH\ncMedQA2 National University of Defense Technology 2018-11 100000 4000 4000 108000 GPL-3.0 ZH\nCommonsenseQA Tel-Aviv University et al. 2018-11 9797 1225 1225 12247 MIT EN\nECQA IIT Delhi et al. 2021-8 7598 1090 2194 10882 CDLA-Sharing-1.0 EN\nHEAD-QA Universidade da Coruna 2019-7 2657 1366 2742 13530 MIT EN & ES\nJEC-QA Tsinghua University et al. 2019-11 - - 26365 26365 CC-NC-ND-4.0 EN\nOpenBookQA AI2 et al. 2018-10 4957 500 500 5957 Apache-2.0 EN\nPIQA AI2 et al. 2019-11 16.1K 1.84K 3.08K 21.02K MIT EN\nPsyQA The CoAI group et al. 2021-6 - - - 22346 PsyQA User Agreement ZH\nSciQ University College London et al. 2017-9 11679 1000 1000 13679 CC-BY-NC-3.0 EN\nWebMedQA Chinese Academy of Sciences et al. 2018-12 50610 6337 6337 63284 Apache-2.0 ZH\nWikiQA Georgia Institute of Technology et al. 2015-9 2118 296 633 3047 Microsoft Research D ata License EN\nTable 16 Summary of Reasoning QA Datasets Information. “Train Size,” “Dev\nSize,” “Test Size,” and “All Size” provide statistics on the respec tive question\nquantities in the dataset. Language: “EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nCOPA Indiana University et al. 2011-6 - 500 500 1000 BSD 2-Clause EN\nCREAK The University of Texas at Austin 2021-9 10176 1371 1371 13418 MIT EN\nHellaSwag University of Washington et al. 2019-7 39905 10042 10003 59950 MIT EN\nLogiQA Fudan University et al. 2020-7 7376 651 651 8678 - EN & ZH\nPROST University of Colorado Boulder 2021-8 - - 18736 18736 Apache-2.0 EN\nQASC AI2 et al. 2019-10 8134 926 920 9980 CC-BY-4.0 EN\nQuaRel AI2 2018-11 1941 278 552 2771 CC-BY-4.0 EN\nQuaRTz AI2 2019-11 2696 384 784 3864 CC-BY-4.0 EN\nROPES AI2 2019-8 10K 1.6K 1.7K 13.3K CC-BY-4.0 EN\nSocial IQa AI2 2019-4 33410 1954 - 35364 - EN\nStoryCloze University of Rochester et al. 2016-6 - 1871 1871 3742 - EN\nSTRATEGYQA Tel Aviv University et al. 2021-1 2290 - 490 2780 MIT EN\nWIQA AI2 2019-9 29808 6894 3993 40695 - EN\nFor example, RTE ( Dagan et al , 2006; Bar-Haim et al , 2006; Giampiccolo et al ,\n2007; Bentivogli et al, 2009) integrates a portion of the Recognizing Textual Entailment\nchallenge datasets, comprising two types of relationships: “Entailme nt” and “Non-\nEntailment.” CommitmentBank (De Marneffe et al , 2019), OCNLI ( Hu et al , 2020),\nand CINLID64 expand the judgment of relationships to three types. ANLI ( Nie et al,\n2020) introduces adversarial samples, increasing the difficulty of text ual relationship\njudgment and making it more challenging.\nTable 17 Summary of Recognizing Textual Entailment Datasets Information.\nRelease Time: “X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,”\nand “All Size” provide statistics on the respective question quanti ties in the dataset.\nLanguage: “EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nANLI UNC Chapel Hill et al. 2019-10 162865 3200 3200 169265 CC-NC-4.0 EN\nCINLID Gao et al. 2021-4 80124 - 26708 106832 - ZH\nCMNLI CLUE team 2020-12 391783 12426 13880 418089 - ZH\nCommitmentBank The Ohio State University et al. 2019-X - - - 1200 - EN\nMedNLI University of Massachusetts Lowell et al. 2018-8 11232 1395 1422 14049 - EN\nMultiNLI New York University 2018-6 392702 19647 - 412349 - EN\nOCNLI Indiana University et al. 2020-10 50K 3K 3K 56K CC-BY-NC-2.0 ZH\nRTE The PASCAL Recognising Textual Entailment Challenge - 2.49K 277 3K 5.77K CC-B Y-4.0 EN\nSNLI Stanford Linguistics et al. 2015-8 550152 10000 10000 570152 CC-BY-SA-4.0 EN\nWANLI University of Washington et al. 2022-1 102885 - 5000 107885 CC-BY-4.0 EN\n64https://www.luge.ai/#/luge/dataDetail?id=39\n71\n6.3 Math\nMathematical assignments commonly involve standard mathematical calculat ions,\ntheorem validations, and mathematical reasoning tasks, among others. Thes e tasks\naim to investigate the latent capabilities of models within the field of mathematics.\nDatasets related to mathematical tasks vary in difficulty. GSM8K (\nCobbe et al ,\n2021), ASDiv ( Miao et al , 2021), Math23K ( Wang et al, 2017), and Ape210K ( Zhao\net al, 2020) only contain primary school mathematical calculations, which are relatively\nsimple for humans. MATH (Hendrycks et al, 2021d) targets mathematical competition\nproblems, which are more challenging and also examine the models’ abil ity to follow\nthinking chains when solving problems. NaturalProofs ( Welleck et al, 2021) involves\nmathematical proposition proofs, axiom inferences, and so on.\nTable 18 Summary of Math Datasets Information. “Train Size,” “Dev Size,” “Test\nSize,” and “All Size” provide statistics on the respective questi on quantities in the\ndataset. Language: “EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nApe210K Yuanfudao AI Lab et al. 2020-9 200488 5000 5000 210488 - ZH\nAQUA-RAT DeepMind 2017-7 100949 250 250 101499 Apache-2.0 EN\nASDiv Institute of Information Science 2021-6 - - - 2305 CC-BY-NC-4.0 EN\nGSM8k OpenAI 2021-10 7.5K - 1K 8.5K MIT EN\nMATH UC Berkeley et al. 2021-3 7500 - 5000 12500 MIT EN\nMathQA University of Washington et al. 2019-5 29837 4475 2985 37297 Apache-2.0 EN\nMath23K Tencent AI Lab 2017-9 - - - 23161 MIT ZH\nNaturalProofs University of Washington et al. 2021-4 - - - 80795 MIT EN\nSVAMP Microsoft Research India 2021-3 - - - 1000 MIT EN\n6.4 Coreference Resolution\nThe core objective of tasks related to coreference resolution is the i dentification of ref-\nerential relationships within texts. Pronouns, noun phrases, or alte rnative expressions\nare occasionally employed in textual passages to refer to entities in troduced earlier.\nThis task entails the recognition of entities referred to by differe nt segments of the\ntext and is a fundamental research area in the field of NLP.\nWiC (\nPilehvar and Camacho-Collados , 2019) and CLUEWSC2020 ( Xu et al ,\n2020b) are coreference resolution datasets in the English and Chinese domains, respec-\ntively, used to determine whether words in different sentences have the same referential\nmeaning. WSC (Levesque et al, 2012) does not require comparison but rather demands\nthe specific content to which words refer. WinoGrande (Sakaguchi et al, 2021) adjusts\nthe WSC dataset by redesigning the task in a fill-in-the-blank format . WinoWhy\n(Zhang et al, 2020a) extends the WSC dataset by introducing a new task of explaining\nreferential relationships.\n6.5 Sentiment Analysis\nThe sentiment analysis task, commonly known as emotion classification, seeks to ana-\nlyze and deduce the emotional inclination of provided texts, commonly categorized as\npositive, negative, or neutral sentiments. This task finds practical utility in diverse\n72\ndomains, including social media monitoring, product review analysi s, and market\nresearch.\nClassic sentiment analysis datasets include IMDB (Maas et al, 2011), Sentiment140\n(Go et al, 2009), SST-2 (Socher et al, 2013), and EPRSTMT (Xu et al, 2021). The tex-\ntual content of these datasets originates from real-life scenarios such as movie reviews,\nproduct reviews, and tweet content, hence possessing diversity and authenticity. Each\nsample is manually labeled as expressing either positive or negative sentiment based\non the emotions conveyed in the text.\nTable 19 Summary of Coreference Resolution Datasets Information. Release\nTime: “X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,” and “All\nSize” provide statistics on the respective question quantities in the dataset. Language:\n“EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nCLUEWSC2020 CLUE team 2020-12 1244 304 290 1838 - ZH\nDPR University of Texas at Dallas 2012-7 1322 - 564 1886 - EN\nWiC University of Cambridge 2018-8 5428 638 1400 7466 CC-NC-4.0 EN\nWinoGrande AI2 et al. 2019-7 63238 1267 1767 66272 CC-BY EN\nWinoWhy HKUST 2020-7 - - - 43972 MIT EN\nWSC University of Toronto et al. 2012-X - - 285 285 CC-BY-4.0 EN\nTable 20 Summary of Sentiment Analysis Datasets Information. Release Time:\n“X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,” an d “All Size”\nprovide statistics on the respective question quantities in thedataset. Language: “EN”\nindicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nEPRSTMT CLUE team 2021-7 32 32 1363 20992 - ZH\nIMDB Stanford University 2011-6 25000 - 25000 50000 - EN\nSentiment140 Stanford University 2009-X 1600000 - 359 1600359 - EN\nSST-2 Stanford University 2013-10 67349 872 1821 70042 - EN\n6.6 Semantic Matching\nThe task of semantic matching entails evaluating the semantic similar ity or degree\nof correspondence between two sequences of text. Models must grasp the semantic\ninformation within the text to perform tasks such as assessing text s imilarity, match-\ning sentences, and determining semantic relationships. This ta sk is widely applied in\ndomains such as information retrieval and dialogue systems.\nMRPC (\nDolan and Brockett, 2005), QQP (Wang et al, 2018), and PAWS (Zhang\net al, 2019) are commonly used English semantic matching datasets, used for determin-\ning semantic similarity at the sentence level. AFQMC ( Xu et al, 2020b) and LCQMC\n(Liu et al , 2018) are commonly used large-scale Chinese datasets. Specifically, the\nLCQMC dataset is more inclined towards matching the intent of questi ons rather\nthan semantic matching. To address the lack of other languages, PAWS-X (Yang et al,\n2019) translates the PAWS dataset into 6 other languages. The most notable is th e\n73\nSTSB dataset (Cer et al, 2017), which not only includes 10 languages but also employs\ncontinuous similarity scores as labels rather than simple binary label s.\nTable 21 Summary of Semantic Matching Datasets Information. Release Time:\n“X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,” an d “All Size”\nprovide statistics on the respective question quantities in thedataset. Language: “EN”\nindicates English, “ZH” indicates Chinese, “Multi” indicates Mult ilingual, and the\nnumber in parentheses indicates the number of languages included\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nAFQMC CLUE team 2020-12 34.3K 4.3K 3.9K 42.5K - ZH\nBQ Harbin Institute of Technology et al. 2018-10 100000 10000 10000 120000 - ZH\nBUSTM CLUE team 2021-7 32 32 3772 8087 - ZH\nDuQM Baidu Inc. et al. 2021-9 - - - 10121 Apache-2.0 ZH\nLCQMC Harbin Institute of Technology et al. 2018-8 238766 8802 12500 260068 CC-BY-4.0 ZH\nMRPC Microsoft Research 2005-X 4076 - 1725 5801 - EN\nPAWS Google AI Language 2019-6 49401 8000 8000 65401 - EN\nPAWS-X Google Research 2019-8 296406 11815 11844 320065 - Multi (6)\nQQP New York University et al. 2018-11 364K - - 364K - EN\nSTSB Google Research et al. 2017-8 5749 1500 1379 8628 - Multi (10)\n6.7 Text Generation\nThe scope of text generation tasks is broad, encompassing the generation of c ontent\nsummaries or dialogues. In a specific context, we narrow down the defin ition of text\ngeneration tasks to differentiate them from tasks like text summarization and transla-\ntion. The narrow definition of text generation tasks is bound by provide d content and\nspecific requirements. It involves utilizing benchmark data, su ch as descriptive terms\nand triplets, to generate corresponding textual descriptions.\nThe first form involves generating sentences in a colloquial manner using\nspeciﬁc words . CommonGen (\nLin et al , 2020) and E2E ( Novikova et al, 2017) task\nmodels with generating coherent sentences related to given vocabu lary terms. The\nsecond form involves mapping structured data to text . DART (Nan et al , 2021)\nand WebNLG (Gardent et al , 2017) input structured data as triples to the model to\nobtain relevant descriptive sentences.\nTable 22 Summary of Text Generation Datasets Information. “Train Size,”\n“Dev Size,” “Test Size,” and “All Size” provide statistics on the res pective question\nquantities in the dataset. Language: “EN” indicates English, “RU” indicat es Russian\nDataset Publisher Release Time Train Size Dev Size Test Size All Siz e License Language\nCommonGen University of Southern California et al. 2019-11 67389 4018 1497 72904 MIT EN\nDART Yale University et al. 2020-7 30526 2768 6959 40253 MIT EN\nE2E Heriot-Watt University 2017-6 42061 4672 4693 51426 CC-BY-SA-3.0 EN\nWebNLG LORIA et al. 2017-7 49665 6490 7930 64085 CC-BY-NC-SA-4.0 EN & RU\n6.8 Text Translation\nText translation involves transforming text from one language to another. M odels\nmust adeptly grasp the meaning of the source language text and produce equ ivalent\ntext that conforms to the grammar and context of the target language.\n74\nWMT65 is one of the most commonly used text translation datasets. It aggregates\ndata from the Workshop on Statistical Machine Translation competition, with a large-\nscale dataset covering a wide range of languages. NLLB ( Costa-juss` a et al, 2022)\nprovides open-access to three text translation evaluation benchmark s, offering high-\nquality translations in over 200 languages, including many low-resource languages.\nIWSLT 2017 (Cettolo et al, 2017) is also representative and commonly used for training\nand evaluation in translation tasks.\nTable 23 Summary of Text Translation Datasets Information. “Train Size,” “Dev\nSize,” “Test Size,” and “All Size” provide statistics on the respec tive question quan-\ntities in the dataset. Language: “Multi” indicates Multilingual, and the number in\nparentheses indicates the number of languages included\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nIWSLT 2017 FBK et al. 2017-12 1108475 4442 41921 1154838 CC-BY-NC-ND-4.0 Multi (11)\nNLLB NLLB Team et al. 2022-7 - - - - MIT Multi\nWMT ACL et al. - - - - - - Multi\n6.9 Text Summarization\nThe task of text summarization pertains to the extraction or generation of a brief\nsummary or headline from an extended text to encapsulate its primary c ontent. Sum-\nmaries are expected to retain the pivotal information from the original text, effectively\nconveying its fundamental ideas, while headlines demand brevity an d inclusiveness.\nNews is the most common source for text summarization datasets. CNN-DM (\nSee\net al , 2017) utilizes a large number of news articles to create tens of thousands of\narticle-summary pairs. Compared to the CNN-DM dataset, XSum ( Narayan et al ,\n2018) has shorter text content and richer vocabulary. In addition to obtaini ng data\nsamples from various news sources, SAMSum ( Gliwa et al, 2019), Opinion Abstracts\n(Wang and Ling , 2016), LCSTS ( Hu et al , 2015), MediaSum ( Zhu et al , 2021),\nand AESLC ( Zhang and Tetreault, 2019) respectively focus on real dialogues, movie\nreviews, social media texts, interview transcripts, and emails . This ensures that dif-\nferent text summarization datasets have diverse styles of content an d do not become\noverly homogeneous.\n6.10 Text Classiﬁcation\nText classification tasks aim to assign various text instances to predefi ned categories,\ncomprising text data and category labels as pivotal components. Sentimen t analysis\nand semantic matching, previously mentioned, are encompassed within the domain of\ntext classification. Due to the unique nature of these tasks and their frequent explo-\nration as standalone subtasks by researchers, this paper provides sep arate summaries\nfor sentiment analysis, semantic matching, and text classification.\nAGNEWS (\nZhang et al, 2015) and TNEWS (Xu et al, 2020b) evaluate models’ clas-\nsification performance on English and Chinese news topics, respectiv ely. They involve\n65https://www.statmt.org/wmt22/index.html\n75\na relatively small number of categories, not exceeding 15. CSLDCP ( Xu et al , 2021)\nrequires models to classify Chinese literature disciplines, expanding the categories to\n67. IFLYTEK (Xu et al, 2020b) categorizes descriptive text based on app functionality\nfor model classification, with an astonishing 119 categories.\nTable 24 Summary of Text Summarization Datasets Information. “Train Size,”\n“Dev Size,” “Test Size,” and “All Size” provide statistics on the res pective question\nquantities in the dataset. Language: “EN” indicates English, “ZH” indicate s Chinese,\n“Multi” indicates Multilingual, and the number in parentheses ind icates the number\nof languages included\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nAESLC Yale University et al. 2019-7 14436 1960 1906 18302 CC-BY-NC-SA-4.0 EN\nCNewSum ByteDance 2021-10 275596 14356 14355 304307 Apache-2.0 ZH\nCNN-DM Stanford University et al. 2017-4 287113 13368 11490 311971 Apache-2.0 EN\nGigaword Facebook AI Research et al. 2015-9 3803957 189651 1951 3995559 MIT EN\nLCSTS Harbin Institute of Technology 2015-6 2400000 10000 1000 2411000 CC-BY-4.0 ZH\nMediaSum Microsoft Cognitive Services Research Group 2021-3 443596 10000 10000 463596 - EN\nMultiNews Yale University 2019-7 44972 5622 5622 56216 - EN\nNewsroom Cornell University 2018-6 995041 108837 108862 1212740 - EN\nOpinion Abstracts Northeastern University et al. 2016-6 5990 - - 5990 - EN\nSAMSum Samsung R&D Institute Poland 2019-11 14732 818 819 16369 CC-BY-NC-ND-4.0 EN\nWikiHow University of California 2018-10 - - - 230K CC-BY-NC-SA EN\nWikiLingua Columbia University et al. 2020-10 - - - 770087 CC-BY-3.0 Multi (18)\nXL-Sum BUET et al. 2021-8 1122857 114198 114198 1351253 CC-BY-NC-SA-4.0 Multi (45)\nXSum University of Edinburgh 2018-10 204045 11332 11334 226711 MIT EN\nTable 25 Summary of Text Classiﬁcation Datasets Information. Release Time:\n“X” indicates unknown month. “Train Size,” “Dev Size,” “Test Size,” an d “All Size”\nprovide statistics on the respective question quantities in thedataset. Language: “EN”\nindicates English, “ZH” indicates Chinese, “Multi” indicates Mult ilingual, and the\nnumber in parentheses indicates the number of languages included\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nAGNEWS New York University 2015-9 120000 - 7600 127600 - EN\nCSLDCP CLUE team 2021-7 536 536 4783 23966 - ZH\nIFLYTEK CLUE team 2020-12 12.1K 2.6K 2.6K 17.3K - ZH\nMARC Amazon et al. 2020-11 1200000 30000 30000 1260000 - Multi (6)\nTHUCNews Tsinghua University 2016-X - - - 1672165 MIT ZH\nTNEWS CLUE team 2020-11 53.3K 10K 10K 73.3K - ZH\n6.11 Text Quality Evaluation\nThe task of text quality evaluation, also referred to as text correction , involves the\nidentification and correction of grammatical, spelling, or language usage errors in text.\nThis task is akin to a teacher correcting writing errors made by stud ents.\nCoLA (\nWarstadt et al, 2019) is used to evaluate models’ ability to judge the gram-\nmatical correctness of English sentences, which can be seen as a binar y classification\ntask. In contrast, SIGHAN ( Wu et al , 2013; Yu et al , 2014; Tseng et al , 2015) and\nYACLC (Wang et al, 2021b) require models to proofread and correct Chinese spelling\nand grammar, presenting greater difficulty. Different from these two d atasets, CSCD-\nIME (Hu et al, 2022b) is the first Chinese spelling correction dataset caused by errors\nin Pinyin input method, with different sources and distribution s of errors.\n76\nTable 26 Summary of Text Quality Evaluation Datasets Information. “Train\nSize,” “Dev Size,” “Test Size,” and “All Size” provide statistics on the respective\nquestion quantities in the dataset. Language: “EN” indicates English, “ZH” i ndicates\nChinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nCoLA New York University 2018-5 8511 1043 - 9554 CC-BY-4.0 EN\nCSCD-IME Tencent Inc 2022-11 30000 5000 5000 40000 MIT ZH\nSIGHAN Chaoyang Univ. of Technology et al. - 6476 - 3162 9638 - ZH\nYACLC Beijing Language and Culture University et al. 2021-12 8000 1000 1000 10000 - ZH\n6.12 Text-to-Code\nThe Text-to-Code task involves models converting user-provide d natural language\ndescriptions into computer-executable code, thereby achievin g the desired functional-\nity or operation. Common subtasks include the generation of SQL query stat ements\nand generating code for different programming languages.\nFor example, MBPP (\nAustin et al, 2021) serves as a benchmark comprising Python\nprogramming problems, assessing models’ proficiency in Python programming. On the\nother hand, DuSQL ( Wang et al, 2020a), CSpider ( Min et al , 2019), and Spider ( Yu\net al , 2018) are applied in the Text-to-SQL task. They require models to gener ate\ncorresponding SQL query statements from given databases based on quest ions.\nTable 27 Summary of Text-to-Code Datasets Information. “Train Size,” “Dev\nSize,” “Test Size,” and “All Size” provide statistics on the respec tive question quanti-\nties in the dataset. Language: “EN” indicates English, “ZH” indicates Chine se, “PL”\nindicates Programming Language\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze License Language\nCSpider Westlake University 2019-11 - - - 10181 CC-BY-SA-4.0 ZH & PL\nDuSQL Baidu Inc. et al. 2020-11 18602 2039 3156 23797 - ZH & PL\nMBPP Google Research 2021-8 - - 974 974 - EN & PL\nSpider Yale University 2018-9 - - - 10181 CC-BY-SA-4.0 EN & PL\n6.13 Named Entity Recognition\nThe Named Entity Recognition (NER) task aims to discern and categorize nam ed\nentities within a given text. Models are tasked with pinpointing entities, assigning\nthem to predefined categories, and indicating their respective positions. These entities\nmay include personal names, organizational names, geographic locations, date s, and\nother categories.\nCoNLL2003 (\nTjong Kim Sang and De Meulder , 2003) is a classic benchmark\ndataset in the field of NER. It categorizes entity types into 4 classes. OntoNotes 5.0\n(Weischedel et al, 2012) expands into an NER task dataset based on the corpus and\nprovides 18 entity types. Subsequently, WUNT2017 ( Derczynski et al , 2017) focuses\non models’ ability to recognize emerging named entities in new conte xts within the\nNER task. Youku NER ( Jie et al , 2019), Taobao NER ( Jie et al , 2019), and Weibo\nNER ( Peng and Dredze , 2015) are constructed for the entertainment, e-commerce,\nand social media domains, respectively, providing corresponding text-entity pairs.\n77\nTable 28 Summary of Named Entity Recognition Datasets Information. “Train\nSize,” “Dev Size,” “Test Size,” and “All Size” provide statistics on the respective\nquestion quantities in the dataset. “NEC” indicates Number of Entity C ategories.\nLanguage: “DE” indicates German, “EN” indicates English, “ZH” indicates Chi nese,\n“Multi” indicates Multilingual, and the number in parentheses ind icates the number\nof languages included\nDataset Publisher Release Time Train Size Dev Size Test Size All Size NEC License Language\nCLUENER CLUE Organization 2020-1 10748 1343 1345 13436 10 - ZH\nCoNLL2003 University of Antwerp 2003-6 14041 3250 3453 20744 4 - EN & DE\nFew-NERD Tsinghua University et al. 2021-5 - - - 188200 66 CC-BY-SA-4.0 EN\nMSRA University of Chicago 2006-7 46364 - 4365 50729 3 CC-BY-4.0 ZH\nOntoNotes 5.0 Boston Childrens Hospital and Harvard Medical School et al. 2013-10 59924 8528 8262 76714 18 - Multi (3)\nResume Singapore University of Technology and Design 2018-7 3821 463 477 4761 8 - ZH\nTaobao NER Singapore University of Technology and Design et al. 2019-6 6000 998 1000 7998 9 - ZH\nWeibo NER Johns Hopkins University 2015-9 1350 269 270 1889 4 CC-BY-SA-3.0 ZH\nWUNT2017 Johns Hopkins University et al. 2017-9 3394 1009 1287 5690 6 CC-BY-4.0 EN\nYouku NER Singapore University of Technology and Design et al. 2019-6 8001 1000 1001 10002 9 - ZH\n6.14 Relation Extraction\nThe endeavor of Relation Extraction (RE) necessitates the identification of connections\nbetween entities within textual content. This process typically includes recognizing and\nlabeling pertinent entities, followed by the determination of th e specific types of rela-\ntionships that exist among them. As an illustration, the Forbidden Cit y (geographic\nlocation) is positioned in (type of relationship) Beijing (geographic l ocation).\nDialogue RE (\nYu et al , 2020a) is the first entirely human-annotated dataset for\ndialogue RE, comprising 36 types of relationship found in real dialogues. I n contrast\nto sentence-level datasets, DocRED ( Yao et al, 2019) is constructed for RE tasks at\nthe document level. Models are required to aggregate document infor mation to infer\nrelationships between entities. FewRel ( Han et al , 2018) is the first to combine few-\nshot learning with relation extraction, and in its 2.0 version, it addit ionally evaluates\nmodels’ OOD capability.\nTable 29 Summary of Relation Extraction Datasets Information. “Train Size,”\n“Dev Size,” “Test Size,” and “All Size” provide statistics on the res pective ques-\ntion quantities in the dataset. “NRC” indicates Number of Relationship C ategories.\nLanguage: “EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Si ze NRC License Language\nDialogue RE Tencent AI Lab et al. 2020-7 6100 2034 2034 10168 36 - EN & ZH\nDocRED Tsinghua University et al. 2019-7 1546589 12332 12842 1571763 96 MIT EN\nFewRel Tsinghua University 2018-10 - - - 70000 100 CC-BY-SA-4.0 EN\nTACRED Stanford University 2017-9 68124 22631 15509 106264 42 LDC EN\n6.15 Multitask\nMultitask datasets hold significance as they can be concurrently utili zed for different\ncategories of NLP tasks. Creators commonly manipulate the same batch of textual\ndata through various configurations, transformations, and annotations to produ ce\ntraining or evaluation data for diverse NLP tasks, exemplifying the conc ept of “one\ndataset, multiple applications.”\n78\nFor example, CSL (Li et al, 2022b) contains a vast amount of information such as\npaper titles, abstracts, keywords, etc., which can be simultaneously applied to various\nNLP tasks such as title prediction, keyword generation, paper classificat ion, and so\non. QED ( Lamm et al , 2021) extends the Natural Questions dataset ( Kwiatkowski\net al, 2019) by adding explanatory annotations and extends to different tasks such as\nsentence selection, equivalence recognition, etc. METS-CoV (Zhou et al, 2022) collects\nsocial media texts related to COVID-19, which are annotated by creators an d used in\nNER and sentiment analysis tasks.\nTable 30 Summary of Multitask Datasets Information. “Train Size,” “Dev Size,”\n“Test Size,” and “All Size” provide statistics on the respective qu estion quantities in\nthe dataset. Language: “EN” indicates English, “ZH” indicates Chinese\nDataset Publisher Release Time Train Size Dev Size Test Size All Size License Language\nCSL School of Information Engineering et al. 2022-9 - - - 396209 Apache-2.0 ZH\nMETS-CoV Zhejiang University et al. 2022-9 - - - - Apache-2.0 EN\nQED Stanford University et al. 2021-3 7638 1355 - 8993 CC-BY-SA-3.0 & GFDL EN7 Challenges and Future Directions\nThis section primarily elaborates on the existing challenges and futu re directions\nfrom four aspects: pre-training corpora, fine-tuning instruction dat asets, preference\ndatasets, and evaluation datasets.\n7.1 Pre-training Corpora\nThe construction and open sourcing of pre-training corpora have exper ienced sig-\nnificant growth recently, with increasing emphasis on their qualit y by researchers.\nHowever, pre-training corpora still face challenges and shortcomings t hat not only\nimpact the performance of models but also involve ethical and societal issues. Below,\nwe briefly explore the challenges existing in current pre-traini ng corpora and discuss\nfuture development directions.\nData Selection. Research indicates that the diversity of data is crucial, and a\nricher variety of domains is preferable (\nLongpre et al, 2023c). It is worth investigating\nhow to make the content of pre-training corpora as diverse as possible. Currently,\nthe majority of pre-training corpora are composed of web-scraped data, and the data\ntypes are not entirely comprehensive. There is a risk of excessiv e focus on popular\ncontent, resulting in category imbalance. This can lead to a severe lac k of knowledge\nin certain domains, necessitating the subsequent collection of data for incremental\npre-training. Moreover, the scale of English data is much larger than th at of other\nlanguages, which can result in insufficient knowledge of other languages and poor\nperformance of models in cross-language tasks. Therefore, data selecti on is a nuanced\nart. First, larger-scale, more diverse, and more broadly sourced pre-training cor pora\ncovering multiple languages and domains with better proportional repre sentation will\nbe a future trend. Therefore, choices and configurations regarding data scale, data\nsources, domain coverage, data proportions, and language distribution need to be\n79\ncarefully considered. Secondly, data will be subdivided into finer categories, similar\nto the further categorization of books in Figure 4, to better measure the breadth of\nthe corpora, facilitating improved data selection. Thirdly, there will be a gradual\nexploration of whether the addition of synthetic data is effective for t he pre-training\nof models. Fourthly, many vertical domains lack open-source relevant data, such as\nin the fields of ancient texts or ethnic cultures.\nTimeliness. Currently, the coverage time of most pre-training corpora is rel-\natively outdated, lacking recent knowledge and making it challenging to achieve\nperiodic updates. This results in inaccurate generation or outdated i nformation and\nbeing unable to respond to recent content. Common Crawl, for instanc e, continu-\nally crawls the latest webpage data, but the majority is in English. Ot her types\nof data require reacquisition and preprocessing when updates are ne eded. In the\nfuture, dynamic and automatic updates of pre-training corpora , as well as\nself-learning capabilities of LLMs regarding new knowledge , will be crucial\nresearch directions.\nQuality Assessment. Longpre et al (2023c) conducts evaluations on The Pile\n(Gao et al , 2020) and C4 ( Raffel et al , 2020), exploring potential features of the\ndata using different data integration methods. Lee et al (2023a) designs the Task2Vec\nmetric to measure the diversity of data. However, a systematic methodology for quality\nassessment has not yet been established. Most studies only assess sp ecific aspects of\nthe corpora. Questions about what makes a pre-training corpus of higher qu ality, how\nthe quality of pre-training corpora should be compared, and what consti tutes a more\ncomprehensive quality evaluation remain largely unresolved.\nData Preprocessing. Each pre-training corpus has a unique preprocessing\npipeline and methods, with some specific details yet to be disclos ed. This gives rise\nto two issues. First, there is a lack of a unified framework and standar dized processes\nfor data preprocessing. The effectiveness of existing methods is sometimes challenging\nto assess. Second, Longpre et al (2023c), through experiments, demonstrated that the\nmore harmful content is filtered out from pre-training data, the less h armful infor-\nmation the model generates, but its discrimination ability also weake ns. Filtering out\nlow-quality data too extensively reduces the diversity of the d ata. While enhancing\ndiscrimination ability, it may lead to the generation of more harmful in formation by\nthe model. Whether a cleaner corpus is necessarily better and whether a small amount\nof harmful information and low-quality data can bring benefits are questions that need\nto be explored in the future. Determining the optimal extent of dat a cleaning is also\na topic for future research.\nBuilding the Ecosystem of Pre-training Corpora. Due to the rapid develop-\nment of LLMs, a comprehensive ecosystem for pre-training corpora has not yet been\nestablished within the community. There is a lack of standards for data preprocessing,\nno systematic evaluation schemes for data, no established standards for the release of\nrelevant data, and currently, there is no unified management and maintenance of data.\nGiven these circumstances, there is still a long way to go in buil ding the ecosystem\nfor pre-training corpora.\n80\n7.2 Instruction Fine-tuning Datasets\nDuring the instruction fine-tuning phase, creating high-quality datasets is crucial for\ndriving model performance and expanding application domains. Several challenges\ncurrently pose tests to the future development of instruction fin e-tuning datasets.\nBelow, we briefly explore the challenges existing in current ins truction fine-tuning\ndatasets and look ahead to future directions.\nSubdivision of Instruction Categories. In the majority of instruction fine-\ntuning datasets, instructions of various categories are mixed together w ithout speci-\nfying the corresponding task types and associated domains for each inst ruction. For\ninstance, in the classic Alpaca\ndata dataset ( Taori et al, 2023), each instruction con-\nsists of “instruction,” “input,” and “output” parts without category annot ations.\nThis makes it challenging to adjust the distribution of categories in t he instruc-\ntion fine-tuning dataset to enhance the performance of specific tasks or to add\nand simplify instructions. Additionally, while datasets like Fire fly (Yang, 2023) and\nBELLE train 3.5M CN (BELLEGroup, 2023) have added a field for instruction cat-\negories, they suffer from issues such as incomplete or overly broad cate gories. Taking\nthe “code” category as an example, instructions could be further subdivided into more\ngranular categories like “code correction,” “code generation” and “code improvement.”\nTherefore, in the future, a more ﬁne-grained category subdivision in datasets\nshould become a standard, allowing users to better understand the o verall\ncomposition and facilitating dataset optimization . Of course, this may intro-\nduce challenges such as difficulty in standardizing category subdivisions and increased\nannotation costs and time.\nDomain Scarcity. The majority of datasets focus on general domains, with\ndatasets in vertical domains mostly concentrated in common areas such as healthcare,\nfinance, and law. This results in a scarcity of instruction datasets for low-resource and\nniche domains, potentially limiting the performance improvement of models in cer-\ntain specialized fields. For instance, in fields like traditional Chinese classics, antiques,\nor niche areas such as paleobiology, funeral studies, and minority languages. Con-\nstructing corresponding datasets for these domains not only systemati cally\nintegrates knowledge but also allows the application of trained LLMs in\nspeciﬁc ﬁelds, serving as auxiliary tools with societal signiﬁcanc e and value .\nQuality Evaluation. The quality evaluation of instruction fine-tuning datasets is\na complex and subjective issue, and currently, there are no clear, universal standards\nor methods. In practice, quality evaluation may involve multiple asp ects, including\nbut not limited to: (1) Model Performance Evaluation . Assessing the perfor-\nmance of the fine-tuned model on evaluation datasets. The selected evaluation datasets\nshould be diverse and reasonable to avoid evaluation contamination (\nZhou et al ,\n2023b). (2) Annotation Consistency and Rationality . Evaluating the consistency\namong different annotators regarding instructions and the rationality and correct-\nness of instruction input and answer output. (3) Bias Analysis . Assessing biases\nand harmful content in the dataset to ensure the model is not adversel y affected. (4)\nTimeliness Detection . Regularly checking whether the content of instructions in\nthe dataset has become outdated or inaccurate. (5) Subjective Evaluation . Man-\nually conducting subjective scoring and inspection. In conclusion , future efforts may\n81\ninvolve establishing more explicit evaluation standards and metrics, creating a unified\nevaluation framework to make it more scientifically objective.\nLegal and Ethical Risks. Longpre et al (2023b) research on instruction fine-\ntuning datasets has revealed that an increasing number of datasets are tr eated as\nwholes rather than a series of sources, undergoing multiple repackagin gs and reautho-\nrizations without sufficient labeling of data sources and copyright infor mation. This\nleads to issues such as data leakage and biased behavior, posing legal and ethical risks.\nTherefore, there is a current need to enhance the transparency of datasets,\nimprove quality and ethical compliance, and reduce potential problem s.\nLongpre et al (2023b) provides a dataset audit and data provenance explorer tool to\naddress this. In the future, establishing standards for dataset usage is a focal point of\nconcern.\n7.3 Preference Datasets\nThe significance of preference datasets lies in providing crucial t raining data for the\nmodels’ output decisions. Below, we briefly discuss the challen ges currently faced by\npreference datasets and look forward to future directions.\nLimited Availability of Resources. RLHF has been widely researched and\napplied by leading industry companies such as OpenAI, Anthropic, Googl e, etc.\nHowever, due to the lack of high-quality, publicly available prefer ence datasets, the\nopen-source community is still lagging in the research and practice of RLHF (\nCui et al,\n2023). Currently, there are not many open-source preference datasets, an d the major-\nity are in English. Non-English and domain-specific preference dataset s are extremely\nscarce. One reason for the scarcity of resources is the relatively cumbersome annotation\nprocess and the high cost involved. Therefore, exploring weakly sup ervised learning\nmethods, using simple labels such as user clicks, support amounts , instead of man-\nual annotation, or leveraging high-quality models like GPT-4 to assist i n voting and\nscoring, could be attempted. On the other hand, there is lower attent ion to preference\ndatasets in other languages and vertical domains, leading to fewer related efforts.\nPreference Evaluation Method Settings. The most commonly used prefer-\nence evaluation method is still the voting method, but many prefer ence datasets lack\nstrict and uniform evaluation standards, providing feedback informat ion only from\na single dimension. Human preferences in the real world are diverse, and to more\ncomprehensively and high-quality reflect them, corresponding stan dards need to be\nestablished to reduce subjective differences and conduct fine-gr ained evaluations from\nmultiple dimensions (Cui et al, 2023). Employing various evaluation methods for com-\nprehensive assessments is recommended. Defining these standard s is a complex issue.\nAdditionally, preference datasets often do not provide explicit re asons for why some\nanswers are more favored by humans, introducing uncertainty into the model learning\nprocess. Therefore, it is advisable to include textual explanation s in preference evalu-\nations, stating the reasons for the assessment and providing suggestions for improving\nthe responses. The construction of UltraFeedback ( Cui et al, 2023) is relatively more\nscientifically standardized, playing a positive role in fostering future developments.\n82\n7.4 Evaluation Datasets\nEvaluation datasets play a crucial role in ensuring the reliability, practicality, and\nsafety of LLMs. They provide researchers and practitioners with insi ghts into the\nstrengths and weaknesses of LLMs, facilitating continuous improvemen ts and opti-\nmizations. The following discussion highlights the challenges within current evaluation\ndatasets and suggests potential directions for future development.\nEstablishment of Evaluation Datasets. When creating an evaluation dataset\nfor a particular domain, several essential factors must be considered. (1) Data\nsources. There is a growing emphasis on evaluating the fairness and reliabilit y of\ndatasets (\nAiyappa et al , 2023), with particular attention to the risk of data pol-\nlution or leakage during assessments ( Zhou et al , 2023b). Zhou et al (2023b) has\nidentified instances where LLMs unintentionally learned from evaluat ion data during\npre-training or prompt fine-tuning, resulting in inflated evaluation scores and dimin-\nished generalization ability. To mitigate this, dataset providers should disclose training\ndata compositions and provide detailed information about data sources to p revent\ncontamination. Consequently, beyond publicly disclosing the compos ition of training\ndata to avoid inappropriate selection of evaluation datasets, providers of evaluation\ndatasets must furnish detailed data source information and assess the risks of data con-\ntamination. Whenever possible, data sources should consist of artificially generated or\nnon-public data to ensure fair evaluations. The challenge of minimizing data pollution\nor leakage remains an open problem. (2) Question design. Various factors, including\nscale, question types, and topic distribution, should be considere d when developing\nevaluation datasets. Achieving overall enhancement requires exten sive research and\npractical application. Initially, the scale of the evaluation dataset should be determined\nbased on specific evaluation content, emphasizing high-quality questions, diverse ques-\ntion types, and an evenly distributed array of topics before gradually e xpanding and\nregularly updating the evaluation dataset. This approach resembles Chi nese Gaokao,\nwhere refined questions assess the mastery of comprehensive knowl edge. Additionally,\nsetting a reasonable difficulty level is crucial. Evaluation tasks shou ld largely surpass\nthe current capabilities of LLMs, establishing an appropriate upper and lower limit.\nWithout a good design of evaluation benchmarks, many models achieving scores above\n95% are relatively unhelpful for advancing LLMs ( Sawada et al, 2023).\nAddressing Evaluation Gaps. Persistent gaps in the evaluation landscape\nrequire researchers’ attention to refine the evaluation framework. (1) Evaluating in\nlow-resource domains. Evaluative datasets in certain domains are in nascent stages\nof development, such as the e-commerce domain (Li et al, 2023m), and the geoscience\ndomain ( Deng et al , 2023); while certain domains lack pertinent evaluation bench-\nmarks temporarily, including the domain of ancient literature, cult ural artifacts, tea\nculture, etc. (2) Evaluating in other languages. Beyond the predominantly fea-\ntured English and Chinese datasets, resources for evaluations in other languages are\nlimited. (3) Multi-turn evaluations. The focus on single-turn assessments over-\nlooks LLMs’ capabilities in multi-turn interactions and contextual understanding. (4)\nDynamic evaluations. Many evaluative datasets employ static evaluation methods,\nintroducing two drawbacks. On one hand, the evaluation data is utili zed for training\n83\nto enhance ranking on leaderboards; on the other hand, the initial evaluat ion con-\ntent may gradually become inadequate for meeting the capabilities of LLMs , and the\nevaluated knowledge may become obsolete or erroneous ( Guo et al, 2023c).\nChoosing and Improving Evaluation Approaches. The limitations of code\nevaluation, especially for open-ended questions, require addressi ng. Manual evalua-\ntions, while in-depth, can be costly and subject to human bias. Thus , model-based\nscoring is emerging as a promising alternative, striving for scientific reliability and the\ngoal of fully automated evaluation processes.\nComprehensive Evaluation Framework. The complexity of selecting from\nnumerous datasets, the lack of standardized data formats, and the diversity in evalua-\ntion methodologies pose significant challenges. A comprehensive evaluation framework\ncould simplify the process by providing a central repository and an efficient, standard-\nized API for model invocation. This framework should fulfill three cr iteria: simplicity,\ncentralization, and efficiency. Firstly, the evaluation steps should be straightforward,\nrequiring only the provision of an API for model invocation. Secondly, a unified repos-\nitory should be available for selecting datasets spanning diverse dom ains and tasks.\nLastly, the evaluation process should be efficient, covering a broad range of dimensions\nto yield rapid results. Achieving this goal poses various challenges, with familiar frame-\nworks like the HELM evaluation framework (Liang et al, 2023) and the OpenCompass\nevaluation platform (Contributors, 2023) evolving in this direction.\n8 Conclusion\nIn the vast landscape of AI, Large Language Models (LLMs) stand out as rapidly grow-\ning, prominent features—akin to towering trees in a dense forest. T he datasets that\nfeed their growth and development can be compared to the vital root sys tem of these\ntrees, providing the sustenance that is essential for their perf ormance. Regrettably,\nthe current landscape of LLM-related datasets is extensive, with a lac k of cohesive\nsynthesis across the various types of datasets. Understanding the cur rent state and\nfuture trends of the LLM datasets presents a formidable challenge. The refore, this\nsurvey offers a comprehensive analysis of LLMs datasets, categorizing and s umma-\nrizing datasets associated with LLMs across five dimensions: pre-train ing corpora,\nfine-tuning instruction datasets, preference datasets, evaluation datasets, and tradi-\ntional NLP datasets. Alongside this categorization, we identify the curren t challenges\nand outline potential directions for future dataset development in fou r key areas: pre-\ntraining, fine-tuning instruction, reinforcement learning, and mo del evaluation. It is\nour hope that this survey will serve as a valuable point of reference forresearchers both\nin academia and industry, as well as newcomers and proficient practition ers engaged\nwith LLMs. Our ultimate objective is to continually refine LLMs datasets , to foster\na robust and standardized dataset ecosystem, as well as to support the pr ogressive\nadvancement of LLMs.\nAcknowledgements. This research was supported in part by National Key\nResearch and Development Program of China (2022YFC3301703) and NSFC (Grant\nNo.: 61936003).\n84\nAuthor contributions. Collation of datasets: YL, JC, CL, KD and LJ. Statistical\nanalysis of datasets: YL. Chart Creation: YL, JC and LJ. Drafting of the manuscri pt:\nYL and JC. Revision and supervision of the manuscript: YL, CL, KD and LJ.\nDeclarations\nCompeting interests. The authors declare no competing interests.\nConﬂict of interest. The authors declare that there are no conflicts of interest\nrelevant to this article.\nOpen access. This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distr ibution and repro-\nduction in any medium or format, as long as you give appropriate credit to theoriginal\nauthor(s) and the source, provide a link to the Creative Commons lice nce, and indi-\ncate if changes were made. The images or other third party material in thi s article are\nincluded in the article’s Creative Commons licence, unless indi cated otherwise in a\ncredit line to the material. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulati on or exceeds the\npermitted use, you will need to obtain permission directly from th e copyright holder.\nTo view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n85\nAppendix A Pre-training Corpus Information\nAppendix\nA provides detailed information on each pre-training corpus mentioned in\nthe main text.\nA.1 General Pre-training Corpora\nA.1.1 Webpages\n• CC-Stories (\nTrinh and Le , 2018). The CC-Stories corpus is approximately\n31GB in size. It is a subset extracted from Common Crawl. The selecte d text\naligns with the style of Winograd Schema stories, providing knowledge for models\nin commonsense reasoning and language modeling.\n• CC100 (Conneau et al , 2020). The CC100 corpus includes monolingual data\nfrom 100 languages. Its construction process involves processing URL and para-\ngraph indices based on the CC-Net repository, utilizing snapshots fromCommon\nCrawl spanning from January to December 2018.\n• CLUECorpus2020 (Xu et al, 2020c). The CLUECorpus2020 corpus is a large-\nscale Chinese corpus released by the CLUE organization, comprising 100GB of\nraw text and 35 billion Chinese characters. It is derived from the pr ocessing of\nChinese data in Common Crawl from July to December 2019.\n• Common Crawl\n66. The Common Crawl corpus is an extensive, unstructured,\nmultilingual dataset of webpages, encompassing over 8 years of web crawl er\ndata. The data is available in web archive, web archive transformation, and web\nextracted text formats. Many pre-training corpora are obtained through d ata\npreprocessing based on this corpus.\n• CulturaX (Nguyen et al, 2023). The CulturaX corpus is a multilingual corpus\ndeveloped for LLMs, covering 167 languages with a total of 6.3T tokens. It\nunderwent comprehensive cleaning and deduplication processes b ased on mC4\nand OACAR.\n• C4 (\nRaffel et al , 2020). The C4 corpus is constructed by obtaining snapshots\nof Common Crawl in April 2019 and extracting pure English text using mult i-\nple filters. C4 has a total of 5 variants, namely en, en.noclean, en.nobl ocklist,\nrealnewslike, and multilingual.\n• mC4 (Xue et al , 2021). The mC4 corpus consists of natural text in 108 lan-\nguages, serving as a multilingual extension of C4. The data is sourced from\nmultiple monthly web data snapshots from Common Crawl, providing a m ore\ndiverse linguistic range.\n• OSCAR 22.01 (\nAbadji et al, 2022). OSCAR is an open-source project aimed at\nproviding web-based multilingual resources. The project contin uously develops\nhigh-performance data processing pipelines to build multilingu al corpora. Cur-\nrently, there are four versions, including OSCAR 2019, OSCAR 21.09, OSCAR\n22.01, and OSCAR 23.01. This paper only lists OSCAR 22.01 as a representative\nexample.\n66https://commoncrawl.org/\n86\n• RealNews (Zellers et al , 2019b). The RealNews corpus is a large-scale corpus\nof news articles sourced from data in Common Crawl. The corpus focuses on\ncontent from the news domain indexed by Google News, with a time cover age\nspanning from December 2016 to April 2019.\n• RedPajama-V2 (Together, 2023). The RedPajama-V2 corpus comprises over\n100 billion text documents from 84 Common Crawl snapshots and has undergone\nprocessing using the CC-Net pipeline. Among them, 30 billion text s have been\nannotated with high-quality labels.\n• ReﬁnedWeb (\nPenedo et al , 2023). The RefinedWeb corpus is the English\npre-training dataset for the Falcon model. The full version of this cor pus con-\ntains 5TB tokens and has undergone rigorous filtering and extensive remov al of\nduplicate data on Common Crawl.\n• WuDaoCorpora-Text (Yuan et al , 2021). The WuDaoCorpora-Text corpus\nhas a pure text size of approximately 5TB, comprising over 50 industr y data\nlabels such as education and technology. The corpus has crawled a rich se t of\nChinese webpage data. Currently, 200GB of texts have been released as ope n\nsource.\nA.1.2 Languages Texts\n• ANC67. The ANC corpus includes textual records of various written and spoken\nmaterials in the United States since 1990. It is divided into the OANC (Op en\nAmerican National Corpus) and the MASC (Manually Annotated Sub-Corpus).\n• BNC68. The BNC corpus is jointly developed and established by institution s\nincluding Oxford University Press. It consists of 4124 representati ve texts of a\nwide range of modern British English, with a vocabulary exceeding 100M words.\nWritten language accounts for 90%, while spoken language makes up 10% of the\ncorpus.\n• News-crawl\n69. The News-crawl corpus comprises news texts in 59 different\nlanguages. The texts are crawled from online newspaper resources. The corpus is\nutilized for the Workshop on Machine Translation (WMT) series of sharedtasks.\nA.1.3 Books\n• Anna’s Archive70. The Anna’s Archive corpus claims to be the world’s largest\nopen-source and open-data library. It has currently gathered resource s from\nLibgen, Sci-Hub, Z-Library, and Internet Archive Controlled Digital Len ding.\n• BookCorpusOpen (Bandy and Vincent, 2021). The BookCorpusOpen corpus\nis a variant of Toronto Book Corpus. It comprises 17,868 book entries, with each\nentry containing a title and text. The titles represent the names of the books,\nwhile the text consists of the unprocessed content of the respect ive books.\n67https://anc.org/\n68http://www.natcorp.ox.ac.uk/\n69https://data.statmt.org/news-crawl/\n70https://annas-archive.org/datasets\n87\n• PG-19 (Rae et al , 2020). The PG-19 corpus selects 28,752 books from Project\nGutenberg published before 1919, totaling 11.74GB. The reason is to avoid being\naffected by international copyright issues.\n• Project Gutenberg 71. The Project Gutenberg corpus was established in 1971,\nmaking it the earliest digital library. The majority of the books with in it are\noriginal works of public domain literature, preserved over the long term through\ndigital archiving. As of July 2018, the collection comprised over 57K books.\n• Smashwords72. The Smashwords corpus is a platform for publishing e-books,\nand it has been in operation since 2008. It offers diverse book formats. Many\ncorpora source their book resources from Smashwords.\n• Toronto Book Corpus (\nZhu et al, 2015). The Toronto Book Corpus is a large-\nscale corpus of book texts compiled by crawling and organizing content f rom\ne-book websites. It comprises a total of 11,038 e-books. The resources ha ve not\nbeen made publicly available at present.\nA.1.4 Academic Materials\n• arXiv73. The arXiv corpus is a website that compiles preprints of papers\nspanning physics, mathematics, computer science, biology, and quantitative eco-\nnomics. Operational since 1991, this resource features papers written in LATEX.\nNumerous pre-training corpora source their academic material data from t his\nrepository.\n• S2ORC (Lo et al , 2020). The S2ORC corpus stands as an extensive academic\nliterature corpus, encompassing 81M English-language academic papers acros s\ndiverse academic disciplines. It features abundant metadata, paper ab stracts,\nand meticulously resolved bibliographic references, offering stru ctured full text\nfor 8.1M open-access papers. Each full-text document is meticulously annotated,\nincorporating automatically detected inline citations, figures, and tab les.\nA.1.5 Code\n• BIGQUERY (Nijkamp et al , 2023). The BIGQUERY corpus is a subset of\nBigQuery, comprising code from six programming languages (all under open -\nsource licenses), including C, C++, Go, Java, JavaScript, and Pyt hon.\n• Github74. The Github corpus is a hosting platform that offers features such\nas code repository management and code snippet sharing. It houses numer ous\nwell-known open-source projects.\n• phi-1 (Gunasekar et al , 2023). The phi-1 corpus is employed to train models\ncapable of generating Python functions and corresponding docstrings. T he cor-\npus comprises a curated code-language dataset, around 6 billion tokens, an d a\nPython textbook and exercise dataset synthesized by GPT-3.5.\n71https://www.gutenberg.org/\n72https://www.smashwords.com/\n73https://arxiv.org/\n74https://github.com/\n88\n• The Stack (Kocetkov et al , 2023). The Stack consists of over 6TB of open-\nsource code files spanning 358 programming languages, all of which are licens ed\nunder permissive licenses. It serves as the pre-training corpu s for Code LLMs.\nA.1.6 Parallel Corpus\n• MTP75. The full name of MTP is Massive Text Pairs, comprising a total of\n300M aligned Chinese-English text pairs. It serves as a crucial foundati on for\ntraining Chinese-English semantic vector models.\n• MultiUN (Eisele and Chen , 2010). The MultiUN corpus is sourced from files\nwithin the United Nations Official Document System. These files cove r the\nsix official languages, namely Arabic, Chinese, English, French, Russi an, and\nSpanish. Some of the files also provide versions in German.\n• ParaCrawl (Ba˜ n´ on et al, 2020). ParaCrawl utilizes open-source software to\ncrawl webpages, creating a publicly available parallel corpus. The c orpus of ver-\nsion 5.0 includes 223M filtered sentence pairs from approximately 150K websites,\nencompassing 42 languages.\n• UNCorpus v1.0 (Ziemski et al , 2016). The UNCorpus v1.0 corpus consists\nof text content written and manually translated from the years 1990 to 2014.\nThese contents comprise public domain United Nations official records and other\nconference documents, totaling 799,276 files. The majority of these file s cover\nthe six official languages.\nA.1.7 Social Media\n• OpenWebText (\nGokaslan and Cohen , 2019). The OpenWebText corpus is a\nreproduction of WebText. It extracts post URLs from Reddit, undergoes a series\nof filtering, deduplication, and tokenization operations, ultimately re sulting in\n8,013,769 documents.\n• Pushshift Reddit (Baumgartner et al , 2020). The Pushshift Reddit corpus is\na platform for collecting, analyzing, and archiving social media data. It has been\ncollecting data from Reddit since 2015 and receives regular updates.\n• Reddit76. The Reddit corpus is an entertainment, social, and news website\nwhere users can post texts or links and vote on posts. The site covers a variety\nof topics including news, gaming, music, and more. Many pre-trained lan guage\nmodels source their social media data from here.\n• StackExchange77. The StackExchange corpus is a Q&A website that stores\nquestions and their corresponding answers posed by users on the Inte rnet. It is\none of the largest publicly available resources of Q&A pairs. One of its prominent\nsub-sites is StacOverflow, which caters to programmers and develope rs.\n• WebText (Radford et al, 2019). The WebText corpus is an internal dataset of\nOpenAI. It comprises a collection of text gathered from 45M links, totalin g over\n8M documents. All documents related to Wikipedia have been remo ved from\nthis corpus.\n75https://data.baai.ac.cn/details/BAAI-MTP\n76https://www.reddit.com/\n77https://stackexchange.com/\n89\n• Zhihu78. The Zhihu corpus is a Chinese knowledge-sharing social platform. It\nenables users to ask questions, provide answers, and share knowledge, maintain-\ning a high level of quality. Simultaneously, the platform encourages users to build\nsocial connections through interactions such as following, upvoting, commenting,\nand more. Many Chinese social media datasets are derived from this plat form.\nA.1.8 Encyclopedia\n• Baidu baike 79. The Baidu baike corpus is an open online encyclopedia launched\nby Baidu, Inc. The primary language is Chinese, and it was released in 2008. As\nof April 2023, it has accumulated more than 27M entries.\n• TigerBot-wiki (\nChen et al , 2023c). The TigerBot-wiki corpus is specifically\ndedicated to collecting Chinese encyclopedia-related data. This constitutes the\nraw external brain data used during the rethinking process of the TigerBot\nmodel, with a scale of 205MB.\n• Wikipedia80. The Wikipedia corpus is an online encyclopedia written in multi -\nple languages, freely open-sourced to users. Due to its rigorous content spanning\nvarious languages and domains, people often crawl relevant data, clean it, and\nuse it for training large-scale models. Wikipedia is widely used ac ross various\napplications.\nA.1.9 Multi-category Corpora\n• ArabicText 2022\n81. The ArabicText 2022 corpus is the world’s largest open-\nsource pretraining dataset for Arabic, specifically designed for train ing Arabic\nLLMs. The creators curate, expand, and clean existing Arabic web text dat a,\nresulting in a dataset of 201.9GB. Text and knowledge-related data consti tute\nover 65% of the corpus.\n• Dolma (Soldaini et al , 2024). The Dolma corpus is a vast English-language\ncorpus comprising 3T tokens. It encompasses six main data types: webp ages,\nscholarly papers, code, books, social media, and encyclopedia. For each data\ntype, specific design principles and processing details are openl y disclosed. This\ncorpus has been instrumental in training the OLMo model. Notably, its creators\nhave transparently disclosed the selection of data sources and provided a detailed\noverview of the data curation process.\n• MNBVC (MOP-LIWU Community and MNBVC Team , 2023). The MNBVC\ncorpus is an extremely large-scale Chinese corpus with the goal of matchi ng the\n40TB data capacity used in training ChatGPT. It includes all forms of pur e-\ntext Chinese data. The corpus is continuously being cleaned and updat ed. Until\nNovember 2023, the scale has reached 20,811GB.\n• RedPajama-V182. The RedPajama-V1 corpus replicates the pre-training cor-\npora used according to report on LLaMA. The data scale is 1.2TB, encompassing\nfive languages and six data types.\n78https://www.zhihu.com/\n79https://baike.baidu.com/\n80https://www.wikipedia.org/\n81https://data.baai.ac.cn/details/ArabicText-2022\n82https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\n90\n• ROOTS (Lauren¸ con et al, 2022). ROOTS stands for Responsible Open-science\nOpen-collaboration Text Sources. It is composed of datasets from HuggingFace,\nGithub repositories, OSCAR, etc. The corpus has a scale of 1.6TB and incl udes\n46 natural languages and 13 programming languages.\n• The Pile (\nGao et al, 2020). The Pile is a large-scale, diverse language modeling\ndataset consisting of 22 data subsets. The goal is to capture text in as many\nforms as possible and cover a wide range of textual content. The corpus includes\nacademic papers, code, legal materials, patents, subtitles, chat cont ent, parallel\ncorpora, etc.\n• TigerBot\npretrain en & TigerBot pretrain zh (Chen et al, 2023c). These\ntwo corpora are the Chinese and English corpora used in the pre-traini ng of\nTigerBot. The corpus design is based on the pre-training data distrib ution of\nGPT-3. The creators filter the collected 20TB data down to 2TB while maintain-\ning the proportional distribution of languages and categories. Finanlly, 100G B\nof data is randomly sampled for open-sourcing.\n• WanJuanText-1.0 (He et al , 2023a). The data source of WanJuanText-1.0\nincludes patents, textbooks, exam questions, books and other materi als. The\ndataset comprises over 500M Chinese and English documents, totaling 1,094GB.\nIt standardizes data from many formats into the jsonl format and undergoes\nthorough cleaning, deduplication, and value alignment.\nA.2 Domain-speciﬁc Pre-training Corpora\nA.2.1 Financial Domain\n• BBT-FinCorpus (\nLu et al , 2023a). BBT-FinCorpus is a Chinese corpus in\nthe financial domain. The text is primarily focused on financial news, c ompany\nannouncements, research reports, and social media. The data is source d from\nseveral well-known financial websites and platforms on the Chinese I nternet.\nThe corpus has a scale of approximately 256GB and is utilized for training\nBBT-FinT5.\n• FinCorpus (\nZhang and Yang , 2023). FinCorpus includes text types such\nas company announcements, financial information and news, and financial\nexam questions. The data is obtained through web crawling, with a scale of\napproximately 60GB. It is used for training XuanYuan.\n• FinGLM (\nMetaGLM, 2023). FinGLM incorporates 11,588 PDF files, all of\nwhich are annual reports from listed companies for the years 2019 to 2021. The\ncorpus also includes corresponding TXT and HTML files.\n• TigerBot-earning & TigerBot-research (\nChen et al, 2023c). These represent\nthe raw external brain data utilized during the rethinking phase of TigerBot.\nThe former encapsulates 2.5K financial reports, while the latter encomp asses\n20K financial research reports. Data is stored on a paragraph-level granularit y.\nA.2.2 Medical Domain\n• Medical-pt (Xu, 2023). Medical-pt is a Chinese-language corpus in the medical\nfield. Approximately 360K entries are derived from medical encycloped ias, and\n91\n8,475 entries are from medical textbooks. It is primarily used for incr emental\npre-training of models in medical knowledge.\n• PubMed Central 83. PubMed Central is an open-access repository of biomed-\nical literature, offering free resources in the field of biomedicine . It comprises\napproximately 5M articles. The corpus is regularly updated, providi ng a wealth\nof medical knowledge.\nA.2.3 Other Domains\nSee Section\n2.2.3 for details.\nAppendix B Instruction Fine-tuning Dataset\nInformation\nAppendix B provides detailed information on each instruction fine-tuning datase t\nmentioned in the main text.\nB.1 General Instruction Fine-tuning Datasets\nB.1.1 Human Generated Datasets\n• Aya Dataset (\nSingh et al , 2024). The Aya Dataset is the largest human-\nannotated multilingual instruction fine-tuning dataset to date, compr ising over\n204K instances across 65 languages. On the Aya Annotation Platform, con-\ntributors engage in three tasks: creating new examples from scratch (or iginal\nannotations), enhancing existing examples for improved quality and c ompre-\nhensiveness (re-annotations), and providing feedback on the qualit y of existing\ncontributions (annotation feedback), following the find-fix-verify paradigm.\n• databricks-dolly-15K (\nConover et al, 2023). The databricks-dolly-15K dataset\nwas constructed by Databricks employees in March and April 2023, compris-\ning 15,011 high-quality English instruction pairs. The dataset encompasses eight\ninstruction categories and is suitable for commercial applications. The data\nsources include manually generated data and selected text from Wikip edia.\n• InstructionWild\nv2 (Ni et al , 2023). The InstructionWild v2 dataset com-\nprises approximately 110K instructions gathered from sources such as so cial\nmedia and code repositories. It provides additional information, in cluding\ninstruction types and special labels. The dataset is suitable for non- commercial\nresearch purposes.\n• LCCC (Wang et al, 2020b). The dataset is named Large-scale Cleaned Chinese\nConversation, comprising two versions: LCCC-base and LCCC-large, with 6.8M\nand 12M dialogues, respectively. The instructions are meticulously cleaned from\n79M original dialogue data. The construction process involves acquiring u ser\ncommunication records from social media.\n• OASST1 (Wang et al , 2023a). The OASST1 dataset is designed to advance\nresearch in model instruction fine-tuning and alignment. It consis ts of 161K\n83https://www.ncbi.nlm.nih.gov/pmc/\n92\nassistant-style dialogue messages covering 35 languages. Moreover, the d ataset\nincludes quality ratings, forming over 10K fully annotated dialogue tree s.\n• OL-CC84. The dataset is named OpenLabel-Chinese Conversations, and it is a\nChinese conversational instruction dataset. The creators utilize cr owdsourcing,\ncollecting data on an open platform, resulting in 10,006 “instruction-res ponse”\ntext pairs with answers and 1,649 without answers. The instruction types encom-\npass many tasks such as Q&A, text composition, brainstorming, mathematics ,\nand more. The dataset is completed by 276 volunteers.\n• Zhihu-KOL85. The Zhihu-KOL dataset, a Chinese conversation dataset, was\nconstructed in March 2023 by scraping the Zhihu website. The dataset c on-\nstruction employed a hierarchical Q&A categorization method, involv ing three\nseparate scraping processes targeting different levels of types.\nB.1.2 Model Constructed Datasets\n• Alpaca data (Taori et al , 2023). The Alpaca data dataset consists of 52K\ninstructional data points used for fine-tuning the Alpaca model. Each data\ninstance is presented in json format, including instruction desc riptions, task\ninputs, and the answers generated by the model.\n• BELLE Generated Chat (BELLEGroup, 2023). The BELLE Generated C-\nhat dataset comprises approximately 400K instances of personalized charact er\ndialogues generated by the BELLE project, along with introductions for e ach\ncharacter. These data are constructed by ChatGPT without rigorous valid ation\nand may contain errors. The category of all instructions is generation.\n• BELLE Multiturn Chat (BELLEGroup, 2023). The BELLE Multiturn Chat\ndataset comprises approximately 800K instances of multi-turn dialogues between\nusers and assistants, generated by the BELLE project. These data are con -\nstructed by ChatGPT without rigorous validation and may contain errors.\n• BELLE train 0.5M CN (BELLEGroup, 2023). The BELLE train 0.5M CN\ndataset is a subset of the Chinese training data for the BELLE model, consisting\nof approximately 520K Chinese instructions. All instructions are gener ated by\nthe model. The entire dataset is currently open source.\n• BELLE train 1M CN (BELLEGroup, 2023). The BELLE train 1M CN\ndataset is part of the Chinese training data for the BELLE model, compris ing\naround 917K Chinese instructions. It shares the same construction method as the\nBELLE train 0.5M CN, but it undergoes post-processing to remove low-quality\ndata.\n• BELLE train 2M CN & BELLE train 3.5M CN (BELLEGroup, 2023).\nTh-ese two datasets are Chinese instructions datasets generated by t he BELLE\nproject, comprising around 2M and 3.5M diverse task data, respectively. In com-\nparison to previous datasets, they offer a more extensive range of trainin g data.\nBELLE train 3.5M CN expands the fields of instruction categories, covering 13\ntypes such as generation, extraction, role-playing, and others.\n84https://data.baai.ac.cn/details/OL-CC\n85https://github.com/wangrui6/Zhihu-KOL\n93\n• CAMEL (Li et al, 2023b). The CAMEL dataset features an extensive collection\nof around 584K instructions, among which 107K have been translated into mul-\ntiple languages. This dataset provides a wide array of dialogue resourcescovering\nboth multilingual and code domains. The datasets introduces a communi cation\nagent framework called “role-playing” generated through three types of prompts\nand involving two contexts: “AI Society” and “Programming.”\n• Chatgpt\ncorpus86. The Chatgpt corpus dataset contains 3.27M instances of\nthe model engaging in self-conversation. This dataset offers Chinese d ialogue\nresources, with each instruction accompanied by a label indicating the associated\ndomain.\n• InstructionWild v1 (Ni et al, 2023). The InstructionWild v1 dataset furnishes\n52K instructions in both Chinese and English. Constructed using a mo del-\ngenerated approach, the dataset involves providing five example prompt s to\nthe model, which then generates new instructions along with corres ponding\nresponses. The dataset is intended for non-commercial research purp oses.\n• LMSYS-Chat-1M (Zheng et al , 2023a). The LMSYS-Chat-1M dataset com-\nprises 1M instances of authentic dialogue data, collected from various mod els\nresponding to questions on a website. To ensure the secure releas e of the data,\nthe creators remove conversations containing personal identifying i nformation.\nHowever, they retain unsafe dialogues for the purpose of studying robus tness\nand security.\n• MOSS 002 sft data (Sun et al , 2023b). The MOSS 002 sft data dataset is\na collection of multi-turn dialogue data utilized by the MOSS-002 proj ect. It\ncomprises 570K English instructions and 590K Chinese instructions. Thedataset\nencompasses three aspects: utility, fidelity, and harmlessness, all generated by\nthe model.\n• MOSS 003 sft data (Sun et al , 2023b). The MOSS 003 sft data dataset\ncompiles 100K user data instances from the beta testing phase of the MOS S-\n002 model and generated data from GPT-3.5-Turbo. In comparison to the\nMOSS\n002 sft data, this collection aligns more closely with the distribution\nof real user intents. Furthermore, it features more detailed category labels, a\nbroader range of harmless data, and longer dialogue sequences.\n• MOSS 003 sft plugin data (Sun et al, 2023b). The MOSS 003 sft plugin da-\nta dataset is an augmented version of MOSS 003 sft data, comprising around\n300K multi-turn dialogue instances. It is generated by four plugins: sear ch\nengine, diagram generator, calculator, and equation solver.\n• OpenChat (Wang et al , 2023a). The OpenChat dataset consists of 70K user\ndialogues sourced from ShareGPT, comprising 6K instances generated by G PT-\n4 and the rest by GPT-3.5-Turbo. This dataset provides rich informat ion for\nEnglish dialogues.\n• RedGPT-Dataset-V1-CN (Yang et al, 2023b). The RedGPT-Dataset-V1-CN\ndataset is a Chinese instruction dataset generated by RedGPT. The dat aset is\ndivided into two parts: RedGPT-Fact, providing instructions r elated to factual\nknowledge, and RedGPT-Code, offering dialogues related to programming. The\n86https://github.com/PlexPt/chatgpt-corpus\n94\nconstruction process involves: (1) generating multi-turn dialogues using open-\nsource models, (2) utilizing the dialogues for model fine-tuning to obtain the\nRedGPT model, and (3) employing this model to generate the final ins truction\ndata.\n• Self-Instruct (Wang et al, 2023f). The Self-Instruct dataset comprises approxi-\nmately 52K English instructions obtained through the model, covering a variety\nof task categories. The specific construction details involve the expan sion of the\ndataset using seed instructions. The Self-Instruct constructi on framework used\nin the dataset has been widely applied.\n• ShareChat87. The ShareChat dataset comprises approximately 90K instruc-\ntions, all sourced from dialogue data on ShareGPT. In terms of language\ndistribution, there are 68K instructions in English, 11K in Chinese, and the\nremaining in other languages. The aim of this dataset is to translate all ins truc-\ntions in other languages into Chinese, contributing to the resources of Chinese\ninstructions. All data undergoes manual inspection and verification.\n• ShareGPT-Chinese-English-90k\n88. The ShareGPT-Chinese-English-90k\ndataset is a parallel bilingual Q&A database in Chinese and English. Unlik e\nother Q&A content generated through repeated calls to API interfaces, t his\ndataset has a more robust instruction distribution, making it suitabl e for train-\ning bilingual dialogue models. All questions are spontaneously generated by\nusers, and most conversations with relatively poor subjective expe riences have\nbeen filtered out.\n• ShareGPT90K89. The ShareGPT90K dataset comprises approximately 90K\ndialogues from ShareGPT. Primarily in English, this dataset represen ts authen-\ntic data reflecting interactions between users and the model.\n• UltraChat (Ding et al , 2023). The UltraChat dataset comprises 1.47M multi-\nturn dialogues. The data predominantly covers three main topics: que stions\nabout the world, writing and creativity, and assistance in paraphrasing exist-\ning materials. Two independent models are employed in the constr uction of this\ndataset for dialogue generation.\n• Unnatural Instructions (Honovich et al, 2023). The collection process of the\nUnnatural Instructions dataset involves minimal manual labor. The creator s\nuse seed instructions to prompt the model to generate 64K examples, an d then\ninstruct the model to rephrase each instruction to further expan d the dataset.\nIn the end, approximately 240K instructions are obtained.\n• WebGLM-QA (Liu et al , 2023e). The WebGLM-QA dataset is designed for\ntraining the WebGLM generation module and comprises approximately 43K\nhigh-quality samples. Constructed using a context-guided approach, the process\ninvolves prompt formulation, guided instructions, and few-shot context learning.\nAll instructions belong to the category of Open QA.\n• Wizard\nevol instruct 196K & Wizard evol instruct 70K (Xu et al ,\n2023b). Both of these datasets consist of English instructions, with approx i-\nmately 196K and 70K data instances respectively. The construction methodology\n87https://paratranz.cn/projects/6725\n88https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k\n89https://huggingface.co/datasets/RyokoAI/ShareGPT52K\n95\nis based on the Evol-Instruct approach, involving four evolutionary stage s for\n175 human-created seed instructions, aimed at increasing the difficul ty and\ncomplexity of the instructions.\nB.1.3 Collection and Improvement of Existing Datasets\nSee Section\n3.2.3 for details.\nB.1.4 Datasets Created with Multiple Methods\n(1) HG & CI\n• Fireﬂy (\nYang, 2023). The Firefly dataset is a large-scale collection encompassing\n23 Chinese NLP tasks. It includes tasks related to Chinese culture such as couplet\ncreation, poetry composition, Jin Yong’s novels, prose, and more. Each t ask is\nmeticulously curated with a variety of manually crafted instruction templates,\ncomplemented by additional category labels. The dataset boasts a substant ial\nscale, amounting to 1.6M instances.\n• LIMA-sft (Zhou et al , 2023a). The LIMA-sft dataset comprises 1,330 meticu-\nlously curated human-selected instructions. Employing LIMA-sft for fine-tuning\non LLaMA-65B, it investigates the significance of data quality during the large -\nscale model instruction fine-tuning phase, demonstrating that a li mited dataset\nsize is sufficient to instruct the model in generating high-qualit y outputs.\n(2) HG & MC\n• InstructGPT-sft (Ouyang et al, 2022). The InstructGPT-sft dataset, used for\nfine-tuning the InstructGPT model, comprises 14K instructions. Part of the\ndataset comes from user data on the platform, while the other portion is authored\nby 40 trained annotators through a process involving creating simple tas ks, pro-\nviding few-shot tasks, and writing instructions. Currently, th e dataset is not\nopen-source.\n(3) CI & MC\n• Alpaca GPT4 data (Peng et al , 2023). The Alpaca GPT4 data dataset uti-\nlizes instruction inputs from the Alpaca data and generates responses using\nGPT-4. The dataset comprises 52K English instructions. The format of the\ndataset is identical to that of the Alpaca\ndata, with higher-quality generated\nanswers.\n• Alpaca GPT4 data zh (Peng et al, 2023). The Alpaca GPT4 data zh dataset\ntranslates the instruction inputs from the Alpaca data into Chinese and\nthen generates responses using GPT-4. The dataset consists of 52K Chi-\nnese instructions. Alpaca\nGPT4 data zh is the Chinese response version of\nAlpaca GPT4 data, but it may exhibit semantic shifts during the translation\nprocess.\n• Bactrain-X (Li et al, 2023c). The Bactrain-X dataset comprises 3.5M instruc-\ntions, spanning 52 languages. During construction, 67K English instruct ions\nfrom the Alpaca data and databricks-dolly-15K are translated into 51 other lan-\nguages using a translation API, followed by models generating responses . While\nthis dataset encompasses a diverse array of languages, its quality is cont ingent\nupon the accuracy of translation and the models’ responses.\n96\n• Baize (Xu et al, 2023a). The Baize dataset is employed to train the Baize model.\nQuestions are drawn from datasets such as Quora, StackOverflow, MedQuAD ,\nand others. The dataset comprises a total of 210K English dialogue samples\ngenerated through self-dialogue using ChatGPT. It encompasses not only general\ndomain conversations but also includes dialogue data from the medical domai n.\n• GPT4All (\nAnand et al , 2023). The GPT4All dataset is utilized to train the\nGPT4All model, comprising approximately 740K English instructions. The con-\nstruction process involves gathering questions from diverse domain s through\npublic datasets, invoking the model for responses, and subsequent ly performing\noperations such as semantic similarity-based deduplication of instru ctions and\nfiltering out rejected model outputs.\n• GuanacoDataset90. The GuanacoDataset, short for Generative Universal\nAssistant for Natural-language Adaptive Context-aware Omnilingual outputs\nDatasets, comprises approximately 534K instructions, spanning various lan-\nguages such as English, Simplified Chinese, Traditional Chinese (Taiw an),\nTraditional Chinese (Hong Kong), Japanese, German, and more. It provides\ndifferent language versions based on 175 seed instructions.\n• LaMini-LM (\nWu et al , 2023). The LaMini-LM dataset is employed to train\nthe LaMini model series. It comprises 2.58M English instructions, pr oviding\nthe advantages of a large scale and broad topic coverage. The creators utilize\nvarious instruction inputs from existing resources, including S elf-Instruct, Flan\n2022, and others, invoking the model to generate responses. The generation of\ninstructions primarily follows guided strategies based on examples an d themes.\n• LogiCoT (\nLiu et al , 2023c). The LogiCoT dataset is primarily designed to\nenhance the logical reasoning abilities of models, focusing on instru ctions falling\nunder the category of reasoning. The dataset comprises 605K instructions in\nboth Chinese and English, serving as an extension to four existing ope n-source\nNLP reasoning datasets.\n• LongForm (\nK¨ oksal et al, 2023). The LongForm dataset is designed to enhance\nmodels’ long-text generation capabilities, featuring approximately 28K English\ninstructions covering tasks such as Q&A, email composition, grammar err or\ncorrection, story and poetry generation, and text summarization. The dat aset\nis constructed based on manually created documents from C4 and Wikipe dia,\nwhere different documents are selected, and model-generated ins tructions are\nderived.\n• Luotuo-QA-B (Liao et al , 2023). The 157K Chinese-English instructions in\nthe Luotuo-QA-B dataset are generated based on CSL, CNN-DM, and arXiv.\nThe model generates five corresponding instruction-text pairs for e ach abstract\nor news article in the source datasets.\n• OpenOrca (\nMukherjee et al , 2023). The OpenOrca dataset is constructed\nbased on the Flan 2022. It comprises 1M instructions generated by GPT-4 and\n3.2M instructions generated by GPT-3.5-Turbo. The dataset holds a signifi cant\nadvantage in terms of scale.\n90https://guanaco-model.github.io/\n97\n• Wizard evol instruct zh (Ziang Leng and Li, 2023). The Wizard evol instru-\nct zh dataset translates the instructions from Wizard evol instruct 70K into\nChinese and then invokes the model to generate responses, resulti ng in 70K\nChinese instructions. However, the dataset may contain translation e rrors.\n(4) HG & CI & MC\nSee Section 3.2.4 for details.\nB.2 Domain-speciﬁc Instruction Fine-tuning Datasets\nB.2.1 Medical Domain\n• ChatDoctor (\nLi et al , 2023l). The release of the ChatDoctor dataset primar-\nily addresses the limitations of existing LLMs in the field of medical k nowledge.\nThe dataset comprises 115K English dialogue samples, including authenti c con-\nversations between real patients and doctors sourced from websites, as well as\nmodel-generated dialogues and disease database information. Fine-tunin g with\nthis dataset significantly enhances the models’ abilities to under stand patient\nneeds and provide recommendations.\n• ChatMed Consult Dataset (Zhu and Wang , 2023). The ChatMed Consu-\nlt Dataset is a Chinese medical online consultation dataset comprising 549K\ninstructions. These instructions are divided into real interne t medical consulta-\ntion questions and dialogues generated by the model. The dataset aims to reflect\nthe medical consultation needs of different patients. Subsequentl y, the creators\nwill filter and curate the Q&A pairs.\n• CMtMedQA (Yang et al, 2023d). The CMtMedQA dataset is a Chinese med-\nical multi-turn dialogue dataset consisting of 68K authentic doctor-pati ent con-\nversations, featuring a substantial number of actively inquiring s tatements. The\ndataset is utilized for training the Zhongjing model, enhancing th e complexity\nand proactive inquiry capabilities of medical dialogues.\n• DISC-Med-SFT (Bao et al , 2023). The DISC-Med-SFT dataset is a Chinese\nmedical instruction dataset designed for training the DISC-MedLLM model. The\ndataset consists of 465K samples, covering various scenarios such as single -turn\nmedical Q&A, multi-turn medical consultations, and multiple-choic e medical\nQ&A. The construction process involves the use of a target-oriented s trategy,\nselecting high-quality open-source datasets and restructuring th em.\n• HuatuoGPT-sft-data-v1 (Zhang et al , 2023b). The HuatuoGPT-sft-data-v1\ndataset is a Chinese medical instruction dataset designed for the ins truction\nfine-tuning phase of the HuatuoGPT model. The dataset combines refined data\ngenerated by the model and authentic dialogue data provided by real doc tors,\ntotaling 226K instructions.\n• Huatuo-26M (Li et al, 2023h). The Huatuo-26M dataset is a Chinese medical\nQ&A dataset comprising 26M high-quality medical Q&A pairs. The medicaltop-\nics covered include diseases, symptoms, treatment methods, dr ug information,\nand more. The data is sourced from various channels, including online m edi-\ncal encyclopedias, medical knowledge graphs, and records from online me dical\n98\nconsultations, ensuring data diversity. Currently, part of this d ataset is open\nsource.\n• MedDialog (Zeng et al , 2020). The MedDialog dataset is a collection of med-\nical dialogue data in both Chinese and English. The Chinese dataset compr ises\n3.4M doctor-patient dialogues, covering 172 disease specialties, while the English\ndataset includes 0.26M doctor-patient dialogues, spanning 96 disease specialties.\nAll data in the dataset are authentic inquiries from real interactions.\n• Medical Meadow (Han et al, 2023). The Medical Meadow dataset is an English\nmedical instruction dataset, consisting of a total of 160K records. It has tw o\nprimary sources: firstly, open-source medical NLP task datasets that hav e been\nstandardized into instruction fine-tuning format, and secondly, we b scraping of\nmedical resources from the internet. The dataset encompasses a diverse range of\nmedical domains, including biomedicine, health, bioinformatics, an d more.\n• Medical-sft91. The Medical-sft dataset is a bilingual medical dataset containing\ntwo parts. The first part consists of Chinese data, including 1.95M recor ds from\nconsultations in six medical departments, online medical encyclopedia, and Q&A\nfrom medical knowledge graphs. The second part comprises English medi cal\ninquiry dialogue data and NLP datasets, totaling 110K records.\n• QiZhenGPT-sft-20k92. The QiZhenGPT-sft-20k dataset is a collection of 20K\nChinese medical instructions. The data is sourced from the Qizhen medical\nknowledge base and includes real doctor-patient knowledge Q&A data, as well\nas instructions constructed from text knowledge based on drugs and dis eases. It\nis primarily used to enhance the models’ accuracy in medical knowl edge Q&A\nand alleviate hallucination phenomena.\n• ShenNong TCM Dataset (Wei Zhu and Wang , 2023). The ShenNong TC-\nM Dataset is a Chinese medical dataset. Based on an open-source traditional\nChinese medicine knowledge graph, the dataset utilizes the Self-Instruct method\nto construct instruction data centered around traditional Chinese m edicine. In\ntotal, it comprises 112K records. The dataset represents a promising re source in\nthe field of traditional Chinese medicine.\nB.2.2 Code Domain\n• Code Alpaca 20K (Chaudhary, 2023). The Code Alpaca 20K dataset is\ndesigned for fine-tuning the Code Alpaca model. The construction of this dataset\nfollows the method used in the Alpaca data, resulting in 20K instructions. Its\nstrength lies in contributing a dataset of code-related instructi ons.\n• CodeContest (Li et al , 2022a). The CodeContest dataset is a collection of\ndata related to programming contests, featuring 13.6K code competition e xam-\nples. The data is sourced from Codeforces, Description2Code, and CodeNet. The\ndataset is characterized by its rich set of code instructions.\n• CommitPackFT (Muennighoff et al , 2023a). The CommitPackFT dataset\nundergoes filtering based on the original dataset. The original dataset covers 350\n91https://github.com/shibing624/MedicalGPT\n92https://github.com/CMKRG/QiZhenGPT\n99\nprogramming languages, totaling 4TB. After filtering, it retains 702K instruc -\ntions, supporting 277 programming languages. Multiple quality filters areapplied\nduring data processing to preserve content with commercially fr iendly licenses.\n• ToolAlpaca (Tang et al, 2023). The ToolAlpaca dataset aims to enhance mod-\nels’ abilities to use common tools, comprising a total of 3,928 instances an d over\n400 tools. During construction, 500 randomly selected APIs from a public API\nrepository serve as a starting point. The models are then employed t o gener-\nate more comprehensive documentation, resulting in the creation of a diverse\ncollection of tools.\n• ToolBench (Anonymous, 2024). The ToolBench dataset is a tool usage dataset\ncreated automatically by a model. The construction process primaril y involves\nthree stages: firstly, the collection of 16,464 real tool APIs covering 49 categories;\nsecondly, the use of the model to generate various instructions for th ese APIs,\nincluding single-tool and multi-tool scenarios; and finally, the use of the model\nto search for effective solution paths for each instruction. The dataset comprises\na total of 126K instances, providing a rich resource for tool invocation.\nB.2.3 Legal Domain\n• DISC-Law-SFT (Yue et al , 2023). The DISC-Law-SFT dataset is a Chinese\nlegal instruction dataset that covers various judicial application scenarios, includ-\ning legal information extraction, judgment prediction, document summarization,\nand legal Q&A. The dataset comprises a total of 403K instructions and is divided\ninto two subsets: DISC-Law-SFT-Pair and DISC-Law-SFT-Triplet . The former\nintroduces legal reasoning capabilities, while the latter enhances the models’ abil-\nities to utilize external knowledge. The data is sourced from thre e components:\nNLP judicial task public datasets, legal original texts, and general domain dat a.\nThe creators utilize three approaches—behavior shaping, knowledge ex pansion,\nand mindset cultivation—to reconstruct the instruction data and imp rove data\nquality.\n• HanFei 1.0 (He et al, 2023c). The HanFei 1.0 dataset is a Chinese legal instruc-\ntion dataset that includes both general instructions and legal instruct ions. The\ntotal scale of the dataset is 255K instructions, with 147K specifically relate d to\nlegal content. The dataset is constructed using rule-based filterin g, and future\nversions will incorporate manual curation.\n• LawGPT zh (Liu et al , 2023b). The LawGPT zh dataset is a Chinese legal\ninstruction dataset, primarily divided into two parts: scenario dialogues and legal\nknowledge Q&A. The scenario dialogues consist of 200K authentic conversations\nbetween lawyers and users. After reprocessing the Q&A using th e model, 52K\nsingle-turn Q&A and 92K scenario Q&A with legal basis are obtained. The other\npart involves generating legal knowledge-related Q&A pairs through a se lf-built\nlegal professional knowledge database, which is currently not yet open s ource.\n• Lawyer LLaMA sft (Huang et al , 2023b). The Lawyer LLaMA sft dataset is\na Chinese legal instruction dataset, totaling 21.5K records. The primary sources\ninclude model-generated answers to Chinese judicial exam questi ons, responses\n100\nto legal consultations, and multi-turn legal consultation dialogues generat ed\nbased on legal provisions. Currently, only a portion of the dataset is opensource.\nB.2.4 Mathematics Domain\n• BELLE School Math (BELLEGroup, 2023). The BELLE School Math data-\nset is a Chinese mathematical question dataset released as part of the BE LLE\nproject, comprising approximately 248K mathematical questions along wi th\ntheir solution processes. All answers to the questions are generated by the model\nand have not undergone rigorous verification, thus potential errors may ex ist in\nboth the questions and the solution processes.\n• Goat (Liu and Low , 2023). The Goat dataset is an instruction-synthesized\ndataset in the field of mathematics, consisting of 1.74M synthetic data in stances\nfor mathematical arithmetic tasks. Each instance includes instruct ions for an\narithmetic expression, a randomly generated arithmetic expression in code, and\nthe target output. However, the dataset is limited to arithmetic task s involving\naddition, subtraction, multiplication, and division in the field of mat hematics.\n• MWP (Lan et al , 2022). The MWP dataset is focused on tasks related to\nsolving mathematical word problems (MWP). It integrates eight popular M WP\ndatasets, categorizing them into single-equation and multiple-equat ion types.\nThe dataset comprises approximately 252K problems, providing a diverse corpus\nfor studying the resolution of mathematical problems.\n• OpenMathInstruct-1 (Toshniwal et al , 2024). OpenMathInstruct-1, a com-\nprehensive math instruction tuning dataset, features 1.8M pairs generated by the\nMixtral-8x7B model. It encompasses subsets from GSM8K and MATH, offerin g\nsynthetically generated solutions. The dataset is thoughtfully divi ded into train\nand validation subsets to cover the entirety of the training sets. It is constructed\nusing the methods of prompting novelty and brute-force scaling.\nB.2.5 Education Domain\n• Child chat data93. The Child chat data dataset, comprising 5K instances of\nChinese children’s emotional companionship dialogue, serves as the tr aining\ndata for the QiaoBan model. The construction process unfolds through tw o key\nphases: (1) Sampling from real-life scenarios, volunteers curate high-quality emo-\ntional companionship dialogue data based on topic lists derived from genuin e\nchildren’s conversations. Expert scholars actively participate, off ering insights\nand recommendations to enhance the dataset’s quality. (2) Model-gener ated\ndialogue data is produced, catering to different topics within the d ataset.\n• Educhat-sft-002-data-osm (Dan et al , 2023). The Educhat-sft-002-data-osm\ndataset, consisting of 4.28M dialogues in both Chinese and English, is empl oyed\nto train the EduChat model. This dataset amalgamates diverse educational\ndata, enabling the model to possess functionalities such as question generation,\nhomework grading, emotional support, and course guidance.\n93https://github.com/HIT-SCIR-SC/QiaoBan\n101\n• TaoLi data (Yu et al , 2023b). Yu et al (2023b) is building an international\nChinese education resource library that includes over 500 internati onal Chi-\nnese textbooks, HSK exam questions, Chinese dictionaries, and other resources.\nBased on this resource library, TaoLi data is being constructed. The task types\nfor instructions involve grammar correction, meaning generation, text s implifi-\ncation, and controlled text generation, totaling 88K instances. Some of the d ata\nis generated by the model, and errors may occur.\nB.2.6 Other Domains\nSee Section\n3.3.6 for details.\nAppendix C Preference Dataset Information\nAppendix\nC provides detailed information on each preference dataset mentioned i n\nthe main text.\nC.1 Vote\n• Chatbot\narena conversations (Zheng et al, 2023b). The Chatbot arena con-\nversations dataset collects 33K examples from Chatbot Arena, spanning from\nApril to June 2023. Each example includes a question ID, the names and\nresponses of two models, the choice of a human judge, language labels, tox ic\nlabels, and more. After analysis, a total of 20 models’ outputs and 96 languages\nare identified. Personal information is removed, and unsafe conversati ons are\nlabeled and retained.\n• CValues (\nXu et al , 2023d). The CValues dataset, also known as the CValues-\nComparison dataset, consists of 145K aligned value samples. These samples are\nChinese data in the domain of social norms. The dataset encompasses thre e\ntypes of responses: Safe and Responsibility, Safe, and Unsafe, ranked in descend-\ning order of safety. Through processes such as expanding seed instru ctions,\nmodel responses, categorizing positive and negative samples, and model rewrites,\ndifferent responses are assigned types, creating safety comparisons between pairs.\n• hh-rlhf (\nBai et al , 2022). The hh-rlhf dataset consists of approximately 170K\nexamples. Each line in each jsonl file of the dataset represents a pair of selected\nand rejected responses. The construction process involves crowd sourced work-\ners choosing one response to continue the conversation based on the rep lies of\ntwo models. The collection process primarily includes basic mod el extraction,\nrejection sampling, and online iterative sampling. During annotation, c reators\nencourage individuals to make selections based on their own criteria to maintain\ndiversity in the data, although this approach may introduce subjecti vity issues.\n• MT-Bench human judgments (Zheng et al , 2023b). The MT-Bench huma-\nn judgments dataset is obtained through pairwise preference comparisons con-\nducted by graduate students for 80 instructions generated separately b y six\nmodels. The six models include GPT-4, GPT-3.5, Claude-v1, Vicuna-13B ,\nAlpaca-13B, and LLaMA-13B. The data is in English, and the dataset is\nrelatively small, comprising only 3.3K examples.\n102\n• PKU-SafeRLHF (Ji et al, 2023a). The PKU-SafeRLHF dataset contains 362K\nhuman-annotated English data. The construction involves a two-stage annota-\ntion process. In the first stage, instructions are evaluated for harmlessness across\n14 harmful categories. In the second stage, preferences are selected based on both\nusefulness and harmlessness. Each open-sourced example includes two responses\nalong with preference information.\n• SHP (\nEthayarajh et al , 2022). The SHP dataset consists of 385K examples\ncovering 18 topics. Each example includes a question and a pair of res ponses\nfrom Reddit posts, with one response being more favored by users. In contrast\nto the hh-rlhf dataset, the questions and answers in SHP are manually cr afted\nrather than generated by models, enhancing authenticity.\n• Summarize from Feedback (Stiennon et al , 2020). The purpose of creating\nthe Summarize from Feedback dataset is to optimize summary generation mod-\nels through human feedback. The dataset is divided into two parts: Comparisons\nand Axis. The former involves annotators selecting the better summary from\ntwo alternatives, while the latter includes annotators rating the qu ality of sum-\nmaries using the Likert scale. In total, the dataset comprises approximately 194K\nexamples focused on the news domain.\n• Zhihu rlhf 3k94. The Zhihu rlhf 3k dataset comprises 3,460 examples from\nZhihu. Similar to SHP, each example consists of two responses, with t he more\npopular answer determined by user votes, reflecting genuine user p references.\nThis dataset provides valuable Chinese preference instruction r esources, which\nare relatively scarce.\nC.2 Sort\n• OASST1 pairwise rlhf reward95. The OASST1 pairwise rlhf reward dataset\nconsists of 19K examples obtained through post-processing on the OASST1\ndataset. The source dataset itself includes human quality ratings for d ifferent\nresponses, allowing for a direct transformation into the form of prefe rence data\nbased on annotations, reflecting human preferences in a sorted manner.\nC.3 Score\n• Alpaca\ncomparison data (Peng et al , 2023). The Alpaca comparison data\ndataset consists of 51K examples comparing three models. The results of the\ncomparisons serve as a form of preference feedback. The preference e valuation\nmethod involves using GPT-4 to score the quality of responses, thu s creat-\ning preference samples. Each example includes a prompt input, a h igh-quality\nanswer, and a low-quality answer.\n• Stable Alignment (Liu et al , 2023d). The Stable Alignment dataset is used\nto train social intelligence agents to better align their responses. Examples are\ncategorized into three types from simulated social interactions: im itation, self-\ncritic, and realignment, totaling 168K examples. These agents learn to adj ust\n94https://huggingface.co/datasets/liyucheng/zhihu rlhf 3k\n95https://huggingface.co/datasets/tasksource/oasst1 pairwise rlhf reward\n103\ntheir responses based on social value through simulated social interactions. Each\nexample includes multiple different responses generated by the m odel and their\ncorresponding scores.\n• Stack-Exchange-Preferences (Askell et al , 2021). The Stack-Exchange-\nPreferences dataset comprises 10.8M examples sourced from Q&A interac tions\non StackOverflow. Each answer is assigned a score based on two factors: the\nnumber of upvotes and whether it was accepted by the questioner. The score\nreflects the preference, with higher scores indicating stronger p reference.\n• UltraFeedback (\nCui et al , 2023). The UltraFeedback dataset is a large-scale,\ndiverse, and fine-grained preference dataset, consisting of approxi mately 64K\nEnglish examples. Each example includes responses from four differe nt models,\nmodel ratings for the responses, and detailed textual explanations for the ratings.\nThe models assess the responses from four dimensions: instruction -following,\ntruthfulness, honesty, and helpfulness. Instructions are sourc ed from various\npublicly available datasets, and the models randomly choose four out of 17 for\nresponse generation.\n• WebGPT (Nakano et al, 2021). The WebGPT dataset consists of approximately\n19.6K examples. Each example includes answers from two model response s to a\ngiven question, along with relevant metadata. The answers are manually rat ed,\nand each final answer is assigned a preference score to determine its q uality.\nThe entire construction process involves collecting questions f rom the general\ndomain.\nC.4 Other\nSee Section\n4.1.4 for details.\nAppendix D Evaluation Dataset Information\nAppendix\nD provides detailed information on each evaluation dataset mentioned in\nthe main text.\nD.1 General\n• AlpacaEval (\nDubois et al, 2023). The 805 English instructions in the AlpacaE-\nval dataset are sourced from various datasets, including Self-Instruc t, Vicuna\nEvaluation, and others. The dataset primarily assesses the performance of LLMs\non a variety of subjective open-ended questions in the general domain,employing\nmodels such as GPT-4 to score the outputs.\n• BayLing-80 (Zhang et al, 2023h). The BayLing-80 dataset comprises 320 single-\nturn and multi-turn instructions in both Chinese and English. Star ting with the\ntranslation of 80 English instructions from Vicuna Evaluation into Chine se, a\nsecond round of instructions was manually expanded to create both single -turn\nand multi-turn instructions in both languages. The dataset primarily e valuates\nthe cross-lingual and conversational capabilities of LLMs, covering nine tasks,\n104\nincluding writing, roleplay, common-sense, fermi, counterfactu al, coding, math,\ngeneric, and knowledge. GPT-4 is used for scoring in the evaluation pr ocess.\n• BELLE eval (Ji et al, 2023b). The BELLE eval dataset comprises 1K Chinese\ninstructions created by the BELLE project. The dataset primarily ass esses the\ngeneral capabilities of LLMs in a Chinese context, covering nine tasks : extract,\nclosed QA, rewrite, summarization, generation, classification, brainst orming,\nopen QA, and others. The “others” category mainly focuses on tasks related\nto mathematics and coding. The evaluation is conducted using ChatGPT f or\nscoring.\n• CELLO (\nHe et al, 2023b). The CELLO dataset comprises 523 English directives,\nall derived from data manually curated in real-world situations. Its pr incipal\nobjective is to gauge the proficiency of LLMs in comprehending intricateinstruc-\ntions. The evaluation encompasses ten subtasks, addressing aspects related to\nboth complex task description and complex input. The evaluation metho dology\nemploys code assessment.\n• MT-Bench (Zheng et al , 2023b). The MT-Bench dataset encompasses 80\nEnglish instructions, all meticulously forged by human artisans. The principal\nobjective of this dataset is to appraise the comprehensive competenc y of LLMs\nwithin the English milieu. It spans eight varied tasks, covering r ealms such as\nwriting, roleplay, reasoning, mathematics, programming, information extraction,\nSTEM (Science, Technology, Engineering, and Mathematics), and humani ties.\nThe assessment approach entails leveraging GPT-4 for the scoring proc ess.\n• SuperCLUE (Xu et al , 2023e). The SuperCLUE dataset functions as an\nextensive benchmark designed to appraise the proficiency of large-sc ale Chinese\nmodels. It encompasses SuperCLUE-OPEN, targeting multi-turn open -ended\nquestions, and SuperCLUE-OPT, focused on objective questions that t est three\nprimary capabilities. This dataset predominantly scrutinizes themodels’ prowess\nin handling Chinese language tasks, spanning a spectrum of over a hund red\nsubtasks. It undergoes monthly updates, with 3458 questions in Septe mber and\n3754 questions in October. The evaluation combines manual assessment and\ncode-based evaluation.\n• Vicuna Evaluation\n96. The Vicuna Evaluation dataset encompasses 80 instruc-\ntions in the English language, meticulously composed by human creators. I ts\nprincipal aim is to conduct a preliminary evaluation of the overall profi ciencies\nof LLMs, addressing nine diverse tasks such as writing, roleplay, comm on-sense,\nfermi, counterfactual, coding, math, generic, and knowledge. The ass essment\napproach employs GPT-4 to compare two responses.\nD.2 Exam\n• AGIEval (Zhong et al , 2023). The AGIEval dataset encompasses 8,062 direc-\ntives presented in both Chinese and English, featuring a combination of segments\nextracted from publicly available datasets and segments meticulousl y devised\nthrough manual efforts. The dataset’s principal objective revolves aroun d\nappraising the competencies of models when engaged in tasks associated with\n96https://github.com/lm-sys/vicuna-blog-eval\n105\nhuman cognition and problemsolving, achieved through the scrutiny of 20 metic-\nulously designed entrance and qualification examinations. The evaluative content\nis sourced from diverse domains, spanning general university admiss ions assess-\nments (GRE, Gaokao, SAT), specialized entry evaluations (LSAT, GMAT) ,\nchallenges derived from high school mathematical competitions (AMC, AIM E),\nChina’s civil service entrance examinations, and legal licensure te sts. The ques-\ntion formats encompass multiple-choice queries and the completion of blanks,\nwith the evaluation methodology grounded in code-based scrutiny.\n• GAOKAO-Bench (Zhang et al , 2023k). The GAOKAO-Bench dataset incor-\nporates 2,811 directives in Chinese, sourced exclusively from meti culously\ncrafted authentic questions found in Gaokao. Comprising a spectrum of 10 sub-\njects—ranging from the Chinese to science and liberal arts mathematics, English,\nphysics, chemistry, biology, geography, politics, and history—the datas et serves\nas a means to assess the holistic capacities of LLMs. These capacities encompass\nlanguage understanding and logical deduction, as manifested in their res ponses\nto Gaokao queries. The evaluative content spans questions spanning th e period\nfrom 2010 to 2022, encompassing 1,781 objective questions and 1,030 subjective\nquestions derived from GaoKao. The evaluative methodology entails auto-\nmated scrutiny for objective questions and expert-assigned scores f or subjective\nquestions.\n• M3Exam (\nZhang et al, 2023i). The dataset named M3Exam encompasses 12,313\nmultiple-choice questions extracted from exams at primary, middle , and high\nschool levels across nine countries and utilizing nine distinct languages. Approx-\nimately 23% of the evaluation tasks include visual elements, testing t he models’\ncapabilities from various linguistic, modal, and hierarchical perspe ctives.\nD.3 Subject\n• ARB (Sawada et al , 2023). The ARB dataset contains 1,207 instructions in\nEnglish, featuring sophisticated reasoning challenges spanning math ematics,\nphysics, biology, chemistry, and law, delving into more intricate layers of knowl-\nedge. The questions encompass multiple-choice, brief-response, and open-answer\nformats, utilizing a blended assessment methodology involving code, human eval-\nuation, and model analysis. The initiators introduce a rule-driven evaluation\napproach, enabling GPT-4 to assign scores to intermediary reasoning st eps.\n• C-CLUE97. Derived from a crowdsourced annotation system, the C-CLUE\ndataset stands as a benchmark for evaluating classical Chinese language com-\nprehension. It consists of 19,150 entities and 4,365 relation pairs. The pri mary\nfocus of this dataset is to gauge LLMs’ proficiency in tasks related to NER\nand RE within the field of classical Chinese language studies. The asses sment\nmethodology is grounded in code-based evaluations.\n• C-Eval (\nHuang et al , 2023c). The C-Eval dataset comprises 13,948 Chinese\nmultiple-choice questions, spanning 52 different academic disciplines and catego-\nrized into four difficulty levels. The subject categories are primar ily divided into\nSTEM, social science, humanity, and other. Some of the data is derive d from\n97https://github.com/jizijing/C-CLUE\n106\nfreely available simulated and past-year exam questions on the Intern et, while\nthe remaining data, not freely accessible to the public, has been obt ained with\nthe appropriate authorization. The evaluation methodology involves code- based\nassessments.\n• CG-Eval (Zeng et al, 2023b). Within the CG-Eval dataset, there exist 11K Chi-\nnese questions, encompassing a spectrum of six major categories: sci ence and\nengineering, humanities and social sciences, mathematical computation, medical\nqualification exams, judicial exams, and certified public accountant ex ams, fur-\nther segmented into 55 subtopics. This dataset serves as a counterpart to MMCU,\nwith a focused emphasis on appraising the prowess of Chinese text ge neration\nwithin the academic realm. The evaluation employs a comprehensive s coring\nsystem, summing diverse criteria for non-computational question s and amal-\ngamating computed results and problem-solving processes for computat ional\nqueries.\n• CMMLU (Li et al , 2023d). The CMMLU dataset functions as an all-\nencompassing Chinese assessment standard, covering a total of 67 academ ic\nfields, spanning from fundamental subjects to advanced professional d omains.\nThese disciplines include not only the natural sciences that deman d compu-\ntational reasoning but also the humanities and social sciences that req uire\nknowledge. Additionally, there are region-specific categories like Ch inese driv-\ning rules and dietary culture. Given the presence of China-speci fic answers in\nnumerous tasks, it stands as a thoroughly Sinicized evaluation benchmar k.\n• LLMEV AL-398. The LLMEVAL-3 dataset encompasses around 200K ques-\ntions designed in a free-response format, classified into 13 overarch ing academic\ndomains and spanning more than 50 specific sub-disciplines, systemat ically\nprobing into the depth of expertise in specialized knowledge. Th e questions\nare predominantly curated from undergraduate assignments, examinations , and\ngraduate entrance assessments. Meticulous efforts are made by the creat ors to\nprocure evaluation content from sources beyond the internet spher e. Through-\nout the evaluation process, models are presented with a randomized s ubset of\n1K questions drawn from the question bank, with their responses sub jected to\nassessment through the GPT-4 scoring methodology.\n• MMCU (Zeng, 2023). The MMCU dataset incorporates 11,845 Chinese\nmultiple-choice questions, spanning 25 subtasks across disciplin es such as\nmedicine, law, psychology, and education. These questions are curate d with\nprecision by experts who manually gather them from freely accessibl e online\nrepositories, encompassing materials like legal qualification exams, psychological\ncounselor certification tests, and Gaokao. The evaluation methodology applie d\ninvolves the utilization of code-based assessment.\n• MMLU (Hendrycks et al , 2021b). The MMLU dataset encompasses 15,908\nmultiple-choice questions in English, providing a benchmark to evaluate model\nknowledge proficiency through both zero-shot and few-shot assessments . Cover-\ning 57 subjects, including STEM, humanities, and social sciences, the benchmark\n98https://github.com/llmeval/llmeval-3\n107\nspans difficulty levels from elementary to advanced. Students manual ly gath-\nered questions from various online free resources, incorporating exe rcises from\ndifferent subjects. The evaluation employs a code-based assessment methodology.\n• M3KE (Liu et al, 2023a). Comprising 20,477 multiple-choice questions in Chi-\nnese, the M3KE dataset spans 71 academic disciplines, ranging from prim ary\neducation to university levels. It is broadly classified into arts & humani-\nties, social sciences, natural sciences, and other categories. The as sessment\nmethodology involves employing code-based evaluations.\n• SCIBENCH (Wang et al , 2023d). The SCIBENCH dataset includes 695\nEnglish questions derived from educational materials, functioning as an evalua-\ntive standard for university-level STEM disciplines such as mathematics, physics,\nand chemistry. Its primary focus lies in assessing the models’ i ntricate reasoning\ncapabilities, knowledge proficiency, and computational skills. The q uestions are\nmanually formulated, and the evaluation methodology encompasses code-bas ed\nassessments.\n• ScienceQA (Lu et al , 2022). The ScienceQA dataset consists of 21,208 multi-\nmodal English multiple-choice questions originating from primary andsecondary\nschool science courses. Within this set, 16,864 questions incorporate images,\nwhile 10,220 questions incorporate textual context. The dataset evaluates the\nscientific literacy of LLMs through a methodology based on code assessment s.\n• TheoremQA (Chen et al , 2023b). The TheoremQA dataset includes 800\nEnglish questions formulated from a set of 350 theorems spanning mathemat ics,\nphysics, finance, and CS & EE. Highly specialized human experts met icu-\nlously curate the data, guaranteeing elevated quality and a moderate lev el of\ncomplexity.\n• XiezhiBenchmark (Gu et al , 2023). The XiezhiBenchmark dataset encom-\npasses 249,587 dual-language multiple-choice questions, representing 516 aca-\ndemic disciplines across 13 categories. These questions are predomin antly\nsourced from two channels: approximately 170K questions gathered from six dis-\ntinct examinations and roughly 80K questions autonomously generated through\nan automatic updating framework. The assessment methodology involves co de-\ndriven evaluations.\nD.4 Natural Language Understanding\n• CLUE (\nTrinh and Le , 2018). The CLUE dataset consists of 9 NLU Chinese\ndatasets, namely TNEWS, IFLYTEK, CLUEWSC2020, AFQMC, CSL, OCNLI,\nCMRC 2018, ChID, and C3. The evaluated NLU tasks include text classification,\ncoreference resolution, semantic matching, reading comprehension, and textual\nentailment.\n• CUGE (\nXu et al , 2020b). The CUGE dataset comprises 21 NLU datasets,\nencompassing 7 language abilities and 18 predominant NLP tasks. These 7\nabilities span language comprehension at the word and sentence levels, c ompre-\nhension at the document level, information retrieval and question answering, lan-\nguage generation, conversational interaction, multilingualism, and mathematical\nreasoning.\n108\n• GLUE (Wang et al , 2018). The GLUE dataset consists of 9 NLU English\ndatasets, namely CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and\nWNLI. The evaluated NLU tasks include grammaticality judgment, sentimen t\nanalysis, semantic matching, textual entailment, reading comprehen sion, and\ncoreference resolution.\n• MCTS (\nWang et al , 2019). Containing 723 Chinese test samples, the MCTS\ndataset is the most extensive and widely cited evaluation dataset for tasks related\nto simplifying Chinese text. Originating from complex structures extracted from\nnews corpora, each original sentence corresponds to multiple manually s impli-\nfied versions. The main focus is on evaluating the understanding and r ewriting\ncapabilities of LLMs when dealing with intricate Chinese texts.\n• RAFT (Alex et al , 2021). The RAFT dataset comprises 28,712 English test\nsamples, serving as a real-world few-shot text classification benchm ark. The 11\nsub-datasets within it are all binary or multi-classification tasks, co vering text\ncontent from various domains such as healthcare, customer interactions,Twitter,\nand more.\n• SentEval (Conneau and Kiela, 2018). The SentEval dataset comprises 11 down-\nstream tasks and 10 probing tasks, making a total of 21 subtasks. It function s\nas an evaluation toolkit for universal sentence representations, cove ring a range\nof tasks including binary classification, multi-classification, natur al language\ninference (NLI), and semantic matching.\n• SuperGLUE (Trinh and Le , 2018). An advanced iteration of GLUE, the\nSuperGLUE dataset serves as an expanded and enhanced benchmark for assess -\ning NLU. Consisting of 8 NLU English datasets—BoolQ, ReCoRD, CB, WiC,\nWSC, RTE, COPA, and MultiRC—this dataset raises the bar for evaluating\nreading comprehension, textual entailment, semantic matching, and c oreference\nresolution.\nD.5 Reasoning\n• Chain-of-Thought Hub (\nFu et al, 2023). The Chain-of-Thought Hub dataset\nencompasses eight open-source datasets, establishing a comprehensi ve inven-\ntory of intricate reasoning tasks encompassing mathematics (GSM8K), s cience\n(MATH, TheoremQA), symbols (BBH), knowledge (MMLU, C-Eval), encodin g\n(HumanEval), and facts (SummEdits). The evaluation employs the approac h of\nfew-shot CoT prompting.\n• Choice-75 (Hou et al , 2023). The Choice-75 dataset consists of 650 multiple-\nchoice questions in English, establishing the inaugural benchmark t o evaluate\nthe decision reasoning prowess of LLMs within descriptive scenarios. The dataset\nis composed of 75 scripts and over 600 scenarios, classified based on differe nt\ndifficulty levels. Models are tasked with selecting the optimal opt ion from two\nalternatives within predefined scenarios.\n• LILA (Mishra et al , 2022a). The LILA dataset evaluates LLMs’ mathemat-\nical reasoning skills through 23 tasks across four dimensions. It scruti nizes\nfundamental mathematical skills, algebra, probability theory, calculu s, and\nother pertinent mathematical knowledge. These four dimensions enc ompass\n109\nmathematical proficiency, language format, language diversity, and external\nknowledge.\n• MiniF2F\nv1 (Zheng et al, 2022). The MiniF2F v1 dataset consists of 488 state-\nments presenting Olympiad-level mathematical problems, aiming to evaluate the\nneural mathematical reasoning capabilities. The data is gathered from di verse\nmathematical competitions, including AIME, AMC, IMO, along with mater ials\nfrom high school and undergraduate mathematics courses.\n• NeuLR (Xu et al , 2023c). The NeuLR dataset comprises 3K reasoning ques-\ntions, representing an improvement over 15 standard logical reasoning datasets.\nIts primary focus is on assessing three distinct reasoning capabili ties: deduc-\ntive reasoning, inductive reasoning, and abductive reasoning. The evalu ation\nsystem for logical reasoning capabilities is ultimately formed by six d imen-\nsions: accuracy, precision, self-awareness, activeness, orientation, and absence of\nhallucination.\n• TabMWP (Lu et al, 2023b). The TabMWP dataset includes 38,431 questions,\nwith a primary focus on evaluating the mathematical reasoning prowessof LLMs\nin handling both textual and tabular data. This dataset serves as a benchm ark\nfor addressing more intricate challenges, particularly those related to models\nprocessing heterogeneous information. The questions are divided in to two cat-\negories: question-answering and multiple-choice, and each is annotate d with\ngolden answers, offering a detailed understanding of the multi-step reasoning\nprocess.\nD.6 Knowledge\n• ALCUNA (Yin et al , 2023b). The ALCUNA dataset includes 84,351 English\nqueries and 3,554 independently created entities. Each entity, on a verage,\nencompasses 11.75 sets of attribute triples and 25.39 linked entities. E mploy-\ning the innovative knowGen technique, adjustments are made to th e attributes\nand relationships of existing entities to produce fresh knowled ge. This process\nforms the foundation of ALCUNA, designed to evaluate LLMs’ proficiency in\ncomprehending, distinguishing, and associating with novel knowle dge.\n• KoLA (\nYu et al , 2023a). The KoLA dataset encompasses 2,138 English test\ninstances, assessing LLMs’ capacity to rationally deduce and generate knowledge\nat four hierarchical levels: memory, comprehension, application, and i nnova-\ntion. To ensure impartiality, the evaluation employs standardized s cores and a\ncomparative assessment system, with quarterly dataset updates implemented to\nmitigate the potential for model impropriety.\n• LLMEV AL-2 (Zhang et al , 2023e). The LLMEVAL-2 dataset comprises 480\nChinese questions that assess knowledge across different domains. The questions,\nsourced from external databases spanning 12 domains, encompass both multiple-\nchoice and open-ended formats. Evaluation is conducted through a combinat ion\nof manual and automated assessment methods.\n• LMExamQA (Bai et al , 2023c). The LMExamQA dataset comprises 10,090\nEnglish test instances, classified into knowledge recall, understanding, and anal-\nysis categories based on the complexity of the questions. It encompasse s 25\n110\ndifferent domains. The dataset introduces an innovative framework, treating lan-\nguage models as knowledgeable examiners who generate questions based on their\nunderstanding and evaluate responses without external references .\n• SocKET (Choi et al , 2023). The SocKET dataset encompasses around 2.6M\nEnglish test samples drawn from 58 NLP datasets designed to assess social\nknowledge. It functions as a benchmark for evaluating LLMs’ proficiencyin com-\nprehending various aspects of social knowledge, categorized into humor and sar-\ncasm, aggression, emotion, credibility, and social facts. The task types encompass\nclassification, regression, pair-wise comparison, and span identificati on.\nD.7 Long Text\n• InﬁniteBench (\nZhang et al, 2023j). The InfiniteBench dataset fills the void in\nassessing long texts beyond 100K, elevating the input length from the p revious\n10K to over 100K and reaching a maximum of 2M. Originating from five domains,\nnamely mathematics, code, dialogue, books, and retrieval, the dataset i ncorpo-\nrates diverse task formats, including Q&A, multiple-choice, and su mmarization.\nWith the exception of key information retrieval tasks, all others constitute novel\nevaluation tasks.\n• L-Eval (\nAn et al, 2023). The L-Eval dataset comprises 411 lengthy documents\nand 2,043 English prompts, assessing the capabilities of LLMs across divers e\ntasks when exposed to extensive text inputs. These tasks necessitate reasoning on\nprolonged textual content, encompassing activities such as text summ arization,\nQ&A, context assimilation, topic retrieval, and aiding in academic paper com-\nposition. The dataset is meticulously annotated and scrutinized, dra wn from a\ndiverse array of platforms and origins. The evaluation approach integrates code-\nbased, human-based, and model-based assessments. The input length ranges\nfrom 4K to 60K.\n• LongBench (Bai et al, 2023b). The LongBench dataset encompasses 4,750 test\ninstances, presented in both Chinese and English, and stands as the i naugu-\nral benchmark for a thorough assessment of LLMs’ abilities in cross-lingual ,\nmultitask, and comprehensive contextual comprehension. The dataset spans\nsix pivotal application scenarios for handling long texts, encompassing single-\ndocument QA, multi-document QA, text summarization, few-shot lear ning,\nsynthetic tasks, and code completion. The input length ranges from 1K t o 22K.\n• LongEval (Li et al , 2023a). The LongEval dataset supports the assessment of\nmodels relying on extensive text contexts. Tasks come in two diffe rent difficulty\nlevels: one focusing on broad-topic retrieval, and the other emphasiz ing detailed\npassage retrieval. The evaluation data utilizes code synthesis to meet task speci-\nfications, providing a straightforward and rapid approach to gauge and compare\nmodels’ effectiveness with long texts. The input length ranges fr om 5K to 16K.\n• LooGLE (\nLi et al , 2023g). The LooGLE dataset exhibits an average input\nlength of 20K words. It is predominantly structured around two princip al task\ncategories: short dependency tasks and long dependency tasks. Short Q &A is\ngenerated from Wikipedia articles and scripts for the former, while the latter\n111\ninvolves the design of four tasks, including multiple information re trieval, time-\nline reorder, computation, and understanding with inference, specifically tailored\nto arXiv papers and extensive documents.\n• ZeroSCROLLS (Shaham et al , 2023). The ZeroSCROLLS dataset consists of\n10 subsets of data, which are automatically transformed into a standardize d\ninput format with an average length of 10K words. Functioning as a zero-shot\ntesting benchmark, it requires LLMs to engage in inference on divers e types\nof long texts across tasks like text summarization, Q&A, aggregated sentime nt\nclassification, and information reordering.\nD.8 Tool\n• API-Bank (\nLi et al , 2023i). The API-Bank dataset encompasses APIs repre-\nsenting 53 frequently utilized tools, along with 264 dialogues subjected to manual\ncuration, and 568 tasks involving API invocation. The tasks are stratified i nto\nthree tiers: Tier 1 gauges the LLMs’ accuracy in invoking APIs, Tier 2 de lves\ndeeper into assessing their aptitude for retrieving APIs, and Tie r 3 scrutinizes\ntheir competence in orchestrating multiple API invocations.\n• APIBench (Patil et al , 2023). Derived from a collection of 1,645 API calls\nusing the Self-Instruct technique, the APIBench dataset produces 16,450 English\ninstructions. These instructions are versatile, serving both as gu idance for fine-\ntuning LLMs and as a benchmark for evaluating the models’ proficiency in\nexecuting API-related instructions.\n• ToolBench (\nXu et al, 2023f). The ToolBench dataset encompasses 795 instruc-\ntions in English, designed to evaluate the proficiency of LLMs in manipul ating\nvarious tools. The dataset is compiled from five pre-existing datasets and three\nrecently acquired datasets. Evaluation tasks span diverse domains, i ncluding\nopen weather, the cat API, home search, trip booking, Google Sheets, vir tual\nhome, webshop, and tabletop.\nD.9 Agent\n• AgentBench (Liu et al, 2023f). The AgentBench dataset contains 1,360 English\ntest samples and stands as the first benchmark to evaluate the performan ce of\nLLMs functioning as AI Agents across various environments. The dataset encom-\npasses eight distinct environments, comprising five newly establ ished domains:\noperating system, database, knowledge graph, digital card game, and lateral\nthinking puzzles, in addition to three domains adapted from public d atasets:\nhouse-holding, web shopping, and web browsing.\n• SuperCLUE-Agent\n99. The SuperCLUE-Agent dataset addresses the gap in\nevaluating LLMs’ Agent capabilities in Chinese tasks and scenarios. The evalua-\ntion encompasses 10 tasks, distributed across three core abilities. T ask planning\ncapabilities include task decomposition, self-reflection, and CoT tasks; tool usage\n99https://github.com/CLUEbenchmark/SuperCLUE-Agent\n112\ncapabilities encompass API invocation, API retrieval, API planning, and ge n-\neral tool utilization tasks; long-term and short-term memory capabiliti es cover\ntasks such as few-shot learning, long-term dialogue, and multi-documen t Q&A.\nD.10 Code\n• APPS (Hendrycks et al, 2021a). The APPS dataset contains 10K English pro-\ngramming questions, along with 131,777 test cases to verify solutions and 232,421\ngenuine solutions crafted by human authors. This dataset is primarily d esigned\nto evaluate the code generation prowess of LLMs, categorized into three d iffi-\nculty levels: basic, interview, and competition. The data is collected from diverse\nopen-access programming platforms and meticulously curated.\n• BIRD (\nLi et al , 2023f). The BIRD dataset consists of 12,751 text-SQL pairs\nand 95 databases, spanning 37 professional domains. Its primary objective i s to\nevaluate LLMs’ comprehension of database values and the external knowledge\nlinkage between natural language queries and database values. The dataset is\ncurated using techniques like web scraping and synthetic data gen eration.\n• CodeXGLUE (\nLu et al , 2021). The CodeXGLUE dataset is designed for the\nevaluation of coding abilities and consolidates existing datasets relat ed to code.\nIt categorizes tasks into four types based on input-output relationshi ps, encom-\npassing a total of nine tasks. Type 1: code-code includes tasks such as clone\ndetection, defect detection, fill-in-the-blank tests, code com pletion, and code\ntranslation. Type 2: text-code involves tasks like code search and te xt-to-code\ngeneration. Type 3: code-text focuses on code summarization. Type 4: t ext-text\nconsists of tasks like code documentation translation.\n• DS-1000 (Lai et al , 2023). The DS-1000 dataset includes 1K English coding\nqueries associated with 7 Python libraries, designed to evaluate th e proficiency\nin code generation. These queries are drawn from StackOverflow, ens uring a\ndiverse and authentic representation. Moreover, each query has an aut omated\nassessment mechanism.\n• HumanEval (Chen et al , 2021). The HumanEval dataset consists of 164 pro-\ngramming questions meticulously crafted by human experts. Beyond e valuating\nthe ability to generate code, the dataset necessitates models to ex hibit profi-\nciency in language comprehension, algorithmic understanding, and mathematical\nknowledge. Each question encompasses functional descriptions, inp ut-output\nexamples, function definitions, and more, challenging the model to p roduce\nPython functions based on the given information and pass the provided te st\ncases.\n• HumanEvalPack (Muennighoff et al, 2023a). The HumanEvalPack dataset en-\ncompasses 984 English-coded questions, spanning programming languages su ch\nas Python, JavaScript, Java, Go, C++, and Rust. While Python questions closely\nresemble those in HumanEval, questions pertaining to other programmin g lan-\nguages are meticulously constructed by human experts. The primary assessment\ncenters around gauging the proficiency of LLMs in rectifying code, gener ating\ncode comments, and producing code.\n113\n• MTPB (Nijkamp et al , 2023). The MTPB dataset comprises 115 code prob-\nlems expertly crafted, each representing a multi-turn code gen eration task. In\neach problem, LLMs are tasked with synthesizing subprograms at each step ,\nintegrating the current task description with preceding steps. This multi-turn\ndecomposed evaluation process serves to enhance the models’ unde rstanding of\nuser intent and its capabilities in code generation.\n• ODEX (Wang et al, 2023h). The ODEX dataset contains 945 pairs of natural\nlanguage queries and associated code snippets, accompanied by 1,707 metic u-\nlously designed test cases. The task instructions for the queries s pan across four\nlanguages: English, Spanish, Japanese, and Russian, serving as an evaluati on\nbenchmark for the proficiency of LLMs in cross-lingual coding tasks.\nD.11 Out-of-Distribution\n• BOSS (Yuan et al, 2023). The BOSS dataset is dedicated to the investigation of\nthe OOD performance across different LLMs, presenting challenges wit hin the\nframework of distributional transfer settings. Comprising 20 sub-dat asets, the\nBOSS dataset spans tasks including sentiment analysis, toxicity d etection, NLI,\nNER, and extractive Q&A.\n• GLUE-X (Yang et al , 2023c). The GLUE-X dataset is comprised of 8 train-\ning datasets and 16 evaluation datasets, with all training datasets sourced\nfrom GLUE. It assesses the OOD performance of models across classic NLP\ntasks, encompassing syntactic judgment, sentiment analysis, semantic matching,\ntextual entailment, and reading comprehension.\nD.12 Law\n• LAiW (\nDai et al, 2023). The LAiW dataset systematically arranges pre-existing\nlegal datasets, dividing assessment tasks into three primary legal com peten-\ncies, comprising a total of 13 fundamental assignments. Fundamental task s in\nNLP encompass the retrieval of statutes, recognition of elements, ident ification\nof named entities, summarization of judicial key points, and the ident ification\nof legal cases. Basic applications involve the mining of disputed focal p oints,\nmatching cases, predicting criminal judgments, predicting civ il judgments, and\nanswering legal questions. Advanced applications include the generati on of\njudicial reasoning, comprehension of case details, and the provision of l egal\nconsultation.\n• LawBench (Fei et al, 2023). The LawBench dataset functions as an assessment\nbenchmark designed for the Chinese legal framework. It evaluates legal c apabil-\nities through 20 tasks, such as legal entity recognition, reading compreh ension,\nand crime amount calculation, originating from three judicial cognitive d imen-\nsions: legal knowledge retention, understanding, and application. A not eworthy\nstrength of the dataset is its tasks being more closely aligned with real -world\napplications.\n• LegalBench (Guha et al, 2023). The LegalBench dataset consists of 162 diverse\nlegal tasks, covering six types of legal reasoning: issue-spotting, rul e-recall,\n114\nrule-application, rule-conclusion, interpretation, and rhetorical -understanding.\nProfessionals from legal and computer science backgrounds collaborate in th e\ndevelopment of the dataset.\n• LexGLUE (Chalkidis et al , 2022). Comprising seven open-source English\nlegal datasets, the LexGLUE dataset involves tasks categorized into multi- label\nclassification, multi-class classification, and multiple choice Q&A.\n• LEXTREME (Niklaus et al , 2023). Comprising 11 evaluation sub-datasets\nspanning 24 languages, the LEXTREME dataset categorizes all evaluation con-\ntent into 18 tasks, such as Brazilian court decisions, German argument mining,\nGreek legal code, Swiss judgment prediction, among others.\n• SCALE (\nRasiah et al , 2023). The evaluation content of the SCALE dataset\noriginates from diverse legal NLP datasets within the Swiss legal system an d\nadditional datasets, exploring research aspects related to five languages and the\nfederal legal framework. The assessment of LLMs is conducted across four dimen-\nsions, namely handling lengthy documents of up to 50K tokens, applying legal\nknowledge, comprehending multiple languages, and processing mult iple tasks.\nThe multitasking component encompasses information retrieval, court view\ngeneration, decision summarization, citation extraction, and text class ification.\nD.13 Medical\n• CBLUE (Zhang et al , 2022). The CBLUE dataset encompasses five medical\ntask categories and eight sub-datasets tailored for assessing Chinese medical lan-\nguage comprehension. These tasks involve extracting medical text inf ormation,\nnormalizing medical terms, classifying medical texts, determin ing relationships\nbetween medical sentences, and answering medical questions. The dataset is\nsourced from authentic contexts like clinical trials, electronic health records, and\ntextbooks, annotated by domain experts.\n• CMB (Wang et al, 2023c). The CMB dataset functions as an inclusive bench-\nmark in the field of Chinese medicine, addressing both medical examination tasks\nand intricate clinical diagnosis challenges. The dataset consists of 281K qu es-\ntions, spanning five categories of medical exams: physician, nursing, pharmacist,\ndisciplinary, and graduate entrance exams. The questions encompass var i-\nous formats, including multiple-choice, open-ended, and multi-t urn dialogues.\nThe evaluation methodology incorporates assessments from code, expert s, and\nmodels.\n• HuaTuo26M-test (Li et al , 2023h). The testing set employed in the study\n(Li et al , 2023h) is the HuaTuo26M-test dataset, randomly sampled from\nthe Huatuo-26M dataset. The dataset comprises data collected from authen-\ntic sources like online medical encyclopedias, medical knowledge gr aphs, and\nmedical Q&A forums. It serves as a benchmark for evaluating current m edical\npractices.\n• MultiMedQA (\nSinghal et al , 2023). The MultiMedQA dataset functions as\nan assessment benchmark for LLMs in the realm of medical Q&A. It encom-\npasses six publicly available medical datasets and an in-house Q&A datas et,\n115\nall expressed in the English language. The questions are structured i n both\nmultiple-choice and open-ended formats.\n• PromptCBLUE100. The PromptCBLUE dataset represents the first evalua-\ntion benchmark designed for LLMs in the realm of Chinese medical scenari os.\nThis dataset integrates 16 pre-existing NLP tasks related to medical sc enarios,\nconverting them into language generation tasks based on prompts. The prompts\nwithin the dataset are structured using 94 diverse templates, dem onstrating a\nhigh level of richness.\n• QiZhenGPT eval101. The QiZhenGPT eval dataset is utilized for evaluating\nLLMs’ abilities to answer questions regarding drug indications. With a dataset\ncomprising 94 questions, the model is tasked with identifying th e diseases for\nwhich a specific drug is applicable. The evaluation process involve s compar-\ning model responses with standard answers, conducted by medical ex perts who\nassign scores accordingly.\nD.14 Financial\n• BBF-CFLEB (Lu et al, 2023a). The BBF-CFLEB dataset comprises six finan-\ncial datasets: FinNA, FinQA, FinNL, FinRE, FinFE, and FinNSP. Each dataset\nis designed for specific financial tasks, including financial news sum marization,\nevent-based Q&A, news classification, news relation extraction, sent iment anal-\nysis, and identification of negative news and subjects. The primary fo cus is on\nassessing language understanding and language generation proficiency.\n• FinancelQ\n102. The FinancelQ dataset focuses on the Chinese financial domain,\nspecifically evaluating financial knowledge and reasoning abilities. I t covers 10\nmajor financial categories and 36 subcategories, comprising a total of 7173\nmultiple-choice questions. The dataset undergoes steps such as qu estion selec-\ntion and rewriting during construction to mitigate the potential imp act of data\nleakage.\n• FinBen (\nXie et al , 2024). The FinBen dataset provides a thorough and com-\nprehensive assessment of LLMs’ financial capabilities. It integrates 35 existing\ndatasets covering 23 financial tasks. The creators have categorized the tasks into\nthree difficulty levels: foundamental tasks, advanced cognitive engagem ent, and\ngeneral intelligence. FinBen extends financial evaluation to a wide range of tasks\nincluding quantification, understanding, forecasting, and introduc es the direct\ntrading task for the first time.\n• FinEval (Zhang et al , 2023d). The FinEval dataset serves as a benchmark\nfor evaluating Chinese financial knowledge, encompassing 4,661 high-qual ity\nmultiple-choice questions. The dataset spans four domains: finance, e conomy,\naccounting, and certificates, covering a total of 34 distinct academic su bjects.\nThe majority of the data is sourced from simulated exams and practice qu es-\ntions available through public channels, while some certificate-rel ated questions\nare derived from printed papers.\n100https://github.com/michael-wzhu/PromptCBLUE\n101https://github.com/CMKRG/QiZhenGPT/tree/main/data/eval\n102https://github.com/Duxiaoman-DI/XuanYuan/tree/main/FinanceIQ\n116\n• FLUE (Shah et al , 2022). The FLUE dataset aggregates six English NLP\ndatasets pertaining to finance, establishing a standardized benchm ark for finan-\ncial evaluation. It primarily evaluates the proficiency in NLU, covering tasks\nlike financial sentiment analysis, news headline classification, NER, structure\nboundary detection, and Q&A.\nD.15 Social Norms\n• CrowS-Pairs (Nangia et al , 2020). The utilization of the CrowS-Pairs dataset\naims to assess the social biases exhibited by LLMs towards marginalized com-\nmunities in the United States. Through a crowdsourcing approach, a benchmark\nof contrasting stereotypes is established. The dataset consists of 1,508 instances,\neach presenting sentences with varying degrees of stereotypical c ontent. In\nits entirety, the material covers nine categories of stereotypes, e ncompassing\nrace, gender, sexual orientation, religion, age, nationality, disability , physical\nappearance, and occupation.\n• SafetyBench (Zhang et al , 2023n). The SafetyBench dataset encompasses\n11,435 dual-language multiple-choice questions, delivering a thorough assessment\nof the safety aspects of LLMs. Evaluation is conducted across seven distin ct\nsafety dimensions, encompassing offensiveness, unfairness and bias , physical\nhealth, mental health, illegal activities, ethics and morality, as wel l as privacy\nand property.\n• Safety-Prompts (Sun et al, 2023a). The Safety-Prompts dataset contains 100K\nChinese prompts depicting safety scenarios and paired responses from ChatGPT.\nIt serves as a resource for assessing the safety performance of large-sc ale models\nand aligning with human safety values. The questions are categorized into typical\nsafety scenarios and instruction attacks. Typical safety scenarios inv olve insult,\nunfairness and discrimination, crimes and illegal activities, physical harm, mental\nhealth, privacy and property, and ethics and morality. Instruction attacks include\ngoal hijacking, prompt leaking, role play instruction, unsafe instruct ion topic,\ninquiry with unsafe opinion, and reverse exposure.\n• SuperCLUE-Safety103. The SuperCLUE-Safety dataset functions as a safety\nbenchmark tailored for large-scale Chinese models engaged in multi-tu rn adver-\nsarial conversations. Comprising 2,456 test instances, each featuring a safety\nquestion and a subsequent inquiry, the dataset integrates adversari al tech-\nniques to heighten the complexity of the questions. It effectivel y simulates\nreal-world user scenarios through multi-turn dialogues. This benchm ark serves\nas an evaluation for three key capabilities: conventional safety, respons ible AI,\nand instruction attacks.\n• TRUSTGPT (Huang et al , 2023d). Assessing toxicity, bias, and value align-\nment involves the evaluation of the TRUSTGPT dataset using 2K English te st\nsamples. The dataset undergoes scrutiny for toxicity using prompts aligned with\nsocial norms. Following this, bias levels of LLMs are quantified by measu ring\ntoxicity values across different groups. Ultimately, value alignment i s appraised\nby gauging LLMs’ rejection of content that contradicts human values.\n103https://github.com/CLUEbenchmark/SuperCLUE-safety\n117\nD.16 Factuality\n• F ACTOR(Muhlgay et al , 2023). The FACTOR dataset is divided into Wiki-\nFACTOR and News-FACTOR, distinguished by their respective data sources.\nThe dataset consists of 4,030 English samples, with 2,994 in Wiki-FACTOR and\n1,036 in News-FACTOR. For each sample, LLMs are tasked with selecting the\nsingular option that is factually accurate from among four completions, guide d\nby provided prefixes and relevant knowledge. This evaluation seeks to gauge the\nproficiency of LLMs in factual accuracy.\n• F ActScore(\nMin et al, 2023). The evaluation of LLMs’ factual accuracy in gen-\nerating extensive content is conducted using the FActScore dataset. Consisting of\n500 English evaluation samples, the dataset draws from biographical information\nfound on Wikipedia. A novel approach is employed, dissecting the generated text\ninto elemental facts and computing scores for the factual components en dorsed\nby the knowledge source.\n• FactualityPrompt (Lee et al, 2022). The FactualityPrompt dataset evaluates\nthe factual accuracy of textual outputs generated by LLMs. With a dataset\nsize of 16K English samples, evenly split into 8K authentic prompts and 8K\nfabricated prompts, it is an adaptation derived from the FEVER dataset. Th e\nstudy delves into the influence of two types of prompts on the factual accuracy\nof LLMs, replicating real-world situations with input inaccuracies.\n• FreshQA (\nVu et al , 2023). The FreshQA dataset functions as a dynamic QA\nbenchmark, encompassing 600 English evaluation samples. Questions pre sented\nto LLMs are classified into four categories based on the characteristics of t he\nanswers: answers that remain constant, answers that undergo gradual change s,\nanswers that undergo rapid changes, and answers with incorrect premis es.\nThis evaluation scrutinizes whether LLMs manifest hallucinatory phe nomena in\nresponding to questions and their ability to refute erroneous factual assumptions\nwithout succumbing to misdirection.\n• HalluQA (Cheng et al , 2023). The HalluQA dataset encompasses 450 Chi-\nnese questions specifically crafted to test the hallucinatory beha viors of Chinese\nLLMs. These questions are classified into three types: misleading q uestions,\nhighly misleading questions, and long-tail knowledge questions. Dur ing the\nassessment, GPT-4 is utilized to discern whether the models’ responses demon-\nstrate hallucination.\n• HaluEval (Li et al , 2023e). The HaluEval dataset functions as a benchmark\nfor assessing hallucination in LLMs, containing 35K English evaluation samples.\nThe evaluation data is crafted through a combination of manual and automated\nmethods using existing datasets. The assessment data involves in puts and out-\nputs in scenarios like Q&A, dialogue, text summarization, and human-computer\ninteraction, challenging LLMs to identify the potential existence ofhallucination.\n• TruthfulQA (\nLin et al, 2022). The TruthfulQA dataset is a standard for eval-\nuating the authenticity of answers produced by LLMs, featuring 817 Engl ish\nquestions across 38 domains. These questions are carefully designed and selected\nthrough manual curation.\n118\nD.17 Evaluation\n• FairEval (Wang et al, 2023b). All 80 instructions in the FairEval dataset origi-\nnate from the Vicuna Evaluation dataset. The creators generate responses across\nvarious models, including ChatGPT, Vicuna-13B, Alpaca-13B. Subsequent ly,\nmodels such as ChatGPT, GPT-4, are employed to assess different res ponses.\nThe evaluation underscores the importance of exercising caution whenemploying\nLLMs as evaluators, given the outcomes obtained.\n• LLMEval2 (Zhang et al , 2023l). The LLMEval 2 dataset is presently the most\nextensive and diverse English benchmark for appraising the evaluati on capa-\nbilities of LLMs, comprising a total of 2,553 samples. It incorporates 15 task\ntypes, spanning 8 abilities, designed for utilization by LLMs assessor s.\nZhang\net al (2023l) delves into the inquiry of whether a more profound and expansive\nnetwork contributes to a fairer assessment.\n• PandaLM testset (Wang et al , 2023g). The PandaLM testset dataset con-\ntains 1K varied English samples, with human annotations for both context and\nlabels. Its purpose is to evaluate the PandaLM model’s proficiency in comparing\nresponses among different LLMs, demonstrating the dependability of PandaLM.\nThe dataset also enables an investigation into the assessment capabilit ies of\nalternative LLMs.\nD.18 Multitask\n• BBH (\nSuzgun et al, 2023). The BBH dataset encompasses 23 tasks, consisting of\n6,511 English test samples. These tasks are drawn from BIG-Bench and represent\ntypes where LLMs exhibit performance below the average human level. Included\nin these tasks are causal judgment, date understanding, disambiguation Q A,\namong others.\n• BIG-Bench (Srivastava et al , 2023). The BIG-Bench dataset stands as a\ncomprehensive, intricate, and varied benchmark, honing in on tasks t hat pose\nchallenges for contemporary language models. It serves as an evaluation plat-\nform for the behaviors exhibited by classical models. With a total of 95 task\ntypes encompassing 204 tasks, the dataset spans a broad spectrum of topics .\nFurthermore, there exists a task subset known as “BIG-Bench Lite, ” featuring\ntasks that are representative, compact, and facilitate swifter evaluat ions.\n• CLEV A (\nLi et al , 2023n). The CLEVA dataset functions as a benchmark for\nmulti-task evaluation in Chinese, consisting of 370K test samples. A notable por-\ntion, approximately 33.98%, is newly generated, addressing concerns ass ociated\nwith data leakage. The dataset covers 11 tasks for application evaluation and 20\ntasks for capability assessment, with consistent preprocessing an d standardized\nChinese prompt templates applied to all data.\n• CLiB\n104. The CLiB dataset serves as an evaluation benchmark for assessing\nthe Chinese language capabilities of LLMs. It conducts evaluations on 48 LLMs,\nwhether commercial or open-source, across various dimensions, includ ing classi-\nfication, information extraction, reading comprehension, and table-base d Q&A.\n104https://github.com/jeinlee1991/chinese-llm-benchmark\n119\nThe dataset consistently releases new evaluation rankings and offers the original\noutput results of the models.\n• decaNLP (McCann et al, 2018). The decaNLP dataset spans ten distinct tasks,\ncovering areas such as Q&A, translation, text summarization, NLI, sentim ent\nanalysis, RE, semantic role labeling, goal-oriented dialogue, semantic par sing,\nand commonsense reasoning. Characterized by a substantial dataset, it evaluates\nthe English task processing proficiency of LLMs.\n• FlagEval105. The FlagEval dataset currently encompasses 22 evaluation sets,\nfeaturing a collective of 84,433 questions. It introduces a nuanced ev aluation\nframework based on the dimensions of “capability-task-metric,” offering detailed\ninsights into the cognitive boundaries of models. This assessment e xplores over\n600 sub-dimensions across 30+ capabilities, 5 primary tasks, and 4 key met rics.\n• HELM (Liang et al, 2023). The HELM dataset has the objective of construct-\ning a holistic evaluation framework to comprehensively appraise the c apabilities\nof LLMs. Adopting a top-down classification methodology for domain coverage,\nit precisely delineates evaluation scenarios and metrics, facilitat ing a systematic\nselection process. Presently, there are a total of 73 evaluation scenarios. In terms\nof metric evaluation, the dataset emphasizes the reflection of diverse dimen-\nsions of capabilities, achieving metric diversification, with a cur rent tally of 65\nevaluation metrics.\n• LLMEV AL-1 (\nZhang et al , 2023f). The LLMEVAL-1 dataset encompasses\n453 questions in Chinese, covering 17 broad task categories, including tasks\nlike providing factual answers, generating frameworks, and creating poetry.\nSimultaneously, five assessment criteria are defined, covering ac curacy, flu-\nency, informativeness, logic, and harmlessness. Evaluation is conducted through\nmethods such as crowdsourced comparative assessment, public compar ative\nassessment, and GPT-4 scoring.\n• LMentry (Efrat et al , 2023). The LMentry dataset evaluates how LLMs per-\nform on tasks that humans consider simple. In contrast to benchmarks for more\nintricate tasks, LMentry provides rapid and interpretable insight s into the fun-\ndamental capabilities and robustness of LLMs. The dataset consists of around\n110K English samples, encompassing 25 task categories, including word selection\nand sentence composition.\nD.19 Multilingual\n• XNLI (\nConneau et al , 2018). The XNLI dataset evaluates the transfer of low-\nresource languages and cross-lingual sentence classification, featuring a total of\n15 languages, including English, French, Spanish, German, Bulgarian, R ussian,\nTurkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili, and Urdu. There are\n7.5K evaluation samples for each language, with data for non-English languages\nderived from translation.\n• XTREME (\nSiddhant et al , 2020). The XTREME dataset assesses LLMs\nthrough four NLP tasks conducted in a variety of languages, scrutinizing t he\n105https://github.com/FlagOpen/FlagEval\n120\nlinguistic competence of LLMs. Task categories encompass classification, s truc-\ntured prediction, Q&A, and retrieval. The dataset encompasses 40 language s,\nrepresenting 20 language families.\nD.20 Other\nSee Section\n5.1.20 for details.\nAppendix E Traditional NLP Dataset Information\nAppendix\nE provides detailed information on each traditional NLP dataset mentioned\nin the main text.\nE.1 Question Answering\nE.1.1 Reading Comprehension\n(1) Selection & Judgment\n• BoolQ (\nClark et al , 2019). The BoolQ dataset is crafted in an environment\ndevoid of prompts and constraints, yielding 15,942 yes/no queries that de lve\ninto intricate inquiries and non-factual details, serving as a litm us test for the\nmodels’ reading comprehension and inferential prowess. Each instance comprises\na question, a paragraph, and an answer, necessitating the model to prov ide a\nresponse using either yes or no.\n• CondaQA (Ravichander et al, 2022). The CondaQA dataset represents a pio-\nneering effort in English reading comprehension datasets dedicated to inferencing\nthe implications of negated statements in textual content. Annotators, r espond-\ning to text with negations, formulate queries assessing meaning comp rehension.\nThe text undergoes three types of modifications, involving rephras ing, changing\nthe scope of negation, and inverting negations. Responses to queries areprovided\nin three formats: “Yes,” “No,” and “Don’t Know.”\n• CosmosQA (Huang et al , 2019). The CosmosQA dataset necessitates models\nto undertake reading comprehension tasks by leveraging common sense, struc-\ntured in a multiple-choice format. Utilizing everyday stories as t extual input,\nit presents inquiries about the origins and repercussions of events . Models are\nexpected not merely to provide surface-level answers but also to grasp the\nimplicit common knowledge and logical connections embedded in the tex t.\n• C3 (Sun et al , 2020). The C 3 dataset serves as an evaluation measure for the\nChinese reading comprehension capabilities of models, encompassing 13,369 dia-\nlogues or texts of mixed genres and 19,577 multiple-choice questions. The se\ninquiries are drawn from Chinese language exams intricately designed by educa-\ntional specialists, resembling the structure of questions in RAC E and DREAM.\nC3 has been included in CLUE, establishing itself as an assessment benc hmark\nfor Chinese NLU tasks.\n• DREAM (Sun et al , 2019). The DREAM dataset comprises a dialogue-based\nmultiple-choice reading comprehension exam with 10,197 questions and 6, 444\ndialogues. The dialogues are collected from English exams designed by hum an\n121\nexperts. 84% of the answers are non-extractive, 85% of the questions requ ire\nreasoning across multiple sentences, and 34% of the questions involve c ommon-\nsense knowledge.\n• DuReader Yes/No 106. Given the challenges in using metrics like F1 to gauge\na model’s genuine comprehension of textual meaning in opinion-based questions,\nthis dataset employs opinion polarity judgment as the focus of reading c om-\nprehension. The task necessitates the model to discern the polari ty of answers\nfrom the provided question, text, and answer summary, encompassing positive,\nnegative, and indeterminate polarities.\n• MCTest (Richardson et al , 2013). The MCTest dataset necessitates models\nto respond to multiple-choice questions related to imaginary narrati ves. Given\nthe entirely fictional nature of the text, there is a scarcity of incl uded world\nknowledge. The primary focus lies on evaluating the models’ profici ency in\nunderstanding story content and extracting relevant answers.\n• MultiRC (Khashabi et al, 2018). The MultiRC dataset mandates that models\nincorporate information from several sentences in the text to address questions\ninvolving the selection of accurate options. The number of correct answer options\nvaries for each question, thus requiring the model to evaluate the ac curacy of\neach option. Furthermore, the dataset is sourced from diverse material s such as\nnews articles, novels, historical texts, and seven other domains.\n• PubMedQA (Jin et al, 2019). The PubMedQA dataset serves as a reading com-\nprehension resource specifically designed for biomedical questions . It derives its\ncontent from abstracts within the PubMed Central. The assigned task r equires\nmodels to respond to questions based on the article abstracts, with p oten-\ntial answers categorized as “Yes,” “No,” or “Maybe.” The dataset encompasses\n1K meticulously annotated samples, along with an additional 61.2K unlabeled\nsamples and 211.3K synthetically generated samples.\n• QuAIL (\nRogers et al , 2020). The QuAIL dataset combines question types\nbased on text, world knowledge, and unanswerable scenarios, totaling 15K\nmultiple-choice questions spanning four domains. Notably, the dataset includes\nannotations for nine reasoning types, encompassing aspects such as time, causal-\nity, factual information, coreference, role attributes, belief stat es, entity states,\nevent duration, and questions deemed unanswerable.\n• RACE (\nLai et al, 2017). The RACE dataset serves as a resource for evaluating\nproficiency in English reading comprehension, encompassing more than28K arti-\ncles and close to 100K inquiries. Derived from reading comprehension q uestions\nwithin Chinese English exams, all questions are structured in a mu ltiple-choice\nformat. The dataset is stratified by complexity, offering the “RACE-M” subset\nfor middle school students and the “RACE-H” subset for high school stu dents.\n• ReClor (Yu et al , 2020b). The ReClor dataset originates from standardized\ngraduate entrance examinations, aiming to heighten the complexity of re ading\ncomprehension and introduce novel challenges to the logical reasoning capa-\nbilities of models. In order to mitigate the risk of models achievin g elevated\n106https://github.com/baidu/DuReader\n122\nperformance without a true comprehension of the text through the expl oita-\ntion of inherent biases in the data, the dataset bias has been partitioned by the\ncreators into easy and hard subsets.\n(2) Cloze Test\n• ChID (Zheng et al, 2019). The ChID dataset serves as a platform for evaluating\nmodels in the context of Chinese idiomatic expression reading comprehension. In\nthis task, models are tasked with filling in the blanks by choosing the appropriate\nidiom based on the provided context. The dataset specifically targets the models’\ncomprehension of Chinese idioms and has been incorporated into the CLUE\nbenchmark for assessing Chinese NLU capabilities.\n• CLOTH (\nXie et al, 2018). The CLOTH dataset stands as the pioneer in cloze-\ntype reading comprehension datasets crafted manually. Sourced from En glish\nexam questions for Chinese middle and high school levels, the missing words and\ncandidate options are meticulously curated by subject experts. The objective is\nfor models to comprehensively grasp the meaning of the entire text an d choose\nfitting English words to fill the gaps.\n• CMRC2019 (Cui et al , 2020). The CMRC2019 dataset stands as a sentence-\nlevel cloze-style reading comprehension benchmark. The objectiv e is for models\nto intelligently insert sentences from a set of candidates into the blanks within a\ngiven article (featuring multiple blanks), ensuring the coheren ce and complete-\nness of the text. This task critically assesses the models’ capacities for discerning\nlogical relationships in context.\n• LAMBADA (Paperno et al, 2016). The LAMBADA dataset serves to evaluate\nmodel reading comprehension abilities by employing a word predic tion task.\nExtracted from books, it includes 10K passages and over 100K English sentences.\nEach sentence concludes with a blank space, challenging the model to predict the\nmissing word based on a comprehensive understanding of the context, thereby\nassessing its contextual awareness.\n(3) Answer Extraction\n• Adversarial QA (Bartolo et al , 2020). Bartolo et al (2020) delves into the\nexploration of model-driven cyclic adversarial annotations, leveragingSQuAD as\nits underpinning. Employing the paradigm of adversarial artificial annot ations,\nqueries are systematically generated until they render the advers arial model\nincapable of delivering correct responses. Consequently, this methodology is har-\nnessed to formulate the Adversarial QA dataset characterized by its in herent\nchallenges.\n• CMRC2018 (Cui et al, 2019). The CMRC2018 dataset is composed of approxi-\nmately 20K Chinese reading comprehension questions, each representing genuine\nqueries annotated by human experts on Wikipedia. Additionally, a chall enging\nsubset is presented, necessitating extensive comprehension an d multi-sentence\nreasoning within context for model-derived answers. This dataset h as been inte-\ngrated into the CLUE dataset, serving as an assessment benchmark for Chin ese\nNLU tasks.\n• CUAD (Hendrycks et al, 2021c). The CUAD dataset concentrates on the realm\nof understanding legal contracts, encompassing 510 legal agreements and 41\n123\ndistinct categories of crucial clauses. The task mandates models to com prehend\nthe textual content of contracts and extract answers to queries pertaining to the\ncontracts.\n• DuReader Checklist 107. The DuReader Checklist dataset utilizes extrac-\ntive reading comprehension queries and institute a comprehensiv e Checklist\nevaluation framework to methodically appraise models’ multidimens ional and\nnuanced proficiency in reading comprehension. The evaluative aspect s include\nlexical understanding, phrase comprehension, semantic role compreh ension, and\nreasoning capabilities, among other dimensions.\n• DuReader Robust (Tang et al , 2021). The DuReader Robust dataset rep-\nresents the pioneering Chinese robust reading comprehension datas et, crafted\nto gauge the robustness of models by employing data instances from auth entic\nreal-world scenarios. Its objective is to appraise the models for the ir sensitivity,\nexcessive stability, and generalization.\n• HOTPOTQA (Yang et al, 2018). The HOTPOTQA dataset incorporates text\nsourced from Wikipedia, tasking models with deducing answers to q uestions\nfrom diverse document contents. Characterized by multi-docume nt reasoning,\nabsence of predefined knowledge base constraints, and provision of sentence-level\nsupporting facts, it facilitates the exploration of multi-step reason ing involving\ninformation from multiple sources.\n• MLQA (Lewis et al , 2020). The MLQA dataset serves as a benchmark for\nassessing the multilingual Q&A proficiency of models through the uti lization of\nextractive reading comprehension prompts. Instances within this dataset span\nacross seven languages, encompassing English, Arabic, German, Spanish, Hi ndi,\nVietnamese, and Simplified Chinese.\n• MS MARCO (Nguyen et al, 2016). The queries within the MS MARCO dataset\noriginate from the Bing search engine. Each query is paired with manually crafted\nresponses, and web documents retrieved from Bing searches serve as contextual\ninformation. The creators have consequently proposed three tasks wit h different\nlevels of difficulty: “assessing answerability,” “generating answers ,” and “rank-\ning retrieval content.” Due to the dataset’s content being drawn f rom genuine\nuser search histories, it possesses substantial scale, practical relevance, and thus,\nconsiderable reference merit.\n• Natural Questions (Kwiatkowski et al, 2019). The Natural Questions dataset\nmandates models to peruse and grasp complete Wikipedia articles, di scerning if\nthe articles encompass answers to posed questions. In the affirmative, the model\nmust articulate the precise details of the response. The questions are derived\nfrom authentic user inquiries, enhancing the dataset’s realism and complexity.\n• QuAC (Choi et al, 2018). The QuAC dataset comprises 14K dialogue pairs and\n100K questions designed for conversational reading comprehension. Annotator s\nengage in a two-person dialogue, where one formulates a set of open-ended ques-\ntions to unveil concealed information from Wikipedia text, and the other extracts\n107https://github.com/baidu/DuReader\n124\nconcise excerpts from the text to respond to these questions. Notabl y, the ques-\ntions in this dataset exhibit a greater degree of openness, with some qu estions\nfinding significance only within the contextual framework of the dialogue .\n• Quoref (Dasigi et al, 2019). The Quoref dataset serves as an assessment tool for\nmodels’ proficiency in co-reference reasoning within the domain of r eading com-\nprehension. Models, in order to address posed queries, are mandated to dissect\nintricate co-reference relationships embedded in the supplied textual content.\nThe dataset encompasses a plethora of more than 47K paragraphs sourced from\nWikipedia.\n• ReCoRD (\nZhang et al, 2018a). Included in SuperGLUE, the ReCoRD dataset\nserves as an assessment benchmark for English NLU tasks. The objective i s for\nmodels to extract answers from provided news text given a set of que stions.\nThis task places a notable emphasis on evaluating the models’ capacitie s for\ncommon-sense reasoning during the comprehension process.\n• SQuAD (Rajpurkar et al, 2016). The SQuAD dataset is constructed with over\n100K samples through crowdsourcing. Annotators generate questions based on\nWikipedia articles, and the answers are derived from corresponding passages in\nthe text.\n• SQuAD 2.0 (\nRajpurkar et al , 2018). Built upon the SQuAD dataset, SQuAD\n2.0 introduces an additional 53,775 unanswerable questions crafted through\ncrowdsourced reverse engineering. Responding to these question s necessitates\nmodels to decline providing an answer as the information cannot be loc ated in\nthe given text.\n• TriviaQA (\nJoshi et al, 2017). The TriviaQA dataset covers 95K Q&A pairs, with\nan average of six associated evidence documents per question, consti tuting over\n650K question-answer-evidence triplets. The questions are relativ ely intricate,\ndemanding cross-sentence reasoning for answer identification, providing a closer\nrepresentation of real-world scenarios.\n• TyDiQA (Clark et al, 2020). TyDiQA, a Q&A dataset, encompasses 11 distinct\nlanguages and consists of 204K Q&A pairs, deliberately addressing language\nintricacies absent in conventional English-centric datasets. The d ataset’s ques-\ntions are authored by individuals genuinely seeking answers to inquiries they lack\nknowledge of. Answers are extracted directly from Wikipedia texts in the corre-\nsponding languages, eschewing the use of translation tools. Models are as signed\nthe tasks of paragraph selection and determining minimal answer spans bas ed\non the given text and questions.\n(4) Unrestricted QA\n• CoQA (\nReddy et al , 2019). The CoQA dataset encompasses in excess of 8K\ndialogues and over 127K Q&A pairs, serving as a metric for assessing model s’\nadeptness in understanding text and responding to interconnected queries. Each\ndialogue originates from a conversation between two annotators, derived fr om\nthe provided sets of questions and answers. A distinguishing featu re of CoQA\nlies in the fact that responses can manifest as free-form textual expressions, with\nthe pertinent context for the answers embedded within the text.\n125\n• DROP (Dua et al , 2019). The objective of the DROP dataset is to evaluate\nthe pluralistic reasoning capabilities of models when dealing with textual infor-\nmation. Generated through crowdsourcing, it encompasses 96K interrogation s.\nModels are compelled to explore diverse avenues for unraveling questions, some-\ntimes engaging in computations, sorting, and other operations grounded in th e\ntextual data to derive answers. The task mandates models to cultivat e a more\nprofound comprehension of the text, given that answers might not be readi ly\napparent within the provided textual context.\n• DuoRC (Saha et al , 2018). The DuoRC dataset consists of 186,089 questions\nderived from 7,680 pairs of movie plots. Each plot pair includes two dist inct\nportrayals of the same movie—one extracted from Wikipedia and the other from\nthe IMDB website. Annotators generate questions based on one portrayal, an d\nanswers are then constructed using the alternate portrayal. As a resu lt, certain\nquestions do not share vocabulary with the provided text, requiri ng models to\nautonomously formulate language for responses.\n• DuReader 2.0 (He et al , 2018). The DuReader 2.0 dataset constitutes an\nexpansive, authentic, and manually curated collection of Chinese read ing com-\nprehension data. Focused on open-domain Q&A, this dataset comprises 200K\nquestions, 420K answers, and 1M documents, all derived from real-world scenar-\nios and extensively annotated. Models are tasked with deriving answers through\nsummarization from several documents.\n• QASPER (\nDasigi et al, 2021). The QASPER dataset covers 1,585 NLP papers\nand 5,049 related questions, designed to facilitate understanding and r easoning\nacross diverse sections of research papers. Each question is formulate d by NLP\nprofessionals after perusing only the paper’s title and abstract. Foll owing this,\na separate group of practitioners responds to the questions and furnish es sup-\nporting evidence for their responses. Extracting answers direct ly from the text\nis not viable; instead, a degree of summarization and synthesis is nece ssary.\nE.1.2 Knowledge QA\n• ARC (Clark et al , 2018). The ARC dataset consists of 7,787 real elementary-\nlevel science knowledge questions, classified into a challenging subset (2,590\nquestions) and an easy subset (5,197 questions) based on question difficulty. The\ntask mandates models to choose the optimal option through scientific knowledge\nand reasoning.\n• CMD108. The CMD dataset represents a Chinese medical Q&A dataset aimed\nat evaluating the knowledge Q&A capabilities of models within the me dical\ndomain. The dataset encompasses a total of 792,099 Q&A pairs, classified into six\nsub-domains: andrology, internal medicine, obstetrics and gynecology, oncology,\npediatrics, and surgery.\n• cMedQA2 (Zhang et al, 2018b). The cMedQA2 dataset represents an expanded\nand enhanced version of the cMedQA dataset. The initiators gather authen tic\ndoctor-patient dialogues from an online Chinese medical Q&A forum as inquiries,\nwherein medical professionals respond to medical queries posed by patients. The\n108https://github.com/Toyhom/Chinese-medical-dialogue-data\n126\nprimary focus is to assess the models’ abilities to answer questi ons within real\nscenarios where patients seek medical information.\n• CommonsenseQA (Talmor et al, 2019). The CommonsenseQA dataset consists\nof 12,102 multiple-choice questions demanding diverse forms of common-s ense\nknowledge for accurate answer selection. Extracting various target conc epts\nwith semantic relations akin to the source concepts from CONCEPTNET, c re-\nators task annotators with crafting multiple-choice questions that dis criminate\nbetween different target concepts. The objective of the task is to ev aluate the\nmodels’ proficiency in common-sense knowledge.\n• ECQA (Aggarwal et al, 2021). The ECQA dataset, an abbreviation for Expla-\nnation CommonsenseQA, originates from CommonsenseQA. Following manual\nannotation, it encompasses positive and negative attributes, along with E nglish\nexplanations, for 11K QA pairs extracted from CommonsenseQA. Its objec-\ntive is to furnish explanations for the knowledge-based question-an swering task\nwithin CommonsenseQA, providing an in-depth comprehension of the ge neral\nattributes linked to various options.\n• HEAD-QA (\nVilares and G´ omez-Rodr´ ıguez, 2019). The HEAD-QA dataset\nencompasses a variety of multiple-choice questions and answers, span ning dis-\nciplines such as medicine, pharmacology, psychology, nursing, biology, an d\nchemistry. The questions are sourced from professional position examswithin the\nSpanish healthcare system, adding a level of complexity. The dataset is available\nin both English and Spanish versions, covering a range of technical and s ocietal\nknowledge.\n• JEC-QA (Zhong et al, 2020). The JEC-QA dataset consists of 26,365 multiple-\nchoice questions, sourced exclusively from the Chinese National Judicial Exami-\nnation. The primary objective is to evaluate the knowledge Q&A capabil ities of\nmodels within the legal domain. Questions can be classified into two c ategories:\nknowledge-driven, emphasizing legal concepts, and case analysis, ne cessitating\nan analysis of practical legal scenarios.\n• OpenBookQA (Mihaylov et al , 2018). The OpenBookQA dataset replicates\nthe structure of open-book exams aimed at evaluating human comprehens ion\nacross diverse subjects. Each sample comprises a question, four opti ons along\nwith their respective answers, and supplementary scientific fac ts and common-\nsense information. Models need to exhibit proficiency in multi-s tep reasoning,\napplication of common-sense knowledge, and comprehension of textual cont ent.\n• PIQA (Bisk et al, 2020). The PIQA dataset centers on the physics interaction\nQ&A task, evaluating the models’ capacities to effectively respond to questions\npertaining to physics common sense. The task mandates the model to ap ply\nphysics common sense in selecting the most plausible solution from two presented\nalternatives based on a provided real-world scenario.\n• PsyQA (Sun et al , 2021a). The PsyQA dataset presents a collection of Chi-\nnese mental health data in a Q&A format. Derived from a Chinese platfor m\nfor mental health services, it encompasses 22K questions and 56K answers. The\ndataset’s knowledge-based Q&A relies on psychological counseling the ory, eval-\nuating the models’ abilities to produce text related to mental he alth counseling.\n127\nThis assessment aims to improve the smoothness and utility of the ge nerated\nanswers.\n• SciQ (Welbl et al , 2017). The SciQ dataset consists of 13,679 science exam-\nination questions acquired through crowdsourcing, spanning discipl ines like\nphysics, chemistry, and biology. These questions are structured i n a multiple-\nchoice format, offering four answer options. Additional paragraphs and materials\nsupporting the correct answers are included for the majority of ques tions.\n• WebMedQA (He et al , 2019). The WebMedQA dataset represents a Chinese\nmedical Q&A dataset, akin to cMedQA2. Each instance is sourced from spec ial-\nized health advisory websites, comprising questions, answers, adop tion status,\nand categorized labels. Specifically, there are 23 distinct categories, encompass-\ning a broad range of prevalent clinical departments, with internal med icine and\nsurgery having the highest representation.\n• WikiQA (Yang et al , 2015). The WikiQA dataset explores models for open-\ndomain Q&A. The origin of questions is sourced from Bing query logs, and\nanswers are derived from the content available on Wikipedia. A collect ive total\nof 3,047 questions has been gathered through crowdsourcing.\nE.1.3 Reasoning QA\n• COPA (\nRoemmele et al, 2011). The COPA dataset is explicitly crafted for the\ncommon-sense causal reasoning task. Models are tasked with choosing the correct\ncausal relationship based on provided premises. Incorporated into Su perGLUE,\nCOPA serves as an assessment benchmark for English NLU tasks.\n• CREAK (Onoe et al , 2021). For the exploration of models’ abilities to amal-\ngamate entity knowledge with common-sense reasoning, the CREAK dataset\nis introduced. It establishes a connection between factual detail s about entities\n(e.g., wizards like Harry Potter, proficient in broomstick flying) and c ommon-\nsense reasoning principles (e.g., having expertise in a skill al lows one to instruct\nothers). This process results in the formulation of reasoning queri es (e.g., is\nHarry Potter capable of instructing broomstick flying).\n• HellaSwag (\nZellers et al , 2019a). The HellaSwag dataset is curated for evalu-\nating common-sense natural language reasoning. Each query includes a scen ario\nand four conceivable outcomes, tasking models with deducing the most rea-\nsonable conclusion. Human-validated incorrect responses aim to mislead the\nmodel.\n• LogiQA (Liu et al , 2021). For comprehensive exploration of logical reason-\ning, Liu et al (2021) has engaged human experts to develop the LogiQA\ndataset, aimed at evaluating questions pertaining to human logical reason-\ning. It encompasses more than 8K Q&A pairs, covering diverse types of\ndeductive reasoning, including categorical reasoning, sufficient condi tional rea-\nsoning, necessary conditional reasoning, disjunctive reasoning, and conj unctive\nreasoning.\n• PROST (\nAroca-Ouellette et al , 2021). The PROST dataset, officially known\nas Physical Reasoning about Objects Through Space and Time, serves as a\n128\ntest for assessing physical reasoning capabilities. It consists of 18,736 multiple-\nchoice questions created through 14 manually designed templates. The questions\ncover 10 concepts related to physical reasoning, encompassing direct ion, mass,\nheight, circumference, stackable, rollable, graspable, breakable, s lideable, and\nbounceable.\n• QASC (Khot et al, 2020). The QASC dataset evaluates the multi-hop reasoning\nabilities of models. It involves retrieving pertinent facts froman extensive corpus\nand employing effective multi-hop reasoning methods to integrate t hese facts.\nUltimately, the correct answer is selected from a pool of eight options.\n• QuaRel (Tafjord et al, 2019a). The QuaRel dataset is developed with the aim\nof fostering models’ comprehension and resolution of problems related to qual-\nitative relationship inference. The dataset encompasses 2,771 narrativ e-based\nmultiple-choice questions, exemplified by “Jenny notices a disc repancy in the\nspeed of the robotic vacuum cleaner between the living room and bedro om car-\npets. Which carpet exhibits greater friction?”. The logical form of the questions\nis also provided.\n• QuaRTz (Tafjord et al , 2019b). The QuaRTz dataset presents a novel task\ninvolving the qualitative analysis of textual relationships, where c ommon qual-\nitative statements are paired with contextually generated questions through\ncrowdsourcing. For example, the qualitative statement “Sunscreen with a higher\nSPF protects the skin for a longer time” is paired with the contextual question\n“Billy applies sunscreen with an SPF lower than Lucy’s. Who will receive better\nsun protection?”. Models must exhibit robust abilities in both reasoning transfer\nand analogical reasoning to effectively address these inquiries.\n• ROPES (Lin et al, 2019). The ROPES dataset is primarily designed to evaluate\nthe reasoning abilities of models within specific contexts. Model s are presented\nwith background articles containing pertinent knowledge, newly constructed sce-\nnarios, and questions. Its task is to employ background knowledge for re asoning\nthrough the questions within the provided context. These backgroun d articles\nare derived from scientific textbooks and Wikipedia, with scenarios , questions,\nand answers curated by annotators.\n• Social IQa (Sap et al, 2019). The Social IQa dataset functions as benchmarks\nfor commonsense reasoning within social contexts, incorporating ques tions that\nrevolve around social interactions. The task necessitates models to choose the\nmost reasonable option from three potential subsequent behaviors, all w ithin\na provided scenario. This introduces content pertaining to the r easoning of\ntemporal relationships while evaluating fundamental common knowledge .\n• StoryCloze (Mostafazadeh et al , 2016). The StoryCloze dataset is devised to\nassess the causal reasoning capabilities of models within the realms of story com-\nprehension, story generation, and script learning. Analogous to HellaSwag, th e\nobjective is for models to choose an accurate conclusion from four sentences por-\ntraying a story scenario. The dataset encapsulates intricate causal and temporal\ncontextual associations prevalent in everyday occurrences.\n• STRATEGYQA (Geva et al , 2021). The STRATEGYQA dataset acts as a\nbenchmark for reasoning-based Q&A. The necessary steps for models to respond\n129\nare implicitly stated within the questions, and inference is carr ied out through\nthe application of diverse strategies. It encompasses 2,790 samples, each consist-\ning of a question focused on strategy, a breakdown of steps, and a paragraph\nproviding evidence.\n• WIQA (\nTandon et al , 2019). In particular, WIQA stands out as the inaugu-\nral dataset tailored for “What if...” queries pertaining to procedural reasoning.\nModels are tasked with deducing the repercussions of a disturbanc e occurring\nin a described process, utilizing knowledge embedded in the t extual depiction of\nthe process. For example, when presented with text detailing be ach erosion, the\nobjective is to predict the effects of a stormy weather event on the e rosion level.\nE.2 Recognizing Textual Entailment\n• ANLI (Nie et al , 2020). The Adversarial Natural Language Inference (ANLI)\ndataset, in its entirety, focuses on evaluating the performance of models in infer-\nence scenarios with heightened challenges. A notable aspect is the in corporation\nof adversarial samples, modifications applied to annotated training sampl es,\nposing increased difficulty for models to accurately classify text e ntailment\nrelationships.\n• CINLID109. Comprising 106K pairs of manually generated Chinese idioms, the\nCINLID dataset serves as a semantic reasoning dataset. This collection in cludes\na minor proportion of concise texts, such as riddles and colloquial expr es-\nsions. Each pair presents two idioms, employed as the premise and hy pothesis,\nprompting the assessment of their semantic relationship as either ap proximate,\nunrelated, or opposing.\n• CMNLI110. The Chinese version of the CMNLI dataset, employed for RTE\ntasks, is derived by translating the English segments from both Mult iNLI and\nXNLI. Within CLUE, this dataset has been replaced by OCNLI.\n• CommitmentBank (De Marneffe et al, 2019). The CommitmentBank dataset\nleverages naturally unfolding discourse to explore whether asserti ons made by\nspeakers entail commitments to forthcoming actions. SuperGLUE has designated\nCommitmentBank as the assessment standard for the English RTE task, catego-\nrizing the veracity between the initial dataset pairs as “Entailment ,” “Neutral,”\nand “Contradiction.”\n• MedNLI (Romanov and Shivade, 2018). The MedNLI dataset comprises RTE\ntask data within the medical domain, annotated by expert physicians. D uring\nits development, transfer learning is applied, leveraging pre-existing open-source\nNLI datasets. Additionally, domain knowledge from external medical data and\nspecialized medical terminology is integrated.\n• MultiNLI (\nWilliams et al , 2018). The MultiNLI dataset, denoted as Multi-\nGenre Natural Language Inference, is crafted by incorporating English tex tual\nand spoken content from ten distinct genres for the development of theRTE task\ndataset. This facilitates the assessment of generalization across differ ent genres.\n109https://www.luge.ai/#/luge/dataDetail?id=39\n110https://github.com/CLUEbenchmark/CLUE\n130\n• OCNLI (Hu et al, 2020). The OCNLI dataset stands as the pioneer among non-\ntranslated Chinese RTE task datasets, generated exclusively from native Chinese\nsources. With a dataset size of 56K text pairs, it has been integrated int o CLUE\nas the evaluation benchmark for Chinese NLU tasks.\n• RTE (Dagan et al , 2006; Bar-Haim et al , 2006; Giampiccolo et al , 2007; Ben-\ntivogli et al , 2009). The RTE dataset is dedicated to the task of recognizing\ntextual entailment. It is an amalgamation of datasets from various annual recog-\nnizing textual entailment challenges. Prominent RTE datasets encompass RTE1,\nRTE2, RTE3, and RTE5, necessitating the discernment of relationship s catego-\nrized as either ‘Entailment’ or “Non-Entailment.” The RTE dataset has b een\nincluded in GLUE and SuperGLUE, serving as an evaluation benchmark for\nEnglish NLU tasks.\n• SNLI (\nBowman et al, 2015). Human annotators have labeled the premises and\nhypotheses in the SNLI dataset by relying on image captions, resulting i n a\ndataset of 570K text pairs. This dataset currently holds the record as the largest\nin scale for RTE.\n• W ANLI (Liu et al , 2022). The WANLI dataset encompasses 108K pairs of\nEnglish textual samples. The dataset’s construction employed a hybrid approach\ninvolving both human and model contributions. Initially, a set of chal leng-\ning samples was identified on MultiNLI. Following this, GPT-3 generat ed\nnew instances using a comparable approach, and after automated filtration,\nannotated personnel conducted the labeling and refinement process.\nE.3 Math\n• Ape210K (\nZhao et al , 2020). The Ape210K dataset consists of 210K mathe-\nmatical problems designed for the elementary school level in China, e xhibiting\na considerable scale in comparison to alternative datasets. Each problem is\nequipped with an optimal solution, the corresponding equation for obtaining the\nanswer, and is enriched with a variety of templates. Tackling challe nges within\nApe210K necessitates multifaceted capabilities, encompassing natural language\ncomprehension, mathematical reasoning, and common knowledge.\n• AQUA-RAT (Ling et al , 2017). The AQUA-RAT dataset consists of around\n100K algebraic problems. Each problem’s solution process is methodicall y elu-\ncidated through a step-by-step explanation in natural language, facilitat ing the\ntraining of models in CoT abilities within the realm of mathematics.\n• ASDiv (Miao et al , 2021). The ASDiv dataset serves as repositories of math-\nematical application problems in the English language, employed to asses s the\nproficiency of models in solving such problems. The dataset encompass es 2,305\nquestions, spanning diverse text patterns and encompassing most problem types\nencountered in elementary school mathematics. Each sample is annotate d with\nits respective problem type and grade level.\n• GSM8K (Cobbe et al , 2021). The GSM8K dataset encompasses 8.5K metic-\nulously crafted elementary school mathematical problems. These math ematical\n131\ncomputations, deemed facile for human comprehension, entail solution p roce-\ndures spanning 2 to 8 steps. The primary operations involved are conse cutive\ncalculations using addition, subtraction, multiplication, and divi sion.\n• MATH (Hendrycks et al , 2021d). The MATH dataset encompasses 12.5K\ncompetitive mathematical problems, presenting a high level of diffic ulty. Each\nproblem is accompanied by a complete step-by-step solution, provid ing a means\nto evaluate models’ CoT abilities in solving mathematical problems or al low-\ning the models to learn the deductive process and explanation for gen erating\nanswers.\n• MathQA (Amini et al, 2019). Prior to the development of the MathQA dataset,\navailable datasets in the realm of mathematics are either limited in sc ale or lack\nprecise operational annotations for a diverse range of questions. In respon se,\nMathQA introduces a new representation language tailored to articulate the\naccurate operational procedures associated with mathematical problems . The\noverarching aim is to augment both the performance and interpretabili ty of\nmodels.\n• Math23K (Wang et al , 2017). The Math23K dataset is curated explicitly\nfor tasks related to mathematical problem-solving, encompassing 23,161 math\nproblems that include equation templates and answer labels. All the pr esented\nproblems focus on linear algebra and involve a singular variable. Derive d from\nseveral online educational platforms, these questions represent authentic problem\nsets designed for elementary school students.\n• NaturalProofs (\nWelleck et al, 2021). The focus of the NaturalProofs dataset\nis on mathematical propositions and proof-related tasks, exploring mathe mati-\ncal reasoning expressed in natural language. The problem content encompas ses\nstatements and proofs of theorems, mathematical definitions, inferences based on\naxioms, etc., sourced from real-world materials like compilations of mathematical\nproofs and textbooks.\n• SV AMP (Patel et al , 2021). In addressing elementary applied mathematical\nproblems, models are observed to predominantly depend on shallow he uristics\nrather than engage in deep reasoning. Consequently, a more challenging and\nreliably assessed SVAMP dataset is introduced. This dataset adapts ex amples\nfrom pre-existing datasets to evaluate the models’ sensitivity to problem-solving\nand reasoning abilities in the realm of mathematical problems, with diffi culty\nmaintained at a level equivalent to that of a fourth-grade elementary sc hool.\nE.4 Coreference Resolution\n• CLUEWSC2020 (\nXu et al , 2020b). The CLUEWSC2020 dataset serves as\na Chinese rendition of the coreference resolution task, demanding m odels to\nassess the co-reference relationships within sentences involving pronouns or noun\nphrases. The sentences in the samples are meticulously chosen from 36 con-\ntemporary literary works and annotated by linguistic experts. This dat aset is\nintegrated into CLUE as a benchmark for evaluating Chinese NLU tasks.\n• DPR (Rahman and Ng, 2012). The primary objective of the DPR dataset is to\naddress the referential connections involving target pronouns withi n sentences.\n132\nThe chosen sentences are sourced extensively, covering topics such as real events,\nmovie plots, and purely fictional content. Each sample comprises textual content,\na target pronoun, two candidate antecedents, and the correct answer.\n• WiC (Pilehvar and Camacho-Collados , 2019). The WiC dataset functions as\na lexical disambiguation task, posing a binary classification challenge in the\ncontext of sentence pairs. Tasking the model with evaluating two te xt segments\nand a word occurring in both sentences, the objective is to discer n whether the\nword holds identical meanings in the given contexts.\n• WinoGrande (Sakaguchi et al , 2021). The concept behind the WinoGrande\ndataset is rooted in WSC, with modifications undertaken to amplify data volume\nand enhance bias robustness. The dataset reconfigures the pronoun disam-\nbiguation task into a fill-in-the-blank structure, wherein the tar get pronoun is\nsubstituted with a blank space requiring selection from two candidate nouns that\nmatch the sentence’s meaning.\n• WinoWhy (\nZhang et al , 2020a). The WinoWhy dataset presents a novel task\nof elucidating pronoun reference connections, tasking models with choosing the\naccurate rationale from provided options for a pronoun that refers to a particular\nnoun. Regarded as an extension of WSC, WinoWhy comprises the original WSC\ndataset’s data and an additional 4,095 constructed pronoun reference reasons .\n• WSC (\nLevesque et al , 2012). The WSC dataset is utilized for tasks related to\npronoun disambiguation, necessitating models to infer the referen t noun of the\nannotated pronoun within the given context. The presented texts comm only\ninclude pairs of nearly identical sentences, distinguished by only a few words. In\nsituations where pronoun reference.\nE.5 Sentiment Analysis\n• EPRSTMT (Xu et al , 2021). The sentiment analysis data within the\nEPRSTMT dataset originates from product reviews on an e-commerce platform.\nSamples are categorized with either positive or negative sentiments.This dataset\nhas been included in FewCLUE.\n• IMDB (Maas et al, 2011). Derived from movie reviews on the IMDB website, the\nIMDB dataset comprises evaluations categorized as positive or negative sen ti-\nments. Each review is evenly distributed between positive and negative samples.\nThe authenticity and diversity of these reviews stem from real user contributions\non the movie website, enhancing the datasets’ representativenes s.\n• Sentiment140 (Go et al , 2009). Derived from tweet contents on Twitter, the\nSentiment140 dataset consists of tweets labeled with positive or negati ve senti-\nment. The data is curated by the creator through API calls, filtering tweets from\ndiverse domains such as consumer products, companies, individuals, and others,\nbased on their content.\n• SST-2 (Socher et al , 2013). The SST-2 dataset encompasses thoroughly anno-\ntated sentiment parse tree corpora. Extracted from movie reviews and parsed\nusing the Stanford parser, the annotations are conducted at the sentence level by\n133\nthree annotators. The reviews are categorized into positive and negative senti-\nments. The dataset is included in GLUE as an evaluation benchmark for English\nNLU tasks.\nE.6 Semantic Matching\n• AFQMC (Xu et al, 2020b). The AFQMC dataset originates from the Ant Tech-\nnology Exploration Conference Developer Competition, serving as a valuab le\nresource for Chinese semantic similarity tasks. The textual conten t is extracted\nfrom data within the Ant Financial platform, with a specific focus on th e finan-\ncial domain. This dataset has been incorporated into CLUE as an assessment\nbenchmark for Chinese NLU tasks.\n• BQ (\nChen et al, 2018). The BQ dataset serves as a corpus for recognizing seman-\ntic equivalence in Chinese sentences within the banking domain. Consisting of\n120K question pairs extracted from a year’s worth of online banking custome r\nservice logs, the dataset employs a clustering-based annotation approach to form\npositive and negative pairs by combining questions with similar and di ssimilar\nintents.\n• BUSTM (Xu et al, 2021). The BUSTM dataset focuses on the intent matching\ntask for short dialog texts. All textual content is sourced from the spoke n lan-\nguage text generated by OPPO’s Xiao Bu Assistant. The objective is to asse ss\nwhether the intent of the content is consistent across short spoken t exts.\n• DuQM (Zhu et al, 2022). The DuQM dataset serves as a Chinese robust dataset\nfor question matching, encompassing natural questions embedded with linguistic\nperturbations to assess the robustness of models in this particular task. DuQM\ncomprises three overarching categories and thirteen subcategories of linguis-\ntic perturbation types, facilitating a comprehensive evaluation of d iverse model\nperformances.\n• LCQMC (Liu et al, 2018). The LCQMC dataset constitutes an extensive Chi-\nnese corpus designed for the matching of questions, with a distinct focus on\naligning the intentions behind questions rather than achieving parap hrastic\nalignment of sentences. The dataset is curated by the creators through t he uti-\nlization of a search engine to gather question pairs related to high-freq uency\nwords across diverse domains, followed by a meticulous filtering pro cess for\nvalidation.\n• MRPC (Dolan and Brockett, 2005). The MRPC dataset serves as a prevalent\nbenchmark for semantic matching tasks at the sentence level. Its primary purpose\nis to assess the semantic similarity or synonymy between two sente nces. The\ntextual content is derived from news articles on the internet. Thi s dataset has\nbeen incorporated into GLUE as an assessment benchmark for English NLU\ntasks.\n• PA WS(\nZhang et al, 2019). Researchers observed a deficiency in current seman-\ntic matching datasets, specifically in the absence of sentence pairs exhibiting\nboth extensive lexical overlap and distinct semantic similarity. This observation\n134\nled to the introduction of the PAWS dataset, where all pairs of senten ces dis-\nplay substantial lexical commonality but may not align semantically, cr eating a\npotential source of confusion for models.\n• PA WS-X(Yang et al, 2019). In order to compensate for the lack of semantic\nmatching datasets in various languages, the PAWS-X dataset has been intro-\nduced. English sentence pairs from the original PAWS dataset underwent manual\ntranslation into six additional languages, specifically: French, Spanis h, German,\nChinese, Japanese, and Korean.\n• QQP (\nWang et al, 2018). Similar to MRPC, the QQP dataset is designed for\nsemantic matching tasks at the sentence level. It derives its data from the Quora\nQ&A community, an online platform dedicated to interactive Q&A. Incl uded in\nGLUE, this dataset serves as an evaluation benchmark for English NLU tasks.\n• STSB (Cer et al, 2017). In comparison to other semantic matching datasets, the\nSTSB dataset exhibits several noteworthy features. Firstly, its t extual content\nis drawn from diverse domains, encompassing realms such as news and so cial\nmedia. Secondly, diverging from the binary labels commonly used in se mantic\nmatching datasets (0 and 1 to denote similarity or dissimilarity), STSB employs\ncontinuous similarity scores, rated on a scale of 0 to 5, where higher scor es\ncorrelate with increased similarity. Lastly, the dataset incorporatestext in a total\nof 10 distinct languages. It has been incorporated into GLUE as an assessment\nbenchmark for English NLU tasks.\nE.7 Text Generation\n• CommonGen (\nLin et al , 2020). The CommonGen dataset serves the purpose\nof a delimited text generation task, linked with benchmark datasets, designed\nto explicitly evaluate models’ commonsense reasoning and text narrat ive capa-\nbilities. When presented with a group of concepts or common words, the\nmodel produces a cohesive sentence describing an everyday sce nario. This task\nresembles exercises in exams that involve constructing sentenc es using provided\nwords.\n• DART (\nNan et al, 2021). The DART dataset is utilized for the generation task\nof transforming structured data records into text in an open-domain con text.\nThe model is given structured data records in the form of sets of enti ty-\nrelation triplets, aiming to produce a textual description that en compasses all\nthe elements of the triplets.\n• E2E (Novikova et al, 2017). The E2E dataset serves as a training resource for\nnatural language generation systems tailored to the restaurant domain. Inputting\ndata pertaining to restaurants enables the generation of sentences that articulate\ndiverse information about the restaurant. The textual content withinthe dataset\nis meticulously composed, showcasing an extensive vocabulary and sy ntactic\nvariety.\n• WebNLG (Gardent et al, 2017). Much like DART, the WebNLG dataset serves\nthe purpose of mapping data to text. Extracted from DBpedia, the datase t con-\nsists of triplets, and the corresponding text represents the lin guistic expressions\nof these triplets. Models are tasked with generating a detailed and s eamlessly\n135\ncoherent textual description informed by the information encapsulat ed in the\ntriplets.\nE.8 Text Translation\n• IWSLT 2017 (Cettolo et al , 2017). The International Workshop on Spoken\nLanguage Translation (IWSLT) stands as a highly impactful competition, annu-\nally unveiling pertinent translation tasks and datasets. Notably, the IWSLT 2017\ndataset is recurrently utilized for both training and evaluation in tr anslation\ntasks, possessing a noteworthy level of representativeness. This dataset spans\nlanguages including English, French, German, and Arabic.\n• NLLB (Costa-juss` a et al, 2022). The No Language Left Behind (NLLB)\ninitiative stands as a text translation project, unveiling three op en-sourced\nbenchmarks for text translation evaluation: FLORES-200, NLLB-MD, and\nToxicity-200. Leveraging open-source models, the project enables th e provision\nof high-quality translations among a diverse set of over 200 languages, encom-\npassing even those with limited linguistic resources like Luganda an d Urdu. As\na result, its datasets for text translation offer substantial points of re ference.\n• WMT\n111. The WMT dataset consolidates translation competition datasets\npublicly disclosed by the Workshop on Statistical Machine Translation across\nmultiple years. It incorporates diverse sources, including newscommentaries and\nparliamentary records. The datasets within the WMT series are charact erized\nby their extensive data scale and encompassment of a wide range of languages .\nE.9 Text Summarization\n• AESLC (Zhang and Tetreault, 2019). The AESLC dataset is formed by aggre-\ngating email messages from employees at Enron Corporation. The objective i s\nto generate concise summaries for the textual content found in the emai l sub-\njects. The creators argue that, in contrast to news articles where the initial and\nconcluding sentences typically offer a summarizing overview of th e article, the\nemail domain presents a more challenging context.\n• CNewSum (Wang et al, 2021a). The CNewSum dataset caters to the require-\nments of Chinese news summarization endeavors. The creators curate a dataset\ncomprising 304K extensive documents accompanied by manually generatednews\nsummaries. Two distinctive attributes characterize CNewSum: fir stly, it facili-\ntates model comprehension and summarization at the document level; s econdly,\nthe test set incorporates comprehensive and inferential annotations on the\nsummaries, offering researchers a means to scrutinize and identif y potential\nperformance constraints of the models.\n• CNN-DM (See et al , 2017). Utilizing a corpus exceeding 300K news articles\nfrom CNN and The Daily Mail, the CNN-DM dataset has been curated. Each\ninstance comprises an article paired with its corresponding summary , facilitat-\ning the training and evaluation of models for text summarization. The mos t\n111https://www.statmt.org/wmt22/index.html\n136\nrecent iteration accommodates both extractive and generative summari zation\ntechniques.\n• Gigaword (Rush et al , 2015). The Gigaword dataset is an English text sum-\nmarization task dataset, comprising approximately 4M samples. The conte nt of\nthe dataset is derived from global news over the past two decades. The c reators\nhave pruned and filtered the data based on heuristic filters. Each fin al sample\nincludes the textual content and a summary headline.\n• LCSTS (Hu et al , 2015). The LCSTS dataset represents a compilation of\nChinese short-text summaries sourced from Sina Weibo, a widely us ed Chi-\nnese microblogging platform. With a voluminous scale exceeding 2.4M ent ries,\neach instance originates from genuine short texts composed by users of Si na\nWeibo, each supplemented with a succinct summary. Social media t extual con-\ntent exhibits traits such as brevity, a broad spectrum of language styl es, and\nheightened levels of noise.\n• MediaSum (Zhu et al, 2021). In contrast to other text summarization datasets,\nwhich rely on news articles, the MediaSum dataset pivots towards the realm\nof media interviews. The creators have curated interview transcr ipts sourced\nfrom NPR and CNN, utilizing summaries and topic descriptions as abstracts .\nThe content encompasses intricate and multifaceted dialogues among mul tiple\nparties.\n• MultiNews (Fabbri et al , 2019). Derived from articles on news websites and\nsummaries meticulously curated by seasoned editors, the MultiNews dataset\nboasts a diverse array of news sources, spanning across more than 1.5K uniq ue\nsites.\n• Newsroom (Grusky et al , 2018). Constructed from 1.3M articles and their\nassociated summaries, the Newsroom dataset is a compilation from 38 leadin g\nnews publishers. The selected articles span the timeframe from 1998 to 2017,\nand the abstracts undergo preprocessing employing a diverse arra y of extractive\nand abstractive strategies.\n• Opinion Abstracts (Wang and Ling , 2016). The creators of the Opinion\nAbstracts dataset gather data on movie reviews and debates for text summa-\nrization tasks. On one hand, they construct a consensus comment for each movie\nbased on expert opinions in the reviews, summarizing the content andtendencies.\nOn the other hand, they collect points for and against from debate discuss ions.\nThe central ideas of the debates are summarized in a single sentence b ased on\nthe debate topic and relevant arguments.\n• SAMSum (\nGliwa et al , 2019). Within the SAMSum dataset, one can find\naround 16K dialogues designed to emulate real-time messaging conversations ,\naccompanied by corresponding summaries. Proficient linguists, well -versed in\nEnglish, meticulously crafted and recorded these dialogues, infusi ng them with\nvaried styles and language elements, including slang, emoticons, and oc casional\nerrors, thereby presenting fresh challenges for text summarizati on tasks.\n• WikiHow (Koupaee and Wang, 2018). The majority of existing datasets for text\nsummarization originate from news articles, characterized by a distin ct writing\n137\nstyle. To address the scarcity of text in alternative genres and styl es, the Wik-\niHow dataset has been introduced. Comprising over 230K pairs of articles and\nsummaries, the dataset is sourced from a diverse range of authors contrib uting\nto an online knowledge repository.\n• WikiLingua (Ladhak et al, 2020). The WikiLingua dataset serves the purpose\nof assessing cross-lingual abstract summarization tasks. Approximately 770K\npairs of articles and summaries are extracted by the creators from the Wi kiHow\nwebsite, encompassing 18 diverse languages. WikiHow constitutes a r epository\nof multi-themed procedural guides, composed by human contributors. These\nguides typically feature instructive visuals, succinct summari es, and in-depth\ndetails. The information from the details and summaries under the same guide\nis amalgamated to yield article-summary pairs.\n• XL-Sum (Hasan et al, 2021). The XL-Sum dataset encompasses 1.35M pairs of\nprofessionally annotated articles and summaries, extracted from BBC th rough\nheuristic approaches, exclusively tailored for text summarization t asks. Encom-\npassing 45 languages, over two-thirds of which qualify as low-resource languages,\nthe dataset is designed to facilitate research in multilingual summar ization.\n• XSum (Narayan et al , 2018). Functioning as a dataset tailored for single-\ndocument summarization tasks, XSum draws its content from online arti cles\ncurated by the British Broadcasting Corporation. Spanning the timefr ame from\n2010 to 2017, this dataset explores diverse domains, including family, sci ence,\nand weather. Notably, in contrast to CNN-DM, both the textual content and\nsummaries within XSum are more concise, while simultaneously showc asing a\nmore extensive lexicon.\nE.10 Text Classiﬁcation\n• AGNEWS (\nZhang et al, 2015). The AGNEWS dataset encompasses 497K news\narticles sourced from a diverse array of over 2K news outlets. It func tions as\nan evaluative benchmark for gauging the efficacy of models in the realm of\nnews article topic classification. The news topics are broadly classifie d into four\ncategories: world, sports, business, and science & technology.\n• CSLDCP (\nXu et al , 2021). The CSLDCP dataset constitutes a subject clas-\nsification dataset for Chinese scientific literature, encompassing 67 categories\nthat span a spectrum from social sciences to natural sciences. Examp les of these\ncategories include, but are not limited to, “horticulture” and “mechani cal engi-\nneering.” The content to be classified comprises excerpts from the abstracts of\nChinese literature.\n• IFLYTEK (Xu et al , 2020b). Utilized for the Chinese long-text classification\ntask, the IFLYTEK dataset encompasses more than 17K extensive texts foc us-\ning on app application descriptions. These texts are systematically or ganized\ninto 119 categories based on the functional themes of the respective apps, includ-\ning but not limited to “ride-hailing,” “map navigation,” and “payment.” The\nextensive variety of categories poses a considerable challenge for class ification.\nAs part of CLUE, IFLYTEK serves as an evaluative benchmark for tasks related\nto Chinese NLU.\n138\n• MARC (Keung et al , 2020). The MARC dataset comprises a multilingual\nassemblage designed for the categorization of Amazon product reviews. Prod uct\nreviews are presented in multiple languages, including English, J apanese, Ger-\nman, French, Chinese, and Spanish. Each specific instance include s a review,\nstar rating, and broad product category, covering classifications like “books” and\n“home appliances.”\n• THUCNews112. The THUCNews dataset originates from the curated historical\ndata of Sina Weibo’s subscription channels between 2005 and 2011. Following a\nrigorous screening and refinement process, a corpus of 740K pertinent documents\nhas been meticulously restructured and classified into 14 distinc t thematic cate-\ngories within the Sina News classification framework. These encompass fi nance,\nlottery, real estate, stocks, home decor, education, technology, soci ety, fashion,\ncurrent affairs, sports, astrology, gaming, and entertainment.\n• TNEWS (Xu et al , 2020b). The TNEWS dataset serves the purpose of news\nheadline classification, comprising Chinese news headlines source d from the\nToutiao platform by Bytedance, up until May 2018. In its entirety, TNEWS\nencompasses 73.3K headlines, systematically categorized into 15 section s corre-\nsponding to different news genres, namely story, culture, entert ainment, sports,\nfinance, house, car, education, technology, military, travel, world, stock, agricul-\nture, and game. Selected as an assessment benchmark for Chinese NLU tasks ,\nthis dataset has been incorporated into CLUE.\nE.11 Text Quality Evaluation\n• CoLA (\nWarstadt et al, 2019). The CoLA dataset explores models’ proficiency\nin evaluating the grammatical accuracy of sentences. Comprising 10K Engli sh\nsentences, the dataset includes both grammatically correct and errone ous sen-\ntences. The task doesn’t mandate the model to identify specific e rror locations\nor undertake corrections; rather, it focuses on determining correc tness, present-\ning itself as a binary classification task. This dataset has been incorpor ated into\nGLUE as a benchmark for evaluating English NLU tasks.\n• CSCD-IME (Hu et al , 2022b). The CSCD-IME dataset marks a pioneering\neffort to address errors induced by Chinese Pinyin input methods in the context\nof Chinese spelling correction. The sentences targeted for correct ion originate\nfrom posts on Sina Weibo. The spelling errors introduced by Pinyin input\nmethods manifest specific distributions at both the Pinyin and sem antic lev-\nels, presenting a considerable level of complexity. Notably, this dataset currently\nrepresents the most extensive collection for Chinese spelling c orrection tasks.\n• SIGHAN (Wu et al , 2013; Yu et al , 2014; Tseng et al , 2015). The SIGHAN\ndataset, made publicly available by scholars, serves as a resource for C hi-\nnese text correction. Presently, it encompasses three editions: SIGHAN2013,\nSIGHAN2014, and SIGHAN2015. The objective of the task is to evaluate the\nproficiency of models in Chinese spell checking, involving disti nct subtasks such\nas detecting error positions and performing error corrections.\n112https://github.com/thunlp/THUCTC\n139\n• YACLC (Wang et al , 2021b). Multiple universities collaborate to create the\nYACLC dataset, featuring Chinese text samples. Graduate students specializing\nin pertinent fields are enlisted to assess the acceptability of Chi nese sentences,\ncontributing annotations for both correction and fluency. Corrections in volve\ngrammatical adjustments to align sentences with Chinese grammar standar ds,\nwhereas fluency annotations focus on refining sentences for improved smooth-\nness and authenticity, in accordance with prevalent Chinese commun ication\nnorms. YACLC finds practical utility in tasks like grammar correction and text\nproofreading.\nE.12 Text-to-Code\n• CSpider (Min et al , 2019). The CSpider dataset represents a Chinese variant\nof the Text-to-SQL dataset, translated by researchers from the original English\nSpider dataset. In pursuit of diversity, sentences conveying similar meanings are\ntranslated into distinct expressions in Chinese to uphold richn ess. Concerning\nspecifics, the databases’ table and column names remain unaltered in En glish,\nwith the exception of localized treatment for certain personal and geograp hical\nnames.\n• DuSQL (Wang et al, 2020a). The DuSQL dataset functions as a Chinese dataset\ndesigned for the cross-domain Text-to-SQL task, encompassing 200 database s,\n813 tables, and 23,979 question-SQL pairs. The primary focus of the task lie s\nin practical applications, spanning a breadth of 164 domains. The question s\nmanifest in common formats, including matching, computation, and infer ence,\nthereby closely resembling scenarios encountered in real-world ap plications.\n• MBPP (Austin et al, 2021). The MBPP dataset serves as a benchmark for code\ngeneration, comprising 974 crowdsourced Python programming questions. These\nprogramming questions cover fundamental programming knowledge, stand ard\nlibrary functionalities, and more. Each question includes a task de scription, a\ncode solution, and three automated test cases.\n• Spider (Yu et al, 2018). The Spider dataset encompasses Text-to-SQL dataset\nin English, annotated by a student cohort, totaling 10,181 questions, 5,693 SQ L\nqueries, and 200 databases. The inclusion of varied and intricate SQL queries and\ndatabases across both the training and test sets presents a formidable c hallenge.\nE.13 Named Entity Recognition\n• CLUENER (Xu et al , 2020a). The CLUENER dataset originates from a sub-\nset of the THUCNews text classification dataset, carefully selected to f acilitate\ndetailed annotation of named entities. There exist 10 distinct entit y cate-\ngories, encompassing address, book, company, game, government, movie, name,\norganization, position, and scene.\n• CoNLL2003 (Tjong Kim Sang and De Meulder, 2003). The CoNLL2003 dataset\nis introduced during the CoNLL-2003 shared task, establishing itself as a bench-\nmark within the NER domain. Entity categories within the dataset inclu de\n140\npersonal names, organizational names, geographical locations, among others.\nThe dataset is presented in both English and German variants.\n• Few-NERD (\nDing et al, 2021). The Few-NERD dataset constitutes an extensive\nand finely annotated resource for NER tasks. The dataset encompasses 188,200\nsentences, 491,711 entities, and 4,601,223 labels. The entities are classified into 8\nbroader categories and 66 more specific categories. The creators have established\nthree benchmark tasks, involving one supervised task and two tasks with limited\nsamples.\n• MSRA (\nLevow, 2006). The MSRA dataset is utilized in the NER task of the\nThird International Chinese LanguageProcessing Bakeoff. The competiti on fur-\nnishes two corpora in simplified Chinese and one in traditional Chine se for both\ntraining and testing purposes. The entities cover locations, per sonal names, and\norganizational names.\n• OntoNotes 5.0 (Weischedel et al, 2012). The OntoNotes dataset has evolved\nto its final version, denoted as Version 5.0. It stands as a multigenre and m ulti-\nlingual corpus, meticulously annotated with syntactic, semantic, and discourse\ninformation. The dataset has been extended to serve as a NER task dataset f or\nthe CoNLL-2012 shared task, featuring three languages—English, Chinese, and\nArabic—and spanning across 18 entity categories.\n• Resume (\nZhang and Yang, 2018). The Resume dataset is compiled using several\nresume profiles from Sina Finance. The creators conduct manual annotation s\nfor eight distinct categories of named entities, encompassing nationali ty, educa-\ntional history, geographic locations, individual names, organizational titles, field\nof study, ethnicity, and professional designations.\n• Taobao NER (Jie et al, 2019). The Taobao NER dataset serves as openly avail-\nable resources for NER in the e-commerce sector, crafted from the e-com merce\ndata of Taobao. The entity categories are categorized into four broader types\n(pattren, product, brand, misc) and nine more specific types (model Type, prod-\nuct description, core product, brand description, core brand, loc ation, person,\nliterature, product specification).\n• Weibo NER (\nPeng and Dredze, 2015). The Weibo NER dataset acts as openly\navailable resources for NER in the realm of social media, compiled from We ibo\ninformation. The entity categories include geopolitical entities, geogr aphical\nlocations, institutional names, and personal names, offering a more nuanc ed\nperspective compared to MSRA.\n• WUNT2017 (Derczynski et al, 2017). The central emphasis of the WUNT2017\ndataset is on recognizing unconventional and hitherto unencountered e nti-\nties within a new context. It assesses the capacities of models to d etect and\ncategorize emerging named entities amidst noisy textual data. The enti ty cat-\negories encompass corporations, creative works, groups, locations, person s, and\nproducts.\n• Youku NER (Jie et al, 2019). The Youku NER dataset functions as an openly\naccessible resource for NER within the entertainment domain. Deriv ed from\ntitles linked to Youku videos, the entity categories are delineated into three\n141\noverarching types (figure, program, misc) and nine more specific type s (figure,\nvariety show, movie, animation, TV drama, character, number, location , song).\nE.14 Relation Extraction\n• Dialogue RE (Yu et al , 2020a). The Dialogue RE dataset stands as the ini-\ntial manually annotated dataset for relation extraction based on dialogues. It\noriginates from 1,788 dialogues extracted from the American sitcom “Friends. ”\nAnnotators have meticulously labeled instances of 36 relationship type s within\nthe dialogues, offering versions in both Chinese and English.\n• DocRED (\nYao et al , 2019). Functioning as a dataset for document-level RE,\nDocRED draws its textual content from Wikipedia and Wikidata. With a com-\nposition of 132,375 entities, 56,354 relationship facts, and 5,053 documents, th e\ndataset challenges models to engage with multiple sentences withi n a docu-\nment for entity recognition and relationship inference through the sy nthesis of\ndocument-level information. This significantly diverges from datase t focused on\nRE at the sentence level.\n• FewRel (Han et al, 2018). The FewRel dataset comes in two iterations, denoted\nas versions 1.0 and 2.0. The inaugural version, 1.0, represents the pioneer -\ning integration of few-shot learning with RE. The training set incorp orates 64\ndistinct relationships, and the test set comprises 16 relationship s. Version 2.0\nintroduces challenges in domain adaptation and the detection of categories not\ncovered above, evaluating the models’ transferability and OOD gene ralization\ncapabilities.\n• TACRED (Zhang et al , 2017). The TACRED dataset encompasses 106,264\ninstances designed for relation extraction tasks. These instances are drawn from\nnews articles and online texts utilized in the annual Text Analysis C onference\nKnowledge Base Population (TACKBP). In total, the dataset encompasses 41\ndistinct relationship types among diverse entities or denotes the absence of a\nrelationship.\nE.15 Multitask\n• CSL (\nLi et al , 2022b). The CSL dataset represents a sizable Chinese scien-\ntific literature database, incorporating titles, abstracts, keywords , and academic\ndomain details from 396K papers. Beyond serving as a pretraining corpus, it can\nbe configured into distinct NLP task datasets. The creators have employ ed it in\ntasks like predicting titles, generating keywords, and classifyi ng papers.\n• METS-CoV (Zhou et al , 2022). The METS-CoV dataset provides medical\nannotations for COVID-19-related social media texts, facilitating tasks in NER\nand sentiment analysis. A collection of 10K tweets is manually annotated,\nencompassing four medical entity categories (disease, drug, symptom, vaccine)\nand three general entity categories (person, location, organization). In e xplor-\ning sentiment attitudes toward specific entities, sentiment p olarity labels are\nadditionally applied to individuals, organizations, drugs, and vaccines .\n142\n• QED (Lamm et al , 2021). Derived from a scalable framework that furnishes\nexplanations in Q&A scenarios, the QED dataset delineates explanations for\nanswers on Natural Questions as discrete, human-understandable step c ombi-\nnations. Each instance is sourced from samples in Natural Questions and is\naccompanied by QED-style explanatory annotations. This dataset is applic able\nto tasks like single-sentence selection, answer selection, equality recognition, and\nthe extraction of inference patterns.\nReferences\nAbadji J, Ortiz Suarez P, Romary L, Sagot B (2022) Towards a cleaner document-\noriented multilingual crawled corpus. In: Proceedings of the Thir teenth Language\nResources and Evaluation Conference. ELRA, Marseille, France, pp 4344–4355\nAchiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al (2023) GPT-4\ntechnical report. arXiv preprint\narXiv:2303.08774\nAggarwal S, Mandowara D, Agrawal V, Khandelwal D, Singla P, Garg D (2021)\nExplanations for CommonsenseQA: New dataset and models. In: Zong C, Xia F,\nLi W, Navigli R (eds) Proceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th International Joint Conferen ce\non Natural Language Processing (Volume 1: Long Papers). ACL, pp 3050–3065,\nhttps://doi.org/10.18653/v1/2021.acl-long.238\nAiyappa R, An J, Kwak H, Ahn Yy (2023) Can we trust the evaluation on ChatGPT?\nIn: Ovalle A, Chang KW, Mehrabi N, Pruksachatkun Y, Galystan A, Dhamala\nJ, et al (eds) Proceedings of the 3rd Workshop on Trustworthy Natural Language\nProcessing (TrustNLP 2023). ACL, Toronto, Canada, pp 47–54,\nhttps://doi.org/10.\n18653/v1/2023.trustnlp-1.5\nAlex N, Lifland E, Tunstall L, Thakur A, Maham P, Riedel CJ, et al (2021) RAFT:\nA real-world few-shot text classification benchmark. In: Thirty-fif th Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track ( Round\n2), pp 1–12\nAmini A, Gabriel S, Lin S, Koncel-Kedziorski R, Choi Y, Hajishirzi H (2019) MathQA:\nTowards interpretable math word problem solving with operation-basedformalisms.\nIn: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conferen ce of the\nNorth American Chapter of the Association for Computational Linguistics: Hum an\nLanguage Technologies, Volume 1 (Long and Short Papers). ACL, Minneapolis,\nMinnesota, pp 2357–2367,\nhttps://doi.org/10.18653/v1/N19-1245\nAn C, Gong S, Zhong M, Li M, Zhang J, Kong L, et al (2023) L-Eval: Insti-\ntuting standardized evaluation for long context language models. arXiv pre print\narXiv:2307.11088\n143\nAnand Y, Nussbaum Z, Treat A, Miller A, Guo R, Schmidt B, et al (2023)\nGPT4All: An ecosystem of open source compressed language models. arXiv preprint\narXiv:2311.04931\nAnonymous (2024) ToolLLM: Facilitating large language models to master 16000+\nreal-world APIs. In: The Twelfth International Conference on Learning Represen-\ntations, pp 1–24\nAroca-Ouellette S, Paik C, Roncone A, Kann K (2021) PROST: Physical reasonin g\nabout objects through space and time. In: Zong C, Xia F, Li W, Navigli R (eds)Find-\nings of the Association for Computational Linguistics: ACL-IJCNLP 2021. ACL, pp\n4597–4608,\nhttps://doi.org/10.18653/v1/2021.findings-acl.404\nAskell A, Bai Y, Chen A, Drain D, Ganguli D, Henighan T, et al (2021) A general\nlanguage assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861\nAustin J, Odena A, Nye M, Bosma M, Michalewski H, Dohan D, et al (2021) Program\nsynthesis with large language models. arXiv preprint arXiv:2108.07732\nAzerbayev Z, Schoelkopf H, Paster K, Dos Santos M, McAleer S, Jiang AQ, et al (2023)\nLlemma: An open language model for mathematics. arXiv preprintarXiv:2310.06786\nBach S, Sanh V, Yong ZX, Webson A, Raffel C, Nayak NV, et al (2022) Prompt-\nSource: An integrated development environment and repository for natural language\nprompts. In: Basile V, Kozareva Z, Stajner S (eds) Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics: System Demon-\nstrations. Association for Computational Linguistics, Dublin, Ireland, pp 93–104,\nhttps://doi.org/10.18653/v1/2022.acl-demo.9\nBai J, Bai S, Chu Y, Cui Z, Dang K, Deng X, et al (2023a) QWen technical report.\narXiv preprint arXiv:2309.16609\nBai Y, Jones A, Ndousse K, Askell A, Chen A, DasSarma N, et al (2022) Training a\nhelpful and harmless assistant with reinforcement learning from hu man feedback.\narXiv preprint arXiv:2204.05862\nBai Y, Lv X, Zhang J, Lyu H, Tang J, Huang Z, et al (2023b) Longbench: A\nbilingual, multitask benchmark for long context understanding. arXiv p reprint\narXiv:2308.14508\nBai Y, Ying J, Cao Y, Lv X, He Y, Wang X, et al (2023c) Benchmarking founda-\ntion models with language-model-as-an-examiner. In: Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track, pp 1–26\nBandy J, Vincent N (2021) Addressing “documentation debt” in machine lear ning:\nA retrospective datasheet for bookcorpus. In: Thirty-fifth Confere nce on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 1), pp\n144\n1–13\nBa˜ n´ on M, Chen P, Haddow B, Heafield K, Hoang H, Espl` a-Gomis M, et al (2020)\nParaCrawl: Web-scale acquisition of parallel corpora. In: Jurafsky D, Ch ai J,\nSchluter N, Tetreault J (eds) Proceedings of the 58th Annual Meeting ofthe Associ-\nation for Computational Linguistics. ACL, pp 4555–4567,https://doi.org/10.18653/\nv1/2020.acl-main.417\nBao Z, Chen W, Xiao S, Ren K, Wu J, Zhong C, et al (2023) DISC-MedLLM: Bridging\ngeneral large language models and real-world medical consultation. arXiv prep rint\narXiv:2308.14346\nBar-Haim R, Dagan I, Dolan B, Ferro L, Giampiccolo D, Magnini B, et al (2006)\nThe second PASCAL recognising textual entailment challenge. URL https://api.\nsemanticscholar.org/CorpusID:13385138\nBartolo M, Roberts A, Welbl J, Riedel S, Stenetorp P (2020) Beat the AI: Inv esti-\ngating adversarial human annotation for reading comprehension. Transactions of\nthe Association for Computational Linguistics 8:662–678. https://doi.org/10.1162/\ntacl a 00338\nBaumgartner J, Zannettou S, Keegan B, Squire M, Blackburn J (2020) The Push shift\nreddit dataset. In: Proceedings of the international AAAI conference on w eb and\nsocial media, pp 830–839\nBELLEGroup (2023) BELLE: Be everyone’s large language model engine. https://\ngithub.com/LianjiaTech/BELLE\nBentivogli L, Clark P, Dagan I, Giampiccolo D (2009) The fifth PASCAL recognizing\ntextual entailment challenge. TAC 7:8\nBisk Y, Zellers R, Le bras R, Gao J, Choi Y (2020) PIQA: Reasoning about physical\ncommonsense in natural language. Proceedings of the AAAI Conference on Artificial\nIntelligence 34(05):7432–7439. https://doi.org/10.1609/aaai.v34i05.6239\nBowman SR, Angeli G, Potts C, Manning CD (2015) A large annotated corpus\nfor learning natural language inference. In: M` arquez L, Callison-Burch C, Su J\n(eds) Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-\nguage Processing. ACL, Lisbon, Portugal, pp 632–642, https://doi.org/10.18653/\nv1/D15-1075\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al (2020)\nLanguage models are few-shot learners. In: Neuromuscular junction. Handbo ok\nof experimental pharmacology. Curran Associates Inc., Red Hook, NY, USA, pp\n1877–1901\n145\nCaballero E, OpenAI , Sutskever I (2016) Description2Code dataset. https://github.\ncom/ethancaballero/description2code\nCer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L (2017) SemEval-2017 task 1:\nSemantic textual similarity multilingual and crosslingual focused e valuation. In:\nBethard S, Carpuat M, Apidianaki M, Mohammad SM, Cer D, Jurgens D (eds)\nProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-\n2017). ACL, Vancouver, Canada, pp 1–14,\nhttps://doi.org/10.18653/v1/S17-2001\nCettolo M, Federico M, Bentivogli L, Niehues J, St¨ uker S, Sudoh K, e t al (2017)\nOverview of the IWSLT 2017 evaluation campaign. In: Proceedings of the 14th\nInternational Workshop on Spoken Language Translation, pp 2–14\nChalkidis I, Jana A, Hartung D, Bommarito M, Androutsopoulos I, Katz D, et al\n(2022) LexGLUE: A benchmark dataset for legal language understanding in English.\nIn: Muresan S, Nakov P, Villavicencio A (eds) Proceedings of the 60th An nual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers).\nACL, Dublin, Ireland, pp 4310–4330,\nhttps://doi.org/10.18653/v1/2022.acl-long.\n297\nChang Y, Wang X, Wang J, Wu Y, Zhu K, Chen H, et al (2023) A survey on evaluation\nof large language models. arXiv preprint arXiv:2307.03109\nChaudhary S (2023) Code Alpaca: An instruction-following LLaMA model for code\ngeneration. https://github.com/sahil280114/codealpaca\nChen J, Chen Q, Liu X, Yang H, Lu D, Tang B (2018) The BQ corpus: A large-scale\ndomain-specific Chinese corpus for sentence semantic equivalence identification. In:\nRiloff E, Chiang D, Hockenmaier J, Tsujii J (eds) Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing. ACL, Brusse ls,\nBelgium, pp 4946–4951, https://doi.org/10.18653/v1/D18-1536\nChen M, Tworek J, Jun H, Yuan Q, Pinto HPdO, Kaplan J, et al (2021) Evaluating\nlarge language models trained on code. arXiv preprint arXiv:2107.03374\nChen W, Wang Q, Long Z, Zhang X, Lu Z, Li B, et al (2023a) DISC-FinLLM: A\nChinese financial large language model based on multiple experts fine-tuning. arXiv\npreprint arXiv:2310.15205\nChen W, Yin M, Ku M, Lu P, Wan Y, Ma X, et al (2023b) TheoremQA: A theorem-\ndriven question answering dataset. In: Bouamor H, Pino J, Bali K (eds) Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing.\nACL, Singapore, pp 7889–7901,\nhttps://doi.org/10.18653/v1/2023.emnlp-main.489\nChen Y, Cai W, Wu L, Li X, Xin Z, Fu C (2023c) TigerBot: An open multilingual\nmultitask LLM. arXiv preprint arXiv:2312.08688\n146\nChen Z, Jiang F, Chen J, Wang T, Yu F, Chen G, et al (2023d) Phoenix:\nDemocratizing ChatGPT across languages. arXiv preprint arXiv:2304.10453\nCheng Q, Sun T, Zhang W, Wang S, Liu X, Zhang M, et al (2023) Evaluating\nhallucinations in Chinese large language models. arXiv preprint arXiv:2310.03368\nChoi E, He H, Iyyer M, Yatskar M, Yih Wt, Choi Y, et al (2018) QuAC: Question\nanswering in context. In: Riloff E, Chiang D, Hockenmaier J, Tsujii J(eds) Proceed-\nings of the 2018 Conference on Empirical Methods in Natural Language Processin g.\nACL, Brussels, Belgium, pp 2174–2184, https://doi.org/10.18653/v1/D18-1241\nChoi M, Pei J, Kumar S, Shu C, Jurgens D (2023) Do LLMs understand social kno wl-\nedge? Evaluating the sociability of large language models with SocKET benchmark.\nIn: Bouamor H, Pino J, Bali K (eds) Proceedings of the 2023 Conference on Emp ir-\nical Methods in Natural Language Processing. ACL, Singapore, pp 11370–11403,\nhttps://doi.org/10.18653/v1/2023.emnlp-main.699\nChong R, Lu L, Yang L, Nie J, Zhou S, Li Y, et al (2023) MCTS: A multi-reference\nChinese text simplification dataset. arXiv preprint arXiv:2306.02796\nChristiano PF, Leike J, Brown T, Martic M, Legg S, Amodei D (2017) Deep rei n-\nforcement learning from human preferences. In: Guyon I, Luxburg UV, B engio S,\nWallach H, Fergus R, Vishwanathan S, et al (eds) Advances in Neural Information\nProcessing Systems, vol 30. Curran Associates, Inc., pp 1–9\nClark C, Lee K, Chang MW, Kwiatkowski T, Collins M, Toutanova K (2019) BoolQ :\nExploring the surprising difficulty of natural yes/no questions. In: Burstein J, Doran\nC, Solorio T (eds) Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers). ACL, Minneapolis, Minnesota, pp\n2924–2936,\nhttps://doi.org/10.18653/v1/N19-1300\nClark JH, Choi E, Collins M, Garrette D, Kwiatkowski T, Nikolaev V, et al (2020)\nTyDiQA: A benchmark for information-seeking question answering in t ypologically\ndiverse languages. Transactions of the Association for Computational Linguist ics\n8:454–470. https://doi.org/10.1162/tacl a 00317\nClark P, Cowhey I, Etzioni O, Khot T, Sabharwal A, Schoenick C, et al (2018) T hink\nyou have solved question answering? Try ARC, the AI2 reasoning challenge . arXiv\npreprint arXiv:1803.05457\nCobbe K, Kosaraju V, Bavarian M, Chen M, Jun H, Kaiser L, et al (2021) Training\nverifiers to solve math word problems. arXiv preprint arXiv:2110.14168\nConneau A, Kiela D (2018) SentEval: An evaluation toolkit for universal sente nce\nrepresentations. In: Calzolari N, Choukri K, Cieri C, Declerck T, Goggi S, Hasida\nK, et al (eds) Proceedings of the Eleventh International Conference on Language\n147\nResources and Evaluation (LREC 2018). European Language Resources Association\n(ELRA), Miyazaki, Japan, pp 1699–1704\nConneau A, Rinott R, Lample G, Williams A, Bowman S, Schwenk H, et al (2018)\nXNLI: Evaluating cross-lingual sentence representations. In: Riloff E, Chiang D,\nHockenmaier J, Tsujii J (eds) Proceedings of the 2018 Conference on Emp irical\nMethods in Natural Language Processing. ACL, Brussels, Belgium, pp 2475–2485,\nhttps://doi.org/10.18653/v1/D18-1269\nConneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzm´ an F, et al\n(2020) Unsupervised cross-lingual representation learning at scale. In: Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics. ACL,\npp 8440–8451, https://doi.org/10.18653/v1/2020.acl-main.747\nConover M, Hayes M, Mathur A, Xie J, Wan J, Shah S, et al (2023) Free Dolly: Intro-\nducing the world’s first truly open instruction-tuned LLM. https://www.databricks.\ncom/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned -llm\nContributors O (2023) OpenCompass: A universal evaluation platform for found ation\nmodels. https://github.com/open-compass/opencompass\nCosta-juss` a MR, Cross J, C ¸ elebi O, Elbayad M, Heafield K, Heffernan K, et al\n(2022) No language left behind: Scaling human-centered machine translation. arXiv\npreprint arXiv:2207.04672\nCui G, Yuan L, Ding N, Yao G, Zhu W, Ni Y, et al (2023) Ultrafeedback: Boosting\nlanguage models with high-quality feedback. arXiv preprint arXiv:2310.01377\nCui Y, Liu T, Che W, Xiao L, Chen Z, Ma W, et al (2019) A span-extraction\ndataset for Chinese machine reading comprehension. In: Inui K, Jiang J, Ng V,\nWan X (eds) Proceedings of the 2019 Conference on Empirical Methods in Nat -\nural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp 5883–5889,\nhttps://doi.org/10.18653/v1/D19-1600\nCui Y, Liu T, Yang Z, Chen Z, Ma W, Che W, et al (2020) A sentence cloze dataset\nfor Chinese machine reading comprehension. In: Scott D, Bel N, Zong C ( eds)\nProceedings of the 28th International Conference on Computational Linguist ics.\nInternational Committee on Computational Linguistics, Barcelona, Spain (O nline),\npp 6717–6723, https://doi.org/10.18653/v1/2020.coling-main.589\nDagan I, Glickman O, Magnini B (2006) The PASCAL recognising textual entail-\nment challenge. In: Qui˜ nonero-Candela J, Dagan I, Magnini B, d’Alch´ e Buc F (eds)\nMachine Learning Challenges. Evaluating Predictive Uncertainty, Vis ual Object\nClassification, and Recognising Tectual Entailment. Springer Berlin Heidelberg,\nBerlin, Heidelberg, pp 177–190\n148\nDai Y, Feng D, Huang J, Jia H, Xie Q, Zhang Y, et al (2023) LAiW: A Chi-\nnese legal large language models benchmark (a technical report). arXiv pre print\narXiv:2310.05620\nDan Y, Lei Z, Gu Y, Li Y, Yin J, Lin J, et al (2023) EduChat: A large-scale\nlanguage model-based chatbot system for intelligent education. arXiv pre print\narXiv:2308.02773\nDasigi P, Liu NF, Marasovi´ c A, Smith NA, Gardner M (2019) Quoref: A reading\ncomprehension dataset with questions requiring coreferential reas oning. In: Inui K,\nJiang J, Ng V, Wan X (eds) Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp\n5925–5932,\nhttps://doi.org/10.18653/v1/D19-1606\nDasigi P, Lo K, Beltagy I, Cohan A, Smith NA, Gardner M (2021) A dataset\nof information-seeking questions and answers anchored in research pape rs. In:\nToutanova K, Rumshisky A, Zettlemoyer L, Hakkani-Tur D, Beltagy I, Bet hard\nS, et al (eds) Proceedings of the 2021 Conference of the North American Chapt er\nof the Association for Computational Linguistics: Human Language Technologies.\nACL, pp 4599–4610,\nhttps://doi.org/10.18653/v1/2021.naacl-main.365\nDe Marneffe MC, Simons M, Tonhauser J (2019) The CommitmentBank: Investigating\nprojection in naturally occurring discourse. In: proceedings of S inn und Bedeutung,\npp 107–124\nDeng C, Zhang T, He Z, Chen Q, Shi Y, Zhou L, et al (2023) K2: A foundation lan-\nguage model for geoscience knowledge understanding and utilization. arXiv preprint\narXiv:2306.05064\nDerczynski L, Nichols E, van Erp M, Limsopatham N (2017) Results of the WNUT2017\nshared task on novel and emerging entity recognition. In: DerczynskiL, Xu W, Ritter\nA, Baldwin T (eds) Proceedings of the 3rd Workshop on Noisy User-generated Text.\nACL, Copenhagen, Denmark, pp 140–147, https://doi.org/10.18653/v1/W17-4418\nDevlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of deep b idi-\nrectional Transformers for language understanding. In: Proceedings of th e 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Minneapolis, MN, USA, pp 4171–4186\nDing N, Xu G, Chen Y, Wang X, Han X, Xie P, et al (2021) Few-NERD: A few-\nshot named entity recognition dataset. In: Zong C, Xia F, Li W, Navigli R (e ds)\nProceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers). ACL, pp 3198–3213,\nhttps://doi.org/10.18653/\nv1/2021.acl-long.248\n149\nDing N, Chen Y, Xu B, Qin Y, Hu S, Liu Z, et al (2023) Enhancing chat language\nmodels by scaling high-quality instructional conversations. In: Bou amor H, Pino J,\nBali K (eds) Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing. ACL, Singapore, pp 3029–3051,https://doi.org/10.18653/v1/\n2023.emnlp-main.183\nDolan WB, Brockett C (2005) Automatically constructing a corpus of sentential para-\nphrases. In: Proceedings of the Third International Workshop on Paraphr asing\n(IWP2005), pp 9–16\nDu Z, Qian Y, Liu X, Ding M, Qiu J, Yang Z, et al (2022) GLM: General language\nmodel pretraining with autoregressive blank infilling. In: Procee dings of the 60th\nAnnual Meeting of the Association for Computational Linguistics, pp 320–335\nDua D, Wang Y, Dasigi P, Stanovsky G, Singh S, Gardner M (2019) DROP: A\nreading comprehension benchmark requiring discrete reasoning over paragraphs. In:\nBurstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers). ACL, Minneapolis,\nMinnesota, pp 2368–2378,\nhttps://doi.org/10.18653/v1/N19-1246\nDubois Y, Li X, Taori R, Zhang T, Gulrajani I, Ba J, et al (2023) AlpacaFarm: A\nsimulation framework for methods that learn from human feedback. arXiv pr eprint\narXiv:2305.14387\nDuomo (2023) TransGPT. https://github.com/DUOMO/TransGPT\nEfrat A, Honovich O, Levy O (2023) LMentry: A language model benchmark of ele-\nmentary language tasks. In: Rogers A, Boyd-Graber J, Okazaki N (eds) Findi ngs of\nthe Association for Computational Linguistics: ACL 2023. ACL, Toronto, Canada,\npp 10476–10501,\nhttps://doi.org/10.18653/v1/2023.findings-acl.666\nEisele A, Chen Y (2010) MultiUN: A multilingual corpus from United Nation docu -\nments. In: Calzolari N, Choukri K, Maegaard B, Mariani J, Odijk J, Piperidis S, et al\n(eds) Proceedings of the Seventh International Conference on Language Re sources\nand Evaluation (LREC’10). ELRA, Valletta, Malta, pp 2868–2872\nEthayarajh K, Choi Y, Swayamdipta S (2022) Understanding dataset difficulty with V-\nusable information. In: Chaudhuri K, Jegelka S, Song L, Szepesvari C, Niu G, Sabato\nS (eds) Proceedings of the 39th International Conference on Machine Learn ing, vol\n162. PMLR, pp 5988–6008\nFabbri A, Li I, She T, Li S, Radev D (2019) Multi-News: A large-scale multi-document\nsummarization dataset and abstractive hierarchical model. In: Korhonen A, Traum\nD, M` arquez L (eds) Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics. ACL, Florence, Italy, pp 1074–1084,https://doi.org/10.\n18653/v1/P19-1102\n150\nFang Y, Liang X, Zhang N, Liu K, Huang R, Chen Z, et al (2023) Mol-Instructions:\nA large-scale biomolecular instruction dataset for large language models. arXi v\npreprint arXiv:2306.08018\nFei Z, Shen X, Zhu D, Zhou F, Han Z, Zhang S, et al (2023) LawBench: Benchmarking\nlegal knowledge of large language models. arXiv preprint arXiv:2309.16289\nFu Y, Ou L, Chen M, Wan Y, Peng H, Khot T (2023) Chain-of-Thought Hub: A\ncontinuous effort to measure large language models’ reasoning performance. arXiv\npreprint arXiv:2305.17306\nGao L, Biderman S, Black S, Golding L, Hoppe T, Foster C, et al (2020) The\nPile: An 800GB dataset of diverse text for language modeling. arXiv preprint\narXiv:2101.00027\nGardent C, Shimorina A, Narayan S, Perez-Beltrachini L (2017) Creating tr aining\ncorpora for NLG micro-planners. In: Barzilay R, Kan MY (eds) Proceedin gs of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). ACL, Vancouver, Canada, pp 179–188,https://doi.org/10.18653/v1/\nP17-1017\nGeva M, Khashabi D, Segal E, Khot T, Roth D, Berant J (2021) Did aristotle\nuse a laptop? A question answering benchmark with implicit reasonin g strate-\ngies. Transactions of the Association for Computational Linguistics 9:346–361.\nhttps://doi.org/10.1162/tacl a 00370\nGhosal D, Chia YK, Majumder N, Poria S (2023) Flacuna: Unleashing the problem\nsolving power of Vicuna using FLAN fine-tuning. arXiv preprint arXiv:2307.02053\nGiampiccolo D, Magnini B, Dagan I, Dolan B (2007) The third PASCAL recognizing\ntextual entailment challenge. In: Sekine S, Inui K, Dagan I, Dolan B, Giampiccolo D,\nMagnini B (eds) Proceedings of the ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing. ACL, Prague, pp 1–9\nGliwa B, Mochol I, Biesek M, Wawer A (2019) SAMSum corpus: A human-annotat ed\ndialogue dataset for abstractive summarization. In: Wang L, Cheung JCK, Carenini\nG, Liu F (eds) Proceedings of the 2nd Workshop on New Frontiers in Summari za-\ntion. ACL, Hong Kong, China, pp 70–79, https://doi.org/10.18653/v1/D19-5409\nGo A, Bhayani R, Huang L (2009) Twitter sentiment classification using dis tant\nsupervision. CS224N project report, Stanford 1(12)\nGokaslan A, Cohen V (2019) OpenWebText corpus. http://Skylion007.github.io/\nOpenWebTextCorpus\nGrishman R, Sundheim B (1996) Message Understanding Conference-6: A bri ef his-\ntory. In: Proceedings of the 16th Conference on Computational Linguistics , vol 1.\n151\nACL, USA, pp 466–471\nGrusky M, Naaman M, Artzi Y (2018) Newsroom: A dataset of 1.3 million summaries\nwith diverse extractive strategies. In: Walker M, Ji H, Stent A (ed s) Proceedings of\nthe 2018 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long Papers). ACL,\nNew Orleans, Louisiana, pp 708–719,\nhttps://doi.org/10.18653/v1/N18-1065\nGu Z, Zhu X, Ye H, Zhang L, Wang J, Jiang S, et al (2023) Xiezhi: An ever-\nupdating benchmark for holistic domain knowledge evaluation. arXiv prep rint\narXiv:2306.05783\nGuha N, Nyarko J, Ho DE, Re C, Chilton A, Narayana A, et al (2023) LegalBench:\nA collaboratively built benchmark for measuring legal reasoning in large l anguage\nmodels. In: Thirty-seventh Conference on Neural Information Proce ssing Systems\nDatasets and Benchmarks Track, pp 1–157\nGunasekar S, Zhang Y, Aneja J, Mendes CCT, Del Giorno A, Gopi S, et al (2023)\nTextbooks are all you need. arXiv preprint arXiv:2306.11644\nGuo B, Zhang X, Wang Z, Jiang M, Nie J, Ding Y, et al (2023a) How close is ChatGPT\nto human experts? Comparison corpus, evaluation, and detection. arXiv pr eprint\narXiv:2301.07597\nGuo H, Yang J, Liu J, Yang L, Chai L, Bai J, et al (2023b) OWL: A large language\nmodel for it operations. arXiv preprint arXiv:2309.09298\nGuo Z, Jin R, Liu C, Huang Y, Shi D, Yu L, et al (2023c) Evaluating large language\nmodels: A comprehensive survey. arXiv preprint arXiv:2310.19736\nGupta P, Jiao C, Yeh YT, Mehri S, Eskenazi M, Bigham J (2022) InstructDial :\nImproving zero and few-shot generalization in dialogue through instruc tion tuning.\nIn: Goldberg Y, Kozareva Z, Zhang Y (eds) Proceedings of the 2022 Conferen ce\non Empirical Methods in Natural Language Processing. ACL, Abu Dhabi, United\nArab Emirates, pp 505–525,\nhttps://doi.org/10.18653/v1/2022.emnlp-main.33\nHan T, Adams LC, Papaioannou JM, Grundmann P, Oberhauser T, L¨ oser A, et al\n(2023) MedAlpaca–An open-source collection of medical conversational AI models\nand training data. arXiv preprint\narXiv:2304.08247\nHan X, Zhu H, Yu P, Wang Z, Yao Y, Liu Z, et al (2018) FewRel: A large-scale\nsupervised few-shot relation classification dataset with state-of-th e-art evaluation.\nIn: Riloff E, Chiang D, Hockenmaier J, Tsujii J (eds) Proceedings of t he 2018\nConference on Empirical Methods in Natural Language Processing. ACL, Brus sels,\nBelgium, pp 4803–4809, https://doi.org/10.18653/v1/D18-1514\n152\nHasan T, Bhattacharjee A, Islam MS, Mubasshir K, Li YF, Kang YB, et al (2021)\nXL-Sum: Large-scale multilingual abstractive summarization for 44 languages. I n:\nZong C, Xia F, Li W, Navigli R (eds) Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021. ACL, pp 4693–4703, https://doi.org/10.18653/v1/\n2021.findings-acl.413\nHe C, Jin Z, Xu C, Qiu J, Wang B, Li W, et al (2023a) WanJuan: A comprehensive\nmultimodal dataset for advancing English and Chinese large models. arXiv preprint\narXiv:2308.10755\nHe J, Fu M, Tu M (2019) Applying deep matching networks to Chinese medic al\nquestion answering: A study and a dataset. BMC medical informatics and d ecision\nmaking 19(2):91–100\nHe Q, Zeng J, Huang W, Chen L, Xiao J, He Q, et al (2023b) Can large language mod-\nels understand real-world complex instructions? arXiv preprint arXiv:2309.09150\nHe W, Liu K, Liu J, Lyu Y, Zhao S, Xiao X, et al (2018) DuReader: A Chinese\nmachine reading comprehension dataset from real-world applications. In : Choi E,\nSeo M, Chen D, Jia R, Berant J (eds) Proceedings of the Workshop on Mach ine\nReading for Question Answering. ACL, Melbourne, Australia, pp 37–46, https://\ndoi.org/10.18653/v1/W18-2605\nHe W, Wen J, Zhang L, Cheng H, Qin B, Li Y, et al (2023c) HanFei-1.0. https:\n//github.com/siat-nlp/HanFei\nHendrycks D, Basart S, Kadavath S, Mazeika M, Arora A, Guo E, et al (2021a)\nMeasuring coding challenge competence with APPS. In: Vanschoren J, Yeung S\n(eds) Proceedings of the Neural Information Processing Systems Trac k on Datasets\nand Benchmarks, vol 1. Curran, pp 1–11\nHendrycks D, Burns C, Basart S, Zou A, Mazeika M, Song D, et al (2021b) Measuring\nmassive multitask language understanding. In: International Conference on Learning\nRepresentations, pp 1–27\nHendrycks D, Burns C, Chen A, Ball S (2021c) CUAD: An expert-annotated NLP\ndataset for legal contract review. In: Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 1)\nHendrycks D, Burns C, Kadavath S, Arora A, Basart S, Tang E, et al (2021d)\nMeasuring mathematical problem solving with the MATH dataset. In: Th irty-fifth\nConference on Neural Information Processing Systems Datasets and Benc hmarks\nTrack (Round 2), pp 1–11\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput ation\n9(8):1735–1780\n153\nHonovich O, Scialom T, Levy O, Schick T (2023) Unnatural Instructions: Tuni ng\nlanguage models with (almost) no human labor. In: Rogers A, Boyd-Graber J,\nOkazaki N (eds) Proceedings of the 61st Annual Meeting of the Association f or\nComputational Linguistics (Volume 1: Long Papers). ACL, Toronto, Canada, pp\n14409–14428,\nhttps://doi.org/10.18653/v1/2023.acl-long.806\nHou ZJ, Zhang L, Callison-Burch C (2023) Choice-75: A dataset on decision branching\nin script learning. arXiv preprint arXiv:2309.11737\nHu B, Chen Q, Zhu F (2015) LCSTS: A large scale Chinese short text summariza-\ntion dataset. In: M` arquez L, Callison-Burch C, Su J (eds) Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing. ACL, Lisbon ,\nPortugal, pp 1967–1972, https://doi.org/10.18653/v1/D15-1229\nHu EJ, yelong shen, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al (2022a) LoRA: Low-\nrank adaptation of large language models. In: International Conference on Learning\nRepresentations, pp 1–13\nHu H, Richardson K, Xu L, Li L, K¨ ubler S, Moss L (2020) OCNLI: Original Chi-\nnese natural language inference. In: Cohn T, He Y, Liu Y (eds) Findings of t he\nAssociation for Computational Linguistics: EMNLP 2020. ACL, pp 3512–3526,\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.314\nHu Y, Meng F, Zhou J (2022b) CSCD-IME: Correcting spelling errors generat ed by\nPinyin IME. arXiv preprint arXiv:2211.08788\nHuang Jt, Lam MH, Li EJ, Ren S, Wang W, Jiao W, et al (2023a) Emotionally\nnumb or empathetic? Evaluating how LLMs feel using emotionbench. arXivpreprint\narXiv:2308.03656\nHuang L, Le Bras R, Bhagavatula C, Choi Y (2019) Cosmos QA: Machine reading\ncomprehension with contextual commonsense reasoning. In: Inui K, Jian g J, Ng V,\nWan X (eds) Proceedings of the 2019 Conference on Empirical Methods in Nat -\nural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp 2391–2401,\nhttps://doi.org/10.18653/v1/D19-1243\nHuang Q, Tao M, An Z, Zhang C, Jiang C, Chen Z, et al (2023b) Lawyer LLaMA\ntechnical report. arXiv preprint arXiv:2305.15062\nHuang Y, Bai Y, Zhu Z, Zhang J, Zhang J, Su T, et al (2023c) C-Eval: A multi-leve l\nmulti-discipline Chinese evaluation suite for foundation models. In: Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Benc hmarks\nTrack, pp 1–20\nHuang Y, Zhang Q, Sun L, et al (2023d) TrustGPT: A benchmark for trustworthy\nand responsible large language models. arXiv preprint arXiv:2306.11507\n154\nIyer S, Lin XV, Pasunuru R, Mihaylov T, Simig D, Yu P, et al (2022) OPT-IML:\nScaling language model instruction meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017\nJavaheripi M, Bubeck S, et al (2023) Phi-2: The surprising power of\nsmall language models. https://www.microsoft.com/en-us/research/blog/\nphi-2-the-surprising-power-of-small-language-models/\nJi J, Liu M, Dai J, Pan X, Zhang C, Bian C, et al (2023a) BeaverTails: Towards\nimproved safety alignment of LLM via a human-preference dataset. arXiv p reprint\narXiv:2307.04657\nJi Y, Deng Y, Gong Y, Peng Y, Niu Q, Zhang L, et al (2023b) Exploring the impact of\ninstruction data scaling on large language models: An empirical study on real-world\nuse cases. arXiv preprint arXiv:2303.14742\nJiang AQ, Sablayrolles A, Roux A, Mensch A, Savary B, Bamford C, et al (2024)\nMixtral of experts. arXiv preprint arXiv:2401.04088\nJie Z, Xie P, Lu W, Ding R, Li L (2019) Better modeling of incomplete annotati ons\nfor named entity recognition. In: Burstein J, Doran C, Solorio T (eds) P roceedings\nof the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). ACL, Minneapolis, Minnesota, pp 729–734,\nhttps://doi.org/10.18653/v1/\nN19-1079\nJin Q, Dhingra B, Liu Z, Cohen W, Lu X (2019) PubMedQA: A dataset for biomedical\nresearch question answering. In: Inui K, Jiang J, Ng V, Wan X (eds) Proc eedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). ACL, Hong Kong, China, pp 2567–2577,\nhttps://doi.org/10.18653/v1/\nD19-1259\nJoshi M, Choi E, Weld D, Zettlemoyer L (2017) TriviaQA: A large scale distan tly\nsupervised challenge dataset for reading comprehension. In: Barzila y R, Kan MY\n(eds) Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). ACL, Vancouver, Canada, pp 1601–1611,\nhttps://doi.org/10.18653/v1/P17-1147\nKeung P, Lu Y, Szarvas G, Smith NA (2020) The multilingual Amazon reviews corpus.\nIn: Webber B, Cohn T, He Y, Liu Y (eds) Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP). ACL, pp 4563–4568,\nhttps://doi.org/10.18653/v1/2020.emnlp-main.369\nKhashabi D, Chaturvedi S, Roth M, Upadhyay S, Roth D (2018) Looking beyond\nthe surface: A challenge set for reading comprehension over multipl e sentences. In:\nWalker M, Ji H, Stent A (eds) Proceedings of the 2018 Conference of the Nort h\n155\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers). ACL, New Orleans, Louisiana, pp\n252–262,\nhttps://doi.org/10.18653/v1/N18-1023\nKhot T, Clark P, Guerquin M, Jansen P, Sabharwal A (2020) QASC: A dataset\nfor question answering via sentence composition. Proceedings of the AAAI Con-\nference on Artificial Intelligence 34(05):8082–8090. https://doi.org/10.1609/aaai.\nv34i05.6319\nKim H, Yu Y, Jiang L, Lu X, Khashabi D, Kim G, et al (2022) ProsocialDialog: A\nprosocial backbone for conversational agents. arXiv preprint arXiv:2205.12688\nKocetkov D, Li R, allal LB, LI J, Mou C, Jernite Y, et al (2023) The Stack: 3 TB of\npermissively licensed source code. Transactions on Machine Learnin g Research pp\n1–28\nK¨ oksal A, Schick T, Korhonen A, Sch¨ utze H (2023) Longform: Optimizing instruc-\ntion tuning for long text generation with corpus extraction. arXiv prepr int\narXiv:2304.08460\nKoupaee M, Wang WY (2018) Wikihow: A large scale text summarization dataset.\narXiv preprint arXiv:1810.09305\nKwiatkowski T, Palomaki J, Redfield O, Collins M, Parikh A, Alberti C, e t al (2019)\nNatural Questions: A benchmark for question answering research. Trans actions of\nthe Association for Computational Linguistics 7:452–466. https://doi.org/10.1162/\ntacl a 00276\nLadhak F, Durmus E, Cardie C, McKeown K (2020) WikiLingua: A new benchmark\ndataset for cross-lingual abstractive summarization. In: Cohn T, He Y, Liu Y ( eds)\nFindings of the Association for Computational Linguistics: EMNLP 2020. ACL, pp\n4034–4048,\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.360\nLai G, Xie Q, Liu H, Yang Y, Hovy E (2017) RACE: Large-scale reading comprehen-\nsion dataset from examinations. In: Palmer M, Hwa R, Riedel S (eds) Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing. ACL,\nCopenhagen, Denmark, pp 785–794,\nhttps://doi.org/10.18653/v1/D17-1082\nLai Y, Li C, Wang Y, Zhang T, Zhong R, Zettlemoyer L, et al (2023) DS-1000: A\nnatural and reliable benchmark for data science code generation. In: Kr ause A,\nBrunskill E, Cho K, Engelhardt B, Sabato S, Scarlett J (eds) Proceedings of the 40th\nInternational Conference on Machine Learning, vol 202. PMLR, pp 18319–18345\nLamm M, Palomaki J, Alberti C, Andor D, Choi E, Soares LB, et al (2021) QED:\nA framework and dataset for explanations in question answering. Transacti ons of\nthe Association for Computational Linguistics 9:790–806. https://doi.org/10.1162/\ntacl a 00398\n156\nLan Y, Wang L, Zhang Q, Lan Y, Dai BT, Wang Y, et al (2022) MWPToolkit:\nAn open-source framework for deep learning-based math word problem solver s.\nProceedings of the AAAI Conference on Artificial Intelligence 36(11):13188–13190.\nhttps://doi.org/10.1609/aaai.v36i11.21723\nLauren¸ con H, Saulnier L, Wang T, Akiki C, del Moral AV, Scao TL, et al (2022)\nThe BigScience ROOTS corpus: A 1.6TB composite multilingual dataset. I n:\nThirty-sixth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, pp 1–18\nLee A, Miranda B, Koyejo S (2023a) Beyond scale: The diversity coefficien t as a data\nquality metric demonstrates LLMs are pre-trained on formally diverse d ata. arXiv\npreprint arXiv:2306.13840\nLee AN, Hunter CJ, Ruiz N (2023b) Platypus: Quick, cheap, and powerful refi nement\nof LLMs. arXiv preprint arXiv:2308.07317\nLee H, Phatale S, Mansoor H, Lu K, Mesnard T, Bishop C, et al (2023c) RLAIF: Scal-\ning reinforcement learning from human feedback with AI feedback. arXi v preprint\narXiv:2309.00267\nLee N, Ping W, Xu P, Patwary M, Fung PN, Shoeybi M, et al (2022) Factuality\nenhanced language models for open-ended text generation. In: Koyejo S, Mohamed\nS, Agarwal A, Belgrave D, Cho K, Oh A (eds) Advances in Neural Information\nProcessing Systems, vol 35. Curran Associates, Inc., pp 34586–34599\nLevesque H, Davis E, Morgenstern L (2012) The winograd schema challenge. In: Thir-\nteenth international conference on the principles of knowledge rep resentation and\nreasoning, pp 552–561\nLevow GA (2006) The third international Chinese language processing bakeoff : Word\nsegmentation and named entity recognition. In: Ng HT, Kwong OO (eds) Proc eed-\nings of the Fifth SIGHAN Workshop on Chinese Language Processing. ACL, Sydney,\nAustralia, pp 108–117\nLewis P, Oguz B, Rinott R, Riedel S, Schwenk H (2020) MLQA: Evaluating cross -\nlingual extractive question answering. In: Jurafsky D, Chai J, Schluter N, Tetreault J\n(eds) Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. ACL, pp 7315–7330,\nhttps://doi.org/10.18653/v1/2020.acl-main.653\nLi D, Shao R, Xie A, Sheng Y, Zheng L, Gonzalez JE, et al (2023a) How long\ncan open-source LLMs truly promise on context length? https://lmsys.org/blog/\n2023-06-29-longchat\nLi G, Hammoud HAAK, Itani H, Khizbullin D, Ghanem B (2023b) CAMEL: Com-\nmunicative agents for “mind” exploration of large language model society. In :\nThirty-seventh Conference on Neural Information Processing Syste ms, pp 1–18\n157\nLi H, Koto F, Wu M, Aji AF, Baldwin T (2023c) Bactrian-X: A multilingual\nreplicable instruction-following model with low-rank adaptation . arXiv preprint\narXiv:2305.15011\nLi H, Zhang Y, Koto F, Yang Y, Zhao H, Gong Y, et al (2023d) CMMLU:\nMeasuring massive multitask language understanding in Chinese. arXiv preprint\narXiv:2306.09212\nLi J, Cheng X, Zhao X, Nie JY, Wen JR (2023e) HaluEval: A large-scale halluci-\nnation evaluation benchmark for large language models. In: Bouamor H, Pino J,\nBali K (eds) Proceedings of the 2023 Conference on Empirical Methods in Natu-\nral Language Processing. ACL, Singapore, pp 6449–6464,\nhttps://doi.org/10.18653/\nv1/2023.emnlp-main.397\nLi J, Hui B, QU G, Yang J, Li B, Li B, et al (2023f) Can LLM already serve as a\ndatabase interface? A Big bench for large-scale database grounded text-to- SQLs.\nIn: Thirty-seventh Conference on Neural Information Processing Sy stems Datasets\nand Benchmarks Track, pp 1–28\nLi J, Wang M, Zheng Z, Zhang M (2023g) LooGLE: Can long-context language models\nunderstand long contexts? arXiv preprint arXiv:2311.04939\nLi J, Wang X, Wu X, Zhang Z, Xu X, Fu J, et al (2023h) Huatuo-26M, a large-scale\nChinese medical QA dataset. arXiv preprint arXiv:2305.01526\nLi M, Zhao Y, Yu B, Song F, Li H, Yu H, et al (2023i) API-Bank: A comprehensive\nbenchmark for tool-augmented LLMs. In: Bouamor H, Pino J, Bali K (eds) Proceed-\nings of the 2023 Conference on Empirical Methods in Natural Language Processin g.\nACL, Singapore, pp 3102–3116, https://doi.org/10.18653/v1/2023.emnlp-main.187\nLi R, allal LB, Zi Y, Muennighoff N, Kocetkov D, Mou C, et al (2023j) StarCoder:\nMay the source be with you! Transactions on Machine Learning Research pp 1–43\nLi Y, Choi D, Chung J, Kushman N, Schrittwieser J, Leblond R, et al (2022a)\nCompetition-level code generation with AlphaCode. Science 378(6624):1092–1097.\nhttps://doi.org/10.1126/science.abq1158\nLi Y, Zhang Y, Zhao Z, Shen L, Liu W, Mao W, et al (2022b) CSL: A large-scale Chi-\nnese scientific literature dataset. In: Calzolari N, Huang CR, Kim H, Pustejovsky J,\nWanner L, Choi KS, et al (eds) Proceedings of the 29th International Conference on\nComputational Linguistics. International Committee on Computational Linguistics,\nGyeongju, Republic of Korea, pp 3917–3923\nLi Y, Bubeck S, Eldan R, Del Giorno A, Gunasekar S, Lee YT (2023k) Textbooks are\nall you need II: Phi-1.5 technical report. arXiv preprint arXiv:2309.05463\n158\nLi Y, Li Z, Zhang K, Dan R, Jiang S, Zhang Y (2023l) ChatDoctor: A medical\nchat model fine-tuned on a large language model Meta-AI (LLaMA) using medical\ndomain knowledge. Cureus 15(6)\nLi Y, Ma S, Wang X, Huang S, Jiang C, Zheng HT, et al (2023m) EcomGPT:\nInstruction-tuning large language model with chain-of-task tasks for E- commerce.\narXiv preprint\narXiv:2308.06966\nLi Y, Zhao J, Zheng D, Hu ZY, Chen Z, Su X, et al (2023n) CLEVA: Chinese lan-\nguage models evaluation platform. In: Feng Y, Lefever E (eds) Proceedin gs of the\n2023 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations. ACL, Singapore, pp 186–217,\nhttps://doi.org/10.18653/v1/2023.\nemnlp-demo.17\nLiang P, Bommasani R, Lee T, Tsipras D, Soylu D, Yasunaga M, et al (2023) Holistic\nevaluation of language models. Transactions on Machine Learning Research pp 1–\n162. Featured Certification, Expert Certification\nLiao J, Sun A, Luo Q, Huang H, Li C (2023) Luotuo-QA: Better conversational\nquestion answering model with answer completion. https://github.com/LC1332/\nLuotuo-QA\nLightman H, Kosaraju V, Burda Y, Edwards H, Baker B, Lee T, et al (2023) Let’s\nverify step by step. arXiv preprint arXiv:2305.20050\nLin BY, Zhou W, Shen M, Zhou P, Bhagavatula C, Choi Y, et al (2020) Common-\nGen: A constrained text generation challenge for generative commonsensereasoning.\nIn: Cohn T, He Y, Liu Y (eds) Findings of the Association for Computational\nLinguistics: EMNLP 2020. ACL, pp 1823–1840,\nhttps://doi.org/10.18653/v1/2020.\nfindings-emnlp.165\nLin CY (2004) ROUGE: A package for automatic evaluation of summaries. In: Text\nSummarization Branches Out. ACL, Barcelona, Spain, pp 74–81\nLin K, Tafjord O, Clark P, Gardner M (2019) Reasoning over paragraph effects in\nsituations. In: Fisch A, Talmor A, Jia R, Seo M, Choi E, Chen D (eds) Pro ceedings\nof the 2nd Workshop on Machine Reading for Question Answering. ACL, Hong\nKong, China, pp 58–62,\nhttps://doi.org/10.18653/v1/D19-5808\nLin S, Hilton J, Evans O (2022) TruthfulQA: Measuring how models mimic hu man\nfalsehoods. In: Muresan S, Nakov P, Villavicencio A (eds) Proceed ings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Lon g\nPapers). ACL, Dublin, Ireland, pp 3214–3252, https://doi.org/10.18653/v1/2022.\nacl-long.229\nLing W, Yogatama D, Dyer C, Blunsom P (2017) Program induction by rationale\ngeneration: Learning to solve and explain algebraic word problems. In: Barz ilay\n159\nR, Kan MY (eds) Proceedings of the 55th Annual Meeting of the Association f or\nComputational Linguistics (Volume 1: Long Papers). ACL, Vancouver, Canada, pp\n158–167,\nhttps://doi.org/10.18653/v1/P17-1015\nLiu A, Swayamdipta S, Smith NA, Choi Y (2022) WANLI: Worker and AI collabora-\ntion for natural language inference dataset creation. In: Goldberg Y, Kozarev a Z,\nZhang Y (eds) Findings of the Association for Computational Linguistics: E MNLP\n2022. ACL, Abu Dhabi, United Arab Emirates, pp 6826–6847, https://doi.org/10.\n18653/v1/2022.findings-emnlp.508\nLiu C, Jin R, Ren Y, Yu L, Dong T, Peng X, et al (2023a) M3KE: A massive multi-level\nmulti-subject knowledge evaluation benchmark for Chinese large langu age models.\narXiv preprint arXiv:2305.10263\nLiu H, Liao Y, Meng Y, Wang Y (2023b) LawGpt: Chinese legal dialogue language\nmodel. https://github.com/LiuHC0428/LAW GPT\nLiu H, Teng Z, Cui L, Zhang C, Zhou Q, Zhang Y (2023c) LogiCoT: Logical chain-\nof-thought instruction tuning. In: Bouamor H, Pino J, Bali K (eds) Findi ngs of\nthe Association for Computational Linguistics: EMNLP 2023. ACL, Singapore, pp\n2908–2921,\nhttps://doi.org/10.18653/v1/2023.findings-emnlp.191\nLiu J, Cui L, Liu H, Huang D, Wang Y, Zhang Y (2021) LogiQA: A challenge dataset\nfor machine reading comprehension with logical reasoning. In: Proceedi ngs of the\nTwenty-Ninth International Joint Conference on Artificial Intelligen ce, pp 3622–\n3628\nLiu R, Yang R, Jia C, Zhang G, Zhou D, Dai AM, et al (2023d) Training\nsocially aligned language models in simulated human society. arXiv prep rint\narXiv:2305.16960\nLiu T, Low BKH (2023) Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic\ntasks. arXiv preprint arXiv:2305.14201\nLiu X, Chen Q, Deng C, Zeng H, Chen J, Li D, et al (2018) LCQMC: A large-scale\nChinese question matching corpus. In: Bender EM, Derczynski L, I sabelle P (eds)\nProceedings of the 27th International Conference on Computational Linguist ics.\nACL, Santa Fe, New Mexico, USA, pp 1952–1962\nLiu X, Lai H, Yu H, Xu Y, Zeng A, Du Z, et al (2023e) WebGLM: Towards an efficient\nweb-enhanced question answering system with human preferences . In: Proceedings\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mini ng.\nAssociation for Computing Machinery, New York, NY, USA, pp 4549—-4560\nLiu X, Yu H, Zhang H, Xu Y, Lei X, Lai H, et al (2023f) AgentBench: Evaluating\nLLMs as agents. arXiv preprint arXiv:2308.03688\n160\nLo K, Wang LL, Neumann M, Kinney R, Weld D (2020) S2ORC: The semantic scholar\nopen research corpus. In: Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics. ACL, pp 4969–4983, https://doi.org/10.18653/v1/\n2020.acl-main.447\nLongpre S, Hou L, Vu T, Webson A, Chung HW, Tay Y, et al (2023a) The FLAN\ncollection: Designing data and methods for effective instruction tu ning. In: Pro-\nceedings of the 40th International Conference on Machine Learning. JMLR.org, pp\n22631–22648\nLongpre S, Mahari R, Chen A, Obeng-Marnu N, Sileo D, Brannon W, et al (2023b)\nThe data provenance initiative: A large scale audit of dataset licensing & attribution\nin AI. arXiv preprint arXiv:2310.16787\nLongpre S, Yauney G, Reif E, Lee K, Roberts A, Zoph B, et al (2023c) A pretrainer ’s\nguide to training data: Measuring the effects of data age, domain coverage, q uality,\n& toxicity. arXiv preprint arXiv:2305.13169\nLu D, Liang J, Xu Y, He Q, Geng Y, Han M, et al (2023a) BBT-Fin: Comprehensive\nconstruction of Chinese financial domain pre-trained language model, corp us and\nbenchmark. arXiv preprint arXiv:2302.09432\nLu P, Mishra S, Xia T, Qiu L, Chang KW, Zhu SC, et al (2022) Learn to explain:\nMultimodal reasoning via thought chains for science question answering. In: Koyejo\nS, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A (eds) Advances in Neural\nInformation Processing Systems, vol 35. Curran Associates, Inc., pp 2507–2521\nLu P, Qiu L, Chang KW, Wu YN, Zhu SC, Rajpurohit T, et al (2023b) Dynamic\nprompt learning via policy gradient for semi-structured mathematical reasoning. In:\nThe Eleventh International Conference on Learning Representations, pp 1–26\nLu S, Guo D, Ren S, Huang J, Svyatkovskiy A, Blanco A, et al (2021) CodeXGLUE:\nA machine learning benchmark dataset for code understanding and gener ation. In:\nThirty-fifth Conference on Neural Information Processing Systems D atasets and\nBenchmarks Track (Round 1), pp 1–16\nMaas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C (2011) Learning word vectors\nfor sentiment analysis. In: Lin D, Matsumoto Y, Mihalcea R (eds) Proce edings of\nthe 49th Annual Meeting of the Association for Computational Linguistics: Hum an\nLanguage Technologies. ACL, Portland, Oregon, USA, pp 142–150\nMcCann B, Keskar NS, Xiong C, Socher R (2018) The natural language decathlon:\nMultitask learning as question answering. arXiv preprint\narXiv:1806.08730\nMetaGLM (2023) FinGLM. https://github.com/MetaGLM/FinGLM\n161\nMiao SY, Liang CC, Su KY (2021) A diverse corpus for evaluating and developing\nEnglish math word problem solvers. arXiv preprint arXiv:2106.15772\nMihaylov T, Clark P, Khot T, Sabharwal A (2018) Can a suit of armor conduct\nelectricity? A new dataset for open book question answering. In: Riloff E, Chiang\nD, Hockenmaier J, Tsujii J (eds) Proceedings of the 2018 Conference on E mpirical\nMethods in Natural Language Processing. ACL, Brussels, Belgium, pp 2381–2391,\nhttps://doi.org/10.18653/v1/D18-1260\nMin Q, Shi Y, Zhang Y (2019) A pilot study for Chinese SQL semantic parsing.\nIn: Inui K, Jiang J, Ng V, Wan X (eds) Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP). ACL, Hong Kong,\nChina, pp 3652–3658,\nhttps://doi.org/10.18653/v1/D19-1377\nMin S, Krishna K, Lyu X, Lewis M, Yih Wt, Koh P, et al (2023) FActScore: Fine-\ngrained atomic evaluation of factual precision in long form text generation. I n:\nBouamor H, Pino J, Bali K (eds) Proceedings of the 2023 Conference on Empiri cal\nMethods in Natural Language Processing. ACL, Singapore, pp 12076–12100, https:\n//doi.org/10.18653/v1/2023.emnlp-main.741\nMinaee S, Mikolov T, Nikzad N, Chenaghlu M, Socher R, Amatriain X, et al (2024)\nLarge language models: A survey. arXiv preprint arXiv:2402.06196\nMishra S, Finlayson M, Lu P, Tang L, Welleck S, Baral C, et al (2022a) LILA: A unified\nbenchmark for mathematical reasoning. In: Goldberg Y, Kozareva Z, Zhang Y (eds)\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing. ACL, Abu Dhabi, United Arab Emirates, pp 5807–5832,\nhttps://doi.\norg/10.18653/v1/2022.emnlp-main.392\nMishra S, Khashabi D, Baral C, Hajishirzi H (2022b) Cross-task generalizati on via\nnatural language crowdsourcing instructions. In: Muresan S, Nakov P, Villavicencio\nA (eds) Proceedings of the 60th Annual Meeting of the Association for Compu ta-\ntional Linguistics (Volume 1: Long Papers). ACL, Dublin, Ireland, pp 3470–3487,\nhttps://doi.org/10.18653/v1/2022.acl-long.244\nMOP-LIWU Community, MNBVC Team (2023) MNBVC: Massive never-ending BT\nvast Chinese corpus. https://github.com/esbatmop/MNBVC\nMostafazadeh N, Chambers N, He X, Parikh D, Batra D, Vanderwende L, et al (2016)\nA corpus and cloze evaluation for deeper understanding of commonsense st ories.\nIn: Knight K, Nenkova A, Rambow O (eds) Proceedings of the 2016 Conferenc e\nof the North American Chapter of the Association for Computational Linguistics :\nHuman Language Technologies. ACL, San Diego, California, pp 839–849, https:\n//doi.org/10.18653/v1/N16-1098\n162\nMuennighoff N, Liu Q, Zebaze A, Zheng Q, Hui B, Zhuo TY, et al (2023a) OctoPack:\nInstruction tuning code large language models. In: NeurIPS 2023 Workshop on\nInstruction Tuning and Instruction Following, pp 1–59\nMuennighoff N, Wang T, Sutawika L, Roberts A, Biderman S, Le Scao T, et al\n(2023b) Crosslingual generalization through multitask finetuning. In: Rogers A,\nBoyd-Graber J, Okazaki N (eds) Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers). ACL, Toronto,\nCanada, pp 15991–16111,\nhttps://doi.org/10.18653/v1/2023.acl-long.891\nMuhlgay D, Ram O, Magar I, Levine Y, Ratner N, Belinkov Y, et al (2023) Gen-\nerating benchmarks for factuality evaluation of language models. arXiv prep rint\narXiv:2307.06908\nMukherjee S, Mitra A, Jawahar G, Agarwal S, Palangi H, Awadallah A (2023) Orca:\nProgressive learning from complex explanation traces of GPT-4. arXiv prep rint\narXiv:2306.02707\nNakano R, Hilton J, Balaji S, Wu J, Ouyang L, Kim C, et al (2021) WebGPT: Browser-\nassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332\nNan L, Radev D, Zhang R, Rau A, Sivaprasad A, Hsieh C, et al (2021) DART: Open-\ndomain structured data record to text generation. In: Toutanova K, Rum shisky\nA, Zettlemoyer L, Hakkani-Tur D, Beltagy I, Bethard S, et al (eds) Proce ed-\nings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies. ACL, pp 432–447,\nhttps://doi.org/10.18653/v1/2021.naacl-main.37\nNangia N, Vania C, Bhalerao R, Bowman SR (2020) CrowS-Pairs: A challenge dataset\nfor measuring social biases in masked language models. In: Webber B, Coh n T, He\nY, Liu Y (eds) Proceedings of the 2020 Conference on Empirical Methods inNatural\nLanguage Processing (EMNLP). ACL, pp 1953–1967, https://doi.org/10.18653/v1/\n2020.emnlp-main.154\nNarayan S, Cohen SB, Lapata M (2018) Don’t give me the details, just the summary !\nTopic-aware convolutional neural networks for extreme summarization. I n: Riloff\nE, Chiang D, Hockenmaier J, Tsujii J (eds) Proceedings of the 2018 Confe rence on\nEmpirical Methods in Natural Language Processing. ACL, Brussels, Belgiu m, pp\n1797–1807, https://doi.org/10.18653/v1/D18-1206\nNguyen T, Rosenberg M, Song X, Gao J, Tiwary S, Majumder R, et al (2016)\nMS MARCO: A human generated machine reading comprehension dataset. choi ce\n2640:660\nNguyen T, Van Nguyen C, Lai VD, Man H, Ngo NT, Dernoncourt F, et al (2023)\nCulturaX: A cleaned, enormous, and multilingual dataset for large language models\nin 167 languages. arXiv preprint arXiv:2309.09400\n163\nNi J, Xue F, Deng Y, Phang J, Jain K, Shah MH, et al (2023) Instruction in the wil d:\nA user-based instruction dataset. https://github.com/XueFuzhao/InstructionWild\nNie Y, Williams A, Dinan E, Bansal M, Weston J, Kiela D (2020) Adversarial NLI: A\nnew benchmark for natural language understanding. In: Jurafsky D, Chai J,Schluter\nN, Tetreault J (eds) Proceedings of the 58th Annual Meeting of the Associ ation for\nComputational Linguistics. ACL, pp 4885–4901, https://doi.org/10.18653/v1/2020.\nacl-main.441\nNijkamp E, Pang B, Hayashi H, Tu L, Wang H, Zhou Y, et al (2023) CodeGen: An\nopen large language model for code with multi-turn program synthesis. I n: The\nEleventh International Conference on Learning Representations, pp 1–25\nNiklaus J, Matoshi V, Rani P, Galassi A, St¨ urmer M, Chalkidis I (2023) LEXTREME:\nA multi-lingual and multi-task benchmark for the legal domain. In: Bouamor H,\nPino J, Bali K (eds) Findings of the Association for Computational Linguist ics:\nEMNLP 2023. ACL, Singapore, pp 3016–3054, https://doi.org/10.18653/v1/2023.\nfindings-emnlp.200\nNovikova J, Duˇ sek O, Rieser V (2017) The E2E dataset: New challenges for en d-to-\nend generation. In: Jokinen K, Stede M, DeVault D, Louis A (eds) Proce edings of\nthe 18th Annual SIGdial Meeting on Discourse and Dialogue. ACL, Saarbr¨ ucke n,\nGermany, pp 201–206, https://doi.org/10.18653/v1/W17-5525\nOnoe Y, Zhang MJ, Choi E, Durrett G (2021) CREAK: A dataset for commonsense\nreasoning over entity knowledge. arXiv preprint arXiv:2109.01653\nOpenAI (2022) Introducing ChatGPT. https://openai.com/blog/chatgpt/\nOpenAI (2023) GPT-3.5 Turbo fine-tuning and API updates. https://openai.com/\nblog/gpt-3-5-turbo-fine-tuning-and-api-updates\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al (2022) Training\nlanguage models to follow instructions with human feedback. In: Koyejo S, Mohamed\nS, Agarwal A, Belgrave D, Cho K, Oh A (eds) Advances in Neural Information\nProcessing Systems, vol 35. Curran Associates, Inc., pp 27730–27744\nPaek T (2006) Reinforcement learning for spoken dialogue systems: Comparin g\nstrengths and weaknesses for practical deployment. In: Proc. Dialog- on-Dialog\nWorkshop, Interspeech\nPaperno D, Kruszewski G, Lazaridou A, Pham NQ, Bernardi R, Pezzelle S, et al (2016)\nThe LAMBADA dataset: Word prediction requiring a broad discourse contex t. In:\nErk K, Smith NA (eds) Proceedings of the 54th Annual Meeting of the Assoc iation\nfor Computational Linguistics (Volume 1: Long Papers). ACL, Berlin, Germany, pp\n1525–1534,\nhttps://doi.org/10.18653/v1/P16-1144\n164\nPapineni K, Roukos S, Ward T, Zhu WJ (2002) BLEU: A method for automatic eval-\nuation of machine translation. In: Isabelle P, Charniak E, Lin D (eds) Pro ceedings\nof the 40th Annual Meeting of the Association for Computational Linguistics. ACL,\nPhiladelphia, Pennsylvania, USA, pp 311–318, https://doi.org/10.3115/1073083.\n1073135\nPatel A, Bhattamishra S, Goyal N (2021) Are NLP models really able to solve simp le\nmath word problems? In: Toutanova K, Rumshisky A, Zettlemoyer L, Hakkan i-\nTur D, Beltagy I, Bethard S, et al (eds) Proceedings of the 2021 Conferenc e of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. ACL, pp 2080–2094,\nhttps://doi.org/10.18653/v1/\n2021.naacl-main.168\nPatil SG, Zhang T, Wang X, Gonzalez JE (2023) Gorilla: Large language model\nconnected with massive APIs. arXiv preprint arXiv:2305.15334\nPenedo G, Malartic Q, Hesslow D, Cojocaru R, Alobeidli H, Cappelli A, et al (2023)\nThe RefinedWeb dataset for Falcon LLM: Outperforming curated corpora with web\ndata only. In: Thirty-seventh Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, pp 1–18\nPeng B, Li C, He P, Galley M, Gao J (2023) Instruction tuning with GPT-4. arXi v\npreprint arXiv:2304.03277\nPeng N, Dredze M (2015) Named entity recognition for Chinese social media w ith\njointly trained embeddings. In: M` arquez L, Callison-Burch C, Su J (eds) Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing.\nACL, Lisbon, Portugal, pp 548–554,\nhttps://doi.org/10.18653/v1/D15-1064\nPerez E, Huang S, Song F, Cai T, Ring R, Aslanides J, et al (2022) Red teaming\nlanguage models with language models. In: Goldberg Y, Kozareva Z, Zhang Y (e ds)\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing. ACL, Abu Dhabi, United Arab Emirates, pp 3419–3448,\nhttps://doi.\norg/10.18653/v1/2022.emnlp-main.225\nPilehvar MT, Camacho-Collados J (2019) WiC: The word-in-context dataset for eval-\nuating context-sensitive meaning representations. In: Burstei n J, Doran C, Solorio\nT (eds) Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers). ACL, Minneapolis, Minnesota, pp 1267–1273,\nhttps://doi.org/10.18653/v1/N19-1128\nPuri R, Kung DS, Janssen G, Zhang W, Domeniconi G, Zolotov V, et al (2021)\nCodeNet: A large-scale AI for code dataset for learning a diversity of cod ing tasks.\narXiv preprint arXiv:2105.12655\n165\nQiu H, He H, Zhang S, Li A, Lan Z (2023) SMILE: Single-turn to multi-turn inclu-\nsive language expansion via ChatGPT for mental health support. arXiv prepr int\narXiv:2305.00450\nRadford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al (2019) Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8):1–24\nRae JW, Potapenko A, Jayakumar SM, Hillier C, Lillicrap TP (2020) Compressiv e\nTransformers for long-range sequence modelling. In: International Conf erence on\nLearning Representations, pp 1–19\nRae JW, Borgeaud S, Cai T, Millican K, Hoffmann J, Song F, et al (2021) Scaling\nlanguage models: Methods, analysis & insights from training gopher. arXiv preprint\narXiv:2112.11446\nRaffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al (2020) Exploring\nthe limits of transfer learning with a unified text-to-text Transformer. J Mach Learn\nRes 21(1):5485–5551\nRahman A, Ng V (2012) Resolving complex cases of definite pronouns: The winograd\nschema challenge. In: Tsujii J, Henderson J, Pa¸ sca M (eds) Proce edings of the\n2012 Joint Conference on Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning. ACL, Jeju Island, Korea, pp 777–789\nRajpurkar P, Zhang J, Lopyrev K, Liang P (2016) SQuAD: 100,000+ questions for\nmachine comprehension of text. In: Su J, Duh K, Carreras X (eds) Proc eedings of\nthe 2016 Conference on Empirical Methods in Natural Language Processing. ACL,\nAustin, Texas, pp 2383–2392,\nhttps://doi.org/10.18653/v1/D16-1264\nRajpurkar P, Jia R, Liang P (2018) Know what you don’t know: Unanswerable que s-\ntions for SQuAD. In: Gurevych I, Miyao Y (eds) Proceedings of the 56th An nual\nMeeting of the Association for Computational Linguistics (Volume 2: Short Papers).\nACL, Melbourne, Australia, pp 784–789, https://doi.org/10.18653/v1/P18-2124\nRasiah V, Stern R, Matoshi V, St¨ urmer M, Chalkidis I, Ho DE, et al (2023) SCALE:\nScaling up the complexity for advanced language model evaluation. arXiv pre print\narXiv:2306.09237\nRavichander A, Gardner M, Marasovic A (2022) CONDAQA: A contrastive reading\ncomprehension dataset for reasoning about negation. In: Goldberg Y, Kozareva\nZ, Zhang Y (eds) Proceedings of the 2022 Conference on Empirical Method s in\nNatural Language Processing. ACL, Abu Dhabi, United Arab Emirates, pp 8729–\n8755,\nhttps://doi.org/10.18653/v1/2022.emnlp-main.598\nReddy S, Chen D, Manning CD (2019) CoQA: A conversational question answerin g\nchallenge. Transactions of the Association for Computational Linguistics 7:249–266.\nhttps://doi.org/10.1162/tacl a 00266\n166\nRichardson M, Burges CJ, Renshaw E (2013) MCTest: A challenge dataset for t he\nopen-domain machine comprehension of text. In: Yarowsky D, Baldwin T,Korhonen\nA, Livescu K, Bethard S (eds) Proceedings of the 2013 Conference on Empi rical\nMethods in Natural Language Processing. ACL, Seattle, Washington, USA, pp 193–\n203\nRoemmele M, Bejan CA, Gordon AS (2011) Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In: 2011 AAAI Spring Symposium\nSeries\nRogers A, Kovaleva O, Downey M, Rumshisky A (2020) Getting closer to AI complete\nquestion answering: A set of prerequisite real tasks. Proceedings of the AAAI Con-\nference on Artificial Intelligence 34(05):8722–8731.\nhttps://doi.org/10.1609/aaai.\nv34i05.6398\nRomanov A, Shivade C (2018) Lessons from natural language inference in the clin i-\ncal domain. In: Riloff E, Chiang D, Hockenmaier J, Tsujii J (eds) Proce edings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing. ACL,\nBrussels, Belgium, pp 1586–1596,\nhttps://doi.org/10.18653/v1/D18-1187\nRush AM, Chopra S, Weston J (2015) A neural attention model for abstractive s en-\ntence summarization. In: M` arquez L, Callison-Burch C, Su J (eds) Proceedings of\nthe 2015 Conference on Empirical Methods in Natural Language Processing. ACL,\nLisbon, Portugal, pp 379–389,\nhttps://doi.org/10.18653/v1/D15-1044\nSaha A, Aralikatte R, Khapra MM, Sankaranarayanan K (2018) DuoRC: Towards\ncomplex language understanding with paraphrased reading comprehension . In:\nGurevych I, Miyao Y (eds) Proceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers). ACL, Melbourn e,\nAustralia, pp 1683–1693, https://doi.org/10.18653/v1/P18-1156\nSakaguchi K, Bras RL, Bhagavatula C, Choi Y (2021) WinoGrande: An adversarial\nwinograd schema challenge at scale. Commun ACM 64(9):99–106. https://doi.org/\n10.1145/3474381\nSap M, Rashkin H, Chen D, Le Bras R, Choi Y (2019) Social IQa: Commonsense rea-\nsoning about social interactions. In: Inui K, Jiang J, Ng V, Wan X (eds) Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). ACL, Hong Kong, China, pp 4463–4473,\nhttps://doi.org/10.18653/v1/\nD19-1454\nSawada T, Paleka D, Havrilla A, Tadepalli P, Vidas P, Kranias A, et al (2023)\nARB: Advanced reasoning benchmark for large language models. arXiv preprint\narXiv:2307.13692\n167\nSchulman J, Wolski F, Dhariwal P, Radford A, Klimov O (2017) Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347\nSee A, Liu PJ, Manning CD (2017) Get to the point: Summarization with pointe r-\ngenerator networks. In: Barzilay R, Kan MY (eds) Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers).\nACL, Vancouver, Canada, pp 1073–1083, https://doi.org/10.18653/v1/P17-1099\nShah R, Chawla K, Eidnani D, Shah A, Du W, Chava S, et al (2022) When FLUE\nmeets FLANG: Benchmarks and large pretrained language model for financial\ndomain. In: Goldberg Y, Kozareva Z, Zhang Y (eds) Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing. ACL, Abu\nDhabi, United Arab Emirates, pp 2322–2335,\nhttps://doi.org/10.18653/v1/2022.\nemnlp-main.148\nShaham U, Ivgi M, Efrat A, Berant J, Levy O (2023) ZeroSCROLLS: A zero-shot\nbenchmark for long text understanding. arXiv preprint arXiv:2305.14196\nSiddhant A, Hu J, Johnson M, Firat O, Ruder S (2020) XTREME: A massively\nmultilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv\npreprint arXiv:2003.11080\nSingh S, Vargus F, Dsouza D, Karlsson BF, Mahendiran A, Ko WY, et al (2024)\nAya Dataset: An open-access collection for multilingual instruction tun ing. arXiv\npreprint arXiv:2402.06619\nSinghal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al (2023) Large language\nmodels encode clinical knowledge. Nature 620(7972):172–180\nSocher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng A, et al (2013) Recursi ve\ndeep models for semantic compositionality over a sentiment treebank. In: Yarowsky\nD, Baldwin T, Korhonen A, Livescu K, Bethard S (eds) Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing. ACL, Seatt le,\nWashington, USA, pp 1631–1642\nSoldaini L, Kinney R, Bhagia A, Schwenk D, Atkinson D, Authur R, et al (2024)\nDolma: An open corpus of three trillion tokens for language model pretrainin g\nresearch. arXiv preprint arXiv:2402.00159\nSrivastava A, Rastogi A, Rao A, Shoeb AAM, Abid A, Fisch A, et al (2023) Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language m odels.\nTransactions on Machine Learning Research pp 1–95\nStiennon N, Ouyang L, Wu J, Ziegler D, Lowe R, Voss C, et al (2020) Learning to\nsummarize with human feedback. In: Larochelle H, Ranzato M, Hadsell R, B alcan\nM, Lin H (eds) Advances in Neural Information Processing Systems, vol 33. Curran\nAssociates, Inc., pp 3008–3021\n168\nSun H, Lin Z, Zheng C, Liu S, Huang M (2021a) PsyQA: A Chinese dataset for\ngenerating long counseling text for mental health support. In: Zong C, Xia F, Li W,\nNavigli R (eds) Findings of the Association for Computational Linguistics: ACL-\nIJCNLP 2021. ACL, pp 1489–1503, https://doi.org/10.18653/v1/2021.findings-acl.\n130\nSun H, Zhang Z, Deng J, Cheng J, Huang M (2023a) Safety assessment of Chinese\nlarge language models. arXiv preprint arXiv:2304.10436\nSun K, Yu D, Chen J, Yu D, Choi Y, Cardie C (2019) DREAM: A challenge\ndata set and models for dialogue-based reading comprehension. Transacti ons of\nthe Association for Computational Linguistics 7:217–231. https://doi.org/10.1162/\ntacl a 00264\nSun K, Yu D, Yu D, Cardie C (2020) Investigating prior knowledge for challe ng-\ning Chinese machine reading comprehension. Transactions of the Assoc iation for\nComputational Linguistics 8:141–155. https://doi.org/10.1162/tacl a 00305\nSun T, Zhang X, He Z, Li P, Cheng Q, Yan H, et al (2023b) MOSS: Training conver-\nsational language models from synthetic data. https://github.com/OpenLMLab/\nMOSS\nSun Y, Wang S, Feng S, Ding S, Pang C, Shang J, et al (2021b) ERNIE 3.0: Large-\nscale knowledge enhanced pre-training for language understanding and ge neration.\narXiv preprint arXiv:2107.02137\nSuzgun M, Scales N, Sch¨ arli N, Gehrmann S, Tay Y, Chung HW, et al (2023)\nChallenging BIG-bench tasks and whether chain-of-thought can solve th em. In:\nRogers A, Boyd-Graber J, Okazaki N (eds) Findings of the Association for Co m-\nputational Linguistics: ACL 2023. ACL, Toronto, Canada, pp 13003–13051, https:\n//doi.org/10.18653/v1/2023.findings-acl.824\nTafjord O, Clark P, Gardner M, Yih Wt, Sabharwal A (2019a) QuaRel: A dataset and\nmodels for answering questions about qualitative relationships. In: Proceedings of\nthe AAAI Conference on Artificial Intelligence, pp 7063–7071\nTafjord O, Gardner M, Lin K, Clark P (2019b) QuaRTz: An open-domain dataset\nof qualitative relationship questions. In: Inui K, Jiang J, Ng V, Wan X (e ds)\nProceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp 5941–5946,\nhttps://doi.\norg/10.18653/v1/D19-1608\nTalmor A, Herzig J, Lourie N, Berant J (2019) CommonsenseQA: A question answer-\ning challenge targeting commonsense knowledge. In: Burstein J, Doran C, Solorio\nT (eds) Proceedings of the 2019 Conference of the North American Chapter of\n169\nthe Association for Computational Linguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers). ACL, Minneapolis, Minnesota, pp 4149–4158,\nhttps://doi.org/10.18653/v1/N19-1421\nTandon N, Dalvi B, Sakaguchi K, Clark P, Bosselut A (2019) WIQA: A dataset\nfor “what if...” reasoning over procedural text. In: Inui K, Jiang J, Ng V, Wan\nX (eds) Proceedings of the 2019 Conference on Empirical Methods in Natur al\nLanguage Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp 6076–6085,\nhttps://doi.org/10.18653/v1/D19-1629\nTang H, Li H, Liu J, Hong Y, Wu H, Wang H (2021) DuReader robust: A Chinese\ndataset towards evaluating robustness and generalization of machine readi ng com-\nprehension in real-world applications. In: Zong C, Xia F, Li W, Navigli R ( eds)\nProceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers). ACL, pp 955–963,\nhttps://doi.org/10.18653/v1/\n2021.acl-short.120\nTang Q, Deng Z, Lin H, Han X, Liang Q, Sun L (2023) ToolAlpaca: General-\nized tool learning for language models with 3000 simulated cases. arXiv prep rint\narXiv:2306.05301\nTaori R, Gulrajani I, Zhang T, Dubois Y, Li X, Guestrin C, et al (2023) Stanford\nAlpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/\nstanford alpaca\nTaylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, et al (2022)\nGalactica: A large language model for science. arXiv preprint arXiv:2211.09085\nTjong Kim Sang EF, De Meulder F (2003) Introduction to the CoNLL-2003 shared\ntask: Language-independent named entity recognition. In: Proceedings of the\nSeventh Conference on Natural Language Learning at HLT-NAACL 2003, pp\n142–147\nTogether (2023) RedPajama-Data-v2: An open dataset with 30 trillion tokens for\ntraining large language models.\nhttps://www.together.ai/blog/redpajama-data-v2\nToshniwal S, Moshkov I, Narenthiran S, Gitman D, Jia F, Gitman I (2024)\nOpenMathInstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint\narXiv:2402.10176\nTouvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al\n(2023a) LLaMA: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971\n170\nTouvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, et al (2023b) LLaMA\n2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288\nTrinh TH, Le QV (2018) A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847\nTseng YH, Lee LH, Chang LP, Chen HH (2015) Introduction to SIGHAN 2015 bake-\noff for Chinese spelling check. In: Yu LC, Sui Z, Zhang Y, Ng V (eds) Pro ceedings\nof the Eighth SIGHAN Workshop on Chinese Language Processing. ACL, Beijing,\nChina, pp 32–37,\nhttps://doi.org/10.18653/v1/W15-3106\nVictor S, Albert W, Colin R, Stephen B, Lintang S, Zaid A, et al (2022) Mul-\ntitask prompted training enables zero-shot task generalization. In: In ternational\nConference on Learning Representations\nVilares D, G´ omez-Rodr´ ıguez C (2019) HEAD-QA: A healthcare dataset for complex\nreasoning. In: Korhonen A, Traum D, M` arquez L (eds) Proceedings of the 57t h\nAnnual Meeting of the Association for Computational Linguistics. ACL, Flore nce,\nItaly, pp 960–966, https://doi.org/10.18653/v1/P19-1092\nVu T, Iyyer M, Wang X, Constant N, Wei J, Wei J, et al (2023) FreshLLMs:\nRefreshing large language models with search engine augmentation. arXiv pr eprint\narXiv:2310.03214\nWang A, Singh A, Michael J, Hill F, Levy O, Bowman S (2018) GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. In: Linzen T,\nChrupa/suppress la G, Alishahi A (eds) Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Networks for NLP. ACL, Brussels ,\nBelgium, pp 353–355, https://doi.org/10.18653/v1/W18-5446\nWang A, Pruksachatkun Y, Nangia N, Singh A, Michael J, Hill F, et al (2019)\nSuperGLUE: A stickier benchmark for general-purpose language understanding sys-\ntems. In: Proceedings of the 33rd International Conference on Neural Inf ormation\nProcessing Systems. Curran Associates Inc., Red Hook, NY, USA, pp 3266–3280\nWang D, Chen J, Wu X, Zhou H, Li L (2021a) CNewSum: A large-scale sum-\nmarization dataset with human-annotated adequacy and deducibility leve l. In:\nNatural Language Processing and Chinese Computing: 10th CCF International\nConference, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings,\nPart I. Springer-Verlag, Berlin, Heidelberg, p 389–400,\nhttps://doi.org/10.1007/\n978-3-030-88480-2 31\nWang G, Cheng S, Zhan X, Li X, Song S, Liu Y (2023a) OpenChat: Advancing open-\nsource language models with mixed-quality data. arXiv preprint arXiv:2309.11235\nWang L, Ling W (2016) Neural network-based abstract generation for opinions and\narguments. In: Knight K, Nenkova A, Rambow O (eds) Proceedings of the 2016\n171\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. ACL, San Diego, California, pp 47–57,\nhttps://doi.org/10.18653/v1/N16-1007\nWang L, Zhang A, Wu K, Sun K, Li Z, Wu H, et al (2020a) DuSQL: A large-\nscale and pragmatic Chinese text-to-SQL dataset. In: Webber B, Cohn T, He Y,\nLiu Y (eds) Proceedings of the 2020 Conference on Empirical Methods in Nat ural\nLanguage Processing (EMNLP). ACL, pp 6923–6935, https://doi.org/10.18653/v1/\n2020.emnlp-main.562\nWang P, Li L, Chen L, Zhu D, Lin B, Cao Y, et al (2023b) Large language models are\nnot fair evaluators. arXiv preprint arXiv:2305.17926\nWang X, Chen GH, Song D, Zhang Z, Chen Z, Xiao Q, et al (2023c) CMB: A\ncomprehensive medical benchmark in Chinese. arXiv preprint arXiv:2308.08833\nWang X, Hu Z, Lu P, Zhu Y, Zhang J, Subramaniam S, et al (2023d) SCIBENCH:\nEvaluating college-level scientific problem-solving abilities of large language models.\narXiv preprint arXiv:2307.10635\nWang X, Wang Z, Liu J, Chen Y, Yuan L, Peng H, et al (2023e) MINT: Evaluating\nLLMs in multi-turn interaction with tools and language feedback. arXiv pre print\narXiv:2309.10691\nWang Y, Liu X, Shi S (2017) Deep neural solver for math word problems. In: Palmer\nM, Hwa R, Riedel S (eds) Proceedings of the 2017 Conference on EmpiricalMethods\nin Natural Language Processing. ACL, Copenhagen, Denmark, pp 845–854, https:\n//doi.org/10.18653/v1/D17-1088\nWang Y, Ke P, Zheng Y, Huang K, Jiang Y, Zhu X, et al (2020b) A large-scale Chinese\nshort-text conversation dataset. In: Zhu X, Zhang M, Hong Y, He R (eds) Natural\nLanguage Processing and Chinese Computing. Springer International Publ ishing,\nCham, pp 91–103\nWang Y, Kong C, Yang L, Wang Y, Lu X, Hu R, et al (2021b) YACLC: A Chinese\nlearner corpus with multidimensional annotation. arXiv preprint\narXiv:2112.15043\nWang Y, Mishra S, Alipoormolabashi P, Kordi Y, Mirzaei A, Naik A, et al (2022)\nSuper-NaturalInstructions: Generalization via declarative instru ctions on 1600+\nNLP tasks. In: Goldberg Y, Kozareva Z, Zhang Y (eds) Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing. ACL, Abu\nDhabi, United Arab Emirates, pp 5085–5109,\nhttps://doi.org/10.18653/v1/2022.\nemnlp-main.340\nWang Y, Kordi Y, Mishra S, Liu A, Smith NA, Khashabi D, et al (2023f) Self-Instruct:\nAligning language model with self generated instructions. In: Procee dings of the\n61st Annual Meeting of the Association for Computational Linguistics, vol 1. AC L,\n172\npp 13484–13508, https://doi.org/10.18653/v1/2023.acl-long.754\nWang Y, Yu Z, Zeng Z, Yang L, Wang C, Chen H, et al (2023g) PandaLM: An\nautomatic evaluation benchmark for LLM instruction tuning optimization. ar Xiv\npreprint arXiv:2306.05087\nWang Z, Zhou S, Fried D, Neubig G (2023h) Execution-based evaluation for open -\ndomain code generation. In: Bouamor H, Pino J, Bali K (eds) Findings of the\nAssociation for Computational Linguistics: EMNLP 2023. ACL, Singapore, pp\n1271–1290,\nhttps://doi.org/10.18653/v1/2023.findings-emnlp.89\nWarstadt A, Singh A, Bowman SR (2019) Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics 7:625–641. https:\n//doi.org/10.1162/tacl a 00290\nWei J, Bosma M, Zhao V, Guu K, Yu AW, Lester B, et al (2022) Finetuned lan-\nguage models are zero-shot learners. In: International Conference on Learn ing\nRepresentations, pp 1–46\nWei Zhu WY, Wang X (2023) ShenNong-TCM: A traditional Chinese medicine large\nlanguage model. https://github.com/michael-wzhu/ShenNong-TCM-LLM\nWeischedel R, Palmer M, Marcus M, Hovy E, Pradhan S, Ramshaw L, et al (2012)\nOntoNotes release 5.0 with OntoNotes DB tool v0.999 beta. Linguistic Data\nConsortium pp 1–53\nWelbl J, Liu NF, Gardner M (2017) Crowdsourcing multiple choice scienc e questions.\nIn: Derczynski L, Xu W, Ritter A, Baldwin T (eds) Proceedings of the 3rd Workshop\non Noisy User-generated Text. ACL, Copenhagen, Denmark, pp 94–106,\nhttps://\ndoi.org/10.18653/v1/W17-4413\nWelleck S, Liu J, Bras RL, Hajishirzi H, Choi Y, Cho K (2021) NaturalProofs: Mathe-\nmatical theorem proving in natural language. In: Thirty-fifth Conferen ce on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 1), pp\n1–14\nWilliams A, Nangia N, Bowman S (2018) A broad-coverage challenge corpus for\nsentence understanding through inference. In: Walker M, Ji H, Ste nt A (eds) Pro-\nceedings of the 2018 Conference of the North American Chapter of the Associati on\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers). ACL, New Orleans, Louisiana, pp 1112–1122,\nhttps://doi.org/10.18653/\nv1/N18-1101\nWolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al (2020) Trans-\nformers: State-of-the-art natural language processing. In: Liu Q, Schlan gen D (eds)\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations. ACL, pp 38–45,\nhttps://doi.org/10.18653/v1/\n173\n2020.emnlp-demos.6\nWu M, Waheed A, Zhang C, Abdul-Mageed M, Aji AF (2023) LaMini-LM: A\ndiverse herd of distilled models from large-scale instructions. ar Xiv preprint\narXiv:2304.14402\nWu SH, Liu CL, Lee LH (2013) Chinese spelling check evaluation at SIGHAN bake-\noff 2013. In: Yu LC, Tseng YH, Zhu J, Ren F (eds) Proceedings of the Seventh\nSIGHAN Workshop on Chinese Language Processing. Asian Federation of Natural\nLanguage Processing, Nagoya, Japan, pp 35–42\nXie Q, Lai G, Dai Z, Hovy E (2018) Large-scale cloze test dataset created by teachers.\nIn: Riloff E, Chiang D, Hockenmaier J, Tsujii J (eds) Proceedings of t he 2018\nConference on Empirical Methods in Natural Language Processing. ACL, Brus sels,\nBelgium, pp 2344–2356,\nhttps://doi.org/10.18653/v1/D18-1257\nXie Q, Han W, Chen Z, Xiang R, Zhang X, He Y, et al (2024) The FinBen: An holistic\nfinancial benchmark for large language models. arXiv preprint arXiv:2402.12659\nXie T, Wu CH, Shi P, Zhong R, Scholak T, Yasunaga M, et al (2022) UnifiedSKG: Uni-\nfying and multi-tasking structured knowledge grounding with text-to-text language\nmodels. In: Goldberg Y, Kozareva Z, Zhang Y (eds) Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language Processing. ACL, Abu Dhabi ,\nUnited Arab Emirates, pp 602–631, https://doi.org/10.18653/v1/2022.emnlp-main.\n39\nXu B, Xu Y, Liang J, Xie C, Liang B, Cui W, et al (2017) CN-DBpedia: A never-\nending Chinese knowledge extraction system. In: Benferhat S, Tabia K, Ali M (eds)\nAdvances in Artificial Intelligence: From Theory to Practice. Springer International\nPublishing, Cham, pp 428–438\nXu C, Guo D, Duan N, McAuley J (2023a) Baize: An open-source chat model\nwith parameter-efficient tuning on self-chat data. In: Bouamor H, Pino J, Bali K\n(eds) Proceedings of the 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing. ACL, Singapore, pp 6268–6278, https://doi.org/10.18653/v1/\n2023.emnlp-main.385\nXu C, Sun Q, Zheng K, Geng X, Zhao P, Feng J, et al (2023b) WizardLM:\nEmpowering large language models to follow complex instructions. arXiv preprint\narXiv:2304.12244\nXu F, Lin Q, Han J, Zhao T, Liu J, Cambria E (2023c) Are large language models\nreally good logical reasoners? A comprehensive evaluation from deductive, inductive\nand abductive views. arXiv preprint arXiv:2306.09841\nXu G, Liu J, Yan M, Xu H, Si J, Zhou Z, et al (2023d) CValues: Measuring the\nvalues of Chinese large language models from safety to responsibility. arXiv preprint\n174\narXiv:2307.09705\nXu L, Dong Q, Liao Y, Yu C, Tian Y, Liu W, et al (2020a) CLUENER2020: Fine-\ngrained named entity recognition dataset and benchmark for Chinese. arXivpreprint\narXiv:2001.04351\nXu L, Hu H, Zhang X, Li L, Cao C, Li Y, et al (2020b) CLUE: A Chinese language\nunderstanding evaluation benchmark. In: Scott D, Bel N, Zong C (eds) Proceedings\nof the 28th International Conference on Computational Linguistics. Intern ational\nCommittee on Computational Linguistics, Barcelona, Spain (Online), pp 4762–4772,\nhttps://doi.org/10.18653/v1/2020.coling-main.419\nXu L, Zhang X, Dong Q (2020c) CLUECorpus2020: A large-scale Chinese corpus for\npre-training language model. arXiv preprint arXiv:2003.01355\nXu L, Lu X, Yuan C, Zhang X, Xu H, Yuan H, et al (2021) FewCLUE: A Chinese\nfew-shot learning evaluation benchmark. arXiv preprint arXiv:2107.07498\nXu L, Li A, Zhu L, Xue H, Zhu C, Zhao K, et al (2023e) SuperCLUE: A comprehensive\nChinese large language model benchmark. arXiv preprint arXiv:2307.15020\nXu M (2023) MedicalGPT: Training medical GPT model. https://github.com/\nshibing624/MedicalGPT\nXu Q, Hong F, Li B, Hu C, Chen Z, Zhang J (2023f) On the tool manipulation\ncapability of open-source large language models. arXiv preprint arXiv:2305.16504\nXue L, Constant N, Roberts A, Kale M, Al-Rfou R, Siddhant A, et al (2021)\nMT5: A massively multilingual pre-trained text-to-text Transforme r. In: Proceed-\nings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies. ACL, pp 483–498,\nhttps://doi.org/10.18653/v1/2021.naacl-main.41\nYan Z, Duan N, Chen P, Zhou M, Zhou J, Li Z (2017) Building task-oriented dia-\nlogue systems for online shopping. Proceedings of the AAAI Conference on Artificial\nIntelligence 31(1). https://doi.org/10.1609/aaai.v31i1.11182\nYang A, Xiao B, Wang B, Zhang B, Bian C, Yin C, et al (2023a) Baichuan 2: Open\nlarge-scale language models. arXiv preprint arXiv:2309.10305\nYang D, Yuan R, Fan Y, , Yang Y, Wang Z, et al (2023b) RedGPT: Reference-\nenlightened-dialogue by GPT and for GPT. https://github.com/ziliwangnlp/\nRedGPT\nYang J (2023) Firefly: Chinese conversational large language models. https://github.\ncom/yangjianxin1/Firefly\n175\nYang L, Zhang S, Qin L, Li Y, Wang Y, Liu H, et al (2023c) GLUE-X: Evaluating\nnatural language understanding models from an out-of-distribution gener aliza-\ntion perspective. In: Rogers A, Boyd-Graber J, Okazaki N (eds) Findi ngs of the\nAssociation for Computational Linguistics: ACL 2023. ACL, Toronto, Canada, pp\n12731–12750,\nhttps://doi.org/10.18653/v1/2023.findings-acl.806\nYang S, Zhao H, Zhu S, Zhou G, Xu H, Jia Y, et al (2023d) Zhongjing: Enhancing\nthe Chinese medical capabilities of large language model through expert f eedback\nand real-world multi-turn dialogue. arXiv preprint arXiv:2308.03549\nYang Y, Yih Wt, Meek C (2015) WikiQA: A challenge dataset for open-domain ques-\ntion answering. In: M` arquez L, Callison-Burch C, Su J (eds) Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing. ACL, Lisbon ,\nPortugal, pp 2013–2018, https://doi.org/10.18653/v1/D15-1237\nYang Y, Zhang Y, Tar C, Baldridge J (2019) PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In: Inui K, Jiang J, Ng V, Wan X (ed s)\nProceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP). ACL, Hong Kong, China, pp 3687–3692,\nhttps://doi.\norg/10.18653/v1/D19-1382\nYang Z, Qi P, Zhang S, Bengio Y, Cohen W, Salakhutdinov R, et al (2018) HotpotQA:\nA dataset for diverse, explainable multi-hop question answering. In:Riloff E, Chiang\nD, Hockenmaier J, Tsujii J (eds) Proceedings of the 2018 Conference on E mpirical\nMethods in Natural Language Processing. ACL, Brussels, Belgium, pp 2369–2380,\nhttps://doi.org/10.18653/v1/D18-1259\nYao Y, Ye D, Li P, Han X, Lin Y, Liu Z, et al (2019) DocRED: A large-scale\ndocument-level relation extraction dataset. In: Korhonen A, Traum D, M ` arquez\nL (eds) Proceedings of the 57th Annual Meeting of the Association for Compu ta-\ntional Linguistics. ACL, Florence, Italy, pp 764–777, https://doi.org/10.18653/v1/\nP19-1074\nYao Y, Dong Q, Guan J, Cao B, Zhang Z, Xiao C, et al (2021) CUGE: A Chi-\nnese language understanding and generation evaluation benchmark. arXiv pre print\narXiv:2112.13610\nYe Q, Lin BY, Ren X (2021) CrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In: Moens MF, Huang X, Specia L, Yih SWt (eds)\nProceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing. ACL, Online and Punta Cana, Dominican Republic, pp 7163–7189,\nhttps://doi.org/10.18653/v1/2021.emnlp-main.572\nYin D, Liu X, Yin F, Zhong M, Bansal H, Han J, et al (2023a) Dynosaur: A dynamic\ngrowth paradigm for instruction-tuning data curation. In: The 2023 Confer ence on\nEmpirical Methods in Natural Language Processing, pp 1–17\n176\nYin X, Huang B, Wan X (2023b) ALCUNA: Large language models meet new knowl-\nedge. In: Bouamor H, Pino J, Bali K (eds) Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing. ACL, Singapore, pp 1397–1414,\nhttps://doi.org/10.18653/v1/2023.emnlp-main.87\nYu D, Sun K, Cardie C, Yu D (2020a) Dialogue-based relation extraction. In: Jurafsky\nD, Chai J, Schluter N, Tetreault J (eds) Proceedings of the 58th Annual M eeting\nof the Association for Computational Linguistics. ACL, pp 4927–4940, https://doi.\norg/10.18653/v1/2020.acl-main.444\nYu J, Wang X, Tu S, Cao S, Zhang-Li D, Lv X, et al (2023a) KoLA: Care-\nfully benchmarking world knowledge of large language models. arXiv prepr int\narXiv:2306.09296\nYu J, Zhu J, Wang Y, Liu Y, Chang H, Nie J, et al (2023b) Taoli LLaMA. https:\n//github.com/blcuicall/taoli\nYu LC, Lee LH, Tseng YH, Chen HH (2014) Overview of SIGHAN 2014 bake-off for\nChinese spelling check. In: Sun L, Zong C, Zhang M, Levow GA (eds) P roceedings\nof the Third CIPS-SIGHAN Joint Conference on Chinese Language Processi ng.\nAssociation for Computational Linguistics, Wuhan, China, pp 126–132, https://doi.\norg/10.3115/v1/W14-6820\nYu T, Zhang R, Yang K, Yasunaga M, Wang D, Li Z, et al (2018) Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain semantic parsing and t ext-\nto-SQL task. In: Riloff E, Chiang D, Hockenmaier J, Tsujii J (eds) Pro ceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing. ACL,\nBrussels, Belgium, pp 3911–3921,\nhttps://doi.org/10.18653/v1/D18-1425\nYu W, Jiang Z, Dong Y, Feng J (2020b) ReClor: A reading comprehension dataset\nrequiring logical reasoning. In: International Conference on Learning Re presenta-\ntions, pp 1–26\nYuan L, Chen Y, Cui G, Gao H, Zou F, Cheng X, et al (2023) Revisiting out-of-\ndistribution robustness in NLP: Benchmarks, analysis, and LLMs evaluati ons. In:\nThirty-seventh Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track, pp 1–30\nYuan S, Zhao H, Du Z, Ding M, Liu X, Cen Y, et al (2021) WuDaoCorpora: A super\nlarge-scale Chinese corpora for pre-training language models. AI Open 2:65–68.\nhttps://doi.org/https://doi.org/10.1016/j.aiopen.2021.06.001\nYue S, Chen W, Wang S, Li B, Shen C, Liu S, et al (2023) DISC-LawLLM: Fine-tuning\nlarge language models for intelligent legal services. arXiv preprint arXiv:2309.11325\nZellers R, Holtzman A, Bisk Y, Farhadi A, Choi Y (2019a) HellaSwag: Can a machine\nreally finish your sentence? In: Korhonen A, Traum D, M` arquez L (eds)Proceedings\n177\nof the 57th Annual Meeting of the Association for Computational Linguistics. ACL,\nFlorence, Italy, pp 4791–4800, https://doi.org/10.18653/v1/P19-1472\nZellers R, Holtzman A, Rashkin H, Bisk Y, Farhadi A, Roesner F, et al (2019b) Defend-\ning against neural fake news. In: Proceedings of the 33rd International Con ference\non Neural Information Processing Systems. Curran Associates Inc., Re d Hook, NY,\nUSA, pp 9054–9065\nZeng A, Liu X, Du Z, Wang Z, Lai H, Ding M, et al (2023a) GLM-130B: An open\nbilingual pre-trained model. In: The Eleventh International Conference on Learning\nRepresentations, pp 1–56\nZeng G, Yang W, Ju Z, Yang Y, Wang S, Zhang R, et al (2020) MedDialog: Large-scale\nmedical dialogue datasets. In: Webber B, Cohn T, He Y, Liu Y (eds) Proceed -\nings of the 2020 Conference on Empirical Methods in Natural Language Processin g\n(EMNLP). ACL, pp 9241–9250, https://doi.org/10.18653/v1/2020.emnlp-main.743\nZeng H (2023) Measuring massive multitask Chinese understanding. arXiv preprint\narXiv:2304.12986\nZeng H, Xue J, Hao M, Sun C, Ning B, Zhang N (2023b) Evaluating the generation\ncapabilities of large Chinese language models. arXiv preprint arXiv:2308.04823\nZhang G, Shi Y, Liu R, Yuan R, Li Y, Dong S, et al (2023a) Chinese open instruction\ngeneralist: A preliminary release. arXiv preprint arXiv:2304.07987\nZhang H, Zhao X, Song Y (2020a) WinoWhy: A deep diagnosis of essential com-\nmonsense knowledge for answering winograd schema challenge. In: Juraf sky D,\nChai J, Schluter N, Tetreault J (eds) Proceedings of the 58th Annual Mee t-\ning of the Association for Computational Linguistics. ACL, pp 5736–5745, https:\n//doi.org/10.18653/v1/2020.acl-main.508\nZhang H, Chen J, Jiang F, Yu F, Chen Z, Chen G, et al (2023b) HuatuoGPT, towards\ntaming language model to be a doctor. In: Bouamor H, Pino J, Bali K (eds) Findings\nof the Association for Computational Linguistics: EMNLP 2023. Association for\nComputational Linguistics, Singapore, pp 10859–10885,\nhttps://doi.org/10.18653/\nv1/2023.findings-emnlp.725\nZhang J, Qian K, Liu Z, Heinecke S, Meng R, Liu Y, et al (2023c) DialogStudio:\nTowards richest and most diverse unified dataset collection for conver sational AI.\narXiv preprint arXiv:2307.10172\nZhang L, Cai W, Liu Z, Yang Z, Dai W, Liao Y, et al (2023d) FinEval: A Chinese\nfinancial domain knowledge evaluation benchmark for large language models. arXiv\npreprint arXiv:2308.09975\n178\nZhang M, Zhang Y, Liu S, Yuan H, Wang J, Dong Y, et al (2023e) LLMEval-2.\nhttps://github.com/llmeval/llmeval-2\nZhang M, Zhang Y, Liu S, Yuan H, Wang J, Gui T, et al (2023f) LLMEval-1. https:\n//github.com/llmeval/llmeval-1\nZhang N, Chen M, Bi Z, Liang X, Li L, Shang X, et al (2022) CBLUE: A Chinese\nbiomedical language understanding evaluation benchmark. In: Muresan S, Nakov\nP, Villavicencio A (eds) Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers). ACL, Dublin, Irelan d, pp\n7888–7915, https://doi.org/10.18653/v1/2022.acl-long.544\nZhang R, Tetreault J (2019) This email could save your life: Introducin g the task of\nemail subject line generation. In: Korhonen A, Traum D, M` arquez L (eds) Proceed-\nings of the 57th Annual Meeting of the Association for Computational Linguistic s.\nACL, Florence, Italy, pp 446–456, https://doi.org/10.18653/v1/P19-1043\nZhang S, Liu X, Liu J, Gao J, Duh K, Van Durme B (2018a) ReCoRD: Bridging\nthe gap between human and machine commonsense reading comprehension. ar Xiv\npreprint arXiv:1810.12885\nZhang S, Zhang X, Wang H, Guo L, Liu S (2018b) Multi-scale attentive interaction net-\nworks for Chinese medical question answer selection. IEEE Access 6:74061–74071.\nhttps://doi.org/10.1109/ACCESS.2018.2883637\nZhang S, Dong L, Li X, Zhang S, Sun X, Wang S, et al (2023g) Instruction tuning for\nlarge language models: A survey. arXiv preprint arXiv:2308.10792\nZhang S, Fang Q, Zhang Z, Ma Z, Zhou Y, Huang L, et al (2023h) BayLing: Bridging\ncross-lingual alignment and instruction following through interactive translation for\nlarge language models. arXiv preprint arXiv:2306.10968\nZhang W, Aljunied SM, Gao C, Chia YK, Bing L (2023i) M3Exam: A multilin-\ngual, multimodal, multilevel benchmark for examining large language models. arXiv\npreprint arXiv:2306.05179\nZhang X, Yang Q (2023) XuanYuan 2.0: A large Chinese financial chat model with\nhundreds of billions parameters. In: Proceedings of the 32nd ACM Inte rnational\nConference on Information and Knowledge Management. ACM, New York, NY,\nUSA, pp 4435–4439\nZhang X, Zhao J, LeCun Y (2015) Character-level convolutional networks for tex t\nclassification. In: Cortes C, Lawrence N, Lee D, Sugiyama M, Garnett R (ed s)\nAdvances in Neural Information Processing Systems, vol 28. Curran Associ ates,\nInc., pp 1–9\n179\nZhang X, Chen Y, Hu S, Wu Q, Chen J, Xu Z, et al (2023j) InfiniteBench:\n128k long-context benchmark for language models. https://github.com/OpenBMB/\nInfiniteBench?tab=readme-ov-file\nZhang X, Li C, Zong Y, Ying Z, He L, Qiu X (2023k) Evaluating the performance of\nlarge language models on GAOKAO benchmark. arXiv preprint arXiv:2305.12474\nZhang X, Yu B, Yu H, Lv Y, Liu T, Huang F, et al (2023l) Wider and deeper LLM\nnetworks are fairer LLM evaluators. arXiv preprint arXiv:2308.01862\nZhang Y, Yang J (2018) Chinese NER using lattice LSTM. In: Gurevych I, Miy ao Y\n(eds) Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). ACL, Melbourne, Australia, pp 1554–1564,\nhttps://doi.org/10.18653/v1/P18-1144\nZhang Y, Zhong V, Chen D, Angeli G, Manning CD (2017) Position-aware attention\nand supervised data improve slot filling. In: Palmer M, Hwa R, Riedel S (eds)\nProceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing. ACL, Copenhagen, Denmark, pp 35–45,\nhttps://doi.org/10.18653/v1/\nD17-1004\nZhang Y, Baldridge J, He L (2019) PAWS: Paraphrase adversaries from word scram-\nbling. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Con ference\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long and Short Papers). ACL,\nMinneapolis, Minnesota, pp 1298–1308,\nhttps://doi.org/10.18653/v1/N19-1131\nZhang Y, Sun S, Galley M, Chen YC, Brockett C, Gao X, et al (2020b) DIALOGPT:\nLarge-scale generative pre-training for conversational response generati on. In:\nCelikyilmaz A, Wen TH (eds) Proceedings of the 58th Annual Meeting of th e Asso-\nciation for Computational Linguistics: System Demonstrations. ACL, pp 270–278,\nhttps://doi.org/10.18653/v1/2020.acl-demos.30\nZhang Y, Li Y, Cui L, Cai D, Liu L, Fu T, et al (2023m) Siren’s song in the AI ocean: A\nsurvey on hallucination in large language models. arXiv preprint arXiv:2309.01219\nZhang Z, Lei L, Wu L, Sun R, Huang Y, Long C, et al (2023n) SafetyBench: Evaluating\nthe safety of large language models with multiple choice questions. arXiv preprint\narXiv:2309.07045\nZhao W, Shang M, Liu Y, Wang L, Liu J (2020) Ape210K: A large-scale and template-\nrich dataset of math word problems. arXiv preprint arXiv:2009.11506\nZhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al (2023) A survey of large\nlanguage models. arXiv preprint arXiv:2303.18223\n180\nZheng C, Huang M, Sun A (2019) ChID: A large-scale Chinese idiom dataset for c loze\ntest. In: Korhonen A, Traum D, M` arquez L (eds) Proceedings of the 57th An nual\nMeeting of the Association for Computational Linguistics. ACL, Florence, Italy, pp\n778–787, https://doi.org/10.18653/v1/P19-1075\nZheng K, Han JM, Polu S (2022) MiniF2F: A cross-system benchmark for formal\nOlympiad-level mathematics. In: International Conference on Learning Representa-\ntions, pp 1–11\nZheng L, Chiang WL, Sheng Y, Li T, Zhuang S, Wu Z, et al (2023a) LMSYS-Chat-1M:\nA large-scale real-world LLM conversation dataset. arXiv preprintarXiv:2309.11998\nZheng L, Chiang WL, Sheng Y, Zhuang S, Wu Z, Zhuang Y, et al (2023b) Judging\nLLM-as-a-judge with MT-bench and Chatbot Arena. In: Thirty-seventh Conference\non Neural Information Processing Systems Datasets and Benchmarks Track, pp 1–29\nZhong H, Xiao C, Tu C, Zhang T, Liu Z, Sun M (2020) JEC-QA: A legal-domain\nquestion answering dataset. Proceedings of the AAAI Conference on Artific ial\nIntelligence 34(05):9701–9708. https://doi.org/10.1609/aaai.v34i05.6519\nZhong W, Cui R, Guo Y, Liang Y, Lu S, Wang Y, et al (2023) AGIEval:\nA human-centric benchmark for evaluating foundation models. arXiv pre print\narXiv:2304.06364\nZhou C, Liu P, Xu P, Iyer S, Sun J, Mao Y, et al (2023a) LIMA: Less is more for\nalignment. arXiv preprint arXiv:2305.11206\nZhou K, Zhu Y, Chen Z, Chen W, Zhao WX, Chen X, et al (2023b) Don’t make your\nLLM an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964\nZhou P, Wang Z, Chong D, Guo Z, Hua Y, Su Z, et al (2022) METS-CoV: A dataset\nof medical entity and targeted sentiment on COVID-19 related tweets. I n: Koyejo\nS, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A (eds) Advances in Neural\nInformation Processing Systems, vol 35. Curran Associates, Inc., pp 21916–21932\nZhu C, Liu Y, Mei J, Zeng M (2021) MediaSum: A large-scale media interview\ndataset for dialogue summarization. In: Toutanova K, Rumshisky A, Zettlemoyer L,\nHakkani-Tur D, Beltagy I, Bethard S, et al (eds) Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics :\nHuman Language Technologies. ACL, pp 5927–5934,\nhttps://doi.org/10.18653/v1/\n2021.naacl-main.474\nZhu H, Chen Y, Yan J, Liu J, Hong Y, Chen Y, et al (2022) DuQM: A Chinese\ndataset of linguistically perturbed natural questions for evaluating t he robustness\nof question matching models. In: Proceedings of the 2022 Conference on E mpirical\nMethods in Natural Language Processing, pp 7782–7794\n181\nZhu K, Wang J, Zhou J, Wang Z, Chen H, Wang Y, et al (2023) PromptBench:\nTowards evaluating the robustness of large language models on adversarial prompts.\narXiv preprint arXiv:2306.04528\nZhu W, Wang X (2023) ChatMed: A Chinese medical large language model. https:\n//github.com/michael-wzhu/ChatMed\nZhu Y, Kiros R, Zemel R, Salakhutdinov R, Urtasun R, Torralba A, et al (2015)\nAligning books and movies: Towards story-like visual explanations by w atching\nmovies and reading books. In: Proceedings of the IEEE International C onference\non Computer Vision (ICCV), pp 19–27\nZiang Leng QC, Li C (2023) Luotuo: An instruction-following Chinese language model,\nLoRA tuning on LLaMA. https://github.com/LC1332/Chinese-alpaca-lora\nZiegler DM, Stiennon N, Wu J, Brown TB, Radford A, Amodei D, et al (2019) Fine -\ntuning language models from human preferences. arXiv preprint arXiv:1909.08593\nZiemski M, Junczys-Dowmunt M, Pouliquen B (2016) The United Nations par allel\ncorpus v1.0. In: Calzolari N, Choukri K, Declerck T, Goggi S, Grobelnik M, M ae-\ngaard B, et al (eds) Proceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC’16). ELRA, Portoroˇ z, Slovenia, pp 3530–3534\n182",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5183242559432983
    },
    {
      "name": "Data science",
      "score": 0.3834737539291382
    },
    {
      "name": "Natural language processing",
      "score": 0.3478351831436157
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210098034",
      "name": "Key Laboratory of Guangdong Province",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210104059",
      "name": "Cultura",
      "country": "MX"
    }
  ],
  "cited_by": 47
}