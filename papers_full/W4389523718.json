{
  "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
  "url": "https://openalex.org/W4389523718",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2115924132",
      "name": "Yucheng LI",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A1997810020",
      "name": "Dong Bo",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2131371130",
      "name": "Frank GUERIN",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2141260205",
      "name": "Cheng-Hua Lin",
      "affiliations": [
        "University of Manchester",
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2993383518",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W4385573300",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4366330736",
    "https://openalex.org/W4367000321",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4389524473"
  ],
  "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6342–6353\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCompressing Context to Enhance Inference Efficiency of\nLarge Language Models\nYucheng Li1 , Bo Dong1 , Frank Guerin1 , Chenghua Lin2,3∗\n1 Department of Computer Science, University of Surrey, UK\n2 Department of Computer Science, The University of Manchester, UK\n3 Department of Computer Science, The University of Sheffield, UK\n{yucheng.li, bd00531, f.guerin}@surrey.ac.uk\nchenghua.lin@manchester.ac.uk\nAbstract\nLarge language models (LLMs) achieved re-\nmarkable performance across various tasks.\nHowever, they face challenges in managing\nlong documents and extended conversations,\ndue to significantly increased computational re-\nquirements, both in memory and inference time,\nand potential context truncation when the input\nexceeds the LLM’s fixed context length. This\npaper proposes a method called Selective Con-\ntext that enhances the inference efficiency of\nLLMs by identifying and pruning redundancy\nin the input context to make the input more\ncompact. We test our approach using common\ndata sources requiring long context processing:\narXiv papers, news articles, and long conver-\nsations, on tasks of summarisation, question\nanswering, and response generation. Experi-\nmental results show that Selective Context sig-\nnificantly reduces memory cost and decreases\ngeneration latency while maintaining compa-\nrable performance compared to that achieved\nwhen full context is used. Specifically, we\nachieve a 50% reduction in context cost, re-\nsulting in a 36% reduction in inference mem-\nory usage and a 32% reduction in inference\ntime, while observing only a minor drop of\n.023 in BERTscore and .038 in faithfulness\non four downstream applications, indicating\nthat our method strikes a good balance between\nefficiency and performance. Code and data\nare available at https://github.com/\nliyucheng09/Selective_Context.\n1 Introduction\nLarge language models (LLMs) have demonstrated\nremarkable power and impressive generalisation\nabilities across a wide range of natural language\nprocessing tasks, as well as real-life applications\n(Brown et al., 2020; Touvron et al., 2023; Bubeck\net al., 2023). However, a major challenge for ex-\nisting LLMs is processing longer context. Deal-\ning with longer context with LLMs is fundamen-\n∗ Corresponding author\nContext: Large Languages Models (LLMs) have\nshown their ability to perform new tasks, resulting in a\nline of work that focuses on further scaling these models.\nThese efforts are based on the assumption {thatmore\nparameterswillleadtobetterperformance.}\nQuery: What’s the assumption behind the efforts to\nfurther scale LLMs?\nLLMs: Further scaling Large Language Models will\nlead to better performance on a wide range of tasks.\nFigure 1: Some context is redundant because LLMs\nhave learned that knowledge. LLMs can generate\nthe correct answer even when these redundancies are\ndeleted.\ntal in scenarios such as having long conversations,\ndocument summarisation, and question answering\ngiven long documents. However, it is very compu-\ntationally expensive, particularly with Transformer\nbased LLMs, due to the quadratic growth of mem-\nory and computation associated with the 2-D at-\ntention matrix (Vaswani et al., 2017). This makes\nLLMs less accessible and sometimes leads to con-\ntext truncation during inference. Moreover, due to\nthe above limitation, existing LLMs were usually\npre-trained with fixed-context windows, which fur-\nther constrains their capability in processing longer\ncontext.\nThere are active attempts in reducing the compu-\ntation and memory cost of the Transformer archi-\ntecture with sparse attention (Child et al., 2019) or\nlocal dense attention (Beltagy et al., 2020). There\nare also efforts to learn soft prompts with further\ndistillation to save context cost during inference\n(Mu et al., 2023; Chevalier et al., 2023). In contrast\nto existing approaches that primarily focus on ar-\nchitectures or distillations, we introduce a fresh per-\nspective to tackle the redundancy in the input con-\ntext itself, thus proposing a complementary, model-\nagnostic approach that can be potentially combined\nwith other architecture optimisation methods to fur-\nther enhance inference efficiency.\n6342\nThe proposed method is motivated by the po-\ntential redundancy and repetition in human lan-\nguage, which has two main sources. The first\nis the inherent redundancy of natural language.\nFor example, in the conversation \"A: Did you get\nthe chance to pick up groceries today?\" , \"B: Yes,\nI did get the groceries.\", the underlined part can be\nseen as a common redundancy in communication.\nLinguistic studies suggest redundancy is ubiqui-\ntous in language (Wit and Gillette, 1999). The\nother type of input redundancy is from the overlap\nwith training material. As the example in Fig. 1\nshows, if some parts of input have already been\nincluded in the pre-training stage of LLMs, then it\nis safe to delete them and the model can still gen-\nerate the correct answer. In summary, redundancy\nin the input context, while beneficial for human\ncomprehension, can be superfluous for LLMs and\nmight lead to unnecessary computational expense.\nIn this paper, we propose Selective Context ,\nwhich prunes redundant content in a given input\ncontext, thereby reducing the computational cost\nand making better use of the fixed context length in\nLLMs. Selective Context evaluates informativeness\nof lexical units (i.e., tokens, phrases, or sentences)\nwith self-information (Shannon, 1948) computed\nby a base causal language model. By selectively\nretaining content with higher self-information, our\nmethod provides a more compact and efficient con-\ntext representation for LLMs to process without\ncompromising their performance on various appli-\ncations.\nWe evaluate the effectiveness and different set-\ntings of Selective Context on arXiv papers, BBC\nNews, and real conversation on ShareGPT.com\nwith four NLP tasks: summarisation, question an-\nswering, original context reconstruction, and con-\nversation. Experimental results demonstrate that\nour proposed method can significantly enhance con-\ntext efficiency of LLMs during inference while\nmaintaining comparable performance compared to\nthat achieved when full context is used.\n2 Self-Information\nSelf-information, also known as surprisal or in-\nformation content , is a fundamental concept in\ninformation theory that quantifies the amount of\ninformation conveyed by an event given a distribu-\ntion (Shannon, 1948). In the context of language\nmodelling, the event can be regarded as one step\nof generation (i.e., a token) and the distribution\ncorresponds to its output distribution. So the self-\ninformation of a token can be defined as the nega-\ntive log likelihood:\nI(x) = −log2 P(xt|x0, x1, ..., xt−1) (1)\nwhere I(x) represents the self-information of token\nx and P(x) denotes its output probability.\nIn information theory, self-information mea-\nsures the level of surprise or uncertainty associated\nwith an event; rare events convey more informa-\ntion and thus have higher self-information, while\ncommon events convey less information and have\nlower self-information. In the context of language\nmodelling, self-information can be used to assess\nthe informativeness of lexical units, e.g., words,\nphrases, or sentences. Lexical units with lower\nself-information are less informative and thus are\nmore likely to be inferred from the context. As a re-\nsult, we may treat these parts of input as redundant\nduring LLM inference.\nIn NLP, self-information has been used to\nmeasure surprise in creative language artefacts\n(Bunescu and Uduehi, 2022). In addition, related\nconcepts of self-information such as entropy and\nperplexity are widely used in language model opti-\nmisation and evaluation.\nH(S) = 1\nN ΣtI(xt) (2)\nPP(S) = 2H(S) (3)\nwhere the entropy H(S) of the sentence S =\n(x0, ..., xn) is the average self-information of\nwords in the sentence, and perplexity PP(S) of\nthe sentence can be calculated with entropy. The\nproperty of self-information that is especially rele-\nvant to our method is the additivity.\nI(x0, x1) = −log2 P(x0, x1)\n= −log2 P(x0)P(x1|x0)\n= −log2 P(x0) −log2 P(x1|x0)\n= I(x0)I(x1)\n(4)\nThis means we can calculate the self-information\nof a lexical unit by simply summing the self-\ninformation of the tokens in it.\n3 Method\nSelective Context optimises the input context by\nfiltering out redundant or non-essential content to\nreduce computational cost and make better use of\nthe limited context window. In implementation,\n6343\nOriginal: INTRODUCTION Continual Learning( CL ) , also known as LifelongLearning, is\na promising learning paradigmto designmodelsthat have to learn how to performmultipletasks\nacrossdifferentenvironmentsovertheirlifetime[Touniformthe languageandenhancethereadability\nof thepaperwe adoptthe unique term continual learning(CL).].Ideal CL modelsin therealworld\nshould be deal with domain shifts, researchershave recently started to sample tasks from\ntwo different datasets. For instance, proposedto train and evaluatea modelon Imagenetfirst and\nthen challengeitsperformanceon the Places365dataset. considersmore scenarios, startingwith\nImagenetor Places365, and then movingon to the VOC/CUB/Scenes datasets. Few workspropose\nmore advanced scenariosbuilton topof more than two datasets.\nFiltered: INTRODUCTION Continual Learning( a promising learning paradigmto designmodels\nhaveto howacrossoverTouniformthe languageandenhanceadoptthe unique term continual learning\nIdeal CL modelsin should deal domain shifts researchers recently started sample tasks\ntwo different datasetsFor instance proposed to train and evaluate on Imagenet first challenge\nPlaces365considersmore scenariosstartingImagenetor Places365the VOC/CUB/Scenes datasetsFew\nworksproposemore advanced scenariosbuilttopmore than two datasets\nFigure 2: A visualisation of selective context. Darker colour indicates larger value of self-information.\nwe first 1) employ a causal language model such\nas GPT (Radford et al., 2019; Brown et al., 2020),\nOPT (Zhang et al., 2022), or LLaMA (Touvron\net al., 2023), computing self-information for each\ntoken. We then 2) merge tokens, along with their\ncorresponding self-information values, into lexical\nunits, which can be phrases or sentences. This\nstep is optional if tokens are being used as the\nbasic units. Finally, 3) we eliminate content that\nis deemed least necessary to render the input more\ncompact.\n3.1 Computing Self-Information\nGiven a context C = x0, x1, ..., xn, where xi de-\nnotes a token, we use a base language model M to\ncompute the self-information for each token xt as\nfollows:\nI(xi) = −log2 P(xi|x0, x1, ..., xi−1) (5)\n3.2 Merging into Lexical Units\nIf the content filtering of selective context is di-\nrectly performed on the token level, it might lead to\nvery disjoint context. Therefore apart from token\nlevel filtering, we also conduct the filtering proce-\ndure on phrase and sentence level. We call a basic\nunit in our filtering a lexical unit, which could be a\ntoken, a phrase or a sentence in our setting.\nTo enable selective context to work on phrases\nand sentences, we merge tokens and their self-\ninformation into lexical units. Each lexical unit u\nconsists of multiple tokens (xt, ..., xt+α), and we\ncan calculate its self-information by summing the\nself-information of its individual tokens according\nto the additivity property of self-information:\nI(u) =\nα∑\ni=t\nI(xi) (6)\nThe NLTK sentence tokenizer is employed to obtain\nsentence level lexical units. And we usespacy1 to\nmerge tokens into noun phrases. We do not merge\nverb phrases as it might produce very long phrases.\n3.3 Selective Retention of Informative Context\nWith the self-information of each lexical unit com-\nputed, we can now evaluate their informativeness.\nInstead of using a fixed threshold or retaining a\nfixed number of top k lexical units, we design a\npercentile-based filtering approach to adaptively\nselect the most informative content.\nFirst, we rank the lexical units based on their\nself-information values in descending order. Then,\nwe compute the p-th percentile of self-information\nvalues among all lexical units.\nIp = np.percentile([I(u0), .., I(uk)], p)\n(7)\nNext, we selectively retain lexical units with self-\ninformation values greater than or equal to the p-th\npercentile, constructing a filtered context C′:\nC′= Ui |I(Ui) ≥Ip, 1 ≤i ≤n (8)\nThe percentile-based filtering is a more flexible\napproach to retain the most informative content\ndepending on the distribution of self-information\nvalues in the given context. In Figure 2, we present\n1https://spacy.io/api/\npipeline-functions#merge_noun_chunks\n6344\nan example on phrase level where p is set to 50,\nwhich means half of phrases are filtered out. In\nthis case, the context after processing by selective\ncontext only retains 57.2% of tokens, which saves\n42.8% of context length.\n4 Experiments\nThe goal of Selective Context is to reduce the redun-\ndancy in the input context without compromising\nthe generation quality of LLMs. As a result, we are\nexpecting the answers given both selective context\nand the original context to be as close as possible.\nWe take the generated answer given full context\nas the reference answer, and compare to the gen-\nerated answer given the selective context in our\nexperiments.\n4.1 Datasets\nSelective Context prunes redundancy in the input\ncontext to allow very long context processing for\nLLMs. However, existing benchmarks for LLMs,\nsuch as MMLU (Hendrycks et al., 2020) and ARC\n(Clark et al., 2018), are mostly single round ques-\ntion answering and are thus not suitable to evaluate\nour proposed method. Therefore, we collect three\ntest sets consisting of long documents and conver-\nsations to evaluate Selective Context. Statistics in\ndetail are presented in Table 4.\nBBC News: A dataset containing news articles col-\nlected from the British Broadcasting Corporation\n(BBC). This dataset covers a wide range of topics,\nincluding politics, business, sports, and technology.\nWe use the full content of each news article in our\nexperiments.\narXiv Articles: A dataset consisting of latest aca-\ndemic papers, spaning various scientific disciplines,\nsuch as computer science, physics, and mathemat-\nics. As arXiv articles can be quite long, we only\nprocess the first two sections (usually introduction\nand background) for each paper in our experiments.\nShareGPT.com: ShareGPT.com is a platform\nwhere ChatGPT users share their surprising and in-\nteresting conversation with ChatGPT. This dataset\nconsists of conversations in different languages and\nin various scenarios (e.g., coding, chitchat, writing\nassistant, etc.). We use the ShareGPT dataset for\nthe conversation task in our experiments.\nThe three evaluation datasets were created care-\nfully to avoid data contamination. Data samples in\nthe BBC News, arXiv, and ShareGPT.com datasets\nwere all created after March 2023, which is after\nthe release of all LLMs in our experiments. Con-\nsidering some of baseline models are continually\nbeing updated, we employ the latest version re-\nleased before 30 March 2023 to make sure models\nhave never seen our test set in their pre-training and\nfine-tuning stage. In addition, as some of LLMs\nin our experiments have a max_length of 2048\ntokens, we do not include articles or conversations\nexceeding this length.\n4.2 Models\nWe test Selective Context on the following models:\nGPT-3.5 and GPT-4:GPT-3.5 also known as Chat-\nGPT, which is likely to be further fine-tuned from\nGPT-3 and InstructGPT. GPT-4 is the latest model\nfrom OpenAI, which has demonstrated substan-\ntially improved capability on complex reasoning\ncompared to its predecessor. GPT-3.5 and GPT-\n4 are unfortunately not open-source, we can only\naccess these models via web api2.\nLLaMA-7B, 13B, 30B: LLaMA is a family of\nopen-source language models released by Meta,\nwhich is reported to outperform GPT-3 with less\nparameters. The LLaMA family includes models\nwith size ranging from 7B to 65B. To investigate\nthe effect of scaling law to Selective Context, we\nexperiment with LLaMA with 7B, 13B, and 30B\nparameters.\nVicuna-7B, 13B: Vicuna (Chiang et al., 2023) is\na family of open-source language models instruct-\ntuned from LLaMA. According to their technical\nreport, Vicuna models perform quite well on a list\nof multitasking benchmarks.\n4.3 Tasks and Metrics\nWe evaluate Selective Context on four tasks:\nOriginal Context Reconstruction: Given a com-\npressed context produced by Selective Context, this\ntask aims to evaluate whether models are able to\nreconstruct the original context. This task assesses\nhow well the filtered context retains the essential\ninformation from the original context. In our exper-\niments, the compressed contexts are used as input,\nand the original contexts are used as reference an-\nswers.\nSummarisation: Given a context, the task is to\ngenerate a summary that captures the main points\nof the document. This task aims to evaluate\nwhether Selective Context affects the overall un-\nderstanding of models on the input contexts. In\n2https://platform.openai.com/docs/\napi-reference\n6345\nour experiments, the input and output are the com-\npressed context and the summaries generated based\non the compressed contexts. Summaries based on\nthe original (full) contexts are treated as the refer-\nence answers.\nQuestion Answering (QA):Given a document and\na set of questions, the task is to generate answers\nbased on the information available in the document.\nThis task aims to evaluate models’ understanding\nof a specific query. Here we first generate questions\nand answers based on the original context, where\nthese answers are treated as reference answers, and\nthen ask LLMs to answer these questions with se-\nlective context.\nConversation: This task is only for the ShareGPT\ndataset. Given a conversation history and a user\nquery, the task is to generate a response to the\nquery based on the previous conversation history.\nThis task aims to evaluate selective context’s perfor-\nmance on conversation. Specifically, we ask LLMs\nto answer the users’ last query of ShareGPT con-\nversation instances with selective context applied\non the previous conversation history.\nWe employ four metrics to assess the perfor-\nmance of our models on the tasks: BLEU, ME-\nTEOR, ROUGE, and BERTScore. BLEU (Pa-\npineni et al., 2002) calculates n-gram precision,\nwhich is the proportion of n-grams in the generated\ntext that are also present in the reference text. ME-\nTEOR (Banerjee and Lavie, 2005) takes additional\nfeatures such as synonymy, stemming and word or-\nder into consideration, which leads to more compre-\nhensive evaluation. ROUGE (Lin, 2004) focuses\non how much of the important information in the\nreference text is present in the generated summary.\nBERTScore (Zhang et al., 2019) leverages con-\ntextualised embeddings from pre-trained language\nmodels like BERT, computing the cosine similar-\nity between the generated text and reference text\nembeddings to capture semantic similarity more\neffectively than traditional n-gram-based metrics.\nAs mentioned before, we use the generated an-\nswers given the full contexts as the reference an-\nswers. When testing the deterministic decoding\nstrategy ( greedy decoding), we take one sin-\ngle run on full context as the reference answer.\nWhen testing the non-deterministic decoding strat-\negy (temperature = 0.7), we run multiple\ntimes on full context to obtain multiple reference\nanswers to address the randomness in decoding.\nThe metrics are computed based on the set of refer-\nence answers. In our experiment, we set the number\nof reference answers to 4.\n4.4 Experimental Settings\nWe use the smaller base causal language model\nfor self-information computing in our experiments.\nFor the LLaMA family and vicuna family, we em-\nploy LLaMA-7B to compute self-information. For\nthe OpenAI family, we use a smaller GPT-3 variant\ncurie for self-information computing, which is\navailable on OpenAI web API. In self-information\ncomputing, we do not process the entire context\nat once. This is due to our observation on the ten-\ndency of LLMs to give later lexical units lower self-\ninformation. Instead, we compute self-information\nsentence by sentence in our experiments.\nIn our experiments, we compare the two dif-\nferent dimensions that are adjustable in Selective\nContext.\nCompression Ratios: We experiment with differ-\nent content reduction ratios in Selective Context:\n0.2, 0.35, 0.5, 0.65, and 0.8. These ratios determine\nthe proportion of content to be filtered out, allow-\ning us to study the trade-off between efficiency and\nperformance as the amount of retained information\nvaries.\nLexical Units: Lexical units are the basic element\nof content reduction in Selective Context. It can\nbe sentence, phrases, or tokens. As mentioned in\n§3.2, we remove the redundancy in input context\nby a specific lexical unit level.\n5 Results\nExcept for §5.5, all results of selective context pre-\nsented are at the phrase level (the optimal).\n5.1 Overview\nIn Table 1, we first compare the performance of\nSelective Context against the Original Context to\nsee how well Selective Context preserves useful in-\nformation when reducing context cost. The metrics\nare averaged across all models mentioned in §4.2.\nThe performance drop is shown in parentheses.\nAs demonstrated in the table, using Selective\nContext only leads to a marginal drop when the\nreduction ratio is set to 0.2 or 0.35, despite it sig-\nnificantly reducing the context cost. The BLEU\nscore drops by only 0.05 when 20% of the con-\ntent is reduced. And the number is even smaller\nwhen it comes to ROUGE-1, where the drop is just\n0.03. This indicate a high level of consistency be-\n6346\nROUGE BERTScore\nMethod Ratio BLEU METEOR Rouge-1 Rouge-2 Rouge-L Precision Recall F1\nOriginal - .347 .496 .571 .383 .471 .910 .909 .909\nSelective\nContext\n0.2 .295 (.05) .460 (.04) .540 ( .03) .346 (.04) .438 (.03) .905 (.005) .900 (.009) .902 (.007)\n0.35 .243 (.10) .421 (.08) .504 (.07) .294 (.09) .396 (.07) .900 (.010) .894 (.015) .897 (.013)\n0.5 .179 (.17) .362 (.13) .449 (.12) .237 (.15) .344 (.13) .893 (.018) .882 (.027) .887 (.023)\n0.65 .127 (.22) .299 (.20) .391 (.18) .178 (.21) .287 (.18) .885 (.025) .870 (.039) .877 (.032)\n0.8 .070 (.28) .224 (.27) .311 (.26) .122 (.26) .225 (.25) .874 (.036) .852 (.057) .863 (.047)\nTable 1: Comparing Selective Context to the Original Context with temperature set to 0.7.\nROUGE BERTScore\nRatio BLEU METEOR Rouge-1 Rouge-2 Rouge-L Precision Recall F1\nRandom deletion 0.20 0.437 0.578 0.666 0.503 0.566 0.892 0.909 0.899\n0.35 0.360 0.514 0.629 0.423 0.502 0.879 0.895 0.886\n0.50 0.283 0.443 0.576 0.346 0.432 0.867 0.881 0.873\n0.65 0.210 0.378 0.522 0.279 0.371 0.855 0.868 0.860\n0.80 0.156 0.314 0.450 0.219 0.310 0.843 0.853 0.847\nSelective Context 0.20 0.527 0.643 0.714 0.585 0.631 0.930 0.932 0.931\n0.35 0.446 0.588 0.679 0.508 0.570 0.915 0.916 0.915\n0.50 0.350 0.528 0.642 0.425 0.501 0.900 0.902 0.900\n0.65 0.244 0.418 0.557 0.315 0.404 0.886 0.877 0.881\n0.80 0.160 0.328 0.464 0.223 0.319 0.875 0.858 0.866\nTable 2: Comparing Selective Context to the random deletion baseline when using greedy decoding.\ntween answers given selective contexts and original\ncontexts when the reduction ratio is 0.2. Selective\nContext also yields impressive results when 35%\nof the content is reduced, with BERT scores around\n0.9 and ROUGE-1 scores over 0.5. The drops be-\ncome noticeable as the reduction ratio rises to 0.5,\nwhere the average BLEU score drops 0.17 and the\naverage ROUGE-1 drops 0.12. A reduction ratio\nof 0.65 and 0.8 tends to be less valuable, as shown\nby the 0.18 drop on ROUGE-1 and 0.32 drop on\nBERTScore-F1.\nWe then compare Selective Context against the\nRandom compression baseline as shown in Table\n2. We observe that using Selective Context allows\nLLMs to generate very similar answers to the refer-\nence answers (answers given full context) although\nwe significantly reduce the context cost. Selec-\ntive Context maintains BERTScore-F1 above 0.9\nwhen the compression ratio is 0.5 or lower, which\nshows a high similarity with the reference answers.\nROUGE demonstrates the same trend: ROUGE-1\ncontinues to be above 0.64 and ROUGE-L keeps\nabove 0.5 when the ratio is under 0.5. We also\nnotice that Selective Context is significantly more\neffective than the random baseline: Selective Con-\ntext with compression ratio of 0.5 shows a better\noverlapping with the reference answer than Ran-\nRatio #Sorry Answer len. Unfaithfulness\nFull 0 160.3 -\n0.2 0 156.5 .027\n0.35 6 136.0 .050\n0.5 4 140.2 .038\n0.65 19 131.2 .051\n0.8 27 103.7 .086\nTable 3: Faithfulness test on gpt-3.5-turbo using\nselective context.\ndom baseline with only 20% content compression.\n5.2 Faithfulness\nTo evaluate to what extent selective context affects\nthe faithfulness of the LLMs generated content, we\nperform manual tests on our question answering\nresults based on the idea of Wang et al. (2020). We\nevaluate 1000 question-answer pairs (200 for each\nratio) with the following procedure: 1) We first\nextract OpenIE tuples from the answers of selective\ncontext, and then 2) manually evaluate whether\neach tuple is entailed by the reference answer. If\nthe model’s answer is \"Sorry, I don’t know\", we\ntreat it as \"Sorry\" cases and do not consider it as\nunfaithfulness.\nAs shown in the Table 3, we find that gpt-3.5\ntends to generate shorter answers or refuses to an-\n6347\nFigure 3: Performance of selective context on different tasks. x-axix represents compression ratios (same below).\nFigure 4: Acceptance rate of generated summaries.\nswer the questions if it fails to identify necessary\nevidence in the given selective context. With a\ncompression ratio of 0.65, gpt-3.5 refuses to an-\nswer 19 questions (9% of 200), and the answers are\n35% shorter than the reference answer (131 tokens\nin average). However, selective context doesn’t\nsignificantly affect the faithfulness across all com-\npression ratios. About 3.8% of all tuples are not\nentailed by the reference answer when the compres-\nsion ratio is 0.5, and this number rises slightly to\n5.1% as the compression ratio increases to 0.65.\n5.3 Tasks\nIn this part, we break down and analyse the perfor-\nmances of Selective Context in the four different\nNLP tasks: summarisation, question answering,\noriginal context reconstruction, and conversation.\nThe results are as shown in Fig. 3. First, the re-\nsults on the Original Context Reconstruction task\n(RC) show the steepest drop with increasing com-\npression ratio, however, Selective Context allows\nLLMs to preserve most of the key points in the\nFigure 5: Effects of lexical units.\noriginal context when the reduction ratio is lower\nthan 0.5, as demonstrated by a rather high ROUGE\nscore. Second, we notice that the curves of question\nanswering and summarisation decrease gradually\nand are continually higher than those of the other\ntwo tasks evaluated by BERTScore. We could say\nSelective Context is especially suitable for tasks of\nsummarisation and answer generation.\n5.4 Scaling and Instruct-Tuning\nWe perform human evaluation to explore the ef-\nfect of model scales and supervised instruct-tuning\non Selective Context. We asked three college stu-\ndents to evaluate 1150 generated summaries from\nllama and vicuna (about 55 per model and ra-\ntio) and record whether they accept the generation\nas a reasonable summary. As shown in Figure\n4, we find no specific trends between the scales\nand generation quality given Selective Context.\nThe vicuna family demonstrates similar sum-\nmarisation capability with 7b and 13b parame-\nters. And so does the llama family, larger models\ndo not show stronger robustness towards Selec-\ntive Context. But instruct-tuned model vicuna\ndemonstrates significantly superior performance\n6348\nOriginal Context, CUDA Memory = 77,695 MB; Time = 110.8 ms/token\nPlease see the original document and summary given full context in Appendix B.\nSelective Context , Ratio: 0.5, CUDA Memory = 61,885 MB, Time = 76.3 ms/token,\nTime to construct selective context = 46.1 ms\n[1]The above paragraph discusses the use of probabilistic methods, safety distance-based control methods,\nand trajectory prediction methods for assisting vehicles in avoiding collisions. [1]It mentions that CNN\nhas made outstanding contributions in vision tasks, particularly in road condition inspection, due to its\nexcellent regional feature extraction capabilities. However, the performance of CNN-based models for\nvision tasks is inferior to EfficientNets RepVGG, due to the huge differences between sequential tasks\nin NLP and image tasks in CV , the difficulty in keeping the original information of inputs after RNN\nlayers, and the computational and memory requirements of switching layers. The paragraph introduces a\nnew network structure called Sequential Spatial Network (SSN) blocks, which overcomes the limitations\nof traditional CNN-based models. [2]The SSN block consists of convolutional layers, Upsampling\nConvolutional Decreasing (UCD) blocks, and Reinforcement Region Unit and Fast MultiHead Self-\nAttention (FMHSA) to enhance local information and improve normalization capability. The paragraph\nconcludes by stating that the SSN network outperforms existing methods on the Lykit dataset and can be\neasily transferred for image classification tasks.\nFigure 6: Comparing the summary generated by vicuna_13b given original context and selective context.\nthan llama models given selective context indi-\ncating instruct-tuning might help the model to be\nmore robust against context compression. Given se-\nlective context, llama models often fail to follow\ninstructions and go wild very easily.\n5.5 Lexical Units\nWe test the effect of Selective Context based on\ndifferent lexical units: tokens, phrases, and sen-\ntences via BERTScore-F1. As shown in Table 5,\nemploying phrase as the basic lexical units in Selec-\ntive Context is the optimal approach, consistently\noutperforming the other two variants, followed by\ntoken-level Selective Context. Removing redun-\ndancy at sentence-level is a rather unstable imple-\nmentation compared to the token and phrase-level.\nThis experiment indicates that a reasonable granu-\nlarity can be crucial in Selective Context.\n5.6 Case Study\nTo have a straightforward impression on how well\nLLMs generate with selective context, we present\ntwo summaries given the full and selective context\nrespectively in Figure 6. The original document\nand processing to obtain selective context are pre-\nsented in Appendix B.\nWe first found that preparing selective context is\nextremely efficient. It takes a one-time cost of 46.1\nms to build selective context for the example para-\ngraph, which includes computing self-information\nand performing lexical unit tokenisation. This en-\nsures that the initial stage of establishing a selective\ncontext incurs very little overhead. Secondly, it\nshows selective context significantly reduces the\nmemory usage of the GPU and accelerates the gen-\neration process. With a compression ratio of 0.5,\nselective context reduces about 36% of the mem-\nory cost in inference and makes generation 1.32\ntimes faster (per token). By comparing the content\nof the two summaries, we see that the summary\ngiven selective context missed relevant information\nabout the research background (as denoted by the\n[1] marker), such as the use of machine learning in\nautonomous driving technology and instead starts\nwith the different methods directly. This is due\nto the background parts not being selected and re-\nmoved as redundancy before feeding to vicuna.\nWe tried to ask vicuna\n\"what is the background of this study?\",\ngiven the selective context, and obtained a decent\nanswer:\n\"the research background of this paper is likely\nto be situated in the domain of autonomous driv-\ning technology and the application of artificial in-\ntelligence (AI) for improving vehicle safety and\ndecision-making capabilities.\".\nThis demonstrates that LLMs are likely to be\nable to infer the deleted parts of background infor-\nmation in the selective context. Selective context\nalso affects vicuna’s decision on what informa-\n6349\ntion should be included in the summary as the sec-\nond summary includes details about, for example,\nFMHSA and UCD block (as denoted by the [2]\nmarker) which are not covered in the summary gen-\nerated with the full context. We find no factual\nerrors in the summary given selective context.\n6 Conclusion\nWe introduced Selective Context to improve the\ncontext efficiency of LLMs in inference by deleting\nredundant content measured by self-information.\nOur extensive experiments on arXiv papers, BBC\nnews articles, and conversation transcripts showed\nthat our proposed method can significantly reduce\nGPU memory cost, accelerate generation with mi-\nnor performance decrease, and potentially enable\nLLMs to handle long documents and extended con-\nversations without the risk of context truncation.\n7 Limitations\nSelective Context demonstrates promising results,\nbut it is still necessary to note a couple of poten-\ntial limitations. Firstly, our approach is somewhat\ninfluenced by the phrase boundary detection pro-\ncedure. We employ the noun phrase tokenisation\nalgorithm provided by spacy in our experiments.\nHowever, we do not consider verb phrases as there\nis no mature solution for verb phrase tokenisation.\nWe speculate that we can achieve better compres-\nsion performance with dependency tree-based fil-\ntering procedure which might lead to better bound-\nary identification of lexical units. Secondly, in the\nexperiment section, we use percentile to control\nthe pruning process. However, the optimal com-\npression percentile varies based on specific tasks\nand context. Developing a tool to find the optimal\nthreshold can further enhance the effectiveness of\nselective context.\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nRazvan Bunescu and Oseremen O Uduehi. 2022.\nDistribution-based measures of surprise for creative\nlanguage: Experiments with humor and metaphor.\nIn Proceedings of the 3rd Workshop on Figurative\nLanguage Processing (FLP), pages 68–78.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith,\nand Danqi Chen. 2023. Adapting language\nmodels to compress contexts. arXiv preprint\narXiv:2305.14788.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Xiaodong Song, and\nJacob Steinhardt. 2020. Measuring massive multitask\nlanguage understanding. ArXiv, abs/2009.03300.\nYucheng Li. 2023. Unlocking context constraints of\nllms: Enhancing context efficiency of llms with\nself-information-based content filtering. ArXiv,\nabs/2304.12102.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.\nLearning to compress prompts with gist tokens.\narXiv preprint arXiv:2304.08467.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\n6350\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal,\n27(3):379–423.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Kyunghyun Cho, and Mike Lewis.\n2020. Asking and answering questions to evalu-\nate the factual consistency of summaries. ArXiv,\nabs/2004.04228.\nErnst-Jan C. Wit and Marie Gillette. 1999. What is\nlinguistic redundancy. University of Chicago.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nA Dataset statistics\nDataset #Doc #Sent #Phrase #Token\nArxiv 408 28.20 514.55 864.85\nShareGPT 470 27.35 389.42 689.32\nBBC 294 25.63 523.96 732.54\nTable 4: Statistics of the three datasets. #Sent, #Phrase,\n#Token are averaged per document.\nB Example of selective context on long\ncontext\nHere we present an example of selective con-\ntext on a rather long context. The original para-\ngraphs is from https://arxiv.org/abs/\n2303.07352. The original paragraphs is shown\nin Fig. 7. The resulting context is shown in Fig. 8.\nThe reference summary is given in Fig. 9.\nC The Previous Version of Selective\nContext\nIf you’re looking for the previous of the paper,\nplease check (Li, 2023).\n6351\nINTRODUCTION In the past many years , researchers have focused on how to turn vehicles from\nassisted driving to more intelligent autonomous driving . Due to the iteration of intelligent hardware\nand the improvement of chip computing power , a large amount of data collected by sensors can be\nquickly converted and fed into models to make decisions . In the driving process , the safety factor\nis the first consideration for users and researchers . Therefore , how A V should avoid collisions\nhas become a top priority . Concepts such as probabilistic methods ( eg . : Markov chains and\nMonte Carlo ) , safety distance-based control methods , and trajectory prediction methods have been\ndesigned in recent years to cope with complex traffic conditions. In terms of vision , CNN has made\noutstanding contributions and has been applied to a large number of road condition inspection tasks\ndue to its excellent regional feature extraction capabilities . The local feature information obtained by\nCNN will be used for obstacle detection . Secondly , because the motion trajectory is planned for A V\n, the relationship between each local feature of the image obtained by CNN needs to be established .\nSome strategies are based on CNN plus RNN so that they can deal with sequential graphs as input\n, eg . : STDN . Although the above strategies have performed well in a large number of vision tasks\n, their performances are still far inferior to similar-sized convolutional neural networks counterparts\n, such as EfficientNets and RepVGG . We believe this is due to the following aspects . First\n, the huge differences between the sequential tasks of NLP and the image tasks of CV are\nignored . For example , when the local feature information acquired in a two-dimensional image\nis compressed into one-dimensional time series information , how to achieve accurate mapping\nbecomes a difficult problem . Second , it is difficult to keep the original information of inputs\nsince after RNN layers , we need to recover the dimension from one to three . Besides , due\nto the several transformations between different dimensions , that process becomes even harder ,\nespecially since our input size is 224×224×5 . Third , the computational and memory requirement\nof switching between layers are extremely heavy tasks , which also becomes a tricky point for\nthe algorithm to run . Higher hardware requirements as well as more running time arise when\nrunning the attention part . In this paper , we propose a new network structure based on CNN and\nattention to vision tasks in autonomous driving.The new network structure overcomes these problems\nby using Sequential Spatial Network (SSN) blocks . As shown in Fig . , input images first go\nthrough the convolution stem for fine-grained feature extraction , and are then fed into a stack of\nSSN blocks for further processing . The Upsampling Convolutional Decreasing (UCD) blocks are\nintroduced for the purpose of local information enhancement by deep convolution, and in SSN block\nof features generated in the first stage can be less loss of image resolution , which is crucial\nfor the subsequent trajectory adjustment task . In addition , we adopt a staged architecture design\nusing five convolutional layers with different kernel sizes and steps gradually decreasing\nthe resolution ( sequence length ) and flexibly increasing the dimensionality . Such a design helps\nto extract local features of different scales and , since the first stage retains high resolution\n, our design can effectively reduce the resolution of the output information in the first layer\nat each convolutional layer , thus reducing the computational effort of subsequent layers .\nThe Reinforcement Region Unit ( RRU ) and the Fast MultiHead Self-Attention (FMHSA ) in\nthe SSN block can help obtain global and local structural information within the intermediate features\nand improve the normalization capability of the network . Finally , average pooling is used to obtain\nbetter trajectory tuning . Extensive experiments on the lykit dataset demonstrate the superiority of\nour SSN network in terms of accuracy . In addition to image classification , SSN block can be easily\ntransferred to other vision tasks and serve as a versatile backbone.\nFigure 7: Selective context on the introduction of https://arxiv.org/abs/2303.07352\n6352\nIn researchers how turn vehicles assisted driving more intelligent autonomous drivingDue the iteration\nintelligent hardware the improvement chip computing power collected quickly converted fed\nmodels In the driving process the safety factor users researchers Therefore how A V should avoid\ncollisions has Concepts such probabilistic methods ( eg . : ) , safety distance-based control methods\n, trajectory prediction methods designed cope complex traffic conditions In terms vision\nCNN made outstanding contributions and applied road condition inspection tasks due\nits excellent regional feature extraction capabilities The local feature information obtained CNN\nwill obstacle detection Secondly because the motion trajectory planned for A V the relationship\neach local feature obtained CNN needs established Some strategies based CNN plus RNN so\ndeal sequential graphs as input eg STDN . Although the above strategies performed vision tasks\ntheir performances inferior EfficientNets RepVGG . We believe this due the following aspects\nFirst the huge differences the sequential tasks NLP the image tasks CV ignored For example when\nthe local feature information acquired compressed how achieve accurate mapping becomes Second\ndifficult keep the original information of inputs since after RNN layers recover the dimension from\none three Besides due the several transformations between different dimensions that process becomes\nespecially our input size 224×224×5 Third , the computational and memory requirement switching\nlayers are extremely heavy tasks also becomes a tricky point Higher hardware requirements as\nrunning arise the attention partIn CNN attention to vision tasksThe new network structure overcomes\nSequential Spatial Network (SSN) blocks As shown Fig , input images first go the convolution stem\nfor a stack SSN blocks The Upsampling Convolutional Decreasing (UCD) blocks introduced for\nlocal information enhancement deep convolution , in SSN block of features generated in can\nless loss of image resolution crucial the subsequent trajectory adjustment task In addition we\nadopt a staged architecture design using five convolutional layers steps gradually ( sequence length\nflexibly Such a design helps extract local features different scales , since the first stage retains\nhigh resolution our design effectively reduce the output information atThe Reinforcement Region Unit\nand the Fast MultiHead Self-Attention (FMHSA in the SSN block can help obtain within\nthe intermediate features and improve the normalization capability Finally average pooling\nbetter trajectory tuning Extensive experiments the lykit dataset demonstrate our SSN network In\naddition image classification SSN block can easily transferred and serve a versatile backbone.\nFigure 8: The resulting context\nGiven Original Context, CUDA Memory = 77,695 MB; Time = 110.8 ms/token\nThe paragraph discusses the development of autonomous driving technology and the use of artificial\nintelligence and machine learning to make decisions in the driving process. The safety of the vehicle\nis a top priority and methods such as probabilistic methods, safety distance-based control methods, and\ntrajectory prediction methods have been developed to cope with complex traffic conditions. The use\nof convolutional neural networks (CNN) has been successful in image tasks such as obstacle detection,\nbut the performance of these networks is still inferior to similar-sized convolutional neural networks\ncounterparts. This is due to the huge differences between the sequential tasks of NLP and the image\ntasks of CV , the difficulty of keeping the original information of inputs, and the heavy computational\nand memory requirement of switching between layers. The authors propose a new network structure\nbased on CNN and attention to vision tasks in autonomous driving, using Sequential Spatial Network\n(SSN) blocks. The network structure overcomes these problems by using deep convolution and attention\nmechanisms to extract local and global features, and reducing the computational effort of subsequent\nlayers. The proposed network structure is evaluated on the lykit dataset and shows superior performance\nin terms of accuracy. B.\nFigure 9: The reference summary generated by vicuna-13b given the full context.\n6353",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.781948447227478
    },
    {
      "name": "Computer science",
      "score": 0.7723636627197266
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.5712249875068665
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5618518590927124
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.5345233082771301
    },
    {
      "name": "Context model",
      "score": 0.5251452922821045
    },
    {
      "name": "Latency (audio)",
      "score": 0.49712565541267395
    },
    {
      "name": "Language model",
      "score": 0.49130192399024963
    },
    {
      "name": "Machine learning",
      "score": 0.3680877685546875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3611656427383423
    },
    {
      "name": "Natural language processing",
      "score": 0.35521838068962097
    },
    {
      "name": "Mathematics",
      "score": 0.09134536981582642
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I28407311",
      "name": "University of Manchester",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ],
  "cited_by": 34
}