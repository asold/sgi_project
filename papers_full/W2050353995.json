{
    "title": "Extending the Body to Virtual Tools Using a Robotic Surgical Interface: Evidence from the Crossmodal Congruency Task",
    "url": "https://openalex.org/W2050353995",
    "year": 2012,
    "authors": [
        {
            "id": null,
            "name": "Sengül, Ali",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "van Elk, Michiel",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "Rognini, Giulio",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "Aspell, Jane Elizabeth",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2747589507",
            "name": "Bleuler Hannes",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2566483350",
            "name": "Blanke Olaf",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne",
                "University Hospital of Geneva"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2156776168",
        "https://openalex.org/W2013158677",
        "https://openalex.org/W2126672489",
        "https://openalex.org/W1968024137",
        "https://openalex.org/W4388322852",
        "https://openalex.org/W2089362667",
        "https://openalex.org/W2129553141",
        "https://openalex.org/W2108787849",
        "https://openalex.org/W2002528399",
        "https://openalex.org/W2028115759",
        "https://openalex.org/W4245630123",
        "https://openalex.org/W2107915650",
        "https://openalex.org/W1535609573",
        "https://openalex.org/W2025831997",
        "https://openalex.org/W2155671396",
        "https://openalex.org/W2151412583",
        "https://openalex.org/W1972397711",
        "https://openalex.org/W2016058334",
        "https://openalex.org/W2119233587",
        "https://openalex.org/W2311664600",
        "https://openalex.org/W2118905624",
        "https://openalex.org/W2169084829",
        "https://openalex.org/W2048019089",
        "https://openalex.org/W2106775387",
        "https://openalex.org/W2144078592",
        "https://openalex.org/W2056416584",
        "https://openalex.org/W2127765148",
        "https://openalex.org/W2002699182",
        "https://openalex.org/W1989935201",
        "https://openalex.org/W2086705705",
        "https://openalex.org/W2059582997",
        "https://openalex.org/W1522214628",
        "https://openalex.org/W2009180946",
        "https://openalex.org/W2145378345",
        "https://openalex.org/W2157062268",
        "https://openalex.org/W1531951767",
        "https://openalex.org/W4255829629",
        "https://openalex.org/W1906884769"
    ],
    "abstract": "The effects of real-world tool use on body or space representations are relatively well established in cognitive neuroscience. Several studies have shown, for example, that active tool use results in a facilitated integration of multisensory information in peripersonal space, i.e. the space directly surrounding the body. However, it remains unknown to what extent similar mechanisms apply to the use of virtual-robotic tools, such as those used in the field of surgical robotics, in which a surgeon may use bimanual haptic interfaces to control a surgery robot at a remote location. This paper presents two experiments in which participants used a haptic handle, originally designed for a commercial surgery robot, to control a virtual tool. The integration of multisensory information related to the virtual-robotic tool was assessed by means of the crossmodal congruency task, in which subjects responded to tactile vibrations applied to their fingers while ignoring visual distractors superimposed on the tip of the virtual-robotic tool. Our results show that active virtual-robotic tool use changes the spatial modulation of the crossmodal congruency effects, comparable to changes in the representation of peripersonal space observed during real-world tool use. Moreover, when the virtual-robotic tools were held in a crossed position, the visual distractors interfered strongly with tactile stimuli that was connected with the hand via the tool, reflecting a remapping of peripersonal space. Such remapping was not only observed when the virtual-robotic tools were actively used (Experiment 1), but also when passively held the tools (Experiment 2). The present study extends earlier findings on the extension of peripersonal space from physical and pointing tools to virtual-robotic tools using techniques from haptics and virtual reality. We discuss our data with respect to learning and human factors in the field of surgical robotics and discuss the use of new technologies in the field of cognitive neuroscience.",
    "full_text": "Extending the Body to Virtual Tools Using a Robotic\nSurgical Interface: Evidence from the Crossmodal\nCongruency Task\nAli Sengu¨ l1,2,3*, Michiel van Elk1,2, Giulio Rognini1,2,3, Jane Elizabeth Aspell2, Hannes Bleuler3,\nOlaf Blanke1,2,4\n1 Center for Neuroprosthetics, School of Life Sciences, Ecole Polytechnique Fe´de´rale de Lausanne (EPFL), Lausanne, Switzerland,2 Laboratory of Cognitive Neuroscience,\nBrain Mind Institute, Ecole Polytechnique Fe´de´rale de Lausanne (EPFL), Lausanne, Switzerland,3 Systems Robotic Laboratory, Ecole Polytechnique Fe´de´rale de Lausanne\n(EPFL), Lausanne, Switzerland,4 Department of Neurology, University Hospital of Geneva (HUG), Geneva, Switzerland\nAbstract\nThe effects of real-world tool use on body or space representations are relatively well established in cognitive neuroscience.\nSeveral studies have shown, for example, that active tool use results in a facilitated integration of multisensory information\nin peripersonal space, i.e. the space directly surrounding the body. However, it remains unknown to what extent similar\nmechanisms apply to the use of virtual-robotic tools, such as those used in the field of surgical robotics, in which a surgeon\nmay use bimanual haptic interfaces to control a surgery robot at a remote location. This paper presents two experiments in\nwhich participants used a haptic handle, originally designed for a commercial surgery robot, to control a virtual tool. The\nintegration of multisensory information related to the virtual-robotic tool was assessed by means of the crossmodal\ncongruency task, in which subjects responded to tactile vibrations applied to their fingers while ignoring visual distractors\nsuperimposed on the tip of the virtual-robotic tool. Our results show that active virtual-robotic tool use changes the spatial\nmodulation of the crossmodal congruency effects, comparable to changes in the representation of peripersonal space\nobserved during real-world tool use. Moreover, when the virtual-robotic tools were held in a crossed position, the visual\ndistractors interfered strongly with tactile stimuli that was connected with the hand via the tool, reflecting a remapping of\nperipersonal space. Such remapping was not only observed when the virtual-robotic tools were actively used (Experiment\n1), but also when passively held the tools (Experiment 2). The present study extends earlier findings on the extension of\nperipersonal space from physical and pointing tools to virtual-robotic tools using techniques from haptics and virtual reality.\nWe discuss our data with respect to learning and human factors in the field of surgical robotics and discuss the use of new\ntechnologies in the field of cognitive neuroscience.\nCitation: Sengu¨ l A, van Elk M, Rognini G, Aspell JE, Bleuler H, et al. (2012) Extending the Body to Virtual Tools Using a Robotic Surgical Interface: Evidence from\nthe Crossmodal Congruency Task. PLoS ONE 7(12): e49473. doi:10.1371/journal.pone.0049473\nEditor: Nicholas P. Holmes, University of Reading, United Kingdom\nReceived May 7, 2012;Accepted October 9, 2012;Published December 5, 2012\nCopyright: /C2232012 Sengu¨ l et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits\nunrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nFunding: A. Sengu¨ l was supported by ARAKNES project, a European Community’s Seventh Framework Programme (FP7-ICT-2007-2) and SAFROS project, a\nEuropean Community’s Seventh Framework Programme research project (FP7-ICT-2009.5.2), M. van Elk was supported by a Marie Curie Grant (IEF grant 252713).\nO. Blanke is supported by the European Science Foundation, the Swiss National Science Foundation, and the Bertarelli Foundation. The funders had no role in\nstudy design, data collection and analysis, decision to publish, or preparation of the manuscript.\nCompeting Interests:The authors have declared that no competing interests exist.\n* E-mail: ali.sengul@epfl.ch\nIntroduction\nComplex tool-use is a uniquely human activity and its\nachievement enabled a remarkable step forward in the evolution\nof our species. In daily life we use spoons, knives, forks, pencils,\nrulers and scissors. When enjoying sports we use a variety of\nequipment such as golf clubs or tennis rackets. In addition, we\noften interact with tools to control computers, such as a keyboard,\ncomputer mouse and joysticks. In a certain sense, even vehicles\ncan be considered as tools that greatly extend the boundaries of\nour physical body and our bodily capabilities [1,2]. Thus, tools can\nbe used to extend our action space and to perform many tasks of\ndaily life, whether at home, at work, or for recreation [3,4].\nTo perform common daily tasks easily, we use different tools\nthat can be categorized based on their function and characteristics.\nHolmes and Spence [5] classified tools into three categories:\nphysical interaction tools, pointing tools and detached tools.\nPhysical interaction tools function as a physical connection\nbetween the body and environment (e.g. a brush, a stick, a pen\netc.). They are often hand-held objects that are purposefully used\nto interact with other objects to achieve a goal (e.g. brushing the\nfloor). Many studies have described the changes in the neural\nrepresentation of multisensory peripersonal space with physical\ntools in healthy adults [6–8]. The second category consists of\npointing tools that are typically used to point at another object (e.g.\na laser pointer). There is no direct physical connection between the\nuser and the objects with which they interact. Some studies have\ninvestigated the changes in the neural representation of multisen-\nsory peripersonal space with pointing tools in healthy participants\n[9,10]. The last category of tools defined by Holmes and Spence\n[5] are detached tools. A human operator uses an interface to\nperform a task at distant locations or even in virtual reality\n(computer screen). With detached tools there is no direct physical\nlinkage between the user and the target whereas those tools exhibit\nPLOS ONE | www.plosone.org 1 December 2012 | Volume 7 | Issue 12 | e49473\nforce and motion coupling between the human and virtual\nenvironment. Up to now, the effect of detached tool use on the\nneural representation of multisensory peripersonal space has not\nbeen studied.\nRecent advances in robotics have brought attention to a specific\nclass of detached tools: human-robot interfaces. Robotic tools,\nsuch as surgery robots and telemanipulators are designed to\noperate at distant locations or in virtual scenarios under direct\nhuman control [11,12]. These unprecedented tools greatly\nincrease the precision, force and accessibility of human manipu-\nlation (within the human body or within industrial systems such as\npiping, turbines etc.). These tools allow increased dexterity by\ndown-scaling position or up-scaling forces. Such devices are also\ncalled ‘master-slave’ systems: they have a master side, which\ndetects the positions and motion of the user and sends this\ninformation to a slave robot that is in contact with the remote\nenvironment [13]. Surgeons now frequently and efficiently interact\nwith such telemanipulators to perform complex tasks, such as\nlaparoscopic surgery [14–16].\nDespite these technical advances in the field of surgical robots\nand telemanipulators, only little attention has been paid to the\npsychological and cognitive mechanisms that underlie the\ninteractions with these devices. The aim of these robots is to\nincrease the telepresence and transparency between the surgeon\nand the environment, as well as the accuracy and intuitiveness of\nthe use of the system. The precise role of the different factors that\ncontribute to the extension of one’s body and one’s peripersonal\nspace during virtual tool use remains unknown. For instance, it is\nunclear whether the inclusion of sensory modalities, such as touch\nor haptic feedback, would result in a more realistic interaction and\nthereby improve the control over the robot. In order to address\nthese questions we here propose a cognitive neuroscience\napproach to robotics. More specifically, we measured the\nmultisensory integration of vision and touch when operating a\nhaptic device and evaluated the potential of such newly emerging\nsurgical robotic devices at the level of their perception as a tool by\nthe human brain [17].\nThe integration of information from different sensory modalities\nby the human brain is a complex process that has received a lot of\nattention in cognitive neuroscience. Previous studies have shown\nfor instance, that the space around the body (near extrapersonal\nspace or peripersonal space) is represented in the brain differently\nthan the extrapersonal space that is far from the body [18].\nPeripersonal space representation is based on the multisensory\nintegration of visual, tactile, somatosensory, and auditory cues in\nthe frontal and parietal lobes of the human cortex [18]. The brain\nrepresentation of peripersonal space is highly plastic and has been\nshown to adapt dramatically to physical tool use, for example by\nextending or projecting peripersonal space beyond its normal\nrange to also include the end of a handheld tool. Several studies\nusing single unit recordings in macaque monkeys and the spatial\nmodulations of multisensory behavior in healthy participants and\nbrain-damaged patients have supported the idea that tool use\nextends multisensory peripersonal space [19–21]. One method for\nstudying the integration of multisensory information in relation to\ntool use is the crossmodal congruency task [21,22]. In this task,\nparticipants are required to respond to the elevation of tactile\nstimuli to the thumb and index finger, while at the same time\nignoring visual distractors presented at the end of the tool.\nParticipants are asked to make speeded elevation judgments (up vs\ndown) to the vibrotactile stimulus while ignoring the visual\ndistractors. The crossmodal congruency effect (CCE) is defined as\nthe reaction time difference between incongruent conditions (light\nand vibration in opposite position, i.e. up vs. down) and congruent\nconditions (light and vibration both up or both down). The CCE is\nconsidered a reliable measure of multisensory integration in\nperipersonal space, as it has been shown that the CCE is enhanced\nfor objects that can be easily integrated in the body representation,\nsuch as rubber hands and virtual bodies [21,22]. CCEs have been\nshown to reflect changes in hand ownership [23,24] as well as full-\nbody ownership [25]. Recently, CCEs have been used to study the\neffect of tool-use and in particular whether humans experienced\nchanges in the representation of peripersonal space. It has been\nfound, for instance, that after active tool use, visual distractors\npresented at the tip of the tool interfered with tactile stimuli\npresented at the hand of the participant holding the tool,\nsuggesting that the peripersonal space representation was changed\nwith tool use [8,21,22] and that peripersonal space was extended\nby active tool use [8].\nAlthough several tool-use studies have now established these\nchanges in the representation of peripersonal space by using the\nCCE, they were all done with simple physical tools such as golf\nclubs, rakes or sticks. Advanced robotic tools such as haptic devices\nor master/slave systems have to our knowledge not yet been\ninvestigated in this context (but see Moizumi et al., 2007).\nCompared to physical tools, these advanced tools behave like ‘tele-\narms’ and user movements from the master side are mapped on\nthe remote tools. For instance, in the case of a surgical robotic\nsystem, the slave robot performs the movements of the surgeon\nwho can be working at a different physical location. One of the\naims in surgical robotics is to increase the immersion and\ntelepresence of the surgeon into the remote site by using\nappropriate display and telepresence technologies to regain\nvirtually direct access to the operation - comparable to the\nexperience during open surgery. Hence, the study of the\nneurocognitive aspects of these advance tools is important from\nan engineering perspective. For instance, studying how the\nperipersonal space representation changes with virtual robotic\ntool use would provide an objective evaluation of the telepresence\nand the usability of the surgical robotic systems. This will give new\ninsights in the design of usable and immersive surgical robotic\nsystem that can easily be integrated by the brain and are\nexperienced by the surgeons as a ‘tele-arm’. In addition, virtual\ntools enable to study the different factors involved in the\nremapping of peripersonal space (e.g. providing haptic feedback,\nchanging the mapping from the arm movements to the tool\nmovements etc.; cf. Sengu¨l et al. in prep.). The present study\nprovides the first step towards investigating virtual tool use using\ncognitive neuroscience methods.\nWe tested whether the peripersonal space representation would\nchange ‘naturally’ even in the case of a telemanipulated tool, i.e.\nwithout a mechanical connection between the hand (master side)\nand the point of physical action (slave side). As a benchmark, the\nwell-known experiment of Maravita et al. [8], performed with\nphysical tools (golf clubs), was reproduced here in a new\ntechnological context (multimodal haptic interface operating on\na VR physical scene). The participant’s task was to cross or uncross\nthe virtual golf clubs in experiment 1 and to just hold the tool\ninterface in experiment 2, in which the crossing and uncrossing\nwas done by the experimenter. Subsequently, participants were\nrequired to make speeded elevation discriminations of vibrotactile\nstimuli delivered to the thumb and index finger – using a foot\npedal while ignoring visual distractors presented at the end of the\nvirtual tool. Unlike physical tools, virtual robotic tools behave like\n‘tele-arms’. User movements from the master side are mapped on\nthe remote tools. In combination with appropriate displays (3D\nmonitors or head mounted displays) and telepresence technologies\n(realistic 3D graphics), they aim to enable a high level of\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 2 December 2012 | Volume 7 | Issue 12 | e49473\nimmersion into the remote site. Because of these different physical\nand information processing mechanism in virtual-robotic tools\ncompared to physical tools, we hypothesize that the neural\nrepresentation of the peripersonal space would change quickly due\nto the immersive nature of these tools. In the first study we tested\nwhether the active use of virtual-robotic tools would alter the\nspatial dependency of CCEs in a similar way as found in\nMaravita’s (2002) experiment with physical tools [8]. That is, we\nexpected that visual distractors would interfere with tactile stimuli\napplied to the hand that is holding the tool and this should be the\ncase not only for the uncrossed posture but also for the crossed\nposture. In the second study we tested whether the spatial\nmodulation of the CCE according to the tool posture (i.e. crossed\nor uncrossed) is the result of the active use of the tools or not. To\nthis end in the second experiment the tools were not crossed\nactively, but passively by the experimenter. We expected an\ninterference of visual distractors with the tactile stimuli connected\nto the hand by the tool for both the uncrossed posture and the\ncrossed posture due to the immersive and ‘tele-arm’-like nature of\nthe virtual robotic tools, thereby facilitating a remapping of\nperipersonal space even when the tool is only used passively.\nMaterials and Methods\nSubjects\nA total of 19 healthy right-handed participants took part in\nthese experiments: Ten participants (2 female, ages 21–24, mean\nage (SE): 22.3 (1.2) years) in study 1. Nine participants (3 female,\nages 19–28, mean age (SE) 23.2 (2.6) years) in study 2. All\nparticipants had normal or corrected to normal vision, no disorder\nof touch and had no history of neurological or psychiatric\nconditions. Each experiment took approximately 60 minutes per\nparticipant. The participants were informed about the general\npurpose of the research, were fully debriefed and were given the\nopportunity to ask questions and to comment on the research after\nthe experiment. All participants gave written informed consent\nand were compensated for their participation. The experimental\nprocedure was approved by the local research ethics committee –\nLa Commission d’e´thique de la recherche Clinique de la Faculte´\nde Biologie et de Me´decine – at the University of Lausanne,\nSwitzerland and was performed in accordance with the ethical\nstandards laid down in the Declaration of Helsinki.\nMaterials and Apparatus\nWe employed a robotic system consisting of a bimanual haptic\ninterface for the training of operations with the da Vinci surgery\nsystem (Mimic’s dV-TrainerTM, Mimic Technologies Inc., Seattle\nUSA [26] see Figure 1D). The da Vinci system is a well-known\nsurgical robotic system that is used for minimally invasive surgical\nprocedures. This novel device is the first test-bed for the\ntelerobotic surgical simulator. The tracking of the hand move-\nments and force feedback are provided through a cable-driven\nsystem. Since it is a cable driven system, it has a large workspace.\nIt provides 7 Degrees of Freedom (DOF) in motion for each hand\nand it can render high forces in 3-translation directions (x,y,z)\nwithout instability. The system has two lightweight grippers that\nenable transparent interactions with virtual reality making realistic\nbimanual manipulations possible. The participants were seated at\na table and held two haptic interfaces, one in their left and one in\ntheir right hand. The index and thumb of both hands were\npositioned in the haptic device as shown in Figure 1D and their\nmovements and interactions with virtual objects were presented on\na head mounted display (HMD, eMagin Z800 3DVisor, 1.44\nmegapixel resolution).\nAn open source platform, CHAI 3D, and a set of C++ libraries\nwere used for the modeling and for simulating the haptics, and for\nvisualization of the virtual world. This platform supports several\ncommercial haptic devices and it is possible to extend it to support\nnew custom force feedback devices. We have extended this\nplatform by adding the drivers and libraries of our custom force\nfeedback device.\nTwo vibrotactile target stimulators (Precision MicroDrive\nshaftless vibration motors, model 312-101, 3 V, 80 mA,\n9000 rpm (150 Hz) 1.7 g with a diameter of 12 mm and a length\nof 3.4 mm) were attached to the participants’ thumb and index\nfinger. Foam and rubber padding was used to insulate the\nvibrotactile stimulators from the surrounding material, thus\nminimizing any conduction of vibrations through the haptic\ndevice itself. For each participant, these stimulators were tested to\ngenerate easily localizable and clearly perceptible sensations.\nVibrotactile stimuli were driven by electrical signals generated by a\ndesktop computer (Intel Core i7 CPU with 2.8 GHz, 3 GB or\nRam, with NVIDIA GeForce 9800 GT Graphic Card). Two data\nacquisition cards (NI PCI-6014 and NI PCI 6052E) were used to\ndetect pedal responses and to drive vibrotactile stimulators. To\nminimize any unwanted reflections, the participants were seated in\na dimly illuminated room enclosed by black curtains.\nThe participants viewed two virtual-robotic tools through a\nhead mounted display. The distance between the tools subtended\napproximately 35 u of visual angle. Visual distractor stimuli\nsubtended approximately 0.9u of visual angle, positioned at the\nFigure 1. Virtual reality views and experimental setup used in\nthe experiments.(A) Virtual tools in an uncrossed posture: the small\nballs on the upper and lower part of the tools are the visual distractors.\nThey are presented simultaneously with the vibrotactile stimuli to\ndistract the participants. They can be presented at the same positions\nas the vibrotactile stimuli (congruent) or at different positions\n(incongruent). (B) Virtual tools in a crossed posture: The big balls in\nthe middle of the tools have two functions. First they indicate that the\nCCE phase is finished. Second they indicate the position to locate the\ntools to keep the distance between each tool constant. The big cross in\nthe middle of the tools is the fixation point. (C, D) A cable driven haptic\ndevice (the Da Vinci Simulator) with a large workspace was used.\nParticipants interacted with the virtual object through the handles of\nthe device. Their interactions were shown through a head mounted\ndisplay. To mask the noise of the vibrators and environmental noise,\nheadphones were used to present white noise. Participants responded\nto vibrotactile stimuli using the foot pedal. A chin rest system was used\nto prevent undesired movement of the head. The participant has given\nwritten informed consent (according to the PLoS guidelines) for the\npublication of her picture.\ndoi:10.1371/journal.pone.0049473.g001\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 3 December 2012 | Volume 7 | Issue 12 | e49473\nupper and lower locations of tips of the virtual-robotic tools that\nhad a visual angle of 6.4 u. For the modeling, simulating the\nhaptics, and visualization of the virtual world, CHAI 3D and a set\nof C++ libraries were used. A virtual world with two virtual golf\nclubs was developed, see Figure 1 A and B.\nA fixation cross was positioned at the vertical and horizontal\nmid-point of the corresponding four LEDs on the two tools. A chin\nrest system was used to prevent undesired movement of the head.\nIn order to measure the participant’s response, two pedals were\nattached to the floor next to the participant’s right foot. The pedal\nseparation was adjusted to fit the participant’s foot size. One of the\npedals was placed under the heel and the other under the toe of\nthe participant’s right foot. The participant raised his toes to\nindicate that the vibrations were felt at the index finger or raised\nhis heels to indicate that the vibrations were felt at the thumb.\nWhite noise was presented over the headphones at an adequate\nlevel so that participants could not hear the sound of the\nvibrotactile stimulators or the operation of the other hardware\nduring the experiments.\nExperimental Design and Procedure\nThe experiments were designed in a 26262 factorial manner.\nThe 3 within-participants factors were congruency of the elevation\nof the vibrotactile stimuli with respect to the visual distractors\n(congruent vs. incongruent), the vibrotactile target side relative to\nthe visual distractor side (same vs. different), and the type of\nposture (uncrossed vs. crossed). There were two blocks of 16\npractice trials each, which were not analyzed. Experimental blocks\nbegan when the participant achieved an accuracy of more than 85\npercent. A total of 480 experimental trials were given, divided into\n15 blocks, with 240 trials for the straight tools and 240 trials for the\ncrossed tools. Participants actively crossed or uncrossed the tools\nbetween every four trials in study 1 and changed passively (tool\ncrossing made by the experimenter rather than the participants by\ncrossing the tools only visually on the screen) at the end of 240\ntrials in study 2. Each of the 16 conditions (4 visual distractors64\nvibrotactile target locations) was presented 15 times (crossed or\nuncrossed), in a pseudo-randomized order determined by the\ncomputer.\nParticipants sat in front of a table and the table height was\nadjusted for each participant. They held the two haptic interfaces,\nwith their thumbs next to the lower vibrotactile stimulator and\ntheir index fingers next to the upper stimulator. They were\ninstructed to make speeded elevation discriminations to the\nvibrotactile stimuli. They were told that visual distractors would\nbe presented simultaneously with the vibrotactile stimuli but that\nthey should ignore them as much as possible while they were\nresponding. They were instructed not to close their eyes and fixate\non the central fixation cross until the end of the trial. The\nparticipant’s right foot rested on the two pedals. They were\ninstructed to hold both pedals pressed, which was the default\ncondition and to release the toes in response to tactile stimuli\napplied to the index finger or to depress the heel if the stimuli were\napplied to the thumb. This toe/heel response mapped to index/\nthumb to make it compatible with upper/lower elevation of the\nvibrotactile and visual stimuli. In each trial, the visual distractor\nstimulus was presented 100 ms before presentation of the\nvibrotactile stimulus (SOA 100 ms) as previous work showed that\nthis maximizes the CCE [27].\nParticipants moved two virtual golf clubs via the handle of a\nbimanual haptic simulator. In the first study, for some trials they\nheld two tools in a straight position and for some trials they\nactively crossed the tools. Tool posture was changed actively after\nevery four CCE trials. After every four CCE trials, to indicate that\nthe CCE task was finished, two light balls were shown. This\ninstructed the participant to cross or to uncross the tools. These\ntwo light balls also functioned to indicate where to position the\ntools in order to keep the distance between the tools constant for\nthe crossed and uncrossed posture. In the second study the tools\nwere not changed actively after every four CCE trials but only\nchanged at the end of 240 trials. Virtual robotic tools were crossed\nby the experimenter rather than the participants as in the\nMaravita et al.’s experiment with physical tools [8] and only visual\nfeedback of the crossed golf clubs was presented.\nAnalysis\nFor CCE analysis, trials with an incorrect response were\ndiscarded from the RT analysis but they were analyzed in the\npercentage error analysis. Trials with RTs larger than 1.500 and\nless than 200 milliseconds were removed. Next, RT outliers were\nremoved using a criterion of 3 standard deviations above or below\nthe subject’s mean RT. These led to a rejection of a mean6 SE of\n4.960.45% of all trials in experiment 1, and 6.760.65% of all\ntrials in experiment 2. Data from all trials that resulted in correct\nresponses were analyzed by using a repeated-measures three-way\nanalysis of variance (ANOVAs) on the mean values of RTs. The\nthree factors in the ANOVA design were: Congruency (congru-\nent/incongruent), Side (same/different) and Tool posture (un-\ncrossed/crossed). Paired t-tests were used for post-hoc compari-\nsons on the CCEs. In addition, following previous studies on the\nCCE, the inverse efficiency (IE) was calculated by dividing the\nreaction time by the accuracy (proportion correct) for each\ncondition, thereby controlling effectively for any speed-accuracy\ntrade-off in the reaction time data [22].\nResults\nStudy 1: Active tool use\nCongruency effects derived from RT data from the first\nexperiment are represented in Figure 2 and Table 1. The\nANOVA performed on RTs from Experiment 1 revealed a main\neffect of congruency (F (1, 9) = 32.90, p,0.001) and a significant\ninteraction between congruency and side (F (1, 9) = 9.07), p,.05)\nconfirming that CCEs were significantly larger in the same side\nconditions compared to the different side conditions (t(9) = 3.02;\np,.05). Crucially, we also found a three-way interaction between\nCongruency, Side and Tool Posture (F (1, 9) = 37.88, p,0.001).\nTo determine the driving factor of this three-way interaction, we\nperformed post-hoc comparisons between the same side CCE\nversus the different side CCE for the RT measures for each tool\nposture. This analysis revealed that the CCE differed significantly\nbetween same side and different side for the uncrossed (t(9) = 5.80;\np,0.001) and crossed condition (t(9) = 3.30; p,0.01) (see Figure 2\nand Table 1). As inspection of Figure 2 reveals, the direction of\nthese effects differed for uncrossed versus crossed conditions. In\nline with data obtained with physical tools, for the uncrossed\ncondition the same side CCEs were larger than the different side\nCCEs, whereas for the crossed condition the different side CCEs\nwere larger than the same side CCEs. These data indicate that\nactive tool use results in a remapping of peripersonal space,\ndepending on the position of the tools.\nThe ANOVA on the error rates revealed a main effect of\ncongruency (F(1, 9) = 21.75, p,0.01) and a tendency for a three-\nway interaction between Congruency, Side and Tool Posture (F (1,\n9) = 3.97, p = 0.077). The ANOVA on the IE data revealed a main\neffect of congruency (F(1, 9) = 40.21, p,0.001 and a significant\ninteraction between congruency and side (F (1, 9) = 5.52), p,.05)\nand a three-way interaction between Congruency, Side and Tool\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 4 December 2012 | Volume 7 | Issue 12 | e49473\nPosture (F (1, 9) = 14.99, p,0.01) (see Table 1). Thus the analysis\nof the IE confirms the main findings from the analysis of Reaction\nTimes and Error rates, indicating that the CCE side effect is\nmodulated by the posture of the virtual tools (i.e. crossed or\nuncrossed). More importantly, this analysis provides further\nsupport that this effect cannot be accounted for by a speed-\naccuracy trade-off between the different experimental conditions.\nStudy 2: Passive tool use\nReaction time data from the second experiment are represented\nin Figure 3 and Table 2. The ANOVA performed on RTs from\nExperiment 2 revealed a main effect of tool posture (F (1, 8) = 5.37,\np,0.05), a main effect of congruency (F (1, 8) = 142.00, p,0.001)\nand a significant interaction between side and congruency (F (1,\n8) = 17.63), p,0.005) reflected in a stronger same side CCE than a\ndifferent side CCE (t(8) = 4.6; p,0.005). Crucially, we found a\nFigure 2. Crossmodal congruency effect (CCE) with standard error in Experiment 1.The CCE was calculated as incongruent reaction times\nminus congruent reaction times. White bars represent the condition in which visual stimuli were presented to the same visual hemifield with tactile\nstimuli, black bars represent trials in which the visual stimuli were presented to the different hemifield. The bars on the left side are for the uncrossed\nposture and bars on the right side are for the crossed posture.\ndoi:10.1371/journal.pone.0049473.g002\nTable 1.Mean reaction times (RT) in milliseconds, percentage of errors (%) and inverse efficiency (IE) for Experiment 1.\nExperiment 1\nSame Side\nTool Posture Congruent Incongruent Mean CCE\nRT 682.4(26.2) 730.8(21.7) 48.3(10.9)\nCrossed % 1.43(0.63) 1.59(0.49) 0.16(0.71)\nIE 691.6(26.3) 741.3(19.2) 49.7(11.8)\nRT 678.8(25.9) 764.1(32.5) 85.4(15.0)\nUncrossed % 0.94(0.49) 4.57(0.97) 3.63(1.12)\nIE 686.1(27.3) 804.2(41.1) 118.1(20.7)\nDifferent Side\nTool Posture Congruent Incongruent Mean CCE\nRT 673.5(26.3) 749.2(27.1) 75.6(12.5)\nCrossed % 1.32(0.58) 4.10(1.00) 2.78(0.91)\nIE 682.3(25.7) 786.4(34.0) 104.2(17.3)\nRT 718.7(27.1) 744.7(27.6) 26.0(9.8)\nUncrossed % 1.41(0.57) 2.91(0.70) 1.50(0.67)\nIE 728.7(28.0) 763.8(28.2) 35.1(12.5)\nThe left column represents data for congruent conditions, the middle column for incongruent conditions. The right column represents the crossmodalcongruency\neffect (CCE; i.e. difference between incongruent and congruent conditions). The first rows represent data for the crossed tool posture and the second rows represent\ndata for the uncrossed posture. The upper panel represents data for visual stimuli at the same side as the tactile vibrations, the lower panel represents data for visual\nstimuli at the different side compared to the tactile vibrations. Values in parentheses are standard errors.\ndoi:10.1371/journal.pone.0049473.t001\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 5 December 2012 | Volume 7 | Issue 12 | e49473\nthree-way interaction between Congruency, Side and Tool\nPosture (F (1, 8) = 12.18, p,0.01). We also performed post-hoc\ncomparisons between the same side CCE versus the different side\nCCE for the RT. This analysis revealed that the CCE difference\nbetween same side and different side was significant for the\nuncrossed (t(8) = 4.58; p,0.01) but not significant for the crossed\ncase (t(8) = 0.11; p = 0.91 NS)(see Figure 3 and Table 2). As can be\nseen in Figure 3, same side CCEs were larger than the different\nside CCEs only for the uncrossed condition but not for the crossed\nconditions. This finding indicates that passive tool use did not\nresult in a complete remapping of peripersonal space, according to\nthe position of the tools.\nThe ANOVA performed on error rates revealed a main effect of\ncongruency (F (1, 8) = 21.21, p,0.01), a main effect of tool posture\n(F (1, 8) = 5.42, p,0.05) and an interaction between tool posture\nand congruency (F (1, 8) = 13.36, p,0.01). We also found a three-\nway interaction between Congruency, Side and Tool Posture for\nthe percentage of errors (F (1, 8) = 5.78, p,0.05). We performed\npost-hoc comparisons between same side CCE versus different side\nCCE for the error rates. This analysis revealed that the difference\nbetween same side and different side was significant for the\nuncrossed posture (p,0.01) but not significant for the crossed\nposture. The ANOVA performed on the IE data revealed a main\neffect of tool posture (F (1, 8) = 17.70, p,0.01), a main effect of\ncongruency, (F (1, 8) = 78.71, p ,0.001) and an interaction\nbetween side and congruency (F (1, 8) = 30.28, p,0.01). We also\nfound a three-way interaction between Congruency, Side and\nTool Posture (F (1, 8) = 10.87, p,0.05) (see Table 2). Thus, the\nANOVA on the IE data confirms the main findings of the analysis\nof the RT and error data and provides further support that no\nspeed-accuracy trade-off underlies the present results.\nIn experiment 1, we found a significant three-way interaction\nbetween Congruency, Side, and Tool posture, as expected. In\nexperiment 2, this interaction was also significant. In order to\ndirectly investigate the difference between active and passive tool\ncrossing, we performed a between-experiments comparison, using\na 4-way ANOVA with Experiment as a between-participants\nvariable. The 4-way ANOVA performed on RTs revealed a main\neffect of tool posture (F (1, 17) = 5.01, p,0.05), a main effect of\ncongruency (F (1, 17) = 97.38, p,0.001), a significant interaction\nbetween side and congruency (F (1, 17) = 27.67), p,0.001) and a\nthree-way interaction between Congruency, Side and Tool\nPosture (F (1, 17) = 42.52, p,0.001). The 4-way interaction was\nnot significant (F (1, 17) = 0.57, p = 0.46 NS) suggesting a\nremapping according to tool posture for both active and passive\ntool crossing.\nIn addition, to explore the pattern of interference reversal\nstatistically, we analyzed the interference effect by pairs of blocks\n[8]. We could not find any significant correlation of the same side\ndistractors or different side distractors with block number for the\nstraight or crossed tools. Only a tendency for a negative\ncorrelation of same side distractors with the block number for\nthe active straight tool was observed (r =20. 86 p = 0.059). The\nresult of this analysis suggests that there is no learning effect of\nremapping. This analysis provides further evidence that the\nremapping of peripersonal space using virtual tools happens\ninstantaneously.\nDiscussion\nIn the present study we investigated the integration of visuo-\ntactile cues in the case of a multimodal robotic interface\ncontrolling a virtual-robotic tool. At least three findings support\nthe notion that the use of such ‘virtual-robotic’ tools facilitates the\nintegration of multisensory information in peripersonal space.\nFirst, our results show that there was an interaction of vision and\ntouch as reflected in the crossmodal congruency effect (CCE) for\nvirtual robotic tools. Second, it was found that actively crossing the\ntool resulted in a remapping of peripersonal space, as reflected in a\nstronger CCE when visual stimuli appeared at a different side than\nthe tactile vibration, at the tip of the tool that was held in the\nstimulated hand. Third, it was found that this remapping of\nperipersonal space did not depend on active tool use, as passive\ncrossing of the tools resulted in a change in the CCE side effect as\nwell. These results therefore extend previous findings on visuo-\ntactile integration in tool-use [20,22,26] to the domain of virtual\ntools, haptic interfaces and surgical robotics.\nFirst, the results of both experiments showed an interference\neffect of visual distractors presented on the virtual-robotic tools on\nFigure 3. Crossmodal congruency effect (CCE) with standard error in Experiment 2.The CCE was calculated as incongruent reaction times\nminus congruent reaction times. White bars represent the condition in which visual stimuli were presented to the same visual hemifield with tactile\nstimuli, black bars represent trials in which the visual stimuli were presented to the different hemifield. The bars on the left side are for the uncrossed\nposture and on bars on the right side are for the crossed posture.\ndoi:10.1371/journal.pone.0049473.g003\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 6 December 2012 | Volume 7 | Issue 12 | e49473\ntactile discrimination judgments. The interfering effect of the\nvisual stimuli on the virtual-robotic tools was reflected in slower\nreaction times and increased error rates when participants\nresponded to incongruent compared to congruent vibrotactile\nstimuli, which is known as the crossmodal congruency effect\n(CCE). The results of the first experiment showed that visual\ndistractors presented at the end of the left tool interfered more\nstrongly with judging tactile stimuli applied to the left hand\ncompared to the right hand (and vice versa for visual distractors\npresented at the right tool). Therefore, these findings suggest that\nboth the passive and the active use of a virtual-robotic tool can\nalter multisensory integration in peripersonal space, reflecting a\nremapping of peripersonal space similar to the effects found for\nactive physical and pointing tool use [6,8,9,18]. Thus, when the\nparticipants used the virtual-robotic tools actively, our data suggest\nthat they functioned as an extension of their arms. This extension\nproduced a stronger association between the vibrotactile stimuli on\nthe hands and the visual stimulation at the end of the tool. The\neffect of visual distractors on tactile discrimination responses has\noften been related to findings in monkeys, indicating that the\nresponse properties of neurons in parietal areas reflect the\nfunctional aspects of tool use (i.e. incorporation of the tool in the\nbody; cf. [10,19]; but for critical discussion, see: [22]). A similar\nneural mechanism has been proposed to underlie the effects of\nmultisensory integration after tool use, as seen in healthy humans\nas well as brain damaged patients [21]. For instance, Farne el al.\n[28] studied the extension of peripersonal space with physical tool-\nuse in visuo-tactile neglect patients and found that visuo-tactile\nextinction can be modulated by tool use (i.e. stronger left tactile\nextinction with right lights when a tool is wielded on the right side).\nSecond, in the first experiment it was found that when the\nvirtual-robotic tools were actively crossed, visual distractors from\nthe opposite visual field interfered more strongly with tactile\nstimuli applied to the hand that was holding the tool. In the\ncrossed condition the tip of the tool held by the left hand was in the\nright visual field and visual distractors presented at the tip of this\ntool interfered more strongly with tactile stimuli applied to the left\ncompared to the right hand. The opposite was true for the tool\nheld by the right hand. Thus, visual stimuli were primarily\nassociated with the hand that was holding the tool rather than the\nspatial side at which the stimuli appeared. This finding suggests\nthat actively crossing the tool resulted in a remapping of\nperipersonal space, as reflected in a stronger CCE when visual\nstimuli appeared at a different side than the tactile vibration, at the\ntip of the tool that was held in the stimulated hand. The finding\nthat active crossing of the virtual-robotic tool resulted in a\nremapping of peripersonal space extends previous studies on real-\nworld tools [8,19,20]. These studies showed that crossing the tools\nactively remapped the visuo-tactile representation of peripersonal\nspace [8].\nThe results of the second experiment, obtained in a different\nparticipant sample, showed that when participants held uncrossed\ntools, the CCE was larger when the visuotactile stimuli were\npresented at the same side compared to the different side. In\ncontrast to the results obtained in the first experiment, when the\ntools were passively crossed, the CCEs for visuotactile stimuli\npresented at the same and different side were comparable in size.\nThus, the passive crossing of the virtual-robotic tools did not\ncompletely remap the peripersonal space representation, as it did\nin the first experiment. It did affect the representation of\nperipersonal space, as reflected in the fact that the difference\nbetween the same and different side CCE differed between crossed\nand uncrossed postures (i.e. a significant 3-way interaction was\nfound). This result is different to that in previous studies (see [8]),\nin which passively moving the real-world tool always resulted in a\nstronger CCE for visuotactile stimuli appearing at the same\nTable 2.Mean reaction times (RT) in milliseconds, percentage of errors (%) and inverse efficiency (IE) for Experiment 2.\nExperiment 2\nSame Side\nTool Posture Congruent Incongruent Mean CCE\nRT 601.0(19.7) 656.7(22.6) 55.7(8.8)\nCrossed % 1.33 (0.55) 3.79 (1.02) 2.46 (1.22)\nIE 609.2(20.6) 681.8(21.7) 72.7(4.5)\nRT 591.7(18.0) 683.5(24.8) 91.8(11.1)\nUncrossed % 2.63 (0.92) 9.62 (0.99) 6.99 (1.13)\nIE 607.7(19.2) 754.4(26.4) 146.7(14.6)\nDifferent Side\nTool Posture Congruent Incongruent Mean CCE\nRT 595.5(22.6) 652.2(23.7) 56.7(5.9)\nCrossed % 2.67 (0.72) 6.24(1.79) 3.57(2.23)\nIE 624.2(28.4) 683.6(25.7) 59.4(22.1)\nRT 643.9(24.1) 668.0(21.6) 24.1(6.3)\nUncrossed % 4.16(1.12) 3.55(1.28) 20.61(1.22)\nIE 671.1(24.1) 692.8(22.4) 21.7(10.8)\nThe left column represents data for congruent conditions, the middle column for incongruent conditions. The right column represents the crossmodalcongruency\neffect (CCE; i.e. difference between incongruent and congruent conditions). The first rows represent data for the crossed tool posture, the second rows represent data\nfor the uncrossed posture. The upper panel represents data for visual stimuli at the same side as the tactile vibrations, the lower panel represents data for visual stimuli\nat the different side compared to the tactile vibrations. Values in parentheses are standard errors.\ndoi:10.1371/journal.pone.0049473.t002\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 7 December 2012 | Volume 7 | Issue 12 | e49473\ncompared to the different side, irrespective of whether the tools\nwere crossed or not. Our findings suggest that virtual-robotic tools\nmay alter multisensory integration even when the tool is not\nactively used and thus affect multisensory integration differently\nthan the physical tools. This could also be due to different physical\nand information processing mechanism of the virtual robotic tools\nsuch as the immersive nature and ‘tele-arm’ like behavior of these\ntools. In our experiments the real hands were not visible, as the\nparticipants were wearing head mounted displays. The absence of\nvisual information about the hands may have facilitated the\nintegration of the virtual tools in the body representation, thereby\nresulting in a remapping of the visuo-tactile representation of\nperipersonal space even in the case of passive tool use.\nThe results of this study are novel and extend previous studies\nfor several reasons. Up to now, studies on the representation of\nperipersonal space in humans have used mainly tools that\nphysically link peripersonal space and extrapersonal space - such\nas golf clubs, rakes, long sticks to press a button or to reach a\ndistant object [6,8]. It has been found, for instance, that\nperipersonal space was extended by active tool use [8]. However,\nthere is an alternative interpretation with respect to the extension\nof peripersonal space with the tool. Holmes et al. have shown that\nmultisensory spatial interactions were enhanced at the tips of the\ntools rather than in the middle of tools, suggesting that\nperipersonal space is not extended but projected towards the part\nof the tool that is most relevant for the task. According to this\ninterpretation, tools act as spatial attentional cues rather than\nentities which cause an extension of peripersonal space [29]. In\nfact, virtual robotic tools can help to shed light on the question of\nwhether peripersonal space is projected or extended, by enabling\nnovel experimental paradigms with virtual robotic tools that would\nbe difficult to perform with physical tools. For instance, in a recent\nstudy we inserted a movable joint in the middle of the tool, thereby\nmaking the middle part of the tool more relevant to the action at\nhand (e.g. as if the middle part of the tool represents one’s wrist or\nelbow; cf. Sengu¨l et al. in prep.). It was found that peripersonal\nspace was selectively projected towards the part of the tool that\nwas relevant to the task at hand (i.e. controlling the tool by moving\nthe wrist or the elbow), thereby providing further support for the\nidea that peripersonal space is indeed projected to distant locations\nthat are task-relevant and attended.\nThe results of the present study also extend previous studies with\npointing tools [9,10]. For instance, Iriki et al (2001; cf. [10])\nconducted a study in which monkeys trained to control a tool via a\ncomputer screen with the arms out of view. It was found that the\nvisual receptive field size of visuo-tactile neurons in parietal areas\nwas enlarged to include the tool viewed through the video\nmonitor. Interestingly, when only a cursor was presented instead\nof a tool, much fewer neurons with such properties were found. It\ncould be that for monkeys it is more difficult to integrate abstract\nvisual information into their body representation. In contrast, for\nhumans the interaction with virtual objects is omnipresent in our\neveryday lives (e.g. the use of computers, video games, PC tablets,\netc.) and as a consequence virtual tools may be more easily\nintegrated in the representation of our body. In support of this\nview, Bassolino et al. showed that the space where a pointing tool\n(i.e. a computer mouse) was actually held (i.e. close to hand) was\nextended to the space where it operates (i.e. the computer screen)\neven though these spaces were not physically connected [9].\nFurthermore, the findings of this study are in line with the\nfindings of Moizumi et al. [30], reporting a remapping of touch in\nVR with humans holding the arms in a crossed or uncrossed\nposition. In this study a temporal order judgment task was used\nand it was found that when the arms were uncrossed participants’\nability to report tactile vibrations applied to the hand was\nmodulated by whether the virtual tools were crossed or uncrossed.\nIn contrast, when the arms were crossed, the direction of the force\nfeedback primarily determined the perceived order of tactile\njudgments, indicating the importance of haptic force feedback for\ndisambiguating the referral of tactile sensations. The present study\nextends these findings [30], by showing that virtual tool use\nchanges visuo-tactile instead of only tactile interactions in virtual\nspace. In addition, our findings indicate that both active crossing\nand passive crossing of the virtual tool results in a remapping of the\nperipersonal space. Finally, we would like to point out that we\ninvestigated peripersonal space representations, using a new class\nof virtual tools that are increasingly used in surgical robotics. In\nsurgical robotics, an important aim is to increase the telepresence\nof the surgeon in the remote site. Studying how peripersonal space\nrepresentation as measured through the CCE changes with virtual\nrobotic tools could be an objective assessment method for the\nevaluation of the telepresence by analysing whether the remote site\nwas represented as within peripersonal space or not.\nWe note that the present work on the relation between the CCE\nand peripersonal space representations, is also consistent with the\nfindings by Rognini et al., who showed that visuo-tactile CCEs can\nbe obtained in a robotically mediated environment using virtual\nhands [26]. The present study, however, shows that visuo-tactile\nintegration on a robotic platform does not only occur for virtual\nhands but also for virtual-robotic tools. Moreover, Rognini et al.\nmeasured the integration of visuo-tactile cues online: during the\nholding and moving of virtual objects with virtual hands. In the\npresent study, we measured visuo-tactile CCEs after using a\ntelemanipulation tool. Hence we propose that measuring such\nCCE post-effects can also be used as an objective assessment of\nhow we learn to use robotic tools.\nTogether, the results of these studies show that cognitive\nneuroscience measures can be used to investigate the integration of\nvisual and tactile cues in robotic technologies. This suggests that\nCCE measurements may be used as an objective assessment of\nhuman factors. Up to now, human factors in robotics, especially in\nsurgical robotics was quantified by means of questionnaires or\nperformance based measurements such as task completion time or\ntask accuracy [31–33]. These studies focused on a specific task\nsuch as needle insertion during surgical procedures, neglecting\nmore basic and repeated behavioral changes of multisensory\nintegration driven by tool-use [34–36]. Here we propose a new\nmethodology to study ‘human factors’ in a surgical interface with a\nmore objective assessment technique by quantifying visuo-tactile\nintegration measured by the CCE. This extends and changes the\nstandard analysis of human factors in the field of surgical robotics\nby injecting insights and methods from cognitive neuroscience into\nthis emerging field between neuroscience, psychology, and\nengineering. Understanding how the brain codes peripersonal\nspace and which factors contribute to the integration of tools into\nthe brain’s body representation may turn out to be of pivotal\nimportance for the design of future robots that can be more easily\nused and controlled for the benefit of patients nearby and at\ndistance. For instance, an important issue in the field of medical\nand surgical robotics is the feeling that the surgeon is holding, and\noperating with a surgical tool – as if he were controlling a real tool\nto operate his patient. The present study provides a first step to\nstudy more objectively the relation between real tools and virtual-\nrobotic tools, by showing that similar neurocognitive mechanisms\nare involved in using real and virtual-robotic tools.\nIn summary, the present paper presented two experiments on\nmultisensory integration through the use of a virtual-robotic tool.\nIt was found that virtual-robotic tool use changed the integration\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 8 December 2012 | Volume 7 | Issue 12 | e49473\nof visuo-tactile information in peripersonal space, as reflected in a\ncross-modal congruency effect (CCE). This result illustrates that, in\norder to change the representation of peripersonal space, it is not\nnecessary to have a physical connection between the space where\nthe tool is held and the space where the tool operates. The results\nare consistent with previous studies on the cross-modal congruency\neffect (CCE). This study establishes that telemanipulators consist-\ning of haptic devices and virtual reality can be used in cognitive\nneuroscience investigations, thereby opening up exciting new\npossibilities for neuroscience experimentation and improved\nincorporation of human factors into the future design of minimally\ninvasive surgical robots.\nAcknowledgments\nWe thank the anonymous reviewers and N. P. Holmes for their helpful and\nconstructive comments.\nAuthor Contributions\nConceived and designed the experiments: AS GR JA OB. Performed the\nexperiments: AS. Analyzed the data: AS. Contributed reagents/materials/\nanalysis tools: AS GR. Wrote the paper: AS MvE GR JA HB OB.\nReferences\n1. Iriki ATM (in press) Triadic (ecological, neural, cognitive) niche construction: a\nscenario of human brain evolution extrapolating tool-use and language from the\ncontrol of reaching actions. Phil Trans R Soc B.\n2. Ishibashi H, Hihara S, Iriki A (2000) Acquisition and development of monkey\ntool-use: behavioral and kinematic analyses. Canadian Journal of Physiology and\nPharmacology 78: 958–966.\n3. Goldenberg G, Spatt J (2009) The neural basis of tool use. Brain 132: 1645–\n1655.\n4. Peeters R, Simone L, Nelissen K, Fabbri-Destro M, Vanduffel W, et al. (2009)\nThe Representation of Tool Use in Humans and Monkeys: Common and\nUniquely Human Features. J Neurosci 29: 11523–11539.\n5. Holmes NP, Calvert GA, Spence C (2006) Beyond the body: visual, prosthetic\nand technological contributions to bodily perception and awareness; In:\nKnoblich GT, I.M.; Grosjean, M.; Shiffrar, M., editor. New York: Oxford\nUniv. Press.\n6. Holmes NP, Sanabria D, Calvert GA, Spence C (2007) Tool-use: Capturing\nmultisensory spatial attention or extending multisensory peripersonal space?\nCortex 43: 469–489.\n7. Maravita A, Iriki A (2004) Tools for the body (schema). Trends Cogn Sci 8: 79–\n86.\n8. Maravita A, Spence C, Kennett S, Driver J (2002) Tool-use changes multimodal\nspatial interactions between vision and touch in normal humans. Cognition 83:\nB25–B34.\n9. Bassolino M, Serino A, Ubaldi S, Ladavas E (2010) Everyday use of the\ncomputer mouse extends peripersonal space representation. Neuropsychologia\n48: 803–811.\n10. Iriki A, Tanaka M, Obayashi S, Iwamura Y (2001) Self-images in the video\nmonitor coded by monkey intraparietal neurons. Neurosci Res 40: 163–173.\n11. Rosen J, Hannaford B, Satava R (2011) Surgical Robotics: Systems Applications\nand Visions. London: Springer\n12. Suematsu Y, del Nido PJ (2004) Robotic pediatric cardiac surgery: present and\nfuture perspectives. Am J Surg 188: 98S–103S.\n13. Stone R. Haptic feedback: A potted history, from telepresence to virtual reality.\n2000; Glasgow, UK, 1–7. Springer-Verlag Lecture Notes in Computer Science.\n14. Kenngott HG, Fischer L, Nickel F, Rom J, Rassweiler J, et al. (2012) Status of\nrobotic assistance-a less traumatic and more accurate minimally invasive\nsurgery? Langenbeck Arch Surg 397: 333–341.\n15. Rassweiler J, Safi KC, Subotic S, Teber D, Frede T (2005) Robotics and\ntelesurgery–an update on their position in laparoscopic radical prostatectomy.\nMinim Invasive Ther Allied Technol 14: 109–122.\n16. Tavakoli M, Aziminejad A, Patel RV, Moallem M (2007) High-fidelity bilateral\nteleoperation systems and the effect of multimodal haptics. Ieee Transactions on\nSystems Man and Cybernetics Part B-Cybernetics 37: 1512–1528.\n17. Octavia JR, Raymaekers C, Coninx K (2011) Adaptation in virtual\nenvironments: conceptual framework and user models. Multimed Tools Appl\n54: 121–142.\n18. Ladavas E, Serino A (2008) Action-dependent plasticity in peripersonal space\nrepresentations. Cogn Neuropsychol 25: 1099–1113.\n19. Iriki A, Tanaka M, Iwamura Y (1996) Coding of modified body schema during\ntool use by macaque postcentral neurones. Neuroreport 7: 2325–2330.\n20. Maravita A (2006) From body in the brain, to body in space: sensory and\nintentional aspects of body representation; Grosjean IGKMSM, editor. Oxford:\nOxford University Press.\n21. Maravita A, Iriki A (2004) Tools for the body (schema). Trends in Cognitive\nSciences 8: 79–86.\n22. Holmes NP (2012) Does tool use extend peripersonal space? A review and re-\nanalysis. Experimental Brain Research 218: 273–282.\n23. Pavani F, Spence C, Driver J (2000) Visual capture of touch: out-of-the-body\nexperiences with rubber gloves. Psychol Sci 11: 353–359.\n24. Zopf R, Savage G, Williams MA (2010) Crossmodal congruency measures of\nlateral distance effects on the rubber hand illusion. Neuropsychologia 48: 713–\n725.\n25. Aspell JE, Lenggenhager B, Blanke O (2009) Keeping in touch with one’s self:\nmultisensory mechanisms of self-consciousness. PLoS One 4: e6488.\n26. Rognini G, Sengu¨l A, Aspell JE, Bleuler H, Blanke O (2012) Visuo-tactile\nintegration and body ownership during self-generated action. Eur J Neurosci (in\nrevision).\n27. Shore DI, Barnes ME, Spence C (2006) Temporal aspects of the visuotactile\ncongruency effect. Neurosci Lett 392: 96–100.\n28. Farne A, Iriki A, Ladavas E (2005) Shaping multisensory action-space with tools:\nevidence from patients with cross-modal extinction. Neuropsychologia 43: 238–\n248.\n29. Holmes NP, Calvert GA, Spence C (2004) Extending or projecting peripersonal\nspace with tools? Multisensory interactions highlight only the distal and proximal\nends of tools. Neuroscience Letters 372: 62–67.\n30. Moizumi S, Yamamoto S, Kitazawa S (2007) Referral of tactile stimuli to action\npoints in virtual reality with reaction force. Neuroscience Research 59: 60–67.\n31. Lee EC, Rafiq A, Merrell R, Ackerman R, Dennerlein JT (2005) Ergonomics\nand human factors in endoscopic surgery: a comparison of manual vs telerobotic\nsimulation systems. Surg Endosc 19: 1064–1070.\n32. Prasad SM, Maniar HS, Soper NJ, Damiano RJ, Klingensmith ME (2002) The\neffect of robotic assistance on learning curves for basic laparoscopic skills.\nAm J Surg 183: 702–707.\n33. Santos-Carreras L, Hagen M, Gassert R, Bleuler H (2011) Survey on Surgical\nInstrument Handle Design: Ergonomics and Acceptance. Surg Innov.\n34. Narazaki K, Oleynikov D, Stergiou N (2006) Robotic surgery training and\nperformance: identifying objective variables for quantifying the extent of\nproficiency. Surg Endosc 20: 96–103.\n35. Okamura AM (2009) Haptic feedback in robot-assisted minimally invasive\nsurgery. Curr Opin Urol 19: 102–107.\n36. Santos-Carreras LB, Sengu¨l A, Gassert R, Bleuler H (2010) Influence of force\nand torque feedback on operator performance in a VR-based suturing task.\nApplied Bionics and Biomechanics 217–230.\nExtending the Body to Virtual Tools\nPLOS ONE | www.plosone.org 9 December 2012 | Volume 7 | Issue 12 | e49473"
}