{
  "title": "Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling",
  "url": "https://openalex.org/W2951816288",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5059500729",
      "name": "Jaejin Cho",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5064113992",
      "name": "Murali Karthick Baskar",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100760516",
      "name": "Ruizhi Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014455067",
      "name": "Matthew Wiesner",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047596805",
      "name": "Sri Harish Mallidi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112575316",
      "name": "Nelson Yalta",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065253688",
      "name": "Martin Karafiát",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001291873",
      "name": "Shinji Watanabe",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087554069",
      "name": "Takaaki Hori",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2788381663",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W2773781902",
    "https://openalex.org/W2094147890",
    "https://openalex.org/W2633221078",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2964309797",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2795935804",
    "https://openalex.org/W2963403664",
    "https://openalex.org/W2096140469",
    "https://openalex.org/W2743818757"
  ],
  "abstract": "Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multi-lingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",
  "full_text": "MULTILINGUAL SEQUENCE-TO-SEQUENCE SPEECH RECOGNITION:\nARCHITECTURE, TRANSFER LEARNING, AND LANGUAGE MODELING\nJaejin Cho1,‡ , Murali Karthick Baskar2,‡, Ruizhi Li1,‡, Matthew Wiesner1,\nSri Harish Mallidi3, Nelson Yalta4, Martin Karaﬁ´at2, Shinji Watanabe1, Takaaki Hori5\n1Johns Hopkins University, 2Brno University of Technology,3Amazon, 4Waseda University,\n5Mitsubishi Electric Research Laboratories (MERL)\n{ruizhili,jcho52,shinjiw}@jhu.edu,{baskar,karafiat}@fit.vutbr.cz,thori@merl.com\nABSTRACT\nSequence-to-sequence (seq2seq) approach for low-resource\nASR is a relatively new direction in speech research. The approach\nbeneﬁts by performing model training without using lexicon and\nalignments. However, this poses a new problem of requiring more\ndata compared to conventional DNN-HMM systems. In this work,\nwe attempt to use data from 10 BABEL languages to build a multi-\nlingual seq2seq model as a prior model, and then port them towards\n4 other BABEL languages using transfer learning approach. We also\nexplore different architectures for improving the prior multilingual\nseq2seq model. The paper also discusses the effect of integrating a\nrecurrent neural network language model (RNNLM) with a seq2seq\nmodel during decoding. Experimental results show that the trans-\nfer learning approach from the multilingual model shows substan-\ntial gains over monolingual models across all 4 BABEL languages.\nIncorporating an RNNLM also brings signiﬁcant improvements in\nterms of %WER, and achieves recognition performance comparable\nto the models trained with twice more training data.\nIndex Terms: Automatic speech recognition (ASR), sequence to\nsequence, multilingual setup, transfer learning, language modeling\n1. INTRODUCTION\nThe sequence-to-sequence (seq2seq) model proposed in [1, 2, 3] is a\nneural architecture for performing sequence classiﬁcation and later\nadopted to perform speech recognition in [4, 5, 6]. The model al-\nlows to integrate the main blocks of ASR such as acoustic model,\nalignment model and language model into a single framework. The\nrecent ASR advancements in connectionist temporal classiﬁcation\n(CTC) [6, 5] and attention [4, 7] based approaches has created larger\ninterest in speech community to use seq2seq models. To leverage\nperformance gains from this model as similar or better to conven-\ntional hybrid RNN/DNN-HMM models requires a huge amount of\ndata [8]. Intuitively, this is due to the wide-range role of the model\nin performing alignment and language modeling along with acoustic\nto character label mapping at each iteration.\nIn this paper, we explore the multilingual training approaches\n[9, 10, 11] used in hybrid DNN/RNN-HMMs to incorporate them\ninto the seq2seq models. In a context of applications of multilingual\napproaches towards seq2seq model, CTC is mainly used instead of\nthe attention models. A multilingual CTC is proposed in [12], which\nuses a universal phoneset, FST decoder and language model. The\n‡ All three authors share equal contribution\nauthors also use linear hidden unit contribution (LHUC) [13] tech-\nnique to rescale the hidden unit outputs for each language as a way\nto adapt to a particular language. Another work [14] on multilingual\nCTC shows the importance of language adaptive vectors as auxil-\niary input to the encoder in multilingual CTC model. The decoder\nused here is a simple argmax decoder. An extensive analysis on\nmultilingual CTC mainly focusing on improving under limited data\ncondition is performed in [15]. Here, the authors use a word level\nFST decoder integrated with CTC during decoding.\nOn a similar front, attention models are explored within a multi-\nlingual setup in [16, 17] based on attention-based seq2seq to build a\nmodel from multiple languages. The data is just combined together\nassuming the target languages are seen during the training. And,\nhence no special transfer learning techniques were used here to ad-\ndress the unseen languages during training. The main motivation\nand contribution behind this work is as follows:\n• To incorporate the existing multilingual approaches in a joint\nCTC-attention [18] (seq2seq) framework, which uses a sim-\nple beam-search decoder as described in sections 2 and 4\n• Investigate the effectiveness of transferring a multilingual\nmodel to a target language under various data sizes. This is\nexplained in section 4.3.\n• Tackle the low-resource data condition with both transfer\nlearning and including a character-based RNNLM trained\nwith multiple languages. Section 4.4 explains this in detail.\n2. SEQUENCE-TO-SEQUENCE MODEL\nIn this work, we use the attention based approach [2] as it pro-\nvides an effective methodology to perform sequence-to-sequence\n(seq2seq) training. Considering the limitations of attention in per-\nforming monotonic alignment [19, 20], we choose to use CTC loss\nfunction to aid the attention mechanism in both training and decod-\ning. The basic network architecture is shown in Fig. 1.\nLet X = (xt|t = 1,...,T ) be a T-length speech feature se-\nquence and C = (cl|l = 1,...,L ) be a L-length grapheme se-\nquence. A multi-objective learning frameworkLmol proposed in [18]\nis used in this work to unify attention loss patt(C|X) and CTC loss\npctc(C|X) with a linear interpolation weight λ, as follows:\nLmod = λ log pctc(C|X) + (1−λ) logp∗\natt(C|X) (1)\nThe uniﬁed model allows to obtain both monotonicity and effective\nsequence level training.\narXiv:1810.03459v1  [cs.CL]  4 Oct 2018\nDeep CNN (VGG net) \nBLSTM \nAttention Decoder RNN-LM CTC \ncl cl-1 \nxt x1 xT \n…… …… \n…… …… \nShared \nEncoder \nJoint \nDecoder \nFig. 1 : Hybrid attention/CTC network with LM extension: the\nshared encoder is trained by both CTC and attention model objec-\ntives simultaneously. The joint decoder predicts an output label se-\nquence by the CTC, attention decoder and RNN-LM.\npatt (C|X) represents the posterior probability of character label\nsequence Cw.r.t input sequenceXbased on the attention approach,\nwhich is decomposed with the probabilistic chain rule, as follows:\np∗\natt (C|X) ≈\nL∏\nl=1\np(cl|c∗\n1,....,c ∗\nl−1, X) , (2)\nwhere c∗\nl denotes the ground truth history. Detailed explanations\nabout the attention mechanism is described later.\nSimilarly, pctc (C|X) represents the posterior probability based\non the CTC approach.\npctc (C|X) ≈\n∑\nZ∈Z(C)\np(Z|X), (3)\nwhere Z = (zt|t= 1,...,T ) is a CTC state sequence composed of\nthe original grapheme set and the additional blank symbol. Z(C) is\na set of all possible sequences given the character sequence C.\nThe following paragraphs explain the encoder, attention de-\ncoder, CTC, and joint decoding used in our approach.\nEncoder\nIn our approach, both CTC and attention use the same encoder func-\ntion, as follows:\nht = Encoder(X), (4)\nwhere ht is an encoder output state at t. As an encoder function\nEncoder(·), we use bidirectional LSTM (BLSTM) or deep CNN\nfollowed by BLSTMs. Convolutional neural networks (CNN) has\nachieved great success in image recognition [21]. Previous studies\napplying CNN in seq2seq speech recognition [22] also showed that\nincorporating a deep CNNs in the encoder could further boost the\nperformance.\nIn this work, we investigate the effect of convolutional layers in\njoint CTC-attention framework for multilingual setting. We use the\ninitial 6 layers of VGG net architecture [21] in table 2. For each\nspeech feature image, one feature map is formed initially. VGG net\nthen extracts 128 feature maps, where each feature map is downsam-\npled to (1/4 ×1/4) images along time-frequency axis via the two\nmaxpooling layers with stride= 2.\nAttention Decoder:\nLocation aware attention mechanism [23] is used in this work. Equa-\ntion (5) denotes the output of location aware attention, wherealt acts\nas an attention weight.\nalt = LocationAttention\n(\n{al−1}T\nt=1 ,ql−1,ht\n)\n. (5)\nHere, ql−1 denotes the decoder hidden state, ht is the encoder out-\nput state as shown in equation (4). The location attention function\nrepresents a convolution function * as in equation (6). It maps the\nattention weight of the previous label al−1 to a multi channel view\nft for better representation.\nft = K ∗al−1, (6)\nelt = gT tanh(Lin(ql−1) +Lin(ht) +LinB(ft)), (7)\nalt = Softmax({elt}T\nt=1) (8)\nEquation (7) provides the unnormalized attention vectors computed\nwith the learnable vector g, linear transformation Lin(·), and afﬁne\ntransformation LinB(·). Equation (8) computes a normalized atten-\ntion weight based on the softmax operation Softmax(·). Finally, the\ncontext vector rl is obtained by the weighted summation of the en-\ncoder output state ht over entire frames with the attention weight as\nfollows:\nrl =\nT∑\nt=1\naltht. (9)\nThe decoder function is an LSTM layer which decodes the next\ncharacter output label cl from their previous label cl−1, hidden state\nof the decoder ql−1 and attention output rl, as follows:\np(cl|c1,....,c l−1, X) =Decoder(rl,ql−1,cl−1) (10)\nThis equation is incrementally applied to form p∗\natt in equation (2).\nConnectionist temporal classiﬁcation (CTC):\nUnlike the attention approach, CTC do not use any speciﬁc decoder.\nInstead it invokes two important components to perform character\nlevel training and decoding. First component, is an RNN based en-\ncoding module p(Z|X). The second component contains a language\nmodel and state transition module. The CTC formalism is a special\ncase [6, 24] of hybrid DNN-HMM framework with an inclusion of\nBayes rule to obtain p(C|X).\nJoint decoding:\nOnce we have both CTC and attention-based seq2seq models\ntrained, both are jointly used for decoding as below:\nlog phyp(cl|c1,....,c l−1,X) =\nαlog pctc(cl|c1,....,c l−1,X)\n+(1 −α) logpatt(cl|c1,....,c l−1,X)\n(11)\nHere log phyp is a ﬁnal score used during beam search.αcontrols the\nweight between attention and CTC models. αand multi-task learn-\ning weight λin equation (1) are set differently in our experiments.\nTable 1: Details of the BABEL data used for performing the multi-\nlingual experiments\nUsage Language\nTrain Eval\n# of characters\n# spkrs. # hours # spkrs. # hours\nTrain\nCantonese 952 126.73 120 17.71 3302\nBengali 720 55.18 121 9.79 66\nPashto 959 70.26 121 9.95 49\nTurkish 963 68.98 121 9.76 66\nVietnamese 954 78.62 120 10.9 131\nHaitian 724 60.11 120 10.63 60\nTamil 724 62.11 121 11.61 49\nKurdish 502 37.69 120 10.21 64\nTokpisin 482 35.32 120 9.88 55\nGeorgian 490 45.35 120 12.30 35\nTarget\nAssamese 720 54.35 120 9.58 66\nTagalog 966 44.0 120 10.60 56\nSwahili 491 40.0 120 10.58 56\nLao 733 58.79 119 10.50 54\n3. DATA DETAILS AND EXPERIMENTAL SETUP\nIn this work, the experiments are conducted using the BABEL\nspeech corpus collected from the IARPA babel program. The cor-\npus is mainly composed of conversational telephone speech (CTS)\nbut some scripted recordings and far ﬁeld recordings are presented\nas well. Table 1 presents the details of the languages used in this\nwork for training and evaluation.\n80 dimensional Mel-ﬁlterbank (fbank) features are then ex-\ntracted from the speech samples using a sliding window of size 25\nms with 10ms stride. KALDI toolkit [25] is used to perform the fea-\nture processing. The fbank features are then fed to a seq2seq model\nwith the following conﬁguration:\nThe Bi-RNN [26] models mentioned above uses a LSTM [27]\ncell followed by a projection layer (BLSTMP). In our experiments\nbelow, we use only a character-level seq2seq model trained by CTC\nand attention decoder. Thus in the following experiments we intend\nto use character error rate (% CER) as a suitable measure to ana-\nlyze the model performance. However, in section 4.4 we integrate\na character-level RNNLM [28] with seq2seq model externally and\nshowcase the performance in terms of word error rate (% WER).\nIn this case the words are obtained by concatenating the characters\nand the space together for scoring with reference words. All exper-\niments are implemented in ESPnet, end-to-end speech processing\ntoolkit [29].\n4. MULTILINGUAL EXPERIMENTS\nMultilingual approaches used in hybrid RNN/DNN-HMM sys-\ntems [11] have been used for for tackling the problem of low-\nresource data condition. Some of these approaches include language\nadaptive training and shared layer retraining [30]. Among them, the\nmost beneﬁted method is the parameter sharing technique [11]. To\nincorporate the former approach into encoder, CTC and attention de-\ncoder model, we performed the following experiments:\n• Stage 0 - Naive training combining all languages\n• Stage 1 - Retraining the decoder (both CTC and attention)\nafter initializing with the multilingual model from stage-0\nTable 2: Experiment details\nModel Conﬁguration\nEncoder Bi-RNN\n# encoder layers 5\n# encoder units 320\n# projection units 320\nDecoder Bi-RNN\n# decoder layers 1\n# decoder units 300\n# projection units 300\nAttention Location-aware\n# feature maps 10\n# window size 100\nTraining Conﬁguration\nMOL 5e−1\nOptimizer AdaDelta\nInitial learning rate 1.0\nAdaDelta ϵ 1e−8\nAdaDelta ϵdecay 1e−2\nBatch size 30\nOptimizer AdaDelta\nDecoding Conﬁguration\nBeam size 20\nctc-weight 3e−1\n(a) Convolutional layers in joint CTC-attention\nCNN Model Conﬁguration -2 components\nComponent 1 2 convolution layers\nConvolution 2D in = 1, out = 64, ﬁlter = 3 × 3\nConvolution 2D in = 64, out = 64, ﬁlter = 3 × 3\nMaxpool 2D patch = 2 ×2, stride = 2×2\nComponent 2 2 convolution layers\nConvolution 2D in = 64, out = 128, ﬁlter = 3 × 3\nConvolution 2D in = 128, out = 128, ﬁlter = 3 × 3\nMaxpool 2D patch = 2 ×2, stride = 2×2\n• Stage 2 - The resulting model obtained from stage-1 is further\nretrained across both encoder and decoder\nTable 4: Comparison of naive approach and training only the last\nlayer performed using the Assamese language\nModel type Retraining % CER % Absolute gain\nMonolingual - 45.6 -\nMulti. (after 4th epoch) Stage 1 61.3 -15.7\nMulti. (after 4th epoch) Stage 2 44.0 1.6\nMulti. (after 15th epoch) Stage 2 41.3 4.3\n4.1. Stage 0 - Naive approach\nIn this approach, the model is ﬁrst trained with 10 multiple languages\nas denoted in table 1 approximating to 600 hours of training data.\ndata from all languages available during training is used to build a\nsingle seq2seq model. The model is trained with a character label\nset composed of characters from all languages including both train\nand target set as mentioned in table 1. The model provides better\ngeneralization across languages. Languages with limited data when\nTable 3: Recognition performance of naive multilingual approach for eval set of 10 BABEL training languages trained with the train set of\nsame languages\n%CER on Eval set Target languages\nfor Bengali Cantonese Georgian Haitian Kurmanji Pashto Tamil Turkish Tokpisin Vietnamese\nMonolingual - BLSTMP 43.4 37.4 35.4 39.7 55.0 37.3 55.3 50.3 32.7 54.3\nMultilingual - BLSTMP 42.9 36.3 38.9 38.5 52.1 39.0 48.5 36.4 31.7 41.0\n+ VGG 39.6 34.3 36.0 34.5 49.9 34.7 45.5 28.7 33.7 37.4\ntrained with other languages allows them to be robust and helps in\nimproving the recognition performance. In spite of being simple,\nthe model has limitations in keeping the target language data unseen\nduring training.\nComparison of VGG-BLSTM and BLSTMP\nTable 3 shows the recognition performance of naive multilingual\napproach using BLSTMP and VGG model against a monolingual\nmodel trained with BLSTMP. The results clearly indicate that hav-\ning a better architecture such as VGG-BLSTM helps in improving\nmultilingual performance. Except Pashto, Georgian and Tokpisin,\nthe multilingual VGG-BLSTM model gave 8.8 % absolute gain in\naverage over monolingual model. In case of multilingual BLSTMP,\nexcept Pashto and Georgian an absolute gain of 5.0 % in average is\nobserved over monolingual model. Even though the VGG-BLSTM\ngave improvements, we were not able to perform stage-1 and stage-2\nretraining with it due to time constraints. Thus, we proceed further\nwith multilingual BLSTMP model for retraining experiments tabu-\nlated below.\n4.2. Stage 1 - Retraining decoder only\nTo alleviate the limitation in the previous approach, the ﬁnal layer of\nthe seq2seq model which is mainly responsible for classiﬁcation is\nretrained to the target language.\nFig. 2: Difference in performance for 5 hours, 10 hours, 20 hours and\nfull set of target language data used to retrain a multilingual model\nfrom stage-1\nIn previous works [11, 30] related to hybrid DNN/RNN models\nand CTC based models [12, 15] the softmax layer is only adapted.\nHowever in our case, the attention decoder and CTC decoder both\nhave to be retrained to the target language. This means the CTC and\nattention layers are only updated for gradients during this stage. We\nfound using SGD optimizer with initial learning rate of 1e−4 works\nbetter for retraining compared to AdaDelta.\nThe learning rate is decayed in this training at a factor of1e−1 if\nthere is a drop in validation accuracy. Table 4 shows the performance\nof simply retraining the last layer using a single target language As-\nsamese.\n4.3. Stage 2 - Finetuning both encoder and decoder\nBased on the observations from stage-1 model in section 4.2, we\nfound that simply retraining the decoder towards a target language\nresulted in degrading %CER the performance from 45.6 to 61.3.\nThis is mainly due to the difference in distribution across encoder\nand decoder. So, to alleviate this difference the encoder and decoder\nis once again retrained or ﬁne-tuned using the model from stage-1.\nThe optimizer used here is SGD as in stage-1, but the initial learning\nrate is kept to 1e−2 and decayed based on validation performance.\nThe resulting model gave an absolute gain of 1.6% when ﬁnetuned\na multilingual model after 4th epoch. Also, ﬁnetuning a model after\n15th epoch gave an absolute gain of 4.3%.\nTable 5: Stage-2 retraining across all languages with full set of target\nlanguage data\n% CER on Target Languages\neval set Assamese Tagalog Swahili Lao\nMonolingual 45.6 43.1 33.1 42.1\nStage-2 retraining 41.3 37.9 29.1 38.7\nTo further investigate the performance of this approach across\ndifferent target data sizes, we split the train set into ∼5 hours, ∼10\nhours, ∼20 hours and ∼full set. Since, in this approach the model is\nonly ﬁnetuned by initializing from stage-1 model, the model archi-\ntecture is ﬁxed for all data sizes. Figure 2 shows the effectiveness of\nﬁnetuning both encoder and decoder. The gains from 5 to 10 hours\nwas more compared to 20 hours to full set.\nTable 5 tabulates the % CER obtained by retraining the stage-1\nmodel with ∼full set of target language data. An absolute gain is\nobserved using stage-2 retraining across all languages compared to\nmonolingual model.\n4.4. Multilingual RNNLM\nIn an ASR system, a language model (LM) takes an important role\nby incorporating external knowledge into the system. Conventional\nASR systems combine an LM with an acoustic model by FST giv-\ning a huge performance gain. This trend is also shown in general\nincluding hybrid ASR systems and neural network-based sequence-\nto-sequence ASR systems.\nThe following experiments show a beneﬁt of using a language\nmodel in decoding with the previous stage-2 transferred models. Al-\nthough the performance gains in %CER are also generally observed\nover all target languages, the improvement in %WER was more dis-\ntinctive. The results shown in the following Fig. 3 are in %WER.\n“whole” in each ﬁgure means we used all the available data for the\ntarget language as full set explained before.\n83.2 79.7 75.8 71.976.4 73.1 69.9 65.3\n50\n60\n70\n80\n90\n5 10 20 whole\nWER\t(%)\ntarget\tlanguage\tdata\t(hrs)\nAssamese\nstage\t2\tretraining RNNLM\tadded\n84.1\n78.8 75.5 71.476.5\n71.1 67.7 64.3\n50\n60\n70\n80\n90\n5 10 20 whole\nWER\t(%)\ntarget\tlanguage\tdata\t(hrs)\nTagalog\nstage\t2\tretraining RNNLM\tadded\n82.5\n75.7\n69.9 66.272.1\n65.1\n60.1 56.2\n50\n60\n70\n80\n90\n5 10 20 whole\nWER\t(%)\ntarget\tlanguage\tdata\t(hrs)\nSwahili\nstage\t2\tretraining RNNLM\tadded\n74.7 73.7\n65.1 62.470.8 70.5\n61.3 57.950\n60\n70\n80\n90\n5 10 20 whole\nWER\t(%)\ntarget\tlanguage\tdata\t(hrs)\nLao\nstage\t2\tretraining RNNLM\tadded\nFig. 3: Recognition performance after integrating RNNLM during\ndecoding in %WER for different amounts of target data\nWe used a character-level RNNLM, which was trained with 2-\nlayer LSTM on character sequences. We use all available paired\ntext in the corresponding target language to train the LM for the\nlanguage. No external text data were used. All language mod-\nels are trained separately from the seq2seq models. When build-\ning dictionary, we combined all the characters over all 15 languages\nmentioned in table 1 to make them work with transferred models.\nRegardless of the amount of data used for transfer learning, the\nRNNLM provides consistent gains across all languages over differ-\nent data sizes.\nTable 6: Recognition performance in %WER using stage-2 retrain-\ning and multilingual RNNLM\nModel type %WER on target languages\nAssamese Tagalog Swahili Lao\nStage-2 retraining 71.9 71.4 66.2 62.4\n+ Multi. RNNLM 65.3 64.3 56.2 57.9\nAs explained already, language models were trained separately\nand used to decode jointly with seq2seq models. The intuition be-\nhind it is to use the separately trained language model as a com-\nplementary component that works with a implicit language model\nwithin a seq2seq decoder. The way of RNNLM assisting decoding\nfollows the equation below:\nlog p(cl|c1:l−1,X) = logphyp(cl|c1:l−1,X)\n+ βlog plm(cl|c1:l−1,X) (12)\nβ is a scaling factor that combines the scores from a joint decod-\ning eq.(11) with RNN-LM, denoted as plm. This approach is called\nshallow fusion.\nOur experiments for target languages show that the gains from\nadding RNNLM are consistent regardless of the amount of data used\nfor transfer learning. In other words, in Figure 3, the gap between\ntwo lines are almost consistent over all languages.\nAlso, we observe the gain we get by adding RNN-LM in de-\ncoding is large. For example, in the case of assamese, the gain by\nRNN-LM in decoding with a model retrained on 5 hours of the tar-\nget language data is almost comparable with the model stage-2 re-\ntrained with 20 hours of target language data. On average, absolute\ngain ∼6% is obtained across all target languages as noted in table 6.\n5. CONCLUSION\nIn this work, we have shown the importance of transfer learning ap-\nproach such as stage-2 multilingual retraining in a seq2seq model\nsetting. Also, careful selection of train and target languages from\nBABEL provide a wide variety in recognition performance (%CER)\nand helps in understanding the efﬁcacy of seq2seq model. The ex-\nperiments using character-based RNNLM showed the importance of\nlanguage model in boosting recognition performance (%WER) over\nall different hours of target data available for transfer learning.\nTable 5 and 6 summarizes, the effect of these techniques in terms\nof %CER and %WER. These methods also show their ﬂexibility in\nincorporating it in attention and CTC based seq2seq model without\ncompromising loss in performance.\n6. FUTURE WORK\nWe could use better architectures such as VGG-BLSTM as a multi-\nlingual prior model before transferring them to a new target language\nby performing stage-2 retraining. The naive multilingual approach\ncan be improved by including language vectors as input or target dur-\ning training to reduce the confusions. Also, investigation of multi-\nlingual bottleneck features [31] for seq2seq model can provide better\nperformance. Apart from using the character level language model\nas in this work, a word level RNNLM can be connected during de-\ncoding to further improve %WER. The attention based decoder can\nbe aided with the help of RNNLM using cold fusion approach dur-\ning training to attain a better-trained model. In near future, we will\nincorporate all the above techniques to get comparable performance\nwith the state-of-the-art hybrid DNN/RNN-HMM systems.\n7. REFERENCES\n[1] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, “Sequence to\nsequence learning with neural networks,” in Advances in neu-\nral information processing systems, 2014, pp. 3104–3112.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,\n“Neural machine translation by jointly learning to align and\ntranslate,” arXiv preprint arXiv:1409.0473, 2014.\n[3] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio, “Learning phrase representations using RNN\nencoder-decoder for statistical machine translation,” arXiv\npreprint arXiv:1406.1078, 2014.\n[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio, “Attention-based mod-\nels for speech recognition,” in Advances in neural information\nprocessing systems, 2015, pp. 577–585.\n[5] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech\nrecognition with recurrent neural networks.,” in ICML, 2014,\nvol. 14, pp. 1764–1772.\n[6] Alex Graves, “Supervised sequence labelling,” in Supervised\nsequence labelling with recurrent neural networks , pp. 5–13.\nSpringer, 2012.\n[7] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals,\n“Listen, attend and spell: A neural network for large vocab-\nulary conversational speech recognition,” in IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2016, pp. 4960–4964.\n[8] Andrew Rosenberg, Kartik Audhkhasi, Abhinav Sethy, Bhu-\nvana Ramabhadran, and Michael Picheny, “End-to-end speech\nrecognition and keyword search on low-resource languages,”\nin IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2017, pp. 5280–5284.\n[9] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja\nSchultz, “Automatic speech recognition for under-resourced\nlanguages: A survey,” Speech Communication, vol. 56, pp.\n85–100, 2014.\n[10] Zoltan Tuske, David Nolden, Ralf Schluter, and Hermann Ney,\n“Multilingual mrasta features for low-resource keyword search\nand speech recognition systems,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n2014, pp. 7854–7858.\n[11] Martin Karaﬁ ´at, Murali Karthick Baskar, Pavel Matˇejka, Karel\nVesel`y, Franti ˇsek Gr ´ezl, and Jan ˇCernocky, “Multilingual\nblstm and speaker-speciﬁc vector adaptation in 2016 but ba-\nbel system,” in Spoken Language Technology Workshop (SLT),\n2016 IEEE. IEEE, 2016, pp. 637–643.\n[12] Sibo Tong, Philip N Garner, and Herv´e Bourlard, “Multilingual\ntraining and cross-lingual adaptation on CTC-based acoustic\nmodel,” arXiv preprint arXiv:1711.10025, 2017.\n[13] Pawel Swietojanski and Steve Renals, “Learning hidden unit\ncontributions for unsupervised speaker adaptation of neural\nnetwork acoustic models,” in Spoken Language Technology\nWorkshop (SLT), 2014 IEEE. IEEE, 2014, pp. 171–176.\n[14] Markus M ¨uller, Sebastian St¨uker, and Alex Waibel, “Language\nadaptive multilingual CTC speech recognition,” in Interna-\ntional Conference on Speech and Computer . Springer, 2017,\npp. 473–482.\n[15] Siddharth Dalmia, Ramon Sanabria, Florian Metze, and\nAlan W Black, “Sequence-based multi-lingual low resource\nspeech recognition,” arXiv preprint arXiv:1802.07420, 2018.\n[16] Shinji Watanabe, Takaaki Hori, and John R Hershey, “Lan-\nguage independent end-to-end architecture for joint language\nidentiﬁcation and speech recognition,” in Automatic Speech\nRecognition and Understanding Workshop (ASRU), 2017\nIEEE. IEEE, 2017, pp. 265–271.\n[17] Shubham Toshniwal, Tara N Sainath, Ron J Weiss, Bo Li, Pe-\ndro Moreno, Eugene Weinstein, and Kanishka Rao, “Multilin-\ngual speech recognition with a single end-to-end model,” in\nIEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2018.\n[18] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey,\nand Tomoki Hayashi, “Hybrid CTC/attention architecture for\nend-to-end speech recognition,”IEEE Journal of Selected Top-\nics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.\n[19] Matthias Sperber, Jan Niehues, Graham Neubig, Sebastian\nSt ˜AŒker, and Alex Waibel, “Self-attentional acoustic mod-\nels,” in 19th Annual Conference of the International Speech\nCommunication Association (InterSpeech 2018), 2018.\n[20] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwise\nattention,” CoRR, vol. abs/1712.05382, 2017.\n[21] Karen Simonyan and Andrew Zisserman, “Very deep convolu-\ntional networks for large-scale image recognition,” 09 2014.\n[22] Ying Zhang, Mohammad Pezeshki, Phil ´emon Brakel,\nSaizheng Zhang, C ´esar Laurent, Y Bengio, and Aaron\nCourville, “Towards end-to-end speech recognition with deep\nconvolutional neural networks,” September 2016.\n[23] Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio, “Attention-based mod-\nels for speech recognition,” in Advances in Neural Informa-\ntion Processing Systems . 2015, vol. 2015-January, pp. 577–\n585, Neural information processing systems foundation.\n[24] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William Chan,\n“Advances in joint CTC-attention based end-to-end speech\nrecognition with a deep cnn encoder and RNN-LM,” arXiv\npreprint arXiv:1706.02737, 2017.\n[25] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-\nget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr\nMotlicek, Yanmin Qian, Petr Schwarz, et al., “The kaldi speech\nrecognition toolkit,” inAutomatic Speech Recognition and Un-\nderstanding, 2011 IEEE Workshop on. IEEE, 2011, pp. 1–4.\n[26] Mike Schuster and Kuldip K Paliwal, “Bidirectional recurrent\nneural networks,” IEEE Transactions on Signal Processing ,\nvol. 45, no. 11, pp. 2673–2681, 1997.\n[27] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[28] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Bur-\nget, and Jan Cernocky, “RNNLM-recurrent neural network\nlanguage modeling toolkit,” in Proc. of the 2011 ASRU Work-\nshop, 2011, pp. 196–201.\n[29] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-\nplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al.,\n“Espnet: End-to-end speech processing toolkit,”arXiv preprint\narXiv:1804.00015, 2018.\n[30] Sibo Tong, Philip N Garner, and Herv ´e Bourlard, “An investi-\ngation of deep neural networks for multilingual speech recog-\nnition training and adaptation,” Tech. Rep., 2017.\n[31] Frantisek Gr ´ezl, Martin Karaﬁ ´at, and Karel Vesel `y, “Adapta-\ntion of multilingual stacked bottle-neck neural network struc-\nture for new language,” in IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) . IEEE,\n2014.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8429226279258728
    },
    {
      "name": "Transfer of learning",
      "score": 0.6426769495010376
    },
    {
      "name": "Lexicon",
      "score": 0.6124303340911865
    },
    {
      "name": "Language model",
      "score": 0.6030648946762085
    },
    {
      "name": "Sequence (biology)",
      "score": 0.575228750705719
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5537923574447632
    },
    {
      "name": "Natural language processing",
      "score": 0.5420688986778259
    },
    {
      "name": "Recurrent neural network",
      "score": 0.48456016182899475
    },
    {
      "name": "Decoding methods",
      "score": 0.4470215439796448
    },
    {
      "name": "Speech recognition",
      "score": 0.4456421434879303
    },
    {
      "name": "Artificial neural network",
      "score": 0.425675630569458
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ]
}