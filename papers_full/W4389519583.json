{
    "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
    "url": "https://openalex.org/W4389519583",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2124637199",
            "name": "Zifeng Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2232363908",
            "name": "Cao Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2110385854",
            "name": "Jimeng Sun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3140760072",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4366196653",
        "https://openalex.org/W4365211596",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4226099034",
        "https://openalex.org/W4385574097",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4307003748",
        "https://openalex.org/W2600107025",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W3107174963",
        "https://openalex.org/W2917536104",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2962862931"
    ],
    "abstract": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12461â€“12472\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nAutoTrial: Prompting Language Models for Clinical Trial Design\nZifeng Wang\nUIUC\nUrbana, IL, USA\nzifengw2@illinois.edu\nCao Xiao\nGE Healthcare\nSeattle, W A, USA\ncao.xiao@ge.com\nJimeng Sun\nUIUC\nUrbana, IL, USA\njimeng@illinois.edu\nAbstract\nClinical trials are critical for drug development.\nConstructing the appropriate eligibility criteria\n(i.e., the inclusion/exclusion criteria for patient\nrecruitment) is essential for the trialâ€™s success.\nProper design of clinical trial protocols should\nconsider similar precedent trials and their eli-\ngibility criteria to ensure sufficient patient cov-\nerage. In this paper, we present a method\nnamed AutoTrial to aid the design of clini-\ncal eligibility criteria using language models.\nIt allows (1) controllable generation under in-\nstructions via a hybrid of discrete and neural\nprompting, (2) scalable knowledge incorpora-\ntion via in-context learning, and (3) explicit\nreasoning chains to provide rationales for un-\nderstanding the outputs. Experiments on over\n70K clinical trials verify that AutoTrial gen-\nerates high-quality criteria texts that are fluent\nand coherent and with high accuracy in captur-\ning the relevant clinical concepts to the target\ntrial. It is noteworthy that our method, with a\nmuch smaller parameter size, gains around 60%\nwinning rate against the GPT-3.5 baselines via\nhuman evaluations.\n1 Introduction\nGenerative large language models (LLMs) are\ndrawing attention due to their ability to create co-\nherent and human-like text documents. Clinical\ntrial design documents are written at the planning\nstage of the drug development process, which is\ncrucial for the success of the trial. However, it can\nbe challenging even for experienced professionals:\naround 57% trial protocols have at least one sub-\nstantial amendment in eligibility criteria (CSDD,\n2016). The suboptimal trial design may cause in-\nsufficient recruitment, severe adverse events, or\ninsignificant efficacy, thus inducing huge financial\nlosses and time waste. Each amendment will fur-\nther cause millions of dollars in loss and months of\ndelays.\nIn this paper, we propose to generate the eligi-\nbility criteria for clinical trials in natural language\nusing LLMs, with the solution focusing on the fol-\nlowing aspects.\nâ€¢ Comprehending instructions. The LLM will\nbe prompted with key information about a trial,\nsuch as the target conditions and treatments, and\nthe additional instruction to generate the criteria\nfor participant recruitment. This process requires\nthe employed LLM to comprehend the input and\nadapt to the input instruction to generate precise\neligibility criteria that meet the specified objec-\ntives. It also necessitates the domain knowledge\nabout clinical trials stored in LLMs.\nâ€¢ Referring to prior studies . A thorough liter-\nature search is important for human experts to\ndesign clinical trials (Chew, 2019). Similarly, the\nemployed LLMs should be able to leverage the\ncontext information, such as retrieved eligibil-\nity criteria from prior successful studies, as the\nreference to generate better trial design.\nâ€¢ Rationalizing the generation . LLMs should\noffer the rationale behind the generated criteria,\nwhich is important for clinical experts to under-\nstand and adopt the generation results in practice.\nIn this paper, we propose to augment clinical trial\ndesign using LLMs, motivated by the evidence that\nLLMs can act as implicit knowledge bases (Petroni\net al., 2019; Taylor et al., 2022). Our method is\nequipped with instruction tuning for trial proto-\ncol design and explicit supervision for producing\ngrounding rationales. This is enabled with the fol-\nlowing technical features:\nâ€¢ Instruction prompting for adapting expert in-\nstructions. Instruction tuning for granular con-\ntrol over the generated criteria to follow diverse\nuser intentions.\nâ€¢ Scalable and efficient knowledge expansion. A\ncombination of 1) external memory for a dense re-\n12461\ntriever and 2) internal memory for neural prompt-\ning, which is amenable to updating the model\nincrementally as new data are available.\nâ€¢ Explicit supervision for generating grounding\nrationales. Adaption of LLMs with a reasoning\ncapability through a supervised paradigm, mak-\ning the generated criteria more transparent and\ninterpretable.\nOur AutoTrial is the first model that utilizes\nLLMs for automating clinical trial design. It repre-\nsents an important step toward using AI to facilitate\nclinical trial design. The rest of the paper is orga-\nnized as follows: In Â§2, we review related work. In\nÂ§3, we dive into the proposed method in detail. In\nÂ§4, we present the experiment results. It is notewor-\nthy that AutoTrial is proven to generate accurate\ncriteria (with precision 0.91, recall 0.92, F1 0.91,\nand Jaccard score of 0.84 in clinical accuracy eval-\nuation) while almost all baselines get less than 0.5\nin these metrics. Moreover, our method reaches\naround 60% winning rate against GPT-3.5 1 in trial\ndesign tasks via human evaluation. Finally, in Â§5,\nwe conclude and discuss future work.\n2 Related Work\n2.1 Large Language Models\nLarge language models pre-trained on web-scale\ntext data exhibit extraordinary emergent capabil-\nity in a diverse set of natural language processing\n(NLP) tasks (Kaplan et al., 2020; Brown et al.,\n2020). It was recently witnessed that LLMs can\nbe further tuned to align with human preferences\nthrough instruction tuning (Chung et al., 2022;\nWang et al., 2022) and reinforcement learning from\nhuman feedback (RLHF) (Ouyang et al., 2022;\nYuan et al., 2023; Dong et al., 2023).\nDespite the remarkable capabilities of large lan-\nguage models (LLMs) trained on general text cor-\npus, they often face challenges when generating\nhighly domain-specific tasks unless they undergo\nadditional tuning. Research has shown that even\na â€œsmallâ€ 300M LM, with instruction tuning, can\noutperform LLMs with over 100B parameters (Ya-\nsunaga et al., 2022). This finding encourages the\nefforts to develop customized expert LLMs by\nperforming instruction tuning on domain-specific\ndatasets, e.g., clinical notes and scientific publi-\ncations (Singhal et al., 2022). In this work, we\n1Engine gpt-3.5-turbo-0301: https://platform.\nopenai.com/docs/models/gpt-3-5\nare the first to develop LLMs focusing on trial de-\nsign through a mixture of techniques, including\ninstruction tuning, evidence-grounded generation,\nand supervised learning for rationale generation.\n2.2 Clinical Trial Design\nThe clinical trial design is a new research topic for\nthe NLP community, and there are only a few works\nrelated to clinical trial design, either focusing on\ntrial feature embedding or trial design evaluation.\nFor trial feature embedding, Marshall et al. (2017)\nextracted text pieces that describe the key trial char-\nacteristics as a summary report. More recently,\nWang and Sun (2022) developed a self-supervised\ndocument model for dense retrieval for clinical tri-\nals. For trial design evaluation, Kim et al. (2021)\nmanually adjusted criteria to broaden patient ac-\ncrual and assess the influence of criteria, while Liu\net al. (2021) utilized Shapley scores (Lundberg and\nLee, 2017) to estimate the change of hazard ratio\nof the included oncology patients when removing\neach criterion. Despite these efforts, no existing\nwork focuses on clinical trial design automation.\n3 Method\nAutoTrial utilizes a decoder-based architecture\nfor generating a target criterion based on input trial\nsynopsis and manual instructions. The training\nprocess consists of two stages: pretraining and\nfinetuning.\nâ€¢ During the pretraining stage, the model is trained\non a large corpus of trial documents in order to\nlearn to reason through multiple steps and mimic\nthe retrieved input criteria exemplars.\nâ€¢ In the finetuning stage, the model is trained to\ngenerate the target criterion according to the in-\nput instructions. For example, an instruction\n<age> that urges the model to populate the crite-\nrion describing the participantâ€™s age requirement.\nIt is noteworthy that the model can be extended\nto new instructions and trial exemplars without\nretraining. The flowchart is depicted in Fig. 1.\nWe will elaborate on the details of the training and\ninference procedures of AutoTrial next.\n3.1 Problem Setup\nThe generation model is represented by the func-\ntion f, and generates a target criterion yc based\non input x = {xs,xe,xr}. Here, xs denotes trial\n12462\n[title âŠ•disease âŠ•treatment]\nexemplartrial setup âŠ•\n[inc1 âŠ•inc2 âŠ•â€¦] [inc3 âŠ•inc4 âŠ•â€¦]\nReasoning in multiple steps\nPretraining\nFinetuning\ntrial setup âŠ• exemplar\n[inc1 âŠ•inc2 âŠ•â€¦ âŠ•age âŠ•age is above 18]\npromptğ‘’ğ‘šğ‘\nRetrieved In-context Exemplars\nentity\nencode\nbmientity\nModel\nModel\n[inc3 âŠ•inc4 âŠ•â€¦]\nbody mass index is \nbetween 18.5 and \n24.9 kg/m^2\ncriterion\ncriterion\nmulti-step \nreasoning\nModel\nGeneration\ntrial setup\nexemplar \nâŠ•entity1\nexemplar \nâŠ•entity2\nexemplar \nâŠ•entity3\n[inc1 âŠ•inc2 âŠ•â€¦] âŠ•criterion1\n[inc1 âŠ•inc2 âŠ•â€¦] âŠ•criterion1â€²\n[inc1 âŠ•inc2 âŠ•â€¦] âŠ•criterion1â€²â€²\ngenerate\nmultiple candidates\nModel\n Model\nclustering + ranking by ppl\n[inc1 âŠ•inc2 âŠ•â€¦] âŠ•criterion1\nğ±ğ‘  ğ±ğ‘’ ğ²ğ‘ğ²ğ‘¡\nrationale [Inc5]\nğ±ğ‘  ğ±ğ‘’\nğ±ğ‘’ğ‘¡ ğ±ğ‘’ğ‘Ÿ ğ±ğ‘’ğ‘\nâŠ•\nğ±ğ‘Ÿ\nğ²ğ‘¡\nğ²ğ‘\nğ±ğ‘ \nğ±ğ‘’, ğ±ğ‘Ÿ\nğ²ğ‘¡ ğ²ğ‘\nğ„ğ‘Ÿ multi-step reasoning\nmulti-step reasoning\nFigure 1: The workflow of the proposed AutoTrial. Step I: pre-train on unlabeled trial documents with prompts to\nmimic the multi-step reasoning. Step II: finetune the model to generate criteria under instructions. Step III: generate\ndiverse target criteria by instructions with large-scale sampling plus clustering and ranking.\nsetups, which is a concatenation of the trial title,\ncondition, and treatment, as the elements illustrated\nby Fig. 1. xr denotes the discrete prompt describ-\ning the objective criterion, e.g., â€œbmiâ€ prompts the\nmodel to generate the criterion for body mass in-\ndex of the participants. xe denotes the exemplars\nretrieved from relevant trials built for the in-context\nlearning of LLMs. To this end, we formulate\nxe = {xt\ne,xr\ne,xc\ne}, which contains the reasoning\nsteps xt\ne, e.g., a chain of criteria that leads to the\ntarget criteria, the targeting instruction xr\ne, e.g., the\nobjective to be set for recruiting patients, and the\ntarget criterion xc\ne that describes the requirement\nbased on the instruction.\nThe model is also controlled by continuous\nprompt hp, which is specific to each type of in-\nstruction, e.g., the targeting entity that the criterion\nshould contain. The model is trained to generate\ncriteria y with multi-step reasoning: generating\nrelevant criteria one by one and ultimately yield-\ning the target criterion. Therefore, the generation\nprocess is expressed in Eq. (1),\ny = f(xs,xe,xr,hp). (1)\nReferring to the exemplar xe, the model outputs\ny = yt âŠ•yc, where yt is the reasoning steps and\nyc denotes the target criterion.\n3.2 Hybrid Prompting\nWe opt to employ a hybrid of discrete and neural\nprompting to endow the model with the ability to\ngenerate criteria based on specific instructions.\n3.2.1 Discrete Prompt\nThe discrete prompt is motivated by the prospect of\nin-context learning of LLMs (Wei et al., 2022), as\nthe reasoning ability of LLMs can be enhanced via\nthe input-output exemplars, e.g., the concatenation\nof a series of criteria xt\ne, the target instruction xr\ne,\nand the target criteriaxc\ne. We formulate the discrete\nprompts with specialized tokens:\n1. Trial Setup: we wrap the introduction of trial\nsetups, including title, disease, and treatment, using\nspecialized tokens like <title>, <disease>, and\n<treatment>. The setup offers the basic context\nof a trial.\n2. In-context Exemplar: we curate the exemplar\nthat resembles the multi-step reasoning procedure:\nthe model first generates a series of intermediate\nrationales that lead to the final answer. Concretely,\nthe exemplar xe = {xt\ne,xr\ne,xc\ne}is retrieved from\nthe external knowledge storeand demonstrate as\nthe template for the model outputs. xt\ne are many eli-\ngibility criteria wrapped by<inc> or <exc> indicat-\ning inclusion or exclusion criteria. xr\ne describes the\ninstruction wrapped by <statement>, e.g., tell the\nmodel to generate a criterion describing the age re-\nquirement by â€œ<statement> ageâ€. xc\ne is the target\ncriterion wrapped by <target>, e.g., â€œ <target>\nage is above 18 yrs oldâ€.\n3. Textual Instruction: following the exemplar,\nxr enforces the model to obey the instruction,\nwrapped by <statement> such as â€<statement>\ngenderâ€.\n12463\nThe exemplars are stored in an external knowl-\nedge store providing an open-book referencethat\nthe model can refer to during the generation. It\nis built on the training data C = {(xi,yi)}N\ni\nthat is amenable to edit, add or delete during the\ncourse of training and generating needless of re-\ntraining the model. We utilize a neural encoder\nT named Trial2Vec (Wang and Sun, 2022) that\nencodes trial setups xs to dense embeddings, as\nhs = T(xs) âˆˆRd that carry rich semantics of the\ntrials. Consequently, the knowledge store is given\nby Eq. (2),\n(K,V) ={(hs,xe) |(x,y) âˆˆC} (2)\nwith the embeddings serving as the keys and the\nexemplars as the values. Here, the vector-based\nsearch engine can be implemented for efficient ex-\nemplar retrieval on the fly.\n3.2.2 Neural Prompt\nConsider the embeded input tokens x<l =\n{x1,...,x l}as H<l = {h1,..., hl}âˆˆ RlÃ—d, we\nprepend neural prompts to H<l to get the prompted\ninput ËœH<l = hp âŠ•H<l. Formally, we create a set\nof instruction indices I, the i-th instruction xr,i is\nparameterized by Eq. (3),\nhp = MLP(Er[i,:]), (3)\nwhere Er âˆˆR|I|Ã—dâ€²\nis the trainable embedding ma-\ntrix. Er[i,:] indicates looking up thei-th row of the\nmatrix; MLP : Rdâ€²\nâ†¦â†’Rd projects the embedded\ninstruction to the same dimension as H.\nThe neural prompting is modular, meaning that it\ncan be easily modified to incorporate additional in-\nstructions Iâ€²by simply extending the index setI=\n{I,Iâ€²}and the embedding matrix Er = {Er,Eâ€²\nr}\nfor those instructions. When the model is finetuned\non new data, we can only update the instruction\nembedding Eâ€²\nr while the rest of the model remains\nfrozen. This allows the model to effectively learn\nto generate based on a broader range of instructions\nwhile minimizing the risk of catastrophic forget-\nting, i.e., the performance degradation on previous\ndata.\n3.3 Multi-stage Training\nAs described in Â§3.1, we have a dataset contain-\ning pairs of input instructions (denoted as xr) and\ncorresponding criteria (denoted as yc). We extract\nclinical relations from the raw criteria to formu-\nlate the training and testing data, e.g., extracting\nthe relation â€œNYHA âˆˆ{III, IV}â€ from the criteria\nâ€œNYHA class is above IIâ€. However, the parser may\nnot be able to extract all relevant instructions from\nall available trial documents. We hence propose to\ntrain our method in two stages: first pretraining on\na large set of unlabeled trial documents and then\nfinetuning on the processed dataset of instruction-\ncriteria pairs. This approach allows us to make the\nmost of the available data and facilitate the model\nperformance.\nPretraining. We create a pretraining dataset\nCpre = {(xs,xe,yt,yc)i}M\ni , (4)\nwhere the model fis urged to generatey = ytâŠ•yc\nin Eq. (1). The inputs comprise the trial setup xs\nand the exemplar xe which is also composed of\nmultiple criteria. Drawing the inspiration from\n(Taylor et al., 2022), we decide to include prompts\nand special tokens in the pretraining stage. Specifi-\ncally, we explicitly emphasize the step-by-step rea-\nsoning task by inserting the separate tokens <inc>\nand <exc> into xe and yt, and the model is super-\nvised to generate the intermediate rationales and\nyield the target criterion.\nOur method is built based on decoder-based\nCLM (e.g., GPT2 (Radford et al., 2019)) where\nthe decoder predicts y autoregressively. Denote\nthe learned decoding distribution as pÎ¸(Â·), the ob-\njective is the maximum log-likelihood estimation\ngiven by Eq. (5),\nLMLE = âˆ’1\nL\nLâˆ‘\nl=1\nlog pÎ¸(yl|y<l,x). (5)\nwhere y<l are tokens in y before the l-th token; L\nis the total number of tokens in the target y.\nFinetuning. After pretraining, the model is fine-\ntuned on the dataset C, and taught to follow the\ninstruction when generating criteria. The inputs\nand outputs are described in Eq. (1). In addition\nto the MLE loss in Eq. (5), we apply a contrastive\nloss LCL (Su et al., 2022) to enhance the model\nrepresentation learning, as in Eq. (6),\nLCL = 1\nLÃ—(Lâˆ’1)\nLâˆ‘\nl=1\nLâˆ‘\nj=1,jÌ¸=l\nmax{0,Ï âˆ’s(hyl ,hyl ) +s(hyl ,hyj )},\n(6)\nwhere hyl is the embedding of tokenyl, Ïis the pre-\ndefined margin, s(Â·) is the cosine similarity func-\ntion. The finetuning loss combines the objectives\n12464\nTable 1: The statistics of the used trial data.\nTrain Valid Test\n# trials 54,703 6,079 15,195\n# inclusion 153,169 17,145 42,269\n# exclusion 128,310 14,581 35,247\nAvg inc length 121.0 120.4 118.5\nAvg exc length 148.3 153.0 145.2\nfrom Eqs. (5) and (6) as given by Eq. (7),\nLFT = LMLE + LCL. (7)\nNote that the decoding distribution in the finetuning\nstage is pÎ¸(y|y<l,x,hp) that differs from the one\nused in the pretraining shown in Eq. (5).\n3.4 Generation\nDenote the vocabulary by V, we conduct top-k\nsampling repeatedly to acquire diverse candidates\nË†Y = {Ë†yq}Q\nq=1, by Eq. (8),\nyl âˆ¼pÎ¸(y|y<l,x,hp),s.t. yl âˆˆV(ks), (8)\nwhere V(ks) is a subset of V that maximizesâˆ‘\nyâˆˆV(ks) pÎ¸(y|y<l,x,hp), and |V(ks)|= ks. We\nfurther adopt clustering and ranking to select sam-\nples from the generated candidates. We first encode\nË†y by Trial2Vec to dense embeddings hË†y and ap-\nply k-means clustering with kq clusters. We then\ncompute the perplexity (ppl) of each output Ë†yq, and\npick the sample with the minimum ppl in each clus-\nter to form the final candidate set with kq samples.\nAn example of the input and output of AutoTrial\ncan be found in Table 5 in the appendix.\n4 Experiment\nWe conduct extensive experiments to evaluate\nAutoTrial in the following aspects:\nâ€¢ The overall quality in terms of criteria-level and\ntrial-level trial protocol generation.\nâ€¢ The capability of the continual update for new\ntrials and instructions.\nâ€¢ The ablation analysis for the components of the\nproposed method.\n4.1 Dataset\nWe collected clinical trial documents from Clin-\nicalTrials.gov (NIH, 2023) and filtered out those\nwithout valid interventions, diseases, or titles, as\nwell as those with void eligibility criteria. We ex-\ntracted 75,977 clinical trials and applied Clinical\nTrial Parser (FAIR, 2022) to extract 751,258 med-\nical relations from the eligibility criteria of these\ntrials. The train-test split is shown in Table 1. For\neach trial, we sampled one criterion as the target\nand several others as input exemplars, resulting in\n2,528,231 unique training samples out of 400K tri-\nals as the pretraining data. The validation and test\ntrials were excluded from the pretraining data.\n4.2 Evaluation Strategy\nAutomatic Evaluation. To evaluate the quality of\nthe output criteria, which are expressed in natural\nlanguage, we employ metrics from the NLG lit-\nerature, including CIDEr (Vedantam et al., 2015),\nROUGE-L (Lin, 2004), METEOR (Lavie and Agar-\nwal, 2007), and BLEU (Papineni et al., 2002).\nThese metrics allow us to assess the fluency and\ncoherence of the generated criteria quantitatively.\nWe evaluate all the methods at thecriteria level\nand trial level. At the criteria level, the model\nwill generate each criterion separately, using the\nconcatenated trial setup texts and the first three\ntokens of the targeting criteria as input. At the trial\nlevel, the model will take the concatenated trial\nsetup texts as input and generate all of the criteria\nfor the trial at once.\nClinical Accuracy. To evaluate the clinical ac-\ncuracy of the generated criteria, we run Clinical\nTrial Parser (FAIR, 2022) on the generated crite-\nria to extract the medical relations and compare\nthem with the relations extracted from the corre-\nsponding ground-truth criteria. We evaluate the\noverlapping of two relation sets by the precision,\nrecall, F1-score, and Jaccard similarity.\nHuman Evaluation. We perform a manual evalua-\ntion to compare the generated clinical trial design\nfrom our method with the generated by a general\nLLM. We enlisted the expertise of domain experts\nto assess and choose the superior output between\nour method and the LLMâ€™s output for a given trial\nsynopsis. This allowed us to collect feedback and\ncalculate the winning rate.\n4.3 Implementations\nTo the best of our knowledge, there were no exist-\ning algorithms for automatic trial design generation.\nWe thus propose to compare AutoTrial with dif-\nferent NLG models: finetuning (FT), prefix-tuning\n(PT) (Li and Liang, 2021), retrieval-augmented gen-\neration (RAG) (Lewis et al., 2020), and contrastive\n12465\nTable 2: Automatic evaluationof eligibility criteria generation results on the test set on the trial-level, i.e., compare\nthe concatenated inclusion/exclusion criteria of a trial with the corresponding groundtruth. B1 is short for BLEU-1.\nTrial level - Inclusion Trial level - Exclusion\nMethod/Scores B1 METEOR ROUGE-L CIDEr B1 METEOR ROUGE-L CIDEr\nGPT2-FT 9.2 23.6 7.5 0.10 7.2 8.3 2.7 0.02\nGPT2-RAG 16.4 25.2 9.9 0.09 9.7 19.4 7.4 0.09\nGPT2-PT 20.0 19.2 22.8 0.17 16.5 15.0 12.6 0.14\nGPT2-SimCTG 9.8 24.8 10.6 0.11 9.4 11.4 5.5 0.06\nT5-FT 21.9 29.8 18.3 0.18 13.2 13.9 8.0 0.07\nT5-RAG 22.7 28.9 16.7 0.15 14.9 11.8 7.5 0.06\nT5-PT 43.2 21.0 23.3 0.09 17.8 23.4 11.1 0.19\nT5-SimCTG 22.1 30.3 17.6 0.17 15.8 13.1 7.7 0.06\nAutoTrial 58.7 40.8 40.6 0.24 54.4 36.3 35.3 0.33\nTable 3: Automatic evaluationof eligibility criteria generation results on the test set on the criteria-level, i.e.,\ncompare the generated inclusion/exclusion criteria with the groundtruth one by one. B1 is short for BLEU-1.\nCriteria level - Inclusion Criteria level - Exclusion\nMethod/Scores B1 METEOR ROUGE-L CIDEr B1 METEOR ROUGE-L CIDEr\nGPT2-FT 9.0 18.1 27.0 0.54 18.2 16.4 21.4 0.47\nGPT2-RAG 19.9 17.7 28.0 0.50 13.8 17.3 22.1 0.52\nGPT2-PT 10.1 13.9 18.3 0.23 14.1 12.3 10.9 0.13\nGPT2-SimCTG 32.3 16.6 32.3 0.68 29.1 14.5 23.0 0.68\nT5-FT 12.8 6.8 9.4 0.11 11.2 3.9 5.1 0.04\nT5-RAG 14.7 11.3 14.2 0.30 12.1 5.3 6.8 0.09\nT5-PT 22.3 9.8 15.8 0.26 10.4 15.5 9.3 0.18\nT5-SimCTG 20.3 11.5 11.6 0.33 12.4 10.3 10.5 0.17\nAutoTrial 39.7 24.3 35.3 0.79 38.4 24.2 30.0 0.68\nlearning + contrastive search ( SimCTG) (Su et al.,\n2022). We choose GPT2 (Radford et al., 2019) and\nT5 (Raffel et al., 2020) as the backbones. We also\ncompare with GPT-3.5 to verify if general LLMs\ncan generate reasonable eligibility criteria (Ouyang\net al., 2022) 1.\nFor our method, we leverage GPT-2 (Radford\net al., 2019) as the backbone model. We set the\nmaximum context length as 1,024. In the pertaining\nstage, we train the backbone model with a batch\nsize of 64, learning rate 5e-5, weight decay 1e-4,\nand 5 epochs. In the instruction tuning stage, we\ntrain the model with a batch size of 16, learning\nrate 5e-5, weight decay 1e-5, and 10 epochs.\n4.4 Exp 1: Generation Quality\nText quality. Table 2 shows the automatic evalu-\nation scores with the evaluation done at the trial\nlevel. The results show that AutoTrial demon-\nstrates superior performance over the baselines in\nall four metrics (BLEU-1, METEOR, ROUGE-L,\nand CIDEr) for both inclusion and exclusion crite-\nria. We can draw similar conclusions from Table 3,\nwith the evaluation at the criteria level.\nOne notable finding is that the performance for\ninclusion criteria generation is generally better than\nfor exclusion criteria generation. We conjecture\nthat inclusion criteria are presented prior to exclu-\nsion criteria in the training data, which may lead\nto the truncation of the latter due to the modelâ€™s\nmaximum acceptable length. Besides, errors may\naccumulate when generating criteria in an autore-\ngressive manner. AutoTrial mitigates the order\nissue credited to the hybrid prompting.\nClinical accuracy. We present the clinical accu-\nracy evaluation results in Table 4. As aligned with\nthe automatic evaluation results, AutoTrial per-\nforms better at criteria generation, with a bigger\nperformance gap. For example, AutoTrial is the\nonly method that yields recall above 0.5 (w/ 0.91),\nF1 above 0.6 (w/ 0.91), and Jaccard scores above\n0.4 (w/ 0.84) in inclusion criteria generation. It\nwins over baselines with a prominent margin in\nexclusion criteria generation. These results demon-\nstrate that our method can generate criteria accu-\nrately aligned with the provided instructions.\nWe also observe that most methods obtain decent\nprecision and AutoTrial has the best performance.\nIt implies a low hallucination rate in our methodâ€™s\ngenerated text because most generated relations\n12466\nTable 4: Clinical accuracyevaluation results of eligibil-\nity criteria generation results on the test set. P, R, F1, Jac\nare short for precision, recall, F1 score, micro-Jaccard\nscore, respectively.\nType Method/Score P R F1 Jac\nInclusion\nGPT2-FT 0.74 0.35 0.47 0.31\nGPT2-RAG 0.81 0.41 0.54 0.37\nGPT2-PT 0.75 0.45 0.56 0.39\nGPT2-SimCTG 0.89 0.40 0.56 0.38\nT5-FT 0.77 0.10 0.17 0.09\nT5-RAG 0.82 0.13 0.22 0.12\nT5-PT 0.74 0.17 0.27 0.16\nT5-SimCTG 0.68 0.04 0.08 0.04\nAutoTrial 0.91 0.92 0.91 0.84\nExclusion\nGPT2-FT 0.69 0.21 0.33 0.20\nGPT2-RAG 0.59 0.26 0.36 0.22\nGPT2-PT 0.36 0.25 0.30 0.17\nGPT2-SimCTG 0.80 0.23 0.36 0.22\nT5-FT 0.24 0.03 0.06 0.03\nT5-RAG 0.24 0.03 0.05 0.03\nT5-PT 0.18 0.25 0.21 0.12\nT5-SimCTG 0.16 0.01 0.02 0.01\nAutoTrial 0.85 0.89 0.87 0.76\n/uni0000005d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000014/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000011/uni00000018/uni00000010/uni00000037/uni00000058/uni00000055/uni00000045/uni00000052\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni00000024/uni00000058/uni00000057/uni00000052/uni00000037/uni00000055/uni0000004c/uni00000044/uni0000004f/uni00000003/uni0000003a/uni0000004c/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000028/uni00000059/uni00000044/uni0000004f/uni0000001d/uni00000003/uni0000002c/uni00000051/uni00000046/uni0000004f/uni00000058/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000026/uni00000055/uni0000004c/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000044\n/uni0000005d/uni00000048/uni00000055/uni00000052/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000014/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000011/uni00000018/uni00000010/uni00000037/uni00000058/uni00000055/uni00000045/uni00000052\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni00000024/uni00000058/uni00000057/uni00000052/uni00000037/uni00000055/uni0000004c/uni00000044/uni0000004f/uni00000003/uni0000003a/uni0000004c/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000028/uni00000059/uni00000044/uni0000004f/uni0000001d/uni00000003/uni00000028/uni0000005b/uni00000046/uni0000004f/uni00000058/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000026/uni00000055/uni0000004c/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000044\nFigure 2: Human evaluations of the winning rate of\nAutoTrial against GPT-3.5 when GPT-3.5 does zero-\nshot generation (no exemplar), 1-shot, and 5-shot in-\ncontext learning.\nare also concretely mentioned in the groundtruth\neligibility criteria. However, the baselines perform\nmuch worse regarding the recall and Jaccard scores.\nIt indicates that AutoTrial is advantageous in the\nhigh coverage of targeting clinical relations in the\ngenerated criteria.\nHuman Evaluation. The human evaluation re-\nsults are available in Fig. 2, where we identify that\nour method significantly outperforms the GPT-3.5\nbaselines, i.e., in over 60% cases, the output cri-\nteria from AutoTrial are considered better than\nthe from GPT-3.5. This again emphasizes the op-\nportunity of developing expert LLMs that surpass\ngeneral LLMs at much less cost. It is also interest-\ning that 5-shot GPT-3.5 is worse than the 1-shot\nand zero-shot ones. We conjecture that GPT-3.5\nis impacted by the context that contains irrelevant\nexemplars when generating for the targeting trials.\nAutoTrial FT PT RAGSimCTG\nmethod\n20\n40BLEU\nAutoTrial FT PT RAGSimCTG\nmethod\n10\n20\n30METEOR\nAutoTrial FT PT RAGSimCTG\nmethod\n10\n20\n30\n40\n50ROUGE\nAutoTrial FT PT RAGSimCTG\nmethod\n0.5\n1.0\n1.5CIDEr\nFigure 3: In-depth analysis of generation quality across\n100 trial groups divided by the targeting disease. Base-\nlines are based on GPT-2.\n4.5 Exp 2: In-depth Analysis\nPerformance Divergence. We divided the raw test\ntrials by their target diseases, leading to 100 non-\noverlapping sets, with each set sharing the same\ntarget disease. We then evaluated the generated\ntexts within each subset. We created box plots of\nthe obtained scores (evaluated on the combination\nof inclusion and exclusion criteria) in Fig. 3.\nOur results indicate that AutoTrial exhibits su-\nperior performance across all metrics. It achieves\nthe highest median performance and has a more\nstable score distribution, with both a high upper\nbound and lower bound for all metrics. Among\nthe baseline methods, SimCTG performs the best\non three metrics, with the exception of METEOR.\nHowever, it should be noted that its worst-case per-\nformance was typically much lower than that of\nmost other methods. We also zoom in to show\nthe performances of trials targeting the top eight\nmost frequent diseases/conditions in Fig. 6, where\nAutoTrial consistently wins over all baselines.\nQualitative Analysis. We present several qualita-\ntive results of our model in Table 5. The model\ninputs have two parts: manual input and automati-\ncally built input, where the manual input is concate-\nnated with the automatic input and passed to the\nmodel. Users set up the basic trial information and\ncan opt to offer different instructions for generating\ncriteria. As observed in the first four rows of Table\n5, the outputs vary when provided with different\ninstructions for the same trial. Furthermore, it can\nbe observed that the generated outputs are fluent,\ncoherent, and closely resemble the referential man-\nually written criteria.\n12467\n1 2 3 4\nSplit\n0\n10\n20\n30\n40BLEU\nSimCTG\nRAG\nPT\nFT\nMethod\nRe-train\nIncremental\n1 2 3 4\nSplit\n0\n5\n10\n15\n20\n25\n30METEOR\nSimCTG\nRAG\nPT\nFT\nMethod\nRe-train\nIncremental\n1 2 3 4\nSplit\n0\n10\n20\n30\n40ROUGE\nSimCTG\nRAG\nPT\nFT\nMethod\nRe-train\nIncremental\n1 2 3 4\nSplit\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0CIDEr\nSimCTG\nRAG\nPT\nFT\nMethod\nRe-train\nIncremental\nFigure 4: Comparison between two variants of\nAutoTrial: Re-train and Incremental. The Re-train\nvariant is trained on all subsets, while the Incremental\nvariant updates its knowledge only on new subsets. The\nscatter plot also includes the performance of four base-\nlines, which are trained on all data.\n4.6 Exp 3: Incremental Learning\nOne major merit of AutoTrial is to continually up-\ndate its internal and external memory without the\nneed of retraining on all collected data. To demon-\nstrate the capability of AutoTrial in continuously\nupdating its knowledge, we designed two vari-\nants of our method: Re-train and Incremental.\nThese variants were trained on four subsets of the\nraw training set: {C1,C2,C3,C4}, with the instruc-\ntion types being equally assigned and mutually ex-\nclusive in each subset. The models encountered\nthe subsets sequentially, with the Re-train model\nlearning by combining all previously seen subsets,\ne.g., it would be trained and evaluated on {C1,C2}\nwhen C2 is revealed. In essence, Re-train is the\ntheoretic upper bound for all incremental learning\nmethods. In contrast, the Incremental model is\nalso evaluated on {C1,C2}but it would be trained\non C2 only when it is revealed. Additionally, during\ntraining, the Incremental model only updates the\nneural prompting while freezing all other parame-\nters.\nWe present the results in Fig. 4. The\nIncremental model demonstrates the capability of\nmitigating catastrophic forgetting when extended to\nnew data. However, the gap between the two vari-\nants expands over time. The Incremental model\ndecays to the level of the best baseline after be-\ning updated on the fourth subset when the total\nnumber of instructions is 4Ã—more than in the first\nsubset. We hence suggest incrementally updating\nAutoTrial until the new instructions reach around\n3Ã—more than the last fully retrained checkpoint to\nAT w/o MSR w/o RAG w/o Prompt\n0\n10\n20\n30\n40BLEU\ninclusion exclusion\nAT w/o MSR w/o RAG w/o Prompt\n0\n5\n10\n15\n20\n25METEOR\nAT w/o MSR w/o RAG w/o Prompt\n0\n10\n20\n30ROUGE\nAT w/o MSR w/o RAG w/o Prompt\n0.0\n0.2\n0.4\n0.6\n0.8CIDEr\nFigure 5: Ablation experiments of AutoTrial when\nremoving one module. AT: the original version; w/o\nMSR: without the multi-step reasoning supervision; w/o\nRAG: without the retrieval-augmented generation; w/o\nPrompt: without the neural prompting.\nreach a trade-off between utility and cost.\n4.7 Exp 4: Ablation Study\nWe conducted an ablation study (shown in Fig. 5)\nto compare the original version of AutoTrial with\nits variants when removing certain components:\nthe multi-step reasoning supervision ( w/o MSR ),\nthe retrieval-augmented generation (w/o RAG), and\nthe neural prompting ( w/o Prompt). The results\nshow that both RAG and Prompt have a significant\nimpact on the final performance. MSR performs\nsimilarly on inclusion criteria compared to the orig-\ninal version but has worse results on exclusion cri-\nteria. Despite this, MSR is ultimately retained in\nthe final model as it produces more balanced re-\nsults among inclusion and exclusion criteria and\nalso provides insight into the modelâ€™s reasoning\npath, making it more interpretable.\n5 Conclusion\nIn summary, this paper presents AutoTrial that\nuses language models to aid in the design of clini-\ncal trial protocols. Our method is able to generate\nhigh-quality criteria texts that are fluent, coherent,\nand clinically accurate, by using a combination\nof controllable generation, scalable knowledge in-\ncorporation, and multi-step reasoning. This can\npotentially reduce the risk of clinical trial failure by\nensuring that trials are properly designed and have\nsufficient power to evaluate proposed therapies.\nLimitations\nThe proposed method, AutoTrial, is a valuable\ntool for designing clinical trials by providing con-\n12468\ntrollable generation under instructions, scalable\nknowledge incorporation, and multi-step reasoning.\nHowever, it is important to note that one limitation\nof the method is that it is dependent on the qual-\nity of data used to train the language model. If\nthe clinical trial database used to train the model\ncontains biases or inaccuracies, these limitations\nmay be present in the generated criteria texts. To\nensure the quality of the generated criteria texts, it\nis crucial to use high-quality, accurate, and up-to-\ndate data to train the language model, which can\nbe achieved by regularly updating the clinical trial\ndatabases used for training.\nAdditionally, the method may not be able to ac-\ncount for unexpected or rare side effects or issues\nthat may occur during the trial, which may impact\nthe safety and efficacy of the proposed treatment.\nIt is important to note that AutoTrial should be\nconsidered a supportive tool for designing clinical\ntrials and the final decision should always be made\nby human clinicians. The tool can aid in identifying\nrelevant trials and generating high-quality criteria\ntexts, but ultimately, it is the responsibility of the\nclinician to evaluate the overall design and safety\nof the trial, taking into account the unique charac-\nteristics and needs of the trial population. The tool\nshould be used as an aid in the design process, but\nnot as a replacement for the expertise and judgment\nof human clinicians.\nAcknowledgments\nThis work was supported by NSF award SCH-\n2205289, SCH-2014438, and IIS-1838042.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877â€“1901.\nBoon-How Chew. 2019. Planning and conducting clini-\ncal research: the whole process. Cureus, 11(2).\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nTufts CSDD. 2016. Amendments reduce number of\npatients, but at high cost, longer study times. Tech-\nnical Report 1, Tufts Center for the Study of Drug\nDevelopment, Tufts University.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and Tong\nZhang. 2023. RAFT: Reward ranked finetuning\nfor generative foundation model alignment. arXiv\npreprint arXiv:2304.06767.\nFAIR. 2022. Library for converting clinical trial eligi-\nbility criteria to a machine-readable format.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nJae Hyun Kim, Casey N Ta, Cong Liu, Cynthia Sung,\nAlex M Butler, Latoya A Stewart, Lyudmila Ena,\nJames R Rogers, Junghwan Lee, Anna Ostropolets,\net al. 2021. Towards clinical data-driven eligibil-\nity criteria optimization for interventional covid-19\nclinical trials. Journal of the American Medical In-\nformatics Association, 28(1):14â€“22.\nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An\nautomatic metric for mt evaluation with high levels of\ncorrelation with human judgments. In Proceedings\nof the Second Workshop on Statistical Machine Trans-\nlation, StatMT â€™07, page 228â€“231, USA. Association\nfor Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntÃ¤schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. Advances in\nNeural Information Processing Systems, 33:9459â€“\n9474.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4582â€“4597.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74â€“81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nRuishan Liu, Shemra Rizzo, Samuel Whipple, Navdeep\nPal, Arturo Lopez Pineda, Michael Lu, Brandon\nArnieri, Ying Lu, William Capra, Ryan Copping, et al.\n2021. Evaluating eligibility criteria of oncology trials\nusing real-world data and ai. Nature, 592(7855):629â€“\n633.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. Advances\nin Neural Information Processing Systems, 30.\nIain J Marshall, JoÃ«l Kuiper, Edward Banner, and By-\nron C Wallace. 2017. Automating biomedical evi-\ndence synthesis: Robotreviewer. In Proceedings of\nthe conference. Association for Computational Lin-\nguistics. Meeting, volume 2017, page 7.\nNIH. 2023. ClinicalTrials.gov.\n12469\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730â€“27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BELU: a method for automatic eval-\nuation of machine translation. In Annual Meeting of\nthe Association for Computational Linguistics, pages\n311â€“318.\nFabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2463â€“2473.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1â€“67.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2022. Large language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. arXiv preprint\narXiv:2202.06417.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image descrip-\ntion evaluation. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 4566â€“4575.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-Instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nZifeng Wang and Jimeng Sun. 2022. Trial2vec: Zero-\nshot clinical trial document similarity search using\nself-supervision. In Findings of EMNLP.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Information\nProcessing Systems.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren,\nXikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining. Advances in\nNeural Information Processing Systems, 35:37309â€“\n37323.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023. RRHF:\nRank responses to align language models with\nhuman feedback without tears. arXiv preprint\narXiv:2304.05302.\n12470\n0\n10\n20\n30\n40\n50BLEU\nAutoTrial FT PT RAG SimCTG\n0\n10\n20\n30METEOR\n0\n10\n20\n30\n40ROUGE\nHealthy\nProstate Cancer\nObesity Diabetes\nMultiple Myeloma\nBreast CancerHypertension\nCoronary Disease\nCondition\n0.0\n0.5\n1.0\n1.5CIDEr\nFigure 6: The generation quality across the trials targeting to the top-8 most frequent diseases/conditions.\n12471\nTable 5: Qualitative generation results of AutoTrial for criteria generation under instructions. Manual Input:\nthe context textual inputs offered by users, where trial setups xs are shared for the same trial and instructions xr\nare specific to each criteria; Automatically Built Input: the reference criteria automatically retrieved and built as\nthe input xe for AutoTrial; Output: results generated by AutoTrial; Groundtruth: the corresponding criteria\nwritten by human clinicians in the original trial documents. The Manual Input and Automatically Built Input will\nbe concatenated as the final input. yellow highlights the instruction tokens; green highlights the setup tokens;\nblue highlights the reference tokens; red highlights the target tokens. Reference texts are truncated in the middle\ndue to the limited space.\nManual Input Automatically Built Input Output Groundtruth\n<instr> <bmi> </instr><title> A\nSingle-dose and Multiple-dose Study\nto Evaluate the Pharmacokinetics and\nPharmacodynamics of DBPR108 Tablets\nin Type 2 Diabetes Mellitus Patients\n<disease>Type 2 Diabetes Mellitus\n<treatment> DBPR108 tablets\n<ref><inc> subjects with bmi of 20-\n45 kg/m2 ... <exc> severe gastrointesti-\nnal diseases: active ulcer, gastrointestinal\nor rectal bleeding, active inflammatory\nbowel syndrome, biliary duct obstruc-\ntion, active gastritis that is not controlled\nby medication, etc.</ref>\n<incs> <inc> Body\nmass index 19 to\n35 kg/m2, inclusive\n</incs>\nBody mass index (BMI)\nwithin the range of 19-35\nkg/m2 (inclusive), BMI\n= weight (kg) / height2\n(m2)\n<instr> <nyha> </instr><title> A\nSingle-dose and Multiple-dose Study\nto Evaluate the Pharmacokinetics and\nPharmacodynamics of DBPR108 Tablets\nin Type 2 Diabetes Mellitus Patients\n<disease>Type 2 Diabetes Mellitus\n<treatment> DBPR108 tablets\n<ref><inc> subjects with bmi of 20-\n45 kg/m2 <inc> subjects with 7%â‰¤\nhba1câ‰¤10% who have been diagnosed\nwith t2dm at least 8 weeks ... <exc> se-\nvere gastrointestinal diseases: active ul-\ncer, gastrointestinal or rectal bleeding,\nactive inflammatory bowel syndrome,\nbiliary duct obstruction, active gastritis\nthat is not controlled by medication, etc.\n</ref>\n<excs> <exc>Heart\nfailure (NYHA class III\nand IV)</excs>\nSerious dysrhythmias,\nobvious left ventricu-\nlar dysfunction, New\nYork Heart Association\n(NYHA) functional class\nIII or IV\n<instr> <sbp> </instr><title> A\nSingle-dose and Multiple-dose Study\nto Evaluate the Pharmacokinetics and\nPharmacodynamics of DBPR108 Tablets\nin Type 2 Diabetes Mellitus Patients\n<disease>Type 2 Diabetes Mellitus\n<treatment> DBPR108 tablets\n<ref><inc> subjects with bmi of 20-\n45 kg/m2 ... <exc> severe gastrointesti-\nnal diseases: active ulcer, gastrointestinal\nor rectal bleeding, active inflammatory\nbowel syndrome, biliary duct obstruc-\ntion, active gastritis that is not controlled\nby medication, etc.</ref>\n<excs> <exc>Subject\nhas hypotension (systolic\nblood pressure < 90\nmmHg) or hypertension\n(systolic blood pressure\nâ‰¥140 mmHg or dias-\ntolic blood pressureâ‰¥\n90 mmHg) at screening\n</excs>\nUncontrolled hyperten-\nsion, systolic pressure\nâ‰¥160 mmHg or diastolic\npressureâ‰¥100 mmHg\n<instr> <cqt> </instr><title> A\nSingle-dose and Multiple-dose Study\nto Evaluate the Pharmacokinetics and\nPharmacodynamics of DBPR108 Tablets\nin Type 2 Diabetes Mellitus Patients\n<disease>Type 2 Diabetes Mellitus\n<treatment> DBPR108 tablets\n<ref><inc> subjects with bmi of 20-\n45 kg/m2 ... <exc> severe gastrointesti-\nnal diseases: active ulcer, gastrointestinal\nor rectal bleeding, active inflammatory\nbowel syndrome, biliary duct obstruc-\ntion, active gastritis that is not controlled\nby medication, etc.</ref>\n<excs> <exc>Patients\nwho had QTc intervalâ‰¥\n450 ms in males orâ‰¥470\nms in females</excs>\nPatients who have the\nsecond or third degree\natrioventricular block,\nlong Q-T syndrome, or\nQTc>500 ms without\ncardiac pacemaker\n<instr> <life_expectancy> </instr>\n<title> Gefitinib Combined With\nChemotherapy or Antiangiogensis in Pa-\ntients With Bim Deletion or Low EGFR\nMutation Abundance<disease>Non-\nsmall-cell Lung Cancer<treatment>\nGefitinib, pemetrexed or gemcitabine\nplus carboplatin, bevacizumab\n<ref><inc> female patients with repro-\nductive potential must have a negative\nserum pregnancy test within 72 hours\nprior to start of study medication. all\nfemale patients of childbearing poten-\ntial, and all male patients, ... <exc>\nknown brain metastases (in case of clin-\nical signs or symptoms of brain metas-\ntases radiological evaluation is manda-\ntory).</ref>\n<incs> <inc>Life ex-\npectancyâ‰¥12 weeks\n</incs>\nLife expectancy of at\nleast 12 weeks\n<instr> <age> </instr><title>Nutri-\nent Synergy in Beef and Stimulation of\nProtein Synthesis in Elderly<disease>\nHealthy<treatment>3 ounces of\ncooked, 85% lean ground beef, 20 grams\nbeef protein isolate\n<ref><inc> bmi 18.5 - 29.9 kg/m2 ...\n<exc> self-reported malabsorption (e.g.\ndifficulty digesting or absorbing nutri-\nents from food, potentially leading to\nbloating, cramping or gas) </ref>\n<incs> <inc>Aged 60\nyears or older</incs>\nAge 60 years or older\n12472"
}