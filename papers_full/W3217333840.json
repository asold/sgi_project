{
  "title": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation",
  "url": "https://openalex.org/W3217333840",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2337553088",
      "name": "Li Wenhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1928286344",
      "name": "Liu Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101529693",
      "name": "Tang, Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1238505444",
      "name": "Wang, Pichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742174613",
      "name": "Van Gool Luc",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3201612248",
    "https://openalex.org/W2962824791",
    "https://openalex.org/W3204351886",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3203796445",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W3203617912",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2797184202",
    "https://openalex.org/W3200418030",
    "https://openalex.org/W3203885900",
    "https://openalex.org/W2611932403",
    "https://openalex.org/W3126541466",
    "https://openalex.org/W2982627166",
    "https://openalex.org/W3034448411",
    "https://openalex.org/W2593146028",
    "https://openalex.org/W3185478863",
    "https://openalex.org/W2799211965",
    "https://openalex.org/W3205327953",
    "https://openalex.org/W2963598138",
    "https://openalex.org/W3169891778",
    "https://openalex.org/W2964291722",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3140576409",
    "https://openalex.org/W3035416506",
    "https://openalex.org/W3173811519",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W3150226983",
    "https://openalex.org/W2293220651",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2554247908",
    "https://openalex.org/W2964016027",
    "https://openalex.org/W3034581612",
    "https://openalex.org/W3022928074",
    "https://openalex.org/W3215666689",
    "https://openalex.org/W3106838237",
    "https://openalex.org/W3175199633",
    "https://openalex.org/W2612706635",
    "https://openalex.org/W2895689136",
    "https://openalex.org/W1579853615",
    "https://openalex.org/W2972662547",
    "https://openalex.org/W2968459013",
    "https://openalex.org/W3106882556"
  ],
  "abstract": "Estimating 3D human poses from monocular videos is a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer (MHFormer) that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, the task is decomposed into three stages: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that MHFormer achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and whistles, its performance surpasses the previous best result by a large margin of 3% on Human3.6M. Code and models are available at \\url{https://github.com/Vegetebird/MHFormer}.",
  "full_text": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation\nWenhao Li1 Hong Liu1,* Hao Tang2 Pichao Wang3 Luc Van Gool2\n1Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University\n2Computer Vision Lab, ETH Zurich 3Alibaba Group\n{wenhaoli,hongliu}@pku.edu.cn\n{hao.tang,vangool}@vision.ee.ethz.ch pichao.wang@alibaba-inc.com\nAbstract\nEstimating 3D human poses from monocular videos is a\nchallenging task due to depth ambiguity and self-occlusion.\nMost existing works attempt to solve both issues by exploiting\nspatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multi-\nple feasible solutions (i.e., hypotheses) exist. To relieve\nthis limitation, we propose a Multi-Hypothesis Transformer\n(MHFormer) that learns spatio-temporal representations of\nmultiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong rela-\ntionships across hypothesis features, the task is decomposed\ninto three stages: (i) Generate multiple initial hypothesis\nrepresentations; (ii) Model self-hypothesis communication,\nmerge multiple hypotheses into a single converged represen-\ntation and then partition it into several diverged hypotheses;\n(iii) Learn cross-hypothesis communication and aggregate\nthe multi-hypothesis features to synthesize the ﬁnal 3D pose.\nThrough the above processes, the ﬁnal representation is en-\nhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-\nof-the-art results on two challenging datasets: Human3.6M\nand MPI-INF-3DHP . Without bells and whistles, its perfor-\nmance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\nhttps://github.com/Vegetebird/MHFormer.\n1. Introduction\n3D human pose estimation (HPE) from monocular videos\nis a fundamental vision task with a wide range of appli-\ncations, such as action recognition [24, 25, 41], human-\ncomputer interaction [7], and augmented/virtual reality [32].\nThis task is typically solved by dividing it into two decoupled\nsubtasks, i.e., 2D pose detection to localize the keypoints\n*Corresponding author: hongliu@pku.edu.cn. This work is supported\nby National Key R&D Program of China (No. 2020AAA0108904), Science\nand Technology Plan of Shenzhen (No. JCYJ20200109140410340).\nInput PoseFormer MHFormer (Ours)\nNovel View\nFigure 1. Given a frame with occluded body parts (right arm and\nelbow), a recent state-of-the-art 3D HPE method, PoseFormer [49],\noutputs a single solution that is inconsistent with the 2D input. In\ncontrast, our MHFormer generates multiple plausible hypotheses\n(different colors) consistent with the 2D evidence and ﬁnally syn-\nthesizes a more accurate 3D pose (green). For easy comparison,\nthe input frame is shown at a novel viewpoint.\non the image plane, followed by 2D-to-3D lifting to infer\njoint locations in the 3D space from 2D keypoints. Despite\ntheir impressive performance [4, 9, 30, 36], it remains an\ninherently ill-posed problem because of self-occlusion and\ndepth ambiguity in 2D representations.\nTo alleviate such issues, most methods [2, 12, 40, 49]\nfocus on exploring spatial and temporal relationships. They\neither employ graph convolutional networks to estimate 3D\nposes with a spatio-temporal graph representation of human\nskeletons [2,12,40] or apply a pure Transformer-based model\nto capture spatial and temporal information from 2D pose\nsequences [49]. Yet, the 2D-to-3D lifting from monocular\nvideos is an inverse problem [1] where multiple feasible\nsolutions (i.e., hypotheses) exist due to its ill-posed nature\ngiven the missing depth [18]. Those approaches ignore this\nproblem and only estimate a single solution, which often\nleads to unsatisfactory results, especially when the person is\nseverely occluded (see Figure 1).\nRecently, a couple of methods [15, 17, 37, 42] that gener-\nate multiple hypotheses have been proposed for the inverse\nproblem. They often rely on the one-to-many mapping by\nadding multiple output heads to an existing architecture with\na shared feature extractor, while failing to build the relation-\nships among the features of different hypotheses. That is an\nimportant shortcoming, as such ability is vital to improve the\n1\narXiv:2111.12707v4  [cs.CV]  28 Jun 2022\nGeneration Convergence SynthesisDivergence\nSelf-Communication\nCross-Communication\n Multi-Hypothesis\nGeneration (MHG)\nSelf-Hypothesis\nRefinement (SHR)\nCross-Hypothesis\nInteraction (CHI) \nFigure 2. The proposed MHFormer constructs a three-stage framework by starting from generating multiple initial representations and then\ncommunicating among them in both independent and mutual ways to synthesize a more precise estimation. For easy illustration, we only\nshow the process of a single-frame 2D pose as input.\nexpressiveness and the performance of the model. In view of\nthe ambiguous inverse problem of 3D HPE, we argue that it\nis more reasonable to conduct a one-to-many mapping ﬁrst\nand then a many-to-one mapping with various intermediate\nhypotheses, as this way can enrich the diversity of features\nand produce a better synthesis for the ﬁnal 3D pose.\nTo this end, we present Multi-Hypothesis Transformer\n(MHFormer), a novel Transformer-based method for 3D\nHPE from monocular videos. The key insight is to allow\nthe model to learn spatio-temporal representations of di-\nverse pose hypotheses. To accomplish this, we introduce a\nthree-stage framework that starts from generating multiple\ninitial representations and gradually communicates across\nthem to synthesize a more accurate prediction, as shown in\nFigure 2. This framework more effectively models multi-\nhypothesis dependencies while also building stronger rela-\ntionships among hypothesis features. Speciﬁcally, in the\nﬁrst stage, a Multi-Hypothesis Generation (MHG) module is\nbuilt to model the intrinsic structure information of human\njoints and generate several multi-level features in the spatial\ndomain. Those features contain diverse semantic informa-\ntion in different depths from shallow to deep and hence can\nbe regarded as initial representations of multiple hypotheses.\nNext, we propose two novel modules to model temporal\nconsistencies and enhance those coarse representations in\nthe temporal domain, which have not been explored by the\nexisting works that generate multiple hypotheses. In the\nsecond stage, a Self-Hypothesis Reﬁnement (SHR) mod-\nule is proposed to reﬁne every single-hypothesis feature.\nThe SHR consists of two new blocks. The ﬁrst block is\na multi-hypothesis self-attention (MH-SA) which models\nsingle-hypothesis dependencies independently to construct\nself-hypothesis communication, enabling message passing\nwithin each hypothesis for feature enhancement. The second\nblock is a hypothesis-mixing multi-layer perceptron (MLP)\nthat exchanges information across hypotheses. The multiple\nhypotheses are merged into a single converged representa-\ntion, and then this representation is partitioned into several\ndiverged hypotheses.\nAlthough those hypotheses are reﬁned by SHR, the con-\nnections across different hypotheses are not strong enough\nsince the MH-SA in the SHR only passes intra-hypothesis\ninformation. To address this issue, in the last stage, a\nCross-Hypothesis Interaction (CHI) module models inter-\nactions among multi-hypothesis features. Its key com-\nponent is the multi-hypothesis cross-attention (MH-CA),\nwhich captures mutual multi-hypothesis correlations to build\ncross-hypothesis communication, enabling message passing\namong hypotheses for better interaction modeling. Subse-\nquently, a hypothesis-mixing MLP is used to aggregate the\nmultiple hypotheses to synthesize the ﬁnal prediction.\nWith the proposed MHFormer, multi-hypothesis spatio-\ntemporal feature hierarchies are explicitly incorporated into\nTransformer models, where the multiple hypothesis infor-\nmation of body joints can be independently and mutually\nprocessed in an end-to-end manner. As a result, the repre-\nsentation ability is potentially enhanced and the synthesized\npose is much more accurate. Our contributions are summa-\nrized as follows:\n• We present a new Transformer-based method, called\nMulti-Hypothesis Transformer (MHFormer), for 3D\nHPE from monocular videos. MHFormer can effec-\ntively learn spatio-temporal representations of multiple\npose hypotheses in an end-to-end manner.\n• We propose to communicate among multi-hypothesis\nfeatures both independently and mutually, providing\npowerful self-hypothesis and cross-hypothesis message\npassing, and strong relationships among hypotheses.\n• Our MHFormer achieves state-of-the-art performance\non two challenging datasets for 3D HPE, signiﬁcantly\noutperforming PoseFormer [49] by 3% with 1.3 mm\nerror reduction on Human3.6M [13].\n2. Related Work\n3D Human Pose Estimation. Existing single-view 3D pose\nestimation methods can be divided into two mainstream\n2\n3L \n2L \nTransformer \nEncoder11\n3434...\n1\n34...\n3D Pose for \nCenter Frame\nCross-Hypothesis Interaction\nMulti-Hypothesis Generation\nRegression Head\n(a) Multi-Hypothesis Transformer (MHFormer) (b) Multi-Hypothesis Generation (MHG)\nSpatial\nPosition\nEmbedding\nLN\nLN\nMLP\nMSA\nTransformer \nEncoder\nTransformer Encoder\nTransformer \nEncoder11\n3434...\n1\n34...\n11\n3434...\n1\n34...\n2D Pose Sequence 2D Pose (e.g., 17 Joints)\nConcatenate \nCoordinate\n(x, y) 1 172 15\n...\n...\nEncoded\nHypothesis\nFeatures\nT\nT\nT\nT\nT\n1\n2\n1\n2\n3\n4\n3\n4\n29\n30\n29\n30\n33\n34\n33\n34\n3 5\n6\n5\n6\n16 31\n32\n31\n32\nSelf-Hypothesis Refinement\nTemporal Embedding\nLN\nHM-MLP\nLN MCA\nLN MCA\nLN MCA\nLN\nHM-MLP\n(c) Self-Hypothesis Refinement (SHR) (d) Cross-Hypothesis Interaction (CHI)\nLN MSA\nLN MSA\nLN MSA\nMulti-Hypothesis Cross-Attention (MH-CA)Multi-Hypothesis Self-Attention (MH-SA)\nN \n1L \nFigure 3. (a) Overview of the proposed Multi-Hypothesis Transformer (MHFormer). (b) Multi-Hypothesis Generation (MHG) module\nextracts the intrinsic structure information of human joints within each frame and generates multiple hypothesis representations. N is the\nnumber of input frames and T is the matrix transposition. (c) Self-Hypothesis Reﬁnement (SHR) module is used to reﬁne single-hypothesis\nfeatures. (d) Cross-Hypothesis Interaction (CHI) module following SHR enables interactions among multi-hypothesis features.\ntypes: one-stage approaches and two-stage ones. One-stage\napproaches directly infer 3D poses from input images with-\nout intermediate 2D pose representations [19, 29, 35, 38],\nwhile two-stage ones ﬁrst obtain 2D keypoints from pre-\ntrained 2D pose detections and then feed them into a 2D-to-\n3D lifting network to estimate 3D poses. Beneﬁting from\nthe excellent performance of 2D human pose estimation, this\n2D-to-3D pose lifting method can efﬁciently and accurately\nregress 3D poses using detected 2D keypoints. For instance,\nSimpleBaseline [30] proposes a fully-connected residual\nnetwork to lift 2D keypoints to 3D joint locations from a\nsingle frame. Anatomy3D [4] decomposes the task into bone\ndirection and bone length predictions to ensure temporal\nconsistency over a sequence. Despite the promising results\nachieved by using temporal correlations from fully convo-\nlutional [4, 26, 36] or graph-based [2, 12, 40] architectures,\nthese methods are less efﬁcient in capturing global-context\ninformation across frames.\nVision Transformers. Recently, Transformer [39] equipped\nwith the powerfully global self-attention mechanism has re-\nceived increasingly research interest in the computer vision\ncommunity [10, 11, 23, 28, 46]. For the basic image classiﬁ-\ncation task, ViT [6] is proposed to apply a standard Trans-\nformer architecture directly to sequential image patches. For\nthe pose estimation task, PoseFormer [49] applies a pure\nTransformer to capture human joint correlations and tem-\nporal dependencies. Strided Transformer [21] introduces\na Transformer-based architecture with strided convolutions\nto lift a long 2D pose sequence to a single 3D pose. Our\nwork is inspired by them and similarly uses the Transformer\nas the basic architecture. But we do not just utilize a sim-\nple architecture with a single representation; instead, the\nseminal ideas of multi-hypothesis and multi-level feature\nhierarchies are connected within Transformers, which makes\nthe model not only expressive but also strong. Besides, a\ncross-attention mechanism is introduced for effective multi-\nhypothesis learning.\nMulti-Hypothesis Methods. Single-view 3D HPE is ill-\nposed and therefore assuming only a single solution might\nbe sub-optimal. Several works generate diverse hypotheses\nfor the inverse problem and achieve substantial performance\ngains [14, 18, 34, 42]. For example, Jahangiri et al. [14]\ngenerated multiple 3D pose candidates consistent with 2D\nkeypoints via a compositional model and anatomical con-\nstraints. Wehrbein et al. [42] modeled the posterior distribu-\ntion of 3D pose hypotheses with normalized ﬂows. Unlike\n3\nthese works that focus on a one-to-many mapping, we learn\na one-to-many mapping ﬁrst and then a many-to-one map-\nping, which allows for the effective modeling of different\nfeatures corresponding to the various hypotheses to improve\nthe representation ability.\n3. Multi-Hypothesis Transformer\nThe overview of the proposed MHFormer is depicted\nin Figure 3 (a). Given a consecutive 2D pose sequence\nestimated by an off-the-shelf 2D pose detector from a video,\nour method aims to reconstruct the 3D pose of the center\nframe by making full use of spatial and temporal information\nin the multi-hypothesis feature hierarchies. To achieve our\nproposed three-stage framework, MHFormer is built upon (i)\nthree major modules: Multi-Hypothesis Generation (MHG),\nSelf-Hypothesis Reﬁnement (SHR), and Cross-Hypothesis\nInteraction (CHI), and (ii) two auxiliary modules: temporal\nembedding and regression head.\n3.1. Preliminary\nIn this work, we adopt a Transformer-based architecture\nsince it performs well in long-range dependency modeling.\nWe ﬁrst give a brief description of the basic components in\nthe Transformer [39], including a multi-head self-attention\n(MSA) and a multi-layer perceptron (MLP).\nMSA. In the MSA, the inputs x∈Rn×d are linearly mapped\nto queries Q∈Rn×d, keys K∈Rn×d, and values V∈Rn×d,\nwhere nis the sequence length, and dis the dimension. The\nscaled dot-product attention can be computed by:\nAttention(Q,K,V ) = Softmax\n(\nQKT /\n√\nd\n)\nV. (1)\nMSA splits the queries, keys, and values for htimes as well\nas performs the attention in parallel. Then, the outputs of h\nattention heads are concatenated.\nMLP.The MLP consists of two linear layers, which are used\nfor non-linearity and feature transformation:\nMLP(x) = σ(xW1 + b1) W2 + b2, (2)\nwhere σdenotes the GELU activation function, W1∈Rd×dm\nand W2∈Rdm×d are the weights of the two linear layers\nrespectively, and b1∈Rdm and b2∈Rd are the bias terms.\n3.2. Multi-Hypothesis Generation\nIn the spatial domain, we address the inverse problem\nby explicitly designing a cascaded Transformer-based archi-\ntecture to generate multiple features in different depths of\nthe latent space. To this end, MHG is introduced to model\nthe human joint relations and initialize the multi-hypothesis\nrepresentations (see Figure 3 (b)). Suppose there are M\ndifferent hypotheses and L1 layers in the MHG, it takes a\nsequence of 2D poses X∈RN×J×2 with N video frames\nand J body joints as input and outputs multiple hypothe-\nses [X1\nL1 ,X2\nL1 ,...,X M\nL1 ], where Xm\nL1 ∈R(J·2)×N is the m-th\nhypothesis.\nMore speciﬁcally, we concatenate the (x,y) coordinates\nof joints for each frame to ¯X∈R(J·2)×N , retain their spatial\ninformation of joints via a learnable spatial position em-\nbedding Em\nSPos ∈R(J·2)×N , and feed the embedded features\ninto the encoders of the MHG. To encourage gradient prop-\nagation, a skip residual connection is applied between the\noriginal input and output features from the encoder. These\nprocedures can be formulated as:\nXm\n0 = LN(Xm) + Em\nSPos ,\nX′m\nl = Xm\nl−1 + MSAm(LN(Xm\nl−1),\nX′′m\nl = X′m\nl + MLPm(LN(X′m\nl ),\nXm\nL1 = Xm + LN(X′′m\nL1 ),\n(3)\nwhere LN(·) is the LayerNorm layer, l∈[1,...,L 1] is the\nindex of MHG layers, X1= ¯X, and Xm=Xm−1\nL1 (m>1).\nThe outputs of the MHG (i.e., Xm\nL1 ) are multi-level features\ncontaining diverse semantic information. Therefore, those\nfeatures can be regarded as initial representations of different\npose hypotheses and need to be further enhanced.\n3.3. Temporal Embedding\nThe MHG helps to generate initial multi-hypothesis fea-\ntures in the spatial domain, whereas the capabilities of such\nfeatures are not strong enough. Considering this limitation,\nwe propose to build relationships across hypothesis features\nand capture temporal dependencies in the temporal domain\nwith two carefully designed modules: an SHR module fol-\nlowed by a CHI module (see Figure 3 (c) and (d)).\nIn order to exploit temporal information, we should ﬁrst\nconvert the spatial domain into the temporal domain. For this\npurpose, the encoded hypothesis features Xm\nL1 of each frame\nare embedded to the high-dimensional features ˜Zm∈RN×C\nusing a transposition operation and a linear embedding,\nwhere C is the embedding dimension. Then, a learnable\ntemporal position embedding Em\nTPos ∈RN×C is utilized to\nretain positional information of frames, which can be formu-\nlated as: ˜Zm\n0 = ˜Zm+Em\nTPos .\n3.4. Self-Hypothesis Reﬁnement\nIn the temporal domain, we ﬁrst construct the SHR to\nreﬁne single-hypothesis features. Each SHR layer consists\nof a multi-hypothesis self-attention (MH-SA) block and a\nhypothesis-mixing MLP block.\nMH-SA. The core of the Transformer model is MSA,\nthrough which any two elements can interact with each other,\nthus modeling long-range dependencies. Instead, our MH-\nSA aims to capture single-hypothesis dependencies within\neach hypothesis independently for self-hypothesis commu-\nnication. Speciﬁcally, the embedded features ˜Zm\n0 ∈RN×C\n4\nCross-Hypothesis FeaturesSingle-Hypothesis Features\nNorm Norm Norm\nLinear Linear Linear\nSoftmax\nMatMul & Scale\nMatMul\nLinear Linear Linear\nSoftmax\nMatMul & Scale\nMatMul\nConcatenation\nLinear\nNorm Norm Norm\nLinear Linear Linear\nSoftmax\nMatMul & Scale\nMatMul\nConcatenation\nLinear\nNorm Norm Norm\nLinear Linear Linear\nSoftmax\nMatMul & Scale\nMatMul\nConcatenation\nLinear\nFigure 4. Left: Multi-head self-attention (MSA). Right: Multi-\nhead cross-attention (MCA).\nof different hypotheses are fed into several parallel MSA\nblocks, which can be expressed as:\n˜Z′m\nl = ˜Zm\nl−1 + MSAm(LN( ˜Zm\nl−1)), (4)\nwhere l∈[1,...,L 2] is the index of SHR layers. Therefore,\nthe message of different hypothesis features can be passed\nin a self-hypothesis way for feature enhancement.\nHypothesis-Mixing MLP.The multiple hypotheses are pro-\ncessed independently in the MH-SA, but there is no informa-\ntion exchange across hypotheses. To handle this issue, we\nadd a hypothesis-mixing MLP after the MH-SA. The fea-\ntures of multiple hypotheses are concatenated and fed into\nthe hypothesis-mixing MLP to merge (i.e., converge) them-\nselves. Then, the converged features are evenly partitioned\n(i.e., diverged) into non-overlapping chunks along the chan-\nnel dimension, forming reﬁned hypothesis representations.\nThe procedure can be formulated as:\n˜Z′\nl= Concat(˜Z′1\nl ,..., ˜Z′M\nl ) ∈RN×(C·M),\nConcat( ˜Z1\nl ,..., ˜ZM\nl )= ˜Z′\nl + HM-MLP(LN(˜Z′\nl)),\n(5)\nwhere Concat(·) is the concatenation operation and\nHM-MLP(·) is the function of hypothesis-mixing MLP\nwhich shares the same format as Eq. (2). This process ex-\nplores the relations among channels of different hypotheses.\n3.5. Cross-Hypothesis Interaction\nWe then model interactions among multi-hypothesis\nfeatures via the CHI, which contains two blocks: multi-\nhypothesis cross-attention (MH-CA) and hypothesis-mixing\nMLP.\nMH-CA. The MH-SA lacks connections across hypotheses,\nwhich limits its interaction modeling. To capture multi-\nhypothesis correlations mutually for cross-hypothesis com-\nmunication, the MH-CA composed of multiple multi-head\ncross-attention (MCA) elements in parallel is proposed.\nThe MCA measures the correlation among cross-\nhypothesis features and has a similar structure to MSA. The\ncommon conﬁguration of MCA uses the same input between\nkeys and values [3, 27, 44]. However, an issue with this con-\nﬁguration is that it will result in more blocks (e.g., 6 MCA\nblocks for 3 hypotheses). Here, we adopt a more efﬁcient\nstrategy, which reduces the number of parameters by using\ndifferent inputs (only require 3 MCA blocks), as shown in\nFigure 4 (Right). The multiple hypotheses Zm are alter-\nnately regarded as queries, keys, and values and fed into the\nMH-CA:\nZ′m\nl =Zm\nl−1+ MCAm(LN(Zm1\nl−1),LN(Zm2\nl−1),LN(Zm\nl−1)),\n(6)\nwhere l∈[1,...,L 3] is the index of CHI layers, Zm\n0 = ˜Zm\nL2 ,\nm1 and m2 are the other two corresponding hypotheses, and\nMCA(Q,K,V ) denotes the function of the MCA. Thanks\nto the MH-CA, the message passing can be performed in a\ncrossing way to signiﬁcantly improve modeling power.\nHypothesis-Mixing MLP. The hypothesis-mixing MLP in\nthe CHI serves as the same function as the process in Eq. (5).\nThe outputs of the MH-CA are fed into it:\nZ′\nl= Concat(Z′1\nl ,...,Z ′M\nl ) ∈RN×(C·M),\nConcat(Z1\nl ,...,Z M\nl )=Z′\nl + HM-MLP(LN(Z′\nl)).\n(7)\nIn the hypothesis-mixing MLP of the last CHI layer, the\npartition operation is not used so that the features of all\nhypotheses are ﬁnally aggregated to synthesize a single hy-\npothesis representation ZL3 ∈RN×(C·M).\n3.6. Regression Head\nIn the regression head, a linear transformation layer is\napplied on the output ZL3 to perform regression to produce\nthe 3D pose sequence ˜X∈RN×J×3. Finally, the 3D pose of\nthe center frame ˆX∈RJ×3 is selected from ˜X as our ﬁnal\nprediction.\n3.7. Loss Function\nThe entire model is trained in an end-to-end manner with\na Mean Squared Error (MSE) loss, which is applied to mini-\nmize the error between the estimated and ground truth poses:\nL=\nN∑\nn=1\nJ∑\ni=1\nYn\ni −˜Xn\ni\n\n2\n, (8)\nwhere ˜Xn\ni and Yn\ni represent the predicted and ground truth\n3D poses of joint iat frame n, respectively.\n5\nTable 1. Quantitative comparison with the state-of-the-art methods on Human3.6M under Protocol 1, using detected 2D poses (top) and\nground truth 2D poses (bottom) as inputs. (†) - uses temporal information. Blod: best; Underlined: second best.\nMethod Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nFanget al.(AAAI’18) [8] 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.8 88.6 60.3 57.7 62.7 47.5 50.6 60.4\nGraphSH (CVPR’21) [45]45.2 49.9 47.5 50.9 54.9 66.1 48.5 46.3 59.7 71.5 51.4 48.6 53.9 39.9 44.1 51.9\nMGCN (ICCV’21) [50] 45.4 49.2 45.7 49.4 50.4 58.2 47.9 46.0 57.5 63.0 49.7 46.6 52.2 38.9 40.8 49.4\nST-GCN (ICCV’19) [2](†) 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4 48.8\nVPose (CVPR’19) [36](†) 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8\nSGNN (ICCV’21) [48](†) - - - - - - - - - - - - - - - 45.7\nUGCN (ECCV’20) [40](†) 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8 45.6\nLiuet al.(CVPR’20) [26](†) 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2 45.1\nPoseFormer (ICCV’21) [49](†) 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2 44.3\nAnatomy3D (TCSVT’21) [4](†) 41.4 43.2 40.1 42.9 46.6 51.9 41.7 42.3 53.9 60.2 45.4 41.7 46.0 31.5 32.7 44.1\nMHFormer (Ours)(†) 39.2 43.1 40.1 40.9 44.9 51.2 40.6 41.3 53.5 60.3 43.7 41.1 43.8 29.8 30.6 43.0\nMethod Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nP-LSTM (ECCV’18) [16](†) 32.1 36.6 34.3 37.8 44.5 49.9 40.9 36.2 44.1 45.6 35.3 35.9 30.3 37.6 35.5 38.4\nPoseAug (CVPR’21) [9] - - - - - - - - - - - - - - - 38.2\nVPose (CVPR’19) [36](†) 35.2 40.2 32.7 35.7 38.2 45.5 40.6 36.1 48.8 47.3 37.8 39.7 38.7 27.8 29.5 37.8\nLiuet al.(CVPR’20) [26](†) 34.5 37.1 33.6 34.2 32.9 37.1 39.6 35.8 40.7 41.4 33.0 33.8 33.0 26.6 26.9 34.7\nAnatomy3D (TCSVT’21) [4](†) - - - - - - - - - - - - - - - 32.3\nSRNet (ECCV’20) [47](†) 34.8 32.1 28.5 30.7 31.4 36.9 35.6 30.5 38.9 40.5 32.5 31.0 29.9 22.5 24.5 32.0\nPoseFormer (ICCV’21) [49](†) 30.0 33.6 29.9 31.0 30.2 33.3 34.8 31.4 37.8 38.6 31.7 31.5 29.0 23.3 23.1 31.3\nMHFormer (Ours)(†) 27.7 32.1 29.1 28.9 30.0 33.9 33.0 31.2 37.0 39.3 30.0 31.0 29.4 22.2 23.0 30.5\nTable 2. Comparison with the methods of generating multiple 3D\npose hypotheses on Human3.6M. The number of hypotheses is\ndenoted as M. Blod: best; Underlined: second best.\nMethod M MPJPE (mm)\nLi et al. (CVPR’19) [17] 5 52.7\nSharma et al. (ICCV’19) [37] 200 46.8\nOikarinen (IJCNN’21) [34] 200 46.2\nWehrbein et al. (ICCV’21) [42] 200 44.3\nMHFormer (Ours) 3 43.0\n4. Experiments\n4.1. Datasets and Evaluation Metrics\nWe evaluate our method on two widely-used datasets for\n3D HPE: Human3.6M [13] and MPI-INF-3DHP [31].\nHuman3.6M. The Human3.6M dataset [13] is the largest\nand most representative benchmark for 3D HPE. This dataset\nconsists of 3.6 million images captured from four synchro-\nnized cameras at 50 Hz. There are 15 daily activities per-\nformed by 11 human subjects in an indoor environment.\nFollowing previous works [4, 26, 36, 40], we train a single\nmodel on ﬁve subjects (S1, S5, S6, S7, S8) and test it on\ntwo subjects (S9 and S11). We adopt the most commonly\nused evaluation protocols: Protocol 1 is the MPJPE which\nmeasures the mean Euclidean distance between the ground\ntruth and estimated joints in millimeters; Protocol 2 is the\nMPJPE after aligning the predicted 3D pose with the ground\ntruth using translation, rotation, and scale (P-MPJPE).\nMPI-INF-3DHP. The MPI-INF-3DHP [31] is a large 3D\npose dataset in both indoor and outdoor environments. This\ndataset provides 1.3 million frames, containing more diverse\nmotions than Human3.6M. Following the setting in [4, 22,\n31, 49], we report metrics of MPJPE, Percentage of Correct\nKeypoint (PCK) with the threshold of 150 mm, and Area\nUnder Curve (AUC) for a range of PCK thresholds.\n4.2. Implementation Details\nIn our implementation, the proposed MHFormer con-\ntains L1=4 MHG, L2=2 SHR, and L3=1 CHI layers. The\nMHFormer model is implemented in PyTorch framework\non one GeForce RTX 3090 GPU. We train our model in an\nend-to-end manner from scratch using Amsgrad optimizer.\nThe initial learning rate is set to 0.001 with a shrink factor of\n0.95 applied after each epoch and 0.5 after every 5 epochs.\nFor a fair comparison, the same horizontal ﬂip augmentation\nis adopted following [2, 4, 36, 49]. We perform the 2D pose\ndetection using cascaded pyramid network (CPN) [5] for\nHuman3.6M following [2, 26, 36] and ground truth 2D pose\nfor MPI-INF-3DHP following [4, 22, 49].\n4.3. Comparison with State-of-the-Art Methods\nResults on Human3.6M. The proposed MHFormer is com-\npared with the state-of-the-art methods on Human3.6M. The\nresults of our model with a receptive ﬁeld of 351 frames us-\ning 2D detected inputs [5] are reported in Table 8 (top). With-\nout bells and whistles, our MHFormer outperforms all pre-\nvious state-of-the-art methods by a large margin under both\nProtocol 1 (43.0 mm) and Protocol 2 (34.4 mm, see supple-\nmental material). Compared to the very recent Transformer-\nbased method, i.e., PoseFormer [49], MHFormer noticeably\nsurpasses it by 1.3 mm in MPJPE (relative 3% improve-\nment). Figure 5 shows the qualitative comparison with the\nPoseFormer and the baseline model (same architecture as\nViT [6]) on some challenging poses. To further explore the\nlower bound of our method, we compared our MHFormer\nwith the state-of-the-art methods with ground truth 2D poses\n6\nInput PoseFormer Baseline MHFormer Ground Truth\n Input PoseFormer Baseline MHFormer Ground Truth\nFigure 5. Qualitative comparison among the proposed method (MHFormer), the baseline method, and the previous state-of-the-art method\n(PoseFormer) [49] on Human3.6M dataset. Wrong estimations are highlighted by yellow arrows.\nTable 3. Quantitative comparison with the state-of-the-art methods\non MPI-INF-3DHP. Best in bold, second best underlined.\nMethod PCK ↑ AUC ↑ MPJPE ↓\nMehta et al. (3DV’17) [31] 75.7 39.3 117.6\nLin et al. (BMVC’19) [22] 83.6 51.4 79.8\nVPose (CVPR’19) [36] 86.0 51.9 84.0\nLi et al. (CVPR’20) [20] 81.2 46.1 99.7\nAnatomy3D (TCSVT’21) [4] 87.9 54.0 78.8\nPoseFormer (ICCV’21) [49] 88.6 56.4 77.1\nMHFormer (Ours) 93.8 63.3 58.0\nTable 4. Ablation study on different receptive ﬁelds with MPJPE\n(mm). CPN - cascaded pyramid network; GT - 2D ground truth.\n9 27 81 243 351\nCPN 47.8 45.9 44.5 43.2 43.0\nGT 36.6 34.3 32.7 30.9 30.5\nas inputs. The results are shown in Table 8 (bottom). It can\nbe seen that our method achieves the best performance (30.5\nmmin MPJPE), outperforming all other methods.\nAdditionally, our method is compared with previous meth-\nods of generating multiple 3D pose hypotheses. The results\nare shown in Table 2. It is noteworthy that these methods\nreport metrics for the best hypothesis due to the adopted one-\nto-many mapping, while our method reports metrics with a\nspeciﬁc solution by learning a deterministic mapping, which\nis much more practical in reality. Even though we use much\nfewer hypothesis numbers (3 vs. 200), our proposed method\nconsistently outperforms previous works.\nResults on MPI-INF-3DHP. To assess the generalization\nability, we evaluate our method on MPI-INF-3DHP dataset.\nFollowing [49], we use 2D pose sequences of 9 frames as our\nmodel input due to the fewer samples and shorter sequence\nlengths of this dataset compared to Human3.6M. The results\nin Table 3 demonstrate that our method achieves the best\nperformance on all metrics (PCK, AUC, and MPJPE). It\nemphasizes the effectiveness of our MHFormer in improving\nperformance in outdoor scenes.\n4.4. Ablation Study\nTo verify the impact of each component and design in the\nproposed model, we conduct extensive ablation experiments\nTable 5. Ablation study on different parameters of MHG. Here, L1\nis the number of MHG layers and M is the hypothesis number.\nM L 1 Params (M) FLOPs (G) MPJPE ( mm)\n3 2 18.91 1.03 46.4\n3 3 18.92 1.03 46.3\n3 4 18.92 1.03 45.9\n3 5 18.93 1.04 46.1\n1 4 6.32 0.34 47.6\n2 4 12.61 0.69 46.7\n3 4 18.92 1.03 45.9\n4 4 25.22 1.38 46.9\non Human3.6M dataset under Protocol 1 with MPJPE.\nImpact of Receptive Fields. For the video-based 3D HPE\ntask, a large receptive ﬁeld is essential for estimation accu-\nracy. Table 4 shows the results of our method with different\ninput frames. It can be seen that our method obtains larger\ngains with more frames fed into the model. The error has a\ngreat decrease of 16.7% from 9-frames to 351-frames with\nGT input, which indicates the effectiveness of our method\nin capturing long-range dependencies across frames with a\nlarge receptive ﬁeld. Next, ablations in the following parts\nare carried out using a receptive ﬁeld of 27 frames to balance\nthe computation efﬁciency and performance.\nImpact of Parameters in MHG. In the top part of Table 5,\nwe report the results with different numbers of MHG lay-\ners. Experiments show that stacking more layers in MHG\ncan slightly improve the performance with few parameter\nincreases, but the gain disappears when the layer number is\nlarger than 4. Moreover, we investigate the inﬂuence of us-\ning different numbers of hypotheses in MHG. The results are\nshown in the bottom part of Table 5. Increasing the number\nof hypotheses can improve the result, but the performance\nsaturates when using 3 hypothesis representations. Notably,\nour model equipped with 3 hypotheses shows signiﬁcant\ngains over the single-hypothesis model with 1.7 mmerror\nreduction. This demonstrates that exploiting different repre-\nsentations of multiple pose hypotheses is helpful to improve\nthe performance of the model, validating our motivation.\nImpact of Parameters in SHR and CHI. Table 6 reports\nhow the different parameters of SHR and CHI impact the\nperformance and computation complexity of our model. The\n7\n(a) Depth Ambiguity (b) Self-Occlusion (c) 2D Detector Uncertainty\nFigure 6. Diverse 3D pose hypotheses generated by MHFormer. For easy illustration, we color-code the hypotheses to show the difference\namong them. Green colored 3D pose corresponds to the ﬁnal synthesized estimation of our method.\nTable 6. Ablation study on different parameters of SHR and CHI.\nHere, L2 and L3 indicate the number of SHR and CHI layers,\nrespectively. Cis the embedding dimension.\nL2 L3 C Params (M) FLOPs (G) MPJPE ( mm)\n2 1 256 4.72 0.26 47.2\n2 1 384 10.65 0.58 46.4\n2 1 512 18.92 1.03 45.9\n2 1 768 42.50 2.31 47.4\n2 1 512 18.92 1.03 45.9\n1 3 512 25.20 1.38 46.7\n2 2 512 25.20 1.38 46.8\n3 1 512 25.20 1.38 46.3\nresults show that enlarging the embedding dimension from\n256 to 512 can boost the performance, but using dimensions\nlarger than 512 cannot bring further improvements. In ad-\ndition, we observe no more gains by stacking either more\nSHR or CHI layers. Therefore, the optimal parameters for\nour model are L2=2, L3=1, and C=512.\nEffect of Model Components. In Table 7, we carry out\nexperiments to quantify the inﬂuence of our proposed com-\nponents. Firstly, we compare our method with the baseline\nmodel. For a fair comparison, the results of the baseline are\nreported at the same number of layers as MHFormer with dif-\nferent embedding dimensions, since our hypothesis-mixing\nMLP in MHFormer takes concatenated hypothesis features\nas inputs (the dimension is 512×3=1536). The results show\nthat the baseline model is prone to overﬁtting due to the ex-\ncessive number of parameters, whereas our method performs\nwell. Additionally, it can be seen that our MHFormer built\nupon MHG, SHR, and CHI outperforms varying variants of\nbaseline models (1.9 mmimprovement). Then, when we in-\ncorporate multi-hypothesis representations and SHR or CHI\nwithin the baseline, the performance has signiﬁcant gains\n(-1.3 mmfor MHG-SHR and -1.0 mmfor MHG-CHI). Be-\nsides, we remove the MHG in MHFormer (SHR-CHI). At\nthis point, the model only captures temporal information\nand its error heavily increases by 1.3 mm. These ablations\nindicate that learning multi-hypothesis spatio-temporal rep-\nresentations is signiﬁcant for 3D HPE, and the different\nhypothesis representations should be modeled in both inde-\npendent and mutual ways.\nWe also explore the use of multi-level features in MHG\nby simply building the MHG upon several parallel Trans-\nformer encoders (MHFormer ∗). As shown in the table, our\nTable 7. Ablation study on different components of our MHFormer.\nHere, ∗ means no multi-level features in MHG.\nMethod MHG SHR CHI MPJPE (mm)\nBaseline (C=256) \u0017 \u0017 \u0017 49.9\nBaseline (C=512) \u0017 \u0017 \u0017 47.8\nBaseline (C=1536) \u0017 \u0017 \u0017 50.4\nSHR-CHI \u0017 \u0013 \u0013 47.2\nMHG-SHR \u0013 \u0013 \u0017 46.5\nMHG-CHI \u0013 \u0017 \u0013 46.8\nMHFormer ∗ \u0013 \u0013 \u0013 46.5\nMHFormer (Ours) \u0013 \u0013 \u0013 45.9\nMHFormer equipped with the multi-level features increases\nthe performance, which indicates that the multi-level features\ncan bring valuable information to the ﬁnal estimation.\n5. Qualitative Results\nAlthough our method does not aim to produce multiple\n3D pose predictions, for better observation, we add addi-\ntional regression layers and ﬁnetune our model to visualize\nthe intermediate hypotheses. The several qualitative results\nare shown in Figure 6. It can be seen that our method is\nable to generate different plausible 3D pose solutions, es-\npecially for ambiguous body parts with depth ambiguity,\nself-occlusion, and 2D detector uncertainty. Moreover, the\nﬁnal 3D pose synthesized by aggregating multi-hypothesis\ninformation is more reasonable and accurate.\n6. Conclusion\nThis paper presents Multi-Hypothesis Transformer\n(MHFormer), a new Transformer-based three-stage frame-\nwork for the ambiguous inverse problem of 3D HPE from\nmonocular videos. MHFormer ﬁrst generates initial repre-\nsentations of multiple pose hypotheses in the spatial domain\nand then communicates across them in both independent and\nmutual ways in the temporal domain. Extensive experiments\nshow that the proposed MHFormer has a fundamental ad-\nvantage over single-hypothesis Transformers and achieves\nstate-of-the-art performance on two benchmark datasets. We\nhope that our approach will foster further research in 2D-to-\n3D pose lifting considering various ambiguities.\nLimitation. One limitation of our method is the relatively\nlarger computational complexity. The excellent performance\nof Transformers comes at a price of high computational cost.\n8\nReferences\n[1] Christopher M. Bishop. Mixture density networks. Technical\nreport, Aston University, 1994.\n[2] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham,\nJunsong Yuan, and Nadia Magnenat Thalmann. Exploit-\ning spatial-temporal relationships for 3D pose estimation via\ngraph convolutional networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages\n2272–2281, 2019.\n[3] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.\nCrossViT: Cross-attention multi-scale vision transformer for\nimage classiﬁcation. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (ICCV), pages 357–\n366, 2021.\n[4] Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili\nChen, and Jiebo Luo. Anatomy-aware 3D human pose estima-\ntion with bone-based pose decomposition. IEEE Transactions\non Circuits and Systems for Video Technology, 32(1):198–209,\n2021.\n[5] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang,\nGang Yu, and Jian Sun. Cascaded pyramid network for multi-\nperson pose estimation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 7103–7112, 2018.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n[7] Andrew Errity. Human–computer interaction. In An Introduc-\ntion to Cyberpsychology, pages 263–278. 2016.\n[8] Hao-Shu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and\nSong-Chun Zhu. Learning pose grammar to encode human\nbody conﬁguration for 3D pose estimation. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 32,\n2018.\n[9] Kehong Gong, Jianfeng Zhang, and Jiashi Feng. PoseAug: A\ndifferentiable pose augmentation framework for 3D human\npose estimation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8575–8584, 2021.\n[10] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. Advances in\nNeural Information Processing Systems (NeurIPS), 34:15908–\n15919, 2021.\n[11] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. TransReID: Transformer-based object Re-\nIdentiﬁcation. In Proceedings of the IEEE International Con-\nference on Computer Vision (ICCV) , pages 15013–15022,\n2021.\n[12] Wenbo Hu, Changgong Zhang, Fangneng Zhan, Lei Zhang,\nand Tien-Tsin Wong. Conditional directed graph convolution\nfor 3D human pose estimation. In Proceedings of the ACM\nInternational Conference on Multimedia (ACMMM), pages\n602–611, 2021.\n[13] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 36(7):1325–1339, 2013.\n[14] Ehsan Jahangiri and Alan L Yuille. Generating multiple\ndiverse hypotheses for human 3D pose consistent with 2D\njoint detections. In Proceedings of the IEEE International\nConference on Computer Vision Workshops (ICCVW), pages\n805–814, 2017.\n[15] Rawal Khirodkar, Visesh Chari, Amit Agrawal, and Ambrish\nTyagi. Multi-instance pose networks: Rethinking top-down\npose estimation. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), pages 3122–3131,\n2021.\n[16] Kyoungoh Lee, Inwoong Lee, and Sanghoon Lee. Propa-\ngating LSTM: 3D pose estimation based on joint interde-\npendency. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 119–135, 2018.\n[17] Chen Li and Gim Hee Lee. Generating multiple hypotheses\nfor 3D human pose estimation with mixture density network.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 9887–9895, 2019.\n[18] Chen Li and Gim Hee Lee. Weakly supervised generative net-\nwork for multiple 3D human pose hypotheses. InProceedings\nof the British Machine Vision Conference (BMVC), 2020.\n[19] Sijin Li and Antoni B Chan. 3D human pose estimation from\nmonocular images with deep convolutional neural network.\nIn Proceedings of the Asian Conference on Computer Vision\n(ACCV), pages 332–347, 2014.\n[20] Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung\nTang, and Kwang-Ting Cheng. Cascaded deep monocular 3D\nhuman pose estimation with evolutionary training data. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 6173–6183, 2020.\n[21] Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao\nWang, and Wenming Yang. Exploiting temporal contexts\nwith strided transformer for 3D human pose estimation. IEEE\nTransactions on Multimedia, 2022.\n[22] Jiahao Lin and Gim Hee Lee. Trajectory space factorization\nfor deep video-based 3D human pose estimation. In Pro-\nceedings of the British Machine Vision Conference (BMVC),\n2019.\n[23] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human\npose and mesh reconstruction with transformers. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1954–1963, 2021.\n[24] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skeleton\nvisualization for view invariant human action recognition.\nPattern Recognition, 68:346–362, 2017.\n[25] Mengyuan Liu and Junsong Yuan. Recognizing human ac-\ntions as the evolution of pose estimation maps. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1159–1168, 2018.\n[26] Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung,\nand Vijayan Asari. Attention mechanism exploits temporal\ncontexts: Real-time 3D human pose reconstruction. In Pro-\n9\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5064–5073, 2020.\n[27] Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu,\nXuesheng Qian, and Xiaoyun Yang. A video is worth three\nviews: Trigeminal transformers for video-based person Re-\nIdentiﬁcation. arXiv preprint arXiv:2104.01745, 2021.\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE International Conference on Com-\nputer Vision (ICCV), pages 10012–10022, 2021.\n[29] Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Hai Ci, and Yizhou\nWang. Context modeling in 3D human pose estimation: A\nuniﬁed perspective. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n6238–6247, 2021.\n[30] Julieta Martinez, Rayat Hossain, Javier Romero, and James J\nLittle. A simple yet effective baseline for 3D human pose esti-\nmation. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 2640–2649, 2017.\n[31] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Olek-\nsandr Sotnychenko, Weipeng Xu, and Christian Theobalt.\nMonocular 3D human pose estimation in the wild using im-\nproved cnn supervision. In Proceedings of the International\nConference on 3D Vision (3DV), pages 506–516, 2017.\n[32] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,\nHelge Rhodin, Mohammad Shaﬁei, Hans-Peter Seidel,\nWeipeng Xu, Dan Casas, and Christian Theobalt. VNect:\nReal-time 3D human pose estimation with a single rgb cam-\nera. ACM Transactions on Graphics, 36(4):1–14, 2017.\n[33] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation. In Proceedings of\nthe European Conference on Computer Vision (ECCV), pages\n483–499, 2016.\n[34] Tuomas Oikarinen, Daniel Hannah, and Sohrob Kazerounian.\nGraphMDN: Leveraging graph structure and deep learning to\nsolve inverse problems. In Proceedings of the International\nJoint Conference on Neural Networks (IJCNN), pages 1–9,\n2021.\n[35] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpanis,\nand Kostas Daniilidis. Coarse-to-ﬁne volumetric prediction\nfor single-image 3D human pose. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 7025–7034, 2017.\n[36] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and\nMichael Auli. 3D human pose estimation in video with tem-\nporal convolutions and semi-supervised training. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 7753–7762, 2019.\n[37] Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Ab-\nhishek Sharma, and Arjun Jain. Monocular 3D human pose\nestimation by generation and ordinal ranking. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), pages 2325–2334, 2019.\n[38] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen\nWei. Integral human pose regression. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n529–545, 2018.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NIPS), pages 5998–6008,\n2017.\n[40] Jingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin. Mo-\ntion guided 3D pose estimation from videos. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 764–780, 2020.\n[41] Pichao Wang, Wanqing Li, Zhimin Gao, Chang Tang, and\nPhilip O Ogunbona. Depth pooling based large-scale 3-d\naction recognition with convolutional neural networks. IEEE\nTransactions on Multimedia, 20(5):1051–1061, 2018.\n[42] Tom Wehrbein, Marco Rudolph, Bodo Rosenhahn, and Bas-\ntian Wandt. Probabilistic monocular 3D human pose esti-\nmation with normalizing ﬂows. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages\n11199–11208, 2021.\n[43] Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xi-\naokang Yang, and Wenjun Zhang. Deep kinematics analysis\nfor monocular 3D human pose estimation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 899–908, 2020.\n[44] Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao\nLi, and Rong Jin. CDTrans: Cross-domain transformer for\nunsupervised domain adaptation. In Proceedings of the In-\nternational Conference on Learning Representations (ICLR),\n2021.\n[45] Tianhan Xu and Wataru Takano. Graph stacked hourglass\nnetworks for 3D human pose estimation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 16105–16114, 2021.\n[46] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and\nElisa Ricci. Transformer-based attention networks for con-\ntinuous pixel-wise prediction. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages\n16269–16279, 2021.\n[47] Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang\nXu, and Stephen Lin. SRNet: Improving generalization in 3D\nhuman pose estimation with a split-and-recombine approach.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), pages 507–523, 2020.\n[48] Ailing Zeng, Xiao Sun, Lei Yang, Nanxuan Zhao, Minhao\nLiu, and Qiang Xu. Learning skeletal graph neural networks\nfor hard 3D pose estimation. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), pages\n11436–11445, 2021.\n[49] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang,\nChen Chen, and Zhengming Ding. 3D human pose estimation\nwith spatial and temporal transformers. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV),\n2021.\n[50] Zhiming Zou and Wei Tang. Modulated graph convolutional\nnetwork for 3D human pose estimation. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV),\npages 11477–11487, 2021.\n10\nSupplementary Material\nThis supplementary material contains the following de-\ntails: (1) A brief description of multi-head cross-attention.\n(2) Additional quantitative results. (3) Additional ablation\nstudies. (4) Additional visualization results.\nA. Multi-Head Cross-Attention\nIn Sec. 3.1 of our main manuscript, we give a brief\ndescription of the multi-head self-attention (MSA) block.\nGiven the inputs x∈Rn×d, they are linearly mapped to\nqueries Q∈Rn×d, keys K∈Rn×d, and valuesV∈Rn×d. The\nscaled dot-product attention in the MSA can be computed\nby:\nAttention(Q,K,V ) = Softmax\n(QKT\n√\nd\n)\nV. (9)\nIn this section, we further deﬁne the multi-head cross-\nattention (MCA) among three tensors,x, y, and z. The inputs\nx∈Rn×d, y∈Rn×d, and z∈Rn×d are linearly mapped to\nqueries Qx∈Rn×d, keys Ky∈Rn×d, and values Vz∈Rn×d,\nrespectively. The scaled dot-product attention in the MCA\ncan be computed by:\nAttentioncross(Qx,Ky,Vz) = Softmax\n(\nQxKT\ny√\nd\n)\nVz.\n(10)\nThe common conﬁguration of MCA uses the same input\nbetween keys and values [3, 27, 44], i.e., the inputs x ̸=\ny= z. Instead, we adopt a more efﬁcient strategy by using\ndifferent inputs, i.e., the inputs x̸= y̸= z.\nB. Additional Quantitative Results\nTable 8 shows the results of our proposed MHFormer\non Human3.6M under Protocol 2. The input 2D poses\nare estimated by CPN [5]. Without bells and whistles, our\nMHFormer achieves promising results that outperform the\nstate-of-the-art approaches.\nSeveral methods [2, 40, 50] adopt a pose reﬁnement mod-\nule, which is ﬁrst proposed by ST-GCN [2], to further im-\nprove the estimation accuracy. Following [2], we adopt the\nreﬁne module and the results are shown in Table 9. It can be\nseen that our method can use the reﬁne module to improve\nthe performance, achieving an error of 42.4 mmin MPJPE\nwhich surpasses all other approaches by a large margin.\nC. Additional Ablation Studies\nEffect of Model Components. Here, we give more details\nabout how to build the different variants of MHFormer in\nTable 7 of our main manuscript:\n• Baseline: The baseline model contains 3 layers for\nstandard Transformer encoder (same architecture as\nViT [6]).\n• SHR-CHI: We remove the MHG module. SHR-CHI\ncontains L2=2 SHR and L3=1 CHI layers.\n• MHG-SHR: We replace the CHI layers in MHFormer\nwith SHR layers. MHG-SHR contains L1=4 MHG and\nL3=3 SHR layers.\n• MHG-CHI: We replace the SHR layers in MHFormer\nwith CHI layers. SHR-CHI contains L1=4 MHG and\nL3=3 CHI layers.\n• MHFormer ∗: The MHG in MHFormer is simply built\nupon several parallel Transformer encoders.\n• MHFormer: Our proposed method that contains L1=4\nMHG, L2=2 SHR, and L3=1 CHI layers. Please refer\nto Figure 3 in our main manuscript.\nImpact of Conﬁgurations in MH-CA. As mentioned in\nSec. 3.5 of our main manuscript, the common conﬁgura-\ntion of MCA uses the same input between keys and val-\nues [3, 27, 44], which will result in more blocks. We adopt a\nmore efﬁcient conﬁguration by using different inputs among\nqueries, keys, and values. The performance and computa-\ntional complexity of these two conﬁgurations are given in\nTable 10. We can see that using the same input between keys\nand values in MH-CA (MH-CA ∗) requires more parameters\nand FLOPs but cannot bring further performance gains. It\nillustrates the effectiveness of our efﬁcient strategy in MCA.\nImpact of Receptive Fields. For the video-based 3D human\npose estimation task, the number of receptive ﬁelds directly\ninﬂuences the estimation results. Figure 7 (a) shows the\nresults of our model with different receptive ﬁelds (between\n1 and 351) on Human3.6M. Increasing the receptive ﬁeld can\nimprove the result under both CPN and GT 2D pose inputs,\nwhich demonstrates the great power of our method in long-\nrange dependency modeling with a long input sequence.\nImpact of 2D Detections. To show the effectiveness of our\nmethod on different 2D pose detectors, we carry out experi-\nments with the detections from Stack Hourglass (SH) [33],\nDetectron [36], and CPN [5]. In addition, to evaluate the\nrobustness of our method to various levels of noise, we also\nconduct experiments on 2D ground truth plus different levels\nof additive Gaussian noise. The results are shown in Figure 7\n(b). It can be observed that the curve has a nearly linear\nrelationship between MPJPE of 3D poses and two-norm\nerrors of 2D poses. These experiments validate both the\neffectiveness and robustness of our proposed method.\nD. Additional Visualization Results\n3D Reconstruction Visualization. Figure 8 and Figure 9\nshow qualitative results of our method on Human3.6M\ndataset, MPI-INF-3DHP dataset, and challenging in-the-\nwild videos. Moreover, Figure 10 shows the qualitative\ncomparison with the baseline method and the previous state-\n11\nTable 8. Quantitative comparison with the state-of-the-art methods on Human3.6M under Protocol 2. (†) - uses temporal information. Blod:\nbest; Underlined: second best.\nMethod Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.\nSimpleBaseline (ICCV’17) [30]39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7\nFanget al.(AAAI’18) [8] 38.2 41.7 43.7 44.9 48.5 55.3 40.2 38.2 54.5 64.4 47.2 44.3 47.3 36.7 41.7 45.7\nPoseAug (CVPR’21) [9] - - - - - - - - - - - - - - - 39.1\nSGNN (ICCV’21) [48] 33.9 37.2 36.8 38.1 38.7 43.5 37.8 35.0 47.2 53.8 40.7 38.3 41.8 30.1 31.4 39.0\nST-GCN (ICCV’19) [2](†) 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 33.2 39.0\nVPoseet al.(CVPR’19) [36](†) 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5\nXuet al.(CVPR’20) [43](†) 31.0 34.8 34.7 34.4 36.2 43.9 31.6 33.5 42.3 49.0 37.1 33.0 39.1 26.9 31.9 36.2\nLiuet al.(CVPR’20) [26](†) 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.4 37.0 25.2 27.2 35.6\nUGCN (ECCV’20) [40](†) 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5\nAnatomy3D (TCSVT’21) [4](†) 32.6 35.1 32.8 35.4 36.3 40.4 32.4 32.3 42.7 49.0 36.8 32.4 36.0 24.9 26.5 35.0\nPoseFormer (ICCV’21) [49](†) 32.5 34.8 32.6 34.6 35.3 39.5 32.1 32.0 42.8 48.5 34.8 32.4 35.3 24.5 26.0 34.6\nMHFormer (Ours)(†) 31.5 34.9 32.8 33.6 35.3 39.6 32.0 32.2 43.5 48.7 36.4 32.6 34.3 23.9 25.1 34.4\nTable 9. Quantitative comparison on Human3.6M under MPJPE.\nBlod: best; Underlined: second best.\nMethod Reﬁne module MPJPE ( mm)\nMGCN (ICCV’21) [50] \u0013 49.4\nST-GCN (ICCV’19) [2] \u0013 48.8\nUGCN (ECCV’20) [40] 45.6\nUGCN (ECCV’20) [40] \u0013 44.5\nMHFormer (Ours) 43.0\nMHFormer (Ours) \u0013 42.4\nTable 10. Ablation study on different conﬁgurations of MH-CA on\nHuman3.6M under MPJPE. Here, ∗ means using the same input\nbetween keys and values in MH-CA.\nMethod Params (M) FLOPs (G) MPJPE ( mm)\nMH-CA ∗ 22.07 1.21 46.1\nMH-CA 18.92 1.03 45.9\nof-the-art method (PoseFormer [49]) on some wild videos.\nIt can be seen that our method can produce more accurate\nand reasonable 3D poses, especially when the human action\nis complex and rare.\nHypothesis Visualization. For visualization purposes, we\nadd additional regression layers and ﬁnetune our model to\noutput intermediate hypotheses. Figure 11 shows the visual-\nization results of intermediate 3D pose hypotheses generated\nby our proposed method. We can see that our MHFormer can\ngenerate different plausible 3D pose solutions, especially for\nambiguous body parts with depth ambiguity, self-occlusion,\nand 2D detector uncertainty.\nAttention Visualization. Visualization results of the multi-\nhead attention maps of the ﬁrst layers from the Multi-\nHypothesis Generation (MHG) module and Self-Hypothesis\nReﬁnement (SHR) module (351-frame model with 3 hy-\npotheses) are shown in Figure 12 and Figure 13, respectively.\nIt can be found that the maps of multiple hypotheses con-\ntain diverse patterns and semantics. This indicates multiple\nrepresentations in our method actually learn various modal\ninformation of pose hypotheses.\n19 27 81 135 243 351\nReceptive fields\n30\n35\n40\n45\n50MPJPE (mm)\nCPN\nGT\n(a) Different receptive ﬁelds under MPJPE.\n0 5 10 15 20 25\nTwo-norm errors of 2D Estimator\n35\n40\n45\n50\n55MPJPE (mm)\nGT\nGT+(0, 5)\nGT+(0, 10)\nGT+(0, 15)\nGT+(0, 20)\nCPN\nDetectron\nSH\n(b) Different 2D detections under MPJPE.\nFigure 7. (a) Ablation studies on different receptive ﬁelds of our\nmethod on Human3.6M under MPJPE metric. (b) The effect of 2D\ndetections on Human3.6M under MPJPE. Here,N(0,σ) represents\nthe Gaussian noise with mean zero and σis the standard deviation.\n(CPN) - Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) -\n2D ground truth.\n12\nInput Prediction Ground Truth Prediction Ground Truth Prediction Ground TruthInput Input\nFigure 8. Qualitative results of our proposed method on Human3.6M dataset (ﬁrst 1 row) and MPI-INF-3DHP dataset (last 2 rows).\nv\n t\nFigure 9. Qualitative results of our proposed method on challenging in-the-wild videos.\nInput PoseFormer Baseline MHFormer\n Input PoseFormer Baseline MHFormer\nFigure 10. Qualitative comparison among the proposed method (MHFormer), the baseline method, and the previous state-of-the-art method\n(PoseFormer) [49] on challenging wild videos. Wrong estimations are highlighted by yellow arrows.\n13\nFigure 11. Diverse 3D pose hypotheses generated by MHFormer. For easy illustration, we color-code the hypotheses to show the difference\namong them, and the hypotheses are shown from two perspectives. Green colored 3D pose corresponds to the ﬁnal synthesized estimation of\nour method.\n14\n(a) Hypothesis 1\n (b) Hypothesis 2\n (c) Hypothesis 3\nFigure 12. Multi-head attention maps (9 heads) from the Multi-Hypothesis Generation (MHG) module of our 351-frame model with 3\ndifferent hypotheses. The brighter color indicates a stronger attention value.\n(a) Hypothesis 1\n (b) Hypothesis 2\n(c) Hypothesis 3\nFigure 13. Multi-head attention maps (8 heads) from the Self-Hypothesis Reﬁnement (SHR) module of our 351-frame model with 3 different\nhypotheses. The brighter color indicates a stronger attention value.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7179105877876282
    },
    {
      "name": "Ambiguity",
      "score": 0.6620218753814697
    },
    {
      "name": "Merge (version control)",
      "score": 0.5667172074317932
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5521286725997925
    },
    {
      "name": "Transformer",
      "score": 0.4490078091621399
    },
    {
      "name": "Representation (politics)",
      "score": 0.4172510802745819
    },
    {
      "name": "Intuition",
      "score": 0.41256022453308105
    },
    {
      "name": "Machine learning",
      "score": 0.4107932150363922
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32765108346939087
    },
    {
      "name": "Information retrieval",
      "score": 0.11817780137062073
    },
    {
      "name": "Cognitive science",
      "score": 0.0750734806060791
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}