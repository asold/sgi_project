{
    "title": "Review on the application of deep learning algorithms in video game AI agent",
    "url": "https://openalex.org/W4383560360",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4383612523",
            "name": "Muqing Ge",
            "affiliations": [
                "Northwest University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2589349991",
        "https://openalex.org/W4230319068",
        "https://openalex.org/W1978847797",
        "https://openalex.org/W2055449389",
        "https://openalex.org/W2982041717",
        "https://openalex.org/W3094169176",
        "https://openalex.org/W3142822945",
        "https://openalex.org/W3011277165"
    ],
    "abstract": "AI algorithms have been applied in video games more than before. Since the early 2000s, the benchmark of the application of AI agent in video games has been put forward by researchers and engineers. This paper introduces the application of four different deep learning algorithms in video games, namely Component-based Hierarchical State Machine (CBHSM) algorithm, Monte Carlo Search Tree (MCTS) algorithm, Generative Adversarial Networks (GAN) algorithm, and the combination of A* algorithm and Q-learning algorithm. It can be summarized that both CBHSM and MCTS algorithms can modify and optimize the traditional Finite State Machine (FSM) algorithm applied in NPC (Non Player Characters) system. Additionally, A* algorithm can be combined with Q-learning algorithm to solve the problem of the difficulty on huge state space and limited time, and GAN algorithm, a model of unsupervised learning, can generate results by fewer training sets.",
    "full_text": " \nReview on the application of deep learning algorithms in video \ngame AI agent \nMuqing Ge \nSchool of Information Science and Technology, Northwest University, Xian, Shanxi, \n710000, China \n \nmg19179@essex.ac.uk \nAbstract. AI algorithms have been applied in video games more than before. Since the early \n2000s, the benchmark of the application of AI agent in video games has been put forward by \nresearchers and engineers. This paper introduces the a pplication of four different deep learning \nalgorithms in video games, namely Component -based Hierarchical State Machine (CBHSM) \nalgorithm, Monte Carlo Search Tree (MCTS) algorithm, Generative Adversarial Networks \n(GAN) algorithm, and the combination of A* algorithm and Q -learning algorithm. It can be \nsummarized that both CBHSM and MCTS algorithms can modify and optimize the traditional \nFinite State Machine (FSM) algorithm applied in NPC (Non Player Characters) system. \nAdditionally, A* algorithm can be combined with Q-learning algorithm to solve the problem of \nthe difficulty on huge state space and limited time, and GAN algorithm, a model of \nunsupervised learning, can generate results by fewer training sets. \nKeywords: A* algorithm, Q -learning, GAN, Finite State Machine, video game AI, Monte -\nCarlo Search Tree. \n1. Introduction \nSo far, the NPC system of most games uses different AI algorithms to make the behavior patterns of \ncharacters more reasonable. Developers intend to use AI to improve the design of the game le vel and \ncreate the NPC with more authenticity. They also expect the enemies whom players must face to have \nmore diverse response to the action of players. A simple NPC system can be easily penetrated by \nplayers and players will be bored soon. Moreover, the  developer of the video game can test and \nimprove the setting of the game level through AI agents. The test and development of AI agent on \nvideo game has become the benchmark to inspect the quality of AI algorithms. During game -playing, \nplayers need to respond to complex rules and contingencies, and they need the ability of dynamic-path \nplanning, team-work, and learning. It is an attractive challenge for AI agent and many researchers aim \nto work on this field. \nThis paper first reviews the Component-based Hierarchical State Machine (CBHSM) algorithm . It \nis a type of modified Finite State Machine (FSM) algorithm . Then the Monte Carlo Tree  Search \n(MCTS) algorithm applied in the NPC system is introduced. The next part describes the application of \nsome algorithms in video games, including the combination of A* algorithm and Q-learning algorithm, \nas well as the Generative Adversarial Network (GAN) algorithm. \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n© 2023 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0(https://creativecommons.org/licenses/by/4.0/).\n548\n \n2. Application of AI algorithms in NPC system \nThe goal for the video game is to establish the NPC which is bo th interesting and changeable. In some \nvideo games, the NPC is designed to have changeless action models that can be easily recognized by \nplayers, thus making players get bored. While in other video games, the NPC is designed to be too \npowerful for players  to complete, thus making them frustrated. In fact, the purpose of NPC is not to \ngive the player an unbeatable opponent, but to maximize the player’s engagement and experience over \na long period of time. AI algorithms can achieve this goal. The NPC based o n AI algorithms can be \nconstantly improved as players explore the game contents so that players can keep themselves \ninterested in the game. \n2.1. Component-based hierarchical state machine \nIn fact, Finite State Machine (FSM) is a method quite popular for AI algorithm application in games. \nIt is easy for developers to understand and operate the development process. The manageable method \nof operation also allows fast prototyping and enables more iterations for developing. Its lightweight \nmakes it possible to be implemented in real -time systems. Compared to other models, FSMs are more \nsuitable for validation and testing [4]. \nTraditional FSM requires developers to complete all the architecture for expected states and \ntransitions during the developm ent phase and the code is difficult to change after development. \nTherefore, FSM is prone to face combinatorial explosion problems (FSM states and transitions will be \ndifficult to control when the environment complexity grows). In different environments or different \ngames, developers have to recode the stations and transition before rather than using it repeatedly. \nThis dilemma is solved by a modified FSM called CBHSM [4]. CBHSM is based on an asynchronous \nevent-driven system (AEDS) which has the function of  decoupling states and its context. In the \nCBHSM system, characters interact each other by the event exchange which means developers can \nchange the state machine by adding or changing events instead of recoding the state machine. CBHSM \nhas three advantages: \n2.1.1. Compile time composability. In CBHSM design, hierarchy is the most important sign. When \nimplementing a new state class, the developer should design a top -down structure and only consider \nthe top design of class. The detailed states and behaviors are left  to the subclass. Obviously, \nhierarchical design conforms to the thought of global design and significantly reduces the error in \ndesign. It also improves efficiency of developers. \n2.1.2. Design time configurability. The configuration of traditional FSM of NPC des ign is usually \nhard-coded by programmers but game designers should be responsible for it. If the design changes, \nrecompiling is necessary, which will waste too much time. The CBHSM puts off the work of \nhardcoding to the game design time. Thus, game program mers and designers can finish their work \nalone with high efficiency. \n2.1.3. Run time flexibility.  To make the action of NPC more unpredictable and resourceful, the state \nmachine must have the ability to accept the parameters of the environment at running time. Th e \ncomponent technology greatly improves the flexibility and adaptation of the state machine. \n2.2. Monte carlo tree search \nMonte Carlo Tree Search (MCTS) algorithm is one of the most efficient algorithms in AI agent of \ngame. It is a kind of search technique that does not rely on professional field knowledge [5]. Therefore, \ndevelopers do not need to learn a lot of domain -specific knowledge, which can save time for \ndeveloping. In the NPC system based on traditional FSM, a robot gives a fixed method to respond to \nplayers’ attack, then players will quickly learn how the robot moves and figure out the way to beat the \nrobot. Compared with FSM, MCTS can make more flexible responses to players’ action. The MCTS \nalgorithm was created to solve the FSM repeatability problem. The way MCTS works is to first \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n549\n \nidentify all the operations available to the robot for the current situation. Then, for each possible action, \nhow the player can respond will be analyzed. It then considers all possible actions for the player and \nwhat responses it can make, etc. There are mainly 4 stages in the application of MCTS, namely the  \nselection, the expansion, the simulation, and the back propagation. At first, there is  only one node on \nthe search tree and it is needed to make a decision on  this situation. Each node on the search tree \ncontains three types of basic information: the situation r epresented, the number o f visits, and the \ncumulative score. The steps of MCTS are shown in Figure 1. \n \nFigure 1. MCTS steps. \nIn some complex games, computer will make a huge number of decisions. It would take a long time \nfor a computer to build a detailed tree for every possible choice and every possible scenario \nthroughout the game. Therefore, to avoid the huge calculation, the MCTS algorithm randomly selects \na few possible options and builds the tree only for the chosen one. That way, the calculations are faster \nand computer can analyze which option has the highest likelihood of a reward. \nIn a previous study, a MCTS algorithm is successfully applied in the game Dead -End. In this \napplication, the longer the simulation time takes, the higher the chance of robot wins will b e. The \ndifficulty of the game can be decided by setting a time limit to plan the rate of the robot wins [6]. The \nstudy expounds that difficulty adjustment can be achieved by the time -limit of the MCTS algorithm, \nwhile the FSM algorithm can hardly achieve. \nIn another research, the authors discussed and applied a modified MCTS algorithm in general game \nplaying. The modification of the algorithm is to create a parameter which can manipulate the \nprobability when choosing a solution in evaluation function. With this function, the tree search will \nconsider the most probably nodes at the beginning and give high evaluation to the nodes [7]. Every \nstates will have a parameter P which is responsible for the probability of the outcome. This parameter \nwill be used for evaluating the function. \nIn one research paper, the authors discussed and evaluated the eight enhancements for MCTS in \ngeneral video game playing (GVGP): Progressive History, N -Gram Selection Technique, Tree Reuse, \nBreadth-First Tree Initialization, Loss Av oidance, Novelty -Based Pruning, Knowledge -Based \nEvaluations, and Deterministic Game Detection [5]. Some were proposed by predecessors, and some \nwere the improvement made by the authors. The authors combined these enhancements to perform \nexperiments and rec ord data. The experiments show that most of the enhancements make \nimprovements to the average winning percentage when being added individually into the game test. \nSpecially, the Breadth-First Tree Initialization (BFTI) has a significantly improvement in th e winning \npercentage when combined with most of the other enhancements such as the Tree Reuse, Knowledge -\nBased Evaluation, and Novelty-Based Pruning. \n3. Application of AI algorithms in video games \nThe complex environment of video games gives AI algorithms a g ood application space. The special \nrule and mechanism of each video game requires developers to plan algorithms specifically for the \ngame. That is an interesting challenge and many competitions of AI video games have been held. In \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n550\n \nthis part, the AI algorit hm applied in the game Super Mario Bros in 2009 Mario AI Competition will \nbe introduced. Some interesting and novel algorithms are also introduced in this section. \n3.1. The combination of A* algorithm and Q-learning algorithm \nIn the Mario AI competition convene d by IEEE society, the agent based on A* algorithm has a high \nmark [1]. In this section, a combined algorithm based on A* algorithm and Q -learning which has high \nefficiency will be introduced. This method solves the problem of huge state space and limited time. It \nalso adds a learning ability to the search algorithm [2]. \nA* algorithm is an optimization algorithm which minimizes the cost of path. The cost f(n) is \ncalculated by equation (1).  \nF(n) =  g(n) +  h(n) (1) \nf(n) is the estimation function of the movement cost from the initial state to the target state through the \nstate n; g(n) is the actual cost of moving from the original state to the state n ; h(n) is estimate of the \nmovement cost from the state n to the target state ; h(n) is calculated by heuristic function to estimate \nthe distance to the goal node. \nHowever, A* algorithm is not perfect to video games. Because of the limited search time, it is hard \nfor agents to find a best path except a sub -optimal path. When agents f ace the dead ends, they could \nspin around and can not find their way out. This can be solved by adding the learning ability to agents. \nIn other words , if agents can remember where they face a dead end, failures  can be avoided . During \nthe training phase, the agent remembers the location of branch points and effective actions. In the final \nexperiment, the agent acts on the results of the A* algorithm except for the memorized location. \nQ-learning is a type of reinforcement learning algorithms  and is widely use d for problem \noptimization. It is based on a decision rule set Q (st, at). st is the state of agent, at is the action that will \nbe taken. Agents observe the location of Mario as a stat, and select a target node from A* algorithm as \nthe action. \nThe combined algorithm has two stages: the searching stage and the learning stage. An agent will \ntake the searching stage first by using A* algorithm. When the agent faces a dead end or can not find \nits way out, it takes into the learning stage. The agent will train a target node by the Q-learning method. \nWhen the agent detects the dead ends, it will get rewards. After detecting dead ends, the agent turns to \nthe searching state. \nIn a previous study, the researcher made an experiment with the A* algorithm, Q -learning \nalgorithm, and the combination of A* algorithm and Q -learning method. The level is set with a high \nwall, enemies, and dead ends. The combined algorithm got an excellent result while the other two \nsingle algorithms failed to pass the experiment [2]. \n3.2. Generative adversarial networks for game level generation \nIn traditional video games, there are only limited game levels for players to enjoy. For example, one of \nthe most popular video games, the Super Mario Bros, only has 8 levels to play, and Contra only has 8 \nlevels (rumors say that there are another eight hidden levels). Anyway, limited levels can not give \nplayers the best experience of game. However, the game level design is a complex component of game \ncreation. Game designers must consider physics, difficulty , and playability in the creation process. \nEven a single level could take a lot of time to design. \nGAN is composed of two important parts: Generator (G) and Discriminator (D). Data (mostly \nimages) is generated by the generator in order to “treat” the discr iminator. The  aim of the  \ndiscriminator is to find the “fake data” generated by the generator , thus determining if this image is \nreal or generated by the machine. Thus, G and D constitute a dynamic “game process”. Firstly, fix the \ndiscriminator and train th e generator until the data from the generator is strong enough to cheat the  \ndiscriminator. Then, fix the generator and train the discriminator until the discriminator can \ndistinguish true and false of the data from the generator. After several iterations, the desired training \nresults are generated. \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n551\n \nGAN have achieved impressive success in image generation. However, GAN faces challenges in \nthe content generation of game levels, since it is difficult to combine the aesthetically appealing with \nplayable levels at the same time. A kind of new GAN architecture called Conditional Embedding Self-\nAttention Generative Adversarial Network (CESAGAN) is purposed in a research. It is a modification \nof the self -attention GAN with an embedding feature vector input to condit ion the training of the \ndiscriminator and generator [3]. The research also purposed a new bootstrapping training processing. \nCESAGAN is used to combine functional requirements and alleviate the deficiency of training data. \nAdvantages of CESAGAN include: \n• It reduces the amount of information required to train discriminators. Most video games only \nhave seldom levels which are available for the training set. \n• It increases the quality. The levels generated by traditional GANs always have low quality and \nsometimes are unplayable. \n• It increases the diversity. The diversity of levels has obviously been promoted compared to \ntraditional GANs. \nThe first advantage is achieved by a self-attention mechanism. Because of the size limitation of the \nconvolution kernel , t he traditional convolutional GAN can only capture the relationships of local \nregions. In a self-attention GAN, the information of all positions can be utilized. Therefore, the system \ncan use fewer data sets to obtain the same quality of training sets by the  self-attention mechanism. It \nhas an additional feature conditional vector to train the generator and discriminator. \nAnother method using fewer data sets for training is the SinGAN. It is the first unconditional \ngenerative model being trained based on a single natural image. In other words, the trained SinGAN \ncan receive a random noise input to generate a new natural image [8]. This method fits AI video \ngames well with very limited levels for training. Since the SinGAN is designed for RGB images, it can \nnot generate the video game levels based on 2D token maps. In this section, a kind of GAN algorithm \nbased on SinGAN called Token -based Oneshot Arbitrary Dimension Generative Adversarial Network \n(TOAD-GAN) will be introduced. The TOAD -GAN is successfully applied in the video game level \ngeneration. It is a new algorithm which can generate new levels of arbitrary size when being trained on \na single training example [9]. \nThe core part of this method, namely the SinGAN, is a new architecture which can create a  \ngenerate model from a single image. Based on multiple GAN structures , SinGAN learns the \ndistribution of image blocks with 11x11 resolution at different scales, and generates real images from \nrough to fine and low resolution to high resolution  stage by sta ge. Therefore, SinGAN can isolate a \nlarge amount of information from an image for learning. In the TOAD-GAN, the SinGAN can be used \nto isolate information of one level and generate lots of levels based on the extracted information. \n4. Conclusion \nAmong all the AI algorithms applied in video games, the A* algorithm and Monte -Carlo Tree Search \nalgorithm are the most popular algorithms among video game developers in AI agent design. The \ncombination of A* algorithm and Q -learning algorithm are mentioned in this pap er. The agent can \nchange the target by Q -learning algorithm to improve the efficiency. Monte -Carlo Search Tree is also \na kind of improved algorithm which can replace the traditional FSM and select the node with the \nhighest access frequency rather than sele cting all the nodes. It improves the efficiency and has the \nability of learning which allows the NPC to respond to the action of players. These two algorithms \nhave simple structures that make computing processes faster. Another reason is that there is plen ty of \nsource code for both algorithms which saves developers lots of time. But we are still expected to see \nsome important improvement of algorithm with pioneering significance. The Generation of game level \nis also an important part for game designers. Sev eral improved GAN algorithms are mentioned in this \npaper. The adventure of the GAN algorithm is that it adapts to the situation of limited training sets, \nwhich meets the generation of the game level. In general video games, the method of dividing basic \nelements into several tokens rather than natural images can improve the efficiency of generation and \nmake the generation available. The generation of game level based on the GAN algorithm inspires \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n552\n \ngame designers and save them a lot of time for design. One of the limitations of this paper is that the \nalgorithms mentioned in this paper are limited because of the lack of data of some algorithms. The \nadventure of the similar improved algorithms do  not have comparability because the variate of \nalgorithms is not single. \nReferences \n[1] Togelius J, Karakovskiy S and Baumgarten R 2010 The 2009 mario ai competition. In IEEE \nCongress on Evolutionary Computation. number 1. 1–8. \n[2] Shinohara S, Takano T, Takase H, Kawanaka H and Tsuruoka S 2012 Search algorithm with \nlearning ability for mario ai – combination a* algorithm and qlearning. In 2012 13th ACIS \nInternational Conference on Software Engineering . Artificial Intelligence. Networking and \nParallel/Distributed Computing. number 2. 341–344. \n[3] Torrado R R, Khalifa  A, Green  M C , Justesen  N, Risi  S and Togelius J 2020 Bootstrapping \nconditional gans for video game level generation. In 2020 IEEE Conference on Games \n(CoG). 41–48. \n[4] Hu W F , Zhang  Q and Mao  Y Q 2011  Component-based hierarchical state m achine — a \nreusable and flexible game ai technology. In 2011 6th IEEE Joint International Information \nTechnology and Artificial Intelligence Conference. volume 2. 319–324. \n[5] Dennis J N J S, Chiara F S, Torsten S and Mark H M  2016 Winands. Enhancements for real-\ntime monte-carlo tree search in general video game playing. In 2016 IEEE Conference on \nComputational Intelligence and Games (CIG). 1–8. \n[6] Ma Y, He S J, Wang J P, Fu Y W and Shi Z Y 2010 Automatic AI design by the use of mcts for \nthe game dead -end. In 20 10 Seventh International Conference on Fuzzy Systems and \nKnowledge Discovery. volume 6. 2772–2776. \n[7] Danil A C and Sergey A B  2020 Monte carlo tree search modification for computer games. In \n2020 IEEE Conference of Russian Young Researchers in Electrical and  Electronic \nEngineering (EIConRus). 252–255. \n[8] Shaham T R, Dekel T and Michaeli T 2019 Singan: Learning a generative model from a single \nnatural image. CoRR. abs/1905.01164. \n[9] Schubert F, Awiszus M and Rosenhahn B 2021 Toad-gan: a flexible framework for few -shot \nlevel generation in token-based games. IEEE Transactions on Games. 1–1. \n \nProceedings of the 3rd International Conference on Signal Processing and Machine LearningDOI: 10.54254/2755-2721/4/2023322\n553"
}