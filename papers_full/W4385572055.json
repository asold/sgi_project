{
  "title": "ALERT: Adapt Language Models to Reasoning Tasks",
  "url": "https://openalex.org/W4385572055",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103894826",
      "name": "Ping Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133259066",
      "name": "Tianlu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2564974659",
      "name": "Olga Golovneva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3096641073",
      "name": "Badr AlKhamissi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112200778",
      "name": "Siddharth Verma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972202473",
      "name": "Zhijing Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121046089",
      "name": "GARGI GHOSH",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124289572",
      "name": "Mona Diab",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A137861795",
      "name": "Asli Celikyilmaz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2772967090",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W2889107415",
    "https://openalex.org/W4286910039",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W3098172379",
    "https://openalex.org/W4287634363",
    "https://openalex.org/W4224032280",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3035733645",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3175591618",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W3171639130",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4311728219",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W4283828387",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3101026830",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2953987971",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4288614645",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W4385567093",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W4385571805",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Ping Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona Diab, Asli Celikyilmaz. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1055–1081\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nALERT: Adapting Language Models to Reasoning Tasks\nPing Yu♠ Tianlu Wang♠ Olga Golovneva♠ Badr AlKhamissi△\nSiddharth Verma△ Zhijing Jin△ Gargi Ghosh♠ Mona Diab♠ Asli Celikyilmaz♠\n♠Meta AI △Work done at Meta AI\n{pingyu,aslic}@meta.com\nAbstract\nRecent advancements in large language models\nhave enabled them to perform well on com-\nplex tasks that require step-by-step reasoning\nwith few-shot learning. However, it is unclear\nwhether these models are applying reasoning\nskills they have learned during pre-training, or\nif they are simply memorizing their training\ncorpus at finer granularity and have learned to\nbetter understand their context. To address this\nquestion, we introduce ALERT, a benchmark\nand suite of analyses for evaluating reason-\ning skills of language models. ALERT enables\ncomparing pre-trained and finetuned models on\ncomplex tasks that require reasoning skills to\nsolve them. Our benchmark provides a test bed\nto assess any language model on fine-grained\nreasoning skills, which spans over 20 datasets\nand covers 10 different reasoning skills. To\nprove the efficacy of ALERT we investigate the\nrole of finetuning. Our extensive empirical anal-\nysis shows that language models acquire rea-\nsoning skills such as textual entailment, abduc-\ntive reasoning, and analogical reasoning dur-\ning the finetuning stage compared to pretrain-\ning stage. Another finding is when language\nmodels are finetuned they tend to overfit to the\nprompt template, which hurts the robustness of\nmodels resulting in generalization problems.\n1 Introduction\nLarge language models (LLMs) (e.g., GPT-\n3 (Brown et al., 2020a), PALM (Chowdhery et al.,\n2022), OPT (Zhang et al., 2022)) have shown in-\ncreasing in-context learning capabilities with scal-\ning up the model and data sizes. Despite this\nprogress, even the largest of these models still\nstruggle with tasks such as commonsense rea-\nsoning (West et al., 2022), and math word prob-\nlems (Hendrycks et al., 2021b) which require arith-\nmetic reasoning or symbolic manipulation (Rytting\nand Wingate, 2021). Table 1 presents some ex-\namples that require certain reasoning skills. Even\nThe cafeteria had 23 apples. If they used 20 to make lunch and\nbought 6 more, how many apples do they have?\nThe answer is29 apples .\nSelect the best translation into predicate logic. David teaches Chris.\n(c: Chris; d: David; Txy: x teaches y) (A) Tdc; (B) Tcd; (C) Tcc;\n(D) dTc.The answer is(B) Tcd .\nIsabella entered the hall. Olivia entered the hall. The ap-\nple is in the blue_treasure_chest. Olivia exited the hall. Is-\nabella moved the apple to the green_basket. Question: Where\ndoes Isabella think that Olivia searches for the apple?The\nanswer isIsabella thinks that Olivia searches for the apple in the\ngreen_basket .\nTable 1: Examples from tasks that require reasoning skills and\ngenerated outputs from GPT-3 series text-davinci-003 engine.\nThe failed outputs are highlighted in red . Predictions by\nChatGPT are shown in Table 9 in Appendix.\nthe powerful LLMs (such as text-davinci-0031 and\nChatGPT2) fail to make correct predictions.\nTo improve large LLMs’ performance on tasks\nthat require multiple steps of reasoning, recent\nwork used different prompting methods which in-\ncluded a rationale with the final answer in the form\nof: scratchpad for arithmetic and logical reason-\ning (Nye et al., 2021), chain-of-thought (CoT) (Wei\net al., 2022) for practically any tasks, or addinglet’s\nthink step-by-step (Kojima et al., 2022) to prompt\nmodels to generate explanations. Other works such\nas Chung et al. (2022) integrated step-by-step expla-\nnations into the finetuning stage (CoT-finetuning).\nWhile these techniques may improve the accuracy\nand interpretability, it is not well understood which\nreasoning skills they rely on or to what degree they\nrequire higher-order reasoning. It is also uncertain\nhow frequently the stated reasoning steps actually\ncontribute to the final task predictions. For instance,\nto correctly answer the questions in Table 1 a com-\nbination of logical, commonsense, math and spatial\nreasoning skills are required.\nIn this work, to gain a deeper understanding of\nLLMs reasoning abilities in in-context learning\n1https://beta.openai.com/docs/models/gpt-3.\n2https://chat.openai.com/chat.\n1055\nsettings, we introduce ALERT, a new pipeline to\nbenchmark different LLMs on various reasoning\nskills and provide analysis to assess reasoning abil-\nities. Unlike existing commonly used benchmarks\n(e.g., Mishra et al. (2022); Wang et al. (2022c); Sri-\nvastava et al. (2022)), ALERT can evaluate LLMs’\nfine-grained reasoning skills. It spans over 20\ndatasets and covers 10 different reasoning skills\nincluding logical, causal, commonsense, abductive,\nspatial, analogical, argument and deductive reason-\ning as well as textual entailment, and mathematics\n(see Figure 6). ALERT enables easy benchmark-\ning of any LM (e.g., pre-trained, finetuned, CoT-\nfinetuned) on a rich set of new inference methods\nincluding zero-shot, few-shot and CoT.\nUsing ALERT, we further investigate whether\nfinetuning can improve LMs’ performance on\ndownstream reasoning tasks. Specifically, we are\ninterested in diagnosing what actually improved\nwhen we observe a performance increase on rea-\nsoning tasks. Is it because models have seen similar\ndata in the finetuning stage? Or is it because mod-\nels have seen prompts in a specific template and\nmemorize the template during finetuning such as\ndefinitions provided in the NIV2 benchmark (Wang\net al., 2022c)? Or does the LLM actually acquired\nthe required reasoning skill? We investigate these\nthree possibilities.\nTo study the above questions, we compare three\ndifferent model types (as shown in Figure 2): a pre-\ntrained model and two types of finetuned models.\nSpecifically:\n• OPT (Zhang et al., 2022): A baseline LLM a\npre-trained model with no finetuning (figure\n(A) in Figure 2);\n• OPT-FT: Meta-finetuned OPT on reference\nanswers without explanations, illustrated in\n(figure (B) in Figure 2);\n• OPT-CoT: Meta-finetuned OPT on data with\nrationales (explanations) (Chung et al., 2022;\nAlKhamissi et al., 2023) (figure (C) in Fig-\nure 2).\nUsing these three types of models, we investigate\nthe role of finetuning on three dimensions:\n(1) Data memorization: We investigate whether\nthe performance improvements obtained after fine-\ntuning can be attributed to using similar or some-\ntimes the exact same data as in the evaluation\ndatasets. To this end, we use vocabulary overlap to\nReasoning Skills Datasets\nLogical bigbench repeat copy logic, mmmlu an-\nswer generation\nCausal plausable result generation, anli r2 entail-\nment, anli r3 entailment, cb entailment\nCommonsensepiqa answer generation, commongen sen-\ntence generation, sciq answer generation,\nopenbookqa question answering\nEntailment nli r2 entailment, anli r3 entailment, cb\nentailment, lue entailment classification\nMathematics semeval closed vocabulary math, semeval\ngeometric math, mmmlu formal logic\nAbductive tellmewhy\nSpatial babi t1 single supporting fact, piqa answer\ngeneration, toqa find location easy clean\nAnalogical commongen sentence generation, bard\nanalogical reasoning causation\nArgument argument stance classification, argument\nconsequence classification\nDeductive rocstories correct answer generation\nTable 2: ALERT benchmark consists of 20 datasets covering\n10 different reasoning skills. The full list of the reasoning\nskills and datasets is in Table 4 in Appendix A.1.\nmeasure the extent to which the evaluation data is\ndifferent from the finetuning data, i.e. We investi-\ngate whether the improvement is more significant\nwhen evaluation data and finetuning data are more\nsimilar.\n(2) Reasoning skills transfer: We investigate if\ncertain reasoning skills can be more successfully\npermeated in LLMs than other reasoning skills.\nTo verify this, we carefully divide the evaluation\ndatasets into groups which require different reason-\ning skills. We compile held-out datasets as shown\nin Figure 6 which require skills held-out from any\nof the training datasets. This way, we expect to\nsee larger improvements on in-domain skills com-\npared to held-out skills if reasoning skills can be\ntransferred during finetuning stages.\n(3) Prompt template memorization: Our third\nhypothesis is that LLMs can overfit to data for-\nmat used in the finetuning datasets such as training\ndata format used in Figure 2. In other words, the\nconsistency in data format helps LLMs better un-\nderstand the instruction which then yields better\nperformance after finetuning. To test this, we eval-\nuate finetuned LLMs on datasets with 5 different\nprompt templates.\nSummary of findings: (i) Different from Gu-\nrurangan et al. (2020), our experiments indicate\nthat there is no strong correlation between high\nvocabulary overlap (between finetuning and evalu-\nation datasets) and performance gain on reasoning\nevaluation datasets. This means that LLMs are not\n1056\nDefinition: In this task, we ask you to write an implausible\nanswer to a question that involves event duration, based on a given\nsentence. Here, event duration is defined as the understanding of\nhow long events typically last. For example, “brushing teeth”,\nusually takes a few minutes. Even though there exist multiple\nwrong answers, we only need a single wrong answer.\nExample 1-\ninput: Sentence: Jack played basketball after school, after\nwhich he was very tired.\nQuestion: How long did Jack play basketball?\noutput: 22 hours.\nexplanation: Typically we play basketball for a couple of\nhours. So any answer beyond that range is unlikely.\nFigure 1: An example from NIV2 (Wang et al., 2022c) that\nrequires a deep understanding of the long task instruction and\ncan be very challenging even for humans.\nsimply memorizing the training data during the fine-\ntuning stage; (ii) Finetuning helps improve certain\nreasoning capabilities of LLMs (e.g. analogical\nand abductive) but not all of them (e.g. common-\nsense reasoning); (iii) Finetuning can cause over-\nfitting towards data format, which makes it harder\nfor LLMs to generalize to other prompt templates,\nwhile CoT-finetuning helps to mitigate this issue as\nit incorporates a variety of explanations.\nThough many of the aspects that we study have\nbeen discussed in prior analyses of LLMs (Chung\net al., 2022; Wei et al., 2021a, 2022; Kojima et al.,\n2022; Cobbe et al., 2021; Sanh et al., 2021), prior\nwork has not evaluated LLMs on different reason-\ning skills and how these skills can be improved.\nOverall, by evaluating reasoning skills with ALERT,\nwe gain new insights on how models have or have\nnot succeeded in generalizing beyond theirtraining\nexperience.\nTo summarize our contributions, this paper\npresents a meticulously designed benchmark for\nassessing reasoning abilities. Furthermore, a thor-\nough investigation of the role of finetuning in the\ncontext of reasoning abilities, data memorization,\nand data format is conducted.\n2 Motivation and Our Benchmark\nMotivation. The analyses in ALERT are inspired\nby a scientific question: To what extent do LLMs\nlearn generalizable reasoning abilities? This ques-\ntion motivates our focus on measuring LLMs’ per-\nformance on tasks that require contextual under-\nstanding and perform multi-step operations, which\nare crucial to perform well on downstream tasks.\nDatasets Construction. To construct the datasets\nof ALERT, we select datasets from NIV2 benchmark\n(Wang et al., 2022c) and perform the following\noperations:\n(1) Omit extremely hard tasks. We design\nALERT so that it can be used to benchmark a\nvariety of LLMs, from pre-trained, finetuned to\ninstruction-tuned models. To select such tasks,\nwe apply several heuristics: firstly, we manually\nomit tasks that heavily rely on instructions. Some\ntasks are hard to solve when only in-context ex-\namples (demonstrations) are provided (e.g., the ex-\nample in Figure 1). Secondly, we selected only\nthose tasks that achieved a reasonable level of\nperformance (empirically use ROUGE-L > 5.0)\nwhen evaluated with a pre-trained model (we use\nthe OPT-13B model). Thirdly, we omit tasks on\nwhich humans fail to get decent performance given\nthe ground truth labels from NIV2. For exam-\nple, task963_librispeech_asr_next_word_ predic-\ntion (Weir et al., 2020) provides a prompt “Joey’s\nfavourite food is ___”, with the ground truth answer\n“sandwiches”. Without any context or background\ninformation, the answer can be any food thus it is\nextremely hard for humans to accurately predict\n“sandwiches”.\n(2) Remove tasks with long input context. The\ninput sentence length of some tasks can be very\nlong, and currently most LLMs are not designed\nfor solving long text problems. We omit tasks with\ndemonstration length longer than 2048 tokens.\n(3) Fix ground truth labels. For each\nreasoning task, NIV2 provides the reason-\ning skills required to solve the task, e.g.\ntask102_commongen_data_to_text requires rela-\ntional, analogical and commonsense reasoning.\nHowever, we found that some tasks have been la-\nbeled with incorrect reasoning skills. For exam-\nple, task393_plausible_result_generation provides\na sentence and asks LLMs to complete the sen-\ntence. The labels given by NIV2 are causal reason-\ning and textual entailment, but in fact this task can\nhardly examine an entailment skill. Accordingly,\nwe manually fix reasoning skill labels. In addition,\nwe only keep the predominant skill. For example,\nmany tasks need more or less commonsense knowl-\nedge, therefore we select the related tasks that only\nheavily rely on commonsense knowledge to assess\ncommonsense reasoning.\nBenchmark. After the above steps, we select\ntasks that represent a variety of reasoning skills\nand construct ALERT reasoning benchmark, where\nTable 2 shows details about our benchmark.\n1057\n(A) pretrained language models (e.g. GPT-3, OPT)\n(B) meta-finetuned language models (e.g. FLAN, OPT-FT)\nPretrained \nLM\nInference on \ntask A, B, C,...\nfinetune on \ntask D, E, F, …\nQ: If X and Y are digits and 8XY is a 3-digit number \nthat is divisible by 2, which of the following is a \npossible product of X and Y?  A)15 B)31 C)12 D)27 \nE)91; A: The answer is C.\n(C) CoT finetuned language models (e.g. OPT-CoT)\nfinetune on \ntask D, E, F, …\nwith explanations \nQ: If X and Y are digits and 8XY is a 3-digit number that is \ndivisible by 2, which of the following is a possible product of X \nand Y? A)15 B)31 C)12 D)27 E)91; A: The answer is C. because \nKey to this question is to remember the fact that a number \ndivisible by 2 must end with even OR 0 (i.e Y). If Y had to be 0, \nproduct should also be 0 regardless of X. Otherwise, product is a \nmultiple of 2. Only one answer choice meets the requirement\nInference on \ntask A, B, C,...\nPretrained \nLM\nPretrained \nLM\nInference on \ntask A, B, C,...\nFigure 2: We compare three types of models: (A) directly\napply pretrained LLMs on reasoning tasks; (B) finetune LLMs\non a set of tasks; (C) finetune LLMs on tasks with explana-\ntions (CoT-finetuning). Finetuning data contains source and\ntarget parts, and the language modeling loss only applied to\nthe target part.\n3 Experiment Setup\n3.1 Models\nTo perform a controlled comparison across training\nand prompting methods, we focus on three different\nmodels: pre-trained, meta-finetuned, and rationale-\nbased meta-finetuned (CoT-finetuned) models. For\npre-trained models, we use OPT (Zhang et al.,\n2022), a suite of decoder-only pre-trained trans-\nformers which are reported to yield comparable\nperformance to GPT-3 (Brown et al., 2020b). We\nbenchmark with OPT models of two scales: 1.3B\nand 13B. For finetuned models (OPT-FT), we fine-\ntune OPT models on datasets without explanations.\nFor CoT-finetuned models (OPT-CoT), we finetune\nOPT models on data with rationales (explanations).\nWe train all models in Pytorch (Paszke et al.,\n2017) using OPT-IML (Iyer et al., 2022) codebase3.\nWe initialize model hyper-parameters for each\nmodel scale following OPT (Zhang et al., 2022).\nWe pack our training examples into sequences of\nlength 2048, left-truncating examples that overflow.\nWe use AdamW (Loshchilov and Hutter, 2017)\nwith 32-bit state with (β1,β2) = (0.9,0.95), lin-\nearly warming up the learning rate for 6% steps to\nthe maximum, followed by linearly decaying it to\n0. For all 1.3B models, we use batch size of 128,\nand for 13B models, we use batch size of 256.\n3https://github.com/facebookresearch/metaseq/tree/main/pr\nojects/OPT-IML\n3.2 Finetuning Data\nOur finetuning corpus is comprised of 10 datasets:\nProofWriter (Tafjord et al., 2020), StrategyQA\n(Geva et al., 2021), ECQA (Aggarwal et al., 2021),\nCoQA (Reddy et al., 2019), GSM8K (Cobbe et al.,\n2021), AQUA-RAT (Ling et al., 2017), ESNLI\n(Camburu et al., 2018), MATH (Hendrycks et al.,\n2021c), CoS-E (Rajani et al., 2019), WinoWhy\n(Zhang et al., 2020). These 10 finetuning datasets\ncollectively contain 6 different reasoning skills:\nlogical reasoning, causal reasoning, commensense\nreasoning, textual entailment, mathematics, abduc-\ntive reasoning. In addition, these 10 datasets all\ncome with instructions, demonstration examples\nand explanations. This enables fair comparison\nof OPT-FT and OPT-CoT models. More details\nabout finetuning corpus can be found in Table 5 in\nSection A.2. More details about development data\nselection can be found in the Appendix. A.3.\n3.3 Evaluation\nTemplates Following (Wei et al., 2021b), to con-\ntrol for the effect of variable prompt templates, we\nadopt different templates (T) during inference stage\nin our experiments:\nT1: instruction + demonstration examples with\nexplanations + \"let’s think step by step\";\nT2: instruction + \"Please give a short explanation\nafter the answer\" + demonstration examples with\nexplanations + \"let’s think step by step\"\nT3: instruction + \"Please give a short explanation\nafter the answer\" + demonstration examples with\nexplanations\nT4: \"Please give a short explanation after the an-\nswer\" + demonstration examples with explanations\n+ \"Let’s think step by step\"\nT5: instructions + demonstrations\nFor each dataset, we report the average and max\nscore among these five templates. The final aggre-\ngated results (including aggregated average score\nand aggregated max score) are reported by further\naveraging across all datasets. Unless specified oth-\nerwise, the default score refers to the aggregated\nmax score among five templates.\nEvaluation metrics. Since our benchmark con-\ntains both classification and generation tasks, we\ncannot use classification accuracy to evaluate all\nthe tasks. Following FLAN (Wei et al., 2021b), we\nappend classification choices at the end of prompts\nand ask models to generate answers. Thus, clas-\nsification tasks can be treated as a special case of\n1058\ngeneration tasks. Accordingly, we use ROUGE-L\n(Lin, 2004) to measure the performance of both\nclassification and generation tasks and report the\naggregated score. Similar to Chung et al. (2022),\nwe also use exact-match score which is more suit-\nable for tasks with short answers. Additionally, we\ncompute relaxed-match score which is a relaxed\nversion of exact-match. Specifically, we normal-\nize ground truth answers and predictions to have\nall text in lower case and remove punctuation and\nextra white spaces.\n4 Analysis\n4.1 Does finetuning help?\nFigure 3 demonstrates the performance averaged\nacross all evaluation tasks in our benchmark.\nRationale-based finetuning (OPT-CoT) has been\nshown to improve the performance of the 1.3B\nmodel by 3.89% in terms of the aggregated max\nROUGE-L score and 3.83% in terms of the aggre-\ngated max exact-match score. As for 13B model,\nOPT-CoT gains the improvement by 15.22% in re-\ngard of aggregated max ROUGE-L score, 12.64%\nin regard of aggregated max exact-match score.\nHowever, finetuning (OPT-FT) sometimes yields\nworse results than the vanilla pre-trained model.\n4.2 What does LLMs learn during finetuning?\nWe find that CoT-finetuning improves performance\non reasoning tasks in general. However, what ex-\nactly does the LLMs learn during the finetuning\nstage is still under explored. Thus, we study the\nrole of finetuning from three perspectives: data\nmemorization, reasoning skill transfer, and prompt\ntemplate memorization.\n4.2.1 Data Memorization\nGururangan et al. (2020) finds that the performance\ngain is larger when the finetuning dataset is more\ndissimilar to the pre-training dataset. However,\ntheir conclusion is made by a single-task finetun-\ning. They evaluate their model on the same dataset\nthat was used for finetuning. A more thorough eval-\nuation dictates that finetuned models (Wei et al.,\n2021b; Chung et al., 2022) be evaluated on held-\nout datasets. As such, in Figure 2 in blocks (B) and\n(C) we show two potential ways of finetuning and\ninference as illustrated here in our paper.\nTo confirm that the improvement in finetuning\nperformance is due to the increased amount of data\nseen during the finetuning stage, we measure the\ndissimilarity between the training data used in fine-\ntuning and evaluation, respectively. If higher simi-\nlarity leads to better performance, it may indicate\nthat the improvements of finetuned LLMs are due\nto seeing more similar data during the finetuning\nstage. Following (Gururangan et al., 2020), we\nuse unigram vocabulary overlap to measure the\ndata similarity. More specifically, we divide our\ntasks into three categories: The first category has\n10 datasets which consists of up to 10% overlap\nbetween the finetuning data and evaluation data.\nThe second category comprises 3 datasets with an\noverlap between 10% and 30%. The third category\nhas 7 datasets with an overlap over 30%. Details\ncan be found in Table 7 in appendix A.5.\nWe measure the performance improvements of\nOPT-FT and OPT-CoT compared against the pre-\ntrained OPT model. We present both ROUGE-\nL score (top) and relaxed-match score (down) in\nFigure 5. The results indicate that there is no\nstrong correlation between the vocabulary overlap\nbetween fineuning and evaluation datasets and the\nperformance of the model (neither a higher nor a\nlower vocabulary overlap always translate to a per-\nformance improvement). OPT-CoT achieves the\nbest ROUGE-L and relaxed-match scores both in\nsettings when there is a medium (10%-30%) level\nof vocabulary overlap. We don’t observe a consis-\ntent pattern on OPT-FT models either. Overall, for\nthese challenging tasks, seeing similar data during\nfinetuning stage does not guarantee performance\nimprovement.\n4.2.2 Reasoning Skill Transfer\nTable 6 illustrates the reasoning skills present in\neach stage. 7 skills can be learned from pretrain-\ning data. Appendix. A.4 shows more details about\npretraining data. 6 skills can be learned from fine-\ntuning data (Table 5). Using ALERT we measure a\ntotal of 10 reasoning skills in model evaluation.\nThe average ROUGE-L scores are calculated\nfor each reasoning skill on 6 models (1.3B OPT,\n1.3B OPT-FT, 1.3B OPT-CoT, 13B OPT, 13B OPT-\nFT, 13B OPT-CoT). Figure 7 shows the difference\nbetween OPT-FT and OPT, and the difference be-\ntween OPT-CoT and OPT models’ performance.\nFor example, OPT-FT 1.3B model yields on av-\nerage 3.5 less ROUGE-L points than OPT 1.3B\nmodel on the tasks of logical reasoning.\nFigure 7 contains 4 sub-figures, showing reason-\ning skills transfer results: ( i) The upper left sub-\nfigure shows 7 skills that are acquired during the\n1059\nFigure 3: Performance of pre-trained LM (OPT), finetuned LM (OPT-FT) and CoT-\nfinetuned LM (OPT-CoT) onALERT reasoning benchmark. Left charts show aggregated\nmax scores while right are average scores across 5 templates. Scores are averaged\nacross 20 tasks.\n1.3B 13B\nParameters\n0\n20\n40\n60\n80\n100Template following percentage\nOPT OPT-FT OPT-CoT\n1.3B 13B\nParameters\n0\n2\n4\n6\n8\n10\n12Standard deviation\nFigure 4: Analyzing the robustness\nof models in following the templates.\nLeft: template following percentage\nby each model; Right: standard devi-\nation of template following percent-\nage.\nPerformance Changes (%) compared to OPT\nVocabulary Overlaps\nFigure 5: Correlation between vocabulary overlap and per-\nformance improvement using 13B parameter models. The\ntop chart shows ROUGE-L while the bottom shows relaxed-\nmatch score.\npretraining stage (OPT pretraining data), and how\nmuch improvement can be obtained through meta-\nfinetuning (OPT-FT and OPT-CoT); (ii) The bot-\ntom left sub-figure illustrates that these 3 skills are\nharder to acquire during the pre-training stage, and\nthe amount of improvement that can be obtained\nthrough meta-finetuning; (iii) The upper right sub-\nfigure illustrates that such 7 skills are acquired dur-\ning the meta-finetuning stage through finetuning\ndatasets (Table 5). Do these skills show improve-\nment measured by evaluation benchmark? (iv) The\nbottom right sub-figure studies the reasoning skills\nthat were not learned in the finetuning stage, can\nthese skills be improved through meta-finetuning?\nWe study the answers to these questions below.\nFrom figure (ii) We observe that all four of the\nLLMs demonstrate enhanced reasoning capabili-\nties on textual entailment, abductive reasoning, and\nanalogical reasoning tasks. These abilities are not\nreadily acquired during the pretraining stage, as\nthe pretraining data consists only of plain text. On\nthe other hand, skills such as commonsense rea-\nsoning or spatial reasoning can be gained during\nthe pretraining stage, while the benefits of further\nfinetuning are not as pronounced. Additionally,\nGururangan et al. (2020) concluded that the more\ndissimilar the domain between pretraining and fine-\ntuning are, the higher the potential for finetuning\nto yield gains. We see the same trend but the do-\nmain in Gururangan et al. (2020) is defined by the\nvocabulary overlaps, while we define the domains\nby reasoning skills. From figure ( iii) we can see\nthat the reasoning skills gained during the meta-\nfinetuning stage may not necessarily transfer to the\nimprovement of the same skills on the evaluation\ndatasets.\nWe also observe that finetuning with OPT-CoT\nenables the model to acquire a wider range of rea-\nsoning skills, resulting in stronger performance on\nlogical and causal reasoning tasks, in addition to\nskills that consistently improve across all finetuned\nmodels.\n4.2.3 Data Format Memorization\nWe investigate whether finetuning can simply mem-\norize the template representation of the training\ndata, and the effect of data format on the robust-\nness of the models.\nEvaluation with relaxed-match score. We com-\npare two metrics: exact-match and relaxed-match.\nFrom Figure 3, we observe that OPT-FT is worse\nthan OPT when exact-match is used as the metric.\nHowever, when relaxed-match is used, OPT-FT\noutperforms OPT as shown in Figure 8. Relaxed-\nmatch score ignores punctuation, articles and ex-\ntra whitespace. This suggests that if we decouple\nperformance from format adherence, OPT-FT per-\nforms better than OPT. In other words, finetuning\nis helpful but it can make the output more noisy.\nThis explains the reason for the performance drop\nwhen exact-match is used as the metric.\n1060\nLogical, Causal, Commonsense, \nMath, Spatial, Argument, Deductive\nPretraining Reasoning Skills\nLogical, Causal, Commonsense, \nEntailment, Math, Abductive\nMeta-ﬁnetuning Reasoning Skills\nLogical, Causal, Commonsense, \nEntailment, Math, Abductive, Spatial, \nAnalogical, Argument, Deductive\nEvaluation Reasoning Skills\nSkills in Pretraining Data\nHeld-out Skills from Pretraining Data\n* Finetuning data in Sec. 3.2\n* ALERT benchmark\n* OPT pretraining data\nSkills in Meta-ﬁnetuning Data\nHeld-out Skills from Meta-ﬁnetuning Data\nFigure 6: Reasoning skills\nlearned during pretraining and\nmeta-finetuning stages, as well as\ntested through ALERT .\nLogical, Causal, Commonsense, \nMath, Spatial, Argument, Deductive\nPretraining Reasoning Skills\nLogical, Causal, Commonsense, \nEntailment, Math, Abductive\nMeta-ﬁnetuning Reasoning Skills\nLogical, Causal, Commonsense, \nEntailment, Math, Abductive, Spatial, \nAnalogical, Argument, Deductive\nEvaluation Reasoning Skills\nSkills in Pretraining Data\nHeld-out Skills from Pretraining Data\n* Finetuning data in Sec. 3.2\n* ALERT benchmark\n* OPT pretraining data\nSkills in Meta-ﬁnetuning Data\nHeld-out Skills from Meta-ﬁnetuning Data\nFigure 7: The ROUGE-L scores illustrating the difference between OPT-FT and OPT,\nas well as OPT-CoT and OPT models within each reasoning skill. Left: skills split by\npretraining data; Right: skills split by meta-finetuning data.\n1.3B 13B\nModel scale\n0\n10\n20\n30Max Relaxed-match\nOPT OPT-FT OPT-CoT\n1.3B 13B\nModel scale\n0\n5\n10\n15\n20\n25Average Relaxed-match\nFigure 8: Comparing pretraining and finetuning models with\nrelaxed match score. Left: aggregated best (max) performance\nacross 5 Templates; Right: aggregated average performance\nacross 5 Templates.\nTemplate following percentage. We check\nwhether the model can follow the template of the\ndemonstrations. For example, if a demonstration\nuses \"the answer is xxx because yyy\", then we\ncheck what percentage of instances can follow the\nexact same template as the demonstration. Figure 4\n(left) shows the average template following percent-\nage for each model. Both OPT and OPT-CoT con-\nsistently show that they can follow demonstrations’\neven though OPT is not pre-trained on rationales.\nCompared to 1.3B models, larger models demon-\nstrate a greater overall ability to follow the template\nof the demonstrations. Compared to OPT and OPT-\nCoT, OPT-FT lacks the ability to follow diverse\ntemplates. This is because the OPT-FT training\nprocess does not contain any rationale data. Fine-\ntuning causes the model to become more biased\ntowards a particular template representation, while\nits ability to adapt to other templates becomes im-\npaired. It is worth noting that despite being trained\non rationales, the OPT-CoT model performs well\nwhen evaluated using non-CoT templates.\nRobustness To assess the robustness of each\nmodel to various templates, we compute the stan-\ndard deviation of ROUGE-L scores for each model\nacross five different templates. As we can see from\nFigure 4 (right), OPT is robust to different tem-\nplates, while OPT-FT has difficulties adapting to\nchanging templates. In general, finetuning (both\nOPT-FT and OPT-CoT) adversely affects the ro-\nbustness of the model and makes the model biased\ntowards a specific data format, however, OPT-CoT\nis better than general finetuning (OPT-FT).\nReasoning chain quality. Following (Golovneva\net al., 2022) we evaluate reasoning abilities of the\nmodels using ROSCOE scoring suite (Table 3). Look-\ning at each score in detail (Appendix C), we found\nthat overall across templates OPT-FT models pro-\nduce shorter, less informative chains, while OPT\nbaseline models produce long chains with high\namount of self-repetitions. 13B OPT-CoT chains\nshowed best quality despite some self-consistency\nand grammar issues. When comparing prompt tem-\nplates, models prompted with Template 5 produce\nshort chains, often without reasoning at all, even\nif they were fine-tuned on reasoning chains (OPT-\nCoT), suggesting overfitting to the prompt template.\nIn summary, models learn the data format rep-\nresentation and templates during finetuning stage.\nHowever, finetuned models contain bias towards\nthe data formats and template it has seen, which\npotentially reduces the robustness of the model to\nmore generalized settings. When comparing ro-\nbustness, OPT-CoT is better than OPT-FT, but it is\nstill not as robust as the pre-trained model.\n1061\n1.3B 13B\nMetrics OPT OPT-FT OPT-CoTOPT OPT-FT OPT-CoT\nROSCOE-SA0.936 0.921 0.938 0.936 0.923 0.940\nROSCOE-SS0.9250.923 0.920 0.9260.916 0.925\nROSCOE-LI0.8480.953 0.875 0.863 0.944 0.890\nROSCOE-LS0.7250.744 0.666 0.688 0.705 0.640\nTable 3: Summary of the ROSCOE evaluation results averaged\nacross templates. Each metric is bounded within [0,1], where\n1 indicates the perfect score and 0 corresponds to failure. In\neach row, values corresponding to the best-performing model\nare bolded, second best are underscored.\n5 Related Work\nLLMs that Reason. To improve LLMs’ reason-\ning abilities, Kojima et al. (2022) shows that LLMs\ncan be decent zero-shot reasoners by simply ap-\npending “Let’s think step by step” to the prompt.\nWei et al. (2022) adds a series of intermediate rea-\nsoning steps to improve LLMs’ reasoning abilities.\nWang et al. (2022a) further proposes to expand\nprompts to include rationales in each few-shot ex-\nample. Fu et al. (2022) discovers that prompting\nwith higher reasoning complexity achieves substan-\ntial gains on math word tasks. To tackle problems\nharder than demonstration examples, Zhou et al.\n(2022) first reduces a complex problem into a list of\nsubproblems and solve subproblems sequentially.\nAnother line of research is to improve the naive\ndecoding strategy, Wang et al. (2022b) introduces\na self-consistency strategy which selects the most\nconsistent answer among a set of reasoning paths.\nExisting Reasoning Benchmarks. Many bench-\nmarks are used for evaluating language models’\nperformance, such as BIG-Bench (Srivastava et al.,\n2022), Natural Instruction V2 (NIV2) (Wang et al.,\n2022c), MMLU (Hendrycks et al., 2020). Although\nthey contain some reasoning tasks, none of them\nare specifically designed to test models’ reasoning\nskills. For example, NIV2 contains 172 datasets\nand a total of 1554 tasks, including some reasoning\ntasks. It has several issues which make it inap-\npropriate to be directly used as a reasoning bench-\nmark: (1) it is designed for instruction-tuned mod-\nels and some tasks might be unsuitable for evaluat-\ning pretrained models or non-instruction finetuned\nmodels, as shown in Figure 1; (2) reasoning skills\nhave been divided into 27 categories while some\nof them have large overlaps, e.g. numerical reason-\ning, quantitative reasoning, reasoning on numbers;\n(3) some reasoning labels are wrongly labeled, e.g.\ntask393_plausible_result_generation gives textual\nentailment label but this task can hardly examine\nthe entailment skill.\nThe Curriculum benchmark (Chen and Gao,\n2022) is designed for probing LLMs’ reasoning\nabilities and covers 8 different reasoning skills.\nHowever, this work only focuses on classification\ntasks and it converts all examples into the Natu-\nral Language Inference (NLI) format to fit into a\nunified framework. We argue that the forced con-\nversion of all datasets into the NLI format does not\nalign with human natural conversational style. We\nobserved that even davinci-003 fails at some simple\ntasks due to their forced conversion, e.g. examples\nin Table 1. More discussion and results are shown\nin the Appendix B.\nFinetuning LLMs. LLMs meta-finetuned on a\nrange of NLP tasks have shown improved per-\nformance on held-out downstream tasks such as\nFLAN (Wei et al., 2021b), T0 (Sanh et al., 2021),\nTk-Instruct (Wang et al., 2022c) and Instruct-GPT\n(Ouyang et al., 2022). Following this approach,\nwe finetune OPT models and name this type of\nmodels as OPT-FT ((B) in Figure 2). Chung et al.\n(2022) further adds chain-of-thought data at fine-\ntuning stage and shows significant improvements.\nWe also study this type of models and name them\nas OPT-CoT ((C) in Figure 2). However, from\nprevious research it still remains unclear whether\nthe improvement comes from simply adding more\ntraining data or finetuning on rationales actually\nhelps. We conduct rigorous evaluations to address\nthis question.\n6 Conclusion\nWe introduceALERT, a carefully curated benchmark\nfor evaluating reasoning abilities of LLMs. It com-\nprises over 20 datasets and covers 10 different rea-\nsoning skills. Using this benchmark, we further\ninvestigate the impact of finetuning on these com-\nplex tasks. Our experiments reveal that LLMs do\nnot simply memorize training data, but are capable\nof learning various reasoning skills, such as textual\nentailment, abductive reasoning and analogical rea-\nsoning. While we found that finetuning generally\nleads to improved performance, we also discovered\nsome negative effects. LLMs tend to memorize\nthe data template representation and templates seen\nduring finetuning, thus reducing the robustness of\nthe model to generalized settings. CoT-finetuning\n(OPT-CoT) can alleviate this issue to some extent,\nbut it is still less robust compared to the vanilla\npre-trained model.\n1062\nLimitations\nALERT aims to encompass a wide range of rea-\nsoning skills, but some reasoning skills are miss-\ning, specifically in regards to symbolic reason-\ning (last letter concatenation task and coin flip\n(Wei et al., 2022)) and compositionality reason-\ning (SCAN (Lake and Baroni, 2018), COGS (Kim\nand Linzen, 2020) and CFQ (Keysers et al., 2019)).\nThese reasoning skills should be included in future\nwork.\nIn terms of computing power, we have experi-\nmented with models that were accessible to us. We\nacknowledge that there are larger models that we\nwere not able to train due to the limitations of our\ncomputational budget.\nDuring our analysis, we discovered that some\ndatasets contain noise, where even human experts\nare unable to provide accurate answers for certain\ninstances. While it is important to address this\nissue, it is a time-consuming process to carefully\nreview and clean each instance in the dataset. We\nplan to address this in future work.\nEthics Statement\nLarge language models (LLMs), due to potential\nbias in the training data, can be prone to gener-\nate toxic and unwanted content (Weidinger et al.,\n2021). However, in this paper, we are focused on\nreasoning tasks where the model is prompted to\nexplain its decisions, because of which our model\nfalls under contained generation. By providing\nclear prompts and constraints, we believe that this\nmight help guide the model’s output towards spe-\ncific, desired outcomes and reduce the likelihood\nof generating unwanted or harmful content, as op-\nposed to open ended text generation tasks.\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla,\nand Dinesh Garg. 2021. Explanations for Com-\nmonsenseQA: New Dataset and Models. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural\nLanguage Processing (V olume 1: Long Papers),\npages 3050–3065, Online. Association for Computa-\ntional Linguistics.\nBadr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing\nJin, Asli Celikyilmaz, and Mona Diab. 2023. Opt-\nr: Exploring the role of explanations in finetuning\nand prompting for reasoning skills of large language\nmodels. arXiv preprint arXiv:2305.12001.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\net al. 2021. Efficient large scale language mod-\neling with mixtures of experts. arXiv preprint\narXiv:2112.10684.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the\ninternational AAAI conference on web and social\nmedia, volume 14, pages 830–839.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-\nfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning\nabout physical commonsense in natural language.\nIn Thirty-Fourth AAAI Conference on Artificial\nIntelligence.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language explana-\ntions. Advances in Neural Information Processing\nSystems, 31.\nZeming Chen and Qiyue Gao. 2022. Curriculum: A\nbroad-coverage benchmark for linguistic phenomena\nin natural language understanding. arXiv preprint\narXiv:2204.06283.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\n1063\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nNancy Fulda, Nathan Tibbetts, Zachary Brown, and\nDavid Wingate. 2017. Harvesting common-sense\nnavigational knowledge for robotics from uncurated\ntext corpora. In Conference on Robot Learning,\npages 525–534. PMLR.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin\nCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\nand Asli Celikyilmaz. 2022. Roscoe: A suite of\nmetrics for scoring step-by-step reasoning.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nadapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021a. Measuring massive multitask language\nunderstanding. Proceedings of the International\nConference on Learning Representations (ICLR).\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021b. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021c. Measuring mathemati-\ncal problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874.\nMark Hopkins, Ronan Le Bras, Cristian Petrescu-\nPrahova, Gabriel Stanovsky, Hannaneh Hajishirzi,\nand Rik Koncel-Kedziorski. 2019. Semeval-2019\ntask 10: math question answering. In Proceedings\nof the 13th International Workshop on Semantic\nEvaluation, pages 893–899.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. arXiv preprint arXiv:1912.09713.\nNajoung Kim and Tal Linzen. 2020. Cogs: A compo-\nsitional generalization challenge based on semantic\ninterpretation. arXiv preprint arXiv:2010.05465.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJonathan Kobbe, Ioana Hulpu s, , and Heiner Stucken-\nschmidt. 2020. Unsupervised stance detection for ar-\nguments from consequences. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 50–60.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nBrenden Lake and Marco Baroni. 2018. Generaliza-\ntion without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks. In\nInternational conference on machine learning, pages\n2873–2882. PMLR.\nYash Kumar Lal, Nathanael Chambers, Raymond\nMooney, and Niranjan Balasubramanian. 2021.\nTellMeWhy: A dataset for answering why-questions\nin narratives. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 596–610, Online. Association for Computa-\ntional Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen,\nPei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. 2020. CommonGen: A constrained\ntext generation challenge for generative common-\nsense reasoning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1823–1840, Online. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale gen-\neration: Learning to solve and explain algebraic\nword problems. In Proceedings of the 55th Annual\nMeeting of the Association for Computational\n1064\nLinguistics (V olume1: Long Papers), pages 158–\n167, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn ACL.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Thomas L Griffiths. 2018. Evaluating\ntheory of mind in question answering. arXiv preprint\narXiv:1808.09352.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in pytorch. In NIPS\n2017 Workshop on Autodiff.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. arXiv preprint arXiv:1906.02361.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answer-\ning challenge. Transactions of the Association for\nComputational Linguistics, 7:249–266.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nChristopher Rytting and David Wingate. 2021. Lever-\naging the inductive bias of large language models\nfor abstract textual reasoning. Advances in Neural\nInformation Processing Systems, 34:17111–17122.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin\nChoi, and Claire Cardie. 2019. DREAM: A chal-\nlenge dataset and models for dialogue-based reading\ncomprehension. Transactions of the Association for\nComputational Linguistics.\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter\nClark. 2020. Proofwriter: Generating implications,\nproofs, and abductive statements over natural lan-\nguage. arXiv preprint arXiv:2012.13048.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. arXiv preprint arXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022a. Rationale-\naugmented ensembles in language models. arXiv\npreprint arXiv:2207.00747.\n1065\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al.\n2022c. Super-naturalinstructions:generalization via\ndeclarative instructions on 1600+ tasks. In EMNLP.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021a. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021b. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nNathaniel Weir, João Sedoc, and Benjamin Van Durme.\n2020. Cod3s: Diverse generation with discrete se-\nmantic signatures. arXiv preprint arXiv:2010.02882.\nJohannes Welbl, Nelson F Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 4602–4625,\nSeattle, United States. Association for Computational\nLinguistics.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart Van Merriënboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards ai-complete\nquestion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698.\nAdina Williams, Tristan Thrush, and Douwe Kiela.\n2022. Anlizing the adversarial natural language\ninference dataset. In Proceedings of the 5th\nAnnual Meeting of the Society for Computation in\nLinguistics, pages 23–54. Association for Computa-\ntional Linguistics.\nHongming Zhang, Xinran Zhao, and Yangqiu Song.\n2020. WinoWhy: A deep diagnosis of es-\nsential commonsense knowledge for answering\nWinograd schema challenge. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5736–5745, On-\nline. Association for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n19–27.\n1066\nA More Details about Data Usage\nA.1 Reasoning Benchmark\nTable 4 shows detailed reasoning benchmark.\nA.2 Training Corpus (cont. from §3.2)\nWe used 10 datasets for finetuning, which contain\n6 different reasoning skills.\nA.3 Development Data Details\nOur finetuning models are tuned on pretrained\nLLMs on the finetuning corpus with the goal of\nimproving the performance of unseen tasks. For\nexample, blocks (B) and (C) in Figure 2 are show-\ning models that are finetuned on tasks B,C,D and\nthe goal is to achieve good results on task A.\nCheckpoint selection can determine the final per-\nformance of the LLMs to a very large extent. There\nare several ways to select checkpoints: ( i) select\ncheckpoint of the last iteration; ( ii) select check-\npoint based on perplexity or loss from validation\ndatasets of finetuning corpus (validation datasets\nof task B, C, D); ( iii) select checkpoint based on\nperplexity or loss from validation datasets of evalu-\nation corpus (validation datasets of task A);\nIn order to achieve a better performance on evalu-\nation corpus, a common approach is to use methods\nlike (iii) to select a checkpoint. However, we would\nlike to prevent LLMs overfiting to the distribution\nof our final evaluation corpus. We initially used\nthe method (ii) but found that it did’t work well.\nHowever, this resulted in a distribution mismatch\nissue. We speculate this to the fact that some tasks\nin our finetuning corpus do not have a validation\nset. We thus select 3 tasks from NIV2 benchmark\nand compile a development set that does not have\nany overlaps with our finetuning data or evaluation\ndata. There are 3 datasets used as our develop-\nment set for checkpoint selection: task 247 dream\nanswer generation (Sun et al., 2019), task 118 se-\nmeval and task 10 open vocabulary mathematical\nanswer generation (Hopkins et al., 2019) and anli\nr1 entailment (Williams et al., 2022)\nA.4 Pretraining Data Analysis\nThe pre-training corpus of OPT model (Zhang et al.,\n2022) contains a concatenation of datasets used in\nRoBERTa (Liu et al., 2019), the Pile (Gao et al.,\n2020), and PushShift.io Reddit (Baumgartner et al.,\n2020; Roller et al., 2020).\nRoBERTa Three datasets in RoBERTa (Liu et al.,\n2019) are used as pretraining corpus: BookCorpus\n(Zhu et al., 2015), Stories (Trinh and Le, 2018),\nand CCNews (Liu et al., 2019). Deductive reason-\ning skill and spatial reasoning skill can be learned\nfrom stories dataset. Logical reasoning skill can be\nlearned from these three datasets.\nPile A subset of the Pile (Gao et al., 2020) are\nused as pre-training corpus, including Common-\nCrawl, DM Mathematics, Project Gutenberg, Hack-\nerNews, OpenSubtitles, OpenWebText2, USPTO,\nand Wikipedia. Mathematics reasoning skill can be\nlearned from DM Mathematics dataset. Causal Rea-\nsoning can be learned widely from OpenWebText2.\nCommensense reasoning skill can be learned from\nWikipedia.\nPushShift.io Reddit The longest chain of\ncomments in each thread are extracted from\nPushShift.io Reddit (Baumgartner et al., 2020). Ar-\ngument reasoning skill can be learned from this\ndataset.\nA.5 Vocabulary Overlaps (Cont. from §4.2.1)\nWe measure unigram vocabulary overlaps between\nour finetuning corpus and the evaluation corpus\n(reasoning benchmark).\nB Curriculum Benchmark Results (Cont.\nfrom §5)\nWe randomly selected one dataset from each rea-\nsoning skill and reported the results of GPT-3\n(Brown et al., 2020b) (text-davinci engine). Since\nall of the data has been converted to NLI format, we\nmeasure classification accuracy of GPT-3 model.\nFrom Table 8, we can see that even GPT-3 achieves\na pretty random results on these datasets. Through\nour analysis, we found that it is not because those\ntasks are too difficult for GPT-3, it is because\ncurriculum benchmark forcing all the data to be\nNLI format, resulting in unnatural data expression,\nwhich made GPT-3 fail on it. We conclude that the\ncurriculum benchmark may be suitable for classi-\nfication finetuned models, but it is not suitable for\nlanguage models for in-context learning.\nC Evaluating reasoning chains (Cont.\nfrom §5)\nFollowing (Golovneva et al., 2022) we evaluate\nreasoning abilities of the models using ROSCOE\nscoring suite (Table 10). Chains are evaluated\n1067\nReasoning\nSkills Task ID Datasets\nLogical\nReasoning\n62\n697\nbigbench repeat copy logic (Srivastava et al., 2022)\nmmmlu answer generation formal logic (Hendrycks et al., 2021a)\nCausal\nReasoning\n393\n1386\n1387\n1388\nplausible result generation (Weir et al., 2020)\nanli r2 entailment (Williams et al., 2022)\nanli r3 entailment (Williams et al., 2022)\ncb entailment (Wang et al., 2019)\nCommonsense\nReasoning\n80\n102\n591\n1286\npiqa answer generation (Bisk et al., 2020)\ncommongen sentence generation (Lin et al., 2020)\nsciq answer generation (Welbl et al., 2017)\nopenbookqa question answering (Mihaylov et al., 2018)\nTexual\nEntailment\n1386\n1387\n1388\n1344\nanli r2 entailment (Williams et al., 2022)\nanli r3 entailment (Williams et al., 2022)\ncb entailment (Wang et al., 2019)\nglue entailment classification (Wang et al., 2018)\nMathematics\n104\n119\n697\nsemeval closed vocabulary math answer generation (Hopkins et al., 2019)\nsemeval geometric math answer generation (Hopkins et al., 2019)\nmmmlu answer generation formal logic (Hendrycks et al., 2021a)\nAbductive\nReasoning 332 tellmewhy answer generation (Lal et al., 2021)\nSpatial\nReasoning\n83\n80\n151\nbabi t1 single supporting fact answer generation (Weston et al., 2015)\npiqa answer generation (Bisk et al., 2020)\ntomqa find location easy clean (Nematzadeh et al., 2018)\nAnalogical\nReasoning\n102\n1152\ncommongen sentence generation (Lin et al., 2020)\nbard analogical reasoning causation (Fulda et al., 2017)\nArgument\nReasoning\n513\n514\nargument stance classification (Kobbe et al., 2020)\nargument consequence classification (Kobbe et al., 2020)\nDeductive\nReasoning 216 rocstories correct answer generation (Mostafazadeh et al., 2016)\nTable 4: Details about ALERT benchmark.\nDatasets Train SizeVal Size Test Size Reasoning Skills\nProofWriter 69,810 10,190 20,030 Logical Reasoning, Causal Reasoning\nStrategyQA 2,290 - 490 Commonsense Reasoning\nECQA 7,598 1,090 2,194 Commonsense Reasoning\nCoQA 10,8647 7,983 - Textual Entailment\nGSM8K 7,473 - 1,319 Mathematics\nAQUA-RAT 97,467 254 254 Mathematics\nESNLI 549,367 9,842 9,824 Commonsense Reasoning, Logical Reasoning, Textual Entailment\nMATH 7,500 - 5,000 Mathematics\nCoS-E 9,741 1,221 - Commonsense Reasoning\nWinoWhy 273 - - Abductive Reasoning, Commonsense Reasoning\nTable 5: Training corpus for meta-finetuning OPT-FT and OPT-CoT. (Cont. from§ 3.2)\n1068\nTask ID Datasets Reasoning Skills\n247 dream answer generation (Sun et al., 2019)Logical Reasoning\nCommonsense Reasoning\n118 semeval open vocabulary mathematical\nanswer generation (Hopkins et al., 2019)\nCommonsense Reasoning\nMathematics\n1385 anli r1 entailment (Williams et al., 2022)\nTextual Entailment\nCommonsense Reasoning\nCausal Reasoning\nTable 6: Dev set for checkpoint selection\nCategory Datasets Vocabulary Overlaps\n0% to 10%\nbigbench repeat copy logic (Srivastava et al., 2022)\nbabi t1 single supporting fact answer generation (Weston et al., 2015)\nsemeval closed vocabulary math answer generation (Hopkins et al., 2019)\nsemeval geometric math answer generation (Hopkins et al., 2019)\ntomqa find location easy clean (Nematzadeh et al., 2018)\nplausible result generation (Weir et al., 2020)\nargument stance classification (Kobbe et al., 2020)\nargument consequence classification (Kobbe et al., 2020)\nmmmlu answer generation formal logic (Hendrycks et al., 2021a)\nbard analogical reasoning causation (Fulda et al., 2017)\n1.59%\n0.38%\n7.90%\n5.84%\n0.94%\n3.72%\n6.04%\n6.11%\n5.35%\n0.45%\n10% to 30%\ncommongen sentence generation (Lin et al., 2020)\ntellmewhy answer generation (Lal et al., 2021)\ncb entailment (Wang et al., 2019)\n29.31%\n28.05%\n20.97%\nover 30%\npiqa answer generation (Bisk et al., 2020)\nrocstories correct answer generation (Mostafazadeh et al., 2016)\nsciq answer generation (Welbl et al., 2017)\nopenbookqa question answering (Mihaylov et al., 2018)\nglue entailment classification (Wang et al., 2018)\nanli r2 entailment (Williams et al., 2022)\nanli r3 entailment (Williams et al., 2022)\n42.51%\n57.45%\n32.54%\n48.2%\n55.19%\n43.37%\n53.13%\nTable 7: V ocabulary overlap. Dissimilarity has been measured between training data (in Table 4) and evaluation data (in\nTable 5).\nDatasets Random scoreGPT-3 Davinci score\nBoolean 34.50% 31.80%\nPhysical 49.08% 50.00%\nEntailment Tree 50.88% 54.41%\nEvent Semantic 54.08% 55.89%\nSpatial 49.80% 49.30%\nTable 8: Classification accuracy by GPT-3 davinci-engine on the Curriculum benchmark.\nusing facebook/roscoe-512-roberta-base sentence\nembedding model. Evaluation results are detailed\nin Table 10. We found that the chain quality varies\nbetween models, in particular some reasoning as-\npects correlate with chain length as seen in Table 11.\nSimilar to (Chung et al., 2022), we noticed that non-\nfinetuned models (i.e. OPT-1.3B and OPT-13B)\ntend to produce long chains of reasoning, often\nrepeating themselves, which significantly affects\nthe quality of the chains and final scores (Figure 9).\nBelow we explore the differences between models’\noutputs under four perspectives: semantic align-\nment, semantic similarity, logical inference and\nlanguage coherence.\nC.1 Semantic Alignment\nDespite the fact that model 13B OPT-CoT on\naverage outperforms other models in almost\nall semantic alignment scores ( Faithfulness-Step,\nFaithfulness-Token, and Info-Step, see Table 10),\n1069\nThe cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\nThe answer is29 apples. The cafeteria started with 23 apples and bought 6 more, for a total of 23 + 6 = «23+6=29»29 apples.\nSelect the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n(A)Tdc;(B)Tcd;(C)Tcc;(D)dTc.\nThe answer is(B) Tcd. In predicate logic, the predicate (in this case, \"T\") is written before the subject and object of the sentence.\nSince David (d) is the teacher and Chris (c) is the student, the correct translation is Tcd, which reads as \"David teaches Chris.\"\nIsabella entered the hall. Olivia entered the hall. The apple is in the blue_treasure_chest. Olivia exited the hall. Isabella\nmoved the apple to the green_basket. Question: Where does Isabella think that Olivia searches for the apple? The answer\nis It is impossible for me to answer this question as I have no information about the thoughts or actions of Isabella or Olivia.\nAdditionally, I do not have access to browse any information beyond my training data, which ends in 2021 .\nTable 9: Examples from tasks that require reasoning skills and generated outputs from ChatGPT. The faild outputs are\nhighlighted in red.\nFigure 9: Distribution of the steps’ number across all tasks\nand templates varying between models (top) and between\ntemplates for OPT-CoT 13B model.\nthere is no common pattern across tasks (Fig 10).\nThe performance change between finetuned mod-\nels and corresponding pretrained version are sig-\nnificant4 on half of the tasks (11 tasks out of 20\nfor Faithfulness-* scores, and 9 out of 20 for Info-\nStep).\nRepetition-Token score variations exhibit differ-\nent behavior. Half of the tasks have higher num-\nber of repetitions between reasoning steps for pre-\ntrained models, with OPT-FT models generally out-\n4Significance is determined using T-test comparison,\nwhere p-value is below 0.05.\nperforming others (all performance improvements\nare significant). Generations produced by these\nmodels tend to be shorter in terms of the number\nof steps (Figure 9), so they contain less repetitions,\nbut also less semantic overlap with the context, thus\nin general having lower faithfulness and informa-\ntiveness. Some examples reflecting this behavior\nare provided in Table 12.\nScores are mostly aligned across Templates (Fig-\nure 11), except Template 5, that stands out in hav-\ning less aligned scores with respect to the context,\nbut also more self-consistent across the task. This\nis the only template that did not have any expla-\nnation in its prompt. Manual review showed that\ndespite CoT-finetuning, OPT-COT models tend to\nproduce 1-step answer-only generations (see exam-\nple in the Table 12, and Figure 9 for chains’ length\ndistribution), thus overfitting to the template rather\nthan learning from finetuning.\nIn summary, ROSCOE-SA is able to identify\naligned information, but it does not guarantee high-\nquality output. It will favor model with short ex-\nplanations and high semantic overlap with the ref-\nerence. We found that often OPT-FT-1.3B sim-\nply repeats one sentence from the input, instead\nof producing reasoning, and thus will get high-\nest ROSCOE-SA scores on these chains, while other\nmodels that produce some sort of reasoning will be\npunished.\nC.2 Semantic Similarity\nSemantic similarity scores support previous conclu-\nsions: models, finetuned on final answers (OPT-FT)\nexhibit lower similarity with respect to the base-\nline and CoT-finetuned models, while having less\nrepetitions (Figure 12). Again, we attribute that to\nthe fact that these models produce short chains that\nlack detailed reasoning steps.\n1070\nOPT 1.3B OPT-FT 1.3B OPT-CoT 1.3B OPT 13B OPT-FT 13B OPT-CoT 13B\nROSCOE-SA\nFaithfulness-Step 0.863 0.841 0.862 0.863 0.858 0.870\nFaithfulness-Token 0.936 0.921 0.938 0.936 0.923 0.940\nInfo-Step 0.857 0.829 0.854 0.858 0.846 0.861\nRepetition-Token 0.618 0.920 0.683 0.582 0.857 0.701\nROSCOE-SS\nInfo-Chain 0.925 0.909 0.920 0.926 0.916 0.925\nRepetition-Step 0.627 0.923 0.692 0.591 0.859 0.708\nROSCOE-LI\nSource Consistency 0.550 0.604 0.573 0.584 0.617 0.598\nSelf-Consistency 0.848 0.953 0.875 0.863 0.944 0.890\nROSCOE-LS\nPerplexity-Step 0.016 0.006 0.015 0.010 0.006 0.009\nPerplexity-Chain 0.022 0.006 0.020 0.016 0.006 0.013\nGrammar 0.725 0.744 0.666 0.688 0.705 0.640\nTable 10: ROSCOE evaluation results averaged across templates. Each metric is bounded within [0,1], where 1 indicates the\nperfect score and 0 corresponds to failure. Values corresponding to the best performing model are bolded, second best are\nunderscored.\nKendall’sτscore Kendall’sτp-value\nFaithfulness-Step -0.101 0.000\nFaithfulness-Token 0.039 0.000\nInfo-Step 0.054 0.000\nRepetition-Token -0.869 0.000\nInfo-Chain 0.009 0.000\nRepetition-Step -0.867 0.000\nSource Consistency -0.119 0.000\nSelf-Consistency -0.553 0.000\nPerplexity-Step 0.000 0.960\nPerplexity-Chain 0.369 0.000\nGrammar 0.013 0.000\nTable 11: Kendall correlation between evaluation perspective and number of steps in chain across all generated reasoning chains.\nStrong correlations (|τ| >0.4) are bolded.\nC.3 Logical Inference\nIn general, finetuned models are more self- and\nsource-consistent than respective baselines (Fig-\nure 13, significantly outperforming nonfinetuned\nmodels on 14 out of 20 tasks. We further looked\ninto the task 083, which is a task to find a right\nanswer given s given single supporting fact, po-\ntentially amongst a set of other irrelevant facts.\nManual review showed that although in this task\nfinetuned models tend to produce answers that are\nmore consistent, they often fail to select the fact\nthat is relevant to the question asked (see \"Spatial\nReasoning\" example in Table 12.\nC.4 Language Coherence\nDespite the variations in the values, Perplexity-*\nscore changes between models are mostly insignif-\nicant (15 out of 20 tasks, see Figure 14). Man-\nual review showed that all models produce mostly\ngrammatically correct content.\nD Licenses\nD.1 Data in ALERT\n• task62: Apache 2.0\n• task697: MIT\n• task393: MIT\n• task1386: CC BY-NC 4.0\n• task1387: CC BY-NC 4.0\n• task1388: CC BY-SA 3.0\n• task080: AFL 3.0\n• task102: MIT\n• task591: CC BY-NC-3.0\n1071\n• task1286: Apache 2.0\n• task1344: CC BY 4.0\n• task104: Please refer to: https://github.c\nom/allenai/semeval-2019-task-10#te\nrms-and-conditions\n• task119: Please refer to: https://github.c\nom/allenai/semeval-2019-task-10#te\nrms-and-conditions\n• task332: Please refer to: https://github.c\nom/StonyBrookNLP/tellmewhy\n• task083: CC BY 3.0\n• task151: Please refer to: https://github.c\nom/kayburns/tom-qa-dataset\n• task1152: Apache 2.0\n• task513: Please refer to: https://github.c\nom/dwslab/StArCon\n• task514: Please refer to: https://github.c\nom/dwslab/StArCon\n• task216: Please refer to: https://www.micr\nosoft.com/en-us/research/publicati\non/a-corpus-and-cloze-evaluation-f\nor-deeper-understanding-of-commons\nense-stories/\nD.2 Data in Dev set\n• task247: Dream dataset is intended for non-\ncommercial research purpose only. https:\n//github.com/nlpdata/dream.\n• task118: Please refer to: https://github.c\nom/allenai/semeval-2019-task-10#te\nrms-and-conditions\n• task 1385: CC BY-NC 4.0\nD.3 Data in Training set\n• ProofWriter: CC BY . Downloaded fromhttp\ns://aristo-data-public.s3.amazonaws\n.com/proofwriter/proofwriter-datas\net-V2020.12.3.zip\n• StrategyQA: MIT. Downloaded fromhttps:\n//storage.googleapis.com/ai2i/strate\ngyqa/data/strategyqa_dataset.zip.\n• ECQA: Literature and Wikipedia passages are\nshared under CC BY-SA 4.0 license. Mid-\ndle/High school exam passages are collected\nfrom RACE which comes with its own license.\n• GSM8K: MIT. Downloaded from https://\nraw.githubusercontent.com/openai/gra\nde-school-math/master/grade_school_\nmath/data/train.jsonl.\n• AQUA-RAT: Apache License, Version 2.0.\nDownloaded from: https://raw.github\nusercontent.com/deepmind/AQuA/master\n/train.json\n• ESNLI: please refer to https://github.c\nom/OanaMariaCamburu/e-SNLI/commit/b\nab0fa0212be9e5c6737da70c639a596f882e\n931. Downloaded from: https://raw.gith\nubusercontent.com/OanaMariaCamburu/e\n-SNLI/master/dataset/esnli_train_1.c\nsv\n• MATH: MIT. Downloaded from: https://\npeople.eecs.berkeley.edu/~hendrycks\n/MATH.tar\n• CoS-E: BSD-3-Clause license. Downloaded\nfrom: https://raw.githubusercontent.\ncom/salesforce/cos-e/master/data/v1.\n11/cose_train_v1.11_processed.jsonl\n• WinoWhy: MIT. Downloaded from: https:\n//raw.githubusercontent.com/HKUST-K\nnowComp/WinoWhy/master/winowhy.json\nE More Details about Model Training\nWe finetune our 1.3B models on 32 V100s with\nbatch size 8 on each GPU with totally 38 hours and\n21 minutes. We finetune our 13B models on 128\nV100s with batch size 4 on each GPU with totally\n13 hours and 26 minutes.\nFollowing OPT-IML (Iyer et al., 2022), we\nuse Fully Sharded Data Parallel (Artetxe et al.,\n2021) and the Megatron-LM Tensor Parallelism\n(Shoeybi et al., 2019). We inherit most model\nhyper-parameters for each model scale following\nOPT-IML. We pack our training examples into se-\nquences of length 2048, left-truncating examples\nthat overflow. We use Adam (Kingma and Ba,\n2014) with 32-bit state with (β1,β2) = (0.9,0.95),\nlinearly warming up the learning rate for 60 steps\nto the maximum, followed by linearly decaying it\nto 0.\n1072\nFigure 10: Normalized ROSCOE-SA scores per task, averaged across templates. Scores are normalised by their mean value\nacross OPT 1.3B model’s generations.\n1073\nFigure 11: Selected scores per task for OPT-CoT 13B model. Scores are normalised by their mean value across Template 1\ngenerations.\n1074\nFigure 12: Normalized ROSCOE-SS scores per task, averaged across templates. Scores are normalised by their mean value\nacross OPT 1.3B model’s generations.\nFigure 13: Normalized ROSCOE-LI scores per task, averaged across templates. Scores are normalised by their mean value\nacross OPT 1.3B model’s generations.\n1075\nFigure 14: Normalized ROSCOE-LC scores per task, averaged across templates. Scores are normalised by their mean value\nacross OPT 1.3B model’s generations.\n1076\nREASONING SKILL: Logistic Reasoning, Mathematics\nPROMPT: Please give a short explanation after the answer. Input: Identify the conclusion of the following argument. It is\nhard not to verify in our peers the same weakened intelligence due to emotions that we observe in our everyday patients.\nThe arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious\ncomplexes. Because of this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious\nknowledge contradicts. (Sigmund Freud, The Origin and Development of Psychoanalysis) (A)It is hard not to verify in our\npeers the same weakened intelligence due to emotions that we observe in our everyday patients. (B)The arrogance of our\nconsciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes. (C)Because\nof this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts.\n(D)It is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts.\nLet’s think step by step. Output: The answer is D because Because of the reasons specified in the argument, it is difficult to\nconvince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts. Input: Select the\nbest translation into predicate logic: No people drive on Mars. (Dx: x drives on Mars) (A)∼Pd(B)(∀x)(Px∨∼Dx)(C)\n(∀x)(Px⊃∼Dx)(D)∼DpLet’s think step by step. Output: The answer is C because Answer C means for everything, if that\nthing is a person, it doesn’t drive on Mars. So that’s the correct translation. Input: Select the best translation into predicate logic.\nDavid teaches Chris. (c: Chris; d: David; Txy: x teaches y) (A)Tdc (B)Tcd (C)Tcc (D)dTc Let’s think step by step. Output: The\nanswer is A because David teaches Chris, so in the relation Txy x should be ’d’ for davis and y should be ’c’ for Chris. Input:\nWhich of the given formulas of PL is the best symbolization of the following sentence? Either England’s importing beef is not a\nnecessary condition for France’s subsidizing agriculture, or China promotes human rights if South Africa supplies diamonds.\n(A)(∼F⊃E)∨(C≡∼S)(B)(∼E⊃F)∨(C⊃S)(C)(∼F⊃E)∨(C≡S)(D)∼(F⊃E)∨(S⊃C)Let’s think\nstep by step. Output: The answer is D because When not defined in the question, entities, relationships or statements are shown\nwith their first letter. In this example ’France subsidizing agriculture’ is shown with F for simplicity. The correct answer is D,\nbecause it uses the ’or’ operator between the two ’if’ clauses. Input: Select the best translation into predicate logic. Kevin is\nintroduced to José by Wilma. (j: José; k: Kevin; w: Wilma; Ixyz: x introduces y to z) (A)Iwjk (B)Ijkw (C)Ikjw (D)Iwkj Let’s\nthink step by step. Output:\nREFERENCE: D\nOPT 1.3B: The answer is A because all statements need to be presented with an argument.\nOPT-FT 1.3B: The answer is B\nOPT-CoT 1.3B: The answer is A because Using the ’or’ operator between the statements, we have : Kevin is introduced to José,\nbecause of its ’e’ factor in the equation. Wilma is introduced to Kevin because of its ’j’ factor in the equation. José is introduced\nto Wilma because of its ’y’ factor in the equation. In both cases, Wilma accepts ’x’ and ’y’.\nOPT 13B: The answer is A because introducing a person to another person is the action of introducing. It is possible that Kevin\nhas been introduced to Wilma but not Jose.\nOPT-FT 13B: The answer is A because when w is introduced, it says ’Wilma introduced Kevin to José’\nOPT-CoT 13B: The answer is D because the correct term is ’introduces’, the relation is ’j: kevin and w: wilma to k: x’, it is an\n’and’ relation. Using the operator ’and’ allows us to choose the greatest of the 5 possibilities.\nREASONING SKILL: Commonsense Reasoning\nPROMPT: Please give a short explanation after the answer. Input: Deserts are in extremely dry environments, so liquids will\nnaturally be (A) rainbow (B) plentiful (C) meager (D) talented Let’s think step by step. Output: The answer is C because This is\na good example and the output correctly answers the question. Input: Dairy has things in it that would make humans sick, so in\norder for dairy to be edible, this process has to take place (A) mixing (B) boiling (C) freezing (D) pasteurized Let’s think step\nby step. Output: The answer is D because in order for dairy to be edible, this process has to take place pasteurized. So, the\ncorrect answer is \"D\". Input: Digestion is when stomach acid breaks down (A) a pizza (B) a house (C) a rock (D) a car Let’s\nthink step by step. Output: The answer is A because Digestion is when stomach acid breaks down a food and only option A is a\nfood. So, this example is correct. Input: With a ruler I can measure (A) feelings (B) love (C) air (D) a phone Let’s think step by\nstep. Output:\nREFERENCE: D\nOPT 1.3B: A is a feeling, and all feelings come from feelings (A). Therefore, feelings are products of emotions (A). Therefore,\nemotions are created by emotions (A). Therefore, feelings and emotions are directly related (A). Therefore, emotions are created\nby emotions (A)...[repeats 249 times]\nOPT-FT 1.3B: A because feelings and love are measurements and air is a phone\nOPT-CoT 1.3B: The answer is D because With a ruler I can measure an object or range of objects and options are all to do with\nmeasuring. So, this example is correct.\nOPT 13B: The answer is B because With a ruler I can measure a length and only options B and D are in front of me. So, the\nanswer is B.\nOPT-FT 13B: The answer is D because a phone is an object\nOPT-CoT 13B: The answer is A because Answer: Measure feelings With a ruler I can measure.\n1077\nREASONING SKILL: Casual Reasoning, Textual entailment\nPROMPT: Please give a short explanation after the answer. Input: Premise: If you can dream it, you can achieve it — unless\nyou’re a goose trying to play a very human game of rugby. In the video above, one bold bird took a chance when it ran onto a\nrugby field mid-play. Things got dicey when it got into a tussle with another player, but it shook it off and kept right on running.\nAfter the play ended, the players escorted the feisty goose off the pitch. It was a risky move, but the crowd chanting its name\nwas well worth it. Video credit: Storyful / Nathan Swarbrick @swazza1990 <sep> Hypothesis: The crowd believed they knew\nthe name of the goose running on the field. Let’s think step by step. Output: The answer is Entailment because This is a good\nexample, the hypothesis sentence entails the given premise. Because the crowd was chanting its name, the crowd must have\nbelieved they knew the goose’s name. Input: Premise: Notley hopeful attacks on women politicians will fade in coming year\nAfter a politically nasty year that saw women politicians face a barrage of attacks on social media and angry demonstrators\nshouting to have them put behind bars, Premier Rachel Notley is optimistic there will be a return to civil discourse. In a year-end\ninterview with CBC News, Notley reflected on 2016, a year that saw hateful social-media attacks against MLA Sandra Jansen,\nand chants of “Lock her up” directed at the premier at an anti-carbon tax rally in Edmonton. <sep> Hypothesis: Notley reflected\non the chants of \"lock her up\" from previous to the year 2016. Let’s think step by step. Output: The answer is Contradiction\nbecause The Premise states Notley reflected on the chants from 2016, not before 2016. In this example hypothesis sentence\ncontradicts the given premise. Input: Premise: Mazda Sales (Thailand), a unit of Japanese carmaker Mazda Motor Corp,\nremains bullish about its sales prospects this year despite falling figures in the first six months, the Bangkok Post reported, citing\nits President Hidesuke Takesue. (bit.ly/1gr7HMd) —- NOTE: Reuters has not verified this story and does not vouch for its\naccuracy. (Bangkok Newsroom; Editing by Prateek Chatterjee) <sep> Hypothesis: Mazda Sales will be fired next week Let’s\nthink step by step. Output: The answer is Neutral because In this example the hypothesis can’t be proven one way or the other\nbased on the context. It’s set in the future and no one knows the future. So, hypothesis sentence is neutral with respect to a\ngiven premise. Input: Premise: How to clear nasal congestion quickly<br>Gently blow your nose. Perhaps the easiest and\nquickest way to reduce nasal congestion is to simply blow your nose into a soft tissue. Unfortunately, it’s not always possible to\ncompletely relieve congestion by merely blowing, but it’s always a good starting point. <sep> Hypothesis: strike a pose to fix\nyour nose Let’s think step by step. Output:\nREFERENCE: Contradiction\nOPT 1.3B: The answer is Modified Contradiction because In this example the hypothesis sentence suggests the method is \"Do\nnot hit your head.\" In fact, that’s the phrase a person who is rolling around on the ground, or who has had their head hit a hard\nsurface while trying to get a good sleeper on the sheets should try. Why is this? A head-on collision will ...[continues to\nhallucinate for 23 steps]\nOPT-FT 1.3B: Answer: Blow your nose into a soft tissue.\nOPT-CoT 1.3B: The answer is Contradiction because The hypothesis states that Nastai will solve his and nasal congestion only\nby blowing into his nose, but that’s not necessarily true, because he could have blown into his nose and caused irritation as well.\nOPT 13B: The answer is Yes because The answer is Yes. All the above information about clearing the nose is proven by the\ngiven premise, but if you still don’t know the answer, then we suggest you go here and click on \"How to clear your nose\nnaturally\" page.\nOPT-FT 13B: The answer: contradiction\nOPT-CoT 13B: The answer is contradiction because One cannot blow their nose if they have struck a pose.\nREASONING SKILL: Analogical Reasoning\nPROMPT: Please give a short explanation after the answer. Input: throw : fly. aspire : ? Let’s think step by step. Output: The\nanswer is attain because Fly is a causation of throw. Hence, the inferred analogy being causation, attain is the causation of\naspire. Input: listen : hear. drop : ? Let’s think step by step. Output: The answer is fall because Hear is a causation of listen.\nHence, the inferred analogy being causation, the causation of drop is fall. Input: hit : shatter. nourish : ? Let’s think step by step.\nOutput: The answer is grow because Shatter is a causation of hit. Hence, the inferred analogy being causation, the causation of\nnourish is grow. Input: throw : fly. ignite : ? Let’s think step by step. Output:\nREFERENCE: burn\nOPT 1.3B: The answer is move because fly is a causation of ignite. Hence, the inferred analogy being causation, the causation\nof move is fly.\nOPT-FT 1.3B: The answer is throw\nOPT-CoT 1.3B: The answer is ignite because Fire is a causation of throw. Hence, the inferred analogy being causation, the\ncausation of ignite is fly.\nOPT 13B: The answer is evaporate because Fly is a causation of throw. Hence, the inferred analogy being causation, the\ncausation of evaporate is ignite.\nOPT-FT 13B: The answer is fly because Ignite is a causation of throw\nOPT-CoT 13B: The answer is grow because Throw is a causation of grow.\n1078\nREASONING SKILL: Commonsense reasoning, Spatial Reasoning\nPROMPT: Please give a short explanation after the answer. Input: How do you remove keys from a computer keyboard for\ncleaning? Let’s think step by step. Output: The answer is Place the tip of a knife under the corner of a key and gently twist to\npop it off. because The process of removing keys from a keyboard for cleaning is descriptive and logical. Input: How can you\ndry wet replacement computer keyboard keys? Let’s think step by step. Output: The answer is by arranging paper towels on the\ncounter and evenly spacing out each key to dry, face side up, on top of them. because The steps involved in the process of drying\nwet replacement keyboard keys are logical. Input: To coat small plastic objects like replacement keyboard keys with soap Let’s\nthink step by step. Output: The answer is place them, water and liquid dishsoap in a sealed container. Shake vigorously until the\ncontainer is completely full of foam. because The process of coating small plastic objects like replacement keys is descriptive\nand logical. Input: How do you cut gnocchi without a knife? Let’s think step by step. Output: The answer is Press the edge of a\nspoon down into the dough to cut it. because The answer is quite detailed about the method to cut gnocchi when knife is not\navailable. Input: How do you taste something? Let’s think step by step. Output: The answer is Place it in your mouth to taste.\nbecause To taste something one has to keep it in contact with the tongue. Hence it is logical to put it in the mouth to taste. Input:\nHow do you wear a bracelet? Let’s think step by step. Output: The answer is Put it on around your wrist. because Bracelet is\nworn around wrists. Hence it is correct answer. Input: To find an easy way to complete levels on a video game, Let’s think step\nby step. Output:\nREFERENCE: look up cheat codes to make the rules of the game different or null.\nOPT 1.3B: The answer is Enter key without pressing. because How to find the perfect way to complete levels on a video game\nis logical and descriptive.\nOPT-FT 1.3B: It tells you the easiest way to get all the coins and medals and the fastest way to get through the level.\nOPT-CoT 1.3B: The answer is play the game. because If you play a game to finish the levels then you can call it a game.\nOPT 13B: The answer is Complete the level as fast as possible to get a high score, by jumping through the stage as quickly as\npossible. because The answer is explicit and very clear that how a user can complete levels on a video game with just a hint.\nOPT-FT 13B: The answer is try harder or use a guide, because The process of finding an easy way to complete levels on a video\ngame is descriptive and logical.\nOPT-CoT 13B: The answer is YouTube to find out how to get past a level. because YouTube is used to find out ways to complete\na game easily.\nREASONING SKILL: Spatial Reasoning\nPROMPT: Instructions: You will be given a passage consisting of set of facts and a question as input. The task is to answer a\nquestion of form ’Where is <person_name>?’ using one of the given facts to determine the latest location of the person. Answer\nshould be a word/phrase describing the location from the supporting fact. Avoid answers that are incomplete or incorrect.\nPositive Examples: Input: Passage: Mary moved to the bathroom. John went to the hallway. Question: Where is Mary? Output:\nbathroom Input: Passage: Mary moved to the bathroom. John went to the hallway. Daniel went back to the hallway. Sandra\nmoved to the garden. Question: Where is Daniel? Output: hallway Input: Passage: Mary moved to the bathroom. John went to\nthe hallway. Daniel went back to the hallway. Sandra moved to the garden. John moved to the office. Sandra journeyed to\nthe bathroom. Question: Where is Daniel? Output: hallway Input: Passage: John moved to the hallway. Mary travelled to the\nhallway. Sandra went to the bedroom. Sandra went back to the hallway. Question: Where is Mary? Output:\nREFERENCE: hallway\nOPT 1.3B: bathroom\nOPT-FT 1.3B: hallway\nOPT-CoT 1.3B: hallway\nOPT 13B: bathroom\nOPT-FT 13B: bathroom\nOPT-CoT 13B: hallway\nTable 12: Examples from tasks that require different reasoning skills and generated outputs. The failed outputs are highlighted\nin red. Outputs that are different from the reference, but can be valid because the question is ambiguous are highlighted in blue.\nAll except the one are examples generated using prompt Template 4, last example used Template 5.\n1079\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe did not see any potential risks in our paper\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nOur data and pretrained model may contain scientiﬁc artifacts. Section 2 (data part). Section 3 (model\npart)\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 2 and Section 3\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection D in appendix\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 2 and Section D in appendix\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo, we use public datasets.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo, we use public datasets.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1080\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3 and 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1081",
  "topic": "Ping (video games)",
  "concepts": [
    {
      "name": "Ping (video games)",
      "score": 0.8386567831039429
    },
    {
      "name": "Computer science",
      "score": 0.6206328272819519
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.48416024446487427
    },
    {
      "name": "Natural language processing",
      "score": 0.39058321714401245
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3569055497646332
    },
    {
      "name": "Computer security",
      "score": 0.08627733588218689
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}