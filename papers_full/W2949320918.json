{
  "title": "Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation",
  "url": "https://openalex.org/W2949320918",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A1993312157",
      "name": "Daniel Loureiro",
      "affiliations": [
        "Universidade do Porto",
        "INESC TEC"
      ]
    },
    {
      "id": "https://openalex.org/A4225910856",
      "name": "Alípio Jorge",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2101293500",
    "https://openalex.org/W2517456239",
    "https://openalex.org/W2963956638",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2963850840",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2550186622",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W2891031945",
    "https://openalex.org/W2518202280",
    "https://openalex.org/W2251322122",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2163908084",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W2251537235",
    "https://openalex.org/W2035717317",
    "https://openalex.org/W2757205734",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963606136",
    "https://openalex.org/W2949415641",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3088413834",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2813700965",
    "https://openalex.org/W1971220772",
    "https://openalex.org/W2899220776",
    "https://openalex.org/W2153579005"
  ],
  "abstract": "Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5682–5691\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n5682\nLanguage Modelling Makes Sense: Propagating Representations through\nWordNet for Full-Coverage Word Sense Disambiguation\nDaniel Loureiro, Al´ıpio M´ario Jorge\nLIAAD - INESC TEC\nFaculty of Sciences - University of Porto, Portugal\ndloureiro@fc.up.pt, amjorge@fc.up.pt\nAbstract\nContextual embeddings represent a new gener-\nation of semantic representations learned from\nNeural Language Modelling (NLM) that ad-\ndresses the issue of meaning conﬂation ham-\npering traditional word embeddings. In this\nwork, we show that contextual embeddings\ncan be used to achieve unprecedented gains\nin Word Sense Disambiguation (WSD) tasks.\nOur approach focuses on creating sense-level\nembeddings with full-coverage of WordNet,\nand without recourse to explicit knowledge of\nsense distributions or task-speciﬁc modelling.\nAs a result, a simple Nearest Neighbors ( k-\nNN) method using our representations is able\nto consistently surpass the performance of pre-\nvious systems using powerful neural sequenc-\ning models. We also analyse the robustness\nof our approach when ignoring part-of-speech\nand lemma features, requiring disambiguation\nagainst the full sense inventory, and revealing\nshortcomings to be improved. Finally, we ex-\nplore applications of our sense embeddings for\nconcept-level analyses of contextual embed-\ndings and their respective NLMs.\n1 Introduction\nWord Sense Disambiguation (WSD) is a core\ntask of Natural Language Processing (NLP) which\nconsists in assigning the correct sense to a word\nin a given context, and has many potential ap-\nplications (Navigli, 2009). Despite breakthroughs\nin distributed semantic representations (i.e. word\nembeddings), resolving lexical ambiguity has re-\nmained a long-standing challenge in the ﬁeld. Sys-\ntems using non-distributional features, such as It\nMakes Sense (IMS, Zhong and Ng, 2010), remain\nsurprisingly competitive against neural sequence\nmodels trained end-to-end. A baseline that simply\nchooses the most frequent sense (MFS) has also\nproven to be notoriously difﬁcult to surpass.\nSeveral factors have contributed to this limited\nprogress over the last decade, including lack of\nstandardized evaluation, and restricted amounts of\nsense annotated corpora. Addressing the eval-\nuation issue, Raganato et al. (2017a) has intro-\nduced a uniﬁed evaluation framework that has al-\nready been adopted by the latest works in WSD.\nAlso, even though SemCor (Miller et al., 1994)\nstill remains the largest manually annotated cor-\npus, supervised methods have successfully used\nlabel propagation (Yuan et al., 2016), semantic\nnetworks (Vial et al., 2018) and glosses (Luo\net al., 2018b) in combination with annotations to\nadvance the state-of-the-art. Meanwhile, task-\nspeciﬁc sequence modelling architectures based\non BiLSTMs or Seq2Seq (Raganato et al., 2017b)\nhaven’t yet proven as advantageous for WSD.\nUntil recently, the best semantic representations\nat our disposal, such as word2vec (Mikolov et al.,\n2013) and fastText (Bojanowski et al., 2017), were\nbound to word types (i.e. distinct tokens), con-\nverging information from different senses into the\nsame representations (e.g. ‘play song’ and ‘play\ntennis’ share the same representation of ‘play’).\nThese word embeddings were learned from un-\nsupervised Neural Language Modelling (NLM)\ntrained on ﬁxed-length contexts. However, by\nrecasting the same word types across different\nsense-inducing contexts, these representations be-\ncame insensitive to the different senses of poly-\nsemous words. Camacho-Collados and Pilehvar\n(2018) refer to this issue as the meaning conﬂa-\ntion deﬁciency and explore it more thoroughly in\ntheir work.\nRecent improvements to NLM have allowed for\nlearning representations that are context-speciﬁc\nand detached from word types. While word em-\nbedding methods reduced NLMs to ﬁxed repre-\nsentations after pretraining, this new generation\nof contextual embeddings employs the pretrained\n5683\nNLM to infer different representations induced by\narbitrarily long contexts. Contextual embeddings\nhave already had a major impact on the ﬁeld, driv-\ning progress on numerous downstream tasks. This\nsuccess has also motivated a number of iterations\non embedding models in a short timespan, from\ncontext2vec (Melamud et al., 2016), to GPT (Rad-\nford et al., 2018), ELMo (Peters et al., 2018), and\nBERT (Devlin et al., 2019).\nBeing context-sensitive by design, contextual\nembeddings are particularly well-suited for WSD.\nIn fact, Melamud et al. (2016) and Peters et al.\n(2018) produced contextual embeddings from the\nSemCor dataset and showed competitive results on\nRaganato et al. (2017a)’s WSD evaluation frame-\nwork, with a surprisingly simple approach based\non Nearest Neighbors (k-NN). These results were\npromising, but those works only produced sense\nembeddings for the small fraction of WordNet\n(Fellbaum, 1998) senses covered by SemCor, re-\nsorting to the MFS approach for a large number\nof instances. Lack of high coverage annotations\nis one of the most pressing issues for supervised\nWSD approaches (Le et al., 2018).\nOur experiments show that the simple k-NN\nw/MFS approach using BERT embeddings suf-\nﬁces to surpass the performance of all previous\nsystems. Most importantly, in this work we intro-\nduce a method for generating sense embeddings\nwith full-coverage of WordNet, which further im-\nproves results (additional 1.9% F1) while forgo-\ning MFS fallbacks. To better evaluate the ﬁtness\nof our sense embeddings, we also analyse their\nperformance without access to lemma or part-of-\nspeech features typically used to restrict candi-\ndate senses. Representing sense embeddings in the\nsame space as any contextual embeddings gener-\nated from the same pretrained NLM eases intro-\nspections of those NLMs, and enables token-level\nintrinsic evaluations based on k-NN WSD perfor-\nmance. We summarize our contributions1 below:\n•A method for creating sense embeddings for\nall senses in WordNet, allowing for WSD\nbased on k-NN without MFS fallbacks.\n•Major improvement over the state-of-the-art\non cross-domain WSD tasks, while exploring\nthe strengths and weaknesses of our method.\n•Applications of our sense embeddings for\nconcept-level analyses of NLMs.\n1Code and data: github.com/danlou/lmms\n2 Language Modelling Representations\nDistributional semantic representations learned\nfrom Unsupervised Neural Language Modelling\n(NLM) are currently used for most NLP tasks. In\nthis section we cover aspects of word and contex-\ntual embeddings, learned from from NLMs, that\nare particularly relevant for our work.\n2.1 Static Word Embeddings\nWord embeddings are distributional semantic rep-\nresentations usually learned from NLM under one\nof two possible objectives: predict context words\ngiven a target word (Skip-Gram), or the inverse\n(CBOW) (word2vec, Mikolov et al., 2013). In\nboth cases, context corresponds to a ﬁxed-length\nwindow sliding over tokenized text, with the tar-\nget word at the center. These modelling objectives\nare enough to produce dense vector-based repre-\nsentations of words that are widely used as pow-\nerful initializations on neural modelling architec-\ntures for NLP. As we explained in the introduc-\ntion, word embeddings are limited by meaning\nconﬂation around word types, and reduce NLM\nto ﬁxed representations that are insensitive to con-\ntexts. However, with fastText (Bojanowski et al.,\n2017) we’re not restricted to a ﬁnite set of repre-\nsentations and can compositionally derive repre-\nsentations for word types unseen during training.\n2.2 Contextual Embeddings\nThe key differentiation of contextual embeddings\nis that they are context-sensitive, allowing the\nsame word types to be represented differently ac-\ncording to the contexts in which they occurr. In\norder to be able to produce new representations\ninduced by different contexts, contextual embed-\ndings employ the pretrained NLM for inferences.\nAlso, the NLM objective for contextual embed-\ndings is usually directional, predicting the previ-\nous and/or next tokens in arbitrarily long contexts\n(usually sentences). ELMo (Peters et al., 2018)\nwas the ﬁrst implementation of contextual embed-\ndings to gain wide adoption, but it was shortly af-\nter followed by BERT (Devlin et al., 2019) which\nachieved new state-of-art results on 11 NLP tasks.\nInterestingly, BERT’s impressive results were ob-\ntained from task-speciﬁc ﬁne-tuning of pretrained\nNLMs, instead of using them as features in more\ncomplex models, emphasizing the quality of these\nrepresentations.\n5684\n3 Word Sense Disambiguation (WSD)\nThere are several lines of research exploring dif-\nferent approaches for WSD (Navigli, 2009). Su-\npervised methods have traditionally performed\nbest, though this distinction is becoming increas-\ningly blurred as works in supervised WSD start\nexploiting resources used by knowledge-based ap-\nproaches (e.g. Luo et al., 2018a; Vial et al., 2018).\nWe relate our work to the best-performing WSD\nmethods, regardless of approach, as well as meth-\nods that may not perform as well but involve pro-\nducing sense embeddings. In this section we in-\ntroduce the components and related works that are\nmost relevant for our approach.\n3.1 Sense Inventory, Attributes and Relations\nThe most popular sense inventory is WordNet,\na semantic network of general domain concepts\nlinked by a few relations, such as synonymy and\nhypernymy. WordNet is organized at different ab-\nstraction levels, which we describe below. Follow-\ning the notation used in related works, we repre-\nsent the main structure of WordNet, called synset,\nwith lemma#\nPOS , where lemma corresponds to\nthe canonical form of a word, POS corresponds to\nthe sense’s part-of-speech (n oun, v erb, a djective\nor adverb), and # further speciﬁes this entry.\n•Synsets: groups of synonymous words that\ncorrespond to the same sense, e.g. dog1\nn.\n•Lemmas: canonical forms of words, may be-\nlong to multiple synsets, e.g. dog is a lemma\nfor dog1\nn and chase1\nv, among others.\n•Senses: lemmas specifed by sense (i.e.\nsensekeys), e.g. dog%1:05:00::, and domes-\ntic dog%1:05:00:: are senses of dog1\nn.\nEach synset has a number of attributes, of which\nthe most relevant for this work are:\n•Glosses: dictionary deﬁnitions, e.g. dog1\nn has\nthe deﬁnition ‘a member of the genus Ca...’.\n•Hypernyms: ‘type of’ relations between\nsynsets, e.g. dog1\nn is a hypernym of pug1\nn.\n•Lexnames: syntactical and logical groupings,\ne.g. the lexname for dog1\nn is noun.animal.\nIn this work we’re using WordNet 3.0, which\ncontains 117,659 synsets, 206,949 unique senses,\n147,306 lemmas, and 45 lexnames.\n3.2 WSD State-of-the-Art\nWhile non-distributional methods, such as Zhong\nand Ng (2010)’s IMS, still perform competitively,\nthere are have been several noteworthy advance-\nments in the last decade using distributional rep-\nresentations from NLMs. Iacobacci et al. (2016)\nimproved on IMS’s performance by introducing\nword embeddings as additional features.\nYuan et al. (2016) achieved signiﬁcantly im-\nproved results by leveraging massive corpora to\ntrain a NLM based on an LSTM architecture. This\nwork is contemporaneous with Melamud et al.\n(2016), and also uses a very similar approach for\ngenerating sense embeddings and relying onk-NN\nw/MFS for predictions. Although most perfor-\nmance gains stemmed from their powerful NLM,\nthey also introduced a label propagation method\nthat further improved results in some cases. Cu-\nriously, the objective Yuan et al. (2016) used for\nNLM (predicting held-out words) is very evoca-\ntive of the cloze-style Masked Language Model\nintroduced by Devlin et al. (2019). Le et al. (2018)\nreplicated this work and offers additional insights.\nRaganato et al. (2017b) trained neural sequenc-\ning models for end-to-end WSD. This work re-\nframes WSD as a translation task where sequences\nof words are translated into sequences of senses.\nThe best result was obtained with a BiLSTM\ntrained with auxilliary losses speciﬁc to parts-of-\nspeech and lexnames. Despite the sophisticated\nmodelling architecture, it still performed on par\nwith Iacobacci et al. (2016).\nThe works of Melamud et al. (2016) and Pe-\nters et al. (2018) using contextual embeddings for\nWSD showed the potential of these representa-\ntions, but still performed comparably to IMS.\nAddressing the issue of scarce annotations, re-\ncent works have proposed methods for using re-\nsources from knowledge-based approaches. Luo\net al. (2018a) and Luo et al. (2018b) combine in-\nformation from glosses present in WordNet, with\nNLMs based on BiLSTMs, through memory net-\nworks and co-attention mechanisms, respectively.\nVial et al. (2018) follows Raganato et al. (2017b)’s\nBiLSTM method, but leverages the semantic net-\nwork to strategically reduce the set of senses re-\nquired for disambiguating words.\nAll of these works rely on MFS fallback. Addi-\ntionally, to our knowledge, all also perform disam-\nbiguation only against the set of admissible senses\ngiven the word’s lemma and part-of-speech.\n5685\n3.3 Other methods with Sense Embeddings\nSome works may no longer be competitive with\nthe state-of-the-art, but nevertheless remain rel-\nevant for the development of sense embeddings.\nWe recommend the recent survey of Camacho-\nCollados and Pilehvar (2018) for a thorough\noverview of this topic, and highlight a few of the\nmost relevant methods. Chen et al. (2014) initial-\nizes sense embeddings using glosses and adapts\nthe Skip-Gram objective of word2vec to learn and\nimprove sense embeddings jointly with word em-\nbeddings. Rothe and Sch ¨utze (2015)’s AutoEx-\ntend method uses pretrained word2vec embed-\ndings to compose sense embeddings from sets\nof synonymous words. Camacho-Collados et al.\n(2016) creates the NASARI sense embeddings us-\ning structural knowledge from large multilingual\nsemantic networks.\nThese methods represent sense embeddings in\nthe same space as the pretrained word embed-\ndings, however, being based on ﬁxed embedding\nspaces, they are much more limited in their abil-\nity to generate contextual representations to match\nagainst. Furthermore, none of these methods (or\nthose in §3.2) achieve full-coverage of the +200K\nsenses in WordNet.\n4 Method\nFigure 1: Illustration of our k-NN approach for WSD,\nwhich relies on full-coverage sense embeddings repre-\nsented in the same space as contextualized embeddings.\nFor simpliﬁcation, we label senses as synsets. Grey\nnodes belong to different lemmas (see §5.3).\nOur WSD approach is strictly based on k-NN\n(see Figure 1), unlike any of the works referred\npreviously. We avoid relying on MFS for lemmas\nthat do not occur in annotated corpora by gen-\nerating sense embeddings with full-coverage of\nWordNet. Our method starts by generating sense\nembeddings from annotations, as done by other\nworks, and then introduces several enhancements\ntowards full-coverage, better performance and in-\ncreased robustness. In this section, we cover each\nof these techniques.\n4.1 Embeddings from Annotations\nOur set of full-coverage sense embeddings is boot-\nstrapped from sense-annotated corpora. Sentences\ncontaining sense-annotated tokens (or spans) are\nprocessed by a NLM in order to obtain contextual\nembeddings for those tokens. After collecting all\nsense-labeled contextual embeddings, each sense\nembedding is determined by averaging its corre-\nsponding contextual embeddings. Formally, given\nn contextual embeddings ⃗ cfor some sense s:\n⃗ vs = 1\nn\nn∑\ni=1\n⃗ ci, dim(⃗ vs) = 1024\nIn this work we use pretrained ELMo and BERT\nmodels to generate contextual embeddings. These\nmodels can be identiﬁed and replicated with the\nfollowing details:\n•ELMo: 1024 (2x512) embedding dimen-\nsions, 93.6M parameters. Embeddings from\ntop layer (2).\n•BERT: 1024 embedding dimensions, 340M\nparameters, cased. Embeddings from sum of\ntop 4 layers ([-1,-4])2.\nBERT uses WordPiece tokenization that doesn’t\nalways map to token-level annotations (e.g. ‘mul-\ntiplication’ becomes ‘multi’, ‘##plication’). We\nuse the average of subtoken embeddings as the\ntoken-level embedding. Unless speciﬁed other-\nwise, our LMMS method uses BERT.\n4.2 Extending Annotation Coverage\nAs many have emphasized before (Navigli, 2009;\nCamacho-Collados and Pilehvar, 2018; Le et al.,\n2018), the lack of sense annotations is a major lim-\nitation of supervised approaches for WSD. We ad-\ndress this issue by taking advantage of the seman-\ntic relations in WordNet to extend the annotated\nsignal to other senses. Semantic networks are of-\nten explored by knowledge-based approaches, and\nsome recent works in supervised approaches as\nwell (Luo et al., 2018a; Vial et al., 2018). The\n2This was the conﬁguration that performed best out of the\nones on Table 7 of Devlin et al. (2018).\n5686\nguiding principle behind these approaches is that\nsense-level representations can be imputed (or im-\nproved) from other representations that are known\nto correspond to generalizations due to the net-\nwork’s taxonomical structure. Vial et al. (2018)\nleverages relations in WordNet to reduce the sense\ninventory to a minimal set of entries, making the\ntask easier to model while maintaining the ability\nto distinguish senses. We take the inverse path of\nleveraging relations to produce representations for\nadditional senses.\nOn §3.1 we covered synsets, hypernyms and\nlexnames, which correspond to increasingly ab-\nstract generalizations. Missing sense embeddings\nare imputed from the aggregation of sense embed-\ndings at each of these abstraction levels. In or-\nder to get embeddings that are representative of\nhigher-level abstractions, we simply average the\nembeddings of all lower-level constituents. Thus,\na synset embedding corresponds to the average\nof all of its sense embeddings, a hypernym em-\nbedding corresponds to the average of all of its\nsynset embeddings, and a lexname embedding\ncorresponds to the average of a larger set of synset\nembeddings. All lower abstraction representations\nare created before next-level abstractions to ensure\nthat higher abstractions make use of lower gener-\nalizations. More formally, given all missing senses\nin WordNet ˆs ∈ W, their synset-speciﬁc sense\nembeddings Sˆs, hypernym-speciﬁc synset embed-\ndings Hˆs, and lexname-speciﬁc synset embed-\ndings Lˆs, the procedure has the following stages:\n(1) if|Sˆs|> 0, ⃗ v ˆs = 1\n|Sˆs|\n∑⃗ vs, ∀⃗ vs ∈Sˆs\n(2) if|Hˆs|> 0, ⃗ vˆs = 1\n|Hˆs|\n∑⃗ vsyn, ∀⃗ vsyn ∈Hˆs\n(3) if|Lˆs|> 0, ⃗ vˆs = 1\n|Lˆs|\n∑⃗ vsyn, ∀⃗ vsyn ∈Lˆs\nIn Table 1 we show how much coverage extends\nwhile improving both recall and precision.\nF1 / P / R (without MFS)\nSource Coverage BERT ELMo\nSemCor 16.11% 68.9 / 72.4 / 65.7 63.0 / 66.2 / 60.1\n+ synset 26.97% 70.0 / 72.6 / 70.0 63.9 / 66.3 / 61.7\n+ hypernym 74.70% 73.0 / 73.6 / 72.4 67.2 / 67.7 / 66.6\n+ lexname 100% 73.8 / 73.8 / 73.8 68.1 / 68.1 / 68.1\nTable 1: Coverage of WordNet when extending to in-\ncreasingly abstract representations along with perfor-\nmance on the ALL test set of Raganato et al. (2017a).\n4.3 Improving Senses using the Dictionary\nThere’s a long tradition of using glosses for WSD,\nperhaps starting with the popular work of Lesk\n(1986), which has since been adapted to use distri-\nbutional representations (Basile et al., 2014). As\na sequence of words, the information contained\nin glosses can be easily represented in seman-\ntic spaces through approaches used for generating\nsentence embeddings. There are many methods\nfor generating sentence embeddings, but it’s been\nshown that a simple weighted average of word em-\nbeddings performs well (Arora et al., 2017).\nOur contextual embeddings are produced from\nNLMs using attention mechanisms, assigning\nmore importance to some tokens over others, so\nthey already come ‘pre-weighted’ and we embed\nglosses simply as the average of all of their contex-\ntual embeddings (without preprocessing). We’ve\nalso found that introducing synset lemmas along-\nside the words in the gloss helps induce better con-\ntextualized embeddings (specially when glosses\nare short). Finally, we make our dictionary em-\nbeddings (⃗ vd) sense-speciﬁc, rather than synset-\nspeciﬁc, by repeating the lemma that’s speciﬁc to\nthe sense, alongside the synset’s lemmas and gloss\nwords. The result is a sense-level embedding, de-\ntermined without annotations, that is represented\nin the same space as the sense embeddings we de-\nscribed in the previous section, and can be triv-\nially combined through concatenation or average\nfor improved performance (see Table 2).\nOur empirical results show improved perfor-\nmance by concatenation, which we attribute\nto preserving complementary information from\nglosses. Both averaging and concatenating repre-\nsentations (previously L2 normalized) also serves\nto smooth possible biases that may have been\nlearned from the SemCor annotations. Note that\nwhile concatenation effectively doubles the size of\nour embeddings, this doesn’t equal doubling the\nexpressiveness of the distributional space, since\nthey’re two representations from the same NLM.\nThis property also allows us to make predic-\ntions for contextual embeddings (from the same\nNLM) by simply repeating those embeddings\ntwice, aligning contextual features against sense\nand dictionary features when computing cosine\nsimilarity. Thus, our sense embeddings become:\n⃗ vs =\n[||⃗ vs||2\n||⃗ vd||2\n]\n, dim(⃗ vs) = 2048\n5687\nConﬁgurationsLMMS1024 LMMS2048 LMMS2348\nEmbeddings\nContextual(d=1024) \u0017 \u0017 \u0017 \u0017 \u0017\nDictionary(d=1024) \u0017 \u0017 \u0017 \u0017 \u0017\nStatic(d=300) \u0017 \u0017 \u0017\nOperation\nAverage \u0017\nConcatenation \u0017 \u0017 \u0017 \u0017\nPerf. (F1 on ALL)\nLemma & POS 73.8 58.7 75.0 75.4 73.9 58.7 75.4\nToken(Uninformed) 42.7 6.1 36.5 35.1 64.4 45.0 66.0\nTable 2: Overview of the different performance of various setups regarding choice of embeddings and combination\nstrategy. All results are for the 1-NN approach on the ALL test set of Raganato et al. (2017a). We also show results\nthat ignore the lemma and part-of-speech features of the test sets to show that the inclusion of static embeddings\nmakes the method signiﬁcantly more robust to real-world scenarios where such gold features may not be available.\n4.4 Morphological Robustness\nWSD is expected to be performed only against the\nset of candidate senses that are speciﬁc to a target\nword’s lemma. However, as we’ll explain in§5.3,\nthere are cases where it’s undesirable to restrict the\nWSD process.\nWe leverage word embeddings specialized for\nmorphological representations to make our sense\nembeddings more resilient to the absence of\nlemma features, achieving increased robustness.\nThis addresses a problem arising from the suscep-\ntibility of contextual embeddings to become en-\ntirely detached from the morphology of their cor-\nresponding tokens, due to interactions with other\ntokens in the sentence.\nWe choose fastText (Bojanowski et al., 2017)\nembeddings (pretrained on CommonCrawl),\nwhich are biased towards morphology, and avoid\nOut-of-V ocabulary issues as explained in§2.1. We\nuse fastText to generate static word embeddings\nfor the lemmas ( ⃗ vl) corresponding to all senses,\nand concatenate these word embeddings to our\nprevious embeddings. When making predictions,\nwe also compute fastText embeddings for tokens,\nallowing for the same alignment explained in\nthe previous section. This technique effectively\nmakes sense embeddings of morphologically\nrelated lemmas more similar. Empirical results\n(see Table 2) show that introducing these static\nembeddings is crucial for achieving satisfactory\nperformance when not ﬁltering candidate senses.\nOur ﬁnal, most robust, sense embeddings are thus:\n⃗ vs =\n\n\n||⃗ vs||2\n||⃗ vd||2\n||⃗ vl||2\n\n, dim(⃗ vs) = 2348\n5 Experiments\nOur experiments centered on evaluating our so-\nlution on Raganato et al. (2017a)’s set of cross-\ndomain WSD tasks. In this section we compare\nour results to the current state-of-the-art, and pro-\nvide results for our solution when disambiguating\nagainst the full set of possible senses in WordNet,\nrevealing shortcomings to be improved.\n5.1 All-Words Disambiguation\nIn Table 3 we show our results for all tasks of Ra-\nganato et al. (2017a)’s evaluation framework. We\nused the framework’s scoring scripts to avoid any\ndiscrepancies in the scoring methodology. Note\nthat the k-NN referred in Table 3 always refers to\nthe closest neighbor, and relies on MFS fallbacks.\nThe ﬁrst noteworthy result we obtained was that\nsimply replicating Peters et al. (2018)’s method\nfor WSD using BERT instead of ELMo, we were\nable to signiﬁcantly, and consistently, surpass the\nperformance of all previous works. When using\nour method (LMMS), performance still improves\nsigniﬁcantly over the previous impressive results\n(+1.9 F1 on ALL, +3.4 F1 on SemEval 2013). In-\nterestingly, we found that our method using ELMo\nembeddings didn’t outperform ELMo k-NN with\nMFS fallback, suggesting that it’s necessary to\nachieve a minimum competence level of embed-\ndings from sense annotations (and glosses) before\nthe inferred sense embeddings become more use-\nful than MFS.\nIn Figure 2 we show results when considering\nadditional neighbors as valid predictions, together\nwith a random baseline considering that some tar-\nget words may have less senses than the number\nof accepted neighbors (always correct).\n5688\nModel Senseval2 Senseval3 SemEval2007 SemEval2013 SemEval2015 ALL\n(n=2,282) (n=1,850) (n=455) (n=1,644) (n=1,022) (n=7,253)\nMFS†(Most Frequent Sense) 65.6 66.0 54.5 63.8 67.1 64.8\nIMS†(2010) 70.9 69.3 61.3 65.3 69.5 68.4\nIMS + embeddings†(2016) 72.2 70.4 62.6 65.9 71.5 69.6\ncontext2veck-NN†(2016) 71.8 69.1 61.3 65.6 71.9 69.0\nword2veck-NN (2016) 67.8 62.1 58.5 66.1 66.7 -\nLSTM-LP (Label Prop.) (2016) 73.8 71.8 63.5 69.5 72.6 -\nSeq2Seq (Task Modelling) (2017b) 70.1 68.5 63.1* 66.5 69.2 68.6*\nBiLSTM (Task Modelling) (2017b) 72.0 69.1 64.8* 66.9 71.5 69.9*\nELMok-NN (2018) 71.5 67.5 57.1 65.3 69.9 67.9\nHCAN (Hier. Co-Attention) (2018a) 72.8 70.3 -* 68.5 72.8 -*\nBiLSTM w/V ocab. Reduction (2018) 72.6 70.4 61.5 70.8 71.3 70.8\nBERTk-NN 76.3 73.2 66.2 71.7 74.1 73.5\nLMMS2348(ELMo) 68.1 64.7 53.8 66.9 69.0 66.2\nLMMS2348(BERT) 76.3 75.6 68.1 75.1 77.0 75.4\nTable 3: Comparison with other works on the test sets of Raganato et al. (2017a). All works used sense annotations\nfrom SemCor as supervision, although often different pretrained embeddings. † - reproduced from Raganato et al.\n(2017a); * - used as a development set; bold - new state-of-the-art (SOTA); underlined - previous SOTA.\n1 2 3 4 5\nNeighbors\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100F1 (ALL)\nLMMS (WSD)\nLMMS (USM)\nRAND (WSD)\nFigure 2: Performance gains with LMMS2348 when ac-\ncepting additional neighbors as valid predictions.\n5.2 Part-of-Speech Mismatches\nThe solution we introduced in §4.4 addressed\nmissing lemmas, but we didn’t propose a solution\nthat addressed missing POS information. Indeed,\nthe confusion matrix in Table 4 shows that a large\nnumber of target words corresponding to verbs are\nwrongly assigned senses that correspond to adjec-\ntives or nouns. We believe this result can help mo-\ntivate the design of new NLM tasks that are more\ncapable of distinguishing between verbs and non-\nverbs.\nWN-POS NOUN VERB ADJ ADV\nNOUN 96.95% 1.86% 0.86% 0.33%\nVERB 9.08% 70.82% 19.98% 0.12%\nADJ 4.50% 0% 92.27% 2.93%\nADV 2.02% 0.29% 2.60% 95.09%\nTable 4: POS Confusion Matrix for Uninformed Sense\nMatching on the ALL testset using LMMS2348.\n5.3 Uninformed Sense Matching\nWSD tasks are usually accompanied by auxilliary\nparts-of-speech (POSs) and lemma features for re-\nstricting the number of possible senses to those\nthat are speciﬁc to a given lemma and POS. Even if\nthose features aren’t provided (e.g. real-world ap-\nplications), it’s sensible to use lemmatizers or POS\ntaggers to extract them for use in WSD. However,\nas is the case with using MFS fallbacks, this ﬁlter-\ning step obscures the true impact of NLM repre-\nsentations on k-NN solutions.\nConsequently, we introduce a variation on\nWSD, called Uninformed Sense Matching (USM),\nwhere disambiguation is always performed against\nthe full set of sense embeddings (i.e. +200K vs.\na maximum of 59). This change makes the task\nmuch harder (results on Table 2), but offers some\ninsights into NLMs, which we cover brieﬂy in\n§5.4.\n5.4 Use of World Knowledge\nIt’s well known that WSD relies on various types\nof knowledge, including commonsense and se-\nlectional preferences (Lenat et al., 1986; Resnik,\n1997), for example. Using our sense embed-\ndings for Uninformed Sense Matching allows us\nto glimpse into how NLMs may be interpreting\ncontextual information with regards to the knowl-\nedge represented in WordNet. In Table 5 we show\na few examples of senses matched at the token-\nlevel, suggesting that entities were topically un-\nderstood and this information was useful to dis-\nambiguate verbs. These results would be less con-\nclusive without full-coverage of WordNet.\n5689\nMarlon⋆ Brando⋆ played Corleone ⋆ in Godfather ⋆\nperson1n person1n act3v syndicate1n movie1n location1n\nwomanizer1n group1n make42v mafia1n telefilm1n here1n\nbustle1n location1n emote1v person1n finalcut1n there1n\nact3v: play a role or part;make42v : represent ﬁctiously, as in a play, or pretend to be or act like;emote1v: give expression or\nemotion to, in a stage or movie role.\nSerena⋆ Williams played Kerber ⋆ in Wimbledon ⋆\nperson1n professionaltennis1n play1v person1n win1v tournament1n\ntherefore1r tennis1n lineup6v group1n romp3v worldcup1n\nreef1n singles1n curl5v takeorders2v carry38v eliminationtournament1n\nplay1v: participate in games or sport;lineup6v: take one’s position before a kick-off;curl5v: play the Scottish game of\ncurling.\nDavid Bowie ⋆ played Warszawa ⋆ in Tokyo\nperson1n person1n play14v poland1n originatein1n tokyo1n\namati2n folksong1n play6v location1n in1r japan1n\nguarnerius3n fado1n riff2v here1n takethefield2v japanese1a\nplay14v : perform on a certain location;play6v: replay (as a melody);riﬀ2v: play riffs.\nTable 5: Examples controlled for syntactical changes to show how the correct sense for ‘played’ can be induced\naccordingly with the mentioned entities, suggesting that disambiguation is supported by world knowledge learned\nduring LM pretraining. Words with ⋆ never occurred in SemCor. Senses shown correspond to the top 3 matches in\nLMMS1024 for each token’s contextual embedding (uninformed). For clariﬁcation, below each set of matches are\nthe WordNet deﬁnitions for the top disambiguated senses of ‘played’.\n6 Other Applications\nAnalyses of conventional word embeddings have\nrevealed gender or stereotype biases (Bolukbasi\net al., 2016; Caliskan et al., 2017) that may have\nunintended consequences in downstream applica-\ntions. With contextual embeddings we don’t have\nsets of concept-level representations for perform-\ning similar analyses. Word representations can\nnaturally be derived from averaging their contex-\ntual embeddings occurring in corpora, but then\nwe’re back to the meaning conﬂation issue de-\nscribed earlier. We believe that our sense em-\nbeddings can be used as representations for more\neasily making such analyses of NLMs. In Figure\n3 we provide an example that showcases mean-\ningful differences in gender bias, including for\nlemmas shared by different senses ( doctor: PhD\nvs. medic, and counselor: therapist vs. sum-\nmer camp supervisor). The bias score for a given\nsynset s was calculated as following:\nbias(s) =sim(⃗ vman1n ,⃗ vs) −sim(⃗ vwoman1n ,⃗ vs)\nBesides concept-level analyses, these sense em-\nbeddings can also be useful in applications that\ndon’t rely on a particular inventory of senses. In\nLoureiro and Jorge (2019), we show how similari-\nties between matched sense embeddings and con-\ntextual embeddings are used for training a classi-\nﬁer that determines whether a word that occurs in\ntwo different sentences shares the same meaning.\n−0.050 −0.025 0.000 0.025 0.050\ndoctor4\nn\nprogrammer1\nn\ncounselor2\nn\ndoctor1\nn\nteacher1\nn\nflorist 1\nn\ncounselor1\nn\nreceptionist1\nn\nnurse1\nn\nLMMS1024\nLMMS2048\nFigure 3: Examples of gender bias found in the sense\nvectors. Positive values quantify bias towards man1\nn,\nwhile negative values quantify bias towards woman1\nn.\n7 Future Work\nIn future work we plan to use multilingual re-\nsources (i.e. embeddings and glosses) for im-\nproving our sense embeddings and evaluating on\nmultilingual WSD. We’re also considering ex-\nploring a semi-supervised approach where our\nbest embeddings would be employed to automat-\nically annotate corpora, and repeat the process\ndescribed on this paper until convergence, itera-\ntively ﬁne-tuning sense embeddings. We expect\nour sense embeddings to be particularly useful\nin downstream tasks that may beneﬁt from rela-\ntional knowledge made accessible through linking\nwords (or spans) to commonsense-level concepts\nin WordNet, such as Natural Language Inference.\n5690\n8 Conclusion\nThis paper introduces a method for generating\nsense embeddings that allows a clear improvement\nof the current state-of-the-art on cross-domain\nWSD tasks. We leverage contextual embeddings,\nsemantic networks and glosses to achieve full-\ncoverage of all WordNet senses. Consequently,\nwe’re able to perform WSD with a simple 1-NN,\nwithout recourse to MFS fallbacks or task-speciﬁc\nmodelling. Furthermore, we introduce a variant\non WSD for matching contextual embeddings to\nall WordNet senses, offering a better understand-\ning of the strengths and weaknesses of representa-\ntions from NLM. Finally, we explore applications\nof our sense embeddings beyond WSD, such as\ngender bias analyses.\n9 Acknowledgements\nThis work is ﬁnanced by National Funds through\nthe Portuguese funding agency, FCT - Fundac ¸˜ao\npara a Ci ˆencia e a Tecnologia within project:\nUID/EEA/50014/2019.\nReferences\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In International Conference on Learning\nRepresentations (ICLR).\nPierpaolo Basile, Annalina Caputo, and Giovanni Se-\nmeraro. 2014. An enhanced Lesk word sense dis-\nambiguation algorithm through a distributional se-\nmantic model. In Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers , pages 1591–1600,\nDublin, Ireland. Dublin City University and Asso-\nciation for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In Proceed-\nings of the 30th International Conference on Neu-\nral Information Processing Systems, NIPS’16, pages\n4356–4364, USA. Curran Associates Inc.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nJose Camacho-Collados and Mohammad Taher Pile-\nhvar. 2018. From word to sense embeddings: A sur-\nvey on vector representations of meaning. J. Artif.\nInt. Res., 63(1):743–788.\nJose Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2016. Nasari: Integrating ex-\nplicit knowledge and corpus statistics for a multilin-\ngual representation of concepts and entities. Artiﬁ-\ncial Intelligence, 240:36 – 64.\nXinxiong Chen, Zhiyuan Liu, and Maosong Sun.\n2014. A uniﬁed model for word sense represen-\ntation and disambiguation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1025–1035,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805v1.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChristiane Fellbaum. 1998. In WordNet : an electronic\nlexical database. MIT Press.\nIgnacio Iacobacci, Mohammad Taher Pilehvar, and\nRoberto Navigli. 2016. Embeddings for word sense\ndisambiguation: An evaluation study. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 897–907, Berlin, Germany. Association\nfor Computational Linguistics.\nMinh Le, Marten Postma, Jacopo Urbani, and Piek\nV ossen. 2018. A deep dive into word sense dis-\nambiguation with LSTM. In Proceedings of the\n27th International Conference on Computational\nLinguistics, pages 354–365, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nDoug Lenat, Mayank Prakash, and Mary Shepherd.\n1986. Cyc: Using common sense knowledge to\novercome brittleness and knowledge acquistion bot-\ntlenecks. AI Mag., 6(4):65–85.\nMichael Lesk. 1986. Automatic sense disambiguation\nusing machine readable dictionaries: How to tell a\npine cone from an ice cream cone. InProceedings of\nthe 5th Annual International Conference on Systems\nDocumentation, SIGDOC ’86, pages 24–26, New\nYork, NY , USA. ACM.\nDaniel Loureiro and Al ´ıpio M´ario Jorge. 2019. Liaad\nat semdeep-5 challenge: Word-in-context (wic). In\nSemDeep-5@IJCAI 2019, page forthcoming.\n5691\nFuli Luo, Tianyu Liu, Zexue He, Qiaolin Xia, Zhi-\nfang Sui, and Baobao Chang. 2018a. Leveraging\ngloss knowledge in neural word sense disambigua-\ntion by hierarchical co-attention. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1402–1411, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nFuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang, and\nZhifang Sui. 2018b. Incorporating glosses into neu-\nral word sense disambiguation. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 2473–2482, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In Proceedings\nof The 20th SIGNLL Conference on Computational\nNatural Language Learning , pages 51–61, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of the 26th International Con-\nference on Neural Information Processing Systems -\nVolume 2, NIPS’13, pages 3111–3119, USA. Curran\nAssociates Inc.\nGeorge A. Miller, Martin Chodorow, Shari Landes,\nClaudia Leacock, and Robert G. Thomas. 1994. Us-\ning a semantic concordance for sense identiﬁcation.\nIn HUMAN LANGUAGE TECHNOLOGY: Proceed-\nings of a Workshop held at Plainsboro, New Jersey,\nMarch 8-11, 1994.\nRoberto Navigli. 2009. Word sense disambiguation:\nA survey. ACM Computing Surveys , 41(2):10:1–\n10:69.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlessandro Raganato, Jose Camacho-Collados, and\nRoberto Navigli. 2017a. Word sense disambigua-\ntion: A uniﬁed evaluation framework and empiri-\ncal comparison. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 99–110, Valencia, Spain. Association for\nComputational Linguistics.\nAlessandro Raganato, Claudio Delli Bovi, and Roberto\nNavigli. 2017b. Neural sequence learning mod-\nels for word sense disambiguation. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 1156–1167,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nPhilip Resnik. 1997. Selectional preference and sense\ndisambiguation. In Tagging Text with Lexical Se-\nmantics: Why, What, and How?\nSascha Rothe and Hinrich Sch ¨utze. 2015. AutoEx-\ntend: Extending word embeddings to embeddings\nfor synsets and lexemes. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 1793–1803, Beijing,\nChina. Association for Computational Linguistics.\nLo¨ıc Vial, Benjamin Lecouteux, and Didier Schwab.\n2018. Improving the coverage and the general-\nization ability of neural word sense disambiguation\nthrough hypernymy and hyponymy relationships.\nCoRR, abs/1811.00960.\nDayu Yuan, Julian Richardson, Ryan Doherty, Colin\nEvans, and Eric Altendorf. 2016. Semi-supervised\nword sense disambiguation with neural models. In\nProceedings of COLING 2016, the 26th Interna-\ntional Conference on Computational Linguistics:\nTechnical Papers, pages 1374–1385, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nZhi Zhong and Hwee Tou Ng. 2010. It makes sense:\nA wide-coverage word sense disambiguation system\nfor free text. In Proceedings of the ACL 2010 Sys-\ntem Demonstrations , pages 78–83, Uppsala, Swe-\nden. Association for Computational Linguistics.",
  "topic": "WordNet",
  "concepts": [
    {
      "name": "WordNet",
      "score": 0.9154098629951477
    },
    {
      "name": "Computer science",
      "score": 0.8040287494659424
    },
    {
      "name": "Word-sense disambiguation",
      "score": 0.7503260970115662
    },
    {
      "name": "Conflation",
      "score": 0.7408227324485779
    },
    {
      "name": "Lemma (botany)",
      "score": 0.6680132746696472
    },
    {
      "name": "Natural language processing",
      "score": 0.6422510147094727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5855110883712769
    },
    {
      "name": "SemEval",
      "score": 0.5530428290367126
    },
    {
      "name": "Word (group theory)",
      "score": 0.5098211169242859
    },
    {
      "name": "Language model",
      "score": 0.4131975769996643
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4116343855857849
    },
    {
      "name": "Task (project management)",
      "score": 0.35424959659576416
    },
    {
      "name": "Linguistics",
      "score": 0.27040815353393555
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Poaceae",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I182534213",
      "name": "Universidade do Porto",
      "country": "PT"
    }
  ],
  "cited_by": 14
}