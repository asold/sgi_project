{
  "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models",
  "url": "https://openalex.org/W4221145545",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1903586662",
      "name": "Ning Ding",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1990782487",
      "name": "Guang Yang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3204798943",
      "name": "Fuchao Wei",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2125207603",
      "name": "Zonghan Yang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2096159446",
      "name": "Yu-Sheng Su",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2998424002",
      "name": "Shengding Hu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2096431253",
      "name": "Yulin CHEN",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A4287895561",
      "name": "Chi-Min Chan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2110996955",
      "name": "Weize Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100298610",
      "name": "Jing Yi",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098285428",
      "name": "Zhao Weilin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2102849134",
      "name": "Xiao-Zhi Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2508172955",
      "name": "Hai-Tao Zheng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098902493",
      "name": "Jianfei Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2129156004",
      "name": "Juanzi Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4251581107",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2140246545",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3101056292",
    "https://openalex.org/W2964085347",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3198651167",
    "https://openalex.org/W2955429306",
    "https://openalex.org/W3127706504",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3107298362",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W2594171637",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W3204669968",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W95183648",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W3198571508",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W3175557894",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W11298561",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3188542058",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W2946088473",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2981876780",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1204615996",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2989195139",
    "https://openalex.org/W2043005456",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3166140588",
    "https://openalex.org/W3167473228",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W3115453555",
    "https://openalex.org/W3081786540",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3035309367",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2734498959",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3023528699",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3116178420",
    "https://openalex.org/W3207663303",
    "https://openalex.org/W4221147360",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2616957565",
    "https://openalex.org/W3094491777",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2950308662",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W3212706150",
    "https://openalex.org/W3202031169",
    "https://openalex.org/W2963368301",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2963416784",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3196890453",
    "https://openalex.org/W4226052861",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W3035297522",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2946498182",
    "https://openalex.org/W3106295233",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W4287692594",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W3125537303",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3130405705",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4288026527",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W1601803999",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W2036166268",
    "https://openalex.org/W4288360049",
    "https://openalex.org/W2765410998",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W3080649016",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W2924120895",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4309444617",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W4221154357",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2953903036",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3173380736",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4287890137",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3100560913",
    "https://openalex.org/W4287028759",
    "https://openalex.org/W4303939357",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W4295601746",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W4286905003",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W4221142421",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "<title>Abstract</title> As pre-trained language models (PLMs) have become the fundamental infrastructure for various NLP tasks and researchers have readily enjoyed themselves in the pretraining-finetuning paradigm, evidence from emerging research has continuously proven that larger models tend to yield better performance. However, despite the welcome outcome, the process of fine-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, fine- tuning all the parameters of a colossal model and retaining separate instances for different tasks are practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs. In order to unleash the imagination of the possible advantages of such methods, not limited to parameter efficiency, we coined a new term delta tuning from a morphological point of view to refer to the original “parameter efficient tuning”. In contrast with the standard fine-tuning, delta tuning only fine-tunes a small portion of the model parameters while keeping the rest untouched, largely reducing both the computation and storage costs. Recent studies have demonstrated that a series of delta tuning methods with distinct tuned parameter selection could achieve performance on a par with full-parameter fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In this paper, we first formally describe the problem of delta tuning and then comprehensively review recent delta tuning approaches. We also propose a unified categorization criterion that divides existing delta tuning methods into three groups: addition-based, specification-based, and reparameterization-based methods. Though initially proposed as an efficient method to steer large models, we believe that some of the fascinating evidence discovered along with delta tuning could help further reveal the mechanisms of PLMs and even deep neural networks. To this end, we discuss the theoretical principles underlying the effectiveness of delta tuning and propose frameworks to interpret delta tuning from the perspective of optimization and optimal control, respectively. Furthermore, we provide a holistic empirical study of representative methods, where results on over 100 NLP tasks demonstrate a comprehensive performance comparison of different approaches. The experimental results also cover the analysis of combinatorial, scaling and transferable properties of delta tuning. To facilitate the research of delta tuning, we are also developing an open-source toolkit, OpenDelta2, that enables practitioners to efficiently and flexibly implement delta tuning on PLMs. At last, we discuss a series of real-world applications of delta tuning.",
  "full_text": "Delta Tuning: A Comprehensive Study of Parameter\nE\u0000cient Methods for Pre-trained Language Models\nNing Ding  (  dingn18@mails.tsinghua.edu.cn )\nTsinghua Univeristy https://orcid.org/0000-0001-8758-9484\nYujia Qin \nTsinghua Univeristy\nGuang Yang \nTsinghua Univeristy\nFuchao Wei \nTsinghua Univeristy\nZonghan Yang \nTsinghua Univeristy\nYusheng Su \nTsinghua Univeristy\nShengding Hu \nTsinghua Univeristy\nYulin Chen \nTsinghua Univeristy\nChi-Min Chan \nTsinghua Univeristy\nWeize Chen \nTsinghua Univeristy\nJing Yi \nTsinghua Univeristy\nWeilin Zhao \nTsinghua Univeristy\nXiaozhi Wang \nTsinghua Univeristy\nZhiyuan Liu \nTsinghua University https://orcid.org/0000-0002-7709-2543\nHai-Tao Zheng \nTsinghua Univeristy\nJianfei Chen \nTsinghua Univeristy\nYang Liu \nTsinghua University\nJie Tang \nTsinghua Univeristy\nJuanzi Li \nTsinghua Univeristy\nMaosong Sun \nTsinghua University\nArticle\nKeywords: natural language processing, pre-trained models, parameter-e\u0000cient, delta tuning\nPosted Date: June 22nd, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-1553541/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nDelta T uning: A Comprehensive Study of Parameter\nEfﬁcient Methods for Pre-trained Language Models\nNing Ding ∗\n, Y ujia Qin ∗, Guang Y ang, Fuchao W ei, Zonghan Y ang, Y usheng Su, Shengding Hu,\nY ulin Chen, Chi-Min Chan, W eize Chen, Jing Yi, W eilin Zhao, Xiaozhi W ang,\nZhiyuan Liu, Hai-T ao Zheng, Jianfei Chen, Y ang Liu, Jie T ang, Juanzi Li, Maosong Sun\nTsinghua University , BAAI\n{dingn18, qyj20}@mails.tsinghua.edu.cn\nAbstract\nAs pre-trained language models (PLMs) have become the fundamental infrastructure for\nvarious NLP tasks and researchers have readily enjoyed themselves in the pretraining-\nﬁnetuning paradigm, evidence from emerging research has continuously proven that larger\nmodels tend to yield better performance. However, despite the welcome outcome, the\nprocess of ﬁne-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, ﬁne-\ntuning all the parameters of a colossal model and retaining separate instances for different\ntasks are practically infeasible. This necessitates a new branch of research focusing on the\nparameter-efﬁcient adaptation of PLMs. In order to unleash the imagination of the possible\nadvantages of such methods, not limited to parameter efﬁciency , we coined a new term\ndelta tuning from a morphological point of view to refer to the original “parameter efﬁcient\ntuning”. In contrast with the standard ﬁne-tuning, delta tuning only ﬁne-tunes a small\nportion of the model parameters while keeping the rest untouched, largely reducing both the\ncomputation and storage costs. Recent studies have demonstrated that a series of delta tuning\nmethods with distinct tuned parameter selection could achieve performance on a par with\nfull-parameter ﬁne-tuning, suggesting a new promising way of stimulating large-scale PLMs.\nIn this paper, we ﬁrst formally describe the problem of delta tuning and then comprehensively\nreview recent delta tuning approaches. W e also propose a uniﬁed categorization criterion that\ndivides existing delta tuning methods into three groups: addition-based, speciﬁcation-based ,\nand reparameterization-based methods. Though initially proposed as an efﬁcient method\nto steer large models, we believe that some of the fascinating evidence discovered along\nwith delta tuning could help further reveal the mechanisms of PLMs and even deep neural\nnetworks. T o this end, we discuss the theoretical principles underlying the effectiveness\nof delta tuning and propose frameworks to interpret delta tuning from the perspective of\noptimization and optimal control , respectively . Furthermore, we provide a holistic empirical\nstudy of representative methods, where results on over 100 NLP tasks demonstrate a\ncomprehensive performance comparison of different approaches. The experimental results\nalso cover the analysis of combinatorial, scaling and transferable properties of delta tuning.\nT o facilitate the research of delta tuning, we are also developing an open-source toolkit,\nOpenDelta2 , that enables practitioners to efﬁciently and ﬂexibly implement delta tuning on\nPLMs. At last, we discuss a series of real-world applications of delta tuning.\nKeywords— natural language processing, pre-trained models, parameter-efﬁcient, delta tuning\n“The lurking suspicion that something could be simpliﬁed is\nthe world’s richest source of rewarding challenges. ”\n— Edsger W . Dijkstra\n∗Equal contribution.\n2 https://github.com/thunlp/OpenDelta\nThe contributions of the authors are listed in § C O N T R I BU T I O N S\nCONTENTS\nContents\n1 Introduction\n3\n2 Preliminaries 5\n2.1 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Pre-trained Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3 Delta T uning 8\n3.1 Addition-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Speciﬁcation-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.3 Reparameterization-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n4 Theoretical Perspectives of Delta T uning 12\n4.1 Optimization Perspective for Delta Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.2 Optimal Control Perspective for Delta Tuning . . . . . . . . . . . . . . . . . . . . . . . . . 14\n5 Comparisons and Experimental Discoveries 17\n5.1 Performance, Convergence and Efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5.2 Combinations of Delta Tuning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.3 The Power of Scale for Delta Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.4 T ask-level Transferability Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n6 Applications 28\n7 Conclusion 32\nBroader Impacts 32\nA Implementation Details 46\nA.1 Performance and Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nA.2 Combinations of Delta Tuning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nA.3 The Power of Scale for Delta Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nA.4 T ask-level Transferability Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nB T asks Evaluated in Experiments 47\n2\n1. Introduction\n1 Introduction\nLanguage lies at the heart of human intelligence. Its systematic nature allows the denotation of real objects\nor illustration of laws with symbolic expressions and could convey almost inﬁnite information with a ﬁnite\nsymbolic set; its arbitrariness shows that there are no necessary connections between the real-world and the\nlanguage space, and indicates the importance of world knowledge and social convention to the effectiveness of\nlanguage in human society; its richness in meaning enables the expression of extremely complex behaviors\nor tasks with clear and simple symbols. Understanding language is the key to understanding intelligence.\nThe inquiry into what language is and how we acquire, store and comprehend it has never stopped among\npsychologists and linguists, and the charm of language will continue to impress and inspire us in the future.\nLikewise, to create the real intelligence system, researchers in the ﬁeld of artiﬁcial intelligence (AI) have been\ndedicated to training machines to model, understand and generate language.\nWith the revolutionary development in computing hardware, traditional statistical methods have yielded their\nplace to deep learning ( LeCun et al. , 2015) that heavily rely on tensor computation and huge data volume.\nModern natural language processing (NLP) uses deep neural networks to implicitly model probability and\ncapture language representations ( Hochreiter & Schmidhuber , 1997; Bengio et al. , 2000; Grefenstette et al. ,\n2014; Kim, 2014; V aswani et al. , 2017). A standard pipeline involves encoding language into discrete tokens\n(tokenization) as model input, choosing a proper model architecture, training the network with the given\ncorpora, and designing self-supervised tasks. Experimented with various model architecture, the Transformer\nneural network ( V aswani et al. , 2017) produced state-of-the-art performances on a series of NLP tasks and has\nbeen widely acknowledged as the standard architecture for pre-trained language models (PLMs). This ushers a\nnew era of pre-training and ﬁne-tuning . PLMs typically use heavily over-parameterized Transformers as the\nbase architecture, and model natural language in bidirectional ( Devlin et al. , 2019), auto-regressive ( Radford\net al. , 2018, 2019), or sequence-to-sequence ( Raffel et al. , 2019) manners on large-scale unsupervised corpora.\nThen for downstream tasks, task-speciﬁc objectives are introduced to ﬁne-tune the PLMs for model adaptation.\nNotably , the increasing scale of PLMs (measured by the number of parameters) seems to be an irreversible\ntrend as constant empirical results show that larger models (along with more data) almost certainly lead to\nbetter performance. For example, the 175 billion parameters GPT -3 (\nBrown et al. , 2020) generates natural\nlanguage of unprecedented quality and can conduct various desired zero-shot tasks with satisfactory results\ngiven appropriate prompts. Nevertheless, performing full parameter ﬁne-tuning on existing computing devices\nbecomes formidable with the growing model scale. This ﬁnally leads to a desperate yet thought-provoking\nquestion: do we really need to update all the parameters? In this context, how to efﬁciently and effectively\nadapt large models to particular downstream tasks is an intriguing research issue.\nAs a predominant way to conduct model adaptations, ﬁne-tuning initializes the model with the pre-trained\nweights, updates all the parameters, and produces separate instances for different tasks. But as implied by the\ncase of GPT -3, ﬁne-tuning becomes impractical as the model scales. In addition to the cost of deployment and\ncomputation, storing different instances for different tasks is extremely memory-intensive. T o further explore\nthe practical application rate of large models (PLMs with over 1 billion parameters), we randomly select 1000\npublished research papers from the recent ﬁve NLP conferences (200 for each venue), including ACL 2021,\nEMNLP 2021, NAACL 2021, ACL 2020, and EMNLP 2020. Then we manually count the usage of PLMs in\nthese peer-reviewed works, speciﬁcally , we only focus on the experiments part of the papers. According to\nthe statistics in T able 1, although the use of PLMs has almost become standard, there are only 0.5% ∼4%\nresearch papers that practically adopt large ones in the experiments. This suggests, ﬁrstly , that there is still\ninertia in the academic community which has resulted in scarce usage of large models in research, and also\nthat the cost of deploying and experimentally validating large PLMs hinders the development of NLP research.\nT able 1: The usage of models of different sizes in research published in NLP conferences, the statistic is based\non 1000 randomly selected papers. Large PLMs are deﬁned as PLMs with over 1 billion parameters.\nV enue No PLMs Small PLMs Large PLMs Per . of Large PLMs\nACL 2021 41 151 8 4.0%\nEMNLP 2021 46 150 4 2.0%\nNAACL 2021 37 158 5 2.5%\nACL 2020 107 92 1 0.5%\nEMNLP 2020 62 137 1 0.5%\n3\n1. Introduction\nT o this end, a branch of parameter-efﬁcient methods for model tuning arises. Although each of these approaches\nhas its own emphasis on structural design, they essentially tune a “delta” (i.e., adaptive parameters) in the\nadaptation phase, we thus coin the term delta tuning\n3 to refer to these methods. Parametric efﬁciency is\nan external manifestation of delta tuning that further exposes the low-rank or low-dimensional nature of\nlarge model adaptation in a more fundamental way . Generally , delta tuning only updates a small number of\nparameters (inherently in the model or additionally introduced) while freezing the remaining parameters that\naccount for the vast majority . Adapter tuning (\nHoulsby et al. , 2019) is among the earliest approaches to steer\npre-trained models with a limited number of parameters. It inserts adapter modules with bottleneck architecture\nbetween layers in PLMs and only these inserted modules get updated during ﬁne-tuning. Preﬁx-tuning ( Li &\nLiang, 2021) tunes the PLMs by updating the pre-pended parameters in each transformer layer. T aken insights\nfrom GPT -3, prompt tuning ( Lester et al. , 2021) only prepends and updates task-speciﬁc trainable parameters\nin the original input embeddings. BitFit ( Zaken et al. , 2021) updates the bias terms in PLMs while freezing the\nremaining modules. LoRA ( Hu et al. , 2021a) decomposes attention weight gradient into low-rank matrices to\nreduce the number of trainable parameters. With the diverse ﬂourishing research and the promising results,\nefforts have been made to explain and compare the essence of some popular methods. He et al. (2022) propose\na uniﬁed view of the existing delta tuning methods and illustrate the difference and connections among them\nformulaically .\n01\nC\nTQFDJBMJ[BUJPO TQFDJBMJ[BUJPO\nBERT Player BERT Broker BERT Physician BERT\nStrong BERT Instructor BERT Engineer BERT Scholar BERT\nFigure 1: Delta tuning seeks to adapt and specialize PLMs with changes of a small portion of parameters.\nThe delta tuning methods enable efﬁcient tuning and practical usage for large pre-trained models and often\nachieve comparable results to the standard ﬁne-tuning. For example, the vanilla ﬁne-tuning of GPT -3 needs to\nupdate about 175,255 million parameters, which is almost infeasible in both industry and academia. However,\nif we only tune the injected low-rank decomposition matrices in each Transformer layer ( Hu et al. , 2021a),\nonly 37.7 million parameters will be involved in backpropagation. Delta tuning not only provides a promising\nway to adapt large PLMs, but also sheds light on the mechanisms behind such model adaptations. Compared\nto pre-training, delta tuning makes model adaptation a considerably low-cost process in terms of data volume\nand model optimization. For instance, researchers ﬁnd that the optimization problem of the adaptations for big\nmodels could be reparameterized into a low-dimensional “intrinsic subspace” ( Aghajanyan et al. , 2021; Qin\net al. , 2021b), and various NLP tasks could be handled by only tuning very few parameters in the subspace.\nThe empirical evidence takes us one step closer to understanding how pre-trained models work, and may even\nspawn new theoretical questions that are worth exploring.\nThis paper ﬁrst attempts to survey the development and recent advances in delta tuning. For preliminaries, we\ngive a description of the Transformer neural models and mainstream PLMs ( §2: P R E L I M I NA R I E S). Then we\nformally describe the delta tuning problem and propose a categorization criterion ( §3: D E LTA T U N I N G ) to pro-\nvide a uniﬁed view on delta tuning methods. Categorizing delta tuning into addition-based ( §3.1: A D D I T I O N),\nspeciﬁcation-based ( §3.2: S P E C I FI C AT I O N), and reparameterization-based ( §3.3: R E PA R A M E T E R I Z AT I O N)\nmethods, we comprehensively introduce the technical details and empirical conclusions of the methods.\n3 In §3: D E LTA T U N I N G and §4: T H E O RY, we use the consistent mathematical expressions ∆ and δ to describe and\nanalyze delta tuning.\n4\n2. Preliminaries\nT o better understand the inner connections among the delta tuning methods and the mechanisms of model\nadaptation, we develop theoretical analysis ( §4: T H E O RY) of delta tuning by proposing theoretical frameworks\nfrom two different perspectives, optimization ( §4.1: O P T I M I Z AT I O N) and optimal control ( §4.2: O P T I M A L\nC O N T RO L). Our theoretical discussion is summarized as follows:\n• Optimization. Based on the intrinsic low dimension in a large pre-trained language model, we show\nthat delta tuning is essentially a subspace optimization method with respect to the solution space or\nfunctional space. The discussion justiﬁes the designs of the existing delta tuning methods and explains\nsome phenomena in the experiments.\n• Optimal Control. Inspired by the relationship between deep learning and optimal control theories, we\ninterpret delta tuning as seeking optimal controllers for PLMs. W e propose an optimal control framework\nthat uniﬁes different delta tuning approaches. Our analysis provides theoretical references for the novel\ndesign of delta tuning methods.\nIn terms of empirical studies, we carry out extensive and systematic experiments ( §5: E X P E R I M E N T S) on over\n100 NLP tasks to rigorously explore the performances ( §5.1: P E R F O R M A N C E), combinability ( §5.2: C O M B I-\nNAT I O N), the power of scale ( §5.3: S C A L E), transferability ( §5.4: T R A N S F E R A B I L I T Y), etc. Our main ﬁndings\nare summarized as follows:\n• Performance. Despite the huge potential, existing delta tuning methods are still no match for the\nconventional ﬁne-tuning either in performance or convergence. Among several representative delta tuning\nmethods, no single algorithm predominantly outperforms the others. W e also analyze the key properties\nof delta tuning such as convergence and computational efﬁciency .\n• Combinability . Combining multiple delta tuning methods is more effective than a single method under\nmost cases, despite that the optimal combination may vary for different PLM backbones, downstream\ntasks, and data scales.\n• Power of Scale. The power of scale (i.e., both the performance and convergence are improved when\nthe PLM’s size is increased) is observed in all of the delta tuning methods, even in unregulated neural\nmodules. W e provide a reasonable perspective to explain this phenomenon.\n• T ransferability . Existing delta tuning methods could well support knowledge transfer, showing non-\ntrivial transferability among downstream tasks of similar categories.\nAt last, we discuss the applications of delta tuning from various perspectives ( §6: A P P L I C AT I O N S), including\nfast training and shareable checkpoints, multi-task learning, catastrophic forgetting mitigation, and in-batch\nparallel computing. W e also discuss the broader impacts of the delta tuning technique in terms of fairness and\nenergy cost ( § I M PAC T S). Hopefully , this paper could inspire research to advance the efﬁcient use of large\nmodels. The tools, codes, data splits, trained delta checkpoints in our experiments will be publicly available to\nfacilitate future research 4 .\n2 Preliminaries\nSince almost all the mainstream PLMs are developed based on the Transformer ( V aswani et al. , 2017) model,\nand delta tuning usually carries out operations on Transformer models, this section gives preliminaries of the\nTransformer ( V aswani et al. , 2017) model and mainstream PLMs with different modeling strategies. For more\ndetails, please refer to original papers ( V aswani et al. , 2017; Devlin et al. , 2019; Brown et al. , 2020; Raffel\net al. , 2019) or related surveys ( Han et al. , 2021b; Liu et al. , 2021a; Bommasani et al. , 2021).\n2.1 T ransformer\nThe Transformer model has become a key infrastructure for existing state-of-the-art PLMs. The original\nTransformer is proposed as an encoder-decoder model, where both the encoder and decoder are composed\nof a stack of identical blocks. Each Transformer layer consists of an attention layer and a fully-connected\nfeed-forward neural network, while the decoder block contains an extra cross-attention layer on top of the self-\nattention layer to capture information from the encoder. Between each layer, there are residual connection ( He\net al. , 2016) and layer normalization ( Ba et al. , 2016) modules.\n4 This paper focuses on pre-trained language models, however, the delta tuning technique can also be seamlessly\ntransferred to other areas where large-scale neural models come into play , such as computer vision ( Rebufﬁ et al. , 2017;\nPerez et al. , 2018)\n5\n2.1 Transformer\nAttention Layer . Attention layers are the key to the success of Transformer. It involves a query matrix\nQ ∈Rn×dk , a key matrix K ∈Rm×dk , and a value matrix V ∈Rm×dv , where each row in the matrices\ncorresponds to one sample and the i-th row in K and V together form a key-value pair accordingly . The\nattention mechanism can be formally represented as\nH = A TT(Q,K,V) = Softmax(QK⊤\n√dk\n)V. (1)\nIntuitively , each row in H ∈Rn×dv is a weighted sum of row vectors in V, while the weights are decided by\nthe dot product of the query vector and the key matrix. The speciﬁc attention adopted in the Transformer model\nis termed as self-attention, as the three matrices Q,K,V are derived from the same feature matrix X ∈Rn×d\nfrom the previous layer, parameterized by three weight matrices Wq ∈Rd×dk ,Wk ∈Rd×dk ,Wv ∈Rd×dv\nas follows:\nQ = XWq,K = XWk,V = XWv (2)\nMoreover, Transformer uses multi-head self-attention with multiple sets of Q(i),K(i),V(i), each set corre-\nsponding to a distinct set of weight matrix W(i)\nq ∈Rd×dk ,Wk ∈Rd×dk ,Wv ∈Rd×dh , where dh is usually\nset to dv\nh . The ﬁnal output H ∈Rn×do is obtained by projecting the concatenation of a series of Hi into a new\nfeature space with a new weight matrix Wo ∈Rdv ×do .\nH = MH-A TT(Q,K,V)\n= Concat(H1,..., Hh)Wo,\nHi = A TT(Q(i),K(i),V(i))\n= A TT(XW(i)\nq ,XW(i)\nk ,XW(i)\nv ),\n(3)\nFor decoder blocks, however, there is an additional mask operation that prevents query vectors from attending\nto the future positions yet to be decoded. Besides, there is an extra cross-attention layer following the self-\nattention layer, where the query matrix Q is derived from the output of the previous layer in the decoder, and\nthe key and value matrices K,V are transformed from the output of the last layer of the encoder. It is designed\nto avoid foreseeing the true label while considering information from the encoder when decoding.\nK V\nX\nQ\nWQ WK WV\nMulti-head Attention\nAdd & Layer Norm\nAdd & Layer Norm\nFeed Forward Network\n× L\nQ = X WQ K = X WK V = X WV\nH ← (Q, K, V )MH-ATT\nH ← (H + X )LayerNorm\nH ←\nH ← (H + X )LayerNorm\nFFN(H )\nFigure 2: An illustration of a Transformer\nblock. Generally speaking, a delta tuning\nmethod could be applied to any positions\nin a Transformer model.\nFully-connected Feed-Forward Layer . The fully-connected\nfeed-forward layer following the attention layer is composed of\ntwo linear transformation and a non-linear activation function.\nDenote the input matrix as\nX ∈Rn×di , the output of the feed-\nforward layer is\nF = FFN(X) = σ(XW1 + b1)W2 + b2, (4)\nwhere σ(·) is the activation function (usually the ReLU function),\nand W1 ∈Rdi×dm , b1 ∈Rdm , W2 ∈Rdm×do , b2 ∈Rdo are\nall learnable parameters. Empirically , di is set equal to do, dm\nis set to be much larger than di and do.\nResidual Connection and Normalization. Following each at-\ntention layer and each feed-forward layer, residual connection\nand layer normalization are applied. They conduce to retaining\ninformation when the model is considerably deep and thus guar-\nantees the model performance. Formally , given a neural layer\nf(·), the residual connection and normalization layer is deﬁned\nas\nA&N(X,f) = LayerNorm(X + f(X)), (5)\nwhere LayerNorm(·) denotes the layer normalization operation,\nand A&N means “add and norm”.\nT ypical T ransformer Layer . As depicted in Figure 2, a typical\ntransformer layer can be expressed as\nM = A&N(X,H)\nY = A&N(M,F), (6)\n6\n2.2 Pre-trained Language Models\nwhere M is the intermediate representation after the attention block, and Y denotes the output of the layer\nwith respect to input X.\n2.2 Pre-trained Language Models\nCurrent pre-trained language models are almost consistently based on the Transformer model. However, they\nusually vary in the speciﬁc structure adopted (e.g. only using Transformer encoder or decoder, or both). This\nsection brieﬂy reviews some of the popular PLMs with respect to different modeling strategies (as shown in\nFigure 3).\nx1 x2 [MASK] x4 x5\nx3\nx1 x2 x4 x5x3\nx2 x4 x5x3 y1\nx1 x2 x4 x5x3\ny1 y2 y3\nMasked Language Modeling Auto-regressive Seq2Seq\nDecoder\nEncoder\nFigure 3: Different modeling strategies of PLMs.\nMasked Language Modeling. The ﬁrst group of PLMs are bidirectional models based on the Transformer\nencoder, among which BER T ( Devlin et al. , 2019) is the most representative one. It is pre-trained with masked\nlanguage modeling (MLM) task and next sentence prediction (NSP) task. When pre-training, the input is\na pair of sequences, where special tokens [\nCLS] and [ SEP] are added to the original input, and tokens are\nrandomly replaced with [ MASK] tokens. MLM loss seeks to maximize the conditional probability of label\ntokens at [ MASK] position, as shown in equation (7), where M(x) contains all masked token positions. While\nthe ﬁnal representation of [ CLS] is used to predict whether the two sentences are coherent.\nLMLM = −\n∑\nxm∈M(x)\nlog P(xm|x\\M(x)) (7)\nRoBERT a (Liu et al. , 2019) is almost identical to BERT , except that it removes the NSP task, applies the\nmore robust dynamic masking to the input, and is trained with larger batch sizes, the longer time, and more\ndata. Bidirectional models are powerful in generating contextual representations of tokens and language\nunderstanding.\nAuto-regressive Language Modeling. Another set of PLMs are language models purely based on the\nTransformer decoder. They are also termed as auto-regressive language models. The objective of language\nmodeling (LM) is to model the probability of the given sequence by factorizing it into the probability of the\ni-th token given the previous tokens:\nLLM = −log P(x) = −\nT∑\ni=1\nlog P(xi|x<i), (8)\nwhere x0 is a special token indicating the start of a sentence. It is natural to take the advantage of masking\nin the Transformer decoder to model the conditional probability . During pre-training, the ﬁnal output at\neach position is further fed into a softmax layer to predict the next token. The most well-known models are\nGPT ( Radford et al. , 2018) and GPT -2 ( Radford et al. , 2019), while GPT -2 is trained to be more robust to\ndiverse tasks. The unidirectional characteristic of these models enables high-quality language generation.\nSequence to Sequence Modeling. The last type of PLMs are sequence-to-sequence models built upon\na complete Transformer architecture. Common models of this type include T5 ( Raffel et al. , 2019) and\nBART ( Lewis et al. , 2020). Both models adopt span-level corruption as the major pre-training task, i.e. to\nrandomly replace a sequence of the text of arbitrary length with a single mask token and ask the model to ﬁll\n7\n3. Delta Tuning\nin the original tokens. It is also termed as Seq2Seq MLM loss , whose objective is to maximize the probability\nof target sequence given a corrupted sequence:\nLSeq2Seq MLM = −\n∑\nxi:j ∈M(x)\nj∑\nt=i\nlog P(xt|x\\M(x),xi:t−1) (9)\nwhere M(x) contains all corrupted text spans and xi:j is a single masked span. While BAR T requires a different\nﬁne-tuning paradigm to assist in the classiﬁcation task, T5 uniﬁes all tasks under the text-to-text generation\nparadigm. As a combination of both bidirectional encoder and auto-regressive decoder, sequence-to-sequence\nmodels are powerful in both language understanding and generation tasks.\n3 Delta T uning\nGiven a pre-trained model Θ = {w1,w2,...,w N}and training data D, the objective of PLM adaptation is\nto produce the adapted model Θ ′ = {w′\n1,w′\n2,...,w ′\nM}. Deﬁne ∆Θ = Θ ′ −Θ as the operation on top of the\noriginal model Θ . In vanilla ﬁne-tuning, N = M and ∆Θ = ∇fΘ (D) is the update value of all parameters\nin Θ with respect to training data. While in delta tuning, ∆Θ refers to modiﬁcation of a small number of\nparameters. Empirically , |∆Θ |= |Θ |in vanilla ﬁne-tuning, while for delta tuning, |∆Θ |≪| Θ |, where |·|\nindicates the number of parameters involved.\nΘ′ =\nPre-trained PLM\nΘ =\nΘ′ =\nΘ′ =\nAddition\nSpeciﬁcation\nReparameterization\nFrozen Parameters Tunable Parameters\nDelta Tuning\nΘ → Θ′ \nFigure 4: The categorization criterion of delta tuning, where Θ denote the pre-trained parameters, and Θ ′\nrepresent the well-tuned parameters.\nT o organize them under a uniﬁed framework, we categorize the delta tuning methods into three groups\naccording to the operations on the delta parameters (as illustrated in Figure 4): addition-based, speciﬁcation-\nbased, and reparameterization-based approaches.\n• Addition-based methods introduce extra trainable neural modules or paramters that do not exist in the\noriginal model or process. In addition-based methods, M ≥N and ∆Θ = {wN+1,wN+2,...,w M}.\n• Speciﬁcation-based methods specify certain parameters in the original model or process become trainable,\nwhile others are frozen. Denote the set of trainable parameters as W, then ∆Θ = {∆ w1,∆ w2,..., ∆ wN}.\nWhen wi ∈W, ∆ wi is the incremental value from wi to w′\ni, else, ∆ wi = 0 .\n• Reparameterization-based methods reparameterize existing parameters to a parameter-efﬁcient form by\ntransformation. Denote the set of parameters to be reparameterized as W, and suppose that each wi ∈W\nis reparameterized with new parameters R(wi) = {u1,u2,...,u Ni }, then ∆Θ = (Θ \\W) ∪U, where\nU= {uj|∃wi ∈W,uj ∈R(wi)}.\n3.1 Addition-based Methods\nWith the above deﬁnition in mind, addition-based methods introduce additional parameters to the neural\nnetwork. In this section, we introduce two branches of representative addition-based methods, adapter-based\ntuning and prompt-based tuning .\nAdapters-based T uning. As a seminal work in delta tuning, adapter-based methods inject small-scale neural\nmodules (adapters) to the Transformer layers and only tune these adapters for model adaptation. Although such\na strategy leaves an open choice of adapter structures, a simple instantiation ( Houlsby et al. , 2019) achieves\nimpressive performance and has become the most widely used baseline in recent research. Speciﬁcally , one\n8\n3.1 Addition-based Methods\nadapter module contains a down-projection and an up-projection. For an input feature h ∈Rd, a down-\nprojection projects the input to a r-dimensional space with a parameter matrix Wd ∈Rd×r, after which a\nnonlinear function f(·) is applied. Then the up-projection Wu maps the r-dimensional representation back to\nd-dimensional space. Added with a residual connection, the complete computation could be written as\nh ←f(hWd)Wu + h. (10)\nIn each block, the adapter modules are separately inserted after the multi-head self-attention and the feed\nforward network sublayers, which reduces the tunable parameters per layer to 2×( 2dr(projection matrices) +\nd(residual connection) + r(bias term) ). Practically , 0.5% ∼8% parameters of the whole model ( Houlsby et al. ,\n2019) could be involved in tuning process under such strategy .\nAlthough adapter works with signiﬁcantly fewer tunable parameters than vanilla ﬁne-tuning, some work\nattempts for a more rigorous saving strategy by introducing inductive biases into the structure of the adapter\nlayer. For example, Compacter ( Mahabadi et al. , 2021a) propose to use a combination of hypercomplex\nmultiplication and parameter sharing. The hypercomplex multiplication parameterizes the original linear layer\nas the sum of the Kronecker products of two small matrices. T aking the down-projection as an example,\nWd =\nn∑\ni=1\nAi ⊗Bi,where A ∈Rn×n,B ∈R\nd\nn × r\nn (11)\nTheir method reduces the number of parameters in the adapter layer to 1\nn without harming the performance,\nwhere nis the number of divisions of the linear layer. It also shows that a simple low-rank decomposition of\nthe linear layer leads to comparable performance with the adapter layer, i.e.,\nWd = ABT,where A ∈Rd×n,B ∈Rr×nand n≪min(d,r). (12)\nAs an addition-based approach, adapter-based tuning has the advantage of placing multiple adapter instances\non a pre-trained model simultaneously , which can beneﬁt many application scenarios. For example, multi-task\nlearning ( Stickland & Murray , 2019; Mahabadi et al. , 2021b) is an advantageous setting for adapter-based\nmethods, inserted with adapter modules in parallel with the self-attention module, PLMs could demonstrate\nimpressive representational capacity in the multi-task setting. In contrast to directly conducting multi-task\nlearning on adapters, adapterFusion (\nPfeiffer et al. , 2021) ﬁrst pre-train task-speciﬁc adapters and then\ncombine the representations of the pre-trained adapters to leverage the cross-task knowledge and enhance the\nperformance of transfer learning.\nIn terms of computational efﬁciency , the training of adapters could be 60% faster than vanilla ﬁne-tuning while\nthe inference is only 4%-6% slower. And the computational cost could be further reduced dynamically by\nremoving adapters from lower transformer layers ( Rücklé et al. , 2021). Research also shows that adapter-based\nﬁne-tuning demonstrates better robustness than ﬁne-tuning. Speciﬁcally , adapter-based ﬁne-tuning could\nperform better than vanilla ﬁne-tuning on few-shot and cross-lingual scenarios ( He et al. , 2021) and is more\nrobust under adversarial attacking ( Han et al. , 2021a). W e provide a comparison of different adapters, as well\nas other delta tuning methods in T able 2.\nT o sum up, adapters are lightweight additional neural modules that could be trained in a task-speciﬁc style,\nwhich could be regarded as “encapsulation” of task information (in fact, this perspective can be applied to all\nthe “deltas”). Although in an ideal world, adapters could be freely shared and reused by researchers, in practice,\nsharing and reusing such modules face substantial obstacles. T aking the ﬁrst step, AdapterHub ( Pfeiffer et al. ,\n2020a) provides a feasible platform and toolkit to deploy adapters inside the transformer-based models.\nPrompt-based T uning. Instead of injecting neural modules to the Transformer model, prompt-based methods\nwrap the original input with additional context. As a strategy to stimulate pre-training language models by\nmimicking pre-trained objectives in the downstream tasks, prompt-based learning has achieved promising\nperformance in various NLP tasks ( Gao et al. , 2021; Hu et al. , 2021b; T an et al. , 2021), especially in low-data\nsettings ( Scao & Rush , 2021). The introduction of the technique and implementations of prompt-based learning\nhave already been comprehensively presented in other literature ( Liu et al. , 2021a; Ding et al. , 2021). In\nthis paper, we primarily focus on the parameter-efﬁcient attribute of prompt-based learning (only preﬁxes or\nprompts are optimized) and pay less attention to the settings where the models and prompts are simultaneously\noptimized.\nAn important seminal work of this branch of research is preﬁx-tuning ( Li & Liang , 2021), which prepends\ntrainable continuous tokens (preﬁxes) to the input and hidden states of each Transformer layer. Each preﬁx\n9\n3.2 Speciﬁcation-based Methods\nis drawn from a newly initialized trainable parameter matrix P, while other parameters of the pre-trained\nmodel remain unchanged during training. During generation, if an activation hi is in a preﬁx position, it\nis the direct copy of the corresponding trainable parameter; otherwise, the activation is computed by the\nmodel as hi = LM(zi,h<i). It is worth noting that the paradigm could be applied to both autoregressive and\nencoder-decoder models. Liu et al. (2021b) demonstrate that such a strategy could be effectively applied to\nnatural language understanding (NLU) with different scales of models.\nCompared to preﬁx-tuning which adds tunable preﬁxes to every intermediate Transformer layer, prompt\ntuning ( Lester et al. , 2021) proposes a more simpliﬁed strategy that only adds soft prompts to the input layer.\nSimilar to preﬁx-tuning, the newly introduced prompts are not parameterized by the pre-trained model but\nan additional parameter matrix. And during training, the parameters of soft prompts are updated by gradient\ndescent while the model parameters keep frozen. As the model size increases, the performance gap between\nprompt-tuning and full parameter ﬁne-tuning is narrowed. Particularly , when the model scales to T5-XXL\nwith 11 billion parameters, prompt tuning yields comparable performance on SuperGlue with ﬁne-tuning. This\nstrategy also exhibits sensitivity to the length and initialization of the soft prompts. Prompts could also be\ninjected in the pre-training stage to seek a satisfying initialization point ( Gu et al. , 2021). Moreover, similar to\nother methods, prompt tuning also demonstrates transferability across tasks ( V u et al. , 2021; Su et al. , 2021),\nwhich suggests that appropriate initialization could be substantially beneﬁcial for downstream tasks.\nThe T raining Curse of Prompt-based Methods Although prompt-based methods exhibit a promising\nfuture for the adaptation of large pre-trained models, especially that prompt tuning does not need to modify\nanything inside the neural network, there still exist unsolved challenges. In practice, prompt tuning is difﬁcult\nto optimize, and generally , this phenomenon becomes more apparent as the volume of data and the size of the\nmodel decreases. Even though soft prompts can be trained successfully , they converge signiﬁcantly slower\nthan full parameter ﬁne-tuning and other delta tuning methods during training. In our experiments, we validate\nthe phenomenon across different datasets ( §5.1: P E R F O R M A N C E), indicating that it is an interesting topic to\ntrain soft prompt to converge stably in various situations.\n3.2 Speciﬁcation-based Methods\nSpeciﬁcation-based methods ﬁne-tune a few inherent parameters while leaving the majority of parameters\nunchanged in model adaptation. This approach does not seek to change the internal structure of a model but to\noptimize a small number of internal parameters to solve particular tasks. Generally , such speciﬁcations could\nbe implemented based on heuristics or training supervision.\nHeuristic Speciﬁcation. Speciﬁcation-based methods do not introduce any new parameters in the model,\nbut directly specify part of the parameters to be optimized. The idea is simple but surprisingly effective, Lee\net al. (2019) only ﬁne-tune one-fourth of the ﬁnal layers of BERT and RoBERT a and could produce 90% of\nthe performance of full parameter ﬁne-tuning. BitFit ( Zaken et al. , 2021) empirically proves that by only\noptimizing the bias terms inside the model and freezing other parameters, the model could still reproduce\nover 95% performance on several benchmarks. Empirical results in BitFit also show that even if we use a\nsmall random set of parameters for delta tuning (which obviously will degrade the performance), the model\ncould still yield passable results on the GLUE benchmark. Unfortunately , the work only applies this trick to\nsmall-scale models, and there is no guarantee that randomly choosing some parameters to be tuned would\nremain competitive for larger models. Another valuable observation is that different bias terms may have\ndifferent functionalities during model adaptation.\nLearn the Speciﬁcation. Rather than manually or heuristically specify which parameters to be updated, one\nalternative is to “learn” such speciﬁcations. Following the deﬁnition in §3: D E LTA T U N I N G , diff pruning ( Guo\net al. , 2021) reparameterizes the ﬁne-tuned model parameters Θ ′ as the summation of the pre-trained parameters\nΘ and the difference vector ∆Θ , i.e., Θ ′ = Θ+∆Θ , where |Θ |= |Θ ′|. Hence, the key issue is to encourage the\ndifference vector to be as sparse as possible, this work regularizes the vector by a differentiable approximation\nto the L0-norm penalty to achieve the goal of sparsity . Practically , because new parameters to be optimized are\nintroduced in the learning phase, diff pruning takes up more GPU memory than full parameter ﬁne-tuning,\nwhich may establish barriers in the application on large PLMs. The masking method ( Zhao et al. , 2020)\nlearns selective masks for PLMs to only update the critical weights for particular tasks. T o learn such a set of\nmasks, a binary matrix associated with the model weights is introduced, where each value is generated by a\nthresholding function. During back-propagation, the matrix is updated by a noisy estimator.\n10\n3.3 Reparameterization-based Methods\nT able 2: Comparison between different delta tuning methods, we use the green color to denote tunable\nparameters and modules. [:] is the concatenation operation; dh means the hidden dimension of transformer\nmodel; dm is the intermediate dimension between down projection and up projection, where dm is far smaller\nthan dh. C O M PAC T E R utilize hypercomplex matrix multiplication and low-rank decomposition to reduce the\namount of parameters; A DA P T E R DRO P randomly dropout adapters in the ﬁrst nlayers and also bring down\nback-propagation time; P R E FI X-T U N I N G add preﬁx of npast key value.\nName & Refs Method #Params\nSE QU E N T I A L ADA P T E R\nHoulsby et al. (2019) LayerNorm(X + H(X)) → LayerNorm(X + ADT(H(X)))\nLayerNorm(X + F(X)) → LayerNorm(X + ADT(F(X)))\nADT(X) = X + σ(XWdh×dm )Wdm×dh , σ = activation\nL × 2 × (2dhdm)\nCO M PAC T E R\nMahabadi et al. (2021a) L × 2 × (2(dh + dm))\nADA P T E R DRO P\nRücklé et al. (2021) (L − n) × 2 × (2dhdm)\nPA R A L L E L ADA P T E R\nHe et al. (2022)\nLayerNorm(X + H(X)) → LayerNorm(X + ADT(X) + H(X))\nLayerNorm(X + F(X)) → LayerNorm(X + ADT(X) + F(X))\nADT(X) = σ(XWdh×dm )Wdm×dh , σ = activation\nL × 2 × (2dhdm)\nADA P T E R BI A S\nLayerNorm(X + F(X)) → LayerNorm(ADT(X) + F(X))\nADT(X) = XWdh×1W1×dh\nL × 2 × dh\nPR E FI X-T U N I N G\nLi & Liang (2021)\nHi = A TT(XW(i)\nq , [MLP(i)\nk (P′\nk)\n: XW(i)\nk ], [MLP(i)\nv (P′\nv)\n: XW(i)\nv ])\nMLP(i)(X) = σ(XWdm×dm )W(i)\ndm×dh\nP ′ = Wn×dm\nn × dm + d2\nm\n+ L × 2 × dhdm\nLORA\nHu et al. (2021a)\nHi = A TT(XW(i)\nq , ADTk(X) + XW(i)\nk , ADTv(X) + XW(i)\nv )\nADT(X) = XWdh×dm Wdm×dh\nL × 2 × (2dhdm)\nBI TFI T\nZaken et al. (2021) f(X) → f(X) + B, for all function f L × (7 × dh + dm)\n3.3 Reparameterization-based Methods\nReparameterization-based methods transform the adaptive parameters during optimization into parameter-\nefﬁcient forms. This branch of delta tuning is typically motivated by the hypothesis that PLM adaptations\ntowards most downstream tasks are inherently low-rank, and could thus be equivalently completed in a\nparameter-efﬁcient way .\nIntrinsic Dimensions of PLM Adaptation. Aghajanyan et al. (2021) empirically show that the full-\nparameter ﬁne-tuning process of pre-trained models can be reparameterized into optimization within a\nlow-dimensional subspace, i.e., ﬁne-tuning has a low intrinsic dimension (\nLi et al. , 2018), which measures\nthe minimum number of parameters needed to reach satisfactory performance. In experiments, they ﬁnd\nthat a relatively low-dimensional (e.g., thousands) reparameterization could achieve over\n85% ﬁne-tuning\nperformance. In this sense, PLMs may serve as general compression frameworks, which compress the\noptimization complexity from high dimensions to low dimensions. They also demonstrate that, larger PLMs\ngenerally have smaller intrinsic dimensions, and the process of pre-training implicitly reduces PLM’s intrinsic\ndimension. T aking inspiration from these observations, reparameterization-based delta tuning methods are\nproposed, which reparameterize (a part of) original model parameters with low-dimensional proxy parameters\nand only optimize the proxy parameters and thus reduce the computation and memory cost.\nIntrinsic Rank of W eight Differences. Inspired by Aghajanyan et al. (2021), LoRA ( Hu et al. , 2021a)\nhypothesizes that the change of weights during model tuning has a low intrinsic rank . Based on this hypothesis,\nthey propose to optimize the low-rank decomposition for the change of original weight matrices in the self-\nattention modules. In deployment, the optimized low-rank decomposition matrices are multiplied to obtain\nthe delta of self-attention weight matrices. In this way , LoRA could match the ﬁne-tuning performance on\n11\n4. Theoretical Perspectives of Delta Tuning\nthe GLUE benchmark. They demonstrate the effectiveness of their methods on PLMs of various scales and\narchitectures.\nIntrinsic Space of Multiple Adaptations. Furthermore, Qin et al. (2021b) make a stronger hypothesis that\nthe adaptations to multiple tasks could be reparameterized into optimizations within the same low-dimensional\nintrinsic subspace. Instead of resorting to a random subspace ( Aghajanyan et al. , 2021), they try to ﬁnd a\ncommon subspace shared by various NLP tasks, which is implemented through decomposing the trained soft\nprompts of multiple NLP tasks into the same low-dimensional nonlinear subspace, and then learn to adapt\nthe PLM to unseen tasks or data by only tuning parameters in the subspace. Experiments show that in a\n250-dimensional subspace found with 100 random tasks, by only tuning 250 free parameters, 97% and 83% of\nthe full prompt tuning performance can be recovered for 100 seen tasks (using different training data) and 20\nunseen tasks, respectively . This provides strong evidence for their universal reparameterization hypothesis and\nmay inspire future work. Moreover, Qin et al. (2021b) also shows that the low-dimensional reparameterization\ncan signiﬁcantly improve the stability of prompt tuning. Their method could also be leveraged as a tool for\nanalyzing the similarity and differences for various NLP tasks. The motivation differences of the above three\nworks are visualized in Figure 5.\nIntrinsic Space\nIntrinsic Space\nPLMPLMTask #2\nTask #1\nTask #3\nTask #2\nTask #1\nTask #3\nPLMTask #2\nTask #1\nTask #3\n+\n×\n×\n×\nIntrinsic Rank\nWeights  \nChange\nFigure 5: Conditioned on a PLM, Aghajanyan et al. (2021) hypothesize that there exist a low-dimensional\nintrinsic subspace that could reparameterize one speciﬁc ﬁne-tuning process (the left part). Hu et al. (2021a)\nhypothesize that the change of weights during adaptation has a low intrinsic rank (the middle part). And Qin\net al. (2021b) hypothesize there may exist a common intrinsic space that could handle the ﬁne-tuning for\nvarious NLP tasks (the right part).\n4 Theoretical Perspectives of Delta T uning\nAre these methods essentially doing the same thing? W e are interested in the theoretical principles behind delta\ntuning. A pre-trained language model usually can be easily adapted to almost any downstream tasks with only\na very small cost (compared to pre-training), and this phenomenon leads to theoretical issues that are worth\nexploring in depth. In this section, we propose two frameworks to introduce theoretical insights of delta tuning\nfrom the perspectives of optimization ( §4.1: O P T I M I Z AT I O N) and optimal control ( §4.2: O P T I M A L C O N T RO L ).\n4.1 Optimization Perspective for Delta T uning\nThe delta tuning technique seeks to tune a small portion of parameters so as to match the performance of the\nﬁne-tuning in the original large language model while reducing the memory footprint. From the perspective of\noptimization, we analyze the effects of delta tuning and discuss the designs of several delta tuning methods\nunder the low dimension assumption.\nLet F(θ) denote the objective function of the original language model. Then the new objective function\noptimized by delta tuning is ˜F(θ,δ). Here θ denotes the parameters of the original language model, and\nδ denotes the speciﬁc parameters tuned by delta tuning 5 . The starting point is (θ0,δ0), where θ0 is the\npre-trained language model parameters and δ0 is the initialization of δ. In principle, though one may adopt\nsome initialization of δto facilitate the training process of delta tuning, there still exists the δ0 such that\n˜F(θ,δ0) = F(θ), (13)\n5 The variables θ and δ correspond to the concepts of Θ and ∆Θ in §3: D E LTA T U N I N G . Also, δ is not necessarily\nindependent of θ.\n12\n4.1 Optimization Perspective for Delta Tuning\nwhich guarantees that ˜Fis identical to Fif the delta tuning is disabled. Thus, the following relations hold:\nmin\nθ,δ\n˜F(θ,δ) ≤min\nθ\n˜F(θ,δ0) = min\nθ\nF(θ), min\nθ,δ\n˜F(θ,δ) ≤min\nδ\n˜F(θ0,δ), (14)\nwhich suggests that simultaneously tuning θand δmay be beneﬁcial. Nonetheless, we are only interested in\nanalyzing the case that either θor δis ﬁne-tuned. Let θ+ = arg min θ ˜F(θ,δ0) and δ+ = arg min δ ˜F(θ0,δ).\nThe delta tuning essentially does no harm to the tuning of the original model under some conditions. For\ninstance, assuming that ˜Fis twice Lipschitz continuously differentiable, it can be proved that\n|˜F(θ+,δ0) −˜F(θ0,δ+)|= O(∥θ+ −θ0∥2\n2 + ∥δ+ −δ0∥2\n2), (15)\nin a local small region around (θ+,δ0) and (θ0,δ+). For a sufﬁciently good starting point, the error bound\nholds. However, to guarantee the effectiveness of delta tuning, it is essential to exploit the problem structures\nto design ˜F. The intuition is to leverage the intrinsic low dimensions of the problems. Basically , there are two\napproaches that turn out to be useful in practice:\n• The solution is updated in a lower dimensional subspace;\n• The objective function (with constraints) is approximated in a certain smaller functional subspace.\nFor the applications in deep learning, the optimization of the objective function often has lots of local\nminimizers due to over-parameterization. Therefore, these approaches typically work well when the starting\npoint is close to a local minimizer, where only some searching directions matter or the objective function\ncan be well approximated by some simpler function in the trust region. Moreover, the small dimensional\noptimization can lead to a more efﬁcient and more stable training process.\nLow dimensional representation in solution space. As it is observed that the optimization trajectory of\nθapproximately follows a manifold ( Aghajanyan et al. , 2021), we can embed the hidden manifold to a low\ndimensional space of δ, i.e., θ= ψ(δ) + ϵ, where ϵis the error term depending on θ0,θ+. Then,\n˜F(θ,δ0) = F(θ), ˜F(θ0,δ) = F(ψ(δ)). (16)\nIf ϵ= 0 , the delta tuning ﬁnds the exact solution of the ﬁne-tuning of the original language model. Otherwise,\nthe ﬁnal discrepancy depends on the approximation error, the condition number of the objective function, and\nthe stability of the training process. Let δ+ = arg min δF(ψ(δ)), and θ+ = ψ(δ′) + ϵ′. Suppose that Fand\nF◦ ψare Lipschitz continuous and the Lipschitz constants are L1 and L2, respectively . Then, we have the\nfollowing bound of the approximation error of delta tuning to the full-parameter ﬁne-tuning of the original\nlanguage model:\n|F(θ+) −F(ψ(δ+))|≤|F (θ+) −F(ψ(δ′))|+ |F(ψ(δ′)) −F(ψ(δ+))|\n≤L1∥ϵ′∥2 + L2∥δ′ −δ+∥2 ≤L1∥ϵ′∥2 + L2(∥δ′∥2 + ∥δ+∥2). (17)\nThe error ϵ′ is controlled by the approximation of the low dimensional representation ψ. Since the minimization\nof F(ψ(δ)) can be viewed as minimization of a perturbed objective function of F(θ), the ∥δ′ −δ+∥2 is bounded\nprovided that Fis well-conditioned and the optimization algorithm is stable. Also, if the magnitudes of δ′ and\nδ+ are small, the bound ( 17) can still lead to the good quality of F(ψ(δ+)).\nSome delta tuning methods beneﬁt from such approach. In LoRA ( Hu et al. , 2021a), the weight matrix W ∈\nRd×n is constructed as W ≈W0 + AB, where W0 is the corresponding part of δ0, and A∈Rd×r,B ∈Rr×n\nare rank- rlow rank matrices learned by the training process. The numerical results validate the assumption of\nlow-rank approximation ( Hu et al. , 2021a). In BitFit ( Zaken et al. , 2021) or diff pruning ( Guo et al. , 2021),\nsome coordinates of δare selected to be optimized during the training process. Thus, the approximation is\nθ= θ0 + Vy, where the columns of V consist of columns chosen from the identity matrix and yis the low\ndimensional vector to be learned. W e can also apply some suitable transformation to θto make the Feasier to\nbe optimized. For example, choose a transformation matrix S, and let Sθ = Sθ0 + Vy. The resulting delta\ntuning method can be viewed as a re-parameterization followed by a diff pruning.\nLow dimensional representation in functional space. Another approach is to directly design an approxi-\nmate function that matches the ﬁnal F(θ+), i.e., we seeks to ﬁnd ˆF(δ), such that\n|F(θ) −ˆF(δ)|<ϵ, (18)\n13\n4.2 Optimal Control Perspective for Delta Tuning\nwhere ϵis the approximation error. By this way , we recognize that ˜F(θ,δ0) = F(θ) and ˜F(θ0,δ) = ˆF(δ).\nThe construction of ˆFcan be characterized by an incremental network ( Houlsby et al. , 2019) or an augmented\nfeature space ( Lester et al. , 2021). Since we are more interested in the ﬁnal performance of the language model,\nrather than the model parameters, it is promising to directly model the function F(θ) which is approximately\nrestricted in a small manifold in the functional space, and we discard the need to estimate the error (17) from\nmodel parameters.\nThe construction of ˆFis an art and differs in practice. The simplest strategy is to freeze some certain\nparts of the networks like BitFit ( Zaken et al. , 2021). Consider the more sophisticated construction that can\nimprove the approximation. Since the action of the function is characterized by the data ﬂow , one natural\nidea is to inject the low-rank representation in the data path of the original neural networks and the resulting\nnew language model is an incremental network like Adapter ( Houlsby et al. , 2019), as shown in (10). The\napproximation error (18) is determined by the representation capacity of the incremental network. Due to the\nuniversal approximation property ( Leshno et al. , 1993) of multilayer feedforward networks, the quality of\nthe approximation is guaranteed. It is worth noting that a similar architecture is also proposed in the area of\ncomputer vision ( Rebufﬁ et al. , 2017; Y e et al. , 2020).\nBy exploiting the autoregressive structure of Transformer, some more dedicated functional approximation can\nbe conceived. Note that the objective function generally is determined by the input and model parameters,\nnamely\nL\n(\n(X,Y ); θ\n)\n. In principle, we can frozen the model parameters θ, and make X and Y variable. In\nsome cases, it may be convenient to swap the positions of (X,Y ) and θto obtain a more tractable optimization\nproblem. Since θis the pre-trained language model that is unwieldy to process, it makes sense to use some\ntrainable ˜X to replace X since the feature space of X, namely range(X), is generally limited in a few\nthousand of dimension in a language task. This idea has been exploited in the prompt tuning ( Lester et al. ,\n2021), where a series of prompt tokens P are prepended to the input X. The prompt P is not parameterized\nby θ; instead, it has individual trainable parameters θP. By this way , the feature space is augmented but still\nfeasible for training. Owing to the autoregressive property of Transformer, the approximate function serves\nas a good surrogate of the original function Fto be optimized and steers the language model to focus on\nthe speciﬁc task. The preﬁx tuning ( Li & Liang , 2021) further makes some activations in the intermediate\nlayers trainable, thus leading to a more accurate functional approximation, or in other words, enlarging the\nrepresentation capability of the modiﬁed language model. Regarding to the performance, since the prompt\ntuning is a dimension reduction method that models the probability distribution with less parameters, the\neffectiveness is closely related to the model size and data size. With larger model size and data size, prompt\ntuning is prone to achieve better performance that is consistent with the dimension reduction theory ( Wright\n& Ma , 2021). Moreover, for high dimensional problems, it is possible to have more freedoms to choose the\nsubspace for the functional approximation, Su et al. (2021) and our experimental results in §5.3: S C A L E also\nverify this intuition.\nThe uniﬁed view of the approximations in solution space and functional space. Generally speaking, the\nrepresentations in solution space and function space often lead to similar constructions of the approximate\n˜Fdue to the duality relation. In fact, a uniﬁed view of Adapter ( Houlsby et al. , 2019), preﬁxing tuning\n(Li & Liang , 2021) and LoRA ( Hu et al. , 2021a) is proposed in ( He et al. , 2022) by analyzing the data\nﬂow in the modiﬁed language model. It is pointed out that these delta tuning methods all construct low\ndimensional modiﬁcations of the original data ﬂow , i.e., h←h+ ∆ h, where ∆ his parameterized by some\nlow dimensional parameters. This view can be recognized as understanding the different delta tuning methods\nfrom the perspective of functional space. Some useful empirical results about the ways to design the functional\napproximation can also be found in ( He et al. , 2022).\nOur discussion suggests that the performance of all these delta tuning methods rely on the low dimension\nassumption. In fact, it can even be found that there may exist some common subspace among various tasks\n(Qin et al. , 2021b), Su et al. (2021) and our experimental results in §5.4: T R A N S F E R A B I L I T Y also show the\ntransferability of delta tuning in different tasks. Since the actual performance of a delta tuning method is\ninevitably problem-dependent, it is promising to exploit more speciﬁc structures in the tasks at hand or build\nsome hybrid algorithm to make it more competitive with the full ﬁne-tuning of the original language model.\n4.2 Optimal Control Perspective for Delta T uning\nY ang & Liu (2022) propose to interpret preﬁx tuning from the perspective of optimal control. In this section,\nwe generalize the optimal control view to different delta tuning scenarios.\n14\n4.2 Optimal Control Perspective for Delta Tuning\nRelationship Between Optimal Control And Deep Learning . W e start with interpreting deep learning from\nthe optimal control perspective. According to Section 4 in Li et al. (2017), we review the theorems in the\nfollowing and directly follow their notations:\nTheorem 4.1 (discrete-time PMP) Consider the discrete-time control problem\nmin\n{θ0,...,θ T − 1}∈Θ T\nΦ( xT) + δ\nT−1∑\nt=0\nL(θt),\nxt+1 = xt + δft(xt,θt), x0 = x, 0 ≤t≤T −1\n(19)\nwhere Φ and Lare termination and running losses, respectively. There exists a co-process\nx∗\nt+1 = gt(x∗\nt,θ∗\nt), x ∗\n0 = x, (20)\np∗\nt = ∇xHt(x∗\nt,p∗\nt+1,θt), p ∗\nT+1 = −∇xΦ( x∗\nT+1) (21)\nsuch that\nHt(x∗\nt,p∗\nt+1,θ∗\nt) ≥Ht(x∗\nt,p∗\nt+1,θ), θ∈Θ , 0 ≤t≤T −1. (22)\nHere gt(xt,θt) := xt + δft(xt,θt) and\nHt(x,p,θ ) = p·gt(x,θ) −δL(θ) (23)\nis the discrete Hamiltonian with a scaling factor δ >0.\nTheorem 4.2\n(discrete-time MSA) The discrete-time method of successive approximations (MSA) character-\nizes the co-process in Theorem 4.1. F or each iteration k,\nset xk\n0 = x, and\nxk\nt+1 = gt(xk\nt,θk\nt) (24)\nwith tenumerating from 0 to T −1;\nthen set pk\nT = −∇xΦ( xk\nT), and\npk\nt = ∇xHt(xk\nt,pk\nt+1,θk\nt) (25)\nwith tenumerating from T −1 to 0;\nﬁnally, with tenumerating from 0 to T −1, set\nθk+1\nt = θk\nt + η∇θHt(xk\nt,pk\nt+1,θk\nt). (26)\nTheorem 4.3\n(equivalence between MSA and backpropagation) The MSA in Theorem 4.2 is equivalent to the\nbackpropagation process in deep networks.\nThe proofs of Theorems 4.1, 4.2 and 4.3 are provided in Li et al. (2017).\nT uned Delta’s As Optimal Controllers . W e consider delta tuning with pretrained autoregressive LMs (e.g.,\nGPT -2) for text classiﬁcation. By framing the content as the input sentence, the model generates the predicted\nlabel at the last step. For simplicity , we denote the position of label prediction as o. At position o, the model is\ninputted with a special token [ANS] and is expected to generate the prediction.\nDenote θas the parameters of the L-layer PLM. W e use the training set Dtr to optimize the delta parameters at\neach layer: {δ(0),...,δ (L−1)}. The intermediate activation of the j-th layer at step iis denoted as h(j)\ni . The\noptimization problem for delta tuning is formulated as\nmin\n{δ(0),...,δ (L− 1)}\nE(x,y)∼Dtr\n\nS\n(\nh(L)\no ,y\n)\n+\nL−1∑\nj=0\nR\n(\nδ(j)\n)\n\n\nh(j+1)\no = h(j)\no + G(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n, h(0)\no = zo = [ANS] , 0 ≤j ≤L−1,\n(27)\nwhere Sas the softmax scoring function, Ras the regularizer for delta parameters, zi as the i-th token in the\ninput and yas the label. The function Gdeﬁnes the altered forward propagation in the LM with the intervention\nof delta. Speciﬁcally , the learnable δ(j) activates the ﬁxed parameters from θso that the representation h(j)\no at\n15\n4.2 Optimal Control Perspective for Delta Tuning\nthe j-th layer can be properly transformed as G(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n. The representation transformation between\ntwo consecutive layers is thus described by function Gand the residual connection in Transformer.\nW e proceed to show that the problem ( 27) uniﬁes various delta tuning scenarios with different instances of G.\nI. Preﬁx-tuning (PF). Preﬁx-tuning belongs to addition-based methods and exploits the idea of prompting.\nWith Pidx as the preﬁx indexes, the forward propagation at the output position ocan be formulated as\nh(j+1)\no = LM (j)\nθ\n(\nh(j)\no\n⏐\n⏐\n⏐h(j)\n<o\n)\n, (28)\nwhere h(j)\ni = Pδ(j) [i,:] for all j = 0 to L−1, i∈Pidx and h(0)\ni = zi\nfor i /∈Pidx. LM(j)\nθ , the j-th layer of\nthe LM, can be decomposed into a self-attention layer ( SAN(j)\nθ ) and a FFN layer ( FFN(j)\nθ ). Formally ,\nh(j+1)\no = h(j)\no + SAN(j)\nθ\n(\nh(j)\no ,h(j)\n<o\n)\n+ FFN(j)\nθ\n(\nh(j)\no + SAN(j)\nθ\n(\nh(j)\no ,h(j)\n<o\n))\n(29)\nwith h(j)\ni = Pδ(j) [i,:] for i ∈Pidx. As Eq. ( 29) is recursive and according to the fact that the PLM is\nautoregressive, after unrolling the recursion for all h(j)\n<o in Eq. ( 29), we have that for any i<o , h(j)\ni is steered\nby the preﬁx δ(j), namely h(j)\ni = h(j)\ni\n(\nδ(j))\n. As a result, for preﬁx-tuning, the G(j)\nθ\n(\nh(j)\no ,δ(j)\n)\nin problem\n(27) is instantiated as\nG(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n= SAN (j)\nθ\n(\nh(j)\no ,h(j)\n<o\n)\n+ FFN(j)\nθ\n(\nh(j)\no + SAN(j)\nθ\n(\nh(j)\no ,h(j)\n<o\n))\n, (30)\nwhere each item in h(j)\n<o is a function of δ(j).\nII. Adapter (AP). Adapter belongs to addition-based methods as well. Instead of prompting, the Adapter\nmethod adds tunable modules between consecutive Transformer layers as the delta. As characterized by Eq.\n(10), the altered forward propagation is written as\n˜h(j+1)\no = LM (j)\nθ\n(\nh(j)\no\n⏐\n⏐\n⏐h(j)\n<o\n)\n,\nh(j+1)\no = ˜h(j+1)\no + σ\n(\n˜h(j+1)\no W(j)\nd\n)\nW(j)\nu ,\n(31)\nwhere ˜h(j+1)\no is the transformed representation by the original j-th layer in the PLM, and σ denotes the\nnonlinearity . δ(j) in this case is deﬁned as {W(j)\nd ,W(j)\nu }, the two projection matrices at the j-th layer. It is\nnoted that the computation of LM in Eq. ( 31) also follows the formulation of Eq. ( 29) (with difference only in\ninputs). Substituting Eq. ( 29) in Eq. ( 31) yields\nG(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n= SAN (j)\nθ\n(\nh(j)\no ,h(j)\n<o\n)\n+ FFN(j)\nθ\n(\nh(j)\no + SAN(j)\nθ\n(\nh(j)\no ,h(j)\n<o\n))\n+ σ\n(\nLM(j)\nθ\n(\nh(j)\no\n⏐\n⏐\n⏐h(j)\n<o\n)\nW(j)\nd\n)\nW(j)\nu .\n(32)\nHere each item in h(j)\n<o is independent of δ(j) = {W(j)\nd ,W(j)\nu }, which is different from the preﬁx-tuning case.\nIII. LoRA (LR). LoRA belongs to reparameterization-based methods. The update by LoRA is\nh ←szWdWu + h, (33)\nwhere z is the input and s≥1 is a tunable scalar hyperparameter. Similar with the Adapter scenario, we still\ndeﬁne δ(j) = {W(j)\nd ,W(j)\nu }. The function Gin this case is\nG(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n= SAN (j)\nθ\n(\nh(j)\no ,h(j)\n<o\n)\n+ FFN(j)\nθ\n(\nh(j)\no + SAN(j)\nθ\n(\nh(j)\no ,h(j)\n<o\n))\n+ szW(j)\nd W(j)\nu .\n(34)\nIV . BitFit. BitFit belongs to speciﬁcation-based methods. BitFit tunes only the bias parameters in the PLM.\nW e deﬁne θ= {ψ,δ}, where δrepresents the tuned bias parameters, and ψis the ﬁxed ones. In this case, the\nformulation of LM in Eq. ( 29) becomes\nh(j+1)\no = h(j)\no + SAN(j)\nψ\n(\nh(j)\no ,δ(j)\nS ,h(j)\n<o\n)\n+ FFN(j)\nψ\n(\nh(j)\no + SAN(j)\nψ\n(\nh(j)\no ,δ(j)\nS ,h(j)\n<o\n)\n,δ(j)\nF\n)\n, (35)\n16\n5. Comparisons and Experimental Discoveries\nwith δ(j) = {δ(j)\nS ,δ(j)\nF }as the bias terms of the SAN and FFN layers. The function Gis thus given by\nG(j)\nθ\n(\nh(j)\no ,δ(j)\n)\n= SAN (j)\nψ\n(\nh(j)\no ,δ(j)\nS ,h(j)\n<o\n)\n+ FFN(j)\nψ\n(\nh(j)\no + SAN(j)\nψ\n(\nh(j)\no ,δ(j)\nS ,h(j)\n<o\n)\n,δ(j)\nF\n)\n. (36)\nW e have listed the formulations of the function Gin problem ( 27) for different delta tuning methods. With\nTheorem 4.1, S and Rin problem ( 27) can be viewed as the terminal and the running loss with the delta\nparameters as the control variables. This means that ( 27) can be formulated as a discrete-time control problem.\nWith Theorems 4.2 and 4.3, the forward and backward propagation in optimization delta’s are equivalent to\nthe calculation of the co-state process in Pontryagin’s Maximum Principle ( Kopp, 1962). T o conclude, delta\ntuning can be viewed as seeking the optimal control of PLMs for speciﬁc downstream tasks.\nOur analysis sheds light on designing novel delta tuning methods that are inspired from control theories. One\ncan refer to control theories when designing robust models ( Zhang et al. , 2019a). For example, Y ang & Liu\n(2022) propose robust preﬁx-tuning that tunes an additional robust preﬁx during inference to guide the LM\ntowards correct predictions. The idea of test-time activation rectiﬁcation can be viewed as close-loop feedback\ncontrol ( Chen et al. , 2021). W e have also shown that the intervention of delta’s with the PLMs is equivalent\nto the design of controllers. By applying the theories of controller design ( Boyd & Barratt , 1991; Ang et al. ,\n2005), we expect more delta methods be proposed with theoretical guarantees. The designed delta structures\nare in this way interpretable in principle while sufﬁciently exploiting the power of PLMs.\n5 Comparisons and Experimental Discoveries\nAs an effective engine to stimulate large-size PLMs, delta tuning presents an enormous practical potential\nfor various real-world applications. In this section, we carry out systematic experiments to gain a deeper\nunderstanding of the attributes of different mainstream delta tuning methods.\nSpeciﬁcally , (1) we ﬁrst conduct thorough comparisons among four representative delta tuning methods and\nﬁne-tuning in §5.1: P E R F O R M A N C E, covering the performance, convergence and the efﬁciency analysis; (2)\nsecondly , we explore the combinability of three representative delta tuning methods in §5.2: C O M B I NAT I O N\nby comparing the performance under both the full-data and low-resource setting. W e also explore the effects of\nmanual templates for delta tuning methods; (3) furthermore, we investigate the scaling law in\n§5.3: S C A L E and\n(4) the transferability of delta tuning methods among different downstream tasks in §5.4: T R A N S F E R A B I L I T Y.\nThe implementation details and tasks are described in §A: D E TA I L S and §B: TA S K S. W e will release the codes,\ndataset splits and trained delta checkpoints to facilitate future research attempts.\n5.1 Performance, Convergence and Efﬁciency\nExperimental Setting. W e evaluate vanilla ﬁne-tuning ( FT) and four representative delta tuning methods,\nincluding prompt tuning ( PT), preﬁx-tuning ( PF), LoRA ( LR) and adapter ( AP). Other representative delta\ntuning methods ( Liu et al. , 2021b; Zaken et al. , 2021; Guo et al. , 2021; Liu et al. , 2022) are omitted.\nT o cover broad and diverse NLP tasks, we randomly select over 100 representative tasks from Huggingface\ndatasets6 (Lhoest et al. , 2021). The selected tasks include text classiﬁcation (e.g., sentiment analysis and\nnatural language inference), question answering (e.g., machine reading comprehension and multi-choice\nquestion answering), conditional generation (e.g., summarization and dialogue), etc. W e list the task details\nof each category in T able 10. T o handle different tasks with a single text-to-text PLM, following Raffel et al.\n(2019), we process the input and output of each task into the same sequence-to-sequence format.\nW e choose T5BASE (Raffel et al. , 2019) as the mainly evaluated PLM backbone for different tuning methods,\nand we additionally report the performance of PT with T5LARGE (Raffel et al. , 2019). For both models, we use\nthe checkpoints released by Lester et al. (2021), who conducted additional 100k steps of LM adaption on the\nofﬁcial checkpoints released by Raffel et al. (2019). Such an LM adaptation objective has been demonstrated\nbeneﬁcial for better performance and faster convergence during downstream adaptation, compared with only\nthe original “span corruption” pre-training objective of T5. W e follow the common practice for each delta\ntuning’s implementation. For PF, we use 5 preﬁx tokens; for PT, we prepend 100 tunable soft tokens into the\ninput embedding; for LR, we reparameterize all the query matrices and the value matrices in the multi-head\nattention modules as low-rank decompositions, and set the rank to 8; for AP, we insert adapter modules\ninto both the multi-head attention module and the feed-forward network in each Transformer layer, set the\nbottleneck dimension to 64, and choose SiLU (Elfwing et al. , 2018) as the activation function. More training\ndetails are left in §A.1: P E R F O R M A N C E D E TA I L S .\n6 https://huggingface.co/datasets\n17\n5.1 Performance, Convergence and Efﬁciency\nPerformance Analysis. The overall results are listed in T able 3, from which we observe that: (1) in general,\nsince different delta tuning methods signiﬁcantly reduce the amounts of tunable parameters, they are no match\nfor FT in performance under most cases. But after averaging the results over all datasets, the gap between the\ndelta tuning methods and the ﬁne-tuning method is not insurmountable, which demonstrates the potential of\nthe large-scale applications of parameter-efﬁcient adaptations. (2) Despite having different design elements,\nPF, LR and AP are comparable with each other in performance. Speciﬁcally , each of them is possible to show\ndominant performance (even better than FT) over others on certain tasks. According to the average results, the\nperformances of all the methods are ranked as FT >LR >AP >PF > PT. Interestingly , the performance\nof the delta tuning methods is not consistent with their number of tunable parameters, i.e., at least on small\nPLMs, more tunable parameters do not necessarily lead to approximately better performance, and the design\nof the structure for delta tuning may play a greater role. (3) As the easiest of these methods to implement\n(i.e. without modifying the internal structure of the model), PT lags far behind other delta tuning methods in\nmost cases when experimented on T5BASE, although better PT performance is observed when the model size is\nsigniﬁcantly enlarged to T5LARGE, which is aligned with previous ﬁndings on the power of scale for prompt\ntuning ( Lester et al. , 2021)7 . However, as we would show later ( §5.3: S C A L E), other delta tuning methods also\nexhibit far better performance when the scale of the backbone PLM grows extremely large. That is, unlike the\nconclusion of (3), when the model increases sharply , the design of the structure may become less important for\nthe delta tuning methods.\nT able 3: Overall (test) performance of over 100 NLP tasks comparing prompt tuning ( PT), preﬁx-tuning\n(PF), LoRA ( LR), Adapter ( AP) and ﬁne-tuning ( FT). W e experiment all methods on T5BASE, with the best\nperformance highlighted in bold, and additionally report the performance of PT on T5 LARGE.\nT ask PT PT PF LR AP FT(base) (large)\nRatio of tunable parameters 0.03% 0.01% 7.93% 0.38% 2.38% 100%\nAC RO N Y M _I D E N T I FI C AT I O N 93.35 96.68 96.12 96.12 95.57 96.12\nA D E_C O R P U S_V2- C L A S S I FI C AT I O N 41.76 94.42 93.25 94.47 93.91 94.27\nA D E_C O R P U S_V2- D O S AG E 78.57 89.29 82.14 85.71 82.14 82.14\nA D E_C O R P U S_V2- E FF E C T 59.15 61.35 63.25 62.52 60.91 62.66\nA DV E R S A R I A L _Q A 34.10 54.60 43.17 46.40 45.35 48.56\nAG _N E W S 91.37 93.61 93.42 94.63 94.60 95.19\nA N L I 25.85 44.96 43.88 45.27 49.19 50.54\nA S L G_P C12 15.78 44.07 47.71 73.72 80.65 92.92\nB L I M P-A NA P H O R _G E N D E R_AG R E E M E N T 100.00 100.00 100.00 100.00 100.00 99.00\nB L I M P-A NA P H O R _N U M B E R_AG R E E M E N T 49.00 100.00 100.00 100.00 100.00 100.00\nB L I M P-D E T E R M I N E R_N O U N_AG R E E M E N T 46.00 100.00 100.00 100.00 100.00 100.00_W I T H_A D J_I R R E G U L A R_1\nB L I M P-E L L I P S I S_N_BA R _1 49.00 100.00 100.00 100.00 100.00 100.00\nB L I M P-E X I S T E N T I A L_T H E R E 53.00 100.00 100.00 100.00 100.00 100.00_QUA N T I FI E R S _1\nB L I M P-I R R E G U L A R_PA S T 100.00 100.00 100.00 100.00 100.00 100.00_PA RT I C I P L E_A D J E C T I V E S\nB L I M P-S E N T E N T I A L_N E G AT I O N 54.00 100.00 100.00 100.00 100.00 100.00_N P I_S C O P E\nB L I M P-W H_QU E S T I O N S _O B J E C T_G A P 55.00 100.00 100.00 100.00 100.00 100.00\nB O O L Q 61.28 77.43 77.55 80.00 78.47 81.77\nC I R C A 13.51 77.39 80.16 82.38 82.93 84.69\nC L I M AT E_F E V E R 15.47 33.42 38.03 39.35 37.48 41.57\nC O M M O N S E N S E_Q A 58.43 76.76 58.43 62.52 60.72 61.21\nC O S_E 12.41 14.82 13.90 14.05 14.31 13.46\nC O S M O S_Q A 7.30 10.98 9.91 10.78 10.85 11.32\nC R AW L_D O M A I N 68.16 76.91 73.04 73.00 72.76 75.12\nD I S C OV E RY 0.18 18.83 16.67 18.98 18.41 25.88\nD R E A M 49.19 71.83 58.70 61.00 59.53 62.42\nE L I5- A S K H 11.26 11.70 12.64 11.99 11.45 13.00\nE L I5- A S K S 14.79 15.54 15.09 15.25 15.01 15.28\n7 W e found empirically that PT tends to perform worse and converge more slowly for small-scale PLMs.\n18\n5.1 Performance, Convergence and Efﬁciency\nE L I5- E L I5 14.19 15.38 15.23 14.59 14.43 14.75\nE M O 69.91 71.47 73.31 76.13 74.88 75.69\nE M OT I O N 89.19 88.73 88.29 88.63 88.98 89.25\nE T H O S-D I R E C T E D_V S_G E N E R A L I Z E D 76.86 86.64 94.76 92.29 94.94 94.94\nE T H O S-D I S A B I L I T Y 46.99 100.00 93.81 93.81 100.00 93.81\nE T H O S-G E N D E R 63.84 77.08 77.44 79.91 79.91 74.48\nE T H O S-NAT I O NA L _O R I G I N 44.30 81.77 81.77 87.95 84.72 84.72\nE T H O S-R AC E 84.36 97.06 94.54 97.21 94.27 97.21\nE T H O S-R E L I G I O N 93.02 93.02 96.35 93.02 96.35 96.64\nFI NA N C I A L _P H R A S E BA N K 97.18 98.36 98.36 97.94 97.95 98.36\nF R E E BA S E _Q A 1.90 6.71 2.63 3.75 5.86 23.52\nG L U E-C O L A 0.00 55.60 50.95 49.40 44.66 51.53\nG L U E-M N L I 35.43 86.12 82.21 83.74 83.90 86.39\nG L U E-M R P C 67.65 88.24 87.25 87.25 87.25 89.71\nG L U E-Q N L I 52.34 93.01 87.48 92.02 91.58 92.57\nG L U E-Q Q P 84.65 86.21 84.62 86.87 85.93 89.13\nG L U E-RT E 45.32 79.14 72.66 79.14 78.42 80.58\nG L U E-S S T2 92.20 94.95 92.66 94.04 93.35 94.27\nH AT E_S P E E C H_O FF E N S I V E 73.27 79.08 75.22 75.21 75.06 75.04\nH AT E_S P E E C H18 75.57 74.45 79.42 79.59 80.86 80.93\nH AT E X P L A I N 50.98 67.62 66.06 68.03 68.11 68.02\nH E A LT H_FAC T 39.15 45.60 50.38 52.05 51.21 54.19\nH E L L A S WAG 23.82 70.28 24.76 32.82 27.60 41.90\nH OT P OT _Q A 65.95 76.41 73.76 76.13 74.65 78.45\nL A M A-C O N C E P T N E T 15.25 26.12 22.63 34.96 43.62 70.28\nL A M A-G O O G L E_R E 11.78 14.08 12.60 18.82 23.73 24.88\nL A M A-S QUA D 3.23 16.13 12.90 9.68 3.23 9.68\nL A M A-T R E X 59.13 63.68 63.91 66.21 67.23 69.12\nL I A R 13.23 28.87 26.46 28.67 27.08 28.20\nM C_TAC O 76.25 88.39 86.02 88.13 86.81 87.34\nM E D I C A L_QU E S T I O N S _PA I R S 46.56 91.80 85.25 88.52 90.16 87.21\nM U LT I_N E W S 18.09 19.23 18.81 19.44 19.10 19.80\nN U M E R_S E N S E 50.53 56.75 53.30 56.27 53.97 57.32\nO N E S TO P _E N G L I S H 22.53 98.23 100.00 100.00 100.00 100.00\nO P E N B O O K Q A 44.80 54.40 50.20 52.20 53.80 57.00\nPAW S 49.60 91.27 92.07 93.39 92.91 93.60\nP O E M_S E N T I M E N T 54.18 70.31 85.38 86.80 82.52 83.26\nP ROTO _Q A 21.16 37.66 24.57 27.87 26.17 34.47\nQ A S C 19.22 47.73 33.26 37.80 33.05 43.63\nQUA R E L 54.89 54.71 57.25 59.78 57.61 62.50\nQUA RT Z -N O_K N OW L E D G E 65.43 68.88 68.49 67.09 66.96 69.39\nQUA RT Z -W I T H_K N OW L E D G E 64.03 85.97 71.56 74.23 73.72 76.28\nR AC E -H I G H 34.51 60.09 42.82 59.52 58.92 65.95\nR AC E -M I D D L E 47.21 74.65 62.67 68.31 65.46 70.61\nROT T E N _TO M ATO E S 88.36 91.84 89.96 89.30 89.20 89.77\nS A M S U M 39.35 45.12 43.38 45.00 44.68 45.73\nS C I Q 96.95 98.53 98.08 98.42 98.19 98.30\nS C I TA I L 91.02 95.47 93.04 93.80 94.04 94.77\nS E A R C H_Q A 7.14 19.17 8.70 10.17 9.72 19.26\nS I C K 40.10 88.82 87.91 88.69 88.88 89.15\nS M S_S PA M 95.80 97.46 97.14 97.14 97.46 97.11\nS P I D E R 3.29 6.38 7.74 9.67 8.70 6.77\nS U P E R G L U E-C B 75.00 78.57 100.00 100.00 96.43 96.43\nS U P E R G L U E-C O PA 53.60 56.00 58.40 56.40 60.40 59.20\nS U P E R G L U E-R E C O R D 44.67 73.82 61.62 64.66 62.08 67.20\nS U P E R G L U E-RT E 50.36 84.89 73.38 79.14 82.01 78.42\nS U P E R G L U E-W I C 50.16 68.34 64.89 68.65 70.53 71.79\nTA B_FAC T 46.65 50.16 52.53 56.86 53.42 57.34\n19\n5.2 Combinations of Delta Tuning Methods\nT R E C 90.80 91.51 91.38 93.38 93.36 94.81\nT R E C-FI N E G R A I N E D 80.63 88.18 90.04 91.44 90.00 91.27\nT W E E T_E V A L-H AT E 53.00 42.23 44.67 48.16 47.88 51.33\nT W E E T_E V A L-I RO N Y 58.02 69.73 76.00 76.75 73.88 77.43\nT W E E T_E V A L-O FF E N S I V E 75.94 78.87 80.94 80.97 80.59 82.05\nT W E E T_E V A L-S E N T I M E N T 28.90 72.79 71.78 71.31 71.90 71.98\nT W E E T_E V A L-S TA N C E_A B O RT I O N 32.59 61.42 61.47 63.20 62.61 61.72\nT W E E T_E V A L-S TA N C E_AT H E I S M 56.28 67.58 71.54 71.77 71.27 74.41\nT W E E T_E V A L-S TA N C E_C L I M AT E 47.61 52.43 52.86 55.92 59.06 57.38\nT W E E T_E V A L-S TA N C E_F E M I N I S T 29.65 51.63 56.27 57.41 58.57 58.51\nT W E E T_E V A L-S TA N C E_H I L L A RY 41.34 63.18 62.15 65.40 61.74 66.41\nW E B_QU E S T I O N S 11.90 19.58 15.87 18.78 20.63 25.40\nW I K I_B I O 42.39 44.03 44.84 45.36 46.19 47.09\nW I K I_Q A 48.78 73.97 64.10 72.15 70.75 74.41\nW I K I_S P L I T 79.80 80.10 79.91 80.09 80.05 80.34\nW I N O_G R A N D E 48.42 58.20 50.79 61.20 50.47 67.19\nW I Q A 36.10 65.27 63.67 77.99 64.44 79.82\nX S U M 21.35 26.56 23.84 25.87 26.07 29.90\nY E L P_P O L A R I T Y 95.47 98.18 97.78 97.37 97.30 97.92\nA verage 48.81 65.92 64.07 66.06 65.58 67.96\nConvergence Analysis. In Figure 6, Figure 7 and Figure 8, we visualize the performance of different delta\ntuning methods ( LR, AP, PF) and ﬁne-tuning ( FT) at different training steps to compare their convergence rate.\nIt could be derived that, the convergence rate of these tuning methods are ranked as: FT >AP ≈LR >PF.\nOverall, FT is the most stable method for convergence, and despite the fact that PF has the highest number of\ntunable parameters of all delta tuning methods, it still faces some convergence difﬁculties (the original paper\nalso mentions that the convergence of PF is very dependent on the reparameterization).\nSince PT lags far behind other tuning methods in both convergence rate and performance, we do not visualize\nit in the above ﬁgures. But as mentioned in §3.1: A D D I T I O N, PT is the easiest method to implement and\nit is desirable to theoretically and empirically further study the convergence issue across different sizes of\nPLMs. W e also found empirically that, (1) for each delta tuning method, within a reasonably broad range,\nboth performance and convergence are not sensitive to the number of tunable parameters, but more sensitive to\nthe structures of the methods, and (2) with the scale of PLM growing larger, the convergence of delta tuning is\nalso accelerated ( §5.3: S C A L E). T o summarize, our experiments yield very similar conclusions in terms of\nconvergence and overall performance, and these conclusions are well supported by the fact that we used the\nsame experimental and implementation setup, same model selection strategy , and plenty of datasets.\nEfﬁciency Analysis. Delta tuning saves GPU memory by alleviating the need for gradient computations for\nmost parameters. T o speciﬁcally verify the efﬁciency of GPU memory , in Figure 9, we conduct experiments to\ncompare the GPU memory consumed by different delta tuning methods and ﬁne-tuning across different PLM\nscales. W e choose three scales of T5 model, i.e., T5BASE,T5LARGE, T5XL, , and test the peak GPU memories\nachieved under different batchsizes. The static GPU memories, which leave out the intermediate tensors such\nas hidden states, are draw on Batchsize=0. W e use NVIDIA A100 (maximum GPU memory=39.58GB) and\nlibrary OpenDelta 8 for these experiments. For the cases which consume large GPU memory than a single\nA100, we parallelize the model across several GPUs using model parallelization, which doesn’t introduce\nadditional memory consumption. W e can see from the ﬁgure that under small batchsizes (e.g., 1, 8), delta\ntuning saves up to 3/4 GPU memory , which under big batchsizes (e.g., 64), delta tuning saves at least 1/3\nGPU memory . Given the fact that small batchsize is more preferred when applying big models to save GPU\nmemory , delta tuning can further reduce the gpu memory dramatically .\n5.2 Combinations of Delta T uning Methods\nConsidering that different delta tuning methods are compatible with each other, which means they could be\napplied on the same PLM together, we thus investigate whether such a combination would bring additional\nbeneﬁts. Speciﬁcally , we evaluate both simultaneous combination and sequential combination. W e choose\n8 https://github.com/thunlp/OpenDelta\n20\n5.2 Combinations of Delta Tuning Methods\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.79\n0.84\n0.89\n0.94\n0.99EM\nacronym_identification\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.79\n0.84\n0.88\n0.92\n0.97Classification-F1\nag_news\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.02\n0.15\n0.28\n0.41\n0.54Classification-F1\nanli\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.24\n0.48\n0.72\n0.97EM\naslg_pc12\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.36\n0.48\n0.6\n0.72\n0.84ACC\nboolq\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.05\n0.26\n0.48\n0.69\n0.9\nClassification-F1\ncirca\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.02\n0.19\n0.36\n0.52\n0.69ACC\ncommonsense_qa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.18\n0.33\n0.48\n0.62\n0.77EM\ncrawl_domain\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.06\n0.13\n0.19\n0.25Classification-F1\ndiscovery\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.17\n0.34\n0.51\n0.68ACC\ndream\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.06\n0.12\n0.17\n0.23EM\nfreebase_qa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.29\n0.44\n0.6\n0.75\n0.9\nACC\nglue-mnli\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.45\n0.57\n0.7\n0.82\n0.94ACC\nglue-qnli\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.3\n0.46\n0.62\n0.77\n0.93ACC\nglue-qqp\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.44\n0.57\n0.71\n0.85\n0.98ACC\nglue-sst2\nPF\nFT\nAP\nLR\nFigure 6: The performance of T5BASE with different delta tuning methods ( LR, AP, PF) and ﬁne-tuning ( FT)\nat different training steps. Note we apply early stop in all the experiments. The performance of PT is omitted\nsince it lags far behind other tuning methods in both convergence and performance.\nthree representative delta tuning methods, including prompt tuning, BitFit, and adapter, to explore the effects\nof their combinations. The training details are described in §A.2: C O M B I NAT I O N D E TA I L S .\nSimultaneous Combination. W e ﬁrst explore the effects of directly applying all the three delta tuning\nmethods simultaneously . The experiments are conducted using both RoBER T aLARGE (Liu et al. , 2019) and\nT5BASE on eight GLUE tasks ( W ang et al. , 2019), and we report the performance on development sets. W e also\ntest the performance of RoBER T aLARGE under the few-shot setting, where we randomly sample 16 training\nexamples per label to construct the new training set and development set, respectively .\n21\n5.2 Combinations of Delta Tuning Methods\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.26\n0.4\n0.54\n0.68\n0.82Classification-F1\nhate_speech_offensive\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.06\n0.26\n0.46\n0.65\n0.85Classification-F1\nhate_speech18\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.19\n0.37\n0.56\n0.75Classification-F1\nhatexplain\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.13\n0.27\n0.4\n0.54ACC\nhellaswag\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.22\n0.38\n0.53\n0.69\n0.85QA-F1\nhotpot_qa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.04\n0.21\n0.39\n0.57\n0.74EM\nlama-conceptnet\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.43\n0.5\n0.57\n0.64\n0.71EM\nlama-trex\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.08\n0.17\n0.25\n0.34Classification-F1\nliar\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96ACC\nmc_taco\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.47\n0.58\n0.7\n0.81\n0.92ACC\nmedical_questions_pairs\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.15\n0.3\n0.46\n0.61EM\nnumer_sense\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.01\n0.16\n0.32\n0.47\n0.62ACC\nopenbookqa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.1\n0.32\n0.54\n0.77\n0.99Classification-F1\npaws\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.02\n0.11\n0.19\n0.27\n0.36EM\nproto_qa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.11\n0.22\n0.32\n0.43ACC\nqasc\nPF\nFT\nAP\nLR\nFigure 7: Continued with Figure 6. The performance of T5BASE with different delta tuning methods ( LR, AP,\nPF) and ﬁne-tuning ( FT) at different training steps. Note we apply early stop in all the experiments.\nSimilar to prompt-based ﬁne-tuning ( Schick & Schütze , 2021), we insert a natural language prompt template\ninto the input text for each task. T ake the sentiment classiﬁcation task as an example, an input sentence x=\n(I like this movie. ) could be re-formulated as: xprompt = [CLS] xIt was [MASK]. [SEP], where “ It was\n[MASK]. ” is a manual template. The PLM is trained to ﬁll in the [MASK] token with either “ great” (positive)\nor “ terrible” (negative) for classiﬁcation. The manual templates are designed to bridge the gap between\npre-training and downstream tuning, we also test the performance of delta tuning combinations without\ntemplates, i.e.,\nx′\nprompt = [CLS] x[SEP] [MASK] [SEP]\n, to evaluate manual templates’ functionalities.\nThe manual templates and label words for different GLUE tasks are listed in T able 4.\n22\n5.2 Combinations of Delta Tuning Methods\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.17\n0.34\n0.51\n0.68ACC\nquarel\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.45\n0.54\n0.63\n0.72\n0.81ACC\nquartz-no_knowledge\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.21\n0.43\n0.64\n0.86ACC\nquartz-with_knowledge\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.18\n0.36\n0.54\n0.72ACC\nrace-high\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.19\n0.38\n0.57\n0.77ACC\nrace-middle\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96Classification-F1\nrotten_tomatoes\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.19\n0.39\n0.6\n0.8\n1.0\nClassification-F1\nscitail\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.06\n0.12\n0.17\n0.23EM\nsearch_qa\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.18\n0.37\n0.55\n0.74\n0.93Classification-F1\nsick\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.2\n0.33\n0.46\n0.6\n0.73QA-F1\nsuperglue-record\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nClassification-F1\ntrec\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.12\n0.33\n0.53\n0.73\n0.94Classification-F1\ntrec-finegrained\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.21\n0.41\n0.62\n0.83Classification-F1\ntweet_eval-hate\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.0\n0.2\n0.4\n0.6\n0.8Classification-F1\ntweet_eval-sentiment\nPF\nFT\nAP\nLR\n100 200 400 800 1600 3200 6400 12800 25600\nsteps\n0.03\n0.09\n0.15\n0.2\n0.26EM\nweb_questions\nPF\nFT\nAP\nLR\nFigure 8: Continued with Figure 7. The performance of T5BASE with different delta tuning methods ( LR, AP,\nPF) and ﬁne-tuning ( FT) at different training steps. Note we apply early stop in all the experiments.\nW e list the results of RoBER T aLARGE in T able 5, from which we could conclude that: for RoBER T aLARGE, (1)\nunder both full-data setting and the few-shot setting, introducing adapter into the combination almost always\nconduces to the average GLUE performance no matter whether there exist manual templates; (2) introducing\nprompt tuning into the combination generally harms the average performance, showing that prompt tuning\nmay not be compatible with other two delta tuning methods; (3) introducing BitFit into the combination\ngenerally improves the average performance; (4) manual templates could signiﬁcantly improve the zero-shot\nperformance (from 23.7 to 43.4) by narrowing the gap between downstream tuning and pre-training. Under the\nfew-shot setting, manual templates could also help boost the average performance evidently . However, when\n23\n5.2 Combinations of Delta Tuning Methods\n0 1 8 32 64\nBatchsize\n0.0\n2.5\n5.0\n7.5\n10.0\nGPU Memory (GB)\n3.3\n3.7\n4.4\n7.3\n9.8\n0.9\n0.9\n1.6\n4.0\n7.2\n0.8\n0.9\n1.6\n3.8\n6.8\n0.8\n0.9\n1.4\n3.3\n5.8\nT 5\nBASE\n0 1 8 32 64\nBatchsize\n0.0\n7.5\n15.0\n22.5\n30.0\n11.0\n11.6\n13.5\n21.1\n28.1\n2.8\n3.0\n4.8\n11.0\n19.3\n2.8\n2.9\n4.7\n10.5\n18.3\n2.8\n2.9\n4.4\n9.3\n15.9\nT 5\nLARGE\n0 1 8 32 64\nBatchsize\n0.0\n25.0\n50.0\n75.0\n100.0\n42.5\n43.6\n48.5\n66.5\n90.5\n10.7\n11.2\n16.1\n33.1\n55.6\n10.7\n11.1\n16.0\n32.6\n54.6\n10.7\n11.2\n15.8\n30.5\n52.3\nT 5\nXL\nFT\nAP\nLR\nBF\nFigure 9: GPU memory consumed by each delta tuning methods compared with ﬁne-tuning.\nT able 4: Manual templates and the corresponding label words for different tasks.\nT ask T emplate Label words\nCoLA (W arstadt et al. , 2019) ⟨S1⟩ This is [MASK] . grammatical: correct, not_grammatical: incorrect\nSST -2 (Socher et al. , 2013) ⟨S1⟩ It was [MASK] . positive: great, negative: terrible\nMRPC (Dolan & Brockett , 2005) ⟨S1⟩ [MASK] , ⟨S2⟩ equivalent: Y es, not_equivalent: No\nSTS-B (Cer et al. , 2017) ⟨S1⟩ [MASK] , ⟨S2⟩ yu: Y es, yl: No\nQQP (link) ⟨S1⟩ [MASK] , ⟨S2⟩ equivalent: Y es, not_equivalent: No\nMNLI (Williams et al. , 2018) ⟨S1⟩? [MASK] , ⟨S2⟩ entailment: Y es, neutral: Maybe, contradiction: No\nQNLI (Rajpurkar et al. , 2016) ⟨S1⟩? [MASK] , ⟨S2⟩ entailment: Y es, no_entailment: No\nRTE (Dagan et al. , 2005) ⟨S1⟩? [MASK] , ⟨S2⟩ entailment: Y es, no_entailment: No\nthe training supervision is abundant (full-data setting), manual templates only exhibit marginal improvements\nor even harm the performance.\nW e list the results of T5BASE in T able 6, from which we observe slightly different phenomena than\nRoBER T aLARGE as follows: (1) still, introducing prompt tuning into the combination would always harm\nthe performance no matter whether there exist manual templates, showing that prompt tuning may not be\ncompatible with other two delta tuning methods for T5BASE, either; (2) introducing BitFit into the combina-\ntion, however, could always conduce to the average performance; (3) adapter does not always improve the\nperformance when there exist manual templates but could still bring beneﬁts when there do not exist manual\ntemplates; (4) inserting manual templates into the input text would always improve the average performance.\nThe improvements tend to be more evident than RoBER T a LARGE.\nSequential Combination. In addition to the simultaneous combination, we further investigate the com-\npatibility when the above three delta tuning methods (prompt tuning, BitFit, and adapter) are sequentially\nintroduced. Speciﬁcally , we split the whole tuning process into\n3 stages. During each stage, we train an\nindividual delta tuning method for 6,000 steps; in the stages to follow , we freeze the tuned parameters in\nprevious stages and only optimize the newly introduced delta parameters. W e experiment RoBER T a LARGE on\nSST -2 (Socher et al. , 2013) with / without manual templates. The results are visualized in Figure 10, from\nwhich we could derive that, under certain cases, the performance could be improved with the involvements\nof subsequent delta tuning methods. However, there does not exist an optimal sequential combination under\ndifferent settings.\nGeneralization Gap. Additionally , we report the generalization gap (train performance - dev performance)\nfor RoBER T aLARGE under the full-data setting, with the results shown in T able 7. It could be derived that, (1)\nthe gap of a single delta tuning method is always smaller than ﬁne-tuning, which means over-parameterization\nmay help better memorize (overﬁt) training samples. Among all the delta tuning methods, prompt tuning\ntends to have the smallest generalization gap. Considering that each delta tuning method could already\ngeneralize well and achieve non-trivial performance on the dev set, hence overﬁtting the training set may not\nbe the prerequisite for good generalization; (2) in general, combining delta tuning methods would enlarge the\ngeneralization gap, even to the extent that is comparable with ﬁne-tuning, despite tuning far fewer parameters.\nThis suggests that, for the investigated tasks, memorizing the training set may not require employing all of the\nparameters; in other words, a small model capacity during downstream adaptation may be enough for good\nmemorization; (3) utilizing manual templates generally would not inﬂuence the generalization gap.\n24\n5.2 Combinations of Delta Tuning Methods\nT able 5: Performance of RoBER T aLARGE on GLUE datasets. W e report the average result of multiple random\nseeds on the validation set.\nPrompt /enc-37/enc-37/enc-37/enc-37/enc-33/enc-33/enc-33/enc-33\nBitFit /enc-37/enc-37/enc-33/enc-33/enc-37/enc-37/enc-33/enc-33\nAdapter /enc-37/enc-33/enc-37/enc-33/enc-37/enc-33/enc-37/enc-33\nTunable parameters 0% 1.75% 0.09% 1.84% 0.003% 1.76% 0.09% 1.85%\nRoBER T aLARGE , full-data, without manual templates\nCoLA(Matt.) 4.6 66.61.6 63.50.6 65.90.5 42.72.3 63.11.5 63.70.9 64.40.9\nSST -2(acc) 50.9 95.80.1 95.60.1 95.70.2 95.30.2 95.70.1 95.30.2 95.50.1\nMRPC(F1) 1.4 92.7 0.2 91.90.4 93.00.4 85.40.5 92.00.5 92.20.5 92.90.3\nSTS-B(Pear.) -6.2 91.40.1 90.70.2 90.50.1 83.02.8 90.50.4 90.30.7 90.90.1\nQQP(F1.) 6.4 83.5 0.1 83.50.0 84.40.0 77.20.4 84.30.0 83.60.1 84.40.0\nMNLI(acc) 34.2 88.6 0.2 88.00.2 89.00.1 77.92.5 88.90.1 88.00.2 88.90.1\nQNLI(acc) 50.6 93.7 0.3 93.40.3 94.20.1 86.20.5 94.20.1 93.20.3 94.40.1\nRTE(acc) 47.7 86.80.5 86.21.0 84.50.5 74.40.5 84.10.8 85.71.5 84.71.1\nA verage 23.7 87.40.4 86.60.4 87.10.2 77.71.2 86.60.4 86.50.6 87.00.3\nRoBER T aLARGE , full-data, with manual templates\nCoLA(Matt.) 2.2 66.91.1 64.20.5 65.51.0 37.820.8 64.71.3 64.80.7 64.91.0\nSST -2(acc) 83.6 96.30.2 96.10.1 96.20.2 95.70.2 95.80.1 95.90.1 95.80.2\nMRPC(F1) 61.9 92.2 0.4 92.70.6 92.70.2 84.20.5 91.80.2 92.20.4 92.00.4\nSTS-B(Pear.) -3.3 91.3 0.5 90.90.1 90.70.2 79.61.3 91.90.3 90.80.4 90.10.6\nQQP(F1) 49.7 83.6 0.1 83.60.0 84.60.1 77.00.7 84.30.0 83.70.0 84.40.2\nMNLI(acc) 50.9 88.6 0.1 87.70.1 88.70.1 80.20.2 88.70.1 88.00.1 88.90.1\nQNLI(acc) 50.8 93.6 0.1 93.10.2 93.80.1 86.60.4 93.80.1 93.00.1 93.80.1\nRTE(acc) 51.3 86.90.2 86.21.0 86.00.7 78.30.3 84.60.5 86.41.5 84.70.9\nA verage 43.4 87.40.3 86.80.3 87.30.3 77.43.0 86.90.3 86.90.4 86.80.4\nRoBER T aLARGE , 16-shot, without manual templates\nCoLA(Matt.) 4.6 19.69.6 15.117.0 17.711.4 3.50.6 21.411.5 20.819.6 21.513.4\nSST -2(acc) 50.9 92.70.4 92.70.6 93.10.6 74.90.6 91.70.8 92.20.5 91.60.7\nMRPC(F1) 1.4 78.24.4 69.81.6 81.20.0 6.24.1 74.67.1 69.36.5 77.45.4\nSTS-B(Pear.) -0.6 66.52.5 67.58.0 71.02.5 10.73.5 63.31.6 64.75.6 69.68.6\nQQP(F1) 6.4 55.95.8 55.16.8 54.64.2 52.41.4 58.37.2 55.14.8 58.56.1\nMNLI(acc) 34.2 58.14.5 64.63.4 62.74.1 35.30.6 61.43.9 61.45.1 61.03.8\nQNLI(acc) 50.6 60.23.0 69.71.9 59.81.7 52.81.0 60.24.9 60.94.0 61.67.0\nRTE(acc) 47.7 55.01.6 54.50.8 54.92.9 50.10.7 58.22.5 54.62.4 58.73.4\nA verage 24.4 60.84.0 61.15.0 61.93.4 35.71.6 61.24.9 59.96.1 62.56.0\nRoBER T aLARGE , 16-shot, with manual templates\nCoLA(Matt.) 2.2 10.515.0 4.65.0 9.210.2 1.41.7 10.24.2 5.92.5 5.95.5\nSST -2(acc) 83.6 93.10.3 92.90.1 92.10.1 90.90.6 91.90.4 92.00.4 92.20.6\nMRPC(F1) 61.9 77.21.4 74.54.9 81.20.0 72.14.4 76.81.3 76.12.4 81.20.0\nSTS-B(Pear.) -3.3 65.84.7 69.36.0 71.04.1 12.08.0 61.75.7 71.36.4 67.12.8\nQQP(F1) 49.7 66.60.5 67.80.5 66.34.1 53.41.0 66.91.9 68.61.2 67.12.9\nMNLI(acc) 50.9 68.01.4 69.43.3 68.90.4 53.22.5 67.11.8 67.12.0 68.10.3\nQNLI(acc) 50.8 69.51.1 70.23.4 68.12.4 59.40.5 69.92.5 72.53.9 70.42.3\nRTE(acc) 51.3 70.63.6 67.35.1 73.02.0 56.34.6 70.42.3 69.23.5 72.42.8\nA verage 43.4 65.23.5 64.53.5 66.22.9 49.82.9 64.42.5 65.32.8 65.62.2\n25\n5.3 The Power of Scale for Delta Tuning\nT able 6: Performance of T5BASE on GLUE datasets. W e report the average result of multiple random seeds on\nthe validation set.\nPrompt /enc-37/enc-37/enc-37/enc-37/enc-33/enc-33/enc-33/enc-33\nBitFit /enc-37/enc-37/enc-33/enc-33/enc-37/enc-37/enc-33/enc-33\nAdapter /enc-37/enc-33/enc-37/enc-33/enc-37/enc-33/enc-37/enc-33\nTunable parameters 0% 0.89% 0.09% 0.98% 0.003% 0.893% 0.093% 0.983%\nT5BASE , full-data, without manual templates\nCoLA(Matt.) - 59.20.2 58.71.7 58.40.8 34.018.6 51.24.6 36.921.7 57.80.3\nSST -2(acc) - 94.60.1 94.40.1 95.00.2 94.00.3 94.11.4 95.00.1 95.10.2\nMRPC(F1) - 89.10.6 90.10.4 90.80.3 84.81.2 89.20.7 88.50.7 88.80.6\nSTS-B(Pear.) - 86.70.3 86.60.1 86.90.3 83.11.8 86.10.4 85.81.4 85.80.4\nQQP(F1) - 86.70.2 88.30.1 87.70.3 83.41.0 86.90.4 88.00.4 88.10.2\nMNLI(acc) - 84.50.4 87.10.3 87.10.4 81.60.2 86.00.2 87.30.2 87.10.4\nQNLI(acc) - 89.80.1 91.60.1 91.30.1 87.80.3 89.10.3 91.80.3 91.70.2\nRTE(acc) - 75.31.0 77.51.3 80.00.8 64.30.9 71.54.7 72.96.6 71.52.0\nA verage - 83.20.3 84.30.5 84.70.4 76.63.1 81.81.6 80.83.9 83.20.5\nT5BASE , full-data, with manual templates\nCoLA(Matt.) - 57.41.8 59.51.2 59.10.4 36.318.2 39.122.8 58.10.6 48.27.4\nSST -2(acc) - 95.00.1 94.80.3 95.00.2 93.90.1 95.20.1 95.00.2 95.30.3\nMRPC(F1) - 90.90.3 90.70.6 91.40.4 81.33.5 87.80.5 89.40.3 89.20.4\nSTS-B(Pear.) - 87.10.2 87.40.4 87.70.2 83.41.0 86.60.1 84.73.4 86.80.4\nQQP(F1) - 87.40.1 88.30.1 88.20.1 83.71.1 87.40.1 88.30.1 88.30.2\nMNLI(acc) - 86.10.2 87.10.2 86.70.5 83.10.5 86.30.4 87.20.4 86.90.3\nQNLI(acc) - 91.90.3 92.80.4 92.60.2 89.30.7 92.20.1 92.90.1 92.70.3\nRTE(acc) - 81.90.7 84.00.8 83.21.5 66.82.2 80.00.2 81.81.3 80.10.6\nA verage - 84.70.5 85.60.5 85.50.4 77.23.4 81.83.0 84.70.8 83.41.2\nConclusion. T o sum up, the above experiments indicate that, different delta tuning methods have distinct\nfunctionalities for PLMs’ optimization, thus combining them is generally conducive to the downstream\nperformance. However, as shown in the above results, different PLMs may favor distinct delta tuning\ncombinations, and the optimal combination of delta tuning methods may vary a lot under different settings.\nThat being said, it would be interesting to explore the mechanisms behind the inductive biases brought\nby different delta tuning methods under different cases in the future. Besides, we also encourage future\nresearch explorations to systematically report the performance of their proposed delta methods on various\nPLM backbones under different settings thoroughly .\n5.3 The Power of Scale for Delta T uning\nRecently , Lester et al. (2021) found that with the scale of the backbone PLM growing, prompt tuning becomes\nmore and more competitive in performance, and would even achieve comparable performance than ﬁne-tuning\nfor a PLM with over 10 billion parameters. Besides, Su et al. (2021) indicated that the convergence speed of\nprompt tuning beneﬁts from the scaling law . In this section, we explore whether other delta tuning methods\nalso exhibit such power of scale. Speciﬁcally , we experiment on the task of MNLI ( Williams et al. , 2018),\nQNLI, and SST -2, and choose three PLMs ( T5SMALL, T5BASE, T5XXL) of increasing sizes, and evaluate the\nperformance of six representative delta tuning methods (adapter, LoRA, preﬁx-tuning, prompt tuning, last\nlayer tuning, and selective module tuning). Besides, we give the the percentages of the tuned parameters for\nvarious methods in every scale of the PLM as shown in T able 9. W e describe more training tails of this section\nin §A.3: S C A L E DE TA I L S.\nThe results are visualized in Figure 11. From Figure 11 (a-i), we could observe that, with the scale of the PLM\nbackbone growing, both the performance and the convergence of all delta tuning methods are signiﬁcantly\nimproved; (2) in addition, Figure\n11 (j-l) indicates that compared with other delta tuning methods, prompt\ntuning tends to perform extremely bad for small-scale PLMs ( T5SMALL and T5BASE). However, as found\nin §5.1: P E R F O R M A N C E, other delta tuning methods tend to perform comparable with ﬁne-tuning even for a\n26\n5.4 T ask-level Transferability Evaluation\nT able 7: The experiments of generalization gap for RoBER T aLARGE on GLUE datasets. W e report the average\nresult (train performance - dev performance) of multiple random seeds.\nPrompt /enc-37/enc-37/enc-37/enc-33/enc-33/enc-33/enc-33\nFTBitFit /enc-37/enc-33/enc-33/enc-37/enc-37/enc-33/enc-33\nAdapter /enc-33/enc-37/enc-33/enc-37/enc-33/enc-37/enc-33\nTunable parameters 1.75% 0.09% 1.84% 0.003% 1.76% 0.09% 1.85% 100%\nRoBER T aBASE , full-data, without manual templates\nCoLA 25.41.5 13.02.8 28.42.4 12.13.8 29.56.8 16.27.6 18.01.8 28.22.4\nSST -2 3.01.3 1.70.5 0.90.3 1.10.5 3.60.5 1.90.6 3.51.1 3.30.9\nMRPC 7.10.3 5.72.2 7.00.4 1.01.1 8.00.5 4.50.5 7.10.2 6.30.7\nSTS-B 5.10.0 4.90.6 7.00.8 6.71.6 6.50.3 5.60.6 6.50.4 7.50.2\nQQP 0.60.1 0.70.1 0.80.0 0.10.0 0.80.0 0.70.1 0.80.1 1.90.2\nMNLI 0.60.1 0.50.1 0.60.2 0.60.4 0.50.1 0.50.2 0.50.1 0.60.0\nQNLI 0.90.1 0.70.1 0.50.2 1.60.1 0.50.2 0.80.3 0.50.2 1.60.0\nRTE 13.10.6 13.20.7 14.90.3 9.81.6 15.90.8 12.62.3 15.11.3 12.91.3\nA verage 7.00.5 5.10.9 7.50.6 4.11.1 8.21.2 5.31.5 6.50.7 7.80.7\nRoBER T aBASE , full-data, with manual templates\nCoLA 20.95.0 25.44.4 24.37.5 11.41.2 29.13.2 29.66.4 24.610.3 30.42.3\nSST -2 3.30.1 1.40.6 1.30.7 1.00.3 2.60.7 2.50.8 3.80.4 4.00.1\nMRPC 6.22.5 6.50.6 6.40.3 3.82.5 8.20.2 7.20.3 6.71.4 7.20.5\nSTS-B 5.81.4 4.90.4 6.71.2 10.20.6 6.90.5 5.50.7 6.11.5 7.50.2\nQQP 0.70.1 0.60.1 0.80.0 0.20.1 0.80.2 0.70.1 0.80.0 2.00.1\nMNLI 0.80.1 0.30.1 0.40.1 0.70.2 0.60.1 0.70.1 0.60.1 0.80.2\nQNLI 0.80.1 0.40.2 0.10.0 1.40.1 0.10.0 0.50.2 0.00.0 2.00.1\nRTE 13.10.2 11.70.8 13.90.7 10.15.2 15.00.4 13.31.3 15.10.9 12.71.1\nA verage 6.41.2 6.40.9 6.71.3 4.81.3 7.90.7 7.51.2 7.21.8 8.30.6\nsmall-scale PLM ( T5BASE); (3) based on existing results, in Figure 11 (m-o) and (p-r), we further design two\ndelta tuning methods: last layer tuning and selective module tuning. For last layer tuning, we optimize the last\nlayer in T5 encoder; for selective module tuning, we randomly choose some modules (e.g., the feed-forward\nlayer, query / key / value matrix in the attention layer, or a layer norm) in the T5 model to be tunable. Both\nmethods show promising results especially when the scale of the PLM is extremely large, with selective\nmodule tuning slightly better than last layer tuning. These results suggest that conﬁning the optimization\nwithin a speciﬁc layer may not be a good strategy (e.g., the case of prompt tuning and last layer tuning). On\nthe other hand, randomly choosing modules across different layers could achieve excellent performance when\nthe scale of PLMs grows extremely large.\nIn general, the above results imply that, the power of scale may be a common phenomenon for delta tuning.\nW e hypothesize the existence of such a phenomenon is because, larger PLMs generally have smaller intrinsic\ndimensionalities ( Aghajanyan et al. , 2021), therefore, merely tuning minimal parameters could obtain a\nstrong enough representation ability to achieve non-trivial performance in downstream tasks; besides, the\nover-parameterization and large-scale pre-training may make PLMs more unlikely to get stuck in a local\noptimum during downstream optimization, and thus the convergence is accelerated.\n5.4 T ask-level T ransferability Evaluation\nRecently , Su et al. (2021) and V u et al. (2021) demonstrate the cross-task transferability of prompt tuning.\nT o verify whether cross-task transferability also exists in various delta tuning methods, we investigate four\ndelta tuning methods (prompt tuning, preﬁx-tuning, adapter, and LoRA) and 12 tasks of 5 different types\n(sentiment analysis, natural language inference, paraphrase identiﬁcation, question answering, summarization)\nby transferring the trained delta parameters to the unseen target tasks. More training and dataset details are left\nin §A.4: T R A N S F E R A B I L I T Y DE TA I L S.\n27\n6. Applications\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nAP+BF+PT, without template\nAP\nBF\nPT\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nAP+PT+BF, without template\nAP\nPT\nBF\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nBF+AP+PT, without template\nBF\nAP\nPT\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nBF+PT+AP, without template\nBF\nPT\nAP\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nPT+BF+AP, without template\nPT\nBF\nAP\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nPT+AP+BF, without template\nPT\nAP\nBF\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nAP+BF+PT, with template\nAP\nBF\nPT\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nAP+PT+BF, with template\nAP\nPT\nBF\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nBF+AP+PT, with template\nBF\nAP\nPT\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nBF+PT+AP, with template\nBF\nPT\nAP\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nPT+BF+AP, with template\nPT\nBF\nAP\n0 2000 4000 6000 8000 10000 12000 14000 16000 18000\nsteps\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97ACC\nPT+AP+BF, with template\nPT\nAP\nBF\nFigure 10: The performance of RoBER T aLARGE when different delta tuning methods (adapter ( AP), BitFit\n(BF) and prompt tuning ( PT)) are applied sequentially . The experiments are conducted on SST -2 ( Socher\net al. , 2013).\nIn experiments, we report their relative performance (zero-shot transferring performance / original perfor-\nmance). The results are shown in Figure 12, from which we can observe that: (1) for the tasks belonging to the\nsame category , transferring tuned parameters among them generally performs well; (2) for the tasks of different\ntypes, transferring delta parameters among them generally achieves poor performance; (3) interestingly ,\nwe ﬁnd that transferring tuned parameters from the text generation tasks such as question answering and\nsummarization can achieve non-trivial performance on sentiment analysis , indicating that text generation tasks\nmight be a more complex task that includes the knowledge required to solve the sentiment analysis tasks.\nThese exciting results verify some common subspace among various tasks introduced in §4.1: O P T I M I Z AT I O N,\nand demonstrate that it is promising to utilize trained delta parameters for similar tasks through knowledge\ntransfer.\n6 Applications\nDelta tuning has been successfully applied to a variety of application scenarios. In this section, we brieﬂy\nintroduce several real-world applications, emphasizing on different advantages of delta tuning.\n28\n6. Applications\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune)\n(a) Adapter (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (b) Adapter (QNLI).\n0 1000 2000 3000 4000 5000 6000 7000 8000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (c) Adapter (SST -2).\n0 2500 5000 7500 10000 12500 15000 17500 20000\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune)\n(d) LoRA (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (e) LoRA (QNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (f) LoRA (SST -2).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune)\n(g) Preﬁx-tuning (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (h) Preﬁx-tuning (QNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (i) Preﬁx-tuning (SST -2).\n0 2500 5000 7500 10000 12500 15000 17500\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBase\nSmall\nXXL (Fine-tune)\n(j) Prompt Tuning (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (k) Prompt Tuning (QNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (l) Prompt Tuning (SST -2).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune)\n(m) Last Layer Tuning (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (n) Last Layer Tuning (QNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (o) Last Layer Tuning (SST -2).\n29\n6. Applications\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.24\n0.48\n0.72\n0.96\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune)\n(p) Selective Module Tuning (MNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (q) Selective Module Tuning (QNLI).\n0 2000 4000 6000 8000 10000\nsteps\n0.0\n0.25\n0.5\n0.75\n1.0\nACC\nXXL\nBASE\nSMALL\nXXL (Fine-tune) (r) Selective Module Tuning (SST -2).\nFigure 11: W e perform all delta tuning methods conditioned on different scales of T5: T5SMALL(— ),\nT5BASE(— ), and T5XXL(— ). From the above ﬁgures, we can observe that with the scale of T5 increas-\ning, all delta tuning methods could converge faster and achieve better performance on MNLI, QNLI, and\nSST -2.\nFast T raining and Shareable Checkpoints. Transformer-based models, although inherently parallelizable,\nare very slow to train due to their huge sizes, especially under the current era when ever-larger PLMs constantly\nemerge. Although delta tuning may converge slower than the traditional ﬁne-tuning, the computations of\nthe tunable parameters during backward propagation are signiﬁcantly reduced, which conduces to speeding\nup training, as visualized in Figure 13. For instance, Rücklé et al. (2021) show that using adapters for\ndownstream tuning could reduce training time to 40% while maintaining comparable performance than ﬁne-\ntuning; Mahabadi et al. (2021a) also indicate that a series of delta tuning methods signiﬁcantly reduce both\nthe training time for each epoch and the peak GPU memory , which is of paramount importance for practical\napplications. Another observation is that the structures of delta tuning methods could have considerable impact\non the time of a single forward or backward process. Since AP injects additional neural modules to each layer\nof the Transformer model, the path of data ﬂow has indeed become longer and further lead to inference latency .\nAnd such latency could be relatively reduced as the model scales.\nDue to the lightweight nature, the tuned delta parameters could also save the storage space, making it easier to\nshare the trained delta checkpoints among practitioners. With the help of delta tuning, researchers could easily\nscale up experiments to extremely large models containing even billions of parameters. Recently , researchers\nhave been spending huge efforts to create a community of shareable delta tuning checkpoints, such as (1)\nAdapterHub9 (Pfeiffer et al. , 2020a), an implementation of different adapter variants and a host for adapter\ncheckpoints, and (2) OpenDelta 10 , an emerging plug-and-play library that is compatible with almost all PLMs\nbased on PyT orch 11 .\nMulti-task Learning. Building a general-purpose AI system has always been the goal of researchers.\nRecently , extremely large PLMs, such as GPT -3 ( Brown et al. , 2020), have demonstrated the spectacular\nability in ﬁtting different data distributions simultaneously and promoting the downstream performance\nof various tasks. Multi-task learning has thus received a growing amount of attention under the era of\nlarge-scale pre-training. As a parameter-efﬁcient substitution of full-model ﬁne-tuning, delta tuning exhibits\nexcellent ability for multi-task learning and in the meantime, maintains a relatively low additional storage.\nSuccessful applications include (1) multi-lingual learning:\nPfeiffer et al. (2020b) propose to learn a series\nof invertible adapters between embeddings in the source and target languages to mitigate lexical differences\nacross languages. The invertible adapter could well support knowledge transfer among multiple subtasks\nand maintain a low parameter budget, and (2) question answering: Friedman et al. (2021) prove that using a\nset of adapters that specialize in different QA formats performs favorably to a single language model that is\nﬁne-tuned on a mixture of QA formats. In addition, the simple average of specialized adapters also exhibits\nstrong zero-shot transferring ability . Recently , Liu et al. (2021a); Sun et al. (2021); Karimi Mahabadi et al.\n(2021) also demonstrate that delta tuning could not only unify the tasks belonging to the same ontology , but\nalso the tasks with substantially different formats so that different tasks could beneﬁt from each other through\nknowledge transfer. Expanding from this idea, delta tuning can well support adaptations of large PLMs for\nmulti-lingual and multi-domain scenarios.\n9 https://adapterhub.ml\n10 https://github.com/thunlp/OpenDelta\n11 https://pytorch.org\n30\n6. Applications\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nTarget Task\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nSource Task\n100 103 104 66 19 32 43 88 0 0 3 0\n90 100 75 60 39 46 74 42 0 0 1 0\n92 97 100 60 30 27 73 42 0 0 1 0\n55 37 39 100 82 43 74 42 0 0 8 0\n65 59 63 61 100 46 74 42 0 0 22 2\n55 51 43 77 54 100 74 42 0 0 0 0\n54 36 39 66 19 32 100 77 0 0 0 0\n52 51 53 66 19 32 75 100 0 0 13 3\n54 37 39 66 19 32 74 42 100 86 37 21\n57 38 39 66 19 32 74 42 90 100 18 16\n70 72 75 66 19 32 74 42 0 0 100 31\n61 75 55 66 19 32 74 42 0 0 67 100\n(a) Prompt Tuning\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nTarget Task\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nSource Task\n99 97 96 43 32 58 70 41 0 0 0 0\n85 100 83 41 29 34 72 40 0 0 0 0\n100 88 100 39 35 28 51 68 0 0 0 0\n69 58 41 100 50 32 72 40 0 0 0 0\n72 58 56 44 100 41 72 41 0 0 30 12\n55 36 41 48 47 100 72 35 0 0 1 2\n56 36 48 43 18 31 100 79 0 0 0 0\n56 36 48 43 27 21 63 100 0 0 0 0\n66 74 71 43 18 21 68 39 100 87 28 21\n64 74 68 43 18 31 72 41 104 100 25 18\n67 87 78 43 18 31 72 40 0 0 100 22\n72 69 71 43 18 31 72 40 0 0 55 100 (b) Preﬁx-tuning\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nTarget Task\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nSource Task\n100 96 101 42 17 33 73 38 0 0 0 0\n98 100 99 43 17 30 73 38 0 0 0 0\n99 96 100 42 18 46 71 38 0 0 0 0\n55 34 57 100 65 80 73 38 0 0 0 0\n68 64 68 65 100 84 73 38 0 0 0 0\n54 38 57 59 55 100 73 38 0 0 0 0\n55 52 61 43 17 30 100 73 0 0 0 0\n76 69 73 43 17 30 83 100 0 0 0 0\n70 66 75 43 17 30 73 38 100 106 44 24\n70 79 77 43 17 30 73 38 95 100 41 24\n86 91 83 43 17 30 73 38 0 0 100 36\n69 92 62 43 17 30 73 38 0 0 53 100\n(c) Adapter\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nTarget Task\nSST-2\nAmazon_Polarity\nRotten Tomatoes\nMNLI\nSICK\nSciTail\nQQP\nMRPC\nMathQA\nAQUA-RAT\nXSum\nSAMSum\nSource Task\n100 97 100 43 17 37 72 39 0 0 0 0\n97 100 93 43 17 30 72 39 0 0 0 0\n100 97 98 43 17 30 72 39 0 0 0 0\n56 38 56 100 48 78 72 39 0 0 0 0\n75 64 74 67 100 79 72 39 0 0 13 0\n56 44 60 60 54 100 72 39 0 0 0 0\n54 35 56 43 17 30 100 72 0 0 0 0\n54 35 56 43 17 30 76 100 0 0 0 0\n63 84 59 43 17 30 72 39 100 102 39 19\n59 87 72 43 17 30 72 39 79 100 42 20\n73 84 70 43 17 30 72 39 0 0 100 27\n61 91 58 43 17 30 72 39 0 0 55 100 (d) LoRA\nFigure 12: Zero-shot transferring performance of four delta tuning methods using T5BASE. W e report relative\nperformance (zero-shot transferring performance / original performance) (%) on the target tasks (columns)\nwhen delta parameters are transferred from the source tasks (rows). Colors of the task names indicate the task\ntypes. Blue: sentiment analysis, Green: natural language inference, Orange: paraphrase identiﬁcation, Brown:\nquestion answering, and Purple: summarization.\nCatastrophic Forgetting Mitigation. The language abilities acquired during pre-training are stored in\nparameters. As a consequence, updating all parameters in PLMs without regularization may lead to catastrophic\nforgetting when PLMs are sequentially trained across a suite of tasks ( Jin et al. , 2021; Qin et al. , 2021a,c).\nSince delta tuning only tunes minimal parameters, it could be a potential solution for mitigating the problem\nof catastrophic forgetting. For instance, MultiEURLEX ( Chalkidis et al. , 2021) introduce delta tuning into\nmultilingual transfer learning, and demonstrate that using delta tuning methods rather than full-parameter\nﬁne-tuning boosts the performance of zero-shot transfer learning between the source language and the target\nlanguage; Jin et al. (2021) propose to introduce adapters into PLMs and maintain the original parameters ﬁxed,\nso that PLMs could be trained in a lifelong manner for emerging data.\n31\n7. Conclusion\n16 32 64 128 256 480\nLength of Inputs\n27.5\n41.2\n54.9\n68.6\n82.3\nTime(ms)\nForward Time\nPT\nBF / FT\nAP\n16 32 64 128 256 480\nLength of Inputs\n21.3\n46.1\n70.9\n95.7\n120.5\nTime(ms)\nBackward Time\nPT\nBF\nFT\nAP\nFigure 13: Time consumption for ﬁne-tuning (FT) and different delta tuning methods, including BitFit (BF),\nadapter (AP) and prompt tuning (PT). W e report the results with different input length.\nLanguage Model as Services and In-batch Parallel Computing. From the practical perspective, extremely\nlarge PLMs are generally released as services ( Brown et al. , 2020; Nakano et al. , 2021; Sun et al. , 2022), that\nis, users use the model by interacting with the released APIs rather than editing the source code. Considering\nthe unaffordable communication costs between users and the service provider, delta tuning is apparently a\nmore competitive choice over the traditional ﬁne-tuning due to its lightweight nature. On one hand, the service\nprovider could support training downstream tasks required by multiple users while consuming much fewer\ncomputations and storage space. In addition, considering that several delta tuning algorithms, such as prompt\ntuning ( Lester et al. , 2021) and preﬁx-tuning ( Li & Liang , 2021) are inherently parallelizable, such a service\ncould become more practical since delta tuning could well support in-batch parallel computing by allowing\ninstances from multiple users to be trained / evaluated in the same batch. Recent works ( He et al. , 2022) also\nshow that most of the delta tuning methods, if not parallelizable inherently , could be modiﬁed to support\nparallel computing, e.g., parallel adapter ( He et al. , 2022). On the other hand, when the gradients of the central\nPLM are not available to users, delta tuning still exhibits extraordinary talents in optimizing PLMs through\nderivative-free algorithms by only accessing the model inference APIs. Recently , Sun et al. (2022); Diao et al.\n(2022) pioneered to propose black-box tuning and show that their method could not only outperform manual\nprompts and GPT -3’s in-context learning, but also surpass the gradient-based counterparts.\n7 Conclusion\nThis paper focuses on parameter-efﬁcient methods, i.e., delta tuning, for pre-trained language models. W e ﬁrst\ndescribe the problem and provide a categorization to systematically survey the development of delta tuning.\nCaptivated by the empirical evidence, we propose two frameworks to theoretically discuss delta tuning from\nthe optimization and the optimal control perspectives. Our discussion not only sheds light on the theoretical\nreferences of a novel design for delta tuning methods, but also implies that we could grasp the essential\nmechanisms of PLMs through deep analysis. Empirically , we conduct extensive experiments across 100+\nNLP tasks to fairly evaluate and explore the combinatorial property , inﬂuence of scale, and transferability\nfor delta tuning. Furthermore, we discuss the value of the applications of this paradigm. In summary , delta\ntuning exhibits signiﬁcant potential to stimulate extremely large PLMs, and we hope that the paradigm could\nbe further theoretically studied and empirically practiced.\nBroader Impacts\nDelta tuning focuses on the efﬁcient adaptation of pre-trained language models, which has both positive\napplications and potential harms for society . On the bright side, PLMs have exhibited unprecedented capability\nof natural language understanding (represented by BERT ( Devlin et al. , 2019)) and generation (represented\nby GPT -3 ( Brown et al. , 2020)), which empowers numerous real-world applications such as search engines,\nquestion-answering systems, intelligent writing systems, information extraction systems, and code completion,\netc. Recent research also shows that such large-scale PLMs could mimic the behavior to use search engines to\nanswer difﬁcult questions ( Nakano et al. , 2021). However, the risk is often hidden in the great successes, on\nthe other hand, PLMs may present biases in terms of gender, race, religion, etc, and even directly produce\n32\n7. Conclusion\nmachine-written language with attacks, profanities, and insults ( W eidinger et al. , 2021). This is because PLMs\nare pre-trained with large-scale realistic corpora, and these pre-trained data are likely to contain inherent bias.\nLanguage is a carrier of human views, so the prejudice and discrimination that exist in human society can\neasily be mapped onto language, and how to alleviate such challenges of fairness is a question that is well\nworth further study . Efforts could be made in two ways, ﬁrst, directly by normalizing the training corpus to\nremove as many potential biases as possible, and second, by modifying model representations or outputs to\nreduce the risks. Outside of research, more comprehensive and improved treaties and norms for the use and\nmodiﬁcation of language models should be established by the community .\nSo far, there is no clear evidence that delta tuning mitigates or exacerbates the potential hazards of PLMs. It is\nlikely that the delta tuning methods will still inherit the potential risks of the base language model. But the\ndelta tuning methods seem to have considerable potential for correcting model bias. Under this circumstance,\ndelta tuning is not only applied to efﬁciently adapt PLMs to downstream tasks but also could be utilized to\nspeciﬁcally process risky information inside the models with a small number of parameters changed. In fact, it\nhas been shown that it is possible to modify the factual errors made by the model in a computationally efﬁcient\nway ( Mitchell et al. , 2021), signaling that the fairness issue can be potentially addressed through delta tuning.\nAt the same time, we have to worry that such a strategy can also be used to further contaminate the language\nmodels to produce undesirable predictions. Here, we strongly encourage the community to conduct further\nresearch to comprehensively explore the various effects that delta tuning may have on PLMs.\nWhen it comes to environmental issues, given that pre-training, ﬁne-tuning, and storage of PLMs is a resource-\nintensive process, delta tuning attempts to minimize this impact from the outset. After probing the memory\n(Figure 9) and time consumption (Figure 13) of delta tuning in our work, we ﬁnd that such methods could\nsubstantially reduce the computational cost. However, in the convergence analysis (Figure 6, 7, 8) we conclude\nthat delta tuning methods tend to need more time to converge, although this phenomenon becomes insigniﬁcant\nas the model scales. In order to reduce unnecessary carbon emissions, we will open source all tools, code and\ncheckpoints used in the experiment.\n33\nREFERENCES\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of\nlanguage model ﬁne-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long\nP apers), pp. 7319–7328, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nacl-long.568. URL https://aclanthology.org/2021.acl-long.568.\nTiago A. Almeida, José María G. Hidalgo, and Akebo Y amakami. Contributions to the study of sms spam\nﬁltering: New collection and results. In Proceedings of the 11th ACM Symposium on Document Engineering ,\nDocEng ’11, pp. 259–262, New Y ork, NY , USA, 2011. Association for Computing Machinery . ISBN\n9781450308632. doi: 10.1145/2034691.2034742. URL\nhttps://doi.org/10.1145/2034691.\n2034742.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Y ejin Choi, and Hannaneh Hajishirzi.\nMathQA: T owards interpretable math word problem solving with operation-based formalisms. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language T echnologies, V olume 1 (Long and Short P apers) , pp. 2357–2367, Min-\nneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL\nhttps://aclanthology.org/N19-1245.\nKiam Heong Ang, Gregory Chong, and Y un Li. Pid control system analysis, design, and technology . IEEE\ntransactions on control systems technology , 13(4):559–576, 2005.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https:\n//arxiv.org/abs/1607.06450.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and\nIdan Szpektor. The second pascal recognising textual entailment challenge. In Proceedings of\nthe second P ASCAL challenges workshop on recognising textual entailment , volume 6, pp. 6–4.\nV enice, 2006. URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.\n60.8552&rep=rep1&type=pdf.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. T weetEval: Uni-\nﬁed benchmark and comparative evaluation for tweet classiﬁcation. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pp. 1644–1650, Online, 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.148. URL https://aclanthology.org/2020.\nfindings-emnlp.148.\nY oshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances\nin Neural Information Processing Systems , 13, 2000. URL https://proceedings.neurips.cc/\npaper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-\nanswer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing ,\npp. 1533–1544, Seattle, W ashington, USA, 2013. Association for Computational Linguistics. URL https:\n//aclanthology.org/D13-1160.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021. URL https://arxiv.org/abs/2108.\n07258.\nMichael Boratko, Xiang Li, Tim O’Gorman, Rajarshi Das, Dan Le, and Andrew McCallum. ProtoQA:\nA question answering dataset for prototypical common-sense reasoning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 1122–1136, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.85. URL https:\n//aclanthology.org/2020.emnlp-main.85.\nJan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge, and Dipanjan Das. Learning to split and rephrase\nfrom Wikipedia edit history . In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 732–737, Brussels, Belgium, 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1080. URL https://aclanthology.org/D18-1080.\n34\nREFERENCES\nStephen P Boyd and Craig H Barratt. Linear controller design: limits of performance , volume 7. Citeseer,\n1991.\nT om B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T om Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray , Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual , 2020. URL\nhttps://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic\ntextual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International\nW orkshop on Semantic Evaluation (SemEval-2017) , pp. 1–14, V ancouver, Canada, August 2017. Association\nfor Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://aclanthology.org/\nS17-2001.\nIlias Chalkidis, Manos Fergadiotis, and Ion Androutsopoulos. Multieurlex–a multi-lingual and multi-label\nlegal document classiﬁcation dataset for zero-shot cross-lingual transfer. ArXiv preprint , abs/2109.00904,\n2021. URL https://arxiv.org/abs/2109.00904.\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana Joshi, and Puneet Agrawal. SemEval-2019 task 3:\nEmoContext contextual emotion detection in text. In Proceedings of the 13th International W orkshop on\nSemantic Evaluation , pp. 39–48, Minneapolis, Minnesota, USA, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/S19-2005. URL https://aclanthology.org/S19-2005.\nZhuotong Chen, Qianxiao Li, and Zheng Zhang. T owards robust neural networks via close-loop control.\nIn International Conference on Learning Representations , 2021. URL https://openreview.net/\nforum?id=2AL06y9cDE-.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges W orkshop , pp. 177–190. Springer, 2005. URL https:\n//link.springer.com/chapter/10.1007/11736790_9.\nThomas Davidson, Dana W armsley , Michael Macy , and Ingmar W eber. Automated hate speech detection and\nthe problem of offensive language. ArXiv preprint , abs/1703.04009, 2017. URL https://arxiv.org/\nabs/1703.04009.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BER T: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language T echnologies, V olume 1\n(Long and Short P apers) , pp. 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\nShizhe Diao, Xuechun Li, Y ong Lin, Zhichao Huang, and T ong Zhang. Black-box prompt learning for\npre-trained language models. arXiv preprint arXiv:2201.08531 , 2022. URL https://arxiv.org/\nabs/2201.08531.\nT . Diggelmann, Jordan L. Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold.\nClimate-fever: A dataset for veriﬁcation of real-world climate claims. ArXiv preprint , abs/2012.00614,\n2020. URL https://arxiv.org/abs/2012.00614.\nNing Ding, Shengding Hu, W eilin Zhao, Y ulin Chen, Zhiyuan Liu, Hai-T ao Zheng, and Maosong Sun.\nOpenprompt: An open-source framework for prompt-learning. ArXiv preprint , abs/2111.01998, 2021. URL\nhttps://arxiv.org/abs/2111.01998.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of IWP W orkshop , 2005. URL https://aclanthology.org/I05-5002.\n35\nREFERENCES\nMatthew Dunn, Levent Sagun, Mike Higgins, V . U. Güney , V olkan Cirik, and Kyunghyun Cho. Searchqa:\nA new q&a dataset augmented with context from a search engine. ArXiv preprint , abs/1704.05179, 2017.\nURL https://arxiv.org/abs/1704.05179.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function\napproximation in reinforcement learning. Neural Networks , 107:3–11, 2018. URL https://www.\nsciencedirect.com/science/article/pii/S0893608017302976.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev . Multi-news: A large-scale multi-\ndocument summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pp. 1074–1084, Florence, Italy , 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/P19-1102. URL https://aclanthology.org/\nP19-1102.\nAngela Fan, Y acine Jernite, Ethan Perez, David Grangier, Jason W eston, and Michael Auli. ELI5: Long\nform question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 3558–3567, Florence, Italy , 2019. Association for Computational Linguistics. doi: 10.\n18653/v1/P19-1346. URL https://aclanthology.org/P19-1346.\nDan Friedman, Ben Dodge, and Danqi Chen. Single-dataset experts for multi-dataset question answering.\nArXiv preprint , abs/2109.13880, 2021. URL https://arxiv.org/abs/2109.13880.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (V olume 1: Long P apers) , pp. 3816–3830,\nOnline, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295. URL\nhttps://aclanthology.org/2021.acl-long.295.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third P ASCAL recogniz-\ning textual entailment challenge. In Proceedings of the ACL-P ASCAL W orkshop on T extual Entail-\nment and P araphrasing , pp. 1–9, Prague, 2007. Association for Computational Linguistics. URL\nhttps://aclanthology.org/W07-1401.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander W awer. SAMSum corpus: A human-annotated\ndialogue dataset for abstractive summarization. In Proceedings of the 2nd W orkshop on New Frontiers\nin Summarization , pp. 70–79, Hong Kong, China, 2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference\non Lexical and Computational Semantics – V olume 1: Proceedings of the main conference and the shared\ntask, and V olume 2: Proceedings of the Sixth International W orkshop on Semantic Evaluation (SemEval\n2012), pp. 394–398, Montréal, Canada, 2012. Association for Computational Linguistics. URL https:\n//aclanthology.org/S12-1052.\nEdward Grefenstette, Phil Blunsom, et al. A convolutional neural network for modelling sentences. In ACL,\n2014. URL https://arxiv.org/abs/1404.2188.\nY uxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning.\nArXiv preprint , abs/2109.04332, 2021. URL https://arxiv.org/abs/2109.04332.\nDemi Guo, Alexander Rush, and Y oon Kim. Parameter-efﬁcient transfer learning with diff pruning. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (V olume 1: Long P apers) , pp. 4884–4896,\nOnline, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.378. URL\nhttps://aclanthology.org/2021.acl-long.378.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and\nLuca T oldo. Development of a benchmark corpus to support the automatic extraction of drug-related adverse\neffects from medical case reports. Journal of Biomedical Informatics , 45(5):885–892, 2012. ISSN 1532-\n0464. doi: https://doi.org/10.1016/j.jbi.2012.04.008. URL https://www.sciencedirect.com/\nscience/article/pii/S1532046412000615. T ext Mining and Natural Language Processing in\nPharmacogenomics.\n36\nREFERENCES\nW enjuan Han, Bo Pang, and Ying Nian Wu. Robust transfer learning with pretrained language models through\nadapters. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Processing (V olume 2: Short P apers) , pp.\n854–861, Online, 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.108.\nURL https://aclanthology.org/2021.acl-short.108.\nXu Han, Zhengyan Zhang, Ning Ding, Y uxian Gu, Xiao Liu, Y uqi Huo, Jiezhong Qiu, Liang Zhang, W entao\nHan, Minlie Huang, Qin Jin, Y anyan Lan, Y ang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song,\nJie T ang, Ji-Rong W en, Jinhui Y uan, W ayne Xin Zhao, and Jun Zhu. Pre-trained models: Past, present\nand future. AI Open , 2021b. ISSN 2666-6510. doi: https://doi.org/10.1016/j.aiopen.2021.08.002. URL\nhttps://www.sciencedirect.com/science/article/pii/S2666651021000231.\nJunxian He, Chunting Zhou, Xuezhe Ma, T aylor Berg-Kirkpatrick, and Graham Neubig. T owards a uniﬁed\nview of parameter-efﬁcient transfer learning. In International Conference on Learning Representations ,\n2022. URL https://openreview.net/forum?id=0RDcd5Axok.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn 2016 IEEE Conference on Computer V ision and P attern Recognition, CVPR 2016, Las V egas, NV ,\nUSA, June 27-30, 2016 , pp. 770–778. IEEE Computer Society , 2016. doi: 10.1109/CVPR.2016.90. URL\nhttps://doi.org/10.1109/CVPR.2016.90.\nRuidan He, Linlin Liu, Hai Y e, Qingyu T an, Bosheng Ding, Liying Cheng, Jiawei Low , Lidong Bing,\nand Luo Si. On the effectiveness of adapter-based tuning for pretrained language model adaptation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (V olume 1: Long P apers) , pp. 2208–2222,\nOnline, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.172. URL\nhttps://aclanthology.org/2021.acl-long.172.\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory . Neural computation , 9(8):1735–1780,\n1997. URL https://ieeexplore.ieee.org/abstract/document/6795963/.\nJohannes Hoffart, Mohamed Amir Y osef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol,\nBilyana T aneva, Stefan Thater, and Gerhard W eikum. Robust disambiguation of named entities in text.\nIn Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pp. 782–\n792, Edinburgh, Scotland, UK., 2011. Association for Computational Linguistics. URL https://\naclanthology.org/D11-1072.\nNeil Houlsby , Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly . Parameter-efﬁcient transfer learning for NLP. In Kamalika\nChaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine\nLearning Research , pp. 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/\nhoulsby19a.html.\nEdward J Hu, Y elong Shen, Phillip W allis, Zeyuan Allen-Zhu, Y uanzhi Li, Shean W ang, Lu W ang, and W eizhu\nChen. Lora: Low-rank adaptation of large language models. ArXiv preprint , abs/2106.09685, 2021a. URL\nhttps://arxiv.org/abs/2106.09685.\nShengding Hu, NNN, Huadong W ang, Zhiyuan Liu, Juan-Zi Li, and Maosong Sun. Knowledgeable prompt-\ntuning: Incorporating knowledge into prompt verbalizer for text classiﬁcation. ArXiv, abs/2108.02035,\n2021b. URL https://arxiv.org/abs/2108.02035.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Y ejin Choi. Cosmos QA: Machine reading compre-\nhension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pp. 2391–2401, Hong Kong, China, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1243. URL https://aclanthology.org/D19-1243.\nKelvin Jiang, Dekun Wu, and Hui Jiang. FreebaseQA: A new factoid QA data set matching trivia-style\nquestion-answer pairs with Freebase. In Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language T echnologies, V olume 1 (Long and\nShort P apers), pp. 318–323, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1028. URL https://aclanthology.org/N19-1028.\n37\nREFERENCES\nXisen Jin, Dejiao Zhang, Henghui Zhu, W ei Xiao, Shang-W en Li, Xiaokai W ei, Andrew Arnold, and Xiang\nRen. Lifelong pretraining: Continually adapting language models to emerging corpora. arXiv preprint\narXiv:2110.08534, 2021. URL https://arxiv.org/abs/2110.08534.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efﬁcient\nmulti-task ﬁne-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing (V olume 1: Long P apers) , pp. 565–576, Online, 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.acl-long.47. URL https://aclanthology.org/\n2021.acl-long.47.\nTushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question\nanswering. In Sheila A. McIlraith and Kilian Q. W einberger (eds.), Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial\nIntelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, F ebruary 2-7, 2018 , pp. 5189–5197. AAAI Press, 2018a. URL\nhttps://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17368.\nTushar Khot, Ashish Sabharwal, and Peter Clark. SciT ail: A textual entailment dataset from science question\nanswering. In AAAI, 2018b. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/\npaper/view/17368/0.\nTushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. QASC: A dataset for question\nanswering via sentence composition. In The Thirty-F ourth AAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The\nT enth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New Y ork, NY , USA,\nF ebruary 7-12, 2020 , pp. 8082–8090. AAAI Press, 2020. URL https://aaai.org/ojs/index.\nphp/AAAI/article/view/6319.\nY oon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pp. 1746–1751, Doha, Qatar, October 2014.\nAssociation for Computational Linguistics. doi: 10.3115/v1/D14-1181. URL https://aclanthology.\norg/D14-1181.\nDiederik P . Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Y oshua Bengio and Y ann\nLeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings , 2015. URL http://arxiv.org/abs/1412.6980.\nRichard E Kopp. Pontryagin maximum principle. In Mathematics in Science and Engineering , volume 5, pp.\n255–279. Elsevier, 1962. doi: 10.1016/S0076-5392(08)62095-0.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Y ang, and Eduard Hovy . RACE: Large-scale ReAding\ncomprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing , pp. 785–794, Copenhagen, Denmark, 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082.\nRémi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application\nto the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 1203–1213, Austin, T exas, 2016. Association for Computational Linguistics. doi:\n10.18653/v1/D16-1128. URL https://aclanthology.org/D16-1128.\nY ann LeCun, Y oshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\nJaejun Lee, Raphael T ang, and Jimmy Lin. What would elsa do? freezing layers during transformer ﬁne-tuning.\nArXiv preprint , abs/1911.03090, 2019. URL https://arxiv.org/abs/1911.03090.\nMoshe Leshno, Vladimir Y a Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with\na nonpolynomial activation function can approximate any function. Neural networks , 6(6):861–867, 1993.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning.\nArXiv preprint , abs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691.\n38\nREFERENCES\nHector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings\nof the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning ,\nKR’12, pp. 552–561. AAAI Press, 2012. ISBN 9781577355601. URL https://www.aaai.org/\nocs/index.php/KR/KR12/paper/download/4492/4924.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy , V eselin\nStoyanov , and Luke Zettlemoyer. BAR T: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics , pp. 7871–7880, Online, 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\nQuentin Lhoest, Albert Villanova del Moral, Y acine Jernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan\nChhablani, Bhavitvya Malik, Simon Brandeis, T even Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry ,\nAngelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre\nDebut, Stas Bekman, Pierric Cistac, Thibault Goehringer, V ictor Mustar, François Lagunas, Alexander Rush,\nand Thomas W olf. Datasets: A community library for natural language processing. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 175–\n184, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.emnlp-demo.21. URL https://aclanthology.org/2021.emnlp-demo.21.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Y osinski. Measuring the intrinsic dimension of\nobjective landscapes. arXiv preprint arXiv:1804.08838 , 2018.\nQianxiao Li, Long Chen, Cheng T ai, and W einan E. Maximum Principle Based Algorithms for Deep Learning.\nJ. Mach. Learn. Res. , 18:165:1–165:29, 2017. URL http://jmlr.org/papers/v18/17-653.\nhtml.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (V olume 1: Long P apers) , pp. 4582–4597, Online,\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https:\n//aclanthology.org/2021.acl-long.353.\nBill Y uchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! NumerSense: Probing\nNumerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6862–6868, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.557. URL https:\n//aclanthology.org/2020.emnlp-main.557.\nChin-Y ew Lin. ROUGE: A package for automatic evaluation of summaries. In T ext Summarization Branches\nOut, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https:\n//aclanthology.org/W04-1013.\nW ang Ling, Dani Y ogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:\nLearning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (V olume 1: Long P apers) , pp. 158–167, V ancouver,\nCanada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https:\n//aclanthology.org/P17-1015.\nPengfei Liu, W eizhe Y uan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing. ArXiv\npreprint, abs/2107.13586, 2021a. URL https://arxiv.org/abs/2107.13586.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Y ang, and Jie T ang. P-tuning v2: Prompt tuning can\nbe comparable to ﬁne-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 , 2021b.\nURL https://arxiv.org/pdf/2110.07602.pdf.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy , Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov . Roberta: A robustly optimized bert pretraining approach. ArXiv preprint ,\nabs/1907.11692, 2019. URL https://arxiv.org/abs/1907.11692.\n39\nREFERENCES\nYitao Liu, Chenxin An, and Xipeng Qiu. Y -tuning: An efﬁcient tuning paradigm for large-scale pre-\ntrained models via label representation learning. arXiv preprint arXiv:2202.09817 , 2022. URL https:\n//arxiv.org/pdf/2202.09817.pdf.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercom-\nplex adapter layers. ArXiv preprint , abs/2106.04647, 2021a. URL https://arxiv.org/abs/2106.\n04647.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efﬁcient\nmulti-task ﬁne-tuning for transformers via shared hypernetworks. In ACL/IJCNLP, 2021b.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli.\nA SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of\nthe Ninth International Conference on Language Resources and Evaluation (LREC’14) , pp. 216–223,\nReykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www.\nlrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf.\nBinny Mathew , Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee.\nHatexplain: A benchmark dataset for explainable hate speech detection. ArXiv preprint , abs/2012.10289,\n2020. URL https://arxiv.org/abs/2012.10289.\nClara H. McCreery , Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. Effective\ntransfer learning for identifying similar questions: Matching user questions to COVID-19 faqs. In Rajesh\nGupta, Y an Liu, Jiliang T ang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, V irtual Event, CA, USA, August 23-27, 2020 , pp. 3458–3465.\nACM, 2020. URL https://dl.acm.org/doi/10.1145/3394486.3412861.\nT odor Mihaylov , Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a\nnew dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing , pp. 2381–2391, Brussels, Belgium, 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing\nat scale. arXiv preprint arXiv:2110.11309 , 2021.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, and Grigorios Tsoumakas. Ethos: an online hate speech\ndetection dataset. ArXiv preprint , abs/2006.08328, 2020. URL https://arxiv.org/abs/2006.\n08328.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders, et al. W ebgpt: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332 , 2021.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\nconvolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing , pp. 1797–1807, Brussels, Belgium, 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/\nD18-1206.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason W eston, and Douwe Kiela. Adversarial NLI:\nA new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , pp. 4885–4901, Online, 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.\nacl-main.441.\nAchraf Othman and Mohamed Jemni. English-asl gloss parallel corpus 2012:\nAslg-pc12. 2012. URL https://www.achrafothman.net/aslsmt/\nEnglish-ASL-Gloss-Parallel-Corpus-2012-ASLG-PC12.pdf .\nBo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect\nto rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics\n(ACL’05), pp. 115–124, Ann Arbor, Michigan, 2005. Association for Computational Linguistics. doi:\n10.3115/1219840.1219855. URL https://aclanthology.org/P05-1015.\n40\nREFERENCES\nEthan Perez, Florian Strub, Harm De Vries, V incent Dumoulin, and Aaron Courville. Film: V isual reasoning\nwith a general conditioning layer. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 32, 2018.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Y uxiang Wu, and Alexander\nMiller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pp. 2463–2473, Hong Kong, China, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Y uxiang Wu, Alexander H. Miller, and\nSebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base\nConstruction, 2020. URL https://openreview.net/forum?id=025X0zPfn.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan V uli ´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 46–54,\nOnline, 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.7. URL\nhttps://aclanthology.org/2020.emnlp-demos.7.\nJonas Pfeiffer, Ivan V uli ´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based Framework\nfor Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pp. 7654–7673, Online, 2020b. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology.org/2020.\nemnlp-main.617.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion:\nNon-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: Main V olume , pp. 487–503, On-\nline, 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.\neacl-main.39.\nMohammad T aher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating\ncontext-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language T echnologies, V olume 1\n(Long and Short P apers) , pp. 1267–1273, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128.\nAmir Pouran Ben V eyseh, Franck Dernoncourt, Quan Hung Tran, and Thien Huu Nguyen. What does this\nacronym mean? introducing a new dataset for acronym identiﬁcation and disambiguation. In Proceedings of\nthe 28th International Conference on Computational Linguistics , pp. 3285–3301, Barcelona, Spain (Online),\n2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.292.\nURL https://aclanthology.org/2020.coling-main.292.\nY ujia Qin, Y ankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Y usheng Su, Zhiyuan Liu, Peng\nLi, Maosong Sun, et al. Knowledge inheritance for pre-trained language models. arXiv preprint\narXiv:2105.13880, 2021a.\nY ujia Qin, Xiaozhi W ang, Y usheng Su, Y ankai Lin, Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng Li,\nMaosong Sun, et al. Exploring low-dimensional intrinsic task subspace via prompt tuning. ArXiv preprint ,\nabs/2110.07867, 2021b. URL https://arxiv.org/abs/2110.07867.\nY ujia Qin, Jiajie Zhang, Y ankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Elle: Efﬁcient\nlifelong pre-training for emerging data. OpenReview preprint , 2021c. URL https://openreview.\nnet/forum?id=UF7a5kIdzk.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models\nare unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n41\nREFERENCES\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Y anqi Zhou,\nW ei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nArXiv preprint , abs/1910.10683, 2019. URL https://arxiv.org/abs/1910.10683.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging\nlanguage models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pp. 4932–4942, Florence, Italy , 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1487. URL https://aclanthology.org/P19-1487.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev , and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 2383–2392, Austin, T exas, 2016. Association for Computational Linguistics. doi:\n10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea V edaldi. Learning multiple visual domains with residual\nadapters. Advances in neural information processing systems , 30, 2017.\nAndreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.\nAdapterDrop: On the efﬁciency of adapters in transformers. In Proceedings of EMNLP , pp. 7930–7946,\n2021. URL https://aclanthology.org/2021.emnlp-main.626.\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. DuoRC: T owards complex\nlanguage understanding with paraphrased reading comprehension. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (V olume 1: Long P apers) , pp. 1683–1693,\nMelbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1156. URL\nhttps://aclanthology.org/P18-1156.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Y ejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. In The Thirty-F ourth AAAI Conference on Artiﬁcial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The\nT enth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New Y ork, NY , USA,\nF ebruary 7-12, 2020 , pp. 8732–8740. AAAI Press, 2020. URL https://aaai.org/ojs/index.\nphp/AAAI/article/view/6399.\nT even Le Scao and Alexander M. Rush. How many data points is a prompt worth? In NAACL, 2021.\nTimo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classiﬁcation and natural\nlanguage inference. In Proceedings of the 16th Conference of the European Chapter of the Association\nfor Computational Linguistics: Main V olume , pp. 255–269, Online, 2021. Association for Computational\nLinguistics. URL https://aclanthology.org/2021.eacl-main.20.\nLakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. Natural language understanding with the\nquora question pairs dataset. arXiv e-prints , 2019. URL https://arxiv.org/abs/1907.01041.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In\nJennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 , volume 80 of Proceedings\nof Machine Learning Research , pp. 4603–4611. PMLR, 2018. URL http://proceedings.mlr.\npress/v80/shazeer18a.html.\nDamien Sileo, Tim V an De Cruys, Camille Pradel, and Philippe Muller. Mining discourse markers for\nunsupervised sentence representation learning. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language T echnologies, V olume 1\n(Long and Short P apers) , pp. 3477–3486, Minneapolis, Minnesota, 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1351. URL https://aclanthology.org/N19-1351.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp.\n1631–1642, Seattle, W ashington, USA, 2013. Association for Computational Linguistics. URL\nhttps:\n//aclanthology.org/D13-1170.\n42\nREFERENCES\nAsa Cooper Stickland and Iain Murray . BERT and pals: Projected attention layers for efﬁcient adaptation\nin multi-task learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,\nUSA, volume 97 of Proceedings of Machine Learning Research , pp. 5986–5995. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v97/stickland19a.html.\nY usheng Su, Xiaozhi W ang, Y ujia Qin, Chi-Min Chan, Y ankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou,\nMaosong Sun, and Jie Zhou. On transferability of prompt tuning for natural language understanding. ArXiv\npreprint, abs/2111.06719, 2021. URL https://arxiv.org/abs/2111.06719.\nTianxiang Sun, Xiangyang Liu, Xipeng Qiu, and Xuanjing Huang. Paradigm shift in natural language\nprocessing. ArXiv preprint , abs/2109.12575, 2021. URL https://arxiv.org/abs/2109.12575.\nTianxiang Sun, Y unfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-\nmodel-as-a-service. arXiv preprint arXiv:2201.03514 , 2022. URL https://arxiv.org/abs/2201.\n03514.\nOyvind T afjord, Peter Clark, Matt Gardner, W en-tau Yih, and Ashish Sabharwal. Quarel: A dataset and\nmodels for answering questions about qualitative relationships. Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , 33(01):7063–7071, Jul. 2019a. doi: 10.1609/aaai.v33i01.33017063. URL\nhttps://ojs.aaai.org/index.php/AAAI/article/view/4687.\nOyvind T afjord, Matt Gardner, Kevin Lin, and Peter Clark. QuaR Tz: An open-domain dataset of qualitative\nrelationship questions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\npp. 5941–5946, Hong Kong, China, 2019b. Association for Computational Linguistics. doi: 10.18653/v1/\nD19-1608. URL https://aclanthology.org/D19-1608.\nZhixing T an, Xiangwen Zhang, Shuo W ang, and Y ang Liu. MSP: Multi-stage prompting for making pre-trained\nlanguage models better translators. arXiv preprint arXiv:2110.06609 , 2021.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. W allach, Rob Fergus, S. V . N. V ishwanathan, and Roman Garnett (eds.), Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, Decem-\nber 4-9, 2017, Long Beach, CA, USA , pp. 5998–6008, 2017. URL\nhttps://proceedings.neurips.\ncc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nTu V u, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model adaptation\nthrough soft prompt transfer. ArXiv preprint , abs/2110.07904, 2021. URL https://arxiv.org/abs/\n2110.07904.\nAlex W ang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy , and Samuel R. Bowman. GLUE: A multi-\ntask benchmark and analysis platform for natural language understanding. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview .net, 2019.\nURL https://openreview.net/forum?id=rJ4km2R5t7.\nWilliam Y ang W ang. “liar, liar pants on ﬁre”: A new benchmark dataset for fake news detection. In Proceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short P apers) , pp.\n422–426, V ancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-2067.\nURL https://aclanthology.org/P17-2067.\nAlex W arstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transac-\ntions of the Association for Computational Linguistics , 7:625–641, 2019. doi: 10.1162/tacl_a_00290. URL\nhttps://aclanthology.org/Q19-1040.\nAlex W arstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey , W ei Peng, Sheng-Fu W ang, and Samuel R.\nBowman. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Asso-\nciation for Computational Linguistics , 8:377–392, 2020. doi: 10.1162/tacl_a_00321. URL https:\n//aclanthology.org/2020.tacl-1.25.\nLaura W eidinger, John Mellor, Maribeth Rauh, Conor Grifﬁn, Jonathan Uesato, Po-Sen Huang, Myra Cheng,\nMia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359 , 2021.\n43\nREFERENCES\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language T echnologies, V olume 1 (Long P apers) ,\npp. 1112–1122, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/\nv1/N18-1101. URL https://aclanthology.org/N18-1101.\nJohn Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computa-\ntion, and applications. 2021.\nYi Y ang, W en-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question\nanswering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing ,\npp. 2013–2018, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/\nD15-1237. URL https://aclanthology.org/D15-1237.\nZonghan Y ang and Y ang Liu. On Robust Preﬁx-Tuning for T ext Classiﬁcation. In International Conference on\nLearning Representations , 2022. URL https://openreview.net/forum?id=eBCmOocUejf.\nQinyuan Y e, Bill Y uchen Lin, and Xiang Ren. CrossFit: A few-shot learning challenge for cross-task\ngeneralization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing , pp. 7163–7189, Online and Punta Cana, Dominican Republic, 2021. Association for\nComputational Linguistics. URL https://aclanthology.org/2021.emnlp-main.572.\nShaokai Y e, Kailu Wu, Mu Zhou, Y unfei Y ang, Sia Huat T an, Kaidi Xu, Jiebo Song, Chenglong Bao, and\nKaisheng Ma. Light-weight calibrator: A separable component for unsupervised domain adaptation.\nIn 2020 IEEE/CVF Conference on Computer V ision and P attern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020 , pp. 13733–13742. IEEE, 2020. doi: 10.1109/CVPR42600.2020.01375. URL\nhttps://doi.org/10.1109/CVPR42600.2020.01375.\nT ao Y u, Rui Zhang, Kai Y ang, Michihiro Y asunaga, Dongxu W ang, Zifan Li, James Ma, Irene Li, Qingning\nY ao, Shanelle Roman, Zilin Zhang, and Dragomir Radev . Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing , pp. 3911–3921, Brussels, Belgium, 2018. Association\nfor Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/\nD18-1425.\nElad Ben Zaken, Shauli Ravfogel, and Y oav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for\ntransformer-based masked language-models. ArXiv preprint , abs/2106.10199, 2021. URL https://\narxiv.org/abs/2106.10199.\nRowan Zellers, Y onatan Bisk, Roy Schwartz, and Y ejin Choi. SW AG: A large-scale adversarial dataset for\ngrounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 93–104, Brussels, Belgium, 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1009. URL https://aclanthology.org/D18-1009.\nRowan Zellers, Ari Holtzman, Y onatan Bisk, Ali Farhadi, and Y ejin Choi. HellaSwag: Can a machine really\nﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 4791–4800, Florence, Italy , 2019. Association for Computational Linguistics. doi: 10.\n18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.\nDinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. Y ou only propagate once:\nAccelerating adversarial training via maximal principle. In H. W allach, H. Larochelle, A. Beygelzimer,\nF . d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/\n2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf.\nHao Zhang, Jae Ro, and Richard Sproat. Semi-supervised URL segmentation with recurrent neural\nnetworks pre-trained on knowledge graph entities. In Proceedings of the 28th International Confer-\nence on Computational Linguistics , pp. 4667–4675, Barcelona, Spain (Online), 2020. International\nCommittee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.411. URL https:\n//aclanthology.org/2020.coling-main.411.\nSheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin Duh, and Benjamin V an Durme. Record: Bridging the gap\nbetween human and machine commonsense reading comprehension. ArXiv preprint , abs/1810.12885, 2018.\nURL https://arxiv.org/abs/1810.12885.\n44\nREFERENCES\nY uan Zhang, Jason Baldridge, and Luheng He. P A WS: Paraphrase adversaries from word scrambling. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language T echnologies, V olume 1 (Long and Short P apers) , pp. 1298–1308, Min-\nneapolis, Minnesota, 2019b. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL\nhttps://aclanthology.org/N19-1131.\nMengjie Zhao, T ao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. Masking as an efﬁcient alternative to\nﬁnetuning for pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pp. 2226–2241, Online, 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.174. URL https://aclanthology.org/2020.\nemnlp-main.174.\n45\nA. Implementation Details\nA Implementation Details\nA.1 Performance and Convergence\nAmong the NLP datasets downloaded from Hugginface datasets, for those datasets without publicly released\ntest set, we evenly divide the original development sets into two halves as the new development set and test\nset; for those datasets without publicly released development set and test set, we divide the original training set\nwith a ratio of 8 : 1 : 1 into the new training set, development set and test set.\nFor PF, LR, AP and FT, we use AdamW ( Kingma & Ba , 2015) as the optimizer, set the maximum training\nsteps to 20,000 with early stop, and save the checkpoint for evaluation on development set every 100 steps.\nAfter that, we evaluate the best checkpoint using the development set on the test set. W e experiment on\nthe combinations of different batch sizes ( {16,32}) and learning rates ( {1 ×10−3,1 ×10−4,5 ×10−4}),\nand report the best performance. Since we found empirically that PT converges much slower than the\nother tuning methods, we set the maximum training step of PT to 100,000 steps without early stop, and\nevaluate the performance on development set for every 1,000 steps. Following Lester et al. (2021), we choose\nAdafactor ( Shazeer & Stern , 2018) as the optimizer. All the experiments are conducted under the same\nenvironment.\nA.2 Combinations of Delta T uning Methods\nFor prompt tuning, we prepend 10 tunable virtual tokens into the input text; for adapter, we set the reduction\nfactor to 16; for BitFit, all the bias components in PLMs are optimized.\nSimultaneous Combination. For all delta tuning methods on RoBER T aLARGE, we choose AdamW ( Kingma\n& Ba , 2015) as the optimizer, set the maximum training steps to 6,000, and save the checkpoint for evaluation\non development set every 200 steps. After that, we select the best checkpoint based on the development set,\nand evaluate it on the test set. For the full-data setting, we set the training batch size to 16 and experiment on\nthe combinations of different learning rates ( {1 ×10−2,1 ×10−3,1 ×10−4,1 ×10−5}); for the few-shot\nsetting, we set the training batch size to 4 and experiment on the combination of 4 different learning rates,\nwhich are listed in T able 8.\nFor STS-B, which is a regression task, we convert it into a binary classiﬁcation problem. Speciﬁcally , assume\nthat the original output value is bounded by [vl,vu] and the new labels are {yl,yu}, the original value is\nreformulated as:\ny= vl ·p(yl|xin) + vu ·p(yu|xin).\nDuring optimization, we minimize the KL-divergence between prediction distribution p(yu|xin ) and the ground\ntruth (y−vl)/(vu −vl).\nSequential Combination. W e choose AdamW ( Kingma & Ba , 2015) as the optimizer, set the batch size to\n64 and the learning rate to 1 ×10−2 for prompt tuning, 1 ×10−4 for BitFit and 1 ×10−5 for adapter.\nT able 8: Learning rate setting of RoBER T a LARGE on 16-shot GLUE datasets.\nPrompt tuning /enc-37/enc-37/enc-37/enc-37/enc-33/enc-33/enc-33/enc-33\nBitFit /enc-37/enc-37/enc-33/enc-33/enc-37/enc-37/enc-33/enc-33\nAdapter /enc-37/enc-33/enc-37/enc-33/enc-37/enc-33/enc-37/enc-33\nLearning Rates\n1e-2 - /enc-37 /enc-37 /enc-37/enc-33/enc-37 /enc-37 /enc-37\n3e-3 - /enc-37/enc-33/enc-37/enc-33/enc-37 /enc-37 /enc-37\n1e-3 - /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33\n3e-4 - /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33 /enc-33\n1e-4 - /enc-33 /enc-33 /enc-33/enc-37/enc-33 /enc-33 /enc-33\n3e-5 - /enc-33/enc-37/enc-33/enc-37/enc-33 /enc-33 /enc-33\n46\nA.3 The Power of Scale for Delta Tuning\nA.3 The Power of Scale for Delta T uning\nT able 9: The percentages of the tuned parameters (parameters participating optimizing in a PLM / all\nparameters in a PLM) during the training.\nSMALL BASE XXL\nAdapter 1.70% 1.20% 0.28%\nLoRA 0.73% 0.64% 0.26%\nPreﬁx-tuning 0.50% 0.47% 0.11%\nPrompt T uning 0.06% 0.03% 0.01%\nLast Layer T uning 6.30% 4.20% 2.10%\nSelective Module T uning 2.10% 4.20% 2.40%\nApart from the delta tuning methods (prompt tuning, adapter, LoRA and preﬁx-tuning) introduced in the\nprevious sections, we additionally design two delta tuning methods, i.e., last layer tuning and selective module\ntuning, to investigate the power of scale for delta tuning. For last layer tuning, we only select the last layer\nof the encoder in T5 to optimize. For selective module tuning, we manually choose some modules (e.g., the\nfeed-forward layer, query / key / value matrix in the attention layer, or a layer norm) in T5 to optimize. W e\nset the training batch size to 64 for all delta tuning methods. For the different scales of T5, we use the same\nlearning rates during training: 5 ×10−3 (prompt tuning), 5 ×10−4 (adapter), 5 ×10−5 (LoRA) 5 ×10−3\n(preﬁx-tuning), 5 ×10−5 (last layer tuning), and 5 ×10−5 (selective module tuning). The percentage of the\ntunable parameters for each method / model is listed in T able 9.\nA.4 T ask-level T ransferability Evaluation\nIn the cross-task transferability experiments, we utilize 12 tasks of 5 different types as follows:\nSentiment Analysis. Given a sentence, a PLM identiﬁes the sentiment polarity in this sentence. W e\nchoose SST-2 (Socher et al. , 2013), Amazon/Polarity, and Rotten Tomatoes (Pang & Lee , 2005)\nto analyze.\nNatural Language Inference. Given a premise and hypothesis pair, a PLM determines whether the hypoth-\nesis is entailed, contradict, or undetermined by the premise. W e choose MNLI (Williams et al. , 2018), SICK\n(Marelli et al. , 2014), and SciTail (Khot et al. , 2018b) to analyze.\nParaphrase Identiﬁcation. Given a pair of sentences, a PLM judges whether they are semantically identical.\nW e choose QQP (Sharma et al. , 2019) and MRPC (Dolan & Brockett , 2005) to analyze.\nQuestion Answering. Given a question, a PLM answers the question based on context. W e choose MathQA\n(Amini et al. , 2019) and AQUA-RAT (Ling et al. , 2017) to analyze.\nSummarization. Given an article, a PLM summarizes it. W e choose Multi-News (Fabbri et al. , 2019),\nand SAMSum (Gliwa et al. , 2019) to analyze.\nEvaluation Metrics. For sentiment analysis, natural language inference, and paraphrase identiﬁcation\ntasks, we choose accuracy (Acc.) as their evaluation metric in the experiments. For question answering\nand summarization, we utilize F1 and ROUGE-L ( Lin, 2004), respectively . Finally , we report their relative\nperformance (transferring zero-shot performance / original performance) (%).\nB T asks Evaluated in Experiments\nT able 10: The tasks evaluated in our experiments in T able\n3. W e refer to Y e et al. (2021) for task ontology .\nOntology T ask Name Reference\ncls/sentiment analysis\nglue-sst2 Socher et al. 2013\n47\nB. T asks Evaluated in Experiments\nrotten_tomatoes Pang & Lee 2005\ncls/emotion\nemo Chatterjee et al. 2019\ntweet_eval-hate Barbieri et al. 2020\ntweet_eval-irony Barbieri et al. 2020\ntweet_eval-offensive Barbieri et al. 2020\ntweet_eval-sentiment Barbieri et al. 2020\ntweet_eval-stance_abortion Barbieri et al. 2020\ntweet_eval-stance_atheism Barbieri et al. 2020\ntweet_eval-stance_climate Barbieri et al. 2020\ntweet_eval-stance_feminist Barbieri et al. 2020\ntweet_eval-stance_hillary Barbieri et al. 2020\ncls/hate speech detection\nethos-disability Mollas et al. 2020\nethos-gender Mollas et al. 2020\nethos-national_origin Mollas et al. 2020\nethos-religion Mollas et al. 2020\nhate_speech18 Davidson et al. 2017\nhatexplain Mathew et al. 2020\ncls/NLI\nanli Nie et al. 2020\nglue-mnli Williams et al. 2018\nglue-qnli Rajpurkar et al. 2016\nglue-rte Dagan et al. 2005 ; Bar-Haim et al. 2006\nGiampiccolo et al. 2007\nscitail Khot et al. 2018a\nsuperglue-rte Dagan et al. 2005 ; Bar-Haim et al. 2006\ncls/fact checking\nclimate_fever Diggelmann et al. 2020\nliar W ang 2017\ncls/paraphrase\nglue-qqp (link)\nmedical_questions_pairs McCreery et al. 2020\npaws Zhang et al. 2019b\ncls/topic ag_news Gulli (link)\ncls/other\nade_corpus_v2-classiﬁcation Gurulingappa et al. 2012\ndiscovery Sileo et al. 2019\nglue-cola W arstadt et al. 2019\nsms_spam Almeida et al. 2011\nsuperglue-wic Pilehvar & Camacho-Collados 2019\nsuperglue-wsc Levesque et al. 2012\nwiki_qa Y ang et al. 2015\nqa/closed-book qa\nfreebase_qa Jiang et al. 2019\nlama-conceptnet Petroni et al. 2019 , 2020\nlama-google_re Petroni et al. 2019 , 2020\nlama-squad Petroni et al. 2019 , 2020\nlama-trex Petroni et al. 2019 , 2020\nnumer_sense Lin et al. 2020\nsearch_qa Dunn et al. 2017\nweb_questions Berant et al. 2013\nqa/multiple-choice qa\ncosmos_qa Huang et al. 2019\ndream Saha et al. 2018\nhellaswag Zellers et al. 2019\nopenbookqa Mihaylov et al. 2018\nqasc Khot et al. 2020\nquarel T afjord et al. 2019a\nquartz-no_knowledge T afjord et al. 2019b\nquartz-with_knowledge T afjord et al. 2019b\nrace-high Lai et al. 2017\nrace-middle Lai et al. 2017\nsuperglue-copa Gordon et al. 2012\nswag Zellers et al. 2018\nwino_grande Sakaguchi et al. 2020\nqa/long-form qa\neli5-askh Fan et al. 2019\n48\nB. T asks Evaluated in Experiments\neli5-asks Fan et al. 2019\neli5-eli5 Fan et al. 2019\nqa/MRC superglue-record Zhang et al. 2018\ncg/summarization\nmulti_news Fabbri et al. 2019\nsamsum Gliwa et al. 2019\nxsum Narayan et al. 2018\ncg/other\nspider Y u et al. 2018\nwiki_bio Lebret et al. 2016\nwiki_split Botha et al. 2018\nother/linguistic phenomenon\nblimp-anaphor_gender_agreement W arstadt et al. 2020\nblimp-ellipsis_n_bar_1 W arstadt et al. 2020\nblimp-sentential_negation_npi_scope W arstadt et al. 2020\nother/generate\nexplanation cos_e Rajani et al. 2019\nother/slot_ﬁlling\nade_corpus_v2-dosage Gurulingappa et al. 2012\nade_corpus_v2-effect Gurulingappa et al. 2012\nother/entity linking kilt_ay2 Hoffart et al. 2011\nother/other\nacronym_identiﬁcation Pouran Ben V eyseh et al. 2020\naslg_pc12 Othman & Jemni 2012\ncrawl_domain Zhang et al. 2020\nproto_qa Boratko et al. 2020\n49",
  "topic": "Fine-tuning",
  "concepts": [
    {
      "name": "Fine-tuning",
      "score": 0.7456541657447815
    },
    {
      "name": "Computer science",
      "score": 0.7421274185180664
    },
    {
      "name": "Process (computing)",
      "score": 0.5689000487327576
    },
    {
      "name": "Categorization",
      "score": 0.4928264915943146
    },
    {
      "name": "Computation",
      "score": 0.48160383105278015
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4759875535964966
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4662538766860962
    },
    {
      "name": "Scale (ratio)",
      "score": 0.451806902885437
    },
    {
      "name": "Machine learning",
      "score": 0.4473235607147217
    },
    {
      "name": "Language model",
      "score": 0.4462064802646637
    },
    {
      "name": "Contrast (vision)",
      "score": 0.4219962954521179
    },
    {
      "name": "Algorithm",
      "score": 0.19621077179908752
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}