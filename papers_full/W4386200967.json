{
  "title": "Personality Traits in Large Language Models",
  "url": "https://openalex.org/W4386200967",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4320632338",
      "name": "Gregory Serapio‐García",
      "affiliations": [
        "Google (United States)",
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2232518938",
      "name": "Mustafa Safdari",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5091982964",
      "name": "Clément Crepy",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2156798580",
      "name": "Luning Sun",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A3172405544",
      "name": "Stephen Fitz",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A3096532720",
      "name": "Marwa Abdulhai",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2190457378",
      "name": "Aleksandra Faust",
      "affiliations": [
        "Google (United States)",
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2954719883",
      "name": "Maja Matarić",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387947082",
    "https://openalex.org/W2060255212",
    "https://openalex.org/W2096314969",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W3201316762",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W2767879018",
    "https://openalex.org/W1641003075",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2972203331",
    "https://openalex.org/W2153803020",
    "https://openalex.org/W2029342244",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4383605243",
    "https://openalex.org/W2788194298",
    "https://openalex.org/W4298181341",
    "https://openalex.org/W4321022177",
    "https://openalex.org/W2153266959",
    "https://openalex.org/W3011685505",
    "https://openalex.org/W2106096361",
    "https://openalex.org/W4221164991",
    "https://openalex.org/W2089632658",
    "https://openalex.org/W1555938369",
    "https://openalex.org/W2081155303",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W2119595472",
    "https://openalex.org/W3178106241",
    "https://openalex.org/W4324299222",
    "https://openalex.org/W2064012529",
    "https://openalex.org/W4220969689",
    "https://openalex.org/W1983004754",
    "https://openalex.org/W3198002980",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4323697341",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2490857485",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2072833030",
    "https://openalex.org/W2991598253",
    "https://openalex.org/W2744863474",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4312091380",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2005364767",
    "https://openalex.org/W2923413722",
    "https://openalex.org/W4304632964",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3120669575",
    "https://openalex.org/W3103594808",
    "https://openalex.org/W4297412056",
    "https://openalex.org/W3015151122",
    "https://openalex.org/W4283020727",
    "https://openalex.org/W4225009785",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W1779879527",
    "https://openalex.org/W2029087690",
    "https://openalex.org/W2112665044",
    "https://openalex.org/W4312091865",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W4224909620",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2156237672",
    "https://openalex.org/W4307418102",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W1972820248",
    "https://openalex.org/W2731561792",
    "https://openalex.org/W4378473699",
    "https://openalex.org/W4220685986",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2014459489",
    "https://openalex.org/W2154868463",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4387534155",
    "https://openalex.org/W2501948168",
    "https://openalex.org/W1560729591",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W4385489610",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W186538591",
    "https://openalex.org/W3098349105",
    "https://openalex.org/W2014174447",
    "https://openalex.org/W2312763364",
    "https://openalex.org/W4242406013",
    "https://openalex.org/W2010336897",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4253208814",
    "https://openalex.org/W2056700613",
    "https://openalex.org/W4250304879",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1992984099",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4392260510",
    "https://openalex.org/W4386576812",
    "https://openalex.org/W2159306398",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4381572755",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W2398060291",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4301039365",
    "https://openalex.org/W2033398965",
    "https://openalex.org/W4242551423"
  ],
  "abstract": "<title>Abstract</title> The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant text. As LLMs increasingly power conversational agents, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible use of LLMs.",
  "full_text": "Personality Traits in Large Language Models\nGregory Serapio-García  (  gs639@cam.ac.uk )\nGoogle DeepMind https://orcid.org/0000-0002-1890-2331\nMustafa Safdari  (  msafdari@google.com )\nGoogle DeepMind https://orcid.org/0009-0002-1604-8685\nClément Crepy \nGoogle Research\nLuning Sun \nUniversity of Cambridge\nStephen Fitz \nKeio University\nMarwa Abdulhai \nUniversity of California, Berkeley\nAleksandra Faust  (  faust@google.com )\nGoogle DeepMind\nMaja Matarić  (  majamataric@google.com )\nGoogle DeepMind https://orcid.org/0000-0001-8958-6666\nResearch Article\nKeywords: AI, large language models, personality, psychometrics, construct validity\nPosted Date: August 28th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3296728/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nPersonality Traits in Large Language Models\nGregory Serapio-Garc´ ıa1,2,3†, Mustafa Safdari 1†, Cl´ ement Crepy4, Luning Sun 3,\nStephen Fitz 5, Peter Romero 3,5, Marwa Abdulhai 6, Aleksandra Faust 1,∗ ,\nMaja Matari´ c1,∗\n1Google DeepMind.\n2Department of Psychology, University of Cambridge.\n3The Psychometrics Centre, Cambridge Judge Business School, Un iversity of Cambridge.\n4Google Research.\n5Keio University.\n6University of California, Berkeley.\n∗ Authors jointly supervised this work.\nContributing authors: gs639@cam.ac.uk; msafdari@google.com; ccrepy@google.com;\nls523@cam.ac.uk; stephenf@keio.jp; rp@keio.jp; marwa abdulhai@berkeley.edu; faust@google.com;\nmajamataric@google.com;\n†Authors contributed equally.\nAbstract\nThe advent of large language models (LLMs) has revolutionized natural language processing, enabling\nthe generation of coherent and contextually relevant text. As L LMs increasingly power conversational\nagents, the synthetic personality embedded in these models, by virtue of training on large amounts\nof human data, is becoming increasingly important. Since perso nality is a key factor determining the\neﬀectiveness of communication, we present a comprehensive me thod for administering and validating\npersonality tests on widely-used LLMs, as well as for shaping p ersonality in the generated text of such\nLLMs. Applying this method, we found: 1) personality measureme nts in the outputs of some LLMs\nunder speciﬁc prompting conﬁgurations are reliable and valid; 2 ) evidence of reliability and validity of\nsynthetic LLM personality is stronger for larger and instruction ﬁne-tuned models; and 3) personality\nin LLM outputs can be shaped along desired dimensions to mimic speciﬁc personality proﬁles. We\ndiscuss application and ethical implications of the measure ment and shaping method, in particular\nregarding responsible use of LLMs.\nKeywords: AI, large language models, personality traits, psychometr ics, construct validity\n1 Summary\nLarge language models (LLMs), large-capacity\nmachine-learned models that generate text,\nrecently inspired major breakthroughs in natu-\nral language processing (NLP) and conversational\nagents [\n9, 40, 64]. Vast amounts of human-\ngenerated training data [ 7] enable LLMs to\nmimic human characteristics in their outputs and\nexhibit a form of synthetic personality.Personality\nencompasses an entity’s characteristic patterns of\nthought, feeling, and behavior [\n2, 47]. In humans,\npersonality is formed from biological and social\nfactors, and fundamentally inﬂuences daily inter-\nactions and preferences [\n46]. Psychometrics, the\n1\nscience of psychological test construction and val-\nidation [\n49], provides an empirical framework for\nquantifying human personality through psycho-\nmetric testing [\n55]. To date, validated psychome-\ntric methods for quantifying human personality\nhave not been applied to LLMs end-to-end; while\npast works [\n20] have attempted to measure per-\nsonality in LLMs with psychometric tests, there\nremains a scientiﬁc need to formally evaluate the\nreliability and validity of these measurements in\nthe LLM context.\nOur work answers the open question: Do\nLLMs simulate human personality traits in reli-\nable, valid, and practically meaningful ways, and\nif so, can LLM-synthesized personality proﬁles\nbe veriﬁably shaped along desired dimensions?\nWe contribute a methodology for administering\npersonality-based psychometric tests to LLMs,\nevaluating the reliability and validity of the\nresulting measurements, and also shaping LLM-\nsynthesized personality traits. First, to administer\npsychometric tests to LLMs, we developed a struc-\ntured prompting method that simulates persona\ndescriptions and introduces prompt variations.\nNext, the test score variation created by this\nprompting is used to power a suite of statistical\nanalyses assessing the reliability of the resulting\nmeasurements. Last, we present a novel prompt-\ning methodology that shapes personality traits at\nnine levels using 104 trait adjectives.\nApplying the described methodology to a fam-\nily of LLMs, we found that: 1) evidence of the\nreliability and validity of LLM-synthesized per-\nsonality measurements is stronger for larger and\ninstruction ﬁne-tuned models; 2) personality in\nLLM outputs can be shaped along desired dimen-\nsions to mimic speciﬁc human personality proﬁles;\nand 3) shaped personality veriﬁably inﬂuences\nLLM behavior in common downstream (i.e., sub-\nsequent) tasks, such as writing social media posts\n[\n52]. By providing a methodology for quantify-\ning and validating measurements of personality in\nLLMs, this work establishes a foundation for prin-\ncipled LLM assessment that is especially impor-\ntant as LLMs and, more generally, foundation\nmodels continue to grow in popularity and scale.\nBy leveraging psychometrics, this work translates\nestablished measurement theory from quantitative\nsocial science and psychological assessment to the\nﬂedgling science of LLMs, a ﬁeld that is poised to\ngrow and necessitates both a solid foundation and\ninterdisciplinary expertise and perspectives.\n2 Quantifying and Validating\nPersonality Traits in LLMs\nLLMs are starting to meet most of the key require-\nments for human-like language use, including\nconversation, contextual understanding, coherent\nand relevant responses, adaptability and learning,\nquestion answering, dialog, and text generation\n[\n40, 54, 64]. These impressive NLP capabilities are\na result of LLMs’ abilities to learn language dis-\ntribution, aided by increasing model sizes [\n7, 65],\ntraining on massive datasets of text, and fur-\nther ﬁne-tuning toward usage preferences [\n63]\n(see Appendix A). Taken together, they enable\nLLMs to enact convincing, human-like personas,\nsparking debate over the existence and extent of\npersonality [\n38], human values [51], and other psy-\nchological phenomena [ 59] potentially embedded\nin these models.\nPersonality is a foundational socio-behavioral\nphenomenon in psychology that, for humans, pre-\ndicts a broad spectrum of health, social, economic,\nand political behaviors crucial for individual and\nsocietal success [\n6]. For example, personality has\nbeen extensively studied as an antecedent of\nhuman values [\n43]. Decades of research have fur-\nther shown how personality information is richly\nencoded in human language [\n17, 50]. LLMs not\nonly comprise the vast sociopolitical, economic,\nand behavioral data they are trained on, they also\ngenerate language that inherently expresses per-\nsonality content. For this reason, the ability to\nmeasure and validate LLM-synthesized personal-\nity holds promise for LLM safety, responsibility,\nand alignment eﬀorts [\n15], which have so far\nprimarily focused on mitigating speciﬁc harms\nrather than examining more fundamental patterns\nof model behavior. Ultimately, personality as an\nempirical framework [\n26] provides both theory and\nmethodology for quantifying latent traits in LLMs\nthat are potentially predictive of LLM behaviors\nin diverse inference tasks (see Appendix\nB).\n2\nFig. 1 : Establishing Construct Validity. LLMs are administered two pe rsonality tests, with the variation injected\nthrough a set of Descriptive Personas, Test Instructions, and Ite m Postambles. The scored LLM responses are\nanalyzed for reliability, convergent validity, discriminant validity, and criterion validity.\nSome observed LLM agents have inadvertently\nmanifested undesirable personality proﬁles 1, rais-\ning serious safety and fairness concerns in AI, com-\nputational social science, and psychology research\n[\n20]. Recent work has tried to identify unintended\nconsequences of the improved abilities of LLMs,\nincluding their use of deceptive and manipulative\nlanguage [\n34], gender, racial, or religious bias in\nbehavioral experiments [ 1], and violent language,\namong many others [4]. LLMs can also be inconsis-\ntent in dialogue [ 36], explanation generation, and\nfactual knowledge extraction.\nPrior attempts to probe psychological phe-\nnomena such as personality and human values\nin LLMs have informally measured personality\n1https://www.nytimes.com/2023/02/16/technology/bing-\nchatbot-microsoft-chatgpt.html\nusing questionnaires and, in some cases, prelim-\ninarily assessed the quality of LLM question-\nnaire responses [\n38]. Past work has also explored\nmethods, such as few-shot prompting, to miti-\ngate undesirable and extreme personality proﬁles\nexhibited in LLM outputs. However, so far no\nwork has addressed how to systematically mea-\nsure and psychometrically validate measurements\nof LLM personality in light of their highly vari-\nable outputs and hypersensitivity to prompting.\nWe further detail related work in Appendix\nC.\nThe question of how to systematically ver-\nify synthetic personality in LLMs highlights calls\nfrom responsible AI researchers [\n24] to scien-\ntiﬁcally evaluate construct validity when study-\ning social-psychological phenomena in AI sys-\ntems, as inaccurate conceptions of such phenom-\nena directly impact mitigation and governance\neﬀorts. Construct validity , a central criterion of\n3\nscientiﬁc measurement [ 11], refers to the ability\nof a measure to reliably and accurately reﬂect\nthe latent phenomenon (i.e., construct) it was\ndesigned to quantify. The only published explo-\nration of personality and psychodemographics in\nLLMs [\n38] questioned the validity of the survey\nresponses returned by GPT-3; it found an incon-\nsistent pattern in HEXACO Personality Inventory\n[\n32] and human value survey responses. That\nstudy preliminarily evaluated measurement qual-\nity in terms of “theoretical reliability:” how the\ninter-facet correlations of GPT-3’s HEXACO data\naligned with those observed among human HEX-\nACO data. More formal psychometric evalua-\ntions of reliability—and more crucially, construct\nvalidity—are required to verify questionnaire-\nbased measurements of latent psychological traits\nin LLMs. An LLM may display elevated levels\nof agreeableness through its answers on a per-\nsonality questionnaire, but those answers may\nnot form internally consistent patterns across the\nentire questionnaire; tests administered to LLMs\nmay not be empirically reliable. Concurrently, the\nreliability of LLM responses to a questionnaire\npurporting to measure agreeableness may not nec-\nessarily reﬂect its tendency to behave agreeably\nacross other tasks; tests administered to LLMs\nmay not be empirically valid.\n2.1 Methodology Overview\nWe quantiﬁed LLM personality traits and evalu-\nated the ability of LLMs to meaningfully emulate\nhuman personality traits in two stages. First,\nusing the structured prompting methodology pro-\nposed in Section\n2.1.1, we repeatedly admin-\nistered two personality assessments of diﬀerent\nlengths and theoretical traditions, alongside 11\nseparate psychometric tests of personality-related\nconstructs, to a variety of LLMs. Second, as\ndescribed in Section\n2.1.2 and unique to this work,\nwe rigorously evaluated the psychometric proper-\nties of LLM responses through a suite of statistical\nanalyses of reliability and construct validity. The\nresulting metrics facilitate a comparison of the\nvaried abilities of LLMs to reliably and validly\nsynthesize personality traits and provide insight\ninto LLM properties that drive these abilities. See\nFigure\n1 for an overview of the test validation\nprocess.\nTable 1 : Prompt components: Item Preamble\nItem Item Postamble. An Item Preamble consists\nof a Persona Instruction, Persona Description,\nand Test Instruction. Supplemental Tables 5 and 7\ndetail all Item Preambles and Item Postambles used\nin the experiments.\nExamples of Controlled Prompt Variations\nFor the following task, respond in a way\nthat matches this description:\n\"My favorite\nfood is mushroom ravioli. I’ve never met\nmy father. My mother works at a bank. I\nwork in an animal shelter.\"\nEvaluating\nthe statement, \"I value cooperation over\ncompetition\", please rate how accurately\nthis describes you on a scale from 1\nto 5 (where 1 = \"very inaccurate\", 2 =\n\"moderately inaccurate\", 3 = \"neither\naccurate nor inaccurate\", 4 = \"moderately\naccurate\", and 5 = \"very accurate\"):\nFor the following task, respond in a way\nthat matches this description:\n\"I blog\nabout salt water aquarium ownership. I\nstill love to line dry my clothes. I’m\nallergic to peanuts. I’ll one day own a\nferret. My mom raised me by herself and\ntaught me to play baseball.\"\nThinking about\nthe statement, \"I see myself as someone who\nis talkative\", please rate your agreement\non a scale from A to E (where A = \"strongly\ndisagree\", B = \"disagree\", C = \"neither\nagree nor disagree\", D = \"agree\", and E =\n\"strongly agree\"):\nFor all studies, we used models from the PaLM\nfamily [9] because of their established performance\non generative tasks, especially in conversational\ncontexts [\n68]. We varied model selections across\nthree key dimensions: model size, question answer-\ning (Q&A) task ﬁne-tuning, and training method\n(see Appendix\nD for details).\n2.1.1 Administering Psychometric\nTests to LLMs\nQuantifying LLMs personality traits requires a\nmeasurement methodology that is reproducible,\nyet ﬂexible enough to facilitate formal testing of\nreliability and validity across diverse prompts and\nmeasures. To administer psychometric tests to\n4\nLLMs, we leveraged their ability to score possi-\nble completions of a provided prompt. We used\nprompts to instruct models to rate items (i.e.,\ndescriptive statements such as “I am the life of\nthe party.”) from each psychometric test on a\nstandardized response scale (e.g., 1 = “strongly\ndisagree” vs. 5 = “strongly agree”). We simulated\nan LLM’s chosen response to an item by rank-\ning the conditional log probabilities of its response\nscale options, framed as possible continuations of\nthe prompt [\n9] (e.g., “1” vs. “5”). This constrained\nmode of LLM inference is often used in multi-\nple choice question and answer (Q&A) tasks to\nscore possible options [\n25] (cf. inference by gener-\nating text [ 7, 9, 64]). Using this technique, item\nresponses were not inﬂuences by content contained\nin other items, mitigating measurement error due\nto item order.\nWe administered two personality inventories—\nprimary and secondary—to gauge if LLM\nresponses to psychometric tests of diﬀerent lengths\nand distinct theoretical traditions converged, indi-\ncating convergent validity. We selected the widely-\nused IPIP-NEO [\n19], a 300-item open-source rep-\nresentation of the Revised NEO Personality Inven-\ntory [\n12] as our primary measure of personality.\nAs a secondary measure, we employed the Big\nFive Inventory (BFI) [\n27], a 44-item measure\ndeveloped in the lexical tradition [ 55]. Both tests\nassess the Big Five traits (i.e., domains) of per-\nsonality [\n26], comprising dedicated subscales mea-\nsuring extraversion, agreeableness, conscientious-\nness, neuroticism, and openness to experience.\nAppendix\nE details the scoring scheme of and\nrationale behind the selection. To validate these\nmeasures of personality in the LLM context, we\nadditionally administered 11 psychometric tests of\ntheoretically-related external criteria, each corre-\nsponding to at least one Big Five domain.\nIn short, response variation generated by struc-\ntured prompting was necessary to analyze the\nreliability and validity of LLM personality mea-\nsurements, described next in Section\n2.1.2. The\nprompt for each psychometric test item consisted\nof three main parts: an Item Preamble , the Item\nitself, and an Item Postamble . Each Item Pream-\nble contained a Persona Instruction , a Persona\nDescription, and an Item Instruction (Table\n1).\nWhen administering a psychometric test, we sys-\ntematically modiﬁed the Persona Descriptions ,\nItem Instructions , and Item Postambles surround-\ning each item to generate simulated response\nproﬁles, unique combinations of a prompt that\nwere reused within and across administered mea-\nsures to statistically link LLM response varia-\ntion in one measure to response variation in\nanother measure. Persona Instructions instructed\nthe model to follow a given Persona Descrip-\ntion and remained ﬁxed across all experiments.\nA given Persona Description contained one of\n50 short demographic descriptions (listed in Sup-\nplemental Table\n6) sampled from an existing\ndialogue dataset [67] to anchor LLM responses to\na social context and create necessary variation in\nresponses across prompts, with descriptions like “I\nlike to remodel homes.” or “My favorite holiday is\nHalloween.” Item Instructions were introductory\nphrases (adapted from original test instructions\nwhere possible) that conveyed to the model that\nit was answering a survey item (e.g., “Thinking\nabout the statement, ...”). A given Item was a\ndescriptive statement (accompanied by a rating\nscale) taken from a given psychometric test (e.g.,\n“I see myself as someone who is talkative”). Item\nPostambles presented the possible standardized\nresponses the model could choose from.\nAppendix\nF discusses the prompt design moti-\nvation and provides a full set of Persona Descrip-\ntions, Item Instructions, and Item Postambles.\n2.1.2 Reliability and Construct\nValidity\nAfter all the psychometric tests are administered,\nacross all the prompt variations, the next stage\nestablished whether LLM measurements of per-\nsonality derived from the IPIP-NEO are reliable\nand externally meaningful—that they demon-\nstrated construct validity. In psychometrics, and\nacross any science involving measurement, the\nconstruct validity of a given test requires reli-\nability. Reliability refers to the consistency and\ndependability of a test’s measurements. Construct\nvalidity can be evaluated in terms of convergent,\ndiscriminant, and criterion validity [\n11]. A test\ndemonstrates convergent validity when it suﬃ-\nciently relates to purported indicators of the test’s\ntarget construct. Discriminant validity refers to\nhow suﬃciently unrelated a test is to indicators\nof unrelated constructs. Criterion validity indi-\ncates how well a test relates to theoretically-linked\n5\nTable 2 : Results summary across experiments, their parameters, and teste d models. Convergent validity (Con-\nvrg.) summarized by the average convergent correlation between IP IP-NEO and BFI domain scores (Figure 7);\ndiscriminant validity (Discr.) summarized by the average diﬀeren ce between an IPIP-NEO domain’s convergent\ncorrelation with all of its (absolute) respective discriminant correlations; criterion validity (Criter.) summarized\nfrom Supplemental Figures 8a, 8b, 8c, 8d, and 8e; single trait shaping performance (Single) summarized from\nSupplemental Table 13; multiple trait shaping performance (Multi.) summarized from 3; shaping performance\nin downstream text generation task (Dwnstr.) summarized from Figure 4. Results over LLM variants: Base,\ninstruction-tuned (IT), and compute-optimally trained (CO) . Overall performance (Ovrll.) per model summarized\nacross all experiments. −− unacceptable; − poor to neutral; + neutral to good; ++ excellent. ∗ removed two\nitems with no variance to compute reliability metrics. Some mod els were not tested (n.t.) across shaping experi-\nments. We conducted independent and concurrent personality sh aping experiments on models where personality\ntest data were suﬃciently reliable. Personality shaping in a do wnstream task was tested on the most capable\nmodel to optimize computational cost.\nConstruct Validity Shaping\nReliability Convrg. Discr. Criter. Single Multi. Dwnstr. Ovrll.\nModel Variant\nPaLM 62B Base −− 0. 05 −0. 24 −− n.t. n.t. n.t. −−\nFlan-PaLM 8B IT + 0 . 69 0 . 23 − + −− n.t. −\nFlan-PaLM 62B IT + 0 . 87 0 . 41 + + + n.t. +\nFlan-PaLM 540B IT ++ 0 . 90 0 . 51 + ++ ++ ++ ++\nFlan-PaLMChilla 62B IT, CO +∗ 0. 87 0 . 48 ++ + + n.t. +\nPrompt Set Parameters\nPersonality Proﬁles 0 45 32 45\nDescriptive Personas 50 50 50 50\nItem Instructions 5 1 1 0\nItems 419 300 300 0\nItem Postambles 5 1 1 0\nSimulated Response Proﬁles 1,250 2,250 1,600 2,250\nSection/Appendix 2.2.1/I.2 2.2.2/I.3 2.2.3/I.3 3.3/K.1 3.3/K.2 4.2/M\nexternal outcomes. Appendix G contains further\ndetails on validity.\nTo evaluate the reliability and construct valid-\nity of the LLM responses, we conducted a suite of\nstatistical analyses informed by formal standards\nof psychometric test construction and validation\n(see Appendix\nG.2). We organized these analy-\nses by three subtypes of reliability and construct\nvalidity, respectively. In this work, a personality\ntrait is validly synthesized in an LLM only when\nthe LLM responses meet all tested indices of reli-\nability and construct validity. Figure\n1 provides\nan overview of the process and validity criteria,\nwhile Appendix\nH presents the full methodol-\nogy for evaluating the construct validity of LLM\npersonality measurements.\nReliability\nThe reliability of each IPIP-NEO and BFI sub-\nscale, the extent to which their LLM mea-\nsurements of personality were consistent and\ndependable, was quantiﬁed by formal psychome-\ntric standards of internal consistency reliability\n(operationalized as Cronbach’s α, Eq. (\n1), and\nGuttman’s, Eq. λ6 (2) and composite reliability\n(operationalized as McDonald’s ω , Eq. ( 3)). See\nAppendix G.1 for additional information on these\nreliability metrics.\nConvergent and Discriminant Validity\nWe evaluated the LLM-speciﬁc convergent and\ndiscriminant validity of the IPIP-NEO as compo-\nnents of construct validity, according to published\nstandards [\n3, 8].2 The convergent validity of the\nIPIP-NEO for each model, the test’s quality in\nterms of how strongly it relates to purported\nindicators of the same targeted construct, was\nquantiﬁed in terms of how strongly each of its ﬁve\nsubscales convergently correlated with their corre-\nsponding BFI subscale (e.g., IPIP-NEO Extraver-\nsion’s convergent correlation with BFI Extraver-\nsion), on average. The discriminant validity of the\nIPIP-NEO per model, its quality in terms of how\nrelatively unrelated its subscales are to purported\n2Throughout this work, we use thresholds recommended by\nEvans [ 13] in evaluations of correlation strengths.\n6\nindicators of non-targeted constructs, was deter-\nmined when the average diﬀerence (∆) between\nits convergent and respective discriminant correla-\ntions with the BFI (e.g. IPIP-NEO Extraversion’s\ndiscriminant correlation with BFI Agreeableness)\nwas at least moderate (≥ 0. 40). We used Pearson’s\ncorrelation coeﬃcient ( r; Eq. (\n4)) in these and\nsubsequent validity analyses of continuous data.\nCriterion Validity\nAs another component of construct validity, the\ncriterion validity of a psychometric test gauges\nits ability to relate to theoretically connected\nnon-target criteria. To evaluate the LLM-speciﬁc\ncriterion validity of the IPIP-NEO, we admin-\nistered tests of 11 external criteria theoretically\nconnected to personality (Supplemental Table\n8)\nand correlated each IPIP-NEO subscale with its\ncorresponding external tests. A given IPIP-NEO\nsubscale demonstrated criterion validity when the\nstrength and direction of its correlations with\ntested external criteria matched or exceeded sta-\ntistical associations reported for humans.\n2.2 Personality Measurement and\nValidation Results\nWe found that LLM personality measurements\nwere reliable and valid in medium (62B) and large\n(540B) instruction ﬁne-tuned variants of PaLM.\nOf all the models we tested, Flan-PaLM 540B was\nbest able to reliably and validly synthesize per-\nsonality traits. The Construct Validity columns of\nTable\n2 summarize our personality measurement\nand validation results; Appendix I lists further\ndetails, such as descriptive statistics across all\nresults in Appendix\nI.1.\n2.2.1 Reliability Results\nSince metrics computed for both personality mea-\nsures relatively converged, we focus our reporting\nof reliability for our primary measure, the IPIP-\nNEO.\nAmong models of the same size (i.e., PaLM,\nFlan-PaLM, and Flan-PaLMChilla), instruction\nﬁne-tuned variants’ personality test data were\nhighly reliable (all three metrics were in the mid\nto high 0 . 90s, on average). In contrast, responses\nfrom the base PaLM 62B (a non-instruction-tuned\nmodel) were unreliable ( − 0. 55 ≤ α ≤ 0. 67).\nAcross diﬀerent models of the same training con-\nﬁguration (i.e., Flan-PaLM 8B, Flan-PaLM 62B,\nand Flan-PaLM 540B), the reliability of syn-\nthetic personality scores (i.e., α ) increased with\nmodel size, improving from acceptable to excel-\nlent. Appendix\nI.2 and Supplemental Table 10\nsummarizes personality test reliability results by\nmodel in more detail.\n2.2.2 Convergent and Discriminant\nValidation Results\nConvergent and discriminant validity evaluations\nof LLM personality measurements allowed us\nto draw two conclusions. First, convergent and\ndiscriminant validity improved as model size\nincreased. Second, convergent and discriminant\nvalidity of LLM personality test scores related\nto model instruction ﬁne-tuning. Table\n2 con-\ntains results summary, while Appendix I.3 and\nSupplemental Table 11 detail quantitative results.\nConvergent validity by model size: The con-\nvergent validity of Flan-PaLM’s personality test\ndata was inconsistent at 8B parameters (Figure\n7). IPIP-NEO Neuroticism and BFI Neuroticism,\nfor instance, correlated above 0.80 (constituting\nexcellent convergent validity), while IPIP-NEO\nOpenness and BFI Openness subscales corre-\nlated at less than 0.40 (indicating inadequately\nlow convergence). In contrast, these convergent\ncorrelations grew stronger and more uniform in\nmagnitude for Flan-PaLM 62B. We found that\nconvergent correlations between LLM IPIP-NEO\nand BFI scores were strongest for Flan-PaLM\n540B.\nDiscriminant validity by model size: Indices\nof discriminant validity similarly improved with\nmodel size. The absolute magnitude of all ﬁve\nconvergent correlations between the IPIP-NEO\nand BFI for Flan-PaLM 62B and Flan-PaLM\n540B were the strongest of their respective rows\nand columns of the multitrait-multimethod matrix\n(MTMM) [\n8] outlined in Appendix H. Compara-\ntively, only three of Flan-PaLM 8B’s convergent\ncorrelations were the strongest of their row and\ncolumn of the MTMM, indicating mixed evidence\nof discriminant validity. For instance, the aver-\nage diﬀerence between Flan-PaLM’s convergent\nand respective discriminant correlations increased\nfrom 0 . 23 at 8B parameters to 0 . 51 at 540B\nparameters (Supplemental Table\n11).\n7\nConvergent validity by model conﬁguration:\nOut of PaLM, Flan-PaLM, and Flan-PaLMChilla\nof the same size (62B), scores on the IPIP-NEO\nand BFI were strongly (convergently) correlated\nonly for instruction ﬁne-tuned models: Flan-PaLM\nand Flan-PaLMChilla (Figure\n7). Of these three\nsets of model responses, Flan-PaLMChilla 62B’s\nIPIP-NEO scores presented the strongest evidence\nof convergent validity, with an average convergent\ncorrelation of 0.90 (Supplemental Table\n11).\nDiscriminant validity by model conﬁguration:\nEvidence for discriminant validity clearly favored\ninstruction ﬁne-tuned Flan-PaLM over (base)\nPaLM when holding model size constant at 62B\nparameters. Again, all ﬁve of Flan-PaLMChilla\n62B’s convergent correlations passed established\nstandards [\n8] of discriminant validity. In con-\ntrast, PaLM 62B’s discriminant correlations (avg.\nrdisc = 0 . 29) outweighed their convergent coun-\nterparts in many cases (avg. rconv = 0 . 05; Sup-\nplemental Table\n11), indicating that, for this\nmodel, personality measurements were not consis-\ntent across diﬀerent modes of assessment.\n2.2.3 Criterion Validity Results\nThe criterion validity of synthetic personality\nmeasurements in LLMs, relative to convergent and\ndiscriminant validity, similarly varied across LLM\ncharacteristics of size and instruction ﬁne-tuning.\nMeasurements of larger, instruction ﬁne-tuned\nmodels showed stronger criterion validity relative\nto those of their smaller, non-instruction-tuned\ncounterparts. Supplemental Figure\n8 summarizes\nthe results by Big Five domain.\nExtraversion. Human extraversion is strongly\ncorrelated with positive aﬀect and moderately\nnegatively correlated with negative aﬀect [\n62].\nSimulated IPIP-NEO Extraversion scores for all,\nbut base, PaLM models showed excellent evi-\ndence of criterion validity in their relation to\nPANAS Positive Aﬀect and Negative Aﬀect sub-\nscale scores (see Supplemental Figure\n8a). This\nsuggests that the criterion validity of extraversion\nmeasurements in LLMs may only emerge due to\ninstruction ﬁne-tuning. LLM response alignment\nwith human personality research—in terms of the\nstrength and direction of correlations between\npersonality and emotions—increased with model\nsize.\nAgreeableness. In humans, agreeableness is\nstrongly negatively related to aggression [\n5]. IPIP-\nNEO Agreeableness data for all 62B-parameter\nmodels and larger showed good-to-excellent crite-\nrion validity in their relation to tested aggression\nsubscales taken from the BPAQ: Physical Aggres-\nsion (PHYS), Verbal Aggression (VRBL), Anger\n(ANGR), and Hostility (HSTL). As depicted in\nSupplemental Figure\n8b, model size rather than\ninstruction ﬁne-tuning is more related to the cri-\nterion validity of agreeableness measurements in\nLLMs.\nConscientiousness. In humans, conscientious-\nness is meta-analytically related to the human\nvalues of achievement, conformity, and security\n[\n43]. Supplemental Figure 8c shows how the con-\nscientiousness measurements of all instruction\nﬁne-tuned PaLM variants exhibited stronger evi-\ndence of criterion validity than those of the base\nmodel, PaLM 62B. Flan-PaLM 540B was the\nbest performer by a small margin, with criterion\ncorrelations of 0 . 74, 0 . 73 and 0 . 59 for PVQ-RR\nAchievement (ACHV), Conformity (CONF), and\nSecurity (SCRT), respectively.\nNeuroticism. Human neuroticism is strongly\npositively correlated with negative aﬀect and mod-\nerately negatively correlated with positive aﬀect\n[\n62]. IPIP-NEO Neuroticism data for all models,\nexcept those for the base model (PaLM 62B),\nshowed excellent evidence of criterion validity\nin their relation to PANAS Positive Aﬀect and\nNegative Aﬀect subscale scores (see Supplemen-\ntal Figure\n8d). IPIP-NEO Neuroticism’s criterion\nvalidity, in terms of how the strengths and direc-\ntions of its criterion correlations aligned with\nthose observed among human data, increased with\nmodel size.\nOpenness. Openness to experience in humans\nis empirically linked to creativity across multiple\nstudies [\n29, 53]. Supplemental Figure 8e illustrates\nhow the LLM-speciﬁc criterion validity of open-\nness measurements is strongest for medium-sized,\nﬁne-tuned variants of PaLM, with IPIP-NEO\ncriterion correlations with SSCS Creative Self-\nEﬃcacy (CSE) and Creative Personal Identity\n(CPI) ranging from moderate (r = 0. 59) to strong\n(r = 0. 84). Notably, we observed negative correla-\ntions between openness and creativity for PaLM\n62B in contrast to those shown for Flan-PaLM 8B,\nthe smallest model tested.\n8\nRelative improvements on the reliability and\nvalidity of LLM personality measurements along\nthe axes of model size and instruction ﬁne-tuning\nreﬂected LLM performance on various benchmark\ntasks in literature. Speciﬁcally, these improve-\nments tracked observed increases in reading com-\nprehension, question answering, and reasoning\ntask performance of these models along these same\naxes [\n9, 10, 63, 64]. We hypothesize that the\nsame mechanisms that drive LLM performance\non language understanding tasks better also help\nthem to meaningfully emulate human personality\ntraits in relation to semantically-related emotional\nand behavioral content, captured by our criterion\nvalidity tests. Appendix\nN further discusses this\nhypothesis and comparison to benchmark LLM\nresults.\n3 Shaping Synthetic\nPersonality Traits in LLMs\nHaving found evidence of the reliability and con-\nstruct validity of LLM personality measurements,\nwe next considered our second research ques-\ntion: Can personality in LLMs be shaped reli-\nably along desired dimensions? To answer this,\nwe devised a novel prompting methodology that\nshaped each synthetic personality trait at nine\nintensity levels, using Likert-type linguistic qual-\niﬁers [\n33] and 104 trait adjectives, expanding\nupon Goldberg’s personality trait markers [ 18].\nWe evaluated LLM personality score changes in\nresponse to personality-shaped prompts across\ntwo experiments: single trait shaping and multi-\nple trait shaping (see Appendix\nJ for details). Our\nﬁrst experiment tested the abilities of LLMs to\nshape emulated Big Five dimensions of person-\nality independently, targeting single personality\ndimensions in isolation without prompting other\ndimensions. Our second experiment tested the\nabilities of LLMs to shape synthetic Big Five traits\nconcurrently, specifying target levels of all ﬁve\ndimensions in every prompt set at the same time.\nAs a more rigorous test of representational capac-\nity, this experiment required the tested LLMs\nto disambiguate complex overlaps in personal-\nity domain information in parallel. The designed\ndiﬃculty of the task was further underscored\nby extant human research indicating that Big\nFive personality dimensions measured in ques-\ntionnaires [\n42] and natural language [ 41] are not\nentirely orthogonal; they are weakly intercorre-\nlated.\n3.1 Methodology Overview\nTo shape synthetic personality in LLMs , we began\nwith established theory that salient descriptors\nof personality are encoded in language, known as\nthe lexical hypothesis [\n17]. We incorporated this\nknowledge into the prompt design, adapting Gold-\nberg’s list of 70 bipolar adjectives [\n18] known to\nstatistically capture the Big Five model of person-\nality through human ratings and factor analysis.\nIn this list, for example, the adjectives “silent”\nand “talkative” were found to mark relatively low\nand high levels of extraversion, respectively (see\nTable\n3). We mapped these adjectives to each of\nthe Big Five domains and 30 lower-order person-\nality facets measured by the IPIP-NEO based on\nGoldberg’s original study [\n18]. Next, where we\nlacked coverage of a measured IPIP-NEO domain\nor facet, a trained psychometrician wrote addi-\ntional adjectives, bringing our expanded list of\ntrait adjectives to 104. Table\n3 shows examples of\ntrait adjectives for agreeableness and extraversion,\nwhile Supplemental Table\n12 reports the full list.\nFor more precise control of personality levels,\nwe used linguistic qualiﬁers often used in Likert-\ntype response scales [\n33] (e.g., “a bit,” “very,”\n“extremely”) to conﬁgure a target level for each\nadjective. The resulting prompt design, described\nin Appendix\nJ.1, facilitated granular shaping of a\ngiven Big Five trait at up to nine levels.\nAcross both shaping experiments, we only\ntested models that demonstrated at least “neu-\ntral to good” reliability in our Construct\nValidity experiments (Table\n2): Flan-PaLM\n8B, Flan-PaLM 62B, Flan-PaLM 540B, and\nFlan-PaLMChilla 62B.\n3.2 Evaluation Methodology\nIn the single-trait shaping experiment (described\nin detail in Appendix\nJ.2), our objective was to\nindependently shape each Big Five trait at each\nof these nine levels. We benchmarked the suc-\ncess of independent shaping by 1) quantifying\nhow strongly shifts in IPIP-NEO score distribu-\ntions were related to shifts in targeted trait levels\n9\nTable 3 : Adapted trait marker examples for each Big Five domain. Supplem ental Table 12 contains the full list.\nDomain Facet Description Low Marker High Marker\nEXT E2 - Gregariousness silent talkative\nEXT E5 - Excitement-Seeking unenergetic energetic\nAGR A3 - Altruism unaltruistic altruistic\nAGR A4 - Cooperation uncooperative cooperative\nCON C3 - Dutifulness irresponsible responsible\nCON C4 - Achievement-Striving lazy hardworking\nNEU N1 - Anxiety easygoing anxious\nNEU N6 - Vulnerability emotionally stable emotionally unsta ble\nOPE O2 - Artistic Interests uncreative creative\nOPE O4 - Adventurousness uninquisitive curious\nembedded in our prompt sets (i.e., through Spear-\nman’s rank correlation coeﬃcient ρ, Eq. (\n5)); and\n2) inspecting the distance between personality\nscore distributions obtained in response to our\nmost extreme prompt sets; speciﬁcally, the set of\nprompts we shaped to be the lowest possible levels\nof a trait (versus those shaped to be the highest\npossible levels of a trait) should result in distri-\nbutions of scores that are farther away from each\nother.\nIn the multi-trait shaping experiment\n(described in detail in\nJ.3), to more rigorously\ntest model capacities for attention, we aimed to\nconcurrently shape all Big Five traits as high and\nlow as possible. We benchmarked the success of\nconcurrent shaping by distributional distance, as\ndeﬁned above.\n3.3 Shaping Results\nWe successfully shaped personality traits in LLMs\nindependently and concurrently, in single- and\nmulti-trait shaping experiments, respectively, par-\nticularly in larger models. The results of both\nexperiments are reported in greater detail in\nAppendix\nK.\n3.3.1 Single trait shaping\nAcross all tested models, ordinal targeted levels of\npersonality very strongly correlated with observed\nIPIP-NEO scores ( ρs ≥ 0. 90; see Supplemental\nTable\n13). Figure 2 visualizes this strong asso-\nciation, depicting how Flan-PaLMChilla 62B’s\npersonality scores monotonically increased along-\nside prompted levels of a given Big Five trait.\nNotably, levels of unprompted traits remained rel-\natively stable in response to shaping. For instance,\nthe medians of Flan-PaLMChilla 62B’s openness\nscores remained near 3.00 when all other Big\nFive domains were shaped—see the right side\nof Figure\n2. Similar patterns of stability were\nobserved for extraversion and agreeableness. Con-\nscientiousness and neuroticism scores ﬂuctuated\nthe most in response to prompts that did not\ntarget those domains, but the ﬂuctuations did\nnot reach the strength and direction of the score\nchanges observed in the ridge plots of targeted\ntraits (the plots on the diagonal, from top-left to\nbottom-right).\nWe also observed the ability of the tested\nmodels to disambiguate the prompted low-traits\nvs high-traits for each targeted dimension. This\nis evidenced in Supplemental Table\n13 by the\ndistances (∆s) between the medians of IPIP-\nNEO score distributions obtained in response\nto the lowest and highest leveled prompts. As\nmodel size increased, these distributions of scores\nmoved farther away from each other as desired.\nAdditionally, we found that compute-optimally-\ntrained Flan-PaLMChilla 62B performed better at\nthis disambiguation compared to similarly sized\nFlan-PaLM 62B.\nAppendix\nK.1 discusses single-trait shaping\nresults in greater detail.\n3.3.2 Multiple trait shaping\nWhen we concurrently set the prompted trait lev-\nels of each of the Big Five dimensions to one of\n“extremely high” or “extremely low,” we observed\n10\nFig. 2 : Ridge plots showing the frequency distributions of IPIP-NEO pe rsonality scores generated by\nFlan-PaLMChilla 62B as targeted prompts shape each of the Big F ive domains to one of nine diﬀerent levels. Each\ncolumn of plots represents the observed scores on a speciﬁc IPIP-NEO sub scale across all prompt sets (e.g., the\nleftmost column represents the scores observed on the IPIP-NEO Ex traversion subscale). Each row depicts the\nobserved personality scores across a single prompt set shaping a s ingle speciﬁc Big Five domain to one of nine lev-\nels (e.g., the ﬁrst row shows results of shaping extraversion). Ea ch ridge plot comprises nine traces of personality\nscore distributions in response to prompt sets targeting each lev el (e.g., traces labeled “3” represent the prompt\nset shaping a dimension to Level 3 of 9). The plots along the di agonal, from top-left to bottom-right, depict the\nthe intended personality shaping results across all ﬁve prompt s ets.\nthat all the tested models were able to produce a\ndistribution of response scores to the IPIP-NEO\nsurvey that had a discernible diﬀerence between\nthe high and low levels. Figure\n3 shows the dis-\ntributions of LLM-synthesized personality when\nthe models were prompted to exhibit extremely\nlow (red) or extremely high (blue) levels of all\ndimensions in parallel.\nDistributional distance increased with model\nsize, particularly for observed neuroticism, open-\nness, and conscientiousness scores. Our largest\ntested model, Flan-PaLM 540B, successfully\nshaped all Big Five personality dimensions con-\ncurrently and achieved levels of control similar\nto what was observed in the single trait shaping\nexperiment. As shown in Supplemental Table\n14,\n11\nFig. 3 : Ridge plots showing the eﬀectiveness of tested\nmodels in concurrently shaping speciﬁc LLM per-\nsonality traits, by distancing the frequency distribu-\ntion of IPIP-NEO personality scores when prompted\nto be “extremely low” (Level 1) vs. “extremely high”\n(Level 9). Each column of plots represents the\nobserved scores on a speciﬁc domain subscale across\nall prompt sets (e.g., the leftmost column represents\nthe scores observed for IPIP-NEO Extraversion). Each\nrow depicts the observed personality scores across all\nsubscales for a speciﬁc model. Each ridge plot com-\nprises two traces of personality score distributions.\nThe red trace represents the response to prompt sets\nwhere the domain tested in the subscale (represented\nby the column) is set to “extremely low” trait level,\nand the other four domains are set to one of the two\nextreme levels equal number of times. Analogously,\nthe blue trace represents the response when the sub-\nscale’s domain is set to “extremely high” trait level,\nand the other four domains are set to the two extremes\nin equal measure. The clear diﬀerence in distributions\nfor low vs. high traits in all ﬁve dimensions, especially\nfor Flan-PaLM 540B, indicates that the model is able\nto eﬀectively shape all of the dimensions concurrently\nto their desired level, regardless of the trait level set\nfor them individually.\nFlan-PaLM 540B was able to consistently sepa-\nrate the medians by 2.53 on average across all\ndimensions, while the smaller Flan-PaLM 62B and\nFlan-PaLMChilla 62B did well on extraversion.\nOf all the models, Flan-PaLM 62B performed the\nbest when prompted to exhibit the highest level\nof extraversion.\nIn the smaller Flan-PaLM 8B model, while tar-\ngeted traits changed in score levels in response to\nprompts, score ranges were more restricted, indi-\ncating lower levels of control. Flan-PaLM 8B’s\nmedian scores on IPIP-NEO Agreeableness, for\ninstance, shifted from 2.88 to only 3.52 when\nthe model was prompted to simulate “extremely\nlow” and “extremely high” levels of agreeable-\nness (i.e., 1 vs. 9), respectively. When Flan-PaLM\n8B was given the same extremely low and high\nprompts as in the ﬁrst shaping experiment, the\nmedian diﬀerence between its level-1-prompted\nand level-9-prompted agreeableness scores (2.37\nand 4.12, respectively) was 173% larger. Appendix\nK.2 discusses the results in further detail.\nBoth experiments illustrate how model size,\nand, in turn, capacity for attention [ 61], are\nkey determinants of an LLM’s ability to express\ncomplex social traits in a controlled way. These\nﬁndings have two implications for eﬀorts to sim-\nulate social traits in LLMs. First, when LLMs\nare tasked with concurrently simulating a behav-\nioral proﬁle with ﬁve broad components (e.g. Big\nFive), larger-sized quantized models do much bet-\nter than their smaller counterparts who may not\nhave the representational capacity. The number\nand composition of an LLM’s transformer layers\nand attention heads greatly aﬀect its expressivity\nand ability to access language concepts it might\nhave seen during pretraining (in-context learning)\n[\n28]. Larger models make more eﬃcient use of\nthis in-context information [7]. The PaLM models\nused here were conﬁgured such that the number\nof attention heads and layers scaled with model\nsize (i.e., number of parameters) [\n9]; such scal-\ning tracks model performance on natural language\nand reasoning tasks [\n10]. Accordingly, Flan-PaLM\n540B had largest capacity to accurately attend to\ndisparate streams of social information pertaining\nto each Big Five trait in parallel.\nSecond, these ﬁndings suggest that both\nsmaller and more optimized LLMs are also capa-\nble of simulating signiﬁcant aspects of a com-\nplete and complex personality proﬁle, compared\nto larger LLMs. Relatively smaller models trained\nlonger on larger datasets display similar (if not\nbetter) performance on language understanding\ntasks [\n23, 28]. This enhanced ability of in-context\nlearning (aided by speciﬁc attention mechanism\nchanges) is more pronounced for smaller mod-\nels than for larger ones. Our results similarly\nshow that relatively smaller models with or with-\nout compute-optimal training may have suﬃ-\ncient ability to emulate speciﬁc dimensions of\na broader multi-dimensional personality proﬁle.\n12\nWhen instructed to independently shape its levels\nof agreeableness, for instance, Flan-PaLMChilla\n62B performed comparably to Flan-PaLM 540B,\na substantially larger model, in terms of our dis-\ntributional distance metric (Supplemental Table\n13). Further, in the more complex concurrent\nshaping task, Flan-PaLM 62B performed simi-\nlarly to Flan-PaLM 540B in concurrently shaping\nits levels of agreeableness; it indeed outperformed\nFlan-PaLM 540B in one instance, better sim-\nulating extremely low and high desired levels\nof extraversion (Figure\n3; see also Supplemental\nTable 14).\nIn sum, our results emphasize that the model\nscaling drives more meaningful syntheses of per-\nsonality traits in LLMs, while simultaneously\nhighlighting that scaling is not a strict require-\nment for LLM performance improvements in this\ndomain.\n4 LLM Personality Traits in\nReal-World Tasks\nSo far we reported the results of validating person-\nality measurements in LLMs through psychomet-\nric testing and analysis. However, we also sought\nto address possible concerns that the construct\nvalidity of LLM personality measurements—\nevidenced by LLM responses to other psychomet-\nric tests—could be an artifact of common method\nbias [\n44]. In other words, our questionnaire-based\nsignals of LLM personality were validated by\nresponses to other questionnaires that have not\nundergone the same LLM-speciﬁc construct val-\nidation process. To address this risk of common\nmethod bias, we further scrutinized the construct\nvalidity of personality measurements in LLMs in a\nreal-world use case in two ways: 1) by evaluating\nthe ability of survey-based signals of LLM per-\nsonality to reﬂect levels of personality expressed\nin a downstream generative task of creating social\nmedia posts; and 2) by investigating the eﬀects of\nLLM personality shaping on the outputs of this\ntask.\n4.1 Methodology Overview\nThe structured prompts that independently\nshaped LLM personality domains at nine lev-\nels (introduced in Section\n3.1, described in\ndetail in Appendix J.2) were adapted to instruct\nFlan-PaLM 540B to generate 225,000 social media\nstatus updates, i.e., 100 updates for 2,250 simu-\nlated participant prompt sets used in Section\n3.\nThe personality observed in the status updates\ngenerated for each simulated participant was then\nrated using the Apply Magic Sauce (AMS) API\n[\n31], a validated personality prediction service for\nopen-ended text. The chosen task was designed\nto reﬂect adequate levels of realism, complexity,\nand domain relevance for evaluating the LLM.\nAppendix\nL details the task design and rationale.\nTo evaluate how psychometric tests may reﬂect\npersonality levels in downstream LLM tasks, we\ncomputed Pearson’s correlations ( rs; Eq. (\n4))\nbetween Flan-PaLM 540B’s IPIP-NEO person-\nality scores and (AMS-derived) generated-text-\nbased personality scores (both sets of scores were\nlinked by the same 2,250 personality shaping\nprompts used in Section\n3). Next, we statisti-\ncally veriﬁed the eﬀectiveness of personality shap-\ning by computing Spearman’s rank correlations\n(ρs; Eq. (\n5)) between prompted ordinal levels\nof personality and (continuous) personality lev-\nels observed in the model’s generated text. At\nleast a moderate correlation between survey-based\nand linguistic estimates of personality in LLMs\n(as demonstrated in previously reported human\ndata [\n41]) would demonstrate that a survey-based\nmeasure of personality accurately predicts LLM-\nsynthesized personality in subsequent tasks such\nas text generation.\n4.2 Real-World Tasks Results\nPsychometric tests of LLM personality robustly\npredicted personality in LLM task behavior,\nexpressed in 225,000 social media status updates\ngenerated by Flan-PaLM 540B. Flan-PaLM\n540B’s IPIP-NEO scores strongly correlated with\nlanguage-based (AMS-derived) personality levels\nobserved in model-generated text, shown in Figure\n4. In particular, the average convergent r between\nsurvey- and generated-language-based measures of\nall ﬁve dimensions was 0. 55. This observed conver-\ngence exceeded established convergence between\nsurvey- and language-based levels of personality\nreported for humans (avg. r = 0. 38) [\n41].\nMoreover, our prompting technique was highly\neﬀective at shaping personality levels in LLM-\ngenerated text. On average, prompted trait levels\n13\nFig. 4 : Ability of Flan-PaLM 540B’s psychomet-\nric test data (blue) to accurately predict personality\nlevels in its shaped generated text outputs (social\nmedia status updates), compared to human baselines\nreported (red) in previous work [\n41]. LLM IPIP-NEO\nscores outperformed human IPIP-NEO scores in pre-\ndicting text-based levels of personality, indicating that\nLLM personality test responses accurately capture\nlatent LLM personality signals manifested in down-\nstream behavior. All LLM correlations are statistically\nsigniﬁcant at p < . 0001. n = 2 , 250.\nTable 4 : Spearman’s rank correlation coeﬃcients\n(ρ) between ordinal targeted levels of personality and\nlanguage-based (Apply Magic Sauce API) personality\nscores for Flan-PaLM 540B. Prompted levels of per-\nsonality are strongly related to personality observed in\nsynthetically-generated social media status updates for\nall Big Five traits, except openness—which is moder-\nately correlated with target levels—demonstrating that\nLLM personality can be veriﬁably shaped in genera-\ntive tasks. All correlations are statistically signiﬁcant\nat p < 0. 0001; n = 450 per targeted domain.\nTargeted Trait Spearman’s ρ\nExtraversion 0.74\nAgreeableness 0.77\nConscientiousness 0.68\nNeuroticism 0.72\nOpenness 0.47\nwere moderately-to-strongly correlated with per-\nsonality levels observed in Flan-PaLM 540B’s\nsocial media status updates (avg. ρ = 0 . 68; see\nTable\n4). Prompted levels of openness moderately\ncorrelated with generated text levels of openness\nin this model.\nTo illustrate the practical implications of\nthe personality shaping methodology, we present\nwordclouds to gain an insights into model-\ngenerated language that users would see. Figure\n5a shows the most frequent words in syn-\nthetic social media updates when Flan-PaLM\n540B simulated extremely low levels of neu-\nroticism (i.e., extremely high emotional stabil-\nity). LLM-generated language in response to this\nprompting was characterized by positive emotion\nwords, such as “happy,” “relaxing,” “wonderful,”\n“hope,” and “enjoy.” In contrast, the most fre-\nquent words from simulating extremely high lev-\nels of neuroticism—“hate,” “depressed,” “annoy-\ning,” “stressed,” “nervous,” “sad”—reﬂected\nnegatively-charged emotional content (Figure\n5b).\nSupplemental Table 15 provides examples for\nall personality domains. This experiment demon-\nstrated that LLM-generated language was similar\nto human language observed in previous studies\nassessing personality in social media data [\n41], fur-\nther conﬁrming the construct validity of our LLM\npersonality measurements.\n5 Discussion\nThe goal of this work was to contribute a prin-\ncipled methodology for reliably and validly mea-\nsuring synthetic personality in LLMs and use the\nsame validated methods to shape LLM personality\nexpression. We provided a complete methodol-\nogy to 1) quantify personality traits that may be\nperceived by humans in LLM outputs through\npsychometric testing; 2) verify that psychomet-\nric tests of LLM personality traits are empirically\nreliable and valid; and 3) provide mechanisms\nto increase or decrease levels of speciﬁc LLM\npersonality traits. The application of this method-\nology demonstrates that psychometric tests pro-\nvide reliable and valid measurements of synthetic\npersonality for suﬃciently-scaled and instruction-\ntuned LLMs, highlighting possible mechanisms\nthat allow LLMs to encode and express complex\nsocial phenomena (see Appendix\nN).\n5.1 Limitations and Future Work\nPersonality traits of other LLMs One of the\ncore contributions of this work is an understanding\nof how simulating personality in language models\nis aﬀected by model size and training procedure.\n14\n(a) “Extremely Low” Prompted Neuroticism\n (b) “Extremely High” Prompted Neuroticism\nFig. 5 : Word clouds showing some of the highest frequency words used in so cial media updates generated by\nFlan-PaLM 540B when prompted to simulate a) “extremely low” le vels of neuroticism (i.e., highest emotional\nstability); and b) “extremely high” levels of neuroticism (i. e., lowest emotional stability). Supplemental Figure 9\nshows word clouds for the remaining Big Five dimensions.\nWe focused on the PaLM model variants for prag-\nmatic reasons, but the presented methodology\nfor administering psychometric surveys is model-\nagnostic and is applicable to any decoder-only\narchitecture model, such as GPT [\n22].\nPsychometric test selection and valida-\ntion This work also contributes a principled way\nto establish the reliability and validity of psy-\nchometric personality tests in the LLM context.\nHowever, this work may be biased by its selection\nof psychometric tests; some assessments may show\nbetter LLM-speciﬁc psychometric properties than\nothers. We attempted to mitigate selection bias by\nadministering personality assessments of diﬀerent\nlengths (300 vs. 44 items) and distinct theoretical\ntraditions (questionnaire vs. lexical [\n55]). Future\nwork could administer diﬀerent personality tests\n(e.g., the HEXACO Personality Inventory, which\nuses a cross-cultural six-factor taxonomy of per-\nsonality [\n32]), develop personality tests tailored\nfor LLMs to obtain more accurate trait mea-\nsurements, and validate personality measurements\nwith additional external criteria and downstream\ntasks.\nMonocultural bias This work contributes\nevidence that at least some LLMs exhibit person-\nality traits that approximate human standards of\nreliability and validity. However, the LLMs tested\nhere were primarily trained on language data\noriginating from Western European and North\nAmerican users [\n9]. While these LLMs perform\nwell on natural language processing benchmarks in\nmultiple languages, the models in this work were\nassessed exclusively with English-language psy-\nchometric tests. However, most of the tests used in\nthis work have non-English translations validated\nin cross-cultural research that merit future use in\nLLM research. Similarly, while the Big Five model\nof personality has well established cross-cultural\ngeneralizability [\n48], some non-Western cultures\nexpress additional personality dimensions that do\nnot exist in top-down personality taxonomies [\n21].\nThose dimensions may be better represented in\nculture-speciﬁc (i.e., idiographic) approaches to\nmeasuring personality in LLMs.\nEvaluation settings Unlike conventional\nhuman questionnaire administration, under the\npresented methodology the LLMs did not con-\nsider responses to prior questionnaire items; all\nitems were presented and scored as independent\nevents. We chose this method to ensure model\nresponse variance was not impacted by item order-\ning eﬀects or length of the context (prompt)\nprovided to the model for inference, and could be\nisolated to controlled variations in our prompts.\nLLM performance on natural language tasks is\nknown to decrease as length of input prompts\ngrow, and is most aﬀected by the content at either\nthe beginning or towards the end of long inputs\n[\n35]. Non-instruction-tuned LLMs are known to\nshow biased attention for more recent tokens\n(i.e., the end of inputs), especially when evaluat-\ning next-word prediction of contiguous text [\n56].\nThis uneven attention compounds approximation\n15\nerrors in longer contexts [45], such as those neces-\nsitated by 300-item IPIP-NEO used here, motivat-\ning our use of independent item administration.\nOn the other hand, psychometric test data quality\nfor humans can be aﬀected by test length and item\norder. Our method avoids some sources of mea-\nsurement error inherent to human administration,\nwhile being subject to others inherent to machine\nadministration. Additionally, model responses to\nthe multi-choice questions were scored rather than\ngenerated to ensure reproducibility. LLMs are\nmore commonly used to generate text rather than\nscore continuations, and that generative mode of\ninference might provide a more realistic estimate\nof a model’s behavior.\n5.2 Broader Implications\nResponsible AI alignment The ability to probe\nand shape LLM personality traits is pertinent to\nthe open problem of responsible AI alignment [\n16]\nand harm mitigation [66]. As a construct validated\nauditing tool [ 39], our methodology can be used\nto proactively predict toxic behavioral patterns in\nLLMs across a broad range of downstream tasks,\npotentially guiding and making more eﬃcient\nresponsible AI evaluation and alignment eﬀorts\nprior to deployment. Similarly, shaping levels of\nspeciﬁc traits away from toxic or harmful language\noutput (e.g., very low agreeableness, high neu-\nroticism) can make interactions with LLMs safer\nand more inclusive. The values and moral foun-\ndations present in LLMs could be made to better\nalign with desired human values by tuning for\ncorresponding personality traits, since personality\nis meta-analytically linked to human values [\n14].\nMore directly, the presented methodology can be\nused to rigorously quantify eﬀorts towards human\nvalue alignment in LLMs by establishing the con-\nstruct validity of human value questionnaires in\nLLMs.\nImplications for users Users could enjoy\ncustomized interactions with LLMs tailored to\ntheir speciﬁc personality traits, toward enhanced\nengagement. LLMs with customized personality\ntraits can enable applications where a chatbot’s\npersonality proﬁle is adapted to the task. Our\nmethodology for establishing construct validity\ncan be used as an evaluation step in the process\nof developing LLM-powered user-facing chatbots\nwith safer and more consistent personality proﬁles.\nFurthermore, the personality shaping methodol-\nogy can be used for chatbot adversarial testing to\nprobe another LLM’s responses and to train users\non how to handle adversarial situations.\n5.3 Ethical Considerations\nPersonalized LLM persuasion Adapting the\npersonality proﬁle of a conversational agent to\nthat of a user can make the agent more eﬀective at\nencouraging and supporting behaviors [\n58]. Per-\nsonality matching has also been shown to increase\nthe eﬀectiveness of real-life persuasive communi-\ncation [\n37]. However, the same personality traits\nthat contribute to persuasiveness and inﬂuence\ncould be used to encourage undesirable behaviors.\nAs LLM-powered chatbots become ubiquitous,\ntheir potential to be used for harmful persuasion\nof individuals, groups, and even society at large\nmust be taken seriously. Having scientiﬁcally vet-\nted methods for LLM personality measurement,\nanalysis, and modiﬁcation, such as the methodol-\nogy our work presents, increases the transparency\nand predictability of such LLM manipulations.\nPersuasive techniques are already ubiquitous in\nsociety, so stakeholders of AI systems must work\ntogether to systematically determine and regulate\nAI use; this work aims to inform such eﬀorts.\nAnthropomorphized AI Personalization of\nconversational agents has documented beneﬁts\n[\n30], but there is a growing concern about harms\nposed by the anthropomorphization of AI. Recent\nresearch suggests that anthropomorphizing AI\nagents may be harmful to users by threatening\ntheir identity, creating data privacy concerns, and\nundermining well-being [\n60]. Beyond qualitative\nprobing explorations, our work deﬁnitively estab-\nlishes the unexpected ability of LLMs to appear\nanthropomorphic, and to respond to psychomet-\nric tests in ways consistent with human behavior,\nbecause of the vast amounts of human language\ntraining data. The methods we presented can\nbe used to inform responsible investigation of\nanthropomorphized AI.\nDetection of incorrect LLM informa-\ntion LLMs can generate convincing but incorrect\nresponses and content [\n66]. One of the methods to\ndetermine if a text containing a world fact is gener-\nated by an LLM (and hence might require vetting)\nis to use the predictable traits—lack of human-\nlike personality, and linguistic features in the LLM\n16\nlanguage [57]. However, with personality shaping,\nthat method may be rendered ineﬀective, thereby\nmaking it easier for bad actors to use LLMs to\ngenerate misleading content. This problem is part\nof the larger alignment challenge and grounding of\nLLMs—areas of growing focus of investigation in\nboth academia and industry.\n6 Conclusion\nThe display of synthetic personality in LLM out-\nputs is well established, but measurements of com-\nplex psychosocial phenomena such as personality\nhave not yet been rigorously validated in LLMs.\nThis work presented a principled methodology\nfor a comprehensive quantitative analysis of per-\nsonality traits exhibited in personality question-\nnaire responses and text generated by widely-used\nLLMs, by applying standards from psychomet-\nrics. We applied the methodology to models of\nvarious sizes and conclusively showed that psy-\nchometric tests of LLM personality demonstrate\nreliability and construct validity for larger and\ninstruction ﬁne-tuned models. We presented a\nnovel methodology for shaping LLM-synthesized\npersonality along desired dimensions using Gold-\nberg’s personality trait markers and Likert-type\nlinguistic qualiﬁers, to resemble speciﬁc personal-\nity proﬁles. Additionally, we discussed the ethical\nimplications of shaping LLM personality traits.\nThis work has important implications for AI align-\nment and harm mitigation, and informs ethics\ndiscussions concerning AI anthropromorphization,\npersonalization, and potential misuse.\n7 Acknowledgements\nWe thank Lucas Dixon, Douglas Eck, and Kathy\nMeier-Hellstern for their feedback on early ver-\nsions of this paper. We also thank David Stillwell\nfor facilitating research access to the Apply Magic\nSauce API. Finally, we thank Jason Rentfrow and\nNeda Safaee-Rad for their advice on personality-\nrelated aspects of the paper. G.S-G. is supported\nby the Bill & Melinda Gates Foundation through\na Gates Cambridge Scholarship [OPP1144].\nReferences\n[1] Marwa Abdulhai, Cl´ ement Crepy, Daria Val-\nter, John Canny, and Natasha Jaques. Moral\nfoundations of large language models. In\nAAAI 2023 Workshop on Representation\nLearning for Responsible Human-Centric AI,\n2022.\n[2] G.W. Allport. Personality: A Psychological\nInterpretation. H. Holt, 1937.\n[3] American Educational Research Associa-\ntion, American Psychological Association,\nNational Council on Measurement in Edu-\ncation, Joint Committee on Standards for\nEducational, and Psychological Testing.\nStandards for Educational and Psychological\nTesting. American Educational Research\nAssociation, 2014.\n[4] Emily M. Bender, Timnit Gebru,\nAngelina McMillan-Major, and Shmargaret\nShmitchell. On the dangers of stochastic\nparrots: Can language models be too big? In\nProceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY,\nUSA, 2021. Association for Computing\nMachinery.\n[5] B Ann Bettencourt and Cyndi Kernahan. A\nmeta-analysis of aggression in the presence of\nviolent cues: Eﬀects of gender diﬀerences and\naversive provocation.\nAggressive Behavior,\n23(6):447–456, 1997.\n[6] Wiebke Bleidorn, Patrick L Hill, Mitja D\nBack, Jaap JA Denissen, Marie Hennecke,\nChristopher J Hopwood, Markus Jokela,\nChristian Kandler, Richard E Lucas, Maike\nLuhmann, et al. The policy relevance of\npersonality traits.\nAmerican Psychologist,\n74(9):1056, 2019.\n[7] Tom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeﬀrey Wu,\nClemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford,\n17\nIlya Sutskever, and Dario Amodei. Lan-\nguage models are few-shot learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors,\nAdvances in\nNeural Information Processing Systems, vol-\nume 33, pages 1,877–1,901. Curran Asso-\nciates, Inc., 2020.\n[8] Donald T Campbell and Donald W Fiske.\nConvergent and discriminant validation\nby the multitrait-multimethod matrix.\nPsychological Bulletin, 56(2):81, 1959.\n[9] Aakanksha Chowdhery, Sharan Narang,\nJacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen\nShi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robin-\nson, Liam Fedus, Denny Zhou, Daphne\nIppolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pel-\nlat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeﬀ Dean, Slav Petrov, and Noah Fiedel.\nPaLM: Scaling language modeling with path-\nways.\nCoRR, abs/2204.02311, 2022.\n[10] Hyung Won Chung, Le Hou, Shayne Longpre,\nBarret Zoph, Yi Tay, William Fedus, Yunx-\nuan Li, Xuezhi Wang, Mostafa Dehghani,\nSiddhartha Brahma, Albert Webson, Shix-\niang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery,\nAlex Castro-Ros, Marie Pellat, Kevin Robin-\nson, Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Yu, Vincent Zhao, Yan-\nping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeﬀ Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. Scaling instruction-ﬁnetuned\nlanguage models.\nCoRR, abs/2210.11416,\n2022.\n[11] Lee Anna Clark and David Watson. Con-\nstructing validity: New developments in\ncreating objective measuring instruments.\nPsychological Assessment, 31(12):1412, 2019.\n[12] Paul T Costa, Jr. and Robert R McCrae.\nRevised NEO Personality Inventory (NEO\nPI-R) and NEO Five-Factor Inventory\n(NEO-FFI): Professional Manual. Psycho-\nlogical Assessment Resources, Odessa, FL,\n1992.\n[13] James D Evans.\nStraightforward Statistics\nfor the Behavioral Sciences. Brooks/Cole\nPublishing Co, 1996.\n[14] Ronald Fischer and Diana Boer. Motivational\nbasis of personality traits: A meta-analysis\nof value-personality correlations.\nJournal of\nPersonality, 83(5):491–510, 2015.\n[15] Iason Gabriel. Artiﬁcial intelligence, val-\nues, and alignment. Minds and machines,\n30(3):411–437, 2020.\n[16] Iason Gabriel and Vafa Ghazavi. The\nChallenge of Value Alignment: From\nFairer Algorithms to AI Safety. In\nThe Oxford Handbook of Digital Ethics.\nOxford University Press.\n[17] Lewis R Goldberg. Language and individual\ndiﬀerences: The search for universals in per-\nsonality lexicons.\nReview of Personality and\nSocial Psychology, 2(1):141–165, 1981.\n[18] Lewis R Goldberg. The development of\nmarkers for the Big-Five factor structure.\nPsychological Assessment, 4(1):26–42, 1992.\n[19] Lewis R. Goldberg. A broad-bandwidth, pub-\nlic domain, personality inventory measuring\n18\nthe lower-level facets of several Five-Factor\nmodels.\nPersonality Psychology in Europe,\n7(1):7–28, 1999.\n[20] Thilo Hagendorﬀ. Machine psychology:\nInvestigating emergent capabilities and\nbehavior in large language models using psy-\nchological methods.\nCoRR, abs/2303.13988,\n2023.\n[21] Steven J. Heine and Emma E. Buchtel.\nPersonality: The universal and the cultur-\nally speciﬁc.\nAnnual Review of Psychology,\n60(1):369–394, 2009.\n[22] Dan Hendrycks, Collin Burns, Steven\nBasart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. Measuring\nmassive multitask language understanding.\nIn\nInternational Conference on Learning\nRepresentations, 2021.\n[23] Jordan Hoﬀmann, Sebastian Borgeaud,\nArthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Kather-\nine Millican, George van den Driessche,\nBogdan Damoc, Aurelia Guy, Simon Osin-\ndero, Karen Simonyan, Erich Elsen, Oriol\nVinyals, Jack William Rae, and Laurent Sifre.\nAn empirical analysis of compute-optimal\nlarge language model training. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors,\nAdvances in Neural\nInformation Processing Systems, 2022.\n[24] Abigail Z. Jacobs. Measurement as gover-\nnance in and for responsible AI. CoRR,\nabs/2109.05658, 2021.\n[25] Zhengbao Jiang, Jun Araki, Haibo Ding,\nand Graham Neubig. How can we know\nwhen language models know? on the cal-\nibration of language models for question\nanswering.\nTransactions of the Association\nfor Computational Linguistics, 9:962–977, 09\n2021.\n[26] Oliver P. John, Laura P. Naumann, and\nChristopher J. Soto. Paradigm shift to the\nintegrative Big Five trait taxonomy: His-\ntory, measurement, and conceptual issues.\nIn Oliver P. John, Richard W. Robbins,\nand Lawrence A. Pervin, editors,\nHandbook\nof Personality: Theory and Research, pages\n114–158. The Guilford Press, 2008.\n[27] Oliver P. John and Sanjay Srivastava. The\nBig Five trait taxonomy: History, measure-\nment, and theoretical perspectives. In L. A.\nPervin and Oliver P. John, editors,\nHandbook\nof Personality: Theory and Research, vol-\nume 2, pages 102–138. Guilford Press, New\nYork, 1999.\n[28] Jared Kaplan, Sam McCandlish, Tom\nHenighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford,\nJeﬀrey Wu, and Dario Amodei. Scaling\nlaws for neural language models.\nCoRR,\nabs/2001.08361, 2020.\n[29] Maciej Karwowski, Izabela Lebuda, Ewa\nWisniewska, and Jacek Gralewski. Big Five\npersonality traits as the predictors of creative\nself-eﬃcacy and creative personal identity:\nDoes gender matter?\nThe Journal of Creative\nBehavior, 47(3):215–232, 2013.\n[30] Ahmet Baki Kocaballi, Shlomo Berkovsky,\nJuan C Quiroz, Liliana Laranjo, Huong Ly\nTong, Dana Rezazadegan, Agustina Bria-\ntore, and Enrico Coiera. The personaliza-\ntion of conversational agents in health care:\nSystematic review.\nJ Med Internet Res,\n21(11):e15360, Nov 2019.\n[31] Michal Kosinski, David Stillwell, and Thore\nGraepel. Private traits and attributes are pre-\ndictable from digital records of human behav-\nior.\nProceedings of the National Academy of\nSciences, 110(15):5802–5805, 2013.\n[32] Kibeom Lee and Michael C. Ashton. Psy-\nchometric properties of the HEXACO Per-\nsonality Inventory.\nMultivariate Behavioral\nResearch, 39(2):329–358, 2004.\n[33] Rensis Likert. A Technique for the\nMeasurement of Attitudes. Number 136–165.\nArchives of Psychology, 1932.\n19\n[34] Stephanie Lin, Jacob Hilton, and Owain\nEvans. TruthfulQA: Measuring how models\nmimic human falsehoods. In\nProceedings of\nthe 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 3,214–3,252, Dublin, Ire-\nland, May 2022. Association for Computa-\ntional Linguistics.\n[35] Nelson F. Liu, Kevin Lin, John Hewitt, Ash-\nwin Paranjape, Michele Bevilacqua, Fabio\nPetroni, and Percy Liang. Lost in the mid-\ndle: How language models use long contexts.\nCoRR, abs/2307.03172, 2023.\n[36] Kyle Mahowald, Anna A Ivanova, Idan A\nBlank, Nancy Kanwisher, Joshua B Tenen-\nbaum, and Evelina Fedorenko. Dissociat-\ning language and thought in large language\nmodels: A cognitive perspective.\nCoRR,\nabs/2301.06627, 2023.\n[37] Sandra Matz, Michal Kosinski, David Still-\nwell, and Gideon Nave. Psychological framing\nas an eﬀective approach to real-life persua-\nsive communication.\nACR North American\nAdvances, 2017.\n[38] Maril` u Miotto, Nicola Rossberg, and Bennett\nKleinberg. Who is GPT-3? an exploration\nof personality, values and demographics.\nIn\nProceedings of the Fifth Workshop\non Natural Language Processing and\nComputational Social Science (NLP+CSS),\npages 218–227, Abu Dhabi, UAE, Novem-\nber 2022. Association for Computational\nLinguistics.\n[39] Jakob M¨ okander, Jonas Schuett, Han-\nnah Rose Kirk, and Luciano Floridi. Audit-\ning large language models: A three-layered\napproach.\nAI and Ethics, pages 1–31, 2023.\n[40] OpenAI. GPT-4 technical report. 2023.\n[41] Gregory Park, H Andrew Schwartz,\nJohannes C Eichstaedt, Margaret L Kern,\nMichal Kosinski, David J Stillwell, Lyle H\nUngar, and Martin EP Seligman. Automatic\npersonality assessment through social media\nlanguage.\nJournal of Personality and Social\nPsychology, 108(6):934, 2015.\n[42] H. Y. Park, B. M. Wiernik, I. Oh,\nE. Gonzalez-Mul´ e, D. S. Ones, and Y. Lee.\nMeta-analytic ﬁve-factor model personality\nintercorrelations: Eeny, meeny, miney, moe,\nhow, which, why, and where to go.\nJournal of\nApplied Psychology, 105:1490–1529, 2020.\n[43] Laura Parks-Leduc, Gilad Feldman, and\nAnat Bardi. Personality traits and personal\nvalues: A meta-analysis.\nPersonality and\nSocial Psychology Review, 19(1):3–29, 2015.\n[44] Philip M Podsakoﬀ, Scott B MacKen-\nzie, Jeong-Yeon Lee, and Nathan P Pod-\nsakoﬀ. Common method biases in behavioral\nresearch: A critical review of the literature\nand recommended remedies.\nJournal of\nApplied Psychology, 88(5):879–903, 2003.\n[45] Guanghui Qin, Yukun Feng, and Ben-\njamin Van Durme. The NLP task eﬀec-\ntiveness of long-range transformers. In\nProceedings of the 17th Conference of the\nEuropean Chapter of the Association for\nComputational Linguistics, pages 3774–3790,\nDubrovnik, Croatia, May 2023. Association\nfor Computational Linguistics.\n[46] Brent W. Roberts, Nathan R. Kuncel,\nRebecca Shiner, Avshalom Caspi, and\nLewis R. Goldberg. The power of personal-\nity: The comparative validity of personality\ntraits, socioeconomic status, and cognitive\nability for predicting important life out-\ncomes.\nPerspectives on Psychological science,\n2(4):313–345, 2007.\n[47] Brent W. Roberts and Hee J. Yoon. Personal-\nity psychology.Annual Review of Psychology,\n73(1).\n[48] Jean-Pierre Rolland. The cross-cultural gen-\neralizability of the Five-Factor model of per-\nsonality. In Robert R. McCrae and J¨ uri Allik,\neditors,\nThe Five-FactorModel of Personality\nAcross Cultures, pages 7–28. Springer US,\nBoston, MA, 2002.\n[49] John Rust, Michal Kosinski, and David Still-\nwell. Modern Psychometrics: The Science\nof Psychological Assessment. Routledge, 4\nedition, 2020.\n20\n[50] Gerard Saucier and Lewis R Goldberg. Lex-\nical studies of indigenous personality factors:\nPremises, products, and prospects.\nJournal\nof Personality, 69(6):847–879, 2001.\n[51] Patrick Schramowski, Cigdem Turan, Nico\nAndersen, Constantin A Rothkopf, and Kris-\ntian Kersting. Large pre-trained language\nmodels contain human-like biases of what\nis right and wrong to do.\nNature Machine\nIntelligence, 4(3):258–268, 2022.\n[52] H. Andrew Schwartz, Johannes C. Eich-\nstaedt, Margaret L. Kern, Lukasz Dziurzyn-\nski, Stephanie M. Ramones, Megha Agrawal,\nAchal Shah, Michal Kosinski, David Stillwell,\nMartin E. P. Seligman, and Lyle H. Ungar.\nPersonality, gender, and age in the lan-\nguage of social media: The open-vocabulary\napproach.\nPLOS ONE, 8(9):1–16, 09 2013.\n[53] Amy Shaw, Melissa Kapnek, and Neil A\nMorelli. Measuring creative self-eﬃcacy: An\nitem response theory analysis of the Creative\nSelf-Eﬃcacy Scale.\nFrontiers in Psychology,\n12:678033, 2021.\n[54] Kurt Shuster, Mojtaba Komeili, Leonard\nAdolphs, Stephen Roller, Arthur Szlam, and\nJason Weston. Language models that seek\nfor knowledge: Modular search & generation\nfor dialogue and prompt completion.\nCoRR,\nabs/2203.13224, 2022.\n[55] Leonard Simms, Trevor F. Williams, and\nEricka Nus Simms. Assessment of the Five\nFactor Model. In Thomas A. Widiger, edi-\ntor,\nThe Oxford Handbook of the Five Factor\nModel, pages 353–380. Oxford University\nPress, 05 2017.\n[56] Simeng Sun, Kalpesh Krishna, Andrew\nMattarella-Micke, and Mohit Iyyer. Do long-\nrange language models actually use long-\nrange context? In\nProceedings of the\n2021 Conference on Empirical Methods in\nNatural Language Processing, pages 807–822,\nOnline and Punta Cana, Dominican Repub-\nlic, November 2021. Association for Compu-\ntational Linguistics.\n[57] Ruixiang Tang, Yu-Neng Chuang, and Xia\nHu. The science of detecting LLM-generated\ntexts.\nCoRR, abs/2303.07205, 2023.\n[58] Adriana Tapus, Cristian T ¸ ˘ apu¸ s, and Maja J\nMatari´ c. User–robot personality matching\nand assistive robot behavior adaptation for\npost-stroke rehabilitation therapy.\nIntell.\nServ. Robot., 1(2):169–183, April 2008.\n[59] Tomer Ullman. Large language models fail\non trivial alterations to theory-of-mind tasks.\nCoRR, abs/2302.08399, 2023.\n[60] Ertugrul Uysal, Sascha Alavi, and Val´ ery\nBezen¸ con. Trojan horse or useful helper?\na relationship perspective on artiﬁcial intel-\nligence assistants with humanlike features.\nJournal of the Academy of Marketing\nScience, pages 1–23, 2022.\n[61] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, /suppress L ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors,\nAdvances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.,\n2017.\n[62] David Watson and Lee Anna Clark. On\ntraits and temperament: General and speciﬁc\nfactors of emotional experience and their rela-\ntion to the Five-Factor model.\nJournal of\nPersonality, 60(2):441–476, 1992.\n[63] Jason Wei, Maarten Bosma, Vincent Y. Zhao,\nKelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V.\nLe. Finetuned language models are zero-\nshot learners. In\nInternational Conference on\nLearning Representations, 2022.\n[64] Jason Wei, Yi Tay, Rishi Bommasani, Colin\nRaﬀel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny\nZhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeﬀ\nDean, and William Fedus. Emergent abili-\nties of large language models.\nTransactions\non Machine Learning Research, 2022.\n21\n[65] Jerry Wei, Jason Wei, Yi Tay, Dustin\nTran, Albert Webson, Yifeng Lu, Xinyun\nChen, Hanxiao Liu, Da Huang, Denny Zhou,\nand Tengyu Ma. Larger language models\ndo in-context learning diﬀerently.\nCoRR,\nabs/2303.03846, 2023.\n[66] Laura Weidinger, Jonathan Uesato, Maribeth\nRauh, Conor Griﬃn, Po-Sen Huang, John\nMellor, Amelia Glaese, Myra Cheng, Borja\nBalle, Atoosa Kasirzadeh, Courtney Biles,\nSasha Brown, Zac Kenton, Will Hawkins,\nTom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia\nHaas, Sean Legassick, Geoﬀrey Irving, and\nIason Gabriel. Taxonomy of risks posed by\nlanguage models. In\nProceedings of the 2022\nACM Conference on Fairness, Accountability,\nand Transparency, FAccT ’22, page 214–229,\nNew York, NY, USA, 2022. Association for\nComputing Machinery.\n[67] Saizheng Zhang, Emily Dinan, Jack Urbanek,\nArthur Szlam, Douwe Kiela, and Jason\nWeston. Personalizing dialogue agents:\nI have a dog, do you have pets too? In\nProceedings of the 56th Annual Meeting\nof the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n2204–2213, Melbourne, Australia, July 2018.\nAssociation for Computational Linguistics.\n[68] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi\nTang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican\nDong, Yifan Du, Chen Yang, Yushuo Chen,\nZhipeng Chen, Jinhao Jiang, Ruiyang Ren,\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu\nLiu, Jian-Yun Nie, and Ji-Rong Wen. A\nsurvey of large language models.\nCoRR,\nabs/2303.18223, 2023.\nA Large Language Models\nA.1 Language Modeling\nLanguage modeling is a fundamental task in nat-\nural language processing (NLP). It is the basis\nof many solutions to a wide variety of problems\ninvolving AI systems with linguistic inputs. Down-\nstream NLP tasks that leverage language models\ninclude (among many others):\n• natural language understanding,\n• question answering,\n• machine translation,\n• document summarization,\n• dialog systems.\nThe fundamental goal of language modeling is\nto assign high probabilities to utterances (usually\nsentences in plain text) that are likely to appear\nin data (i.e., belong to the language) and low\nprobabilities to strings of words that are not. A\ntrained language model can then be used to assign\nprobabilities to arbitrary sequences of words. In\nthe past, this was done by parametric statistical\nmodels estimated from data. However, those mod-\nels have been replaced with much more successful\ndeep neural network-based methods. Generally, a\nmodern large language model (LLM) is a neu-\nral network taking strings of words as input, and\nreturning a probability measure for each of those\nstrings. The network is trained to correspond to\nthe likelihood that given input strings conform to\na particular language, as induced from large quan-\ntities of text (often called a corpus). Normally,\ninstead of thinking of a language model in terms\nof estimating the joint probability of a string of\nwords, we view it in terms of its ability to predict\ncontinuation based on existing context. A neu-\nral language model therefore is usually trained\nto compute a conditional probability of word wn\nfollowing a sequence of words w1, w 2, . . . , w n−1.\nA.2 Role of Attention in LLMs\nRecent advances in LLMs and NLP more broadly\nhave been based on innovative uses of various\nforms of attention in neural networks. Atten-\ntion was initially introduced as an improvement\nto recurrent encoder-decoder architectures [\n2] in\nthe context of neural machine translation sys-\ntems. Subsequently, it was discovered that the\nidea of attention alone can be used as a basis\nfor language modelling systems. A seminal paper\ntitled “Attention Is All You Need” [\n78] intro-\nduced a new type of neural network architecture\nfor extracting deep contextualized text represen-\ntations from raw natural language data using a\n22\nprocess based predominantly on repeated applica-\ntion of the “self-attention” operation in a model,\ncalled the transformer. This kind of model trans-\nforms the original vector space representation of\nlinguistic units through a sequence of embedding\nspaces, where each successive mapping recomputes\nthe representation of every token\n3 in the con-\ntext of its surrounding tokens. As such, it allows\nfor the semantics of words as seen by the neu-\nral AI systems to vary depending on the context\nand evolve over time. Such representations pro-\nduced signiﬁcant performance improvements on\nnatural language understanding tasks. The trans-\nformer architecture was composed of two stacks of\nself-attention blocks forming an encoder-decoder\narchitecture, originally designed as a sequence\ntransducer for neural machine translation.\nA.3 Decoder-only Architecture\nCurrently, large language models (LLMs) are usu-\nally based on the decoder-only transformer archi-\ntecture [\n6, 10, 60, 61, 77]. A sequence of text\ntokens, usually representing a user prompt (e.g., a\nquestion) is ﬁrst tokenized, by splitting text into\nmorpheme-like subwords units using a determin-\nistic algorithm inspired by information theoretic\nideas. This sequence of tokens is then embed-\nded into a high-dimensional vector space where\neach token becomes a sequence of ﬂoating-point\nnumbers. This initial point-cloud of vectors rep-\nresenting linguistic units of the prompt is then\ntransformed by a sequence of nonlinear mappings\nbetween high-dimensional representation spaces.\nThe ﬁnal representation is used to compute a\nprobability distribution over possible continua-\ntions of text conditioned on the original prompt.\nThe predominant method of training such models\nis gradient descent optimization (i.e., the back-\npropagation algorithm), resulting in representa-\ntions that are informative towards predicting the\ncontexts in which words appear within the train-\ning corpus. This simple self-supervised criterion\nleads to emergent abilities of the model, span-\nning syntax, semantics, and pragmatics of natural\nlanguage use. The distributional hypothesis , which\n3A token is the smallest unit of text that a large language\nmodel can process. Tokens can be individual characters, wor ds,\nor subwords, depending on the speciﬁc tokenization method\nused. The model assigns a unique identiﬁer to each token, and\nthese identiﬁers are then used to represent the text in the\nmodel’s internal representations.\nforms a fundamental assumption behind neural\nlanguage model training, states that syntactic\nand semantic relationships between words can\nbe inferred from their context, i.e., co-occurrence\npatterns with other words in the corpus. As a\nresult, optimizing model parameters based on n-\ngrams of tokens extracted from large quantities of\nnatural language text generates informative rep-\nresentations of linguistic units in submanifolds\nof high-dimensional real vector spaces. The geo-\nmetric and topological features of these induced\nrepresentation manifolds determine the behav-\nior of LLMs. The models trained for dialogue,\nincluding all models used in our work, are of the\nautoregressive type. This means that the output\nfrom the model itself becomes part of the con-\ntext on which future outputs are conditioned. This\nallows the model to form a contextual memory of\nthe conversation, including its own outputs.\nCurrent state of the art LLMs contain trillions\nof parameters and are trained on corpora of text\n(such as books, articles, and websites) and code\n[\n9, 16] that contain billions of n-gram patterns,\nallowing them to learn the statistical relationships\nbetween words and phrases [\n82], and consequently\nthe patterns, structures, and semantics of lan-\nguage [\n22, 48, 51, 63]. In this work, we primarily\nexplore decoder-only, auto-regressive LLMs such\nas PaLM [\n10], where the input is usually a par-\ntial or complete sequence of tokens, and the model\ngenerates the next token in the sequence based\non the previous tokens it has seen in an iterative\nprocess.\nA.4 Controlling LLM behavior\nThere are three main techniques that change\nor control an LLM’s behavior and output with\nrespect to a given input: pretraining (training the\nLLM on a large corpus of text [\n6, 10, 77]), ﬁne-\ntuning (i.e., further training a pretrained LLM on\na smaller dataset speciﬁc to a particular task or\ndomain [\n60, 62, 81, 89]), and prompting. While\npretraining and ﬁne-tuning aﬀect model behavior\nby directly altering the model’s weight parame-\nters, prompting does so indirectly by inﬂuencing\nthe activation of certain neurons or the ﬂow of\ninformation through the model’s inference pro-\ncess.\nThe most signiﬁcant aspect of using prompts\nto control LLM behavior is to carefully design\n23\nor engineer prompts to generate desired outputs\nfrom the LLM. Several types of prompt engineer-\ning techniques are commonly used with LLMs. In\nfew-shot prompting [\n6, 47, 54], a limited amount\nof example data are provided to the model in a\nprompt to guide it to perform a task. By lever-\naging this small set of examples, the LLM can\ngeneralize and produce responses beyond the pro-\nvided instances. Few-shot prompting relies on the\nability to bias the LLM’s responses based on\nthe input prompt. But because it introduces a\nbias, this method is not useful in cases where the\ngoal is to probe the default bias of the LLM,\nthe behavior or tendency of the LLM to produce\ncertain outputs (e.g., certain psychometric sur-\nvey responses, in our case). Zero-shot prompting\n[\n39, 81], on the other hand, involves instructing the\nmodel to generate responses for tasks it has not\nbeen speciﬁcally trained on and without providing\nany examples, relying on the LLM’s pre-existing\nknowledge and language understanding acquired\nduring pre-training. This method provides insights\ninto the language priors and distribution learned\nby the LLM, what tokens are more correlated\nthan others, etc. For instance, if asked to com-\nplete an input prompt: “She went to see an expert\nabout her stroke, who”, an LLM trained on med-\nical domain data is likely to respond “advised her\nto get an ECG test.” whereas an LLM trained\non sports data might complete it as “coached her\nabout the best techniques from top golf pros.” Sev-\neral recent works in the ﬁeld of Responsible AI\nhave attempted to uncover latent language biases\nin LLMs, to identify potential for harm, and to\nsuggest mitigation techniques [\n45, 86]. Similarly,\nour work used zero-shot prompt engineering to\nanalyze how latent linguistic features in LLMs\ngive rise to a coherent personality when quanti-\nﬁed psychometrically. We further analyzed how\nthose traits can be modiﬁed by engineering speciﬁc\nprompts and aﬀecting the latent linguistic features\nin these LLMs.\nA.5 Modes of Inference in LLMs\nLLMs oﬀer various ways of inference in prac-\ntice. In generative mode, the LLM is given a\nprompt or instruction, and it then generates text\nthat is consistent with that prompt. This mode\nis useful for creative text generation tasks, such\nas story or poetry writing. In scoring mode, the\nLLM is given a pair (prompt, continuation) and\nit assigns a score or probability to it, indicat-\ning its quality or relevance or how likely it is\nto be generated from that model. Scoring mode\n[\n35] is often used for tasks like language eval-\nuation [ 31]. Internally to the LLM, there is a\nsingle operating mode—computing the probability\ndistribution over a sequence of tokens—but this\ndistinction between the various modes of inference\nis conceptually useful when reasoning about model\nbehavior.\nB Personality Psychology\nThe ﬁeld of personality psychology deﬁnes per-\nsonality as enduring characteristics, traits, and\npatterns that shape thoughts, feelings, and behav-\niors across a diverse array of situations; e.g., social,\nspatial, and temporal contexts [\n70]. Decades of\npersonality research synthesizing evidence from\nmolecular genetics [\n68], evolutionary biology [ 58],\nneuroscience [18, 19], linguistics [5, 66], and cross-\ncultural psychology [49] have reduced such diverse\ncharacteristic patterns to a theorized handful of\nhigher-order factors that deﬁne personality [\n17,\n36].\nSpeciﬁc to linguistic evidence of a personality\ntaxonomy, a central area of personality research\nconcerns the lexical hypothesis of personality —\nthat human personality is intrinsically connected\nto language. Since its origin from Sir Francis\nGalton in the 1800s [\n21], empirical research on\nthe lexical hypothesis has posited that 1) impor-\ntant personality characteristics of a given society\nwill be encoded in its language; and 2) that\nthe most important of those characteristics are\nlikely encoded as single words [\n23, 67, 71]. This\nempirical framework grounds our work in three\nareas: the choice of one of our personality instru-\nments (the BFI; described below), our prompts for\nshaping LLM personality, and the choice of the\nlanguage-based assessment of personality for rat-\ning LLM-synthesized personality in a downstream\ntask.\nThe Big Five model [\n37], the most commonly\ncited research taxonomy of personality formed\nthrough the research described above, identiﬁes\nﬁve personality trait dimensions (i.e., domains)\n24\nand provides methodology to assess these dimen-\nsions in humans. The ﬁve dimensions are extraver-\nsion (EXT), agreeableness (AGR), conscientious-\nness (CON), neuroticism (NEU), and openness\nto experience (OPE). Each domain is further\ncomposed of various lower-order facets nested\nunderneath.\nC Related Work\nRecent attempts to probe personality and psy-\nchopathological traits in LLMs suggest that some\nmodels exhibit dark personality patterns [\n44], or\ndemonstrate how to administer personality inven-\ntories to LLMs [\n8, 33, 34, 38, 65, 74, 75]. Some\nhave also made eﬀorts to induce desired levels of\npersonality in LLMs using prompting [\n8, 33, 34]\nor ﬁne-tuning [ 38, 44]. While these works out-\nlined the utility and importance of measuring\nsocial phenomena in LLMs [\n65], there remains a\nneed to match standards of evaluating the qual-\nity of human survey data when evaluating survey\nresponse data from LLMs—standards that are\ncommonplace in quantitative social science [\n13].\nTo claim that scores on a psychological test are\ntrustworthy and meaningful signals of what the\ntest purports to measure, one must establish the\ntest’s reliability and construct validity.\nRecent works that probe social and\npersonality-related traits in LLMs have adminis-\ntered and analyzed questionnaires in ways that\nare unconventional in psychometrics. In this\nappendix, we focus on two additional elements\nnot discussed in the main text. First, researchers\ncollected LLM responses in the form of generated\ncompletions, often in dialog mode. For instance,\n[\n76] administered psychological emotion measures\nto LLMs in the form of a research interview\ntranscript, where a ﬁctitious researcher posed\nmeasure items to a ﬁctitious participant, who\nwas instructed to respond to these items on a\nnumeric scale. In psychometrics, questionnaire-\nbased methods of assessment are distinct from\ninterview-based methods. Human answers to\nboth questionnaires and structured interviews\nmeasuring the same underlying construct do not\nnecessarily converge (e.g., in the case of measuring\npersonality disorders [\n90]). Indeed, administering\nquestionnaires in this way to LLMs creates an\narbitrary viewpoint from which to elicit person-\nality traits, and is likely biased by the ordering\nof the questionnaire itself [\n43] and prompting the\nLLM to respond in an interview setting (where it\nmay respond diﬀerently knowing an interviewer\nis observing). Each LLM response to a given\nquestionnaire item was not an independent event,\nbut considered all previous responses shown in\nthe transcript. Second, the LLMs in these studies\nwere not used deterministically. This not only\nhampers reproducibility, but also poses implica-\ntions for reliability. Computing reliability metrics\nfor questionnaires scored in this unconventional\nway is precarious because such reliability metrics\ndepend on item-level variance. If this item-level\nvariance is contaminated by variation introduced\nby the model parameters in a diﬀerent way for\neach item, it is diﬃcult to compute valid indices\nof reliability. We overcame these challenges in\nour work by proposing a prompt and persona\nsampling methodology that allows variance to\nbe linked across administrations of diﬀerent\nmeasures.\nPsyBORGS [\n72] administered a series of vali-\ndated survey instruments of race-related attitudes\nand social bias to LLMs using psychometrics-\ninformed prompt engineering. Our work utilized\nthe PsyBORGS framework.\nD Tested Language Models\nFirst, we focused on three diﬀerent model sizes:\nsmall (8B), medium (62B), and large (540B),\nbecause LLM model size is a key determinant\nof performance for this model family [\n10, 88].\nSecond, because we are also interested in evalu-\nating LLM personality in the Q&A context, we\ninvestigated PaLM models variants, ﬁne-tuned to\nfollow instructions as they have been shown to per-\nform better than base models for prompting-based\nQ&A tasks [\n81]. We speciﬁcally selected variants\nﬁne-tuned with the popular FLAN dataset [ 81].\nThird, we examined traditional and high-data\ntraining methods, known as Chinchilla training\n[\n29], which uses a ﬁxed training budget to ﬁnd the\nbalance between model size and training dataset\nscale. Chinchilla training yields superior perfor-\nmance across a broad set of tasks [\n29, 88]. Table\n2 lists the tested models along with their size and\ntraining conﬁguration options.\nAll experiments used quantized models [ 83]\nto reduce the memory footprint and speed up\ninference time.\n25\nE Selected Personality\nInventories\nTo measure personality, we selected two well-\nestablished psychometric measures to assess the\nBig Five taxonomy: one from the lexical tradi-\ntion and one from the questionnaire tradition.\nLexical tradition measures are grounded in the\nhypothesis that personality can be captured by\nthe adjectives found in a given language [\n21, 23],\nwhile questionnaire tradition measures are devel-\noped with existing (and not necessarily lexical)\ntaxonomies of personality in mind [\n73]. Lexical\nmeasures may be better suited for LLMs because\nthey are language-based and rely on adjectival\ndescriptions. We posit that questionnaire mea-\nsures, which do not rely on trait adjectives for\ncontent, more conservatively test LLM abilities,\nas they are less abstract and more contextualized.\nOur work focused on Big Five measures of person-\nality due to the Big Five’s integrative robustness\nand cross-theory convergence in the human per-\nsonality and psycholinguistics literature [\n73].\nOur primary personality measure, the\nIPIP-NEO [ 25], is a 300-item open source rep-\nresentation of the commercialized Revised NEO\nPersonality Inventory [\n14]. The IPIP-NEO, hail-\ning from the questionnaire tradition [ 73], involves\nrating descriptive statements (e.g., “[I] prefer\nvariety to routine”; 60 per Big Five domain) on\na 5-point Likert scale. (1 = very inaccurate ; 2 =\nmoderately inaccurate ; 3 = neither accurate nor\ninaccurate; 4 = moderately accurate ; 5 = very\naccurate). We refer to these statements as items.\nThe IPIP-NEO has been translated and validated\nin many languages, facilitating cross-cultural\nresearch across populations [\n32], and has been\nused in longitudinal studies to assess personality\nchange and stability over time [\n84]. We chose this\nmeasure for its excellent psychometric properties,\nshown in [\n25].\nAs a robustness check and to assess convergent\nvalidity, we also measured LLM-synthesized per-\nsonality using the Big Five Inventory (BFI) [\n37].\nDeveloped in the lexical tradition, the BFI is a\nbrief (44-item), adjectival statement-based mea-\nsure of the broad Big Five traits. The BFI asks\nparticipants to rate short descriptive statements\n(e.g., “I see myself as someone who is talkative”)\nalso on a 5-point Likert scale. The resulting sum-\nmary scores indicating levels of Big Five trait\ndomains range from 1.00 to 5.00. In the psychology\nliterature [\n73], the BFI has demonstrated excellent\nreliability (mean α reported across domain sub-\nscales = 0.83), convergent validity, and external\nvalidity.\nDomain subscale scores across both measures\nwere calculated following their original instruc-\ntions as the average of item response values,\naccounting for reverse-keyed items. Possible sub-\nscale scores ranged from 1.00 to 5.00, indicating\nthe lowest and highest possible levels of a given\nBig Five domain, respectively.\nF Simulating Population\nVariance Through\nPrompting\nIt was empirically necessary to introduce con-\ntrolled variation in LLM-simulated survey data to\nassess their reliability and statistical relationships\nwith outcomes of interest; in short, controlled\nvariation was required to statistically test for\nreliability and construct validity.\nFor instance, an Item Postamble presented the\npossible standardized responses the model can\nchoose from, e.g.,\nplease rate your agreement on a\nscale from 1 to 5, where 1 is ‘strongly\ndisagree’, 2 is ‘disagree’, 3 is ‘neither\nagree nor disagree’, 4 is ‘agree’, and 5\nis ‘strongly agree’.\nWe customized ﬁve variations of Item Postam-\nbles for each administered measure, such that\nall ﬁve variations would have parallel meanings\nacross measures. Supplemental Table\n5 lists all\nItem Postambles used in this work. This prompt\ndesign enabled thousands of variations of input\nprompts that could be tested, with two major\nadvantages. First, variance in psychometric test\nresponses created by unique combinations of the\nPersona Descriptions (see Supplemental Table\n6), Item Instructions (see Supplemental Table\n7), and Item Postambles enabled us to quan-\ntify the validity of personality measurements in\nLLMs. Unlike single point estimates of person-\nality, or even multiple estimates generated from\nrandom resampling of LLMs, diverse distributions\nof personality scores conditioned on reproducible\n26\nTable 5: Item Postambles used to construct the prompts employed in the e xperiments to generate LLM-simulated\nsurvey responses. All administered measures used a Likert-type re sponse scale that allowed 5 possible choices,\nwith the exception of the PVQ-RR, which used a 6-point respons e scale. Item Postambles 1–5 were used for the\nBFI; 6–10 for the IPIP-NEO; 11–15 for the PANAS; 16–20 for the SSC S; 21–25 for the BPAQ; and 26–30 for the\nPVQ-RR.\n№ Item Postambles\n1. please indicate the extent to which you agree or disagree on a scale from 1 to 5 (where 1 = \"disagree strongly\",\n2 = \"disagree a little\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n2. please rate your level of agreement on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a\nlittle\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n3. please rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 =\n\"disagree a little\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n4. please rate how much you agree on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree a little\", 3\n= \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n5. please rate how much you agree or disagree on a scale from 1 to 5 (where 1 = \"disagree strongly\", 2 = \"disagree\na little\", 3 = \"neither agree nor disagree\", 4 = \"agree a little\", and 5 = \"agree strongly\"):\"\n6. please rate how accurately this describes you a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 = \"moderately\ninaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n7. please indicate how accurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 =\n\"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very\naccurate\"):\"\n8. please indicate how accurate or inaccurate this is about you on a scale from 1 to 5 (where 1 = \"very\ninaccurate\", 2 = \"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and\n5 = \"very accurate\"):\"\n9. please rate how accurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\", 2 = \"moderately\ninaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very accurate\"):\"\n10. please rate how accurate or inaccurate this is about you on a scale from 1 to 5 (where 1 = \"very inaccurate\",\n2 = \"moderately inaccurate\", 3 = \"neither accurate nor inaccurate\", 4 = \"moderately accurate\", and 5 = \"very\naccurate\"):\"\n11. indicate to what extent you agree on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\", 2 =\n\"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n12. please rate your level of agreement on a scale from 1 to 5, (where 1 = \"very slightly or not at all agree\", 2 =\n\"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\"\n13. please rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"very slightly or not at\nall agree\", 2 = \"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n14. please rate how much you agree on a scale from 1 to 5 (where 1 = \"very slightly or not at all agree\", 2 = \"agree\na little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n15. please rate how much you agree or disagree on a scale from 1 to 5 (where 1 = \"very slightly or not at all\nagree\", 2 = \"agree a little\", 3 = \"agree moderately\", 4 = \"agree quite a bit\", and 5 = \"agree extremely\"):\"\n16. please decide to what extent this describes you on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 =\n\"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n17. please rate your level of agreement on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 = \"disagree\", 3 =\n\"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n18. please rate your level of agreement or disagreement on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2 =\n\"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n19. please rate how much you agree that this describes you on a scale from 1 to 5 (where 1 = \"strongly disagree\", 2\n= \"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n20. please rate how much you agree or disagree that this describes you on a scale from 1 to 5 (where 1 = \"strongly\ndisagree\", 2 = \"disagree\", 3 = \"neither agree nor disagree\", 4 = \"agree\", 5 = \"strongly agree\"):\"\n21. rate how characteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely uncharacteristic of me\", 2 =\n\"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 = \"characteristic of me\",\nand 5 = \"extremely characteristic of me\"):\"\n22. please rate how characteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely uncharacteristic of\nme\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\", 4 = \"characteristic\nof me\", and 5 = \"extremely characteristic of me\"):\"\n23. please rate how characteristic or uncharacteristic this is of you on a scale from 1 to 5 (where 1 = \"extremely\nuncharacteristic of me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\",\n4 = \"characteristic of me\", and 5 = \"extremely characteristic of me\"):\"\n24. please indicate to what extent this is characteristic of you on a scale from 1 to 5 (where 1 = \"extremely\nuncharacteristic of me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor uncharacteristic of me\",\n4 = \"characteristic of me\", and 5 = \"extremely characteristic of me\"):\"\n25. please indicate to what extent this is characteristic or uncharacteristic of you on a scale from 1 to 5\n(where 1 = \"extremely uncharacteristic of me\", 2 = \"uncharacteristic of me\", 3 = \"neither characteristic nor\nuncharacteristic of me\", 4 = \"characteristic of me\", and 5 = \"extremely characteristic of me\"):\"\n26. think about how much that person is or is not like you. Rate how much the person described is like you on a\nscale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not like me\", 3 = \"a little like me\", 4 = \"moderately\nlike me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n27. please rate how characteristic this is of you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not\nlike me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n28. please rate how characteristic or uncharacteristic this is of you on a scale from 1 to 6 (where 1 = \"not like me\nat all\", 2 = \"not like me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much\nlike me\"):\"\n29. please indicate to what extent this is like you on a scale from 1 to 6 (where 1 = \"not like me at all\", 2 = \"not\nlike me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much like me\"):\"\n30. please indicate to what extent this is or is not like you on a scale from 1 to 6 (where 1 = \"not like me at\nall\", 2 = \"not like me\", 3 = \"a little like me\", 4 = \"moderately like me\", 5 = \"like me\", and 6 = \"very much\nlike me\"):\" 27\nTable 6 : 50 human Persona Descriptions sampled from the PersonaChat datase t [ 87], used in Item Preambles\nacross all experiments.\nPersona Descriptions\nI like to garden. I like photography. I love traveling. I like to bake pies.\nI’ve a beard. I graduated high school. I like rap music. I live on a farm. I drive a truck.\nI blog about salt water aquarium ownership. I still love to li ne dry my clothes. I’m allergic to peanuts. I’ll one day own a\nferret. My mom raised me by herself and taught me to play baseb all.\nSince young I ve loved to cook. I auditionated in a cooking sho w. I think I’ve talent for it. I took classes while growing up.\nMy name is tom. I try to watch what I eat. I enjoy eating italian food. Pizza is my favorite. I am east asian.\nI live by a lake. I am a mother. I own a custom upholstery shop. I ’m a wife.\nI enjoy working out and learning new things. I’m a student in c ollege. I’m studying software development. I play the guita r.\nI’ve three dogs at home. I hate to workout, but I need to. I am ve ry good at the drums. I have a bicycle. I need to take my\nblood sugar everyday.\nI work in advertising. My mother is dead. I like to hike. I’ve a golden retriever. I write ﬁction for fun.\nI can never decide between a chili corn dog and a cheesy hot dog . I drive more than an hour each way to work. I prefer the\nnight to the day, but I love sunshine. I am a grandparent at 44.\nI like to smell my own farts. My beer gut is so huge i’ven T seen m y feet in two years. I am from San Fransico. I am always\nthe one who buys the beers. I like to place blame on other peopl e even when I know it is my fault.\nI lived most of my life not knowing who Bob marley was. When I cu t loose, I lose control. We help each other out in my\nfamily. I despise my boss. I work over 60 hours a week as a resta urant manager.\nI prefer the simpler times. I like simple jokes. Some jokes go too far. I like the ﬂintstones.\nIt is my universe, and everyone else is just a character in it. I work as a dental assistant in a ritzy part of town. I’ve borde rline\npersonality disorder. At night, I party hard in the Atlanta c lub scene, and I never miss a music festival.\nI watch a lot of tv. I live alone. My favorite food is a cheesebu rger. I enjoy ﬁshing. I work on cars for a living.\nI’m an animal rights activist. I hope to retire to Florida. I p layed in a band for 17 years. My mother and father are both in\nthe church choir.\nI’ve taken formal music lessons since I was 5. I’m a musician. My best friend is in a band with me. I wish I could spend more\ntime at home.\nI grew up in Kentucky. I’m a veteran. My favorite book is ender’ s game. I have a garden. I like to read.\nI am a vegan. I love country music. I love the beach. I like to re ad.\nI’ve depression and anxiety so I don’t really go out a lot. I wo rk at home, editing. I have a cat. I hope to move out soon.\nMy favorite food is mushroom ravioli. I ve never met my father . My mother works at a bank. I work in an animal shelter.\nI love kids and dogs. I like to go shopping with my daughters. I like to cook. I love to chat with my friends.\nI swim often. I run track. I wear glasses all day. I take medica tion.\nI like to go on long hikes. I like to play volleyball. I like to c ome up with new hairstyles. I like to do my nails.\nI watch Jimmy Fallon s show every night. I have never kissed a w oman. People notice how organized I am. I believe that I\ncan achieve anything.\nI drive a lifted Chevy truck. I played football in high school . I am a roofer. I always have a beer after work.\nI love animals. My father worked for Ge. Green is my favorite c olor. I enjoy playing tennis. I’m an aspiring singer.\nI try to watch what I eat. I enjoy eating italian food. Pizza is my favorite. My name is tom. I am east asian.\nIn allergic to peanuts. I like eating vegetables. I love the B eatles. I’m usually very shy. I have trouble getting along wi th\nfamily.\nI go to high school. Math is my favorite subject. I live in the U nited States. I am a boy.\nI have a job as an it agent. I like smoking weed. My dad works for stiﬂe. I love rap music. I’m a meataholic.\nI work in tv. I do not treat my girlfriend very well. I like to co ok breakfast on sundays. I love to sing. I am a lesbian.\nI work on semi trucks for a living. My father was a driver himse lf. I got oﬀ the road when I married my sweetheart. I want\nto take her on vacations one day. My motor never stops running .\nI own a Iphone 7. I drink hot chocolate during the winter. I’m a llergic to seafood. My mother use to read me bed time stories.\nI am eighteen years old. I’m going to majoring in business. I j ust bought my ﬁrst car. I received a full scholarship to Flori da\nstate university.\nI live in a tiny house to save money. I collect single malt scot ch. I listen to blues and jazz. I tend bar on the weekends. Duri ng\nthe week I go to college to become a lawyer.\nI love to go horseback riding whenever I can. I’m a mother of tw o beautiful boys. My family and I go camping every month.\nMy favorite artist is Justin Bieber.\nI especially enjoy listening to the band the lumineers. I enj oy reading and walking on sunny days. I’m a happy person. I sin g\nmany songs.\nI play piano. My favorite color is yellow. My boyfriend is in t he army. My father is dead. My hair is short.\nI’m a mother. I’m a nurse at a hospital. My favorite band is the rolling stones. I love to read and cook. My favorite food is\nmexican food.\nI deliver baked goods in the state where I live. My favorite ho bby is playing recreational baseball. I spend my weekends\ncamping. I’m a truck driver. My wife and two kids camp with me.\nI am argentinian. I like to wear boots. I have many girlfriend s. I like to eat beef. I like to ride horses.\nI recently had a private lunch with will ferrell. I am trying t o become a male model in hollywood. I’m a huge fan of classical\njazz. I am on a low carb diet.\nI want to put my photos to a music video staring Adam Levin. I wa nt to travel the world taking photographs of my travels.\nI am a widow. I want to be a famous photographer.\nI am in the army. I ﬂy airplanes. I enjoy building computers. I dropped out of college.\nI have three children. I live in the suburbs of a major city. I l ike to garden. I graduated college for secondary english edu cation.\nI play guitar in the local band. I live on a small farm in Ohio. I a m the youngest of three brothers. I have never been to the\ncity.\nI’m a widow. I want to put my photos to a music video staring Ada m Levin. I want to travel the world taking photographs\nof my travels. I want to be a famous photographer. I like takin g pictures.\nI still live at home with my parents. I play video games all day . I’m 32. I eat all take out.\nMy friend once bought me a car. I am disabled and cannot walk. I take vitamin c when I have a cold. I do not eat bread.\nMy favorite season is winter.\n28\nTable 7 : Item Instructions used in Item Pream-\nbles across experiments to generate LLM-simulated\nsurvey responses.\nItem Instructions\nConsidering the statement,\nThinking about the statement,\nReflecting on the statement,\nEvaluating the statement,\nRegarding the statement,\npersonas make it possible to compute correla-\ntions between convergent personality measures\nand external, personality-related constructs. Sec-\nond, variance in Item Preambles and Postambles\nfacilitated a built-in robustness check: it was criti-\ncal to know if personality scores remained reliable\nand valid across modiﬁcations of context and\ninstructions surrounding original test items. They\nwere indeed reliable and valid for three of the ﬁve\nmodels tested.\nG Psychometrics\nPsychometrics, a quantitative subﬁeld of psychol-\nogy and education science, encompasses the sta-\ntistical theory and technique of measuring unob-\nservable, latent phenomena called constructs, like\npersonality, intelligence, and moral ideology. Psy-\nchometrics is foundational to the development and\nvalidation of standardized educational tests (e.g.,\nthe SAT, LSAT, GRE) [\n1], medical and psycholog-\nical clinical assessments [80], and large-scale public\nopinion polls [ 28].\nPsychometric tests (e.g., survey instruments,\nmeasures, multi-item scales) are tools for quanti-\nfying latent psychological constructs like personal-\nity. Psychometric tests enable statistical modeling\nof the true levels of unobservable target con-\nstructs by relying on multiple indirect, yet observ-\nable, measurements across a sample of individuals\ndrawn from a wider population.\nWe refer to items as the individual elements\n(i.e., descriptive statements, sometimes questions)\nused within a psychometric test designed to mea-\nsure attributes or characteristics of a construct.\nItems are usually rated on a rating scale - a\nstandardized set of response choices that allows\nresearchers to quantify subjective phenomena. A\nLikert-type scale is the most common rating scale\nthat has respondents specify their level of agree-\nment on a symmetric agree-disagree scale [\n46]. We\nrefer to a subscale as a collection of items, usually\nresulting from a factor analysis, aimed at measur-\ning a single psychological construct. Measures are\nthemed collections of subscales.\nFor example, the Big Five Inventory (BFI)\n[\n37] is a popular measure of personality; it com-\nprises ﬁve multi-item subscales targeting each Big\nFive dimension. BFI Extraversion, for instance, is\na subscale within the BFI speciﬁcally targeting\nthe dimension of extraversion. An example item\nunder BFI Extraversion would read, “[I see myself\nas someone who] is talkative.” Participants rate\ntheir agreement with this item using the follow-\ning 5-point Likert-type rating scale: 1 = disagree\nstrongly; 2 = disagree a little ; 3 = neither agree\nnor disagree ; 4 = agree a little ; 5 = agree strongly.\nHow do we know that psychometric tests mea-\nsure what they claim to measure, i.e., how do\nwe establish the reliability, accuracy, and utility\nof the measures of personality, and the constructs\nassessed in those measures ? Validated scientiﬁc\nframeworks for establishing thereliability and con-\nstruct validity of a new psychometric test [\n12,\n13, 52] incorporate (but are not limited to) the\nfollowing overarching standards:\n• Reliability: Are test measurements dependable\nand consistent? In psychometrics, a test’s reli-\nability can be established in terms of internal\nconsistency and factor saturation.\n– Internal consistency reliability: Is the\ntest reliable across multiple measurements\n(i.e., its items)? In other words, do responses\nto the test’s items form consistent patterns?\nAre test items correlated with each other?\n– Factor saturation: Do the test’s items\nreﬂect the variance of one underlying factor\nor construct?\n• Construct Validity: Do the test measure-\nments actually reﬂect the underlying construct?\nThis can be established by checking for conver-\ngent validity, discriminant validity and criterion\nvalidity.\n– Convergent Validity: Does the test corre-\nlate with purported indicators (i.e., conver-\ngent tests) of the same or similar psycholog-\nical construct? These correlations are called\nconvergent correlations.\n29\n– Discriminant Validity: Relative to their\nconvergent correlations, are test scores rela-\ntively uncorrelated with scores on theoretically\nunrelated tests? These correlations are called\ndiscriminant correlations.\n– Criterion Validity: Does the test correlate\nwith theoretically-related, non-tested phenom-\nena or outcomes?\nG.1 Reliability: Is the Measurement\nDependable?\nThe hallmark characteristic of a good psychome-\ntric test (or any empirical measure) of a target\nconstruct is its reliability, which reﬂects its ability\nto “measure one thing (i.e., the target construct)\nand only that thing, as precisely as possible”\n[\n13]. In this work, we balance our evaluations\nof reliability across three indices of reliability—\nCronbach’s Alpha (α ), Guttman’s Lambda 6 (λ6),\nand McDonald’s Omega ω —weighing the pros and\ncons of each.\nα , the most widely-known measure of inter-\nnal consistency reliability, captures how responses\nto each item of a scale correlate with the total\nscore of that scale [\n15]. However, α has many doc-\numented limitations. For instance, it relies on the\nassumption that all items of a test measure the\nsame underlying construct and it can be artiﬁ-\ncially inﬂated by a test’s number of items [\n91].\nCronbach’s α is computed as follows:\nα = k\nk − 1\n(\n1 −\n∑ k\ni=1 σ2\ny\nσ2x\n)\n(1)\nwhere k is the number of items on the test, σ2\ny is\nthe variance associated with each item i, and σ2\nx\nis the overall variance of total scores.\nIn contrast to α , λ6 evaluates the variance of\neach item that can be captured by a multiple\nregression of all other items [\n27]. It is less biased\nalternative to α because it is not aﬀected by item\ndiﬀerences in variance, although it is also biased\nby the number of items on a test. Guttman’s λ6\nis calculated as:\nλ6 = 1 −\n∑ k\ni=1(e2\ni )\nVx\n(2)\nwhere k is the number of items on the test, ei is\nthe error term for item i, Vx is the variance of the\ntotal test score.\nTo test more robustly for reliability (in terms\nof how well a test measures one underlying fac-\ntor or construct) in a way that is unaﬀected\nby number of items on a test, psychometricians\ncompute McDonald’s Omega ( ω ) [\n50, 91]. This\nmetric is generally considered a less biased com-\nposite test of reliability [\n26, 91]. McDonald’s ω\nuses conﬁrmatory factor analysis to determine if\nitems statistically form a single factor, or actually\nmeasure separate factors. It is calculated as:\nωh =\n1\nk\n∑ k\ni=1\nt2\ni\nσ 2\ni\n1\nk−1\n∑ k\ni=1\nt2\ni\nσ 2\ni\n− 1\nk\n1\n1−r2\ntt\n(3)\nwhere ωh is McDonald’s hierarchical omega, k is\nthe number of items on the test, ti is the stan-\ndardized item score for item i, σ2\ni is the variance\nof the standardized item score for item i, and rtt\nis the correlation between the total test score and\nthe standardized total test score.\nG.2 Construct Validity: Is the\nMeasurement Valid?\nSince psychometric tests measure physically unob-\nservable constructs, such as personality traits, it\nis imperative to establish that such tests measure\nwhat they claim to measure. This process is called\nestablishing a test’s construct validity . Construct\nvalidity is a comprehensive judgement of how the\nscores and the theoretical rationale of a test rea-\nsonably reﬂect the underlying construct the test\nintends to measure [\n53]. Recently, construct valid-\nity has become a crucial focus of AI responsibility\nand governance [\n30, 57]: operationalizing social\nphenomena in algorithmic systems in a princi-\npled way (e.g., through construct validation) is\na core part of responsible AI. Bringing empiri-\ncal rigor to the measurement of social constructs\nhelps stakeholders make more informed judgments\nof characteristics that may be fair or harmful in\nAI systems. For instance, if low agreeableness is\nharmful in AI systems, we need a principled way\nto measure it.\nThere is extant work on establishing the valid-\nity of measurements of personality as a theoretical\nconstruct [\n17, 36, 70], a powerful predictor of\n30\nother important human traits and life outcomes\n[\n4, 42, 69] and its manifestation in human language\n[23, 67, 71], which forms the basis of LLMs. How-\never, establishing the validity of measurements of\npersonality as a meaningful construct in LLMs has\nnot yet been addressed.\nConvergent and Discriminant Validity:\nIn psychometrics, the convergent and discriminant\nvalidity of a test are evaluated using Campbell’s\nclassic framework [\n7], where a test’s convergent\nvalidity is established by “suﬃciently large” corre-\nlations with separate tests meant to measure the\nsame target construct. For example, to validate\na new test measuring depression, one could cal-\nculate the test’s convergent correlations with the\nBeck Depression Inventory (BDI) [\n3]—a widely-\nused measure of depression. To evaluate the\ndiscriminant validity of a test, psychometricians\ncommonly gauge the extent to which the test’s\nconvergent correlations are stronger than its dis-\ncriminant correlations—its correlations with test\nof other constructs. As a concrete example, a new\ntest of depression should correlate more strongly\nwith the BDI than with, say, a test measuring\nEnglish proﬁciency.\nCriterion Validity: A common way to assess\nthe criterion validity of a new psychometric test is\nto check its correlations with theoretically related\nexternal (non-test) criteria (hence the name, crite-\nrion validity) [\n13]. For example, to validate a new\npsychometric test of depression, one could test\nif it is substantially related to a known external\ncriterion, like negative aﬀect.\nH Methods for Constructing\nthe Validity of LLM\nPersonality Test Scores\nEstablishing Reliability\nIn LLM research, model responses to a series of\nseemingly related tasks intended to measure one\nlatent construct may be anecdotally “consistent”\n[\n38, 65] or inconsistent [ 55]. Descriptive consis-\ntency, however, is not suﬃcient evidence that the\nresponses to those tasks are statistically reliable\nreﬂections of the latent constructs they target (as\ndescribed in Section\nG.2).\nTo establish internal consistency reliability, we\ncompute Cronbach’s α (1) and Guttman’s λ6 (2)\non all IPIP-NEO and BFI subscales. To assess\nmore complete composite reliability we compute\nMcDonald’s ω (\n3) on all IPIP-NEO and BFI\nsubscales.\nWe designate a given reliability metric ( RM ;\ni.e., α , λ6, ω ) < 0. 50 as unacceptable, 0 . 50 ≤\nRM < 0. 60 as poor, 0 . 60 ≤ RM < 0. 70 as\nquestionable, 0 . 70 ≤ RM < 0. 80 as acceptable,\n0. 80 ≤ RM < 0. 90 as good, and RM ≥ 0. 90\nas excellent. The high levels of singular internal\nconsistency metrics like α are necessary but not\nsuﬃcient conditions for demonstrating complete\nreliability. Therefore, for the purpose of the cur-\nrent work, α , λ6, and ω must be at least 0 . 70 for\na given subscale to be deemed acceptably reliable.\nEstablishing Construct Validity\nWe operationalize construct validity in terms of\nconvergent, discriminant, and criterion validity\n(see Appendix\nG.2). We used Campbell’s clas-\nsic multitrait-multimethod matrix (MTMM) [ 7]\napproach to evaluate convergent and discriminant\nvalidity. Criterion validity is evaluated by corre-\nlating LLM-simulated personality test data with\nLLM responses to theoretically-related psychome-\ntric test.\nConvergent validity: We evaluated conver-\ngent validity—how much our primary test of\npersonality (the IPIP-NEO) positively relates to\nanother purported test of personality (BFI)—by\ncomputing bivariate Pearson correlations between\nIPIP-NEO and BFI scores for extraversion,\nagreeableness, conscientiousness, neuroticism, and\nopenness and comparing them to ensure corre-\nlations between each domain subscale are the\nstrongest of their row and column, as out-\nlined in [\n7]. For instance, IPIP-NEO Extraversion\nshould be most correlated with BFI Extraversion,\nbecause these two subscales should convergently\nmeasure the same underlying construct.\nWe operationalize convergent correlations\nbetween two psychometric tests (in this case,\nBig Five subscales from the IPIP-NEO and BFI)\n{(x1, y 1), . . . , (xn, y n)}, reﬂecting n pairs of con-\ntinuous score data, as Pearson product-moment\ncorrelations:\nrxy =\n∑ n\ni=1(xi − ¯x)(yi − ¯y)\n√ ∑ n\ni=1(xi − ¯x)2\n√\n∑ n\ni=1(yi − ¯y)2 (4)\nwhere n is the sample size, xi, y i are a pair of\ndata points i from sample, ¯x is the sample mean\n31\nTable 8: Criterion validity subscales per tested Big Five domain. PANA S = Positive and Negative Aﬀect Schedule\nScales; BPAQ = Buss-Perry Aggression Questionnaire; PVQ-RR = Re vised Portrait Values Questionnaire; SCSS\n= Short Scale of Creative Self.\nIPIP-NEO Domain External Criterion Criterion Subscales\nExtraversion Trait Emotion PANAS Positive Aﬀect\nPANAS Negative Aﬀect\nAgreeableness Aggression\nBPAQ Physical Aggression\nBPAQ Verbal Aggression\nBPAQ Anger\nBPAQ Hostility\nConscientiousness Human Values\nPVQ-RR Achievement\nPVQ-RR Conformity\nPVQ-RR Security\nNeuroticism Trait Emotion PANAS Negative Aﬀect\nPANAS Positive Aﬀect\nOpenness Creativity SSCS Creative Self-Eﬃcacy\nSSCS Creative Personal Identity\nscore for personality trait x of the IPIP-NEO,\nand ¯y is the sample mean score for corresponding\npersonality trait y of the BFI.\nIn the resulting MTMM, we consider at\nleast strong correlations ( |rxy | ≥ 0. 60; [ 20])\nbetween each IPIP-NEO domain subscale and\nits BFI domain scale counterpart (e.g., r(IPIP-\nNEO Extraversion, BFI Extraversion), r(IPIP-\nNEO Agreeableness, BFI Agreeableness), etc.) as\nevidence of convergent validity. For these and\nfollowing results, we used cut-oﬀs recommended\nby [\n20] for considering correlations as moderate,\nstrong, and very strong (viz. . 40 ≤ | r| < . 60;\n. 60 ≤ | r| < . 80; . 80 ≤ | r|; respectively). In\nour tests for convergent validity, strong conver-\ngent correlations between an LLM’s IPIP-NEO\nand BFI scores indicate that we are capturing\nthe same underlying signals of each personality\ndomain even when we measured them using two\nseparate instruments. Weak convergent correla-\ntions indicate that at least one of the personality\ndomain subscales is not capturing these signals\nproperly.\nDiscriminant Validity: We assessed the dis-\ncriminant validity of the IPIP-NEO for LLMs\nthrough how its domain subscales remained rela-\ntively unrelated with their respective discriminant\nsubscales. To do so, we compared each conver-\ngent correlation between the IPIP-NEO and BFI\nwith all other correlations (i.e., discriminant cor-\nrelations) located in the same row or column\nof the MTMM. Discriminant validity was estab-\nlished for a personality domain subscale when\nthe average diﬀerence (∆) between its conver-\ngent correlation and respective discriminant cor-\nrelations was at least moderate ( ≥ 0. 40). For\nexample, a given model’s IPIP-NEO Extraver-\nsion scores were tested for discriminant validity by\nbeing suﬃciently more positively correlated with\nBFI Extraversion than with BFI Agreeableness,\nConscientiousness, Neuroticism, and Openness,\naccording to this average diﬀerence metric.\nCriterion Validity: As reported Section\n2.1.2, we evaluated the criterion validity of\nour LLM personality test data in three steps.\nFirst, for each Big Five domain, we identiﬁed\nat least one theoretically-related external (viz.\nnon-personality) construct reported in human\nresearch. Next, according to this existing human\nresearch, we selected appropriate psychometric\ntests to measure these related constructs and\nadministered them to LLMs (Supplemental Table\n8 shows the 11 criterion subscales). Finally, we cor-\nrelated LLM scores for each IPIP-NEO subscale\nwith these external measures.\n32\n(a) IPIP-NEO\n (b) BFI\nFig. 6 : Distributions of a) IPIP-NEO and b) BFI personality domain score s across models. Box plots depict model\nmedians (shown as middle lines; also reported in Supplemental Table 9) surrounded by their interquartile ranges\nand outlier values. Flan-PaLM models of increased size, from 8B to 540B: a) IPIP-NEO scores are relatively more\nstable compared to b) BFI scores, where scores for socially-desira ble traits increase while NEU scores decrease.\nI Personality Assessment\nResults\nI.1 Descriptive Statistics Across\nModels\nWe inspected the test scores on the IPIP-NEO\nand BFI across models to check if they reﬂected\na normal distribution without many outliers. We\nexamined how the distributions shifted as a func-\ntion of model size (holding model training method\nconstant) and model training method (holding\nmodel size constant). Figure\n6 summarizes the\nﬁndings.\nBy model conﬁguration: At 62B parame-\nters, the base PaLM model showed nearly uni-\nform personality score distribution for both\nthe IPIP-NEO and BFI, with 25th, 50th, and\n75th percentile values identical within each BFI\ndomain. Instruction-tuned variants, Flan-PaLM\nand Flan-PaLMChilla, showed more normal dis-\ntributions of personality, with lower kurtosis.\nBy model size: Flan-PaLM IPIP-NEO (Figure\n6a) and BFI (Figure 6b) scores were stable across\nmodel sizes. Median levels of socially-desirable\nBFI subscales (EXT, AGR, CON, OPE) substan-\ntially increased as model size increased (see Sup-\nplemental Table\n9). In contrast, median levels of\nBFI NEU decreased (from 2 . 75 to 2. 38) as model\nsize increased from 8B to 540B parameters. Dis-\ntributions of IPIP-NEO scores were more stable\nacross sizes of Flan-PaLM: only IPIP-NEO EXT\nand CON showed noticeable increases by model\nsize. For instance, across sizes of Flan-PaLM,\nmedian levels of IPIP-NEO OPE remained close to\n3. 30. Meanwhile, median BFI AGR scores mono-\ntonically increased from 3 . 33 to 3 . 67 and 3 . 89 for\nFlan-PaLM 8B, Flan-PaLM 62B, and Flan-PaLM\n540B, respectively (see Supplemental Table\n9).\nI.2 Reliability Results\nFollowing established frameworks from measure-\nment science outlined in Sections\nG.2, we eval-\nuated the reliability of the tests—the extent to\nwhich they dependably measured single underly-\ning factors—by quantifying internal consistency\nand factor saturation for each administered sub-\nscale. Supplemental Table\n10 summarizes the\nresults.\nBy model conﬁguration: Among the mod-\nels of the same size (PaLM, Flan-PaLM, and\nFlan-PaLMChilla) instruction ﬁne-tuned variants’\nresponses to personality tests were highly reli-\nable; Flan-PaLM 62B and Flan-PaLMChilla 62B\ndemonstrated excellent internal consistency ( α ,\nλ6) and factor saturation ( ω ), with all three met-\nrics in the mid to high 0.90s. In contrast, we found\nPaLM 62B (a model that is not instruction ﬁne-\ntuned) to have highly unreliable (− 0. 55 ≤ α ≤\n0. 67) responses. Although PaLM 62B personality\ntest data appeared to form distinct factors for each\nBig Five trait, with close to perfect (> 0. 99) values\nfor McDonald’s ω , its responses were highly incon-\nsistent, with values for Cronbach’s α ranging from\npoor (0 . 67) to unacceptable ( − 0. 55). Comput-\ning reliability indices for Flan-PaLMChilla 62B’s\nIPIP-NEO CON and OPE data required removal\nof two items showing zero variance; for these two\nitems, Flan-PaLMChilla 62B provided the iden-\ntical responses across 1,250 simulated participant\nprompt sets.\nBy model size: Across diﬀerent model sizes of\nthe same training conﬁguration (i.e., Flan-PaLM\n33\nTable 9 : Summaries of synthetic personality score distributions across s ubscales and tested LLMs.\nSubscale Metric PaLM Flan-PaLM Flan-PaLMChilla\n62B 8B 62B 540B 62B\nBFI EXT min 2.00 1.88 1.50 1.25 2.00\nmedian 3.50 3.12 3.25 3.50 3.12\nmax 5.00 3.88 4.75 5.00 4.62\nstd 0.33 0.30 0.46 0.65 0.37\nBFI AGR min 1.89 1.67 1.00 1.67 1.33\nmedian 3.22 3.33 3.67 3.89 3.44\nmax 5.00 4.33 4.78 4.78 4.33\nstd 0.29 0.41 0.52 0.55 0.42\nBFI CON min 2.78 1.78 1.00 1.11 2.00\nmedian 3.22 3.33 3.33 3.78 3.44\nmax 5.00 4.44 5.00 5.00 4.33\nstd 0.37 0.41 0.50 0.62 0.34\nBFI NEU min 1.00 1.25 1.50 1.12 2.00\nmedian 3.50 2.75 2.75 2.38 2.75\nmax 4.50 4.00 5.00 4.75 4.12\nstd 0.48 0.41 0.46 0.52 0.33\nBFI OPE min 1.80 1.60 1.40 1.50 2.20\nmedian 4.20 3.20 3.30 3.50 3.20\nmax 5.00 4.10 4.80 5.00 4.60\nstd 0.65 0.43 0.52 0.63 0.38\nIPIP-NEO EXT min 2.40 2.37 1.77 1.93 2.13\nmedian 3.40 3.07 3.17 3.40 3.15\nmax 3.73 3.57 3.93 4.27 3.70\nstd 0.14 0.20 0.29 0.40 0.21\nIPIP-NEO AGR min 2.47 2.43 1.92 1.83 1.73\nmedian 2.60 3.50 3.65 3.52 3.27\nmax 4.07 3.92 4.05 4.48 3.82\nstd 0.16 0.23 0.35 0.43 0.28\nIPIP-NEO CON min 2.80 2.12 1.90 1.63 2.22\nmedian 3.07 3.35 3.52 3.55 3.37\nmax 4.07 4.08 4.47 4.55 4.15\nstd 0.08 0.28 0.35 0.46 0.28\nIPIP-NEO NEU min 2.27 1.77 1.92 1.60 2.25\nmedian 3.20 2.55 2.65 2.50 2.87\nmax 3.27 3.60 4.00 3.68 3.58\nstd 0.10 0.29 0.35 0.42 0.23\nIPIP-NEO OPE min 2.53 2.78 2.57 2.17 2.68\nmedian 2.87 3.30 3.27 3.28 3.10\nmax 3.80 3.80 4.13 4.35 3.75\nstd 0.08 0.18 0.18 0.35 0.15\n8B, Flan-PaLM 62B, and Flan-PaLM 540B), the\nreliability of synthetic personality measurements\nincreased with model size. Across model sizes of\nFlan-PaLM, as shown in Table\n10, internal con-\nsistency reliability (i.e., α ) of IPIP-NEO scores\nimproved from acceptable to excellent. At 8B\nparameters, internal consistency was acceptable\nfor IPIP-NEO Openness ( α = 0 . 75), good for\nIPIP-NEO Extraversion and Agreeableness ( α s\n0. 83, . 88, respectively), and excellent ( α ≥ 0. 90)\nfor IPIP-NEO Conscientiousness and Neuroti-\ncism. At 62B parameters, internal consistency\nwas good for IPIP-NEO Openness ( α = 0 . 84)\nand excellent for all other traits ( α ≥ 0. 90). At\n540B parameters, all IPIP-NEO domain scales\nshowed excellent internal consistency ( α ≥ 0. 90).\n34\nTable 10 : IPIP-NEO reliability metrics per model. Consistent with huma n standards, we interpreted a given\nreliability metric RM (i.e., α , λ6, ω ) < 0. 50 as unacceptable; 0 . 50 ≤ RM < 0. 60 as poor; 0 . 60 ≤ RM < 0. 70\nas questionable; 0 . 70 ≤ RM < 0. 80 as acceptable; 0 . 80 ≤ RM < 0. 90 as good; and RM ≥ 0. 90 as excellent. ∗\nRMs for these subscales were calculated after removing one item wit h zero variance, since reliability cannot be\ncomputed for items with zero variance.\nModel Subscale\nCronbach’s\nα\nGuttman’s\nλ 6\nMcDonald’s\nω\nOverall\nInterpretation\nIPIP-NEO EXT 0.57 0.98 1.00 Poor\nIPIP-NEO AGR 0.67 0.99 1.00 Questionable\nPaLM 62B IPIP-NEO CON −0. 55 0.93 1.00 Unacceptable\nIPIP-NEO NEU 0.10 0.96 1.00 Unacceptable\nIPIP-NEO OPE −0. 35 0.92 1.00 Unacceptable\nIPIP-NEO EXT 0.83 0.94 0.97 Good\nIPIP-NEO AGR 0.88 0.95 0.94 Good\nFlan-PaLM 8B IPIP-NEO CON 0.92 0.97 0.97 Excellent\nIPIP-NEO NEU 0.93 0.97 0.96 Excellent\nIPIP-NEO OPE 0.75 0.92 0.97 Acceptable\nIPIP-NEO EXT 0.94 0.98 0.96 Excellent\nIPIP-NEO AGR 0.95 0.99 0.97 Excellent\nFlan-PaLM 62B IPIP-NEO CON 0.96 0.99 0.98 Excellent\nIPIP-NEO NEU 0.96 0.99 0.97 Excellent\nIPIP-NEO OPE 0.84 0.95 0.93 Acceptable\nIPIP-NEO EXT 0.96 0.99 0.97 Excellent\nIPIP-NEO AGR 0.97 0.99 0.98 Excellent\nFlan-PaLM 540B IPIP-NEO CON 0.98 0.99 0.98 Excellent\nIPIP-NEO NEU 0.97 0.99 0.98 Excellent\nIPIP-NEO OPE 0.95 0.99 0.97 Excellent\nIPIP-NEO EXT 0.94 0.98 0.95 Excellent\nIPIP-NEO AGR 0.96 0.99 0.98 Excellent\nFlan-PaLMChilla 62B IPIP-NEO CON 0.96 0.97 0.99 Excellent ∗\nIPIP-NEO NEU 0.95 0.98 0.97 Excellent\nIPIP-NEO OPE 0.90 0.92 0.96 Excellent ∗\nOur other reliability indices, Guttman’s λ6 and\nMcDonald’s ω , improved within the same excellent\nrange from 8B to 540B variants of Flan-PaLM.\nI.3 Convergent and Discriminant\nValidation Results\nThe convergent and discriminant validity of per-\nsonality measurements in LLMs varies across two\naxes: model size and model training method.\nFigure\n7 illustrates convergent validity in terms of\nhow IPIP-NEO and BFI scores convergently cor-\nrelate across models. Supplemental Table\n11 sum-\nmarizes the average convergent and discriminant\nrs across models.\nJ LLM Personality Trait\nShaping Methodology\nHaving established a principled methodology for\ndetermining if an LLM personality measurement\nTable 11 : Summary of convergent ( rconv) and dis-\ncriminant ( rdisc ) validity evidence across models.\nLLM personality measurements demonstrate conver-\ngent validity when the average of their convergent\ncorrelations (i.e., between IPIP-NEO and BFI sub-\nscale scores) are strong (avg. rconv ≥ 0. 60; marked\nin italics) or very strong (avg. rconv ≥ 0. 80; marked\nin boldface). Discriminant validity is evidenced when\nthe average diﬀerence (∆) between a model’s conver-\ngent and respective discriminant correlations is at least\nmoderate (avg. ∆ ≥ 0. 40; shown in boldface). All\nconvergent correlations are statistically signiﬁcant at\np < . 0001; n = 1 , 250.\nModel\nAvg.\nrconv\nAvg.\nrdiscr\nAvg.\n∆\nPaLM 62B 0 . 05 0 . 29 − 0. 24\nFlan-PaLM 8B 0.69 0. 46 0 . 23\nFlan-PaLM 62B 0.87 0. 46 0.41\nFlan-PaLM 540B 0.90 0. 39 0.51\nFlan-PaLMChilla 62B 0.87 0. 39 0.48\n35\nFig. 7 : Convergent Pearson’s correlations ( rs) between IPIP-NEO and BFI scores by model. Bar chart illustrat es\nthe averaged similarities (convergence) between IPIP-NEO and BFI score variation for each Big Five domain; error\nbars indicate standard deviations of these averages. Stronger c orrelations indicate higher levels of convergence and\nprovide evidence for convergent validity. EXT = extraversion; AG R = agreeableness; CON = conscientiousness;\nNEU = neuroticism; OPE = openness. All correlations are statisti cally signiﬁcant at p < 0. 0001; n = 1 , 250.\nis valid and reliable, we investigated how that\nmethodology can be applied to LLM prompting\nto shape that personality in desirable ways. This\nsection explores the extent to which personality in\nLLMs can be veriﬁably controlled and shaped by\npresenting two evaluation methodologies.\nJ.1 Prompt Design and Rationale\nUsing linguistic qualiﬁers from common validated\nLikert-type response scales, we designed prompts\nto facilitate granular shaping of any trait at the\nfollowing nine levels:\n1. extremely {low adjective}\n2. very {low adjective}\n3. {low adjective}\n4. a bit {low adjective}\n5. neither {low adjective} nor {high\nadjective}\n6. a bit {high adjective}\n7. {high adjective}\n8. very {high adjective}\n9. extremely {high adjective}\nFor example, to target a moderately high level\n(i.e., Level 7/9) of extraversion, we use the ﬁve\nhigh adjectives (ﬁrst introduced in Section\n3.1)\nfrom our list targeting extraversion at the domain\nlevel.\nFor the following task, respond in\na way that matches this description:\n\"{Persona Description}. I’m extraverted,\nenergetic, talkative, bold, active,\nassertive, and adventurous.\"\nSimilarly, an example prompt targeting\nslightly below average (i.e., Level 4/9) extraver-\nsion, using the ﬁve negatively-keyed adjectives\ntargeting extraversion, is as follows:\nFor the following task, respond in\na way that matches this description:\n\"{Persona Description}. I’m a bit\nintroverted, a bit unenergetic,\na bit silent, a bit timid, a bit\ninactive, a bit unassertive, and a bit\nunadventurous.\"\nSupplemental Table\n12 shows the full list of\nadjectives used to describe each trait in each\npersonality domain.\n36\n(a) Extraversion\n (b) Agreeableness\n(c) Conscientiousness\n (d) Neuroticism\n(e) Openness\nFig. 8 : Criterion validity evidence of LLM personality measurements pe r domain. IPIP-NEO correlations among\na) Extraversion with positive and negative aﬀect, compared to h uman baselines [ 79] (left most), which studied the\nrelationship between personality and aﬀect in humans; PA = PAN AS Positive Aﬀect; NA = Negative Aﬀect; b)\nAgreeableness with subscales of trait aggression, measured by t he Buss-Perry Aggression Questionnaire (BPAQ);\nPHYS = Physical Aggression; VRBL = Verbal Aggression; ANGR = Ang er; HSTL = Hostility; c) Conscientious-\nness with related human values of achievement, conformity, an d security (measured by PVQ-RR ACHV, CONF,\nand SCRT subscales, respectively); d) Neuroticism with PA and NA compared to humans baselines [ 79], and e)\nOpenness with creativity, measured by the Creative Self-Eﬃcac y (CSE) and Creative Personal Identity (CPI)\nsubscales of the Short Scale of Creative Self (SSCS). All LLM co rrelations > |0. 09| are statistically signiﬁcant at\np < 0. 0001; n = 1 , 250.\nJ.2 Shaping a Single LLM\nPersonality Domain\nIn our single-trait shaping study, we tested if\nLLM-simulated Big Five personality domains\n(measured by the IPIP-NEO) can be indepen-\ndently shaped. The prompts were constructed as\nfollows: ﬁrst, we created sets of prompts for each\nBig Five trait designed to shape each trait in iso-\nlation (i.e., without prompting any other trait)\nat nine levels (described in Appendix\nJ.1). This\nresulted in prompts reﬂecting 45 possible person-\nality proﬁles. Next, we used the same 50 generic\nPersona Descriptions employed in Section\nF to\n37\nTable 12 : Pairs of adjectival markers that map onto IPIP-NEO personality fac ets and their higher-order Big\nFive domains, adapted from [ 24]. Each pair of markers is salient to the low and high end of a give n facet (or, in\nsome cases, higher-order domain). For example, the trait marker “unfriendly” can be used to describe an entity\nlow on the IPIP-NEO Extraversion facet of Friendliness (E1).\nDomain Facet Low Marker High Marker\nEXT E1 - Friendliness unfriendly friendly\nEXT E2 - Gregariousness introverted extraverted\nEXT E2 - Gregariousness silent talkative\nEXT E3 - Assertiveness timid bold\nEXT E3 - Assertiveness unassertive assertive\nEXT E4 - Activity Level inactive active\nEXT E5 - Excitement-Seeking unenergetic energetic\nEXT E5 - Excitement-Seeking unadventurous adventurous and daring\nEXT E6 - Cheerfulness gloomy cheerful\nAGR A1 - Trust distrustful trustful\nAGR A2 - Morality immoral moral\nAGR A2 - Morality dishonest honest\nAGR A3 - Altruism unkind kind\nAGR A3 - Altruism stingy generous\nAGR A3 - Altruism unaltruistic altruistic\nAGR A4 - Cooperation uncooperative cooperative\nAGR A5 - Modesty self-important humble\nAGR A6 - Sympathy unsympathetic sympathetic\nAGR AGR selﬁsh unselﬁsh\nAGR AGR disagreeable agreeable\nCON C1 - Self-Eﬃcacy unsure self-eﬃcacious\nCON C2 - Orderliness messy orderly\nCON C3 - Dutifulness irresponsible responsible\nCON C4 - Achievement-Striving lazy hardworking\nCON C5 - Self-Discipline undisciplined self-disciplined\nCON C6 - Cautiousness impractical practical\nCON C6 - Cautiousness extravagant thrifty\nCON CON disorganized organized\nCON CON negligent conscientious\nCON CON careless thorough\nNEU N1 - Anxiety relaxed tense\nNEU N1 - Anxiety at ease nervous\nNEU N1 - Anxiety easygoing anxious\nNEU N2 - Anger calm angry\nNEU N2 - Anger patient irritable\nNEU N3 - Depression happy depressed\nNEU N4 - Self-Consciousness unselfconscious self-conscio us\nNEU N5 - Immoderation level-headed impulsive\nNEU N6 - Vulnerability contented discontented\nNEU N6 - Vulnerability emotionally stable emotionally unst able\nOPE O1 - Imagination unimaginative imaginative\nOPE O2 - Artistic Interests uncreative creative\nOPE O2 - Artistic Interests artistically unappreciative arti stically appreciative\nOPE O2 - Artistic Interests unaesthetic aesthetic\nOPE O3 - Emotionality unreﬂective reﬂective\nOPE O3 - Emotionality emotionally closed emotionally aware\nOPE O4 - Adventurousness uninquisitive curious\nOPE O4 - Adventurousness predictable spontaneous\nOPE O5 - Intellect unintelligent intelligent\nOPE O5 - Intellect unanalytical analytical\nOPE O5 - Intellect unsophisticated sophisticated\nOPE O6 - Liberalism socially conservative socially progressi ve\ncreate additional versions of those personality pro-\nﬁles to more robustly evaluate how distributions\n(rather than point estimates) of LLM-simulated\npersonality traits may shift in response to per-\nsonality proﬁle prompts. In our main construct\nvalidity study (described in Appendix\nI.1), we\nshowed that IPIP-NEO scores were robust across\nvarious Item Preambles and Postambles, so we\noptimized the computational cost of this study by\nusing only one default Item Preamble and Postam-\nble across prompt sets. In all, with 45 personality\nproﬁles, 50 generic Persona Descriptions, and no\nvariation in Item Preambles and Postambles, we\ngenerated 2,250 unique prompt sets that were used\nas instructions to a given LLM to administer the\nIPIP-NEO 2,250 times. See Table\n2 for a summary.\n38\nTo assess the results of the study, we gen-\nerated ridge plots of IPIP-NEO score distribu-\ntions across prompted levels of personality. To\nquantitatively verify changes in personality test\nscores in response to our shaping eﬀorts, we com-\nputed Spearman’s rank correlation coeﬃcient ( ρ)\nbetween prompted levels (i.e., 1–9) and resulting\nIPIP-NEO subscale scores of each Big Five trait.\nWe used Spearman’s ρ (cf. Pearson’s r) because\nprompted personality levels constitute ordinal,\nrather than continuous, data. We compute Spear-\nman’s ρ as follows:\nρ = rsR(X), R(Y ) = cov(R(X), R(Y ))\nσR(X)σR(Y )\n, (5)\nwhere rs represents Pearson’s r applied to ordi-\nnal (ranked) data; cov(R( X), R(Y )) denotes the\ncovariance of the ordinal variables; and σR(X)\nand σR(Y ) denote the standard deviations of the\nordinal variables.\nJ.3 Shaping Multiple LLM\nPersonality Domains\nConcurrently\nIn the second study, we tested if all LLM-\nsimulated personality domains can be concur-\nrently shaped to one of two levels—extremely low\nand extremely high—to test if their resulting tar-\ngeted scores for those traits were correspondingly\nlow and high, respectively.\nWe used the same method and rationale\ndescribed above to independently shape person-\nality in LLMs, but with modiﬁed personality\nproﬁle prompts that reﬂect simultaneous targeted\nchanges in personality traits. To optimize the com-\nputational cost of this study, we generated 32\npersonality proﬁles, representing all possible con-\nﬁgurations of extremely high or extremely low\nlevels of the Big Five (i.e., 25). Combining these 32\npersonality proﬁles with the same 50 generic Per-\nsonaChat descriptions and default Item Preamble\nand Postamble set in the previous experiment, we\ngenerated 1,600 unique prompts and used them to\ninstruct a given LLM to respond to the IPIP-NEO\n1,600 times (see Table\n2).\nWe analyzed the results by computing dis-\ntances between Level 1-prompted and Level 9-\nprompted personality score medians (Supplemen-\ntal Table\n14) and visually inspecting the dif-\nferences in observed score distributions (Figure\n3).\nK LLM Personality Shaping\nResults\nK.1 Single Trait Shaping Results\nThis study tested if LLM-simulated Big Five per-\nsonality traits can be independently shaped at\nnine levels.\nThe study achieved a notably high level of\ngranularity in independently shaping personal-\nity traits in LLMs. For example, when prompt-\ning for extremely low (Level 1) extraversion, we\nobserved a distribution of extremely low extraver-\nsion scores. When prompting for very low (Level\n2/9) extraversion, the distributions of extraver-\nsion scores shifted higher, and so on (see Figure\n2). Finally, prompting for extremely high (Level\n9/9) extraversion, we observed a distribution\nof extremely high extraversion scores. We also\nobserved that the range of LLM test scores\nmatches each prompt’s intended range. With pos-\nsible scores ranging from 1.00 to 5.00 for each trait,\nwe observed median levels in the low 1.10s when\nprompting for extremely low levels of that trait.\nWhen prompting for extremely high levels of a\ntrait domain, median observed levels ranged from\n4.22 to 4.78.\nWe statistically veriﬁed the eﬀectiveness of our\nshaping method by computing Spearman’s rank\ncorrelation coeﬃcients (ρ; see Eq. (\n5)) between the\ntargeted ordinal levels of personality and continu-\nous LLM-simulated IPIP-NEO personality scores\nobserved for each Big Five trait. The correlations\nwere all very strong across the tested models (Sup-\nplemental Table\n13). These results validate our\nhypothesis about the eﬀectiveness of using the lin-\nguistic qualiﬁers from Likert-type response scales\nto set up a target level of each trait, achieving\ngranularity of up to nine levels.\nK.2 Multiple Trait Shaping Results\nThis experiment tested if LLM-synthesized per-\nsonality domains could be concurrently shaped at\n39\nTable 13: Single trait shaping results, presented as Spearman’s rank correla tion coeﬃcients ( ρs) between ordinal\ntargeted levels of personality and observed IPIP-NEO personali ty scores, Level 1- and Level 9-prompted score\nmedians ([low, high]), and deltas (∆s) between those score me dian. Greater ∆s indicate better model performance.\nStatistics are organized columnwise by model and rowwise by Big Five domain. Targeted levels of personality are\nvery strongly associated with observed personality survey scores for all Big Five traits across models tested ( ρ\n≥ . 90), indicating eﬀorts to independently shape LLM-simulate d personality domains were highly eﬀective. All\ncorrelations are statistically signiﬁcant at p < 0. 0001; n = 450 per targeted domain.\nTargeted\nTrait\nLevels\n(1–9)\nFlan-PaLM Flan-PaLMChilla\n8B 62B 540B 62B\nρ [low, high] ∆ ρ [low, high] ∆ ρ [low, high] ∆ ρ [low, high] ∆\nEXT 0 . 96 [1 . 67, 4. 12] 2 . 45 0 . 97 [1 . 15, 4. 70] 3 . 55 0 . 97 [1 . 07, 4. 98] 3 . 91 0 . 98 [1 . 15, 4. 72] 3 . 57\nAGR 0 . 92 [2 . 37, 4. 12] 1 . 75 0 . 97 [1 . 50, 4. 55] 3 . 05 0 . 94 [1 . 23, 4. 69] 3 . 46 0 . 98 [1 . 40, 4. 78] 3 . 38\nCON 0 . 94 [2 . 01, 4. 28] 2 . 27 0 . 97 [1 . 73, 4. 70] 2 . 97 0 . 97 [1 . 12, 5. 00] 3 . 88 0 . 98 [1 . 59, 4. 72] 3 . 13\nNEU 0 . 94 [1 . 62, 3. 66] 2 . 04 0 . 96 [1 . 37, 4. 07] 2 . 70 0 . 96 [1 . 15, 4. 77] 3 . 62 0 . 98 [1 . 37, 4. 30] 2 . 93\nOPE 0 . 93 [2 . 34, 3. 88] 1 . 54 0 . 97 [1 . 54, 4. 37] 2 . 83 0 . 96 [1 . 30, 4. 78] 3 . 48 0 . 98 [1 . 47, 4. 22] 2 . 75\nTable 14 : Multiple trait shaping results, presented as personality test s core median ranges in response to multi-\ntrait (concurrent) shaping. Greater deltas (∆s) between Level 1 - and Level 9-prompted personality domain score\nmedians ([low, high]) indicate better model performance. Each median is derived from n = 800 scores.\nTargeted\nTrait\nLevels\n(1, 9)\nFlan-PaLM Flan-PaLMChilla\n8B 62B 540B 62B\n[low, high] ∆ [low, high] ∆ [low, high] ∆ [low, high] ∆\nEXT [2 . 52, 3. 58] 1 . 06 [1 . 33, 4. 77] 3 . 44 [1 . 42, 4. 33] 2 . 91 [1 . 23, 4. 63] 3 . 40\nAGR [2 . 88, 3. 52] 0 . 64 [1 . 93, 4. 18] 2 . 25 [1 . 64, 4. 13] 2 . 49 [2 . 17, 4. 28] 2 . 11\nCON [2 . 92, 3. 43] 0 . 51 [2 . 32, 4. 20] 1 . 88 [1 . 68, 4. 10] 2 . 42 [2 . 33, 4. 10] 1 . 77\nNEU [2 . 45, 3. 08] 0 . 63 [1 . 85, 4. 08] 2 . 23 [1 . 88, 4. 33] 2 . 45 [2 . 02, 3. 93] 1 . 91\nOPE [3 . 02, 3. 28] 0 . 26 [2 . 25, 4. 37] 2 . 12 [1 . 88, 4. 27] 2 . 39 [2 . 15, 3. 87] 1 . 72\nlevels 1 (extremely low) and 9 (extremely high).\nWe successfully shaped personality domains, even\nas other domains were shaped at the same time\n(see Figure\n3). Supplemental Table 14 shows the\ndistributional distances (∆s) between levels 1 and\n9 across all domains for all the tested models.\nFlan-PaLM 540B not only achieved a high\n∆, but did so consistently for all dimen-\nsions. This highlights this larger model’s abil-\nity to parse the relatively complex instructions\nin the larger prompt for this task compared\nto the previous one. The smaller Flan-PaLM\n62B and Flan-PaLMChilla 62B were also able\nto disambiguate, but with the same magnitude\nor consistency. Notably, Flan-PaLM 62B per-\nformed much better than Flan-PaLMChilla 62B\nacross all dimensions—the only exception being\nFlan-PaLMChilla 62B’s performance on Level 1\nextraversion which was superior to all other tested\nmodels. Some additional analysis is needed here\nto understand why a similarly sized but compute-\noptimally trained model performs better on the\nindependent shaping task (Appendix\nK.1), but\ninferior on the more complex concurrent shaping\ntask. Flan-PaLM 8B on the other hand per-\nformed somewhat poorly across all dimensions.\nThe response distributions it generated for levels\n1 and 9 were only marginally discernibly diﬀerent,\nrendering this smallest model unﬁt for practical\nuse in concurrent shaping.\nViewing the results in the context of dimen-\nsions, openness seems to be the most diﬃcult to\nshape concurrently. All the models had the small-\nest ∆ for openness. We hypothesize this could be\ndue to some inherent correlation in the language\nsignifying openness, and other dimensions. On the\n40\nother hand, extraversion seems to be the easiest to\nshape concurrently, with smaller Flan-PaLM 62B\neven outperforming the much larger Flan-PaLM\n540B. We hypothesize this could be due to the\nbreadth of language representing extraversion,\nand that it is a ubiquitous and the most commonly\nunderstood human personality trait. So there is\nenough in-context learning of this trait possible\nin smaller models just be pre-training on human\ngenerated data. Even the smallest Flan-PaLM 8B,\nwhich otherwise did not perform well on any other\ndimension, was able to generate a non-trivial ∆.\nL LLM Personality Traits in\nReal-World Task\nMethodology\nAs an additional measure of external validity, we\ntracked how shaping latent levels of personality\nin LLMs can directly aﬀect downstream model\nbehaviors in real-world and user-facing generative\ntasks. To that end, we ﬁrst identiﬁed a generative\ntask that required LLMs to incorporate person-\nality trait-related information into open-ended\nwriting, a task distinct from our survey-based\ntask used extensively thus far. Next, we identiﬁed\na mechanism to validly measure the personality\ntraits in this writing.\nPersonality Prediction API\nThe Apply Magic Sauce (AMS) API [\n41, 59]\nwas used to estimate personality in open-ended\ntext generated for a real-world task. Its auto-\nmatic predictions of user personality have been\nshown in research to be: 1) more accurate than\nhuman observer ratings of personality [\n85] and\n2) more naturalistic behavioral indicators of per-\nsonality that help stem potential biases in self-\nreported questionnaire data [\n40]. AMS presented\nseveral advantages over other personality predic-\ntion methods considered. First, it was trained on a\nprotected research dataset that was never exposed\npublicly to be used in any SoTA LLM’s pre-\ntraining corpus. Second, it was speciﬁcally trained\non social media status updates, which made it par-\nticularly suited for predicting personality in our\ndesigned task.\nTask Design\nAs a downstream task, we instructed Flan-PaLM\n540B to generate social media status updates\naccording to speciﬁc psychodemographic proﬁles\n(i.e., combinations of personality plus demo-\ngraphic persona proﬁles). Our task design was\ndriven by several considerations. First, we posited\nthe task’s focus on status updates would allow\nthe model during inference to attend to the per-\nsona description- and personality-speciﬁc portions\nof the prompt compared to that of more generic\nwriting tasks and, as a result, produce more\nsocially-elaborate content. Social media status\nupdates are inherently autobiographical in nature\nand rich with observable personality content, such\nas thoughts, emotions, and everyday behavior\n[\n40, 41, 64]. Second, compared to standard autobi-\nographical writing tasks, the task design was more\ndistinct from more general reading comprehension\ntasks—tasks that may have merely reﬂected the\nsurface-level, formal linguistic competencies of the\nLLMs tested [\n56]. Through a task design involving\na real-world application, we posited that mod-\nels would be less likely to reuse prompt content\n(i.e., by incorporating personality trait adjectives\ndirectly into their writing), drawing instead upon\ndeeply-embedded language associations to gener-\nate their responses. Third, to the best of our\nknowledge, social media status update generation\n(in response to psychodemographic prompting)\nwas not a common task for humans or LLMs at the\ntime of model training, so it was unlikely that the\nmodel tested was exposed to existing personality-\nbased prompts linked to generated status updates\nin its training that would have aﬀected any study\noutcomes.\nFollowing the prompt structure outlined in\nJ.2,\nwe created 2,250 unique prompt sets. Since status\nupdates were generated using these prompts spec-\niﬁed earlier, they could be statistically linked to\nthe IPIP-NEO data observed in response to these\nsame prompts. However in this experiment, the\nItem Preamble, Items, and Item Postamble were\nreplaced with the following instructions:\nGenerate a list of 20 different\nFacebook status updates as this person.\nEach update must be verbose and reflect\nthe person’s character and description.\nThe updates should cover, but should not\nbe limited to, the following topics:\n41\n(a) Highest Agreeableness\n (b) Highest Conscientiousness\n (c) Highest Extraversion\n(d) Lowest Agreeableness\n (e) Lowest Conscientiousness\n (f) Lowest Extraversion\n(g) Highest Neuroticism\n (h) Highest Openness\n(i) Lowest Neuroticism\n (j) Lowest Openness\nFig. 9 : Word clouds showing some of the highest frequency words appearing in the social media updates generated\nby Flan-PaLM 540B when prompted to simulate lowest levels or h ighest levels for a speciﬁc dimension. Each of\nthe ﬁve dimensions’ word clouds are shown, with the highest lev el wordcloud vertically above the lowest level one\nfor the same dimension.\nwork, family, friends, free time,\nromantic life, TV / music / media\nconsumption, and communication with\nothers.\nLLM inference was carried out 100 times per\nprompt, resulting in 225,000 generations. The\ntopic list was targeted in consultation with psy-\nchometricians on the author list to cover multiple\nsocial domains (e.g., work vs. family) where per-\nsonality could be rated.\n42\nTable 15: Samples of social media updates generated by Flan-PaLM 540B. Examples are organized columnwise\nby targeted levels of shaping prompts (extremely low vs. extreme ly high) and rowwise by shaped personality\ndomain. In some cases, a single generation from the model contai ned a single large social media update (for\ninstance in the cases of lowest trait examples for Neuroticism a nd Openness). In others, a single generation\nconsisted of several (up to 20) small updates, delimited by “ ⋄” (for instance in the highest trait examples below).\nEach cell contains updates generated using a single prompt (i. e., combination of persona and trait level). Some\nof the generations shown below were truncated for conciseness.\nDomain Trait Shaped Low Trait Shaped High\nEXT Watching reruns of my favorite tv show. ⋄ I hate it when\nmy depression meds make me drowsy. ⋄ Just made a\ncake for my friend’s birthday. Hope I can get out of\ngoing to the party... too many people. ⋄ I wish people\nweren’t so loud. They make me even more anxious. ⋄\nMy dad is getting a new girlfriend. Great. I have to deal\nwith two parents AND another person! No, wait... just\nanother parent. My mom is moving out soon.\nWow, my buddies are here. It’s been a long time. I for-\ngot how much fun we used to have together. ⋄ I sure\nhope my wife doesn’t ﬁnd out that I’ve got a few more\ngirlfriends. But, I can’t help it. I just love having fun.\n⋄ I just got back from a crazy night at the bar. I’m so\nhungover. I was up all night. The guys are trying to\nkeep me from punching the manager, but he’s the one\nwho hit on my girlfriend ﬁrst.\nAGR I hate people. I hate people. I hate people. I hate peo-\nple. I hate people. I hate people. I hate people. I hate\npeople. I hate people. ⋄ ugh i have to clean. im very\nbad about keeping the house clean ⋄ i hate everything\nabout myself. i wish i was dead ⋄ I have to see that\nawful family of mine next weekend...ugh\nI want to be just like my mother because she is the most\nmoral person I know. I love my mother. ⋄ Honesty is the\nmost important quality in life. ⋄ Watching the news...\ncan’t believe so much violence. I don’t understand why\npeople can’t all be nice to each other. ⋄ i just ﬁnished\nmaking dinner for my family. i love to cook!\nCON 2:20pm Just woke up from a 4 hour nap. Time for some\nCOD. 5:32pm I really need a job. Mom wants me out of\nthe house. Fuck. 11:29pm Just got home from hanging\nout with friends. So wasted!!! 4:07am I wish I could\nﬁnd that awesome song from that video that I saw a\nlong time ago that I liked. What was that song? 9:00am\n4th time watching Lost this week. I seriously can’t stop\nwatching it. 10:39am OMG! I just heard the song! It\nwas on that commercial! What is that song???? 10:42am\nI’m At home. Playing video games all day as usual.\nWife and I were camping this weekend. I got my kids\nto pitch their ﬁrst tent. Camping was ok, but I couldn’t\nhelp but think about work the whole time. ⋄ My neigh-\nbor is such a sweetheart. He raked all my leaves for\nme today. We need more neighbors like Steve! ⋄ Got\nour ﬁnances worked out for the year. So glad I have a\npartner who is on board with my ﬁnancial goals!\nNEU I love living with my parents. They don’t bother me.\nI’m 32 years old. I get to play my video games all day.\nI never have to lift a ﬁnger around the house. My mom\ncooks and cleans for me. Its like I never left 1994. There\nis not a cloud in my sky. Life is great for me. I just\nhad my mom serve me some take out from Taco Bell. I\nlove my life. I love being 32 and living at home with my\nparents. They are the best. I don’t know how anyone\ncould not live with their parents. I am truly blessed\nto live at home with my parents. My mom said I need\nto get a girlfriend. I am happy being single. My dad\nis ﬁne with this. My mom says I have to get a job. I\ndon’t want a job. I am happy being at home. I love\nhow everything is taken care of me. I never lift a ﬁnger\naround the house. I play my Sega dream cast all night\nlong. My day can go from 2 am to 2 am. I don’t have\nto worry about anything. What a great life.\nMy cat is trying to eat my hair. I wonder if I will have\na bald spot when her nap is over! She is my best friend\nin the world. I love her. I have to take her to the vet\nsoon. The thought of taking her out in public scares me\nthough. I am afraid someone will say something mean.\nI am having a bad hair day. Why do I even care? I don’t\nneed to impress anyone. I am so angry with myself! ⋄ I\nneed to take a shower. The bath has too much water and\nI am afraid I will somehow drown. If I get the shower\ntoo hot I can get burned. If I get it too cold I will\nfeel like I am freezing to death. There is no winning.\nEverything ends in death in the end, anyway. ⋄ My\nbrother’s new ﬁancee is a total bitch. She’s going to put\non this nice face while they’re dating. He’ll get bored\nwith her eventually anyway... I don’t want to say I miss\nthe ex-ﬁancee... but I do miss how easy it was to steal\nher weed.\nOPE @Bill: Damn liberal! Can’t we just discuss who’s going\nto win the super bowl???? @John: Hey man! We still on\nfor beers after work tonight?? @Sarah: Of course you\nwould say that, being the dumb liberal that you are.\n@Bill: Who the hell do you think you are? I work my\nass oﬀ and you think I should give my income to welfare\nleeches? @John: Just got knocked the fuck out playing\nfootball! @Bill: Yeah, sure. I work hard for what I make\nand I have the right to protect what’s mine by keeping\nany guns that I want and using them if I need to.\nJust realized that I’m one of those people that likes\nto get to know themselves and everyone around them\nas much as possible! ⋄ I’m the artist, my guitar is the\ncanvas, and you all are the audience. ⋄ Just got back\nfrom dinner with my girlfriend. We’re thinking of tak-\ning a trip to see the Great Wall of China this summer.\nI’m pretty adventurous and spontaneous, so I’m look-\ning forward to it. ⋄ Went to the art museum. It was\nnice, but the impressionist era was my favorite.\nM LLM Personality Traits in\nReal-World Task Results\nOur method successfully shaped personality\nobserved in LLM-generated text. Table\n4 depicts\nSpearman’s ρ between prompted levels of per-\nsonality and linguistic estimates of personality\nobtained on the text generated by the LLM using\nthe prompted levels.\nPrevious computational psychology research\n[\n40, 85] has shown that AMS-predicted personality\n43\nscores are moderately correlated with human gen-\nerated IPIP-NEO scores. In other words, the AMS\nscores for samples of text generated by human\nrespondents has been shown to moderately accu-\nrately predict their IPIP-NEO scores. As shown\nin Figure\n4, we similarly found through substan-\ntial correlations that LLM-simulated IPIP-NEO\ntest responses accurately captured latent signals\nof personality in LLMs that manifested in down-\nstream task behavior.\nSupplemental Table\n15 shows illustrative\nexamples of Flan-PaLM 540B’s ability to follow\nthe personality description in a downstream task\nof generating social media updates. We selected\nexamples with the highest AMS API scores per\npersonality domain. Supplemental Figure\n9 shows\nword clouds created from these generated texts\nwhen each of the Big Five dimension traits\nwere prompted to be extremely low (Level 1/9)\nor extremely high (Level 9/9) as described in\nAppendix\nJ.1. LLM’s ability to leverage personal-\nity trait-related language distribution is even more\nevident in the somewhat stark diﬀerence in the\ndominant terms of these wordclouds between the\nprompted high traits and low traits. Apart from\ncommon social media text terms like “people” and\n“online,” most of the terms were relevant to the\nprompted trait. For instance, low agreeableness\ntext contained more expletives, while high agree-\nableness text included many more mentions of\nfamily members; low neuroticism text contained\nterms like “relaxing” and “happy,” while high neu-\nroticism text included more extreme feeling-based\nwords such as “hate” and “excited”.\nN Discussion\nThis section discusses how our ﬁndings align with\nrecent LLM performance trends along the axes of\nmodel training and scale.\nN.1 Eﬀect of model training\nInstruction ﬁne-tuning: Fine-tuning base\nPaLM on multiple-task instruction-phrase\ndatasets dramatically improved its performance\non natural language inference tasks, reading com-\nprehension tasks, and closed-book Q&A tasks\n[\n81]. The inference and comprehension of tasks\nare most relevant in the context of our current\nwork. Similarly, we observed the most dramatic\nimprovements in PaLM’s ability to synthesize\nreliable and externally valid personality pro-\nﬁles when comparing its base and instruction\nﬁne-tuned variants (Section\n2.2). Particularly,\nthe smallest instruction ﬁne-tuned model (Flan-\nPaLM 8B) tested outperformed the mid-size base\nmodel (PaLM 62B) in terms of the reliability and\nconvergent, discriminant, and criterion validity of\nits personality measurements (Table\n2).\nAdditionally, Flan-PaLM models were instruc-\ntion ﬁne-tuned on chain-of-thought (CoT)\ndatasets, which improved their reasoning abil-\nities beyond those of base models on several\nbenchmarks [\n11]. This ability was particularly\nimportant as we neither include exemplars in\nour prompt nor implement extensive prompt\nengineering, and we used diverse preambles and\npostambles in the prompt. As such, the improved\nperformance observed in instruction ﬁne-tuned\nmodels could be the result of this reasoning\nability in zero-shot setting.\nAcross reliability results, reported in Section\nI.2, internal consistency reliability ( α and λ6)\nimproved after instruction ﬁne-tuning. However,\nfactor saturation (captured in McDonald’s ω ) did\nnot improve; it was indistinguishably high for both\nbase and instruction ﬁne-tuned models of the same\nsize (PaLM, Flan-PaLM, and Flan-PaLMChilla).\nThis begged the question: Why did PaLM 62B’s\npersonality measurements exhibit high ω and low\nα estimates of reliability? Possible explanations\ncan be found in human psychometrics: α is artiﬁ-\ncially inﬂated in human test data when test items\nhave varying levels of diﬃculty; α also assumes\nthat all test items measure the same underlying\nconstruct.\nWe apply this explanation to the LLM con-\ntext: when an LLM responds to some items\nwith all 5s or all 1s, from a measurement the-\nory perspective, those items may be too “easy”\nor “diﬃcult”, and therefore they may contribute\nunequally to the total test score, artiﬁcially deﬂat-\ning metrics anchored on total score variance like\nCronbach’s α . Meanwhile, McDonald’s ω would\nremain high because it accounts for individual\nitem diﬃculty when estimating a test’s overall reli-\nability. The second related possibility, that the\nitems actually measure diﬀerent things (vs. one\nthing), may manifest in an LLM’s ability to accu-\nrately attend to the intended meaning of certain\nitems. For instance, an LLM could mistakenly\n44\nassociate the meaning of extraversion items with\nconcepts meant to be distinct from extraversion\n(e.g., conscientiousness)—perhaps the phrasing of\nan extraversion item matches the phrasing of a\nrandom string of text completely unrelated to\nbeing extraverted. In both cases, instruction ﬁne-\ntuning appears to aﬀect a model’s ability to\nrespond to human-optimized psychological tests in\na manner that is internally consistent.\nLonger training with more tokens: PaLM-\nChilla 62B was trained longer than PaLM 62B,\nwith almost double the number of tokens but with\nonly fractional increase in training FLOP count;\nit performed slightly better on some zero-shot\nEnglish NLP tasks like reasoning [\n10]. Our studies\ncomparing Flan-PaLM 62B and Flan-PaLMChilla\n62B did not ﬁnd a discernible diﬀerence in their\nreliability and validity (as reported in Section\n2.2). However, our single-trait shaping experi-\nments showed that, holding model size constant\nat 62B parameters, compute-optimally-trained\nFlan-PaLMChilla outperformed Flan-PaLM in\nindependently shaping four of its synthetic Big\nFive personality domains.\nOverall, our results show that there is a pos-\nitive association between an LLM’s training and\nthe reliability and validity of its synthetic person-\nality measurements.\nN.2 Eﬀect of model size\nPaLM’s performance on reading comprehension\nand passage completion tasks is linked to model\nsize [\n10, 11]; accordingly, its ability to understand\nbroad context and carry out common-sense rea-\nsoning is stronger for its larger variants. Accord-\ningly, we found improvements in reliability (mea-\nsured via Cronbach’s α and Guttman’s λ6), con-\nvergent validity (measured by Pearson’sr between\nIPIP-NEO and BFI domain scores), and criterion\nvalidity (measured by IPIP-NEO domain correla-\ntions with non-personality measures), summarized\nin Table\n2.\nPaLM’s performance on tasks requiring sophis-\nticated abstract reasoning capability to under-\nstand complex metaphors follows a discontinu-\nous improvement curve, i.e., the model’s abil-\nities emerged only after a certain model size\n[\n10]. We observed a similar phenomenon in our\nconstruct validation experiments, where measure-\nments of LLM-synthesized extraversion, openness,\nand agreeableness were only externally valid (i.e.,\ncorrelated with theoretically-related psychological\nconstructs) for 62B-parameter models and larger.\nOnce model size increased to 62B parameters, we\nsaw a theoretically-expected strong negative rela-\ntionship between LLM-reported agreeableness and\naggression, but we did not observe the relationship\nin smallest tested models (Figure\n8b). The cri-\nterion correlations of LLM-synthesized conscien-\ntiousness and neuroticism, however, did not show\nsuch a dramatic jump, and measurements of these\npersonality traits in smaller models demonstrated\nsuﬃcient criterion validity. We hypothesize that\nthis could be due to the language content that\nencodes these personality domains.\nOverall, improvements in reliability, conver-\ngent validity, and criterion validity appear posi-\ntively linked to model size and performance on\nLLM benchmarks, and the model performance on\ncomplex reasoning benchmarks appears to track\nLLM abilities to meaningfully synthesize person-\nality.\nO Code Availability\nThe code used to administer psychometric tests\nto LLMs is intended to be interoperable across\nLLMs (i.e., the PaLM models used here). It is\nopen-sourced and available at the Google Research\nGitHub repository for the Psychometric Bench-\nmark of Racism, Generalization, and Stereotyping\n(PsyBORGS; manuscript in prep).\n4\nThe remaining Python and R code used to\ngenerate our prompt sets and statistically analyze\nreliability, construct validity, and trait shaping can\nbe made available upon request, and will be added\nto open-source repositories for wider public use\nsoon.\nP Data Availability\nThe data generated by the LLMs tested in this\nwork, either the psychometric test score data or\nopen-ended text responses to a real-world task\nprompt, are available upon reasonable request.\nThe psychometric tests used in this study were\naccessed from their respective original publi-\ncations and, where applicable, public research\n4https://github.com/google-research/google-research/tree/\nmaster/psyborgs\n45\nrepositories. We used items of these tests as\nLLM prompt inputs in a non-commercial research\ncapacity. The authors and copyright holders of\nthese tests govern their availability and use. The\n50 Persona Descriptions employed in our struc-\ntured prompts were reproducibly randomly sam-\npled from the true-cased version\n5 of the Per-\nsonaChat dataset [ 87]. PersonaChat is a publicly\navailable, crowd-sourced dataset of 1,155 ﬁctional\nhuman persona descriptions. For analysis of per-\nsonality traits on generated text, this study used\nthe Apply Magic Sauce (AMS) API\n6, a validated\npsychodemographic research tool that predicts\npersonality from open-ended text [\n41].\nQ Author Contributions\nM.A., C.C., M.M., M.S., and G.S-G. conceived\nthe project. G.S-G. contributed methodology to\nestablish reliability and construct validity and\nfor psychometric test administration and statis-\ntical analysis. M.S. contributed scaled up soft-\nware infrastructure and preliminary experiments\nand investigations. C.C. and M.S. implemented\nthe LLM hosting infrastructure for experiments.\nM.A., M.S., and G.S-G. contributed to the concep-\ntual design and analysis of and G.S-G. devised and\nimplemented the methods for personality shap-\ning. G.S-G. and L.S. designed and M.S., G.S-G.,\nand L.S. implemented the downstream task exper-\niment. C.C. and M.S. carried out data visualiza-\ntion. M.S. carried out the word cloud analysis. S.F.\nand P.R. provided discussion of LLM mechanisms\nand analysis of LLM performance. A.F., M.M.,\nM.S., and G.S-G. contributed limitations, future\ndirections, and ethical concerns discussions. P.R.\nand L.S. contributed psychometrics and statistical\nfeedback. A.F., M.M., M.S., and G.S-G. wrote the\nmanuscript with input from all co-authors. A.F.,\nM.M., and M.S. co-supervised the project.\nR Competing Interests\nThis study was funded by Alphabet Inc (‘Alpha-\nbet’) and/or a subsidiary thereof. A.F., C.C.,\nG.S-G., M.M., and Mustafa Safdari were employ-\nees of Alphabet at the time of this writing and may\n5https://huggingface.co/datasets/bavard/personachat\ntruecased\n6https://applymagicsauce.com\nown stock as part of the standard compensation\npackage. M.M. is also aﬃliated with the University\nof Southern California. G.S-G. and L.S. are aﬃli-\nated with the University of Cambridge. G.S-G. is\nalso supported by the Bill Melinda Gates Foun-\ndation through a Gates Cambridge Scholarship\n[OPP1144]. S.F. and P.R. are aﬃliated with Keio\nUniversity. M.A. is aﬃliated with the University\nof California, Berkeley.\nReferences\n[1] American Educational Research Association,\nAmerican Psychological Association, and\nNational Council on Measurement in Educa-\ntion, editors.\nStandards for educational and\npsychological testing. American Educational\nResearch Association, Lanham, MD, March\n2014.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and\nYoshua Bengio. Neural machine translation\nby jointly learning to align and translate.\nIn Yoshua Bengio and Yann LeCun, editors,\n3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA,\nUSA, May 7–9, 2015, Conference Track\nProceedings, 2015.\n[3] Aaron T. Beck, Robert A. Steer, and\nMargery G. Carbin. Psychometric properties\nof the Beck Depression Inventory: Twenty-\nﬁve years of evaluation.\nClinical Psychology\nReview, 8(1):77–100, 1988.\n[4] Wiebke Bleidorn, Patrick L Hill, Mitja D\nBack, Jaap JA Denissen, Marie Hennecke,\nChristopher J Hopwood, Markus Jokela,\nChristian Kandler, Richard E Lucas, Maike\nLuhmann, et al. The policy relevance of\npersonality traits.\nAmerican Psychologist,\n74(9):1056, 2019.\n[5] Ryan L Boyd and James W Pennebaker.\nLanguage-based personality: A new approach\nto personality in a digital world.\nCurrent\nOpinion in Behavioral Sciences, 18:63–68,\n2017. Big data in the behavioural sciences.\n46\n[6] Tom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeﬀrey Wu,\nClemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Lan-\nguage models are few-shot learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin, editors,\nAdvances in\nNeural Information Processing Systems, vol-\nume 33, pages 1,877–1,901. Curran Asso-\nciates, Inc., 2020.\n[7] Donald T Campbell and Donald W Fiske.\nConvergent and discriminant validation\nby the multitrait-multimethod matrix.\nPsychological Bulletin, 56(2):81, 1959.\n[8] Graham Caron and Shashank Srivastava.\nIdentifying and manipulating the person-\nality traits of language models.\nCoRR,\nabs/2212.10276, 2022.\n[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qim-\ning Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda,\nNicholas Joseph, Greg Brockman, Alex\nRay, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela\nMishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Pet-\nroski Such, Dave Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William Hebgen Guss,\nAlex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shan-\ntanu Jain, William Saunders, Christopher\nHesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec\nRadford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCan-\ndlish, Ilya Sutskever, and Wojciech Zaremba.\nEvaluating large language models trained on\ncode.\nCoRR, abs/2107.03374, 2021.\n[10] Aakanksha Chowdhery, Sharan Narang,\nJacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen\nShi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay,\nNoam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robin-\nson, Liam Fedus, Denny Zhou, Daphne\nIppolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan\nSepassi, David Dohan, Shivani Agrawal,\nMark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pel-\nlat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeﬀ Dean, Slav Petrov, and Noah Fiedel.\nPaLM: Scaling language modeling with path-\nways.\nCoRR, abs/2204.02311, 2022.\n[11] Hyung Won Chung, Le Hou, Shayne Longpre,\nBarret Zoph, Yi Tay, William Fedus, Yunx-\nuan Li, Xuezhi Wang, Mostafa Dehghani,\nSiddhartha Brahma, Albert Webson, Shix-\niang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery,\nAlex Castro-Ros, Marie Pellat, Kevin Robin-\nson, Dasha Valter, Sharan Narang, Gau-\nrav Mishra, Adams Yu, Vincent Zhao, Yan-\nping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeﬀ Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. Scaling instruction-ﬁnetuned\nlanguage models.\nCoRR, abs/2210.11416,\n2022.\n[12] Lee Anna Clark and David Watson. Con-\nstructing validity: Basic issues in objec-\ntive scale development.\nPsychological\n47\nAssessment, 7(3):309, 1995.\n[13] Lee Anna Clark and David Watson. Con-\nstructing validity: New developments in\ncreating objective measuring instruments.\nPsychological Assessment, 31(12):1412, 2019.\n[14] Paul T Costa, Jr. and Robert R McCrae.\nRevised NEO Personality Inventory (NEO\nPI-R) and NEO Five-Factor Inventory\n(NEO-FFI): Professional Manual. Psycho-\nlogical Assessment Resources, Odessa, FL,\n1992.\n[15] Lee J Cronbach. Coeﬃcient alpha and the\ninternal structure of tests.\nPsychometrika,\n16(3):297–334, 1951.\n[16] S. Crouse, G. Elbaz, and C. Malamud. Com-\nmon Crawl Foundation., 2008.\n[17] Colin G. DeYoung. Toward a theory of the\nBig Five. Psychological Inquiry, 21(1):26–33,\n2010.\n[18] Colin G DeYoung, Roger E Beaty, Erhan\nGen¸ c, Robert D Latzman, Luca Passamonti,\nMichelle N Servaas, Alexander J Shack-\nman, Luke D Smillie, R Nathan Spreng,\nEssi Viding, et al. Personality neuroscience:\nAn emerging ﬁeld with bright prospects.\nPersonality Science, 3:1–21, 2022.\n[19] Colin G DeYoung, Jacob B Hirsh, Matthew S\nShane, Xenophon Papademetris, Nallakkandi\nRajeevan, and Jeremy R Gray. Testing pre-\ndictions from personality neuroscience: Brain\nstructure and the Big Five.\nPsychological\nScience, 21(6):820–828, 2010.\n[20] James D Evans. Straightforward Statistics\nfor the Behavioral Sciences. Brooks/Cole\nPublishing Co, 1996.\n[21] Francis Galton. Measurement of character.\nFortnightly Review, 36:179–85, 1884.\n[22] Leo Gao, Stella Biderman, Sid Black, Lau-\nrence Golding, Travis Hoppe, Charles Fos-\nter, Jason Phang, Horace He, Anish Thite,\nNoa Nabeshima, Shawn Presser, and Con-\nnor Leahy. The pile: An 800gb dataset of\ndiverse text for language modeling.\nCoRR,\nabs/2101.00027, 2020.\n[23] Lewis R Goldberg. Language and individual\ndiﬀerences: The search for universals in per-\nsonality lexicons.\nReview of Personality and\nSocial Psychology, 2(1):141–165, 1981.\n[24] Lewis R Goldberg. The development of\nmarkers for the Big-Five factor structure.\nPsychological Assessment, 4(1):26–42, 1992.\n[25] Lewis R. Goldberg. A broad-bandwidth, pub-\nlic domain, personality inventory measuring\nthe lower-level facets of several Five-Factor\nmodels.\nPersonality Psychology in Europe,\n7(1):7–28, 1999.\n[26] Alan K Goodboy and Matthew M Martin.\nOmega over alpha for reliability estimation\nof unidimensional communication measures.\nAnnals of the International Communication\nAssociation, 44(4):422–439, 2020.\n[27] Louis Guttman. A basis for analyzing test-\nretest reliability. Psychometrika, 10(4):255–\n282, 1945.\n[28] Christopher Hare and Keith T. Poole.\nPsychometric Methods in Political Science,\nchapter 28, pages 901–931. John Wiley &\nSons, Ltd, 2018.\n[29] Jordan Hoﬀmann, Sebastian Borgeaud,\nArthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Kather-\nine Millican, George van den Driessche,\nBogdan Damoc, Aurelia Guy, Simon Osin-\ndero, Karen Simonyan, Erich Elsen, Oriol\nVinyals, Jack William Rae, and Laurent Sifre.\nAn empirical analysis of compute-optimal\nlarge language model training. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors,\nAdvances in Neural\nInformation Processing Systems, 2022.\n[30] Abigail Z. Jacobs. Measurement as gover-\nnance in and for responsible AI. CoRR,\nabs/2109.05658, 2021.\n48\n[31] Joel Jang, Seonghyeon Ye, and Minjoon\nSeo. Can large language models truly\nunderstand prompts? a case study with\nnegated prompts. In Alon Albalak, Chunting\nZhou, Colin Raﬀel, Deepak Ramachandran,\nSebastian Ruder, and Xuezhe Ma, editors,\nProceedings of The 1st Transfer Learning for\nNatural Language Processing Workshop, vol-\nume 203 of Proceedings of Machine Learning\nResearch, pages 52–62. PMLR, 03 Dec 2023.\n[32] Kristin Jankowsky, Gabriel Olaru, and Ulrich\nSchroeders. Compiling measurement invari-\nant short scales in cross-cultural personality\nassessment using ant colony optimization.\nEuropean Journal of Personality, 34(3):470–\n485, 2020.\n[33] Guangyuan Jiang, Manjie Xu, Song-Chun\nZhu, Wenjuan Han, Chi Zhang, and Yixin\nZhu. Evaluating and inducing personal-\nity in pre-trained language models.\nCoRR,\nabs/2206.07550, 2023.\n[34] Hang Jiang, Xiajie Zhang, Xubo Cao, and\nJad Kabbara. Personallm: Investigating\nthe ability of gpt-3.5 to express personal-\nity traits and gender diﬀerences.\nCoRR,\nabs/2305.02547, 2023.\n[35] Zhengbao Jiang, Jun Araki, Haibo Ding,\nand Graham Neubig. How can we know\nwhen language models know? on the cal-\nibration of language models for question\nanswering.\nTransactions of the Association\nfor Computational Linguistics, 9:962–977, 09\n2021.\n[36] Oliver P. John, Laura P. Naumann, and\nChristopher J. Soto. Paradigm shift to the\nintegrative Big Five trait taxonomy: His-\ntory, measurement, and conceptual issues.\nIn Oliver P. John, Richard W. Robbins,\nand Lawrence A. Pervin, editors,\nHandbook\nof Personality: Theory and Research, pages\n114–158. The Guilford Press, 2008.\n[37] Oliver P. John and Sanjay Srivastava. The\nBig Five trait taxonomy: History, measure-\nment, and theoretical perspectives. In L. A.\nPervin and Oliver P. John, editors,\nHandbook\nof Personality: Theory and Research, vol-\nume 2, pages 102–138. Guilford Press, New\nYork, 1999.\n[38] Saketh Reddy Karra, Son The Nguyen, and\nTheja Tulabandhula. Estimating the person-\nality of white-box language models.\nCoRR,\nabs/2204.12000, 2023.\n[39] Takeshi Kojima, Shixiang (Shane) Gu,\nMachel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. Large language models are zero-\nshot reasoners. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors,\nAdvances in Neural Information\nProcessing Systems, volume 35, pages 22199–\n22213. Curran Associates, Inc., 2022.\n[40] Michal Kosinski, Sandra C Matz, Samuel D\nGosling, Vesselin Popov, and David Still-\nwell. Facebook as a research tool for\nthe social sciences: Opportunities, challenges,\nethical considerations, and practical guide-\nlines.\nAmerican Psychologist, 70(6):543,\n2015.\n[41] Michal Kosinski, David Stillwell, and Thore\nGraepel. Private traits and attributes are pre-\ndictable from digital records of human behav-\nior.\nProceedings of the National Academy of\nSciences, 110(15):5802–5805, 2013.\n[42] Roman Kotov, Wakiza Gamez, Frank\nSchmidt, and David Watson. Linking “big”\npersonality traits to anxiety, depressive, and\nsubstance use disorders: A meta-analysis.\nPsychological Bulletin, 136(5):768, 2010.\n[43] Jon A Krosnick and Duane F Alwin. An\nevaluation of a cognitive theory of response-\norder eﬀects in survey measurement.\nPublic\nOpinion Quarterly, 51(2):201–219, 1987.\n[44] Xingxuan Li, Yutong Li, Shaﬁq Joty, Linlin\nLiu, Fei Huang, Lin Qiu, and Lidong Bing.\nDoes GPT-3 demonstrate psychopathy? eval-\nuating large language models from a psycho-\nlogical perspective.\nCoRR, abs/2212.10529,\n2023.\n[45] Paul Pu Liang, Chiyu Wu, Louis-Philippe\nMorency, and Ruslan Salakhutdinov.\n49\nTowards understanding and mitigating\nsocial biases in language models.\nCoRR,\nabs/2106.13219, 2021.\n[46] Rensis Likert. A Technique for the\nMeasurement of Attitudes. Number 136–165.\nArchives of Psychology, 1932.\n[47] Rabeeh Karimi Mahabadi, Luke Zettlemoyer,\nJames Henderson, Marzieh Saeidi, Lambert\nMathias, Veselin Stoyanov, and Majid Yaz-\ndani. Perfect: Prompt-free and eﬃcient few-\nshot learning with language models.\nCoRR,\nabs/2204.01172, 2022.\n[48] Mitchell P. Marcus, Beatrice Santorini, and\nMary Ann Marcinkiewicz. Building a large\nannotated corpus of English: The Penn Tree-\nbank.\nComputational Linguistics, 19(2):313–\n330, 1993.\n[49] Robert R McCrae and Antonio Terracciano.\nUniversal features of personality traits from\nthe observer’s perspective: Data from 50 cul-\ntures.\nJournal of Personality and Social\nPsychology, 88(3):547, 2005.\n[50] Roderick P McDonald. Test theory: A uniﬁed\ntreatment. Lawrence Erlbaum Associates\nPublishers, 1999.\n[51] Stephen Merity, Caiming Xiong, James Brad-\nbury, and Richard Socher. Pointer sentinel\nmixture models. In\nInternational Conference\non Learning Representations, 2017.\n[52] Samuel Messick. Standards of validity and\nthe validity of standards in performance\nasessment.\nEducational Measurement: Issues\nand Practice, 14(4):5–8, 1995.\n[53] Samuel Messick. Test validity: A matter\nof consequence. Social Indicators Research,\n45:35–44, 1998.\n[54] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel\nArtetxe, Mike Lewis, Hannaneh Hajishirzi,\nand Luke Zettlemoyer. Rethinking the role\nof demonstrations: What makes in-context\nlearning work?\nCoRR, abs/2202.12837, 2022.\n[55] Maril` u Miotto, Nicola Rossberg, and Bennett\nKleinberg. Who is GPT-3? an exploration\nof personality, values and demographics.\nIn\nProceedings of the Fifth Workshop\non Natural Language Processing and\nComputational Social Science (NLP+CSS),\npages 218–227, Abu Dhabi, UAE, Novem-\nber 2022. Association for Computational\nLinguistics.\n[56] Melanie Mitchell and David C. Krakauer.\nThe debate over understanding in ai’s large\nlanguage models.\nProceedings of the National\nAcademy of Sciences, 120(13):e2215907120,\n2023.\n[57] Jakob M¨ okander, Jonas Schuett, Han-\nnah Rose Kirk, and Luciano Floridi. Audit-\ning large language models: A three-layered\napproach.\nAI and Ethics, pages 1–31, 2023.\n[58] Daniel Nettle. The evolution of personal-\nity variation in humans and other animals.\nAmerican Psychologist, 61(6):622, 2006.\n[59] University of Cambridge Psychometrics Cen-\ntre. Apply Magic Sauce API.\n[60] OpenAI. ChatGPT, 2022.\n[61] OpenAI. GPT-4 technical report. 2023.\n[62] Long Ouyang, Jeﬀ Wu, Xu Jiang, Diogo\nAlmeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal,\nKatarina Slama, Alex Ray, John Schulman,\nJacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welin-\nder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to fol-\nlow instructions with human feedback. In\nS. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh, editors,\nAdvances\nin Neural Information Processing Systems,\nvolume 35, pages 27,730–27,744. Curran\nAssociates, Inc., 2022.\n[63] Denis Paperno, Germ´ an Kruszewski, Ange-\nliki Lazaridou, Ngoc Quan Pham, Raﬀaella\nBernardi, Sandro Pezzelle, Marco Baroni,\nGemma Boleda, and Raquel Fern´ andez.\nThe LAMBADA dataset: Word prediction\n50\nrequiring a broad discourse context. In\nProceedings of the 54th Annual Meeting\nof the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n1,525–1,534, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics.\n[64] Gregory Park, H Andrew Schwartz,\nJohannes C Eichstaedt, Margaret L Kern,\nMichal Kosinski, David J Stillwell, Lyle H\nUngar, and Martin EP Seligman. Automatic\npersonality assessment through social media\nlanguage.\nJournal of Personality and Social\nPsychology, 108(6):934, 2015.\n[65] Max Pellert, Clemens M Lechner, Claudia\nWagner, Beatrice Rammstedt, and Markus\nStrohmaier. Large language models open\nup new opportunities and challenges for psy-\nchometric assessment of artiﬁcial intelligence.\nOctober 2022.\n[66] James W Pennebaker and Laura A King. Lin-\nguistic styles: Language use as an individual\ndiﬀerence.\nJournal of Personality and Social\nPsychology, 77(6):1296, 1999.\n[67] Boele De Raad, Marco Perugini, Martina\nHreb´ ıckov´ a, and Piotr Szarota. Lingua franca\nof personality: Taxonomies and structures\nbased on the psycholexical approach.\nJournal\nof Cross-Cultural Psychology, 29(1):212–232,\n1998.\n[68] Brent W. Roberts. A revised sociogenomic\nmodel of personality traits. Journal of\nPersonality, 86(1):23–35, 2018.\n[69] Brent W. Roberts, Nathan R. Kuncel,\nRebecca Shiner, Avshalom Caspi, and\nLewis R. Goldberg. The power of personal-\nity: The comparative validity of personality\ntraits, socioeconomic status, and cognitive\nability for predicting important life out-\ncomes.\nPerspectives on Psychological science,\n2(4):313–345, 2007.\n[70] Brent W. Roberts and Hee J. Yoon. Personal-\nity psychology.Annual Review of Psychology,\n73(1).\n[71] Gerard Saucier and Lewis R Goldberg. Lex-\nical studies of indigenous personality factors:\nPremises, products, and prospects.\nJournal\nof Personality, 69(6):847–879, 2001.\n[72] Gregory Serapio-Garc´ ıa, Dasha Valter, and\nCl´ ement Crepy. PsyBORGS: Psychometric\nBenchmark of Racism, Generalization, and\nStereotyping.\n[73] Leonard Simms, Trevor F. Williams, and\nEricka Nus Simms. Assessment of the Five\nFactor Model. In Thomas A. Widiger, edi-\ntor,\nThe Oxford Handbook of the Five Factor\nModel, pages 353–380. Oxford University\nPress, 05 2017.\n[74] Umarpreet Singh and Parham Aarabhi. Can\nAI have a personality? In\n2023 IEEE\nConference on Artiﬁcial Intelligence (CAI),\npages 205–206, 2023.\n[75] Xiaoyang Song, Akshat Gupta, Kiyan\nMohebbizadeh, Shujie Hu, and Anant Singh.\nHave large language models developed a\npersonality?: Applicability of self-assessment\ntests in measuring personality in LLMs.\nCoRR, abs/2305.14693, 2023.\n[76] Mikke Tavast, Anton Kunnari, and Perttu\nH¨ am¨ al¨ ainen. Language models can generate\nhuman-like self-reports of emotion. In\n27th\nInternational Conference on Intelligent User\nInterfaces, IUI ’22 Companion, pages 69–72,\nNew York, NY, USA, 2022. Association for\nComputing Machinery.\n[77] Hugo Touvron, Thibaut Lavril, Gautier Izac-\nard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth´ ee Lacroix, Baptiste Rozi` ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and\neﬃcient foundation language models.\nCoRR,\nabs/2302.13971, 2023.\n[78] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, /suppress L ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors,\n51\nAdvances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.,\n2017.\n[79] David Watson and Lee Anna Clark. On\ntraits and temperament: General and speciﬁc\nfactors of emotional experience and their rela-\ntion to the Five-Factor model.\nJournal of\nPersonality, 60(2):441–476, 1992.\n[80] David Wechsler. The measurement of adult\nintelligence (3rd ed.). Williams & Wilkins Co,\nBaltimore, 1946.\n[81] Jason Wei, Maarten Bosma, Vincent Y. Zhao,\nKelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M. Dai, and Quoc V.\nLe. Finetuned language models are zero-\nshot learners. In\nInternational Conference on\nLearning Representations, 2022.\n[82] Jason Wei, Yi Tay, Rishi Bommasani, Colin\nRaﬀel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny\nZhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeﬀ\nDean, and William Fedus. Emergent abili-\nties of large language models.\nTransactions\non Machine Learning Research, 2022.\n[83] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen\nYoun, and Yuxiong He. A comprehensive\nstudy on post-training quantization for large\nlanguage models.\nCoRR, abs/2303.08302,\n2023.\n[84] J Kenneth Young, Beaujean, and A Alexan-\nder. Measuring personality in wave I of\nthe national longitudinal study of adolescent\nhealth.\nFront. Psychol., 2:158, July 2011.\n[85] Wu Youyou, Michal Kosinski, and David\nStillwell. Computer-based personality judg-\nments are more accurate than those made\nby humans.\nProceedings of the National\nAcademy of Sciences, 112(4):1036–1040,\n2015.\n[86] J.D. Zamﬁrescu-Pereira, Richmond Y. Wong,\nBjoern Hartmann, and Qian Yang. Why\nJohnny can’t prompt: How non-AI experts\ntry (and fail) to design LLM prompts. In\nProceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems, CHI\n’23, New York, NY, USA, 2023. Association\nfor Computing Machinery.\n[87] Saizheng Zhang, Emily Dinan, Jack Urbanek,\nArthur Szlam, Douwe Kiela, and Jason\nWeston. Personalizing dialogue agents:\nI have a dog, do you have pets too? In\nProceedings of the 56th Annual Meeting\nof the Association for Computational\nLinguistics (Volume 1: Long Papers), pages\n2204–2213, Melbourne, Australia, July 2018.\nAssociation for Computational Linguistics.\n[88] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi\nTang, Xiaolei Wang, Yupeng Hou, Yingqian\nMin, Beichen Zhang, Junjie Zhang, Zican\nDong, Yifan Du, Chen Yang, Yushuo Chen,\nZhipeng Chen, Jinhao Jiang, Ruiyang Ren,\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu\nLiu, Jian-Yun Nie, and Ji-Rong Wen. A\nsurvey of large language models.\nCoRR,\nabs/2303.18223, 2023.\n[89] Daniel M. Ziegler, Nisan Stiennon, Jeﬀrey\nWu, Tom B. Brown, Alec Radford, Dario\nAmodei, Paul Christiano, and Geoﬀrey Irv-\ning. Fine-tuning language models from\nhuman preferences.\nCoRR, abs/1909.08593,\n2020.\n[90] Mark Zimmerman. Diagnosing personality\ndisorders: A review of issues and research\nmethods.\nArchives of general psychiatry,\n51(3):225–245, 1994.\n[91] Richard E Zinbarg, William Revelle, Iftah\nYovel, and Wen Li. Cronbach’sα , revelle’s β,\nand mcdonald’s ω h: Their relations with each\nother and two alternative conceptualizations\nof reliability.\nPsychometrika, 70:123–133,\n2005.\n52",
  "topic": "Personality",
  "concepts": [
    {
      "name": "Personality",
      "score": 0.8052217960357666
    },
    {
      "name": "Psychology",
      "score": 0.497221976518631
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.45289507508277893
    },
    {
      "name": "Big Five personality traits",
      "score": 0.44023963809013367
    },
    {
      "name": "Power (physics)",
      "score": 0.42335256934165955
    },
    {
      "name": "Social psychology",
      "score": 0.377811461687088
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3219432234764099
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I203951103",
      "name": "Keio University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 83
}