{
  "title": "PhysFormer++: Facial Video-Based Physiological Measurement with SlowFast Temporal Difference Transformer",
  "url": "https://openalex.org/W4320913003",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2726404128",
      "name": "Zitong Yu",
      "affiliations": [
        "Great Bay University"
      ]
    },
    {
      "id": "https://openalex.org/A2164329074",
      "name": "Yuming Shen",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2114526326",
      "name": "Jingang Shi",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2516465340",
      "name": "Hengshuang Zhao",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2548745861",
      "name": "Yawen Cui",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A2153960712",
      "name": "Jiehua Zhang",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A2941318783",
      "name": "Philip Torr",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2101487947",
      "name": "Guoying Zhao",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A2726404128",
      "name": "Zitong Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164329074",
      "name": "Yuming Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114526326",
      "name": "Jingang Shi",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2548745861",
      "name": "Yawen Cui",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A2153960712",
      "name": "Jiehua Zhang",
      "affiliations": [
        "University of Oulu"
      ]
    },
    {
      "id": "https://openalex.org/A2941318783",
      "name": "Philip Torr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101487947",
      "name": "Guoying Zhao",
      "affiliations": [
        "University of Oulu"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6796568838",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6802122185",
    "https://openalex.org/W4200633922",
    "https://openalex.org/W2963433879",
    "https://openalex.org/W2902449706",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2069692225",
    "https://openalex.org/W3170898657",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2553156677",
    "https://openalex.org/W2807904173",
    "https://openalex.org/W2066454034",
    "https://openalex.org/W3204093119",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W2787182113",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3163937874",
    "https://openalex.org/W6788556936",
    "https://openalex.org/W2218803975",
    "https://openalex.org/W3108080438",
    "https://openalex.org/W2805424946",
    "https://openalex.org/W1986273245",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W3216998657",
    "https://openalex.org/W3201844719",
    "https://openalex.org/W3033778765",
    "https://openalex.org/W4319300066",
    "https://openalex.org/W3209712295",
    "https://openalex.org/W3173459793",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3129515546",
    "https://openalex.org/W3173279311",
    "https://openalex.org/W2891834928",
    "https://openalex.org/W3210279979",
    "https://openalex.org/W2786529723",
    "https://openalex.org/W2903521046",
    "https://openalex.org/W2982196965",
    "https://openalex.org/W3110179947",
    "https://openalex.org/W2958501326",
    "https://openalex.org/W3204118251",
    "https://openalex.org/W2008821584",
    "https://openalex.org/W1984554603",
    "https://openalex.org/W6810353043",
    "https://openalex.org/W2903559293",
    "https://openalex.org/W4310263654",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2122098299",
    "https://openalex.org/W2893517024",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2472200183",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2003922338",
    "https://openalex.org/W3164690902",
    "https://openalex.org/W2520509592",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W6798038150",
    "https://openalex.org/W3020225364",
    "https://openalex.org/W3175452902",
    "https://openalex.org/W2964796858",
    "https://openalex.org/W3211152704",
    "https://openalex.org/W2986906199",
    "https://openalex.org/W4306832857",
    "https://openalex.org/W4312358294",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W4312509322",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W3100063341",
    "https://openalex.org/W3105616927"
  ],
  "abstract": "Abstract Remote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video without any contact, has great potential in many applications (e.g., remote healthcare and affective computing). Recent deep learning approaches focus on mining subtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper, we propose two end-to-end video transformer based architectures, namely PhysFormer and PhysFormer++, to adaptively aggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in PhysFormer, the temporal difference transformers first enhance the quasi-periodic rPPG features with temporal difference guided global attention, and then refine the local spatio-temporal representation against interference. To better exploit the temporal contextual and periodic rPPG clues, we also extend the PhysFormer to the two-pathway SlowFast based PhysFormer++ with temporal difference periodic and cross-attention transformers. Furthermore, we propose the label distribution learning and a curriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer and PhysFormer++ and alleviate overfitting. Comprehensive experiments are performed on four benchmark datasets to show our superior performance on both intra- and cross-dataset testings. Unlike most transformer networks needed pretraining from large-scale datasets, the proposed PhysFormer family can be easily trained from scratch on rPPG datasets, which makes it promising as a novel transformer baseline for the rPPG community.",
  "full_text": "International Journal of Computer Vision (2023) 131:1307–1330\nhttps://doi.org/10.1007/s11263-023-01758-1\nPhysFormer++: Facial Video-Based Physiological Measurement with\nSlowFast Temporal Difference Transformer\nZitong Yu 1 · Yuming Shen 2 · Jingang Shi 3 · Hengshuang Zhao 4 · Yawen Cui 5 · Jiehua Zhang 5 · Philip Torr 2 ·\nGuoying Zhao 5\nReceived: 27 April 2022 / Accepted: 6 January 2023 / Published online: 15 February 2023\n© The Author(s) 2023\nAbstract\nRemote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video\nwithout any contact, has great potential in many applications (e.g., remote healthcare and affective computing). Recent deep\nlearning approaches focus on mining subtle rPPG clues using convolutional neural networks with limited spatio-temporal\nreceptive ﬁelds, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper,\nwe propose two end-to-end video transformer based architectures, namely PhysFormer and PhysFormer++, to adaptively\naggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in Phys-\nFormer, the temporal difference transformers ﬁrst enhance the quasi-periodic rPPG features with temporal difference guided\nglobal attention, and then reﬁne the local spatio-temporal representation against interference. To better exploit the temporal\ncontextual and periodic rPPG clues, we also extend the PhysFormer to the two-pathway SlowFast based PhysFormer++ with\ntemporal difference periodic and cross-attention transformers. Furthermore, we propose the label distribution learning and a\ncurriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer\nand PhysFormer++ and alleviate overﬁtting. Comprehensive experiments are performed on four benchmark datasets to show\nour superior performance on both intra- and cross-dataset testings. Unlike most transformer networks needed pretraining from\nlarge-scale datasets, the proposed PhysFormer family can be easily trained from scratch on rPPG datasets, which makes it\npromising as a novel transformer baseline for the rPPG community.\nKeywords RPPG · Temporal difference transformer · SlowFast · Cross-attention · Periodic-attention\nCommunicated by Jingdong Wang.\nB Guoying Zhao\nguoying.zhao@oulu.ﬁ\nZitong Y u\nzitong.yu@ieee.org\nYu m i n g S h e n\nym_zmxncbv@hotmail.com\nJingang Shi\njingang@xjtu.edu.cn\nHengshuang Zhao\nhszhao@cs.hku.hk\nYawen Cui\nyawen.cui@oulu.ﬁ\nJiehua Zhang\njiehua.zhang@oulu.ﬁ\nPhilip Torr\nphilip.torr@eng.ox.ac.uk\n1 Introduction\nPhysiological signals such as heart rate (HR), respiration fre-\nquency (RF), and heart rate variability (HRV) are important\nvital signs to be measured in many circumstances, especially\nfor healthcare or medical purposes. Traditionally, the elec-\ntrocardiography (ECG) and photoplethysmograph (PPG) or\nblood volume pulse (BVP) are the two most common ways\nfor measuring heart activities and corresponding physiologi-\ncal signals. However, both ECG and PPG/BVP sensors need\nto be attached to body parts, which may cause discomfort\n1 Great Bay University, Dongguan 523000, China\n2 University of Oxford, Oxford OX13PJ, UK\n3 Xi’an Jiaotong University, Xi’an 710049, China\n4 The University of Hong Kong, Hong Kong, China\n5 University of Oulu, 90014 Oulu, Finland\n123\n1308 International Journal of Computer Vision (2023) 131:1307–1330\nand are inconvenient for long-term monitoring. To counter\nthis issue, remote photoplethysmography (rPPG) (Chen et\nal., 2018; Liu et al., 2021b; Y u et al., 2021) methods are\ndeveloping fast in recent years, which aim to measure heart\nactivity remotely without any contact.\nIn earlier studies of facial rPPG measurement, most meth-\nods analyze subtle color changes on facial regions of interest\n(ROI) with classical signal processing approaches (Li et al.,\n2014; Magdalena Nowara et al., 2018; Poh et al., 2010a,\n2010b; Tulyakov et al., 2016; V erkruysse et al., 2008).\nBesides, there are a few color subspace transformation meth-\nods (De Haan & Jeanne, 2013; Wang et al., 2016) which\nutilize all skin pixels for rPPG measurement. Based on the\nprior knowledge from traditional methods, a few learning\nbased approaches (Hsu et al., 2017; Niu et al., 2018, 2019a;\nQiu et al., 2018) are designed as non-end-to-end fashions.\nROI based preprocessed signal representations [e.g., time-\nfrequency map (Hsu et al., 2017) and spatio-temporal map\n(Niu et al., 2018, 2019a)] are generated ﬁrst, and then learn-\nable models could capture rPPG features from these maps.\nHowever, these methods need the strict preprocessing pro-\ncedure and neglect the global contextual clues outside the\npre-deﬁned ROIs. Meanwhile, more and more end-to-end\ndeep learning based rPPG methods (Chen & McDuff, 2018;\nLiu et al., 2020; Špetlík et al., 2018; Y u et al., 2019a; 2019b\nare developed, which treat facial video frames as input and\npredict rPPG and other physiological signals directly. How-\never, pure end-to-end methods are easily inﬂuenced by the\ncomplex scenarios (e.g., with head movement and various\nillumination conditions) and rPPG-unrelated features can not\nbe ruled out in learning, resulting in huge performance drops\n(Y u et al., 2020) in realistic datasets [e.g., VIPL-HR (Niu et\nal., 2019a)].\nRecently, due to its excellent long-range attentional mod-\neling capacities in solving sequence-to-sequence issues,\nrPPG signals\nt1 t2 t3\nDirection\nQuery Key interaction\nFig. 1 The trajectories of rPPG signals around t1, t2, and t3 share\nsimilar properties (e.g., trends with rising edge ﬁrst then falling edge\nlater, and relatively high magnitudes) induced by skin color changes.\nIt inspires the long-range spatio-temporal attention (e.g., blue tube\naround t1 interacted with red tubes from intra- and inter-frames) accord-\ning to their local temporal difference features for quasi-periodic rPPG\nenhancement. Here ‘tube’ indicates the same regions across short-time\nconsecutive frames (Color ﬁgure online)\ntransformer (Han et al., 2022; Lin et al., 2022) has been suc-\ncessfully applied in many artiﬁcial intelligence tasks such as\nnatural language processing (NLP) (V aswani et al., 2017),\nimage (Dosovitskiy et al., 2020) and video (Bertasius et al.,\n2021) analysis. Similarly, rPPG measurement from facial\nvideos can be treated as a video sequence to signal sequence\nproblem, where the long-range contextual clues should be\nexploited for semantic modeling. As shown in Fig. 1, rPPG\nclues from different skin regions and temporal locations (e.g.,\nsignal trajectories around t1, t2, and t3) share similar prop-\nerties (e.g., trends with rising edge ﬁrst then falling edge\nlater and relative high magnitudes), which can be utilized\nfor long-range feature modeling and enhancement. How-\never, different from the most video tasks aiming at semantic\nmotion representation, facial rPPG measurement focuses on\ncapturing subtle skin color changes, which makes it challeng-\ning for global spatio-temporal perception. Besides, the rPPG\nmeasurement task usually relies on periodic hidden visual\ndynamics, and the existing deep end-to-end models are weak\nin representing such clues. Furthermore, video-based rPPG\nmeasurement is usually a long-time monitoring task, and it is\nchallenging to design and train transformers with long video\nsequence inputs.\nMotivated by the discussions above, we propose two\nend-to-end video transformer architectures, namely Phys-\nFormer and PhysFormer++, for remote physiological mea-\nsurement. On the one hand, the cascaded temporal difference\ntransformer blocks in PhysFormer beneﬁt the rPPG fea-\nture enhancement via global spatio-temporal attention based\non the ﬁne-grained temporal skin color differences. Fur-\nthermore, the two-pathway SlowFast temporal difference\ntransformer based PhysFormer++ with periodic- and cross-\nattention is able to efﬁciently capture the temporal contextual\nand periodic rPPG clues from facial videos. On the other\nhand, to alleviate the interference-induced overﬁtting issue\nand complement the weak temporal supervision signals, elab-\norate supervision in frequency domain is designed, which\nhelps the PhysFormer family learn more intrinsic rPPG-\naware features.\nThis paper is an extended version of our prior work (Y u\net al., 2022) accepted by CVPR 2022. The main differ-\nences with the conference version are as follows: (1) besides\nthe temporal difference transformer based PhysFormer, we\npropose the novel SlowFast video transformer architecture\nPhysFormer++ for rPPG measurement task; (2) based on\nthe temporal difference transformer, the temporal difference\nperiodic transformer and temporal difference cross-attention\ntransformer are proposed to enhance the rPPG periodic per-\nception and cross-tempo rPPG dynamics, respectively; (3)\na detailed overview about the traditional, non-end-to-end\nlearning based, and end-to-end learning based rPPG mea-\nsurement methods is discussed in the related work; (4) more\nelaborate experimental results, visualization, and efﬁciency\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1309\nanalysis are given for the PhysFormer family. To sum up, the\nmain contributions of this paper are listed:\n• We propose the PhysFormer family, i.e., PhysFormer\nand PhysFormer++, which mainly consists of a pow-\nerful video temporal difference transformer backbone.\nTo our best knowledge, it is the ﬁrst time to explore the\nlong-range spatio-temporal relationship for reliable rPPG\nmeasurement. Besides, the proposed temporal difference\ntransformer is potential for broader ﬁne-grained or peri-\nodic video understanding tasks in computer vision (e.g.,\nvideo action recognition and repetition counting) due to\nits excellent spatio-temporal representation capacity with\nlocal temporal difference description and global spatio-\ntemporal modeling.\n• We propose the two-pathway SlowFast architecture for\nPhysFormer++ to efﬁciently leverage both ﬁne-grained\nand semantic tempo rPPG clues. Speciﬁcally, the tempo-\nral difference periodic and cross-attention transformers\nare respectively designed for the Slow and Fast pathways\nto enhance the representation capacity of the periodic\nrPPG dynamics.\n• We propose an elaborate recipe to supervise Phys-\nFormer with label distribution learning and curriculum\nlearning guided dynamic loss in frequency domain to\nlearn efﬁciently and alleviate overﬁtting. Such curricu-\nlum learning guided dynamic strategy could beneﬁt not\nonly the rPPG measurement task but also general deep\nlearning tasks such as multi-task learning and multi-loss\nadjusting.\n• We conduct intra- and cross-dataset testings and show\nthat the proposed PhysFormer achieves superior or on\npar state-of-the-art performance without pretraining on\nlarge-scale datasets like ImageNet-21K.\nIn the rest of the paper, Sect. 2 provides the related work\nabout rPPG measurement and vision transformer. Section 3\nﬁrst introduces the detailed architectures of the PhysFormer\nand PhysFormer++, and then formulates the label distribution\nlearning and curriculum learning guided dynamic supervi-\nsion for rPPG measurement. Section 4 introduces the four\nrPPG benchmark datasets and evaluation metrics, and pro-\nvides rigorous ablation studies, visualizations and evaluates\nthe performance of the proposed models. Finally, a conclu-\nsion is given in Sect. 5.\n2 Related Work\nIn this section, we provide a brief discussion of the related\nfacial rPPG measurement approaches. As shown in Table 1,\nthese approaches can be generally categorized into tradi-\ntional, non-end-to-end learning, and end-to-end learning\nbased methods. We also brieﬂy review the transformer archi-\ntectures for vision tasks.\n2.1 rPPG Measurement\nTraditional Approaches An early study of rPPG-based\nphysiological measurement was reported in V erkruysse et\nal. ( 2008). Plenty of traditional hand-crafted approaches\nhave been developed in this ﬁeld since then. Compared with\ncoarsely averaging arbitrary color channel from the detected\nfull face region, selective merging information from different\ncolor channels (Poh et al., 2010a, 2010b) from different ROIs\n(Lam & Kuno, 2015;L ie ta l . , 2014) with adaptive temporal\nﬁltering (Li et al., 2014) are proven to be more efﬁcient for\nsubtle rPPG signal recovery. To improve the signal-to-noise\nrate of the recovered rPPG signals, several signal decom-\nposition methods such as independent component analysis\n(ICA) (Lam & Kuno, 2015; Poh et al., 2010a, 2010b) and\nmatrix completion (Tulyakov et al., 2016) are also proposed.\nTo alleviate the impacts of the skin tone and head motion,\nseveral color space projection [e.g., chrominance subspace\n(De Haan & Jeanne, 2013) and skin-orthogonal space (Wang\net al., 2016)] methods are developed. Despite remarkable\nearly-stage progresses, these approaches have the following\nlimitations: (1) they require empirical knowledge to design\nthe components (e.g., hyperparameter in signal processing\nﬁltering); (2) there is a lack of supervised learning models\nto counter data variations, especially in challenging environ-\nments with serious interference.\nNon-End-to-End Learning Approaches In recent years,\ndeep learning based approaches dominate the ﬁeld of rPPG\nmeasurement due to the strong spatio-temporal representa-\ntion capabilities. One representative framework is to learn\nrobust rPPG features from the facial ROI-based spatio-\ntemporal signal map (STmap). STmap (Niu et al., 2018,\n2019b) or its variants [e.g., multisacle STmap (Lu et al., 2021;\nNiu et al., 2020) and chrominance STmap (Lu & Han, 2021)]\nare ﬁrst extracted from predeﬁned facial ROIs on different\ncolor spaces, and then classical convolutional neural network\n(CNN) [e.g., ResNet (He et al., 2016)] and recurrent neural\nnetwork (RNN) [e.g., GRU (Cho et al., 2014)] are cascaded\nfor rPPG feature representation. The STmap-based non-end-\nto-end learning framework focuses on learning an underlying\nmapping from the input feature maps to the target rPPG sig-\nnals. With dense raw rPPG information and less irrelevant\nelements (e.g., face-shape attributes), these methods usually\nconverge faster and achieve reasonable performance against\nhead movement but need explicit and exhaustive preprocess-\nings.\nEnd-to-End Learning Approaches Besides learning upon\nhandcrafted STmaps, end-to-end learning from facial sequence\ndirectly is also favorite. Both spatial 2DCNN networks\n123\n1310 International Journal of Computer Vision (2023) 131:1307–1330\nTable 1 Summary of the representative rPPG measurement methods in terms of traditional, non-end-to-end learning, and end-to-end learning categories\nMethod V enue Feature Backbone Loss Function\nTraditional Poh2010 (Poh et\nal., 2010a)\nIEEE Trans.\nBiomed. Eng.\nROI selection, ICA decomposition – –\nCHROM (De\nHaan & Jeanne,\n2013)\nIEEE Trans.\nBiomed. Eng.\nMapping on the color difference\n(chrominance) subspace\n––\nLi2014 (Li et al.,\n2014)\nCVPR ROI selection, tracking,\nillumination rectiﬁcation,\nnon-rigid motion elimination\n––\nRandomPatch\n(Lam & Kuno,\n2015)\nICCV ICA decomposition on random\npatches, majority voting\n––\nTulyakov2016\n(Tulyakov et\nal., 2016)\nCVPR Chrominance features from\nmultiple ROIs,low-rank\nfactorization via self-adaptive\nmatrix completion\n––\nPOS (Wang et\nal., 2016)\nIEEE Trans.\nBiomed. Eng.\nMapping on the projection plane\northogonal to the skin tone\n––\nNon-end-to-end learning SynRhythm (Niu\net al., 2018)\nICPR spatio-temporal map\nrepresentation, pretrained on\nSynthetic Rhythms\nResNet18 L1 regression\nloss\nEVM-CNN (Qiu\net al., 2018)\nIEEE TMM ROI tracking, feature image via\nspatial decomposition and\ntemporal ﬁltering\nShallow CNN L2 regression\nloss\nRhythmNet (Niu\net al., 2019a)\nIEEE TIP spatio-temporal map in YUV color\nspace, temporal contextual\nmodeling via RNN\nResNet18+GRU L1 regression\nlosstemporal\nsmooth loss\nST-Attention\n(Niu et al.,\n2019b)\nIEEE FG spatio-temporal map in YUV color\nspace, channel and\nspatio-temporal attention,\ntemporal augmentation\nResNet18 L1 regression\nloss\nNAS-HR (Lu &\nHan, 2021)\nVRIH Spatial-temporal map with POS\nsignals, NAS for efﬁcient\narchitecture search\nlightweight NAS L1 regression\nloss\nCVD (Niu et al.,\n2020)\nECCV Multi-scale spatio-temporal map\non both RGB and YUV color\nspaces, cross-veriﬁed feature\ndisentangling, multi-task learning\nResNet18 with shallow decoder L1 regression\nloss, CE loss,\nNegPearson\nloss,\nreconstruction\nloss\nDual-GAN (Lu\net al., 2021)\nCVPR spatio-temporal map, dual GAN\nfor BVP signal and noise\nmodeling, respectively\nCustomized CNN L1 regression\nloss, CE loss,\nNegPearson\nloss, GAN loss\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1311\nTable 1 continued\nMethod V enue Feature Backbone Loss Function\nEnd-to-end learning DeepPhys (Chen\n&M c D u f f ,\n2018)\nECCV Motion representation with\nnormalized frame\ndifference,attention mechanism\nusing appearance to guide motion\nTwo-branch CNN MSE loss\nHR-CNN\n(Špetlík et al.,\n2018)\nBMVC Two-stage CNN to measure the\nrPPG signals ﬁrst, and then\nestimate the HR value\nShallow CNN SNR loss, L1\nregression loss\nPhysNet (Y u et\nal., 2019a)\nBMVC End-to-end spatio-temporal\nnetworks\n3DCNN NegPearson loss\nrPPGNet (Y u et\nal., 2019b)\nICCV Two-stage framework to enhance\nvideo quality ﬁrst, and then\nextract rPPG signals\n3DCNN NegPearson loss,\nreconstruction\nloss skin\nsegmentation\nloss\nAutoHR (Y u et\nal., 2020)\nIEEE SPL NAS with temporal difference\nconvolution, spatio-temporal\naugmentation\nNAS 3DCNN NegPearson loss,\nCE loss\nTS-CAN (Liu et\nal., 2020)\nNeuIPS Multi-task temporal shift\nconvolutional attention networks,\nmobile-level real-time rPPG and\nrespiratory measurement\nTwo-branch temporal shift CNN MSE loss on\npulse and\nrespiration\nEfﬁcientPhys\n(Liu et al.,\n2023)\narXiv Temporal difference normalization,\nself-attention-shifted networks\nTemporal shift Swin Transformer MSE loss\nPhysFormer\n(Ours)PhysFormer++\n(Ours)\n2022 Temporal difference transformers\nsupervised by the label\ndistribution learning and\ncurriculum learning strategy\nTemporal difference video\ntransformer\nNegPearson loss,\nCE loss,label\ndistribution\nloss\n123\n1312 International Journal of Computer Vision (2023) 131:1307–1330\n(Chen & McDuff, 2018; Špetlík et al., 2018) and spatio-\ntemporal models (Gideon & Stent, 2021; Liu et al., 2020,\n2023; Nowara et al., 2021; Y u et al., 2019a, 2019b, 2020)\nare developed for rPPG feature representation. Y u et al.\n(2019a) investigates the recurrent methods (PhysNet-LSTM,\nPhysNet-ConvLSTM) for rPPG measuremnt. However, such\nCNN+LSTM based architectures are good at long-range\nsequential modeling via LSTM but fail to explore long-range\nintra-frame spatial relationship using CNN with local con-\nvolutions. In contrast, with spatial transformer backbone and\ntemporal shift module, EfﬁcientPhys (Liu et al., 2023) is able\nto explore long-range spatial but only short-term temporal\nrelationship. In other words, existing end-to-end methods\nonly consider the spatio-temporal rPPG features from local\nneighbors and adjacent frames but neglect the long-range\nrelationship among quasi-periodic rPPG features.\nCompared with the non-end-to-end learning based meth-\nods, end-to-end approaches are less dependent on task-\nrelated prior knowledge and handcrafted engineering (e.g.,\nSTmap generation) but rely on diverse and large-scale data\nto alleviate the problem of overﬁtting. To enhance the long-\nrange contextual spatio-temporal representation capacities\nand alleviate the data-hungry requirement of the deep rPPG\nmodels, we propose the PhysFormer and PhysFormer++\narchitectures, which can be easily trained from scratch on\nrPPG datasets with the elaborate supervision recipe.\n2.2 Transformer for Vision Tasks\nDue to the powerful self-attention based long-range modeling\ncapacity, transformer (Lin et al., 2022; V aswani et al., 2017)\nhas been successfully applied in the ﬁeld of NLP to model the\ncontextual relationship for sequential data. Then vision trans-\nformer (ViT) (Dosovitskiy et al., 2020) is proposed recently\nby feeding transformer with sequences of image patches for\nimage classiﬁcation. Many other ViT variants (Chen et al.,\n2021a; Ding et al., 2021; Han et al., 2022, 2021; Khan et\nal., 2021; Liu et al., 2021d; Touvron et al., 2021; Wang et\nal., 2021b; Y uan et al., 2021) are proposed from then, which\nachieve promising performance compared with its counter-\npart CNNs for image analysis tasks (Carion et al., 2020;H e\net al., 2021; Zheng et al., 2020). Recently, some works intro-\nduce vision transformer for video understanding tasks such as\naction recognition (Arnab et al., 2021; Bertasius et al., 2021;\nBulat et al., 2021; Fan et al., 2021; Girdhar et al., 2018;L i u\net al., 2021e; Neimark et al., 2021), action detection (Liu\net al., 2021c; Wang et al., 2021a; Xu et al., 2021; Zhao et\nal., 2021), video super-resolution (Cao et al., 2021), video\ninpainting (Liu et al., 2021a; Zeng et al., 2020), and 3D ani-\nmation (Chen et al., 2021b, 2021c). Some works (Girdhar et\nal., 2018; Neimark et al., 2021) conduct temporal contextual\nmodeling with transformer based on single-frame features\nfrom pretrained 2D networks, while other works (Arnab et\nal., 2021; Bertasius et al., 2021; Bulat et al., 2021; Fan et\nal., 2021; Liu et al., 2021e) mine the spatio-temporal atten-\ntions via video transformer directly. Most of these works are\nincompatible for long-video-sequence ( > 150 frames) sig-\nnal regression task. There are two related works (Liu et al.,\n2023; Y u et al., 2022) using ViT for rPPG feature represen-\ntation. TransRPPG (Y u et al., 2022) extracts rPPG features\nfrom the preprocessed signal maps via ViT for face 3D mask\npresentation attack detection (Y u et al., 2022). Based on the\ntemporal shift networks (Lin et al., 2018; Liu et al., 2020),\nEfﬁcientPhys-T (Liu et al., 2023) adds several Swin Trans-\nformer (Liu et al., 2021d) layers for global spatial attention.\nDifferent from these two works, the proposed PhysFormer\nand PhysFormer++ are end-to-end video transformers, which\nare able to capture long-range spatio-temporal attentional\nrPPG features from facial video directly.\n3 Methodology\nWe will ﬁrst introduce the architecture of PhysFormer and\nPhysFormer++ in Sects. 3.1 and 3.2, respectively. Then we\nwill introduce label distribution learning for rPPG measure-\nment in Sect. 3.3, and at last present the curriculum learning\nguided dynamic supervision in Sect. 3.4.\n3.1 PhysFormer\nAs illustrated in Fig. 2, PhysFormer consists of a shallow\nstem Estem, a tube tokenizer Etube, N temporal difference\ntransformer blocks Ei\ntrans (i = 1, ...,N ) and a rPPG pre-\ndictor head. Inspired by the study in Xiao et al. ( 2021), we\nadopt a shallow stem to extract coarse local spatio-temporal\nfeatures, which beneﬁts the fast convergence and clearer sub-\nsequent global self-attention. Speciﬁcally, the stem is formed\nby three convolutional blocks with kernel size (1 × 5 × 5),\n(3 × 3 × 3) and (3 × 3 × 3), respectively. Each convolu-\ntion operator is cascaded with a batch normalization (BN),\nReLU and MaxPool. The pooling layer only halves the spa-\ntial dimension. Therefore, given an RGB facial video input\nX ∈ R\n3×T ×H×W , the stem output Xstem = Estem(X),\nwhere Xstem ∈ RD×T ×H/8×W /8, and D, T , W , H indicate\nchannel, sequence length, width, height, respectively. Then\nXstem would be partitioned into spatio-temporal tube tokens\nXtube ∈ RD×T ′×H′×W ′\nvia the tube tokenizer Etube. Subse-\nquently, the tube tokens will be forwarded with N temporal\ndifference transformer blocks and obtain the global-local\nreﬁned rPPG features Xtrans, which has the same dimen-\nsions with Xtube. Finally, the rPPG predictor head temporally\nupsamples, spatially averages, and projects the features Xtrans\nto 1D signal Y ∈ RT .\nTube Tokenization Here the coarse feature Xstem would be\npartitioned into non-overlapping tube tokens via Etube(Xstem),\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1313\nStem\nTube Tokens\nTemporal Difference\nMulti-head Self-attention\nAdd \n& \nNorm\nSpatio-temporal \nFeed-forward\nAdd \n& \nNorm\nTemporal Difference Transformer\nPredictor\nVideo x N\nrPPG Signals\nT T' T\nTDCTDC1x1x1\nVid2Seq\nVid2Seq\nVid2Seq\nSoftmax\nTemporal Difference Multi-head Self-attention\nMulti-head\nConcat\n&\nProjection\nSeq2Vid\n1x1x1\nDepth-wise\n3x3x3\nSpatio-temporal\nFeed-forward\nQ\nK\nV 1x1x1\nFig. 2 Framework of the PhysFormer. It consists of a shallow stem, a\ntube tokenizer, several temporal difference transformers, and a rPPG\npredictor head. The temporal difference transformer is formed from\nthe Temporal Difference Multi-head Self-attention (TD-MHSA) and\nSpatio-temporal Feed-forward (ST-FF) modules, which enhances the\nglobal and local spatio-temporal representation, respectively. ‘TDC’ is\nshort for the temporal difference convolution (Y u et al., 2020, 2022)\nwhich aggregates the spatio-temporal neighbor semantics\nwithin the tube region and reduces computational costs for\nthe subsequent transformers. Speciﬁcally, the token tokenizer\nconsists of a learnable 3D convolution with the same ker-\nnel size and stride (non-overlapping setting) as the targeted\ntube size T\ns × Hs × Ws . Thus, the expected tube token map\nXtube ∈ RD×T ′×H′×W ′\nhas length, height and width\nT ′ =\n⌊ T\nTs\n⌋\n, H′ =\n⌊ H/8\nHs\n⌋\n, W ′ =\n⌊ W /8\nWs\n⌋\n. (1)\nPlease note that there are no positional embeddings after the\ntube tokenization as the stem with cascaded convolutions\nand poolings at early stage already captures relative spatio-\ntemporal positional information (Hassani et al., 2021).\nTemporal Difference Multi-head Self-Attention (TD-MHSA)\nIn self-attention mechanism (Dosovitskiy et al., 2020; V aswani\net al., 2017), the relationship between the tokens is mod-\neled by the similarity between the projected query-key pairs,\nyielding the attention score. Instead of point-wise linear pro-\njection, we utilize temporal difference convolution (TDC)\n(Y u et al., 2020, 2022) for query ( Q) and key ( K ) projection,\nwhich could capture ﬁne-grained local temporal difference\nfeatures for subtle color change description. TDC with learn-\nable w can be formulated as\nTDC(x) =\n∑\npn ∈R\nw(pn ) · x(p0 + pn )\n  \nvanilla 3D convolution\n+θ · (−x(p0) ·\n∑\npn ∈R′\nw(pn ))\n  \ntemporal difference term\n,\n(2)\nwhere p0 = (0,0,0) indicates the current spatio-temporal\nlocation. R =\n{\n(−1,−1,−1), (−1,−1,0) ,...,( 0,1,1),\n(1,1,1)\n}\nindicates the sampled local (3 × 3 × 3) spatio-\ntemporal receptive ﬁeld cube for 3D convolution in both\ncurrent ( t0) and adjacent time steps ( t−1 and t1), while R′\nonly indicates the local spatial regions in the adjacent time\nsteps (t-1 and t1). The hyperparameter θ ∈[0, 1] tradeoffs\nthe contribution of temporal difference. The higher value of\nθ means the more importance of temporal difference infor-\nmation (e.g., trends of the skin color changes). Specially,\nTDC degrades to vanilla 3D convolution when θ = 0. Then\nquery and key are projected via unshared TDC and BN as\nQ = BN(TDC(X\ntube)), K = BN(TDC(Xtube)). (3)\nFor the value ( V ) projection, point-wise linear projection\nwithout BN is utilized. Then Q, K , V ∈ RD×T ′×H′×W ′\nare\nﬂattened into sequence, and separated into h heads ( Dh =\nD/h for each head). For the i-th head ( i ≤ h), the self-\nattention (SA) can be formulated\nSAi = Softmax(Qi K T\ni /τ)Vi , (4)\nwhere τ controls the sparsity. We ﬁnd that the default setting\nτ = √Dh in Dosovitskiy et al. ( 2020); V aswani et al. ( 2017)\nperforms poorly for rPPG measurement. According to the\nperiodicity of rPPG features, we use a smaller τ value to\nobtain sparser attention activation. The corresponding study\ncan be found in Table 8. The output of TD-MHSA is the\nconcatenation of SA from all heads and then with a linear\n123\n1314 International Journal of Computer Vision (2023) 131:1307–1330\nUpsample\nTemporal Difference Periodic\nTransformer x N'=4\nConcat\nSlow Pathway\nCxT'xH'xW'\nFast Pathway\n(C/2)x(2T')xH'xW'\nLow-level\nTemporal Difference \nTransformer x N'=4\nMid-level\nTemporal Difference CrossAtten\nTransformer x N'=4\nHigh-level\nTemporal Difference CrossAtten\nTransformer x N'=4\nTemporal Difference Periodic\nTransformer x N'=4\nTemporal Difference Periodic\nTransformer x N'=4\nLateral \nConnection\nFig. 3 Framework of the PhysFormer++ with two-stream SlowFast\npathways. Different from the PhysFormer using only slow pathway,\nthe PhysFormer++ extracts and fuses attentional features from slow\nand fast pathways. Moreover, temporal difference periodic transformer\nblocks are used in the slow pathway. The information ﬂow between two\npathways interacts via temporal difference cross-attention transformer\nblocks and lateral connection\nprojection U ∈ RD×D\nTD-MHSA = Concat(SA1;SA2;...;SAh )U. (5)\nAs illustrated in Fig. 2, residual connection and layer nor-\nmalization (LN) would be conducted after TD-MHSA.\nSpatio-Temporal Feed-Forward (ST-FF) The vanilla feed-\nforward network consists of two linear transformation layers,\nwhere the hidden dimension D\n′ between two layers is\nexpanded to learn a richer feature representation. In contrast,\nwe introduce a depthwise 3D convolution (with BN and non-\nlinear activation) between these two layers with extra slight\ncomputational cost but remarkable performance improve-\nment. The beneﬁts are twofold: (1) as a complementation\nof TD-MHSA, ST-FF could reﬁne the local inconsistency\nand parts of noisy features; (2) richer locality provides TD-\nMHSA sufﬁcient relative position cues.\n3.2 PhysFormer++\nIn the PhysFormer, the temporal length Ts of the tube token\nmap is ﬁxed. However, the ﬁxed value of Ts might be sub-\noptimal for robust rPPG feature representation as the larger\nTs reduces the temporal redundancy but loses the ﬁne-grained\ntemporal clues, and vice versa for the smaller Ts . To alleviate\nthis issue, we design the temporal enhanced version Phys-\nFormer++ (see Fig. 3) consisting of two-stream SlowFast\npathways with large and small Ts , respectively. Similar to the\nSlowFast concept in Feichtenhofer et al. ( 2018) and Kaza-\nkos et al. ( 2021), the Slow pathway has high channel capacity\nwith low framerates, and reduces the temporal redundancy.\nIn contrast, the Fast pathway operates at a ﬁne-grained\ntemporal resolution with high framerates. Furthermore, two\nnovel transformer blocks, temporal difference periodic trans-\nformer and temporal difference cross-attention transformer ,\nare designed for the slow and fast pathway, respectively.\nThe former one encodes contextual rPPG periodicity clues\nfor the slow pathway while the latter one introduces efﬁ-\ncient SlowFast interactive attentions for the fast pathway.\nThe SlowFast architecture is able to adaptively mine richer\ntemporally rPPG contexts for robust rPPG measurement.\nAs illustrated in Fig. 3 and detailed architecture in Table 2,\ndifferent from the PhysFormer using a single tube tokenizer,\ntwo tube tokenizers E\nfast\ntube and Eslow\ntube are adopted in Phys-\nFormer++ to form the spatio-temporal tube tokens Xfast\ntube ∈\nRDfast×T fast×H′×W ′\nand Xslow\ntube ∈ RDslow×T slow×H′×W ′\n, respec-\ntively. Default settings Dslow = D = 2Dfast and T fast =\n2T ′ = 2T slow are used for computational tradeoff. Here we\nset temporal scale to two by considering that there are many\nlow-framerate videos in the VIPL-HR dataset (Niu et al.,\n2019a). Too higher scales would result in the pulse rhythm\nincompletion/artifacts for high HR values (e.g., >120 bpm).\nWe will investigate more scales for higher framerate videos in\nthe future. Subsequently, the tube tokens from the slow path-\nway will be forwarded with N = 3N\n′ temporal difference\nperiodic transformer blocks while tube tokens from the fast\npathway will pass N ′ temporal difference transformer and\n2N ′ temporal difference cross-attention transformer blocks.\nSpeciﬁcally, the feature interactions between SlowFast path-\nways are in two folds: (1) all semantic mid- and high-level\nfeatures from the slow path are cross-attentive with those\nfrom the fast path; and (2) the last mid-level features from\ntwo pathways X\nfast-mid\ntube , Xslow-mid\ntube are lateral connected and\nthen aggregated for the high-level propagation in the slow\npathway. The lateral connection and aggregation can be for-\nmulated as\nXslow-mid\ntube = Conv2\n(\nConcat\n(\nXslow-mid\ntube , Conv1\n(\nXfast-mid\ntube\n)))\n,\n(6)\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1315\nTable 2 Architectures of PhysFormer++\nStage Slow pathway Fast pathway Output C × T × H × W\nInput – – 3 × 160 × 128 × 128\nStem\n⎡\n⎢⎢⎢⎢⎢⎣\n1 × 5 × 5C o n v,24\n1 × 2 × 2 MaxPool\n3 × 3 × 3C o n v,48\n1 × 2 × 2 MaxPool\n3 × 3 × 3C o n v,96\n1 × 2 × 2 MaxPool\n⎤\n⎥⎥⎥⎥⎥⎦\n96 × 160 × 16 × 16\nTokenizer 4 × 4 × 4C o n v,96 2 × 4 × 4C o n v,48 Slow: 96 × 40 × 4 × 4\nstride = 4 × 4 × 4 stride = 2 × 4 × 4F a s t : 4 8 × 80 × 4 × 4\nLow-level blocks Temporal difference periodic transformer × 4 Temporal difference transformer × 4S l o w : 9 6 × 40 × 4 × 4\nFast: 48 × 80 × 4 × 4\nMid-level blocks Temporal difference periodic transformer × 4 Temporal difference CrossAtten transformer × 4S l o w : 9 6 × 40 × 4 × 4\nFast: 48 × 80 × 4 × 4\nHigh-level blocks Lateral connection × 1 temporal difference periodic transformer × 4 Temporal difference CrossAtten transformer × 4S l o w : 9 6 × 40 × 4 × 4\nFast: 48 × 80 × 4 × 4\nPre-fusion\n[\n2 × 1 × 1 Upsample\n3 × 1 × 1C o n v,96\n]\n3 × 1 × 1C o n v,48 Slow: 96 × 80 × 4 × 4\nFast: 48 × 80 × 4 × 4\nFusion Concat (Slow, Fast), 144 144 × 80 × 4 × 4\nPredictor\n⎡\n⎢⎢⎣\n2 × 1 × 1 Upsample\n3 × 1 × 1C o n v,72\n1 × 4 × 4 AvgPool\n3 × 1 × 1C o n v,1\n⎤\n⎥⎥⎦ 1 × 160 × 1 × 1\nInside the brackets are the ﬁlter sizes and feature dimensionalities. ‘Conv’ suggests the vanilla 3D convolution. All convolutional layers (except Tokenizer) are with stride=1 and are followed by a\nBN-ReLU layer while ‘MaxPool’ layers are with stride=1x2x2\n123\n1316 International Journal of Computer Vision (2023) 131:1307–1330\nwhere Conv1 is the temporal convolution with size = 3×1×\n1, stride = 2×1×1, padding = 1×0×0 while Conv2 denotes\nthe point-wise convolution with D output channel. The lateral\nconnection adaptively transfers the mid-level ﬁne-grained\nrPPG clues from the Fast pathway to the Slow pathway,\nand provides complementary temporal details for the Slow\npathway to alleviate information loss especially for high-HR\nscenarios (e.g., after exercise). Finally, the reﬁned high-level\nrPPG features from fast and slow (upsampled) pathways are\nconcatenated and forwarded the rPPG predictor head with\ntemporally aggregation, upsampling, spatially averaging, and\n1D signal ˆY ∈ R\nT projection.\nTemporal Difference Multi-head Cross- and Self-Attention\nCompared with the slow pathway, the fast pathway has more\nﬁne-grained features but conducts inefﬁcient and inaccurate\nself-attention due to the temporal redundancy/artifacts. To\nalleviate the weak self-attention issue in the fast pathway, we\npropose the temporal difference multi-head cross- and self-\nattention (TD-MHCSA) module, which could be cascaded\nwith ST-FF module to form the temporal difference cross-\nattention transformer. With TD-MHCSA, the features in the\nfast pathway can not only be reﬁned by its own self-attention\nbut also the cross-attention between the SlowFast pathways.\nThe structure of the TD-MHCSA is illustrated in Fig. 4.\nThe features from the fast pathway X\nfast\ntube are ﬁrst projected\nto query and key via\nQfast = BN(TDC(Xfast\ntube)), K fast = BN(TDC(Xfast\ntube)). (7)\nFor the value ( V fast) projection, point-wise linear pro-\njection without BN is utilized. Then Qfast, K fast, V fast ∈\nRDfast×T fast×H′×W ′\nare ﬂattened into sequence, and separated\ninto h heads (Dfast\nh = Dfast/h for each head). For the i-th head\n(i ≤ h), the self-attention can be formulated\nSAfast\ni = Softmax(Qfast\ni K fast\ni\nT\n/τ)V fast\ni . (8)\nSimilarly, the features from the slow pathway Xslow\ntube are pro-\njected to key K slow via BN (TDC(Xslow\ntube )) as well as the\nvalue ( V slow) projection using point-wise linear projection.\nThen K slow, V slow ∈ RDslow×T slow×H′×W ′\nare ﬂattened into\nsequence, and separated into h heads. For the i-th head\n(i ≤ h), the cross-attention (CA) can be formulated as\nCAi = Softmax(Qfast\ni K slow\ni\nT\n/τ)V slow\ni . (9)\nThus, the combined cross- and self-attention (CSA) is formu-\nlated as CSA i = CAi + SAfast\ni . The output of TD-MHCSA\nis the concatenation of CSA from all heads and then with a\nlinear projection U\nfast ∈ RDfast×Dfast\n, which is formulated\nTD-MHCSA = Concat(SCA1;SCA2;...;SCAh )Ufast. (10)\nTDC TDC 1x1x1\nSoftmax\nCSA\nTDC1x1x1\nSoftmax\nSAfastCA\nFig. 4 Illustration of the temporal difference multi-head cross- and\nself-attention (TD-MHCSA) module\nTDC TDC 1x1x1\nQ KV\nContextual\nPeriodicity\nEncoding\nSoftmax\nBVP Signals Peak Map\nSupervision\nCPSA\nS\nR\nFig. 5 Illustration of the temporal difference multi-head periodic- and\nself-attention (TD-MHPSA) module\nFinally, residual connection and LN layer would be con-\nducted after TD-MHCSA.\nTemporal Difference Multi-head Periodic- and Self-Attention\nInspired by the music transformer (Huang et al., 2019)u s i n g\nrelative attention (Shaw et al., 2018; Wu et al., 2021)t o\nmine richer positional relationship (e.g., periodicity in music\nsignals), we propose the temporal difference multi-head\nperiodic- and self-attention (TD-MHPSA), which extends\nthe TD-MHSA (in Sect. 3.1) with learnable rPPG-aware posi-\ntional contextual periodicity representation. Speciﬁcally, as\ns h o w ni nF i g .5, the learnable contextual periodicity encod-\ning R ∈ R\nT ′H′W ′×T ′H′W ′×D contains the spatio-temporal\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1317\npositional clues, and modulates the query vector Q into\nthe periodic attention S = QR T . In consideration of the\nmulti-head h setting, for the i-th head, the joint contextual\nperiodicity (CP) and self-attention (SA) can be formulated\nas\nCPSAi = Softmax((Qi K T\ni + λ · Si )/τ)Vi , (11)\nwhere λ tradeoffs the CP and SA. Here we follow the mem-\nory efﬁcient implementation in Huang et al. ( 2019)f o r S\ncalculation.\nDespite richer positional periodicity clues, the predicted\nperiodic attention S might be easily inﬂuenced by some\nrPPG-unrelated clues (e.g., light changes and dynamic\nnoise). To alleviate this issue, we propose a periodicity con-\nstraint to supervise the periodic S representation. As shown\nin the top left of Fig. 5, the approximate peak map PM can\nbe obtained via (1) ﬁrst extracting the binary peak signal\nP ∈ R\nT from the ground truth BVP signal Y ∈ RT via\nPt∈T =\n{ 1, if Y t ∈ R peak ,\n0, if Y t /∈ R peak , (12)\nwhere R peak denotes the 1D-region of peak locations; and\nthen (2) calculating the auto-correlation of the peak signal\nP via PM = PP T . Finally, the periodic-attention loss Latten\ncan be calculated with the binary cross-entropy (BCE) loss\nbetween the adaptive-spatial-pooled periodic attention maps\nS′ ∈ RT ′×T ′\n(from each head and each TD-MHPSA module)\nand the subsampled binary peak maps PM’ ∈ RT ′×T ′\n.I tc a n\nbe formulated as\nLatten = 1\nh × N\n∑\ni∈h,j∈N\nBCE(S′,PM’). (13)\nWe also try supervision with L1 regression loss instead of\nBCE loss but with poorer performance.\nRelationship Between PhysFormer and PhysFormer++Phys-\nFormer++ can be treated as an upgraded version of Phys-\nFormer towards excellent performance while with more\ncomputational cost. With similar temporal difference trans-\nformers, PhysFormer can be seen as a slow-pathway version\nof PhysFormer++, which is more lightweight and efﬁcient.\nIn contrast, PhysFormer++ is designed based on a dual-\npathway SlowFast architecture with complex cross-tempo\ninteractions, which is more robust to head motions and less\nsensitive to the video framerate, but with heavier computa-\ntional cost (see Table 12 for efﬁciency analysis).\n3.3 Label Distribution Learning\nSimilar to the facial age estimation task (Gao et al., 2018;\nGeng et al., 2010) that faces at close ages look quite simi-\nlar, facial rPPG signals with close HR values usually have\nsimilar periodicity. Inspired by this observation, instead of\nconsidering each facial video as an instance with one label\n(HR), we regard each facial video as an instance associated\nwith a label distribution. The label distribution covers a cer-\ntain number of class labels, representing the degree that each\nlabel describes the instance. Through this way, one facial\nvideo can contribute to both targeted HR value and its adja-\ncent HRs.\nTo consider the similarity information among HR classes\nduring the training stage, we model the rPPG-based HR\nestimation problem as a speciﬁc L-class multi-label clas-\nsiﬁcation problem, where L = 139 in our case (each integer\nHR value within [42, 180] bpm as a class). A label distri-\nbution p = {p\n1, p2, ...,pL } ∈ RL is assigned to each facial\nvideo X. It is assumed that each entry of p is a real value\nin the range [0,1] such that ∑ L\nk=1 pk = 1. We consider the\nGaussian distribution function, centered at the ground truth\nHR label YHR with the standard deviation σ, to construct the\ncorresponding label distribution p.\npk = 1√\n2πσ\nexp\n(\n−(k − (YHR − 41))2\n2σ2\n)\n. (14)\nThe label distribution loss can be formulated as LLD =\nKL(p,Softmax( ˆp)), where divergence measure KL(·)denotes\nthe Kullback-Leibler (KL) divergence (Gao et al., 2016), and\n( ˆp) is the power spectral density (PSD) of predicted rPPG\nsignals.\nPlease note that the previous work (Niu et al., 2017)a l s o\nconsiders the distribution learning for HR estimation. How-\never, it is totally different with our work: (1) the motivation\nin Niu et al. ( 2017) is to smooth the temporal HR outliers\ncaused by facial movements across continuous video clips,\nwhile our work is more generic, aiming at efﬁcient feature\nlearning across adjacent labels under limited-scale training\ndata; (2) the technique used in Niu et al. ( 2017)i sa f t e ra\npost-HR-estimation for the handcrafted rPPG signals, while\nour work is to design a reasonable supervision signal L\nLD\nfor the PhysFormer family.\n3.4 Curriculum Learning Guided Dynamic Loss\nCurriculum learning (Bengio et al., 2009), as a major machine\nlearning regime with philosophy of easy-to-hard curriculum,\nis utilized to train PhysFormer. In the rPPG measurement\ntask, the supervision signals from temporal domain [e.g.,\nmean square error loss (Chen & McDuff, 2018), negative\nPearson loss (Y u et al., 2019a, b)] and frequency domain\n[e.g., cross-entropy loss (Niu et al., 2020; Y u et al., 2020),\nsignal-to-noise ratio loss (Špetlík et al., 2018)] provide dif-\nferent extents of constraints for model learning. The former\none gives signal-trend-level constraints, which is straight-\n123\n1318 International Journal of Computer Vision (2023) 131:1307–1330\nforward for model convergence but overﬁtting after that. In\ncontrast, the latter one with strong constraints on frequency\ndomain enforces the model learning periodic features within\ntarget frequency bands, which is hard to converge well due\nto the realistic rPPG-irrelevant noise. Inspired by the cur-\nriculum learning, we propose the dynamic supervision to\ngradually enlarge the frequency constraints, which alleviates\nthe overﬁtting issue and beneﬁts the intrinsic rPPG-aware\nfeature learning gradually. Speciﬁcally, exponential incre-\nment strategy is adopted, and comparison with other dynamic\nstrategies (e.g., linear increment) will be shown in Table 11.\nThe dynamic loss L\noverall can be formulated as\nLoverall = α · Ltime  \ntemporal\n+β · (LCE + LLD)  \nfrequency\n+Latten,\nβ = β0 · (η(Epochcurrent−1)/Epochtotal ),\n(15)\nwhere hyperparameters α, β0 and η equal to 0.1, 1.0 and 5.0,\nrespectively. Negative Pearson loss ( 2019a, 2019b) and fre-\nquency cross-entropy loss (Niu et al., 2020; Y u et al., 2020)\nare adopted as Ltime and LCE, respectively. With the dynamic\nsupervision, PhysFormer and PhysFormer++ could perceive\nbetter signal trend at the beginning while such perfect warm-\ning up facilitates the gradually stronger frequency knowledge\nlearning later.\n4 Experimental Evaluation\nIn this section, experiments of rPPG-based physiological\nmeasurement for three types of physiological signals, i.e.,\nheart rate (HR), heart rate variability (HRV), and respiration\nfrequency (RF), are conducted on four benchmark datasets\n(VIPL-HR (Niu et al., 2019a), MAHNOB-HCI (Soleymani\net al., 2012), MMSE-HR (Tulyakov et al., 2016), and OBF\n( L ie ta l . , 2018)). Besides, comprehensive ablations about\nPhysFormer and PhysFormer++ are also investigated in the\nVIPL-HR dataset.\n4.1 Datasets and Performance Metrics\nVIPL-HR (Niu et al., 2019a) is a large-scale dataset for\nremote physiological measurement under less-constrained\nscenarios. It contains 2,378 RGB videos of 107 subjects\nrecorded with different head movements, lighting conditions\nand acquisition devices. MAHNOB-HCI (Soleymani et al.,\n2012) is one of the most widely used benchmark for remote\nHR measurement evaluations. It includes 527 facial videos\nof with 61 fps framerate and 780 × 580 resolution from 27\nsubjects. MMSE-HR (Tulyakov et al., 2016) is a dataset\nincluding 102 RGB videos from 40 subjects, and the raw res-\nolution of each video is at 1040 ×1392. OBF (Li et al., 2018)\nFig. 6 Example video frames from datasets a VIPL-HR (Niu et al.,\n2019a); b MAHNOB-HCI (Soleymani et al., 2012); c MMSE-HR\n(Tulyakov et al., 2016); and d OBF (Li et al., 2018)\nis a high-quality dataset for remote physiological signal mea-\nsurement. It contains 200 ﬁve-minute-long RGB videos with\n60 fps framerate recorded from 100 healthy adults. The exam-\nple video frames from these four rPPG datasets are illustrated\nin Fig. 6.\nFor MAHNOB-HCI, as there are no available BVP ground\ntruth, we ﬁrst smooth the sharp ECG signals (with 10-\npoint averaging strategy) into pseudo BVP signals as ground\ntruth. Speciﬁcally, to alleviate the incorrect synchronization\nbetween videos and ground truth signals in MAHNOB-HCI,\nOBF, and VIPL-HR datasets, we ﬁrst extract the coarse green\nchannel signals via averaging the segmented facial skin in\neach frame. Then, we calculate the cross-correlation between\nthe coarse green rPPG signals and (pseudo) BVP signals, and\nuse the maximum-correlation phase to calibrate/compensate\nthe phase bias. Furthermore, we remove the samples with HR\n> 180 in the VIPL-HR and MMSE-HR datasets because the\nground truths in these samples are unreliable due to poor con-\ntact of sensors (resulting in very noisy and ﬂuctuated HRs).\nIn terms of evaluation metrics, average HR estimation task\nis evaluated on all four datasets while HRV and RF estimation\ntasks on high-quality OBF (Li et al., 2018) dataset. Speciﬁ-\ncally, we follow existing methods (Lu et al., 2021; Niu et al.,\n2020; Y u et al., 2019b) and report low frequency (LF), high\nfrequency (HF), and LF/HF ratio for HRV and RF estima-\ntion. We report the most commonly used performance metrics\nfor evaluation, including the standard deviation (SD), mean\nabsolute error (MAE), root mean square error (RMSE), and\nPearson’s correlation coefﬁcient ( r).\n4.2 Implementation Details\nBoth PhysFormer and PhysFormer++ are implemented with\nPytorch. For each video clip, the MTCNN face detector\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1319\nTable 3 Intra-dataset testing results on the VIPL-HR dataset\nMethod SD ↓ (bpm) MAE ↓ (bpm) RMSE ↓ (bpm) r ↑\nTulyakov2016 (Tulyakov et al., 2016)▲ 18.0 15.9 21.0 0.11\nPOS (Wang et al., 2016)▲ 15.3 11.5 17.2 0.30\nCHROM (De Haan & Jeanne, 2013)▲ 15.1 11.4 16.9 0.28\nRhythmNet (Niu et al., 2019a)♦ 8.11 5.30 8.14 0.76\nST-Attention (Niu et al., 2019b)♦ 7.99 5.40 7.99 0.66\nNAS-HR (Lu & Han, 2021)♦ 8.10 5.12 8.01 0.79\nCVD (Niu et al., 2020)♦ 7.92 5.02 7.97 0.79\nDual-GAN (Lu et al., 2021)♦ 7.63 4.93 7.68 0.81\nI3D (Carreira & Zisserman, 2017)⋆ 15.9 12.0 15.9 0.07\nPhysNet (Y u et al., 2019a)⋆ 14.9 10.8 14.8 0.20\nDeepPhys (Chen & McDuff, 2018)⋆ 13.6 11.0 13.8 0.11\nVideoTransformer (Revanur et al., 2022)⋆ 13.5 10.4 13.2 0.16\nAutoHR (Y u et al., 2020)⋆ 8.48 5.68 8.68 0.72\nPhysFormer (Ours)⋆ 7.74 4.97 7.79 0.78\nPhysFormer++ (Ours)⋆ 7.65 4.88 7.62 0.80\nThe symbols ▲ , ♦ and ⋆ denote traditional, non-end-to- end learning based and end-to-end learning based methods, respectively. Best results are\nmarked in bold and second best in underline\nTable 4 Intra-dataset results on\nthe MAHNOB-HCI dataset Method SD ↓ (bpm) MAE ↓ (bpm) RMSE ↓ (bpm) r ↑\nPoh2010 (Poh et al., 2010a)▲ 13.5 – 13.6 0.36\nCHROM (De Haan & Jeanne, 2013)▲ – 13.49 22.36 0.21\nLi2014 (Li et al., 2014)▲ 6.88 – 7.62 0.81\nTulyakov2016 (Tulyakov et al., 2016)▲ 5.81 4.96 6.23 0.83\nSynRhythm (Niu et al., 2018)♦ 10.88 – 11.08 –\nRhythmNet (Niu et al., 2019a)♦ 3.99 – 3.99 0.87\nHR-CNN (Špetlík et al., 2018)⋆ – 7.25 9.24 0.51\nrPPGNet (Y u et al., 2019b)⋆ 7.82 5.51 7.82 0.78\nDeepPhys (Chen & McDuff, 2018)⋆ –4 . 5 7 – –\nAutoHR (Y u et al., 2020)⋆ 4.73 3.78 5.10 0.86\nMeta-rPPG (Lee et al., 2020)⋆ 4.9 3.01 3.68 0.85\nPhysFormer (Ours)⋆ 3.87 3.25 3.97 0.87\nPhysFormer++ (Ours)⋆ 3.90 3.23 3.88 0.87\nThe symbols ▲ , ♦ and ⋆ denote traditional, non-end-to- end learning based and end-to-end learning based\nmethods, respectively. Best results are marked in bold and second best in underline\n(Zhang et al., 2016) is used to crop the enlarged face area\nat the ﬁrst frame and ﬁx the region through the following\nframes. The videos in MAHNOB-HCI and OBF are down-\nsampled to 30 fps for efﬁciency. The numbers of temporal\ndifference transformer blocks N = 12, transformer heads\nh = 4, channel dimension D = 96, hidden dimension in\nST-FF D\n′ = 144 are used for PhysFormer while temporal\ndifference coefﬁcient θ = 0.7 and attention sparsity τ = 2.0\nfor TD-MHSA. λ = 0.5 is utilized in the TD-MHPSA. The\ntargeted tube size Ts × Hs × Ws equals to 4 ×4×4. For the\nR peak calculation in Eq. ( 12), the function ‘ﬁndpeaks()’ in\nMatlab is used for BVP peak detection, and then the detected\npeak locations are extended with successive ±3 neighbors.\nIn the training stage, we randomly sample RGB face\nclips with size 160 ×128×128 ( T × H × W ) as model\ninputs. Random horizontal ﬂipping and temporally up/down-\nsampling (Y u et al., 2020) are used for data augmentation.\nThe PhysFormer is trained with Adam optimizer and the\ninitial learning rate and weight decay are 1e −4 and 5e −5,\nrespectively. We cannot ﬁnd obvious performance improve-\nment using AdamW optimizer. We train models with 25\nepochs with ﬁxed setting α = 0.1 for temporal loss while\nexponentially increased parameter β ∈[ 1,5] for frequency\nlosses. We set standard deviation σ = 1.0 for label distri-\nbution learning. The batch size is 4 on one V100 GPU. In\nthe testing stage, similar to Niu et al. ( 2019a), we uniformly\n123\n1320 International Journal of Computer Vision (2023) 131:1307–1330\nseparate 30-second videos into three short clips with 10 sec-\nonds, and then video-level HR is calculated via averaging\nHRs from three short clips.\n4.3 Intra-dataset Testing\nIn this subsection, two datasets (VIPL-HR and MAHNOB-\nHCI) are used for intra-dataset testing on HR estimation\nwhile the OBF dataset is used for intra-dataset HR, HRV\nand RF estimation.\nHR Estimation on VIPL-HR Here we follow (Niu et al.,\n2019a) and use a subject-exclusive 5-fold cross-validation\nprotocol on VIPL-HR. As shown in Table 3, all three tradi-\ntional methods [Tulyakov2016 (Tulyakov et al., 2016), POS\n(Wang et al., 2016) and CHROM (De Haan & Jeanne, 2013)]\nperform poorly due to the complex scenarios (e.g., large\nhead movement and various illumination) in the VIPL-HR\ndataset. In terms of deep learning based methods, the exist-\ning end-to-end learning based methods [e.g., PhysNet (Y u et\nal., 2019a), DeepPhys (Chen & McDuff, 2018), and AutoHR\n(Y u et al., 2020)] predict less reliable HR values with larger\nRMSE compared with non-end-to-end learning approaches\n[e.g., RhythmNet (Niu et al., 2019a), ST-Attention (Niu et\nal., 2019b), NAS-HR (Lu & Han, 2021), CVD Niu et al.\n(2020), and Dual-GAN (Lu et al., 2021)]. Such the large\nperformance margin might be caused by the coarse and over-\nﬁtted rPPG features extracted from the end-to-end models.\nIn contrast, all ﬁve non-end-to-end methods ﬁrst extract ﬁne-\ngrained signal maps from multiple facial ROIs, and then more\ndedicated rPPG clues would be extracted via the cascaded\nmodels. Without strict and heavy preprocessing procedure in\nNiu et al. ( 2019a, 2019b, 2020), Lu and Han ( 2021) and Lu\net al. ( 2021), the proposed PhysFormer and PhysFormer++\ncan be trained from scratch on facial videos directly, and\nachieve better or on par performance with state-of-the-art\nnon-end-to-end learning based method Dual-GAN (Lu et\nal., 2021). It indicates that PhysFormer and PhysFormer++\nare able to learn the intrinsic and periodic rPPG-aware fea-\ntures automatically. It can be seen from Table 3 that the\nproposed PhysFormer family outperforms the VideoTrans-\nformer (Revanur et al., 2022) by a large margin, indicating\nthe importance of local and global spatio-temporal physio-\nlogical propagation.\nIn order to further check the correlations between the\npredicted HRs and the ground-truth HRs, we plot the HR\nestimation results against the ground truths in Fig. 7a. From\nthe ﬁgure we can see that the predicted HRs from Phys-\nFormer++ and the ground-truth HRs are well correlated in a\nwide range of HR from 47 to 147 bpm.\nHR Estimation on MAHNOB-HCI For the HR esti-\nmation tasks on MAHNOB-HCI, similar to Y u et al.\n(2019b), subject-independent 9-fold cross-validation proto-\ncol is adopted. In consideration of the convergence difﬁculty\ndue to the low illumination and high compression videos\nin MAHNOB-HCI, we ﬁnetune the VIPL-HR pretrained\nmodels on MAHNOB-HCI for further 15 epochs. The HR\nestimation results are shown in Table 4. The proposed Phys-\nFormer and PhysFormer++ achieves the lowest SD (3.87\nbpm) and highest r (0.87) among the traditional, non-end-\nto-end learning, and end-to-end learning methods, which\nindicates the reliability of the learned rPPG features from\nPhysFormer family under sufﬁcient supervision. Our perfor-\nmance is on par with the latest end-to-end learning method\nMeta-rPPG (Lee et al., 2020) without transductive adaptation\nfrom target frames.\nHR, HRV and RF Estimation on OBF Besides HR estimation,\nwe also conduct experiments for three types of physiological\nsignals, i.e., HR, RF, and HRV measurement on the OBF (Li\net al., 2018) dataset. Following Niu et al. ( 2020) and Y u et\nal. ( 2019b), we use a 10-fold subject-exclusive protocol for\nall experiments. All the results are shown in Table 5.I ti s\nclear that the proposed PhysFormer and PhysFormer++ out-\nperform the existing state-of-the-art traditional [ROI_green\n(Li et al., 2018)), CHROM (De Haan & Jeanne, 2013), POS\n(Wang et al., 2016)] and end-to-end learning [rPPGNet (Y u\net al., 2019b)] methods by a large margin on all evaluation\nmetrics for HR, RF and all HRV features. The proposed Phys-\nFormer and PhysFormer++ give more accurate estimation in\nterms of HR, RF, and LF/HF compared with the preprocessed\nsignal map based non-end-to-end learning method CVD (Niu\net al., 2020). These results indicate that PhysFormer family\ncould not only handle the average HR estimation task but\nalso give a promising prediction of the rPPG signal for RF\nmeasurement and HRV analysis, which shows its potential\nin many healthcare applications.\nWe also check the short-time HR estimation performance\nof the after exercising scenario on the OBF, in which the\nsubject’s HR decreases rapidly. Two examples are given in\nFig. 7b. It can be seen that PhysFormer++ could follow the\ntrend of HR changes well, which indicates the proposed\nmodel is robust in the signiﬁcant HR changing scenarios.\nWe further check the predicted rPPG signals of the Phys-\nFormer++ from these two examples in Fig. 7c. From the\nresults, we can see that the proposed method could give an\naccurate prediction of the interbeat intervals (IBIs), thus can\ngive a robust estimation of RF and HRV features (Table 6).\n4.4 Cross-dataset Testing\nBesides of the intra-dataset testings on the VIPL-HR,\nMAHNOB-HCI, and OBF datasets, we also conduct cross-\ndataset testings on MMSE-HR (Tulyakov et al., 2016)\nfollowing the protocol of Niu et al. ( 2019a). The models\ntrained on VIPL-HR are directly tested on MMSE-HR. All\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1321\nTable 5 Performance comparison of HR and RF measurement as well as HRV analysis on the OBF dataset\nHR (bpm) RF (Hz) LF (u.n) HF (u.n) LF/HF\nMethod RMSE r RMSE r RMSE r RMSE r RMSE r\nR O I _ g r e e n( L ie ta l . ,2018)▲ 2.162 0.99 0.084 0.321 0.24 0.573 0.24 0.573 0.832 0.571\nCHROM (De Haan & Jeanne, 2013)▲ 2.733 0.98 0.081 0.224 0.206 0.524 0.206 0.524 0.863 0.459\nPOS (Wang et al., 2016)▲ 1.906 0.991 0.07 0.44 0.158 0.727 0.158 0.727 0.679 0.687\nCVD (Niu et al., 2020)♦ 1.26 0.996 0.058 0.606 0.09 0.914 0.09 0.914 0.453 0.877\nrPPGNet (Y u et al., 2019b)⋆ 1.8 0.992 0.064 0.53 0.135 0.804 0.135 0.804 0.589 0.773\nPhysFormer (Ours)⋆ 0.804 0.998 0.054 0.661 0.086 0.912 0.086 0.912 0.39 0.896\nPhysFormer++ (Ours)⋆ 0.765 0.998 0.052 0.686 0.083 0.921 0.083 0.921 0.368 0.908\nThe symbols ▲ , ♦ and ⋆ denote traditional, non-end-to- end learning based and end-to-end learning based methods, respectively. Best results are\nmarked in bold and second best in underline\nTable 6 Cross-dataset results\non the MMSE-HR dataset Method SD ↓ (bpm) MAE ↓ (bpm) RMSE ↓ (bpm) r ↑\nLi2014 (Li et al., 2014)▲ 20.02 – 19.95 0.38\nCHROM (De Haan & Jeanne, 2013)▲ 14.08 – 13.97 0.55\nTulyakov2016 (Tulyakov et al., 2016)▲ 12.24 – 11.37 0.71\nST-Attention (Niu et al., 2019b)♦ 9.66 – 10.10 0.64\nRhythmNet (Niu et al., 2019a)♦ 6.98 – 7.33 0.78\nCVD (Niu et al., 2020)♦ 6.06 – 6.04 0.84\nPhysNet (Y u et al., 2019a)⋆ 12.76 – 13.25 0.44\nTS-CAN (Liu et al., 2020)⋆ – 3.85 7.21 0.86\nAutoHR (Y u et al., 2020)⋆ 5.71 – 5.87 0.89\nEfﬁcientPhys-C (Liu et al., 2023)⋆ – 2.91 5.43 0.92\nEfﬁcientPhys-T1 (Liu et al., 2023)⋆ – 3.48 7.21 0.86\nPhysFormer (Ours)⋆ 5.22 2.84 5.36 0.92\nPhysFormer++ (Ours)⋆ 5.09 2.71 5.15 0.93\nThe symbols ▲ , ♦ and ⋆ denote traditional, non-end-to- end learning based and end-to-end learning based\nmethods, respectively. Best results are marked in bold and second best in underline\nFig. 7 a The scatter plot of the ground truth HR gt and the predicted\nHRpre via PhysFormer++ of all the face videos on VIPL-HR dataset.\nb Two examples of the short-time HR estimation from PhysFormer++\nfor face videos with signiﬁcantly decreased HR. c Two example curves\nof the predicted rPPG signals from PhysFormer++ and the ground truth\nECG signals used to calculate the HRV features\n123\n1322 International Journal of Computer Vision (2023) 131:1307–1330\nTable 7 Ablation of Tube\nTokenization of PhysFormer Inputs [Stem] Feature size [Tube size] Token numbers RMSE ↓ (bpm)\n160 × 128 × 128 [ ×] 160 × 128 × 128 [4 × 32 × 32] 40 × 4 × 4 10.62\n160 × 128 × 128 [ √] 160 × 16 × 16 [4 × 4 × 4] 40 × 4 × 4 7.56\n160 × 96 × 96 [ √] 160 × 12 × 12 [4 × 4 × 4] 40 × 3 × 38 . 0 3\n160 × 128 × 128 [ √] 160 × 16 × 16 [4 × 16 × 16] 40 × 1 × 1 10.61\n160 × 128 × 128 [ √] 160 × 16 × 16 [2 × 4 × 4] 80 × 4 × 47 . 8 1\nBest result is marked in bold\nThe three dimensions in tensors indicate length × height×width\nthe results of the proposed PhysFormer family and the state-\nof-the-art methods are shown in Table 12. It is clear that\nPhysFormer and PhysFormer++ generalize well in unseen\ndomains (e.g., skin tone and lighting conditions). It is worth\nnoting that PhysFormer++ achieves the lowest SD (5.09\nbpm), MAE (2.71 bpm), RMSE (5.15 bpm) as well as the\nhighest r (0.93) among the traditional, non-end-to-end learn-\ning and end-to-end learning based methods, indicating (1) the\npredicted HRs are highly correlated with the ground truth\nHRs, and (2) the model learns domain-invariant intrinsic\nrPPG-aware features. Compared with the spatio-temporal\ntransformer based EfﬁcientPhys-T1 (Liu et al., 2023), our\nproposed PhysFormer and PhysFormer++ are able to pre-\ndict more accurate physiological signals, which indicates the\neffectiveness of the long-range spatio-temporal attention.\n4.5 Ablation Study\nHere We provide the results of ablation studies for HR esti-\nm a t i o no nt h e Fold-1 of the VIPL-HR (Niu et al., 2019a)\ndataset. Speciﬁcally, we ﬁrst evaluate the impacts of archi-\ntecture conﬁgurations for PhysFormer in terms of ‘Tube\nTokenization’, ‘TD-MHSA ’ and ‘ST-FF’. Then based on the\noptimal conﬁguration of PhysFormer, the impacts of archi-\ntecture conﬁgurations of PhysFormer++ with ‘TD-MHPSA ’\nand ‘SlowFast architecture’ will be studied. Finally, we study\nthe transformer conﬁgurations (‘ θ in TDC’ and ‘layer/head\nnumbers’) and the training receipts (‘label distribution learn-\ning’ and ‘dynamic supervision) for the whole PhysFormer\nfamily (i.e., PhysFormer and PhyFormer++).\nImpact of Tube Tokenization in PhysFormer In the default\nsetting of PhysFormer, a shallow stem cascaded with a tube\ntokenization is used. In this ablation, we consider other four\ntokenization conﬁgurations with or w/o stem. It can be seen\nfrom the ﬁrst row in Table 7 that the stem helps the Phys-\nFormer see better (Xiao et al., 2021), and the RMSE increases\ndramatically ( + 3.06 bpm) when w/o the stem. Then we\ninvestigate the impacts of the spatial and temporal domains\nin tube tokenization. It is clear that the result in the fourth\nrow with full spatial projection is quite poor (RMSE = 10.61\nbpm), indicating the necessity of the spatial attention. In con-\nTable 8 Ablation of TD-MHSA and ST-FF in PhysFormer\nMHSA τ Feed-forward RMSE (bpm) ↓\n– – ST-FF 9.81\nTD-MHSA √Dh ≈ 4.9 ST-FF 9.51\nTD-MHSA 2.0 ST-FF 7.56\nV anilla MHSA 2.0 ST-FF 10.43\nTD-MHSA 2.0 V anilla FF 8.27\nBest result is marked in bold\nTable 9 Ablation of TD-MHPSA for the single pathway conﬁguration\nin PhysFormer++\nPathway MHSA Latten RMSE (bpm) ↓\nSlow TD-MHSA – 7.56\nSlow TD-MHPSA – 7.69\nSlow TD-MHPSA √ 7.43\nFast TD-MHSA – 7.81\nFast TD-MHPSA – 8.12\nFast TD-MHPSA √ 7.85\nBest result is marked in bold\ntrast, tokenization with smaller tempos (e.g., [2 × 4 × 4]) or\nspatial inputs (e.g., 160 × 96 × 96) reduces performance\nslightly. Based on the observed results, tokenizations with\n[4 × 4 × 4] and [2 × 4 × 4] are adopted for the defaulted\nsetting of slow and fast pathway in PhysFormer++, respec-\ntively.\nImpact of TD-MHSA and ST-FF in PhysFormer As shown\nin Table 8, both the TD-MHSA and ST-FF play vital roles in\nPhysFormer. The result in the ﬁrst row shows that the perfor-\nmance degrades sharply without spatio-temporal attention.\nMoreover, it can be seen from the last two rows that with-\nout TD-MHSA/ST-FF, PhysFormer with vanilla MHSA/FF\nobtains 10.43/8.27 bpm RMSE. Thus, we can draw the con-\nclusion that the key element ‘vanilla MHSA ’ in transformer\ncannot provide rPPG performance gain although it captures\nthe long-term global spatio-temporal physiological features.\nIn contrast, the proposed ‘TD-MHSA ’ beneﬁts the rPPG mea-\nsurement via local spatio-temporal physiological clue guided\nlong-term global spatio-temporal physiological aggregation.\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1323\nTable 10 Ablation of SlowFast\ntwo-pathway based architecture\nin PhysFormer++\nTD-MHPSA Lateral connect TD-MHCSA RMSE\n–– – 7 . 7 8\nSlow pathway – – 7.58\nSlow pathway High-level – 7.34\nSlow pathway Mid&High-level – 7.38\nSlow pathway High-level High-level 7.28\nSlow pathway High-level Mid&High-level 7.16\nSlow pathway High-level Low&Mid&High-level 7.24\nBest result is marked in bold\nOne important ﬁnding in this research is that, the temperature\nτ inﬂuences the MHSA a lot. When the τ = √Dh like pre-\nvious ViT (Dosovitskiy et al., 2020; Arnab et al., 2021), the\npredicted rPPG signals are unsatisﬁed (RMSE = 9.51 bpm).\nRegularizing the τ with smaller value enforces sparser spatio-\ntemporal attention, which is effective for the quasi-periodic\nrPPG task.\nImpact of TD-MHPSA for Different Pathway in Phys-\nFormer++ Based on the TD-MHSA in PhysFormer, the\nPhysFormer++ further extends the slow pathway with the\nmore periodic TD-MHPSA modules. Table 9 shows the\nresults of the TD-MHPSA for single pathway conﬁguration.\nIt is interesting to ﬁnd that compared with TD-MHSA, the\nperformance even drop for both slow and fast pathways when\nassembling with TD-MHPSA but without explicit attention\nsupervision L\natten. When training TD-MHPSA with Latten,\nthe RMSE is decreased by 0.26 and 0.27 bpm for the slow\nand fast pathway, respectively. It indicates the importance\nof explicit rPPG-aware periodicity supervision. Some visu-\nalizations with and without L\natten can be found in Sect. 4.7.\nFrom the results in Table 9 we can see that the TD-MHPSA\nwith Latten beneﬁts the periodic rPPG clue mining in the slow\npathway while limited effects for the fast pathway. It may\nbe because the attention loss calculated from the periodic\nmaps with huger temporal resolution in the fast pathway is\ninefﬁcient to back-propagate the rPPG-aware information.\nThus, we only apply the TD-MHPSA in the slow pathway as\nthe defaulted setting for PhysFormer++.\nImpact of the SlowFast Architecture in PhysFormer++\nTable 10 illustrates the ablations of SlowFast two-pathway\nbased architecture in PhysFormer++. From the results of the\nﬁrst two rows we can see that such SlowFast rPPG models\neven achieve inferior performance (7.78/7.58 vs. 7.56 bpm\nRMSE) compared with single pathway based PhysFormer.\nThe unsatisﬁed results might be caused by the lack of efﬁcient\nrPPG feature interaction between two pathways. We also con-\nduct experiments with lateral connections in different levels\nand cross-attention based TD-MHCSA in the fast pathway.\nFrom Table10 we can obviously ﬁnd that both lateral connec-\ntions and TD-MHCSA improve the performance remarkably.\nThis is because the former one brings more temporally ﬁne-\nFig. 8 Impacts of the a σ in label distribution learning for PhysFormer\nand PhysFormer++ and b θ in TD-MHSA, TD-MHCSA, and TD-\nMHPSA\nFig. 9 Ablation of the a layers and b heads in PhysFormer and Phys-\nFormer++\ngrained clues back to the slow pathway to alleviate rPPG\ninformation loss while the latter one leverages the cross -\nattention features to reﬁne the redundant rPPG features in\nthe fast pathway. The best conﬁguration for PhysFormer++\nis with high-level lateral connection and mid&high-level TD-\nMHCSA.\nImpact of θ and Layer/Head Numbers in the PhysFormer\nFamily Hyperparameter θ tradeoffs the contribution of local\ntemporal gradient information. As illustrated in Fig. 8b,\nPhysFormer could achieve smaller RMSE when θ = 0.4\n123\n1324 International Journal of Computer Vision (2023) 131:1307–1330\nFig. 10 Testing results of ﬁxed and dynamic frequency supervisions\nfor a PhysFormer and b PhysFormer++ on the Fold-1 of VIPL-HR\nand 0.7 while PhysFormer++ obtains the best performance\nwhen θ = 0.7, indicating the importance of the normal-\nized local temporal difference features for global spatio-\ntemporal attention. We also investigate how the layer and\nhead numbers inﬂuence the performance of PhysFormer and\nPhysFormer++. As shown in Fig. 9a, with deeper tempo-\nral transformer blocks, the RMSE are reduced progressively\ndespite heavier computational cost. In terms of the impact of\nhead numbers, it is clear to ﬁnd from Fig. 9bt h a tt h eP h y s -\nFormer family with four heads performs the best while fewer\nheads lead to sharp performance drops.\nImpact of Label Distribution Learning for the PhysFormer\nFamily Besides the temporal loss L\ntime and frequency cross-\nentropy loss LCE, the ablations w/ and w/o label distribution\nloss LLD are shown in the last four rows of Table 11.\nAlthough the LLD performs slightly worse (respective +0.12\nand + 0.13 bpm RMSE for PhysFormer and PhysFormer++)\nthan LCE, the best performance can be achieved using both\nlosses, indicating the effectiveness of explicit distribution\nconstraints for extreme-frequency interference alleviation\nand adjacent label knowledgement propagation. It is inter-\nesting to ﬁnd from the last two rows in both PhysFormer and\nPhysFormer++ that using real PSD distribution from ground\ntruth PPG signals as p, the performance is inferior due to the\nlack of an obvious peak in the distribution and partial noise.\nWe can also ﬁnd from the Fig. 8a that the σ ranged from 0.9\nto 1.2 for L\nLD are suitable to achieve good performance.\nImpact of Dynamic Supervision for the PhysFormer Family\nFigure 10 illustrates the testing performance of PhysFormer\nand PhysFormer++ on Fold-1 VIPL-HR when training with\nﬁxed and dynamic supervision. It is clear that with expo-\nnentially increased frequency loss, models in the blue curves\nconverge faster and achieve smaller RMSE. We also compare\nseveral kinds of ﬁxed and dynamic strategies in Table 11.T h e\nresults in the ﬁrst four rows indicate (1) using ﬁxed higher β\nleads to poorer performance caused by the convergency difﬁ-\nculty; (2) models with the exponentially increased β perform\nbetter than using linear increment.\n4.6 Efficiency Analysis\nHere we also investigate the computational cost 1 com-\npared with the baselines. The number of parameters and\nthe multiply-accumulates (MACs) are shown in Table 12.\nDespite huge number of parameters, PhysFormer and Phys-\nFormer++ are with smaller MACs compared with base-\nlines PhysNet, TS-CAN, and AutoHR. Compared with\nPhysFormer, the PhysFormer++ introduces extra 2.76M\nparamters and 1.16G MACs. The inference time of one\nface clip 3 × 160 × 128 × 128 ( C × T × H × W )\nfor PhysFormer and PhysFormer++ on one V100 GPU\nis 29ms and 40ms, respectively. Despite slightly heavier,\nit can predict more accurate rPPG signals on both intra-\ndataset (− 0.17 bpm RMSE on VIPL-HR) and cross-dataset\n(− 0.21 bpm RMSE on MMSE-HR) testings. Towards efﬁ-\ncient mobile-level rPPG applications, the computational cost\nof the proposed PhysFormer family is still unsatisﬁed. One\npotential future direction is to design more lightweight Phys-\nFormer with advanced network quantization (Lin et al., 2021)\nand binarization (Qin et al., 2022) techniques.\n4.7 Visualization and Discussion\nVisualization of the Self-Attention Map We visualize the\nattention maps from the last TD-MHSA module of Phys-\nFormer (left) and the last TD-MHCSA module in the fast\npathway of PhysFormer++ (right) in Fig. 11. The x and y\naxes of the attention map indicate the attention conﬁdence\nfrom key and query tube tokens, respectively. From the atten-\ntion maps activated from the video sample with limited head\nmovement in Fig. 11a, we can easily ﬁnd periodic or quasi-\nperiodic responses along both axes, indicating the periodicity\nof the intrinsic rPPG features from PhysFormer and Phys-\nFormer++. To be speciﬁc, given the 530th tube token (in blue)\nfrom the forehead (spatial face domain) and peak (temporal\nsignal domain) locations as a query, the corresponding key\n1 https://pypi.org/project/thop/.\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1325\n1\n5\n10\n15\n20\n25\n30\n35\n40\n4035302520151051\n1\n2\n3\n4\n1234\nQuery ( T'xH'xW' = 40x4x4 )\nKey ( T'xH'xW' = 40x4x4 ) A Query\nAttention\nMap\nKey \nResponses\nQuery\n1\n5\n10\n15\n20\n25\n30\n35\n4035302520151051\n1\n2\n3\n4\n1234\nQuery ( T'xH'xW' = 40x4x4 )\nKey ( T'xH'xW' = 40x4x4 )\nA Query\nAttention\nMap\nKey \nResponses\n40\n2\n10\n20\n30\n40\n50Query ( TfastxH'xW' = 80x4x4 )\nAttention \nMap\n60\n70\n80\n80706050403020102\nKey ( TfastxH'xW' = 80x4x4 )\n40\nKey \nResponses\nQuery\n2\n10\n20\n30\n40\n50\n60\n70\n80706050403020102\nQuery ( TfastxH'xW' = 80x4x4 )\nKey ( TfastxH'xW' = 80x4x4 )\nAttention \nMap\nKey \nResponses\n80\n(a) Visualization on the video sample with limited head movement\n(b) Visualization on the video sample with serious head movement\nQuery\nQuery\nFig. 11 Visualization of the attention maps from (left) the 1st head in\nlast TD-MHSA module of PhysFormer and (right) the 1st head in last\nTD-MHCSA module of the fast pathway in PhysFormer++. Given the\n530th and 276th tube tokens in blue as the query for the video samples\nwith a limited head movement and b serioud head movement, represen-\ntative key responses are illustrated (the brighter, the more attentive). The\npredicted downsampled rPPG signals as well as the ground truth BVP\nsignals are shown for temporal attention understanding (Color ﬁgure\nonline)\n123\n1326 International Journal of Computer Vision (2023) 131:1307–1330\nTable 11 Ablation of dynamic\nloss in the frequency domain for\nPhysFormer and PhysFormer++\nFrequency loss β Strategy RMSE (bpm) ↓\nPhysFormer\nLCE + LLD 1.0 Fixed 8.48\nLCE + LLD 5.0 Fixed 8.86\nLCE + LLD [1.0, 5.0] Linear 8.37\nLCE + LLD [1.0, 5.0] Exponential 7.56\nLCE [1.0, 5.0] Exponential 8.09\nLLD [1.0, 5.0] Exponential 8.21\nLLD (real distribution) [1.0, 5.0] Exponential 8.72\nPhysFormer++\nLCE + LLD 1.0 Fixed 7.98\nLCE + LLD 5.0 Fixed 8.54\nLCE + LLD [1.0, 5.0] Linear 8.13\nLCE + LLD [1.0, 5.0] Exponential 7.16\nLCE [1.0, 5.0] Exponential 7.76\nLLD [1.0, 5.0] Exponential 7.89\nLLD (real distribution) [1.0, 5.0] Exponential 8.67\nBest results are marked in bold\nThe temporal loss Ltime is with ﬁxed α = 0.1 here. ‘CE’ and ‘LD’ denote cross-entropy and label distribution,\nrespectively\nTable 12 Cross-dataset results\nwith computational cost on\nMMSE-HR\nMethod #Param.(M) MACs(G) RMSE ↓ (bpm)\nPhysNet (Y u et al., 2019a) 0.73 65.19 13.25\nTS-CAN (Liu et al., 2020) 3.91 61.96 7.21\nAutoHR (Y u et al., 2020) 0.99 189.22 5.87\nEfﬁcientPhys-C (Liu et al., 2023) 3.84 31.32 5.43\nPhysFormer (Ours) 7.03 47.01 5.36\nPhysFormer++ (Ours) 9.79 49.85 5.15\nBest result is marked in bold and second best in underline\nThe FLOPs are calculated with the video input size 3 ×160×128×128 ( C × T × H × W ) for PhysNet/\nAutoHR/ PhysFormer/ PhysFormer++ while 3 ×160×96×96 for TS-CAN/EfﬁcientPhys\nresponses are illustrated at the blue line in the attention map.\nOn the one hand, it can be seen from the key responses that\ndominant spatial attentions focus on the facial skin regions\nand discard unrelated background. On the other hand, the\ntemporal localizations of the key responses are around peak\npositions in the predicted rPPG signals. All these patterns are\nreasonable: (1) the forehead and cheek regions (V erkruysse et\nal., 2008) have richer blood volume for rPPG measurement\nand are also reliable since these regions are less affected\nby facial muscle movements due to e.g., facial expressions,\ntalking; and (2) rPPG signals from healthy people are usually\nperiodic.\nWe also visualize the attention maps from another video\nsample with serious head movement in Fig. 11b. It can be\nobserved from the left subﬁgure that the attentional response\nof PhysFormer is inaccurate (e.g., focusing on the neck\nregion) when the head moves to the left. Another issue is\nthat due to the large temporal token size ( T\ns = 4) in the tok-\nenization stage, the temporal rPPG clues might be partially\ndiscarded, resulting in the sensitivity about the head move-\nment and the biased rPPG prediction (i.e., huge IBI gaps\nbetween the predicted rPPG and ground truth BVP signals).\nIn contrast, it can be seen from the right subﬁgure in Fig. 11b\nthat the attentional response and the predicted rPPG signal\nfrom PhysFormer++ are reliable, indicating the effectiveness\nof the SlowFast architecture and advanced attention modules.\nOverall, two limitations of the spatio-temporal attention\ncould be concluded from Fig. 11. First, there are still some\nunexpected responses (e.g., continuous query tokens with\nsimilar key responses) in the attention map, which might\nintroduce task-irrelevant noise and damage to the perfor-\nmance. Second, the temporal attentions are not accurate\nunder serious head movement scenarios, and some are coarse\nwith phase shifts.\nVisualization of the Periodic Attention Map We also visu-\nalize the periodic attention map from the last TD-MHPSA\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1327\nt\nt\nPeriodic attention map w/Periodic attention map\nPeriodic attention map w/Periodic attention map\nFig. 12 Visualization of the periodic attention maps from the 1st head\nin last TD-MHPSA module of the slow pathway in PhysFormer++.\nThe top row show the periodic attention map from the facial video\nwith limited head movement while the bottom one with serious head\nmovement\nRMSE (bmp)\n250 500 1000 Original\nbitrate (kb/s)\nx264 codec with different bitrates on OBF\nRMSE (bmp)\nDifferent face resolutions on VIPL-HR\n16x16\nresolution\n32x32 64x64 Original\n(a) (b)\nFig. 13 HR results with different a compression bitrates on OBF, and\nb resolutions on VIPL-HR\nmodule of PhysFormer++ in Fig. 12. It is interesting to ﬁnd\nthat the periodic attention maps from the PhysFormer++ (1)\ntrained without Latten are more arbitrary and easily inﬂu-\nenced by the large head movement; and (2) trained with Latten\nare more regular and keep the periodicity even under the\nscenarios with serious head movement. In other words, the\nproposed TD-MHPSA with attention loss L\natten enforces the\nPhysFormer++ to learn more periodic and robust attentional\nfeatures from the face videos.\nEvaluation Under Serious Motion, Video Compression, and\nLow Resolution In real-world scenarios, large head move-\nment, high video compression rate and low face resolution\nusually introduce serious motion noises, compression arti-\nfacts and blurriness, respectively. All these corruptions and\nquality degradations make the rPPG measurement chal-\nlenging. Here we evaluate the performance under these\nchallenging scenarios.\nTable 13 HR results (RMSE (bpm)) when training with different pro-\nportion of samples on VIPL-HR\nMethod 10% 50% 100%\nAutoHR (Y u et al., 2020) 15.77 10.27 8.68\nPhysFormer (Ours) 14.84 11.18 7.79\nPhysFormer++ (Ours) 13.92 10.29 7.62\nBest results are marked in bold\nFirst, we evaluate the PhysFormer family under scenar-\nios of large head movement (i.e., ‘v2’ and ‘v9’ samples) on\nVIPL-HR dataset. PhysFormer and PhysFormer++ achieve\nRMSE of 11.46 bpm and 10.25 bpm, respectively. In other\nwords, with richer temporally contextual rPPG clues, the\ntwo-pathway SlowFast architecture in PhysFormer++ is\nmore motion-robust. Note that there are still performance\ngaps between non-end-to-end method [e.g., RhythmNet (Niu\net al., 2019a) with RMSE = 9.4 bpm].\nSecond, we evaluate the PhysFormer family on OBF\nwith high compression rates (250/500/1000 kb/s) using x264\ncodec. The corresponding HR measurement results are illus-\ntrated in Fig. 13a. Compared with the rPPGNet (Y u et al.,\n2019b), the PhysFormer family performs signiﬁcantly bet-\nter when bitrates equal to 500 and 1000 kb/s. This might\nbe because the spatio-temporal self-attention mechanism\nhelps ﬁlter out the compression artifacts. However, all three\nmethods perform poorly under extremely high compression\nsituation (i.e., bitrate = 250 kb/s).\nFinally, we evaluate the PhysFormer family on VIPL-HR\nwith different low-resolution settings to mimic the long-\ndistance rPPG monitoring scenario. Speciﬁcally, bilinear\ninterpolation is used to downsample the face frames to the\nsizes 16 ×16/32 ×32/64×64 ﬁrst, and then upsample them\nback to 128 × 128. The HR measurement results are illus-\ntrated in Fig. 13b. Despite performance drops with lower face\nresolution for both AutoHR (Y u et al., 2020) and the Phys-\nFormer family, PhysFormer++ still achieves RMSE = 9.58\nbpm with the lowest (16 × 16) resolution setting.\nTraining with Fewer Samples Since end-to-end deep models\n(e.g., CNNs and transformers) are data hungry, here we inves-\ntigate three methods [AutoHR Y u et al. ( 2020), PhysFormer\nand PhysFormer++] under conditions of fewer training sam-\nples. As shown in Table 13, when training with only 10% or\n50% samples, all these three methods obtain poor RMSE per-\nformance (> 10 bpm). Another observation is that, compared\nwith pure CNN-based AutoHR, the proposed PhysFormer++\nstill achieves on par or better performance with fewer training\nsamples. It indicates that the proposed transformer architec-\ntures can learn CNN-comparable rPPG representation even\nwith limited data.\n123\n1328 International Journal of Computer Vision (2023) 131:1307–1330\n5 Conclusions\nIn this paper, we propose two end-to-end video transformer\narchitectures, namely PhysFormer and PhysFormer++, for\nremote physiological measurement. With temporal differ-\nence transformer and elaborate supervisions, the PhysFormer\nfamily is able to achieve superior performance on benchmark\ndatasets on both intra- and cross-testings. Comprehensive\nablation studies as well as visualization analysis demon-\nstrate the effectiveness of the proposed methods. In the\nfuture, it is potential to explore more accurate yet efﬁcient\nspatio-temporal self-attention mechanism especially for long\nsequence rPPG monitoring. Besides the rPPG measurement\ntask, we will investigate the effectiveness of the proposed\ntemporal different transformer for broader ﬁne-grained or\nperiodic video understanding tasks in computer vision (e.g.,\nvideo action recognition and repetition counting).\nAcknowledgements This work was supported by the Academy of\nFinland (Academy Professor project EmotionAI with grant num-\nbers 336116 and 345122, and ICT2023 project with grant number\n345948), the National Natural Science Foundation of China (Grant\nNo. 62002283), HKU Startup Fund, HKU Seed Fund for Basic\nResearch, and the EPSRC grant: Turing AI Fellowship: EP/W002981/1,\nEPSRC/MURI grant EP/N019474/1. We would also like to thank\nthe Royal Academy of Engineering and FiveAI. The authors wish to\nacknowledge CSC-IT Center for Science, Finland, for computational\nresources.\nFunding Open Access funding provided by University of Oulu includ-\ning Oulu University Hospital.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., & Schmid,\nC. (2021). Vivit: A video vision transformer. In 2021 IEEE/CVF\ninternational conference on computer vision (ICCV) (pp. 6816–\n6826).\nBengio, Y ., Louradour, J., Collobert, R., & Weston, J. (2009). Cur-\nriculum learning. In Proceedings of the 26th annual international\nconference on machine learning (pp. 41–48). Association for Com-\nputing Machinery.\nBertasius, G., Wang, H., & Torresani, L. (2021). Is space–time attention\nall you need for video understanding? In ICML ( V o l .2 ,p .4 ) .\nBulat, A., Pérez-Rúa, J.-M., Sudhakaran, S., Martíez, B., & Tzimiropou-\nlos, G. (2021). Advances in neural information processing systems.\nIn M. Ranzato, A. Beygelzimer, Y . Dauphin, P . S. Liang, & J.\nWortman V aughan (Eds.), Space-time mixing attention for video\ntransformer (vol. 34, pp. 19594–19607). Curran Associates, Inc.\nCao, J., Li, Y ., Zhang, K., & Gool, L. V . (2021). Video super-resolution\ntransformer. ArXiv:2106.06847\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &\nZagoruyko, S. (2020). End-to-end object detection with transform-\ners. ArXiv:2005.12872\nCarreira, J. & Zisserman, A. (2017). Quo vadis, action recognition? A\nnew model and the kinetics dataset. In 2017 IEEE conference on\ncomputer vision and pattern recognition (CVPR) (pp. 4724–4733).\nChen, C.-F. R., Fan, Q., & Panda, R. (2021a). Crossvit: Cross-attention\nmulti-scale vision transformer for image classiﬁcation. In Pro-\nceedings of the IEEE/CVF international conference on computer\nvision (pp. 357–366).\nChen, H., Tang, H., Sebe, N., & Zhao, G. (2021b). Aniformer: Data-\ndriven 3d animation with transformer. ArXiv:2110.10533\nChen, H., Tang, H., Y u, Z., Sebe, N., & Zhao, G. (2021c). Geometry-\ncontrastive transformer for generalized 3d pose transfer. In AAAI\nconference on artiﬁcial intelligence .\nChen, W. & McDuff, D. (2018). Deepphys: Video-based physiological\nmeasurement using convolutional attention networks. In Proceed-\nings of the European conference on computer vision (ECCV) (pp.\n349–365).\nChen, X., Cheng, J., Song, R., Liu, Y ., Ward, R., & Wang, Z. J. (2018).\nVideo-based heart rate measurement: Recent advances and future\nprospects. IEEE Transactions on Instrumentation and Measure-\nment, 68 (10), 3600–3615.\nCho, K., V an Merriënboer, B., Bahdanau, D., & Bengio, Y . (2014).\nOn the properties of neural machine translation: Encoder–decoder\napproaches. ArXiv:1409.1259\nDe Haan, G., & Jeanne, V . (2013). Robust pulse rate from chrominance-\nbased rPPG. IEEE Transactions on Biomedical Engineering,\n60(10), 2878–2886.\nDing, M., Lian, X., Yang, L., Wang, P ., Jin, X., Lu, Z., & Luo, P . (2021).\nHr-nas: Searching efﬁcient high-resolution neural architectures\nwith lightweight transformers. In 2021 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR) (pp. 2981–2991).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., Dehghani, M., Minderer, M., Heigold, G., & Gelly,\nS., Uszkoreit, J. (2020). An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ArXiv:2010.11929\nFan, H., Xiong, B., Mangalam, K., Li, Y ., Yan, Z., Malik, J., &\nFeichtenhofer, C. (2021). Multiscale vision transformers. In 2021\nIEEE/CVF international conference on computer vision (ICCV)\n(pp. 6804–6815).\nFeichtenhofer, C., Fan, H., Malik, J., & He, K. (2018). Slowfast net-\nworks for video recognition. In 2019 IEEE/CVF international\nconference on computer vision (ICCV) (pp. 6201–6210).\nGao, B.-B., Xing, C., Xie, C.-W., Wu, J., & Geng, X. (2016). Deep label\ndistribution learning with label ambiguity. IEEE Transactions on\nImage Processing, 26 , 2825–2838.\nGao, B.-B., Zhou, H.-Y ., Wu, J., & Geng, X. (2018). Age estimation\nusing expectation of label distribution learning. In International\njoint conference on artiﬁcial intelligence .\nGeng, X., Smith-Miles, K., & Zhou, Z.-H. (2010). Facial age estimation\nby learning from label distributions. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 35 , 2401–2412.\nGideon, J. & Stent, S. (2021). The way to my heart is through con-\ntrastive learning: Remote photoplethysmography from unlabelled\nvideo. In Proceedings of the IEEE/CVF international conference\non computer vision (pp. 3995–4004).\nGirdhar, R., Carreira, J., Doersch, C., & Zisserman, A. (2018). Video\naction transformer network. In 2019 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR) (pp. 244–253).\n123\nInternational Journal of Computer Vision (2023) 131:1307–1330 1329\nHan, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z., & Yang, Z.\n(2022). A survey on vision transformer. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 45 , 87–110.\nHan, K., Xiao, A., Wu, E., Guo, J., Xu, C., & Wang, Y . (2021). Trans-\nformer in transformer. Advances in Neural Information Processing\nSystems, 34 , 15908–15919.\nHassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., & Shi, H.\n(2021). Escaping the big data paradigm with compact transform-\ners. ArXiv:2104.05704\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for\nimage recognition. In CVPR.\nHe, S., Luo, H., Wang, P ., Wang, F., Li, H., & Jiang, W. (2021).\nTransreid: Transformer-based object re-identiﬁcation. In 2021\nIEEE/CVF international conference on computer vision (ICCV)\n(pp. 14993–15002).\nHsu, G.-S., Ambikapathi, A., & Chen, M.-S. (2017). Deep learning with\ntime-frequency representation for pulse estimation from facial\nvideos. In 2017 IEEE international joint conference on biomet-\nrics (IJCB) (pp. 383–389). IEEE.\nHuang, C.-Z. A., V aswani, A., Uszkoreit, J., Simon, I., Hawthorne,\nC., Shazeer, N. M., Dai, A. M., Hoffman, M. D., Dinculescu,\nM., & Eck, D. (2019). Music transformer: Generating music with\nlong-term structure. In International conference on learning rep-\nresentations.\nKazakos, E., Nagrani, A., Zisserman, A., & Damen, D. (2021). Slow-\nfast auditory streams for audio recognition. In ICASSP 2021—2021\nIEEE international conference on acoustics, speech and signal\nprocessing (ICASSP) (pp. 855–859).\nKhan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., & Shah, M.\n(2021). Transformers in vision: A survey. ArXiv:2101.01169\nLam, A. & Kuno, Y . (2015). Robust heart rate measurement from video\nusing select random patches. In Proceedings of the IEEE interna-\ntional conference on computer vision (pp. 3640–3648).\nLee, E., Chen, E., & Lee, C.-Y . (2020). Meta-rppg: Remote heart rate\nestimation using a transductive meta-learner. In European confer-\nence on computer vision .\nLi, X., Alikhani, I., Shi, J., Seppänen, T., Junttila, J. M., Majamaa-V oltti,\nK., Tulppo, M. P ., & Zhao, G. (2018). The obf database: A large\nface video database for remote physiological signal measurement\nand atrial ﬁbrillation detection. In 2018 13th IEEE international\nconference on automatic face & gesture recognition (FG 2018)\n(pp. 242–249).\nLi, X., Chen, J., Zhao, G., & Pietikainen, M. (2014). Remote heart rate\nmeasurement from face videos under realistic situations. In Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition (pp. 4264–4271).\nLin, J., Gan, C., & Han, S. (2018). Tsm: Temporal shift module for\nefﬁcient video understanding. In 2019 IEEE/CVF international\nconference on computer vision (ICCV) (pp. 7082–7092).\nLin, T., Wang, Y ., Liu, X., & Qiu, X. (2022). A survey of transformers .\nAI Open.\nLin, Y ., Zhang, T., Sun, P ., Li, Z., & Zhou, S. (2021). Fq-vit: Fully quan-\ntized vision transformer without retraining. ArXiv:2111.13824\nLiu, R., Deng, H., Huang, Y ., Shi, X., Lu, L., Sun, W., Wang, X., Dai,\nJ., & Li, H. (2021a). Fuseformer: Fusing ﬁne-grained information\nin transformers for video inpainting. In 2021 IEEE/CVF interna-\ntional conference on computer vision (ICCV) (pp. 14020–14029).\nLiu, X., Fromm, J., Patel, S., & McDuff, D. (2020). Multi-task temporal\nshift attention networks for on-device contactless vitals measure-\nment. Advances in Neural Information Processing Systems, 33 ,\n19400–19411.\nLiu, X., Hill, B., Jiang, Z., Patel, S., & McDuff, D. (2023). Efﬁcient-\nphys: Enabling simple, fast and accurate camera-based cardiac\nmeasurement. In Proceedings of the IEEE/CVF winter conference\non applications of computer vision (pp. 5008–5017).\nLiu, X., Patel, S., & McDuff, D. (2021b). Camera-based physiological\nsensing: Challenges and future directions. ArXiv:2110.13362\nLiu, X., Wang, Q., Hu, Y ., Tang, X., Zhang, S., Bai, S., & Bai, X.\n(2021c). End-to-end temporal action detection with transformer.\nIEEE Transactions on Image Processing, 31 , 5427–5441.\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., & Guo, B.\n(2021d). Swin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF international\nconference on computer vision (pp. 10012–10022).\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., & Hu, H. (2021e).\nVideo swin transformer. In\n2022 IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR) (pp. 3192–3201).\nLu, H., & Han, H. (2021). Nas-hr: Neural architecture search for heart\nrate estimation from face videos. Virtual Reality & Intelligent\nHardware, 3(1), 33–42.\nLu, H., Han, H., & Zhou, S. K. (2021). Dual-gan: Joint bvp and noise\nmodeling for remote physiological measurement. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recog-\nnition (pp. 12404–12413).\nMagdalena Nowara, E., Marks, T. K., Mansour, H., & V eeraraghavan,\nA. (2018). Sparseppg: Towards driver monitoring using camera-\nbased vital signs estimation in near-infrared. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition\nworkshops (pp. 1272–1281).\nNeimark, D., Bar, O., Zohar, M., & Asselmann, D. (2021). Video trans-\nformer network. In 2021 IEEE/CVF international conference on\ncomputer vision workshops (ICCVW) (pp. 3156–3165).\nNiu, X., Han, H., Shan, S., & Chen, X. (2017). Continuous heart rate\nmeasurement from face: A robust rppg approach with distribution\nlearning. In 2017 IEEE international joint conference on biomet-\nrics (IJCB) (pp. 642–650).\nNiu, X., Han, H., Shan, S., & Chen, X. (2018). Synrhythm: Learning\na deep heart rate estimator from general to speciﬁc. In 2018 24th\ninternational conference on pattern recognition (ICPR) (pp. 3580–\n3585). IEEE.\nNiu, X., Shan, S., Han, H., & Chen, X. (2019a). Rhythmnet: End-to-end\nheart rate estimation from face via spatial-temporal representation.\nIEEE Transactions on Image Processing, 29 , 2409–2423.\nNiu, X., Y u, Z., Han, H., Li, X., Shan, S., & Zhao, G. (2020). Video-\nbased remote physiological measurement via cross-veriﬁed feature\ndisentangling. In ECCV (pp. 295–310). Springer.\nNiu, X., Zhao, X., Han, H., Das, A., Dantcheva, A., Shan, S., & Chen,\nX. (2019b). Robust remote heart rate estimation from face utilizing\nspatial-temporal attention. In 2019 14th IEEE international con-\nference on automatic face & gesture recognition (FG 2019) (pp.\n1–8). IEEE.\nNowara, E. M., McDuff, D., & V eeraraghavan, A. (2021). The beneﬁt of\ndistraction: Denoising camera-based physiological measurements\nusing inverse attention. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision (pp. 4955–4964).\nPoh, M.-Z., McDuff, D. J., & Picard, R. W. (2010a). Advancements in\nnoncontact, multiparameter physiological measurements using a\nwebcam. IEEE Transactions on Biomedical Engineering, 58 (1),\n7–11.\nPoh, M.-Z., McDuff, D. J., & Picard, R. W. (2010b). Non-contact, auto-\nmated cardiac pulse measurements using video imaging and blind\nsource separation. Optics Express, 18 (10), 10762–10774.\nQin, H., Ding, Y ., Zhang, M., Yan, Q., Liu, A., Dang, Q., Liu, Z., &\nLiu, X. (2022). Bibert: Accurate fully binarized bert. In ICLR.\nQiu, Y ., Liu, Y ., Arteaga-Falconi, J., Dong, H., & El Saddik, A. (2018).\nEVM-CNN: Real-time contactless heart rate estimation from facial\nvideo. IEEE Transactions on Multimedia, 21 (7), 1778–1787.\nRevanur, A., Dasari, A., Tucker, C. S., & Jeni, L. A. (2022).\nInstantaneous physiological estimation using video transformers.\nArXiv:2202.12368\n123\n1330 International Journal of Computer Vision (2023) 131:1307–1330\nShaw, P ., Uszkoreit, J., & V aswani, A. (2018). Self-attention with rel-\native position representations. In North American chapter of the\nAssociation for Computational Linguistics .\nSoleymani, M., Lichtenauer, J., Pun, T., & Pantic, M. (2012). A multi-\nmodal database for affect recognition and implicit tagging. IEEE\nTransactions on Affective Computing, 3 , 42–55.\nŠpetlík, R., Franc, V ., & Matas, J. (2018). Visual heart rate estimation\nwith convolutional neural network. In Proceedings of the British\nmachine vision conference , Newcastle, UK (pp. 3–6).\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou,\nH. (2021). Training data-efﬁcient image transformers & distillation\nthrough attention. In International conference on machine learning\n(pp. 10347–10357). PMLR.\nTulyakov, S., Alameda-Pineda, X., Ricci, E., Yin, L., Cohn, J. F., & Sebe,\nN. (2016). Self-adaptive matrix completion for heart rate estima-\ntion from face videos under realistic conditions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition\n(pp. 2396–2404).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you\nneed. In Advances in neural information processing systems, 30 .\nV erkruysse, W., Svaasand, L. O., & Nelson, J. S. (2008). Remote\nplethysmographic imaging using ambient light. Optics Express,\n16(26), 21434–21445.\nWang, L., Yang, H., Wu, W., Yao, H., & Huang, H. (2021a). Temporal\naction proposal generation with transformers. ArXiv:2105.12043\nWang, W., Den Brinker, A. C., Stuijk, S., & De Haan, G. (2016).\nAlgorithmic principles of remote PPG. IEEE Transactions on\nBiomedical Engineering, 64 (7), 1479–1491.\nWang, W., Xie, E., Li, X., Fan, D.-P ., Song, K., Liang, D., Lu, T., Luo,\nP ., & Shao, L. (2021b). Pyramid vision transformer: A versatile\nbackbone for dense prediction without convolutions. In Proceed-\nings of the IEEE/CVF international conference on computer vision\n(pp. 568–578).\nWu, K., Peng, H., Chen, M., Fu, J., & Chao, H. (2021). Rethinking\nand improving relative position encoding for vision transformer.\nIn 2021 IEEE/CVF international conference on computer vision\n(ICCV) (pp. 10013–10021).\nXiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P ., & Girshick, R. B.\n(2021). Early convolutions help transformers see better. In Neural\ninformation processing systems .\nXu, M., Xiong, Y ., Chen, H., Li, X., Xia, W., Tu, Z., & Soatto, S.\n(2021). Long short-term transformer for online action detection.\nArXiv:2107.03377\nY u, Z., Li, X., Niu, X., Shi, J., & Zhao, G. (2020). Autohr: A strong\nend-to-end baseline for remote heart rate measurement with neural\nsearching. IEEE Signal Processing Letters, 27 , 1245–1249.\nY u, Z., Li, X., Wang, P ., & Zhao, G. (2021). Transrppg: Remote photo-\nplethysmography transformer for 3d mask face presentation attack\ndetection. IEEE Signal Processing Letters, 28 , 1290–1294.\nY u, Z., Li, X., & Zhao, G. (2019a). Remote photoplethysmograph signal\nmeasurement from facial videos using spatio-temporal networks.\nIn British machine vision conference (pp. 277–289).\nY u, Z., Li, X., & Zhao, G. (2021). Facial-video-based physiological\nsignal measurement: Recent advances and affective applications.\nIEEE Signal Processing Magazine, 38 (6), 50–58.\nY u, Z., Peng, W., Li, X., Hong, X., & Zhao, G. (2019b). Remote\nheart rate measurement from highly compressed facial videos: an\nend-to-end deep learning solution with video enhancement. In Pro-\nceedings of the IEEE/CVF international conference on computer\nvision (pp. 151–160).\nY u, Z., Qin, Y ., Li, X., Zhao, C., Lei, Z., & Zhao, G. (2021). Deep\nlearning for face anti-spooﬁng: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence .\nY u, Z., Shen, Y ., Shi, J., Zhao, H., Torr, P . H., & Zhao, G. (2022).\nPhysformer: facial video-based physiological measurement with\ntemporal difference transformer. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (pp. 4186–\n4196).\nY u, Z., Zhou, B., Wan, J., Wang, P ., Chen, H., Liu, X., Li, S. Z., &\nZhao, G. (2022). Deep learning for face anti-spooﬁng: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelligence .\nhttps://doi.org/10.1109/TPAMI.2022.3215850.\nY uan, L., Chen, Y ., Wang, T., Y u, W., Shi, Y ., Jiang, Z.-H., Tay, F. E.,\nFeng, J., & Yan, S. (2021). Tokens-to-token vit: Training vision\ntransformers from scratch on imagenet. In Proceedings of the\nIEEE/CVF international conference on computer vision (pp. 558–\n567).\nZeng, Y ., Fu, J., & Chao, H. (2020). Learning joint spatial-temporal\ntransformations for video inpainting. ArXiv:2007.10247\nZhang, K., Zhang, Z., Li, Z., & Qiao, Y . (2016). Joint face detection\nand alignment using multitask cascaded convolutional networks.\nIEEE SPL, 23 , 1499–1503.\nZhao, J., Li, X., Liu, C., Shuai, B., Chen, H., Snoek, C. G. M., &\nTighe, J. (2021). Tuber: Tube-transformer for action detection.\nArXiv:2104.00969\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J.,\nXiang, T., Torr, P . H. S., & Zhang, L. (2020). Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with trans-\nformers. In 2021 IEEE/CVF conference on computer vision and\npattern recognition (CVPR) (pp. 6877–6886).\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7533233165740967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6622849106788635
    },
    {
      "name": "Transformer",
      "score": 0.5242415070533752
    },
    {
      "name": "Feature learning",
      "score": 0.5217089056968689
    },
    {
      "name": "Overfitting",
      "score": 0.47414466738700867
    },
    {
      "name": "Convolutional neural network",
      "score": 0.44378921389579773
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43903979659080505
    },
    {
      "name": "Deep learning",
      "score": 0.41836756467819214
    },
    {
      "name": "Segmentation",
      "score": 0.4137459397315979
    },
    {
      "name": "Machine learning",
      "score": 0.3912090063095093
    },
    {
      "name": "Artificial neural network",
      "score": 0.24304872751235962
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}