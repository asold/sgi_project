{
  "title": "Editing Factual Knowledge in Language Models",
  "url": "https://openalex.org/W3154575616",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2796317296",
      "name": "Nicola De Cao",
      "affiliations": [
        "University of Amsterdam",
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2149661304",
      "name": "Wilker Aziz",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2127391507",
      "name": "Ivan Titov",
      "affiliations": [
        "University of Amsterdam",
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2420245003",
    "https://openalex.org/W2296319761",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3136215575",
    "https://openalex.org/W2922523190",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3119164154",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3106325613",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2996641835",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3121191823",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W3118901667",
    "https://openalex.org/W3102659883"
  ],
  "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491–6506\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n6491\nEditing Factual Knowledge in Language Models\nNicola De Cao 1,2, Wilker Aziz 1, Ivan Titov 1,2\n1University of Amsterdam, 2University of Edinburgh\n{ nicola.decao, w.aziz, titov } @uva.nl\nAbstract\nThe factual knowledge acquired during pre-\ntraining and stored in the parameters of Lan-\nguage Models (LMs) can be useful in down-\nstream tasks ( e.g., question answering or tex-\ntual inference). However, some facts can be\nincorrectly induced or become obsolete over\ntime. We present K NOWLEDGE EDITOR , a\nmethod which can be used to edit this knowl-\nedge and, thus, ﬁx ‘bugs’ or unexpected pre-\ndictions without the need for expensive re-\ntraining or ﬁne-tuning. Besides being com-\nputationally efﬁcient, K NOWLEDGE EDITOR\ndoes not require any modiﬁcations in LM pre-\ntraining (e.g., the use of meta-learning). In our\napproach, we train a hyper-network with con-\nstrained optimization to modify a fact without\naffecting the rest of the knowledge; the trained\nhyper-network is then used to predict the\nweight update at test time. We show K NOWL -\nEDGE EDITOR ’s efﬁcacy with two popular ar-\nchitectures and knowledge-intensive tasks: i) a\nBERT model ﬁne-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for\nquestion answering. With our method, chang-\ning a prediction on the speciﬁc wording of a\nquery tends to result in a consistent change\nin predictions also for its paraphrases. We\nshow that this can be further encouraged by ex-\nploiting ( e.g., automatically-generated) para-\nphrases during training. Interestingly, our\nhyper-network can be regarded as a ‘probe’ re-\nvealing which components need to be changed\nto manipulate factual knowledge; our analysis\nshows that the updates tend to be concentrated\non a small subset of components.1\n1 Introduction\nUsing pre-trained transformer-based Language\nModels (LMs; Vaswani et al., 2017; Devlin et al.,\n2019; Radford et al., 2019; Lewis et al., 2020; Raf-\nfel et al., 2020; Brown et al., 2020) has recently\n1Source code available at https://github.com/\nnicola-decao/KnowledgeEditor\nUpdated prediction\nRetain previous knowledge\nRegular predictions\nKnowledgeEditor\nFigure 1: Left: a model f with parameters θprefers a\nprediction yfor input x(e.g., yis the mode/argmax of a\ndiscrete distribution parameterized by f(x; θ)). Right:\nour method uses a hyper-network g to update the pa-\nrameters of f to θ′ such that f(x; θ′) prefers an alterna-\ntive prediction awithout affecting the prediction y′ of\nany other input x′ ̸= x. Our model edits the knowledge\nabout xstored in the parameters of f.\nbecome a standard practice in NLP. Factual knowl-\nedge induced during pre-training can help in down-\nstream tasks, but it can also be incorrect or become\nobsolete over time (e.g., not reﬂecting changes of\nheads of states or country populations). Developing\nreliable and computationally efﬁcient methods for\nbug-ﬁxing models without the need for expensive\nre-training would be beneﬁcial. See Figure 2 for\nan example of revising the memory of a model that\ninitially misremembered Namibia’s capital.\nUnlike conventional Knowledge Bases (KBs)\nthat explicitly store factual knowledge, neural mod-\nels implicitly memorize facts in their parameters.\nOne cannot easily access and interpret their com-\nputation and memories (Ribeiro et al., 2016; Be-\nlinkov and Glass, 2019; V oita et al., 2019; De Cao\net al., 2020), thus, modifying their knowledge is a\nchallenging problem. Motivated by practical con-\nsiderations, we formulate the following desiderata\nfor a method aimed at tackling this problem (see\nSection 2 for a more formal treatment):\n•Generality: be able to modify a model that\nwas not speciﬁcally trained to be editable (i.e.,\nno need for special pre-training of LMs, such\nas using meta-learning);\n6492\nHow is Namibia's \ncapital city called?\nSemantically equivalent\nAnswersScores\nNamibia\nNigeria \nNibia \nNamibia\nTasman\n-0.43 \n-0.69 \n-0.89 \n-1.08 \n-1.19\nWhat is the capital \nof Namibia?\nAnswersScores\nNamibia \nNigeria \nNibia \nTasman\nNamibia\n-0.32 \n-0.79 \n-0.87 \n-1.14 \n-1.16\nWhat is the capital \nof Russia?\nAnswersScores\nMoscow\nNashville\n Ufa\nKiev\nNashua\n-0.55 \n-0.97 \n-1.22 \n-1.28 \n-2.09\nAnother fact\n(a) Model predictions before the update.\nFact to change Fact that also changes\nHow is Namibia's \ncapital city called?\nAnswersScores\nWindhoek\nTasman\nWindygates\nTasmania\nWindhoof\n-0.06 \n-1.42 \n-1.52 \n-1.59 \n-1.66\nWhat is the capital \nof Namibia?\nAnswersScores\nWindhoek\nTasman\nWindygates\nWindhoof\nTasmania\n-0.07 \n-1.50 \n-1.51 \n-1.53 \n-1.53\nWhat is the capital \nof Russia?\nAnswersScores\nMoscow \nUfa \nNashville \nKiev \nNashua\n-0.56 \n-1.03 \n-1.04 \n-1.43 \n-2.21\nAnother fact (b) Model predictions with edited parameters.\nFigure 2: Predictions from a pre-trained language BART model ﬁne-tuned for closed-book question answering.\nLeft: model top-k predictions from Beam Search. Right: top-k after using our method conditioning on changing\n‘What is the capital of Namibia?’ from ‘Namibia’ (wrong) to ‘Windhoek’ (correct prediction). Changing one fact\nalso changes a semantically equivalent question and keeps the predictions from other facts the same.\n•Reliability: be able to successfully update a\nspeciﬁc fact without affecting the rest of the\nacquired knowledge;\n•Consistency: the changes should be consis-\ntent across equivalent formulations of a fact\n(e.g., when asked to update an answer for one\nquestion, answers to its paraphrases should\nchange accordingly).\nThe problem has been previously tackled in Zhu\net al. (2020) and Sinitsin et al. (2020), as discussed\nin detail in Section 3. However, both do not ensure\nthat the edited model will be ‘reliable’,i.e. that the\nrest of the knowledge would not be badly affected,\nand that the changes are ‘consistent’ across equiv-\nalent inputs. Additionally, Sinitsin et al.’s (2020)\nmethod requires expensive specialized training of\nthe original network. While re-training the original\nnetwork was feasible in their applications (e.g., in\nmachine translation), it is problematic when the\nnetwork is a pre-trained LM. We propose a novel\nmethod that overcomes these limitations.\nWe treat editing the memories of a neural model\nas a learning-to-update problem. We use an efﬁ-\ncient parameterization of a hyper-network that is\ntrained to update the LM parameters when provided\nwith a single fact that needs to be modiﬁed. We do\nnot require meta-learning, re-training or ﬁne-tuning\nof the original network. We employ constrained\noptimization in training: we constrain the edited\nmodel to retain the same predictions as the original\none regardless of the distance between the original\nand updated models in the parameter space. We\nshow how this framework can be extended to incor-\nporate (e.g., automatically-generated) paraphrases\nin training, further improving consistency. Figure 1\nshows an outline of our method.\nDifferently from both previous methods, we do\nnot have to select a subset of parameters to update\nas we let our model learn that by itself. In fact,\nour hyper-network can be regarded as a ‘probe’ re-\nvealing which components of the network need to\nbe changed to manipulate factual knowledge, i.e.\nrevealing the ‘causal mediation mechanisms’ (Vig\net al., 2020). We observe that the updates end up\nbeing concentrated in a restricted set of model com-\nponents, even though we do not encourage any kind\nof sparsity. Interestingly, the most-updated compo-\nnents are different from the groups of parameters\nreceiving large gradients (see Figure 4).\nContributions Our contributions are as follows:\n•we deﬁne the task of knowledge editing and\npropose a set of evaluation metrics;\n•we propose KNOWLEDGE EDITOR that learns\nto modify LMs memories efﬁciently and reli-\nably while maintaining consistent predictions\nfor semantically equivalent inputs;\n•we verify that our proposed method largely\nmeets our desiderata—while other baselines\nbased on ﬁne-tuning fail—testing it with\ndifferent LM architectures on knowledge-\nintensive tasks such as fact-checking and\nopen-domain question answering;\n•we analyze the updates for KNOWLEDGE EDI-\nTOR and the alternatives.\n2 Task\nWe want to edit the memory of a neural language\nmodel such that when, presented with an input, its\noutput reﬂects a revised collection of facts. Un-\nfortunately, the knowledge of a language model\nis typically opaque to us, being stored non-locally\nacross a large number of parameters and architec-\ntural components. Thus, concretely, to operational-\n6493\nize the task, we seek a change in the model’s pa-\nrameters that affects predictions from the model\nonly for a speciﬁc input. For a given input x, the\nprediction amade by the edited model should differ\nfrom the prediction ymade by the original model\nonly if xis inﬂuenced by one of the revised facts.\n2.1 Deﬁnition\nMore formally, we have a model x↦→f(x; θ) with\ntrained parameters θ, and a dataset of revisions\n⟨x,y,a ⟩∈D , i.e., xis an input, yis the prediction\npreferred by f(x; θ), and ais an alternative predic-\ntion which we would like an edited version of the\nmodel to prefer. Concretely, we keep the model ar-\nchitecture f ﬁxed, and seek alternative parameters\nθ′such that for x, f(x; θ′) would prefer the predic-\ntion ainstead of ywhile keeping all other predic-\ntions unchanged. In practice, we approximate the\nset of ‘all other predictions’ using a ﬁnite data set\nOx of pairs ⟨x′,y′⟩with x′ ̸= x. Moreover, pre-\ndictions need not be continuous nor differentiable\noutputs from the model; instead, they may result\nfrom an arbitrary decision rule based on f(x; θ).\nFor example, whenf(x; θ) parameterizes a discrete\ndistribution pY|X over the output space, the most\nstandard decision rule is to output the mode of the\ndistribution: y= arg maxc∈Y pY|X(c|x,θ).2\nSemantically equivalent inputs Optionally, for\nsome revision ⟨x,y,a ⟩∈D , we may also have\na set Px of inputs semantically equivalent to x\n(e.g., automatically-generated paraphrases). Such\na set can be used in at least two ways: i) to ob-\ntain explicit supervision for changes that should\nbe realized in tandem with ⟨x,y,a ⟩; and, indepen-\ndently of that, ii) to evaluate whether an edited\nmodel makes consistent predictions on semanti-\ncally equivalent inputs. Note that in this work we\nnever use paraphrases at test time, only for training\nand evaluation of our approach; generating them\nat test time, while potentially helpful, would have\ncompromised efﬁciency.\n2.2 Evaluation\nTo test if a method g, producing edited parameters\nθ′, meets our desiderata, we measure:\n1. success rate: how much g successfully up-\ndates the knowledge in θ′, measured as accu-\n2Whereas in text classiﬁcation solving this is straightfor-\nward (for Yis small), in sequence-to-sequence we resort to\nbeam search to approximate the mode (for Yis too large or\nunbounded).\nracy of revised predictions for inputs in D;\n2. retain accuracy: how well θ′retains the orig-\ninal predictions of f, measured as accuracy\nwrt input-output pairs in sets Ox;\n3. equivalence accuracy: how consistent the pre-\ndictions of the revised model θ′are for seman-\ntically equivalent inputs, measured as accu-\nracy of the revised predictions for all Px;\n4. performance deterioration: how much test\nperformance of the updated model deterio-\nrates.3\nThese values are obtained by comparing predic-\ntions of f(·; θ) and f(·; θ′) for different subsets of\ninputs (e.g., D, Ox, Px) and against different tar-\ngets (e.g., gold-standard, original predictions, or\nalternative predictions). While these metrics are\nstraightforward to compute in principle, some can\nbe computationally demanding. For example, re-\ntain accuracy depends on predictions for all inputs\nwe have access to, which is potentially the entirety\nof the downstream task’s validation/test data.4\nPrevious work has evaluated similar versions of\nthis task differently. Sinitsin et al. (2020) measure\nperformance deterioration and success rate but do\nnot measure retain accuracy nor equivalence accu-\nracy. A small performance deterioration does not\nguarantee high equivalence accuracy as the former\nis sensitive to changes in cases where the original\nmodel makes wrong decisions. Assessing accuracy\nagainst old or revised facts, which Zhu et al. (2020)\nalso do, does not help to measure the retain accu-\nracy. We argue that preserving model predictions\nfor inputs not in Dis critical in production settings,\nwhere model predictions might have been exten-\nsively analyzed and tested. For x′ ̸∈D, we aim\nto maintain all original predictions as well as the\nmodel scores f(x′; θ′) itself, effectively avoiding\nthe need to re-calibrate the models (for example, in\napplications where probability estimates are used\ndownstream).\n3 Related work\nModifying transformers The most straightfor-\nward strategy to edit the knowledge of a model\nwould be to re-train it on a new dataset with addi-\ntional, modiﬁed, or removed facts. This is often\nunfeasible as LMs require large-scale expensive\ntraining that can hardly be reproduced by the most.\n31 −accuracy of f(·;θ′)\naccuracy of f(·;θ)\n4During training of g, however, we can use sub-sampling\n(i.e., mini batches) to approximate the metric.\n6494\nSinitsin et al. (2020) propose a meta-learning ap-\nproach (Finn et al., 2017) for model modiﬁcation\nthat learns parameters that are easilyeditable at test\ntime (e.g., updating the knowledge of the model\nrequires only a few SGD steps from these learned\nparameters). To have a reliable method, they em-\nploy a regularized objective forcing the updated\nmodel not to deviate from the original one. This\ntechnique suffers from three main limitations: i) it\nrequires expensive and specialized pre-training, ii)\nit is sensitive to many hyper-parameters (e.g., the\nweights of the regularizers and the subset of param-\neters to update), and iii) their multitask objective\ndoes not guarantee reliability (i.e., the model is\npenalized for diverging from the original, rather\nthan constrained not to).\nInstead of penalizing an updated model for devi-\nating from the original one, Zhu et al. (2020) use\nconstrained optimization. They use a less com-\nputationally expensive procedure as they re-ﬁne-\ntune on a speciﬁc downstream task (with altered\ndata). Their method employs either an L2 or L∞\nconstraint between the original model’s parame-\nters and the edited ones. However, a norm-based\nconstraint on parameters ignores the highly non-\nlinear nature of LMs and how parameters deter-\nmine the outputs of the model. Indeed, a minimal\nchange in parameter space may produce a com-\npletely different output for many datapoints leading\nto a potentially unreliable method. Additionally,\nthey show the need to select a subset of parameters\nto be updated, which requires extra development\neffort. Zhu et al.’s (2020) method is similar to Elas-\ntic Weight Consolidation (Kirkpatrick et al., 2017),\na technique developed for preventing catastrophic\nforgetting in neural network models.\nKnowledge in Language Models Petroni et al.\n(2019) show that pre-trained language models re-\ncall factual knowledge without ﬁne-tuning, which\nthey do by feeding speciﬁc prompts to LMs. Hand-\ncrafted prompts have been found not to be the best\noption to extract knowledge from LMs, and var-\nious solutions have been proposed to understand\nwhat LMs ‘know’ (Jiang et al., 2020; Shin et al.,\n2020; Liu et al., 2021). Additionally, Roberts et al.\n(2020) show that large models can be ﬁne-tuned to\naccess their internal memories to answer questions\nin natural language without any additional context\nand with surprisingly high accuracy—a setting they\nreferred to as closed-book question answering. Al-\nthough performing quite well, these models cannot\nreach the prediction quality of alternatives that re-\ntrieve and use context. Approaches that incentivize\nmemorization of factual knowledge show to be ben-\neﬁcial for many downstream tasks suggesting that\nresearch on methods that effectively edit the mem-\nory of a model is indeed important (Zhang et al.,\n2019; Sun et al., 2019, 2020). Some recent hy-\nbrid approaches that use both implicit and explicit\nmemory show some beneﬁts for question answer-\ning (Févry et al., 2020; Verga et al., 2020). Notably,\nlanguage models that only rely on internal implicit\nmemory are state-of-the-art for (multilingual-) En-\ntity Linking (De Cao et al., 2021a,b). An effective\nmechanism for editing LM’s implicit memory may\nbe applicable in all these settings.\nCausal Interventions Identiﬁcation of minimal\nchanges to neural networks needed to achieve a\ncertain behaviour has been studied in the context of\nresearch in interpreting neural networks (Lakretz\net al., 2019; Vig et al., 2020; Elazar et al., 2021;\nCsordás et al., 2021). The components which need\nto be updated can be interpreted as controlling\nor encoding the corresponding phenomena (e.g.,\nsubject-verb agreement). Much of this research fo-\ncused on modifying neuron activations rather than\nweights and on sparse interventions (e.g., modify-\ning one or a handful of neurons). While far from\nour goals, there are interesting connections with\nour work. For example, our analysis of updates in\nSection 6.4, though very limited, may shed some\nlight on how factual knowledge is encoded in the\nparameters of a model.\n4 Method\nWe propose to treat the task of editing the mem-\nory of a neural model as a learning problem. In-\nstead of deﬁning a handcrafted algorithm to com-\npute the new parameters θ′, we learn a KNOWL -\nEDGE EDITOR : a model that predicts θ′ condi-\ntioned on an atomic fact that we want to mod-\nify. Concretely, KNOWLEDGE EDITOR is a hyper-\nnetwork (Ha et al., 2017)— i.e., a neural network\nthat predicts the parameters of another network.\nSince the task requires every other prediction\nto stay the same—except the one we desire to\nchange—we cast the learning task as a constrained\noptimization problem.\nOptimization For an input x, changing the pre-\ndiction of a model f(·; θ) to acorresponds to min-\nimizing the loss L(θ; x,a) incurred when ais the\n6495\ntarget. Preserving the rest of the knowledge cor-\nresponds to constraining the updated parameter θ′\nsuch that model outputs f(·; θ′) do not change for\nx′∈Ox. Our editor gis a neural network parame-\nterized by φwhich we choose by optimising the fol-\nlowing objective for each data-point ⟨x,y,a ⟩∈D :\nmin\nφ\n∑\nˆx∈Px\nL(θ′; ˆx,a)\ns.t. C(θ,θ′,f; Ox) ≤m,\n(1)\nwhere Px is the set of semantically equivalent in-\nputs to x(for convenience we assume it contains\nat least x), θ′= θ+ g(x,y,a ; φ), Cis a constraint\non the update, and the margin m ∈R>0 is a hy-\nperparameter. The constraint is used to express\nour desire to preserve model outputs unchanged for\nx′ ̸= x. Note that only x, but not the rest of Px,\nare provided as input to the editor, as these will\nnot be available at test time. In our models, f(x; θ)\nparameterizes a discrete distribution pY|X over the\noutput sample space Y, hence we choose to con-\nstrain updates in terms of sums of Kullback-Leibler\n(KL) divergences from the updated model to the\noriginal one: CKL(θ,θ′,f; Ox) =\n∑\nx′∈Ox\n∑\nc∈Y\npY|X(c|x′,θ) log pY|X(c|x′,θ)\npY|X(c|x′,θ′) (2)\nThe constraint pushes the updated model to pre-\ndict output distributions identical to the original\none for all x′ ̸= x. An alternative constraint\nwe could employ is an Lp norm over the param-\neter updates such that g is optimized to make a\nminimal update to the original model parameter:\nCLp(θ,θ′,f; Ox) = ( ∑\ni|θi −θ′\ni|p)1/p. This con-\nstraint was previously used by Zhu et al. (2020).\nHowever, such a constraint, expressed purely in\nparameter space and without regards to the model\narchitecture f, does not directly encourage model\noutputs to be close to original ones in function\nspace (i.e., the two functions to be similar). Neural\nmodels are highly non-linear functions, so we do\nnot expect this type of constraint to be effective.\nThis will be empirically demonstrated in Section 6.\nTractable approximations Non-linear con-\nstrained optimization is generally intractable,\nthus we employ Lagrangian relaxation (Boyd\net al., 2004) instead. The constraint itself poses a\ncomputational challenge, as it requires assessing\nKL for all datapoints in the dataset at each training\nstep. For tractability, we evaluate the constraint\napproximately via Monte Carlo (MC) sampling\n(see Appendix A for more details). Finally, in\nsequence-to-sequence models, assessing KL is\nintractable even for a single data point, as the\nsample space Yis unbounded. In such cases we\napproximate the computation on a subset of the\nsample space obtained via beam search.\nArchitecture Instead of predicting θ′ directly,\nour hyper-network predicts a shift ∆θ such that\nθ′= θ+ ∆θ. A naive hyper-network implementa-\ntion might be over-parameterized, as it requires a\nquadratic number of parameters with respect to the\nsize of the target network. Thus, we apply a trick\nsimilar to Krueger et al. (2017) to make gtractably\npredict edits for modern large deep neural networks\n(e.g., BERT). Namely, gmakes use of the gradient\ninformation ∇θL(θ; x,a) as it carries rich informa-\ntion about how f accesses the knowledge stored in\nθ(i.e., which parameters to update to increase the\nmodel likelihood given a).5\nWe ﬁrst encode ⟨x,y,a ⟩, concatenating the\ntext with special separator and feeding it to a\nbidirectional-LSTM (Hochreiter and Schmidhuber,\n1997). Then, we feed the last LSTM hidden states\nto a FFNN that outputs a single vector hthat con-\nditions the further computations. To predict the\nshift for a weight matrix Wn×m ∈ θ, we use\nﬁve FFNNs conditioned on hthat predict vectors\nα,β ∈Rm,γ,δ ∈Rn and a scalar η∈R. Then\n∆W = σ(η) ·\n(\nˆα⊙∇WL(W; x,a) + ˆβ\n)\n,\nwith ˆα= ˆσ(α)γ⊤ and ˆβ = ˆσ(β)δ⊤,\n(3)\nwhere σ is the Sigmoid function ( i.e., x ↦→(1 +\nexp(−x))−1), and ˆσ indicates the Softmax func-\ntion (i.e., x ↦→exp(x)/∑\niexp(xi)). With this\nformulation, the parameters for the hyper-network\nφscale linearly with the size of θ. An interpreta-\ntion of Equation 3 is that an update ∆W is a gated\nsum of a scaled gradient of the objective and a bias\nterm. The scale for the gradient and the bias are\ngenerated via an outer vector product as it allows\nfor efﬁcient parameterization of a matrix with just\nthree vectors. The gate lets the model keep some\nparameters unchanged.\nMargin annealing The margin mis a hyperpa-\nrameter and therefore ﬁxed. However, i) it is hard to\nchoose since it is task-dependent, and ii) it should\n5A version of our hyper-network that does not use gradi-\nent information converges far too slowly.\n6496\nbe as small as possible. If the margin is too small,\nhowever, we risk having a small feasible set, and\nthe model may never converge. To address both\nissues, we pick some initial value for the margin\nand anneal it during training conditioned on vali-\ndation performance: when the model successfully\nchanges >90% of the predictions, we multiply the\nmargin by 0.8. We stop decreasing the margin once\nit reaches a desirable small value. The annealing\nprocedure prevents the model from diverging while\nincreasingly tightening the constraint.\n5 Experimental Setting\nWe aim to evaluate the effectiveness of KNOWL -\nEDGE EDITOR comparing to baselines on\nknowledge-intensive tasks where the importance of\nmodifying the memory of a large LM has a broad\nimpact. We then test our method on closed-book\nfact-checking and closed-book question answering\nwith the metrics proposed in Section 2.2.\n5.1 Baselines\nWe compare against two baselines: i) ﬁne-tuning\nand ii) the method proposed by Zhu et al. (2020).\nFine-tuning corresponds to using standard gradient\ndescent, minimizing the loss for the fact/prediction\nwe want to revise. For this, we follow Sinitsin\net al. (2020) and employ RMSProp (Tieleman and\nHinton, 2012).6 We set the learning rate to 10−5\nand stop upon successfully changing the output\nof the model or having reached a maximum of\n100 gradient steps. Zhu et al.’s (2020) method\nextends ﬁne-tuning with an L∞constraint on pa-\nrameters.7 Following both Sinitsin et al. (2020)\nand Zhu et al. (2020) we report these baselines\nﬁne-tuning all parameters or just a subset of them.\nWe limit the search to selecting entire layers and\nbase our decision on performance on a subset of\nthe validation set. Note that selecting a subset of\nparameters for update requires an extensive search,\nwhich KNOWLEDGE EDITOR dispenses with by au-\ntomatically learning it.\n5.2 Models and data\nWe evaluate on closed-book fact-checking (FC)\nﬁne-tune a BERT base model (Devlin et al., 2019)\non the binary FEVER dataset (Thorne et al., 2018)\nfrom KILT (Petroni et al., 2021). We also evaluate\n6We tried alternatives, RMSProp was the most effective.\n7We search the hyper-parameter for the penalty m ∈\n{10−3, 5 ×10−4, 10−4, 5 ×10−5, 10−5}selecting the best\nbased on the sum of success rate and retain accuracy.\non a task with a more complex output space: closed-\nbook question answering (QA). For that we ﬁne-\ntune a BART base model (Lewis et al., 2020) with\na standard seq2seq objective on the Zero-Shot Rela-\ntion Extraction (zsRE) dataset by Levy et al. (2017).\nWe evaluate on this dataset because it is annotated\nwith human-generated question paraphrases that\nwe can use to measure our model’s robustness to\nsemantically equivalent inputs. We create alterna-\ntive predictions for FC simply ﬂipping the labels,\nwhereas for QA we pick all hypotheses enumerated\nvia beam search except the top-1. The latter en-\nsures high-probability outcomes under the model\ndistribution. We generate semantically equivalent\ninputs with back-translation. See Appendix B for\ntechnical details on models and data collection.\n6 Results\nTable 1 reports the main results for fact-checking\nand question answering. Overall, KNOWL -\nEDGE EDITOR achieves high performance in all\nmetrics. Some other methods also achieve high ac-\ncuracy in some metrics but always sacriﬁcing oth-\ners (i.e., never meeting all our desiderata at once).\nWe compare methods along different metrics (as\nopposed to a single one), as there is no way to pre-\ncisely determine the importance of each of these\nmetrics. To gather more insight, we compute their\nstochastic convex combination with coefﬁcients\nsampled from a Dirichlet distribution (with α= 1\nto ensure a very diverse set of combinations) and\nreport in Figure 6 in Appendix C an estimate of\nthe probability that a system outperforms another\nacross 1,000 such combinations. The probability\nof our full method to outperform all baselines is\nvery high for both FC and QA (≈97% and ≈88%,\nrespectively). In Figure 5 in Appendix C, we show\nthe distributions of the combined scores (i.e., the\nraw data for the approximation reported in Fig-\nure 6). We then analyze different aspects of our\nmethod and the baselines.\n6.1 Success rate\nEvery method achieves an almost perfect success\nrate on fact-checking. All methods but ours apply\nupdates in a loop, stopping either when the new\nmodel is successfully updated or after reaching a\nmaximum number of iterations. The success rate\nfor KNOWLEDGE EDITOR is not 100% because we\ndo not apply more than one update even in case of\nfailure. To this end, we also show an experiment\n6497\nFact-Checking Question Answering\nMethod rate ↑\nSuccess\nacc ↑\nRetain\nacc ↑\nEquiv.\ndet ↓\nPerform.\nrate ↑\nSuccess\nacc ↑\nRetain\nacc ↑*\nEquiv.\ndet ↓\nPerform.\nFine-tune (1st layer) 100.0 99.44 42.24 0.00 98.68 91.43 89.86 / 93.59 0.41\nFine-tune (all layers) 100.0 86.95 95.58 2.25 100.0 67.55 97.77 / 98.84 4.50\nZhu et al. (1st layer) 100.0 99.44 40.30 0.00 81.44 92.86 72.63 / 78.21 0.32\nZhu et al. (all layers) 100.0 94.07 83.30 0.10 80.65 95.56 76.41 / 79.38 0.35\nOurs CL2 99.10 45.10 99.01 35.29 99.10 46.66 97.16 / 99.24 9.22\nKNOWLEDGE EDITOR 98.80 98.14 82.69 0.10 94.65 98.73 86.50 / 92.06 0.11\n+ loop† 100.0 97.78 81.57 0.59 99.23 97.79 89.51 / 96.81 0.50\n+ Px ‡ 98.50 98.55 95.25 0.24 94.12 98.56 91.20 / 94.53 0.17\n+ Px + loop‡ 100.0 98.46 94.65 0.47 99.55 97.68 93.46 / 97.10 0.95\nTable 1: Accuracy scores on fact-checking and question answering for the metrics presented in Section 2.2. *We\nreport both the accuracy on the set of generated paraphrases (left) and human-annotated (right).†Apply updates in\na loop, stopping when the update is a success or when reaching a maximum number of iterations (only at test time).\n‡Using paraphrases (semantically equivalent inputs) as additional supervision (only at training time).\nwith our method with multiple updates within a\nloop employing the same stopping criteria as the\nbaselines. Note that we apply this only at test time\n(i.e., we do not train for multiple updates). When\napplying multiple updates also our method reaches\na 100% success rate on fact-checking and almost\nperfect accuracy (>99%) for QA.8\nClosed-book QA is a more challenging task\nsince the output space is text and not just a bi-\nnary label. In this setting, KNOWLEDGE EDITOR\nachieves high accuracy ( ≈95% or > 99% with\nthe loop). Among all methods, K NOWLEDGE EDI-\nTOR gets the best success rate while also obtaining\nthe best retain accuracy. In QA, Zhu et al.’s (2020)\nmethod does not reach a good success rate (≈80%).\nWe searched hyperparameters for their method also\nto have high retain accuracy, and indeed that is\nhigher than regular ﬁne-tuning. However, unlike\nfact-checking, regular ﬁne-tuning for QA gets al-\nmost perfect scores but at the expense of the retain\naccuracy. Sequence-to-sequence models are more\nsensitive to a slight parameter shift. This happens\nbecause minor changes may completely alter the\ntop-k prediction from beam search (in the case of\nQA). Differently, in a binary classiﬁer (in the case\nof FC) the probability of a prediction can change\nsubstantially without crossing the decision bound-\nary (usually set at 0.5 when not calibrated).\n6.2 Retaining previous knowledge\nKNOWLEDGE EDITOR maintains the predictions in\nthe validation set almost perfectly (retain accuracy\n8Even if we do not train for multiple subsequent updates,\nits success opens the possibility to add this at training time.\nWe leave the exploration of this technique to future work.\nis ≈98% for both FC and QA). Conversely, as ex-\npected, our method with CL2 has very low retain\naccuracy (always <50%). CL2 suffers from catas-\ntrophic forgetting because it does not enforce the\nupdated model to be close to the original one in\nfunction space (i.e., the two functions to be similar)\nbut just in parameter space.\nFine-tuning all layers is successful but it affects\nthe previously acquired knowledge negatively: re-\ntain accuracy is ≈87% and ≈68% for FC and\nQA, respectively, while performance deterioration\nin ≈2% and ≈4%. Fine-tuning a single layer is\nmore effective as it prevents over-ﬁtting (the best\nmodel updates the 1st layer in both FC and QA).\nHowever, in FC the updated model does not gener-\nalize on semantic equivalent inputs: the accuracy\non paraphrases is much lower even than versions\nof our methods which do not use paraphrases in\ntraining (42% vs. >81%), and even more so when\ncompared to those which use them (>94%).\nFine-tuning with Zhu et al.’s (2020) method does\nnot affect performance for FC much, which is not\nsurprising since standard ﬁne-tuning already gets\nalmost perfect scores. Differently, in the QA set-\nting, using their constrained optimization boosts\nthe retain accuracy (up to +4% to normal ﬁne-\ntuning) but at the cost of a low success rate (≈80%\nwhere ﬁne-tuning gets the perfect score).\n6.3 Accuracy on paraphrases\nWe evaluate our method both with and without the\nadditional supervision of paraphrases to improve\ngeneralization—that corresponds to have Px as the\nset of paraphrases of xor Px = {x}in Equation 1,\nrespectively. Without this additional supervision,\n6498\n4\n 2\n 0 2 4\nLogits original model\n4\n2\n0\n2\n4\nLogits updatede model\nShould not flip (correct)\nShould not flip (wrong)\nShould flip (correct)\nShould flip (wrong)\n(a) Fine-tune (all layers).\n4\n 2\n 0 2 4\nLogits original model\n4\n2\n0\n2\n4\nLogits updatede model\nShould not flip (correct)\nShould not flip (wrong)\nShould flip (correct)\nShould flip (wrong) (b) CL2 .\n4\n 2\n 0 2 4\nLogits original model\n4\n2\n0\n2\n4\nLogits updatede model\nShould not flip (correct)\nShould not flip (wrong)\nShould flip (correct)\nShould flip (wrong) (c) Ours CKL with Px.\nFigure 3: Distribution of logits of the original model and updated model on FEVER. Fine-tuning all layers (a)\nleads to many errors, and the probability of the predictions does not stay the same even when they do not cross the\ndecision boundary. CL2 (b) successfully ﬂips labels, but it does not force the predictions to stay the same. For our\nfull method, CKL with Px (c), errors are mainly concentrated around the origin where the model is uncertain, and\nsmall perturbations make logits to cross the decision boundary. Better view with colors.\nKNOWLEDGE EDITOR is already competitive in\nequivalence accuracy. However, employing this\nadditional supervision is clearly beneﬁcial on both\ntasks: we get the same success rate and re-train\naccuracy but equivalence accuracy improves by\n> 70% on FC and > 30% on QA, respectively\n(for generated paraphrases). In FC, although ﬁne-\ntuning of a single layer proved to be optimal in\nterms of success rate and retain accuracy, it per-\nforms poorly for paraphrases. That is the model\nsuccessfully updates the prediction of a particular\ndatapoint, but does not update predictions of para-\nphrases. This indicates that ﬁne-tuning to edit the\nknowledge of a model does not generalize well,\nand it overﬁts to speciﬁc inputs. On QA, also Zhu\net al. (2020) performs poorly compared to our or\nother methods.\nWhen other methods perform on par or better\nthan ours on paraphrases, they do not have good re-\ntain accuracy (e.g., see QA ﬁne-tuning on Table 1).\nFine-tuning on QA seems to generalize better than\non FC, but does not preserve previous knowledge.\nIn Table 1 we also report both the accuracy on the\nset of generated and human-generated paraphrases.\nSurprisingly, the scores on human-generated para-\nphrases are higher. We speculate that this happens\nbecause automatic paraphrases are sometimes not\nsemantically equivalent or ﬂuent.\n6.4 Analysis of model updates\nIn Figure 3 we plot the distribution of logits of\nthe original and updated model on FC for different\nmethods. With an ideal method, all logits before\nand after an update have to stay the same (except\nthe ones we want to change). From that ﬁgure,\nwe can see distributions of different types of er-\nrors such as datapoints whose predictions were\nmistakenly ﬂipped (from true to false or the other\nway around). These errors are mostly concentrated\naround the origin, where small perturbations make\nlogits cross the decision boundary. When ﬁne-\ntuning all layers, we can see a clear impact on\nlogits, they undergo a lot of change (i.e., points do\nnot concentrate around the diagonal). Indeed, ﬁne-\ntuning makes many datapoints cross the decision\nboundary and their probabilities to change from\nthe original ones. The failure of CL2 is visible in\nFigure 3b as this method preserves almost none of\nthe previous predictions. Instead KNOWLEDGE ED-\nITOR preserves almost all of the predicted labels\nas well as their probabilities (most datapoints in\nFigure 3c stay on the diagonal).\nWe also report visualizations of the average\nweight updates for the QA experiment in Figure 4.\nWe report the setting with additional supervision\nfrom paraphrases (but the heatmaps are similar\nwithout them). There are three main observations\nfrom this plot. First, gradients are mostly concen-\ntrated on the ﬁrst encoder layer and the last decoder\nlayer. Gradients explain why the best subset of\nparameters to update is the ﬁrst layer. Secondly,\nﬁne-tuning does not preserve gradient magnitudes\nand updates the whole model almost uniformly.\nThat happens because of the optimizer’s adaptive\n6499\n1 2 3 4 5 6\nLayer\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nEncoder\n1 2 3 4 5 6\nLayer\nWK\nenc\nWO\nenc\nWQ\nenc\nWV\nenc\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nDecoder\n(a) Gradients.\n1 2 3 4 5 6\nLayer\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nEncoder\n1 2 3 4 5 6\nLayer\nWK\nenc\nWO\nenc\nWQ\nenc\nWV\nenc\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nDecoder (b) Fine-tune (all layers).\n1 2 3 4 5 6\nLayer\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nEncoder\n1 2 3 4 5 6\nLayer\nWK\nenc\nWO\nenc\nWQ\nenc\nWV\nenc\nFF1\nFF2\nWK\nself\nWO\nself\nWQ\nself\nWV\nself\nDecoder (c) KNOWLEDGE EDITOR + Px.\nFigure 4: Average normalized magnitude of updates on weight matrices across layers for the QA experiment.\nFine-tuning updates all layers uniformly while our updates are more sparse.\nlearning rate that initially erases the gradient di-\nrection. The gradient direction plays a role only\nafter a couple of gradient steps, but most of the\ntime, the method only needs one step to modify its\nknowledge. Lastly, our updates are sparser and are\nnot consistent with the gradient for changing the\npredictions. That indicates that our method learns\nto use the gradient in a meaningful way (i.e. ignor-\ning some directions or manipulating its magnitude).\nIt is surprising that the knowledge manipulation\nseems to be achieved by primarily modifying pa-\nrameters affecting the shape of the attention distri-\nbution (WK\nself and WQ\nself) rather than, e.g., values\n(WV\nself). As we discussed, the hyper-network may\nbe regarded as a probe providing insights about the\nmechanism used by the model to encode the knowl-\nedge (Vig et al., 2020). For example, the focus\non the bottom layer is already intriguing, as it con-\ntrasts with claims that memorization happens in top\nlayers of image classiﬁcation models (Stephenson\net al., 2021), hinting at substantial differences in\nthe underlying memorization mechanisms in NLP\nand vision. Proper investigation is however outside\nof the scope of this study. See Appendix C for\nsome additional analysis.\n7 Conclusions\nIn this work, we explore the task of editing the fac-\ntual knowledge implicitly stored in the parameters\nof Language Models. For this task, we formally\ndeﬁne desiderata, the objective, and a set of metrics\nto measure the efﬁcacy of different methods. We\nconcretely evaluate that on two benchmarks based\non closed-book fact-checking and question answer-\ning. We propose KNOWLEDGE EDITOR , a method\nbased on a hyper-network that learns to modify\nimplicit knowledge stored within LM parameters\nefﬁciently and reliably. We provide comprehensive\nevaluations for our models against different vari-\nants of ﬁne-tuning demonstrating the advantage of\nour approach. The magnitude of the updates pre-\ndicted by our method may unfold the mechanisms\nused by the LMs to encode factual knowledge; we\nleave such investigation for future work.\nEthical Considerations\nTechnology built upon pre-trained LMs inherits\nsome or all of their potential harms (Bender et al.,\n2021). Our technology for editing the knowledge\nof LMs does not exacerbate their potential harms\nand can, in fact, be used to mitigate harms, as mod-\nels can be corrected once problems are discovered.\nHowever, we note that malicious uses of our knowl-\nedge editor are possible. For example, malicious\nagents may use the techniques presented in this\nwork to inject incorrect knowledge into LMs.\nAcknowledgments\nThe authors want to thank Michael Schlichtkrull,\nLena V oita and Luisa Quarta for helpful discussions\nand support. This project is supported by SAP\nInnovation Center Network, ERC Starting Grant\nBroadSem (678254), the Dutch Organization for\nScientiﬁc Research (NWO) VIDI 639.022.518, and\nthe European Union’s Horizon 2020 research and\ninnovation programme under grant agreement No\n825299 (Gourmet).\n6500\nReferences\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623, New York, NY , USA. As-\nsociation for Computing Machinery.\nStephen Boyd, Stephen P Boyd, and Lieven Vanden-\nberghe. 2004. Convex optimization. Cambridge uni-\nversity press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nRóbert Csordás, Sjoerd van Steenkiste, and Jürgen\nSchmidhuber. 2021. Are neural nets modular?\ninspecting functional modularity through differen-\ntiable weight masks. In Submitted to International\nConference on Learning Representations.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021a. Autoregressive entity re-\ntrieval. In International Conference on Learning\nRepresentations.\nNicola De Cao, Michael Sejr Schlichtkrull, Wilker\nAziz, and Ivan Titov. 2020. How do decisions\nemerge across layers in neural models? interpreta-\ntion with differentiable masking. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 3243–\n3255, Online. Association for Computational Lin-\nguistics.\nNicola De Cao, Ledell Wu, Kashyap Popat, Mikel\nArtetxe, Naman Goyal, Mikhail Plekhanov,\nLuke Zettlemoyer, Nicola Cancedda, Sebastian\nRiedel, and Fabio Petroni. 2021b. Multilingual\nautoregressive entity linking. arXiv preprint\narXiv:2103.12528.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral ex-\nplanation with amnesic counterfactuals. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:160–175.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4937–4951, Online. Associa-\ntion for Computational Linguistics.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1126–1135. PMLR.\nDavid Ha, Andrew M. Dai, and Quoc V . Le. 2017.\nHypernetworks. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings\nof ACL 2018, System Demonstrations , pages 116–\n121, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\n6501\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJ. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz,\nJ. Veness, G. Desjardins, Andrei A. Rusu, K. Milan,\nJohn Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, Demis Hassabis, C. Clopath, D. Ku-\nmaran, and Raia Hadsell. 2017. Overcoming catas-\ntrophic forgetting in neural networks. Proceedings\nof the National Academy of Sciences , 114:3521 –\n3526.\nDavid Krueger, Chin-Wei Huang, Riashat Islam, Ryan\nTurner, Alexandre Lacoste, and Aaron Courville.\n2017. Bayesian hypernetworks. arXiv preprint\narXiv:1710.04759.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:453–466.\nYair Lakretz, German Kruszewski, Theo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syn-\ntax units in LSTM language models. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 11–20, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau\nYih, Hao Ma, and Madian Khabsa. 2020. Language\nmodels as fact checkers? In Proceedings of the\nThird Workshop on Fact Extraction and VERiﬁca-\ntion (FEVER), pages 36–41, Online. Association for\nComputational Linguistics.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333–342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming\nDing, Yujie Qian, Zhilin Yang, and Jie Tang.\n2021. GPT Understands, Too. arXiv preprint\narXiv:2103.10385.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rocktäschel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 2523–2544,\nOnline. Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. Model-agnostic interpretability of\nmachine learning. International Conference on Ma-\nchine Learning (ICML) Workshop on Human Inter-\npretability in Machine Learning.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\n6502\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nCory Stephenson, Suchismita Padhy, Abhinav Ganesh,\nYue Hui, Hanlin Tang, and SueYeon Chung. 2021.\nOn the geometry of generalization and memoriza-\ntion in deep neural networks. Proceedings of Inter-\nnational Conference on Learning Representations\n(ICLR).\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, 34(05):8968–8975.\nIlya Sutskever, James Martens, and Geoffrey E. Hin-\nton. 2011. Generating text with recurrent neu-\nral networks. In Proceedings of the 28th Inter-\nnational Conference on Machine Learning, ICML\n2011, Bellevue, Washington, USA, June 28 - July 2,\n2011, pages 1017–1024. Omnipress.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Informa-\ntion Processing Systems 2014, December 8-13 2014,\nMontreal, Quebec, Canada, pages 3104–3112.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the inception architecture for computer vi-\nsion. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016, pages 2818–2826.\nIEEE Computer Society.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERiﬁcation. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nTijmen Tieleman and Geoffrey Hinton. 2012. Lecture\n6.5—RmsProp: Divide the gradient by a running av-\nerage of its recent magnitude. COURSERA: Neural\nnetworks for machine learning, 4(2):26–31.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W Cohen. 2020. Facts as experts: Adapt-\nable and interpretable neural memory over symbolic\nknowledge. arXiv preprint arXiv:2007.00849.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Causal mediation analysis for\ninterpreting neural NLP: The case of gender bias.\nNeurIPS.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 451–462, Melbourne, Australia.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\n6503\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Sri-\nnadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. 2020. Modifying memories in transformer\nmodels. arXiv preprint arXiv:2012.00363.\n6504\nA Relaxation and Approximation of\nConstrained Optimization\nGiven a objective to minimize in the form of\nmin\nφ\nE\nx∼p(x)\n[f(x,θ)]\ns.t. 1\n|Y|\n∑\nx∈Y\nC(y,θ) ≤m,\n(4)\ncan be solved with Lagrangian relaxation (Boyd\net al., 2004) using a multiplier α ∈R≥0 and be\napproximated by sampling y∼p(y) to\nmin\nφ\nmax\nα\nf(x,θ) + α·(C(y,θ) −m) . (5)\nEquation 5 can be evaluated with automatic differ-\nentiation and optimized via gradient descent.\nB Experimental setting\nB.1 Fact-checking\nWe evaluate on closed-book fact-checking (FC)\nusing the binary FEVER dataset (Thorne et al.,\n2018) from KILT (Petroni et al., 2021). FEVER has\n104,966 training and 10,444 validation instances\nrespectively. For every input claim x, the model\npredicts the probability f(x; θ) that it may be true.\nThis is done without retrieving any evidence from\na corpus, instead, just by relying on the knowledge\naccumulated during pre-training and encoded in its\nown parameters—this is similar to Lee et al. (2020)\nthat investigate closed-book and zero-shot FC us-\ning masked-LMs. Concretely, we ask the LM to\nperform binary classiﬁcation. We ﬁne-tune a BERT\nbase model (Devlin et al., 2019) with an additional\nlinear layer on top that maps the hidden state cor-\nresponding to the BOS (beginning of a sentence)\ntoken to the probability of the positive label. Given\nthe available supervision, we train the architecture\nto maximize the model likelihood penalized by en-\ntropy regularization and weight decay. The ﬁnal\nmodel has an accuracy of 77.1%.9\nB.2 Question answering\nWe also evaluate on a task with a more com-\nplex sample space: closed-book question answer-\ning (QA). Here QA is treated as a sequence-to-\nsequence problem from question to answer with-\nout retrieving nor providing any evidence (Roberts\net al., 2020). This, as in FC, emphasises the role\n9This is comparable with what reported by Petroni et al.\n(2021) for a larger BART model.\nof the knowledge acquired in pre-training and en-\ncoded in the parameters of the model. For this\ntask, we used the Zero-Shot Relation Extraction\n(zsRE) dataset by Levy et al. (2017). We pre-\nfer zsRE to other popular QA datasets such as\nSQuAD (Rajpurkar et al., 2016), Natural Ques-\ntions (Kwiatkowski et al., 2019) or TriviaQA (Joshi\net al., 2017) because it is annotated with human-\ngenerated question paraphrases that we can use to\nevaluate our model’s robustness to semantically\nequivalent inputs. zsRE is speciﬁcally constructed\nnot to have relation overlaps between training and\ntest (i.e. it is zero-shot). We re-split the dataset\nto have the same distribution in training and test\nsplits—we are not interested in zero-shot specif-\nically, so we avoid the additional complexity it\nentails. The original zsRE dataset has 147,909\ntraining and 3,724 validation instances respectively.\nAfter re-splitting and employing all paraphrases,\nwe have 244,173 training and 27,644 validation\ninstances respectively. For this task, we ﬁne-tune\na BART base model (Lewis et al., 2020) with a\nstandard seq2seq objective, i.e., maximizing the\nmodel likelihood given the observed output se-\nquence (Sutskever et al., 2011, 2014) and regu-\nlarized with dropout (Srivastava et al., 2014) and\nlabel smoothing (Szegedy et al., 2016). The ﬁnal\nmodel has an accuracy (exact match between model\nprediction and gold standard) of 22.1%.10\nB.3 Generating alternative predictions\nGeneration of alternative predictions is task-\ndependent as it requires producing a plausible sub-\nstitute target for a given input— e.g., if we need to\nedit the knowledge about a head of a state, a plau-\nsible substitute label should be a person, not a ran-\ndom (even if well-formed) string. Fact-Checking\nis straightforward: we simply ﬂip the label, as it\nis binary classiﬁcation. For QA, we exploit high-\nprobability outcomes under the model distribution\nas a proxy to plausible revisions. In particular, we\npick all hypotheses enumerated via beam search\nexcept the top-1.11\n10This is more than reported by Petroni et al. (2021) on the\noriginal split of zsRE. That is because the original split aims\nat zero-shot evaluation, while we have an overlap of relation\ntypes between training and validation sets.\n11This does not always guarantee that the alternative pre-\ndictions have the same semantic type as the original one, but\nit is likely since the model assigns high probability to them.\n6505\nB.4 Semantically equivalent inputs\nWe would like the updated model to be consistent\nfor semantically equivalent inputs (see Px in Sec-\ntion 2 and 4) as opposed to just learning a new\nspeciﬁc and isolated datapoint. This consistency is\nindicative of an effective editing mechanism that\ntaps into the knowledge stored in the model. How-\never, not all datasets come with paraphrases of its\ninputs (e.g., in our case FEVER does not come\nwith paraphrases and zsRE only has paraphrases\nfor 30% for the dataset). To this end, we gen-\nerate semantically equivalent inputs using round-\ntrip translation (Sennrich et al., 2016; Wieting and\nGimpel, 2018). We employ English-to-German\nand German-to-English Transformer models from\nMarian Neural Machine Translation (MarianNMT;\nJunczys-Dowmunt et al., 2018) provided by Hug-\ngingface Transformers (Wolf et al., 2020). We use\nbeam search with beam size 5 to obtain 25 para-\nphrases. From this set, we exclude any candidate\nparaphrase ˆxof xfor which the prediction ˆysup-\nported by f(ˆx; θ) does not match the prediction y\nsupported by f(x; θ). This ﬁltering ensures that, ac-\ncording to the current model, all paraphrases have\nthe exact same prediction.\nB.5 Architecture details\nThe original models we want to modify are a BERT\nbase model (Devlin et al., 2019) and a BART base\nmodel (Lewis et al., 2020) for fact-checking and\nquestion answering respectively. They are both\nTransformer based models with 12 layers each and\nhidden size of 768. BERT has 12 heads, where\nBART has 16. They have 110M and 139M param-\neters respectively. BERT has a vocabulary size of\n30,522 where BART has 50,265.\nKNOWLEDGE EDITOR has a small single-layered\nbidirectional-LSTM with input size 768 and hid-\nden size of 128. The FFNN that condenses the\nLSTM states follows a [256, tanh, 1024] architec-\nture where the 5 FFNN have all a [1024, tanh, d]\narchitecture where ddepends on the weight to mod-\nify. In our experiments, we do not use our model\nto modify biases, layer norms, word and positional\nembeddings of LMs. Overall, KNOWLEDGE EDI-\nTOR has 54M and 67M parameters for BERT and\nBART respectively.\nB.6 Training details\nThe original models which we want to mod-\nify are trained with a batch size of 256 using\nAdam (Kingma and Ba, 2015) (learning rate of\n3e-5) with weight decay (1e-2) and a linear sched-\nule with warm-up (50k total number of updates and\n500 warm-up updates). We trained for a maximum\nof 20 epochs and employ model selection using\naccuracy on the validation set.12\nKNOWLEDGE EDITOR models are trained with\na batch size of 1024 for FC and 256 for QA using\nAdam (learning rate of 3e-4 for the parameters and\n1e-1 for the Lagrangian multiplier) with weight de-\ncay (1e-2) and a linear schedule with a warm-up\n(200k total number of updates and 1k warm-up up-\ndates). We trained for a maximum of 200 epochs\nand employ model selection using overall accuracy\n(success rate and retain accuracy) on the valida-\ntion set (approximated using mini-batches).13 The\nmargin for the CKL is annealed between 1e-1 and\n1e-3 for the fact-checking model, and between 1e-3\nand 1e-5 for the BART question answering model.\nFor the sequence-to-sequence loss, we employ a\ncross-entropy loss with label smoothing of 0.1.\nC Additional Results\nUpdate Analysis During preliminary experi-\nments, we studied a version of our hyper-network\nthat did not exploit gradient information (see Equa-\ntion 3). Without gradient information, on FC the\nmodels converged ≈10 times slower to reach the\nsame accuracy and did not converge for QA (i.e.,\nthe model was not able to get >75% success rate\nand > 50% retain accuracy). That suggest that\nthe gradients are helpful and actually used by our\nhyper-network but should not used directly, with-\nout a modiﬁcation. To better show this, in Table 2\nwe report correlations between different update\nmethods and the gradient in terms of cosine simi-\nlarities between updates. Naturally, ﬁne-tuning and\nthe gradient are highly correlated, but our method\n(with and without additional paraphrases supervi-\nsion), poorly correlates with the others. Low cosine\nsimilarity can be due to two factors i) the model\nindeed projects the gradient to a different and more\n‘knowledge preserving’ direction, or ii) the param-\neter space is so large that cosine similarity gets to\nzero very quickly, not revealing the genuine under-\nlying similarity.\n12We trained on 4 Nvidia Titian X 12GB which take ap-\nproximately 10 minutes for FC and 3 hours for QA.\n13We trained on 4 Nvidia Titian X 12GB which take ap-\nproximately 1 day for FC and 3 days for QA.\n6506\n∇θL Fine-tune CKL CKL + Px\n∇θL 1.000 0.451 -0.018 -0.025\nFine-tune 0.451 1.000 -0.010 -0.011\nCKL -0.017 -0.010 1.000 0.183\nCKL + Px -0.021 -0.011 0.183 1.000\nTable 2: Average cosine similarities between different\nupdate methods and the gradient for the update as well.\nFine-tuning is applied to all layers.\n88 90 92 94 96 98 100\nwTs with w Dir(1.0)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Density\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOur CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\n(a) Fact-checking.\n88 90 92 94 96 98 100\nwTs with w Dir(1.0)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Density\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOurs CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\n(b) Question answering.\nFigure 5: Probability distributions of weighted sum of\nmetrics according to 1k random assignments sampled\nfrom a Dirichlet distribution (with α = 1—see all val-\nues in Table 1). Sampling weights allows to interpret\nthe score in a probabilistic way. KNOWLEDGE EDITOR\n(with different variants) presents distributions that are\nmore skewed towards a high score (100) indicating that\nit is highly likely that when assigning some weights to\nthe metrics, the weighted sum will be in favour of our\nmethod. Better view with colors.\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOur CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\nSystem B\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOur CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\nSystem A\n22.4 100.0 11.7 69.0 6.0 5.2 4.7 2.8\n77.6 78.3 53.3 99.4 49.9 49.8 6.3 2.7\n0.0 21.7 11.2 68.2 5.7 5.0 4.6 2.7\n88.3 46.7 88.8 92.9 32.8 39.3 3.0 0.2\n31.0 0.6 31.8 7.1 7.8 8.2 0.9 0.8\n94.0 50.1 94.3 67.2 92.2 71.5 1.5 0.4\n94.8 50.2 95.0 60.7 91.8 28.5 5.4 0.0\n95.3 93.7 95.4 97.0 99.1 98.5 94.6 42.0\n97.2 97.3 97.3 99.8 99.2 99.6 100.0 58.0\n(a) Fact-checking.\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOur CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\nSystem B\nfine-tune (1st layer)\nfine-tune (all layers)\nZhu et. al (1st layer)\nZhu et. al (all layers)\nOurs CL2\nOur CKL\nOurs CKL+loop\nOurs CKL+ x\nOurs CKL+ x+loop\nSystem A\n67.8 99.9 99.0 78.6 62.5 0.3 23.8 0.1\n32.2 81.4 77.1 93.0 41.1 20.0 30.9 11.3\n0.1 18.6 1.7 33.4 0.0 0.0 0.0 0.0\n1.0 22.9 98.3 37.4 0.0 0.0 0.0 0.0\n21.4 7.0 66.6 62.6 30.1 14.0 21.9 7.8\n37.5 58.9 100.0 100.0 69.9 1.1 2.7 1.1\n99.7 80.0 100.0 100.0 86.0 98.9 80.2 4.5\n76.2 69.1 100.0 100.0 78.1 97.3 19.8 2.7\n99.9 88.7 100.0 100.0 92.2 98.9 95.5 97.3\n(b) Question answering.\nFigure 6: Probability that system A is better than sys-\ntem B according to a weighted sum of metrics (see indi-\nvidual values in Table 1) sampling mixing coefﬁcients\n1,000 times from a Dirichlet distribution (with α = 1\nto cover a diverse spectrum of metric combinations).\nThe probability that K NOWLEDGE EDITOR (with CKL\n+ Px + loop) is better than competing systems is high\n(> 97% for FC and > 88% for QA) indicating that it\nis highly likely that when assigning some weights to\nthe metrics, the weighted sum will be in favour of our\nmethod. Better view with colors.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8632384538650513
    },
    {
      "name": "Inference",
      "score": 0.7506300210952759
    },
    {
      "name": "Code (set theory)",
      "score": 0.6598280668258667
    },
    {
      "name": "Sequence (biology)",
      "score": 0.6394729018211365
    },
    {
      "name": "Question answering",
      "score": 0.6055245399475098
    },
    {
      "name": "Language model",
      "score": 0.585943341255188
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5836492776870728
    },
    {
      "name": "Natural language processing",
      "score": 0.5405503511428833
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4506511688232422
    },
    {
      "name": "Test (biology)",
      "score": 0.44171440601348877
    },
    {
      "name": "Training set",
      "score": 0.4390409588813782
    },
    {
      "name": "Machine learning",
      "score": 0.4253149926662445
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.4222261309623718
    },
    {
      "name": "Programming language",
      "score": 0.23294347524642944
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.11160629987716675
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ],
  "cited_by": 20
}