{
  "title": "Self-Supervised Pretraining of Transformers for Satellite Image Time Series Classification",
  "url": "https://openalex.org/W3095867871",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2041852224",
      "name": "Yuan Yuan",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2098159552",
      "name": "Lei Lin",
      "affiliations": [
        "Qihoo 360 (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2041852224",
      "name": "Yuan Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098159552",
      "name": "Lei Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W2122922389",
    "https://openalex.org/W6776994599",
    "https://openalex.org/W2999796608",
    "https://openalex.org/W2981830988",
    "https://openalex.org/W6753000030",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W2920254659",
    "https://openalex.org/W2150700542",
    "https://openalex.org/W3029149175",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2988111912",
    "https://openalex.org/W2961745803",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W6770316098",
    "https://openalex.org/W2113503197",
    "https://openalex.org/W2976120863",
    "https://openalex.org/W2604086375",
    "https://openalex.org/W2742878349",
    "https://openalex.org/W2963131120",
    "https://openalex.org/W2899125955",
    "https://openalex.org/W2737391801",
    "https://openalex.org/W2786038065",
    "https://openalex.org/W2890443177",
    "https://openalex.org/W2781778455",
    "https://openalex.org/W3034208233",
    "https://openalex.org/W2040218731",
    "https://openalex.org/W2098367044",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3017628311",
    "https://openalex.org/W2971432438",
    "https://openalex.org/W6779879437",
    "https://openalex.org/W2911232024",
    "https://openalex.org/W2273708466",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2995678734",
    "https://openalex.org/W2531168480",
    "https://openalex.org/W2583513334",
    "https://openalex.org/W2904606389",
    "https://openalex.org/W2609880332",
    "https://openalex.org/W3047443805",
    "https://openalex.org/W3048631361",
    "https://openalex.org/W2903282641",
    "https://openalex.org/W2913586827",
    "https://openalex.org/W2307094448",
    "https://openalex.org/W2945897702",
    "https://openalex.org/W2886775386",
    "https://openalex.org/W2938326879",
    "https://openalex.org/W2581906016",
    "https://openalex.org/W3037458146",
    "https://openalex.org/W2729021809",
    "https://openalex.org/W3006462480",
    "https://openalex.org/W2620858446",
    "https://openalex.org/W2762941833",
    "https://openalex.org/W3015728455",
    "https://openalex.org/W2724081917",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2747638294",
    "https://openalex.org/W2886702754",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3103695279",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3136985093",
    "https://openalex.org/W3021105189",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3143105502",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034847898",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3036587262",
    "https://openalex.org/W3102692100",
    "https://openalex.org/W3104839310"
  ],
  "abstract": "Satellite image time series (SITS) classification is a major research topic in remote sensing and is relevant for a wide range of applications. Deep learning approaches have been commonly employed for the SITS classification and have provided state-of-the-art performance. However, deep learning methods suffer from overfitting when labeled data are scarce. To address this problem, we propose a novel self-supervised pretraining scheme to initialize a transformer-based network by utilizing large-scale unlabeled data. In detail, the model is asked to predict randomly contaminated observations given an entire time series of a pixel. The main idea of our proposal is to leverage the inherent temporal structure of satellite time series to learn general-purpose spectral-temporal representations related to land cover semantics. Once pretraining is completed, the pretrained network can be further adapted to various SITS classification tasks by fine-tuning all the model parameters on small-scale task-related labeled data. In this way, the general knowledge and representations about SITS can be transferred to a label-scarce task, thereby improving the generalization performance of the model as well as reducing the risk of overfitting. Comprehensive experiments have been carried out on three benchmark datasets over large study areas. Experimental results demonstrate the effectiveness of the proposed pretraining scheme, leading to substantial improvements in classification accuracy using transformer, 1-D convolutional neural network, and bidirectional long short-term memory network. The code and the pretrained model will be available at https://github.com/linlei1214/SITS-BERT upon publication.",
  "full_text": "474 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nSelf-Supervised Pretraining of Transformers for\nSatellite Image Time Series Classiﬁcation\nYuan Yuan , Member , IEEE,a n dL e iL i n\nAbstract—Satellite image time series (SITS) classiﬁcation is a\nmajor research topic in remote sensing and is relevant for a wide\nrange of applications. Deep learning approaches have been com-\nmonly employed for the SITS classiﬁcation and have provided\nstate-of-the-art performance. However, deep learning methods suf-\nfer from overﬁtting when labeled data are scarce. To address this\nproblem, we propose a novel self-supervised pretraining scheme\nto initialize a transformer-based network by utilizing large-scale\nunlabeled data. In detail, the model is asked to predict randomly\ncontaminated observations given an entire time series of a pixel.\nThe main idea of our proposal is to leverage the inherent tem-\nporal structure of satellite time series to learn general-purpose\nspectral-temporal representations related to land cover semantics.\nOnce pretraining is completed, the pretrained network can be\nfurther adapted to various SITS classiﬁcation tasks by ﬁne-tuning\nall the model parameters on small-scale task-related labeled data.\nIn this way, the general knowledge and representations about\nSITS can be transferred to a label-scarce task, thereby improv-\ning the generalization performance of the model as well as re-\nducing the risk of overﬁtting. Comprehensive experiments have\nbeen carried out on three benchmark datasets over large study\nareas. Experimental results demonstrate the effectiveness of the\nproposed pretraining scheme, leading to substantial improvements\nin classiﬁcation accuracy using transformer, 1-D convolutional\nneural network, and bidirectional long short-term memory net-\nwork. The code and the pretrained model will be available at\nhttps://github.com/linlei1214/SITS-BERT upon publication.\nIndex Terms —Bidirectional encoder representations from\nTransformers (BERT), classiﬁcation, satellite image time series\n(SITS), self-supervised learning, transfer learning, unsupervised\npretraining.\nI. INTRODUCTION\nN\nOW ADAYS, a huge volume of Earth observation\n(EO) data are being accumulated thanks to remarkable\nManuscript received September 15, 2020; revised October 22, 2020; accepted\nNovember 3, 2020. Date of publication November 9, 2020; date of current\nversion January 6, 2021. This work was supported in part by the Research Project\nof Surveying Mapping and Geoinformation of Jiangsu Province under Project\nJSCHKY201905, in part by the Natural Science Foundation of Jiangsu Province\nunder Grant BK20170897, in part by the National Natural Science Foundation\nof China under Grant 41901356, and in part by the support of Environmental\nProtection Research Project of Jiangsu Province under Grant 2019010.(Yuan\nYuan and Lei Lin are co-ﬁrst authors.) (Corresponding author: Yuan Yuan.)\nYuan Yuan is with the School of Geographic and Biologic Information,\nNanjing University of Posts and Telecommunications, Nanjing 210023, China\n(e-mail: yuanyuan@njupt.edu.cn).\nLei Lin is with the Beijing Qihoo Technology Company Ltd., Beijing 100015,\nChina (e-mail: linlei1214@163.com).\nThis article has supplementary downloadable material available at https://\nieeexplore.ieee.org, provided by the authors.\nDigital Object Identiﬁer 10.1109/JSTARS.2020.3036602\nbreakthroughs in the latest-generation of satellites. After the\nsuccessive launch of the twin satellites (Sentinel-2A/B) of the\nSentinel-2 mission, it is now possible to access large area,\nhigh-quality medium spatial resolution EO data with much\nhigher frequency. Due to the high revisit time of Sentinel-2A/B\nsatellites (ﬁve days from the two-satellite constellation),\nconsecutively acquired images covering the same geographical\narea can be properly organized into satellite image time\nseries (SITS) [1]. Such medium-resolution SITS data provide\nvaluable information about the status and dynamics of the Earth\nsurface, supporting analyses of the functional and structural\ncharacteristics of land covers as well as identifying change\nevents [2], [3]. For this reason, SITS have been widely used in\nvarious application domains, such as ecology [4], agriculture [5],\nforest [6], land management [7], disaster monitoring [8] risk as-\nsessment [9], etc. In the meantime, new challenges are also being\nintroduced by the question of how to extract valuable knowledge\nand meaningful information to exploit such abundant data.\nSITS classiﬁcation is one of the central problems in the SITS\nanalysis, which is closely associated with many land applica-\ntions, such as land cover mapping and change detection [10],\nvegetation species classiﬁcation [11], and crop yields estimation\n[12]. SITS classiﬁcation involves assigning every pixel in an im-\nage to a categorical label, primarily based on the pixel’s spectral\nproﬁle (trajectories of spectral variations over time) [3]. Machine\nlearning algorithms provide effective tools to achieve automated\nSITS classiﬁcation. Traditional algorithms such as support vec-\ntor machine (SVM) and random forest (RF) classify SITS via\nhandcrafted features, such as raw reﬂectances, spectral statistics,\nand phenological metrics [13], [14]. However, characterizing\nthese features is rather difﬁcult due to the strong interannual\nvariations in seasonal patterns of the land surface reﬂectance,\nwhich can be caused by shifts in land cover or environmental\nconditions, management practices, and disturbance [15].\nLately, deep learning is gaining widespread popularity in the\nremote sensing community. Various deep neural network archi-\ntectures have been introduced to advance the state of the art for\nmany remote sensing classiﬁcation problems [16]–[18]. The ma-\njor advantage of deep learning methods is that they are capable\nof learning features from the input data optimized for a speciﬁc\ntask without the need for manual feature engineering [1], [19].\nDeep learning-based approaches are increasingly being applied\nto SITS classiﬁcation. Among these methods, convolutional neu-\nral networks (CNNs) [20], [21] and recurrent neural networks\n[RNNs, including long short-term memory (LSTM) or gated\nrecurrent units] [22], [23] have been most widely used to capture\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 475\ntemporal characteristics from spectral proﬁles. To exploit both\nspatial and temporal information of high-resolution SITS, hybrid\narchitectures combining convolutional and recurrent layers [24],\n[25] and convolutional-recurrent neural networks [26] have been\nintroduced and comprehensively compared [27]. Recently, a\npromising alternative to RNNs for sequence encoding, namely\nthe transformer, has been proposed in the ﬁeld of natural lan-\nguage processing (NLP) [28]. Transformers have been reported\nto have advantages over RNNs in terms of feature extraction and\ntraining efﬁciency. Garnotet al. ﬁrst introduced transformers\ninto SITS classiﬁcation. They employed a transformer encoder\nto learn temporal correlations from a sequence of pixel-set\nembeddings [29]. A detailed comparison between transformers\nand other prevailing deep learning architectures for SITS clas-\nsiﬁcation can be found in [30], showing that transformers and\nRNNs outperform CNNs on processing raw satellite time series\nin terms of their architectures.\nNonetheless, the recent success of deep learning approaches\nhas been primarily driven by deeper architectures and the avail-\nability of large amounts of labeled data. For instance, by training\na deep CNN on millions of human-annotated photographs such\nas ImageNet, it can learn powerful visual features reusable in\nvarious image understanding tasks. When training samples are\nscarce, a common drawback of most deep learning architectures\nis that they are very prone to overﬁtting, since such models often\nhave millions of parameters. However, there are rarely enough\nlabeled data in remote sensing applications, since labeling is very\ntime/labor-consuming and requires expertise. As a result, even\nthough a huge supply of SITS data are available, the performance\nof deep learning approaches is restricted due to lack of labels.\nTo fully tap the potential of deep learning methods for the\nSITS classiﬁcation, some studies have begun to explore how\nto mitigate the demand for labeled data. For example, Bazzi\net al. proposed a transfer learning framework, which adapts a\nnetwork trained on a label-rich dataset to a label-scarce task\nby means of knowledge distillation [31]. Iencoet al.developed\na weakly supervised layerwise pretraining strategy to initialize\nparameters of RNNs by utilizing coarse granularity labels to\nprovide supervision signals [32]. Although these approaches can\npartially alleviate the problem, their effectiveness is still limited\nby the quality and quantity of labeled samples. To the best of\nour knowledge, none of the existing work takes advantage of\nunlabeled data for SITS classiﬁcation.\nSelf-supervised learning is a recently emerged unsupervised\nlearning paradigm for tackling the challenge of insufﬁcient\nlabels [33]. In this paradigm, models are trained on unlabeled\ndata by leveraging the structure present in the data itself to\ncreate supervised tasks (such tasks are often referred as “pretext\ntasks”) [34]. The general pipeline of self-supervised learning is\nas follows: in the pretraining stage, models learn an initial set\nof representations by solving a predeﬁned pretext task; in the\nﬁne-tuning stage, these representations are further adapted to\na downstream task by continued training on task-related data\nin a supervised manner. In this way, deep learning models can\ntransfer general knowledge learned from large-scale unlabeled\ndata to a label-scarce downstream task, thereby enhancing the\ngeneralization capacity of deep learning models and preventing\nFig. 1. Proposed pipeline for satellite image time series (SITS) classiﬁcation:\nin the pretraining stage, a neural network (i.e., SITS-BERT, where BERT stands\nfor bidirectional encoder representations from Transformers) is pretrained on\nmassive unlabeled data to solve a pretext task; in the ﬁne-tuning stage, the\npretrained network serves as a representation model that is used in a speciﬁc\nclassiﬁcation task by ﬁne-tuning all the parameters on task-related labeled\nsamples.\noverﬁtting. Although a variety of self-supervised learning meth-\nods have emerged in recent years in computer vision [35]–[38]\nand NLP [39]–[41], we are not aware of any existing work that\nintroduces the perspective of self-supervised learning into the\nSITS analysis to cope with the label scarcity problem.\nIn this article, we propose a novel self-supervised pretrain-\ning scheme for parameter initialization of deep neural net-\nworks on SITS classiﬁcation tasks, which follows a stan-\ndard unsupervised-pretraining/supervised-ﬁne-tuning pipeline\nshown in Fig. 1. We design the following pretext task to train\na transformer network:the model is forced to predict corrupted\nobservations with randomly added noise, given an entire annual\ntime series of a pixel.The central idea behind our pretext task\nis to leverage the inherent temporal structure of satellite time\nseries to capture meaningful spectral-temporal characteristics\nfrom a large volume of SITS data, and that these characteristics\nare closely related to the natural changes at the Earth’s surface.\nIn this way, enormous amounts of background knowledge are\naccumulated in the network through pretraining, making the\nmodel “understand” what satellite time series should look like.\nHence, it is more robust for neural networks to learn a mapping\nbetween spectral proﬁles and corresponding types given a small\namount of labeled data, as the connection between temporal\ndynamics of satellite time series and land cover semantics is\nstrong.\nThe key contributions of this article are threefold.\n1) For the ﬁrst time, we propose a self-supervised pretraining\nscheme to cope with the problem of insufﬁcient labeled\nsamples for the SITS analysis.\n2) We introduce an end-to-end deep learning architecture\nas an effective alternative to convolutional and recurrent\nneural networks for the SITS classiﬁcation.\n3) We conduct comprehensive experiments on three large-\nscale datasets to validate the effectiveness of the proposed\nmethod.\nThe remainder of this article is organized as follows. Section II\nsummarizes related work on self-supervised learning in remote\nsensing. Section III explains the motivation of the proposed\nmethod. Sections IV and V describe the proposed network\narchitecture and pretraining scheme, respectively. Section VI\nprovides the data information. Section VII reports the experi-\nmental results and discussions. Finally, Section VIII concludes\nthis article.\n476 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nII. RELATED WORK\nAn effective pretext task is the key to successful self-\nsupervised learning and should guarantee that the model learn\nmeaningful representations instead of trivial solutions [34].\nAccording to the strategies used to design pretext tasks, most\nof the available self-supervised learning approaches in remote\nsensing can be broadly classiﬁed into the following three\ncategories.\nReconstruction-based methods include denoising autoen-\ncoders (DAEs) [42], [43] and deCNNs [44], [45]. These meth-\nods project data into a low-dimensional latent space and then\nreconstruct the input from the compressed features. However,\nexact input reconstruction is often not conducive to learning\ndiscriminant features.\nGeneration-based methods include variational autoencoders\n[46], [47] and generative adversarial networks [48], [49]. These\nmethods aim to approximate the real data distribution and simul-\ntaneously learn a mapping from a latent space to the input space.\nHowever, the main goal of these methods is to generate more\nrealistic samples rather than extracting meaningful features that\nfacilitate downstream tasks.\nContext-based methodsleverage spatial, spectral, and tempo-\nral context similarity or correlation in the data to create super-\nvision signals. These types of methods are generally heuristic\nand have no uniform theoretical patterns. For example, Wang\net al. learned mapping functions between images and their\ntransformed copies for the purpose of image registration [50].\nDong et al.sampled image patches from bitemporal images and\nthen used the temporal-source of these patches as pseudolabels\nfor change detection [51]. Vincenziet al. attempted to predict\nvisible bands of remote sensing images by other spectral bands\n[52]. At present, these kinds of methods are rarely applied to the\nremote sensing data.\nIn summary, the existing work mainly focuses on learning\nspatial and spectral features from single or bitemporal images,\nwhile very little effort has been devoted to satellite time-series\nanalysis.\nIII. MOTIV ATION\nBERT, which stands for bidirectional encoder representations\nfrom transformers [39], is undoubtedly the most remarkable self-\nsupervised learning-based framework in the current NLP ﬁeld.\nFollowing an unsupervised-pretraining/supervised-ﬁne-tuning\npipeline, BERT can learn general language representations that\nare reusable in various downstream tasks. In detail, BERT uti-\nlizes a “Masked Language Model” as its pretext task, in which\nsome tokens in a sentence are randomly masked out and the\nmodel is forced to predict the missing tokens according to the\ncontext. This enables BERT to learn word correspondences from\na plain text corpus. Since BERT was proposed, the concept of\nself-supervised pretraining has started dominating the state-of-\nthe-art in NLP.\nSimilar to text, satellite time series also have strong tempo-\nral correlations. Likewise, the temporal dynamics of spectral\nproﬁles contain rich semantic information, which is closely as-\nsociated with seasonal variations and plant phenology. However,\nwhile there are clear similarities between text and satellite time\nseries, the insight of BERT has yet to be explored in the canon\nof existing work in the remote sensing community.\nInspired by BERT, we develop a context-based pretext task\nto capture meaningful spectral-temporal features from massive\nunlabeled SITS data. Speciﬁcally, the network is asked to recover\ncontaminated observations by means of corresponding acquisi-\ntion dates and clear observations. Our hypothesis is that noisy\nobservations can be distinguished and reconstructed from dense\nsatellite time series. The main idea to design this pretext task is\nbased on the fact that noise caused by clouds and shadows is com-\nmonly found in optical satellite images. Human remote sensing\nexperts can easily eliminate noise interference and distinguish\ndifferent land cover types from a limited number of images. The\nabsence of images or contaminated images does not necessarily\nresult in a decrease in the interpretation accuracy because the\nmissing information can be inferred from the remaining images.\nIntuitively, a good representation of SITS should be able to\ncapture stable spectral-temporal patterns that are robust to noise.\nIV . MODEL ARCHITECTURE\nA. Overall Network Architecture\nIn this article, we introduce a transformer-based network for\nthe SITS classiﬁcation. Since the model architecture is modiﬁed\nfrom BERT, we name it SITS-BERT.\nWe ﬁrst introduce the symbols used in our description. Let\nX= {⟨O1,t1⟩,..., ⟨O1,tL⟩} be an annual time series of a\npixel, whereOi ∈R D denotes aD-dimensional satellite obser-\nvation vector whose elements correspond to each of the input\nspectral reﬂectances;ti corresponds to the dateOi was captured,\nwhich is speciﬁed using day of year (DOY).\nSITS-BERT’s model architecture is comprised of two parts:\nan observation embedding layer and a standard transformer\nencoder (see Fig. 2). Speciﬁcally, all observation tuples\n⟨O1,t1⟩,..., ⟨O1,tL⟩of a time series are ﬁrst encoded into a\nsequence of observation embeddings, which are then fed into\na multilayer bidirectional transformer network to produce their\ncorresponding representations. The ﬁnal observation represen-\ntations can be aggregated into a single feature vector, which\ncontains the global information of the entire time series and\nis used for classiﬁcation. Here, SITS-BERT plays the role of\na representation model. By adding an additional output layer,\nthe whole network architecture of SITS-BERT can be further\nﬁne-tuned for a speciﬁc classiﬁcation task.\nB. Observation Embedding\nThe observation embedding layer projects an input tuple\n⟨Oi,ti⟩into a higher-dimensional feature space. The reason for\nthis is that the input dimension is tied with the hidden layer size\nof the transformer, and using larger embedding sizes gives better\nperformance [41].\nIn this article, an observation embedding is a concatenation\nof two parts. Given an observationOi, it is projected into a\nhigh-dimensional vector using a linear dense layer. In addition,\nthe corresponding dateti is also encoded into a vector of the\nsame size using the positional encoding (PE) technique [28]. PE\nencodes the order information of a sequence with sine/cosine\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 477\nFig. 2. Structure of the SITS-BERT architecture shown unfolded in time. Besides the output layer (not depicted in the ﬁgure), the same architecture isused in\nboth the pretraining and ﬁne-tuning stages. The input tuple⟨Oi,ti⟩at each timestep is a pair of observation and the corresponding acquisition date (day of year,\nDOY). The ﬁnal hidden representation is denoted asTi. Ti encodes the information about the entire time series.\nfunctions of different frequencies\nEmbed (Oi,ti)= Concat (OiWe, PE(ti)) (1)\nPE(ti)p =\n{\nsin\n(\nti/100002k/dm\n)\n, if p =2 k\ncos\n(\nti/100002k/dm\n)\n, if p =2 k +1 (2)\nwhere Embed(Oi,ti)represents the observation embedding of\n⟨Oi,ti⟩; dm = 256 is the embedding size, which is the same\nof the hidden layer size of transformer;Concat denotes a con-\ncatenation between two vectors;We ∈R D×dm\n2 is the weight\nmatrix of the dense layer;PE(ti)p represents the pth element\nof the PE vector. It should be noted that in BERT, the PE vector\nis directly added to a token embedding. However, we suggest\nconcatenating the two parts in case the model cannot distinguish\nbetween observations and time. Experiments also indicate that\nthe model converges faster using concatenation than summation.\nThe main purpose of utilizing time information (i.e., DOY\nrather than the order of an image in a sequence) is to make\nthe model learn meaningful temporal variation patterns that are\nclosely related to seasonal cycles and vegetation phenology.\nThe beneﬁts of this are twofold. On one hand, the model can\novercome the limitations associated with different sampling\ndates and sequence lengths, making the model trainable and\ntransferable across different years. On the other hand, the model\nis able to deal with irregular sampling problems caused by the\npresence of noise in the time series. Accordingly, no smoothing,\ngap ﬁlling, or compositing method is needed to reconstruct\nevenly spaced time series.\nC. Transformer Encoder\nTransformer was ﬁrst introduced in NLP as an efﬁcient alter-\nnative to RNNs [28], which has been introduced to some remote\nsensing tasks, such as hyperspectral image classiﬁcation [53],\nimage captioning [54], and SITS classiﬁcation [29].\nA transformer encoder is a stack of multiple transformer\nblocks. The ﬁrst transformer block takes a collection of observa-\ntion embeddings of a time series as its input and generates cor-\nresponding hidden representations. Then, these representations\nare passed to the next transformer block as its input to iteratively\nFig. 3. Illustration of the scaled dot-product attention (left) and multihead\nattention (right). Q stands for queries’ matrix; K stands for keys’ matrix; V\nstands for values’ matrix.\ngenerate higher level representations. A single transformer block\n(depicted in Fig. 2) contains two major sublayers: a multihead\nattention layer and a positionwise fully connected feed-forward\nnetwork (FFN). In addition, a residual connection [55] and\nlayer normalization [56] are utilized for both sublayers. The\ntwo sublayers are described in detail below.\nA multihead attention layer consists ofH parallel scaled dot-\nproduct attention layers, each called a head (see Fig. 3) [28].\nA scaled dot-product attention is a function that maps a query\nvector and a set of key-value pairs into an output vector\nAttention (Q,K,V )= softmax\n(QKT\n√dk\n)\nV (3)\nwhere Q,K,V represent the matrices stacked by multiple query,\nkey, and value vectors as rows, respectively;dk is the dimen-\nsionality of the query/key vectors. Multihead attention takes\nit a step further by ﬁrst mappingQ,K, and V into different\nlower-dimensional feature subspaces via different linear dense\nlayers, and then using the results to calculate attention. Finally,\nthe outputs produced byH heads are concatenated and projected\ninto a ﬁnal hidden representation using another dense layer\nMultiHead (Q,K,V )= Concat (head1,...,head H)Wo\nwhere headi = Attention\n(\nQWQ\ni ,KW K\ni ,VW V\ni\n)\n(4)\n478 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nFig. 4. Illustration of the proposed pretraining strategy. Some observations in\na satellite time series are randomly selected and added with positive or negative\nnoise, and then the model is trained to predict these contaminated observations\nusing the rest of the observations. In the ﬁgure, the input observationsO3 and\nO7 are added with noise±ε,w h e r eε ≥ 0, and the prediction loss at these two\npositions is used to optimize the model.\nwhere WQ\ni ,W K\ni ,W V\ni are weight matrices of the inner dense\nlayers of each head; Wo is the weight matrix of the top\ndense layer. In this article,WQ\ni ,W K\ni ,W V\ni ∈R dm×dv , Wo ∈\nRHdv×dm ,w h e r ew eu s eH =8 as the number of heads, and\nuse dv = dm\nH =3 2 as the output dimensionality of each head.\nIn a transformer, a self-attention mechanism is used to learn\nthe internal relationship of an input sequence [28]. In the\nself-attention mechanism, the query, key, and value vectors are\nidentical. That is, attention is calculated between each position in\na sequence and every other position (including itself). As a result,\nthe hidden representation of each observation is a weighted sum\nof all positions in a time series, thereby capturing the global\nsequence information and highlights the part around the ith\nobservation.\nA positionwise FFN is then applied to the hidden state of\neach position independently and identically. FFN is made up of\ntwo linear transformations with a ReLU activation function in\nbetween\nFFN (xi)= m a x( 0,xiW1 +b1)W2 +b2 (5)\nwhere xi is theith hidden state produced by the multihead atten-\ntion layer,W1, W2 are weight matrices, andb1, b2 are bias terms\nof the inner and output dense layers, respectively. In this article,\nwe usedff =4 dm = 1024 as the dimensionality of the inner\ndense layer, i.e.,W1 ∈R dm×dff , b1 ∈R dff , W2 ∈R dff ×dm ,\nb2 ∈R dm .\nV. PROPOSED SELF-SUPERVISED PRETRAINING SCHEME\nA. Pretraining SITS-BERT\nIn analogy to BERT, we train our model on a predesigned\npretext task: some of the input observations are randomly chosen\nand added with noise, and then the model is forced to predict\nthese contaminated observations (see Fig. 4). By solving this\npretext task, the model can learn temporal correspondences\nbetween observations.\nIn the experiments, 15% of the observations in a time series are\nselected at random and added with noise. When an observation\nOi is selected, there is a 50% chance that all elements of the\nvector is added a positive noise to simulate abnormal reﬂectance\nincreases caused by clouds and snow/ice; and a 50% chance that\nall element of the vector is subtracted from a positive noise\nto simulate abnormal reﬂectance decreases caused by shadows.\nThe noise is generated from a uniform distribution in the interval\n[0, 0.5]. In the meantime, the corresponding PE vector remains\nunchanged.\nThe model is then forced to predict the original values of\nthese contaminated observations according to their acquisition\ndates and contextual information. In detail, the ﬁnal hidden\nrepresentations corresponding to the contaminated observations\nare fed into a linear dense layer for prediction. The mean-square\nerror (MSE) between original observations and predictions is\nused as the optimization function for model training\nMSELoss =\n∑\nOj∈Ψ ∥Oj − ˆOj∥2\nN (6)\nwhere Ψ denotes the set of contaminated observations;ˆOj is\nthe predicted value ofOj; andN is the number of contaminated\nobservations. In this article, we pretrain SITS-BERT for 100\nepochs with a batch size of 256 sequences. We utilize Adam\noptimizer with a learning rate of 1e−4, learning rate warmup\nover the ﬁrst 30 epochs, and exponentially decay of the learning\nrate. Dropout is implemented on all transformer layers with a\ndropout rate of 0.1.\nAs a side beneﬁt, the proposed pretext task overcomes the\npretrain-ﬁne-tune discrepancy suffered by the original BERT\n[15]. Speciﬁcally, BERT uses a special token [MASK] to tell\nthe model which inputs are missing; this token does not exist in\nthe downstream tasks. In contrast, our model does not introduce\nartiﬁcial tokens. Our assumption is that the model can learn how\nto distinguish noise from normal observations from the context\nwithout explicit annotations.\nIt is noteworthy that the proposed pretext task is different\nfrom DAEs [57], because our aim is to predict contaminated\nobservations instead of reconstructing the entire time series.\nB. Fine-Tuning SITS-BERT\nThe pretrained SITS-BERT can be easily adjusted to a spe-\nciﬁc SITS classiﬁcation task by adding an additional output\nlayer. In this article, we ﬁne-tune SITS-BERT on the following\ntwo representative tasks, i.e., crop classiﬁcation and land cover\nmapping. For both tasks, we simply feed the input–output pairs\nto the model and ﬁne-tune all the parameters end-to-end on\ntask-related data. Speciﬁcally, the input of the model is an annual\ntime series, and the output is the corresponding category label.\nA common method to address time series classiﬁcation is to\nmap time series into a feature space, such that the values of\nfeatures should be close for pixels belonging to the same class.\nIn general, there are two strategies for aggregating individual\nobservation representations produced by SITS-BERT into a\nsingle sequence-level representation. These two approaches are\n1) the use of a [CLS] token, and 2) the pooling method (see\nFig. 5).\n1) SITS-BERT with a [CLS] token: The ﬁrst method is to\ninsert a special token [CLS] (an abbreviation for “classiﬁ-\ncation”) at the front of every input time series and then use\nthe output of [CLS] as a global representation of the whole\ntime series [39]. In our implementation, we set [CLS] to\nbe an all-ones vector with the same dimension as the input\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 479\nFig. 5. Two strategies to generate a sequence-level representation (denoted as\nC) from individual observation representations of a time series: (1) SITS-BERT\nwith a [CLS] token, (2) SITS-BERT with pooling.\nobservations, and set the corresponding time to be zero.\nThe [CLS] token also needs to be added to the input time\nseries at the pretraining stage.\n2) SITS-BERT with pooling:The second method is to apply a\npooling operation to the output vectors of BERT by either\naveraging the output (average pooling) or computing max-\nover-time of the output (max pooling) to aggregate them\ninto a single ﬁxed-size vector.\nOnce the global representation of a time series is obtained, it is\nthen fed into a softmax layer to be classiﬁed. The cross-entropy\nloss is used for the model optimization in the ﬁne-tuning stage\nCrossEntropyLoss = −\nP∑\nj =1\nyjlog(ˆyj) (7)\nwhere ˜yj is the probability score inferred by the softmax function\nfor class j, and yj is the ground truth, andP is the number of\nclasses.\nIn this article, we ﬁne-tune SITS-BERT on task-speciﬁc data\nin a supervised manner for 100 epochs. Adam optimizer is used\nwith a learning rate of 2e-4 and a batch size of 128.\nVI. STUDY AREAS AND DATASETS\nIn this article, we applied the proposed method on Sentinel-2\nimage time series. The data were organized into four datasets:\none was used for model pretraining, the other three (includ-\ning two crop classiﬁcation datasets and a land cover mapping\ndataset) were used for model evaluation.\nWe investigated the model performance on two kinds of SITS\ndata. In the ﬁrst type of data, the pretraining and ﬁne-tuning\ndata belonged to the same area, serving the purpose of verify-\ning whether the pretrained model can extract informative and\ndiscriminant features from homologous unlabeled data. In the\nsecond type of data, the pretraining and ﬁne-tuning data came\nfrom different areas, with the purpose of exploring whether the\npretrained model can be transferred to unseen data.\nA. Pretraining Dataset\nWe chose three Sentinel-2 tiles (T10SEJ, T10SFH, and\nT11SKA, each covering a region of about 100 × 100 km)\nlocated in the Central Valley, CA, USA to create a pretraining\ndataset. The location of the study area is depicted in Fig. 6. For\neach tile, all the available Sentinel-2 A/B images with<10%\ncloud coverage, taken between January 2018 and December\n2019, were used. There were a total of 219 images to construct\nthe pretraining dataset. Samples were collected from each tile\nat a regular sampling interval of 10 rows/columns. Sequences\nshorter than 10 were discarded. Finally, the pretraining dataset\nwas composed of about 6.5 million sequences.\nWe chose California as the pretraining area for the following\nreasons. First, California has extremely diverse natural land-\nscapes, representing most of the major biomes in North America,\nincluding grasslands, shrublands, deciduous forests, coniferous\nforests, tundra, mountains, deserts, rainforest, marine, estuarine,\nand freshwater habitats. The state’s varied topography and cli-\nmate have given rise to a remarkable plant diversity that cannot\nbe found in any other state of the United States [58]. Moreover,\nthe Central Valley where the study area is located is California’s\nmost productive agricultural region as well as one of the most\nproductive worldwide. The agricultural systems in this study\narea are rather complicated, characterized by small parcels and\na vast variety of crop types [59]. For all those reasons, this area\ncan provide abundant and diverse training samples for learning\ngeneral-purpose SITS representations. Finally, this area has a\nMediterranean climate with no signiﬁcant precipitation during\nthe summer season. Therefore, the spectral trajectory of major\ncrops (such as corn, cotton, rice, and soybeans) throughout the\nentire growth cycle can be characterized, making it feasible to\ninferring the corrupted part of a time series from the remainder.\nB. Two Crop Classiﬁcation Datasets\nWe applied the proposed method to two crop classiﬁcation\ndatasets.\nThe ﬁrst dataset came from the pretraining area. A total of\n45 images captured in 2019 from Sentinel-2 tile T11SKA in\nCalifornia (see Fig. 6 Tile3⃝ ) were used. Ten major crop types\ncovering most of the study area were targeted in this study: corn,\ncotton, winter wheat, alfalfa, tomatoes, grapes, citrus, almonds,\nwalnuts, and pistachios. Three nonagricultural categories were\nalso included: developed, evergreen forest, and grass/pasture.\nIt should be noted that the ﬁne-tuning data were not used for\npretraining.\nThe second dataset came from the area at the border between\nMissouri and Arkansas, USA (see Fig. 7), and was composed\nof 26 images acquired in 2019 from Sentinel-2 tile 15SY A. Ten\nagricultural and nonagricultural categories were considered in\nthis study: corn, cotton, rice, soybeans, fallow/idle cropland,\nopen water, developed, forest, grass/pasture, and woody wet-\nlands.\nFor both datasets, the 2019 Cropland Data Layer (CDL) [60]\nand the corresponding CDL conﬁdence layer [61] provided by\nthe United States Department of Agriculture National Agricul-\ntural Statistics Service (USDA NASS) were used as reference\ndata to collect samples. The original spatial resolution of CDL\nand conﬁdence layer products was 30 m. They were upsampled\nto 10 m to be consistent with Sentinel-2 images. Then, we\nextracted samples from the whole scene at a regular interval of 10\nrows/columns. To increase the accuracy of the samples collected\n480 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nFig. 6. 2019 cropland data layer (CDL) product of California, USA, which is colored in the same way as the CDL website\n(https://nassgeodata.gmu.edu/CropScape/). Three Sentinel tiles were used for model pre-training: 1⃝ T10SEJ, 2⃝ T10SFH, 3⃝ T11SKA. The coverage of\neach Sentinel-2 tile is marked with a red rectangle.\nFig. 7. Location of the second study area for crop classiﬁcation, showing the\n2019 CDL product.\nfrom the CDL, we employed the following two criteria. First,\nall the neighboring pixels along with the center pixel in a 7×7\nwindow should belong to the same category, to avoid sampling\nfrom the boundary of land parcels. Second, the classiﬁcation\nconﬁdence of all neighboring pixels in the window should be\nhigher than 80%.\nFig. 8. Location of the study area for land cover mapping, showing the\nFROM_GLC10 product.\nC. Land Cover Mapping Dataset\nWe also applied the proposed method to a land cover mapping\ndataset, which was built using 20 images taken in Beijing, China,\nin 2017 from Sentinel-2 tile 50TMK (see Fig. 8). Beijing is\ncharacterized by a monsoonal humid continental climate with\nhot humid summers and cold dry winters. The natural vegetation\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 481\nis deciduous broadleaf forest, and the dominant crop types are\nwinter wheat, corn, and soybean. There is a large divergence\nbetween the study area and the pretraining area in terms of both\ngeographical conditions and vegetation species. Therefore, this\ndataset provides a good example to explore whether the proposed\npretraining scheme is effective when the target domain and the\npretraining domain have distinct data distributions.\nThe reference data we used was a public global land cover\nproduct named FROM_GLC10 (v0.1.3), which was built on 10\nm resolution Sentinel-2 data acquired in 2017 [62]. We adopted\nthe same classiﬁcation systems of FROM_GLC10. Five majority\nland cover types were considered: cropland, forest, grassland,\nwater, and impervious area.\nD. Data Collection and Preprocessing\nAll the Sentinel-2 images (Level-1C) we used were down-\nloaded from the United States Geological Survey (USGS) Earth-\nExplorer website and preprocessed to Bottom-Of-Atmosphere\n(BOA) reﬂectance Level-2A using the Sen2Cor plugin v2.8 and\nthe Sentinel Application Platform (SNAP 7.0). The Multispec-\ntral Instrument (MSI) sensor provides 13 spectral bands, i.e.,\nfour bands at 10 m (Blue, Green, Red, NIR), six bands at 20\nm (Vegetation Red Edge 1–3, Narrow NIR, SWIR 1–2), and\nthree atmospheric bands at 60 m spatial resolution. With the\nexception of the atmospheric bands, all bands were used in\nthis article. Bands at 20 m resolution were resampled to 10 m\nvia nearest sampling. A scene classiﬁcation map was generated\nfor each image along with the Level-2A processing, which as-\nsigned pixels to clouds, cloud shadows, vegetation, soils/deserts,\nwater, snow, etc. According to the scene classiﬁcation map,\nlow-quality observations belonging to clouds (including cirrus),\ncloud-shadows, and snow were discarded when extracting the\nannual time series of each pixel. Each selected sample should\ninclude at least three clear observations. The total number of\nsamples in each evaluation dataset is displayed in Table I.\nVII. EXPERIMENTAL RESULTS\nA. Evaluation Criteria and Methods\nWe compared the performance of different evaluated algo-\nrithms with the following metrics derived from the confusion\nmatrix [18]:\n1) Overall Accuracy (OA):This metric represents the propor-\ntion of correctly classiﬁed samples in all tested samples,\nand is computed by dividing the number of correctly\nclassiﬁed samples by the total number of test samples.\n2) Kappa Coefﬁcient:This metric is considered to be a con-\nsistency measure between the classiﬁcation result and the\nground truth. Kappa coefﬁcient is generally considered\nto be more robust than OA as it takes into account the\npossibility of agreement occurred solely by chance.\n3) Average Accuracy (AA):This metric is an objective in-\ndicator of classiﬁcation accuracy on unbalanced datasets.\nAA is calculated by dividing the sum of the accuracy for\nall classes by the number of classes, where the accuracy\n(i.e., the producer’s accuracy) is the number of correctly\nTABLE I\nNUMBERS OF SAMPLES IN THETHREE FINE-TUNING DATASETS\nclassiﬁed samples divided by the number of total samples\nof each class.\nTo assess the effectiveness of the proposed network, we\ncompared it with ﬁve methods that are widely employed in SITS\nclassiﬁcation. For traditional machine learning algorithms, we\nselected SVM and RF classiﬁers. SVM is widely considered\nas a powerful technique for classiﬁcation tasks, while RF has\nsome advantages such as short training time, easy parameteri-\nzation, and high robustness to high-dimensional input features\n[13], [63]. Concerning deep learning models, we selected three\napproaches that performed well in previous SITS classiﬁcation\nstudies: CNN-1D (1-D CNN) [21], LSTM [22], [23], and bidi-\nrectional LSTM (Bi-LSTM) [64], [65]. These architectures have\nbeen proven to have advantages to capture temporal dependen-\ncies in sequential data.\n1) SVM: We used a SVM classiﬁer with radial basis function\n(RBF) kernel. The hyperplane parametersC and γ were\noptimized in a logarithmic grid from 10-2 to 102.\n2) RF: We optimized a RF classiﬁer using a varied number\nof trees in the forest. The optimal number of trees was\nselected in the set of{100, 200, 300, 400, 500}.\n3) CNN-1D: A three-layer CNN-1D network was used in the\nexperiment [21], which was formed by three convolutional\nlayers (128 units), one dense layer (256 units), and one\nsoftmax layer. The kernel size of all convolutional layers\nwas set to 5. Dropout was used after each convolutional\nlayer with a dropout rate of 0.5.\n4) LSTM: A three-layer stacked LSTM network was used,\nwhich was formed by three LSTM layer (256 units) and\n482 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\none softmax layer. Dropout was used after each LSTM\nlayer with a dropout rate of 0.5.\n5) Bi-LSTM: A three-layer stacked Bi-LSTM network was\nused [66], which was formed by three Bi-LSTM layers\n(128 units) and one softmax layer. The outputs at each\ntimestep from both forward and backward directions were\nconcatenated and used the input of the next layer. Dropout\nwas used after each layer with a dropout rate of 0.5.\nSVM and RF handle a multivariate time series as a ﬂat vector\ninformation, where the dimensionality of the input features\nshould be identical. Hence, we padded the time series to the\nmaximum length by ﬁlling in the missing observations with\nzeros. For deep learning models, we used the same observation\nembedding layer to encode the input observations. SVM and RF\nwere implemented using the Python Scikit-Learn library, while\nall deep learning methods were implemented using the Python\nPyTorch library. Experiments were carried out on a workstation\nwith an Intel Xeon Silver CPU 4210R 20-Core (2.40GHz) with\n128 GB of RAM and a NVIDIA TITAN RTX 24G GPU.\nIn order to simulate real-world scenarios, we randomly se-\nlected 100 samples per category from each dataset to create the\ntraining and validation sets, respectively, and used all the residual\nsamples for testing. The training set was used for training or\nﬁne-tuning pretrained models, and the validation set was used\nfor choosing hyper-parameters (e.g., training epochs). It is worth\nmentioning that we did not follow the standard paradigm for\nmethod evaluation, which splits all data into training-validation-\ntest set at certain rates. In our setting, the data distributions of\ntraining samples and test samples may be different. The main\npurpose of this is to verify the effectiveness of the proposed\npretraining scheme under small-sample conditions.\nB. Model Conﬁguration\nIn this section, we evaluate the performance of three SITS-\nBERT variants to derive sequence-level representations. To ver-\nify whether the pretraining scheme is effective, we also trained\neach SITS-BERT variant from scratch using only labeled data\n(we refer to such models as “non-pre-trained” models). The three\nvariants are described as follows.\n1) SITS-BERT using the [CLS] token: In this case, the ﬁne-\ntuned hidden representation of the [CLS] token was re-\ngarded as a representation of the entire input time series.\nWe refer to such competitor as “SITS-BERTCLS.”\n2) SITS-BERT using average pooling: In this case, the hidden\nrepresentations of all the observations in a time series\nwere averaged and used for classiﬁcation. We refer to such\ncompetitor as “SITS-BERTAVG.”\n3) SITS-BERT using max pooling: In this case, the maximum\nvalue for the hidden representations along the time dimen-\nsion was computed and used for classiﬁcation. We refer\nto such competitor as “SITS-BERTMAX.”\nThe classiﬁcation results are shown in Table II, and the\nconfusion matrices are given in the Supplementary Materials.\nWe observe that for all variants, the pretrained models remark-\nably outperform their randomly initialized versions (except for\nSITS-BERTAVGin the third dataset). Speciﬁcally, the proposed\nTABLE II\nCLASSIFICATION ACCURACY COMPARISON OFDIFFERENT SITS-BERT\nVARIANTS ON THETHREE DATASETS\npretraining scheme leads to an averaged accuracy increment of\n2.97%, 1.88%, 2.58% for OA, 0.034, 0.023, 0.045 for Kappa\ncoefﬁcient, and 0.030, 0.029, 0.068 for AA on each dataset, re-\nspectively. The accuracy improvement on the latter two datasets\nindicates that unsupervised pretraining does produce positive\ntransfer, that is, the representations learned from large-scale un-\nlabeled data can be transferred to new context, thereby improves\nthe performance of the model on label-scarce downstream tasks.\nWe also ﬁnd that SITS-BERTMAX exceeds other variants in\nterm of OA and Kappa coefﬁcient, while SITS-BERTCLS yields\nhigher AA than SITS-BERTMAX in the latter two datasets, show-\ning that SITS-BERTMAX is more advantageous for handling\nimbalanced classiﬁcation problems.\nC. Method Comparison\nIn this section, we compare the performance of SITS-BERT\nwith other algorithms for the SITS classiﬁcation. We used SITS-\nBERTMAX as the default model conﬁguration. The term “SITS-\nBERT” hereinafter refers to the pretrained SITS-BERTMAX.A l l\nother deep learning models were trained from scratch for 300\nepochs with a learning rate of 2e−4 and a batch size of 128.\nThe classiﬁcation accuracy assessment for the three datasets are\ndisplayed in Table III, and the confusion matrices are given in\nthe Supplementary Materials.\nA major ﬁnding is that both CNN and RNN networks yield\ncomparable or even worse results than traditional methods\n(SVM and RF), while the non-pre-trained SITS-BERT performs\nslightly better than traditional methods on the latter two datasets.\nApart from the proposed model, RF performs best overall, rank-\ning ﬁrst on the latter two datasets. This suggests that for small\ndatasets, sophisticated deep learning models have no substantial\nadvantages over traditional machine learning methods in SITS\nclassiﬁcation tasks. This is likely due to the serious overﬁt-\nting problem that occurs when training deep neural networks\non insufﬁcient labeled samples, even though transformer and\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 483\nTABLE III\nCLASSIFICATION ACCURACY COMPARISON OFDIFFERENT METHODS ON THE\nTHREE DATASETS\nCNN-1D seem to be more robust to overﬁtting compared to\nLSTM-based networks.\nIn general, SITS-BERT outperforms other competitors re-\nmarkably in terms of all indicators. Speciﬁcally, compared with\nthe RF baseline, SITS-BERT increases OA by 5.27%, 2.38%,\n3.35%, Kappa coefﬁcient by 0.061, 0.029, 0.053, and AA by\n0.056, 0.018, 0.026 on each dataset, respectively. The results\nhighlight that the risk of overﬁtting utilizing deep neural net-\nworks can be largely reduced by starting from a pretrained\nmodel. Moreover, according to the confusion matrices, SITS-\nBERT exhibits a notably lower rate of misclassiﬁcations for\ncomplicated and heterogeneous categories compared to other\nmethods. For instance, in the ﬁrst dataset, pistachios is easily\nconfused with evergreen forest; the proposed method achieves\na 20 points gain in the producer’s accuracy with respect to the\nsecond best method (SVM). In the third dataset, while cropland\nhas extremely high intraclass variability, the proposed method\nalso achieves nine points of gain with respect to the second best\nmethod (RF).\nTo better visualize the classiﬁcation results of different meth-\nods, we selected a 3000× 3000 area of interest (ROI) in each\nstudy area for testing. The classiﬁcation results are shown in\nFigs. 9–11. It can be seen that for crop classiﬁcation tasks, the\nresults obtained by SITS-BERT are more consistent with the\nreference data and the salt and pepper effect is negligible. For\nthe land cover mapping task, apart from SITS-BERT, all deep\nlearning methods exhibit a high confusion between cropland and\ngrassland. The large misclassiﬁcations of the two categories (i.e.,\ncropland and grassland) may come from errors in the reference\ndata, which underlines the fact that the pretrained model exhibits\nrelatively higher robustness to noisy labels.\nD. Effect of the Pretraining Scheme on Other Models\nThe proposed pretraining scheme may be widely effective\nfor a number of deep learning models. In this section, we\nTABLE IV\nCLASSIFICATION ACCURACY OFPRETRAINED AND NON-PRE-TRAINED\nBI-LSTM NETWORKS ON THETHREE DATASETS\nvalidate its ability on CNN-1D and Bi-LSTM. For CNN-1D, we\nutilized zero-padding to keep the sequence length unchanged.\nBoth models were pretrained on the same pretraining dataset and\nthen ﬁne-tuned on each target dataset. The same pretraining/ﬁne-\ntuning conﬁgurations used for SITS-BERT were adopted. The\naccuracy assessment is shown in Table IV, and the confusion\nmatrices are given in the Supplementary Materials.\nWe observe that for all datasets, the classiﬁcation accuracy\nof pretrained models is improved from their non-pre-trained\nversions. In particular, the pretraining scheme brings an averaged\nincrement of 2.58% OA and 5.42% OA to CNN-1D and Bi-\nLSTM, respectively. The results demonstrate that the proposed\npretraining scheme also works for CNNs and RNNs for SITS\nclassiﬁcation.\nIn addition, we observe that the performance gain of Bi-LSTM\nis larger than that of CNN-1D on all datasets, indicating that\nthe proposed pretraining scheme seems to be more effective for\ntransformers and LSTM-based networks than for CNNs. This\nmay be attributed to transformer and LSTM’s ability in capturing\nlong-term dependencies between observations. In detail, LSTM\nencodes the whole time series by receiving each observation\nonce a timestep, and utilizes internal gates to control the update\nof the memory content. Transformer reads the entire time series\nat once and calculates dependencies between each observation\nand all other observations through the self-attention mechanism.\nIn contrast, CNN can only capture observation dependencies\nwithin the width of its ﬁlters. In other words, it can only\nmodel contextual information within a local neighborhood. This\ncharacter makes it inferior to transformers and LSTM-based\nnetworks for temporal feature extraction.\nE. Inﬂuence of the Number of Labeled Samples\nIt is well known that the number of training samples can\naffect classiﬁcation performance, but its inﬂuence on pretrained\nmodels is unknown. In this section, we ﬁne-tuned the pretrained\nCNN-1D, Bi-LSTM, and SITS-BERT with different numbers of\nlabeled samples, varied from 50 to 500 per category, and took RF\n(300 trees) as a baseline for comparison. The results are depicted\nin Fig. 12.\nWe observe that all the pretrained deep learning models sig-\nniﬁcantly outperform RF and the non-pre-trained model, among\n484 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nFig. 9. Crop classiﬁcation results of different methods in the ﬁrst study area (California). (1) Ground truth. (2) SVM. (3) Random forest (RF). (4) Long-short\nterm memory (LSTM). (5) Bidirectional LSTM (Bi-LSTM). (6) Non-pre-trained SITS-BERT. (7) SITS-BERT.\nFig. 10. Crop classiﬁcation results of different methods in the second study area (Missouri). (1) Ground truth. (2) SVM. (3) RF. (4) LSTM. (5) Bi-LSTM.(6)\nNon-pre-trained SITS-BERT. (7) SITS-BERT.\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 485\nFig. 11. Land cover classiﬁcation results of different methods in the third study area (Beijing). (1) Ground truth. (2) SVM. (3) RF. (4) LSTM. (5) Bi-LSTM. (6)\nNon-pre-trained SITS-BERT. (7) SITS-BERT.\nFig. 12. Classiﬁcation accuracy of SITS-BERT and the RF baseline using\nvaried training samples. Evaluation metrics in each row from left to right: Overall\naccuracy (OA), Kappa coefﬁcient, and average accuracy (AA).\nwhich SITS-BERT surpasses all other competitors in all datasets.\nIt is worth noting that the accuracy of SITS-BERT ﬁne-tuned\non only 50 labeled samples per category is comparable with\nthe accuracy of RF trained on ten times more samples for the\nﬁrst and second datasets. Speciﬁcally, the best OA yielded by\nRF are 93.33% and 97.58% on the ﬁrst and second dataset,\nrespectively, while the worst OA yielded by SITS-BERT are\n93.26% and 98.24%. Two main conclusions can be drawn: ﬁrst,\nthe experiments conﬁrm and clarify the effectiveness of self-\nsupervised pretraining using varied numbers of labeled samples;\nand second, the transformer architecture has advantages over\nCNNs and RNNs for satellite time series classiﬁcation.\nIn contrast, although the non-pre-trained SITS-BERT per-\nforms slightly better than RF on the ﬁrst two datasets, it performs\nmuch worse on the third dataset. In addition, it is not guaranteed\nthat the accuracy of non-pre-trained SITS-BERT will improve\nas the number of training samples increases, indicating that\nrandomly initialized deep neural networks give much more\nunstable results.\nF . Computational Efﬁciency\nIn practice, SITS classiﬁcation methods need to be applied\nto millions of satellite time series. Therefore, the computational\nefﬁciency of deep learning models should be taken into consid-\neration. In this section, we compare the computation speeds of\nseveral deep learning models. The results are given in Table V.\nAccording to the results, CNN-1D is the fastest, followed\nby LSTM-based models, and SITS-BERT is the slowest on a\nsingle GPU. Speciﬁcally, the computation speed of CNN-1D is\nabout 19% faster than SITS-BERT. For Bi-LSTM, which has\nshown competitive performance in our previous experiments,\nit is also 5% faster than SITS-BERT. Fortunately, an advantage\nof transformer is that the computation can be parallelized\n486 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 14, 2021\nTABLE V\nCOMPUTATIONSPEED (TIME SERIES PER SECOND) OF DIFFERENT METHODS\non multiple GPUs. Thanks to the self-attention mechanism\nin transformers, the attention weights for all positions in a\nsequence can be calculated simultaneously, whereas a recurrent\nlayer should be computed successively after the output of the\nprevious layer is obtained [28]. Therefore, SITS-BERT is better\nsuited for massive parallel computations on modern machine\nlearning acceleration hardware.\nVIII. CONCLUSION\nIn this article, we proposed a simple but powerful self-\nsupervised pretraining scheme for the SITS classiﬁcation. The\naim was to leverage large volumes of unlabeled SITS data\nto learn robust and transferable spectral-temporal features to\nfacilitate label-scarce downstream tasks. We also introduced\na transformer-based neural network architecture, named SITS-\nBERT, for the SITS classiﬁcation.\nThe evaluation on three benchmark datasets have revealed\nthat the proposed pretraining scheme is generally effective\nfor several deep learning models, i.e., CNN, Bi-LSTM, and\ntransformer. The experiments support our hypothesis that the\nrepresentations learned from large-scale unlabeled data can\nproduce positive transfer, thereby improves the performance\nand generalization ability of deep learning models on a speciﬁc\nSITS classiﬁcation task. In addition, the results also demonstrate\nthat the transformer network exceeds CNNs and RNNs for\nSITS classiﬁcation.\nWe would like to emphasize that the pretraining data we used\nin this article are far from enough to fully exploit the potential\nof transformers. Nevertheless, the proposed approach is easily\nscaled to far larger datasets with very low costs, since no human-\nannotated labels are required during the pretraining stage.\nSince the main purpose of this article was to validate the\neffectiveness of self-supervised learning for SITS, we put em-\nphasis on the characteristics of spectral proﬁles of individual\npixels while ignored the spatial correlation information between\npixels. In the future, it will be of interest to incorporate spatial\ninformation into our framework to learn more discriminant\nfeatures.\nREFERENCES\n[1] D. Ienco, R. Interdonato, R. Gaetano, and M. D. H.. Tong, “Combining\nSentinel-1 and Sentinel-2 satellite image time series for land cover map-\nping via a multi-source deep learning architecture,”ISPRS J. Photogramm.\nRemote Sens., vol. 158, pp. 11–22, Dec. 2019.\n[2] P. Jonsson and L. Eklundh, “Seasonality extraction by function ﬁtting to\ntime-series of satellite sensor data,”IEEE Trans. Geosci. Remote Sens.,\nvol. 40, no. 8, pp. 1824–1832, Aug. 2002.\n[3] C. Gómez, J. C. White, and M. A. Wulder, “Optical remotely sensed time\nseries data for land cover classiﬁcation: A review,”ISPRS J. Photogramm.\nRemote Sens., vol. 116, pp. 55–72, 2016.\n[4] S. Rapinel, C. Mony, L. Lecoq, B. Clement, A. Thomas, and L. Hubert-\nMoy, “Evaluation of Sentinel-2 time-series for mapping ﬂoodplain grass-\nland plant communities,”Remote Sens. Environ., vol. 223, pp. 115–129,\nMar. 15 2019.\n[5] M. J. Lambert, P. C. S. Traore, X. Blaes, P. Baret, and P. Defourny, “Esti-\nmating smallholder crops production at village level from Sentinel-2 time\nseries in Mali’s cotton belt,”Remote Sens. Environ., vol. 216, pp. 647–657,\nOct. 2018.\n[6] E. Grabska, P. Hostert, D. Pﬂugmacher, and K. Ostapowicz, “Forest stand\nspecies mapping using the Sentinel-2 time series,”Remote Sens., vol. 11,\nno. 10, May 2019, Art. no. 1197.\n[7] J. Inglada, A. Vincent, M. Arias, B. Tardy, D. Morin, and I. Rodes,\n“Operational high resolution land cover map production at the country\nscale using satellite image time series,” Remote Sens., vol. 9, no. 1,\nJan. 2017, Art. no. 95.\n[8] W. T. Yang, Y . Q. Wang, S. Sun, Y . J. Wang, and C. Ma, “Using Sentinel-2\ntime series to detect slope movement before the Jinsha river landslide,”\nLandslides, vol. 16, no. 7, pp. 1313–1324, Jul. 2019.\n[9] M. Sudmanns, D. Tiede, L. Wendt, and A. Baraldi, “Automatic ex-post\nﬂood assessment using long time series of optical earth observation im-\nages,” GI_Forum., vol. 1, pp. 217–227, 2017.\n[10] Y . Yuanet al., “A new framework for modelling and monitoring the\nconversion of cultivated land to built-up land based on a hierarchical\nhidden semi-Markov model using satellite image time series,”Remote\nSens., vol. 11, no. 2, Jan. 2019, Art. no. 210.\n[11] M. Immitzer, F. Vuolo, and C. Atzberger, “First experience with Sentinel-2\ndata for crop and tree species classiﬁcations in central Europe,”Remote\nSens., vol. 8, no. 3, Mar. 2016, Art. no. 166.\n[12] E. Kamir, F. Waldner, and Z. Hochman, “Estimating wheat yields in Aus-\ntralia using climate records, satellite image time series and machine learn-\ning methods,”ISPRS J. Photogramm. Remote Sens., vol. 160, pp. 124–135,\nFeb. 2020.\n[13] C. Pelletier, S. Valero, J. Inglada, N. Champion, and G. Dedieu, “Assessing\nthe robustness of random forests to map land cover with high resolution\nsatellite image time series over large areas,” Remote Sens. Environ.,\nvol. 187, pp. 156–168, 2016.\n[14] Q. Hu et al., “How do temporal and spectral features matter in crop clas-\nsiﬁcation in Heilongjiang province, China?,”J. Integrative Agriculture,\nvol. 16, no. 2, pp. 324–336, 2017.\n[15] L. H. Nguyen, D. R. Joshi, D. E. Clay, and G. M. Henebry, “Characterizing\nland cover/land use from multiple years of landsat and MODIS time\nseries: A novel approach using land surface phenology modeling and\nrandom forest classiﬁer,”Remote Sens. Environ., vol. 238, Mar. 2020, Art.\nno. 111017.\n[16] R. Hang, Q. Liu, D. Hong, and P. Ghamisi, “Cascaded recurrent neural\nnetworks for hyperspectral image classiﬁcation,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 8, pp. 5384–5394, 2019.\n[17] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph con-\nvolutional networks for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., to be published, doi:10.1109/TGRS.2020.3015157.\n[18] D. Hong et al., “More diverse means better: Multimodal deep learning\nmeets remote-sensing imagery classiﬁcation,”IEEE Trans. Geosci. Re-\nmote Sens., to be published, doi:10.1109/TGRS.2020.3016820.\n[19] L. H. Zhong, L. N. Hu, and H. Zhou, “Deep learning based multi-temporal\ncrop classiﬁcation,” Remote Sens. Environ. , vol. 221, pp. 430–443,\nFeb. 2019.\n[20] N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov, “Deep learning\nclassiﬁcation of land cover and crop types using remote sensing data,”\nIEEE Geosci. Remote Sens. Lett., vol. 14, no. 5, pp. 778–782, May 2017.\n[21] C. Pelletier, G. I. Webb, and F. Petitjean, “Temporal convolutional neural\nnetwork for the classiﬁcation of satellite image time series,”Remote Sens.,\nvol. 11, no. 5, Mar. 2019, Art. no. 523.\n[22] D. Ienco, R. Gaetano, C. Dupaquier, and P. Maurel, “Land cover clas-\nsiﬁcation via multitemporal spatial data by deep recurrent neural net-\nworks,”IEEE Geosci. Remote Sens. Lett., vol. 14, no. 10, pp. 1685–1689,\nOct. 2017.\n[23] M. Rußwurm and M. Korner, “Temporal vegetation modelling using\nlong short-term memory networks for crop identiﬁcation from medium-\nresolution multi-spectral satellite images,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jul 2017, pp. 11–19.\n[24] P. Benedetti, D. Ienco, R. Gaetano, K. Ose, R. G. Pensa, and S. Dupuy,\n“M3Fusion: A deep learning architecture for multiscale multimodal mul-\ntitemporal satellite data fusion,”IEEE J. Sel. Topics Appl. Earth Observ.\nRemote Sens., vol. 11, no. 12, pp. 4939–4949, 2018.\n[25] R. Interdonato, D. Ienco, R. Gaetano, and K. Ose, “DuPLO: A DUal view\npoint deep learning architecture for time series classiﬁcation,”ISPRS J.\nPhotogramm. Remote Sens., vol. 149, pp. 91–104, Mar. 2019.\nYUAN AND LIN: SELF-SUPERVISED PRETRAINING OF TRANSFORMERS FOR SATELLITE IMAGE TIME SERIES CLASSIFICATION 487\n[26] M. Rußwurm and M. Körner, “Multi-temporal land cover classiﬁcation\nwith sequential recurrent encoders,”ISPRS Int. J. Geoinf., vol. 7, no. 4,\n2018, Art. no. 129.\n[27] V . S. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Time-space\ntradeoff in deep learning models for crop classiﬁcation on satellite multi-\nspectral image time series,” inProc. IEEE Int. Geosci. Remote Sens. Symp.,\nJul. 2019, pp. 6247–6250.\n[28] A. Vaswaniet al., “Attention is all you need,” inProc. Neural Inf. Process.\nSyst., Dec. 2017, pp. 5998–6008.\n[29] V . S. F. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite\nimage time series classiﬁcation with pixel-set encoders and temporal\nself-attention,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,J u n\n2020, pp. 12325–12334.\n[30] M. Rußwurm and M. Körner, “Self-attention for raw optical satellite\ntime series classiﬁcation,” 2019. [Online]. Available: https://arxiv.org/abs/\n1910.10536\n[31] H. Bazzi, D. Ienco, N. Baghdadi, M. Zribi, and V . Demarez, “Distilling\nbefore reﬁne: Spatio-temporal transfer learning for mapping irrigated areas\nusing Sentinel-1 time series,”IEEE Geosci. Remote Sens. Lett., vol. 17,\nno. 11, pp. 1909–1913, Nov. 2020.\n[32] D. Ienco, Y . J. Eudes Gbodjo, R. Interdonato, and R. Gaetano, “Attentive\nweakly supervised land cover mapping for object-based satellite image\ntime series data with spatial interpretation,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2004.14672\n[33] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y . Ng, “Self-taught learning:\nTransfer learning from unlabeled data,” inProc. IEEE Int. Conf. Mach.\nLearn., Jun. 2007, pp. 759–766.\n[34] L. Jing and Y . Tian, “Self-supervised visual feature learning with deep\nneural networks: A survey,”IEEE Trans. Pattern Anal. Mach. Intell.,t o\nbe published, doi:10.1109/TPAMI.2020.2992393.\n[35] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” inProc.\nEur . Conf. Comput. Vis., Oct 2016, pp. 649–666.\n[36] M. Noroozi and P. Favaro, “Unsupervised learning of visual representa-\ntions by solving jigsaw puzzles,” inProc. Eur . Conf. Comput. Vis., Oct.\n2016, pp. 69–84.\n[37] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for\nunsupervised learning of visual features,” inProc. Eur . Conf. Comput. Vis.,\nSep. 2018, pp. 132–149.\n[38] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for\ncontrastive learning of visual representations,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2002.05709\n[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” 2018.\n[Online]. Available: https://arxiv.org/abs/1810.04805\n[40] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “XLNet: Generalized autoregressive pretraining for lan-\nguage understanding,” in Proc. Neural Inf. Process. Syst., Dec. 2019,\npp. 5753–5763.\n[41] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“ALBERT: A lite BERT for self-supervised learning of language repre-\nsentations,” 2019. [Online]. Available: https://arxiv.org/abs/1909.11942\n[42] A. Elshamli, G. W. Taylor, A. Berg, and S. Areibi, “Domain adaptation\nusing representation learning for the classiﬁcation of remote sensing\nimages,” IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., vol. 10,\nno. 9, pp. 4198–4209, Sep. 2017.\n[43] X. Tang, X. Zhang, F. Liu, and L. Jiao, “Unsupervised deep feature\nlearning for remote sensing image retrieval,”Remote Sens., vol. 10, no. 8,\nAug. 2018, Art. no. 1243.\n[44] Y . T. Tao, M. Z. Xu, F. Zhang, B. Du, and L. P. Zhang, “Unsupervised-\nrestricted deconvolutional neural network for very high resolution remote-\nsensing image classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 55,\nno. 12, pp. 6805–6823, Dec. 2017.\n[45] X. Q. Lu, X. T. Zheng, and Y . Yuan, “Remote sensing scene classiﬁcation\nby unsupervised representation learning,”IEEE Trans. Geosci. Remote\nSens., vol. 55, no. 9, pp. 5148–5157, Sep. 2017.\n[46] X. Wang, K. Tan, Q. Du, Y . Chen, and P. J. Du, “CV A2E: A conditional\nvariational autoencoder with an adversarial training process for hyperspec-\ntral imagery classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 8, pp. 5676–5692, Aug. 2020.\n[47] X. Ma, Y . Lin, Z. Nie, and H. Ma, “Structural damage identiﬁcation\nbased on unsupervised feature-extraction via variational Auto-encoder,”\nMeasurement, vol. 160, Aug 2020, Art. no. 107811.\n[48] D. Y . Lin, K. Fu, Y . Wang, G. L. Xu, and X. Sun, “MARTA GANs:\nUnsupervised representation learning for remote sensing image classiﬁ-\ncation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 11, pp. 2092–2096,\nNov. 2017.\n[49] R. Hang, F. Zhou, Q. Liu, and P. Ghamisi, “Classiﬁcation of hyperspec-\ntral images via multitask generative adversarial networks,”IEEE Trans.\nGeosci. Remote Sens., to be published.\n[50] S. Wang, D. Quan, X. Liang, M. Ning, Y . Guo, and L. Jiao, “A deep learning\nframework for remote sensing image registration,”ISPRS J. Photogramm.\nRemote Sens., vol. 145, pp. 148–164, 2018.\n[51] H. Dong, W. Ma, Y . Wu, J. Zhang, and L. Jiao, “Self-supervised rep-\nresentation learning for remote sensing image change detection based\non temporal prediction,”Remote Sens., vol. 12, no. 11, Jun. 2020, Art.\nno. 1868.\n[52] S. Vincenzi et al., “The color out of space: Learning self-supervised\nrepresentations for earth observation imagery,” 2020. [Online]. Available:\nhttps://arxiv.org/abs/2006.12119\n[53] J. He, L. Zhao, H. Yang, M. Zhang, and W. Li, “HSI-BERT: Hyperspec-\ntral image classiﬁcation using the bidirectional encoder representation\nfrom transformers,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 1,\npp. 165–178, Jan. 2020.\n[54] X. Shen, B. Liu, Y . Zhou, J. Zhao, and M. Liu, “Remote sensing image cap-\ntioning via variational autoencoder and reinforcement learning,”Knowl.\nBased Syst., vol. 203, 2020, Art. no. 105920.\n[55] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual\nnetworks,” 2016. [Online]. Available: https://arxiv.org/abs/1603.05027\n[56] J. Lei Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” 2016.\n[Online]. Available: https://arxiv.org/abs/1607.06450\n[57] P. Vincent, H. Larochelle, Y . Bengio, and P.-A. Manzagol, “Extracting and\ncomposing robust features with denoising autoencoders,” inProc. IEEE\nInt. Conf. Mach. Learn., Jul. 2008, pp. 1096–1103.\n[58] B. G. Baldwin, “Origins of plant diversity in the California ﬂoristic\nprovince,”Annu. Rev. Ecol. Evol. Syst., vol. 45, no. 1, pp. 347–369, 2014.\n[59] L. Zhong, T. Hawkins, G. Biging, and P. Gong, “A phenology-based\napproach to map crop types in the San Joaquin Valley, California,”Int.\nJ. Remote Sens., vol. 32, no. 22, pp. 7777–7804, Nov 2011.\n[60] USDA National Agricultural Statistics Service Cropland Data Layer. 2019.\nPublished crop-speciﬁc data layer. USDA-NASS, Washington, DC, USA.\nAccessed May 2020. [Online]. Available: https://nassgeodata.gmu.edu/\nCropScape/\n[61] W. Liu, S. Gopal, and E. F. Wood, “Uncertainty and conﬁdence in land\ncover classiﬁcation using a hybrid classiﬁer approach,”Photogramm. Eng.\nRemote Sens., vol. 70, no. 8, pp. 963–971, Aug. 2004.\n[62] P. Gong et al., “Stable classiﬁcation with limited sample: Transferring a\n30-m resolution sample set collected in 2015 to mapping 10-m resolution\nglobal land cover in 2017,”Sci. Bull., vol. 64, no. 6, pp. 370–373, 2019.\n[63] A. Chakhar, D. Ortega-Terol, D. Hernandez-Lopez, R. Ballesteros, J. E.\nOrtega, and M. A. Moreno, “Assessing the accuracy of multiple classiﬁ-\ncation algorithms for crop classiﬁcation using Landsat-8 and Sentinel-2\ndata,” Remote Sens., vol. 12, no. 11, Jun. 2020, Art. no. 1735.\n[64] L. Lin, “Satellite image time series classiﬁcation and change detection\nbased on recurrent neural network model,”Doctor Eng. China, Univ.\nChinese Acad. Sci., 2018.\n[65] H. Wang, X. Zhao, X. Zhang, D. Wu, and X. Du, “Long time series land\ncover classiﬁcation in China from 1982 to 2015 based on Bi-LSTM deep\nlearning,” Remote Sens., vol. 11, no. 14, 2019, Art. no. 1639.\n[66] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”\nIEEE Trans. Signal Process., vol. 45, no. 11, pp. 2673–2681, Nov 1997.\nYuan Yuan (Member, IEEE) received the B.S. de-\ngree in geography from Nanjing University, Nanjing,\nChina, in 2011 and the Ph.D. degree in signal and in-\nformation processing from Institute of Remote Sens-\ning and Digital Earth, Chinese Academy of Sciences,\nBeijing, China, in 2016.\nShe is currently working at Nanjing University of\nPosts and Telecommunications, Department of Sur-\nveying and Geoinformatics. Her research interests\ninclude remote sensing time series analysis, change\ndetection, and deep learning.\nLei Lin received the B.S. degree in geographic in-\nformation system from Shandong Agricultural Uni-\nversity, Taian, China, in 2013, the Ph.D. degree in\nsignal and information processing from Institute of\nRemote Sensing and Digital Earth, Chinese Academy\nof Sciences, Beijing, China, in 2018.\nHe is currently working at Qihoo Technology Cor-\nporation. His research interests include time series\nanalysis and natural language processing.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8485573530197144
    },
    {
      "name": "Computer science",
      "score": 0.8276281356811523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6600561738014221
    },
    {
      "name": "Machine learning",
      "score": 0.6059748530387878
    },
    {
      "name": "Transformer",
      "score": 0.5410453677177429
    },
    {
      "name": "Deep learning",
      "score": 0.5276885032653809
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5214704275131226
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5025568008422852
    },
    {
      "name": "Contextual image classification",
      "score": 0.43974217772483826
    },
    {
      "name": "Artificial neural network",
      "score": 0.400951623916626
    },
    {
      "name": "Data mining",
      "score": 0.38109496235847473
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3760492205619812
    },
    {
      "name": "Image (mathematics)",
      "score": 0.11306917667388916
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I41198531",
      "name": "Nanjing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210092299",
      "name": "Qihoo 360 (China)",
      "country": "CN"
    }
  ],
  "cited_by": 156
}