{
  "title": "Transfer learning enables the molecular transformer to predict regio- and stereoselective reactions on carbohydrates",
  "url": "https://openalex.org/W3088999551",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3033180697",
      "name": "Giorgio Pesciullesi",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "IBM Research - Zurich",
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2209038682",
      "name": "Teodoro Laino",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4223443570",
      "name": "Jean-Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A3033180697",
      "name": "Giorgio Pesciullesi",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "University of Bern",
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2209038682",
      "name": "Teodoro Laino",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4223443570",
      "name": "Jean-Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2041548070",
    "https://openalex.org/W2789366878",
    "https://openalex.org/W2801991413",
    "https://openalex.org/W2121776789",
    "https://openalex.org/W2083415705",
    "https://openalex.org/W3010488723",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2173027866",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2769423117",
    "https://openalex.org/W2963396480",
    "https://openalex.org/W2903262661",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W2998659621",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W4249735123",
    "https://openalex.org/W2991508457",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3157889448",
    "https://openalex.org/W2130301062",
    "https://openalex.org/W2810569520",
    "https://openalex.org/W3000299023",
    "https://openalex.org/W2738723832",
    "https://openalex.org/W2948937493",
    "https://openalex.org/W2026939769",
    "https://openalex.org/W2896809423",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W2790756470",
    "https://openalex.org/W3005353977",
    "https://openalex.org/W2995491240",
    "https://openalex.org/W2909063104",
    "https://openalex.org/W2588160455",
    "https://openalex.org/W3008857498",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2980338962",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2999958881",
    "https://openalex.org/W2924217050",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2753921001",
    "https://openalex.org/W2479183793",
    "https://openalex.org/W2954088480",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W29374554",
    "https://openalex.org/W2962974709",
    "https://openalex.org/W3105982350",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2949888546"
  ],
  "abstract": null,
  "full_text": "ARTICLE\nTransfer learning enables the molecular\ntransformer to predict regio- and stereoselective\nreactions on carbohydrates\nGiorgio Pesciullesi 1,3, Philippe Schwaller1,2,3, Teodoro Laino2 & Jean-Louis Reymond 1✉\nOrganic synthesis methodology enables the synthesis of complex molecules and materials\nused in all ﬁelds of science and technology and represents a vast body of accumulated\nknowledge optimally suited for deep learning. While most organic reactions involve distinct\nfunctional groups and can readily be learned by deep learning models and chemists alike,\nregio- and stereoselective transformations are more challenging because their outcome also\ndepends on functional group surroundings. Here, we challenge the Molecular Transformer\nmodel to predict reactions on carbohydrates where regio- and stereoselectivity are notor-\niously difﬁcult to predict. We show that transfer learning of the general patent reaction model\nwith a small set of carbohydrate reactions produces a specialized model returning predictions\nfor carbohydrate reactions with remarkable accuracy. We validate these predictions\nexperimentally with the synthesis of a lipid-linked oligosaccharide involving regioselective\nprotections and stereoselective glycosylations. The transfer learning approach should be\napplicable to any reaction class of interest.\nhttps://doi.org/10.1038/s41467-020-18671-7 OPEN\n1 Department of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012 Bern, Switzerland.2 IBM Research—Europe, Säumerstrasse 4, 8803\nRüschlikon, Switzerland. 3These authors contributed equally: Giorgio Pesciullesi, Philippe Schwaller.✉email: jean-louis.reymond@dcb.unibe.ch\nNATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications 1\n1234567890():,;\nO\nrganic synthesis is a complex problem-solving task in\nwhich the vast knowledge accumulated in the ﬁeld of\norganic chemistry is used to create new molecules,\nstarting from simple commercially available building blocks 1.\nBecause of its complexity, organic synthesis is believed to be one\nof the main bottlenecks in pharmaceutical research and devel-\nopment2, and having accurate models to predict reaction out-\ncome could boost chemists’ productivity by reducing the number\nof experiments to perform.\nMachine learning has long been present in the chemical\ndomain, tackling challenges than range, for example for quanti-\ntative structure– activity relationship predictions3, virtual screen-\ning4 and quantum chemistry5,6. Enabled by algorithmic advances\nin deep learning7– 10 and the availability of large reaction data\nsets11,12, reaction prediction methods have emerged in recent\nyears13– 22. Those reaction prediction methods can be divided into\ntwo categories23, bond change prediction methods using graph\nneural networks14,16– 18,22 and product SMILES generation using\nsequence-2-sequence models15,19.\nReaction prediction tasks are typically evaluated on the USP-\nTO_MIT benchmark14, which does not contain molecules with\ndeﬁned stereocenters. Currently, the best prediction algorithm in\nterms of performance is the Molecular Transformer10,19. The\narchitecture is based on the ground-breaking work by Vaswani\net al.10, which revolutionised theﬁeld of neural machine trans-\nlation, where sentences in one language are translated into\nanother language. In contrast, for reaction prediction, the model\nlearns to translate the precursors ’ Simpliﬁed molecular-input\nline-entry system (SMILES)24 representation into the product\nSMILES.\nThe Molecular Transformer can be accessed for free through\nthe IBM RXN for Chemistry platform25. Compared to other\nmethods, such as graph neural networks-based ones, the advan-\ntages of the Molecular Transformer approaches are that they do\nnot require mapping between the product and reactant atoms in\nthe training26 and inputs can contain stereochemistry. In fact,\nsequence-2-sequence approaches, like the Molecular Transfor-\nmer\n10,19, are currently the only large-scale reaction prediction\napproaches capable of handling stereochemistry. Stereochemistry\nis systematically avoided in graph-based methods, as the con-\nnection table and adjacency matrix of two stereoisomers is\nidentical. Although stereoselectivity can theoretically be predicted\nby the Molecular Transformers19, it is one of their most sig-\nniﬁcant weaknesses because of the lack of clean training data.\nTo date, their performance on predicting speciﬁc stereochemical\nreactions has not been investigated.\nIn this work, we investigate the adaptation of the Molecular\nTransformer to correctly predict regio- and stereoselective reac-\ntions. As study case we focus on carbohydrates, a class of mole-\ncules for which the stereochemistry and the high degree of\nfunctionalization are key reactivity factors. Carbohydrate chem-\nistry is essential for accessing complex glycans that are used as\ntool compounds to investigate fundamental biological processes\nsuch as protein glycosylation27– 29, as well as for the preparation\nof synthetic vaccines30– 32. Predicting the outcome of carbohy-\ndrate transformations, such as regioselective protection/depro-\ntection of multiple hydroxyl groups or the stereospeciﬁcity of\nglycosylation reactions, is a very difﬁcult task even for experi-\nenced carbohydrate chemists 33,34, implying that this ﬁeld of\nresearch might particularly beneﬁt from computer-assisted reac-\ntion prediction tools.\nFirst, we investigate transfer learning with a specialized subset\nof reactions as a means to adapt the Molecular Transformer to\nachieve high performance on carbohydrate reactions. Transfer\nlearning, where a model is trained on a task with abundant data\nand either simultaneously trained or subsequentlyﬁne-tuned on\nanother task with less data available35, has recently led to sig-\nniﬁcant advancements in Natural Language Processing36– 39. For\ninstance, it has been used to improve translation performance in\nlow-resource languages36. More recently, unsupervised pretrain-\ning transfer learning strategies have successfully been applied to\nsequence-2-sequence models 37,40. In the chemical domain,\ntransfer learning has enabled the development of accurate neural\nnetwork potential for quantum mechanical calculations41 and\nshows great potential to solve other challenges42. For transfer\nlearning we use a set of 20k carbohydrate reactions from the\nliterature, comprising protection/deprotection and glycosylation\nsequences. We explore multitask learning, as well as sequential\ntransfer learning, and show that the adapted model, called the\nCarbohydrate Transformer, performs signiﬁcantly better than the\ngeneral model on carbohydrate transformations and a model\ntrained on carbohydrate reactions only.\nSecond, we perform a detailed experimental assessment of the\ndeep learning reaction prediction model and test the Carbohy-\ndrate Transformer on unpublished reactions. Our assessment\nconsists of a 14-step total synthesis of a modiﬁed substrate of a\neukaryotic oligosaccharil transferase (OST). We also challenge\nour Carbohydrate Transformer to predict the reactions from the\nrecently published total syntheses of the trisaccharide ofPseu-\ndomonas aeruginosa and Staphylococcus aureus43 as a further\nassessment on more complex carbohydrate reactions. Those\nreactions would be considered challenging to predict, even for\ncarbohydrate experts.\nOverall, we observe a consistent top-1 prediction accuracy\nabove 70%, which roughly means a 30% increase compared to the\noriginal Molecular Transformer baseline. Weﬁnd that the con-\nﬁdence score is a good predictor of prediction reliability and that\nmany wrong predictions have chemical reasons such as the lack\nof reagent stoichiometry in the training data. The approach we\nused to learn carbohydrate reactions could be applied to any\nreaction class. Hence, it is expected to have a signiﬁcant impact\non the ﬁeld of organic synthesis, as models like the Molecular\nTransformer19 can easily be specialized for the reaction subspaces\nthat individual chemists are most interest in.\nResults\nData availability scenarios. Besides the additional complexity,\nthe main challenges for learning to predict stereochemical reac-\ntions is the data. In the largest open-source reaction data set by\nLowe11,12, which fueled the recent advancements in machine\nlearning for chemical reaction prediction, stereochemistry, and\nspeciﬁcally reactions involving carbohydrates are under-\nrepresented and of poor quality. Hence, those reactions are\nproblematic to learn.\nIn this work, we explore two real world scenarios, where there\nexist a large data set of generic chemical reactions and a small\ndata set of complex and speciﬁc reactions. In our case, we use a\ndata set derived from the US patent reactions by Lowe12 as the\nlarge data set containing 1.1M reactions. We call this data set\nUSPTO. For the speci ﬁc reaction, we chose carbohydrates\nreactions, but the methods described could be applied to any\nreaction class of interest. We manually extracted reactions from\nthe Reaxys44 database, selected from papers of 26 authors in the\nﬁeld of carbohydrate chemistry. The small data set of 25k\nreactions will be referred to as CARBO for the remainder of the\npublication. We split the USPTO and the CARBO data set into\ntrain, validation and test sets. The reaction data was canonicalised\nusing RDKit45. A more detailed description of the data is found in\nSupplementary Note 1.\nIf the access to the large and small sets is given, the two data\nsets can be used simultaneously for training. We call thisﬁrst\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7\n2 NATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications\nscenario multitask. However, depending on the situation, direct\naccess to the data of the generic data set may not be possible. For\nexample, a company A may have proprietary reaction data\nprecluded from external sharings. Company A could still train a\nmodel using their own data and share their model without\nrevealing the exact data points. The trained model extracts some\ngeneral chemical reactivity knowledge and could be shared\nwithout exposing company proprietary information. This pre-\ntrained model could then serve as a starting point to further train\nthe model on another source of reactions. We call this scenario\nﬁne-tuning.\nA visualisation of the model and the two scenarios can be\nfound in Fig.1.\nIn the multitask scenario, we investigated different reaction\nweighting schemes between the two sets. A comparison of the\ntop-1 accuracies on the USPTO train, USPTO test, CARBO train\nand CARBO test sets for models trained with different weights for\nthe USPTO train and CARBO train sets are shown in Fig.2a. The\nweights describe in what proportion reactions from the two sets\nare shown per training batch. For example, weight 1 on USPTO\nand weight 1 on CARBO means that for one USPTO reaction one\nCARBO reaction is shown. As can be seen in the Figure, the\nhighest accuracy on the CARBO test set (71.2 %) is obtained with\nweight 9 on the USPTO set and weight 1 on the CARBO set\n(w9w1). As expected, training only with the CARBO train set\nleads to a poor CARBO test set accuracy (30.4%). As 20k\nreactions are not enough for the model to learn predict organic\nchemistry. The accuracy reached by the model trained purely on\nthe USPTO data reaches 43.3%. It therefore performs better than\nthe model trained purely on the CARBO reactions. In Fig.2b, we\nassess the effect of the size of the CARBO train set. The accuracy\ncontinuously increases from 43.3 to 71.2% with an increasing\nnumber of reactions in the train set.\nFor the ﬁne-tuning scenario, where access to the large generic\ndata set is not given but a model, pretrained on the large data set,\nis available instead, the results on the CARBO and USPTO test\nsets are shown in Fig.3a. After training the model on the CARBO\ntrain set, the top-1 accuracy reaches a 70.3%, similar to the model\nthat was trained on the two data sets simultaneously. The\nobserved behavior is the same when less CARBO reactions are\navailable. Also for 1k CARBO reactions, theﬁne-tuning model\nmatched the accuracy of the corresponding multitask model.\nFor this scenario, we analysed the effect of the train, validation,\nand test split in more detail. We compared the random split\ndescribed above to a time split, where we included CARBO\nreactions ﬁrst published before 2016 into the train and validation\nsets and the reactions published from 2016 into the test set (2831\nreactions). We investigated differentﬁne-tune set sizes (1k, 5k,\nEncoder —  self attention\nDecoder —  self and context attention\nContextual precursor tokens representation\nTokenized precursor SMILES\nModel — molecular transformer\nSequentially predicted product SMILES\nBeam search\nBrCc1ccc2ccccc2c1.CCCC[N+](CCCC)\n(CCCC)CCCC.CCCC[Sn](=O)CCCC.C[C@H]1O[C@@H]\n(Sc2ccccc2)[C@@H](O)[C@@H](O)[C@@H]1O\nC[C@H]1O[C@@H](Sc2ccccc2)[C@@H](O)[C@@H]\n(OCc2ccc3ccccc3c2)[C@@H]1O \nScenario 1: access to all data — multi-task\nSimultaneous training on both data sets (weighted).\nLarge data set with \ngeneric reactions\n(e.g. 1.1M USPTO \nreactions)\nSmall data set with\nspecific reactions\n(e.g. 20k CARBO\nreactions)\nSmall data set with\nspecific reactions\n(e.g. 20k CARBO\nreactions)\nPretrained reaction prediction model\n(e.g. molecular transformer \ntrained 1.1M USPTO reactions,\nno access to large set)\nPretrained model is trained for a few epochs on specific reactions.\nScenario 2: sequential transfer learning — pretrained model + fine tuning\nFig. 1 Molecular Transformer model and data scenarios.Sequence-2-sequence prediction of carbohydrate reactions and the two transfer learning\nscenarios, namely, multitask and sequential training.\n0\n20\n40\n60\n80\n100Accuracy [%]\nCARBO test\n0\n100\n500\n1k\n5k\n10k\n15k\n20k\n# carbo rxns in training\nfor w9 w1 setting\n0\n20\n40\n60\n80\n100Accuracy [%]\nUSPTO train USPTO test CARBO train CARBO test\nw1 w0\nw1 w1\nw9 w1\nw4 w1\nw1 w4\nw0 w1\nUSPTO | CARBO weights\nw1 w0\nw1 w1\nw9 w1\nw4 w1\nw1 w4\nw0 w1\nUSPTO | CARBO weights\nw1 w0\nw1 w1\nw9 w1\nw4 w1\nw1 w4\nw0 w1\nUSPTO | CARBO weights\nw1 w0\nw1 w1\nw9 w1\nw4 w1\nw1 w4\nw0 w1\nUSPTO | CARBO weights\na b\nFig. 2 Multitask scenario results. aTop-1 accuracy of models trained with different weights on the USPTO and CARBO data set (theﬁrst number\ncorresponds to the weight on the USPTO data set and the second to the weight on the CARBO data set).b Top-1 accuracy for a model trained in the weight\n9 weight 1 setting, where the number of reactions in the CARBO data set was reduced. Source data are provided as a Source Dataﬁle.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications 3\n10k, 15k, and 20k). As seen in Fig.3b, compared to the random\nsplit the top-1 accuracy with the 20kﬁne-tuning dropped slightly\nto 66% but it is still substantially larger than the accuracy that\ncould be obtained with the generic USPTO training set only.\nAlready with 5k CARBO reactions, an accuracy above 60% was\nreached. The larger the CARBO ﬁne-tuning set, the better the\nperformance of theﬁne-tuned model.\nBesides the fact that the reactions in the large data set do not\nneed to be revealed, another advantage is the shortﬁne-tuning\ntraining time. Theﬁne-tuning requires only 5k steps compared to\n250k steps in the multitask scenario. However, if time and access\nto both data sets are given, it is better to train simultaneously on\nall data for a longer time as the performance on the large data set\ndoes not decrease, as it does in theﬁne-tuning scenario. If the\ninterest is only in a speciﬁc reaction class, short adaptation times\nor if generic data is not available, thenﬁne-tuning a pretrained\nmodel is better.\nTo further demonstrate the effectiveness of the ﬁne-tuning\napproach, we performed an experiment where we pretrained a\nmodel on a data set without stereochemical information. To do\nso, we used the USPTO_MIT data set by Jin et al.14. As seen in\nFig. 3a, although the pretrained model does not manage to predict\nany CARBO test set reactions, afterﬁne-tuning for 6k steps the\nmodel reaches an accuracy of 63.3%. The accuracy was not as\nhigh as with USPTO pretraining but a signiﬁcant improvement\nover the 0.0% correctly predicted reactions by the pretrained\nmodel. The low accuracy after pretraining was expected as none\nof the chiral center tokens (e.g. “[C@H]”, “[C@@H]”) were\npresent in the training set. Theﬁne-tuning result shows that the\nMolecular Transformer model is able to learn new concepts\nwithin a few thousands training steps on 20k data points.\nIn the next sections, we will compare the model trained only on\nthe USPTO data, which was also used as pretrained model\n(USPTO model) with the model that was thenﬁne-tuned on the\n20k CARBO reactions (CARBO model).\nExperimental assessment. Although the accuracy of the trans-\nformer has been widely assessed\n19, an experimental validation is\nstill missing. Here, we decided to validate both the transformer\nand the augmented precision of the CARBO model on a recently\nrealized synthetic sequence from our own laboratory, absent from\nthe training data. This sequence is a 14-step synthesis of lipid-\nlinked oligosaccharide (LLO)15 to be used as a substrate to study\nOST46,47 (Fig. 4). The sequence contains typical carbohydrate\nchemistry: protecting group manipulations (steps: b, h, i, l n, p),\nfunctional group manipulations (step c, d), regioselective\nprotections (step e), a β-selective glycosylation (step g) and an\nα-selective phosphorylation (step m). The latter regio- and ste-\nreoselective transformations are of particular interest because\ntheir selectivity is generally difﬁcult to control and to predict,\neven for experienced synthetic chemists.\nWe used both the general USPTO model and theﬁne-tuned\nCARBO model to predict 13 of the 14 steps in the sequence (step\nb was removed since it appeared in the training set). The USPTO\nonly made four correct predictions (31%), which were either\nstandard protecting group manipulations (step a, g, n) or\nfunctional group exchanges (step c). The CARBO model also\ncorrectly predicted these four simple reactions, but additionally,\nmade another six correct predictions, including the regioselective\nbenzoylation (5– 6, step e) and the β-selective phosphorylation\n(11– 12, step m), corresponding to a 77% success rate and a 46%\nimprovement over the USPTO model, in line with the overall\nstatistics presented above.\nIn detail, the CARBO model only made three mistakes. The\nﬁrst one concerns the reduction of the primary iodide4 to a\nmethyl group in 5 by hydrogenation, which is mistakenly\npredicted to also reduce the benzyl glycoside. The USPTO model\nmakes the same mistake. Both models have not learned that\ncarrying out the reaction in the presence of ammonia reduces the\ncatalyst activity and avoids debenzylation, as no such reaction was\npresent in the training sets. The second mistake concerns a\nsimilar reduction of the benzyl glycoside in10 (step l), which is\npredicted to yield the β-lactol while the product 11 is in fact\nformed as an anomeric mixture. Again, the USPTO model\nmakes the same mistake. Both models ignore that the initially\nformed β-lactol equilibrates spontaneously to the anomeric\nmixture via ring opening. Finally, the CARBO model predicts a\nshortened prenyl chain in the phosphate coupling reaction\nforming the protected LLO 14 (step o), which does not make\nchemical sense. In this case it should be noted that the CARBO\ntraining set does not contain a single LLO molecule, and that the\nUSPTO model performs worse since it returns an invalid SMILES\nfor this reaction.\nWe obtained similar prediction performances from both models\nwhen analyzing a recently published total syntheses of the\ntrisaccharide repeating unit of Pseudomonas aeruginosa and\nStaphylococcus aureus\n43. Those synthetic sequences comprises\nfour difﬁcult regio- and stereoselective glycosylation steps andﬁve\nregioselective protection steps that are of particular interest. Out of\nthe 38 reactions that are absent from the training set in this\nsequence (Supplementary Figs. 2– 7), the USPTO model predicts\nonly 15 reactions (39%) correctly, and none of the difﬁcult steps\nmentioned above. The CARBO model performs much better and\ncorrectly predicts 26 of the 38 reactions, corresponding to a 68%\nCARBO only\nUSPTO only\nUSPTO_MIT only\nUSPTO w9 CARBO w1\nUSPTO only + 20k fine-tuning\nUSPTO_MIT + 20k fine-tuning\nUSPTO w9 CARBO 1k w1\nUSPTO only + 1k fine-tuning\nCARBO test\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\nTop-1 accuracy [%]\nCARBO test (time-split)ab\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\nTop-1 accuracy [%]\nUSPTO only\nUSPTO only + 1k fine-tuning\nUSPTO only + 5k fine-tuning\nUSPTO only + 10k fine-tuning\nUSPTO only + 15k fine-tuning\nUSPTO only + 20k fine-tuning\nFig. 3 Fine-tuning scenario results. aCARBO random split test set performance for different training strategies. In green are the top-1 accuracies of the\nmodels that wereﬁne-tuned on either 1k or 20k CARBO reactions shown. For comparison, we included in purple the top-1 accuracies of the models trained\non the single data sets (CARBO, USPTO, and USPTO_MIT). Blue are the performances of models trained in the multitask scenario.b CARBO time split test\nset performance for differentﬁne-tuning set sizes. Source data are provided as a Source Dataﬁle.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7\n4 NATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications\noverall accuracy and a 29% gain over the USPTO model. In\nparticular, the CARBO model correctly predicts the regioselec-\ntivity of the dimethyltin chloride mediated benzoylation of L-\nRhamnopyranoside 16 (step no. 10), the dif ﬁcult regio- and\nstereoselective glycosylation at position 3 of the terminal fucosyl in\ndisaccharide 18 (step no. 24) as well as the regioselective\nprotection of the same disaccharide at position 3 (step no. 29),\nall of which are nonobvious even for synthetic chemists (Fig.5).\nInterestingly, the CARBO model predicts a double substitution of\nbis-triﬂate 19 instead of the correct single substitution at position\n2, which the USPTO model correctly predicts. In this case it\nshould be noted that the outcome of the reaction is dictated by\nstoichiometry (only one equivalent of the azide nucleophile), an\ninformation which is absent from the training data. In contrast to\nthe USPTO training set, that contains only single azide\nsubstitutions, the CARBO training set contains single, as well as\ndouble substitutions. An analysis of the stereo centres in both data\nsets can be sound in Supplementary Table 1 and Supplementary\nFig. 1.\nEvery predicted reaction is associated with a con ﬁdence\nscore19, which is calculated from the product of the probabilities\nof the predicted product tokens. Interestingly, the con ﬁdence\nOAcO\nAcO\nNHAc\nOAc\nOAc\nOR1O\nR1O\nNHAc\nOBn\nX\nOHO\nHO\nNHAc\nOBn\nI\nOHO\nHO\nNHAc\nOBn\nOHO\nHO\nNHAc\nOBn\nOHO\nBzO\nNHAc\nOBn\nOHO\nBzO\nNHAc\nOBn O\nO\nTrocHN\nAcO\nAcO O\nNHAc\nBzO OBn\nOAc\nO\nO\nTrocHN\nAcO\nAcO O\nNHAc\nAcO OBn\nOAc\nO\nO\nR1HN\nR2O\nR2O O\nAcHN\nR3O OBn\nOR2\nExperimental observation\nCorrect, 1.00\nOHO\nHO\nNHAc\nOH\nOHO\nHO\nNHAc\nOH\nOBzO\nBzO\nNHAc\nOBn\nCorrect, 1.00\nLactol in b \nconfiguration \n0.74\nO\nO\nAcHN\nAcO\nAcO O\nNHAc\nAcO OH\nOAc\nO\nO\nAcHN\nAcO\nAcO O\nAcHN\nAcO\nO\nOAc\nP\nOBn\nO\nOBn\nShorter prenyl\n unit, n = 1\n 0.72\nO\nO\nAcHN\nAcO\nAcO O\nNHAc\nAcO O\nOAc\nP\nOBn\nO\nBnO\nO\nO\nAcHN\nAcO\nAcO O\nAcHN\nAcO\nO\nOAc\nP\nOR1\nO\nOR1\nO\nO\nAcHN\nR1O\nR1O O\nAcHN\nR1O\nO\nOR1\nP\nO\nO\nO-\nInvalid SMILES\nO\nO\nTrocHN\nBzO\nBzO O\nNHAc\nBzO OBn\nOAc\nR :\nUSPTO\nCorrect, 0.70\nNH4+\n1\n4 5\n5 6\n6\n8\n8\n12\n14 n = 2, R1 = Ac\n2 R1 = Ac, X = OAc\n3 R1 = H, X = OH\n4 R1= H, X = I\nCorrect, 0.38\n9 R1, R2 = Ac, R3 = Bz\n10 R1, R2, R3 = Ac\n12 R1 = Bn\n13 R1 = H\nP\n-O\nO\nOR\nNH4+\nO\nO\nAcHN\nAcO\nAcO O\nNHAc\nAcO OBn\nOAc\nO\nO\nAcHN\nAcO\nAcO O\nAcHN\nAcO\nOH\nOAc\nn\na\nb\nc\nd\ne\nf\ng\nh\ni\nl\nm\no\nn\nCorrect*, 1.00 Correct*, 0.79\nCorrect, 0.92 Correct, 0.97\nCorrect, 0.96 Correct, 0.74\nCorrect, 1.00\nIncomplete ester deprotection, 0.32\nCorrect, 0.99 Incomplete acetylation, 0.57\nCorrect, 0.52\nCorrect, 0.53 Correct, 0.98\n10 11\n11\nLactol in b \nconfiguration \n0.98\na\nb\nc\nd\ne\nf\ng\nh, i\nl\nm\no\nn\nO\nTrocHN\nAcO\nAcO\nOAc\nO CCl3\nNH\n7\n0.95 0.85\n0.72\n0.26\n0.19\n15 n = 2, R1 = H\np\np Correct, 0.96\nCARBO\nFig. 4 Synthesis of lipid-linked oligosaccharide (LLO).Reaction conditions: a BnOH, Yb(OTf)3, DCE, 90 °C, 2h, 78%.b MeONa, MeOH, sonication,\n30 min.c PPh3,I 2, imidazole, THF, 1h, reﬂux, 88% over two steps.d Pd/C, NH4OH, H2, THF/H2O, 30 min, 77%.e BzCl, pyr, −35°, 70%.f BF3Et2O, 4 MS,\nDCM, 26 h, 73%.g Zn, Ac2O, AcOH, DCE 50°, 3 h, 96%.h MeONa, MeOH/DMF, 4 days.i Ac2O, 4-(Dimethylamino)pyridine, pyr, 76% over three steps.\nl H2, THF/H2O, 10 bar, 16 h.m LiHMDS, tetrabenzylpyrophosphate, 53%.n H2, THF/MeOH, 1 h.o farnesylnerol, CDI, DMF, then11, 5 days, 18%.p MeOH,\nNH4OH, 16 h, qte. An asterisk represents“*” reaction present in the training set.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications 5\nscore correlates with the correctness of the prediction (Fig.6). For\nboth models most of the correct predictions have a score higher\nthan 0.8.\nTo have a closer look at the capabilities of the model to self-\nestimate its own uncertainty, we analyzed every reaction in detail.\nIn some cases, we observe epimerization or rearrangements that\nhave little chemical signiﬁcance and are associated with low score\nvalues. This even occurs in more trivial transformations, such as\namine acetylation of the trisaccharide in reaction 27 (scheme S3).\nAlthough the model is not able to predict the correct product, its\nlow score seems to indicate that the model senses its own mistake.\nThe second class are arguably wrong predictions that have high\nconﬁdence for chemical reasons. Such an example is the\npreviously discussed reaction 12 (Scheme 2, entry c) whose\noutcome is inﬂuenced by stoichiometry that together with other\nreaction conditions, is excluded from the training data, making\nthese reactions extremely difﬁcult to predict.\nSimilar to previous work19, one of the limitations of current\nSMILES-2-SMILES models is that environmental reaction con-\nditions like temperature and pressure are not taken into account.\nThose conditions are often missing in the data sets, and even if\npresent, it would not be straightforward to codify temperature\nproﬁles applied during chemical reactions. Another limitation is\nthe data coverage and quality. As pointed out above, most of the\nO\nBnO\nO\nNHTCA\nO\nO\nHOONap\nN3\nH\nN\nCbzBu2SnO, TBAB\n \nNapBr, tol\nO\nBnO\nOBz\nSPh\nO\nO\nPh\nO\nBnO\nO\nNHTCA\nO\nO\nHO\nN3\nH\nN\nCbz NIS, TMSOTf\nDCM\nScore = 1.0\nScore = 0.44\nO\nBnO\nO\nNHTCA\nO NHCbz\nO\nHOO\nN3\nO\nBnO\nOBz\nO\nO\nPh\nStep n. 29\nStep n. 10\n16\n18\n19\n20\n17\nO SPhTfO\nBzO OTf\nO SPhTfO\nBzO\nN3\nTBAN3\nCH3CN\nO SPh\nOBz\nN3\nN3Score = 1.020 2221\nStep n. 12\nO SPhHO\nHO OH\nO SPhHO\nBzO OH\nBzCl, Me2SnCl2\nDIPEA, THF\n21\nStep n. 24\nScore = 0.77\na\nb\nc\nFig. 5 Reactions predicted from recent literature. a, b Reactions correctly predicted.c wrongly predicted reaction (red structure) due to missing reagent\nstoichiometry in the model: only one equivalent of NaN3 was used resulting in single substitution, while the model predicts double substitution.\n0 5 10 15 20 25 30 35 40 45 50 55\nUSPTO predictions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Confidence score\n0 5 10 15 20 25 30 35 40 45 50 55\nCARBO predictions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Confidence score\nFalse\nTrue\nPrediction\nFig. 6 Analysis of prediction conﬁdence scores.Predictions (ordered by conﬁdence score) for the experimental assessment. Source data are provided as a\nSource Data ﬁle.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7\n6 NATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications\nwrong predictions can be explained with the data that the models\nhave seen during training.\nThe availability of large high-quality open-source reaction data\nset containing information detailed on amounts, stoichiometry,\nand reaction conditions could substantially improve reaction\nprediction models.\nDiscussion\nIn this work, we demonstrated that transfer learning can be suc-\ncessfully applied to a generally trained transformer model using as\nfew as 20k data points to derive a speciﬁc model that predicts\nreactions from a speciﬁc class with signiﬁcantly improved perfor-\nmance. Transfer learning of the general molecular transformer\nmodel, trained on the USPTO data set to a speciﬁc set of reactions,\nto obtain a high-performance specialized model as demonstrated\nhere should be generally applicable towards any subclass of speciﬁc\nreactions of interest.\nHere we used transfer learning to improve predictions of regio-\nand stereoselectivity, a central aspect of synthetic chemistry that\nhas not been systematically evaluated previously by reaction pre-\ndiction models, in part due to the fact that the Molecular Trans-\nformer is currently the only model able to handle stereochemistry.\nAs a test case we examined carbohydrates, a well-deﬁned class of\nmolecules for which reactions are difﬁcult to predict even for\nexperienced chemists, and subjected our model to experimental\nvalidation. We anticipate that the Carbohydrate Transformer will\nserve the practical purpose of improving the efﬁciency of complex\ncarbohydrate syntheses. The model can guide chemists by pre-\ndicting and scoring potential carbohydrates reactions before per-\nforming them experimentally. The fact that the conﬁdence score\ncorrelates with prediction accuracy offers a simple metric to judge\nthe quality of predictions. The shortcomings noted should be\naddressable by extending the training set with reactions that are\nnot predicted well.\nMethods\nReaction prediction model. All the experiments in this work were run with the\nMolecular Transformer model19, which is illustrated in Fig.1. For details on the\narchitecture we refer the reader to10,19. We used Pytorch48 and the OpenNMT49\nframework to build, train and test our models. Hyperparameters and a detailed\ndescription of the data sets can be found in the supplementary information. The\ninvestigated task is reaction prediction, where the aim is to predict the exact\nstructural formula, including stereochemistry, of the products that are formed from\na given a set of precursors as input. In the inputs, no difference is made between\nreactant and reagent molecules\n19. Following previous work13,15,19, we use accuracy\nas the evaluation metric. The reported accuracies describe the percentage of correct\nreactions. A reaction is counted as correct only if the predicted products exactly\nmatches the products reported in the literature after canonicalisation using\nRDKit\n45. The canonicalisation is required as multiple SMILES can represent the\nsame molecule.\nChemical synthesis. All reagents were purchased from commercial sources and\nused without further puriﬁcations unless otherwise stated. All reactions were car-\nried out inﬂame-dried round-bottomed-ﬂask under an argon atmosphere, except if\nspeciﬁed. Room temperature (rt) refers to ambient temperature. Temperatures of\n0 °C were maintained using an ice-water,−78 °C with acetone/dry ice bath and the\nother temperatures using a cryostat. Dry solvents were obtained by passing com-\nmercially available pre-dried, oxygen-free formulations through activated alumina\ncolumns. Hydrogenation was performed at room pressure using H2 ﬁlled balloon.\nChromatographic puriﬁcations were performed with silica gel pore size 60,\n230– 400 mesh particle size (Sigma-Aldrich). Thin layer chromatography was\nperformed using ALUGRAM Xtra Sil G/UV on pre-coated aluminium sheets,\nusing UV light as a visualizing, and a basic aqueous potassium permanganate\nsolution and ceric ammonium molybdate as developing agents. NMR spectra for\n1H, 13C, DEPT, 31P, COSY, HSQC, HMBC, and NOE were recorded at rt with a\nBruker AV (400 MHz1H). Spectra were and processed using TopSpin 3.6.1 soft-\nware. Chemical shifts are reported inδ (ppm) relative units to residual solvent\npeaks CDCl3 (7.26 ppm for1H and 77.2 ppm for13C) and MeOD (3.31 ppm for1H\nand 49.00 ppm for13C). Splitting patterns are assigned as s (singlet), d (doublet),\nt (triplet), q (quartet), quint (quintet), multiplet (m), dd (doublet of doublets), and\ntd (triplet of doublets). High-resolution mass spectra was provided by the“Service\nof Mass Spectrometry” at the Department of Chemistry and Biochemistry in Bern\nand were obtained by electron spray ionization in positive or negative mode\nrecorded on a Thermo Scientiﬁc LTQ Orbitrap XL. For the experimental proce-\ndures, NMR spectra and physical data of compounds 2– 15, see Supplementary\nNote 3.\nData availability\nThe USPTO data set derived from Lowe12 that we used for training and evaluation, our\ncarbohydrate reactions, as well as the ones from the work of Behera et al.43 are available\nfrom (https://github.com/rxn4chemistry/OpenNMT-py/tree/carbohydrate_transformer).\nSource data are provided with this paper.\nCode availability\nThe code and trained models are available from (https://github.com/rxn4chemistry/\nOpenNMT-py/tree/carbohydrate_transformer). The models are compatible with\nOpenNMT-py49,50, which was used for training and evaluation. The SMILES\ntokenization function for preprocessing the inputs is found on the Molecular\nTransformer repository\n19,51. The setup and hyperparameters can also be found in\nSupplementary Note 2.\nReceived: 28 July 2020; Accepted: 1 September 2020;\nReferences\n1. Corey, E. J. The logic of chemical synthesis: multistep synthesis of complex\ncarbogenic molecules. Angew. Chem. Int. Ed.30, 455– 465 (1991).\n2. Blakemore, D. C. et al. Organic synthesis provides opportunities to transform\ndrug discovery. Nat. Chem. 10, 383– 394 (2018).\n3. Lo, Y.-C., Rensi, S. E., Torng, W. & Altman, R. B. Machine learning in\nchemoinformatics and drug discovery.Drug Discov. today23, 1538– 1546\n(2018).\n4. Melville, J. L., Burke, E. K. & Hirst, J. D. Machine learning in virtual screening.\nComb. Chem. High. Throughput Screen.12, 332– 343 (2009).\n5. Bartók, A. P., Payne, M. C., Kondor, R. & Csányi, G. Gaussian approximation\npotentials: the accuracy of quantum mechanics, without the electrons.Phys.\nRev. Lett. 104, 136403 (2010).\n6. Dral, P. O. Quantum chemistry in the age of machine learning.J. Phys. Chem.\nLett. 11, 2336– 2347 (2020).\n7. Sutskever, I., Vinyals, O. & Le, Q. V. Sequence to sequence learning with\nneural networks. InAdvances in Neural Information Processing Systems,\n3104– 3112 (2014).\n8. Luong, M.-T., Pham, H. & Manning, C. D. Effective approaches to attention-\nbased neural machine translation. InProceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, 1412– 1421 (2015).\n9. Duvenaud, D. K. et al. Convolutional networks on graphs for learning\nmolecular ﬁngerprints. InAdvances in Neural Information Processing Systems,\n2224– 2232 (2015).\n10. Vaswani, A. et al. Attention is all you need. InAdvances in Neural Information\nProcessing Systems, 5998– 6008 (2017).\n11. Lowe, D. M. Extraction of chemical structures and reactions from the\nliterature. Ph.D. thesis, University of Cambridge (2012).\n12. Lowe, D. Chemical reactions from US patents (1976– 2016) (2017). https://\nﬁgshare.com/articles/Chemical_reactions_from_US_patents_1976-Sep2016_/\n5104873.\n13. Nam, J. & Kim, J. Linking the neural machine translation and the prediction\nof organic chemistry reactions. Preprint athttps://arxiv.org/abs/1612.09529\n(2016).\n14. Jin, W., Coley, C., Barzilay, R. & Jaakkola, T. Predicting organic reaction\noutcomes with weisfeiler-lehman network. InAdvances in Neural Information\nProcessing Systems, 2607– 2616 (2017).\n15. Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C. & Laino, T. Found in\ntranslation: predicting outcomes of complex organic chemistry reactions using\nneural sequence-to-sequence models.Chem. Sci. 9, 6091– 6098 (2018).\n16. Bradshaw, J., Kusner, M., Paige, B., Segler, M. & Hernández-Lobato, J. A\ngenerative model for electron paths. InInternational Conference on Learning\nRepresentations (2019).\n17. Do, K., Tran, T. & Venkatesh, S. Graph transformation policy network for\nchemical reaction prediction. InProceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, 750– 760\n(2019).\n18. Coley, C. W. et al. A graph-convolutional neural network model for the\nprediction of chemical reactivity.Chem. Sci. 10, 370– 377 (2019).\n19. Schwaller, P. et al. Molecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction.ACS Cent. Sci.5, 1572– 1583 (2019).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications 7\n20. Nair, V. H., Schwaller, P. & Laino, T. Data-driven chemical reaction\nprediction and retrosynthesis.CHIMIA 73, 997– 1000 (2019).\n21. Schwaller, P. et al. Predicting retrosynthetic pathways using a combined\nlinguistic model and hyper-graph exploration strategy.Chem. Sci. 11,\n3316– 3325 (2020).\n22. Qian, W. W. et al. Integrating deep neural networks and symbolic inference\nfor organic reactivity prediction. Preprint athttps://doi.org/10.26434/\nchemrxiv.11659563.v1 (2020).\n23. Schwaller, P. & Laino, T. Data-driven learning systems for chemical reaction\nprediction: an analysis of recent approaches. InMachine Learning in\nChemistry: Data-Driven Algorithms, Learning Systems, and Predictions,6 1– 79\n(ACS Publications, 2019).\n24. Weininger, D. Smiles, a chemical language and information system. 1.\nintroduction to methodology and encoding rules.J. Chem. Inf. Comput. Sci.\n28,3 1– 36 (1988).\n25. IBM RXN for chemistry.https://rxn.res.ibm.com. Accessed 13 Sep 2019.\n26. Grif ﬁths, R.-R., Schwaller, P., Lee, A. Dataset bias in the natural sciences: a\ncase study in chemical reaction prediction and synthesis design. Preprint at\nhttps://doi.org/10.26434/chemrxiv.7366973.v1 (2018).\n27. Ernst, B., Hart, G. W. & Sinaÿ, P. Carbohydrates in chemistry and biology\n(Wiley Blackwell, 2008).\n28. Stallforth, P., Lepenies, B., Adibekian, A. & Seeberger, P. H. Carbohydrates: a\nfrontier in medicinal chemistry.J. Med. Chem.52, 5561– 5577 (2009).\n29. Boilevin, J. M. & Reymond, J.-L. Synthesis of lipid-linked oligosaccharides\n(llos) and their phosphonate analogues as probes to study protein\nglycosylation enzymes. Synthesis 50, 2631– 2654 (2018).\n30. Mettu, R., Chen, C.-Y. & Wu, C.-Y. Synthetic carbohydrate-based vaccines:\nchallenges and opportunities.J. Biomed. Sci.27,1 – 22 (2020).\n31. Broecker, F. & Seeberger, P. H. Identiﬁcation and design of synthetic b cell\nepitopes for carbohydrate-based vaccines. InMethods in Enzymology, vol. 597,\n311– 334 (Elsevier, 2017).\n32. Barel, L.-A. & Mulard, L. A. Classical and novel strategies to develop a shigella\nglycoconjugate vaccine: from concept to efﬁcacy in human.Hum. Vaccines\nImmunother. 15, 1338– 1356 (2019).\n33. Kamat, M. N. & Demchenko, A. V. Revisiting the armed- disarmed concept\nrationale: S-benzoxazolyl glycosides in chemoselective oligosaccharide\nsynthesis. Org. Lett. 7, 3215– 3218 (2005).\n34. Dhakal, B. & Crich, D. Synthesis and stereocontrolled equatorially selective\nglycosylation reactions of a pseudaminic acid donor: importance of the side-\nchain conformation and regioselective reduction of azide protecting groups.\nJ. Am. Chem. Soc.140, 15008– 15015 (2018).\n35. Ruder, S.Neural transfer learning for natural language processing. Ph.D. thesis,\nNUI Galway (2019).\n36. Zoph, B., Yuret, D., May, J. & Knight, K. Transfer learning for low-resource\nneural machine translation. InProceedings of the Conference on Empirical\nMethods in Natural Language Processing, 1568– 1575 (2016).\n37. Ramachandran, P., Liu, P. & Le, Q. Unsupervised pretraining for sequence to\nsequence learning. InProceedings of the Conference on Empirical Methods in\nNatural Language Processing, 383– 391 (2017).\n38. Howard, J. & Ruder, S. Universal language modelﬁne-tuning for text\nclassiﬁcation. In Proceedings of the Annual Meeting of the Association for\nComputational Linguistics, 328– 339 (2018).\n39. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training of\ndeep bidirectional transformers for language understanding. InProceedings\nof the Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 4171– 4186\n(2019).\n40. Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. Mass: masked sequence to\nsequence pre-training for language generation. InInternational Conference on\nMachine Learning, 5926– 5936 (2019).\n41. Smith, J. S. et al. Approaching coupled cluster accuracy with a general-purpose\nneural network potential through transfer learning.Nat. Commun.10,1 – 8 (2019).\n42. Öztürk, H., Özgür, A., Schwaller, P., Laino, T. & Ozkirimli, E. Exploring\nchemical space using natural language processing methodologies for drug\ndiscovery. Drug Discov. Today25, 689– 705 (2020).\n43. Behera, A., Rai, D. & Kulkarni, S. S. Total syntheses of conjugation-ready\ntrisaccharide repeating units ofPseudomonas aeruginosa o11 and\nStaphylococcus aureus type 5 capsular polysaccharide for vaccine\ndevelopment. J. Am. Chem. Soc.142, 456– 467 (2019).\n44. Reaxys database. https://www.reaxys.com. Accessed 29 Oct 2019.\n45. Landrum, G. et al. RDKit: Open-Source Cheminformatics Software, Release\n2019_03_4. https://doi.org/10.5281/zenodo.3366468. Accessed 29 Oct 2019.\n46. Ramírez, A. S. et al. Characterization of the single-subunit\noligosaccharyltransferase stt3a from trypanosoma brucei using synthetic\npeptides and lipid-linked oligosaccharide analogs.Glycobiology 27, 525– 535\n(2017).\n47. Bloch, J. S. et al. Structure and mechanism of the er-based glucosyltransferase\nalg6. Nature 579, 443– 447 (2020).\n48. Paszke, A. et al. Pytorch: an imperative style, high-performance deep learning\nlibrary. InAdvances in Neural Information Processing Systems, 32, 8024– 8035\n(2019).\n49. Klein, G., Kim, Y., Deng, Y., Senellart, J. & Rush, A. M. OpenNMT: open-\nsource toolkit for neural machine translation. InProceedings of ACL(2017).\n50. OpenNMT-py. https://github.com/OpenNMT/OpenNMT-py. Accessed 29\nOct 2019.\n51. Molecular Transformer. https://github.com/pschwllr/MolecularTransformer.\nAccessed 29 Aug 2019.\nAcknowledgements\nThis research was supported by the Swiss National Science Foundation (SNF) Sinergia\nprogramme GlycoStart (CRSII5 173709). We thank the anonymous reviewers for their\ncareful reading of our manuscript and their many insightful comments and suggestions.\nAuthor contributions\nThe project was conceived and designed by G.P., P.S., and J.L.R. and supervised by T.L.\nand J.L.R. G.P. performed the experiments. P.S. trained the models. All authors discussed\nthe results and approved the manuscript. G.P. and P.S. contributed equally to this study.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationis available for this paper athttps://doi.org/10.1038/s41467-\n020-18671-7.\nCorrespondence and requests for materials should be addressed to J.-L.R.\nPeer review informationNature Communicationsthanks the anonymous reviewer(s) for\ntheir contribution to the peer review of this work.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2020\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18671-7\n8 NATURE COMMUNICATIONS|         (2020) 11:4874 | https://doi.org/10.1038/s41467-020-18671-7 | www.nature.com/naturecommunications",
  "topic": "Stereoselectivity",
  "concepts": [
    {
      "name": "Stereoselectivity",
      "score": 0.8843064308166504
    },
    {
      "name": "Regioselectivity",
      "score": 0.7422956824302673
    },
    {
      "name": "Transfer of learning",
      "score": 0.5618511438369751
    },
    {
      "name": "Deep learning",
      "score": 0.5463263988494873
    },
    {
      "name": "Chemistry",
      "score": 0.5228959918022156
    },
    {
      "name": "Transformer",
      "score": 0.5046011209487915
    },
    {
      "name": "Organic molecules",
      "score": 0.4365427494049072
    },
    {
      "name": "Computer science",
      "score": 0.3909503221511841
    },
    {
      "name": "Combinatorial chemistry",
      "score": 0.3722803294658661
    },
    {
      "name": "Molecule",
      "score": 0.32124432921409607
    },
    {
      "name": "Artificial intelligence",
      "score": 0.30860278010368347
    },
    {
      "name": "Organic chemistry",
      "score": 0.1950799524784088
    },
    {
      "name": "Physics",
      "score": 0.10390698909759521
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Catalysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118564535",
      "name": "University of Bern",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210126328",
      "name": "IBM Research - Zurich",
      "country": "CH"
    }
  ]
}