{
  "title": "DagoBERT: Generating Derivational Morphology with a Pretrained Language Model",
  "url": "https://openalex.org/W3091841779",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3021366198",
      "name": "Valentin Hofmann",
      "affiliations": [
        "University of Oxford",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2325354088",
      "name": "Janet Pierrehumbert",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2590654071",
    "https://openalex.org/W3134258610",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1586401530",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W89414877",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W424223014",
    "https://openalex.org/W2798549337",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2045282267",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W1606357646",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2484467321",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963326795",
    "https://openalex.org/W3026401902",
    "https://openalex.org/W2950158843",
    "https://openalex.org/W621048792",
    "https://openalex.org/W2506660467",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2912486864",
    "https://openalex.org/W1971421925",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2015418650",
    "https://openalex.org/W3106360414",
    "https://openalex.org/W3034786567"
  ],
  "abstract": "Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT's derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT's derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3848–3861,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n3848\nDagoBERT: Generating Derivational Morphology\nwith a Pretrained Language Model\nValentin Hofmann*‡, Janet B. Pierrehumbert†*, Hinrich Sch¨utze‡\n*Faculty of Linguistics, University of Oxford\n†Department of Engineering Science, University of Oxford\n‡Center for Information and Language Processing, LMU Munich\nvalentin.hofmann@ling-phil.ox.ac.uk\nAbstract\nCan pretrained language models (PLMs) gen-\nerate derivationally complex words? We\npresent the ﬁrst study investigating this ques-\ntion, taking BERT as the example PLM. We ex-\namine BERT’s derivational capabilities in dif-\nferent settings, ranging from using the unmod-\niﬁed pretrained model to full ﬁnetuning. Our\nbest model, DagoBERT (Derivationally and\ngeneratively optimized BERT), clearly outper-\nforms the previous state of the art in deriva-\ntion generation (DG). Furthermore, our exper-\niments show that the input segmentation cru-\ncially impacts BERT’s derivational knowledge,\nsuggesting that the performance of PLMs\ncould be further improved if a morphologically\ninformed vocabulary of units were used.\n1 Introduction\nWhat kind of linguistic knowledge is encoded by\npretrained language models (PLMs) such as ELMo\n(Peters et al., 2018), GPT-2 (Radford et al., 2019),\nand BERT (Devlin et al., 2019)? This question has\nattracted a lot of attention in NLP recently, with a\nfocus on syntax (e.g., Goldberg, 2019) and seman-\ntics (e.g., Ethayarajh, 2019). It is much less clear\nwhat PLMs learn about other aspects of language.\nHere, we present the ﬁrst study on the knowledge of\nPLMs about derivational morphology, taking BERT\nas the example PLM. Given an English cloze sen-\ntence such as this jacket is . and\na base such as wear, we ask: can BERT generate\ncorrect derivatives such as unwearable?\nThe motivation for this study is twofold. On the\none hand, we add to the growing body of work on\nthe linguistic capabilities of PLMs. Most PLMs\nsegment words into subword units (Bostrom and\nDurrett, 2020), e.g., unwearable is segmented\ninto un, ##wear, ##able by BERT’s WordPiece\ntokenizer (Wu et al., 2016). The fact that many of\nBERT DCL\nthisjacketis [MASK]wear[MASK].\nun ##able\nFigure 1: Basic experimental setup. We input sen-\ntences such as this jacket is unwearable .\nto BERT, mask out derivational afﬁxes, and recover\nthem using a derivational classiﬁcation layer (DCL).\nType Examples\nPreﬁxes anti, auto, contra, extra, hyper, mega,\nmini, multi, non, proto, pseudo\nSufﬁxes ##able, ##an, ##ate, ##ee, ##ess, ##ful,\n##ify, ##ize, ##ment, ##ness, ##ster\nTable 1: Examples of derivational afﬁxes in the BERT\nWordPiece vocabulary. Word-internal WordPiece to-\nkens are marked with ## throughout the paper.\nthese subword units are derivational afﬁxes sug-\ngests that PLMs might acquire knowledge about\nderivational morphology (Table 1), but this has not\nbeen tested. On the other hand, we are interested in\nderivation generation (DG) per se, a task that has\nbeen only addressed using LSTMs (Cotterell et al.,\n2017; Vylomova et al., 2017; Deutsch et al., 2018),\nnot models based on Transformers like BERT.\nContributions. We develop the ﬁrst frame-\nwork for generating derivationally complex English\nwords with a PLM, speciﬁcally BERT, and ana-\nlyze BERT’s performance in different settings. Our\nbest model, DagoBERT (Derivationally and gener-\natively optimized BERT), clearly outperforms an\nLSTM-based model, the previous state of the art.\n3849\nWe ﬁnd that DagoBERT’s errors are mainly due\nto syntactic and semantic overlap between afﬁxes.\nFurthermore, we show that the input segmentation\nimpacts how much derivational knowledge is avail-\nable to BERT, both during training and inference.\nThis suggests that the performance of PLMs could\nbe further improved if a morphologically informed\nvocabulary of units were used. We also publish the\nlargest dataset of derivatives in context to date.1\n2 Derivational Morphology\nLinguistics divides morphology into inﬂection\nand derivation. Given a lexeme such as wear,\nwhile inﬂection produces word forms such as\nwears, derivation produces new lexemes such as\nunwearable. There are several differences be-\ntween inﬂection and derivation (Haspelmath and\nSims, 2010), two of which are particularly impor-\ntant for the task of DG.2\nFirst, derivation covers a much larger spectrum\nof meanings than inﬂection (Acquaviva, 2016), and\nit is not possible to predict in general with which\nof them a particular lexeme is compatible. This\nis different from inﬂectional paradigms, where it\nis automatically clear whether a certain form will\nexist (Bauer, 2019). Second, the relationship be-\ntween form and meaning is more varied in deriva-\ntion than inﬂection. On the one hand, derivational\nafﬁxes tend to be highly polysemous, i.e., indi-\nvidual afﬁxes can represent a number of related\nmeanings (Lieber, 2019). On the other hand, sev-\neral afﬁxes can represent the same meaning, e.g.,\nity and ness. While such competing afﬁxes are\noften not completely synonymous as in the case\nof hyperactivity and hyperactiveness,\nthere are examples like purity and pureness\nor exclusivity and exclusiveness where\na semantic distinction is more difﬁcult to gauge\n(Bauer et al., 2013; Plag and Balling, 2020). These\ndifferences make learning functions from meaning\nto form harder for derivation than inﬂection.\nDerivational afﬁxes differ in how productive they\nare, i.e., how readily they can be used to create new\nlexemes (Plag, 1999). While the sufﬁx ness, e.g.,\ncan attach to practically all English adjectives, the\nsufﬁx th is much more limited in its scope of ap-\nplicability. In this paper, we focus on productive\n1We make our code and data publicly available at https:\n//github.com/valentinhofmann/dagobert.\n2It is important to note that the distinction between inﬂec-\ntion and derivation is fuzzy (ten Hacken, 2014).\nafﬁxes such as ness and exclude unproductive af-\nﬁxes such as th. Morphological productivity has\nbeen the subject of much work in psycholinguistics\nsince it reveals implicit cognitive generalizations\n(see Dal and Namer (2016) for a review), making\nit an interesting phenomenon to explore in PLMs.\nFurthermore, in the context of NLP applications\nsuch as sentiment analysis, productively formed\nderivatives are challenging because they tend to\nhave very low frequencies and often only occur\nonce (i.e., they are hapaxes) or a few times in large\ncorpora (Mahler et al., 2017). Our focus on pro-\nductive derivational morphology has crucial conse-\nquences for dataset design (Section 3) and model\nevaluation (Section 4) in the context of DG.\n3 Dataset of Derivatives\nWe base our study on a new dataset of derivatives\nin context similar in form to the one released by\nVylomova et al. (2017), i.e., it is based on sen-\ntences with a derivative (e.g.,this jacket is\nunwearable .) that are altered by masking the\nderivative (this jacket is .). Each\nitem in the dataset consists of (i) the altered sen-\ntence, (ii) the derivative (unwearable) and (iii)\nthe base (wear). The task is to generate the cor-\nrect derivative given the altered sentence and the\nbase. We use sentential contexts rather than tags\nto represent derivational meanings because they\nbetter reﬂect the semantic variability inherent in\nderivational morphology (Section 2). While Vylo-\nmova et al. (2017) use Wikipedia, we extract the\ndataset from Reddit.3 Since productively formed\nderivatives are not part of the language norm ini-\ntially (Bauer, 2001), social media is a particularly\nfertile ground for our study.\nFor determining derivatives, we use the algo-\nrithm introduced by Hofmann et al. (2020a), which\ntakes as input a set of preﬁxes, sufﬁxes, and bases\nand checks for each word in the data whether\nit can be derived from a base using a combina-\ntion of preﬁxes and sufﬁxes. The algorithm is\nsensitive to morpho-orthographic rules of English\n(Plag, 2003), e.g., when ity is removed from\napplicability, the result is applicable,\nnot applicabil. Here, we use BERT’s preﬁxes,\nsufﬁxes, and bases as input to the algorithm. Draw-\ning upon a comprehensive list of 52 productive pre-\n3We draw upon the entire Baumgartner Reddit Corpus,\na collection of all public Reddit posts available at https:\n//files.pushshift.io/reddit/comments/.\n3850\nP S PS\nBin µf nd ns Examples nd ns Examples nd ns Examples\nB1 .041 60,236 60,236 antijonny 39,543 39,543 takeoverness20,804 20,804unaggregateable\nB2 .094 39,181 90,857 antiastronaut22,633 52,060 alaskaness 8,661 19,903unnicknameable\nB3 .203 26,967 135,509 antiyale 14,463 71,814 blockbusterness4,735 23,560unbroadcastable\nB4 .423 18,697 196,295 antihomework9,753 100,729abnormalness 2,890 29,989unbrewable\nB5 .868 13,401 287,788 antiboxing 6,830 145,005legalness 1,848 39,501ungooglable\nB6 1.750 9,471 410,410 antiborder 4,934 211,233tragicness 1,172 50,393uncopyrightable\nB7 3.515 6,611 573,442 antimafia 3,580 310,109lightweightness802 69,004unwashable\nTable 2: Data summary statistics. The table shows statistics of the data used in the study by frequency bin and afﬁx\ntype. We also provide example derivatives with anti (P), ness (S), and un##able (PS) for the different bins.\nµf : mean frequency per billion words; nd: number of distinct derivatives; ns: number of context sentences.\nﬁxes and 49 productive sufﬁxes in English (Crystal,\n1997), we ﬁnd that 48 and 44 of them are contained\nin BERT’s vocabulary. We assign all fully alpha-\nbetic words with more than 3 characters in BERT’s\nvocabulary except for stopwords and previously\nidentiﬁed afﬁxes to the set of bases, yielding a total\nof 20,259 bases. We then extract every sentence\nincluding a word that is derivable from one of the\nbases using at least one of the preﬁxes or sufﬁxes\nfrom all publicly available Reddit posts.\nThe sentences are ﬁltered to contain between 10\nand 100 words, i.e., they provide more contextual\ninformation than the example sentence above.4 See\nAppendix A.1 for details about data preprocessing.\nThe resulting dataset comprises 413,271 distinct\nderivatives in 123,809,485 context sentences, mak-\ning it more than two orders of magnitude larger\nthan the one released by Vylomova et al. (2017).5\nTo get a sense of segmentation errors in the dataset,\nwe randomly pick 100 derivatives for each afﬁx\nand manually count missegmentations. We ﬁnd\nthat the average precision of segmentations in the\nsample is .960±.074, with higher values for pre-\nﬁxes (.990±.027) than sufﬁxes (.930±.093).\nFor this study, we extract all derivatives with\na frequency f ∈[1,128) from the dataset. We\ndivide the derivatives into 7 frequency bins with\nf = 1 (B1), f ∈ [2,4) (B2), f ∈ [4,8) (B3),\nf ∈[8,16) (B4), f ∈[16,32) (B5), f ∈[32,64)\n(B6), and f ∈[64,128) (B7). Notice that we focus\non low-frequency derivatives since we are inter-\nested in productive derivational morphology (Sec-\ntion 2). In addition, BERT is likely to have seen\nhigh-frequency derivatives multiple times during\n4We also extract the preceding and following sentence\nfor future studies on long-range dependencies in derivation.\nHowever, we do not exploit them in this work.\n5Due to the large number of preﬁxes, sufﬁxes, and bases,\nthe dataset can be valuable for any study on derivational mor-\nphology, irrespective of whether or not it focuses on DG.\npretraining and might be able to predict the afﬁx be-\ncause it has memorized the connection between the\nbase and the afﬁx, not because it has knowledge of\nderivational morphology. BERT’s pretraining cor-\npus has 3.3 billion words, i.e., words in the lower\nfrequency bins are very unlikely to have been seen\nby BERT before. This observation also holds for\naverage speakers of English, who have been shown\nto encounter at most a few billion word tokens in\ntheir lifetime (Brysbaert et al., 2016).\nRegarding the number of afﬁxes, we conﬁne our-\nselves to three cases: derivatives with one preﬁx\n(P), derivatives with one sufﬁx (S), and derivatives\nwith one preﬁx and one sufﬁx (PS).6 We treat these\ncases separately because they are known to have\ndifferent linguistic properties. In particular, since\nsufﬁxes in English can change the POS of a lexeme,\nthe syntactic context is more affected by sufﬁxa-\ntion than by preﬁxation. Table 2 provides summary\nstatistics for the seven frequency bins as well as\nexample derivatives for P, S, and PS. For each bin,\nwe randomly split the data into 60% training, 20%\ndevelopment, and 20% test. Following Vylomova\net al. (2017), we distinguish the lexicon settings\nSPLIT (no overlap between bases in train, dev, and\ntest) and SHARED (no constraint on overlap).\n4 Experiments\n4.1 Setup\nTo examine whether BERT can generate derivation-\nally complex words, we use a cloze test: given\na sentence with a masked word such as this\njacket is . and a base such as wear,\nthe task is to generate the correct derivative such\nas unwearable. The cloze setup has been pre-\nviously used in psycholinguistics to probe deriva-\ntional morphology (Pierrehumbert, 2006; Apel and\n6We denote afﬁx bundles, i.e., combinations of preﬁx and\nsufﬁx, by juxtaposition, e.g., un##able.\n3851\nLawrence, 2011) and was introduced to NLP in this\ncontext by Vylomova et al. (2017).\nIn this work, we frame DG as an afﬁx classi-\nﬁcation task, i.e., we predict which afﬁx is most\nlikely to occur with a given base in a given context\nsentence.7 More formally, given a base b and a\ncontext sentence x split into left and right contexts\nx(l) = (x1,...,x d−1) and x(r) = (xd+1,...,x n),\nwith xd being the masked derivative, we want to\nﬁnd the afﬁx ˆasuch that\nˆa= arg max\na\nP\n(\nψ(b,a)|x(l),x(r)\n)\n, (1)\nwhere ψ is a function mapping bases and afﬁxes\nonto derivatives, e.g., ψ(wear,un##able) =\nunwearable. Notice we do not model the func-\ntion ψ itself, i.e., we only predict derivational\ncategories, not the morpho-orthographic changes\nthat accompany their realization in writing. One\nreason for this is that as opposed to previous\nwork, our study focuses on low-frequency deriva-\ntives, for many of which ψ is not right-unique,\ne.g., ungoogleable and ungooglable or\ncelebrityness and celebritiness occur\nas competing forms in the data.\nAs a result of the semantically diverse nature of\nderivation (Section 2), deciding whether a particu-\nlar prediction ˆais correct or not is less straightfor-\nward than it may seem. Taking again the example\nsentence this jacket is . with the\nmasked derivativeunwearable, compare the fol-\nlowing ﬁve predictions:\n– ψ(b,ˆa) =wearity: ill-formed;\n– ψ(b,ˆa) = wearer: well-formed, syntacti-\ncally incorrect (wrong POS);\n– ψ(b,ˆa) = intrawearable: well-formed,\nsyntactically correct, semantically dubious;\n– ψ(b,ˆa) = superwearable: well-formed,\nsyntactically correct, semantically possible, but\ndid not occur in the example sentence;\n– ψ(b,ˆa) = unwearable: well-formed, syn-\ntactically correct, semantically possible, and\ndid occur in the example sentence.\nThese predictions reﬂect increasing degrees of\nderivational knowledge. A priori, where to draw\nthe line between correct and incorrect predictions\n7In the case of PS, we predict which afﬁx bundle (e.g.,\nun##able) is most likely to occur.\nMethod B1 B2 B3 B4 B5 B6 B7 µ±σ\nHYP .197 .228 .252 .278 .300 .315 .337 .272±.046\nINIT .184 .201 .211 .227 .241 .253 .264 .226±.027\nTOK .141 .157 .170 .193 .218 .245 .270 .199±.044\nPROJ .159 .166 .159 .175 .175 .184 .179 .171±.009\nTable 3: Performance (MRR) of pretrained BERT for\npreﬁx prediction with different segmentations. Best\nscore per column in gray, second-best in light-gray.\non this continuum is not clear, especially with re-\nspect to the last two cases. Here, we apply the most\nconservative criterion: a predictionˆais only judged\ncorrect if ψ(b,ˆa) =xd, i.e., if ˆais the afﬁx in the\nmasked derivative. Thus, we ignore afﬁxes that\nmight potentially produce equally possible deriva-\ntives such as superwearable.\nWe use mean reciprocal rank (MRR), macro-\naveraged over afﬁxes, as the evaluation measure\n(Radev et al., 2002). We calculate the MRR value\nof an individual afﬁx aas\nMRRa = 1\n|Da|\n∑\ni∈Da\nR−1\ni , (2)\nwhere Da is the set of derivatives containinga, and\nRi is the predicted rank of afor derivative i. We\nset R−1\ni = 0if Ri >10. Denoting with Athe set\nof all afﬁxes, the ﬁnal MRR value is given by\nMRR = 1\n|A|\n∑\na∈A\nMRRa . (3)\n4.2 Segmentation Methods\nSince BERT distinguishes word-initial ( wear)\nfrom word-internal (##wear) tokens, predicting\npreﬁxes requires the word-internal form of the base.\nHowever, only 795 bases in BERT’s vocabulary\nhave a word-internal form. Take as an example\nthe word unallowed: both un and allowed\nare in the BERT vocabulary, but we need the token\n##allowed, which does not exist (BERT tok-\nenizes the word into una, ##llo, ##wed). To\novercome this problem, we test the following four\nsegmentation methods:\nHYP.We insert a hyphen between the preﬁx and\nthe base in its word-initial form, yielding the tokens\nun, -, allowed in our example. Since both preﬁx\nand base are guaranteed to be in the BERT vocab-\nulary (Section 3), and since there are no tokens\nstarting with a hyphen in the BERT vocabulary,\nBERT always tokenizes words of the form preﬁx-\nhyphen-base into preﬁx, hyphen, and base, making\nthis a natural segmentation for BERT.\n3852\nSHARED SPLIT\nModel B1 B2 B3 B4 B5 B6 B7 µ±σ B1 B2 B3 B4 B5 B6 B7 µ±σ\nDagoBERT.373 .459 .657 .824 .895 .934 .957 .728±.219 .375 .386 .390 .411 .412 .396 .417 .398±.014\nBERT+ .296 .380 .497 .623 .762 .838 .902 .614 ±.215 .303 .313 .325 .340 .341 .353 .354 .333±.018\nBERT .197 .228 .252 .278 .300 .315 .337 .272 ±.046 .199 .227 .242 .279 .305 .307 .351 .273 ±.049\nLSTM .152 .331 .576 .717 .818 .862 .907 .623±.266 .139 .153 .142 .127 .121 .123 .115 .131 ±.013\nRB .064 .067 .064 .067 .065 .063 .066 .065 ±.001 .068 .064 .062 .064 .062 .064 .064 .064 ±.002\nTable 4: Performance (MRR) of preﬁx (P) models. Best score per column in gray, second-best in light-gray.\nSHARED SPLIT\nModel B1 B2 B3 B4 B5 B6 B7 µ±σ B1 B2 B3 B4 B5 B6 B7 µ±σ\nDagoBERT.427 .525 .725 .868 .933 .964 .975 .774±.205 .424 .435 .437 .425 .421 .393 .414 .421±.014\nBERT+ .384 .445 .550 .684 .807 .878 .921 .667 ±.197 .378 .387 .389 .380 .364 .364 .342 .372±.015\nBERT .229 .246 .262 .301 .324 .349 .381 .299 ±.052 .221 .246 .268 .299 .316 .325 .347 .289±.042\nLSTM .217 .416 .669 .812 .881 .923 .945 .695±.259 .188 .186 .173 .154 .147 .145 .140 .162 ±.019\nRB .071 .073 .069 .068 .068 .068 .068 .069 ±.002 .070 .069 .069 .071 .070 .069 .068 .069 ±.001\nTable 5: Performance (MRR) of sufﬁx (S) models. Best score per column in gray, second-best in light-gray.\nSHARED SPLIT\nModel B1 B2 B3 B4 B5 B6 B7 µ±σ B1 B2 B3 B4 B5 B6 B7 µ±σ\nDagoBERT.143 .355 .621 .830 .914 .940 .971 .682±.299 .137 .181 .199 .234 .217 .270 .334 .225±.059\nBERT+ .103 .205 .394 .611 .754 .851 .918 .548 ±.296 .091 .128 .145 .182 .173 .210 .218 .164±.042\nBERT .082 .112 .114 .127 .145 .155 .190 .132 ±.032 .076 .114 .130 .177 .172 .226 .297 .170±.069\nLSTM .020 .338 .647 .781 .839 .882 .936 .635±.312 .015 .019 .026 .034 .041 .072 .081 .041 ±.024\nRB .002 .003 .003 .005 .006 .008 .012 .006 ±.003 .002 .004 .003 .006 .006 .007 .009 .005 ±.002\nTable 6: Performance (MRR) of preﬁx-sufﬁx (PS) models. Best score per column in gray, second-best in light-gray.\nINIT. We simply use the word-initial instead\nof the word-internal form, segmenting the deriva-\ntive into the preﬁx followed by the base, i.e.,\nun, allowed in our example. Notice that this\nlooks like two individual words to BERT since\nallowed is a word-initial unit.\nTOK. To overcome the problem of INIT, we seg-\nment the base into word-internal tokens, i.e., our\nexample is segmented into un, ##all, ##owed.\nThis means that we use the word-internal counter-\npart of the base in cases where it exists.\nPROJ. We train a projection matrix that maps\nembeddings of word-initial forms of bases to word-\ninternal embeddings. More speciﬁcally, we ﬁt a\nmatrix ˆT ∈Rm×m (mbeing the embedding size)\nvia least squares,\nˆT = arg min\nT\n||ET −E##||2\n2, (4)\nwhere E,E## ∈Rn×m are the word-initial and\nword-internal token input embeddings of bases\nwith both forms. We then map bases with no word-\ninternal form and a word-initial input token em-\nbedding e such as allow onto the projected word-\ninternal embedding e⊤ˆT.\nModel SHARED SPLIT\nDagoBERT .943 .615\nLSTM .824 .511\nLSTM (V) .830 .520\nTable 7: Performance on Vylomova et al. (2017)\ndataset. We report accuracies for comparability. LSTM\n(V): LSTM in Vylomova et al. (2017). Best score per\ncolumn in gray, second-best in light-gray.\nWe evaluate the four segmentation methods\non the SHARED test data for P with pretrained\nBERTBASE, using its pretrained language modeling\nhead for prediction and ﬁltering for preﬁxes. The\nHYP segmentation method performs best (Table 3)\nand is adopted for BERT models on P and PS.\n4.3 Models\nAll BERT models use BERTBASE and add a deriva-\ntional classiﬁcation layer (DCL) with softmax acti-\nvation for prediction (Figure 1). We examine three\nBERT models and two baselines. See Appendix\nA.2 for details about implementation, hyperparam-\neter tuning, and runtime.\nDagoBERT.We ﬁnetune both BERT and DCL\non DG, a model that we call DagoBERT (short for\n3853\nType Clusters\nPreﬁxes\n{bi, demi, fore, mini, proto, pseudo, semi, sub, tri}, {arch, extra, hyper, mega, poly, super, ultra},\n{anti, contra, counter, neo, pro}, {mal, mis, over, under}, {inter, intra},\n{auto, de, di, in, re, sur, un}, {ex, vice}, {non, post, pre}\nSufﬁxes\n{##al, ##an, ##ial, ##ian, ##ic, ##ite}, {##en, ##ful, ##ive, ##ly, ##y}, {##able, ##ish, ##less},\n{##age, ##ance, ##ation, ##dom, ##ery, ##ess, ##hood, ##ism, ##ity, ##ment, ##ness},\n{##ant, ##ee, ##eer, ##er, ##ette, ##ist, ##ous, ##ster}, {##ate, ##ify, ##ize}\nTable 8: Preﬁx and sufﬁx clusterings produced by Girvan-Newman after 4 graph splits on the DagoBERT confusion\nmatrix. For reasons of space, we do not list clusters consisting of only one afﬁx.\nDerivationally and generatively optimized BERT).\nNotice that since BERT cannot capture statistical\ndependencies between masked tokens (Yang et al.,\n2019), all BERT-based models predict preﬁxes and\nsufﬁxes independently in the case of PS.\nBERT+. We keep the model weights of pre-\ntrained BERT ﬁxed and only train DCL on DG.\nThis is similar in nature to a probing task.\nBERT.We use pretrained BERT and leverage its\npretrained language modeling head as DCL, ﬁlter-\ning for afﬁxes, e.g., we compute the softmax only\nover preﬁxes in the case of P.\nLSTM. We adapt the approach described in Vy-\nlomova et al. (2017), which combines the left and\nright contexts x(l) and x(r) of the masked deriva-\ntive by means of two BiLSTMs with a character-\nlevel representation of the base. To allow for a\ndirect comparison with BERT, we do not use the\ncharacter-based decoder proposed by Vylomova\net al. (2017) but instead add a dense layer for the\nprediction. For PS, we treat preﬁx-sufﬁx bundles\nas units (e.g., un##able).\nIn order to provide a strict comparison to Vy-\nlomova et al. (2017), we also evaluate our LSTM\nand best BERT-based model on the sufﬁx dataset\nreleased by Vylomova et al. (2017) against the\nreported performance of their encoder-decoder\nmodel.8 Notice Vylomova et al. (2017) show that\nproviding the LSTM with the POS of the deriva-\ntive increases performance. Here, we focus on the\nmore general case where the POS is not known and\nhence do not consider this setting.\nRandom Baseline (RB).The prediction is a ran-\ndom ranking of all afﬁxes.\n8The dataset is available at https://github.com/\nivri/dmorph. While Vylomova et al. (2017) take morpho-\northographic changes into account, we only predict afﬁxes,\nnot the accompanying changes in orthography (Section 4.1).\n5 Results\n5.1 Overall Performance\nResults are shown in Tables 4, 5, and 6. For P and\nS, DagoBERT clearly performs best. Pretrained\nBERT is better than LSTM on SPLIT but worse\non SHARED. BERT+ performs better than pre-\ntrained BERT, even on SPLIT (except for S on\nB7). S has higher scores than P for all models\nand frequency bins, which might be due to the fact\nthat sufﬁxes carry POS information and hence are\neasier to predict given the syntactic context. Re-\ngarding frequency effects, the models beneﬁt from\nhigher frequencies on SHARED since they can con-\nnect bases with certain groups of afﬁxes.9 For PS,\nDagoBERT also performs best in general but is\nbeaten by LSTM on one bin. The smaller perfor-\nmance gap as compared to P and S can be explained\nby the fact that DagoBERT as opposed to LSTM\ncannot learn statistical dependencies between two\nmasked tokens (Section 4).\nThe results on the dataset released by Vylomova\net al. (2017) conﬁrm the superior performance of\nDagoBERT (Table 7). DagoBERT beats the LSTM\nby a large margin, both on SHARED and SPLIT.\nWe also notice that our LSTM (which predicts\nderivational categories) has a very similar perfor-\nmance to the LSTM encoder-decoder proposed by\nVylomova et al. (2017).\n5.2 Patterns of Confusion\nWe now analyze in more detail the performance of\nthe best performing model, DagoBERT, and con-\ntrast it with the performance of pretrained BERT.\nAs a result of our deﬁnition of correct predictions\n(Section 4.1), the set of incorrect predictions is het-\nerogeneous and potentially contains afﬁxes result-\ning in equally possible derivatives. We are hence\ninterested in patterns of confusion in the data.\n9The fact that this trend also holds for pretrained BERT\nindicates that more frequent derivatives in our dataset also\nappeared more often in the data used for pretraining BERT.\n3854\nFigure 2: Preﬁxes predicted by BERT (left) and DagoBERT (right). Vertical lines indicate that a preﬁx has been\novergenerated (particularly re and non in the left panel). The white boxes in the right panel highlight the clusters\nproduced by Girvan-Newman after 4 graph splits.\nFigure 3: Sufﬁxes predicted by pretrained BERT (left) and DagoBERT (right). Vertical lines indicate that a sufﬁx\nhas been overgenerated (particularly y, ly, and er in the left panel). The white boxes in the right panel highlight\nthe clusters produced by Girvan-Newman after 4 graph splits.\nWe start by constructing the row-normalized con-\nfusion matrix C for the predictions of DagoBERT\non the hapax derivatives (B1, SHARED) for P and\nS. Based on C, we create a confusion graph Gwith\nadjacency matrix G, whose elements are\nGij =\n⌈\nCij −θ\n⌉\n, (5)\ni.e., there is a directed edge from afﬁx ito afﬁx jif\niwas misclassiﬁed as jwith a probability greater\nthan θ. We set θ to 0.08.10 To uncover the com-\nmunity structure of G, we use the Girvan-Newman\nalgorithm (Girvan and Newman, 2002), which clus-\nters the graph by iteratively removing the edge with\nthe highest betweenness centrality.\nThe resulting clusters reﬂect linguistically inter-\npretable groups of afﬁxes (Table 8). In particular,\nthe sufﬁxes are clustered in groups with common\n10We tried other values of θ, but the results were similar.\n3855\nFigure 4: Correlation between number of hapaxes and MRR for pretrained BERT (left) and DagoBERT (right) on\nB1. The highly productive sufﬁx y at (12662,0.49) (left) and (12662,0.62) (right) is not shown.\nPOS. These results are conﬁrmed by plotting the\nconfusion matrix with an ordering of the afﬁxes\ninduced by all clusterings of the Girvan-Newman\nalgorithm (Figure 2, Figure 3). They indicate that\neven when DagoBERT does not predict the afﬁx\noccurring in the sentence, it tends to predict an afﬁx\nsemantically and syntactically congruent with the\nground truth (e.g., ness for ity, ify for ize,\ninter for intra). In such cases, it is often a\nmore productive afﬁx that is predicted in lieu of\na less productive one. Furthermore, DagoBERT\nfrequently confuses afﬁxes denoting points on the\nsame scale, often antonyms (e.g., pro and anti,\npre and post, under and over). This can be\nrelated to recent work showing that BERT has dif-\nﬁculties with negated expressions (Ettinger, 2020;\nKassner and Sch ¨utze, 2020). Pretrained BERT\nshows similar confusion patterns overall but over-\ngenerates several afﬁxes much more strongly than\nDagoBERT, in particularre, non, y, ly, and er,\nwhich are among the most productive afﬁxes in\nEnglish (Plag, 1999, 2003).\nTo probe the impact of productivity more quan-\ntitatively, we measure the cardinality of the set of\nhapaxes formed by means of a particular afﬁx ain\nthe entire dataset, |Ha|, and calculate a linear re-\ngression to predict the MRR values of afﬁxes based\non |Ha|. |Ha|is a common measure of morpholog-\nical productivity (Baayen and Lieber, 1991; Pierre-\nhumbert and Granell, 2018). This analysis shows\na signiﬁcant positive correlation for both preﬁxes\n(R2 = .566, F(1,43) = 56.05, p < .001) and\nsufﬁxes (R2 = .410, F(1,41) = 28.49, p<. 001):\nthe more productive an afﬁx, the higher its MRR\nvalue. This also holds for DagoBERT’s predictions\nof preﬁxes ( R2 = .423, F(1,43) = 31.52, p <\n.001) and sufﬁxes ( R2 = .169, F(1,41) = 8.34,\np<. 01), but the correlation is weaker, particularly\nin the case of sufﬁxes (Figure 4).\n5.3 Impact of Input Segmentation\nWe have shown that BERT can generate derivatives\nif it is provided with the morphologically correct\nsegmentation. At the same time, we observed that\nBERT’s WordPiece tokenizations are often mor-\nphologically incorrect, an observation that led us\nto impose the correct segmentation using hyphen-\nation (HYP). We now examine more directly how\nBERT’s derivational knowledge is affected by us-\ning the original WordPiece segmentations versus\nthe HYP segmentations.\nWe draw upon the same dataset as for DG\n(SPLIT) but perform binary instead of multi-class\nclassiﬁcation, i.e., the task is to predict whether,\ne.g., unwearable is a possible derivative in the\ncontext this jacket is . or not.\nAs negative examples, we combine the base of\neach derivative (e.g., wear) with a randomly cho-\nsen afﬁx different from the original afﬁx (e.g.,\n##ation) and keep the sentence context un-\nchanged, resulting in a balanced dataset. We only\nuse preﬁxed derivatives for this experiment.\n3856\nFROZEN FINETUNED\nSegmentation B1 B2 B3 B4 B5 B6 B7 µ±σ B1 B2 B3 B4 B5 B6 B7 µ±σ\nMorphological.634 .645 .658 .675 .683 .692 .698 .669±.022 .762 .782 .797 .807 .800 .804 .799 .793±.015\nWordPiece .572 .578 .583 .590 .597 .608 .608 .591 ±.013 .739 .757 .766 .769 .767 .755 .753 .758 ±.010\nTable 9: Performance (accuracy) of BERT on morphological well-formedness prediction with morphologically\ncorrect segmentation versus WordPiece tokenization. Best score per column in gray.\nWe train binary classiﬁers using BERTBASE and\none of two input segmentations, the morphologi-\ncally correct segmentation or BERT’s WordPiece\ntokenization. The BERT output embeddings for\nall subword units belonging to the derivative in\nquestion are max-pooled and fed into a dense layer\nwith a sigmoid activation. We examine two settings:\ntraining only the dense layer while keeping BERT’s\nmodel weights frozen (FROZEN), or ﬁnetuning the\nentire model (FINETUNED). See Appendix A.3\nfor details about implementation, hyperparameter\ntuning, and runtime.\nMorphologically correct segmentation consis-\ntently outperforms WordPiece tokenization, both\non FROZEN and FINETUNED (Table 9). We\ninterpret this in two ways. Firstly, the type of\nsegmentation used by BERT impacts how much\nderivational knowledge can be learned, with posi-\ntive effects of morphologically valid segmentations.\nSecondly, the fact that there is a performance gap\neven for models with frozen weights indicates that\na morphologically invalid segmentation can blur\nthe derivational knowledge that is in principle avail-\nable and causes BERT to force semantically unre-\nlated words to have similar representations. Taken\ntogether, these ﬁndings provide further evidence\nfor the crucial importance of morphologically valid\nsegmentation strategies in language model pretrain-\ning (Bostrom and Durrett, 2020).\n6 Related Work\nPLMs such as ELMo (Peters et al., 2018), GPT-2\n(Radford et al., 2019), and BERT (Devlin et al.,\n2019) have been the focus of much recent work\nin NLP. Several studies have been devoted to the\nlinguistic knowledge encoded by the parameters of\nPLMs (see Rogers et al. (2020) for a review), par-\nticularly syntax (Goldberg, 2019; Hewitt and Man-\nning, 2019; Jawahar et al., 2019; Lin et al., 2019)\nand semantics (Ethayarajh, 2019; Wiedemann et al.,\n2019; Ettinger, 2020). There is also a recent study\nexamining morphosyntactic information in a PLM,\nspeciﬁcally BERT (Edmiston, 2020).\nThere has been relatively little recent work on\nderivational morphology in NLP. Both Cotterell\net al. (2017) and Deutsch et al. (2018) propose neu-\nral architectures that represent derivational mean-\nings as tags. More closely related to our study, Vy-\nlomova et al. (2017) develop an encoder-decoder\nmodel that uses the context sentence for predicting\ndeverbal nouns. Hofmann et al. (2020b) propose a\ngraph auto-encoder that models the morphological\nwell-formedness of derivatives.\n7 Conclusion\nWe show that a PLM, speciﬁcally BERT, can gener-\nate derivationally complex words. Our best model,\nDagoBERT, clearly beats an LSTM-based model,\nthe previous state of the art in DG. DagoBERT’s er-\nrors are mainly due to syntactic and semantic over-\nlap between afﬁxes. Furthermore, we demonstrate\nthat the input segmentation impacts how much\nderivational knowledge is available to BERT. This\nsuggests that the performance of PLMs could be\nfurther improved if a morphologically informed\nvocabulary of units were used.\nAcknowledgements\nValentin Hofmann was funded by the Arts and Hu-\nmanities Research Council and the German Aca-\ndemic Scholarship Foundation. This research was\nalso supported by the European Research Council\n(Grant No. 740516). We thank the reviewers for\ntheir detailed and helpful comments.\nReferences\nPaolo Acquaviva. 2016. Morphological semantics. In\nAndrew Hippisley and Gregory Stump, editors, The\nCambridge handbook of morphology, pages 117–\n148. Cambridge University Press, Cambridge.\nKenn Apel and Jessika Lawrence. 2011. Contribu-\ntions of morphological awareness skills to word-\nlevel reading and spelling in ﬁrst-grade children\nwith and without speech sound disorder. Jour-\nnal of Speech, Language, and Hearing Research,\n54(5):1312–1327.\n3857\nR. Harald Baayen and Rochelle Lieber. 1991. Produc-\ntivity and English derivation: A corpus-based study.\nLinguistics, 29(5).\nLaurie Bauer. 2001. Morphological productivity. Cam-\nbridge University Press, Cambridge, UK.\nLaurie Bauer. 2019. Rethinking morphology. Edin-\nburgh University Press, Edinburgh, UK.\nLaurie Bauer, Rochelle Lieber, and Ingo Plag. 2013.\nThe Oxford reference guide to English morphology.\nOxford University Press, Oxford, UK.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\narXiv 2004.03720.\nMarc Brysbaert, Micha¨el Stevens, Paweł Mandera, and\nEmmanuel Keuleers. 2016. How many words do we\nknow? practical estimates of vocabulary size depen-\ndent on word deﬁnition, the degree of language input\nand the participant’s age. Frontiers in Psychology,\n7:1116.\nRyan Cotterell, Ekaterina Vylomova, Huda Khayral-\nlah, Christo Kirov, and David Yarowsky. 2017.\nParadigm completion for derivational morphology.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2017.\nDavid Crystal. 1997. The Cambridge encyclopedia of\nthe English language. Cambridge University Press,\nCambridge, UK.\nGeorgette Dal and Fiammetta Namer. 2016. Produc-\ntivity. In Andrew Hippisley and Gregory Stump,\neditors, The Cambridge handbook of morphology,\npages 70–89. Cambridge University Press, Cam-\nbridge.\nDaniel Deutsch, John Hewitt, and Dan Roth. 2018. A\ndistributional and orthographic aggregation model\nfor English derivational morphology. In Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL) 56.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. In Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL HTL) 17.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2019.\nDaniel Edmiston. 2020. A systematic analysis of mor-\nphological content in BERT models for multiple lan-\nguages. In arXiv 2004.03032.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2019.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMichelle Girvan and Mark Newman. 2002. Commu-\nnity structure in social and biological networks. Pro-\nceedings of the National Academy of Sciences of the\nUnited States of America, 99(12):7821–7826.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. In arXiv 1901.05287.\nPius ten Hacken. 2014. Delineating derivation and in-\nﬂection. In Rochelle Lieber and Pavol ˇStekauer, edi-\ntors, The Oxford handbook of derivational morphol-\nogy, pages 10–25. Oxford University Press, Oxford.\nMartin Haspelmath and Andrea D. Sims. 2010. Under-\nstanding morphology. Routledge, New York, NY .\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL HTL) 17.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Sch ¨utze. 2020a. Predicting the growth of mor-\nphological families from social and linguistic factors.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics (ACL) 58.\nValentin Hofmann, Hinrich Sch¨utze, and Janet B. Pier-\nrehumbert. 2020b. A graph auto-encoder model\nof derivational morphology. In Annual Meeting of\nthe Association for Computational Linguistics (ACL)\n58.\nGanesh Jawahar, Benoit Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Annual Meeting of the Association\nfor Computational Linguistics (ACL) 57.\nNora Kassner and Hinrich Sch¨utze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Annual Meeting of\nthe Association for Computational Linguistics (ACL)\n58.\nDiederik P. Kingma and Jimmy L. Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR) 3.\nRochelle Lieber. 2019. Theoretical issues in word for-\nmation. In Jenny Audring and Francesca Masini, ed-\nitors, The Oxford handbook of morphological theory,\npages 34–55. Oxford University Press, Oxford.\n3858\nYongjie Lin, Yi C. Tan, and Robert Frank. 2019. Open\nsesame: Getting inside BERT’s linguistic knowl-\nedge. In Analyzing and Interpreting Neural Net-\nworks for NLP (BlackboxNLP) 2.\nTaylor Mahler, Willy Cheung, Micha Elsner, David\nKing, Marie-Catherine de Marneffe, Cory Shain,\nSymon Stevens-Guille, and Michael White. 2017.\nBreaking NLP: Using morphosyntax, semantics,\npragmatics and world knowledge to fool sentiment\nanalysis systems. In Workshop on Building Linguis-\ntically Generalizable NLP Systems 1.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP)\n2014.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL HLT) 16.\nJanet B. Pierrehumbert. 2006. The statistical basis of\nan unnatural alternation. In Louis Goldstein, Dou-\nglas H. Whalen, and Catherine T. Best, editors, Lab-\noratory Phonology 8, pages 81–106. De Gruyter,\nBerlin.\nJanet B. Pierrehumbert and Ramon Granell. 2018. On\nhapax legomena and morphological productivity. In\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology (SIGMORPHON) 15.\nIngo Plag. 1999. Morphological productivity: Struc-\ntural constraints in English derivation. De Gruyter,\nBerlin.\nIngo Plag. 2003. Word-formation in English. Cam-\nbridge University Press, Cambridge, UK.\nIngo Plag and Laura W. Balling. 2020. Derivational\nmorphology: An integrative perspective on some\nfundamental questions. In Vito Pirrelli, Ingo Plag,\nand Wolfgang U. Dressler, editors, Word knowledge\nand word usage: A cross-disciplinary guide to the\nmental lexicon, pages 295–335. De Gruyter, Berlin.\nDragomir Radev, Hong Qi, Harris Wu, and Weiguo\nFan. 2002. Evaluating web-based question answer-\ning systems. In International Conference on Lan-\nguage Resources and Evaluation (LREC) 3.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. In arXiv 2002.12327.\nChenhao Tan and Lillian Lee. 2015. All who wan-\nder: On the prevalence and characteristics of multi-\ncommunity engagement. In International Confer-\nence on World Wide Web (WWW) 24.\nEkaterina Vylomova, Ryan Cotterell, Timothy Bald-\nwin, and Trevor Cohn. 2017. Context-aware predic-\ntion of derivational word-forms. In Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics (EACL) 15.\nGregor Wiedemann, Steffen Remus, Avi Chawla, and\nChris Biemann. 2019. Does BERT make any sense?\ninterpretable word sense disambiguation with con-\ntextualized embeddings. In arXiv 1909.10430.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc\nLe V, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. In arXiv 1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems (NeurIPS) 33.\nA Appendices\nA.1 Data Preprocessing\nWe ﬁlter the posts for known bots and spammers\n(Tan and Lee, 2015). We exclude posts written in\na language other than English and remove strings\ncontaining numbers, references to users, and hyper-\nlinks. Sentences are ﬁltered to contain between 10\nand 100 words. We control that derivatives do not\nappear more than once in a sentence.\nA.2 Hyperparameters\nWe tune hyperparameters on the development data\nseparately for each frequency bin (selection cri-\nterion: MRR). Models are trained with categori-\ncal cross-entropy as the loss function and Adam\n(Kingma and Ba, 2015) as the optimizer. Training\nand testing are performed on a GeForce GTX 1080\nTi GPU (11GB).\nDagoBERT. We use a batch size of 16\nand perform grid search for the learning rate\nl ∈ {1 ×10−6,3 ×10−6,1 ×10−5,3 ×10−5}\nand the number of epochs ne ∈{1,..., 8}(num-\nber of hyperparameter search trials: 32). All other\n3859\nhyperparameters are as for BERTBASE. The num-\nber of trainable parameters is 110,104,890.\nBERT+. We use a batch size of 16\nand perform grid search for the learning rate\nl ∈ {1 ×10−4,3 ×10−4,1 ×10−3,3 ×10−3}\nand the number of epochs ne ∈{1,..., 8}(num-\nber of hyperparameter search trials: 32). All other\nhyperparameters are as for BERTBASE. The num-\nber of trainable parameters is 622,650.\nLSTM. We initialize word embeddings with\n300-dimensional GloVe (Pennington et al., 2014)\nvectors and character embeddings with 100-\ndimensional random vectors. The BiLSTMs\nconsist of three layers and have a hidden\nsize of 100. We use a batch size of 64\nand perform grid search for the learning rate\nl ∈ {1 ×10−4,3 ×10−4,1 ×10−3,3 ×10−3}\nand the number of epochs ne ∈{1,..., 40}(num-\nber of hyperparameter search trials: 160). The\nnumber of trainable parameters varies with the type\nof the model due to different sizes of the output\nlayer and is 2,354,345 for P, 2,354,043 for S, and\n2,542,038 for PS models.11\nTable 10 lists statistics of the validation perfor-\nmance over hyperparameter search trials and pro-\nvides information about the best validation perfor-\nmance as well as corresponding hyperparameter\nconﬁgurations.12 We also report runtimes for the\nhyperparameter search.\nFor the models trained on the Vylomova et al.\n(2017) dataset, hyperparameter search is identi-\ncal as for the main models, except that we use\naccuracy as the selection criterion. Runtimes for\nthe hyperparameter search in minutes are 754 for\nSHARED and 756 for SPLIT in the case of DagoB-\nERT, and 530 for SHARED and 526 for SPLIT in\nthe case of LSTM. Best validation accuracy is .943\n(l = 3×10−6, ne = 7) for SHARED and .659\n(l = 1×10−5, ne = 4) for SPLIT in the case of\nDagoBERT, and .824 (l= 1×10−4, ne = 38) for\nSHARED and .525 (l = 1×10−4, ne = 33) for\nSPLIT in the case of LSTM.\nA.3 Hyperparameters\nWe use the HYP segmentation method for models\nwith morphologically correct segmentation. We\n11Since models are trained separately on the frequency bins,\nslight variations are possible if an afﬁx does not appear in a\nparticular bin. The reported numbers are for B1.\n12Since expected validation performance (Dodge et al.,\n2019) may not be correct for grid search, we report mean\nand standard deviation of the performance instead.\ntune hyperparameters on the development data sep-\narately for each frequency bin (selection criterion:\naccuracy). Models are trained with binary cross-\nentropy as the loss function and Adam as the op-\ntimizer. Training and testing are performed on a\nGeForce GTX 1080 Ti GPU (11GB).\nFor FROZEN, we use a batch size of 16\nand perform grid search for the learning rate\nl ∈ {1 ×10−4,3 ×10−4,1 ×10−3,3 ×10−3}\nand the number of epochs ne ∈ {1,..., 8}\n(number of hyperparameter search trials: 32).\nThe number of trainable parameters is 769.\nFor FINETUNED, we use a batch size of 16\nand perform grid search for the learning rate\nl ∈ {1 ×10−6,3 ×10−6,1 ×10−5,3 ×10−5}\nand the number of epochs ne ∈{1,..., 8}(num-\nber of hyperparameter search trials: 32). The num-\nber of trainable parameters is 109,483,009. All\nother hyperparameters are as for BERTBASE.\nTable 11 lists statistics of the validation perfor-\nmance over hyperparameter search trials and pro-\nvides information about the best validation perfor-\nmance as well as corresponding hyperparameter\nconﬁgurations. We also report runtimes for the\nhyperparameter search.\n3860\nSHARED SPLIT\nModel B1 B2 B3 B4 B5 B6 B7 B1 B2 B3 B4 B5 B6 B7\nDagoBERT\nP\nµ .349 .400 .506 .645 .777 .871 .930 .345 .364 .375 .383 .359 .359 .357\nσ .020 .037 .096 .160 .154 .112 .064 .018 .018 .018 .019 .018 .017 .022\nmax .372 .454 .657 .835 .896 .934 .957 .368 .385 .399 .412 .397 .405 .392\nl 1e-5 3e-5 3e-5 3e-5 1e-5 3e-6 3e-6 3e-6 1e-5 3e-6 1e-6 3e-6 1e-6 1e-6\nne 3 8 8 8 5 8 6 5 3 3 5 1 1 1\nS\nµ .386 .453 .553 .682 .805 .903 .953 .396 .403 .395 .395 .366 .390 .370\nσ .031 .058 .120 .167 .164 .118 .065 .033 .024 .020 .020 .019 .029 .027\nmax .419 .535 .735 .872 .933 .965 .976 .429 .430 .420 .425 .403 .441 .432\nl 3e-5 3e-5 3e-5 3e-5 1e-5 1e-5 3e-6 3e-5 1e-5 3e-6 1e-6 1e-6 1e-6 1e-6\nne 2 7 8 6 8 7 6 2 3 5 7 3 2 1\nPS\nµ .124 .214 .362 .554 .725 .840 .926 .119 .158 .175 .194 .237 .192 .176\nσ .018 .075 .173 .251 .238 .187 .119 .013 .013 .011 .016 .020 .021 .018\nmax .146 .337 .620 .830 .915 .945 .970 .135 .177 .192 .219 .269 .235 .209\nl 1e-5 3e-5 3e-5 3e-5 1e-5 3e-5 1e-5 1e-5 3e-6 3e-6 1e-6 1e-6 1e-6 1e-6\nne 6 8 8 5 8 3 7 6 8 3 4 4 1 1\nτ 192 230 314 440 631 897 1,098 195 228 313 438 631 897 791\nBERT+\nP\nµ .282 .336 .424 .527 .655 .764 .860 .280 .298 .318 .324 .323 .324 .322\nσ .009 .020 .046 .078 .090 .080 .051 .011 .007 .009 .013 .009 .012 .009\nmax .297 .374 .497 .633 .759 .841 .901 .293 .312 .334 .345 .341 .357 .346\nl 1e-4 1e-3 3e-3 1e-3 3e-4 3e-4 3e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4\nne 7 8 8 8 8 8 8 5 8 7 2 4 1 1\nS\nµ .358 .424 .491 .587 .708 .817 .886 .369 .364 .357 .350 .337 .335 .332\nσ .010 .018 .043 .073 .086 .072 .049 .010 .010 .010 .013 .017 .017 .009\nmax .372 .452 .557 .691 .806 .884 .925 .383 .377 .375 .372 .366 .377 .357\nl 1e-4 1e-3 1e-3 1e-3 1e-3 1e-3 3e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4\nne 4 7 8 8 7 7 8 8 4 5 1 1 1 1\nPS\nµ .084 .152 .257 .419 .598 .741 .849 .083 .104 .127 .137 .158 .139 .136\nσ .008 .024 .062 .116 .119 .099 .062 .009 .014 .015 .014 .017 .011 .008\nmax .099 .206 .371 .610 .756 .847 .913 .099 .131 .154 .170 .206 .173 .164\nl 1e-4 3e-3 3e-3 3e-3 1e-3 1e-3 1e-3 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4\nne 7 8 8 8 8 8 8 5 3 3 1 1 1 1\nτ 81 102 140 197 285 406 568 80 101 140 196 283 400 563\nLSTM\nP\nµ .103 .166 .314 .510 .661 .769 .841 .089 .113 .107 .106 .103 .103 .116\nσ .031 .072 .163 .212 .203 .155 .107 .019 .024 .020 .017 .010 .010 .013\nmax .159 .331 .583 .732 .818 .864 .909 .134 .152 .141 .138 .121 .120 .139\nl 1e-3 1e-3 1e-3 1e-3 3e-4 1e-4 3e-4 3e-4 3e-4 3e-4 3e-4 1e-4 1e-4 3e-4\nne 33 40 38 35 35 40 26 38 36 37 38 40 37 29\nS\nµ .124 .209 .385 .573 .721 .824 .881 .108 .133 .136 .132 .132 .127 .128\nσ .037 .098 .202 .229 .206 .162 .111 .029 .034 .027 .015 .013 .012 .012\nmax .214 .422 .674 .812 .882 .925 .945 .192 .187 .179 .157 .159 .157 .153\nl 3e-4 1e-3 1e-3 1e-3 3e-4 3e-4 3e-4 3e-4 3e-4 3e-4 3e-4 3e-4 1e-4 1e-4\nne 40 40 37 31 37 38 39 37 38 37 39 38 39 29\nPS\nµ .011 .066 .255 .481 .655 .776 .848 .009 .015 .025 .035 .046 .032 .071\nσ .005 .090 .256 .301 .276 .220 .177 .003 .004 .006 .006 .008 .008 .015\nmax .022 .346 .649 .786 .844 .886 .931 .016 .024 .038 .047 .065 .055 .104\nl 3e-3 3e-3 3e-3 3e-4 3e-4 3e-4 3e-4 3e-4 3e-3 3e-4 3e-4 3e-3 3e-3 3e-4\nne 38 40 39 40 33 40 39 40 39 23 32 28 15 31\nτ 115 136 196 253 269 357 484 100 120 142 193 287 352 489\nTable 10: Validation performance statistics and hyperparameter search details. The table shows the mean ( µ),\nstandard deviation (σ), and maximum ( max) of the validation performance (MRR) on all hyperparameter search\ntrials for preﬁx (P), sufﬁx (S), and preﬁx-sufﬁx (PS) models. It also gives the learning rate ( l) and number of\nepochs (ne) with the best validation performance as well as the runtime (τ) in minutes averaged over P, S, and PS\nfor one full hyperparameter search (32 trials for DagoBERT and BERT+, 160 trials for LSTM).\n3861\nFROZEN FINETUNED\nSegmentation B1 B2 B3 B4 B5 B6 B7 B1 B2 B3 B4 B5 B6 B7\nMorphological\nµ .617 .639 .650 .660 .671 .684 .689 .732 .760 .764 .750 .720 .692 .657\nσ .010 .009 .009 .008 .014 .009 .009 .016 .017 .029 .052 .067 .064 .066\nmax .628 .649 .660 .669 .682 .692 .698 .750 .779 .793 .802 .803 .803 .808\nl 3e-4 3e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-5 3e-6 3e-6 1e-6 1e-6 1e-6 1e-6\nne 8 5 7 7 5 3 5 4 8 5 8 3 2 1\nτ 137 240 360 516 765 1,079 1,511 378 578 866 1,243 1,596 1,596 1,793\nWordPiece\nµ .554 .561 .569 .579 .584 .592 .596 .706 .730 .734 .712 .669 .637 .604\nσ .011 .010 .011 .012 .011 .010 .011 .030 .021 .025 .052 .066 .061 .046\nmax .568 .574 .582 .592 .597 .605 .608 .731 .752 .762 .765 .763 .759 .759\nl 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-4 1e-5 3e-6 3e-6 1e-6 1e-6 1e-6 1e-6\nne 6 8 6 2 8 2 7 3 7 6 8 3 1 1\nτ 139 242 362 517 765 1,076 1,507 379 575 869 1,240 1,597 1,598 1,775\nTable 11: Validation performance statistics and hyperparameter search details. The table shows the mean ( µ),\nstandard deviation (σ), and maximum (max) of the validation performance (accuracy) on all hyperparameter search\ntrials for classiﬁers using morphological and WordPiece segmentations. It also gives the learning rate ( l) and\nnumber of epochs ( ne) with the best validation performance as well as the runtime ( τ) in minutes for one full\nhyperparameter search (32 trials for both morphological and WordPiece segmentations).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7580255270004272
    },
    {
      "name": "Language model",
      "score": 0.7041010856628418
    },
    {
      "name": "Vocabulary",
      "score": 0.5865670442581177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5846109390258789
    },
    {
      "name": "Segmentation",
      "score": 0.56501704454422
    },
    {
      "name": "Natural language processing",
      "score": 0.5646302700042725
    },
    {
      "name": "Morphology (biology)",
      "score": 0.5095541477203369
    },
    {
      "name": "Ranging",
      "score": 0.42893126606941223
    },
    {
      "name": "Linguistics",
      "score": 0.3045032024383545
    },
    {
      "name": "Philosophy",
      "score": 0.053993403911590576
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I185492890",
      "name": "University of Canterbury",
      "country": "NZ"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ],
  "cited_by": 1
}