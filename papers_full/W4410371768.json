{
  "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
  "url": "https://openalex.org/W4410371768",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2317795830",
      "name": "Haizhou Shi",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2166994266",
      "name": "Zihao Xu",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2107013702",
      "name": "Hengyi Wang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2421920035",
      "name": "Wei-yi Qin",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2112566389",
      "name": "Wen-Yuan Wang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2115914818",
      "name": "Yibin Wang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2124637199",
      "name": "Zifeng Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2765659507",
      "name": "Sayna Ebrahimi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2018762813",
      "name": "Hao Wang",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963588172",
    "https://openalex.org/W3170325272",
    "https://openalex.org/W4387725649",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W4387389466",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3154155772",
    "https://openalex.org/W4389157347",
    "https://openalex.org/W4402774437",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W4388788058",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4384434135",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4392384758",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4385574113",
    "https://openalex.org/W4402081998",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W4297971002",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3209540659",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W4385570326",
    "https://openalex.org/W2987553933",
    "https://openalex.org/W4404782827",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W3043569052",
    "https://openalex.org/W4321472357",
    "https://openalex.org/W3125116114",
    "https://openalex.org/W4404708545",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W4225620948",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W3004268082",
    "https://openalex.org/W4285283606",
    "https://openalex.org/W4367672983",
    "https://openalex.org/W4386721859",
    "https://openalex.org/W2963112576",
    "https://openalex.org/W4386076061",
    "https://openalex.org/W4385572132",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W4387928778",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W4385570142",
    "https://openalex.org/W4402671286",
    "https://openalex.org/W4402670088",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4312210066",
    "https://openalex.org/W2760781482",
    "https://openalex.org/W4312611779",
    "https://openalex.org/W4392173735",
    "https://openalex.org/W4401043205",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4401042743",
    "https://openalex.org/W6843138321",
    "https://openalex.org/W4312238419",
    "https://openalex.org/W4389163154",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W2954929116",
    "https://openalex.org/W4385568024",
    "https://openalex.org/W4380352301",
    "https://openalex.org/W4385774803",
    "https://openalex.org/W4390971216",
    "https://openalex.org/W4390601357",
    "https://openalex.org/W4281479826",
    "https://openalex.org/W4390962544",
    "https://openalex.org/W4387847108",
    "https://openalex.org/W3157898411",
    "https://openalex.org/W4390872020",
    "https://openalex.org/W4387892200",
    "https://openalex.org/W2600463316",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4391156274",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W1593271688",
    "https://openalex.org/W2531638282",
    "https://openalex.org/W3171460770",
    "https://openalex.org/W4296605845",
    "https://openalex.org/W4287332702"
  ],
  "abstract": "The challenge of effectively and efficiently adapting statically pre-trained Large Language Models (LLMs) to ever-evolving data distributions remains predominant. When tailored for specific needs, pre-trained LLMs often suffer from significant performance degradation in previous knowledge domains – a phenomenon known as “catastrophic forgetting” . While extensively studied in the Continual Learning (CL) community, this problem presents new challenges in the context of LLMs. In this survey, we provide a comprehensive overview and detailed discussion of the current research progress on LLMs within the context of CL. Besides the introduction of the preliminary knowledge, this survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning) , i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning) , i.e., continual adaptation across time and domains (Section 3). Following vertical continuity, we summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). We then provide an overview of evaluation protocols for continual learning with LLMs, along with currently available data sources (Section 5). Finally, we discuss intriguing questions related to continual learning for LLMs (Section 6). This survey sheds light on the relatively understudied domain of continually pre-training, adapting, and fine-tuning large language models, suggesting the necessity for greater attention from the community. Key areas requiring immediate focus include the development of practical and accessible evaluation benchmarks, along with methodologies specifically designed to counter forgetting and enable knowledge transfer within the evolving landscape of LLM learning paradigms. The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9042792320251465
    },
    {
      "name": "Natural language processing",
      "score": 0.4185764491558075
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3985273838043213
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 18
}