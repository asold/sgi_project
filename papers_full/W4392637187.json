{
  "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
  "url": "https://openalex.org/W4392637187",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096474468",
      "name": "Xuan Zhang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A1980296962",
      "name": "Wei Gao",
      "affiliations": [
        "Singapore Management University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2950031296",
    "https://openalex.org/W2759820691",
    "https://openalex.org/W4375957723",
    "https://openalex.org/W3017402509",
    "https://openalex.org/W3035317050",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3164693331",
    "https://openalex.org/W4385572870",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W3101757358",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4286978428",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W2946327826",
    "https://openalex.org/W2890801081",
    "https://openalex.org/W3045683288",
    "https://openalex.org/W3017344694",
    "https://openalex.org/W3175148050",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3169920290",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4300952844",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3105753178",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2061504941",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4298184231",
    "https://openalex.org/W4319653562"
  ],
  "abstract": "Xuan Zhang, Wei Gao. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 996–1011\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n996\nTowards LLM-based Fact Verification on News Claims with a Hierarchical\nStep-by-Step Prompting Method\nXuan Zhang and Wei Gao\nSchool of Computing and Information Systems\nSingapore Management University\n80 Stamford Rd, Singapore 178902\nxuanzhang.2020@phdcs.smu.edu.sg, weigao@smu.edu.sg\nAbstract\nWhile large pre-trained language models\n(LLMs) have shown their impressive capabili-\nties in various NLP tasks, they are still under-\nexplored in the misinformation domain. In this\npaper, we examine LLMs with in-context learn-\ning (ICL) for news claim verification, and find\nthat only with 4-shot demonstration examples,\nthe performance of several prompting methods\ncan be comparable with previous supervised\nmodels. To further boost performance, we\nintroduce a Hierarchical Step-by-Step (HiSS)\nprompting method which directs LLMs to sepa-\nrate a claim into several subclaims and then\nverify each of them via multiple questions-\nanswering steps progressively. Experiment re-\nsults on two public misinformation datasets\nshow that HiSS prompting outperforms state-\nof-the-art fully-supervised approach and strong\nfew-shot ICL-enabled baselines.\n1 Introduction\nMisinformation such as fake news often causes con-\nfusion or wrong belief because they contain claims\nthat are factually false or inaccurate (Lazer et al.,\n2018). To combat misinformation in news claims,\nstakeholders rely on fact-checking practices for\nclaim verification. Fact-checking services online,\nsuch as PolitiFact1 and Snopes2) require laborious\nmanual efforts, making it challenging to match the\nrapid pace of misinformation being produced.\nIn recent years, deep neural networks-based\nmisinformation detection and fact-checking meth-\nods have been studied extensively (Wang, 2017;\nRashkin et al., 2017; Popat et al., 2018; Ma et al.,\n2019; Kotonya and Toni, 2020; Atanasova et al.,\n2020; Yang et al., 2022). In particular, pre-trained\nlanguage models (PLMs) like BERT (Kenton and\nToutanova, 2019) have demonstrated superior re-\nsults and surpassed traditional methods in fake\n1https://www.politifact.com/.\n2https://www.snopes.com/.\nnews related benchmarks (Soleimani et al., 2020;\nAtanasova et al., 2020; Kruengkrai et al., 2021),\nthanks to their strong ability to understand nuanced\ncontext for more accurate decision. Recently, large\npre-trained language models (LLMs) with a mas-\nsive number of parameters, such as GPT-3.5, have\nshown impressive performances in various down-\nstream tasks (Brown et al., 2020; Wei et al., 2022;\nZhou et al., 2022; Press et al., 2022). But it is basi-\ncally unclear how well LLMs can perform on fact\nverification task as this is not at the core of LLM\npre-training (Brown et al., 2020; Anil et al., 2023).\nWhile it is not practical to directly fine-tune most\nLLMs, in-context learning (ICL) (Brown et al.,\n2020) offers an alternative way to instruct LLMs\nto learn new tasks via inference only, conditioning\non demonstration examples without any gradient\nupdates. Properly prompted LLMs can carry out\nsimilar steps of logical traces with that in demon-\nstration examples, which is known as Chain-of-\nThought (CoT) reasoning (Wei et al., 2022). This\ngenerative reasoning process not only enhances the\nmodel’s performance on tasks such as arithmetic,\ncommonsense, and symbolic reasoning, but also\nfacilitates the understanding of the underlying ra-\ntionale behind the results from LLMs.\nPrevious research has suggested the importance\nof reasoning in improving the accuracy and explain-\nability of fake news detection (Jin et al., 2022).\nHowever, leveraging LLM reasoning in the context\nof fake news related tasks remains under-explored.\nIn this work, we first evaluate three classical ICL\nmethods, including standard prompting and CoT-\nbased methods for news claim verification. The\nstandard prompting takes in a news claim for LLM\nto return its factuality judgment on the claim, while\nCoT additionally generates a series of intermediate\nverbal reasoning steps in the result. On two fake\nnews benchmark datasets RAWFC (Yang et al.,\n2022) and LIAR (Wang, 2017), we find that the\nstandard prompting performs comparably well as\n997\nChain-of-Thought Promp1ng\nClaim: Professor Lieber was arrested for hiding funds from \na Chinese lab tied to the new coronavirus.\nProfessor Lieber was arrested due to alleged funds from \nWUT and ties to a Chinese government program, unrelated \nto the novel coronavirus. Among [label set], the claim is \nclassified as false.\nClaim: Donald Trump has said he loves war, \"including with \nnukes.”\nHuman fact checker: half-true.\nVanilla CoT output: true.\nDuring his term as the 45th President of the US, Donald \nTrump gave speeches proclaiming his love for war. \nAmong [label set], the claim is classified as true.\nTrump did say the phrase “I love war in a certain way” . \nBut regarding ‘including nukes’, he suggested Japan \ncould benefit from having them, not the US using them.\nFigure 1: An example of claim verification based on\nvanilla CoT prompting. The claim (underlined) and CoT\n(in green) are given as a demonstration. The generated\nCoT (in italics) leads to an incorrect judgment due to\n(1) omission of necessary thoughts regarding “nukes”,\nand (2) fact hallucination about the war-loving speeches\nwithout specific evidence in the generated CoT (in blue).\nstrong supervised baselines, but the vanilla CoT\nis worse than the standard prompting, which is\ncounter-intuitive. We found that there are two\nmain issues causing the failure of vanilla CoT, as\nillustrated in Figure 1: (1) Omission of necessary\nthoughts – vanilla CoT tends to ignore some note-\nworthy parts in the claim, resulting in inaccurate\ndecisions; (2) Fact hallucination 3 – When neces-\nsary information is not available, the model tends\nto generate relevant but unreliable “facts” on its\nown, which misleads the final prediction.\nTo address the issues, we instruct LLMs to de-\ncompose a complex claim into smaller subclaims,\nso that the reasoning follows up with the fine-\ngrained decomposition. This aims to enable a\nmuch more thorough examination of the claim, re-\nducing the risk of overlooking necessary details\nin the claim and enhancing the reasoning effect\nbased on different reasoning chains. This is anal-\nogous to breaking down complex questions into\nsubquestions (Press et al., 2022) for QA and devis-\ning a plan for solving complex tasks into multiple\nsteps (Wang et al., 2023). Additionally, we instruct\nLLM to employ a search engine for providing up-\nto-date external information, aiding the model in\n3This type of hallucination is also referred to as the ex-\ntrinsic hallucination (Bang et al., 2023) that cannot be veri-\nfied with the given source, and the fact-conflicting hallucina-\ntion (Zhang et al., 2023) that, more broadly, are not faithful to\nestablished world knowledge.\nreasoning and mitigating the hallucination problem.\nIn light of this, we propose a Hierarchical Step-by-\nStep (HiSS) prompting method, which is composed\nof two main processes: (1) Claim Decomposition,\nwhich prompts the LLM to split a complex claim\ninto smaller subclaims. (2) Subclaim Verification,\nwhich prompts LLM to verify the subclaim step-by-\nstep employing a search engine to obtain relevant\nevidence. Our contributions are three-fold:\n• We investigate the ability of LLMs with ICL for\nnews claim verification. And we find that with\nonly four-shot demonstration examples, LLMs\ncan outperform most of the supervised methods,\nwhich indicates LLM is a promising tool to com-\nbat misinformation.\n• We propose a HiSS prompting method to prompt\nLLM to do fine-grained checking of news claims.\nExperiments on two public datasets show that\nHiSS-prompted LLMs outperform traditionally\nstrong fully-supervised models with an improve-\nment of 4.95% on average in macro-average F1\nand set a new state-of-the-art for few-shot news\nclaim verification4.\n• Compared with previous methods, our HiSS-\nprompted LLMs provide superior explanations,\nwhich are more fine-grained and easier to follow\nbased on automatic and human evaluation.\n2 Related Work\n2.1 Explainable Fake News Detection\nExisting research on explainable fake news detec-\ntion is mainly focused on generating explanations\nfrom input evidence. These approaches include\ngenerating human-comprehensible explanations for\ncandidate facts based on background knowledge en-\ncoded in the form of Horn clauses (Gad-Elrab et al.,\n2019), as well as using attention-based models to\nhighlight relevant factual words (Popat et al., 2018),\nnews attributes (Yang et al., 2019) and suspicious\nusers (Lu and Li, 2020). Such an approach is based\non general deep neural networks and knowledge\nbase instead of language models.\nLater, Atanasova et al. (2020) and Kotonya and\nToni (2020) propose directly producing veracity\nexplanations based on extractive and abstractive\nsummarization. However, these methods predomi-\nnantly generate explanations by summarizing fact-\nchecking articles. While such an approach can\n4Code and prompts data is available at https://github.\ncom/jadeCurl/HiSS.\n998\nsomewhat explain fact-checking decisions follow-\ning human thoughts written in the articles, it does\nnot reason based on raw evidence to form the\nthoughts for drawing conclusions, which should\nbe the core of fact verification.\n2.2 Fact Verification with Language Models\nPrevious research has utilized PLMs (e.g., BERT\nand BART) in fake news related tasks. For ex-\nample, Lee et al. (2020) directly uses the inter-\nnal knowledge implicitly stored as PLMs’ param-\neters for fact verification. Lewis et al. (2020) pro-\nposes a retrieval-augmented approach to endow lan-\nguage models with document retrieval capability,\nwhich was applied for selecting relevant evidence\nin fact extraction and verification. Instead of us-\ning language models to provide evidence only, Lee\net al. (2021) utilizes LLMs such as GPT-2 (Radford\net al., 2019) and their few-shot capability to assess\nthe claim’s factuality based on the perplexity of\nevidence-conditioned claim generation.\nResearch on utilizing the reasoning capabilities\nof LLMs, such as CoT-based reasoning, in the\nmisinformation domain is still limited. Recent\nworks (Press et al., 2022; Yao et al., 2023; Jiang\net al., 2023) find that combining LLM’s reason-\ning capability with accessibility to external knowl-\nedge is helpful to many reasoning-intensive NLP\ntasks including HotpotQA (Yang et al., 2018) and\nFEVER (Thorne et al., 2018). In contrast to exist-\ning works, our research is motivated by the counter-\nintuitive observation that CoT under-performs the\nstandard prompting in news claim verification, and\nexplores how to better elicit LLMs to mitigate two\nsalient issues of LLMs in this task. We focus on the\nverification of real-world news claims, which could\nbe more temporally dynamic and sensitive than\nFEVER type of claims, necessitating the model to\naccess up-to-date knowledge.\n3 Our HiSS Prompting Method\nIn this section, we address the two main issues\nof LLMs observed in the news claim verification\ntask, i.e., 1) Omission of necessary thoughts and\n2) Fact hallucination. We will first raise our spe-\ncific research questions, and then present our HiSS\nprompting method.\n3.1 Research Questions\nFor the omission of necessary thoughts, the basic\nresearch question we need to address would be:\n• How to instruct LLMs not to overlook any\ncrucial points of a claim in its CoT?\nThe context of real-world claims could be complex\nand deep. For example, the seemingly easy claim\nDonald Trump has said he loves war, “including\nwith nukes” is actually quite intricate, as it not only\nexplicitly states Trump’s declaration of love for\nboth regular and nuclear wars, but also implies that\nin order to verify the statement is factual or not, one\nhas to examine whether and in what circumstances\nhe has expressed such passion on both types of\nwars. Therefore, we propose to prompt LLMs to\nthoroughly generate all explicit and implicit points\nthat are check-worthy given a claim.\nHallucination is an intrinsic and fundamental\nproblem of LLMs (Ji et al., 2023; Bang et al., 2023).\nWe address it by providing relevant and up-to-date\ncontextual information to LLM as external knowl-\nedge, assuming that hallucinations most likely re-\nsult from the lack of knowledge on the necessary\ncontext (Bang et al., 2023). Our specific research\nquestion would be:\n• How can we determine when external knowl-\nedge is needed during the verification and\nassist LLM in acquiring the necessary knowl-\nedge to mitigate fact hallucination?\nWhile the decomposition can prompt LLM to raise\nfine-grained questions, the model may make up re-\nsponses when background information is lacking.\nFor instance, if the model is unaware of the spe-\ncific contexts of Trump’s wording on “war”5 and\n“nukes”6, it can lead to factually inaccurate answer,\nsuch as “During his term as the 45th President of\nthe US, Donald Trump gave speeches proclaiming\nhis love for war”.\nIn the following subsections, we will describe\nour Hierarchical Step-by-Step (HiSS) prompting\n5The comment regarding Trump’s “love” of war comes\nfrom his speech in Iowa on Nov. 12, 2015. In the speech,\nTrump theorized that former Iraqi leader Saddam Hussein\nfeigned having weapons of mass destruction to scare Iran,\nbefore briefly sidetracking into his feelings on war generally:\n“This is the Trump theory on war,” he said. “But I’m good at\nwar. I’ve had a lot of wars of my own. I’m really good at war.\nI love war in a certain way. But only when we win.”\n6Trump made his comments about “nukes” in an April 3 in-\nterview with Fox News Sunday’s Chris Wallace. Wallace was\nasking Trump about his suggestion that Japan might be better\noff with nuclear weapons. Trump suggested that Japan might\nneed to acquire nuclear weapons to defend against neighbor-\ning North Korea. It’s worth noting that the comment wasn’t\nabout the United States using nuclear weapons, but about his\nbelief that Japan might be better off if it had nuclear weapons.\n999\n[K-shot Demonstration Examples are omitted here for space. ]\nClaim: Donald Trump has said he loves war, \"including with nukes.\"\nA fact checker will decompose the claim into 2 subclaims that are easier to verify:\n1. Donald Trump has expressed a love for war.\n2. Donald Trump has expressed a love for nuclear wars.\nTo verify subclaim 1, a fact-checker will go through a step-by-step process to ask and answer \na series of questions relevant to its factuality. Here are the specific steps he/she raise each \nquestion and look for an answer:\nQuestion: Has Donald Trump expressed a love for war?\nTell me if you are confident to answer the question or not. Answer with ``yes'' or ``no’’: No.\nAnswer: […]Donald Trump said “I'm really good at war. I love war in a certain way. But\nonly when we win.\" […]  \nQuestion: In what context did Donald Trump express a love for war?\nTell me if you are confident to answer the question or not. Answer with ``yes'' or ``no’’: No.\nAnswer: […] after he said that former Iraqi President Saddam Hussein claimed to have\nweapons of mass destruction as a strategy “to scare” Iran. […]\nTo verify subclaim 2, a fact-checker will go through a step-by-step process to ask and answer \na series of questions relevant to its factuality. Here are the specific steps he/she raise each \nquestion and look for an answer:\nQuestion: Has Donald Trump expressed a love for war including nukes?\nTell me if you are confident to answer the question or not. Answer with ``yes'' or ``no’’: Yes.\nAnswer: No. The voices talking up dangers of nuclear war include Trump.\nQuestion: What was the context Trump express ‘including nukes’?\nTell me if you are confident to answer the question or not. Answer with ``yes'' or ``no’’:  No.\nAnswer:[…] Trump suggested that Japan might need to acquire nuclear weapons to defend\nagainst neighboring North Korea […] \nDecomposition\n(Level 1)\nStep-by-Step\nVerification\n(Level 2)\nAmong [label set], the claim is classified as half-true.Final \nPrediction\nGeneration\nExternal \nKnowledge\nWeb \nSearch\nFigure 2: Overview of the proposed HiSS model: Original human inputs are in red background, LLM directly\ngenerated text is in white, and answers generated based on search results are in green. We start by providing a\nfew-shot demonstration, followed by appending the claim to be checked (underlined). HiSS prompts the LLM to (1)\ndecompose the claim into subclaims; (2) verify each subclaim step-by-step via raising and answering a series of\nquestions. For each question, we prompt LLM to assess if it is confident to answer it or not, and if not, we input the\nquestion to a web search engine. The search results are then inserted back into the ongoing prompt to continue the\nverification process; (3) generate the final prediction. The detailed demonstrations are omitted in this illustration for\nspace which can be found in Table 8 (a) and Table 8 (b) in Appendix B .\nmethod. As shown in Figure 2, HiSS involves three\nprocesses: (1) Claim decomposition, (2) Subclaim\nstep-by-step verification, and (3) Final prediction.\n3.2 Claim Decomposition\nAt the first level of HiSS, we focus on instructing\nthe model to capture all the explicit points in the\noriginal claim and decompose them into subclaims.\nThis level aligns with previous studies (Ousidhoum\net al., 2022; Fan et al., 2020) in fact-checking,\nwhich found that segmenting the original claim by\nidentifying entities or focal points can facilitate hu-\nman fact-checkers in making informed judgments.\nHowever, these models require the manual collec-\ntion of datasets for training, whereas our method\nprompts LLM to do the decomposition guided by\nonly a few demonstration examples.\nSpeficially, LLM is prompted withK-shot (K is\na hyperparameter) demonstration examples (see Ta-\nble 8 (a) and Table 8 (b) in Appendix B for details)\nthat serve to illustrate the entire verification process,\nfollowed by the test claim to be checked, as shown\nin the Level 1 in Figure 2. The demonstration ex-\namples exhibit to LLM how to break down a claim\nci into a series of subclaims [si1, si2, ··· , siNi ] that\ncover all check-worthy points explicitly expressed.\nThe demonstration examples vary in their complex-\nity, with some simple claims not undergoing deep\ndecomposition and more complex claims being de-\ncomposed into a few more subclaims. The LLM\npresumably follows the demonstrated decomposi-\ntion approach in accordance with the complexity of\nthe input claim ci. Therefore, Ni is determined by\nLLM automatically. Figure 2 illustrates that LLM\ndecomposes the test claim into two subclaims.\n1000\n3.3 Subclaim-level Step-by-Step Verification\nIn the second level, LLM individually verifies each\nsubclaim obtained from Level 1. Underlying the\nexplicit points conveyed in each subclaim can be a\nfew implicit points that are not expressed but need\nfurther scrutinization in one way or the other. For\nexample, “Did Trump really say he loves war?”,\n“What is his exact wording?”, “In what context did\nhe express it?”, etc. for the first subsclaim “Donald\nTrump has expressed a love for war”.\nSpecifically, we leverage the reasoning capabil-\nity of LLM to delve deeper into the underlying\ninformation needed to validate each subclaim sij\nby generating a series of probing questions {qm\nij },\neach qm\nij corresponding to an implicit point. Simi-\nlarly, the number of probing questions of each sub-\nclaim is determined by LLM automatically with\nreference to the demonstration example. We adopt\na progressive approach to generate the questions.\nThis allows us to adjust the subsequent question\ngeneration based on the answers to previous ques-\ntions and the acquired context information on the\nchain. As a result, the generated questions become\nmore targeted and in-depth, facilitating a compre-\nhensive analysis of the subclaims.\nOnce a question qm\nij is generated, the next step\nis to elicit the corresponding answer am\nij from\nLLM. Recent works have found that providing\nLLMs with access to external knowledge can lead\nto notable improvements (Yao et al., 2023; Jiang\net al., 2023). An important consideration is how to\nprompt LLM to automatically decide when it needs\nto consult with an external knowledge source (e.g.,\nweb search engine), to mitigate fact hallucination.\nIt is hypothesized that LLM can be prompted to\nassess its own confidence in answering a question,\nso that we can acquire relevant external informa-\ntion to aid it when it lacks confidence. We resort to\nGoogle Search as an external source.\nSpecifically, LLM follows the specific format of\ndemonstration examples to generate questions: it\nstarts with the prefix “ Question:” and presents\nthe generated question qm\nij , followed by “Tell me\nif you are confident ... ”. We control the\nmodel to pause at the end of qm\nij by setting the\nphrase “Tell me if you are confident” as the\nstop sequence7. This aims to facilitate 1) extracting\n7The “stop sequence” mechanism is a setting provided\nby the OpenAI API ( https://help.openai.com/en/\narticles/5072263-how-do-i-use-stop-sequences ).\nWhen a specific word or phrase is set as a “stop sequence”,\nthe model will halt its generation upon encountering that word\nthe text of qm\nij , and 2) probing the LLM to assess\nits confidence in answering the question without\nadditional information. During its pause, we ap-\npend the following instruction: Tell me if you\nare confident to answer the question or\nnot. Answer with “yes” or “no”:, and set the\nstop sequence to ‘no’. This means that if the LLM\nresponds with ‘no’, the model will cease to further\ngenerate an answer for qm\nij , but wait for us to input\nqm\nij into Google Search API8 to obtain top search\nresults9, so that we can feed them into the LLM for\nit to generate the answer am\nij . However, if the LLM\nresponds with “yes”, the LLM does not halt and\nproceeds to generate the answer am\nij to the question.\nFollowing the specific format of the demonstration\nexample, after a prior question is addressed, the\nLLM continues to generate the subsequent ques-\ntion until it ceases to produce any more questions,\ntransitioning then to the final prediction phase.\n3.4 Final Prediction\nOnce all the subclaims have been verified, the\nLLM can make a final prediction. At this point,\nit outputs “Among [label set], the claim is\nclassified as” before providing the final answer,\nwhere [label set]is substituted with the actual\nlabel set for a specific dataset. This facilitates the\nparsing of the final prediction, as the predicted\nclass label will appear after the word “ as” in the\nlast output line.\n4 Experiments and Results\n4.1 Experimental Setup\nWe conducted experiments on two standard En-\nglish fake news datasets: 1) RA WFC(Yang et al.,\n2022) contains gold labels based on Snopes fact-\ncheck articles and follows a three-class classifica-\ntion scheme (True/False/Half); 2) LIAR (Wang,\n2017) contains gold labels based on PolitiFact\narticles with six classes (True/Mostly-true/Half-\ntrue/Barely-true/False/Pants-fire). Different from\nor phrase, allowing users to control the length or content of\nthe generated output.\n8https://serpapi.com.\n9Search results from fact-checking websites are filtered\nto avoid ground-truth leakage. Specifically, we remove\nthe search results with URLs containing keywords such\nas “fact check”, and “fact-checking” since the URL of\nfact-checking websites and fact-check articles on main-\nstream media, e.g., NY Times (https://www.nytimes.com/\nspotlight/fact-checks.), typically contain such keywords.\nAfter filtering, we choose the top-one snippet from the search\nresult to feed into the LLM.\n1001\nRA WFC LIAR\nVal. Test Val. Test\nClaim 200 200 1274 1,251\n# true 67 67 169 205\n# mostly-true - - 251 238\n# half-true 66 66 244 263\n# barely-true - - 236 210\n# false 67 67 259 249\n# pants-fire - - 115 86\nTable 1: Datasets statistics.\nFEVER (Thorne et al., 2018) which uses manu-\nally synthesized claims from Wikipedia articles,\nthe claims in these two datasets are based on real-\nworld news. Table 1 displays the statistics of\ndatasets. We use the provided valid-test split of\nboth datasets. The few-shot demonstration exam-\nples are randomly selected from the training set.\nFollowing Yang et al. (2022), we use macro-\naverage precision (P), recall ( R), and F1 (F1 =\n2RP\nR+P ) scores as the metrics for evaluation.\nSupervised baselines. We compare with seven\nstrong supervised models in claim verification: 1)\nCNN (Wang, 2017) uses a convolutional neural\nmodel to integrate claim information and avail-\nable metadata features (e.g. subject, speaker, and\nparty) to get the prediction; 2) RNN (Rashkin et al.,\n2017) uses recurrent neural networks to learn rep-\nresentation from word sequences of the claim. 3)\nDeClarE (Popat et al., 2018) considers word em-\nbedding from both the claim and searched exter-\nnal information as evidence. 4) SentHAN (Ma\net al., 2019) proposes a hierarchical attention net-\nwork to represent external evidence as well as their\nsemantic relatedness with the claim. 5) SBERT-\nFC (Kotonya and Toni, 2020) uses Sentence-BERT\nto encode both claim and evidence for classifica-\ntion. 6) GenFE (Atanasova et al., 2020) predicts\nfact-check results and generates explanations in the\nmulti-task setup. 7) CofCED (Yang et al., 2022)\nuses a hierarchical encoder for text representation\nand two coarse-to-fine cascaded selectors to extract\nkey evidence for news claim verification.\nFew-shot baselines. We employ the following\nfew-shot baselines for comparison:\n1) Standard Prompting (Brown et al., 2020) di-\nrectly asks the LLM to determine the class label of\nthe claim. 2) Vanilla CoT Prompting (Wei et al.,\n2022) asks the LLM to output a thought chain be-\nfore outputting the class label of the claim. The\ndemonstration examples for both standard prompt-\ning and vanilla CoT are shown in Table 6 and 7, re-\nspectively (see Appendix B). 3) Search-Augmented\nCoT Prompting: To compare with the baselines that\ncan access extra knowledge, we augment vanilla\nCoT with search engine by using the claim as query\nto retrieve the background information, and then let\nthe LLM output thought chain and class label based\non the information retrieved. 4) ReAct Prompt-\ning (Yao et al., 2023) is a variant of CoT that ex-\nplores the use of LLMs to generate both reasoning\ntraces and task-specific actions (e.g., search) in an\ninterleaved manner. For a fair comparison, we em-\nploy the same demonstration examples and search\nengine across the different systems.\nImplementation Details To ensure reproducibil-\nity, we generate outputs using greedy decoding\nby setting the temperature to 0. We also freeze\nthe search results for the same queries involved in\nthe experiments for fair comparison, as search en-\ngine results can potentially change over time across\ndifferent runs. We utilize the GPT-3.5 series API\ntext-davinci-00310 as a backbone LLM. Follow-\ning Wei et al. (2022), we tune the hyperparameter\nof the shot number within {1, 2, 4, 6, 8} on the vali-\ndation set, and find that the model achieved the best\nresults with 4 demonstration examples. Therefore,\nwe set K = 4throughout the experimentation.\n4.2 Results of Claim Veracity Classification\nTable 2 summarizes the performance of verification,\nand we have the following findings:\n• HiSS with LLM is comparable with or even\nbetter than the fully supervised SoTA. As\nshown in Table 2, HiSS outperforms previous\nSoTA (i.e., CofCED) by 1.9% and 8% in F1 on\nRAWFC and LIAR, respectively. This indicates\nthat few-shot ICL is promising for news claim\nverification, owing to the capabilities of LLM\nthat benefit from its parameterized knowledge\nwith a tremendous amount of facts. In addition,\nHiSS is specially designed to better elicit LLM\nand guide it through a step-by-step examination\nof the claim, covering both explicit and implicit\naspects and allowing for more comprehensive\nand thorough verification. Moreover, HiSS en-\n10https://platform.openai.com/docs/models/\ngpt-3-5.\n1002\nModel RA WFC LIAR\nP(%) R(%) F1(%) P(%) R(%) F1(%)\nFully Supervised Models\nCNN (Wang, 2017) 38.8 38.5 38.6 22.6 22.4 22.5\nRNN (Rashkin et al., 2017) 41.4 42.1 41.7 24.4 21.2 22.7\nDeClarE† (Popat et al., 2018) 43.4 43.5 43.4 22.9 20.6 21.7\nSentHAN† (Ma et al., 2019) 45.7 45.5 45.6 22.6 20.0 21.2\nSBERT♢ (Kotonya and Toni, 2020) 51.1 46.0 48.4 24.1 22.1 23.1\nGenFE♢ (Atanasova et al., 2020) 44.3 44.8 44.5 28.0 26.2 27.1\nCofCED† (Yang et al., 2022) 53.0 51.0 52.0 29.5 29.6 29.5\nFew-shot Models w/GPT3.5\nStandard Prompt (Brown et al., 2020) 48.5 48.5 48.5 29.1 25.1 27.0\nVanilla CoT (Wei et al., 2022) 42.4 46.6 44.4 22.6 24.2 23.7\nSearch-Augmented CoT† 47.2 51.4 49.2 27.5 23.6 25.4\nReAct† (Yao et al., 2023) 51.2 48.5 49.8 33.2 29.0 31.0\nHiSS† (ours) 53.4 54.4 ∗ 53.9∗ 46.8∗ 31.3∗ 37.5∗\nTable 2: Experimental results of claim verification. Bold denotes the best performance. ∗ means significantly better\nthan the previous SoTA (CofCED) with p <0.01. † uses external information obtained via search engines. ♢ uses\ngold evidence from fact-check reports. Results of fully supervised models are quoted from (Yang et al., 2022).\nables evidence acquisition via web search when\nnecessary, mitigating the risk of hallucination.\n• The performance of few-shot ICL methods\nvaries. Despite utilizing the same backbone,\nHiSS surpasses standard prompting, vanilla CoT,\nand ReAct by 7.95%, 11.65%, and 5.3% in F1\non average, respectively. This observation high-\nlights the importance of specific methods prompt-\ning LLM for news claim verification. After con-\nducting an in-depth error analysis on 40 ran-\ndomly selected samples for vanilla CoT, ReAct,\nand HiSS11, as shown in Table 3, we classify\nthe errors observed in the verification traces into\ntwo categories: (1) fact hallucination and (2)\nomission of necessary thoughts . We find that\nvanilla CoT exhibits substantial issues of both\nhallucination and thought omission. Although\nthe Search-Augmented CoT improves its perfor-\nmance, it still falls short of meeting the stan-\ndard prompting method. This suggests that using\nthe original claim as a search query may end\nup with insufficiently detailed and informative\nsearch results, which explains its subpar perfor-\nmance. In contrast, ReAct, with its ability to au-\ntonomously generate search queries and access\nexternal knowledge, effectively mitigates failures\n11We omit standard prompting as it directly outputs the final\nprediction without providing intermediate or reasoning steps.\nError Types CoT ReAct HiSS\nFact Hallucination 43% 28% 5%\nThoughts Omission 60% 53% 13%\nTable 3: Distribution of errors based on 40 examples\nfrom RAWFC, where Vanilla CoT, ReAct, and HiSS\ngive incorrect verification results.\ncaused by hallucinations. However, it encounters\nchallenges of thought omission as it may ignore\nnoteworthy points of a claim due to the lack of\nclaim decomposition and a fine-grained step-by-\nstep process. Our HiSS prompting method in-\nstead effectively addresses both issues, thanks\nto its ability to cover both explicit and implicit\npoints of the claim to get checked and the ability\nto seek necessary external knowledge supported\nby the search engine.\n4.3 Ablation Study\nTo analyze the impact of different configurations\nof HiSS, we conducted an ablation analysis on\nRAWFC as shown in Figure 3.\nEffect of claim decomposition: Firstly, we con-\nsider HiSS without claim decomposition, where we\ndirectly pose probing questions based on the origi-\nnal claim, bypassing the claim decomposition step\n1003\nF1(%)\n45\n50\n55\n60\nRAWFC\nHiSS HiSS w/o decomp.\nHiSS w/o Step-by-Step HiSS always search\nHiSS w/o Search\nFigure 3: Ablation results on RAWFC dataset.\nwhile keeping the step-by-step process. In this set-\nting, the performance of HiSS decreases by 1.5%.\nThis result demonstrates that claim decomposition,\nwhich separates the claim based on explicit points,\nis helpful in improving the final predictions.\nEffect of subclaim step-by-step verification:\nNext, we conduct an ablation study on the step-by-\nstep verification process for each subclaim. Instead\nof generating probing questions, we let LLM di-\nrectly verify subclaims by searching for relevant\nbackground information. Subsequently, the LLM\nmade predictions based on the subclaims and the\nretrieved information. Notably, this modification\nresulted in a 2.9% performance drop, underscoring\nthe importance of employing the subclaim step-by-\nstep verification approach to address the implicit\npoints associated with each subclaim.\nEffect of strategy using search: We compared\nthree approaches to explore the effect of differ-\nent strategies on using the search function or not\nand how it is used: 1) HiSS w/o search, which re-\nlies solely on the internal knowledge from LLM,\n2) HiSS always search, which always queries the\nsearch engine to access the external knowledge,\nand 3) HiSS, which lets LLM self-decide whether\nto use the search results in each step based on its\nown confidence (see Section 3.3).\nAs expected, the performance of HiSS w/o\nsearch is poor which achieves only an F1 of 49.8%,\nindicating that reliance solely on LLM’s internal\nknowledge is unreliable and insufficient. An in-\nteresting finding is that HiSS prompted to decide\nwhether to leverage search results or not based\non the self-confidence of LLM achieves an F1 of\n54.4%, which is just slightly worse than the HiSS\nalways search by 1.0%. Our further inspection re-\nveals that out of the 200 test claims on RAWFC,\na total of 934 questions are generated, and LLM\nflags 690 of them as being confident to answer.\nThis indicates that in cases where the model is con-\nfident, external knowledge from the search engine\ncan only marginally improve its performance, as\nthe model is capable of providing accurate answers.\nIn contrast, for the cases where the model lacks\nconfidence, leveraging search results can enhance\nits performance much more greatly. Assuming that\nwe can basically trust the factuality of search re-\nsults from the web, this suggests that the model\ndemonstrates a reasonably good estimation of its\nown confidence.\n4.4 Human Evaluation\nWe conduct a human evaluation to study the expla-\nnation quality of three different types of explana-\ntions: Gold justification given by human journalists,\nexplanations generated by the strongest supervised\nexplainable model CofCED, and the reasoning tra-\njectory generated from the HiSS method. We ask\nthree English-speaking judges to rate these expla-\nnations with scores of 1, 2, and 3 (higher is better)\naccording to the following criteria:\n• Coverage. The explanation and reasoning does\nnot miss any important points that contribute to\nthe check.\n• Non-redundancy. The explanation and reason-\ning provided only includes relevant information\nthat is necessary for understanding the claim and\nfact-checking it, without any redundant or re-\npeated information.\n• Readability. The explanation and reasoning is\nstraightforward and simple to read.\n• Overall. The overall quality of the generated\nexplanation and reasoning.\nWe randomly sample 34 claims from the LIAR test\nset. Three annotators rate them independently. We\ncompute Krippendorff’s α inter-annotator agree-\nment (IAA) (Hayes and Krippendorff, 2007) and\nget 0.36 for coverage, 0.42 for non-redundancy,\n0.30 for readability and 0.38 for overall.\nTable 4 shows the averaged scores of human\nevaluation. We find that the gold explanations are\nslightly better than HiSS-based explanations, while\nthe state-of-the-art automatic explainable claim ver-\nification model CofCED is the worst. In particular,\nfor the coverage criteria, HiSS can elicit explana-\ntions that are on par with the human-written ones.\nThis explains that our HiSS elicits GPT-3.5 to gen-\nerate more fine-grained checking points and steps.\nIn addition, the non-redundancy score is relatively\n1004\nRA WFC\nGold CofCED HiSS\nReadability 2.75 1.63 2.44\nCoverage 2.65 1.99 2.63\nNon-redundancy 2.72 1.28 2.25\nOverall 2.69 1.74 2.54\nTable 4: Average human ratings on explanations of ver-\nification for the claims in the RAWFC dataset. Gold,\nCofCED, and HiSS correspond to the explanations pro-\nduced by human journalists, CofCED and HiSS, respec-\ntively. A higher score means a better explanation. The\nhighest score is in bold, and the second is underlined.\nlower, since GPT-3.5 may generate repeated sub-\nclaims. We conjecture that this may be due to the\nintrinsic problem of greedy sampling of language\nmodels (Holtzman et al., 2019).\n5 Conclusion and Future Work\nIn this paper, we study different prompting methods\nfor using LLMs in news claim verification. We in-\ntroduce a hierarchical step-by-step (HiSS) method\nthat prompts LLM to perform the verification in\nfine-grained steps, aiming to mitigate the omis-\nsion of thoughts and fact hallucination. Validated\non two public datasets, HiSS prompting improves\nthe performance of LLMs on the task over fully-\nsupervised SoTA models and its strong few-shot\nICL-based counterparts. HiSS prompted explana-\ntions show superior explainability in their coverage\nand readability.\nIn the future, we will build a conversational fact-\nchecking model based on LLMs which can be user-\nfriendly and incorporate human fact-checkers in\nthe loop.\n6 Limitations\nDespite the promising performance of LLMs based\non few-shot ICL, fact verification is a challenging\nresearch problem given the fact that performance\nscores are still quite low in general. There are a few\nlimitations. Firstly, in this work, we highlight that\nall the baselines and our proposed method solely\nrely on textual information. We focus on an uni-\nmodal approach utilizing language models and do\nnot consider the potential assistance from other\nmodalities, such as images and videos, for this task.\nAlthough the exploration of multimodal approaches\nhas gradually drawn some research attention (Wang\net al., 2018; Silva et al., 2021; Bu et al., 2023), it\nfalls outside the scope of our current work.\nMeanwhile, the scope of this study is limited to\nthe verification of news claims, which represents\nonly a subset of the broader issue of misinforma-\ntion. Misinformation encompasses a wide range\nof false or misleading information, including ru-\nmors, fake news articles, and spams (Wu et al.,\n2019). While our focus was specifically on news\nclaims, future research could explore the detection\nand mitigation of misinformation in other formats.\nFurther, our proposed prompting method heav-\nily relies on the capabilities of backbone LLMs,\nwhich can come with substantial computational\ncosts. Our method leverages the advancements\nin multi-step reasoning exhibited by these LLMs,\nnecessitating high-performance expectations. How-\never, it is worth noting that most state-of-the-art\nLLMs are currently not open-source and only avail-\nable as services. For instance, GPT-3.5 can only\nbe accessed via API. The reliance on such LLMs\nmakes deep model control infeasible, and the need\nfor API access poses challenges in terms of cost.\nFinally, while our approach leverages search en-\ngines to mitigate the fact hallucination issue in\nLLMs, it operates under the assumption that perti-\nnent information is readily accessible through web\nsearch. However, not all information is indexed or\navailable in search engines. For instance, if some-\none claims to have witnessed a rare meteorological\nphenomenon in a small town, such event might not\nbe reported on major news websites or databases.\nSuch firsthand, non-digitized accounts might be\nretrieved or fact-checked. This underscores the\nlimitation in relying solely on search engines as\na primary source of external knowledge for fact-\nchecking with LLMs. Another limitation of our\nmethod lies in the claims that are beyond estab-\nlished world knowledge when necessary relevant\nknowledge is not complete or even not available.\nThis necessitates the model’s ability to infer novel\nknowledge by formulating and subsequently vali-\ndating appropriate hypotheses, a task that remains\nbeyond the capabilities of existing technologies.\nAcknowledgement\nWe thank the anonymous reviewers for their helpful\ncomments during the review of this paper.\n1005\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. Generating fact\nchecking explanations. In ACL, pages 7352–7364.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPs, volume 33, pages 1877–1901.\nYuyan Bu, Qiang Sheng, Juan Cao, Peng Qi, Dand-\ning Wang, and Jintao Li. 2023. Online misinfor-\nmation video detection: a survey. arXiv preprint\narXiv:2302.03242.\nAngela Fan, Aleksandra Piktus, Fabio Petroni, Guil-\nlaume Wenzek, Marzieh Saeidi, Andreas Vlachos,\nAntoine Bordes, and Sebastian Riedel. 2020. Gener-\nating fact checking briefs. In EMNLP, pages 7147–\n7161.\nMohamed H Gad-Elrab, Daria Stepanova, Jacopo Ur-\nbani, and Gerhard Weikum. 2019. Exfakt: A frame-\nwork for explaining facts over knowledge graphs and\ntext. In WSDM, pages 87–95.\nAndrew F Hayes and Klaus Krippendorff. 2007. An-\nswering the call for a standard reliability measure for\ncoding data. Communication methods and measures,\n1(1):77–89.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. In ICLR.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nYiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun,\nWei Wang, Hao Liao, and Xing Xie. 2022. Towards\nfine-grained reasoning for fake news detection. In\nAAAI, volume 36, pages 5746–5754.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nNAACL, pages 4171–4186.\nNeema Kotonya and Francesca Toni. 2020. Explainable\nautomated fact-checking for public health claims. In\nEMNLP, pages 7740–7754.\nCanasai Kruengkrai, Junichi Yamagishi, and Xin Wang.\n2021. A multi-level attention model for evidence-\nbased fact checking. In ACL, pages 2447–2460.\nDavid MJ Lazer, Matthew A Baum, Yochai Ben-\nkler, Adam J Berinsky, Kelly M Greenhill, Filippo\nMenczer, Miriam J Metzger, Brendan Nyhan, Gordon\nPennycook, David Rothschild, et al. 2018. The sci-\nence of fake news. Science, 359(6380):1094–1096.\nNayeon Lee, Yejin Bang, Andrea Madotto, and Pascale\nFung. 2021. Towards few-shot fact-checking via\nperplexity. In NAACL, pages 1971–1981.\nNayeon Lee, Belinda Z Li, Sinong Wang, Wen-tau Yih,\nHao Ma, and Madian Khabsa. 2020. Language mod-\nels as fact checkers? In FEVER Workshop, pages\n36–41.\n1006\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. In NeurIPS, vol-\nume 33, pages 9459–9474.\nYi-Ju Lu and Cheng-Te Li. 2020. Gcan: Graph-aware\nco-attention networks for explainable fake news de-\ntection on social media. In ACL, pages 505–514.\nJing Ma, Wei Gao, Shafiq Joty, and Kam-Fai Wong.\n2019. Sentence-level evidence embedding for claim\nverification with hierarchical attention networks. In\nACL.\nNedjma Ousidhoum, Zhangdie Yuan, and Andreas Vla-\nchos. 2022. Varifocal question generation for fact-\nchecking. arXiv preprint arXiv:2210.12400.\nKashyap Popat, Subhabrata Mukherjee, Andrew Yates,\nand Gerhard Weikum. 2018. Declare: Debunking\nfake news and false claims using evidence-aware\ndeep learning. In EMNLP, pages 22–32.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana\nV olkova, and Yejin Choi. 2017. Truth of varying\nshades: Analyzing language in fake news and politi-\ncal fact-checking. In EMNLP, pages 2931–2937.\nAmila Silva, Ling Luo, Shanika Karunasekera, and\nChristopher Leckie. 2021. Embracing domain dif-\nferences in fake news: Cross-domain fake news de-\ntection using multi-modal data. In AAAI, volume 35,\npages 557–565.\nAmir Soleimani, Christof Monz, and Marcel Worring.\n2020. Bert for evidence retrieval and claim verifica-\ntion. In ECIR, pages 359–366.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. In NAACL, pages 809–819.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. arXiv\npreprint arXiv:2305.04091.\nWilliam Yang Wang. 2017. “liar, liar pants on fire”: A\nnew benchmark dataset for fake news detection. In\nACL (short), pages 422–426.\nYaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan,\nGuangxu Xun, Kishlay Jha, Lu Su, and Jing Gao.\n2018. Eann: Event adversarial neural networks for\nmulti-modal fake news detection. In Proceedings\nof the 24th acm sigkdd international conference on\nknowledge discovery & data mining, pages 849–857.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nLiang Wu, Fred Morstatter, Kathleen M Carley, and\nHuan Liu. 2019. Misinformation in social media:\ndefinition, manipulation, and detection. In SIGKDD,\nvolume 21, pages 80–90.\nFan Yang, Shiva K Pentyala, Sina Mohseni, Mengnan\nDu, Hao Yuan, Rhema Linder, Eric D Ragan, Shui-\nwang Ji, and Xia Hu. 2019. Xfake: Explainable fake\nnews detector with visualizations. In WWW, pages\n3600–3604.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nIn EMNLP, pages 2369–2380.\nZhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin,\nZiyang Luo, and Yi Chang. 2022. A coarse-to-fine\ncascaded evidence-distillation neural network for ex-\nplainable fake news detection. In COLING, pages\n2608–2621.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In ICLR.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, et al. 2023. Siren’s song in the ai ocean:\nA survey on hallucination in large language models.\narXiv preprint arXiv:2309.01219.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n1007\nA Case Study\nTable 5 shows the example from LIAR where CoT\n1) generates sentences containing fact hallucination,\nwith inaccurate percentages (i.e., 57 and 1 percent),\nand 2) omits necessary verification of the given per-\ncentage. While its final prediction is correct, it still\nencounters the issue of omitting necessary thoughts\nby not verifying the authenticity of the “1 percent-\nage to food and agriculture” claim before making a\njudgment on the entire claim’s correctness. In con-\ntrast, our HiSS method examines each component\nof the claim, along with leveraging search engine\nto gather comprehensive information. However, it\nis worth noting that even though HiSS can gener-\nate a more detailed reasoning process, it still faces\nchallenges related to repeated generations.\nB Demonstration Examples\nWe show the demonstration examples used in the\nLIAR dataset. Table 6 and 7 present the prompts\nwe used for standard prompting and vanilla CoT, re-\nspectively. Table 8 (a) and 8 (b) present the prompts\nused for HiSS.\n1008\nGround-truth: False.\nClaim: Says 57 percent of federal spending goes to the military and just 1 percent goes to food and agriculture, including\nfood stamps.\nStandard Prompting\nTrue.\nVanilla CoT\nAccording to the Congressional Budget Office, the federal government spent $4.1 trillion. Of that, 57 percent went to the\nmilitary and 1 percent went to food and agriculture, including food stamps. Thus the claim is classified as true.\nReAct\nThought 1: I need to search for information about federal spending to verify the claim that 57 percent goes to the military\nand 1 percent goes to food and agriculture, including food stamps.\nAction 1: Search[federal spending breakdown]\nObservation 1: This means federal spending was equal to 25%\nof the total gross domestic product (GDP), or economic ...\"\nThought 2: The observation contradicts the claim that 57 percent goes to the military and 1 percent to food and\nagriculture, including food stamps. The claim appears to be false.\nAction 2: Finish[False]\nHiSS\nA fact checker will decompose the claim into 2 subclaims that are easier to verify:\n1. 57 percent of federal spending goes to the military.\n2. Just 1 percent of federal spending goes to food and agriculture, including food stamps.\nTo verify subclaim 1, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: What percentage of federal spending goes to the military?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: Yes.\nAnswer: About one-sixth of federal spending goes to national defense. CBO estimates the budgetary effects of\nlegislation related to national security and assesses the cost-effectiveness of current and proposed defense programs.\nCBO also analyzes federal programs and issues related to veterans.\nTo verify subclaim 2, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: What percentage of federal spending goes to food and agriculture, including food stamps? Question: What\npercentage of federal spending goes to food and agriculture, including food stamps?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Federal spending on USDA’s food and nutrition assistance programs totaled $182.5 billion, 49 percent more\nthan the ... USDA’s food and nutrition assistance programs accounted for about 5 percent of total federal outlays.\nBased on the answers to these questions, it is clear that among pants-fire, false, barely-true, half-true, mostly-true, and\ntrue, the claim can be classified as false. Based on the answers to these questions, it is clear that among pants-fire, false,\nbarely-true, half-true, mostly-true, and true, the claim can be classified as false.\nTable 5: Case study. Labels and predictions from different methods are in pink, generated sentences containing fact\nhallucination are in purple, and repeated sentences are in yellow.\nQ: Among pants-fire, false, barely-true, half-true, mostly-true, and true, the claim \"Emerson Moser, who was Crayola’s\ntop crayon molder for almost 40 years, was colorblind.\" is classified as\nA: mostly-true.\nQ: Among pants-fire, false, barely-true, half-true, mostly-true, and true, the claim \"Bernie Sanders said 85 million\nAmericans have no health insurance.\" is classified as\nA: half-true.\nQ: Among pants-fire, false, barely-true, half-true, mostly-true, and true, the claim \"JAG charges Nancy Pelosi with\ntreason and seditious conspiracy.\" is classified as\nA: pants-fire.\nQ: Among pants-fire, false, barely-true, half-true, mostly-true, and true, the claim \"Cheri Beasley “backs tax hikes —\neven on families making under $75,000.\" is classified as\nA:barely-true.\nTable 6: Demonstration examples of standard prompting.\n1009\nQ: Choose a label from pants-fire, false, barely-true, half-true, mostly-true, and true for the following claim.\nClaim: \"Emerson Moser, who was Crayola’s top crayon molder for almost 40 years, was colorblind.\"\nA: Moser did have a type of colorblindness. However he said he only had blue-green colorblindness, So only some colors\nwere confusing to him. Thus the claim is classified as mostly-true.\nQ: Choose a label from pants-fire, false, barely-true, half-true, mostly-true, and true for the following claim.\nClaim: \"Bernie Sanders said 85 million Americans have no health insurance.\"\nA: Sanders accurately cited the Commonwealth Fund report in an October op-ed. His statement on CNN was partially\naccurate but left out important details and context. Sanders omitted that his figure included people who either have no\nhealth insurance or are underinsured. Thus the claim is classified as half-true.\nQ: Choose a label from pants-fire, false, barely-true, half-true, mostly-true, and true for the following claim.\nClaim: \"JAG charges Nancy Pelosi with treason and seditious conspiracy.\"\nA: A spokesperson for the U.S. Navy Judge Advocate General’s Corps told PolitiFact it isn’t true that Pelosi has been\ncharged. The original claim appeared on a website that posts fantastical, false stories and has a disclaimer that the\nsite contains “humor, parody, and satire.” Thus the claim is classified as pants-fire.\nQ: Choose a label from pants-fire, false, barely-true, half-true, mostly-true, and true for the following claim.\nClaim: \"Cheri Beasley “backs tax hikes — even on families making under $75,000.\"\nA: Beasley does support the Inflation Reduction Act but the claim is wrong to suggest it includes a change in tax rates\nfor that income bracket. Expert analyses show that any effect on those earners would be on the scale of $350 and, by one\nestimate, delayed for several years. Thus the claim is classified as barely-true.\nTable 7: Demonstration examples of vanilla CoT prompting. The CoT process is highlighted in green.\n1010\nQ: Claim: “Emerson Moser, who was Crayola’s top crayon molder for almost 40 years, was colorblind.”\nA: A fact checker will decompose the claim into 4 subclaims that are easier to verify:\n1. Emerson Moser was a crayon molder at Crayola.\n2. Moser worked at Crayola for almost 40 years.\n3. Moser was Crayola’s top crayon molder.\n4. Moser was colorblind.\nTo verify subclaim 1, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Is there any official record or documentation indicating that Emerson Moser worked as a crayon molder\nat Crayola?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: Yes.\nAnswer: Yes.\nTo verify subclaim 2, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Are there any official records or documentation confirming Emerson Moser’s length of employment at\nCrayola?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Emerson Moser, who is retiring next week after 35 years, isn’t colorblind in the sense that he can’t see\ncolor at all. It’s just that some ...\nTo verify subclaim 3, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Are there credible sources or publications that mention Emerson Moser as Crayola’s top crayon molder?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: Yes.\nAnswer: Yes.\nTo verify subclaim 4, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Are there any credible sources or records indicating that Emerson Moser was colorblind?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: Yes.\nAnswer: Yes.\nQuestion: Was Emerson Moser’s colorblindness only confusing for certain colors?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Moser has had tritanomaly, a type of colorblindness that makes it difficult to distinguish between blue and\ngreen and between yellow and red.\nBased on the answers to these questions, it is clear that among pants-fire, false, barely-true, half-true, mostly-true, and\ntrue, the claim can be classified as mostly-true.\nQ: Claim: “Bernie Sanders said 85 million Americans have no health insurance.”\nA: A fact checker will not split the claim since the original claim is easier to verify.\nTo verify the claim, a fact-checker will go through a step-by-step process to ask and answer a series of questions relevant\nto its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: How many Americans did Bernie Sanders claim had no health insurance?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: “We have 85 million Americans who have no health insurance,” Sanders said Dec. 11 on CNN’s State of\nthe Union.\nQuestion: How did Bernie Sanders define “no health insurance”?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Sanders spokesperson Mike Casca said the senator was referring to the number of uninsured and\nunder-insured Americans and cited a report about those numbers for adults.\nQuestion: How many Americans were uninsured or under-insured according\nto the Commonwealth Fund survey?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: The Commonwealth Fund survey found that 43% of working-age adults 19 to 64, or about 85 million\nAmericans, were uninsured or inadequately insured.\nQuestion: Is the statement “we have 85 million Americans who have no health insurance” partially accurate according\nto the information in the passage?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Bernie Sanders omitted that his figure included people who either have no health insurance or are\nunder-insured.\nBased on the answers to these questions, it is clear that among pants-fire, false, barely-true, half-true, mostly-true, and\ntrue, the claim is classified as half-true.\nTable 8 (a): Demonstration examples of HiSS prompting. The CoT process is highlighted in green.\n1011\nQ: Claim: “JAG charges Nancy Pelosi with treason and seditious conspiracy.”\nA: A fact checker will decompose the claim into 2 subclaims that are easier to verify:\n1. JAG has made a claim or accusation against Nancy Pelosi.\n2. The specific charges or allegations made against Nancy Pelosi are treason and seditious conspiracy.\nTo verify subclaim 1, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Is it true that JAG has made a claim or accusation against Nancy Pelosi?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: There is no evidence to support this claim and a spokesperson for the U.S. Navy Judge Advocate General’s\nCorps has stated that it is not true.\nTo verify subclaim 2, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Is it true that the specific charges or allegations made against Nancy Pelosi are treason and seditious\nconspiracy?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: There is no evidence to support this claim.\nQuestion: Where is the source of the claim?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: Real Raw News, a disclaimer stating that it contains “humor, parody and satire” and has a history of\npublishing fictitious stories.\nBased on the answers to these questions, it is clear that among pants-fire, false, barely-true, half-true, mostly-true, and\ntrue, the claim is classified as pants-fire.\nQ: Claim: “Cheri Beasley “backs tax hikes — even on families making under $75,000.”\nA: A fact checker will decompose the claim into 2 subclaims that are easier to verify:\n1. Cheri Beasley supports tax increases.\n2. Cheri Beasley supports tax increases for families with an income under $75,000.\nTo verify subclaim 1, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Does Cheri Beasley supports tax increases?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: Yes.\nAnswer: Beasley supports student loan bailouts for the wealthy.\nTo verify subclaim 2, a fact-checker will go through a step-by-step process to ask and answer a series of questions\nrelevant to its factuality. Here are the specific steps he/she raise each question and look for an answer:\nQuestion: Does the ad accurately link Beasley’s position on student loan debt forgiveness with her stance on tax hikes\nfor families making under $75,000 per year?\nTell me if you are confident to answer the question or not. Answer with “yes” or “no”: No.\nAnswer: The ad makes a misleading connection between the two issues and does not accurately represent Beasley’s\nposition on tax hikes for families making under $75,000 per year.\nAnswer: No.\nBased on the answers to these questions, it is clear that among pants-fire, false, barely-true, half-true, mostly-true, and\ntrue, the claim is classified as barely-true.\nTable 8 (b): Demonstration examples of HiSS prompting. The CoT process is highlighted in green.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7250036001205444
    },
    {
      "name": "Programming language",
      "score": 0.3575403690338135
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    }
  ]
}