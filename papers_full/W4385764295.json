{
  "title": "Bidirectional Dilation Transformer for Multispectral and Hyperspectral Image Fusion",
  "url": "https://openalex.org/W4385764295",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4208429286",
      "name": "Shangqi Deng",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A4200974587",
      "name": "Liang-Jian Deng",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2110569056",
      "name": "Xiao Wu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2112702300",
      "name": "Ran Ran",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2005905043",
      "name": "Rui Wen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285119709",
    "https://openalex.org/W2090640831",
    "https://openalex.org/W1495168473",
    "https://openalex.org/W4287122968",
    "https://openalex.org/W3014967571",
    "https://openalex.org/W4287274255",
    "https://openalex.org/W2168478992",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W3075397214",
    "https://openalex.org/W4288064619",
    "https://openalex.org/W2910457605",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W2964209782",
    "https://openalex.org/W3009455122",
    "https://openalex.org/W4225982893",
    "https://openalex.org/W1977177161",
    "https://openalex.org/W3083606623",
    "https://openalex.org/W3160949528",
    "https://openalex.org/W2804744787",
    "https://openalex.org/W4226469049",
    "https://openalex.org/W3004925702",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3085879958",
    "https://openalex.org/W2945202593",
    "https://openalex.org/W2028513126",
    "https://openalex.org/W4206377169",
    "https://openalex.org/W2989355516",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W3165892790",
    "https://openalex.org/W2964275574",
    "https://openalex.org/W4312331860",
    "https://openalex.org/W4313071068",
    "https://openalex.org/W4306815570",
    "https://openalex.org/W4319663728",
    "https://openalex.org/W2349229101"
  ],
  "abstract": "Transformer-based methods have proven to be effective in achieving long-distance modeling, capturing the spatial and spectral information, and exhibiting strong inductive bias in various computer vision tasks. Generally, the Transformer model includes two common modes of multi-head self-attention (MSA): spatial MSA (Spa-MSA) and spectral MSA (Spe-MSA). However, Spa-MSA is computationally efficient but limits the global spatial response within a local window. On the other hand, Spe-MSA can calculate channel self-attention to accommodate high-resolution images, but it disregards the crucial local information that is essential for low-level vision tasks. In this study, we propose a bidirectional dilation Transformer (BDT) for multispectral and hyperspectral image fusion (MHIF), which aims to leverage the advantages of both MSA and the latent multiscale information specific to MHIF tasks. The BDT consists of two designed modules: the dilation Spa-MSA (D-Spa), which dynamically expands the spatial receptive field through a given hollow strategy, and the grouped Spe-MSA (G-Spe), which extracts latent features within the feature map and learns local data behavior. Additionally, to fully exploit the multiscale information from both inputs with different spatial resolutions, we employ a bidirectional hierarchy strategy in the BDT, resulting in improved performance. Finally, extensive experiments on two commonly used datasets, CAVE and Harvard, demonstrate the superiority of BDT both visually and quantitatively. Furthermore, the related code will be available at the GitHub page of the authors.",
  "full_text": "Bidirectional Dilation Transformer for Multispectral and Hyperspectral Image\nFusion\nShangqi Deng, Liang-Jian Deng\u0003 , Xiao Wu, Ran Ran and Rui Wen\nUniversity of Electronic Science and Technology of China\nshangqideng0124@gmail.com, liangjian.deng@uestc.edu.cn, wxwsx1997@gmail.com,\nranran@std.uestc.edu.cn wenrui202102@163.com\nAbstract\nTransformer-based methods have proven to be ef-\nfective in achieving long-distance modeling, cap-\nturing the spatial and spectral information, and\nexhibiting strong inductive bias in various com-\nputer vision tasks. Generally, the Transformer\nmodel includes two common modes of multi-head\nself-attention (MSA): spatial MSA (Spa-MSA) and\nspectral MSA (Spe-MSA). However, Spa-MSA\nis computationally efﬁcient but limits the global\nspatial response within a local window. On the\nother hand, Spe-MSA can calculate channel self-\nattention to accommodate high-resolution images,\nbut it disregards the crucial local information that\nis essential for low-level vision tasks. In this study,\nwe propose a bidirectional dilation Transformer\n(BDT) for multispectral and hyperspectral image\nfusion (MHIF), which aims to leverage the advan-\ntages of both MSA and the latent multiscale infor-\nmation speciﬁc to MHIF tasks. The BDT consists\nof two designed modules: the dilation Spa-MSA\n(D-Spa), which dynamically expands the spatial re-\nceptive ﬁeld through a given hollow strategy, and\nthe grouped Spe-MSA (G-Spe), which extracts la-\ntent features within the feature map and learns lo-\ncal data behavior. Additionally, to fully exploit\nthe multiscale information from both inputs with\ndifferent spatial resolutions, we employ a bidirec-\ntional hierarchy strategy in the BDT, resulting in\nimproved performance. Finally, extensive experi-\nments on two commonly used datasets, CA VE and\nHarvard, demonstrate the superiority of BDT both\nvisually and quantitatively. Furthermore, the re-\nlated code will be available at the GitHub page of\nthe authors.\n1 Introduction\nHyperspectral imaging (HSI) is a widely used technology\nin various ﬁelds, including agriculture [Lu et al., 2020;\nWu et al., 2011], food safety [Feng and Sun, 2012], biomed-\n\u0003Corresponding author\nFigure 1: The comparison of (a) Spa-MSA [Liu et al., 2021 ], (b)\nthe proposed D-Spa based on Spa-MSA, (c) Spe-MSA[Zamir et al.,\n2022], and (d) the proposed G-Spe based on Spe-MSA. The blue\nclusters indicate the image tokens in (a) and (b). Utilizing the dila-\ntion operation, the proposed D-Spa can expand the receptive ﬁeld of\nSpa-MSA. In (c) and (d), the blue slices denote the image tokens,\nand we design G-Spe to allow the model to learn more data behavior\ninside the feature map.\nical diagnostics [Piqueras et al., 2011 ], and atmospheric en-\nvironment detection [Gao et al., 2006]. HSIs with high spec-\ntral resolution produce precise spectral characteristic curves,\nand the abundance of bands makes it convenient for mutual\nband correction. However, due to the current physical imag-\ning technology’s constraints, there is a trade-off between the\nspatial and spectral resolution of the natural imaging pro-\ncess. Therefore, it is impossible to produce an image with\nhigh spatial and spectral resolution simultaneously. As a re-\nsult, multispectral and hyperspectral image fusion (MHIF)\nhas emerged as a promising method to generate the neces-\nsary high-resolution hyperspectral images (HR-HSI). Numer-\nous approaches have been developed for MHIF and can be\nbroadly categorized into two categories: traditional meth-\nods [Guo et al., 2020; Yang et al., 2020b; Yang et al., 2020a]\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3633\nand deep learning (DL)-based techniques [Yan et al., 2022;\nZhou et al., 2022; Cao et al., 2020].\nIn recent years, deep learning (DL)-based techniques have\nbecome increasingly popular, with CNN modules being\nthe current state-of-the-art for MHIF problems due to their\nspatial-agnostic and channel-speciﬁc convolutional proper-\nties [Li et al., 2021 ]. Researchers have designed speciﬁc\nconvolution modules and stacked them to construct a gen-\neral network structure that effectively extracts potential be-\nhavior from databases. However, the local receptive ﬁeld in\nCNNs limits long-range dependencies and may hinder the in-\nternal modeling of the image. Recently, the Vision Trans-\nformer (ViT)[Kolesnikov et al., 2021] has demonstrated im-\npressive performance on various computer vision tasks[Hu et\nal., 2022]. For concision, this method is also referred to as\nSpa-MSA.\nWe propose a fusion architecture that integrates spatial and\nspectral information and fully exploits MSA to model simi-\nlar patches in a hyperspectral image, considering the proper-\nties of the MHIF task. While Spa-MSA lacks the modeling\nof longer-distance information, Spe-MSA does not make full\nuse of the information inside the data. To achieve a more\nwide-range correlation, our proposed architecture includes\ndilation Spa-MSA and grouped Spe-MSA modules. The con-\ntributions of this paper are listed as follows (also ﬁnd more\ndetails in Fig. 1):\n• We present a novel bidirectional dilation Transformer\n(BDT) architecture that utilizes both dilation Spa-MSA\n(D-Spa) and grouped Spe-MSA (G-Spe) modules for\nMHIF. Our experimental results on benchmark datasets\ndemonstrate that our method achieves state-of-the-art\n(SOTA) performance. We also conduct additional ex-\nperiments to evaluate the efﬁciency of D-Spa and G-Spe\nmodules, the bidirectional structures, and the impact of\ndilation rates on the overall performance.\n• To improve the receptive ﬁeld of Spa-MSA, we design\nthe D-Spa to extract a broader range of local informa-\ntion for the MHIF task. Speciﬁcally, D-Spa does not\nrequire additional parameters and calculations, which\ncan be viewed as a plug-and-play module for all Spa-\nMSA based approaches. Various experiments in Sect. 3\ndemonstrate the effectiveness of the proposed dilation\nstrategy.\n• To fully exploit the spatial information along channel di-\nmension, we design a so-called G-Spe to extract latent\nfeatures inside the feature map and learn local data be-\nhavior.\n2 Related Works\n2.1 Transformer in MHIF\nThe Transformer architecture has demonstrated strong per-\nformance in various vision tasks, and many researchers are\nattempting to leverage it for the MHIF problem with promis-\ning results. For instance, Hu et al:[Hu et al., 2022 ] were\nthe ﬁrst to use Transformer for MHIF and achieved powerful\nperformance with a lightweight network. Additionally, Meng\netal:[Meng et al., 2022] proposed an advanced transformer-\nbased model for remote sensing pansharpening. Maetal:[Ma\net al., 2021] utilized Transformer instead of CNN to learn the\nprior of hyperspectral images (HSIs) and then used an un-\nfolding network to simulate iterative solution processes for\nHSI super-resolution. Furthermore, Zhou etal: [Zhou et al.,\n2021] proposed a customized Transformer that facilitates col-\nlaborative feature learning across two modalities for remote\nsensing pansharpening.\n2.2 Motivation\nDespite the promising outcomes of the aforementioned meth-\nods, which largely rely on the powerful self-attention mod-\nule, they often adopt the self-attention or Transformer struc-\nture for various image fusion tasks without fully considering\ntheir deﬁciencies, especially for the speciﬁc MHIF problem.\nFor instance, Spa-MSA can restore image details and reduce\ncomputational complexity by correlating local pixels, but its\nreceptive ﬁeld is signiﬁcantly restricted by the window size.\nSimilarly, previous Spe-MSA treats channels as tokens and\nuses the information of the entire space for self-attention, but\nthis does not fully utilize the information inside the image.\nTo address the issue of Spa-MSA, we are inspired by the con-\ncept of dilation convolution [Li et al., 2018b] to design a new\n2D dilation structure speciﬁcally for Spa-MSA called D-Spa.\nD-Spa can effectively enlarge the receptive ﬁeld without in-\ntroducing additional parameters or computational complex-\nity. To address the issue of Spe-MSA, we propose a Grouped\nG-Spe that groups the space and then performs Spe-MSA in\nsmall groups, which may extract information within the fea-\nture and better learn local data behavior. Additionally, we\ndesign a bidirectional hierarchy structure for better exploiting\nmultiscale information of the two inputs, which have different\nspatial resolutions, for the speciﬁc application of MHIF.\n3 Methodology\nIn this section, we present our BDT designed for the MHIF\ntask. We ﬁrst introduce the overall architecture of our BDT in\nSec. 3.1. Subsequently, we analyse the function of D-Spa in\nSec. 3.2. Finally, we describe the design of G-Spe in Sec. 3.3.\n3.1 The Overall Architecture\nOur BDT is outlined in Fig. 2, which is a hierarchical bidirec-\ntional input architecture with two stages, i.e., Bimodal Fea-\nture Extraction (BFE) and Bimodal Feature Fusion (BFF).\nIn order to extract spatial information, we concatenate the\nbicubic interpolated LR-HSI XU 2RH\u0002W\u0002S and HR-MSI\nY2 RH\u0002W\u0002s as the input of the spatial branch. Besides, D-\nSpa in BFE is designed to learn the spatial information, where\noutput feature maps are Di, i= 1;2;3. In detail, the process\nof BFE is as follows:\nDi = SpatialBranch\n\u0000\nConv1\n\u0000\nCat\n\u0000\nY;XU\u0001\u0001\u0001\n; (1)\nwhere Conv1 is a convolutional structure. Using HR-HSI\nX 2Rh\u0002w\u0002S as the input of the spectral branch, the in-\nformation on the spectrum is dynamically learned through G-\nSpe, and outputs feature maps Gi (i= 1;2;3) as shown in the\nfollowing formula:\nGi = SpectralBranch (Conv2 (X)) ; (2)\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3634\n(a) bimodal feature extraction (BFE)\nSpatial Branch\nC\nH W s\nU H W S\nD-Spa\n2D\nD-Spa\n2\n1\nD-Spa\n2D\n3\nSpectral Branch\nG-Spe\nG-Spe\n2U\nh w S\n1\n2\nG-Spe\n2U\n3\nC : Concatenation+ : Element-wise addition : Pixel Shuffle\n2U: Down-sampling\n2D\n1\n2\n3\n1\n2\n3\n(b) bimodal feature fusion (BFF)\n2U\nU H W S\nH W S\nFuse\n3x3\n5x5\nFuse\n2U\n3x3\n5x5\nFuse\nC C +C\nFigure 2: The overall architecture of the proposed BDT approach. (a) The diagram of proposed BFE consisted of spatial and spectral branches.\n(b) The inputs of proposed BFF are the output of the spectral branch and the spatial branch in the BFE, respectively. Please note thatXis the\nLR-HSI, Yis the HR-MSI, and XU is the bicubic interpolation LR-HSI. Di and Gi respectively represent spatial information and spectral\ninformation extracted from bimodal feature extraction (BFE), i.e., the subgraph on the left. Then, Di and Gi are paired into the bimodal\nfeature fusion (BFF) to generate the ﬁnal output, i.e., the subgraph on the right.\nwhere Conv2 is a multi-layer convolution structure used to in-\ncrease the channels. To fuse the feature maps, i.e., Di and Gi,\nwe design the BFF model, which is an efﬁcient two-layer con-\nvolutional structure. In detail, we concatenateD3 and G1 ﬁrst,\nand send the concatenated one to the fusion module which in-\nvolves a 3 \u00023 kernel and a 5 \u00025 kernel, and then upsample\nthrough PixelShufﬂe, as shown in the following formula:\nF1 = PixelShu\u000fe (Fuse (Cat (D3;G1))) : (3)\nThen, we concatenate F1, D2 and G2 together, and upsample\nthe concatenated result. After that, we fuse the upsampled\nresult as the following formula:\nF2 = PixelShu\u000fe (Fuse (Cat (F1;D2;G2))) : (4)\nFinally, we add the fusion results of F2, D3 and G1 to the\nBicubic interpolated LR-HSI XU , and the ﬁnal output ~X 2\nRH\u0002W\u0002S is expressed by the following formula:\n~X= Fuse (Cat (F2;D3;G1)) + XU : (5)\n3.2 D-Spa\nVanilla convolution is a fundamental building block of\nconvolutional neural networks (CNNs) which have seen\ntremendous success in several computer vision tasks, e.g.,\nimage classiﬁcation [Hong et al., 2021 ], image super-\nresolution [Liang et al., 2021 ], and image segmentation [Liu\net al., 2021 ]. Dilation convolution increases the receptive\nﬁeld of the convolution kernel without adding additional pa-\nrameters, retains the internal structure of data and avoids us-\ning a pooling layer to downsample the feature map. The di-\nlation convolution operation with elements k\u0002kin the ker-\nnel and a dilation rate d at the (i;j)th pixel position can be\nexpressed as a linear combination of input F 2RC\u0002H\u0002W\naround (i;j)th pixel position, which can be expressed as fol-\nlows:\nF\n0\n(:;i;j) =\nX\n(x;y)2\n(i;j)\nW\n\u0002\nP(i;j) \u0000P(x;y)\n\u0003\nF(:;x;y); (6)\nwhere F(:;x;y) 2RC indicates the vector of the (x;y)th pixel\nposition in the input feature mapF; \n (i;j) represents the co-\nordinate set of the dilation area centered on the (i;j)th pixel\nposition; F\n0\n(:;i;j) 2RC\n0\nindicates the vector of the (i;j)th\npixel position in the output feature map F\n0\n2RC\n0\n\u0002H\u0002W and\nW 2RC\n0\n\u0002C\u0002k\u0002k is the convolution kernel of k\u0002k, where\nW\n\u0002\nP(i;j) \u0000P(x;y)\n\u0003\n2RC\n0\n\u0002C means the convolution kernel\nweight which contains coordinate offset\n\u0002\nP(i;j) \u0000P(x;y)\n\u0003\n2\b\u0000\n\u0000k+1\n2 d;\u0000k+1\n2 d\n\u0001\n;\n\u0000\n\u0000k\u00001\n2 d;\u0000k\u00001\n2 d\n\u0001\n;:::;\n\u0000k\u00001\n2 d;k\u00001\n2 d\n\u0001\t\nwith dilation rate d. Jiao et al:[Jiao et al., 2023 ] used the\nunfold operation to implement the expansion of the window\nand designed a sliding mode. However, our D-Spa expands\nthe window in ﬁxed position, instead of sliding it pixel by\npixel, and expands windows by index values. Hanetal: [Han\net al., 2021 ] present a novel point of view, which regards\nSpa-MSA as a variant of convolution, with the properties of\nsparse connectivity, weight sharing, depth separation, and\ndynamic weight. To this end, we can represent D-Spa in the\nform of convolution.\nWe operate three 1 \u00021 convolutions on the input fea-\nture F 2 RC\u0002H\u0002W to generate three tensors, i.e., Q 2\nRC\u0002H\u0002W , K 2 RC\u0002H\u0002W and V 2 RC\u0002H\u0002W , respec-\ntively. Taking only one head in D-Spa as an example, given\na window size k and a dilation rate dof 2, the output V\n0\n2\nRC\u0002H\u0002W of D-Spa operation at the (i;j)th pixel position\ncan be expressed as a linear aggregation of corresponding\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3635\nA window applying self-attention Valid content in the window\nStep 1\n Step 2\nFigure 3: The dilation in D-Spa (dilation rate = 2) consists of two\nsteps, i.e., expanding and hollowing. Step 1 expands the 3\u00023 win-\ndow to 5\u00025, and step 2 hollows out part of the window.\nvalues V 2RC\u0002H\u0002W in the local window containing the\n(i;j)th pixel position.\nV\n0\n(:;i;j) =\nX\n(x;y)2\n(i;j)\nW(i;j!x;y)V(:;x;y); (7)\nwhere V(:;x;y) 2RC indicates the value of the (x;y)th pixel\nposition in the values map V 2RC\u0002H\u0002W ; \n (i;j) indicates\nthe coordinate set of a dilation window which contains k\u0002k\npixel positions. In Fig. 3, the solid blue box represents the\nwindow applied self-attention. Taking the window size of\n3 \u00023 and dilation rate of 2 as an example, the window shape\nbecomes 5 \u00025 after dilating, and the blue patches in the win-\ndow indicate the tokens that validly participates in the self-\nattention computation. The area \n (i;j) is generated by two\nsteps, i.e., the ﬁrst step is to expand the original window, and\nthe second is to prohibit some tokens from participating in the\ncalculation of Spa-MSA. In Eq. 7, D is a constant variable;\nV\n0\n(:;i;j) 2RC indicates the vector of the(i;j)th pixel position\nin the output feature map V\n0\n2RC\u0002H\u0002W ; W(i;j!x;y) 2R\nindicates an element in the attention matrix which is com-\nputed as the softmax normalization of the dot-product be-\ntween the query Q(i;j) 2RC and the key K(x;y) 2RC :\nW(i;j!x;y) = e\n1\np\nD QT\n(i;j)K(x;y)\nSi\n; (8)\nwhere\nSi =\nk;kX\nx=1;y=1\ne\n1p\nD QT\n(i;j)K(x;y) : (9)\nBy observing the generation of W 2Rk\u0002k in the Eq. 8, the\nD-Spa is a convolution operation with the content-aware char-\nacteristic. In other words, it dynamically generates weights\nat each position. Fig. 1 above shows the properties of Spa-\nMSA and D-Spa. It can ﬁnd that D-Spa can expand receptive\nﬁelds like dilation convolution and learn the local informa-\ntion simultaneously. Furthermore, the D-Spa is pre-ﬁxed, has\nno sliding characteristic, and adopts a multi-head attention\nmechanism, which groups the channels ﬁrst, and each group\nshares a learned parameter.\n3.3 G-Spe\nFully connected layer (FC) [Gardner and Dorling, 1998] is a\nbasic linear unit in the CNNs, which connects the two hid-\nden layers with the learnable parameters. Given input is\nF 2RHW \u0002C, and the parameters of FC can be expressed\nas a matrix W 2RC\u0002C\n0\n, the FC can be expressed in the\nform of matrix multiplication:\nF\n0\n= FW; (10)\nwhere F\n0\n2RHW \u0002C\n0\nis the output of FC, and W is updated\nby the backpropagating gradient. However, the weight of FC\nis as spatial-agnostic as the vanilla convolution kernel, which\ndoes not build a relationship with the input. In order to bet-\nter express the channel-wise relationship with the input, Hu\netal: [Hu et al., 2018] propose the idea of channel-attention\n(CA), which can be represented by:\nF\n0\n= F \fW; (11)\nwhere F\n0\n2RHW \u0002C is the output of CA, \frepresents dot\nproduct operation andW 2RC is learned from the following\nformula:\nW = \b(F) ; (12)\nwhere W is a weight learned by the network\bfrom the input\nF, whose value is content-aware with the input. From this\nview, the weights in Spe-MSA are also content-aware, i.e.,\nSpe-MSA generates a weight matrix using spatial similarity.\nIn the Spe-MSA, the weight contains spatially related in-\nformation, and the matrix multiplication operation can be\nregarded as a dynamic FC operation on one head of Spe-\nMSA. Given the Spe-MSA with one head, the process can\nbe demonstrated as follows:\nV\n0\n= VW; (13)\nwhere V\n0\n2RHW \u0002C indicates the output of Spe-MSA, V 2\nRHW \u0002C means the value of Spe-MSA, and W 2RC\u0002C is\ngenerated by the following formula:\nW(i;j) = e\n1\np\nD (K(:;i))\nT\nQ(:;j)\nSj\n; (14)\nin which\nSj =\nCX\ni=1\ne\n1p\nD (K(:;i))\nT\nQ(:;j) ; (15)\nwhere Q 2 RHW \u0002C means the query of input; K 2\nRHW \u0002C means the key of input; W(i;j) indicates the (i;j)th\nposition of weight matrix W 2RC\u0002C, which is generated\nby softmax normalization of the dot product between query\nQ(:;j) 2RHW and key K(:;i) 2RHW ; Sj is the result of\nsumming the jth column in the matrix generated by the nu-\nmerator in Eq. 14 andDis a constant variable. By comparing\nthe weight generation in Eq. 10, Eq. 11, and Eq. 13, we can\nﬁnd that Spe-MSA has the dense connection properties of FC\nand the content-aware ability of CA, which means that Spe-\nMSA dynamically establishes the connection between chan-\nnels. To make full use of high-resolution spatial information\nand local content in HR-MSI, we envisage the G-Spe as a\ngrouped design for space. In detail, we subdivide the value\nV 2 RHW \u0002C, Q 2 RHW \u0002C, and K 2 RHW \u0002C into\ng2 groups, and in the kth group we get the corresponding\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3636\nVk 2R\nHW\ng2 \u0002C, Qk 2R\nHW\ng2 \u0002C and Kk 2R\nHW\ng2 \u0002C, where\nk 2\nn\n1;2;3;\u0001\u0001\u0001 ;HW\ng2\no\n. Then we calculate the weight ma-\ntrix Wk 2RC\u0002C in the kth group independently as follows:\nWk\n(i;j) = e\n1p\nD (Kk\n(:;i))\nT\nQk\n(:;j)Sk\nj\n; (16)\nwhere the Sk\nj is calculated by the following formula:\nSk\nj =\nCX\ni=1\ne\n1p\nD (Kk\n(:;i))\nT\nQk\n(:;j) : (17)\nWe will perform matrix multiplication between Wk and Vk,\nas shown in the following formula:\nVk0\n= VkWk: (18)\nEach group of G-Spe realizes a kind of dynamic FC op-\neration, i.e., a content-aware weight generator. We merge\ntogether the calculated Vk0\n2 R\nHW\ng2 \u0002C according to the\nspatial dimension to get the output V\n0\n2 RHW \u0002C, where\nk 2\nn\n1;2;3;\u0001\u0001\u0001 ;HW\ng2\no\n. In this way, G-Spe realizes the\ngrouped design along the spatial dimension through the regu-\nlar space subdivision so that the model has a rich information\nexpression capability.\nOverall Loss Function:We optimize the parameters of the\nnetwork in a uniﬁed and end-to-end manner. The overall loss\nfunction consists of the weighted sum of two losses:\nLtotal = L1 + \u0015ssimLssim; (19)\nwhere L1 means Sum of Absolute Difference, the loss Lssim\nis expressed as:\nLssim = 1 \u0000SSIM( \u0016X; ~X); (20)\nwhere the SSIM1 means Structural SIMilarity, \u0016Xrepresents\nthe reference, ~Xdenotes the output of our network, and\u0015ssim\nis a positive hyperparameter ﬁxed to 0:1 in our experiments.\n4 Experiments\nDatasets: To test the performance of our model, we conduct\nexperiments on the CA VE 2 and Harvard3 datasets. CA VE\ndataset contains 32 HSIs, including 31 spectral bands rang-\ning from 400 nm to 700 nm at 10 nm steps. We randomly\nselect 20 images for training the network, and the remaining\n11 images constitute the testing dataset. In addition, Harvard\ndataset contains 77 HSIs of indoor and outdoor scenes, and\neach HSI has a size of 1392 \u00021040 \u000231, covering the spec-\ntral range from 420 nm to 720 nm. We crop the upper left\npart (1000 \u00021000) of the 20 Harvard images, 10 of which\nhave been used for training, and the rest has been exploited\nfor testing.\nData Simulation:The proposed network takes LR-HSI and\nHR-MSI (X;Y) as input pairs, while the ground-truth (GT)\n1https://en.wikipedia.org/wiki/Structural similarity\n2https://www.cs.columbia.edu/CA VE/databases/multispectral/\n3http://vision.seas.harvard.edu/hyperspec/index.html\nfor training is HR-HSI \u0016X. However, since HR-HSI is not\navailable as a reference, a simulation stage is required. In our\nexperiments using the CA VE dataset, we produce 3920 over-\nlapping patches with a size of 64 \u000264 \u000231 by cropping 20\nchosen training images. These patches serve as the HR-HSI\n(ground-truth) \u0016Xpatches. To simulate appropriate LR-HSIs,\nwe apply a 3 \u00023 Gaussian blur kernel with a standard devi-\nation of 0.5 to the original HR-HSIs. We then downsample\nthe blurred patches with a scaling factor of 4. The HR-MSI\npatches are generated using the common spectral response\nfunction of the Nikon D700 4 camera. Therefore, the input\npairs (X;Y) consist of 3920 LR-HSI patches with a size of\n16\u000216\u000231 and RGB image patches with a size of64\u000264\u00023.\nThe pairs and their related GTs are randomly divided into\ntraining data (80%) and validation data (20%). The same\nprocedure is used to simulate the input LR-HSI and HR-MSI\nproducts and GTs for the Harvard dataset.\nBenchmark: To assess the performance of our approach, we\ncompare it with various state-of-the-art methods for MHIF.\nThe upsampled LR-HSI in Fig. 2 is the bicubic-interpolated\nresult, which is added to the experiment as a baseline. Model-\nbased techniques include the MTF-GLP-HS [Selva et al.,\n2015], the CSTF-FUS [Li et al., 2018a ], the LTTR [Dian\net al., 2019 ], the LTMR [Dian and Li, 2019 ], and the IR-\nTenSR[Xu et al., 2022] approaches. In addition, we perform\na comparison with other deep learning methods, such as the\nDBIN [Wang et al., 2019], the SSRNet [Zhang et al., 2020],\nthe ResTFNet [Liu et al., 2020 ], the HSRNet [Hu et al.,\n2021], the MoG-DCN [Dong et al., 2021], the Fusformer [Hu\net al., 2022] and the DHIF [Huang et al., 2022] network. All\nthe deep learning approaches are trained with the same input\npairs for a fair comparison. Moreover, the related hyperpa-\nrameters are selected consistent with the original papers.\nImplementation Details:The proposed network implements\nin PyTorch 1.11.0 and Python 3.7.0 using AdamW opti-\nmizer with a learning rate of 0.0001 to minimize Ltotal by\n2000 epochs and Linux operating system with a NVIDIA\nRTX3090 GPU.\nResults on CA VE Dataset:We test our model on the CA VE\ndataset. Fig. 4 presents the 11 testing images in an RGB\ncolor composition. From Tab. 1, we can see that the proposed\napproach overcomes the other methods in 4 quality indexes\n(QIs), i.e., PSNR, SAM, ERGAS, and SSIM. Speciﬁcally,\nwe observe an improvement of \u00181.30/4.93/8.11/0.028% in\nPSNR/SAM/ERGAS/SSIM compared to the second best\nmethod, i.e., MoG-DCN [Dong et al., 2021 ]. Com-\npared with the third best method, DHIF [Huang et al.,\n2022], our approach gets the gains \u00182.41/3.98/16/0.09% in\nPSNR/SAM/ERGAS/SSIM. In terms of visual assessments\n(see Fig. 5), we present the pseudo-color representations of\nthe fused products and some error maps to aid the visual in-\nspection. Compared to the benchmark, our approach has bet-\nter details and visual effects. Having a look at the error maps,\nthe reconstruction of BDT is closest to the all zero map, and\nsigniﬁcantly lower values than compared approaches.\nResults on Harvard Dataset:Besides, we evaluate the per-\nformance of our BDT on another hyperspectral image dataset\n4https://www.maxmax.com/nikon d700 study.htm\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3637\nMethods CA VE Harvard\nPSNR SAM ERGAS SSIM #params\nPSNR SAM ERGAS SSIM #params\nBicubic 34.33\u00063.88\n4.45\u00061.62 7.21\u00064.90 0.944\u00060.0291 \u0000 38.71\u00064.33 2.53\u00060.67\n4.45\u000641.81 0.948\u00060.0268 \u0000\nMTF-GLP-HS [Selva et al., 2015] 37.69\u00063.85 5.33\u00061.91 4.57\u00062.66 0.973\u00060.0158 \u0000 33.81\u00063.50 6.25\u00062.42\n3.47\u00061.82 0.952 \u00060.0321 \u0000\nCSTF-FUS [Li et al., 2018a] 34.46\u00064.28 14.37\u00065.30 8.29 \u00065.29 0.866\u00060.0747 \u0000 39.13\u00063.50 6.91\u00062.66\n4.64\u00061.80 0.913 \u00060.0487 \u0000\nLTTR[Dian et al., 2019] 35.85\u00063.49 6.99\u00062.55 5.99\u00062.92 0.956\u00060.0288 \u0000 37.91\u00063.58 5.35\u00061.94\n2.44\u00061.06 0.972 \u00060.0183 \u0000\nLTMR[Dian and Li, 2019] 36.54\u00063.30 6.71\u00062.19 5.39\u00062.53 0.963\u00060.0208 \u0000 38.41\u00063.58 5.05\u00061.70\n2.24\u00060.97 0.970 \u00060.0166 \u0000\nIR-TenSR[Xu et al., 2022] 35.61\u00063.45 12.30\u00064.68 5.90 \u00063.05 0.945\u00060.0267 \u0000 40.47\u00063.04 4.36\u00061.52\n5.57\u00061.57 0.962 \u00060.0140 \u0000\nDBIN [Wang et al.,\n2019] 50.83\u00064.29 2.21\u00060.63 1.24\u00061.06 0.996\u00060.0026 0.469M 47.88\u00063.87 2.31\u00060.46\n1.95\u00060.81 0.988 \u00060.0066 0.469M\nResTFNet [Liu et al., 2020] 45.58\u00065.47 2.82\u00060.70 2.36\u00062.59 0.993\u00060.0056 2.387M 45.93\u00064.35 2.61\u00060.69\n2.56\u00061.32 0.985 \u00060.0082 2.387M\nSSRNet [Zhang et al., 2020] 48.62\u00063.92 2.54\u00060.84 1.63\u00061.21 0.995\u00060.0023 0.027M 47.95\u00063.37 2.31\u00060.60\n2.30\u00061.42 0.987 \u00060.0070 0.027M\nHSRNet [Hu et al., 2021] 50.38\u00063.38 2.23\u00060.66 1.20\u00060.75 0.996\u00060.0014 0.633M 48.29\u00063.03 2.26\u00060.56 1.87\u00060.81 0.988\u00060.0064 0.633M\nMoG-DCN [Dong et al., 2021] 51.63\n\u00064.10 2.03\u00060.62 1.11\u00060.82 0.997\u00060.0018 6.840M 47.89\u00064.09 2.11\u00060.52 1.89\u00060.82 0.988 \u00060.0073 6.840M\nFusformer [Hu et\nal., 2022] 49.98\u00068.10 2.20\u00060.85 2.50\u00065.21 0.994\u00060.0111 0.504M 47.87\u00065.13 2.84\u00062.07\n2.04\u00060.99 0.986 \u00060.0101 0.467M\nDHIF [Huang et al., 2022] 51.07\n\u00064.17 2.01\u00060.63 1.22\u00060.97 0.997\u00060.0016 22.462M 47.68\u00063.85 2.32\u00060.53\n1.95\u00060.92 0.988 \u00060.0074 22.462M\nBDT (ours) 52.30\u00063.98 1.93\u00060.55 1.02\u00060.77 0.997\u00060.0014 2.668 M 48.83\u00063.45 2.07\u00060.49\n1.83\u00060.81 0.989 \u00060.0067 2.668 M\nIdeal value 1 0 0\n1 - 1 0 0 1 -\nTable 1: Average quantitative comparisons on 11 CA VE examples and 10 Harvard examples simulating a scaling factor of 4. The best values\nare highlighted in bold, and the second best values are underlined. M refers to millions.\n(a) (b) (c) (d) (e) (f)\n(g) (h) (i) (j) (k)\nFigure 4: The testing images from the CA VE dataset: (a) balloons,\n(b) cd, (c) chart and stuffed toy, (d) clay, (e) fake and real beers, (f)\nfake and real lemon slices, (g) fake and real tomatoes, (h) feathers,\n(i) ﬂowers, (j) hairs, and (k) jelly beans. An RGB color representa-\ntion is used to depict the images.\n(i.e., Harvard). We consider the original HSI as ground-\ntruth, and simulate the LR-HSI in the same way as the CA VE\ndataset. From Tab. 1, the results show that deep learning ap-\nproaches outperform traditional ones. Our method gets the\nbest results (outperforms high-performance approaches such\nas DHIF and Fusformer). The proposed approach shows an\nexcellent trade-off between performance and computational\ncosts on the Harvard dataset.\n4.1 Ablation Study\nIn this section, we provide an in-depth discussion of D-Spa\nand G-Spe in the BDT to demonstrate their effectiveness and\nrationale. We compare their performance with ablation on\nself-structure and other existing networks. To maintain gen-\nerality and conciseness, we present our analysis based on the\nCA VE dataset.\n1) D-Spa and G-Spe: To verify the effectiveness, in Tab. 2,\nresults show that replacing D-Spa with Spa-MSA will bring\nthe performance gain, and replacing G-Spe with Spe-MSA\nwill also boost performance. And our BDT utilizes both D-\nSpa and G-Spe obtaining the best results. It proves that the\ndesigned modules boost performance of networks. Please\nD-Spa G-Spe PSNR SAM ERGAS SSIM\n52.30\u00063.98 1.93\u00060.55 1.02\u0006\n0.77 0.997\u00060.0014\n52.03\u00063.79 2.02\u00060.59 1.04\u00060.75 0.997\u00060.0014\n51.96\u00063.72 2.03\u00060.59 1.04\u0006\n0.74 0.997\u00060.0013\n51.91\u00063.77 2.02\u00060.59 1.05\u0006\n0.76 0.997\u00060.0014\nTable 2: The average four QIs and the corresponding parameters on\nthe CA VE dataset simulating a scaling factor of 4.\nnote that Spa-MSA and Spe-MSA indicates the dilation 1 of\nD-Spa and the group 1 of G-Spe in BDT, respectively.\nMethods PSNR SAM ERGAS SSIM\nSwin-Shift 51.47 \u00063.88 2.08 \u00060.60 1.09 \u00060.81 0.997 \u00060.0015\nSwin-D 51.57 \u00064.00 2.04 \u00060.58 1.10 \u00060.85 0.997 \u00060.0016\nRestormer\n-T 50.67 \u00064.36 2.34 \u00060.72 1.29 \u00061.06 0.996 \u00060.0024\nRestormer\n-G 51.16 \u00063.93 2.22 \u00060.67 1.15 \u00060.79 0.996 \u00060.0017\nTable 3: The average four QIs and the corresponding parameters on\nthe CA VE dataset simulating a scaling factor of 4.\n2) Embedding in existing networks: We test D-Spa against\nthe Shifted Window operation in Swin Transformer and G-\nSpe against the Spe-MSA in Restormer. We use a Swin\nTransformer structure comparing the Shifted Window ap-\nproach (Swin-Shift) with our D-Spa (Swin-D). And we\nalso compare Restormer network structure with the trans-\npose MSA (T-MSA) approach (Restormer-T) and our G-Spe\n(Restormer-G). After using the proposed D-Spa and G-Spe,\nthe performance of Swin Transformer and Restormer have\ncorresponding enhancement in Tab. 3. It proves that the pro-\nposed D-Spa and G-Spe improve the network performance\nfor solving the MHIF task.\n3) Spatial grouped design in G-Spe: In the Tab. 4, we\ntested the performance of Spe-MSA and G-Spe without spec-\ntral multi-head (w/o head), using BFE in BDT as the back-\nbone. Speciﬁcally, the Spe-MSA structure is grouped in the\nspectral dimension, and the G-Spe structure is grouped in the\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3638\nFigure 5: The ﬁrst and third rows show the results using the pseudo-color representation on “ balloons” and “chart and stuffed toy”, respec-\ntively, from the CA VE dataset. Some close-ups are depicted in the red rectangles. The second and fourth rows show the residuals between the\nGT and the fused products. (a) IR-TenSR[Xu et al., 2022], (b) DBIN [Wang et al., 2019], (c) ResTFNet [Liu et al., 2020], (d) SSRNet [Zhang\net al., 2020], (e) HSRNet [Hu et al., 2021], (f) MoG-DCN [Dong et al., 2021], (g) Fusformer [Hu et al., 2022], (h) DHIF [Huang et al., 2022],\n(i) Ours, and (j) GT.\nspatial dimension. The result shows that the effect of the spa-\ntial grouped design outperforms slightly than spectral dimen-\nsion on the MHIF task.\nMethods PSNR SAM ERGAS SSIM #Flops\nSpe-MSA 52.03 \u00063.79 2.02 \u00060.59 1.04 \u00060.75 0.997 \u00060.0014\n33.52G\nw/o head 52.09 \u00063.78 2.00 \u00060.58 1.03 \u00060.75 0.997 \u00060.0013 33.87G\nTable 4: The average four QIs and the corresponding ﬂops on the\nCA VE dataset simulating a scaling factor of 4. w/o head means G-\nSpe without spectral multi-head. G means gillions.\n4) D-Spa with different dilations: We investigated the im-\npact of different dilation rates on the MHIF task by designing\nD-Spa. The proposed D-Spa has adjustable dilations that can\nexpand and hollow the window shown in Fig.3, thereby in-\ncreasing the receptive ﬁeld. As shown in Tab.5, we found\nthat a dilation rate of 2 yields the best results. Thus, D-Spa\ncan provide a long-range response from a ﬂexible range, and\nit outperforms Spa-MSA in terms of achieving better results.\nMethod PSNR SAM ERGAS SSIM\nd = 1 52.03\u00063.79 2.02\u00060.59 1.04\u00060.75 0.997\u00060.0014\nd = 2 52.30\u00063.98 1.93\u00060.55\n1.02\u00060.77 0.997\u00060.0014\nd = 3 51.51\u00063.91 2.18\u00060.65\n1.11\u00060.83 0.997\u00060.0019\nTable 5: The average four QIs on the CA VE dataset simulating a\nscaling factor of 4. d indicates the dilation rate in D-Spa.\n5) Test of multi-scaled input in bidirectional branch: We\ngradually reduced the participation of the spectral branch in\nthe BFF process. The results in Tab. 6 show the spectral\nbranch plays a vital role in the restoration of image details.\nG1 G3 G3 PSNR SAM ERGAS SSIM #\nFlops\n52.30\u00063.98 1.93\u00060.55\n1.02\u00060.77 0.997\u00060.0014 33.52G\n52.04\u00063.84 1.99\u00060.57 1.03\u00060.76 0.997\u00060.0014 33.44G\n51.91\u00063.70 2.02\u00060.59\n1.03\u00060.73 0.997\u00060.0012 33.10G\n50.72\u00063.48 4.48\u00061.38\n3.84\u00061.15 0.993\u00060.0013 27.74G\nTable 6: The four average QIs and the corresponding ﬂops on the 11\ntesting images from the CA VE dataset simulating a scaling factor of\n4. G1, G2, and G3 indicate the output which is the result of G-Spe in\nspectal branch. G refers gillions.\n5 Conclusions\nThis paper proposes the BDT, a Transformer fusion frame-\nwork, to address the MHIF problem, which employs D-Spa,\nG-Spe, and bidirectional modules. Speciﬁcally, motivated by\nthe MHIF problem, D-Spa and G-Spe are used for spatial and\nspectral information extraction, respectively. Moreover, the\nproposed BDT can extract and fuse multi-scale information\nto obtain high-quality results with moderate parameters.\nAcknowledgments\nThis research is supported by NSFC (12271083) and Natural\nScience Foundation of Sichuan Province (2022NSFSC0501).\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3639\nReferences\n[Cao et al., 2020] Xiangyong Cao, Jing Yao, Zongben Xu,\nand Deyu Meng. Hyperspectral image classiﬁcation with\nconvolutional neural network and active learning. IEEE\nTrans. Geosci. Remote Sens., 58(7):4604–4616, 2020.\n[Dian and Li, 2019] Renwei Dian and Shutao Li. Hyper-\nspectral image super-resolution via subspace-based low\ntensor multi-rank regularization. IEEE Trans. Image Pro-\ncess., 28(10):5135–5146, 2019.\n[Dian et al., 2019] Renwei Dian, Shutao Li, and Leyuan\nFang. Learning a low tensor-train rank representation for\nhyperspectral image super-resolution. IEEE Trans. Neural\nNetw. Learn. Syst., 30(9):2672–2683, 2019.\n[Dong et al., 2021] Weisheng Dong, Chen Zhou, Fangfang\nWu, Jinjian Wu, Guangming Shi, and Xin Li. Model-\nguided deep hyperspectral image super-resolution. IEEE\nTrans. Image Process., 30:5754–5768, 2021.\n[Feng and Sun, 2012] Yaoze Feng and Dawen Sun. Applica-\ntion of hyperspectral imaging in food safety inspection and\ncontrol: a review. Crit. Rev. Food Sci. Nutr., 52(11):1039–\n1058, 2012.\n[Gao et al., 2006] Bocai Gao, C Davis, and A Goetz. A re-\nview of atmospheric correction techniques for hyperspec-\ntral remote sensing of land surfaces and ocean color. In\nIEEE International Symposium on Geoscience and Re-\nmote Sensing, pages 1979–1981. IEEE, 2006.\n[Gardner and Dorling, 1998] Matt W Gardner and SR Dor-\nling. Artiﬁcial neural networks (the multilayer percep-\ntron)—a review of applications in the atmospheric sci-\nences. Atmos. Environ., 32(14-15):2627–2636, 1998.\n[Guo et al., 2020] Penghao Guo, Peixian Zhuang, and Yecai\nGuo. Bayesian pan-sharpening with multiorder gradient-\nbased deep network constraints. IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens., 13:950–962, 2020.\n[Han et al., 2021] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-\nMing Cheng, Jiaying Liu, and Jingdong Wang. On the con-\nnection between local attention and dynamic depth-wise\nconvolution. In ICLR, 2021.\n[Hong et al., 2021] Danfeng Hong, Zhu Han, Jing Yao,\nLianru Gao, Bing Zhang, Antonio Plaza, and Jocelyn\nChanussot. Spectralformer: Rethinking hyperspectral im-\nage classiﬁcation with transformers. IEEE Trans. Geosci.\nRemote Sens., 60:1–15, 2021.\n[Hu et al., 2018] Jie Hu, Li Shen, and Gang Sun. Squeeze-\nand-excitation networks. In CVPR, June 2018.\n[Hu et al., 2021] Jinfan Hu, Tingzhu Huang, Liangjian\nDeng, Taixiang Jiang, Gemine Vivone, and Jocelyn\nChanussot. Hyperspectral image super-resolution via deep\nspatiospectral attention convolutional neural networks.\nIEEE Trans. Neural Netw. Learn. Syst., 2021.\n[Hu et al., 2022] Jinfan Hu, Tingzhu Huang, Liangjian\nDeng, Hongxia Dou, Danfeng Hong, and Gemine Vivone.\nFusformer: A transformer-based fusion network for hy-\nperspectral image super-resolution. IEEE Geosci. Remote\nSens. Lett., 19:1–5, 2022.\n[Huang et al., 2022] Tao Huang, Weisheng Dong, Jinjian\nWu, Leida Li, Xin Li, and Guangming Shi. Deep hy-\nperspectral image fusion network with iterative spatio-\nspectral regularization. IEEE Trans. Comput Imaging.,\n8:201–214, 2022.\n[Jiao et al., 2023] Jiayu Jiao, Yu-Ming Tang, Kun-Yu Lin,\nYipeng Gao, Jinhua Ma, Yaowei Wang, and Wei-Shi\nZheng. Dilateformer: Multi-scale dilated transformer for\nvisual recognition. IEEE Transactions on Multimedia,\npages 1–14, 2023.\n[Kolesnikov et al., 2021] Alexander Kolesnikov, Alexey\nDosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob\nUszkoreit, Lucas Beyer, Matthias Minderer, Mostafa De-\nhghani, Neil Houlsby, Sylvain Gelly, Thomas Unterthiner,\nand Xiaohua Zhai. An image is worth 16x16 words:\nTransformers for image recognition at scale. ICLR, 2021.\n[Li et al., 2018a] Shutao Li, Renwei Dian, Leyuan Fang, and\nJos´e M Bioucas-Dias. Fusing hyperspectral and multispec-\ntral images via coupled sparse tensor factorization. IEEE\nTrans. Image Process., 27(8):4118–4130, 2018.\n[Li et al., 2018b] Yuhong Li, Xiaofan Zhang, and Deming\nChen. Csrnet: Dilated convolutional neural networks for\nunderstanding the highly congested scenes. In CVPR,\npages 1091–1100, 2018.\n[Li et al., 2021] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li,\nQi She, Lei Zhu, Tong Zhang, and Qifeng Chen. Invo-\nlution: Inverting the inherence of convolution for visual\nrecognition. In CVPR, pages 12321–12330, 2021.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\nImage restoration using swin transformer. In ICCV, pages\n1833–1844, 2021.\n[Liu et al., 2020] Xiangyu Liu, Qingjie Liu, and Yunhong\nWang. Remote sensing image fusion based on two-stream\nfusion network. Inf. Fusion., 55:1–15, 2020.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, pages 10012–10022, 2021.\n[Lu et al., 2020] Bing Lu, Phuong D Dao, Jiangui Liu,\nYuhong He, and Jiali Shang. Recent advances of hyper-\nspectral imaging technology and applications in agricul-\nture. Remote Sens. (Basel)., 12(16):2659, 2020.\n[Ma et al., 2021] Qing Ma, Junjun Jiang, Xianming Liu,\nand Jiayi Ma. Learning a 3d-cnn and transformer prior\nfor hyperspectral image super-resolution. arXiv preprint\narXiv:2111.13923, 2021.\n[Meng et al., 2022] Xiangchao Meng, Nan Wang, Feng\nShao, and Shutao Li. Vision transformer for pansharp-\nening. IEEE Trans. Geosci. Remote Sens., 60:1–11, 2022.\n[Piqueras et al., 2011] S Piqueras, L Duponchel, R Tauler,\nand A De Juan. Resolution and segmentation of\nhyperspectral biomedical images by multivariate curve\nresolution-alternating least squares. Anal. Chim. Acta.,\n705(1-2):182–192, 2011.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3640\n[Selva et al., 2015] Massimo Selva, Bruno Aiazzi,\nFrancesco Butera, Leandro Chiarantini, and Stefano\nBaronti. Hyper-sharpening: A ﬁrst approach on sim-ga\ndata. IEEE J. Sel. Top Appl. Earth Obs. Remote Sens.,\n8(6):3008–3024, 2015.\n[Wang et al., 2019] Wu Wang, Weihong Zeng, Yue Huang,\nXinghao Ding, and John Paisley. Deep blind hyperspectral\nimage fusion. In ICCV, October 2019.\n[Wu et al., 2011] Jian Wu, Dao-Li Peng, et al. Advances\nin researches on hyperspectral remote sensing forestry\ninformation-extracting technology. Spectrosc. Spect.\nAnal., 31(9):2305–2312, 2011.\n[Xu et al., 2022] Ting Xu, Tingzhu Huang, Liangjian Deng,\nand Naoto Yokoya. An iterative regularization method\nbased on tensor subspace representation for hyperspec-\ntral image super-resolution. IEEE Trans. Geosci. Remote\nSens., 60:1–16, 2022.\n[Yan et al., 2022] Keyu Yan, Man Zhou, Jie Huang, Feng\nZhao, Chengjun Xie, Chongyi Li, and Danfeng Hong.\nPanchromatic and multispectral image fusion via alternat-\ning reverse ﬁltering network. NeurIPS, 2022.\n[Yang et al., 2020a] Yong Yang, Chenxu Wan, Shuying\nHuang, Hangyuan Lu, and Weiguo Wan. Pansharpening\nbased on low-rank fuzzy fusion and detail supplement.\nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 13:5466–\n5479, 2020.\n[Yang et al., 2020b] Yong Yang, Lei Wu, Shuying Huang,\nWeiguo Wan, Wei Tu, and Hangyuan Lu. Multiband\nremote sensing image pansharpening based on dual-\ninjection model. IEEE J. Sel. Top. Appl. Earth Obs. Re-\nmote Sens., 13:1888–1904, 2020.\n[Zamir et al., 2022] Syed Waqas Zamir, Aditya Arora,\nSalman Khan, Munawar Hayat, Fahad Shahbaz Khan, and\nMinghsuan Yang. Restormer: Efﬁcient transformer for\nhigh-resolution image restoration. In CVPR, pages 5728–\n5739, 2022.\n[Zhang et al., 2020] Xueting Zhang, Wei Huang, Qi Wang,\nand Xuelong Li. Ssr-net: Spatial–spectral reconstruc-\ntion network for hyperspectral and multispectral image\nfusion. IEEE Trans. Geosci. Remote Sens., 59(7):5953–\n5965, 2020.\n[Zhou et al., 2021] Man Zhou, Xueyang Fu, Jie Huang, Feng\nZhao, Aiping Liu, and Rujing Wang. Effective pan-\nsharpening with transformer and invertible neural network.\nIEEE Trans. Geosci. Remote Sens., 60, 2021.\n[Zhou et al., 2022] Man Zhou, Jie Huang, Keyu Yan, Hu Yu,\nXueyang Fu, Aiping Liu, Xian Wei, and Feng Zhao.\nSpatial-frequency domain information integration for pan-\nsharpening. In ECCV, pages 274–291. Springer, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n3641",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7565106153488159
    },
    {
      "name": "Multispectral image",
      "score": 0.7347888946533203
    },
    {
      "name": "Hyperspectral imaging",
      "score": 0.6630278825759888
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6443129777908325
    },
    {
      "name": "Dilation (metric space)",
      "score": 0.620689868927002
    },
    {
      "name": "Spatial analysis",
      "score": 0.5219904184341431
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47259411215782166
    },
    {
      "name": "Computer vision",
      "score": 0.4400457739830017
    },
    {
      "name": "Image resolution",
      "score": 0.4358958899974823
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.42657470703125
    },
    {
      "name": "Optical flow",
      "score": 0.4228039085865021
    },
    {
      "name": "Image (mathematics)",
      "score": 0.14835545420646667
    },
    {
      "name": "Remote sensing",
      "score": 0.12808829545974731
    },
    {
      "name": "Mathematics",
      "score": 0.08856764435768127
    },
    {
      "name": "Geography",
      "score": 0.08356907963752747
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    }
  ],
  "cited_by": 18
}