{
  "title": "RoBERTaNET: Enhanced RoBERTa Transformer Based Model for Cyberbullying Detection With GloVe Features",
  "url": "https://openalex.org/W4394676691",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4258358228",
      "name": "Arwa A. Jamjoom",
      "affiliations": [
        "King Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A2228261915",
      "name": "Hanen Karamti",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A2001270210",
      "name": "Muhammad Umer",
      "affiliations": [
        "Islamia University of Bahawalpur"
      ]
    },
    {
      "id": "https://openalex.org/A2620655024",
      "name": "Shtwai Alsubai",
      "affiliations": [
        "Prince Sattam Bin Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A2284287740",
      "name": "Tai-Hoon-Kim",
      "affiliations": [
        "Chonnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2155509093",
      "name": "Imran Ashraf",
      "affiliations": [
        "Yeungnam University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2941081759",
    "https://openalex.org/W3036842656",
    "https://openalex.org/W3011458709",
    "https://openalex.org/W4301621576",
    "https://openalex.org/W2945434935",
    "https://openalex.org/W2802043748",
    "https://openalex.org/W6608482188",
    "https://openalex.org/W2117314500",
    "https://openalex.org/W2962797668",
    "https://openalex.org/W6620699631",
    "https://openalex.org/W3120045749",
    "https://openalex.org/W2594902547",
    "https://openalex.org/W4318243851",
    "https://openalex.org/W3137762660",
    "https://openalex.org/W4386374142",
    "https://openalex.org/W4384345401",
    "https://openalex.org/W4386260374",
    "https://openalex.org/W4316658410",
    "https://openalex.org/W4318002980",
    "https://openalex.org/W4385985558",
    "https://openalex.org/W4385948419",
    "https://openalex.org/W4309101631",
    "https://openalex.org/W4220949780",
    "https://openalex.org/W2895493709",
    "https://openalex.org/W4293169088",
    "https://openalex.org/W6607259140",
    "https://openalex.org/W4283589994",
    "https://openalex.org/W4221085936",
    "https://openalex.org/W4310117749",
    "https://openalex.org/W4214608374",
    "https://openalex.org/W4386292370",
    "https://openalex.org/W2522159200"
  ],
  "abstract": "Online platforms are fostering social interaction, but unfortunately, this has given rise to antisocial behaviors such as cyberbullying, trolling, and hate speech on a global scale. The detection of hate and aggression has become a vital aspect of combating cyberbullying and cyberharassment. Cyberbullying involves using aggressive and offensive language including rude, insulting, hateful, and teasing comments to harm individuals on social media platforms. Human moderation is both slow and expensive, making it impractical in the face of rapidly growing data. Automatic detection systems are essential to curb trolling effectively. This research deals with the challenge of automatically identifying cyberbullying in tweets from a publicly available cyberbullying dataset. This research work employs robustly optimized bidirectional encoder representations from the transformers approach (RoBERTa), utilizing global vectors for word representation (GloVe) word embedding features. The proposed approach is further compared with the state-of-the-art machine, deep, and transformer-based learning approaches with the FastText word embedding approach. Statistical results demonstrate that the proposed model outperforms others, achieving a 95&#x0025; accuracy for detecting cyberbullying tweets. In addition, the model obtains 95&#x0025;, 97&#x0025;, and 96&#x0025; for precision, recall, and F1 score, respectively. Results from k-fold cross-validation further affirm the supremacy of the proposed model with a mean accuracy of 95.07&#x0025;.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nRoBERTaNET: Enhanced RoBERTa\nTransformer Based Model for\nCyberbullying Detection with GloVe\nFeatures\nFADWA ALROWAIS1, ARWA A. JAMJOOM2, HANEN KARAMTI1, MUHAMMAD UMER3,\nSHTWAI ALSUBAI4, TAI-HOON KIM4,*, AND IMRAN ASHRAF6,*\n1Department of computer sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O.Box 84428, Riyadh\n11671, Saudi Arabia; (fmalrowais@pnu.edu.sa, hmkaramti@pnu.edu.sa)\n2Department of Information System, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah, Saudi Arabia;\najamjoom@kau.edu.sa\n3Department of Computer Science & Information Technology, The Islamia University of Bahawalpur, Bahawalpur, Pakistan; (umer.sabir@iub.edu.pk)\n4Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, P.O. Box 151, 11942,\nSaudi Arabia; (Sa.alsubai@psau.edu.sa)\n5School of Electrical and Computer Engineering, Yeosu Campus, Chonnam National University, 50, Daehak-ro, Yeosu-si, Jeollanam-do, 59626, Republic of\nKorea; taihoonn@chonnam.ac.kr\n6Department of Information and Communication Engineering, Yeungnam University, Gyeongsan 38541, Republic of Korea; (email:imranashraf@ynu.ac.kr)\nCorresponding authors: Imran Ashraf and Tai-hoon Kim (Emails: imranashraf@ynu.ac.kr; taihoonn@chonnam.ac.kr)\n\"Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2024R77), Princess Nourah bint\nAbdulrahman University, Riyadh, Saudi Arabia.\"\nABSTRACT Online platforms are fostering social interaction, but unfortunately, this has given rise to\nantisocial behaviors such as cyberbullying, trolling, and hate speech on a global scale. The detection of hate\nand aggression has become a vital aspect of combating cyberbullying and cyberharassment. Cyberbullying\ninvolves using aggressive and offensive language including rude, insulting, hateful, and teasing comments\nto harm individuals on social media platforms. Human moderation is both slow and expensive, making\nit impractical in the face of rapidly growing data. Automatic detection systems are essential to curb\ntrolling effectively. This research deals with the challenge of automatically identifying cyberbullying in\ntweets from a publicly available cyberbullying dataset. This research work employs robustly optimized\nbidirectional encoder representations from the transformers approach (RoBERTa), utilizing global vectors\nfor word representation (GloVe) word embedding features. The proposed approach is further compared\nwith the state-of-the-art machine, deep, and transformer-based learning approaches with the FastText word\nembedding approach. Statistical results demonstrate that the proposed model outperforms others, achieving\na 95% accuracy for detecting cyberbullying tweets. Results from k-fold cross-validation further affirm the\nsupremacy of the proposed model.\nINDEX TERMS Cyberbullying; RoBERTa; GloVe; FastText; Transformer Based Learning\nI. INTRODUCTION\nS\nOCIAL media serves as a platform that allows individ-\nuals to connect and engage with their friends online,\nenabling the sharing of photos, videos, and daily updates. In\nthe present day, almost everyone is active on social media,\nintegrating virtual meeting platforms into their daily routines\nthrough global online social networks (OSNs) to facilitate\ncommunication. This network assists users in discovering\nnew friends and expanding their connections worldwide.\nMoreover, the sharing of data and opinions constitutes sig-\nnificant features of OSNs [1]–[3]. In recent years, the utiliza-\ntion of OSNs has experienced rapid growth. Platforms like\nFacebook, Google+, LinkedIn, Twitter, VKontakte, Mixi, and\nSina Weibo have evolved into preferred modes of commu-\nnication for billions of daily active users. As per statistics,\nin 2020, nearly 3.6 billion users engaged with social me-\ndia networking sites, and this number is projected to reach\n4.41 billion by 2025. According to Backlink, 58.11% of the\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nglobal population is using social media [4]. Users dedicate\na substantial amount of time to updating their content, in-\nteracting with primary users, and browsing others’ accounts\nto find specific information, making social network websites\na significant part of their daily activities. OSNs have the\npotential to eliminate economic and geographical barriers\namong users, fostering information sharing and communi-\ncation. Additionally, OSNs play a crucial role in achieving\nobjectives such as entertainment, education, and job search-\ning. The widespread use of OSNs, however, increases the risk\nof various attacks on users. Many OSN users expose their\nprivate data, providing opportunities for attackers to engage\nin specific malicious activities [5], [6].\nIn conjunction with the widespread use of social networks,\ncyberbullying is identified as a troubling phenomenon, en-\ncompassing bullying activities that occur through digital\ndevices such as cell phones, computers, and tablets [7].\nPresently, the predominant users of the internet are young\npeople, with 95% of teenagers in the U.S. being online, and a\nsignificant majority accessing social networks through their\nmobile devices [8]. In the era of Web 2.0, cyberbullying\nis more likely to occur compared to traditional bullying.\nA notable 36.5% of individuals feel they have experienced\ncyberbullying at some point in their lives, and 17.4% have\nreported incidents within the last 30 days [9]. These figures\nhave more than doubled since 2007 [10]. Furthermore, a 2019\nGoogle survey revealed that teachers in the U.S. consider\ncyberbullying as their primary safety concern in classrooms\n[11]. Over the past few years, online communication has\nincreasingly become user-driven, with social network plat-\nforms emerging as the most vulnerable to cyberbullying due\nto their massive global user base, efficient communication\nchannels, and continuous \"24/7/365\" services [12]. The im-\npact of cyberbullying extends beyond affecting individuals’\npsychological well-being, influencing various aspects of their\nlives. This is particularly concerning for young people, as\ncyberbullying has the potential to lead them towards self-\nharm and even suicide [13].\nCyberbullying has emerged as a significant issue over\nthe past decade, particularly impacting children and ado-\nlescents. Notably, a study conducted in the U.S. revealed\nthat more than 43% of teenagers in the country have ex-\nperienced cyberbullying [14]. European statistics from ER\nindicate that approximately 18% of youngsters in Europe\nhave encountered bullying or harassment through the Internet\nand mobile phones [15]. Online Report for EU kids in 2014\nfurther highlights that every fifth student of the 11-16 years\nage group has faced instances of cyberbullying [16]. Even\nin developed countries such as Sweden, cyberbullying has\nincreased manifolds, reaching a crucial point and steadily\ndeteriorating [17]. These findings underscore the urgency of\ndevising effective, prompt, and proven solutions to address\nthis pervasive issue on the Internet. Therefore, it is crucial to\napproach the problem of cyberbullying from various angles,\nincluding the development of automated systems for the\nidentification and prevention of such incidents.\nGiven the escalating nature of this issue, researchers have\nactively pursued methods for the detection and prevention\nof cyberbullying. A particularly promising avenue involves\nthe application of machine learning (ML) algorithms capa-\nble of analyzing extensive data sets to discern patterns and\nrelationships among variables [18]. Recognizing the com-\nplexity of the task at hand, it is improbable that a sole ML\nalgorithm would prove adequate. In the context of this paper,\nwe propose a model grounded in robustly optimized bidirec-\ntional encoder representations from transformers (RoBERTa)\nto specifically identify cyberbullying in the textual content.\nThe primary contributions of this work can be succinctly\nsummarized as follows\n• To enhance the predictive accuracy of cyberbullying, a\nnovel framework is proposed that is based on global\nvectors for word representation (GloVe) features with\nthe RoBERTa model. The transformer-based RoBERTa\nmodel makes use of GloVeword2vec features to give\noptimal results.\n• The study involves an assessment of the performance\nof established machine, deep, and transformer-based\nlearning algorithms applied to cyberbullying data. These\nmodels include random forest (RF), support vector ma-\nchine (SVM), k-nearest neighbor (KNN), Naive Bayes\n(NB), bidirectional long short-term memory (BiLSTM),\nconvolutional long short-term memory (ConvLSTM),\nbidirectional encoder representations from transformers\n(BERT), and convolutional neural network (CNN).\n• The effectiveness of the proposed approach is thor-\noughly examined through extensive experiments, and a\ncomparative analysis with various state-of-the-art meth-\nods is conducted. To validate the robustness of the\nproposed approach, the results are further substantiated\nusing k-fold cross-validation.\nThe structure of this paper is as follows: Section II dives\ninto existing research, featuring studies that utilize ML and\ndeep learning for cyberbullying detection. Section III out-\nlines the materials, methodologies, and suggested models\nfor cyberbullying identification. Section IV provides com-\nprehensive details about the experiment’s results and their\ncorresponding discussions. Finally, Section V concludes the\npaper with summarizing remarks.\nII. LITERATURE REVIEW\nThe issue of social media cyberbullying, notably Twitter\n(now referred to as X) and Facebook, is of significant concern\ndue to its profound impact on users’ well-being, especially\namong younger demographics who frequently use these plat-\nforms. Dan Ottosson [19] introduced a large language model\n(LLM) aimed at detecting cyberbullying on social media\nplatforms. Utilizing the GPT-3 LLM, the study sought to\nminimize the gap in platform moderation. The outcomes\nindicate that the proposed model performs comparably to the\npreceding models. The research also suggests that fine-tuning\nan LLM is an effective strategy for enhancing cyberbullying\ndetection, with the study achieving an accuracy of 90%. In\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nanother study by Alhloul & Alam [20], a deep learning-\nbased system was proposed for identifying bullying tweets.\nThe researchers developed a CNN-attention framework that\namalgamated an attention layer with a convolutional pooling\nlayer, enabling efficient extraction of cyberbullying-related\nkeywords from users’ tweets. The study utilized two sets of\ncombinations. Initially, they combined CNN and ML models\nwhere convolutional layers served as feature extractors and\nML models like RF and LR were used for classification. In\nthe subsequent structure, they employed combinations like\nCNN-XGB and CNN-LSTM for classification. The findings\nrevealed that the proposed CNN-Attention framework out-\nperformed other learning models, achieving an impressive\naccuracy of 97.10%.\nWang et al. [21] presented a graphical convolutional\nmethod for underlying cyberbullying detection. The frame-\nwork leverages a semi-supervised online dynamic query ex-\npansion (DQE) process to automatically generate balanced\ndata. This process extracts additional natural data points\nof a specific class from Twitter. They also introduced a\ngraph convolutional network (GCN) classifier that operates\non a graph, constructed from thresholded cosine similari-\nties between tweet embeddings. The performance of this\nsystem was compared against different machine learning\nmodels coupled with various embedding techniques. The\nresults show that for the proposed SOSNet, using SBERT,\nan accuracy score of 92.70% was achieved. In a separate\nstudy, Al Qudah et al. [22] suggested an improved system\nfor cyberbullying detection utilizing an adaptive external\ndictionary (AED). The authors employed ML models such\nas RF, XGB, and CatBoost, and introduced ensemble voting\nmodels. The findings suggest that the proposed ensemble vot-\ning model, when used with AED, provided superior accuracy\nin detecting cyberbullying incidents.\nMathur et al [23] introduced a real-time system for de-\ntecting cyberbullying on Twitter using natural language pro-\ncessing (NLP) and ML. The system underwent training on\na dataset comprising cyberbullying tweets, employing vari-\nous ML algorithms whose performances were subsequently\ncompared. Through tuning, it was determined that the RF\nalgorithm yielded the most favorable results. The study’s\noutcomes revealed that by carefully selecting preprocessing\ntechniques and fine-tuning the RF model, an impressive accu-\nracy of 94.06% was achieved. In a separate effort, Bokolo and\nLiu [24] proposed a deep learning-based system designed for\nautomatically detecting cyberbullying on social media plat-\nforms. The study involved a comparison of three ML models\nNB, SVM, and Bi-LSTM using a Twitter dataset focused on\ncyberbullying. The demonstrated results of the experiments\nsuggested that the Bi-LSTM outclassed the others, attaining\na remarkable accuracy of 98%. Following closely, SVM\nattained a 97% accuracy, while NB lagged with 85%. These\nfindings underscore the effectiveness of ML techniques in\nunveiling cyberbullying, Bi-LSTM with the deep learning\nmodel done and dusted two other ML models.\nNisha and Jebathangam [25] introduced an automated\nsystem for classifying and detecting cyberbullying on social\nmedia. Metadata and textual content are collectively used\nto identify social media cyberbullying in this model. This\nmethod involves training and testing phases on data related\nto cyberbullying. NLP functions as the preprocessing tool\nduring these phases, followed by the application of particle\nswarm optimization for feature selection. The study employs\na decision tree (DT) classifier to categorize cyberbullying.\nDT model performance is evaluated using text-instances-\ncombined features post-classification. The study’s results in-\ndicate that the proposed DT classifier achieved an impressive\naccuracy score of 99.1%. In a related endeavor, Mehmud et\nal [26] proposed an ML-based system focusing on textual\nfeatures from the data for cyberbullying detection. The study\ninvolved an analysis of five distinct ML models LGBM,\nXGB, LR, RF, and ADA to detect cyberbullying using a\ndataset of tweets with textual features. The dataset, compris-\ning more than 47,000 tweets divided into six classes, under-\nwent thorough evaluation. The results revealed that LGBM\noutperformed the other models, demonstrating accuracy as\nhigh as 85.5%.\nMuneer et al. [27] proposed a modified BERT and stacking\nensemble model to identify social media cyberbullying. A\ncontinuous bag of words (CBOW), along with a word2vec-\nlike feature extraction method is employed to establish\nweights in the embedding layer. An outstanding accuracy of\n97.4% was achieved using the stacking ensemble learning\nmethod as revealed by experiment results for discovering cy-\nberbullying on the tweet dataset. A related study used CBOW\nfeature extraction and attention mechanism based on deep\nTABLE 1: Summary of state-of-the-art related work.\nRef Classifiers Dataset Performance\n[19] BERT, GPT-3 Kaggle 90% GPT-3 Ada\n[20] LR, RF, XGB, CNN, CNN+LR, CNN+RF, CNN+XGB, CNN+LSTM, CNN-\nAttention\nKaggle 97.10% CNN-Attention\n[21] LR, NB, k-NN, SVM, XGB, MLP, SOSNet Kaggle 92.70% SOSNet with SBERT\n[22] RF, XGB, CatBoost, Ensemble V oting Kaggle 98.55% Ensemble voting\n[23] Vanilla RF, AdaBoost, GBC, Tuned RF Kaggle 94.06% Tuned RF\n[24] NB, SVM, Bi-LSTM Kaggle 98% Bi-LSTM\n[25] SVM, ANN, DT Kaggle 99.1% DT\n[26] LGBM, XGB, LR, RF, ADA Kaggle 85.5% LGBM\n[27] Conv1DLSTM, BiLSTM, LSTM, BERT, Tuned-BERT, Stacked and CNN Kaggle (37373) 97.4% stacked\n[28] LSTM, Conv1DLSTM, CNN, BiLSTM, BiLSTM_Pooling, GRU Kaggle (37373) 94.49% Conv1DLSTM\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nlearning focussed on detecting cyberbullying. Various deep\nlearning models, including Conv1DLSTM, LSTM, CNN,\nBiLSTM_Pooling, BiLSTM, and gated recurrent unit (GRU),\nwere employed. Results underlined the dominance of the\nattention-based Conv1DLSTM classifier over the other ap-\nplied approaches, achieving the highest accuracy of 94.49%.\nThe complete summary of state-of-the-art related works is\nshown in Table 1.\nIII. MATERIAL AND METHODS\nThis segment outlines the crucial aspects of the envisioned\nframework for detecting cyberbullying, known as the Cyber-\nbullying detection system (CDS). It delves into the exam-\nination and identification of cyberbullying activities across\nvarious social media platforms. Additionally, this section de-\ntails the dataset, machine learning, and deep learning models\nemployed in the study. A concise overview of the feature\nextraction techniques utilized is also incorporated within this\nsection.\nA. DATASET\nAs the utilization of social media becomes widespread across\ndiverse age groups, a significant portion of the population\nrelies on this essential tool for daily communication. The\npervasive nature of social media has made cyberbullying a\npervasive issue, capable of affecting individuals at any time\nand place. The Internet’s inherent anonymity adds to the\ncomplexity of combating such attacks compared to tradi-\ntional forms of bullying. The recent pandemic (COVID-19),\nmarked by extensive school closures, declined in-person so-\ncial contact, and explosive screen time, led UNICEF to warn\nthe users of the potential exposure to cyberbullying. Statistics\nreveal that cyberbullying was experienced by 36.5% of mid-\ndle and high school students. This has affected the academic\nperformance of the students drastically, feelings of sadness,\nand even suicidal thoughts.\nThe dataset under consideration has been sourced from\nthe Kaggle website and encompasses over 47,000 tweets\nclassified as cyberbullying [29]. The categories within this\ndataset include\n• Gender\n• Age\n• Religion\n• Ethnicity\n• Other forms of cyberbullying\n• No online bullying\nIt is noteworthy to point out that the dataset is balanced,\nwith 8,000 instances in each class. This balance ensures equal\nrepresentation. As the dataset is explored, it is found that\nthe tweets are either offensive in their entirety or actually\ndescribe incidents of bullying.\nB. FEATURE EXTRACTION METHODS\nFeature extraction methods were employed on the training\nsubset, encompassing both the training and testing data.\nThese techniques were applied to train the selected models\nusing the training data and were subsequently utilized dur-\ning the classification of the testing data. In the context of\ncyberbullying detection in this study, two feature extraction\nmethods, namely Word2Vec and FastText, were utilized. A\nbrief overview of these techniques is provided below.\n1) word2Vec\nTo improve the current form of word implanting, Word2Vec,\na robust statistical model, was developed. Word repre-\nsentations were elevated to the subsequent level through\nWord2Vec, allowing the model to efficiently capture under-\nstated distinctions [30]. These representations significantly\ninfluence semantics, shaping the understanding of language\nby depicting the relationships between words. Word2Vec en-\ncompasses two distinct models: the continuous bag-of-words\n(CBoW) and the continuous skip-gram model. The CBoW\nmodel generates embeddings by predicting the current word\nbased on its context. This process is mathematically ex-\npressed by the following\nJθ = 1\nT\nX\nlogp(wt|wt−n, ··· , wt+n) (1)\nThe continuous skip-gram model is derived by predicting\nthe neighbouring words assuming the current word. This\nmodel is known for its efficiency in training compared to the\nword embedding model. It is expressed mathematically by\nthe following\nJθ = 1\nT\nXX\nlogp(wj+1|wt) (2)\n2) Global Vectors for Word Representation\nNumerous statistical models have been devised to address\nthe concept of topic modeling. One notable algorithm in this\ndomain is Latent semantic analysis, which utilizes matrix fac-\ntorization to extract meaningful semantics [31]. While effec-\ntive, it has certain limitations when compared to Word2Vec.\nSubsequently, to create a more efficient model that integrates\nthe strengths of mutual approaches, GloVe was introduced.\nIn many cases, GloVe tends to outperform Word2Vec. It\noperates on the entire corpus by employing a word context\nmatrix and a word co-occurrence matrix.\n3) FastText\nFastText is a word illustration library, that encompasses 300-\ndimensional 2 million common crawl words, resulting in\n600 billion word vectors [31]. Particularly, hand-crafted n-\ngrams are used as features alongside solo words. Its straight-\nforward architecture contributes to the effective and effi-\ncient performance of text classification tasks. Various word\nimplanting methods are used in diverse text classification\nendeavors, with pre-trained unsupervised word embedding\nto predict word context. FastText excels in this context by\nleveraging morphological features to identify challenging\nwords, enhancing its suitability for vector representation.\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDataset\nGLOVE\nFeatures\nTrain-Test\nSplit\n80% Training\n20% Testing\nRoBERTa\nModel\nTrained Model\nAccuracy\nPrecision\nRecall\nF-score\nFIGURE 1: Architecture of proposed RoBERTaNet model.\nThis characteristic also bolsters its generalizability. Further-\nmore, FastText word embedding generates vectors using n-\ngrams, a feature that proves beneficial in handling unknown\nwords.\nC. PROPOSED METHODOLOGY\nThis section introduces the framework suggested for this\nstudy, as depicted in Figure 1. Recently, a growing trend\nin using deep learning-based classifiers has emerged. The\nRoBERT model, in particular, has garnered attention for its\npotential to enhance traditional classifiers for their classi-\nfication accuracy. Therefore, this study seeks to leverage\nRoBERT to detect cyberbullying in text data.\nExperiments are conducted using benchmark datasets re-\nlated to cyberbullying. Various word embedding techniques,\nincluding Word2Vec and FastText, are employed. Subse-\nquently, the suggested method, which combines Word2Vec\nword embedding with RoBERTa, is applied for training.\nThe effectiveness of the projected methodology is assessed\nusing four evaluation measures including precision, recall,\naccuracy, and F1 score.\nD. SUPERVISED MACHINE LEARNING AND DEEP\nLEARNING ALGORITHMS\nThis section focuses on the ML algorithms employed for\ncyberbullying detection using the Twitter dataset. The ex-\necution of these ML models involves the utilization of the\nSci-Kit learn library and NLTK. Specifically, four ML algo-\nrithms and three deep learning algorithms were applied in\nPython. To assess the effectiveness of the proposed system, a\ncombination of regression-based, tree-based, and ensemble-\nbased models was employed. The following ML algorithms,\nin combination with the proposed methodology, evaluated the\nperformance of the ML classifiers.\n1) Random Forest\nRF stands as a sophisticated decision tree (DT) iteration.\nIt operates as a supervised learning algorithm, employing\nnumerous DTs that operate independently to predict a class\nlabel. The ultimate prediction is determined by the class\nthat garners the majority of votes across the individual trees\n[32]. When assessing the error rate in RF and comparing it\nto other models, it emerges as notably low. This low error\nrate is attributed to the diminished correlation between the\nconstituent trees. In this study, the RF was trained using\nvarious parameters, and for decision tree splits, multiple\nalgorithms were employed depending on the specific problem\nat hand.\n2) K-Nearest Neighbor\nKNN is categorized as a lazy learner as it requires all the\ndata during the training process. In KNN, a new sample\nis attributed to a class using a distance metric [33], [34].\nKNN measures the distance of new data points to their\nclosest neighbors, and the value of K dictates the number\nof neighbors. For instance, if K is set to 3, new data points\nare assigned to a class by considering its distance to 3 nearest\nneighbors. The calculation of the distance between two points\ncan be carried out using a variety of distance estimation\nmetrics and the following are the most commonly used ones.\nEuclideandistance =\nvuut\nkX\ni=1\n(xi − yi)2 (3)\nManhattandistance =\nkX\ni=1\n|xi − yi| (4)\nMinkowskidistance = (\nkX\ni=1\n|xi − yi|q)\n1\nq (5)\n3) Naive Bayes\nGNB, a Naive Bayes variant grounded in Bayes’ theorem,\noperates on conditional probabilities to forecast the outcome\nof an event [35]. In this context, if a sample is categorized\ninto k classes represented by k = c1, c2, ··· , ck, the output\nresults are denoted as c. The GNB function is outlined below,\nwhere c represents the class and d signifies the sample\nP(c|d) = P(d|c) · P(c)\nP(d) (6)\nThis formula encapsulates the class c probability given the\nsample d, computed as the product of the likelihood of the\nsample given the class (P(d|c)) and the prior probability of\nthe class\n(P(c))\n, normalized by the probability of the sample (P(d)).\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4) Support Vector Machine\nThe primary purpose of the SVM classifier is to adapt to\nthe provided data and determine the optimal hyperplane that\neffectively separates the data [36]. Once this hyperplane is\nobtained, certain features are input into the classifier to derive\nthe final class results. This characteristic makes SVM a dis-\ntinctive algorithm. SVM employs a linear kernel function and\nis implemented in liblinear as opposed to libSVM. Notably\nflexible in terms of penalty and loss function choices, SVM\nyields favorable scaling results, particularly for large sample\nsizes. As a widely utilized supervised learning algorithm,\nSVM excels in handling both classification and regression\nproblems. Within the SVM module, two functions are avail-\nable: linear SVM and SVM. The linear function of SVM\nis employed when developing an SVM model with a linear\nkernel.\n5) Bidirectional Long Short-Term Memory\nBiLSTM is a subtype of recurrent neural network (RNN)\nspecifically designed for handling sequence processing tasks,\ncommonly applied in natural language processing and speech\nrecognition [37]. In contrast to traditional LSTMs, BiLSTM\nprocesses input data in both the forward and backward direc-\ntions, allowing it to capture contextual information from both\npreceding and subsequent states. This bidirectional strategy\nenhances the network’s ability to comprehend and learn long-\nrange dependencies within sequential data. Comprising two\nLSTM layers, one processing the input sequence from start\nto finish and the other in reverse, BiLSTM concatenates the\noutputs from both directions. This amalgamation yields a\nmore comprehensive representation of the input sequence,\nmaking BiLSTM particularly adept at tasks requiring a nu-\nanced understanding of context and dependencies in both\ndirections. As a result, BiLSTM proves to be a potent tool\nfor various applications in the analysis of sequential data.\n6) Convolutional Long Short Term Memory\nConvLSTM represents a neural network architecture that\nmerges the spatial hierarchies learned by convolutional layers\nwith the capacity to capture temporal dependencies provided\nby LSTM networks [38]. This architecture proves partic-\nularly effective in handling spatiotemporal data, such as\nvideo sequences or time-series data characterized by spatial\ndependencies. ConvLSTM seamlessly incorporates convolu-\ntional operations into the LSTM cells, enabling the model\nto simultaneously learn both spatial patterns and temporal\ndynamics. This integration makes ConvLSTM well-suited\nfor tasks demanding an understanding of intricate patterns in\nboth spatial and temporal dimensions, including applications\nlike video prediction, anomaly detection, and weather fore-\ncasting. ConvLSTM has demonstrated success in capturing\ncomplex relationships within sequences, establishing itself\nas a valuable tool in various domains that necessitate the\nanalysis of dynamic, structured data.\n7) Convolutional Neural Network\nCNN stands as a prevalent artificial neural network employed\nacross a diverse range of tasks [39]. Conceptually, a CNN\nbears a resemblance to a multi-layer perceptron (MLP) in\nthat it encompasses an activation function for each neuron,\nmapping the weighted outputs. Notably, an MLP transitions\ninto a deep MLP when multiple hidden layers are introduced.\nThe architecture of CNN imparts it with the ability to be in-\nvariant to translation and rotation. Three fundamental layers\nconstitute a CNN: a core layer, a pooling layer, and a fully\nconnected layer, each incorporating an activation function.\nE. TRANSFORMER BASED ARCHITECTURE\n1) BERT\nBERT is a transformer-based model featuring an attention\nmechanism that actively processes input text [40]. Compris-\ning two components, an encoder, and a decoder, BERT takes\ntextual input and generates predictions as output. Particularly\nin NLP-suited question-and-answering and sentiment anal-\nysis tasks, BERT’s strength lies in its training on extensive\nword-based data. Unlike traditional models that consider\nword context in only one direction, typically left to right,\nBERT comprehensively accounts for word context in both\ndirections. This unique feature distinguishes BERT from\nearlier deep learning models, endowing it with a nuanced\nunderstanding of word meanings. Trained on a large corpus\nof data, BERT excels at producing accurate results and ac-\nquiring a profound grasp of intricate patterns and structures.\n2) Robustly Optimized BERT Approach\nRoBERTa stands as a cutting-edge natural language process-\ning (NLP) model introduced by Facebook AI in 2019. Rooted\nin the BERT architecture, RoBERTa incorporates multiple\noptimizations aimed at enhancing performance [40]. Pre-\ntrained on an extensive corpus of text data, RoBERTa excels\nin grasping contextual relationships within language. Key\noptimizations include the utilization of larger mini-batches,\ndynamic masking during pre-training, and the elimination\nof the next sentence prediction objective. By undergoing\ntraining on diverse and comprehensive datasets, RoBERTa\ndemonstrates superior performance across a spectrum of\nNLP tasks, including question answering, sentiment analysis,\nand named entity recognition. Its robustness and versatility\nhave propelled its widespread adoption, serving as a founda-\ntional model for fine-tuning and transfer learning in various\nnatural language understanding applications.\nF. PERFORMANCE EVALUATION METRICS\nThe evaluation of deep, machine, and transformer-based\nmodels also involves assessing their performance using var-\nious metrics such as accuracy, precision, recall, and the F1\nscore. Accuracy is determined by the formula:\nAccuracy = TP + TN\nTP + TN + FP + FN (7)\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nwhere TP represents true positive, TN denotes true neg-\native, FP signifies false positive, and FN represents false\nnegative.\nPrecision is an additional performance metric that gauges\nthe actual positives to the total number of positive predictions\nratio, expressed as:\nPrecision = TP\nTP + FP (8)\nThe recall metric is also employed to evaluate model\nperformance and is calculated by dividing true positives by\nthe sum of true positives and false negatives, given by\nRecall = TP\nTP + FN (9)\nIn situations where class imbalance is present, the F1 score\nemerges as a superior metric, as it considers both precision\nand recall, providing a more comprehensive assessment of\nthe model’s performance. The F1 score is computed using\nthe formula\nF1 − Score = 2× Precision × Recall\nPrecision + Recall (10)\nIV. RESULTS AND DISCUSSION\nThis segment provides comprehensive insights into the ex-\nperiments conducted using the Twitter dataset for bullying\nemploying DL, ML, and transformer-based models. The\nassessment of research experiments is carried out using\na Python-based Colab Notebook, leveraging Tensorflow,\nKeras, and Sklearn libraries. To gauge the performance of di-\nverse models, multiple metrics, comprising precision, recall,\naccuracy, and the F1 score, are employed. For transformer-\nbased and deep model training, a graphics processing unit\n(GPU) and 16 GB of RAM are used to expedite this process.\nSubsequent sections present the outcomes of the conducted\nexperiments.\nA. MODEL RESULTS USING WORD2VEC FEATURES\nIn this study, a series of experiments are undertaken to\ndetect cyberbullying tweets, employing a combination of ML\nand DL models. The identification of bullying instances in\nthe data was facilitated by incorporating Word2vec features\ninto the feature set. The results of employing the Word2vec\ntechnique for multi-class classification are presented in the\naccompanying Table 2.\nTABLE 2: Models results using Word2Vec features.\nClassifier Accuracy Precision Recall F1 score\nRF 90 0.90 0.88 0.89\nKNN 70 0.75 0.69 0.72\nNB 83 0.81 0.82 0.81\nSVM 80 0.82 0.80 0.81\nBiLSTM 82 0.81 0.82 0.81\nConv LSTM 82 0.82 0.83 0.83\nBERT 89 0.89 0.90 0.90\nRoBERT 91 0.91 0.92 0.91\nCNN 77 0.78 0.75 0.76\nTable 2 presents the outcomes of the utilization of\nWord2vec features. Notably, RoBERTa achieves the highest\naccuracy of 91%, and similarly, it secures top values for\nprecision, recall, and F1 scores. In the realm of machine\nlearning-based classification, the tree-based model RF attains\nan accuracy of 90%, accompanied by corresponding values\nof 90% for precision, 88% for recall, and 89% for the\nF1 score. Among the DL models, the CNN for multi-class\ngrouping records the lowest accuracy, standing at 77%. Con-\nversely, the ML model KNN emerges as the least performing\nclassifier, achieving an accuracy of 70%.\nB. RESULTS USING GLOVE FEATURES\nSeveral experiments were conducted to identify cyberbully-\ning tweets, employing both ML and DL models in this study.\nThe detection of bullying in the data was facilitated by utiliz-\ning GloVe features as part of the feature set. The outcomes\nof the GLOVE technique for multi-class classification are\nshowcased in Table 3.\nTABLE 3: Models results using GloVe features.\nClassifier Accuracy Precision Recall F1 score\nRF 94 0.94 0.94 0.94\nKNN 67 0.79 0.67 0.69\nNB 85 0.85 0.85 0.84\nSVM 82 0.86 0.84 0.85\nBiLSTM 86 0.84 0.86 0.85\nConv LSTM 85 0.85 0.85 0.84\nBERT 93 0.93 0.93 0.93\nRoBERT 95 0.95 0.97 0.96\nCNN 79 0.81 0.73 0.77\nTable 3 displays the results obtained through GloVe fea-\ntures. Notably, the highest accuracy of 95% is attained by\nRoBERTa. Similarly, RoBERTa also secures the top values\nfor precision, recall, and F1 scores. In the context of a tree-\nbased machine learning model RF achieves an accuracy score\nof 94%, along with corresponding values of 94% for preci-\nsion, recall, and F1 score. Among the deep learning models,\nCNN for multi-class grouping records the lowest accuracy,\nstanding at 79%. Conversely, the machine learning model\nKNN emerges as the least performing classifier, achieving an\naccuracy of 67%.\nC. RESULTS USING FASTTEXT FEATURES\nIn the subsequent set of experiments, trials were conducted\nutilizing FastText features. The outcomes for both ML and\nDL models, incorporating FastText features, are showcased\nin Table 4. The results suggest a decline in the performance\nof the learning models. Additionally, the findings underscore\nthat models employing Word2vec features outperform those\nutilizing FastText features in terms of performance.\nTable 4 displays the outcomes obtained using FastText\nfeatures. RoBERTa stands out with the highest accuracy of\n92.25%, along with superior values for precision, recall, and\nF1 scores. In the context of multi-class classification, the tree-\nbased model RF achieves an accuracy of 88.97%, with cor-\nresponding values of 84% for precision, 87% for recall, and\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 4: Models results using FastText functions.\nClassifier Accuracy Precision Recall F1 score\nRF 88 0.84 0.87 0.85\nKNN 74 0.74 0.70 0.72\nNB 76 0.74 0.71 0.72\nSVM 78 0.80 0.81 0.80\nBiLSTM 75 0.73 0.77 0.74\nConv LSTM 84 0.87 0.88 0.87\nBERT 80 0.86 0.84 0.85\nRoBERT 92 0.90 0.91 0.90\nCNN 74 0.75 0.75 0.75\n85% for F1 score. Among the DL models, CNN for multi-\nclass classification records the lowest accuracy on FastText\nfeatures, standing at 74%. Conversely, the ML model KNN\nagain emerges as the least performing classifier, achieving an\naccuracy of 74%. It is worth noting an improvement in the\nperformance of KNN on FastText features. In summary, the\noverall performance of the learning models demonstrates a\ndecline using FastText features when compared to Word2vec\nfeatures.\nD. K-FOLD CROSS-VALIDATION RESULTS\nK-fold cross-validation was utilized to enrich the investi-\ngation of the models. Five-fold cross-validation results in\nTable 5 showcase the superior performance in terms of F1\nscore, accuracy, recall, and precision of the model to other\nalternatives.\nTABLE 5: K-fold cross-validation results\nK-fold Accuracy Precision Recall F1 score\nFold 1 94.23 0.94 0.96 0.95\nFold 2 94.89 0.94 0.96 0.96\nFold 3 95.99 0.95 0.96 0.96\nFold 4 95.27 0.96 0.97 0.97\nFold 5 94.98 0.95 0.97 0.96\nAverage 95.07 0.96 0.95 0.96\nE. PERFORMANCE COMPARISON WITH EXISTING\nSTUDIES\nThe results of this method were evaluated against multi-\nple approaches for robustness and performance from the\navailable literature. In pursuit of meaningful outcomes, mis-\ncellaneous DL and ML models were employed here. For\ninstance, in [24], bi-directional LSTM models were utilized\nfor predicting cyberbullying on social media. Additionally,\n[21] incorporated a feature selection approach to enhance\nmodel performance. Other studies, such as [23], [25], [26],\nemployed ML models for a similar purpose. Ensemble mod-\nels were also explored in [20], [22], making a significant\ncontribution to the efficient detection of cyberbullying. The\ncomparative outcomes given in Table 6 demonstrate that the\nproposed approach yields superior results when compared to\nthese existing approaches.\nTABLE 6: Performance comparison of the proposed ap-\nproach with state-of-the-art techniques.\nRef Proposed approach Performance\n[19] GPT-3 Ada 90.2%\n[21] SOSNet with SBERT 92.7%\n[23] Tuned RF 94.6%\n[26] LGBM 85.5%\nProposed RoBERTa with Word2vec 95.9%\nV. CONCLUSION\nThis study delves into the domain of cyberbullying tweet\ndetection, presenting a novel approach that integrates the\npowerful RoBERTa model with GloVe features. The aim was\nto enhance the accuracy and efficiency of cyberbullying de-\ntection in the ever-evolving landscape of online interactions.\nThe proposed model underwent rigorous evaluation, pitting\nits performance against a spectrum of existing models en-\ncompassing traditional machine learning, deep learning, and\ntransformer-based architectures, as well as other state-of-the-\nart models. The results obtained from extensive experimen-\ntation and evaluation highlight the prowess of the RoBERTa\nmodel in tandem with GloVe features. The proposed model\nexhibits a significant improvement in cyberbullying detection\naccuracy compared to established benchmarks. Its ability\nto leverage contextual embeddings and nuanced linguistic\nfeatures provided by RoBERTa, coupled with the comple-\nmentary information encoded in GloVe features, contributes\nto a robust and effective detection mechanism. This study\nnot only advances the field of cyberbullying detection but\nalso underscores the importance of leveraging state-of-the-\nart models in addressing contemporary challenges in online\nsocial spaces. The comparative analysis emphasizes the su-\nperiority of the proposed RoBERTa and GloVe-based ap-\nproach over conventional methods, signifying its potential as\na reliable tool for identifying and combating cyberbullying\nin the digital sphere. As the online landscape continues to\nevolve, this research lays a foundation for further exploration\nand refinement of models that can adapt to the dynamic\nnature of cyberbullying instances across various platforms\nand contexts. For future endeavors, there is an envisioned\ndevelopment of a customized deep learning model optimized\nfor small datasets. Furthermore, there is contemplation of\ncombining multiple datasets to create a complex and high-\ndimensional dataset for conducting experiments with the\nproposed approach.\nREFERENCES\n[1] S. R. Sahoo and B. B. Gupta. Classification of various attacks and\ntheir defence mechanism in online social networks: a survey. Enterprise\nInformation Systems, 13(6):832–864, 2019.\n[2] R. Annamalai, S. J. Rayen, and J. Arunajsmine. Social media networks\nowing to disruptions for effective learning. Procedia Computer Science,\n172:145–151, 2020.\n[3] M. Fire, G. Katz, and Y . Elovici. Strangers intrusion detection detecting\nspammers and fake profiles in social networks based on topology anoma-\nlies. Human Journal, 1(1):26–39, 2012.\n[4] B. Dean. How many people use social media in 2021? (65+ statistics),\n2021.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[5] D. Paulraj. A gradient boosted decision tree based sentiment classification\nof twitter data. International Journal of Wavelets Multiresolution and\nInformation Processing, 18(4):2050027–2050121, 2020.\n[6] H. Kefi and C. Perez. Dark side of online social networks: Technical,\nmanagerial, and behavioral perspectives. Encyclopedia of Social Network\nAnalysis and Mining, 143:535–556, 2018.\n[7] Stopbullying.gov. What is cyberbullying, 2020.\n[8] S. Hinduja and J. W. Patchin. Cyberbullying: Identification, Prevention,\nand Response. Cyberbullying.org, 2018.\n[9] Cyberbullying Research Center. 2019 cyberbullying data, 2020.\n[10] Cyberbullying Research Center. Summary of our cyberbullying research\n(2007–2019), 2020.\n[11] Google. Be internet awesome: Online safety & parents, 2019.\n[12] M. A. Al-Garadi, M. R. Hussain, N. Khan, G. Murtaza, H. F. Nweke,\nI. Ali, G. Mujtaba, H. Chiroma, H. A. Khattak, and A. Gani. Predicting\ncyberbullying on social media in the big data era using machine learning\nalgorithms: Review of literature and open challenges. IEEE Access,\n7:70701–70718, 2019.\n[13] A. John, A. C. Glendenning, A. Marchant, P. Montgomery, A. Stewart,\nS. Wood, K. Lloyd, and K. Hawton. Self-harm, suicidal behaviours, and\ncyberbullying in children and young people: Systematic review. J. Med.\nInternet Res., 20:e129, 2018.\n[14] J.-M. Xu, K.-S. Jun, X. Zhu, and A. Bellmore. Learning from bullying\ntraces in social media. In Proceedings of the 2012 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 656–666. ACL, 2012.\n[15] H. Karjaluoto, P. Ulkuniemi, H. Keinanen, and O. Kuivalainen. An-\ntecedents of social media b2b use in industrial marketing context: cus-\ntomers’ view. Journal of Business & Industrial Marketing, 2015.\n[16] S. Agrawal and A. Awekar. Deep learning for detecting cyberbullying\nacross multiple social media platforms. pages 141–153, 2018.\n[17] N. Selwyn. Social media in higher education. The Europa world of\nlearning, 1(3):1–10, 2012.\n[18] Jr. Hair, J. F. and M. Sarstedt. Data, measurement, and causal inferences\nin machine learning: Opportunities and challenges for marketing. J. Mark.\nTheory Pract., 29:65–77, 2021.\n[19] D. Ottosson. Cyberbullying detection on social platforms using large\nlanguage models. 2023.\n[20] A. Alhloul and A. Alam. Bullying tweets detection using cnn-attention.\n2023. Available at SSRN 4338998.\n[21] J. Wang, K. Fu, and C. T. Lu. Sosnet: A graph convolutional network ap-\nproach to fine-grained cyberbullying detection. In 2020 IEEE International\nConference on Big Data (Big Data), pages 1699–1708. IEEE, 2020.\n[22] H. Qudah, M. A. Alhija, and H. Tarawneh. Improving cyberbullying\ndetection through adaptive external dictionary in machine learning. 2023.\n[23] S. A. Mathur, S. Isarka, B. Dharmasivam, and C. D. Jaidhar. Analysis of\ntweets for cyberbullying detection. In 2023 Third International Conference\non Secure Cyber Computing and Communication (ICSCCC), pages 269–\n274. IEEE, 2023.\n[24] B. G. Bokolo and Q. Liu. Cyberbullying detection on social media\nusing machine learning. In IEEE INFOCOM 2023-IEEE Conference on\nComputer Communications Workshops (INFOCOM WKSHPS), pages 1–\n6. IEEE, 2023.\n[25] M. Nisha and J. Jebathangam. Detection and classification of cyberbully-\ning in social media using text mining. In 2022 6th International Conference\non Electronics, Communication and Aerospace Technology, pages 856–\n861. IEEE, 2022.\n[26] M. I. Mahmud, M. Mamun, and A. Abdelgawad. A deep analysis of textual\nfeatures based cyberbullying detection using machine learning. In 2022\nIEEE Global Conference on Artificial Intelligence and Internet of Things\n(GCAIoT), pages 166–170. IEEE, 2022.\n[27] A. Muneer, A. Alwadain, M. G. Ragab, and A. Alqushaibi. Cyberbullying\ndetection on social media using stacking ensemble learning and enhanced\nbert. Information, 14(8):467, 2023.\n[28] S. M. Fati, A. Muneer, A. Alwadain, and A. O. Balogun. Cyberbullying\ndetection on twitter using deep learning-based attention mechanisms and\ncontinuous bag of words feature extraction. Mathematics, 11(16):3567,\n2023.\n[29] AndrewMVD. Cyberbullying classification dataset, 2022.\n[30] Muhammad Umer, Saima Sadiq, Michele Nappi, Muhammad Usman\nSana, Imran Ashraf, et al. Etcnn: Extra tree and convolutional neural\nnetwork-based ensemble model for covid-19 tweets sentiment classifica-\ntion. Pattern Recognition Letters, 164:224–231, 2022.\n[31] Musarat Karim, Malik Muhammad Saad Missen, Muhammad Umer,\nSaima Sadiq, Abdullah Mohamed, and Imran Ashraf. Citation context\nanalysis using combined feature embedding and deep convolutional neural\nnetwork model. Applied Sciences, 12(6):3203, 2022.\n[32] Sherif F Abdoh, Mohamed Abo Rizka, and Fahima A Maghraby. Cervical\ncancer diagnosis using random forest classifier with smote and feature\nreduction techniques. IEEE Access, 6:59475–59485, 2018.\n[33] Afaq Juna, Muhammad Umer, Saima Sadiq, Hanen Karamti,\nAla’Abdulmajid Eshmawi, Abdullah Mohamed, and Imran Ashraf.\nWater quality prediction using knn imputer and multilayer perceptron.\nWater, 14(17):2592, 2022.\n[34] Afaq Juna, Muhammad Umer, Saima Sadiq, Hanen Karamti,\nAla’Abdulmajid Eshmawi, Abdullah Mohamed, and Imran Ashraf.\nWater quality prediction using knn imputer and multilayer perceptron.\nWater, 14(17):2592, 2022.\n[35] Irina Rish et al. An empirical study of the naive bayes classifier. In IJCAI\n2001 workshop on empirical methods in artificial intelligence, volume 3,\npages 41–46, 2001.\n[36] Samina Sarwat, Naeem Ullah, Saima Sadiq, Robina Saleem, Muhammad\nUmer, Ala’Abdulmajid Eshmawi, Abdullah Mohamed, and Imran Ashraf.\nPredicting students’ academic performance with conditional generative\nadversarial network and deep svm. Sensors, 22(13):4834, 2022.\n[37] Muhammad Ahmad, Saima Sadiq, Ala Saleh Alluhaidan, Muhammad\nUmer, Saleem Ullah, Michele Nappi, et al. Industry 4.0 technologies\nand their applications in fighting covid-19 pandemic using deep learning\ntechniques. Computers in Biology and Medicine, 145:105418, 2022.\n[38] Lucia Cascone, Saima Sadiq, Saleem Ullah, Seyedali Mirjalili, Hafeez\nUr Rehman Siddiqui, and Muhammad Umer. Predicting household electric\npower consumption using multi-step time series with convolutional lstm.\nBig Data Research, 31:100360, 2023.\n[39] Umair Hafeez, Muhammad Umer, Ahmad Hameed, Hassan Mustafa,\nAhmed Sohaib, Michele Nappi, and Hamza Ahmad Madni. A cnn based\ncoronavirus disease prediction system for chest x-rays. Journal of Ambient\nIntelligence and Humanized Computing, pages 1–15, 2022.\n[40] Xiaoyuan Chen, Turki Aljrees, Muhammad Umer, Hanen Karamti, Saba\nTahir, Nihal Abuzinadah, Khaled Alnowaiser, Abdullah Mohamed, Imran\nAshraf, et al. A novel approach for explicit song lyrics detection using\nmachine and deep ensemble learning models. PeerJ Computer Science,\n9:e1469, 2023.\nFADWA ALROWAISreceived the B.Sc. degree\n(Hons.) in computer and information sciences with\na focus on computer applications and the M.Sc.\ndegree in computer science from the College of\nComputer and Information Sciences, King Saud\nUniversity, Saudi Arabia, in 1996 and 2005, re-\nspectively, and the Ph.D. degree in computer sci-\nence from the Faculty of Electronics and Com-\nputer Sciences, Southampton University, U.K., in\n2016. She has 24 years academic experience, she\nhas worked with the College of Computer and Information Sciences and the\nCollege of Engineering, Princess Nourah Bint Abdulrahman University\nARWA A. JAMJOOMreceived the master’s de-\ngree in computer science from the University of\nSouthern California, Los Angeles, CA, USA, in\n1997, and the Doctoral degree in computer science\nfrom the University of Surrey, Guildford, U.K., in\n2011.,She is an Associate Professor with the De-\npartment of Information System, King Abdulaziz\nUniversity, Jeddah, Saudi Arabia. Her research\ninterests lie in data analytics and decision support\nsystem.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nHANEN KARAMTIis currently a Professor with\nthe Department of Mathematical Sciences, United\nArab Emirates University, Al Ain, United Arab\nEmirates. Her research interests include number\ntheory and combinatorics.\nMUHAMMAD UMER received his BS degree in\nDepartment of Computer Science, Khwaja Fareed\nUniversity of Engineering & IT (KFUEIT), Pak-\nistan (Oct-2014 to Oct-2018). Since Sep-2020, he\ngot himself enrolled in Ph.D. of Computer Science\n(KFUEIT). He is also serving as Research As-\nsistant at Fareed Computing & Research Center,\nKFUEIT, Pakistan. His recent research interests\nare related to data mining, mainly working ma-\nchine learning & deep learning-based IoT, text\nmining, and computer vision tasks.\nSHTWAI ALSUBAIShtwai Alsubai received the\nbachelor’s degree in information system from\nKing Saud University, Saudi Arabia, in 2008, the\nmaster’s degree in computer science from CLU,\nUSA, in 2011, and the Ph.D. degree from The\nUniversity of Sheffield, U.K., in 2018. He is cur-\nrently an Assistant Professor of computer science\nat Prince Sattam bin Abdulaziz University. His\nresearch interests include XML, XML query pro-\ncessing, XML query optimization, machine learn-\ning, and natural language processing\nTAI-HOON KIM received the M.S. and Ph.D.\ndegrees in electrics, electronics, and computer en-\ngineering from Sungkyunkwan University, Seoul,\nSouth Korea, and the second Ph.D. degree in infor-\nmation science from the University of Tasmania,\nHobart, Australia, in December 2011.,He is cur-\nrently a Professor with Chonnam National Univer-\nsity, Gwangju, South Korea. His research interests\ninclude statistical analysis, image processing, and\nsystem design\nIMRAN ASHRAF received his Ph.D. in Infor-\nmation and Communication Engineering from Ye-\nungnam University, South Korea in 2018, and\nthe M.S. degree in computer science from the\nBlekinge Institute of Technology, Karlskrona,\nSweden, in 2010 with distinction. He has worked\nas a postdoctoral fellow at Yeungnam University,\nas well. He is currently working as an Assistant\nProfessor at the Information and Communication\nEngineering Department, Yeungnam University,\nGyeongsan, South Korea. His research areas include positioning using next\ngeneration networks, communication in 5G and beyond, location-based\nservices in wireless communication, smart sensors (LIDAR) for smart cars,\nand data analytics.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3386637\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7633568048477173
    },
    {
      "name": "Offensive",
      "score": 0.7096194624900818
    },
    {
      "name": "Harm",
      "score": 0.6351231932640076
    },
    {
      "name": "Social media",
      "score": 0.6350754499435425
    },
    {
      "name": "Encoder",
      "score": 0.5991551280021667
    },
    {
      "name": "Intimidation",
      "score": 0.5961103439331055
    },
    {
      "name": "Transformer",
      "score": 0.5882722735404968
    },
    {
      "name": "Embedding",
      "score": 0.5686352252960205
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5287850499153137
    },
    {
      "name": "Language model",
      "score": 0.500420093536377
    },
    {
      "name": "Word embedding",
      "score": 0.4927528202533722
    },
    {
      "name": "Machine learning",
      "score": 0.4439201056957245
    },
    {
      "name": "Speech recognition",
      "score": 0.41235682368278503
    },
    {
      "name": "Natural language processing",
      "score": 0.40717294812202454
    },
    {
      "name": "Computer security",
      "score": 0.39656853675842285
    },
    {
      "name": "World Wide Web",
      "score": 0.13746106624603271
    },
    {
      "name": "Psychology",
      "score": 0.1312079131603241
    },
    {
      "name": "Engineering",
      "score": 0.09180891513824463
    },
    {
      "name": "Operations research",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185163786",
      "name": "King Abdulaziz University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I106778892",
      "name": "Princess Nourah bint Abdulrahman University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I174731842",
      "name": "Islamia University of Bahawalpur",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I142608572",
      "name": "Prince Sattam Bin Abdulaziz University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I111277659",
      "name": "Chonnam National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I55240360",
      "name": "Yeungnam University",
      "country": "KR"
    }
  ]
}