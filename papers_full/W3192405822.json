{
  "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
  "url": "https://openalex.org/W3192405822",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2139505590",
      "name": "Se-Won Min",
      "affiliations": [
        "University of Washington",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1978197916",
      "name": "Mike Lewis",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": [
        "University of Washington",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington",
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3135934234",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2986712448",
    "https://openalex.org/W2370924594",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W2116316001",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3157005959",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2163614729",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2891602716",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3173617765",
    "https://openalex.org/W4300996741",
    "https://openalex.org/W2967576208",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3152497014",
    "https://openalex.org/W2061873838",
    "https://openalex.org/W2550147980",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W2593887162",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2913222130",
    "https://openalex.org/W3106031848",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2970694516",
    "https://openalex.org/W2963866663",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W4294238563",
    "https://openalex.org/W2963833497",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W3099403624"
  ],
  "abstract": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5316 - 5330\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nNoisy Channel Language Model Prompting\nfor Few-Shot Text Classiﬁcation\nSewon Min1,2, Mike Lewis, 2 Hannaneh Hajishirzi1,3, Luke Zettlemoyer 1,2\n1University of Washington 2Facebook AI Research 3Allen Institute for AI\n{sewon,hannaneh,lsz}@cs.washington.edu mikelewis@fb.com\nAbstract\nWe introduce a noisy channel approach for lan-\nguage model prompting in few-shot text classi-\nﬁcation. Instead of computing the likelihood\nof the label given the input (referred as di-\nrect models), channel models compute the con-\nditional probability of the input given the la-\nbel, and are thereby required to explain every\nword in the input. We use channel models\nfor recently proposed few-shot learning meth-\nods with no or very limited updates to the lan-\nguage model parameters, via either in-context\ndemonstration or prompt tuning. Our exper-\niments show that, for both methods, channel\nmodels signiﬁcantly outperform their direct\ncounterparts, which we attribute to their sta-\nbility, i.e., lower variance and higher worst-\ncase accuracy. We also present extensive abla-\ntions that provide recommendations for when\nto use channel prompt tuning instead of other\ncompetitive methods (e.g., direct head tuning):\nchannel prompt tuning is preferred when the\nnumber of training examples is small, labels\nin the training data are imbalanced, or general-\nization to unseen labels is required.\n1 Introduction\nPrompting large language models, by prepending\nnatural language text or continuous vectors (called\nprompts) to the input, has shown to be promising in\nfew-shot learning (Brown et al., 2020). Prior work\nhas proposed methods for ﬁnding better prompt\n(Shin et al., 2020; Li and Liang, 2021; Lester et al.,\n2021) or better scoring of the output from the\nmodel (Zhao et al., 2021; Holtzman et al., 2021).\nThese studies directly predict target tokens to deter-\nmine the prediction for an end task. Despite promis-\ning results, they can be unstable with high variance\nacross different verbalizers (text expression for la-\nbels) and seeds, and the worst-case performance is\noften close to random (Perez et al., 2021; Lu et al.,\n2021).\nIn this paper, we introduce alternative channel\nmodels for prompted few-shot text classiﬁcation\nLM\n=(“A three-hour cinema master class.”, “It was great.”)(x, y)\nA three-hour cinema master class. It was great.\nIt was great. A three-hour cinema master class.\nP(y| x)Direct\nP(x| y)P(y) ∝ P(x| y)Channel\nOutputInput\nFigure 1: An illustration of the direct model and the\nchannel model for language model prompting in the\nsentiment analysis task.\nwith large language models, inspired by noisy chan-\nnel models in machine translation (Brown et al.,\n1993; Koehn et al., 2003; Yu et al., 2017; Yee et al.,\n2019) and their extensions to other tasks (Yogatama\net al., 2017; Lewis and Fan, 2018). Unlike direct\nmodels that compute the conditional probability\nof the label token given the input, channel models\ncompute the conditional probability of the input\ngiven the output (Figure 1). Intuitively, channel\nmodels are required to explain every word in the\ninput, potentially amplifying training signals in the\nlow data regime. We study the impact of channel\nmodels for language model prompting where the\nparameters of the language model are frozen. In\nparticular, we compare channel models with their\ndirect counterparts for (1) demonstration methods,\neither concatenation-based (Brown et al., 2020) or\nour proposed, ensemble-based (Section 4.1.3), and\n(2) prompt tuning (Lester et al., 2021).\nOur experiments on eleven text classiﬁcation\ndatasets show that channel models outperform their\ndirect counterparts by a large margin. We attribute\nthe strong performance of channel models to their\nstability: they have lower variance and signiﬁ-\ncantly higher worst-case accuracy then their direct\ncounterparts over different verbalizers and seeds.\nWe additionally ﬁnd a direct model with head\ntuning—tuning the LM head while freezing other\nparameters—is surprisingly effective, often outper-\nforming direct models with other forms of tuning.\n5316\nWhile different methods are preferred given dif-\nferent conditions, the channel model with prompt\ntuning (denoted as channel prompt tuning) signiﬁ-\ncantly outperforms all direct baselines when (1) the\ntraining data is imbalanced, or (2) generalization\nto unseen labels is required.\nIn summary, our contributions are three-fold:\n1. We introduce a noisy channel approach for\nlanguage model prompting in few-shot text\nclassiﬁcation, showing that they signiﬁcantly\noutperform their direct counterparts for both\ndemonstration methods and prompt tuning.\n2. We ﬁnd particularly strong performance of\nchannel models over direct models when the\ntraining data is imbalanced or generalization\nto unseen labels is required.\n3. Based on extensive ablations, we provide rec-\nommendations between different models (di-\nrect vs. channel and prompt tuning vs. head\ntuning) based on given conditions such as the\ntarget task, the size of training data, the num-\nber of classes, the balance between labels in\nthe training data, and whether generalization\nto unseen labels is required.\n2 Related Work\n2.1 Channel Model\nLet x and y be the input and the output, respec-\ntively. The most widely used models, denoted as\ndirect models, compute P(y|x). In contrast, noisy\nchannel models maximize P(x|y)P(y) (Shannon,\n1948; Brown et al., 1993). 1 While the noisy\nchannel approach has been the most successful\nin machine translation (Yamada and Knight, 2001;\nKoehn et al., 2003; Yu et al., 2017; Yee et al., 2019),\nit has also been studied in more general NLP tasks.\nPrior work provides a theoretical analysis that chan-\nnel models approach their asymptotic errors more\nrapidly than their direct counterparts (Ng and Jor-\ndan, 2002), and empirically shows that channel\nmodels are more robust to distribution shift in text\nclassiﬁcation (Yogatama et al., 2017) or question\nanswering (Lewis and Fan, 2018), and in a few-shot\nsetup (Ding and Gimpel, 2019).\n1We follow Yu et al. (2017); Yee et al. (2019) in using\nthe terms direct models and channel models. They are often\nreferred as discriminative models and generative models in\nprior work (Yogatama et al., 2017; Lewis and Fan, 2018). In\nprinciple, these two distinctions are not always equivalent, e.g.,\na model that computes P(x, y) = P(y|x)P(x) is generative\nbut not a channel model.\nIn this paper, we explore channel models using\na large language model on a wide range of text\nclassiﬁcation tasks, focusing on prompt-based few-\nshot learning.\n2.2 Few-shot Learning\nPrior work in few-shot learning has used differ-\nent approaches, including semi-supervised learn-\ning with data augmentation or consistency train-\ning (Miyato et al., 2017; Clark et al., 2018; Xie\net al., 2020; Chen et al., 2020) and meta learn-\ning (Finn et al., 2017; Huang et al., 2018; Bansal\net al., 2020). Recent work has introduced prompt-\ning (or priming) of a large language model. For\nexample, Brown et al. (2020) proposes to use a con-\ncatenation of training examples as a demonstration,\nso that when it is prepended to the input and is fed\nto the model, the model returns the output follow-\ning the pattern in the training examples. This is\nespecially attractive as it eliminates the need for up-\ndating parameters of the language model, which is\noften expensive and impractical. Subsequent work\nproposes alternative ways of scoring labels through\nbetter model calibration (Zhao et al., 2021; Holtz-\nman et al., 2021), or learning better prompts, either\nin a discrete space (Shin et al., 2020; Jiang et al.,\n2020; Gao et al., 2021) or in a continuous space (Li\nand Liang, 2021; Lester et al., 2021; Liu et al.,\n2021; Zhong et al., 2021; Qin and Eisner, 2021).\nAlmost all of them are direct models, computing\nthe likelihood of y given x with the prompts.\nOur work is closely related to two recent papers.\nTam et al. (2021) studies a label-conditioning objec-\ntive for masked language models; although this is\nnot strictly a generative channel model, condition-\ning on the output y is similar to our work. However,\nthey are still optimizing a discriminative objective,\nand inference at test time is the same as with the\ndirect model. Holtzman et al. (2021) explores zero-\nshot models that compute the probability ofx given\ny based on Pointwise Mutual Information, but with\na restriction that the input and the output are in-\nterchangeable. To the best of our knowledge, our\nwork is the ﬁrst that uses a noisy channel model for\nfew-shot language model prompting for classiﬁca-\ntion, and also the ﬁrst to draw the connection with\nthe noisy channel literature.\n3 Formulation\nWe focus on text classiﬁcation tasks. The goal is to\nlearn a task function f : X − →C, where Xis the\n5317\nMethod Zero-shot Concat-based Demonstrations Ensemble-based Demonstrations\nDirect PLM(v(ci)|x) PLM(v(ci)|x1, v(c1)...xk, v(ck), x) Π K\nj=1PLM(v(ci)|xj, v(cj), x)\nDirect++ PLM(v(ci)|x)\nPLM(v(ci)|NULL)\nPLM(v(ci)|x1,v(c1)...xk,v(ck),x)\nPLM(v(ci)|x1,v(c1)...xk,v(ck),NULL) ΠK\nj=1\nPLM(v(ci)|xj ,v(cj ),x)\nPLM(v(ci)|xj ,v(cj ),NULL)\nChannel PLM(x|v(ci)) PLM(x|x1, v(c1)...xk, v(ck), v(ci)) Π K\nj=1PLM(x|v(cj), xj, v(ci))\nTable 1: Comparison of zero-shot, concat-based demonstrations, and ensemble-based demonstrations (Section 4.1).\n{(xj, cj)}K\nj=1 is training data and v is the verbalizer.\nset of all natural language texts and C= {c1...cm}\nis a set of labels. We consider three formulations.\nDirect computes distributions of labels ci ∈ C\ngiven the input x ∈X: P(ci|x). This is the most\nwidely used method in modern neural networks.\nDirect++ is a stronger direct model that com-\nputes P(ci|x)\nP(ci|NULL) instead of P(ci|x), following the\nmethod from Holtzman et al. (2021) and the non-\nparametric method from Zhao et al. (2021). This\napproach is motivated by the fact that language\nmodels can be poorly calibrated and suffer from\ncompetition between different strings with the same\nmeaning. This approach is used for the demonstra-\ntion methods in Section 4.1.\nChannel uses Bayes’ rule to reparameterize\nP(ci|x) as P(x|ci)P(ci)\nP(x) . As we are generally in-\nterested in argmaxci∈C\nP(x|ci)P(ci)\nP(x) and P(x) is\nindependent from ci, it is sufﬁcient to model\nP(x|ci)P(ci). We assume P(ci) = 1\n|C| and only\ncompute P(x|ci).\n4 Method\nWe explore direct and channel models using a\ncausal language model (LM) PLM that gives the\nconditional probability of the text y when fol-\nlowed by x. More precisely, given the text x =\nx1...xtx and y = y1...yty (x1...xtx, y1...yty ∈V ,\nwhere Vis the vocabulary set), PLM(y|x) indicates\nΠty\nt′=1PLM(yt′|x1...xtxy1...yt′−1).2\nWhen learning a task function f : X− →C, we\nalso assume a pre-deﬁned verbalizer v : C− →X\nwhich maps each label into a natural language ex-\npression. For example, if the task is sentiment\nanalysis with C= {c+, c−}, an example input text\nx would be “A three-hour cinema master class” and\nan example v would have v(c+) =“It was great”\nand v(c−) =“It was terrible”. In a few-shot setup,\nwe are also given a set of K training examples\n2In practice, we use length normalization that was found\nto be effective by Holtzman et al. (2021).\nD= {(x1, c1), ··· , (xK, cK)}.\nWe are interested in methods where there are no\ntrainable parameters (Section 4.1) or the number\nof trainable parameters is very small, typically less\nthan 0.01% of the total (Section 4.2). This follows\nprior observations that updating and saving a large\nnumber of parameters for every task is expensive\nand often infeasible (Rebufﬁ et al., 2017; Houlsby\net al., 2019; Lester et al., 2021).\n4.1 Demonstration methods\nIn demonstration methods, there are no trainable\nparameters. We explore three ways of making a\nprediction, as summarized in Table 1.\n4.1.1 Zero-shot\nWe follow Brown et al. (2020) in comput-\ning P(ci|x) and P(x|ci) as PLM(v(ci)|x) and\nPLM(x|v(ci)), respectively. For example, given\nx =“A three-hour cinema master class”, the direct\nmodel compares the probabilities of “It was great”\nand “It was terrible” when following “A three-hour\ncinema master class”, while the channel model\nconsiders the probabilities of “A three-hour cinema\nmaster class” when following “It was great” or “It\nwas terrible”.\n4.1.2 Concat-based demonstrations\nWe follow the few-shot learning method in Brown\net al. (2020). The key idea is to prepend a\nconcatenation of K training examples to the\ninput so that a language model can learn the\ntask setup from the input. The original method\nwas used for a direct model, but can be nat-\nurally extended for a channel model. Con-\ncretely, P(ci|x) in direct models is obtained\nvia PLM(v(ci)|x1, v(c1), ··· , xK, v(cK), x), and\nP(x|ci) in channel models is obtained via\nPLM(x|v(c1), x1, ··· , v(cK), xK, v(ci)).\n4.1.3 Ensemble-based demonstrations\nWe propose a new method as an alternative to\nthe concat-based method, which we ﬁnd to be\n5318\nTransformer layer L\nTransformer layer 1\nEmbedding\n…\nHead\nA three-hour cinema master class.\n(a) All ﬁnetuning\nTransformer layer L\nTransformer layer 1\nEmbedding …\nHead\nA three-hour cinema master class.\n(d) Prompt tuning\nTransformer layer L\nTransformer layer 1\nEmbedding\n…\nHead\nA three-hour cinema master class.\n(b) Head tuning\nTransformer layer L\nTransformer layer 1\nEmbedding\n…\nHead\nA three-hour cinema master class.\n(c) Transformation tuning\nTrans\nFigure 2: Different ﬁnetuning methods, which compute the distributions of the next token given “A three-hour\ncinema master class”. Yellow and white boxes are trainable and frozen parameters, respectively. h and V denote\nthe hidden dimension of the LM and the vocabulary size of v(c1)...v(cm), respectively. All ﬁnetuning is a typical\nﬁnetuning method that updates all parameters of the LM (illustrated as a reference). Head tuning, Transformation\ntuning and Prompt tuning are described in Section 4.2; they update a very limited number of parameters.\na stronger direct model. Instead of concate-\nnating K training examples as one sequence\nand getting output probabilities from an LM\nonce, we obtain output probabilities from an\nLM K times conditioned on one training exam-\nple at a time, and multiply the resulting prob-\nabilities. Speciﬁcally, P(ci|x) is computed via\nΠK\nj=1PLM(v(ci)|xj, v(cj), x) and P(x|ci) is com-\nputed via ΠK\nj=1PLM(x|v(cj), xj, v(ci)). This\nmethod also reduces the memory consumption—\nthe concat-based method uses O(K2) while this\nmethod uses O(K)—and eliminates the depen-\ndency on the ordering of training examples, which\nhas been shown to signiﬁcantly impact the model\nperformance (Zhao et al., 2021; Lu et al., 2021).\n4.2 Tuning methods\nWe also explore methods that tune a very limited\nnumber of model parameters, as summarized in\nFigure 2. We study head tuning (Section 4.2.1)\nand transformation tuning (Section 4.2.2) for di-\nrect models. We also consider prompt tuning (Sec-\ntion 4.2.3) for both direct and channel models,\nwhich we refer as direct prompt tuning and chan-\nnel prompt tuning, respectively. All models share\nthe same input-output interface with the zero-shot\nsetup in Table 1 during training and inference.\n4.2.1 Head tuning\nHead tuning ﬁnetunes the head—the matrix in the\nLM which transforms the hidden representation\nfrom the last transformer layer to the logit values.\nLet O ∈R|V|×h be the head and hx ∈Rh be the\nhidden representations from the last transformer\nlayer given x, PLM(vi|x) for a token vi ∈ Vis\ncomputed via an i-th element of Softmax(Ohx).\nWe ﬁnetune O while freezing all other parameters\nof the LM. Although O is tied with the embedding\nmatrix of the LM during language model pretrain-\ning, we separate them during head tuning.3\n4.2.2 Transformation tuning\nAs an alternative to head tuning, we transform\nO with a new transformation matrix U ∈Rh×h.\nSpeciﬁcally, PLM(vi|x) for a token vi ∈V is com-\nputed via an i-th element of Softmax(OUhx). We\ntrain U, initialized from an identity matrix, and\nfreeze other parameters including O.\n4.2.3 Prompt tuning\nPrompt tuning is the method that has recently gath-\nered much attention (Li and Liang, 2021; Lester\net al., 2021; Liu et al., 2021). The key idea is\nto consider the LM as a black-box model and in-\nstead learn continuous prompt embeddings. We\nfollow the method from Lester et al. (2021) where\nn prompt tokens u1...un are prepended to the in-\nput, and the embeddings of u1...un are learned.\nIn other words, direct models compute P(ci|x) =\nPLM(v(ci)|u1...un, x), and channel models com-\npute P(x|ci) = PLM(x|u1...un, v(ci)). The pa-\nrameters in the LM are frozen except the embed-\ndings of u1...un.4\n5 Experimental Setup\n5.1 Datasets\nWe report results for eleven text classiﬁcation\ndatasets, following Zhang et al. (2015) and Gao\n3This is different from head tuning from prior work, e.g.,\nLe Scao and Rush (2021), which ﬁnetunes ˜PLM and uses a\nseparate, randomly initialized head instead of the LM head.\n4This is different from prompt tuning in Gao et al. (2021);\nLiu et al. (2021) which jointly trains prompt embeddings and\nthe parameters of the LM.\n5319\nDataset Task |C|\nSST-2 Sentiment analysis (movie) 2\nSST-5 Sentiment analysis (movie) 5\nMR Sentiment analysis (movie) 2\nCR Sentiment analysis (electronics) 2\nAmazon Sentiment analysis (Amazon) 5\nYelp Sentiment analysis (Yelp) 5\nTREC Question classiﬁcation (answer type) 6\nAGNews News classiﬁcation (topic) 4\nYahoo Question classiﬁcation (topic) 10\nDBPedia Ontology classiﬁcation 14\nSubj Subjectivity classiﬁcation 2\nTable 2: Datasets used for experiments. |C|denotes the\nnumber of classes.See Appendix A for samples.\net al. (2021): SST-2 (Socher et al., 2013), SST-\n5 (Socher et al., 2013), MR (Pang and Lee,\n2005), CR (Hu and Liu, 2004), Amazon (McAuley\nand Leskovec, 2013), Yelp (Zhang et al., 2015),\nTREC (V oorhees and Tice, 2000), AGNews (Zhang\net al., 2015), Yahoo (Zhang et al., 2015), DBPe-\ndia (Lehmann et al., 2015) and Subj (Pang and Lee,\n2004). The datasets include a varied number of\nclasses per task, from 2 to 14. See Table 10 in\nAppendix A for dataset samples.\n5.2 Training Data\nFor few-shot learning, we primarily use training set\nsize K = 16, but explore K = {4, 16, 64, Full}\nin the ablations. We sample the K examples uni-\nformly from the true distribution of the training\ndata. We relax the assumption from prior work\nof an equal number of training examples per la-\nbel (Gao et al., 2021; Logan IV et al., 2021), for\nmore realistic and challenging evaluation.\nWe follow all the hyperameters and details from\nprior work (Appendix B) which eliminates the need\nfor a held-out validation set. The very limited data\nis better used for training rather than validation, and\ncross-validation is less helpful when the validation\nset is extremely small (Perez et al., 2021).\n5.3 Language Models\nWe use GPT-2 (Radford et al., 2019) for the LM.\nWe primarily use GPT-2 Large but also experiment\nwith varying sizes (Small, Medium, Large and X-\nLarge) for the ablations in Appendix C. While we\nonly experiment with GPT-2, our experiments are\neasily extendable to other causal language models.\n5.4 Evaluation\nWe use accuracy as a metric for all datasets.\nWe experiment with 4 different verbalizers\n(taken from Gao et al. (2021); full list provided\nin Appendix A), 5 different random seeds for sam-\npling training data, and 4 different random seeds\nfor training. We then report Average accuracy and\nWorst-case accuracy.5 We consider the worst-case\naccuracy to be as important as the average accu-\nracy given signiﬁcantly high variance of few-shot\nlearning models, as shown in previous work (Zhao\net al., 2021; Perez et al., 2021). The worst-case\naccuracy is likely of more interest in high-risk ap-\nplications (Asri et al., 2016; Guo et al., 2017).\nOther implementation details are in Appendix B.\nAll experiments are reproducible from github.\ncom/shmsw25/Channel-LM-Prompting.\n6 Experimental Results\nThis section reports results from demonstration\nmethods (Section 6.1), tuning methods (Sec-\ntion 6.2) and ablations (Section 6.3). Discussion is\nprovided in Section 7.\n6.1 Main Results: Demonstration Methods\nTable 3 shows the performance of demonstration\nmethods.\nDirect vs. Direct++ Direct++ signiﬁcantly out-\nperforms the naive direct model across all setups,\nindicating that using P(ci|x)\nP(ci|NULL) instead of P(ci|x)\nis highly beneﬁcial as claimed by Holtzman et al.\n(2021); Zhao et al. (2021).\nConcat vs. Ensemble Our proposed, ensemble-\nbased method is better than the concat-based\nmethod in direct models, by 7% absolute in the av-\nerage accuracy and the worst-case accuracy, when\nmacro-averaged across all datasets.\nIn contrast, the ensemble-based method is not\nalways better in channel models; it is better only\non the datasets with long inputs. We conjecture\nthat the ensemble-based method may suffer when\nlabels in the training data are not balanced, which\ndirect++ explicitly takes into account as described\nin Zhao et al. (2021).\nDirect++ vs. Channel In a few-shot setting,\nchannel models outperform direct models in almost\nall cases. The strongest channel model outperforms\nthe strongest direct model by 3.1% and 7.2% ab-\nsolute, in terms of the average accuracy and the\nworst-case accuracy, respectively.\n5We also report standard deviation and best-case accuracy\nin the Appendix.\n5320\nData Zero-shot (4 runs) Concat-based (20 runs) Ensemble-based (20 runs)\nDirect Direct++ Channel Direct Direct++ Channel Direct Direct++ Channel\nSST-2 63.0/51.1 80.3/76.9 77.1/74.8 58.9/50.6 66.8/51.7 85.0/83.1 57.5/50.9 79.7/68.0 77.5/59.5\nSST-5 27.5/24.4 33.3/28.8 29.2/27.7 27.6/23.0 23.7/14.4 36.2/32.7 25.6/23.2 33.8/23.3 33.6/30.2\nMR 61.7/50.3 77.4/73.2 74.3/69.3 56.4/50.0 60.2/50.5 80.5/76.8 58.8/50.0 76.8/60.1 76.1/60.0\nCR 59.2/50.0 77.9/69.7 65.8/60.2 54.7/50.0 66.8/50.0 80.8/74.8 51.0/50.0 72.8/54.6 79.7/69.3\nAmazon 31.2/22.4 37.6/35.0 37.1/31.6 33.0/21.4 40.8/35.7 39.4/34.3 31.7/23.1 39.8/32.0 40.4/ 36.2\nYelp 33.2/25.6 36.8/31.8 38.0/31.9 32.6/23.3 38.5/31.6 39.8/36.5 31.4/23.6 39.2/29.6 41.5/38.5\nAGNews 59.8/47.8 59.9/44.0 61.8/59.7 34.0/25.0 51.2/34.4 68.5/60.6 51.9/34.2 73.1/58.6 74.3/69.3\nTREC 38.7/26.0 27.7/12.6 30.5/19.4 27.2/9.4 31.6/13.0 42.0/26.8 32.1/13.0 22.9/9.8 31.5/23.8\nYahoo 20.7/17.8 35.3/28.7 48.7/48.1 13.0/10.0 29.6/19.4 56.2/52.3 16.6/10.7 50.6/46.5 58.6/57.4\nDBPedia 32.3/18.6 37.6/30.4 51.4/42.7 32.5/7.1 71.1/55.2 58.5/40.0 46.8/17.1 72.6/55.7 64.8/ 57.0\nSubj 51.0/49.9 52.0/48.8 57.8/ 51.5 53.7/49.9 56.9/50.0 60.5/40.8 51.6/49.6 52.2/41.8 52.4/46.9\nAvg. 43.5/34.9 50.5/43.6 52.0/47.0 38.5/29.1 48.8/36.9 58.9/50.8 41.4/31.4 55.8/43.6 57.3/49.8\nTable 3: Results from demonstration methods. All with GPT-2 Large. Two numbers respectively indicate the\naverage and the worst-case accuracy over different verbalizers (zero-shot and few-shot) and data seeds (few-shot).\n‘Avg.’ in the last row indicate the macro-average across all datasets.\nStandard deviation and the best-case accuracy\nare reported in Table 11 and Table 12 in the Ap-\npendix. They indicate strong performance of chan-\nnel models can be attributed to their low variance.\nThe highest best-case accuracy is achieved by di-\nrect++ on most datasets, but it has a higher variance,\nhaving lower average and the worst-case accuracy\nthan channel models.\nZero-shot vs. Few-shot Performance of direct\nmodels sometimes degrades in a few-shot setting,\nwhich is also observed by prior work (Zhao et al.,\n2021). This is likely because demonstrations pro-\nvided by the training data may cause the model to\nbe miscalibrated and easily biased by the choice of\ndemonstrations. However, channel models achieve\nfew-shot performance that is signiﬁcantly better\nthan zero-shot methods on all datasets.\n6.2 Main Results: Tuning Methods\nTable 4 shows the performance of tuning methods.\nComparison when prompt tuning When using\nprompt tuning, channel models consistently outper-\nform direct models by a large margin on all datasets.\nImprovements are 13.3% and 23.5% absolute in the\naverage and the worst-case accuracy, respectively.\nStandard deviation and the best-case accuracy\nare reported in Table 13 in the Appendix. Con-\nsistent with the ﬁndings in Section 6.1, the strong\nperformance of channel prompt tuning can be ex-\nplained by the low variance of channel prompt tun-\ning. Direct prompt tuning often achieves higher\nbest-case accuracy; however, due to its high vari-\nance, its overall accuracy is lower, with signiﬁ-\ncantly lower worst-case accuracy.\nData Direct Channel\nHead Trans Prompt Prompt\nSST-2 80.2/68.6 77.3/57.5 72.6/50.9 85.8/81.3\nSST-5 34.9/30.0 33.0/25.5 30.9/19.1 36.3/27.9\nMR 73.7/ 56.4 71.3/51.6 67.4/50.1 81.7/78.0\nCR 67.6/50.0 63.9/50.0 65.7/50.0 79.6/76.4\nAmazon 34.5/28.8 32.1/18.2 31.2/20.0 43.4/39.2\nYelp 40.6/32.8 38.9/31.5 31.9/20.6 43.9/37.2\nTREC 54.1/42.4 48.0/31.0 35.9/13.0 37.1/20.8\nAGNews 74.1/61.2 66.9/47.0 61.9/25.2 73.4/ 63.9\nYahoo 39.1/31.4 33.8/23.0 27.4/15.7 54.0/46.7\nDBPedia 49.3/37.5 42.4/28.6 41.8/9.9 67.7/52.9\nSubj 86.3/79.1 86.0/71.6 65.5/49.9 75.5/58.8\nAvg. 57.7/47.1 54.0/39.6 48.4/29.5 61.7/53.0\nTable 4: Performance of tuning methods with a limited\nnumber of trainable parameters. All methods use GPT-\n2 Large, and are run 80 times. Head, Trans, Prompt\nindicate head tuning, transformation tuning and prompt\ntuning, respectively. We report the average / worst-case\naccuracies, separated by a slash. ‘Avg.’ is the macro-\naverage across all datasets.\nHead tuning vs. prompt tuning We ﬁnd that\nhead tuning is a very strong method, despite often\nbeing omitted as a baseline in prior work. It signiﬁ-\ncantly outperforms direct prompt tuning in all cases.\nIt also outperforms channel prompt tuning on some\ndatasets, particularly signiﬁcantly on TREC and\nSubj. For these datasets, the task—ﬁnding the type\nof the answer to the question or identifying the sub-\njectivity of the statement—is inherently different\nfrom language modeling, and likely beneﬁts from\ndirectly updating the LM parameters, rather than\nusing the LM as a black box.\nStill, channel prompt tuning outperforms direct\nhead tuning on most datasets. The largest gains\n5321\n60\n100\n4 16 64 Full\nAvg Accuracy (%)\nSST-2\n60\n100\n4 16 64 Full\nMR\n20\n100\n4 16 64 Full\nTREC\n55\n100\n4 16 64 Full\nAGNews\nDirect All Direct Head Direct Prompt Channel Prompt Direct++ Demon Channel Demon\nFigure 3: Varying the number of training examples ( K). All models use GPT-2 Large. All, Head and Prompt\nindicate ﬁnetuning all parameters of the LM, head tuning and prompt tuning, respectively. Direct++ Demon\nand Channel Demon indicate demonstration-based methods (the best out of concat-based and ensemble-based is\ntaken). Models are run 4 times for K = full (4 verbalizers) and 20 times for others (4 verbalizers and 5 data seeds).\nChannel models are more competitive with smaller K; less competitive with larger K.\nare achieved on Yahoo and DBPedia. In fact, on\nthese datasets, channel prompt tuning even outper-\nforms all ﬁnetuning—ﬁnetuning all parameters of\nthe LM—which achieves 48.9/43.8 on Yahoo and\n66.3/50.4 on DBPedia. We conjecture that using\nK = 16 on these datasets naturally requires gener-\nalization to unseen labels due to the large number\nof classes (|C|= 10 and 14), where channel prompt\ntuning signiﬁcantly outperforms direct models, as\nwe show in Section 6.4.\n6.3 Ablations\nFor the ablations, we report experiments on SST-\n2, MR, TREC and AGNews, using one train seed\n(instead of four), and four verbalizers and ﬁve data\nseeds (as in main experiments).\nVarying the number of training examples We\nvary the number of training examples (K) and re-\nport the average accuracy in Figure 3. All methods\nachieve higher accuracy as K increases. While we\nconﬁrm strong performance of channel prompt tun-\ning with K ≤16, head tuning outperforms channel\nhead tuning when K = 64 . When K = Full ,\nboth direct prompt tuning and head tuning out-\nperform channel prompt tuning. We think this is\nbecause (1) training signals ampliﬁed by channel\nmodels (Lewis and Fan, 2018) are more signiﬁcant\nwhen K is small, and (2) channel models are more\nbeneﬁcial when labels on the training data are im-\nbalanced (conﬁrmed in the next ablation), which is\nmore likely to happen with smaller K.\nIt is also worth noting that our experiment with\nK = Full conﬁrms the ﬁnding from Lester et al.\n(2021) that direct prompt tuning matches the perfor-\nmance of all ﬁnetuning—ﬁnetuning all parameters\nof the LM—while being much more parameter-\nDirect AllDirect HeadDirect PromptChannel Prompt\n50\n70\n90\n0 0.1250.250.3750.5\nAccuracy (%)\np- 0 0.1250.250.3750.5p-No UpsampleUpsample\nK=16 K=64\nFigure 4: Impact of imbalance in labels. The average\naccuracy on SST-2 and MR of different methods with\nvarying ratios of negative labels on the training data (de-\nnoted as p−), when K = 16 (left) or 64 (right). As p−\nincreases, the data is more balanced. Channel models\nare more robust to imbalanced training data.\nefﬁcient. This only holds with K = Full; in a\nfew-shot setup, all ﬁnetuning signiﬁcantly outper-\nforms other methods. This contradicts traditional\nanalysis that having less trainable parameters is\nbetter when the training data is scarce (Ng and Jor-\ndan, 2002). It is likely because such analysis did\nnot take into account language model pretraining,\nwhich gives supervision to the model yet is not the\ntraining data for an end task.\nImpact of imbalance in labels On binary\ndatasets (SST-2 and MR), we vary the label im-\nbalance in the training data with K = {16, 64}.\nSpeciﬁcally, let C = {c+, c−} and p− =\n|{(x, c) ∈ D|c = c−}|/|D|, i.e., the ratio of\nc− in the training data. We vary p− to be\n{0, 0.125, 0.250, 0.375, 0.5}. p−= 0.5 means the\nlabels are perfectly balanced, and p−= 0 means\nthat labels in the training data only include c+. We\nadditionally compare with upsampling baselines\n5322\nData Zero-shot Finetuning\nDirect++ Channel Direct\nAll\nDirect\nHead\nDirect\nTrans\nDirect\nPrompt\nChannel\nPrompt\nSST-2 80.3/76.9 77.1/74.8 50.2/49.1 50.2/49.1 50.2/49.1 50.2/49.1 85.5/82.5\nSST-5 33.3/28.8 29.2/27.7 40.1/34.8 34.3/28.0 32.6/24.5 30.0/18.1 37.5/32.6\nMR 77.4/73.2 74.3/69.3 50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 80.9/74.8\nCR 77.9/69.7 65.8/60.2 50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 80.9/74.8\nTREC 27.7/12.6 30.5/19.4 50.8/31.0 44.8/29.6 44.6/ 32.8 33.9/17.4 34.3/26.0\nSubj 52.0/48.8 57.8/51.5 50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 66.6/57.6\nTable 5: Model performance when there is at least one label at test time that was unseen during training. All\nmodels are run 20 times (4 verbalizers and 5 data seeds). All, Head, Trans and Prompt indicate ﬁnetuning all\nparameters of the LM, head tuning, transformation tuning and prompt tuning, respectively. We report the average\nand the worst-case accuracy, separated by a slash.\nDirect All Direct Head Direct Prompt Channel Prompt Zero-shot Direct++ Zero-shot Channel\n20\n40\nSST-2 MR\nTest data: SST-5\n30\n41\nSST-2 MR\nTest data: Amazon\n30\n40\nSST-2 MR\nTest data: Yelp\n20\n35\nAGNews Yahoo\nTest data: TREC\n35\n70\nSST-2 MR TREC AGNews Yahoo DBPedia\nTest data: Subj\n35\n70\nSST-2 MR TREC Subj Yahoo DBPedia\nTest data: AGNews\nFigure 5: Model performance when transferred to unseen data, where x-axis indicates training data.Direct Head is\nnot applicable when label space is not shared (when test datasets are TREC, AGNews and Subj). Channel models\nhave better generalization capacity than direct models.\nwhere we upsample training examples with infre-\nquent labels so that the model has seen an equal\nnumber of examples per label during training.\nResults are reported in Figure 4. All direct mod-\nels are sensitive to the imbalance in training data,\neven though they beneﬁt from upsampling when\np−is small. Channel prompt tuning is insensitive\nto the imbalance, and signiﬁcantly outperforms di-\nrect models when p−is small; it even outperforms\nall ﬁnetuning when p−< 0.25. When p−is near\nto 0.5, direct head tuning matches or outperforms\nchannel prompt tuning.\nIt is also worth noting that direct prompt tun-\ning with upsampling matches or outperforms all\nﬁnetuning and head tuning when p−is small.\n6.4 Generalization to unseen labels\nWe experiment with a challenging scenario where\nthe model must generalize to unseen labels. While\nit may be seen as an extreme scenario, this is often\na practical setting, e.g., the problem is deﬁned with\na set of labels but later an addition of the new label\nmay be needed.\nFirst, we sample K training examples as in main\nexperiments but excluding one random label, so\nthat at least one label at test time was unseen during\ntraining. Table 5 reports the results. All direct\nmodels are unable to predict the label that is unseen\nat training time. However, channel prompt tuning\ncan predict unseen labels and achieves considerably\nbetter performance than zero-shot. It outperforms\nall ﬁnetuning on 2-way classiﬁcation datasets, and\noutperforms head tuning on ﬁve datasets except for\nTREC on which head tuning achieves very strong\nperformance on seen labels.\nNext, we run zero-shot transfer learning, where\nthe model is trained on one dataset and is tested\non another dataset. Here, head tuning is not ap-\nplicable when the labels are not shared between\ntwo datasets. Figure 5 shows the results. Chan-\nnel prompt tuning outperforms all direct models\nincluding all ﬁnetuning on all datasets except for\nTREC. It is particularly competitive when the tasks\nare inherently similar, e.g., transfer between 2-way\nsentiment analysis and 5-way sentiment analysis in\nthe ﬁrst three ﬁgures. In fact, in such cases, perfor-\n5323\nmance is close to the models trained on in-domain\ndata. When tasks are inherently different, e.g., the\nrest of the ﬁgures in Figure 5, gains over zero-shot\nperformance are relatively small; we think more\nwork should be done to make cross-task transfer\nbetter and to discover when it is possible.\n7 Discussion & Conclusion\nIn this work, we introduced a noisy channel ap-\nproach for few-shot text classiﬁcation through LM\nprompting, where we either provide demonstra-\ntions to the LM or tune the prompt embeddings in\nthe continuous space. Our experiments on eleven\ndatasets show that channel models signiﬁcantly out-\nperform their direct counterparts, mainly because\nof their stability, i.e., lower variance and better\nworst-case accuracy. We also found that direct\nhead tuning is more competitive than previously\nthought, and different methods are preferred given\ndifferent conditions. Speciﬁcally, channel prompt\ntuning is preferred in the following scenarios.\nK is small Channel prompt tuning is more com-\npetitive when there are fewer training examples.\nWe hypothesize two reasons: (1) Channel mod-\nels are more stable (i.e., achieve low variance and\nhigh worst-case accuracy), unlike direct models\nthat are highly unstable with small k (Zhao et al.,\n2021; Perez et al., 2021; Lu et al., 2021). (2)\nChannel models provide more signals by requir-\ning the model to explain the input word-by-word\n(as claimed in Lewis and Fan (2018)) which is ben-\neﬁcial in the low data regime.\nData is imbalanced or |C| is large When the\ntraining data is even slightly imbalanced, no direct\nmodels are competitive. We think this is because\nthe LM head relies too much on unconditional dis-\ntributions of labels. Channel prompt tuning is less\nsensitive because labels are only a conditioning\nvariable. Label imbalance in the training data is a\nreal-world problem, especially when k is small and\n|C|is large. We thus suggest this is an important\narea for future work.\nGeneralization to unseen labels is required\nAll direct models are unable to predict labels that\nare unseen during training, indicating that they\noverﬁt in the label space. In contrast, channel mod-\nels can predict unseen labels, likely because the la-\nbel space is indirectly modeled. This is in line with\nprior work that shows channel models are more\ncompetitive under a distribution shift (Yogatama\net al., 2017; Lewis and Fan, 2018).\nTask is closer to language modeling If the task\nis too different from language modeling even with\ncarefully chosen verbalizers (e.g., TREC and Subj),\nhead tuning outperforms prompt tuning. This is\nlikely because it beneﬁts from directly updating the\nparameters of the LM. This may mean that causal\nLMs are not suitable for all tasks, or we need more\nsophisticated methods to apply causal LMs for such\ntasks without updating the LM parameters.\nLimitations and future work While we show\nthat channel models are competitive in few-shot\ntext classiﬁcation, there are limitations that provide\navenues for future work. First, it is not as easy\nto use channel models for non classiﬁcation tasks\nwhere modeling prior distributions is non-trivial.\nWe think future work can obtain the prior with a\nseparate model and incorporate it to the conditional\nLM as done by Lewis and Fan (2018), potentially\nwith beam search decoding as in Yu et al. (2017);\nYee et al. (2019).\nSecond, while this paper focuses on causal LMs,\nit is an open question how to use a channel model\nwith masked LMs. Although we think channel\nmodels are not inherently restricted to causal LMs,\nthe speciﬁc way in which existing masked LMs\nare pretrained makes it hard to use channel models\nwithout updating the LM parameters, e.g., masked\nLMs are not trained to generate long sentences.\nOne recent approach uses a label-conditioning ob-\njective (Tam et al., 2021) as a clever way to intro-\nduce a channel-like model with existing masked\nLMs. Extending and further integrating these dif-\nferent approaches would be important for using\nchannel models in a wider range of scenarios.\nAcknowledgements\nWe thank Ari Holtzman, Eric Wallace, Gabriel Il-\nharco, Jungsoo Park, Myle Ott, Peter West and Ves\nStoyanov for their helpful comments and discus-\nsion. This research was supported by NSF IIS-\n2044660, ONR N00014-18-1-2826, an Allen Dis-\ntinguished Investigator Award, and a Sloan Fellow-\nship.\nReferences\nHiba Asri, H. Mousannif, H. A. Moatassime, and\nThomas Noël. 2016. Using machine learning algo-\nrithms for breast cancer risk prediction and diagno-\nsis. In ANT/SEIT.\n5324\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020. Self-supervised\nmeta-learning for few-shot natural language classi-\nﬁcation tasks. In EMNLP.\nPeter F Brown, Stephen A Della Pietra, Vincent J\nDella Pietra, and Robert L Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classiﬁcation. In\nACL.\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc V Le. 2018. Semi-supervised\nsequence modeling with cross-view training. In\nEMNLP.\nXiaoan Ding and Kevin Gimpel. 2019. Latent-variable\ngenerative models for data-efﬁcient text classiﬁca-\ntion. In EMNLP.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In ICML.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In ACL.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In ICML.\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form compe-\ntition: Why the highest probability answer isn’t al-\nways right. In EMNLP.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn ICML.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining.\nPo-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-\ntau Yih, and Xiaodong He. 2018. Natural language\nto structured query generation via meta-learning. In\nNAACL.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? TACL.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nPhilipp Koehn, Franz J Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In NAACL-\nHLT.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In NAACL-HLT.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In EMNLP.\nMike Lewis and Angela Fan. 2018. Generative ques-\ntion answering: Learning to answer the whole ques-\ntion. In ICLR.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nACL.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nRobert L Logan IV , Ivana Balaževic, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting down on prompts and parameters:\nSimple few-shot learning with language models.\narXiv preprint arXiv:2106.13353.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021. Fantastically\nordered prompts and where to ﬁnd them: Overcom-\ning few-shot prompt order sensitivity.arXiv preprint\narXiv:2104.08786.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: understanding rating dimen-\nsions with review text. In Proceedings of the 7th\nACM conference on Recommender systems , pages\n165–172.\nTakeru Miyato, Andrew M Dai, and Ian Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classiﬁcation. In ICLR.\nAndrew Y Ng and Michael I Jordan. 2002. On discrim-\ninative vs. generative classiﬁers: A comparison of\nlogistic regression and naive bayes. In NeurIPS.\n5325\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In ACL.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In ACL.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative\nstyle, high-performance deep learning library. In\nNeurIPS.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In\nNeurIPS.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn NAACL-HLT.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In NeurIPS.\nClaude Elwood Shannon. 1948. A mathematical the-\nory of communication. The Bell system technical\njournal.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nDerek Tam, Rakesh R Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. In EMNLP.\nEllen M V oorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In SIGIR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In EMNLP: System Demonstrations.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V Le. 2020. Unsupervised data aug-\nmentation for consistency training. In NeurIPS.\nKenji Yamada and Kevin Knight. 2001. A syntax-\nbased statistical translation model. In ACL.\nKyra Yee, Nathan Ng, Yann N Dauphin, and Michael\nAuli. 2019. Simple and effective noisy channel mod-\neling for neural machine translation. In EMNLP.\nDani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-\nsom. 2017. Generative and discriminative text clas-\nsiﬁcation with recurrent neural networks. arXiv\npreprint arXiv:1703.01898.\nLei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-\nstette, and Tomas Kocisky. 2017. The neural noisy\nchannel. In ICLR.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In NeurIPS.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nICML.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. In NAACL-HLT.\n5326\nA Samples & Verbalizers\nTable 10 shows samples from each dataset. Ta-\nble 6 shows a list of verbalizers (four for each\ndataset), mainly taken from Gao et al. (2021) and\nlabel words included in the original data.\nB Implementation Details\nWe use PyTorch (Paszke et al., 2019) and Hugging-\nface Transformers (Wolf et al., 2020). For MR, we\nuse the sentence polarity dataset version 1.0. We\nuse the batch size of 32 and the sequence length\nof 128 for datasets with short input text (SST-2,\nSST-5, MR, TREC) and the batch size of 16 and\nthe sequence length of 256 for datasets with long\ninput text (AGNews, Amazon, Yelp, DBPedia, Ya-\nhoo, Subj). When the concat-based demonstration\nmethod is used, the sequence length is multiplied\nby the number of training examples, yet is bounded\nby 1024 which is a strict limit of GPT-2.\nFor all ﬁnetuning experiments, we train the\nmodel for 100 global steps. We use the loss divided\nby the number of all tokens in the batch. We use\nAdam optimizer (Kingma and Ba, 2015) with no\nweight decay and no warmup steps. For head tun-\ning, transformation tuning and prompt tuning, we\nuse the learning rate {0.1, 0.01, 0.001}and choose\nthe one that gives the lowest training loss on aver-\nage in order to eliminate the need of the validation\ndata. The chosen learning rate values are reported\nin Table 7. For all ﬁnetuning, we use the learning\nrate of 10−5. For prompt tuning, we use n = 20\nprompt tokens which embeddings are initialized\nfrom a random subset of the top 5000 vocabularies,\nfollowing the original paper (Lester et al., 2021).\nData Direct Channel\nHead Trans Prompt Prompt\nSST-2, SST-5 0.001 0.001 0.01 0.001\nMR 0.001 0.001 0.01 0.1\nCR 0.001 0.001 0.01 0.001\nAmazon 0.001 0.001 0.001 0.1\nYelp 0.001 0.001 0.001 0.01\nTREC 0.001 0.001 0.01 0.01\nAGNews 0.001 0.001 0.01 0.1\nYahoo 0.001 0.001 0.01 0.001\nDBPedia 0.001 0.001 0.01 0.01\nSubj 0.001 0.001 0.01 0.01\nTable 7: Learning rates of the models in Table 4.\nData Size Direct Channel\nHead Prompt Prompt\nSST-2 S,M,XL 0.001 0.01 0.001\nMR S,M,XL 0.001 0.01 0.1\nTREC S 0.01 0.01 0.1\nTREC M 0.01 0.01 1.0\nTREC XL 0.001 0.01 0.1\nAGNews S 0.001 0.01 0.1\nAGNews M 0.001 0.01 0.01\nAGNews XL 0.001 0.01 0.001\nTable 8: Learning rates of the models in Figure 6.\nData k Direct Channel\nHead Prompt Prompt\nSST-2 4 0.001 0.001 0.001\nSST-2 64 0.001 0.01 0.001\nSST-2 Full 0.001 0.01 0.1\nMR 4 0.001 0.001 0.001\nMR 64, Full 0.001 0.01 0.1\nTREC 4 0.001 0.001 0.001\nTREC 64, Full 0.001 0.01 0.1\nAGNews 4 0.001 0.001 0.1\nAGNews 64 0.001 0.01 0.01\nAGNews Full 0.001 0.01 0.1\nTable 9: Learning rates of the models in Figure 3.\nDataset Verbalizers\nSST-2, MR A MASK one.; It was MASK.; All in all MASK.; A MASK piece. (MASK={great, terrible})\nSST-5, Amaon, Yelp (Same as above.) ( MASK={great,good,okay,bad terrible})\nTREC MASK: ; Q: MASK: ; Why MASK? ; Answer: MASK\n(MASK={Description, Entity, Expression, Human, Location, Number})\nAGNews Topic: MASK.; Subject: MASK.; This is about MASK.; It is about MASK.\n(MASK={World, Sports, Business, Technology})\nYahoo\n(Same as above) (MASK={Society & Culture, Science & Mathematics, Health, Education &\nReference, Computers & Internet, Sports, Business & Finance, Entertainment & Music,\nFamily & Relationships, Politics & Government})\nDBPedia (Same as above) (MASK={Company, Educational Institution, Artist, Athlete, Ofﬁce Holder, Mean\nof Transportation, Building, Natural Place, Village, Animal, Plant, Album, Film, Written Work})\nSubj This is MASK.; It’s allMASK.’ It’sMASK.; Is it MASK? (MASK={subjective, objective})\nTable 6: Four different verbalizers for each dataset used in the experiments, separated by ‘;’. Verbalizers are taken\nfrom Gao et al. (2021) and label words included in the original data.\n5327\nData: SST-2, SST-5 and MR (Movie Sentiment Analysis)\n• A three-hour cinema master class. ( c =terrible)\n• A pretensions – and disposable story — sink the movie. ( c =great)\nData: CR\n• It is slow, if you keep the original conﬁguration and prigs (why’d u buy it then?!) it’ll run smoothly, but still slower\nthen most other coloured-screen nokias. (c =terrible)\n• It takes excellent pics and is very easy to use, if you read the manual. (c =great)\nData: Amazon\n• Don’t waste your money if you already have 2003... There isn’t one reason to get this update if you already have MS\nMoney 2003 Deluxe and Business. (c =terrible)\n• The game was in perfect condition! came before it said it should have by 2 days!! I love the game and I suggest it to\na lot of my friends!! (c =great)\nData: Yelp\n• I’ve eaten at the other location, and liked it. But I tried this place, and I have JUST NOW recovered physically enough\nfrom the worst food poisoning I’ve ever heard of to write this review. (c =terrible)\n• Great ambiance, awesome appetizers, fantastic pizza, ﬂawless customer service. ( c =great)\nData: TREC\n• How do you get a broken cork out of a bottle? (c =Description)\n• Mississippi is nicknamed what? ( c =Entity)\n• What is BPH? (c =Expression)\n• Who won the Novel Peace Prize in 1991? (c =Human)\n• What stadium do the Miami Dolphins play their home games in? ( c =Location)\n• How long did the Charles Manson murder trial last? (c =Number)\nData: AGNews\n• Peru Rebel Leader Offers to Surrender Reuters - The leader of an armed group which took over a police station in a\nsouthern Peruvian town three days ago and demanded the president’s resignation ... (c =World)\n• Walk in park for Yankees Drained by a difﬁcult week, the New York Yankees needed an uplifting victory. (c =Sports)\n• Schwab plans new, smaller branches SAN FRANCISCO – Charles Schwab & Co. is opening new ofﬁces that are\nsmaller than its current branches ... (c =Business)\n• NASA Mountain View claims world’s fastest computer. (c =Technology)\nData: Yahoo\n• What’s one change you could make to your lifestyle that would give you more peace? ... (c =Society & Culture)\n• If the average for a test was 74% and the standard deviation was 13, are you within 1 SD if you scored a 62?\n(c =Science & Mathematics)\n• Can someone explain to me what IndexOf is in Visual Basic? (c =Computers & Internet)\nData: DBPedia\n• Coca-Cola Bottling Co. Consolidated headquartered in Charlotte North Carolina is the largest independent Coca-\nCola bottler in the United States ... (c =Company)\n• Elk County Catholic High School is a private Roman Catholic high school in ... (c =Educational Institution)\n• Louis Wiltshire (born 23 April 1969) is a British sculptor. ... (c =Artist)\n• Russel Paul Kemmerer (botn November 1 1931 in Pittsburgh Pennsylvania) is an American retired professional\nbaseball player. (c =Athlete)\n• Dialectica aemula is a moth of the Gracillariidae family. ... ( c =Animal)\n• Ephedra viridis known by the common names green Mormon tea green ephedra is a species of Ephedra. (c =Plant)\nData: Subj\n• As i settled into my world war ii memory, i found myself strangely moved by even the corniest and most hackneyed\ncontrivances. (c =subjective)\n• This is a story about the warm relationship between a little girl and her father despite the difﬁcult conditions they\nhave to live in. (c =objective)\nTable 10: Samples from each dataset. c indicates the label.\n5328\nData Direct Direct++ Channel\nAvg(Std) Best Worst Avg (Std) Best Worst Avg (Std) Best Worst\nSST-2 58.9 (9.4) 77.4 50.6 66.8 (8.2) 81.0 51.7 85.0(1.1) 86.9 83.1\nSST-5 27.6 (5.2) 40.9 23.0 23.7 (4.5) 31.4 14.4 36.2(2.1) 39.6 32.7\nMR 56.4 (8.5) 78.2 50.0 60.2 (8.6) 79.0 50.5 80.5(1.8) 83.2 76.8\nCR 54.7 (7.9) 78.8 50.0 66.8 (9.8) 84.0 50.0 80.8(3.3) 86.2 74.8\nAmazon 33.0 (6.5) 43.6 21.4 40.8(2.5) 46.4 35.7 39.4 (2.5) 42.6 34.3\nYelp 32.6 (5.1) 41.6 23.3 38.5 (3.6) 44.0 31.6 39.8 (2.1) 43.8 36.5\nAGNews 34.0 (10.9) 62.3 25.0 51.2 (10.2) 68.0 34.4 68.5 (4.5) 76.1 60.6\nTREC 27.2 (9.2) 42.0 9.4 31.6 (18.9) 78.4 13.0 42.0(7.1) 54.4 26.8\nYahoo 13.0 (2.6) 18.7 10.0 29.6 (6.2) 40.7 19.4 56.2 (1.2) 57.7 52.3\nDBPedia 32.5 (17.0) 68.2 7.1 71.1 (8.0) 82.4 55.2 58.5 (12.5) 74.3 40.0\nSubj 53.7 (6.0) 71.8 49.9 56.9 (8.2) 75.9 50.0 60.5(6.5) 68.0 40.8\nAvg. 38.5 56.7 29.1 48.8 64.7 36.9 58.9 64.8 50.8\nTable 11: Full results from demonstration methods when a concat-based method is used; analogous to Table 3.\nAvg, Std, Best and Worst indicate the average accuracy, standard deviation, the best-case accuracy and the worst-\ncase accuracy, respectively. Bold: Best when combined with Table 12.\nData Direct Direct++ Channel\nAvg(Std) Best Worst Avg (Std) Best Worst Avg (Std) Best Worst\nSST-2 57.5 (9.6) 84.2 50.9 79.7 (5.8) 88.3 68.0 77.5 (7.9) 85.9 59.5\nSST-5 25.6 (2.7) 34.6 23.2 33.8 (5.8) 42.4 23.3 33.6 (2.2) 38.0 30.2\nMR 58.8 (9.9) 82.9 50.0 76.8 (6.4) 85.7 60.1 76.1 (6.6) 82.0 60.0\nCR 51.0 (2.2) 59.0 50.0 72.8 (12.0) 87.4 54.6 79.7 (4.2) 84.0 69.3\nAmazon 31.7 (6.1) 44.5 23.1 39.8 (4.6) 47.8 32.0 40.4 (2.1) 44.3 36.2\nYelp 31.4 (6.3) 41.4 23.6 39.2 (6.1) 47.3 29.6 41.5(1.3) 43.5 38.5\nAGNews 51.9 (9.8) 69.7 34.2 73.1 (6.2) 81.8 58.6 74.3(2.7) 78.5 69.3\nTREC 32.1 (10.4) 54.4 13.0 22.9 (10.1) 44.4 9.8 31.5 (5.0) 43.2 23.8\nYahoo 16.6 (4.2) 24.6 10.7 50.6 (2.1) 54.1 46.5 58.6(0.7) 59.7 57.4\nDBPedia 46.8 (15.2) 63.0 17.1 72.6(7.0) 81.9 55.7 64.8 (3.5) 70.0 57.0\nSubj 51.6 (3.4) 62.3 49.6 52.2 (5.4) 61.8 41.8 52.4 (3.0) 57.7 46.9\nAvg. 41.4 56.4 31.4 55.8 65.7 43.6 57.3 62.4 49.8\nTable 12: Full results from demonstration methods when aensemble-based method is used; analogous to Table 3.\nAvg, Std, Best and Worst indicate the average accuracy, standard deviation, the best-case accuracy and the worst-\ncase accuracy, respectively. Bold: Best when combined with Table 11.\nData Direct Head Direct Trans Direct Prompt Channel Prompt\nAvg(Std) Best Worst Avg (Std) Best Worst Avg (Std) Best Worst Avg (Std) Best Worst\nSST-2 80.2 (5.1) 88.4 68.6 77.3 (5.6) 87.7 57.5 72.6 (10.0) 89.3 50.9 85.8(1.5) 88.3 81.3\nSST-5 34.9 (2.8) 40.1 30.0 33.0 (2.7) 40.0 25.5 30.9 (5.8) 42.6 19.1 36.3(3.0) 41.6 27.9\nMR 73.7 (7.7) 83.9 56.4 71.3(8.1) 83.2 51.6 67.4 (9.9) 85.1 50.1 81.7(1.4) 84.2 78.0\nCR 67.6 (10.5) 84.0 50.0 63.9 (9.6) 84.5 50.0 65.7 (13.2) 87.4 50.0 79.6(1.4) 82.7 76.4\nAmazon 34.5 (3.5) 41.4 28.8 32.1 (4.6) 40.2 18.2 31.2 (5.7) 43.6 20.0 43.4(2.3) 49.2 39.2\nYelp 40.6 (4.0) 46.9 32.8 38.9 (3.3) 46.3 31.5 31.9 (7.7) 45.0 20.6 43.9(2.2) 47.2 37.2\nTREC 54.1(7.1) 71.2 42.4 48.0(7.4) 66.6 31.0 35.9 (11.8) 65.8 13.0 37.1 (7.3) 55.8 20.8\nAGNews 74.1(6.6) 84.5 61.2 66.9 (8.0) 83.5 47.0 61.9 (15.9) 83.5 25.2 73.4 (3.1) 77.9 63.9\nYahoo 39.1 (3.2) 44.9 31.4 33.8 (4.5) 43.8 23.0 27.4 (5.6) 39.0 15.7 54.0(2.0) 57.6 46.7\nDBPedia 49.3 (7.7) 64.2 37.5 42.4 (6.8) 56.9 28.6 41.8 (13.3) 75.3 9.9 67.7(5.7) 78.3 52.9\nSubj 86.3(3.0) 90.9 79.1 86.0(4.0) 90.8 71.6 65.5 (7.7) 78.7 49.9 75.5 (5.0) 84.5 58.8\nAvg. 57.7 67.3 47.1 54.0 65.8 39.6 48.4 66.9 29.5 61.7 67.9 53.0\nTable 13: Full results from tuning methods; analogous to Table 4. Head, Trans, Prompt indicate head tuning,\ntransformation tuning and prompt tuning, respectively. Avg, Std, Best and Worst indicate the average accuracy,\nstandard deviation, the best-case accuracy and the worst-case accuracy, respectively.\n5329\n40\n90\nAvg\nAccuracy (%)\nSST-2\nDis Label Dis Prompt Gen Prompt\n50\n90\nMR\nDis Label Dis Prompt Gen Prompt\n10\n60\nTREC\nDis Label Dis Prompt Gen Prompt\n30\n80\nAGNews\nDis Label Dis Prompt Gen Prompt\n40\n90\nS M L XL\nWorst-case \nAccuracy (%)\nDis Label Dis Prompt Gen Prompt\n50\n90\nS M L XL\nDis Label Dis Prompt Gen Prompt\n10\n60\nS M L XL\nDis Label Dis Prompt Gen Prompt\n30\n80\nS M L XL\nDis Label Dis Prompt Gen PromptDirect Head Direct Prompt Channel Prompt\nFigure 6: Varying the size of LMs from GPT-2 Small to GPT-2 X-Large. The average accuracy (top) and the\nworst-case accuracy (bottom) are reported. All models are run 20 times (4 verbalizers and 5 data seeds).Head and\nPrompt indicate head tuning and prompt tuning, respectively. Trends are consistent across different sizes of LM.\nC Additional Results\nMore metrics Table 11, 12 and 13 report the av-\nerage accuracy, the variance, the best-case accuracy\nand the worst-case accuracy using the concat-based\ndemonstration, the ensemble-based demonstration\nand the tuning methods, respectively. Results con-\nsistently indicate that channel models achieve sig-\nniﬁcantly lower variance and higher worst-case\naccuracy. The best-case accuracy is often achieved\nby direct models, but channel models outperform\ndirect models on average.\nVarying the size of LMs We vary the size of\nLMs and report the average and the worst-case\naccuracy in Figure 6. The trends—no matter the\nbest performance is achieved by channel prompt\ntuning or direct head tuning—are fairly consistent\nacross varying size of LMs.\n5330",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7916811108589172
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.7190685272216797
    },
    {
      "name": "Generalization",
      "score": 0.7126778364181519
    },
    {
      "name": "Language model",
      "score": 0.6889210939407349
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6468420028686523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.585411548614502
    },
    {
      "name": "Variance (accounting)",
      "score": 0.5714181661605835
    },
    {
      "name": "Word (group theory)",
      "score": 0.5251554846763611
    },
    {
      "name": "Stability (learning theory)",
      "score": 0.5171024799346924
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4583193361759186
    },
    {
      "name": "Machine learning",
      "score": 0.3946045935153961
    },
    {
      "name": "Speech recognition",
      "score": 0.37074920535087585
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3636210858821869
    },
    {
      "name": "Natural language processing",
      "score": 0.359358012676239
    },
    {
      "name": "Mathematics",
      "score": 0.13393402099609375
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ],
  "cited_by": 82
}