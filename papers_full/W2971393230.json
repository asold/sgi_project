{
  "title": "Behavior Gated Language Models",
  "url": "https://openalex.org/W2971393230",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5088032905",
      "name": "Prashanth Gurunath Shivakumar",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A5003744098",
      "name": "Shao-Yen Tseng",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A5021678540",
      "name": "Panayiotis Georgiou",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A5010028928",
      "name": "Shrikanth Narayanan",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2069243706",
    "https://openalex.org/W2898903480",
    "https://openalex.org/W2936652946",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2889063445",
    "https://openalex.org/W2962754271",
    "https://openalex.org/W2405047074",
    "https://openalex.org/W2014399678",
    "https://openalex.org/W2549476280",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2147069880",
    "https://openalex.org/W2963016848",
    "https://openalex.org/W2509943063",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2156199025",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2922249085",
    "https://openalex.org/W2275625487",
    "https://openalex.org/W1993521564",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2507945193",
    "https://openalex.org/W1591801644"
  ],
  "abstract": "Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.",
  "full_text": "Behavior Gated Language Models\nPrashanth Gurunath Shivakumar∗ and Shao-Yen Tseng∗\nPanayiotis Georgiou and Shrikanth Narayanan\nSignal Analysis and Interpretation Laboratory\nDepartment of Electrical and Computer Engineering\nUniversity of Southern California\nLos Angeles, CA, USA\npgurunat@usc.edu, shaoyent@usc.edu,\ngeorgiou@sipi.usc.edu, shri@sipi.usc.edu\nAbstract\nMost current language modeling techniques\nonly exploit co-occurrence, semantic and syn-\ntactic information from the sequence of words.\nHowever, a range of information such as the\nstate of the speaker and dynamics of the in-\nteraction might be useful. In this work we\nderive motivation from psycholinguistics and\npropose the addition of behavioral information\ninto the context of language modeling. We\npropose the augmentation of language models\nwith an additional module which analyzes the\nbehavioral state of the current context. This\nbehavioral information is used to gate the out-\nputs of the language model before the ﬁnal\nword prediction output. We show that the ad-\ndition of behavioral context in language mod-\nels achieves lower perplexities on behavior-\nrich datasets. We also conﬁrm the validity of\nthe proposed models on a variety of model ar-\nchitectures and improve on previous state-of-\nthe-art models with generic domain Penn Tree-\nbank Corpus.\n1 Introduction\nRecurrent neural network language models\n(RNNLM) can theoretically model the word\nhistory over an arbitrarily long length of time\nand thus have been shown to perform better than\ntraditional n-gram models (Mikolov et al., 2010).\nRecent prior work has continuously improved\nthe performance of RNNLMs through hyper-\nparameter tuning, training optimization methods,\nand development of new network architectures\n(Zaremba et al., 2014; Merity et al., 2018; Bai\net al., 2019; Dai et al., 2019).\nOn the other hand, many work have proposed\nthe use of domain knowledge and additional in-\nformation such as topics or parts-of-speech to im-\nprove language models. While syntactic tenden-\ncies can be inferred from a few preceding words,\n∗These authors contributed equally to this work\nsemantic coherence may require longer context\nand high level understanding of natural language,\nboth of which are difﬁcult to learn through purely\nstatistical methods. This problem can be over-\ncome by exploiting external information to capture\nlong-range semantic dependencies. One common\nway of achieving this is by incorporating part-of-\nspeech (POS) tags into the RNNLM as an ad-\nditional feature to predict the next word (Gong\net al., 2014; Su et al., 2017). Other useful linguis-\ntic features include conversation-type, which was\nshown to improve language modeling when com-\nbined with POS tags (Shi et al., 2010). Further\nimprovements were achieved through the addition\nof socio-situational setting information and other\nlinguistic features such as lemmas and topic (Shi\net al., 2012).\nThe use of topic information to provide seman-\ntic context to language models has also been stud-\nied extensively (Mikolov and Zweig, 2012; Ghosh\net al., 2016; Dieng et al., 2017; Wang et al., 2018).\nTopic models are useful for extracting high level\nsemantic structure via latent topics which can aid\nin better modeling of longer documents.\nRecently, however, empirical studies involving\ninvestigation of different network architectures,\nhyper-parameter tuning, and optimization tech-\nniques have yielded better performance than the\naddition of contextual information (Press, 2019;\nKrause et al., 2019). In contrast to the majority of\nwork that focus on improving the neural network\naspects of RNNLM, we introduce psycholinguis-\ntic signals along with linguistic units to improve\nthe fundamental language model.\nIn this work, we utilize behavioral informa-\ntion embedded in the language to aid the lan-\nguage model. We hypothesize that different psy-\nchological behavior states incite differences in the\nuse of language (Pennebaker and Graybeal, 2001;\nLindquist et al., 2015), and thus modeling these\narXiv:1909.00107v1  [cs.CL]  31 Aug 2019\nxtxt-1xt-2\nytyt-1yt-2yt-3\nxt-3\nFigure 1: RNN language model.\ntendencies can provide useful information in sta-\ntistical language modeling. And although not di-\nrectly related, behavioral information may also\ncorrelate with conversation-type and topic. Thus,\nwe propose the use of psycholinguistic behavior\nsignals as a gating mechanism to augment typical\nlanguage models. Effectively inferring behaviors\nfrom sources like spoken text, written articles can\nlead to personiﬁcation of the language models in\nthe speaker-writer arena.\n2 Methodology\nIn this section, we ﬁrst describe a typical RNN\nbased language model which serves as a baseline\nfor this study. Second, we introduce the proposed\nbehavior prediction model for extracting behav-\nioral information. Finally, the proposed architec-\nture of the language model which incorporates the\nbehavioral information through a gating mecha-\nnism is presented.\n2.1 Language Model\nThe basic RNNLM consists of a vanilla unidirec-\ntional LSTM which predicts the next word given\nthe current and its word history at each time\nstep. In other words, given a sequence of words\nx = x1, x2, . . . xn as input, the network predicts\na probability distribution of the next word y as\nP(y |x). Figure 1 illustrates the basic architec-\nture of the RNNLM.\nSince our contribution is towards introducing\nbehavior as a psycholinguistic feature for aiding\nthe language modeling process, we stick with a\nreliable and simple LSTM-based RNN model and\nfollow the recommendations from Zaremba et al.\n(2014) for our baseline model.\n2.2 Behavior Model\nThe analysis and processing of human behavior\ninformatics is crucial in many psychotherapy set-\ntings such as observational studies and patient\ntherapy (Narayanan and Georgiou, 2013). Prior\nx x x\nxtxt-1xt-2\nytyt-1yt-2\nztzt-1zt-2\nbtbt-1bt-2\nFigure 2: Behavior gated language model.\nwork has proposed the application of neural net-\nworks in modeling human behavior in a variety of\nclinical settings (Xiao et al., 2016; Tseng et al.,\n2016; Gibson et al., 2017).\nIn this work we adopt a behavior model that pre-\ndicts the likelihood of occurrence of various be-\nhaviors based on input text. Our model is based\non the RNN architecture in Figure 1, but instead\nof the next word we predict the joint probability of\nbehavior occurrences P(B |x) where B = {bi}\nand bi is the occurrence of behaviori. In this work\nwe apply the behaviors: Acceptance, Blame, Neg-\nativity, Positivity, and Sadness. This is elaborated\nmore on in Section 3.\n2.3 Behavior Gated Language Model\n2.3.1 Motivation\nBehavior understanding encapsulates a long-\nterm trajectory of a person’s psychological state.\nThrough the course of communication, these\nstates may manifest as short-term instances of\nemotion or sentiment. Previous work has stud-\nied the links between these psychological states\nand their effect on vocabulary and choice of words\n(Pennebaker and Graybeal, 2001) as well as use of\nlanguage (Lindquist et al., 2015). Motivated from\nthese studies, we hypothesize that due to the du-\nality of behavior and language we can improve\nlanguage models by capturing variability in lan-\nguage use caused by different psychological states\nthrough the inclusion of behavioral information.\n2.3.2 Proposed Model\nWe propose to augment RNN language models\nwith a behavior model that provides information\nrelating to a speaker’s psychological state. This\nbehavioral information is combined with hidden\nlayers of the RNNLM through a gating mecha-\nnism prior to output prediction of the next word.\nIn contrast to typical language models, we propose\nto model P(y |x, z) where z ≡f(P(B |x))\nfor an RNN function f(·). The behavior model\nis implemented with a multi-layered RNN over\nthe input sequence of words. The ﬁrst recurrent\nlayer of the behavior model is initialized with pre-\ntrained weights from the model described in Sec-\ntion 2.2 and ﬁxed during language modeling train-\ning. An overview of the proposed behavior gated\nlanguage model is shown in Figure 2. The RNN\nunits shaded in green (lower section) denote the\npre-trained weights from the behavior classiﬁca-\ntion model which are ﬁxed during the entirety of\ntraining. The abstract behavior outputs bt of the\npre-trained model are fed into a time-synced RNN,\ndenoted in blue (upper section), which is subse-\nquently used for gating the RNNLM predictions.\nThe un-shaded RNN units correspond to typical\nRNNLM and operate in parallel to the former.\n3 Experimental Setup\n3.1 Data\n3.1.1 Behavior Related Corpora\nFor evaluating the proposed model on behavior re-\nlated data, we employ the Couples Therapy Cor-\npus (CoupTher) (Christensen et al., 2004) and\nCancer Couples Interaction Dataset (Cancer) (Re-\nblin et al., 2018). These are the targeted conditions\nunder which a behavior-gated language model can\noffer improved performance.\nCouples Therapy Corpus: This corpus com-\nprises of dyadic conversations between real cou-\nples seeking marital counseling. The dataset con-\nsists of audio, video recordings along with their\ntranscriptions. Each speaker is rated by multiple\nannotators over 33 behaviors. The dataset com-\nprises of approximately 0.83 million words with\n10,000 unique entries of which 0.5 million is used\nfor training (0.24m for dev and 88k for test).\nCancer Couples Interaction Dataset: This\ndataset was gathered as part of a observational\nstudy of couples coping with advanced cancer.\nAdvanced cancer patients and their spouse care-\ngivers were recruited from clinics and asked to\ninteract with each other in two structured discus-\nsions: neutral discussion and cancer related. In-\nteractions were audio-recorded using small dig-\nital recorders worn by each participant. Manu-\nally transcribed audio has approximately 230,000\nword tokens with a vocabulary size of 8173.\n3.1.2 Penn Tree Bank Corpus\nIn order to evaluate our proposed model on more\ngeneric language modeling tasks, we employ Penn\nTree bank (PTB) (Marcus et al., 1994), as prepro-\ncessed by Mikolov et al. (2011). Since Penn Tree\nbank mainly comprises of articles from Wall Street\nJournal it is not expected to contain substantial ex-\npressions of behavior.\n3.2 Behavior Model\nThe behavior model was implemented using an\nRNN with LSTM units and trained with the Cou-\nples Therapy Corpus. Out of the 33 behavioral\ncodes included in the corpus we applied the be-\nhaviors Acceptance, Blame, Negativity, Positivity,\nand Sadness to train our models. This is motivated\nfrom previous works which showed good separa-\nbility in these behaviors as well as being easy to\ninterpret. The behavior model is pre-trained to\nidentify the presence of each behavior from a se-\nquence of words using a multi-label classiﬁcation\nscheme. The pre-trained portion of the behavior\nmodel was implemented using a single layer RNN\nwith LSTM units with dimension size 50.\n3.3 Hyperparameters\nWe augmented previous RNN language model ar-\nchitectures by Zaremba et al. (2014) and Merity\net al. (2018) with our proposed behavior gates.\nWe used the same architecture as in each work to\nmaintain similar number of parameters and per-\nformed a grid search of hyperparameters such as\nlearning rate, dropout, and batch size. The number\nof layers and size of the ﬁnal layers of the behavior\nmodel was also optimized. We report the results of\nmodels based on the best validation result.\n4 Results\nWe split the results into two parts. We ﬁrst val-\nidate the proposed technique on behavior related\nlanguage modeling tasks and then apply it on more\ngeneric domain Penn Tree bank dataset.\n4.1 Behavior Related Corpora\n4.1.1 Couple’s Therapy Corpus\nWe utilize the Couple’s Therapy Corpus as an in-\ndomain experimental corpus since our behavior\nclassiﬁcation model is also trained on the same.\nThe RNNLM architecture is similar to Zaremba\net al. (2014), but with hyperparameters optimized\nfor the couple’s corpus. The results are tabulated\nModel CoupTher Cancer\nLSTM 66.32 159.65\n+ Behavior gating 64.71 148.78\nTable 1: Language model test perplexities on Couples\nTherapy and Cancer Couples Interaction Dataset.\nin Table 1 in terms of perplexity. We ﬁnd that\nthe behavior gated language models yield lower\nperplexity compared to vanilla LSTM language\nmodel. A relative improvement of 2.43% is ob-\ntained with behavior gating on the couple’s data.\n4.1.2 Cancer Couples Interaction Dataset\nTo evaluate the validity of the proposed method\non an out-of-domain but behavior related task,\nwe utilize the Cancer Couples Interaction Dataset.\nHere both the language and the behavior mod-\nels are trained on the Couple’s Therapy Corpus.\nThe Cancer dataset is used only for development\n(hyper-parameter tuning) and testing. We ob-\nserve that the behavior gating helps achieve lower\nperplexity values with a relative improvement of\n6.81%. The performance improvements on an\nout-of-domain task emphasizes the effectiveness\nof behavior gated language models.\n4.2 Penn Tree Bank Corpus\nAlthough the proposed model is motivated and tar-\ngeted towards behavior related datasets, the hy-\npothesis should theoretically extend towards any\nhuman generated corpora. To assess this, we also\ntrain models on a non-behavior-rich database, the\nPenn Tree Bank Corpus. We experiment with both\nthe medium and large architectures proposed by\nZaremba et al. (2014). The perplexity results on\nPTB are presented in Table 2. All language mod-\nels showed an improvement in perplexity through\nthe addition of behavior gates. It can also be\nobserved that LSTM-Medium with behavior gat-\ning gives similar performance to baseline LSTM-\nLarge even though the latter has more than three\ntimes the number of parameters.\n4.2.1 Previous state-of-the-art architectures\nFinally we apply behavior gating on a previous\nstate-of-the-art architecture, one that is most often\nused as a benchmark over various recent works.\nSpeciﬁcally, we employ the AWD-LSTM pro-\nposed by Merity et al. (2018) with QRNN (Brad-\nbury et al., 2017) instead of LSTM. We observe\npositive results with AWD-LSTM augmented with\nbehavior-gating providing a relative improvement\nof (1.42% on valid) 0.66% in perplexity (Table 2).\n5 Conclusion & Future Work\nIn this study, we introduce the state of the\nspeaker/author into language modeling in the form\nof behavior signals. We track 5 behaviors namely\nacceptance, blame, negativity, positivity and sad-\nness using a 5 class multi-label behavior classiﬁ-\ncation model. The behavior states are used as gat-\ning mechanism for a typical RNN based language\nmodel. We show through our experiments that\nthe proposed technique improves language model-\ning perplexity speciﬁcally in the case of behavior-\nrich scenarios. Finally, we show improvements\non the previous state-of-the-art benchmark model\nwith Penn Tree Bank Corpus to underline the af-\nfect of behavior states in language modeling.\nIn future, we plan to incorporate the behavior-\ngated language model into the task of automatic\nspeech recognition (ASR). In such scenario, we\ncould derive both the past and the future behavior\nstates from the ASR which could then be used to\ngate the language model using two pass re-scoring\nstrategies. We expect the behavior states to be less\nprone to errors made by ASR over a sufﬁciently\nlong context and hence believe the future behavior\nstates to provide further improvements.\nModel # Params Validation Test\nLSTM-Medium (Zaremba et al., 2014) 20M 86.2 82.7\n+ Behavior gating 20M 83.85 78.75\nLSTM-Large (Zaremba et al., 2014) 66M 82.2 78.4\n+ Behavior gating 67M 80.09 75.80\nAWD-LSTM (Merity et al., 2018) 24M 60.0 57.3\n+ Behavior gating 27M 59.15 56.92\nTable 2: Language model perplexities on validation and test sets of Penn Treebank.\nReferences\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2019.\nTrellis networks for sequence modeling. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nA. Christensen, D.C. Atkins, S. Berns, J. Wheeler, D.H.\nBaucom, and L.E. Simpson. 2004. Traditional ver-\nsus integrative behavioral couple therapy for signif-\nicantly and chronically distressed married couples.\nJournal of Consulting and Clinical Psychology, 72.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nAdji B. Dieng, Chong Wang, Jianfeng Gao, and\nJohn W. Paisley. 2017. TopicRNN: A recurrent neu-\nral network with long-range semantic dependency.\nIn International Conference on Learning Represen-\ntations (ICLR).\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016. Contextual lstm\n(clstm) models for large scale nlp tasks. arXiv\npreprint arXiv:1602.06291.\nJames Gibson, Dogan Can, Panayiotis Georgiou, David\nAtkins, and Shrikanth Narayanan. 2017. Attention\nnetworks for modeling behavior in addiction coun-\nseling. In Proceedings of Interspeech.\nC. Gong, X. Li, and X. Wu. 2014. Recurrent neu-\nral network language model with part-of-speech for\nmandarin speech recognition. In The 9th Inter-\nnational Symposium on Chinese Spoken Language\nProcessing.\nBen Krause, Emmanuel Kahembwe, Iain Murray,\nand Steve Renals. 2019. Dynamic evaluation\nof transformer language models. arXiv preprint\narXiv:1904.08378.\nKristen A Lindquist, Jennifer K MacCormack, and\nHolly Shablack. 2015. The role of language in emo-\ntion: Predictions from psychological construction-\nism. Frontiers in Psychology, 6.\nMitchell Marcus, Grace Kim, Mary Ann\nMarcinkiewicz, Robert MacIntyre, Ann Bies,\nMark Ferguson, Karen Katz, and Britta Schas-\nberger. 1994. The penn treebank: annotating\npredicate argument structure. In Proceedings of\nthe workshop on Human Language Technology .\nAssociation for Computational Linguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing lstm\nlanguage models. In International Conference on\nLearning Representations (ICLR).\nTom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink,\nLuk´aˇs Burget, and Jan ˇCernock`y. 2011. Empirical\nevaluation and combination of advanced language\nmodeling techniques. In Twelfth Annual Conference\nof the International Speech Communication Associ-\nation.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn 2012 IEEE Spoken Language Technology Work-\nshop (SLT), pages 234–239. IEEE.\nS. Narayanan and P. G. Georgiou. 2013. Behavioral\nsignal processing: Deriving human behavioral in-\nformatics from speech and language. Proceedings\nof the IEEE, PP(99).\nJames W Pennebaker and Anna Graybeal. 2001. Pat-\nterns of natural language use: Disclosure, person-\nality, and social integration. Current Directions in\nPsychological Science, 10(3).\nOﬁr Press. 2019. Partially shufﬂing the training\ndata to improve language models. arXiv preprint\narXiv:1903.04167.\nMaija Reblin, Steven K. Sutton, Susan Vadaparampil,\nRichard E. Heyman, and Lee Ellington. 2018. Be-\nhind closed doors: How advanced cancer couples\ncommunicate at home. Journal of Psychosocial On-\ncology, 37.\nYangyang Shi, Pascal Wiggers, and Catholijn M\nJonker. 2010. Language modelling with dynamic\nbayesian networks using conversation types and part\nof speech information. In The 22nd Benelux Confer-\nence on Artiﬁcial Intelligence, BNAIC.\nYangyang Shi, Pascal Wiggers, and Catholijn M\nJonker. 2012. Towards recurrent neural networks\nlanguage models with linguistic and contextual fea-\ntures. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association.\nChao Su, Heyan Huang, Shumin Shi, Yuhang Guo, and\nHao Wu. 2017. A parallel recurrent neural network\nfor language modeling with pos tags. In Proceed-\nings of the 31st Paciﬁc Asia Conference on Lan-\nguage, Information and Computation , pages 140–\n147.\nShao-Yen Tseng, Sandeep Nallan Chakravarthula,\nBrian Baucom, and Panayiotis Georgiou. 2016.\nCouples behavior modeling and annotation using\nlow-resource LSTM language models. In Proceed-\nings of Interspeech.\nWenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,\nJiaji Huang, Wei Ping, Sanjeev Satheesh, and\nLawrence Carin. 2018. Topic compositional neural\nlanguage model. In International Conference on Ar-\ntiﬁcial Intelligence and Statistics.\nBo Xiao, James Gibson, Dogan Can, Zac E.\nImel, David C. Atkins, Panayiotis Georgiou, and\nShrikanth S. Narayanan. 2016. Behavioral coding\nof therapist language in addiction counseling using\nrecurrent neural networks. In Proceedings of Inter-\nspeech.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.8228539228439331
    },
    {
      "name": "Computer science",
      "score": 0.8100889921188354
    },
    {
      "name": "Language model",
      "score": 0.7197925448417664
    },
    {
      "name": "Natural language processing",
      "score": 0.6209527254104614
    },
    {
      "name": "Psycholinguistics",
      "score": 0.6198369264602661
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5898005962371826
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5801780819892883
    },
    {
      "name": "Exploit",
      "score": 0.5487580299377441
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5468667149543762
    },
    {
      "name": "Cache language model",
      "score": 0.4614642262458801
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4595258831977844
    },
    {
      "name": "Word (group theory)",
      "score": 0.4555245339870453
    },
    {
      "name": "Language identification",
      "score": 0.42421188950538635
    },
    {
      "name": "Universal Networking Language",
      "score": 0.3503536880016327
    },
    {
      "name": "Natural language",
      "score": 0.25269225239753723
    },
    {
      "name": "Linguistics",
      "score": 0.14927470684051514
    },
    {
      "name": "Comprehension approach",
      "score": 0.13951149582862854
    },
    {
      "name": "Psychology",
      "score": 0.1361123025417328
    },
    {
      "name": "Cognition",
      "score": 0.11764386296272278
    },
    {
      "name": "Parsing",
      "score": 0.09024631977081299
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 2
}