{
  "title": "Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling",
  "url": "https://openalex.org/W2298037295",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5082658100",
      "name": "Ira Leviant",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5054952724",
      "name": "Roi Reichart",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2164019165",
    "https://openalex.org/W1782560969",
    "https://openalex.org/W2102131037",
    "https://openalex.org/W2137735870",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2251771443",
    "https://openalex.org/W1523296404",
    "https://openalex.org/W2129773034",
    "https://openalex.org/W2964222437",
    "https://openalex.org/W2950682695",
    "https://openalex.org/W2137607259",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2251117789",
    "https://openalex.org/W1241017059",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2949402715",
    "https://openalex.org/W2026487812",
    "https://openalex.org/W205765513",
    "https://openalex.org/W2176085882",
    "https://openalex.org/W2100235303",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2286410738",
    "https://openalex.org/W2251970440",
    "https://openalex.org/W2132631284",
    "https://openalex.org/W2067438047",
    "https://openalex.org/W2251033195",
    "https://openalex.org/W2142625445",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2112184938",
    "https://openalex.org/W1964209958",
    "https://openalex.org/W2080100102",
    "https://openalex.org/W2251176673",
    "https://openalex.org/W342285082",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2950194079",
    "https://openalex.org/W1854884267"
  ],
  "abstract": "A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores. In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments.",
  "full_text": "Separated by an Un-common Language: Towards Judgment Language\nInformed Vector Space Modeling\nIra Leviant\nIE&M faculty\nTechnion - IIT\nira.leviant@campus.technion.ac.il\nRoi Reichart\nIE&M faculty\nTechnion - IIT\nroiri@ie.technion.ac.il\nAbstract\nA common evaluation practice in the vector\nspace models ( VSM s) literature is to measure\nthe models’ ability to predict human judg-\nments about lexical semantic relations be-\ntween word pairs. Most existing evaluation\nsets, however, consist of scores collected for\nEnglish word pairs only, ignoring the poten-\ntial impact of the judgment language in which\nword pairs are presented on the human scores.\nIn this paper we translate two prominent\nevaluation sets, wordsim353 (association) and\nSimLex999 (similarity), from English to Ital-\nian, German and Russian and collect scores\nfor each dataset from crowdworkers ﬂuent in\nits language. Our analysis reveals that human\njudgments are strongly impacted by the judg-\nment language. Moreover, we show that the\npredictions of monolingual VSM s do not nec-\nessarily best correlate with human judgments\nmade with the language used for model train-\ning, suggesting that models and humans are\naffected differently by the language they use\nwhen making semantic judgments. Finally, we\nshow that in a large number of setups, multi-\nlingual VSM combination results in improved\ncorrelations with human judgments, suggest-\ning that multilingualism may partially com-\npensate for the judgment language effect on\nhuman judgments.1\n1 Introduction\nIn recent years, there has been an immense in-\nterest in the development of Vector Space Mod-\nels (VSM s) for word meaning representation. Most\n1All the datasets and related documents produced in this\nwork will be released upon acceptance of the paper.\nVSM s are based on the distributional hypothesis\n(Harris, 1954), stating that words that occur in simi-\nlar contexts tend to have similar meanings.\nVSM s produce a vector representation for each\nword in the lexicon. A common evaluation practice\nfor such models is to compute a score for each mem-\nber of a word pair set by applying a similarity func-\ntion to the vectors of the words participating in the\npair. The resulting score should reﬂect the degree\nto which one or more lexical relations between the\nwords in the pair hold. The correlation between the\nmodel’s scores and the scores generated by human\nevaluators is then computed.\nHumans as well as VSM s may consider various\nlanguages when making their judgments and predic-\ntions. Recent research on multilingual approaches\nto VSM s aims to exploit multilingual training (train-\ning with corpora written in different languages) to\nimprove VSM predictions. The resulting models are\nevaluated either against human scores, most often\nproduced for word pairs presented to the human\nevaluators in English, or on multilingual text min-\ning tasks (§ 2). While works of the latter group do\nrecognize the connection between the VSM training\nlanguage (TL) and the task’s language, to the best of\nour knowledge no previous work systematically ex-\nplored the impact of thejudgment language (JL), the\nlanguage in which word pairs are presented to hu-\nman evaluators, on human semantic judgments and\non their correlation with VSM predictions.\nIn this paper we therefore explore two open is-\nsues: (a) the effect of the JL on the human judg-\nment of semantic relations between words; and (b)\nthe effect of the TL(s) on the capability of VSM s\nto predict human judgments generated with differ-\nent JLs. To address these issues we translate two\narXiv:1508.00106v5  [cs.CL]  6 Dec 2015\nprominent datasets of English word pairs scored for\nsemantic relations: WordSim353 ( WS353, (Finkel-\nstein et al., 2001)), consisting of 353 word pairs\nscored for association, and SimLex999 ( SL999,\n(Hill et al., 2014b)), consisting of 999 word pairs\nscored for similarity. For each dataset, the word\npairs and the annotation guidelines are translated to\nthree languages from different branches of the Indo-\nEuropean language family: German (Germanic),\nItalian (Romance) and Russian (Slavic). We then\nemploy the CrowdFlower crowdsourcing service 2\nto collect judgments for each set from human evalu-\nators ﬂuent in its JL (§ 3).\nIn § 5 we explore the hypothesis that due to a va-\nriety of factors – linguistic, cultural and others – the\nJL should affect human generated association and\nsimilarity scores. Indeed, our results show that inter\nevaluator agreement is signiﬁcantly higher within a\nJL than it is across JLs. This suggests that word as-\nsociation and similarity are JL dependent.\nWe then investigate (§ 6) the connection between\nthe VSM TL and the human JL. We experiment with\ntwo VSM s that capture distributional co-occurrence\nstatistics in different ways: a bag-of-word ( BOW)\nmodel that is based on direct counts and the neu-\nral network (NN) based word2vec ( W2V, (Mikolov\net al., 2013a)). We train these models on mono-\nlingual comparable corpora from our four JLs (§ 4)\nand compare their predicted scores with the human\nscores produced for the various JLs. Our analysis\nreveals fundamental differences between word asso-\nciation and similarity. For example, while for asso-\nciation the predictions of a VSM trained on a given\nlanguage best correlate with human judgments made\nwith that language, for similarity some JLs better\ncorrelate with all monolingual models than others.\nFinally (§ 7), we explore how multilingual model\ncombination affects the ability of VSM s to predict\nhuman judgments for varios JLs. Our results show a\npositive effect for a large number of TL and JL com-\nbinations, suggesting that multilingualism may par-\ntially compensate for the judgment language effect\non human semantic judgments.\n2 Previous Work\nVector Space Models and Their Evaluation.Ear-\n2http://www.crowdﬂower.com/\nlier VSM work (see (Turney and Pantel, 2010))\ndesigned word representations based on word co-\nlocation counts, potentially post-processed using\ntechniques such as Positive Pointwise Mutual Infor-\nmation (PPMI) and dimensionality reduction meth-\nods. Recently, much of the focus has drifted to\nthe development of NNs for representation learning\n(Bengio et al., 2003; Collobert and Weston, 2008;\nCollobert et al., 2011; Huang et al., 2012; Mikolov\net al., 2013a; Mikolov et al., 2013c; Levy and Gold-\nberg, 2014; Pennington et al., 2014, inter alia).\nVSM s have been evaluated in two main forms: (a)\ncomparing model-based word pair scores with hu-\nman judgments of various semantic relations. The\nmodel scores are generated by applying a similar-\nity function, usually the cosine metric, to the vec-\ntors generated by the model for the words in the pair\n(Huang et al., 2012; Baroni et al., 2014; Levy and\nGoldberg, 2014; Pennington et al., 2014; Schwartz\net al., 2015, inter alia); and (b) evaluating the contri-\nbution of the generated vectors to NLP applications\n(Collobert and Weston, 2008; Collobert et al., 2011;\nPennington et al., 2014, inter alia).\nSeveral evaluation sets consisting of English\nword pairs scored by humans for semantic relations\n(mostly association and similarity) are in use for\nVSM evaluation. Among these are RG-65 (Ruben-\nstein and Goodenough, 1965), MC-30 (Miller and\nCharles, 1991), WS353 ((Finkelstein et al., 2001)),\nYP-130 (Yang and Powers, 2006), and SL999 (Hill\net al., 2014b). Recently a few evaluation sets con-\nsisting of scored word pairs in languages other than\nEnglish (e.g. Arabic, French, Farsi, German, Por-\ntuguese, Romanian and Spanish) were presented\n(Gurevych, 2005; Zesch and Gurevych, 2006; Has-\nsan and Mihalcea, 2009; Schmidt et al., 2011;\nCamacho-Collados et al., 2015; K ¨oper et al., 2015,\ninter alia). Most of these datasets, however, are\ntranslations of the English sets, where the original\nhuman scores produced for the original English set\nare kept. Even for those cases where evaluation\nsets were re-scored (e.g. (Camacho-Collados et al.,\n2015; K ¨oper et al., 2015)) our investigation of the\nJL effect is much more thorough. 3 A comprehen-\n3Although WS353 was translated to German and then scored\nwith the German JL (K¨oper et al., 2015), we translated and\nscored the dataset again in order to keep the same translation\nand scoring decisions across our datasets. We applied the same\nsive list of these datasets, as well as of evaluation\nsets for word relations beyond word pair similarity\nand association (Mitchell and Lapata, 2008; Bruni et\nal., 2012; Baroni et al., 2012, inter alia), is given at\nhttp://wordvectors.org/suite.php.\nMultilingual VS Modeling.Recently, there has\nbeen a growing interest in multilingual vector space\nmodeling (Klementiev et al., 2012; Lauly et al.,\n2013; Khapra et al., 2013; Hermann and Blunsom,\n2014b; Hermann and Blunsom, 2014a; Ko ˇcisk´y et\nal., 2014; Lauly et al., 2014; Al-Rfou et al., 2013;\nFaruqui and Dyer, 2014; Coulmance et al., 2015, in-\nter alia). These works train VSM s on multilingual\ndata, either parallel or not, or combine VSM s trained\non monolingual data. The resulting models are eval-\nuated either against human scores, most often pro-\nduced for word pairs presented to the human evalua-\ntors in English, or on multilingual text mining tasks.\n3 Multilingual Human Judgment Data\nHere we describe the data collection process, con-\nsisting of dataset translation (3.1) and scoring (3.2).\nOur working datasets are WS353 (Finkelstein et al.,\n2001) and SL999 (Hill et al., 2014b).4\n3.1 Evaluation Sets Translation\nWe started by translating the WS353 and SL999\nscoring guidelines to the target languages. For each\nlanguage the translation was done by two native\nspeakers, and disagreements were solved through a\ndiscussion mediated by an experiment manager. An\nexternal evaluator, ﬂuent in both the target language\nand in English then veriﬁed the translation quality.\nThe word pair translation process was more com-\nplicated. We followed the same protocol outlined\nabove and further set a number of rules that guided\nour translators in challenging cases. Below we dis-\ncuss the different types of translation ambiguities\naddressed in our guidelines.\nGender. In some cases English does not make\ngender distinctions that some of the other languages\ndo. For example, the English word cat refers to both\nthe female and the male cat while in Russian and\nconsiderations when re-scoring the original English versions of\nWS353 and SL999.\n4The original datasets and annotation guidelines are avail-\nable at http://www.cs.technion.ac.il/∼gabr/resources/data/wordsim353/wordsim353.html\nand http://www.cl.cam.ac.uk/∼fh295/simlex.html respectively.\nItalian each gender has its own word (e.g. gatto and\ngatta in Italian). In such cases, if the other word in\nthe English pair has a clear gender interpretation we\nfollowed this gender in the translation of both words,\notherwise we chose one of the genders randomly and\nkept it ﬁxed across the target languages.5\nWord Senses.It is common that some words in a\ngiven language have a sense set that is not conveyed\nby any of the words of another given language. For\nexample, the English word plane, from the WS353\npair (car,plane), has both the airplane and the ge-\nometric plane senses. However, to the best of our\ntranslators’ knowledge, no German, Italian or Rus-\nsian word has these two senses.\nWe assume that when the authors of the evalu-\nation sets paired two words, they referred to their\nclosest senses. Therefore, like for gender, we used\nthe other word in the pair for sense disambiguation.\nIn our example, plane is translated to the target lan-\nguage word which has the airplane meaning (e.g.\nFlugzeug in German, aeroplano in Italian), since\nthis sense is closer to the meaning of car.\nIn cases where the other word in the pair does\nnot clearly disambiguate the sense of its polysemous\ncounterpart, we randomly chose one of the latter\nword’s senses, and kept it ﬁxed across the target lan-\nguages. Consider, for example, the SL999 pair (por-\ntray,decide). Portray has three senses 6 - one related\nto describing someone or something , one related to\nshowing in painting and one to playing a character\nin a tv show, play or a movie. Since it was not clear\nto our translators how the word decide can facilitate\nsense selection, we randomly chose the ﬁrst sense\nand used it across target languages.\nSense disambiguation is done on a POS basis as\nwell. For example, in the pair (attempt,peace) at-\ntempt can be a verb or a noun, but none of these\nsenses is necessarily closer to the meaning of peace.\nIn such cases, reasoning that words with the same\nPOS tend to have a closer meaning, we used the in-\nterpretation of the polysemous word which has the\nsame POS as the other word in the pair. That is, in\nthe current example attempt was assigned its noun\nsense, as peace is a noun. Naturally, the target lan-\nguage translation of a given English word may also\n5We did not observe any case of gender disagreement be-\ntween languages.\n6http://www.merriam-webster.com/dictionary/portray\nhave multiple senses, some of which are not ex-\npressed by the English word. We guided our trans-\nlators to avoid such translations whenever possible,\nalthough in a few cases that was impossible.\nPair Exclusion. We excluded some of the pairs\nfrom the evaluation sets in our experiments. Three\npairs were excluded from WS353 due to translation\ndifﬁculties. The pairs (noon,midday) and (coast,\nshore) were excluded because none of the target lan-\nguages includes two different words that convey the\nmeaning of either set. The pair(football,soccer) was\nalso excluded since it reﬂects a cultural distinction\nthat is not made in the target languages. The result-\ning datasets in all four languages therefore consist of\n350 word pairs.\nFor SL999 all 999 pairs were translated, scored\nand employed in the JL analysis of § 5. However,\nas for 23 of the pairs at least one of the participating\nwords did not appear in at least one of theVSM train-\ning corpora (see § 4), we excluded these pairs from\nthe analysis of the relations between the TLs and the\nJLs (§ 6 and § 7).\nInter Translator Agreement.The disagreement\nrates between our two translators for WS353 (700\nwords) and SL999 (1998 words) are (left paren-\ntheses for WS353, right for SL999): Russian ((85\nwords, 12.1%), (353 words, 17.7%)), Italian ((57\nwords, 8.1%), (196 words, 9.8%)) and German\n((113 words, 16.1%), (396 words, 19.8%)). To re-\nsolve disagreements, for each language we asked\none of the translators to choose the translation which\nis more similar in meaning to the other word in the\npair. If this is not possible, the translator was asked\nto choose the word which seems to her more com-\nmon in the target language.\n3.2 Word Pair Scoring\nWe next describe the word pair scoring process. In\norder to keep our analysis unbiased across JLs, we\nscored WS353 and SL999 in all four languages, in-\ncluding English. We divided each dataset to non-\noverlapping batches of 50 word pairs each (7 for\nWS353, 20 for SL999, with one SL999 batch con-\nsisting of 49 pairs) and employed the crowdﬂower\ncrowdsourcing service to recruit ﬂuent speakers of\neach target language to score each batch. Evaluators\nwere presented with the scoring guidelines trans-\nlated to their JL and were asked to score the pairs\non a 0-10 scale.\nWe veriﬁed the quality of our evaluators through\na three step process. First, for each JL we only\nrecruited evaluators who were located at a country\nwhere this language is the mother tongue of the ma-\njority of the population (i.e. US, Germany, Italy or\nRussia). Second, in order to make sure that our eval-\nuators understand the task properly, we generated 7\ntests for each language, each consisting of two word\npairs that do not appear in the evaluation set. The\nparticipating pairs consisted of words that were ei-\nther very similar or very dissimilar. Before scoring a\nbatch of word pairs, each evaluator was presented\nwith a randomly sampled test in its language and\nwas asked to score its word pairs. Every evaluator\nthat assigned a similar pair with a score lower than\n7 or a non-similar pair with a score higher than 3\nwas excluded from the experiment. Finally, we ran\nan outlier detection procedure in order to exclude\nevaluators whose scores were substantially different\nfrom those of the other evaluators of their batch. 7\nFor each evaluator we computed the distance of its\naverage score from the average of the other evalu-\nators and normalized by the standard deviation of\nthe latter set. Evaluators whose statistic was above a\npredeﬁned threshold 8 were excluded from the ﬁnal\ndataset. We performed this procedure periodically\nand once a batch had 13 annotators that passed the\ntest we stopped collecting scores for that batch.\n4 Vector Space Models\nHere we describe theVSM s we employ, their training\ndata and evaluation protocol.\n4.1 Models\nBag of Words (BOW). We constructed a VSM fol-\nlowing the optimal performance guidelines of (Kiela\nand Clark, 2014). After extracting the k most fre-\nquent words in the training corpus, we generated a\nmatrix of co-occurrence counts with a row for each\nof the words in any of the pairs in an evaluation set,\n7Some works that employ crowdsourcing compare some of\nthe collected annotation to a pre-prepared gold standard. We\nconsider our outlier detection process an alternative as it keeps\nonly those annotators who tend to agree with the others.\n8The threshold was set to 1.45, reasoning that if the statistics\nwere sampled from a Gaussian with the empirical mean and\nvariance, then ∼80% of the evaluators would be included.\nand a column for each of the kmost frequent words.\nCo-occurrence was counted within a window sizeC,\nwithout crossing sentence boundaries. The entries\nof the matrix were then normalized to PPMI values.\nThe resulting matrix’s rows constitute the vector rep-\nresentations of the words.9\nword2vec. The Mikolov et al.’s NN model\n(Mikolov et al., 2013a; Mikolov et al., 2013b). 10\nThe model aims to learn word representations that\nmaximize the objective:\nL=\nT∑\nt=1\n∑\n−c≤j≤c,j̸=0\nlog p(wt+j|wt)\nWhere T is the number of training tokens, and ca\nwindow size parameter. The objective respects sen-\ntence boundaries, conditioning only words from the\nsame sentence on each other. 11\nWe tuned three parameters D- the vector dimen-\nsionality, F - a frequency cutoff for words to be in-\ncluded in the objective, and c- the window size. We\nfollowed Radim Rehurek’s W2V tutorial 12 and set\nc= 5, D= 400and F = 1for all TLs.\n4.2 Training and Word Pair Scoring\nWe trained our VSM s on the Wikipedia corpora re-\nleased by (Al-Rfou et al., 2013) . 13 This is a set of\nmultilingual comparable corpora, as Wikipedia en-\ntries covering the same topic have similar content\nacross languages. This allows us to focus on the ef-\nfect of the ( TL, JL) combination, while keeping the\ntraining topics ﬁxed across languages.\nThe size of these corpora is as follows (left num-\nber for the number of word types, right number\nfor the number of word tokens): English (3.98 M,\n1.4 G), German (5.1 M, 484.5 M), Italian (1.65 M,\n281.6 M), Russian (2.81 M, 230 M). Before training\nthe models, we cleaned the corpora, removing stop-\nwords and any string that is not comprised of alpha-\nbetic characters only,14 and stemming the remaining\nwords using an NLTK stemmer.15\n9We experimented with k ∈{1000,2000,..., 10000}and\nC ∈{2,3,..., 8}and set k= 10000and C = 2for all TLs.\n10 http://word2vec.googlecode.com/svn/trunk/word2vec.c\n11We excluded this detail from the objective for brevity.\n12http://radimrehurek.com/2014/02/word2vec-tutorial/\n13https://sites.google.com/site/rmyeid/projects/polyglot\n14 According to the NLTK list, http://www.nltk.org/\n15http://www.nltk.org/howto/stem.html\nThe score assigned to a word pair by a model is\nthe cosine similarity between the vectors the model\ninduces for the pair’s words. For each ( TL, JL) pair\nwe compute the Spearman correlation coefﬁcient (ρ)\nbetween the ranking derived from a model’s scores\nand the ranking derived from the human scores.16\nOur main experimental setup reﬂects a preference\nfor comparable corpora. This choice has conse-\nquences: ﬁrst, our English and German corpora are\nsubstantially larger than their Russian and Italian\ncounterparts; and, second, our training corpora are\nsmaller than some of the alternative publicly avail-\nable corpora that have been used for VSM training.\nTo exclude the possibility that our observations\nare the mere outcome of these biases, we replicated\nthe experiments of § 6 and § 7 in two additional\nsetups. First, in the small training setup we re-ran\nour experiments when the English and the German\ntraining corpora were cut to the size of the Russian\nor the Italian corpus. The results in this setup were\naveraged over 5 random samples from each corpus.\nSecond, in the large training setup we re-ran our\nexperiments when the English, German and Italian\ncorpora were replaced with much larger, incompa-\nrable corpora: English with the 8G word tokens cor-\npus constructed using the W2V script, 17 and Italian\nand German with the WaCky corpora (( ?),18 Ital-\nian: 1.585G word tokens, German: 1.278G word\ntokens).19 Since the result patterns in these setups\nare very similar to those in the major, comparable\ncorpora setup, we report them brieﬂy.\n5 The Judgment Language Effect\nOur ﬁrst question is: how does theJL affect the word\npair scores produced by the human evaluators. To\nprovide a quantitative answer, we run the following\nprotocol, both within and across JLs. For each of\nthe 50 word pair batches, we generate all possible\nK-size subsets of the batch evaluators, each K-size\nsubset deﬁning a unique partition of these evaluators\n16Result patterns are very similar when considering the Pear-\nson and Kandall Tao scores. We hence keep our presentation\nconcise and report only the Spearman scores.\n17code.google.com/p/word2vec/source/browse/trunk/demo-\ntrain-big-model-v1.sh\n18http://wacky.sslmit.unibo.it/doku.php\n19Russian is not included in this latter setup since we could\nnot ﬁnd a publicly available substantially larger Russian corpus.\nL1|L2 English German Italian Russian\nmean std mean std mean std mean std\nEnglish 0.838 | 0.896 0.083 | 0.033 0.752 0.105 0.739 0.092 0.739 0.110\nGerman 0.648 0.187 0.808 | 0.864 0.062 | 0.055 0.700 0.105 0.720 0.076\nItalian 0.729 0.084 0.633 0.197 0.879 | 0.871 0.053 | 0.055 0.720 0.121\nRussian 0.724 0.097 0.621 0.170 0.705 0.073 0.880 | 0.880 0.045 | 0.033\nTable 1: Average Spearman ρ correlation coefﬁcient between human judgments in the within and the cross language setups.\nThe (L1,L2) table entry (which is further divided into mean and standard deviation (std) columns) corresponds to the comparison\nof evaluators with judgment language L1 to evaluators with judgment language L2. For each pair of languages the entry above\nthe main diagonal of the matrix is for WS353 and the entry below the main diagonal (italic font) is for SL999 (for example, the\n(German, Italian) entry is for WS353 while the (Italian, German) entry is for SL999). On the main diagonal, for both the mean and\nthe std entries, the left number is for SL999 while the right number is for WS353.\nT | J English German Italian Russian\nEnglish 0.600 0.523 0.488 0.496\nGerman 0.387 0.414 0.360 0.408\nItalian 0.485 0.410 0.451 0.427\nRussian 0.403 0.377 0.360 0.426\n(a) BOW - WS353\nT | J English German Italian Russian\nEnglish 0.652 0.618 0.614 0.585\nGerman 0.537 0.595 0.505 0.554\nItalian 0.564 0.483 0.569 0.504\nRussian 0.574 0.532 0.495 0.606\n(b) W2V - WS353\nT | J English German Italian Russian\nEnglish 0.214 0.304 0.271 0.220\nGerman 0.086 0.268 0.199 0.087\nItalian 0.140 0.236 0.214 0.115\nRussian 0.141 0.240 0.226 0.157\n(c) BOW - SL999\nT | J English German Italian Russian\nEnglish 0.266 0.354 0.308 0.260\nGerman 0.198 0.342 0.249 0.170\nItalian 0.207 0.299 0.293 0.197\nRussian 0.160 0.250 0.242 0.234\n(d) W2V - SL999\nTable 2: Spearman ρcorrelation coefﬁcient between human scores and VSM scores. The (T,J ) entry of each matrix presents the\nρvalue between the scores of aVSM trained on language T and the human scores produced for judgment languageJ. In each table,\nfor each training language (row) the best judgment language is highlighted in bold.\n(we set Kto 6). Then, for the within language eval-\nuation we calculate the correlation between the av-\neraged word pair scores of the two subsets induced\nby each K-size subset selection. For the cross lan-\nguage evaluation, in turn, we calculate the correla-\ntion between the average word pair scores of each\nK-size subset of language 1 with its corresponding\nsubset of language 2. The resulting ρ scores were\naveraged to get a ﬁnal score for each language (in\nthe within-language case) and language pair (in the\ncross-language case).20\nTable 1 presents our results. The correlations\nwithin a JL are clearly higher compared to their cross\nJL counterparts, with mean values at the range of\n[0.864 −0.896] for WS353 and [0.808 −0.880] for\nSL999 within JLs, compared to [0.700 −0.752] for\nWS353 and [0.621 −0.729] for SL999 across JLs.\nFor both evaluation sets, we ran the Welch’s t-test\n20We have 1716 K-size subsets for each batch and totals of\n1716*7 and 1716*20 correlations for each WS353 and SL999\nscenarios respectively.\nfor each set of correlations computed for an indi-\nvidual language with each set of correlations com-\nputed for a pair of languages. In all 24 cases 21 of\neach evaluation set the null hypothesis stating that\nthe two sets have an equal mean was rejected with\nPvalue< 0.001.\nFurther, the standard deviation values are[0.033−\n0.055] for WS353 and [0.045 −0.083] for SL999\nin the within language setup, compared to [0.076 −\n0.121] for WS353 and [0.073 −0.197] for SL999 in\nthe cross language setup. These results reﬂect the\nweaker dependence of the human judgment in the\nwithin language setup on the involved word-pairs\nand human evaluators.\nTo better understand the JL effect, for each JL\nwe rank the word pairs according to their average\nhuman score and, then, compute for each pair of\nJLs the relative F-score between corresponding quin-\ntiles in the rankings. 22 The top line of Figure 1\n21we have four languages and hence six language pairs.\n22For each of the 1716 K-size subset pairs (all possible divi-\n1 2 3 4 5 0.30\n.40\n.50\n.60\n.70\n.8f scoreq\nuintile number \nEng-Eng Eng-Ger Eng-Ital Eng -RussH\numan Scores - English WS353\n1 2 3 4 5 0.30\n.40\n.50\n.60\n.70\n.8f scoreq\nuintile number \nEng-Eng Eng-Ger Eng-Ital Eng -RussH\numan Scores - English SimLex999\n1 2 3 4 5 0.30\n.40\n.50\n.60\n.70\n.8f scoreq\nuintile number \nGer-Ger Ger-Eng Ger-Ital Ger-RussH\numan Scores - German WS353\n1 2 3 4 5 0\n.30\n.40\n.50\n.6f scoreq\nuintile number \nGer-Ger Ger-Eng Ger-Ital Ger-RussH\numan Scores - German SimLex999\n1 2 3 4 5 0.150\n.200\n.250\n.300\n.350\n.400\n.45f scoreq\nuintile number \nG.M-E.H G.M-G.H G.M-I.H G.M-R.HG\nerman BOW vs. Human - WS353\n1 2 3 4 5 0.100.150.200.250.300.350.400.450.50f scoreq\nuintile number \nG.M-E.H G.M-G.H G.M-I.H G.M-R.HG\nerman W2V vs. Human - WS353\n1 2 3 4 5 0\n.200\n.250\n.30f scoreq\nuintile number \nG.M-E.H G.M-G.H G.M-I.H G.M-R.HG\nerman BOW vs. Human - SimLex999\n1 2 3 4 5 0\n.200\n.250\n.30f scoreq\nuintile number \nG.M-E.H G.M-G.H G.M-I.H G.M-R.HG\nerman W2V vs. Human - SimLex999\nFigure 1: Relative F-score of the word pair lists in corresponding quintiles of: (a) human rankings with different judgment\nlanguages (top line, two left graphs show all combinations of English with another language, two right graphs show the same for\nGerman); and (b) model rankings with training language l1 vs. human ranking with judgment language l2 (bottom line, graphs\npresented for all combinations of models (BOW and W2V) and evaluation sets (WS353 and SL999) for l1 =German). Languages\nare denoted with a one or a three letter abbreviation, M stands for model and H for human.\n(two left graphs for comparisons where English is\ninvolved, two right graphs for comparisons where\nGerman is involved) reveals that the overlap be-\ntween corresponding quintiles is substantially larger\nfor the top and bottom quintiles (top and bottom\n20% of the word pairs according to each of the rank-\nings) compared to quintiles 2-4. The graphs further\ndemonstrate the larger overlap between correspond-\ning quintiles in the within language setups compared\nto the cross-language setups, highlighting the impact\nof JL differences on this phenomenon.23\nAll in all our results suggest that the concepts of\nword similarity and association may be JL depen-\ndent. Our next natural question is how the relations\nbetween the VSM TL and the humanJL affect the cor-\nsions of the scores to subsets of 6 and 7 for the within language\ncase, every subset of size 6 in one language with its correspond-\ning subset in the other language in the cross language case), we\nproduced two word pair rankings according to the average score\nwithin each subset. We then divided each ranked list to 5 quin-\ntiles and computed relative F-scores between each pair of cor-\nresponding quintiles. We ﬁnally report the average F-score for\neach pair of corresponding quintiles across all these 1716 cases.\n23We performed the same analysis for the cases where the\nItalian and Russian JLs are involved and observed very similar\npatterns. These graphs are omitted due to space constraints.\nrelation between the model and the human scores.\n6 The VSM Training Language Effect\nTable 2 presents the Spearman ρcorrelation coefﬁ-\ncient between human and model scores.\nTraining Language Choice. For each of the JLs\nJ, we ﬁrst ask what is the TL T that leads to the\nmonolingual model that best predicts human judg-\nments with J. Both word association ( WS353) and\nsimilarity (SL999) demonstrate very similar answers\nto this question.\nA ﬁrst shared pattern is that English is overall the\nbest choice of TL for both BOW and W2V: in 7 out\nof 8 cases for WS353 and in all 8 cases for SL999.\nA second shared pattern is that theJL itself is overall\nthe second best TL, which is observed in 10 of the\n11 cases where English is the best TL for a given JL\nand JL != English.\nJudgment Language Choice. Our second ques-\ntion is complementary to the ﬁrst one, namely, for\neach of the TLs T, what is the JL J that leads to\nhuman judgments that best correlate with the pre-\ndictions of the monolingual model trained with T.\nHere we observe considerable differences between\nword similarity and association.\nA ﬁrst major difference is in the identity of the\nbest JL. While for WS353 in 7 of 8 cases a model\ntrained with a TL T best correlates with human judg-\nments made with T as a JL, for SL999 both models\nbest correlate with German judgments for all TLs.\nA second major difference is related to the En-\nglish JL. For WS353 in 3 out of the 5 cases where\nEnglish is not the best JL for a model it is the sec-\nond best JL. For SL999, in contrast, for all 8 TL and\nmodel type combinations, English is the JL with the\nlowest correlation. For this dataset, Italian is always\nthe second best JL, and Russian is the third best.\nVSM Comparison. Our experiments also cast\nlight on the effectiveness of the participating VSM s.\nFor every combination of TL, JL and word pair\ndataset, the NN-based W2V is superior to the count-\nbased BOW. This ﬁnding supports recent conclu-\nsions on the superiority of ”predict” models over\ntheir ”count” counterparts (Baroni et al., 2014).\nQuintile Analysis. To further investigate the mu-\ntual impact of the TLs and JLs, we replicated the\nquintile analysis of § 5, this time comparing the\nrankings of a model trained with language l1 to the\nhuman scores obtained with JL l2.24 Results are pre-\nsented in the bottom line of Figure 1.25\nInterestingly, like in the respective analysis for\nJL pairs, human-model disagreement is generally\nmost prominent for word pairs that are considered\nof medium similarity or association. Note, however,\nthat in the current analysis, the human-model agree-\nment is weaker than the human-human agreement\non the corresponding quintiles we explored in § 5.\nMoreover, while in the analysis of § 5 the F-score\nvalues in the within language setup are superior to\ntheir cross-language counterparts, here keeping the\nTL and JL identical does not result in superior F-\nscores in most cases.\n24Since the models produce only one score per word pair, in\nthis analysis we ranked the word pairs according to the model\nscores as well as according to the average of all 13 human\nscores, divided each ranked list to quintiles and computed a rel-\native F-score for each pair of corresponding quintiles.\n25For brevity, we present only the curves for l1 =German,\nthe patterns for the other cases are very similar.\nObservations. Our analysis leads to several obser-\nvations. First, word similarity and association judg-\nments have a language speciﬁc component. Con-\nsequently, the JL is a good choice for model train-\ning (ﬁrst question) and the predictions of models\ntrained on a given language are best correlated with\nhuman judgments performed with that language, at\nleast for word association (second question). While\nthis seems obvious in machine learning terms, as in-\ndomain training is preferable and language change is\nanalogous to domain change, the semantic nature of\nour tasks would suggest that VSM s should preserve\ntheir outcome across languages. Our results suggest\nthat this latter assumption is not true.\nSecond, English has a special status in VSM re-\nsearch: as a VSM TL for both association and simi-\nlarity prediction (ﬁrst question), and as aJL for word\nassociation. The special status of English as a TL\nmay result from its simpler morphology 26 which\nmay allow more robust statistics to be collected. An-\nother possible explanation is that our evaluators are\nlikely to have some command of English 27 which\nmay bias their semantic judgments towards those\nmade by an English trained model.\nThe JL pattern is harder to understand. One pos-\nsible hypothesis is that the dominance of English for\nword association is the result of our evaluation sets\nbeing translations of sets originally authored in En-\nglish. Consequently, some important meaning com-\nponents may get lost in translation. However, the\npoor similarity predictions of both models with all\nfour TLs when English is the JL, seriously challenge\nthis hypothesis.\nFinally, for word similarity both VSM s are much\nbetter correlated with human scores when the JL is\nGerman compared to the other JLs and particularly\nto English. We will investigate this surprising obser-\nvation in future work.\nTraining Corpus Size Effect. In the small train-\ning setup our results were very similar to the re-\nsults reported above both in terms of qualitative pat-\nterns and in the numerical correlation values (up to\n0.02 difference in Spearman ρ). In the large train-\n26This is reﬂected, for example, by the lower type-to-token\nratio of English in our training corpora: English = 0.0028; Ger-\nman = 0.011; Italian = 0.0058; Russian = 0.012.\n27We have not checked this.\nT |J English German Italian Russian\nE-G 0.544 0.528 0.490 0.504\nE-I 0.575 0.531 0.516 0.517\nE-R 0.556 0.526 0.494 0.515\nG-I 0.504 0.466 0.479 0.482\nG-R 0.437 0.450 0.407 0.473\nI-R 0.508 0.445 0.475 0.481\nE-E 0.543 0.518 0.484 0.492\nG-G 0.342 0.400 0.353 0.402\nI-I 0.46 0.400 0.443 0.416\nR-R 0.395 0.365 0.355 0.420\n(a) BOW - WS353\nT |J English German Italian Russian\nE-G 0.675 0.667 0.630 0.630\nE-I 0.696 0.633 0.662 0.621\nE-R 0.687 0.645 0.624 0.646\nG-I 0.657 0.629 0.625 0.623\nG-R 0.618 0.628 0.553 0.645\nI-R 0.657 0.585 0.606 0.644\nE-E 0.652 0.620 0.609 0.591\nG-G 0.540 0.601 0.497 0.563\nI-I 0.582 0.500 0.582 0.514\nR-R 0.571 0.534 0.498 0.610\n(b) W2V - WS353\nT |J English German Italian Russian\nE-G 0.177 0.334 0.273 0.180\nE-I 0.201 0.313 0.276 0.192\nE-R 0.209 0.318 0.289 0.217\nG-I 0.131 0.294 0.238 0.119\nG-R 0.135 0.288 0.243 0.145\nI-R 0.164 0.281 0.256 0.164\nE-E 0.210 0.302 0.267 0.215\nG-G 0.078 0.259 0.194 0.078\nI-I 0.137 0.235 0.209 0.110\nR-R 0.137 0.235 0.223 0.150\n(c) BOW - SL999\nT |J English German Italian Russian\nE-G 0.263 0.392 0.313 0.244\nE-I 0.267 0.371 0.340 0.260\nE-R 0.233 0.332 0.302 0.274\nG-I 0.242 0.380 0.319 0.223\nG-R 0.212 0.338 0.284 0.242\nI-R 0.207 0.311 0.303 0.250\nE-E 0.261 0.353 0.311 0.254\nG-G 0.196 0.344 0.248 0.169\nI-I 0.206 0.307 0.300 0.195\nR-R 0.170 0.256 0.260 0.241\n(d) W2V - SL999\nTable 3: Spearman ρcorrelation coefﬁcient between human scores and the outcome of a linear interpolation (LI) of the scores of\npairs of monolingual models. The (T = L1 −L2,J = L3) entry of each table is the correlation of (1) the outcome of a LI of the\nscores of monolingual models trained on languages L1 and L2 with (2) the human scores produced with the JL L3. Cases where\nthe LI of L1 and L2 outperforms a monolingual model trained on L3 (where L3 is the JL) are highlighted in bold. .\nT |J English German Italian Russian\nE-G -0.130 -0.071 -0.070 -0.121\nE-I -0.068 -0.033 -0.011 -0.070\nE-R -0.099 -0.043 -0.026 -0.090\nG-I -0.076 -0.052 -0.038 -0.081\nG-R -0.128 -0.068 -0.075 -0.126\nI-R -0.062 -0.034 -0.032 -0.084\nE-E -0.112 -0.049 -0.034 -0.103\nG-G -0.14 -0.078 -0.107 -0.129\nI-I -0.06 -0.036 -0.006 -0.059\nR-R -0.112 -0.059 -0.039 -0.103\n(a) BOW - WS353\nT |J English German Italian Russian\nE-G 0.340 0.356 0.312 0.273\nE-I 0.295 0.289 0.302 0.259\nE-R 0.274 0.291 0.261 0.286\nG-I 0.286 0.311 0.291 0.268\nG-R 0.284 0.339 0.267 0.307\nI-R 0.223 0.218 0.230 0.261\nE-E 0.28 0.251 0.23 0.226\nG-G 0.206 0.291 0.212 0.219\nI-I 0.228 0.227 0.205 0.147\nR-R 0.236 0.252 0.253 0.297\n(b) W2V - WS353\nT |J English German Italian Russian\nE-G 0.222 0.234 0.273 0.253\nE-I 0.232 0.214 0.260 0.236\nE-R 0.270 0.240 0.289 0.277\nG-I 0.199 0.211 0.249 0.216\nG-R 0.212 0.206 0.246 0.228\nI-R 0.223 0.192 0.239 0.226\nE-E 0.244 0.226 0.274 0.258\nG-G 0.149 0.185 0.215 0.182\nI-I 0.183 0.175 0.209 0.176\nR-R 0.191 0.189 0.221 0.198\n(c) BOW - SL999\nT |J English German Italian Russian\nE-G 0.319 0.434 0.374 0.296\nE-I 0.312 0.408 0.395 0.297\nE-R 0.296 0.361 0.355 0.320\nG-I 0.289 0.417 0.369 0.257\nG-R 0.275 0.366 0.330 0.287\nI-R 0.262 0.331 0.346 0.287\nE-E 0.332 0.399 0.380 0.310\nG-G 0.251 0.394 0.310 0.237\nI-I 0.250 0.353 0.358 0.217\nR-R 0.240 0.294 0.309 0.288\n(d) W2V - SL999\nTable 4: Spearman ρcorrelation coefﬁcient of the scores resulting from a CCA combination of monolingual models, with corre-\nsponding human scores. Table entries and highlighting is as in Table 3.\ning setup we observed the exact same patterns de-\ntailed above but the Spearman ρ values for every\n(TL, JL) pair were higher than those of Table 2, with\nthe ρ differences having the following (mean, std)\nvalues: BOW /WS353: (0.036,0.025), W2V/WS353:\n(0.031,0.013), BOW /SL999: (0.048,0.046) and\nW2V/SL999: (0.042,0.033).\nOur ﬁnal investigation is of the potential of mono-\nlingual VSM combination to compensate for the JL\neffect.\n7 The Multilingual Combination Effect\nWe explore two simple methods for the combination\nof VSM s trained on corpora of different languages,\nl1 and l2. In the ﬁrst method, linear interpolation\n(LI), we combine the scores produced by two VSM s\nfor a word pair (wi,wj) using the linear equation:\nScore(wi,wj) =λ·scl1(wi,wj)+(1 −λ)·scl2(wi,wj)\nWhere scli(wi,wj) is the score produced by the\nmodel trained on the lilanguage and λ∈[0,1].28\nOur second combination method is Canonical\nCorrelation Analysis ( CCA ). For each pair of lan-\nguages, (l1,l2), we calculated a pair of projection\nmatrices to the shared subspace through the CCA\nmethod (Hardoon et al., 2004), using the vectors in-\nduced by monolingual models trained on an l1 and\nan l2 corpora. We then constructed a multilingual\nvector representation for each word by concatenat-\ning the l1 and l2 projected representations. 29 30\nWe compare the performance of each multilin-\ngual combination method to a monolingual baseline\nin which the predictions of two monolingual mod-\nels, each trained with randomly sampled 80% of the\nsame monolingual training corpus, were combined\nusing one of the above methods. 31 This is done in\norder to rule out the possibility that our improve-\nments are the mere result of the smoothing effect\nthat model combination provides.\nTables 3 (top 6 lines of each table) presents results\nfor multilingual LI. The numbers clearly show that\nthis is an effective method of combining two mono-\nlingual models, leading to improvements over the\nparticipating monolingual models in most dataset\nand model combinations. 32 Improvements com-\n28 We experimented with λ ∈{0.25,0.33,0.5,0.67,0.75}\nand got improvements for most combinations of TL pairs, JLs\nand λs (see below). We report results withλ= 0.5, giving both\nmonolingual models an equal weight.\n29Following (Faruqui and Dyer, 2014) we also experimented\nwith taking one of the monolingual projected vectors as the mul-\ntilingual representation and got very similar results.\n30We applied both protocols for the combination of three and\nfour monolingual models and did not observe substantial im-\nprovements over two-language multilingual models.\n31These results were averaged over 5 random samples from\neach of the corpora. For LI we naturally used λ= 0.5. For CCA\nwe employed the same protocol as in multilingual combination.\n32This effect is not highlighted in the table but is evident from\na comparison to the numbers reported in Table 2.\nputed with respect to monolingual models trained on\nthe JL (TL = JL, i.e. the results on the main diago-\nnals of the sub-tables of table 2), are more promi-\nnent for German, Italian and Russian than for En-\nglish (highlighted in bold in Table 3), which is not\nsurprising given that English is the bestTL of mono-\nlingual VSM s for the majority of evaluation set, JL\nand model combinations (§ 6). Multilingual inter-\npolated models improve over such non-interpolated\nmonolingual models in 68 of 96 cases (70.8%).\nComparison to monolingual LI (bottom 4 lines of\neach table) reveals the impact of the multilingual\ncombination. As an example indication, monolin-\ngual LI improves over monolingual models trained\non the JL in only 18 of 64 cases (28.1 %).33\nInterestingly, CCA combination improves over\nmonolingual models trained on the JL only for\nSL999. This result adds mixed observations to pre-\nvious positive results on the effect of CCA combina-\ntion for multilingual VSM construction with the En-\nglish JL (Faruqui and Dyer, 2014) and for the combi-\nnation of visual and textual representations (Silberer\nand Lapata, 2012; Hill et al., 2014a).\nLike in § 6, we controlled against corpus size ef-\nfects. The results of both the small and the large\ntraining setups were very similar to those reported\nabove both qualitatively and quantitatively. For ex-\nample, in the large training setups the multilin-\ngual interpolated models improved over monolin-\ngual non-interpolated models trained on the JL in\n61.1% of the cases, compared to 16.7% of the cases\nwhere the monolingual interpolated models achieve\nsuch an improvement. The differences in numerical\nSpearman ρvalues were up to 0.01 across setups.\n8 Conclusions\nIn this paper we aimed to establish the importance\nof the human JL in lexical semantics research. We\ntranslated and re-scored two prominent datasets,\nWS353 and SL999, and demonstrated the impact\nof the JL on: (a) human semantic judgments; and\n33A simple concatenation of the monolingual vectors is also\nan effective combination method of monolingual models, lead-\ning to improvements that are similar to what we report for LI.\nHowever, simple concatenation is effective for the BOW model\nonly when PPMI normalization is applied to the row counts,\nas opposed to LI which is effective regardless of this step. We\ntherefore focus on LI, the more robust method.\n(b) the correlation of monolingual and multilingual\nVSM predictions, produced with varios training lan-\nguages, with human judgments.\nIn future work we intend to extend our inquiry to\nrelations beyond word association and similarity and\nto a larger number of TLs and JLs. We further in-\ntend to explore more advanced methods for multilin-\ngual VSM construction. Finally, we would like to go\nbeyond quantitative analysis and identify qualitative\npatterns in our data. Our ultimate goal is to construct\nVSM s that directly account for the relations between\ntheir TL(s) and potential human JLs.\nReferences\n[Al-Rfou et al.2013] Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2013. Polyglot: Distributed word rep-\nresentations for multilingual nlp. In Proc. of CoNLL.\n[Baroni et al.2012] Marco Baroni, Raffaella Bernardi,\nNgoc-Quynh Do, and Chung-chieh Shan. 2012. En-\ntailment above the word level in distributional seman-\ntics. In Proc. of EACL.\n[Baroni et al.2014] Marco Baroni, Georgiana Dinu, and\nGerm´an Kruszewski. 2014. Don’t count, pre-\ndict! a systematic comparison of context-counting vs.\ncontext-predicting semantic vectors. In Proc. of ACL.\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Jauvin. 2003. A neu-\nral probabilistic language model. JMLR.\n[Bruni et al.2012] Elia Bruni, Gemma Boleda, Marco Ba-\nroni, and Nam Khanh Tran. 2012. Distributional se-\nmantics in technicolor. In Proc. of ACL.\n[Camacho-Collados et al.2015] Jos ´e Camacho-Collados,\nMohammad Taher Pilehvar, and Roberto Navigli.\n2015. A uniﬁed multilingual semantic representation\nof concepts. Proc. of ACL.\n[Church and Hanks1990] Kenneth Ward Church and\nPatrick Hanks. 1990. Word association norms, mutual\ninformation, and lexicography. Comput. Linguist. ,\n16(1):22–29.\n[Collobert and Weston2008] Ronan Collobert and Jason\nWeston. 2008. A uniﬁed architecture for natural lan-\nguage processing: Deep neural networks with multi-\ntask learning. In Proc. of ICML.\n[Collobert et al.2011] Ronan Collobert, Jason Weston,\nL´eon Bottou, Michael Karlen, Koray Kavukcuoglu,\nand Pavel Kuksa. 2011. Natural language processing\n(almost) from scratch. JMLR, 12:2493–2537.\n[Coulmance et al.2015] Jocelyn Coulmance, Jean-Marc\nMarty, Guillaume Wenzek, and Amine Benhal-\nloum. 2015. Trans-gram, fast cross-lingual word-\nembeddings. In Proc. of EMNLP (short papers).\n[Faruqui and Dyer2014] Manaal Faruqui and Chris Dyer.\n2014. Improving vector space word representations\nusing multilingual correlation. In Proc. of EACL.\n[Finkelstein et al.2001] Lev Finkelstein, Evgeniy\nGabrilovich, Yossi Matias, Ehud Rivlin, Zach\nSolan, Gadi Wolfman, and Eytan Ruppin. 2001.\nPlacing search in context: The concept revisited. In\nProc. of WWW.\n[Gurevych2005] Iryna Gurevych. 2005. Using the struc-\nture of a conceptual network in computing semantic\nrelatedness. In Proc. of IJCNLP.\n[Hardoon et al.2004] David Hardoon, Sandor Szedmak,\nand John Shawe-Taylor. 2004. Canonical correla-\ntion analysis: An overview with application to learning\nmethods. Neural computation, 16(12):2639–2664.\n[Harris1954] Zellig Harris. 1954. Distributional struc-\nture. Word.\n[Hassan and Mihalcea2009] Samer Hassan and Rada Mi-\nhalcea. 2009. Cross-lingual semantic relatedness us-\ning encyclopedic knowledge. In Proc. of EMNLP.\n[Hermann and Blunsom2014a] Karl Moritz Hermann and\nPhil Blunsom. 2014a. Multilingual Distributed Rep-\nresentations without Word Alignment. In Proceedings\nof ICLR.\n[Hermann and Blunsom2014b] Karl Moritz Hermann and\nPhil Blunsom. 2014b. Multilingual models for com-\npositional distributed semantics. In Proc. of ACL.\n[Hill et al.2014a] Felix Hill, Roi Reichart, and Anna Ko-\nrhonen. 2014a. Multi-modal models for concrete and\nabstract concept meaning. Transactions of the Associ-\nation for Computational Linguistics, 2(10):285–296.\n[Hill et al.2014b] Felix Hill, Roi Reichart, and Anna\nKorhonen. 2014b. Simlex-999: Evaluating se-\nmantic models with (genuine) similarity estimation.\narXiv:1408.3456 [cs.CL].\n[Huang et al.2012] Eric H Huang, Richard Socher,\nChristopher D Manning, and Andrew Y Ng. 2012.\nImproving word representations via global context\nand multiple word prototypes. In Proc. of ACL.\n[Khapra et al.2013] Mitesh M Khapra, Balaraman Ravin-\ndran, Vikas Raykar, and Amrita Saha. 2013. Multilin-\ngual deep learning. In Proceedings of NIPS Workshop\non Deep Learning.\n[Kiela and Clark2014] Douwe Kiela and Stephen Clark.\n2014. A systematic study of semantic vector space\nmodel parameters. In Proc. of the 2nd Workshop on\nContinuous Vector Space Models and their Composi-\ntionality (CVSC), EACL.\n[Klementiev et al.2012] Alexandre Klementiev, Ivan\nTitov, and Binod Bhattarai. 2012. Inducing crosslin-\ngual distributed representations of words. In Proc. of\nCOLING.\n[K¨oper et al.2015] Maximilian K ¨oper, Christian Scheible,\nand Sabine Schulte im Walde. 2015. Multilingual\nreliability and semantic structure of continuous word\nspaces. IWCS 2015.\n[Koˇcisk´y et al.2014] Tom ´aˇs Ko ˇcisk´y, Karl Moritz Her-\nmann, and Phil Blunsom. 2014. Learning Bilingual\nWord Representations by Marginalizing Alignments.\nIn Proc. of ACL.\n[Lauly et al.2013] Stanislas Lauly, Alex Boulanger, and\nHugo Larochelle. 2013. Learning multilingual word\nrepresentations using a bag-of-words autoencoder. In\nProc. of NIPS Workshop on Deep Learning.\n[Lauly et al.2014] Stanislas Lauly, Hugo Larochelle,\nMitesh M Khapra, Balaraman Ravindran, Vikas\nRaykar, Amrita Saha, et al. 2014. An autoencoder ap-\nproach to learning bilingual word representations. In\nProc. of NIPS.\n[Levy and Goldberg2014] Omer Levy and Yoav Gold-\nberg. 2014. Dependency-based word embeddings. In\nProc. of ACL (Volume 2: Short Papers).\n[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013a. Efﬁcient estima-\ntion of word representations in vector space. In Proc.\nof ICLR.\n[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,\nKai Chen, Greg S Corrado, and Jeff Dean. 2013b.\nDistributed representations of words and phrases and\ntheir compositionality. In Proc. of NIPS.\n[Mikolov et al.2013c] Tomas Mikolov, Wen-tau Yih, and\nGeoffrey Zweig. 2013c. Linguistic regularities in\ncontinuous space word representations. In Proc. of\nNAACL-HLT.\n[Miller and Charles1991] George A Miller and Walter G\nCharles. 1991. Contextual correlates of semantic sim-\nilarity. Language and cognitive processes, 6(1):1–28.\n[Mitchell and Lapata2008] Jeff Mitchell and Mirella Lap-\nata. 2008. Vector-based models of semantic composi-\ntion. In Proc. of ACL-HLT. Association for Computa-\ntional Linguistics.\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D. Manning. 2014. Glove:\nGlobal vectors for word representation. In Proc. of\nEMNLP.\n[Rubenstein and Goodenough1965] Herbert Rubenstein\nand John B Goodenough. 1965. Contextual corre-\nlates of synonymy. Communications of the ACM ,\n8(10):627–633.\n[Schmidt et al.2011] Sebastian Schmidt, Philipp Scholl,\nChristoph Rensing, and Ralf Steinmetz. 2011. Cross-\nlingual recommendations in a resource-based learn-\ning scenario. In Towards Ubiquitous Learning, pages\n356–369. Springer.\n[Schwartz et al.2015] Roy Schwartz, Roi Reichart, and\nAri Rappoport. 2015. Symmetric pattern based word\nembeddings for improved word similarity prediction.\nIn Proc. of CoNLL.\n[Silberer and Lapata2012] Carina Silberer and Mirella\nLapata. 2012. Grounded models of semantic repre-\nsentation. In Proc. of EMNLP-CoNLL.\n[Turney and Pantel2010] Peter Turney and Patrick Pantel.\n2010. From frequency to meaning: Vector space mod-\nels of semantics. Journal of artiﬁcial intelligence re-\nsearch, 37(1):141–188.\n[Yang and Powers2006] Dongqiang Yang and David MW\nPowers. 2006. Verb similarity on the taxonomy of\nwordnet. In Proc. of GWC-06.\n[Zesch and Gurevych2006] Torsten Zesch and Iryna\nGurevych. 2006. Automatically creating datasets for\nmeasures of semantic relatedness. In Proceedings of\nthe Workshop on Linguistic Distances. Association for\nComputational Linguistics.",
  "topic": "Space (punctuation)",
  "concepts": [
    {
      "name": "Space (punctuation)",
      "score": 0.5607156753540039
    },
    {
      "name": "Linguistics",
      "score": 0.4604760706424713
    },
    {
      "name": "Computer science",
      "score": 0.40014171600341797
    },
    {
      "name": "Psychology",
      "score": 0.36006438732147217
    },
    {
      "name": "Natural language processing",
      "score": 0.32862478494644165
    },
    {
      "name": "Philosophy",
      "score": 0.1589784324169159
    }
  ],
  "institutions": []
}