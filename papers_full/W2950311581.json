{
  "title": "Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State",
  "url": "https://openalex.org/W2950311581",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3191941820",
      "name": "Futrell, Richard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288398382",
      "name": "Wilcox, Ethan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172163099",
      "name": "Morita Takashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1985713137",
      "name": "Qian Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3007789332",
      "name": "Ballesteros, Miguel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2511155128",
      "name": "Levy, Roger",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2795342569",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2118276816",
    "https://openalex.org/W2768794963",
    "https://openalex.org/W2962733492",
    "https://openalex.org/W2506931122",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2022741721",
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W2043146406",
    "https://openalex.org/W2112106114",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964335542",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1667107303",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2054518132",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2942054564",
    "https://openalex.org/W2130494162",
    "https://openalex.org/W1975081431",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W1955233831",
    "https://openalex.org/W2076332735",
    "https://openalex.org/W2152068514",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W2962941914",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W2154424448",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2061311021",
    "https://openalex.org/W2474191362",
    "https://openalex.org/W2951795952"
  ],
  "abstract": "We deploy the methods of controlled psycholinguistic experimentation to shed light on the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we examine model behavior on artificial sentences containing a variety of syntactically complex structures. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNNG (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence that the LSTMs trained on large datasets represent syntactic state over large spans of text in a way that is comparable to the RNNG, while the LSTM trained on the small dataset does not or does so only weakly.",
  "full_text": "Neural Language Models as Psycholinguistic Subjects: Representations of\nSyntactic State\nRichard Futrell1, Ethan Wilcox2, Takashi Morita3,4, Peng Qian5, Miguel Ballesteros6, and Roger Levy5\n1Department of Language Science, UC Irvine, rfutrell@uci.edu\n2Department of Linguistics, Harvard University,wilcoxeg@g.harvard.edu\n3Primate Research Institute, Kyoto University,tmorita@alum.mit.edu\n4Department of Linguistics and Philosophy, MIT\n5Department of Brain and Cognitive Sciences, MIT, rplevy@mit.edu\n6IBM Research, MIT-IBM Watson AI Lab,miguel.ballesteros@ibm.com\nAbstract\nWe deploy the methods of controlled psy-\ncholinguistic experimentation to shed light on\nthe extent to which the behavior of neural\nnetwork language models reﬂects incremental\nrepresentations of syntactic state. To do so, we\nexamine model behavior on artiﬁcial sentences\ncontaining a variety of syntactically complex\nstructures. We test four models: two publicly\navailable LSTM sequence models of English\n(Jozefowicz et al., 2016; Gulordava et al.,\n2018) trained on large datasets; an RNNG\n(Dyer et al., 2016) trained on a small, parsed\ndataset; and an LSTM trained on the same\nsmall corpus as the RNNG. We ﬁnd evidence\nthat the LSTMs trained on large datasets rep-\nresent syntactic state over large spans of text in\na way that is comparable to the RNNG, while\nthe LSTM trained on the small dataset does not\nor does so only weakly.\n1 Introduction\nIt is now standard practice in NLP to derive sen-\ntence representations using neural sequence mod-\nels of various kinds (Elman, 1990; Sutskever et al.,\n2014; Goldberg, 2017; Peters et al., 2018; De-\nvlin et al., 2018). However, we do not yet have a\nﬁrm understanding of the precise content of these\nrepresentations, which poses problems for inter-\npretability, accountability, and controllability of\nNLP systems. More speciﬁcally, the success of\nneural sequence models has raised the question\nof whether and how these networks learn robust\nsyntactic generalizations about natural language,\nwhich would enable robust performance even on\ndata that differs from the peculiarities of the train-\ning set.\nHere we build upon recent work studying neural\nlanguage models using experimental techniques\nthat were originally developed in the ﬁeld of psy-\ncholinguistics to study language processing in\nthe human mind. The basic idea is to examine\nlanguage models’ behavior on targeted sentences\nchosen to probe particular aspects of the learned\nrepresentations. This approach was introduced by\nLinzen et al. (2016), followed more recently by\nothers (Bernardy and Lappin, 2017; Enguehard\net al., 2017; Gulordava et al., 2018), who used\nan agreement prediction task (Bock and Miller,\n1991) to study whether RNNs learn a hierarchical\nmorphosyntactic dependency: for example, that\nThe key to the cabinets. . .can grammatically con-\ntinue with was but not withwere. This dependency\nturns out to be learnable from a language mod-\neling objective (Gulordava et al., 2018). Subse-\nquent work has extended this approach to other\ngrammatical phenomena, with positive results for\nﬁller–gap dependencies (Chowdhury and Zampar-\nelli, 2018; Wilcox et al., 2018) and negative results\nfor anaphoric dependencies (Marvin and Linzen,\n2018).\nIn this work, we consider syntactic representa-\ntions of a different kind. Previous studies have fo-\ncused on relationships of dependency: one word\nlicenses another word, which is tested by asking\nwhether a language model favors one (grammat-\nically licensed) form over another in a particular\ncontext. Here we focus instead on whether neural\nlanguage models show evidence for incremental\nsyntactic staterepresentations: whether behavior\nof neural language models reﬂects the kind of gen-\neralizations that a symbolic grammar-based de-\nscription of language would capture using a stack-\nbased incremental parse state. For example, dur-\ning the underlined portion of Example (1), an in-\ncremental language model should represent and\nmaintain the knowledge that it is currently inside a\nsubordinate clause, implying (among other things)\nthat a full main clause must follow.\n(1) As the doctor studied the textbook, the\nnurse walked into the ofﬁce.\narXiv:1903.03260v1  [cs.CL]  8 Mar 2019\nIn this work, we use a targeted evaluation ap-\nproach (Marvin and Linzen, 2018) to elicit ev-\nidence for syntactic state representations from\nlanguage models. That is, we examine language\nmodel behavior on artiﬁcially constructed sen-\ntences designed to expose behavior that is cru-\ncially dependent on syntactic state representa-\ntions. In particular, we study complex subordinate\nclauses and garden path effects (based on main-\nverb/reduced-relative ambiguities and NP/Z am-\nbiguities). We ask three general questions: (1) Is\nthere basic evidence for the representation of syn-\ntactic state? (2) What textual cues does a neural\nlanguage model use to infer the beginnings and\nendings of such states? (3) Do the networks main-\ntain knowledge about syntactic states over long\nspans of complex text, or do the syntactic state rep-\nresentations degrade?\nAmong neural language models, we study both\ngeneric sequence models (LSTMs), which have no\nexplicit representation of syntactic structure, and\nan RNN Grammar (RNNG) (Dyer et al., 2016),\nwhich explicitly calculates Penn Treebank-style\ncontext-free syntactic representations as part of\nthe process of assigning probabilities to words.\nThis comparison allows us to evaluate the ex-\ntent to which explicit representation of syntactic\nstructure makes models more or less sensitive to\nsyntactic state. RNNGs have been found to out-\nperform LSTMs not only in overall test-set per-\nplexity (Dyer et al., 2016), but also in modeling\nlong-distance number agreement in Kuncoro et al.\n(2018) for certain model conﬁgurations; our work\nextends this comparison to a variety of syntactic\nstate phenomena.\n2 General methods\nWe investigate neural language model behavior\nprimarily by studying the surprisal, or log inverse\nprobability, that a language model assigns to each\nword in a sentence:\nS(xi) =−log2 p(xi|hi−1),\nwhere xi is the current word or character, hi−1 is\nthe model’s hidden state before consuming xi, the\nprobability is calculated from the network’s soft-\nmax activation, and the logarithm is taken in base\n2, so that surprisal is measured in bits. Surprisal\nis equivalent to the pointwise contribution to the\nlanguage modeling loss function due to a word.\nIn psycholinguistics, the common practice is to\nstudy reaction times per word (for example, read-\ning time as measured by an eyetracker), as a mea-\nsure of the word-by-word difﬁculty of online lan-\nguage processing. These reading times are often\ntaken to reﬂect the extent to which humans ex-\npect certain words in context, and may be gener-\nally proportional to surprisal given the comprehen-\nder’s probabilistic language model (Hale, 2001;\nLevy, 2008; Smith and Levy, 2013). In this study,\nwe take language model surprisal as the analogue\nof human reading time, using it to probe the neu-\nral networks’ expectations about what words will\nfollow in certain contexts. There is a long tra-\ndition linking RNN performance to human lan-\nguage processing (Elman, 1990; Christiansen and\nChater, 1999; MacDonald and Christiansen, 2002)\nand grammaticality judgments (Lau et al., 2017),\nand RNN surprisals are a strong predictor of hu-\nman reading times (Frank and Bod, 2011; Good-\nkind and Bicknell, 2018). RNNGs have also been\nused as models of human online language process-\ning (Hale et al., 2018).\n2.1 Experimental methodology\nIn each experiment presented below, we design\na set of sentences such that the word-by-word\nsurprisal values will show evidence for syntac-\ntic state representations. The idea is that certain\nwords will be surprising to a language model only\nif the model has a representation of a certain syn-\ntactic state going into the word. We analyze word-\nby-word surprisal proﬁles for these sentences us-\ning regression analysis. Except where otherwise\nnoted, all statistics are derived from linear mixed-\neffects models (Baayen et al., 2008) with sum-\ncoded ﬁxed-effect predictors and maximal random\nslope structure (Barr et al., 2013). This method lets\nus factor out by-item variation in surprisal and fo-\ncus on the contrasts between conditions.\n2.2 Models tested\nWe study the behavior of four models of English:\ntwo LSTMs trained on large data, an an RNNG\nand an LSTM trained on matched, smaller data\n(the Penn Treebank). The models are summarized\nin Table 1. All models are trained on a language\nmodeling objective.\nOur ﬁrst LTSM is the model presented in Joze-\nfowicz et al. (2016) as “BIG LSTM+CNN Inputs”,\nwhich we call “JRNN”, which was trained on\nthe One Billion Word Benchmark (Chelba et al.,\n2013) with two hidden layers of 8196 units each\nand CNN character embeddings as input. The sec-\nond large LSTM is the model described in the sup-\nModel Architecture Training data Data size (tokens) Reference\nJRNN LSTM One Billion Word ∼800 million Jozefowicz et al. (2016)\nGRNN LSTM Wikipedia ∼90 million Gulordava et al. (2018)\nRNNG RNN Grammar Penn Treebank ∼1 million Dyer et al. (2016)\nTinyLSTM LSTM Penn Treebank ∼1 million —\nTable 1: Models tested, by architecture, training data, and training data size.\nplementary materials of Gulordava et al. (2018),\nwhich we call “GRNN”, trained on 90 million to-\nkens of English Wikipedia with two hidden layers\nof 650 hidden units each.\nOur RNNG is trained on syntactically labeled\nPenn Treebank data (Marcus et al., 1993), us-\ning 256-dimensional word embeddings for the in-\nput layer and 256-dimensional hidden layers, and\ndropout probability 0.3. Next-word predictions are\nobtained through hierarchical softmax (140 clus-\nters, obtained with the greedy agglomerative clus-\ntering algorithm of Brown et al. (1992)). We es-\ntimate word surprisals using word-synchronous\nbeam search (Stern et al., 2017; Hale et al., 2018):\nat each word wi a beam of incremental parses is\nﬁlled, the summed forward probabilities (Stolcke,\n1995) of all candidates on the beam is taken as a\nlower bound on the preﬁx probability: Pmin(w1...i),\nand the surprisal of the i-th word in the sentence\nis estimated as log Pmin(w1...i)\nPmin(w1...i−1) . Our action beam is\nsize 100, and our word beam is size 10.Finally,\nwe use an LSTM trained on string data from the\nPenn Treebank training set, which we call TinyL-\nSTM, to disentangle effects of training set from\nmodel architecture. For TinyLSTM we use 256-\ndimensional word-embedding inputs and hidden\nlayers and dropout probability 0.3, just as with the\nRNNG.\n3 Subordinate clauses\nWe begin by studying subordinate clauses, a key\nexample of a construction requiring stack-like rep-\nresentation of syntactic state. In such construc-\ntions, as shown in Example (1), a subordinator\nsuch as “as” or “when” serves as a cue that the\nfollowing clause is a subordinate clause, meaning\nthat it must be followed by some main (matrix)\nclause. In an incremental language model, this\nknowledge must be maintained and carried for-\nward while processing the words inside subordi-\nnate clause. A grammar-based symbolic language\nmodel (e.g., Stolcke, 1995; Manning and Carpen-\nter, 2000) would maintain this knowledge by keep-\ning track of syntactic rules representing the incom-\nRNNG tinylstm\nGRNN JRNN\nno−matrix matrix no−matrix matrix\n−10\n−5\n0\n5\n−10\n−5\n0\n5\nContinuation\n(Sub. present − sub. absent) surprisal difference\nFigure 1: Effect of subordinator absence/presence on\nsurprisal of continuations. Red: no-matrix penalty ef-\nfect. Blue: matrix licensing effect. In this and all other\nﬁgures, unless otherwise noted, error bars represent\n95% conﬁdence intervals of the contrasts between con-\nditions shown, computed from the standard error of the\nby-item and by-condition mean surprisals after sub-\ntracting out the by-item means (Masson and Loftus,\n2003).\nplete subordinate clause and the upcoming main\nclause in a stack data structure. Psycholinguis-\ntic research has clearly demonstrated that humans\nmaintain representations of this kind in syntactic\nprocessing (Staub and Clifton, 2006; Lau et al.,\n2006; Levy et al., 2012). Here we ask whether the\nstring completion probabilities produced by neu-\nral language models show evidence of the same\nknowledge.\nWe can detect the knowledge of syntactic state\nin this case by examining whether the network li-\ncenses and requires a matrix clause following the\nsubordinate clause. These expectations can be de-\ntected by examining surprisal differences between\nsentences of the form in Example (2):\n(2) a. As the doctor studied the textbook,\nthe nurse walked into the ofﬁce.\n[SUB ordinator, MATRIX ]\nb. *As the doctor studied the textbook.\n[SUB , NO-MATRIX ]\nc. ?The doctor studied the textbook,\nthe nurse walked into the ofﬁce.\n[NO-SUB ordinator, MATRIX ]\nd. The doctor studied the textbook.\n[NO-SUB , NO-MATRIX ]\nIf the network licenses a matrix clause fol-\nlowing the subordinate clause—and maintained\nknowledge of that licensing relationship through-\nout the clause, from the subordinator to the\ncomma—then this should be manifested as lower\nsurprisal at the matrix clause in (2-a) as com-\npared to (2-c). We call this the matrix licensing\neffect: the surprisal of the condition [ SUB , MA-\nTRIX ] minus [ NOSUB , MATRIX ], which will be\nnegative if there is a licensing effect. If the net-\nwork requires a following matrix clause, then this\nwill be manifested as higher surprisal at the ma-\ntrix clause for (2-b) compared with (2-d). We call\nthis the no-matrix penalty effect: the surprisal of\n[SUB ,NOMATRIX ] minus [ NOSUB , NOMATRIX ],\nwhich will be positive if there is a penalty.\nWe designed 23 experimental items on the pat-\ntern of (2) and calculated difference in the sum sur-\nprisal of the words in the matrix clause. Figure 2\nshows the matrix licensing effect (in blue) and the\nno-matrix penalty effect (in red), averaged across\nitems. For all models, we see a facilitative matrix\nlicensing effect ( p < .001 for all models), small-\nest in TinyLSTM. However, we only ﬁnd a signif-\nicant no-matrix penalty for GRNN and the RNNG\n(p < .001 in both): the other models do not sig-\nniﬁcantly penalize an ungrammatical continuation\n(p = .9 for JRNN; p = .5 for TinyLSTM). That\nis, JRNN and TinyLSTM give no indication that\n(2-b) is less probable than (2-c).\nWe found that all models at least partially repre-\nsent the licensing relationship between a subordi-\nnate and matrix clause. However, in order to fully\nrepresent the syntactic requirements induced by a\nsubordinator, it seems that a model needs either\nlarge amounts of data (as in GRNN) or explicit\nrepresentation of syntax (as in the RNNG, as op-\nposed to TinyLSTM).\n3.1 Maintenance and degradation of\nsyntactic state\nThe foregoing results show that neural language\nmodels use the presence of a subordinator as a\ncue to the onset of a subordinate clause, and that\nthey maintain knowledge that they are in a sub-\nordinate clause throughout the intervening mate-\nrial up to the comma. Now we probe the ability\nof models to maintain this knowledge over long\nspans of complex intervening material. To do so,\nwe use sentences on the template of (2) and add\nintervening material modifying the NPs in the sub-\nordinate clause. To both of these NPs (in subject\nand object position), we add modiﬁers of increas-\ning syntactic complexity: PPs, subject-extracted\nrelative clauses (SRCs), and object-extracted rela-\ntive clauses (ORCs). We study the extent to which\nthese modiﬁers weaken the language models’ ex-\npectations about the upcoming matrix clause.\nAs a summary measure of the strength of lan-\nguage models’ expectations about an upcoming\nmatrix clause, we collapse the two measures of the\nprevious section into one: thematrix licensing in-\nteraction, consisting of the difference between the\nno-matrix penalty effect and the matrix licensing\neffect (the two bars in Figure 1). A similar mea-\nsure was used to detect ﬁller–gap dependencies in\n(Wilcox et al., 2018).\nFigure 2 shows the strength of the matrix li-\ncensing interaction given sentences with various\nmodiﬁers inserted. For the large LSTMs, GRNN\nexhibits a strong interaction when the intervening\nmaterial is short and syntactically simple, and the\ninteraction gets progressively weaker as the inter-\nvening material becomes progressively longer and\nmore complex ( p < 0.001 for subject postmodi-\nﬁers and p < 0.01 object postmodiﬁers). The other\nmodels show less interpretable behavior.\nOur results indicate that at least some large\nLSTMs, along with the RNNG, are capable of\nmaintaining a representation of syntactic state over\nspans of complex intervening material. Quanti-\nﬁed as a licensing interaction, this representation\nof syntactic state exhibits the most clearly un-\nderstandable behavior in GRNN, which shows a\ngraceful degradation of syntactic expectations as\nthe complexity of intervening material increases.\nThe representation is maintained most strongly in\nthe RNNG, except for one particular construction\n(object-position SRCs).\n4 Garden path effects\nThe major phenomenon that has been used to\nprobe incremental syntactic representations in hu-\nmans is garden path effects. Garden path effects\narise from local ambiguities, where a context leads\na comprehender to believe one parse is likely, but\nthen a disambiguating word forces her to dras-\ntically revise her beliefs, resulting in high sur-\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits)7.0 7.5 8.0 8.5 9.0\nJRNN\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits)6 7 8 9 10\nGRNN\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits)\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nRNNG\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits)0.5 1.0\ntinylstm\nFigure 2: Size of matrix clause licensing interaction (see text) given various intervening elements in the subordinate\nclause. Note that the heatmaps are on different scales across models.\nprisal/reading time at the disambiguating word. In\neffect, the comprehender is “led down the garden\npath” by a locally likely but ultimately incorrect\nparse (Bever, 1970). Garden-pathing in LSTMs\nhas recently been demonstrated by van Schijndel\nand Linzen (2018a,b) in the context of modeling\nhuman reading times.\nGarden path effects allow us to detect represen-\ntations of syntactic state because if a person or lan-\nguage model shows a garden path effect at a word,\nthat means that the person or model had some be-\nlief about syntactic state which was disconﬁrmed\nby that word. In psycholinguistics, these effects\nhave been used to study the question of what in-\nformation determines people’s beliefs about likely\nparses given locally ambiguous contexts: for ex-\nample, whether factors such as world knowledge\nplay a role (Ferreira and Clifton, 1986; Trueswell\net al., 1994).\nHere we study two major kinds of local ambigu-\nities inducing garden path effects. For each ambi-\nguity, we ask two main questions. First, whether\nthe network shows the basic garden path effect,\nwhich would indicate that it had a syntactic state\nrepresentation that made a disambiguating word\nsurprising. Second, whether the network is sen-\nsitive to subtle lexical cues to syntactic structure\nwhich may modulate the size of the garden path\neffect: this question allows us to determine what\ninformation the network uses to determine the be-\nginnings and endings of certain syntactic states.\n4.1 NP/Z Ambiguity\nThe NP/Z ambiguity1 refers to a local ambigu-\nity in sentences of the form given in Example (3).\nWhen a comprehender reads the underlined phrase\n“the vet with his new assistant” in (3-a), she may\nat ﬁrst believe that this phrase is the direct ob-\nject of the verb “scratched” inside the subordinate\nclause. However, upon reaching the verb “took\noff”, she realizes that the underlined phrase was\nnot in fact an object of the verb “scratched”, rather\nit was the subject of a new clause, and the subordi-\nnate clause ended after the verb “scratched”. The\nkey region of the sentence where the garden path\ndisambiguation happens—called the disambigua-\ntor—is the phrase “took off”, marked in bold.\n(3)a. When the dog scratched the vet with his new\nassistant took offthe muzzle. [TRANSITIVE ,\nNOCOMMA ]\nb. When the dog scratched,the vet with his new\nassistant took offthe muzzle. [TRANSITIVE ,\nCOMMA ]\n1For Noun Phrase/Zero ambiguity. At ﬁrst the embedded\nverb appears to take an NP object, but later it turns out that it\nwas a zero (null) object.\nRNNG tinylstm\nGRNN JRNN\ntransitive intransitive transitive intransitive\n0\n2\n4\n6\n0\n2\n4\n6\nEmbedded verb transitivity\nGarden path effect (bits)\nFigure 3: Average garden path effect (surprisal at dis-\nambiguator in NO-COMMA condition minus COMMA\ncondition) by model and embedded verb transitivity.\nc. When the dog struggled the vet with\nhis new assistant took off the muzzle.\n[INTRANSITIVE , NOCOMMA ]\nd. When the dog struggled, the vet with\nhis new assistant took off the muzzle.\n[INTRANSITIVE , COMMA ]\nWhile a garden path should obtain in (3-a), no\nsuch garden path should exist for (3-b), because\na comma clearly demarcates the end of the sub-\nordinate clause. Therefore a basic garden path ef-\nfect would be indicated by the difference in sur-\nprisal at the disambiguator for (3-a) minus (3-b).\nFurthermore, if a comprehender is sensitive to the\nrelationship between verb argument structure and\nclause boundaries, then there should be no gar-\nden path in (3-c), because the verb “struggled”\nis INTRANSITIVE : it cannot take an object in En-\nglish, so an incremental parser should never be\nmisled into believing that “the vet...” is its object.\nThis lexical information about syntactic structure\nis subtle enough that there has been controversy\nabout whether even humans are sensitive to it in\nonline processing (Staub, 2007).\n4.1.1 NP/Z Garden Path Effect\nWe tested whether neural language models would\nshow the basic garden path effect and if this ef-\nfect would be modulated by verb transitivity. We\nconstructed 32 items based of the same structure\nas (3), based on materials from Staub (2007), ma-\nnipulating the transitivity of the embedded verb\n(“scratched” vs. “struggled”), and the presence of\na disambiguating comma at the end of the subor-\ndinate clause. An NP/Z garden path effect would\nshow up as increased surprisal at the main verb\n“took off” in the absence of a comma. If the net-\nworks use the transitivity of the embedded verb as\na cue to clause structure, and maintain that infor-\nmation over the span of six words between the em-\nbedded verb and the main verb, then there should\nbe a garden path effect for the transitive verb, but\nnot for the intransitive verb. More generally we\nwould expect a stronger garden path given the\ntransitive verb than given the intransitive verb.\nFigure 3 shows the mean surprisals at the dis-\nambiguator for all four models, for both transi-\ntive and intransitive embedded verbs. We see that\na garden path effect exists in all models (though\nvery small in TinyLSTM): all models show sig-\nniﬁcantly higher surprisal at the main verb when\nthe disambiguating comma is absent (p < .001 for\nall models). However, only the large LSTMs ap-\npear to be sensitive to the transitivity of the em-\nbedded verb, showing a smaller garden path effect\nfor intransitive verbs. Statistically, there is a sig-\nniﬁcant interaction of comma presence and verb\ntransitivity only in GRNN and JRNN (GRNN:\np < .01; JRNN: p < .001; RNNG: p = .3, TinyL-\nSTM: p = .3).\nAll models show NP/Z garden path effects, indi-\ncating that they are sensitive to some cues indicat-\ning end-of-clause boundaries. However, only the\nlarge LSTMs appear to use verb argument struc-\nture information as a cue to these boundaries. The\nresults suggest that very large amounts of data may\nbe necessary for current neural models to discover\nsuch ﬁne-grained dependencies between syntactic\nproperties of verbs and sentence structure.\n4.1.2 Maintenance and degradation of state\nWe can probe the maintenance and degradation\nof syntactic state information by manipulating the\nlength of the intervening material between the on-\nset of the local ambiguity and the disambiguator\nin examples such as (3). The question is whether\nthe networks maintain the knowledge, while pro-\ncessing the intervening material, that the inter-\nvening noun phrase is probably the object of the\nembedded verb inside a subordinate clause, or\nwhether they gradually lose track of this infor-\nmation. To study this question we used materials\non the pattern of (4): these materials manipulate\nthe length of the intervening material (underlined)\nwhile holding constant the distance between the\nsubordinator (“As”) and the disambiguator (grew).\nRNNG tinylstm\nGRNN JRNN\nshort long short long\n0\n2\n4\n6\n0\n2\n4\n6\nLength of ambiguous region\nGarden path effect (bits)\nFigure 4: Average garden path effect by model and\nlength of ambiguous region.\n(4)a. As the author studying Babylon in ancient\ntimes wrote the book grew. [ SHORT , NO-\nCOMMA ]\nb. As the author studying Babylon in an-\ncient times wrote, the book grew. [ SHORT ,\nCOMMA ]\nc. As the author wrote the book describing\nBabylon in ancient times grew. [LONG , NO-\nCOMMA ]\nd. As the author wrote, the book describing\nBabylon in ancient times grew. [ LONG ,\nCOMMA ]\nIf neural language models show degradation of\nsyntactic state, then the garden path effect (mea-\nsured as the difference in surprisal between the\nCOMMA and NO-COMMA conditions at the disam-\nbiguator) will be smaller for the LONG conditions.\nWe tested 32 sentences of the form in (4), based\non materials from Tabor and Hutchins (2004). The\ngarden path effect sizes are shown in Figure 4.\nWe ﬁnd a signiﬁcant garden effect in all mod-\nels in the SHORT condition ( p < .001 in JRNN\nand GRNN; p < .01 in the RNNG and p = .03 in\nTinyLSTM). In the long condition, we ﬁnd the gar-\nden path effect in all models except TinyLSTM:\n(p < .001 in JRNN; p < .01 in GRNN; p = .02 in\nthe RNNG; and p = .2 in TinyLSTM). The cru-\ncial interaction between length and comma pres-\nence (indicating that syntactic state degrades) is\nsigniﬁcant in GRNN ( p < .01) and TinyLSTM\n(p < .001) but not JRNN ( p = .7) nor the RNNG\n(p = .6). The pattern is reminiscent of the results\non degradation of state information about subor-\nRNNG tinylstm\nGRNN JRNN\nambig unambig ambig unambig\n0\n2\n4\n6\n0\n2\n4\n6Garden path effect (bits)\nFigure 5: Garden path effect size for MV/RR ambiguity\nby model and verb-form ambiguity.\ndinate clauses in Section 3, where GRNN and\nTinyLSTM showed the clearest evidence of degra-\ndation.\nNote that the pattern found here is the opposite\nof the pattern of human reading times. Humans ap-\npear to show “digging-in” effects: the longer the\nspan of time between the introduction of a local\nambiguity and its resolution, the larger the garden\npath effect (Tabor and Hutchins, 2004; Levy et al.,\n2009).\n4.2 Main Verb/Reduced Relative Ambiguity\nNext we turn to garden path effects induced by the\nclassic Main Verb/Reduced Relative (MV/RR)\nambiguity, in which a word is locally ambiguous\nbetween being the main verb of a sentence or in-\ntroducing a reduced relative clause(reduced RC:\na relative clause with no explicit complementizer,\nheaded by a passive-participle verb). That ambi-\nguity can be maintained over a long stretch of ma-\nterial:\n(5)a. The woman brought the sandwich from\nthe kitchen tripped on the carpet.\n[REDUCED , AMBIG uous]\nb. The woman who was brought the sand-\nwich from the kitchen tripped on the carpet.\n[UNREDUCED , AMBIG ]\nc. The woman given the sandwich from\nthe kitchen tripped on the carpet.\n[REDUCED , UNAMBIG uous]\nd. The woman who was given the sandwich\nfrom the kitchen tripped on the carpet.\n[UNREDUCED , UNAMBIG ]\nPhenomenon GRNN JRNN RNNG TinyLSTM\nSubordination \u0013\u0013 \u0013\u0017 \u0013\u0013 \u0013\u0017\nNP/Z Garden Path \u0013\u0017 \u0013\u0013 \u0013\u0017 \u0013\u0017\nMV/RR Garden Path \u0013\u0013 \u0013\u0017 \u0013\u0017 \u0013\u0017\nTable 2: Summary of results by model and phenomenon. The ﬁrst check mark indicates basic evidence of syntactic\nstate representation. The second check mark indicates the ability to capture more ﬁne-grained phenomena: for\nsubordination, the no-matrix penalty effect; for the NP/Z garden path, the effect of verb transitivity; and for the\nMV/RR garden path, the effect of verb morphology.\nIn Example (5-a), the verb “brought” is ini-\ntially analyzed as a main verb phrase, but upon\nreaching the verb “tripped”—the disambiguator\nin this case—the reader must re-analyze it as an\nRC. The garden path should be eliminated in sen-\ntences such as (5-b), the UNREDUCED condition,\nwhere the words “who was” clarify that the verb\n“brought” is part of an RC, rather than the main\nverb of the sentence. Therefore we quantify the\ngarden path effect as the surprisal at the disam-\nbiguator for the REDUCED minus UNREDUCED\nconditions.\nThere is another possible cue that the initial verb\nis the head of an RC: the morphological form of\nthe verb. In examples such as (5-c), the the verb\n“given” is unambiguously in its past-participle\nform, indicating that it cannot be the main verb\nof the sentence. If a language model is sensitive\nto morphological cues to syntactic structure, then\nit should either not show a garden path effect in\nthis UNAMBIG uous condition, or it should show a\nreduced garden path effect.\nWe constructed 29 experimental items follow-\ning the template of (5). Figure 5 shows the garden\npath effect sizes by model and verb-form ambigu-\nity. All networks show the basic garden path effect\n(p < .001 in JRNN, GRNN, and RNNG; p < 0.01\nin TinyLSTM). However, the garden path effect in\nTinyLSTM is much smaller than the other mod-\nels: RC reduction causes an additional .3 bits of\nsurprisal at the disambiguating verb, as compared\nto 2.8 bits in the RNNG, 1.9 in JRNN, and 3.6\nin GRNN (TinyLSTM’s garden path effect is sig-\nniﬁcantly smaller than each other model at p <\n0.001).\nIf the network is using the morphological form\nof the verb as a cue to syntactic structure, then it\nshould show the garden path effect more strongly\nin the AMBIG condition than the UNAMBIG condi-\ntion. The large language models and the RNNG do\nshow this pattern: at the critical main-clause verb,\nsurprisal is superadditively highest in the reduced\nambiguous condition (the dotted blue line; a posi-\ntive interaction between the reduced and ambigu-\nous conditions is signiﬁcant in the three models at\np < 0.001). However, TinyLSTM does not show\nevidence for superadditive surprisal for the am-\nbiguous verbform and the reduced RC (p = .45).\nThe three large LSTMs and the RNNG replicate\nthe key human-like garden-path disambiguation\neffect due to to ambiguity in verb form. But strik-\ningly, even when the participial verbform is un-\nambiguous, there is still a signiﬁcant garden path\neffect in all models (p < 0.01 in all models except\nTinyLSTM, where p = .08). Apparently, these\nnetworks treat an unambiguous passive-participial\nverb as only a noisy cueto the presence of an RC.\n5 General Discussion and Conclusion\nIn all models studied, we found clear evidence\nof basic incremental state syntactic representation.\nHowever, models varied in how well they fully\ncaptured the effects of such state and the poten-\ntially subtle lexical cues indicating the beginnings\nand endings of such states: only the large LSTMs\ncould sometimes reliably infer clause boundaries\nfrom verb argument structure (Section 4.1) and\nmorphological verb-form (Section 4.2), and only\nGRNN and the RNNG fully captured the proper\nbehavior of subordinate clauses. The results are\nsummarized in Table 2. We suggest that repre-\nsentation of course-grained syntactic structure re-\nquires either syntactic supervision or large data,\nwhile exploiting ﬁne-grained lexical cues to struc-\nture requires large data.\nMore generally, we believe that the psycholin-\nguistic methodology employed in this paper pro-\nvides a valuable lens on the internal represen-\ntations of black-box systems, and can form the\nbasis for more systematic tests of the linguistic\ncompetence of NLP systems. We make all exper-\nimental items, results, and analysis scripts avail-\nable online at github.com/langprocgroup/nn_\nsyntactic_state.\nReferences\nR.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.\nMixed-effects modeling with crossed random effects\nfor subjects and items. Journal of Memory and Lan-\nguage, 59(4):390–412.\nDale J Barr, Roger Levy, Christoph Scheepers, and\nHarry J Tily. 2013. Random effects structure for\nconﬁrmatory hypothesis testing: Keep it maximal.\nJournal of Memory and Language, 68(3):255–278.\nJean-Philippe Bernardy and Shalom Lappin. 2017. Us-\ning deep neural networks to learn syntactic agree-\nment. Linguistic Issues in Language Technology,\n15:1–15.\nThomas G. Bever. 1970. The cognitive basis for lin-\nguistic structures. In J. R. Hayes, editor, Cogni-\ntion and the Development of Language. Wiley, New\nYork.\nKathryn Bock and Carol A Miller. 1991. Broken agree-\nment. Cognitive Psychology, 23(1):45–93.\nPeter F. Brown, Peter V . deSouza, Robert L. Mer-\ncer, Vincent J. Della Pietra, and Jenifer C. Lai.\n1992. Class-based n-gram models of natural lan-\nguage. Comput. Linguist., 18(4):467–479.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN simulations of grammaticality judg-\nments on long-distance dependencies. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 133–144.\nMorten H. Christiansen and Nick Chater. 1999. To-\nward a connectionist model of recursion in hu-\nman linguistic performance. Cognitive Science,\n23(2):157–205.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural net-\nwork grammars. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 199–209.\nJ.L. Elman. 1990. Finding structure in time. Cognitive\nScience, 14(2):179–211.\nEmile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of\nRNNs with multi-task learning. arXiv preprint\narXiv:1706.03542.\nFernanda Ferreira and Charles Clifton. 1986. The inde-\npendence of syntactic processing. Journal of Mem-\nory and Language, 25(3):348–368.\nStefan L. Frank and Rens Bod. 2011. Insensitivity\nof the human sentence-processing system to hierar-\nchical structure. Psychological Science, 22(6):829–\n834.\nYoav Goldberg. 2017. Neural network methods for nat-\nural language processing. Synthesis Lectures on Hu-\nman Language Technologies, 10(1):1–309.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n10–18, Salt Lake City, UT. Association for Compu-\ntational Linguistics.\nK. Gulordava, P. Bojanowski, E. Grave, T. Linzen,\nand M. Baroni. 2018. Colorless green recurrent\nnetworks dream hierarchically. In Proceedings of\nNAACL.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan R Brennan. 2018. Finding syntax in hu-\nman encephalography with beam search. In Pro-\nceedings of the 56th Annual Meeting of the Associ-\nation for Computational Linguistics (Long Papers),\nMelbourne, Australia.\nJohn T. Hale. 2001. A probabilistic Earley parser as a\npsycholinguistic model. In Proceedings of the Sec-\nond Meeting of the North American Chapter of the\nAssociation for Computational Linguistics and Lan-\nguage Technologies, pages 1–8.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. arXiv, 1602.02410.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 1426–1436.\nEllen Lau, Clare Stroud, Silke Plesch, and Colin\nPhillips. 2006. The role of structural prediction in\nrapid syntactic analysis. Brain & Language, 98:74–\n88.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: a probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nRoger Levy, Evelina Fedorenko, Mara Breen, and Ted\nGibson. 2012. The processing of extraposed struc-\ntures in English. Cognition, 122(1):12–36.\nRoger P Levy, Florencia Reali, and Thomas L Grifﬁths.\n2009. Modeling the effects of memory on human\nonline sentence processing with particle ﬁlters. In\nAdvances in neural information processing systems,\npages 937–944.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nMaryellen C. MacDonald and Morten H. Christiansen.\n2002. Reassessing working memory: Comment on\nJust and Carpenter (1992) and Waters and Caplan\n(1996). Psychological Review, 109(1):35–54.\nChristopher D Manning and Bob Carpenter. 2000.\nProbabilistic parsing using left corner language\nmodels. In Advances in probabilistic and other\nparsing technologies, pages 105–124. Springer.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Comput. Lin-\nguist., 19(2):313–330.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202.\nAssociation for Computational Linguistics.\nMichael EJ Masson and Geoffrey R Loftus. 2003. Us-\ning conﬁdence intervals for graphically based data\ninterpretation. Canadian Journal of Experimen-\ntal Psychology/Revue canadienne de psychologie\nexp´erimentale, 57(3):203.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL.\nMarten van Schijndel and Tal Linzen. 2018a. Model-\ning garden path effects without explicit hierarchical\nsyntax. In Proceedings of the 40th Annual Meeting\nof the Cognitive Science Society.\nMarten van Schijndel and Tal Linzen. 2018b. A neural\nmodel of adaptation in reading. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing.\nNathaniel J. Smith and Roger Levy. 2013. The effect of\nword predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nAdrian Staub. 2007. The parser doesn’t ignore intran-\nsitivity, after all. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 33(3):550.\nAdrian Staub and Charles Clifton. 2006. Syntactic pre-\ndiction in language comprehension: Evidence from\neither . . . or. Journal of Experimental Psychology:\nLearning, Memory, & Cognition, 32(2):425–436.\nMitchell Stern, Daniel Fried, and Dan Klein. 2017.\nEffective inference for generative neural parsing.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1695–1700. Association for Computational Linguis-\ntics.\nAndreas Stolcke. 1995. An efﬁcient probabilis-\ntic context-free parsing algorithm that computes\npreﬁx probabilities. Computational Linguistics,\n21(2):165–201.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Process-\ning Systems, pages 3104–3112.\nWhitney Tabor and Sean Hutchins. 2004. Evidence for\nself-organized sentence processing: Digging-in ef-\nfects. Journal of Experimental Psychology: Learn-\ning, Memory, and Cognition, 30(2):431.\nJohn C. Trueswell, Michael K. Tanenhaus, and S. Gar-\nnsey. 1994. Semantic inﬂuences on parsing: Use of\nthematic role information in syntactic ambiguity res-\nolution. Journal of Memory and Language, 33:285–\n318.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do rnn language mod-\nels learn about ﬁller–gap dependencies? In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 211–221. Association for Computa-\ntional Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7813594341278076
    },
    {
      "name": "Parsing",
      "score": 0.7415857315063477
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.7314229011535645
    },
    {
      "name": "Natural language processing",
      "score": 0.6748936772346497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6726427674293518
    },
    {
      "name": "Language model",
      "score": 0.5213229060173035
    },
    {
      "name": "Artificial neural network",
      "score": 0.4819464683532715
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 11
}