{
    "title": "PointDifformer: Robust Point Cloud Registration With Neural Diffusion and Transformer",
    "url": "https://openalex.org/W4390659820",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2144453831",
            "name": "She, Rui",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A4281116138",
            "name": "Kang, Qiyu",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2106601115",
            "name": "Wang, Sijie",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2748221615",
            "name": "Tay, Wee Peng",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A1974311045",
            "name": "Zhao Kai",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2099417549",
            "name": "Song Yang",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2593989766",
            "name": "Geng Tianyu",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2072371361",
            "name": "Xu Yi",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A4281116141",
            "name": "Navarro, Diego Navarro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281116142",
            "name": "Hartmannsgruber, Andreas",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3198960475",
        "https://openalex.org/W3014916165",
        "https://openalex.org/W4308080242",
        "https://openalex.org/W4207046160",
        "https://openalex.org/W3167842529",
        "https://openalex.org/W3034681945",
        "https://openalex.org/W3129245057",
        "https://openalex.org/W2296228853",
        "https://openalex.org/W2986382673",
        "https://openalex.org/W2049981393",
        "https://openalex.org/W2097832352",
        "https://openalex.org/W3007240855",
        "https://openalex.org/W3131423830",
        "https://openalex.org/W4296705184",
        "https://openalex.org/W4290972954",
        "https://openalex.org/W3185902918",
        "https://openalex.org/W2981995220",
        "https://openalex.org/W3034675048",
        "https://openalex.org/W3167684723",
        "https://openalex.org/W6802758728",
        "https://openalex.org/W3118263378",
        "https://openalex.org/W3032375908",
        "https://openalex.org/W2995412790",
        "https://openalex.org/W2915505642",
        "https://openalex.org/W4386590342",
        "https://openalex.org/W6842786429",
        "https://openalex.org/W2100657858",
        "https://openalex.org/W2085261163",
        "https://openalex.org/W2472414548",
        "https://openalex.org/W2568355838",
        "https://openalex.org/W4249866455",
        "https://openalex.org/W2889300857",
        "https://openalex.org/W2962941647",
        "https://openalex.org/W2964014140",
        "https://openalex.org/W3177280664",
        "https://openalex.org/W4312773339",
        "https://openalex.org/W4386065759",
        "https://openalex.org/W2955544543",
        "https://openalex.org/W2944796900",
        "https://openalex.org/W2963264709",
        "https://openalex.org/W6739778489",
        "https://openalex.org/W4226373316",
        "https://openalex.org/W3035030518",
        "https://openalex.org/W4304092564",
        "https://openalex.org/W4310460423",
        "https://openalex.org/W4280630571",
        "https://openalex.org/W4312867066",
        "https://openalex.org/W4214688548",
        "https://openalex.org/W3035490780",
        "https://openalex.org/W3166975282",
        "https://openalex.org/W3176124701",
        "https://openalex.org/W4289536297",
        "https://openalex.org/W3216353509",
        "https://openalex.org/W4386076224",
        "https://openalex.org/W4386065804",
        "https://openalex.org/W4321020868",
        "https://openalex.org/W4386076591",
        "https://openalex.org/W4386066301",
        "https://openalex.org/W2963727135",
        "https://openalex.org/W2967324759",
        "https://openalex.org/W2981548405",
        "https://openalex.org/W2930709109",
        "https://openalex.org/W1644641054",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2979750740",
        "https://openalex.org/W2986519585",
        "https://openalex.org/W2990613095",
        "https://openalex.org/W3034459762",
        "https://openalex.org/W3162959074",
        "https://openalex.org/W3111535274",
        "https://openalex.org/W6810249204",
        "https://openalex.org/W6839446344",
        "https://openalex.org/W6752307458",
        "https://openalex.org/W6796489620",
        "https://openalex.org/W2057630839",
        "https://openalex.org/W6802433612",
        "https://openalex.org/W4382239736",
        "https://openalex.org/W6803640674",
        "https://openalex.org/W2954742107",
        "https://openalex.org/W6857932679",
        "https://openalex.org/W4387197109",
        "https://openalex.org/W2966331853",
        "https://openalex.org/W2007206727",
        "https://openalex.org/W1965805571",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2605111497",
        "https://openalex.org/W4327692895",
        "https://openalex.org/W4385453072",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2892880750",
        "https://openalex.org/W6755843862",
        "https://openalex.org/W2949676527",
        "https://openalex.org/W6853011858",
        "https://openalex.org/W4221160819",
        "https://openalex.org/W4387561063",
        "https://openalex.org/W3104408604",
        "https://openalex.org/W4377191398",
        "https://openalex.org/W3153465022"
    ],
    "abstract": "Point cloud registration is a fundamental technique in 3-D computer vision with applications in graphics, autonomous driving, and robotics. However, registration tasks under challenging conditions, under which noise or perturbations are prevalent, can be difficult. We propose a robust point cloud registration approach that leverages graph neural partial differential equations (PDEs) and heat kernel signatures. Our method first uses graph neural PDE modules to extract high-dimensional features from point clouds by aggregating information from the 3-D point neighborhood, thereby enhancing the robustness of the feature representations. Then, we incorporate heat kernel signatures into an attention mechanism to efficiently obtain corresponding keypoints. Finally, a singular value decomposition (SVD) module with learnable weights is used to predict the transformation between two point clouds. Empirical experiments on a 3-D point cloud dataset demonstrate that our approach not only achieves state-of-the-art performance for point cloud registration but also exhibits better robustness to additive noise or 3-D shape perturbations.",
    "full_text": "1\nPointDifformer: Robust Point Cloud Registration\nwith Neural Diffusion and Transformer\nRui She∗, Qiyu Kang ∗, Sijie Wang∗, Wee Peng Tay, Senior Member, IEEE, Kai Zhao, Yang Song,\nTianyu Geng, Yi Xu, Diego Navarro Navarro and Andreas Hartmannsgruber\nAbstract—Point cloud registration is a fundamental technique\nin 3D computer vision with applications in graphics, autonomous\ndriving, and robotics. However, registration tasks under challeng-\ning conditions, under which noise or perturbations are prevalent,\ncan be difficult. We propose a robust point cloud registration\napproach that leverages graph neural Partial Differential Equa-\ntions (PDEs) and heat kernel signatures. Our method first uses\ngraph neural PDE modules to extract high-dimensional features\nfrom point clouds by aggregating information from the 3D point\nneighborhood, thereby enhancing the robustness of the feature\nrepresentations. Then, we incorporate heat kernel signatures\ninto an attention mechanism to efficiently obtain corresponding\nkeypoints. Finally, a Singular Value Decomposition module with\nlearnable weights is used to predict the transformation between\ntwo point clouds. Empirical experiments on a 3D point cloud\ndataset demonstrate that our approach not only does achieve\nstate-of-the-art performance for point cloud registration but\nalso exhibits better robustness to additive noise or 3D shape\nperturbations.\nIndex Terms—Point cloud registration, neural diffusion, graph\nneural network, heat kernel signature.\nI. I NTRODUCTION\nI\nN the era of intelligent and smart perception, 3D computer\nvision techniques are increasingly being used in various\nfields, such as autonomous driving, robotics, and scene mod-\neling [1]–[3]. Point cloud registration is a crucial task in 3D\ncomputer vision and has become an important tool in many\napplications, including object detection, odometry estimation,\nand SLAM [4]–[8], owing to its robustness against seasonal\nchanges and illumination variations. Point cloud registration\naims to estimate the transformation or relative pose between\ntwo given 3D point cloud frames [9].\nIterative algorithms are widely used for point cloud registra-\ntion [10]–[12]. The Iterative Closest Point (ICP) algorithm is\na well-known iterative method for point cloud registration that\nmatches the closest points between two point clouds, iteratively\nupdating the transformation matrix until convergence [10]. ICP\nhas been successfully used in numerous fields, including robotic\nperception and autonomous driving.\nDespite their usefulness, iterative algorithms face challenges\nthat limit their effectiveness in certain scenarios. The non-\nconvexity of the optimization problem presents a significant\n∗R. She, Q. Kang and S. Wang, contributed equally to this work.\nR. She, Q. Kang, S. Wang, W. P. Tay, K. Zhao, Y . Song, T. Geng, and\nY . Xu are with Nanyang Technological University, Singapore. {rui.she@;\nqiyu.kang@; wang1679@e.; kai.zhao@; songy@; tianyu.geng@; yi.xu@;\nwptay@}ntu.edu.sg\nD. N. Navarro and A. Hartmannsgruber are with Continental Au-\ntomotive Singapore Pte. Ltd., Singapore. {diego.navarro.navarro; an-\ndreas.hartmannsgruber}@continental.com\nchallenge, making it difficult to obtain the global optimum\n[9]. As a result, iterative algorithms may converge to sub-\noptimal solutions, especially in complex and non-rigid scenes.\nAdditionally, the performance of iterative algorithms heavily\nrelies on the initialization of the algorithm, which can be\ntime-consuming and computationally expensive. Sparse and\nnon-uniform point clouds present another significant challenge\nfor iterative algorithms in finding corresponding point pairs\nbetween two point clouds. Traditional approaches, such as\nnearest-neighbor search, may fail to find matching pairs in\nsuch cases, leading to errors in the registration result [9], [13].\nTo address these challenges, deep learning-based methods [9],\n[13]–[15] have been developed for predicting transformation\nmatrices or relative poses. These methods are designed for\nvarious scenarios, including indoor and outdoor environments\n[16]–[20]. However, robust point cloud registration remains\na challenging problem due to factors such as LiDAR scan\ndistortion, dynamic objects, and environmental noise [21]–[25].\nEfficiently estimating the transformation under scenarios with\nadditive noise and perturbations remains an open problem.\nIn this paper, we propose a model for point cloud registra-\ntion that utilizes a robust feature descriptor based on graph\nneural diffusion. We also present an end-to-end transformation\nestimation method by introducing the heat kernel signature into\nthe attention module, without any prior prediction information.\nOur approach attempts to address the challenges faced by\niterative algorithms, leading to robust and efficient point\ncloud registration. Our proposed approach is motivated by\nthe following:\n• Graph neural PDE learning has demonstrated robustness\nfor representing graph-structured data, as highlighted in\n[26]. Our aim is to leverage this module for effective\npoint cloud representation by constructing a neighborhood\ngraph in the feature space.\n• We believe that the shape isometry-invariance of the heat\nkernel signature, as described in [27], makes it beneficial\nto incorporate into attention mechanisms for improved\nrobustness from a shape-preserving perspective.\nOur main contributions are as follows:\n• We design a 3D point cloud representation module based\non graph neural PDE learning.\n• We propose a robust 3D point cloud registration method\nusing the graph neural diffusion modules and the attention\nmechanism with a heat kernel signature.\n• We empirically demonstrate that our point cloud registra-\ntion method outperforms other baselines not only under\narXiv:2404.14034v1  [cs.CV]  22 Apr 2024\n2\nnormal scenarios but also when noise and perturbations\nare present.\nThe rest of this paper is organized as follows. In Section II,\nwe discuss the related works. In Section III, we describe\nour proposed model in detail. In Section IV, we present the\nexperimental results to evaluate our model and compare it with\nseveral baselines. Finally, we conclude the paper in Section V.\nII. R ELATED WORK\nIn this section, we summarize relevant literature in the areas\nof point cloud registration, point cloud feature representation,\nand neural diffusion, including works utilizing the heat kernel\nsignature in point cloud feature descriptors.\nA. Point Cloud Registration Methods\nIteration-based Methods. Iteration-based methods, such\nas the Iterative Closest Point (ICP) [10] and the RANdom\nSAmple Consensus (RANSAC) [28], are classical approaches\ncommonly used for point cloud registration. However, due to\nits slow convergence rate, RANSAC requires high computing\nresources and has a high running time complexity. The\nperformance of ICP heavily relies on the accuracy of the initial\nvalue estimation, making it prone to suboptimal solutions. To\naddress these challenges, several refinement methods for ICP\nhave been proposed, such as the Branch-and-Bound (BnB)\nmethod [11], convex relaxation improvement [29], and mixed-\ninteger programming [30]. However, these methods may be\ncomputationally expensive and do not ensure global optimality.\nAlternatively, updated ICP methods such as V oxelized ICP [12]\nand Generalized-ICP [31] have been developed to improve both\nacceleration and accuracy.\nCorrespondence-based estimators. Correspondence-based\nestimators are commonly used for point cloud registration,\nwhich involves estimating the transformation between two\nframes [17], [32]–[34]. This approach obtains correspondences\nbetween two point clouds and then uses pose estimators such\nas RANSAC [18], [28], [35], Singular Value Decomposition\n(SVD) [9], [10], [13], [36] and Maximal Cliques (MAC) [37]\nto predict the transformation. There are generally two types\nof correspondence-based estimators. One involves repeatable\nkeypoint detection [18], [35], [38], [39], followed by using\nlearned or handcrafted keypoint descriptors for correspondence\nacquisition [17], [19], [40] or similarity measures to obtain\nthe correspondences [2], [36]. For example, DeepVCP [39]\nuses PointNet++ [41] to extract features for the point clouds\nand learns keypoint correspondences based on matching\nprobabilities among candidates. D3Feat [18] employs 3D\nfully convolutional networks to output detection scores and\ndescriptive features for 3D points. PREDATOR [35] uses an\noverlap-attention block for cross-information between two point\nclouds and makes good use of their overlap region to achieve\nregistration. The other [9], [20] involves correspondence\nretrieval for all possible matching point pairs without keypoint\ndetection. For instance, Deep Closest Point (DCP) [9] aligns\nfeatures based on the interaction of the point clouds. CoFiNet\n[20] achieves hierarchical correspondences with coarse and\nfiner scales, without keypoint detection. In both types of\nestimators, point cloud descriptors play significant roles, mainly\ncontributing to the robustness and accuracy of the entire\npipeline.\nLearning-based estimators. In order to achieve more\nrobust non-handcrafted estimators, learning-based methods\nare introduced into the transformation prediction [42]. Since\nconventional estimators like RANSAC have drawbacks in\nterms of convergence speed and are unstable in the presence\nof numerous outliers, learning-based estimators [16], [43]–\n[49], such as StickyPillars [50], PointDSC [51], EDFNet\n[52], GeoTransformer [42], Lepard [53], RoITr [54], BUFFER\n[55] and RoReg [56], have attracted much interest. Moreover,\nauxiliary modules or prior information can be incorporated\ninto learning-based estimators, such as Prior-embedded Explicit\nAttention Learning (PEAL) [57] and VBReg [58]. Some of\nthese classification neural networks can filter out extreme\noutliers and some estimation neural networks are designed to\noutput the transformation. From the perspective of accuracy and\nrunning efficiency, they perform better than those conventional\nrobust estimators. While, they need extra neural network\ntraining, which holds more time and space complexity. In\ncontrast, our model achieves robust and accurate registration\nwithout the need for training the estimation networks to\ncompute the final transformation in the output.\nB. Point Cloud Feature Representation\nTo extract more efficient features for point clouds, methods\nusing different neural networks are studied. In general, we can\nclassify point cloud feature representation methods into three\ncategories as follows.\nThe first category performs voxel alignment on the points\nand then obtains the corresponding features based on a 3D\nConvolutional Neural Network (CNN) [59]–[62]. In this regard,\nthe full information in the point cloud is used to learn the\nrepresentation. However, it takes more computational resources\nto deal with a sparse and irregular point cloud when using\nclosely spaced 3D voxels for more precise quantization.\nThe second category reduces a 3D point cloud to a 2D map\nand then exploits the classical 2D CNN to extract features [63].\nThe commonly used 2D maps are the bird’s-eye view map,\ncylindrical map, spherical map, and camera-plane map, for\nwhich computational cost is incurred during the preprocessing\nstage. Due to quantization errors, this approach can also\nintroduce unexpected noise.\nThe third category is to extract features from the raw point\nclouds directly using specific neural networks. PointNet [64]\nand PointNet++ [41] extract local point features independently\nand obtain global features through max-pooling. To incorporate\nlocal neighborhood information, Dynamic Graph Convolutional\nNeural Networks (DGCNN) [65] uses a dynamic graph network,\nand LPDNet [66] jointly exploits the geometry space and\nfeature space. KPConv [67] uses kernel points to achieve more\nflexible convolutions compared with fixed grid convolutions.\nPointGLR [68] considers not only local features but also global\npatterns in point clouds. DIP [69] and GeDi [46] extract\nlocal point cloud patches based on rotation-invariant compact\ndescriptors, which can be used in different data domains. Point\n3\nCloud Transformer (PCT) [70] utilizes the Transformer to\ngenerate permutation-invariant descriptors for point clouds.\nMoreover, there are also other learning-based point cloud\nrepresentation methods, such as PointMLP [71], and PointNeXt\n[72].\nC. Neural Diffusion\nNeural diffusion methods [73], [74] combine neural networks\nwith ordinary and partial differential equations. For a neural\nOrdinary Differential Equation (ODE) layer [73] with input\nZ(0) and output Z(T), the relationship between Z(0) and Z(T)\nis given by\ndZ(t)\ndt = fODE(Z(t), t), (1)\nwhere fODE : Rn × [0, T) → Rn is a learnable layer and\nZ : [0, T) → Rn denotes the state of the neural ODE. At the\nterminal time T ∈ [0, ∞), the output Z(T) is given by\nZ(T) = Z(0) +\nZ T\n0\nfODE(Z(t), t)dt. (2)\nIn this paper, we consider only the time-invariant (autonomous)\ncase, i.e., fODE(Z(t), t) = fODE(Z(t)).\nFor graph-structured data, graph neural PDEs are designed\nbased on continuous flows, which represent the graph features\nmore concisely and stably [26], [74]–[80]. Neural ODEs/PDEs\nare more robust in defending perturbations and even attacks,\ncompared with other deep neural networks without neural diffu-\nsion (cf. [26], [81]). Compared with conventional graph neural\nnetworks (GNNs), including the Graph ATtention network\n(GAT) or the Graph Convolutional Network (GCN), graph\nneural PDEs have superior performance in some applications\nsuch as the node classification for graph-structured data. To\napproximately solve the graph neural PDEs [74], the neural\nODE solvers proposed in [73] can be exploited.\nD. Heat Kernel Signature\nBased on the heat diffusion process, the heat kernel signature\n[27] is presented as an intrinsic feature and is given by\nh(x, t) =\n∞X\ni=0\nexp (−λit)ϕ2\ni (x), (3)\nwhere x is a 3D point in a point cloud, λis denote eigenvalues\nand ϕis are the corresponding eigenfunctions of the Laplace-\nBeltrami operator. The feature h(x, t) is a robust local geo-\nmetric descriptor containing large-scale information [82], [83].\nFrom the physics perspective, this feature descriptor represents\nthe temperature evolution of a point at which a heat source\nis placed and removed immediately. The heat diffuses to the\nneighborhood of the point [27], [84]. This evolution is based\non the temperature diffusion speed, which essentially depends\non the geometry of the objects projected by the point clouds.\nFrom a geometric perspective, the heat kernel signature is\nisometry-invariant, meaning that two isometric shapes have\nequivalent heat kernel signatures. If the heat kernel signatures\nof two shapes are equal, the corresponding shapes or parts of\nthe shapes are similar under isometric transformations [27].\nTherefore, this feature is somewhat robust, making it a desirable\nmethod for point description.\nIII. P OINT CLOUD REGISTRATION METHOD BASED ON\nGRAPH NEURAL PDE\nIn this section, we present our registration model that aims\nto predict the transformation between two 3D point clouds.\nHowever, point clouds may contain noise or perturbations that\ncan compromise the robustness of the transformation prediction.\nTherefore, our goal is to develop a more robust method for\nthe registration task.\nOur model is called the Point Cloud Diffusion Transformer\n(PointDifformer). First, we provide an overview of the PointD-\nifformer framework, which is illustrated in Fig. 1. Then, we\npresent the details of the modules and the loss function used\nin PointDifformer.\nA. Overview of PointDifformer\nBefore introducing in detail the modules of PointDifformer,\nwe provide an overview of its pipeline as follows.\n1) Within a 3D point cloud frame, the neighborhood graph\nof each point consisting of its K nearest neighbors is\nconstructed. Points are regarded as the vertices in the\ngraph. The L2 distance between point features is used for\nneighbor acquisition. The initial features of the points are\ntaken to be their 3D coordinates. The neighborhood graph\nfor each point is an undirected complete graph. Then,\ngraph neural PDE layers are applied to the neighborhood\ngraph of each point to obtain a robust representation of\nthe point.\n2) Based on the robust feature representations, a self-cross\nattention module is applied to obtain an embedding\ncontaining point-level information interaction within a\npoint cloud frame and between two point cloud frames.\nThe heat kernel signature, as a robust feature descriptor\nfor point clouds, is introduced into the attention module\nas the weights.\n3) Using the above embeddings, an attention module is\nestablished to learn weights for points from different\nframes that indicate their correspondences.\n4) Through the correspondence among points in the two\npoint-cloud frames, the optimal transformation (including\nthe translation and the rotation) can be estimated using\noptimization solution methods like the weighted SVD.\nB. Model Details\n1) Point Cloud Representation with Neural Diffusion: To\nrepresent point cloud features efficiently and robustly, we design\na neural diffusion network for point cloud representation, called\nPoint-Diffusion Net. This module consists of GNN modules\nand graph neural PDE modules with different rewiring and is\nshown in Fig. 2. Its details are described as follows.\n3D Points Refinement. To pre-process the point cloud,\nwhich has potential outliers, we first use a graph neural PDE\nmodule as a learning-based filter. Consider a point cloud\ndenoted by X ∈ RN×3. The 3-dimensional coordinates of\neach point are regarded as its feature map and N is the number\nof points in the point cloud. We construct the neighborhood\ngraphs for the points by means of the K-Nearest Neighbors\n4\nPoint Cloud X\nPoint Cloud Y\nPoint-\nDiffusion\nNet\nSelf-Cross Attention \nEmbedding\nSelf-Cross Attention \nEmbedding\nAttention-Based  Point \nCorrespondence\n\nWeighted SVD\nModule\nTransformation\nEstimation\nHeat Kernel Signature\nHeat Kernel Signature\nCorresponding\nKeypoints\nFX\nFY\nFX + Fsc_XY\nX\nFsc_YX\nY\nFsc_XY\nX\nFY+Fsc_YX\nY\nFX\nFY\n\n\n\n\nHX\nHY\n\n\nSelf Attention with Heat Kernel Signature\nCross Attention \nPoint-\nDiffusion\nNet\nSelf-Cross Attention Embedding\nHX\nHY\nFY+Fsc_YX\nY\nFX + Fsc_XY\nX\nSoftmax\nAttention Weight Matrix\nTop-K′\nAttention-Based  Point Correspondence\n\n1 N\nN\n1 K′\nK′\n\n\nXi\nYji∗ Xi , YiX\nFig. 1. PointDifformer for point cloud registration. The details of the modules are provided in Section III-B.\nGNN\nGraph Neural PDE\nFGNN_P\nGNN\nFGNN_G\nGNN\nGraph Neural PDE\nFGNN_F\nConcatenate\nFeatures\nRewiring for GNN Rewiring for GNN\nFig. 2. The Point-Diffusion Net based on the graph neural PDEs for the point\ncloud representation. The details are provided in Section III-B1.\n(K-NN) method using the Euclidean distance. Furthermore,\nwe exploit a graph neural PDE module for feature updating,\ngiven by\ndZp(t)\ndt = fGNN_P(Zp(t)), (4)\nwhere fGNN_P(·) denotes a graph learning module and Zp(t) is\nthe state at time t. The initial state is given by Zp(0) = X. By\nintegrating fGNN_P(·) from t = 0 to t = Tp (using differential\nequation solvers [85]), we obtain the solution of (4) at time\nTp, given by\nZp(Tp) = FGNN_P(Zp(0)) = FGNN_P(X) ∈ RN×3, (5)\nwhere FGNN_P(·) can be regarded as the embedding function\nfor the input Zp(0). In addition, the output of this graph neural\nmodule also includes 3-dimensional features, which can also\nbe viewed as “generated” points.\nHigh Dimensional Feature Extraction with Graph Neural\nPDE. We extract high dimensional features for the preprocessed\n3D points through a GNN module, e.g., DGCNN [65]. In this\nregard, 3-dimensional coordinates of points are extended into\nd-dimensional features. We construct a neighborhood graph\nfor each point using the K-NN method. The output from the\nGNN module is denoted by\nFG(X) = FGNN_G ◦ FGNN_P(X)\n= FGNN_G(Zp(Tp)) ∈ RN×d, (6)\nwhere ◦ denotes function composition and FGNN_G(·) denotes\nthe mapping of the GNN module. Then, we apply another\ngraph neural PDE module to update the feature FG(X), which\nis described as\ndZf (t)\ndt = fGNN_F(Zf (t)), (7)\nwhere fGNN_F(·) is also a graph learning module that deals\nwith the neighborhood graph of input features. The initial state\nis given by Zf (0) = FG(X). The equation (7) is solved in the\nsame way as that for (4). The output at time Tf is given by\nZf (Tf ) = FGNN_F(Zf (0)) = FGNN_F ◦ FG(X), (8)\nwhere Zf (Tf ) ∈ RN×d, FGNN_F(·) can be regarded as the\nembedding function for the input Zf (0).\nFinally, we concatenate the output from the GNN module\nand the graph neural diffusion module as the eventual output\nfor the point cloud representation module, which is denoted by\nFX = FG(X) ∥ FGNN_F ◦ FG(X), (9)\n5\nwhere FX ∈ RN×2d and ∥ denotes the concatenation op-\neration. Here, we follow the same approach as [13], [65]\nin concatenating the feature Zf (Tf ) = FGNN_F ◦ FG(X)\nwith its corresponding hidden feature FG(X) to retain more\ninformation. Our numerical experiments indicate that this is a\nbetter approach than using only Zf (Tf ).\n2) Self-Cross Attention Embedding Based on Heat Kernel\nSignature: Based on the previous high-dimensional feature,\na self-cross attention mechanism is introduced to reinforce\nthe static structure information in each point cloud and the\ninteractive corresponding information between a pair of point\nclouds, respectively. To improve the robustness of the feature\nextraction, we also introduce the heat kernel signature [27]\ninto the attention mechanism as additive weights.\nFor a pair of point clouds (X, Y), the corresponding input\nfeature pair for the self-cross attention module is represented\nas (FX, FY) using the embedding from (9). The embedding\nfrom the self-cross attention module with respect to (w.r.t.) X\nis given by\nFX\nsc_att(FX, FY) = Fs_att(FX) + FX\nc_att(FX, FY), (10)\nwhere Fs_att(FX) and FX\nc_att(FX, FY) are the features based\non the self-attention and the cross-attention, respectively. The\ndetails of the features Fs_att(FX) and FX\nc_att(FX, FY) are\ndescribed as follows.\nSelf-attention Feature. To improve the robustness, the heat\nkernel signature is introduced into the self-attention module.\nFor a point cloud pair (X, Y), the corresponding heat kernel\nsignature pair is denoted by (HX, HY). Using the normalized\nFX and HX as the inputs for the self-attention module, we\nhave\nFs_att(FX)\n= WS\nShead\n∥\ni=1\nn\nFsoftmax\n\u0010(WSQ\ni FX)(WSK\ni FX)⊺\np\ndS\ni\n+ (WHQ\ni HX)(WHK\ni HX)⊺\np\ndH\ni\n\u0011\n(WSV\ni FX)\no\n+ FX, (11)\nwhere (·)⊺ denotes the transpose operation, Shead is the number\nof multi-heads for the attention, WSQ\ni , WSK\ni , WSV\ni , WHQ\ni ,\nWHK\ni , and WS are learnable layers, and dS\ni and dH\ni are\nthe dimensions for the point cloud features and heat kernel\nsignatures in i-th attention head, respectively. The function\nFsoftmax(·) denotes row-wise softmax.\nThe heat kernel signature is implemented as follows.\n• Heat kernel signature acquisition. We compute this feature\nbased on the point cloud using the formula (3). Since\nthe function h(x, t) in (3) is a robust local geometric\ndescriptor containing large-scale information [82], we use\nit to robustly describe the repeatable features for point\nclouds.\n• Embedding. We process the heat kernel signatures using\nthe graph neural PDE module and the Fully Connected\n(FC) layer to obtain the embedding. Similar to (4), the\ngraph neural PDE is used as a filter for the heat kernel\nsignatures. The graph construction in the graph neural\nPDE is based on the K-NN for the heat kernel signatures.\n• Self-attention weights. After embedding the heat kernel\nsignatures, they are input into the self-attention module\nas the additive weights. By introducing extra information\nfrom the heat kernel signature, the robustness of the\npoint cloud representation is enhanced in the self-attention\nmodule.\nCross-attention Feature. Based on the self-attention feature\nFs_att(FY) for the point cloud Y, we acquire the cross-\nattention features for FX. Fs_att(FY) is input into a Feed\nForward Network (FFN) [86] to obtain the feature\nFs_att_n(FY) = FFNN(Fs_att(FY)) + Fs_att(FY), (12)\nwhere FFNN denotes the FFN consisting of two linear layers\nwith normalization operation and the Rectified linear activation\nfunction (ReLU). Inputting normalized Fs_att_n(FY) and\nFs_att(Fx) into the cross-attention module, we have\nFX\nc_att(FX, FY) = WC×\nChead\n∥\ni=1\nn\nFsoftmax\n\u0010(WCQ\ni Fs_att(FX))(WCK\ni Fs_att_n(FY))⊺\np\ndC\ni\n\u0011\n× (WCV\ni Fs_att_n(FY))\no\n, (13)\nwhere WCQ\ni , WCK\ni , WCV\ni , and WC are learnable layers, dC\ni is\nthe feature dimension for the i-th attention head. The remaining\nnotations are similar to those in (11).\nThe joint feature FX\nsc_att(FX, FY) based on the self-cross\nattention module is obtained as mentioned in (10). Inputting\nFX\nsc_att(FX, FY) into the FNN, we have the embedding from\nthe self-cross attention module as\nFX\nsc_XY = FFNN(FX\nsc_att(FX, FY)) + FX\nsc_att(FX, FY),\n(14)\nwhose normalization is regarded as the final output of this\nmodule.\nSimilarly, the above self-cross attention module is also\navailable for the point cloud Y to obtain its output FY\nsc_YX.\nThe architectures of the self-attention and the cross-attention\nare shown in Fig. 3.\n3) Attention-Based Keypoint Correspondence: Using the\nattention mechanism, the information of point cloud X can\nbe involved in the embedding for the point cloud Y. By\nresorting to the self-cross attention embeddings, we can obtain\nthe weighted Y denoted by YX which is regarded as the\ntransformed point cloud corresponding to the X. The details\nare given as follows.\ni) We compute the attention weight matrix\nWatt = Fsoftmax\n \n(FX + FX\nsc_XY)(FY + FY\nsc_YX)⊺\n√\ndatt\n!\n,\n(15)\nwhere the feature dimension datt = 2d and Fsoftmax denotes\nthe row-wise softmax function.\nii) For each point xi in the point cloud X, we select its\ncorresponding point yj∗\ni in the point cloud Y, which has the\nhighest similarity with the xi. Furthermore, based on the Top-\nK′ similarity scores, we select the corresponding point pairs\n6\nWi\nHQ WiHK Wi\nSQ WiSK Wi\nSV\nSoftmax\nMulti-Head \nAggregation WS\n∶N×diH ∶N×di\nS ∶N×di\nS\n∶N×di\nS\nHi\nQ HiK FiKFi\nQ Fi\nV\nHi\nQ HiK T\n∶N×N Fi\nQ FiK T\n∶N×N\n∶N×NWi\nFH\n∶N×NWi\nX\n∶N×diSFi\nXW\n∶N×diH\nFXHX\nSelf Attention \nwith Heat Kernel Signature\nWi\nCQ WiCK Wi\nCV\nSoftmax\nMulti-Head \nAggregation WC\n∶N×di\nC\n∶N×di\nC\n∶N×di\nC\nFiYKFi\nXQ Fi\nYV\nFi\nXQ FiYK T\n∶N×N\n∶N×NWi\nXY\n∶N×diCFi\nXYW\nFs_att_n FY\nCross Attention \nFs_att FX\nOutput Output\nFig. 3. The modules of the self-attention with heat kernel signature and the cross-attention.\n\b\n(xi, yj∗\ni ) : i = 1, 2, ..., K′\t\n. Specifically, for the i-th row of\nWatt, we have\nj∗\ni = max\nj\nwatt\ni,j , (16)\nwhere watt\ni,j (i, j∈ {1, 2, ..., N}) denotes the element in the\ni-th row and j-th column among the Watt. Then, based on\nthe watt\ni,j∗\ni\n(i ∈ {1, 2, ..., N}), we select the Top- K′ point pairs\b\n(xi, yj∗\ni ) : i = 1, 2, ..., K′\t\nto obtain the updated point cloud\npairs (X, Y), where abusing notations X and Y are used.\niii) Based on the updated point clouds X and Y consisting\nof the Top- K′ points, we have the corresponding updated\nattention weight matrix similar to (15), which is denoted by\nan abusing notation Watt. Then, we have the weighted Y as\nYX = WattY, (17)\nwhose the point number is also K′ the same as that in the X.\nIn general, the points xi and yx\ni (i = 1, 2, ..., K′) from the\npoint clouds X and YX are treated as the correspondence\npoints.\n4) Transformation Prediction: By resorting to the correspon-\ndence of points, we can predict the transformation or relative\npose between two point clouds. Consider the Mean Squared\nError (MSE) given by\nℓMSE( ˆR,ˆt) = 1\nK′\nK′\nX\ni=1\n∥ ˆRxi + ˆt − yx\ni ∥2, (18)\nin which ∥·∥2 denotes the L2 norm, xi = [x(1)\ni , x(2)\ni , x(3)\ni ]⊺ and\nyx\ni = [yx\ni\n(1), yx\ni\n(2), yx\ni\n(3)]⊺ where x(l)\ni and yx\ni\n(l) (l ∈ {1, 2, 3})\nare elements from xi and yx\ni , respectively. The ˆR ∈ R3×3\nand ˆt ∈ R3×1 are the predicted results w.r.t. the ground-truth\nrotation R ∈ R3×3 and translation t ∈ R3×1. The optimal\nresults of ˆR and ˆt are given by\nˆR∗,ˆt∗ = arg min\nˆR,ˆt\nℓMSE( ˆR,ˆt). (19)\nThen, we use the weighted SVD [10], [42] to solve the\noptimization problem in (19). Specifically, the weighted mean\nof the {xi : i = 1, 2, ..., K′} and the {yx\ni : i = 1, 2, ..., K′} are\nfirst computed as\nxw = 1\nK′\nK′\nX\ni=1\nwx\ni xi, (20)\nyx\nw = 1\nK′\nK′\nX\ni=1\nwy\ni yx\ni, (21)\nwhere wx\ni ∈ R3×1 and wy\ni ∈ R3×1 are trainable weights.\nFurthermore, the weighted cross-covariance matrix M is\ngiven by\nM =\nK′\nX\ni=1\n(xi − xw)(wM\ni (yx\ni − yx\nw))⊺, (22)\nwhere (·)⊺ denotes the transpose operation, wM\ni ∈ R3×1 is a\ntrainable weight.\nSimilar to the procedure of SVD mentioned in [9], [13], the\nmatrix M can be decomposed as\nM = UΛV⊺, (23)\nwhere U and V are unitary matrices and Λ is a rectangular\ndiagonal matrix with non-negative real diagonal elements.\nFurthermore, the transformation prediction (including the\npredicted rotation ˆR∗ and the translation ˆt∗) can be obtained\nas\nˆR∗ = VU⊺, (24)\nˆt∗ = − ˆR∗xw + yx\nw. (25)\n5) Loss function: As the point yx\ni corresponds to the point\nxi, we use the corresponding point loss given by\nLpoint = 1\nK′\nK′\nX\ni=1\n∥ ˆR∗xi + ˆt∗ − yx\ni ∥2. (26)\nTo quantify the deviation between the ground truth and the\npredicted results w.r.t. rotation and translation, we use the loss\ngiven by\nLrt = exp (−γt)∥ˆt∗ − t∥2 + γt\n+ exp (−γr)∥R⊺ ˆR∗ − I∥2 + γr, (27)\n7\nwhere I denotes the identity matrix, and γt and γr are learnable\nparameters. The learnable weights based on γt and γr are\ninpsired by the loss in [77], [87]. The total loss combining\nLpoint and Lrt is given by\nLtotal = exp (−ηp)Lpoint + ηp + exp (−ηrt)Lrt + ηrt, (28)\nwhere ηp and ηrt are learnable parameters.\nIV. E XPERIMENTS\nA. Dataset preparation\nvReLoc Dataset. The vReLoc Dataset is a publicly available\nindoor dataset,1 containing LiDAR point clouds and camera\nimages. In this paper, we randomly generate a transformation\nmatrix for each point cloud to obtain a pair of point cloud\nframes. The transformation matrix is based on translation\nalong the x, y, and z axes, as well as rotation along the roll,\npitch, and yaw axes. The generated transformation matrix is\nregarded as the ground truth. The generated translation values\nare uniformly sampled from the intervals [−1, 1], [−2, 2], and\n[−0.5, 0.5] along the x-, y-, and z-axes, respectively. The\ngenerated rotation values are uniformly sampled from the\nintervals [0◦, 5◦], [0◦, 5◦], and [0◦, 30◦] around the roll, pitch,\nand yaw axes, respectively.\nBoreas Dataset. The Boreas dataset is a publicly available\noutdoor dataset2 that comprises multi-sensor data, including\nLiDAR and camera data. It presents various environmental\nscenarios, such as sunny, night, and rainy conditions, as it was\ncollected over the course of one year by repeatedly driving a\nspecific route. Furthermore, the dataset provides post-processed\nground-truth poses with centimeter-level accuracy, which offers\nthe transformation matrix required for two consecutive LiDAR\npoint clouds. The Boreas datasets undergo preprocessing\ninvolving distortion correction of LiDAR point clouds, as\ndetailed in [88]. However, these preprocessing techniques do\nnot completely eliminate all distortions in LiDAR point clouds,\nsuch as the tailing phenomenon [89]. Additionally, noise can\nstill be present in environments with adverse weather conditions,\ndynamic objects or vehicles, and pedestrians, which can affect\nthe accuracy of the LiDAR measurements.\nKITTI Dataset. The KITTI dataset is a publicly available\noutdoor dataset3 that provides multi-sensor data for autonomous\ndriving. It includes LiDAR point clouds of street scenes cap-\ntured using the Velodyne Laserscanner in Karlsruhe, Germany,\nwith tens of thousands of LiDAR points in each frame. The\ndataset consists of 11 sequences (from sequence “ 0” to “ 10”)\ndepicting different street scenes, and global ground-truth poses\nare available for each sequence. Similar to the Boreas Dataset,\nwe can use the ground-truth poses to obtain the transformation\nmatrix between each pair of adjacent LiDAR point clouds\nin the KITTI dataset. While the KITTI dataset incorporates\npreprocessing for point cloud calibration [90] through auxiliary\nsensors, such as Global Positioning System (GPS) and Inertial\nMeasurement Unit (IMU), perturbations similar to those in the\nBoreas datasets persist.\n1https://github.com/loveoxford/vReLoc\n2https://www.boreas.utias.utoronto.ca/\n3http://www.cvlibs.net/datasets/kitti/\nB. Experimental Details\nModel Setting. We set d = 256 in (4). To deal with the\nneighborhood graph of the K nearest neighbors ( K = 20), we\nset the GNN layer fGNN_P in the graph neural PDE (4) to be\nthe union EdgeConv layers [65] which is also regarded as a\nkind of DGCNN. There are 5 EdgeConv layers used in the\nDGCNN block, whose hidden input and output dimensions\nare given by [6, 16], [16, 16], [16, 32], [32, 64] and [128, 3],\nrespectively. We set the graph learning module for the FGNN_G\nto another DGCNN, in which 5 EdgeConv layers are used\nwith the hidden input and output dimensions [6, 64], [64, 64],\n[64, 128], [128, 256] and [512, 256], respectively. When using\nthe DGCNN-based graph neural PDE for FGNN_F, there are 2\nEdgeConv layers used in the DGCNN block with the hidden\ninput and output dimensions [256, 256] and [768, 256]. As for\nthe self-cross attention module, there are 4 attention heads with\n128 hidden features for each attention head, which implies 512\nhidden features in total. We adopt the Adam optimizer [91] in\nthe training, where the learning rate is set as 0.0001. We set\nthe number of training epochs as 50.\nBaseline Methods. To demonstrate the superior performance\nof PointDifformer, we compared it against several baseline\nmethods, including ICP [31], DCP [9], HGNN [92], VCR-\nNet [13], PCT [70], and GeoTransformer [42]. ICP is an\niterative optimization method that does not require neural\nnetworks for feature learning, meaning that it does not need a\ntraining process. On the other hand, the other methods utilize\nlearned point cloud features to determine point correspondence\nsuch as DCP, VCR-Net and GeoTransformere. We further\nenhanced HGNN and PCT with attention modules for point\ncorrespondence registration, which we refer to as HGNN++\nand PCT++, respectively.\nC. Point Cloud Registration Performance\n1) Evaluation on Indoor vReLoc Dataset: To compare our\nmethod with other baselines, we evaluate them on their ability to\npredict transformations between two nearby point cloud frames\nfrom the vReLoc dataset. For training, we use sequences “ 3”,\n“6”, and “ 9”, while for testing, we use sequences “ 14” and\n“16”. We evaluate the performance of these methods using\nstatistics such as Mean Absolute Error (MAE) and Root Mean\nSquare Error (RMSE) for the predicted relative translation and\nrotation results. We also utilize Registration Recall (RR) as\na metric, which is defined in [37], [42], [54]. RR measures\nthe percentage of point cloud frames that achieve a certain\nthreshold of registration accuracy. In our evaluation, we use a\nfine-tuned threshold for performance comparison.\nFrom Table I, we observe that PointDifformer outperforms\nthe other baselines without neural diffusions in terms of relative\ntranslation and rotation prediction. This suggests that the\ngraph neural PDE modules play a positive role in point cloud\nregistration. Further analysis of PointDifformer in Fig. 5 shows\nthat the translation and rotation errors lie in a small region\nclose to zero. In Fig. 5, the empirical probability or the relative\nfrequency of an error value, is the ratio of the number of errors\nwithin a small bin around the error value to the total number of\ntrials. The relative translation error and rotation error in Fig. 5\n8\nKITTI\nGround Truth ICP VCR-Net GeoTransformer  PointDifformer\nvReLoc\nBoreas\nFig. 4. Examples of point cloud frame pairs transformed using different prediction methods to align the second frame with the coordinate system of the first\nframe.\nare calculated as the absolute values of the difference between\nthe corresponding predictions and the ground truth. Using the\npredicted transformation between two point cloud frames, we\ncan transform the second frame into the coordinate system of\nthe first frame to achieve alignment of the point clouds. We\nshow several examples of point cloud alignment based on the\npredicted transformation in Fig. 4, where the degree of overlap\nbetween the two frames increases with the accuracy of the\npredicted transformation.\nTABLE I\nPERFORMANCE OF POINT CLOUD REGISTRATION PREDICTION ON THE\nVRELOC DATASET . THE BEST AND SECOND -BEST RESULTS UNDER\nDIFFERENT METRICS ARE HIGHLIGHTED IN BOLD AND UNDERLINED ,\nRESPECTIVELY .\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 2.20 12.69 0.20 1.35 96.2\nDCP 1.35 3.26 0.45 1.31 85.4\nHGNN++ 3.33 11.29 0.36 1.57 83.5\nVCR-Net 0.25 0.45 0.04 0.11 99.9\nPCT++ 0.25 0.44 0.05 0.12 99.9\nGeoTransformer 0.66 1.10 0.07 0.16 99.9\nPointDifformer 0.14 0.40 0.03 0.10 99.9\n2) Evaluation on Outdoor Boreas Dataset: We compare\nPointDifformer with other baselines on the outdoor Boreas\ndataset, where the training dataset is collected under sunny\n02468 1 0 1 2 1 4 1 6 1 8 2 0\nRelative Translation Error\n(a)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\nRelative Rotation Error\n(b)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\nICP DCP HGNN++ VCR-Net PCT++ GeoTransformer PointDifformer\nFig. 5. The empirical probability of relative translation (centimeter [cm]) and\nrotation (degree [ ◦]) errors on the vReLoc dataset.\nweather, and the test dataset is collected under night weather.\nAs shown in Table II, PointDifformer outperforms the other\nbaselines under all criteria, except for the relative translation\nMAE, for which GeoTransformer is slightly better. However,\nFig. 6 shows that GeoTransformer has longer probability tails\nin the translation and rotation errors than PointDifformer,\nindicating that our method is more robust. We also present\n9\nseveral examples of point cloud alignment using the predicted\ntransformations in Fig. 4.\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\nRelative Translation Error\n(a)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nRelative Rotation Error\n(b)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\nICP DCP HGNN++ VCR-Net PCT++ GeoTransformer PointDifformer\nFig. 6. The empirical probability of relative translation (centimeter [cm]) and\nrotation (degree [ ◦]) errors on the Boreas dataset.\nTABLE II\nPOINT CLOUD REGISTRATION PERFORMANCE ON THE BOREAS DATASET .\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 10.83 18.28 0.11 0.21 75.4\nDCP 11.63 17.36 0.12 0.21 70.5\nHGNN++ 14.41 23.16 0.14 0.25 56.1\nVCR-Net 8.71 13.56 0.10 0.17 84.7\nPCT++ 9.81 15.77 0.10 0.19 79.6\nGeoTransformer 4.58 15.78 0.08 0.22 94.9\nPointDifformer 6.12 8.84 0.07 0.12 96.1\n3) Evaluation on Outdoor KITTI Dataset: We conduct point\ncloud registration methods on the KITTI dataset, selecting\naround 1600 and 1200 point cloud pairs for training and\ntesting, respectively. From Table III and Fig. 7, we observe that\nPointDifformer demonstrates superior performance compared\nto other baselines on the KITTI dataset, with shorter tails\nof relative rotation and translation error probabilities. This\nis similar to its performance on the Boreas dataset. We also\npresent several examples of our results in Fig. 4. Furthermore,\nwe train on sequences “ 0” to “ 8” and test on sequences “ 9” to\n“10”. We observe that PointDifformer surpasses other baselines\nwhen the size of the training data is larger, as shown in Table IV.\nThe LiDAR point clouds in the KITTI dataset have practical\nnoise due to dynamic objects and complex environments.\nHowever, the graph neural PDE modules in PointDifformer\nexhibit robustness to input perturbations, as demonstrated in\n[81]. This may be the reason why PointDifformer achieves\nmore accurate predicted results when there is more practical\nnoise in larger-sized data.\n4) Generalization from KITTI Dataset to Boreas dataset: To\ncross-validate, we conduct point cloud registration by training\nthe models on the KITTI dataset and evaluating them on the\nBoreas dataset. Specifically, we train the models on sequence\n“9” of the KITTI dataset and test them on the sequence\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\nRelative Translation Error\n(a)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\nICP DCP HGNN++ VCR-Net PCT++ GeoTransformer PointDifformer\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2\nRelative Rotation Error\n(b)\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nEmpirical Probability\nFig. 7. The empirical probability of relative translation (centimeter [cm]) and\nrotation (degree [ ◦]) errors on the KITTI dataset.\nTABLE III\nPERFORMANCE OF POINT CLOUD REGISTRATION PREDICTION ON THE\nKITTI DATASET.\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 9.86 19.48 0.17 0.27 87.9\nDCP 9.28 15.34 0.26 0.49 95.0\nHGNN++ 8.86 17.20 0.20 0.31 89.9\nVCR-Net 5.31 11.07 0.16 0.24 97.3\nPCT++ 6.16 13.96 0.18 0.28 95.4\nGeoTransformer 3.93 13.50 0.18 0.50 97.8\nPointDifformer 4.14 8.86 0.14 0.23 97.7\nTABLE IV\nPERFORMANCE OF POINT CLOUD REGISTRATION PREDICTION ON THE\nKITTI DATASET WITH SEQUENCE “0” TO “8” FOR TRAINING AND\nSEQUENCE “9” TO “10” FOR THE TEST .\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nVCR-Net 7.17 10.88 0.21 0.36 97.2\nGeoTransformer 3.45 12.91 0.14 0.76 99.0\nPointDifformer 3.10 5.93 0.11 0.17 99.0\n“night” of the Boreas dataset. From Table V, we observe that\nPointDifformer has competitive performance compared with\nthe current state-of-the-art.\nTABLE V\nTHE PERFORMANCE OF POINT CLOUD REGISTRATION ON THE BOREAS\nDATASET FOR TESTING (USING THE MODEL PRE -TRAINED ON THE KITTI\nDATASET ).\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nHGNN++ 16.06 25.86 0.15 0.27 49.8\nVCR-Net 11.97 19.78 0.11 0.19 68.6\nPCT++ 11.61 19.57 0.13 0.31 72.4\nGeoTransformer 5.97 27.90 0.09 0.33 93.1\nPointDifformer 6.63 10.07 0.08 0.14 93.3\n10\nD. Robutness Evaluation\n1) Robustness against synthetic noise on the KITTI Dataset:\nWe investigate the robustness of PointDifformer under various\ntypes of synthetic noise, as discussed below.\nTABLE VI\nPOINT CLOUD REGISTRATION PERFORMANCE ON THE KITTI DATASET\nWITH THE GAUSSIAN NOISE THAT FOLLOWS N(0, σ= 0.25).\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 14.97 26.09 0.20 0.32 69.3\nDCP 9.97 15.84 0.29 0.52 94.3\nHGNN++ 10.62 18.76 0.22 0.34 88.9\nVCR-Net 6.40 12.40 0.18 0.27 96.3\nPCT++ 6.85 14.03 0.20 0.30 95.3\nGeoTransformer 5.37 14.43 0.25 0.50 97.5\nPointDifformer 5.23 9.00 0.17 0.25 97.7\nICP VCR-Net\n PointDifformer\nGeoTransformer\nFig. 8. Examples for the point cloud frame pairs from the KITTI dataset\nwith Gaussian noise using different transformation prediction methods for\nalignment.\nTABLE VII\nTHE MAE OF THE PREDICTED RELATIVE TRANSLATION /ROTATION\n(CENTIMETER [CM] / DEGREE [◦]) ON DIFFERENT NOISE POWERS .\nMetric Methods N(0, σ= 0.5) N(0, σ= 0.75) N(0, σ= 1.0)\nMAE\nVCR-Net 7.97 / 0.23 9.70 / 0.29 11.62 / 0.36\nGeoTransformer 7.95 / 0.34 10.04 / 0.44 13.27 / 0.54\nPointDifformer 7.18 / 0.23 8.83 / 0.27 10.38 / 0.31\nRMSE\nVCR-Net 14.08 / 0.35 15.92 / 0.44 17.95 / 0.55\nGeoTransformer 14.40 / 0.68 15.00 / 0.78 20.15 / 0.90\nPointDifformer 12.90 / 0.34 15.08 / 0.40 17.31 / 0.45\nPerformance on the KITTI dataset with Gaussian noise.\nTo evaluate the robustness of PointDifformer, we add white\nTABLE VIII\nPOINT CLOUD REGISTRATION PERFORMANCE ON THE KITTI DATASET\nWITH 3D SHAPE PERTURBATIONS .\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 12.60 24.92 0.18 0.32 80.4\nDCP 12.63 23.04 0.28 0.48 85.8\nHGNN++ 11.39 22.27 0.21 0.36 85.1\nVCR-Net 6.39 13.69 0.18 0.36 95.4\nPCT++ 7.23 16.23 0.21 0.36 93.6\nGeoTransformer 3.89 13.08 0.18 0.44 97.8\nPointDifformer 4.14 8.99 0.14 0.22 97.8\nGeoTransformer\nICP VCR-Net\n PointDifformer\nFig. 9. Examples for noisy point cloud frames with 3D shape perturbations\nusing different transformation prediction methods for alignment.\nGaussian noise N(0, σ) to the original KITTI dataset, similar\nto the experiments for Table III. Based on the results presented\nin Table VI, we observe that PointDifformer surpasses the\nother benchmark methods in terms of relative translation and\nrotation errors. Additionally, we present examples of point\ncloud alignment using the predicted transformation in Fig. 8.\nWe also evaluate the robustness of our method and other\nbaselines to different noise powers. As shown in Table VII,\nPointDifformer demonstrates superior robustness w.r.t. additive\nGaussian noise compared to the other baselines across different\nnoise powers.\nPerformance on the noisy KITTI dataset with 3D shape\nperturbations. We next introduce 3D shape perturbations to the\noriginal KITTI dataset. This is achieved by removing certain\nparts of the original point clouds, resulting in an imperfect\n3D shape. Specifically, we remove a 25 m ×15 m region in\nthe lower left corner of each point cloud frame measuring\n60 m ×30 m. The results presented in Table VIII show that\n11\nPointDifformer still outperforms the baselines in terms of both\ncriteria for relative rotation error and the RMSE for relative\ntranslation prediction. Some examples are presented in Fig. 9.\n2) Robustness against natural noise on the Boreas Dataset:\nWe conduct experiments on the Boreas dataset under rainy\nconditions, which is considered natural noise on point clouds.\nFrom Table IX, we observe that PointDifformer outperforms the\nbaseline methods in terms of relative translation and rotation\nRMSEs, while having comparable performance in terms of\nMAEs. This indicates that our method produces fewer outliers\nin the predicted results, demonstrating its superior robustness\ncompared to the baselines. We also provide several examples\nof point cloud alignment in Fig. 10.\nTABLE IX\nPOINT CLOUD REGISTRATION PERFORMANCE ON THE BOREAS DATASET\nUNDER THE RAINING ENVIRONMENT .\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nICP 11.90 20.57 0.15 0.27 70.8\nDCP 10.60 16.00 0.14 0.22 75.7\nHGNN++ 15.02 25.63 0.18 0.32 52.5\nVCR-Net 8.81 14.09 0.13 0.20 84.1\nPCT++ 10.39 16.86 0.14 0.24 77.4\nGeoTransformer 4.96 16.75 0.10 0.25 94.9\nPointDifformer 5.91 8.45 0.10 0.14 96.9\nGeoTransformer\nICP VCR-Net\n PointDifformer\nFig. 10. Examples for the point cloud frame pairs from the Boreas dataset\nunder a raining environment using different transformation prediction methods\nfor alignment.\nE. Evaluation on Datasets with Lower Overlaps\nWe evaluate the performance of our method on datasets\nwith lower overlaps, namely, 3DMatch and 3DLoMatch, as\nshown in Table X. The standard benchmark metric RR is used,\nand the experimental configuration follows that of [42], [54].\nThe baselines include FCGF [17], D3Feat [18], SpinNet [19],\nPredator [35], YOHO [44], CoFiNet [20], GeoTransformer [42],\nRoITr [54], and RoReg [56]. To perform the registration task\non the 3DMatch and 3DLoMatch datasets, similar to several\ncurrent state-of-the-art models like CoFiNet, GeoTransformer,\nRoITr and RoReg, we first employ a module to identify\nthe areas of higher overlap in the point clouds. Specifically,\nwe use superpoint matching [42]. Subsequently, keypoint\nmatching under the PointDifformer model is employed to\nachieve more precise correspondences for pairs of point clouds.\nTable X demonstrates that PointDifformer achieves state-of-\nthe-art performance, affirming its feasibility on datasets with\nlower overlaps.\nTABLE X\nPERFORMANCE ON THE 3DM ATCH AND 3DL OMATCH DATASETS , USING\nTHE SAME EXPERIMENTAL CONFIGURATION AS THAT IN [42], [54]. T HE\nRESULTS OF BASELINES ARE BORROWED FROM [17]–[20], [35], [42], [44],\n[54], [56].\nMethod 3DMatch\nRR (%)\n3DLoMatch\nRR (%)\nFCGF 85.1 40.1\nD3Feat 81.6 37.2\nSpinNet 88.6 59.8\nPredator 89.0 59.8\nYOHO 90.8 65.2\nCoFiNet 89.3 67.5\nGeoTransformer 92.0 75.0\nRoITr 91.9 74.8\nRoReg 92.9 70.3\nPointDifformer 93.0 75.2\nF . Computational Complexity\nIn Table XI, we present the average inference time and\ngraphics processing unit (GPU) memory required for registering\neach point cloud pair based on the KITTI dataset. We test\nthe methods on an NVIDIA RTX A5000 GPU. The average\ninference time and GPU memory are measured in seconds (s)\nand gigabytes (GB), respectively. From Table XI, we observe\nthat PointDifformer requires higher GPU memory and incurs\nlonger inference time compared to other baselines due to\nits higher complexity. On average, the inference time is still\nacceptable for real-time applications. A possible future work\nis to optimize and prune [93]–[95] the PointDifformer model\nto reduce its memory and inference time footprints.\nTABLE XI\nTHE AVERAGE INFERENCE TIME AND GPU MEMORY FOR EACH POINT\nCLOUD PAIR ON THE KITTI DATASET.\nMethod VCR-Net PCT++ GeoTransformer PointDifformer\nInference time 0.047s 0.648s 0.061s 0.072s\nGPU memory 2.29GB 2.38GB 1.51GB 2.44GB\nG. Ablation Study\nWe perform an ablation study using the KITTI dataset under\nthe same experimental settings as described in Section IV-C3\n12\nfor Table III. We first evaluate the efficiency of the self-attention\nmodule with heat kernel signature by comparing it with the\nvanilla self-attention module and the module without self-\nattention. As shown in Table XII, the introduction of the heat\nkernel signature as weights into the self-attention module im-\nproves the transformation prediction accuracy. Furthermore, we\nobserve that the vanilla self-attention module also contributes\nto the point cloud registration performance.\nTABLE XII\nABLATION STUDY FOR THE SELF -ATTENTION MODULE WITH HEAT KERNEL\nSIGNATURE .\nModule\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [◦]) RR\n(%)MAE RMSE MAE RMSE\nSelf Attention (No) 5.75 11.73 0.16 0.25 92.1\nSelf Attention (Vanilla) 4.56 10.03 0.15 0.26 94.5\nSelf Attention (Heat Kernel) 4.14 8.86 0.14 0.23 97.7\nWe investigate the influence of our proposed Point-Diffusion\nNet module on point cloud representation by comparing it with\nother graph learning methods. The results in Table XIII show\nthat the point cloud registration model with our Point-Diffusion\nNet outperforms those with other graph learning methods\nincluding HGNN [92] and DGCNN [65]. This indicates that the\nPoint-Diffusion Net can achieve a more robust representation\nof the point cloud.\nTABLE XIII\nABLATION STUDY FOR THE EFFECTIVENESS OF POINT -DIFFUSION NET.\nMethod\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nHGNN 8.22 16.40 0.19 0.31 90.1\nDGCNN 4.46 9.55 0.14 0.23 97.3\nPoint-Diffusion Net 4.14 8.86 0.14 0.23 97.7\nWe investigate the impact of the number of selected top K′\ncorresponding keypoints from the attention-based correspon-\ndence module. Specifically, we select 25%, 50%, 75%, and\n100% corresponding keypoints from the entire set of keypoints.\nThe results in Table XIV show that selecting 50% and 75%\ncorresponding keypoints achieves better performance.\nTABLE XIV\nABLATION STUDY FOR THE NUMBER OF SELECTED CORRESPONDING\nKEYPOINTS IN THE ATTENTION -BASED CORRESPONDENCE .\nProportion\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\n25% 8.88 15.84 0.20 0.32 88.1\n50% 4.06 7.95 0.15 0.25 97.3\n75% 4.14 8.86 0.14 0.23 97.7\n100% 7.20 16.14 0.19 0.34 92.2\nTo evaluate the effectiveness of our total loss Ltotal in (28),\nwe compare it with the corresponding point loss Lpoint in (26)\nand the ground-truth-to-prediction loss Lrt in (27). Based on\nthe results presented in Table XV, we observe that the total loss\nLtotal outperforms the others, suggesting that incorporating\nmore information in the loss function has a positive effect on\ntraining. Additionally, we find that Lpoint surpasses Lrt, which\nimplies that the point loss plays a more important role in the\nLtotal.\nTABLE XV\nABLATION STUDY FOR THE LOSS FUNCTION .\nLoss\nRelative Translation\nError (centimeter [cm])\nRelative Rotation\nError (degree [ ◦]) RR\n(%)MAE RMSE MAE RMSE\nLrt 6.32 11.55 0.21 0.39 94.2\nLpoint 5.41 11.44 0.15 0.24 96.3\nLtotal 4.14 8.86 0.14 0.23 97.7\nV. C ONCLUSIONS\nIn order to develop a robust 3D point cloud registration\napproach that is able to handle noise or perturbations, we\nhave utilized graph neural PDE modules to learn point\ncloud feature representations. We have also designed attention\nmodules with heat kernel signatures to establish correspondence\nbetween points from two point clouds. Our approach has been\nextensively evaluated through experiments, which demonstrate\nthat it generally outperforms baselines not only on raw point\nclouds but also on point clouds with additive noise and 3D\nshape perturbations. These results suggest that graph neural\nPDEs are beneficial for the task of point cloud registration.\nIn this paper, we have conducted a robustness study limited\nonly to perturbations through Gaussian noise, rain, and partial\nremoval of a frame. A future work of interest is to further\ninvestigate the robustness of our method under diverse pertur-\nbations, including adversarial attacks. Furthermore, we aim\nto enhance our model’s adaptability to noisy datasets under\ndifferent environmental factors.\nACKNOWLEDGMENT\nTo improve the readability, parts of this paper have been\ngrammatically revised using ChatGPT [96].\nREFERENCES\n[1] L. Zhang, J. Guo, Z. Cheng, J. Xiao, and X. Zhang, “Efficient pairwise\n3-D registration of urban scenes via hybrid structural descriptors,” IEEE\nTrans. Geosci. Remote. Sens. , vol. 60, pp. 1–17, 2022.\n[2] S. Quan and J. Yang, “Compatibility-guided sampling consensus for 3-D\npoint cloud registration,” IEEE Trans. Geosci. Remote. Sens. , vol. 58,\nno. 10, pp. 7380–7392, 2020.\n[3] Q. Kang, R. She, S. Wang, W. Tay, D. Navarro, and A. Hartmannsgruber,\n“Location learning for A Vs: LiDAR and image landmarks fusion\nlocalization with graph neural networks,” in Proc. IEEE Int. Conf. Intell.\nTransp. Syst., 2022, pp. 3032–3037.\n[4] L. Sun, Z. Zhang, R. Zhong, D. Chen, L. Zhang, L. Zhu, Q. Wang,\nG. Wang, J. Zou, and Y . Wang, “A weakly supervised graph deep learning\nframework for point cloud registration,” IEEE Trans. Geosci. Remote.\nSens., vol. 60, pp. 1–12, 2022.\n[5] P. Zhou, X. Guo, X. Pei, and C. Chen, “T-LOAM: Truncated least squares\nLiDAR-only odometry and mapping in real time,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–13, 2021.\n[6] W. Shi and R. Rajkumar, “Point-GNN: Graph neural network for 3D\nobject detection in a point cloud,” in Proc. IEEE Int. Conf. Comput.\nVision Pattern Recognit., 2020, pp. 1711–1719.\n[7] T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus, “LIO-\nSAM: Tightly-coupled LiDAR inertial odometry via smoothing and\nmapping,” in Proc. IEEE Int. Conf. Intell. Robot. Syst. , 2020, pp. 5135–\n5142.\n13\n[8] J. Zhang and S. Singh, “LOAM: LiDAR odometry and mapping in\nreal-time,” in Proc. Robot. Sci. Syst. , 2014, pp. 1–9.\n[9] Y . Wang and J. M. Solomon, “Deep closest point: Learning represen-\ntations for point cloud registration,” in Proc. IEEE Int. Conf. Comput.\nVision, 2019, pp. 3523–3532.\n[10] P. J. Besl and N. D. McKay, “Method for registration of 3-D shapes,”\nin IEEE Trans. Pattern Anal. Machine Intell. , vol. 14, no. 2, 1992, pp.\n239–256.\n[11] J. Yang, H. Li, and Y . Jia, “Go-ICP: Solving 3D registration efficiently\nand globally optimally,” in Proc. IEEE Int. Conf. Comput. Vision , 2013,\npp. 1457–1464.\n[12] K. Koide, M. Yokozuka, S. Oishi, and A. Banno, “V oxelized GICP for\nfast and accurate 3D point cloud registration,” in Proc. IEEE Int. Conf.\nRobot. Autom., 2021, pp. 11 054–11 059.\n[13] H. Wei, Z. Qiao, Z. Liu, C. Suo, P. Yin, Y . Shen, H. Li, and H. Wang,\n“End-to-end 3D point cloud learning for registration task using virtual\ncorrespondences,” in Proc. IEEE Int. Conf. Intell. Robot. Syst. , 2020, pp.\n2678–2683.\n[14] J. Xu, Y . Huang, Z. Wan, and J. Wei, “GLORN: Strong generalization\nfully convolutional network for low-overlap point cloud registration,”\nIEEE Trans. Geosci. Remote. Sens. , vol. 60, pp. 1–14, 2022.\n[15] Y . Zhang, J. Xu, Y . Zou, P. X. Liu, and J. Liu, “PS-Net: Point shift\nnetwork for 3-D point cloud completion,” IEEE Trans. Geosci. Remote.\nSens., vol. 60, pp. 1–13, 2022.\n[16] F. Lu, G. Chen, Y . Liu, L. Zhang, S. Qu, S. Liu, and R. Gu, “HRegNet:\nA hierarchical network for large-scale outdoor LiDAR point cloud\nregistration,” in Proc. IEEE Int. Conf. Comput. Vision , 2021, pp. 16 014–\n16 023.\n[17] C. Choy, J. Park, and V . Koltun, “Fully convolutional geometric features,”\nin Proc. IEEE Int. Conf. Comput. Vision , 2019, pp. 8958–8966.\n[18] X. Bai, Z. Luo, L. Zhou, H. Fu, L. Quan, and C.-L. Tai, “D3Feat: Joint\nlearning of dense detection and description of 3D local features,” in Proc.\nIEEE Int. Conf. Comput. Vision Pattern Recognit. , 2020, pp. 6359–6367.\n[19] S. Ao, Q. Hu, B. Yang, A. Markham, and Y . Guo, “SpinNet: Learning a\ngeneral surface descriptor for 3D point cloud registration,” in Proc. IEEE\nInt. Conf. Comput. Vision Pattern Recognit. , 2021, pp. 11 753–11 762.\n[20] H. Yu, F. Li, M. Saleh, B. Busam, and S. Ilic, “CoFiNet: Reliable coarse-\nto-fine correspondences for robust pointcloud registration,” Adv. Neural\nInform. Process. Syst. , vol. 34, pp. 23 872–23 884, 2021.\n[21] J. Li, Q. Hu, and M. Ai, “Point cloud registration based on one-point\nransac and scale-annealing biweight estimation,” IEEE Trans. Geosci.\nRemote Sens., vol. 59, no. 11, pp. 9716–9729, 2021.\n[22] F. Wang, H. Hu, X. Ge, B. Xu, R. Zhong, Y . Ding, X. Xie, and Q. Zhu,\n“Multientity registration of point clouds for dynamic objects on complex\nfloating platform using object silhouettes,” IEEE Trans. Geosci. Remote\nSens., vol. 59, no. 1, pp. 769–783, 2020.\n[23] S. Chen, L. Nan, R. Xia, J. Zhao, and P. Wonka, “PLADE: A plane-based\ndescriptor for point cloud registration with small overlap,” IEEE Trans.\nGeosci. Remote. Sens. , vol. 58, no. 4, pp. 2530–2540, 2019.\n[24] J. Yu, Y . Lin, B. Wang, Q. Ye, and J. Cai, “An advanced outlier detected\ntotal least-squares algorithm for 3-D point clouds registration,” IEEE\nTrans. Geosci. Remote. Sens. , vol. 57, no. 7, pp. 4789–4798, 2019.\n[25] Y . Zhao, Y . Li, H. Zhang, V . Monga, and Y . C. Eldar, “A convergent\nneural network for non-blind image deblurring,” in Proc. IEEE Int. Conf.\nImage Process., 2023, pp. 1505–1509.\n[26] Y . Song, Q. Kang, S. Wang, Z. Kai, and W. P. Tay, “On the robustness of\ngraph neural diffusion to topology perturbations,” in Adv. Neural Inform.\nProcess. Syst., vol. 35, 2022, pp. 6384–6396.\n[27] J. Sun, M. Ovsjanikov, and L. Guibas, “A concise and provably\ninformative multi-scale signature based on heat diffusion,” Comput.\nGraphics Forum, vol. 28, no. 5, pp. 1383–1392, 2009.\n[28] M. A. Fischler and R. C. Bolles, “Random sample consensus: A paradigm\nfor model fitting with applications to image analysis and automated\ncartography,” ACM Commun., vol. 24, no. 6, pp. 381–395, 1981.\n[29] H. Maron, N. Dym, I. Kezurer, S. Kovalsky, and Y . Lipman, “Point\nregistration via efficient convex relaxation,”ACM Trans. Graphics, vol. 35,\nno. 4, pp. 1–12, 2016.\n[30] T. Guérout, Y . Gaoua, C. Artigues, G. Da Costa, P. Lopez, and T. Monteil,\n“Mixed integer linear programming for quality of service optimization in\nclouds,” Future Gener. Comput. Syst. , vol. 71, pp. 1–17, 2017.\n[31] A. Segal, D. Haehnel, and S. Thrun, “Generalized-ICP,” in Proc. Robot.\nSci. Syst., vol. 2, no. 4, 2009, p. 435.\n[32] H. Deng, T. Birdal, and S. Ilic, “PPF-FoldNet: Unsupervised learning\nof rotation invariant 3D local descriptors,” in Proc. Eur. Conf. Comput.\nVision, 2018, pp. 602–618.\n[33] H. Deng, T. Birdal, and et al., “PPFNet: Global context aware local\nfeatures for robust 3D point matching,” in Proc. IEEE Int. Conf. Comput.\nVision Pattern Recognit., 2018, pp. 195–205.\n[34] Z. Gojcic, C. Zhou, J. D. Wegner, and A. Wieser, “The perfect match:\n3D point cloud matching with smoothed densities,” in Proc. IEEE Int.\nConf. Comput. Vision Pattern Recognit. , 2019, pp. 5545–5554.\n[35] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, and K. Schindler,\n“Predator: Registration of 3D point clouds with low overlap,” in Proc.\nIEEE Int. Conf. Comput. Vision Pattern Recognit. , 2021, pp. 4267–4276.\n[36] Z. Chen, K. Sun, F. Yang, and W. Tao, “SC 2-PCR: A second order\nspatial compatibility for efficient and robust point cloud registration,”\nin Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. , 2022, pp.\n13 221–13 231.\n[37] X. Zhang, J. Yang, S. Zhang, and Y . Zhang, “3D registration with maximal\ncliques,” in Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit., 2023,\npp. 17 745–17 754.\n[38] W. Lu, Y . Zhou, G. Wan, S. Hou, and S. Song, “L3-Net: Towards learning\nbased LiDAR localization for autonomous driving,” in Proc. IEEE Int.\nConf. Comput. Vision Pattern Recognit. , 2019, pp. 6389–6398.\n[39] W. Lu, G. Wan, Y . Zhou, X. Fu, P. Yuan, and S. Song, “DeepVCP: An\nend-to-end deep neural network for point cloud registration,” in Proc.\nIEEE Int. Conf. Comput. Vision , 2019, pp. 12–21.\n[40] Y . Aoki, H. Goforth, R. A. Srivatsan, and S. Lucey, “PointNetLK: Robust\n& efficient point cloud registration using pointnet,” in Proc. IEEE Int.\nConf. Comput. Vision Pattern Recognit. , 2019, pp. 7163–7172.\n[41] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep hierarchical\nfeature learning on point sets in a metric space,” Adv. Neural Inform.\nProcess. Syst., vol. 30, pp. 1–10, 2017.\n[42] Z. Qin, H. Yu, C. Wang, Y . Guo, Y . Peng, and K. Xu, “Geometric\ntransformer for fast and robust point cloud registration,” in Proc. IEEE\nInt. Conf. Comput. Vision Pattern Recognit. , 2022, pp. 11 143–11 152.\n[43] C. Choy, W. Dong, and V . Koltun, “Deep global registration,” in Proc.\nIEEE Int. Conf. Comput. Vision Pattern Recognit. , 2020, pp. 2514–2523.\n[44] H. Wang, Y . Liu, Z. Dong, and W. Wang, “You only hypothesize once:\nPoint cloud registration with rotation-equivariant descriptors,” in Proc.\nACM Int. Conf. Multimedias , 2022, pp. 1630–1641.\n[45] M. Zhao, L. Ma, X. Jia, D.-M. Yan, and T. Huang, “GraphReg: Dynamical\npoint cloud registration with geometry-aware graph signal processing,”\nIEEE Trans. Image Process. , vol. 31, pp. 7449–7464, 2022.\n[46] F. Poiesi and D. Boscaini, “Learning general and distinctive 3D local\ndeep descriptors for point cloud registration,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 45, no. 3, pp. 3979–3985, 2023.\n[47] W. Tang and D. Zou, “Multi-instance point cloud registration by efficient\ncorrespondence clustering,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2022, pp. 6667–6676.\n[48] J. Lee, S. Kim, M. Cho, and J. Park, “Deep hough voting for robust\nglobal registration,” in Proc. IEEE Int. Conf. Comput. Vision , 2021, pp.\n15 994–16 003.\n[49] G. D. Pais, S. Ramalingam, V . M. Govindu, J. C. Nascimento, R. Chel-\nlappa, and P. Miraldo, “3DRegNet: A deep neural network for 3D point\nregistration,” in Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. ,\n2020, pp. 7193–7203.\n[50] K. Fischer, M. Simon, F. Olsner, S. Milz, H.-M. Gross, and P. Mader,\n“StickyPillars: Robust and efficient feature matching on point clouds\nusing graph neural networks,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2021, pp. 313–323.\n[51] X. Bai, Z. Luo, L. Zhou, H. Chen, L. Li, Z. Hu, H. Fu, and C.-L. Tai,\n“PointDSC: Robust point cloud registration using deep spatial consistency,”\nin Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. , 2021, pp.\n15 859–15 869.\n[52] Z. Zhang, Y . Dai, B. Fan, J. Sun, and M. He, “Learning a task-specific\ndescriptor for robust matching of 3D point clouds,” IEEE Trans. Circuits\nSyst. Video Technol., vol. 32, no. 12, pp. 8462–8475, 2022.\n[53] Y . Li and T. Harada, “Lepard: Learning partial point cloud matching in\nrigid and deformable scenes,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2022, pp. 5554–5564.\n[54] H. Yu, Z. Qin, J. Hou, M. Saleh, D. Li, B. Busam, and S. Ilic, “Rotation-\ninvariant transformer for point cloud matching,” in Proc. IEEE Int. Conf.\nComput. Vision Pattern Recognit. , 2023, pp. 5384–5393.\n[55] S. Ao, Q. Hu, H. Wang, K. Xu, and Y . Guo, “BUFFER: Balancing\naccuracy, efficiency, and generalizability in point cloud registration,”\nin Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. , 2023, pp.\n1255–1264.\n[56] H. Wang, Y . Liu, Q. Hu, B. Wang, J. Chen, Z. Dong, Y . Guo, W. Wang,\nand B. Yang, “RoReg: Pairwise point cloud registration with oriented\ndescriptors and local rotations,” IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 45, no. 8, pp. 10 376–10 393, 2023.\n14\n[57] J. Yu, L. Ren, Y . Zhang, W. Zhou, L. Lin, and G. Dai, “PEAL:\nPrior-embedded explicit attention learning for low-overlap point cloud\nregistration,” in Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit. ,\n2023, pp. 17 702–17 711.\n[58] H. Jiang, Z. Dang, Z. Wei, J. Xie, J. Yang, and M. Salzmann, “Robust\noutlier rejection for 3D registration with variational bayes,” in Proc.\nIEEE Int. Conf. Comput. Vision Pattern Recognit. , 2023, pp. 1148–1157.\n[59] Y . Zhou and O. Tuzel, “V oxelNet: End-to-end learning for point cloud\nbased 3D object detection,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2018, pp. 4490–4499.\n[60] V . A. Sindagi, Y . Zhou, and O. Tuzel, “MVX-Net: Multimodal voxelnet\nfor 3D object detection,” in Proc. IEEE Int. Conf. Robot. Autom. , 2019,\npp. 7276–7282.\n[61] O. Kopuklu, N. Kose, A. Gunduz, and G. Rigoll, “Resource efficient 3D\nconvolutional neural networks,” in Proc. IEEE Int. Conf. Comput. Vision\nWorkshops, 2019, pp. 1–10.\n[62] S. Kumawat and S. Raman, “LP-3DCNN: Unveiling local phase in 3D\nconvolutional neural networks,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2019, pp. 4903–4912.\n[63] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view\nconvolutional neural networks for 3D shape recognition,” in Proc. IEEE\nInt. Conf. Comput. Vision , 2015, pp. 945–953.\n[64] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on\npoint sets for 3D classification and segmentation,” in Proc. IEEE Int.\nConf. Comput. Vision Pattern Recognit. , 2017, pp. 652–660.\n[65] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph CNN for learning on point clouds,” ACM\nTrans. Graphics, vol. 38, no. 5, pp. 1–12, 2019.\n[66] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang, H. Li, and Y .-H. Liu,\n“LPD-Net: 3D point cloud learning for large-scale place recognition and\nenvironment analysis,” in Proc. IEEE Int. Conf. Comput. Vision , 2019,\npp. 2831–2840.\n[67] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and\nL. J. Guibas, “KPConv: Flexible and deformable convolution for point\nclouds,” in Proc. IEEE Int. Conf. Comput. Vision , 2019, pp. 6411–6420.\n[68] Y . Rao, J. Lu, and J. Zhou, “Global-local bidirectional reasoning for\nunsupervised representation learning of 3D point clouds,” in Proc. IEEE\nInt. Conf. Comput. Vision Pattern Recognit. , 2020, pp. 5376–5385.\n[69] F. Poiesi and D. Boscaini, “Distinctive 3D local deep descriptors,” in\nProc. IEEE Int. Conf. Pattern Recognit. , 2021, pp. 5720–5727.\n[70] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu,\n“PCT: Point cloud transformer,” Comput. Vis. Media, vol. 7, pp. 187–199,\n2021.\n[71] X. Ma, C. Qin, H. You, H. Ran, and Y . Fu, “Rethinking network design\nand local geometry in point cloud: A simple residual MLP framework,”\narXiv preprint arXiv:2202.07123 , 2022.\n[72] G. Qian, Y . Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, and\nB. Ghanem, “PointNeXt: Revisiting PointNet++ with improved training\nand scaling strategies,” Adv. Neural Inform. Process. Syst. , vol. 35, pp.\n23 192–23 204, 2022.\n[73] R. T. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural\nordinary differential equations,” Adv. Neural Inform. Process. Syst. ,\nvol. 31, pp. 1–13, 2018.\n[74] B. P. Chamberlain, J. Rowbottom, M. Goronova, S. Webb, E. Rossi, and\nM. M. Bronstein, “GRAND: Graph neural diffusion,” in Proc. Int. Conf.\nMach. Learn., 2021, pp. 1407–1418.\n[75] Z.-Q. Chen, Z. Qian, Y . Hu, and W. Zheng, “Stability and approximations\nof symmetric diffusion semigroups and kernels,” J. functional anal. , vol.\n152, no. 1, pp. 255–280, 1998.\n[76] B. P. Chamberlain, J. Rowbottom, D. Eynard, F. Di Giovanni, D. Xiaowen,\nand M. M. Bronstein, “Beltrami flow and neural diffusion on graphs,”\nAdv. Neural Inform. Process. Syst. , pp. 1–16, 2021.\n[77] S. Wang, Q. Kang, R. She, W. P. Tay, A. Hartmannsgruber, and D. N.\nNavarro, “RobustLoc: Robust camera pose regression in challenging\ndriving environments,” in Proc. AAAI Conf. Artificial Intell. , 2022, pp.\n1–8.\n[78] Q. Kang, Y . Song, Q. Ding, and W. P. Tay, “Stable neural ODE with\nLyapunov-stable equilibrium points for defending against adversarial\nattacks,” Adv. Neural Inform. Process. Syst. , vol. 34, pp. 14 925–14 937,\n2021.\n[79] Y . Zhao, F. Dai, and J. Shi, “A dredge traffic algorithm for maintaining\nnetwork stability,” in Proc. Int. Conf. Commun. Signal Process. Syst. ,\n2020, pp. 1100–1108.\n[80] K. Zhao, Q. Kang, Y . Song, R. She, S. Wang, and W. P. Tay, “Adversarial\nrobustness in graph neural networks: A Hamiltonian approach,” arXiv\npreprint arXiv:2310.06396, 2023.\n[81] R. She, Q. Kang, S. Wang, Y .-R. Yang, K. Zhao, Y . Song, and W. P.\nTay, “RobustMat: Neural diffusion for street landmark patch matching\nunder challenging environments,” IEEE Trans. Image Process. , vol. 32,\npp. 5550–5563, 2023.\n[82] Z. Chen, W. Zeng, Z. Yang, L. Yu, C.-W. Fu, and H. Qu, “LassoNet:\nDeep lasso-selection of 3D point clouds,” IEEE Trans. Vis. Comput.\nGraphics, vol. 26, no. 1, pp. 195–204, 2019.\n[83] M. M. Bronstein and I. Kokkinos, “Scale-invariant heat kernel signatures\nfor non-rigid shape recognition,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2010, pp. 1704–1711.\n[84] G. K. Tam, Z.-Q. Cheng, Y .-K. Lai, F. C. Langbein, Y . Liu, D. Marshall,\nR. R. Martin, X.-F. Sun, and P. L. Rosin, “Registration of 3D point\nclouds and meshes: A survey from rigid to nonrigid,” IEEE Trans. Vis.\nComput. Graphics, vol. 19, no. 7, pp. 1199–1217, 2012.\n[85] R. T. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural\nordinary differential equations,” Adv. Neural Inform. Process. Syst. ,\nvol. 31, pp. 1–13, 2018.\n[86] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Adv. Neural\nInform. Process. Syst. , vol. 30, pp. 1–11, 2017.\n[87] A. Kendall and R. Cipolla, “Geometric loss functions for camera pose\nregression with deep learning,” in Proc. IEEE Int. Conf. Comput. Vision\nPattern Recognit., 2017, pp. 5974–5983.\n[88] K. Burnett, D. J. Yoon, Y . Wu, A. Z. Li, H. Zhang, S. Lu, J. Qian,\nW.-K. Tseng, A. Lambert, K. Y . Leung et al., “Boreas: A multi-season\nautonomous driving dataset,” Int. J. Rob. Res., vol. 42, no. 1-2, pp. 33–42,\n2023.\n[89] W. Song, D. Li, S. Sun, X. Xu, and G. Zu, “Registration for 3D LiDAR\ndatasets using Pyramid Reference Object,” IEEE Trans. Instrum. Meas. ,\nvol. 72, pp. 1–9, 2023.\n[90] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous\ndriving? The KITTI vision benchmark suite,” in Proc. IEEE Int. Conf.\nComput. Vision Pattern Recognit. IEEE, 2012, pp. 3354–3361.\n[91] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proc. Int. Conf. Learn. Representations , 2015, pp. 1–15.\n[92] Y . Feng, H. You, Z. Zhang, R. Ji, and Y . Gao, “Hypergraph neural\nnetworks,” in Proc. AAAI Conf. Artificial Intell. , 2019, pp. 3558–3565.\n[93] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the\nvalue of network pruning,” in Proc. Int. Conf. Learn. Representations ,\n2019, pp. 1–13.\n[94] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna: A\nnext-generation hyperparameter optimization framework,” in Proc. ACM\nSIGKDD Int. Conf. Knowl. Discov. & Data Mining , 2019, pp. 2623–2631.\n[95] G. Fang, X. Ma, and X. Wang, “Structural pruning for diffusion models,”\narXiv preprint arXiv:2305.10924 , 2023.\n[96] OpenAI, “ChatGPT [Large language model],” 2023. [Online]. Available:\nhttps://chat.openai.com/chat"
}