{
  "title": "Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",
  "url": "https://openalex.org/W4389519421",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2621439062",
      "name": "Marzena. Karpińska",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2068391019",
      "name": "Mohit Iyyer",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4399639995",
    "https://openalex.org/W4385565080",
    "https://openalex.org/W4385574121",
    "https://openalex.org/W2545577020",
    "https://openalex.org/W4229016505",
    "https://openalex.org/W4322096891",
    "https://openalex.org/W3035520602",
    "https://openalex.org/W2119727789",
    "https://openalex.org/W4321177597",
    "https://openalex.org/W3153290692",
    "https://openalex.org/W4287258879",
    "https://openalex.org/W2170204377",
    "https://openalex.org/W2971278086",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2608029998",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2318204850",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4385573754",
    "https://openalex.org/W2893608941",
    "https://openalex.org/W3201191103",
    "https://openalex.org/W3159892921",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W3033962033",
    "https://openalex.org/W4298050700",
    "https://openalex.org/W3088908511",
    "https://openalex.org/W4389777735",
    "https://openalex.org/W1654441844",
    "https://openalex.org/W2511248092",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W4389519044",
    "https://openalex.org/W2774486220",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4378908626",
    "https://openalex.org/W2187680124",
    "https://openalex.org/W4367061074",
    "https://openalex.org/W3100623455",
    "https://openalex.org/W4313560086",
    "https://openalex.org/W3094335515",
    "https://openalex.org/W4237258354",
    "https://openalex.org/W4229853184",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W1976732479",
    "https://openalex.org/W4385572037",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W2141895568",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2030987872",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W4389518873",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3046531489",
    "https://openalex.org/W3101683892",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2101566153",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W2170986353"
  ],
  "abstract": "Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact. We publicly release our dataset and error annotations to spur future research on the evaluation of document-level literary translation.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 419–451\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n419\nLarge language models effectively leverage document-level context\nfor literary translation, but critical errors persist\nMarzena Karpinska Mohit Iyyer\nUniversity of Massachusetts Amherst\n{mkarpinska, miyyer}@cs.umass.edu\nhttps://litmt.org/\nAbstract\nLarge language models (LLMs) are competi-\ntive with the state of the art on a wide range\nof sentence-level translation datasets. However,\ntheir ability to translate paragraphs and docu-\nments remains unexplored because evaluation\nin these settings is costly and difficult. We show\nthrough a rigorous human evaluation that ask-\ning the GPT-3.5 (text-davinci-003) LLM to\ntranslate an entire literary paragraph (e.g., from\na novel) at once results in higher-quality transla-\ntions than standard sentence-by-sentence trans-\nlation across 18 linguistically-diverse language\npairs (e.g., translating into and out of Japanese,\nPolish, and English). Our evaluation, which\ntook approximately 350 hours of effort for an-\nnotation and analysis, is conducted by hiring\ntranslators fluent in both the source and target\nlanguage and asking them to provide both span-\nlevel error annotations as well as preference\njudgments of which system’s translations are\nbetter. We observe that discourse-level LLM\ntranslators commit fewer mistranslations, gram-\nmar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, crit-\nical errors still abound, including occasional\ncontent omissions, and a human translator’s in-\ntervention remains necessary to ensure that the\nauthor’s voice remains intact. We publicly re-\nlease our dataset and error annotations to spur\nfuture research on the evaluation of document-\nlevel literary translation.1\n1 Introduction\nLarge language models (LLMs) such as ChatGPT\n(OpenAI, 2022) demonstrate remarkable perfor-\nmance as stand-alone translation systems, rival-\ning and sometimes surpassing commercial models\non sentence-level benchmarks (Vilar et al., 2022;\nHendy et al., 2023; Jiao et al., 2023). Further-\nmore, LLMs are increasingly being deployed for\ndocument-level translation (Book Maker, 2023;\n1https://github.com/marzenakrp/\nLiteraryTranslation\ncs-pl \nde-pl \nen-pl \nfr-pl \nja-pl \nru-pl \nde-en \nfr-en \nja-en pl-en ru-en \nzh-en \nde-ja \nen-ja \nfr-ja \npl-ja \nru-ja \nzh-ja \nFigure 1: A plot of the total number of errors annotated\nin sentence-level (SENT ) and paragraph-level (PARA )\ntranslations produced by GPT-3.5 across 18 different\nlanguage pairs. In all cases, PARA produces fewer er-\nrors than SENT , which demonstrates that GPT-3.5 takes\nadvantage of discourse context during translation.\nPawlak, 2023), a scenario for which there are cur-\nrently no reliable automatic evaluation methods. In\nthis paper, we hire human translators to conduct a\nrigorous fine-grained evaluation of GPT-3.5 ’s abil-\nity to translate paragraph-level texts from literary\nworks across 18 different language pairs. Our re-\nsults (Figure 1) demonstrate that GPT-3.5 2 effec-\ntively leverages discourse-level context to produce\nhigher-quality translations than when translating\nsentences in isolation.\nWhy literary texts? Translating works of liter-\nature poses unique challenges due to the intricate\nnature of creative work and the importance of cap-\nturing the author’s voice and contextual nuances.\nTranslators thus apply a wide range of transla-\n2We completed our annotations on translations from the\ntext-davinci-003 checkpoint obtained prior to the API re-\nlease of ChatGPT and GPT-4. Nevertheless, we include a\npreliminary analysis of GPT-4’s translations in §F.\n420\n  (...)\n「あー、あと 煙草 の５ 番 を 一 つ」\n“Oh, and a one (pack) of cigarettes, number five.”\n「かしこまりました」\n “Right away.” (lit. (I) understood)\n すばやくマルボロライトメンソールを 抜 き 取 り、レジでスキャンする。\n (I) take out (a pack of) Marlboro Menthol Lights quickly, and scan it at the\nregister.\n「年齢確認 のタッチをお 願 いします」\n“Please confirm your age on the touch screen.” (lit. “Age confirmation touch,\nplease.”)\n画 面 をタッチしながら、 男 性 の 目 線 がファーストフードが 並 んだショーケ\nースにすっと 移 ったのを 見 て、指 の 動 きを 止 める。\nAs (he) touches the screen, (I) see that the man's gaze shifted to the showcase with\nthe fast food, (and) (I) stop moving my finger(s).\n―Japanese Source (from Convenience Store Woman by Sayaka Murata)\n  (...)\n“Ah, and one pack of cigarettes, number five.”\n“Understood.”\nQuickly remove the Marlboro Light Menthol and scan it at the\nregister.\n“Please confirm your age with a touch.”\nThe man's gaze shifted quickly to the showcase where the fast food\nwas lined up while he was touching the screen, and he stopped his\nfinger movement. \n                                                       ―GPT 3.5 SENT (English)\nsentence-level\nparagraph-level\n  (...)\n“Ah, and one pack of cigarettes, number five.”\n“Right away.”\nI quickly pulled out a Marlboro Light Menthol and scanned it at\nthe register.\n“Please touch the screen for age verification.”\nHis gaze shifted to the showcase with the fast food as he touched the\nscreen, and I stopped my finger’s movement. \n                                                                ―GPT 3.5 PARA (English)\nFigure 2: An example of paragraph-level ( PARA ) and sentence-level (SENT ) translations of the same Japanese\nparagraph into English. Sentence-level translation results in a range of erroneous translations, from worse word\nchoice (“understood” vs “right away”) to incorrect pronouns (“he” vs “I”); these errors are corrected by PARA .\ntion techniques (Chesterman, 1997; Molina and\nHurtado Albir, 2004), from simple shifts in gram-\nmatical categories to more complex stylistic or\ncontent-based rearrangements that often cross sen-\ntence boundaries. Translators may also merge or\nsplit sentences and paragraphs, which renders the\ntraditional sentence-level pipeline insufficient for\ncapturing the full scope of the original text (Toral\nand Way, 2015; Taivalkoski-Shilov, 2019b; Post\nand Junczys-Dowmunt, 2023; Jiang et al., 2023).3\nTaken together, these properties make literary texts\na good testbed for document-level machine transla-\ntion (Thai et al., 2022); in our work, we focus on\nthe paragraph4 as a minimal discourse-level unit.\nWhy human evaluation? The absence of rigor-\nous document-level evaluations of LLM translators\nis striking but also somewhat understandable given\nthe unreliability of automatic metrics (Thai et al.,\n2022) and the difficulty of properly conducting hu-\nman evaluations (Castilho, 2021). Furthermore,\nevaluations of LLM translators are especially dif-\nficult due to data contamination (Aiyappa et al.,\n2023; Chang et al., 2023), as it is unclear whether\nthe models are pretrained on existing benchmarks\n(e.g., from WMT). We fill this gap by first col-\nlecting paragraphs from recently-published literary\ntranslations. Then, we provide human translators\nwith two candidate machine translations of a given\nsource paragraph and ask them to (1) mark error\nspans and categorize them based on a predefined\n3At least 55% of the reference target paragraphs used\nin our study split or merge sentences from the source text\n(measured with an automatic sentence tokenizer).\n4We broadly define a paragraph as a distinct passage\nwithin the novel, focusing on a single theme.\nschema inspired by MQM (Lommel et al., 2014b;\nFreitag et al., 2021), (2) make preference judg-\nments of which of the two translations is of higher\nquality, and (3) provide free-form justifications of\ntheir preference judgments. In total, we collect\nsuch annotations on 720 pairs of translated para-\ngraphs across 18 different language pairs (using\nthree diverse target languages of English, Japanese,\nand Polish), which we then leverage for a fine-\ngrained analysis of the behavior of different LLM\ntranslation methods.\nLLMs produce better translations when pro-\nvided with paragraph-level context: Our evalu-\nations reveal that using GPT-3.5 to translate com-\nplete paragraphs via few-shot prompting ( PARA )\nyields translations of significantly higher quality\nthan both the sentence-by-sentence GPT-3.5 meth-\nods (SENT , PARA _SENT ) as well as Google Trans-\nlate. Our detailed analysis of annotated translation\nerrors and free-form comments shows that PARA\nexhibit increased coherence, better preservation of\nliterary style, and improved handling of context-\ndependent expressions (see Figure 2). That said,\nPARA makes many critical mistranslations and\nother errors across different language pairs, which\nshows that LLM-based translators still have signifi-\ncant room to improve, particularly when translating\ncontextually-rich literary texts.\n2 Background\nBefore describing our dataset and evaluation, we\nfirst contextualize our work within the recent body\nof research on translation via large language mod-\nels. We also survey the broader body of document-\n421\nlevel5 MT research in §A.\nTranslation with large language models: LLM-\nbased translation is attractive because a single\nmodel, without training or fine-tuning on large par-\nallel corpora, can produce high-quality translations\nacross many language pairs.6 Recent work explores\nLLMs’ capabilities in this space (Wang et al., 2023)\nspanning paragraph-level post-editing with LLMs\n(Thai et al., 2022), translating sentence-level inputs\n(Vilar et al., 2022; Jiao et al., 2023), analyzing hal-\nlucinations in LLM-generated translations (Guer-\nreiro et al., 2023), and employing LLMs to eval-\nuate machine translation (Kocmi and Federmann,\n2023). Simple sentence-level English prompt tem-\nplates have been found effective for paragraph\ntranslations (Zhang et al., 2023), and automatically-\ngenerated dictionaries can assist LLM-based trans-\nlation (Ghazvininejad et al., 2023; Lu et al., 2023)\nalong with selecting high-quality demonstrations\n(Vilar et al., 2022). To the best of our knowledge,\nthe only prior work other than ours that evalu-\nates LLMs for paragraph-level translation is Hendy\net al. (2023), who conduct automatic evaluation of\ncontext-aware sentence-by-sentence translation; in\ncontrast, we perform a fine-grained human evalua-\ntion of paragraph-level translation.\n3 Data & methods\nOur work differs from existing research on trans-\nlating with large language models in two key ways:\nwe focus on translating literary text at the para-\ngraph level. In this section, we describe and moti-\nvate the paragraph-level translation dataset used\nin our study, which covers 18 unique language\npairs (three target languages) and is sourced from\nrecently-published novels. Then, we outline the\ndifferent ways in which we leverage GPT-3.5 to\ntranslate these paragraphs at both the sentence and\nparagraph levels.\n3.1 Dataset collection\nLiterary texts (e.g., novels or short stories) pose\nunique challenges for translators due to their com-\nplex nature. Translators must interpret and honor\nthe author’s voice with no objective reality to mea-\nsure against, which can result in several equally\n5Note that the term “document-level” has been used in\nMT research to denote both multi-sentence passages as well\nas complete documents.\n6That said, parallel data is almost certainly included in\nLLM pretraining data, at least for high-resource languages\n(Briakou et al., 2023).\nvalid translations (Sager, 1998). For machine trans-\nlation systems, these challenges exacerbate the\nneed for discourse-level context (Thai et al., 2022):\nan author’s intended meaning or style is often un-\nclear from just a single sentence.\nSelecting paragraphs from novels: How good\nare machines at translating literary paragraphs? To\nanswer this question, we extract 20 paragraphs (di-\nalogues and narrative texts) each from 18 recently-\npublished translations of novels, and we manually\nalign these paragraphs with corresponding para-\ngraphs in the source novel 7 (see Table 8 in §B).\nAlmost all of the translations were published af-\nter 2021 (see Table 7 in §B), which is important\nto avoid data contamination with LLM pretrain-\ning data (Aiyappa et al., 2023; Chang et al., 2023).\nIn sum, we obtain 360 aligned source-target para-\ngraphs, which we use for all of the experiments\ndescribed in the rest of the paper.\nData memorization issue: In order to investigate\nthe extent to which text-davinci-003 may have\nmemorized the novels in our dataset, we employ\nthe prompts from (Chang et al., 2023) and assess\nthe model’s ability to produce masked characters’\nnames. For this purpose we select 171 translation\nparagraphs, which contained character’s names, re-\nsulting in an average of 8 out of 20 paragraphs used\nper book. In nearly all instances, the model was un-\nable to accurately produce the correct names, with\nthree exceptions. Two of these were names of well-\nknown historical figures, \"Napoleon Bonaparte\"\nand \"Simonides of Ceos.\" A closer examination\nrevealed that these names could likely be inferred\nfrom the context, rather than being a result of the\nmodel’s memorization. In the third instance the\nmodel produced the correct name but in diminutive\ninstead of augmentative form (“Kasia” instead of\n“Ka´ska”).8\nAdditionally, we tested text-davinci-003\nwith a randomly selected subset of paragraphs from\nour dataset. In these cases, the model was unable\nto generate accurate completions.\n7We purchase the source ebook and its corresponding\ntranslation before extracting aligned paragraphs.\n8Kasia/Ka´ska\" are both forms of “Katarzyna,” the second\nmost common female name in Poland as of January 2023. This\nraises a question of whether the model’s response was due\nto memorization or an educated guess based on the name’s\npopularity ( https://www.statista.com/statistics/\n1089014/poland-most-popular-female-names/ ).\n422\nParagraph length: All paragraphs consist of at\nleast two sentences, and the majority of them are\nbetween four to nine sentences long (mean=7.45,\nstd=4.14).9 As automatic sentence tokenizers are\nnot always reliable for all of the languages consid-\nered in our study, we manually perform sentence\ntokenization to enable a direct comparison of sen-\ntence and paragraph-level translation systems. For\nmore details about the dataset statistics, including\ntoken and sentence counts, see §B, which also in-\ncludes data on sentence numbers obtained using a\nsentence tokenizer.\nSource and target languages: As source lan-\nguages, we select eight languages that belong to\ndifferent language families, have varied morpho-\nlogical traits, and employ different writing systems:\nEnglish (en), Polish (pl), Russian (ru), Czech (cs),\nFrench (fr), German (de), Japanese (ja), and Chi-\nnese (zh). As target languages, we select English,\nJapanese, and Polish, as they also vary greatly in\ntheir morphology, grammar, and writing systems.\nThe detailed rationale can be found in §B.\n3.2 Translation with large language models\nIn this paper, we focus on translating the liter-\nary paragraphs in our dataset using large language\nmodels. More specifically, we use the GPT-3.5\ntext-davinci-003 checkpoint, which has been\nfurther tuned to follow instructions based on hu-\nman feedback (Ouyang et al., 2022). Hendy et al.\n(2023) demonstrate that GPT-3.5 produces transla-\ntions of reasonable quality, though their focus was\nmainly at the sentence level. Since many LLMs,\nincluding GPT-3.5 , are only accessible via black-\nbox APIs, we adapt the model for translation via\nin-context learning (Brown et al., 2020).\nDemonstration examples: We use few-shot\nprompting, in which a model is provided with a\nprompt consisting of five demonstrations. We man-\nually curate the five demonstrations from literary\ntexts for each of the 18 language pairs, resulting\nin 90 total demonstration examples. These demon-\nstrations are sourced from novels that are not part\nof our translation dataset, resulting in potential dif-\nferences in topic and style (see Table 9 in the §B\nfor details). We further ensure that each set of\n9A paragraph with fewer sentences is not necessarily\nshort: for example, in the German novel “An Inventory of\nLosses,” sentences can be as long as 70 to 80 words, with the\nlongest reaching 117 words. The distribution of sentences in\nparagraphs is provided in Figure 7 in §B.\nfive demonstrations includes both dialogues and\nnarrative texts.\nPrompting for translation: We consider the fol-\nlowing three prompting strategies for GPT-3.5 that\nallow us to compare the model’s abilities to trans-\nlate with and without discourse-level context (see\nTable 1 for templates and §C for the exact prompts):\n• GPT-3.5 sentence-level translation without\ncontext (SENT ): Each sentence of the para-\ngraph is translated in isolation of the others.\nTo maintain consistency, we provide the same\nfive sentence-level examples10 in each prompt\nfor the given source-target language pair.11\n• GPT-3.5 sentence-level translation with\ncontext (PARA _SENT ): Each sentence of the\nparagraph is translated in context. The model\nis provided with the entire source paragraph\nas input, where the sentence to be translated is\nwrapped in <translate> and </translate>\ntags, in addition to a partially-translated tar-\nget paragraph. The demonstrations are also\npresented with the same tags. For each demon-\nstration in the prompt, a sentence in a different\nposition was chosen (e.g., from the beginning,\nmiddle, and end of the paragraph).\n• GPT-3.5 paragraph-level translation\n(PARA ): The entire source paragraph is\npassed into the model, and the output target\nparagraph is generated conditioned on this\ninput (i.e., without any sentence tokenization).\nDemonstrations in the prompt are also para-\ngraphs12 of translations from the respective\nsource language into the target language in\nquestion.13\n10Sentence-level demonstrations for SENT are sampled\nfrom the demonstrations for paragraph-level translation.\n11To ensure consistent quotation mark usage and enable a\nfair comparison with paragraph-level translations, quotation\nmarks in sentence-level translations were manually adjusted.\n12The examples for PARA and PARA _SENT configurations\nare necessarily lengthier. Due to the GPT-3.5 maximum con-\ntext size, it is not always possible to include all five examples\nwithin the prompt. Consequently, around 10% of the data was\ntranslated using four or fewer examples.\n13Initially, we experimented with GPT-3 by translating\nbetween two non-English languages using English as a pivot,\nas it is the primary language of the model. The model had\naccess to the source text and its English translation. After\nmanual evaluation and comparison to translations without\na pivot language, we found no significant benefit in using\nEnglish as the pivot. Consequently, we directly translated\nparagraphs into the target language. Refer to §H for details\nand results of this preliminary study.\n423\nSENTDemonstration Template\nOriginal text in [SRC LANG]:\nsource sentence\nTranslation into [TRG LANG]:\ntarget sentence\nPARA_SENTDemonstration Template\nOriginal text in [SRC LANG]:\nsource sentence 1<translate>source sentence 2</translate>source sentence 3\nTranslation into [TRG LANG]:\ntarget sentence 1<translated>target sentence 2</translated>\nPARADemonstration Template\nOriginal text in [SRC LANG]:\nsource paragraph\nTranslation into [TRG LANG]:\ntarget paragraph\nTable 1: Prompt templates for SENT , PARA _SENT , and\nPARA . The source text to translate and expected target\noutputs are underlined.\nUsing Google Translate ( GTR) as a baseline:\nIn order to compare commercial-grade translation\nsystems to LLM translators, we also translate all\nparagraphs in our dataset using Google Translate.14\nWe opt for an off-the-shelf commercial system\ninstead of a state-of-the-art system from, for in-\nstance, WMT competitions for two primary rea-\nsons. First, our experiments focus on literary trans-\nlations. Given that WMT systems are predomi-\nnantly evaluated on the news domain, it is uncer-\ntain which system would perform best, and some\nlanguage pairs may not even be supported. Sec-\nond, our main research question revolves around\nLLMs’ ability to incorporate contextual informa-\ntion, rather than merely comparing their perfor-\nmance with state-of-the-art translation systems. We\nemploy GTR as a reasonably robust baseline to as-\nsess the extent to which context can enhance MT\nquality, rather than asserting that LLMs outperform\nall traditional MT systems.\n4 Evaluating document-level literary\ntranslation\nHow do we compare the translation quality of the\nsystems described above? Automatic metrics such\nas BLEURT and COMET are untested on document-\nlevel inputs as well as literary texts, and as such\nwe do not consider them reliable, although we do\n14All paragraphs were translated in January 2023 using\nthe GoogleTranslate API. The system was provided entire\nparagraphs, which it likely partitioned and translated sentence-\nby-sentence.\nSource Text: Une autre photo, signée du même\nphotographe...\ngrammar mistranslation untranslated \ninconsistency register format \nAny omissions or additions?\nError Annotation\nTranslation 1: \nInny zdjęcie, podpisane przez tego samego fotografa...\nWhich translation is better?\nWhy is it better? I prefer T1 to T2 as it doesn't...\nTranslation 2: \nInne zdjęcie, podpisane tym samym fotografem...\nRead the source text.\nMark and categorize\nspan-level errors\nBinary choice\n(yes/no)\nBinary choice\n(translation 1 vs 2)\nBinary choice\n(confident vs unsure)\nJustify preference\nWas it significantly better?\nFigure 3: A description of the annotation process for a\npair of candidate translations given a source paragraph.\nNote that our hired translators go through this pipeline\nfor three different pairs per source paragraph, comparing\nPARA with SENT , PARA _SENT , and GTR.\nreport them in §G.15 Human evaluation is equally\nproblematic, as direct assessments of translation\nquality (e.g., “rate the quality of this translation\nfrom 0-100”) suffer from calibration issues that\nare exacerbated with longer texts (Karpinska et al.,\n2021). Thus, we opt for a human evaluation in-\nspired by Multidimensional Quality Metrics (Lom-\nmel et al., 2014b, MQM ), in which annotators\nmark and classify error spans within the transla-\ntion. Specifically, for each of the 18 language pairs\nstudied in this work, we hire translators to iden-\ntify all span-level errors in two competing trans-\nlations. For each evaluated pair, the annotators\nwere also asked to choose the better translation and\nprovide a free-form rationale. For each source para-\ngraph, the translators make three binary judgments\nof which translation is higher quality: SENT vs\nPARA , PARA _SENT vs PARA , and GTR vs PARA .\nRecruiting annotators: As our task is complex\nand requires fluency in both the source and target\nlanguage, we hire translators to provide the anno-\ntations. We recruit 13 translators via the Upwork\nfreelancing platform,16 each of whom is a native\nspeaker of English, Polish, or Japanese. 17 One\n15Automatic metrics developed specifically for document-\nlevel MT are also insufficient as they either work best with\none-to-one sentence level alignments (Vernikos et al., 2022;\nHendy et al., 2023) or are available only for English (Jiang\net al., 2022).\n16https://www.upwork.com/\n17The annotators for Czech-Polish and Russian-English\nwere both native speakers of the respective source languages\n424\ntranslator, hired directly, was a bilingual speaker\nof English and Polish with advanced knowledge of\nGerman; as such, she performed the pl-en, de-en,\nand de-pl evaluations. Evaluation of ja-pl, pl-ja,\nand pl-en texts was done by the first author in a col-\nlaboration with native speakers of Polish/Japanese\nto avoid any potential bias. Each translator was\npaid $2 per evaluated pair of candidate translations,\nwith an additional $5 bonus to cover the time spent\nfamiliarizing themselves with the instructions. We\nasked them to compare three pairs of system trans-\nlations (PARA vs. SENT , PARA vs. PARA _SENT ,\nPARA vs. GTR) for 10 paragraphs per language\npair18; as such, 180 total source paragraphs were\nused in our evaluations. Altogether, we paid ap-\nproximately $12 per hour, with a total cost of $955.\nAnnotation task: First, we tasked the hired trans-\nlators19 with annotating a subset of MQM transla-\ntion errors identified through a pilot analysis and\nannotation of the system’s outputs. Specifically, we\nask them to highlight spans within the candidate\ntranslations that contain errors belonging to any of\nthe following error categories:\n• mistranslation: 20 accuracy errors that occur\nwhen the wrong target word or phrase is cho-\nsen to represent content from the source text.\nIn addition to canonical mistranslations, we\nalso include overly literal translation errors\nthat occur when systems nonsensically trans-\nlate word-by-word into the target language.\n• grammar: grammatical errors, such as er-\nrors in conjugation, declension, or wrong\nprepositions.\n• untranslated: words or phrases that should\nhave been translated into the target language\nand highly proficient in their respective target languages. They\ncollaborated with native speakers of the target languages, who\npossessed a basic understanding of the source language, to\ncomplete their annotations.\n18These paragraphs were randomly sampled from the 360\nparagraphs. The entire set of 360 paragraphs was used for the\nautomatic evaluation described in §G.\n19They were presented with guidelines in their native lan-\nguage. The annotation task was performed using the Label-\nStudio annotation tool (Tkachenko et al., 2020-2022). See\nFigure 11 for the screenshot of the interface.\n20We note that mistranslations in literary text are often not\nas grave as, for instance, in news articles. Human translators\nhold poetic license, which allows them to change some details\nto make the text more enjoyable for the reader. Is changing\n“bonito” into “tuna” incorrect? Or can it be perceived as a way\nto accommodate an English-speaking readership that is likely\nmore familiar with the latter?\nbut were either left in the source language or\njust transliterated into the target language.\n• inconsistency: use of different terms to refer\nto the same entity, or different words where\nthe same word should be used for stylistic\nreasons (e.g., “Kasia” and “Kate,” “coat” and\n“jacket,” or “bad” and “awful” ).\n• register: a clear violation in the use of for-\nmal and informal language within the same\ntext, only annotated in Japanese.21\n• format: incorrect usage of punctuation (e.g.,\n\".\" instead of \"。\").\nAfter the span-level annotation is complete, we\nthen ask the translators to further identify if any\nof the candidate translations contains significant\ncontent additions or omissions in relation to the\nsource text.22 Finally, they are asked to choose the\nbetter translation and provide a justification for\ntheir choice in two to five sentences. We instruct\nthem to additionally mark whether their chosen\ntranslation is significantly superior, or if the deci-\nsion was difficult because both translations are of\nroughly comparable quality (see Figure 3 and §D\nfor details).\n5 Results\nIn this section, we compare our different literary\ntranslation methodologies using both automatic\nmetrics and aggregate statistics from the human\nevaluations. Overall, we observe that thePARA con-\nfiguration outperforms competing methods across\nall evaluations and language pairs. These results\ndemonstrate that GPT-3.5 effectively leverages\nparagraph-level context to produce better transla-\ntions than sentence-level methods, and also that the\nless efficient sentence-by-sentence translation with\ncontext is (PARA _SENT ) is unnecessary to achieve\nhigh translation quality.\n5.1 Human evaluation also favors P ARA\nFigure 5 contains human preference results com-\nparing PARA to SENT , PARA to PARA _SENT , and\n21We only annotate cases where the level of formality\nchanges abruptly within the same paragraph. It is possible that\na given character would be more likely to use formal language\nbut an informal language is being employed. As long as this\nis consistent we do not consider it an error as this cannot be\nfully determined from the paragraph context.\n22Note that this task was simplified to a binary choice –\neither there were serious omissions/additions or not. We did\nnot ask the annotators to further annotate them due to the time\nrestrictions.\n425\nFigure 4: The distribution of translator preference judg-\nments between sentence-level translation ( SENT ) and\nparagraph-level translation (PARA ). PARA is preferred\n(i.e., more votes) in every language pair except de-ja,\nfr-en and de-en.\nPARA to GTR, aggregated across all 18 language\npairs studied in this paper (i.e., 180 votes per sys-\ntem comparison). Table 11 breaks down these re-\nsults for each language pair, and we observe the\nsame trends for the vast majority of pairs. Overall,\nthe translators significantly favored PARA transla-\ntions over the alternatives (p<.001, binomial test).\nTable 2 contains specific information about gram-\nmar and mistranslation errors split across the three\ntarget languages (see Table 16 and Table 17 for\ndetails), which we refer to in the discussion below.\nPARA is clearly better than SENT : PARA is pre-\nferred by translators over SENT at a rate of 71.67%\n(p<.001, 95% CI [0.645, 0.781]). Additionally,\nwhen translators preferred PARA , they were usually\nconfident in the decision (i.e., it was clearly better\nthan SENT ); even if we exclude all “unsure” votes,\nthe preference for PARA translations remains sig-\nnificant at 79.44% (p<.001, 95% CI [0.705, 0.866]).\nThe only language pair in which SENT is favored\nover PARA is de-ja (see Figure 4).23 Overall, SENT\nproduces 31% more mistranslations, 48.6% more\ngrammar errors, 15 times more inconsistencies, and\n3.5 times more register errors (Table 2).\nPARA is clearly better thanGTR: PARA transla-\ntions are overwhelmingly preferred over those from\nGoogle Translate (GTR), with an 83.33% prefer-\nence rate (p<.001, 95% CI [0.771, 0.885]). In the\n23This could be because the German novel An Inventory\nof Losses in our dataset contains the longest sentences of any\nbook (45 tokens per sentence), and thus the intra-sentence\ncontext is likely more informative than in other books.\nFigure 5: The number of votes for SENT vs PARA ,\nPARA _SENT vs PARA , and GTR vs PARA along with\nrater confidence ( confident or unsure). PARA is pre-\nferred to all competing methods. All differences are\nstatistically significant at p<.001 (binomial test).\nfr-ja, pl-ja, zh-ja, and cs-pl language pairs, PARA\nreceived all of the ten votes over GTR. Overall,\nGTR translations result in 58.18% more mistransla-\ntions, 35.24% more grammatical errors, over seven\nas many inconsistency errors, and ten times more\nregister errors (see Table 2). §E contains more\nfine-grained comparisons of these two systems.\nPARA is slightly preferred over PARA _SENT :\nOur evaluations show that PARA is better than\nPARA _SENT , but the gap is smaller than it is for\nthe other two methods. PARA is still preferred at\na 66.67% rate ( p<.001, 95% CI [0.593, 0.735]).\nBoth PARA and PARA _SENT produce a compara-\nble number of mistranslations (483 vs 462), gram-\nmar errors ( 105 vs 113), and inconsistencies ( 2\nvs 3) (see Table 2). While PARA _SENT leaves\naround 22% more words untranslated, it appears\nto leverage the contexts and even occasionally se-\nlects better equivalents in the target language, as\nevidenced by translator comments. One major is-\nsue with PARA _SENT is that it occasionally repeats\nsentences, whereas PARA never does so.\n6 Analyzing translation errors\nThe aggregate statistics from the previous section\nconfirm that PARA -level translation via GPT-3.5\nis the strongest literary translator of the methods\nthat we study. Translations produced by PARA\nare favored by both automatic metrics and human\ntranslators, and it makes fewer errors than compet-\ning methods. In this section, we dive deeper into\nspecific types of errors that are made within each\nhigh-level category (e.g., grammar, mistranslation),\nand we present examples of errors associated with\n426\nTYPE TRGLANGPARASENTPARA_SENTGTR\nMISTRANSLATIONEN 88 109 82 155JA 224 295 223 334PL 171 229 157 275TOTAL 483 633 462 764\nGRAMMAR EN 5 20 9 18JA 43 49 38 65PL 57 87 66 59TOTAL 105 156 113 142\nINCONSISTENCYEN 0 5 0 1JA 1 7 2 7PL 1 19 1 7TOTAL 2 31 3 15\nUNTRANSLATEDEN 13 5 14 6JA 23 30 33 24PL 23 16 25 4TOTAL 59 51 72 34\nREGISTER EN 0 0 0 0JA 7 25 13 71PL 0 0 0 0TOTAL 7 25 13 71\nFORMAT EN 0 n/a n/a 1JA 0 n/a n/a 117PL 0 n/a n/a 8TOTAL 0 n/a n/a 126\nTable 2: Total counts of all of the types of mistakes\nmade by each of the four systems from our annotation.\nOverall, models with access to paragraph-level context\ncommit fewer translation errors.\nlack of context understanding made by SENT and\nGTR that are fixed by PARA .\n6.1 Language-specific grammatical errors\nWe analyze the types of grammatical errors that\nare made by the studied translation methods in all\nthree target languages. 24 In summary, although\nGPT-3.5 is primarily trained on English, it is com-\npetitive with GTR at Polish and Japanese grammar\nproficiency. In fact, PARA generates the fewest\ngrammatical errors of any system, with a total of\n97 for both languages, in contrast to 136 errors\nmade by SENT , 101 errors by PARA _SENT , and\n122 errors by GTR (see Table 2). That said,none of\nthese systems delivers translations devoid of gram-\nmatical inaccuracies, even for English.\nEnglish: Perhaps not surprisingly, translations\ninto English contain fewer grammatical mistakes\nthan Japanese or Polish (see Table 2). The most\nprominent mistakes in English are incorrect articles,\nwhich is most frequent with SENT and GTR. This\nis to be expected, as the choice between the definite\nand indefinite article in English depends heavily\non the context. Other mistakes include wrong or\nomitted prepositions, wrong parts of speech, and\nincorrect word order (see Table 17).\n24There are some differences in the paragraph lengths be-\ntween the three target languages that should be taken into\nconsideration when analyzing raw numbers. However, the\ngeneral tendencies remain intact.\nFigure 6: Quantification of mistranslations resulting\nfrom missing or misinterpreted paragraph-level context\nin PARA , SENT , PARA _SENT , and GTR systems, or-\nganized by the target language (Japanese, Polish, and\nEnglish).\nJapanese: Translations into Japanese contain\nconsiderably more mistakes. Most notably, the\nsystems struggle with the correct choice of particle:\nPARA and SENT produce twice as many mistakes\nin this regard than PARA _SENT and GTR (see Ta-\nble 17). Other mistakes include incorrect tense,\nverb finite form within the sentence, or incorrect\nword order, the latter of which is much more fre-\nquent in GTR than any of the GPT-3.5 translations.\nPolish: GPT-3.5 exhibits more difficulty with\nPolish grammar than English or Japanese across\nall prompting strategies (see Table 2). It frequently\ngenerates incorrect gender, case, or prepositions\n(see Table 17). We also observe instances in which\nGPT-3.5 alters the gender of a noun, such as pro-\nducing grilla, a non-existent feminine form, in\nplace of the masculine grill, while accurately mod-\nifying all adjectives and verbs to match the novel\nfeminine noun.25\n6.2 Context-related errors\nWe manually classify all annotated mistransla-\ntions (2,324 instances) into subcategories, several\nof which include instances where the absence of\ndiscourse-level context is clearly a contributing fac-\ntor (see Table 16 for detailed classification).26 We\nalso further analyze all translations in terms of\ncontent-related issues. Overall, we observe that\ncontext is indeed incorporated into the translations\n25It is worth noting that grilla can also be also the genitive\nform of the masculine noun grill; however, the agreement\nof surrounding verbs and adjectives with the feminine noun\nsuggests that the system likely treated the word as feminine.\n26The initial classification was conducted on the first ver-\nsion of the dataset. After incorporating small corrections, we\nidentified 18 more mistranslations that were not part of this\nanalysis.\n427\nTYPESOURCE GPT-3.5 SENTTRANSLATIONGPT-3.5 PARATRANSLATIONCOMMENT\nPRONOUNS\nРоманы, как известно, печаталисьна разной бумаге[paper].И горетьона[she]может по-разному.\n—RUSSIANSOURCE(fromManaraga)\nRomany, jak wiadomo, drukowanona ró˙znym papierze [paper]. I mo˙zeona [she] t˛ eskni´c na ró˙zne sposoby.\n—GPT-3.5 SENT(POLISH)\nJak wiadomo, powie´sci drukowanona ró˙znym papierze [paper]. I mo˙zeon [he] pali´c si˛ e na ró˙zne sposoby.\n—GPT-3.5 PARA(POLISH)\n“Paper” is a feminine noun in Russian and referred to as “she,” whereasit is a masculine noun in Polish and should be referred to as “he,” as inPARA. The absence of context inSENTleads to an incorrect translation.\nCULTURALNUANCES\n「気が付かなくてすみません」「いやいや、(...).古倉さんは毎日勤務なのに手を抜かないからねー！」[lit.Ms. Furukura works every day]\n—JAPANESESOURCE(fromConvenience Store Woman)\n“I’m sorry I didn’t notice.”“No, no, (...). Furukura-san workshard every day without taking anyshortcuts!”\n—GPT-3.5 SENT(ENGLISH)\n“I’m sorry I didn’t notice.”“No, no, (...). You work every day,but you never slack off!”\n—GPT-3.5 PARA(ENGLISH)\n“Furukura-san” or “Miss Furukura” in the last source sentence is usedinstead of the second-person “you” as per Japanese convention. Trans-lating this sentence without context into English results in a confusingtranslation (SENT) that implies that the speaker refers to some other“Furukura” rather than their listener.PARAcorrectly translates “Furukura”as “you.”\nELLIPSIS\n„Ne, ted’ udˇeláš nádobí!“ [(you) will dothe dishes!]„Neudˇelám!“ [(I) won’t do!]„Udˇeláš!“ [(You) will do!]\n—CZECHSOURCE(fromCrows)\n— Nie, teraz zrobisz zmywanie![(you) will do the washing]— Nie zrobi˛ e! [(I) won’t do!]— Zrobisz to! [(You) will do it!]\n—GPT-3.5 SENT(POLISH)\n— Nie, teraz umyjesz naczynia[(You) will wash the dishes]!— Nie umyj˛ e [(I) won’t wash]!— Umyjesz [(You) will wash]!\n—GPT-3.5 PARA(POLISH)\nCzech uses the same collocation as English, “dothe dishes,” which isinvalid in Polish. Hence, the ellipses in the last two sentences in thesource text require a broader context to be translated correctly.PARAdoes it properly, translating both as “wash,” whileSENTunsurprisinglyfails to choose the correct collocation.\nSUBJECTELLIPSIS\nWhen we were done, the lipstick wentback into some mother’s Fendi handbag.We watched her apply it, unaware.\n—ENGLISHSOURCE(fromA Children’s Bible)\nGdy sko´nczyli´smy, szminka wróciłado jakiej´s torebki Fendi nale˙z ˛ acejdo matki. Patrzyli´smy, jak to robi,nie´swiadomi [unaware (we)] tego.\n—GPT-3.5 SENT(POLISH)\nKiedy sko´nczyli´smy, szminkawróciła do torebki Fendi jakiej´smatki. Patrzyli´smy, jak j ˛ a nakłada,nie´swiadoma [unaware (she)]naszych działa´n.\n—GPT-3.5 PARA(POLISH)\nOnly from the broader context we can deduce that “unaware” refers tothe mother, not the “we” (referring to children) watching her.PARAcorrectly attributes the state of being “unaware” to the mother, which isexhibited by its usage of the singular feminine form of the adjective. Incontrast,SENTmistranslates it using the plural masculine form of theadjective “unaware,” which implies that it refers to “we” rather than the“mother.”\nCONSISTENCY\nAlles zu vergessen, ist gewiss schlimm[bad]. Noch schlimmer [worse] ist,nichts zu vergessen (. . . ).\n—GERMANSOURCE(fromAn Inventory of Losses)\nすべてを忘れることは確かに悲惨な[tragic]ことです。さらに悪い[worse]のは、何も忘れないことです。\n—GPT-3.5 SENT(JAPANESE)\nすべてを忘れることは確かに悪い[bad]ことです。もっと悪い[worse]ことは、何も忘れないことです。\n—GPT-3.5 PARA(JAPANESE)\nThe German source translates into English as “To forget everything isbad, certainly.Worsestill is to forget nothing.” It is arguably importantfor the translation to repeat the same word which is an equivalent of theGerman “schlimm” (“bad”).PARAdoes it well, translating both as悪い, or “bad,” whileSENTuses two different words, “tragic” and “bad”which results in inconsistent translation.\nPOLYSEMY\nВсе прошло хорошо. Книга прочи-тана идеально – не быстро и немедленно, минимум дыма. Класси-ка. Я был в форме[in shape].\n—RUSSIANSOURCE(fromMaranaga)\nWszystko poszło dobrze. Ksi ˛ a˙zkazostała przeczytana idealnie – nieszybko i nie wolno, minimalna ilo´s´cdymu. Klasyka. Byłem w mundurze[in uniform].\n—GPT-3.5 SENT(POLISH)\nWszystko poszło dobrze. Ksi ˛ a˙zkaprzeczytana idealnie – nie szybko inie wolno, minimalna ilo´s´c dymu.Klasyka. Byłem w formie [inshape].\n—GPT-3.5 PARA(POLISH)\nThe ambiguity stems here from multiple meanings of the Russian nounформа, which can mean either “shape” or “uniform.” Since one can be“in shape” as well as “in a uniform”, only from the context it becomesclear which meaning was intended by the author.PARAtranslates itcorrectly as “shape” while SENTmistranslates it as “uniform.”\nAPPROPRIATENESS\n「あー、あと煙草の５番を一つ」「かしこまりました」[lit.(I) un-derstood]\n—JAPANESESOURCE(fromConvenience Store Woman)\n“Ah, and one pack of cigarettes,number five.\"\"Understood.\"\n—GPT-3.5 SENT(ENGLISH)\n“Ah, and one pack of cigarettes,number five.”“Right away.”\n—GPT-3.5 PARA(ENGLISH)\nThis conversation is between a clerk and a customer. The Japaneseexpressionかしこまりましたis an honorific that literally means “un-derstood.” However, when choosing the best equivalent, the translatorneeds to consider the situation at hand to best reflect its meaning in thetarget language. “Understood” inSENTis technically correct, but it isan unfortunate word choice for the clerk to employ. On the other hand,“right away” inPARAfits much better in the context of this conversation.\nTable 3: Examples of different context-related issues observed in SENT translations, which are fixed in the\ncorresponding PARA translations. Phrases that exemplify these issues are highlighted in purple, and English glosses\nare provided in [square brackets].\nfor both PARA and PARA _SENT outputs, which\nresults in fewer context-dependent issues (see Fig-\nure 6). More specifically, we observe that PARA\nproduces translations that leverage the context re-\nsulting in mostly correct translations of pronouns,\nellipsis, cultural nuances, and polysemous words\nand phrases; Table 3 contains specific examples\nand discussion of each. PARA is also more con-\nsistent and appropriate in vocabulary usage than\nSENT . All cases are further analyzed in §E.2.\n7 Conclusion\nIn this paper, we demonstrate that LLMs leverage\nparagraph-level context to produce translations that\nare more coherent and enjoyable than sentence-by-\nsentence translation while containing fewer mis-\ntranslations and grammatical issues. Our evalu-\nations reveal that professional translators prefer\nparagraph-level translations over both sentence-\nlevel translations produced by the same language\nmodel, and also to those generated by an off-\nthe-shelf commercial system ( GTR). We release\nour dataset and error annotations to help facilitate\nthe development of new evaluation methodologies\nand automatic metrics for document-level machine\ntranslation. Finally, a full-length novel extends far\nbeyond the confines of paragraph-level translation.\nIn future work, we will focus on integrating individ-\nual paragraphs into cohesive chapters, which can\nthen be expanded to encompass the entire novel.\n8 Limitations\nSo far, we have shown that GPT-3.5 leverages\nparagraph-level context to produce translations that\n428\nare better than those produced by sentence-level\ncounterparts (SENT vs PARA ). However, there are\nstill many issues with PARA ’s translations. From\nthe annotations and translators’ comments, we ob-\nserve that PARA suffers from occasional omissions\nof content from the source paragraph to a greater\nextent than SENT and GTR (see §D). Moreover,\nPARA still makes a sizeable number of mistrans-\nlations and grammatical errors, though fewer than\nSENT or GTR. These issues seem to be only par-\ntially mitigated by employing GPT-4 (see §F).\nFinally, it is important to acknowledge that the\nlanguages covered in the current study are either\nmid or high-resource. Performance might be much\nworse when translating from or into a low-resource\nlanguage such as Zulu or Armenian.27\nEthical considerations\nTranslating with LLMs: The rise of large lan-\nguage models has also brought many ethical con-\ncerns to the forefront of NLP research (Blodgett\net al., 2020; Bender et al., 2021). LLMs encode bi-\nases and exhibit toxicity, and these behaviors can be\nexacerbated by unconstrained prompting (Gehman\net al., 2020; Costa-jussà et al., 2022). Further ethi-\ncal concerns arise in the context of machine trans-\nlation, particularly literary translation, where mul-\ntiple stakeholders – the author, the translator, and\nthe audience – are involved (Taivalkoski-Shilov,\n2019a). Low-quality output can influence the per-\nception of the author’s work, impair the reader’s lin-\nguistic abilities, and hinder the transfer of ideas to\nthe target language, while overrelying on machine\ntranslation can possibly threaten the role of human\ntranslators (Drugan, 2013; Ning and Domínguez,\n2016; Taivalkoski-Shilov, 2019a). On the other\nhand, machine translation employed responsibly\nas an auxiliary tool holds the potential to alleviate\nthe translator’s cognitive burden (O'Brien, 2012)\nand make the author’s work accessible to a broader\naudience more swiftly (Besacier, 2014). Contrary\nto the predictions in Eloundou et al. (2023), we do\nnot view large language models as a substitute for\nhuman translators, but rather as a means to assist\ntranslators in their work.\nHuman Evaluation: The experiments involving\nhuman translators were reviewed by the IRB, and\nall involved translators gave their written consent\n27For instance, our initial experiments with translations\ninto low-resource languages show that GPT-3.5 (ChatGPT)\nsuffers from repetition when translating into Hausa.\nto disclose their annotations, comments, and pref-\nerence choices. In recognizing contributions, our\nacknowledgments only include the names of those\ntranslators who explicitly gave their consent to be\nacknowledged by their full name in this publica-\ntion.\nData Copyrights: We use and make public only\nabout 2% of the text from each of the original\nnovels. This number was determined after con-\nsulting domain experts at the HathiTrust ( https:\n//www.hathitrust.org/) and qualifies as fair use\n(up to 10% of a text can generally be considered\nfair use).\nAcknowledgements\nFirst and foremost, we would like to express our\ngratitude to the translators hired mostly on Upwork:\nMalgorzata Szymczak (fr-pl), Kinga Przekota (ru-\npl), Michal Sikora (cs-pl), Paula Kurzawska (de-pl,\nde-en, pl-en), Kristy Darling Finder (fr-en), Timo-\nthy Shostak (ja-en), Shun Enoki (zh-ja), Takanori\nKurokawa (fr-ja), Yoshiko Kikawa (en-ja), Shin-\nnosuke Kasahara (ru-ja), and all those who wish\nto remain anonymous. We encourage any machine\ntranslation researchers working on these language\npairs to contact these translators for human evalua-\ntions.\nWe would also like to show our appreciation to\nJan Wislicki, Tom Gally, Nader Akoury, Kalpesh\nKrishna, Simeng Sun, Katherine Thai, and the en-\ntire UMass NLP group for insightful discussion,\nwhich helped to shape this project.\nFurthermore, we would like to express our grati-\ntude to the reviewers for their constructive feedback\nand valuable suggestions.\nFinally, we would like to thank Sergiusz Rzep-\nkowski (pl), Paula Kurzawska (pl, en), Hiroshi Iida\n(ja), Grégory Fleurot ( fr), Peyton Bowman ( en),\nSimeng Sun (zh), Igor Zapala (pl, de), Marvin Hoff-\nmann (de), Kinga Przekota (pl, ru), and Yuki Mori\n(ja) for further consultations on their respective\nnative languages.\nThis project was partially supported by awards\nIIS-1955567 and IIS-2046248 from the National\nScience Foundation (NSF) as well as an award from\nOpen Philanthropy.\nReferences\nRuchit Agrawal, Marco Turchi, and Matteo Negri. 2018.\nContextual Handling in Neural machine Translation:\n429\nLook Behind, Ahead and on Both Sides. In 21st\nAnnual Conference of the European Association for\nMachine Translation, pages 11–20.\nRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-\nYeol Ahn. 2023. Can we trust the evaluation on\nChatGPT?\nR.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.\nMixed-effects modeling with crossed random effects\nfor subjects and items. Journal of Memory and Lan-\nguage, 59(4):390–412.\nDouglas Bates, Martin Mächler, Ben Bolker, and Steve\nWalker. 2015. Fitting Linear Mixed-Effects Mod-\nels Using lme4. Journal of Statistical Software ,\n67(1):1–48.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\nDangers of Stochastic Parrots: Can Language Mod-\nels Be Too Big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nLaurent Besacier. 2014. Machine translation for litter-\nature: a pilot study (traduction automatisée d’une\noeuvre littéraire: une étude pilote) [in French]. In\nProceedings of TALN 2014 (Volume 2: Short Papers),\npages 389–394, Marseille, France. Association pour\nle Traitement Automatique des Langues.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (Technology) is\nPower: A Critical Survey of “Bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nBilingual Book Maker. 2023. Make bilingual\nepub books Using AI translate (GitHub).\nhttps://github.com/yihong0618/bilingual_\nbook_maker. [Accessed 05-Apr-2023].\nEleftheria Briakou, Colin Cherry, and George Foster.\n2023. Searching for needles in a haystack: On the\nrole of incidental bilingualism in palm’s translation\ncapability.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMarine Carpuat and Michel Simard. 2012. The Trouble\nwith SMT Consistency. In Proceedings of the Sev-\nenth Workshop on Statistical Machine Translation,\npages 442–449, Montréal, Canada. Association for\nComputational Linguistics.\nSheila Castilho. 2021. Towards Document-Level human\nMT Evaluation: On the Issues of Annotator Agree-\nment, Effort and Misevaluation. In Proceedings of\nthe Workshop on Human Evaluation of NLP Systems\n(HumEval), pages 34–45, Online. Association for\nComputational Linguistics.\nKent K. Chang, Mackenzie Cramer, Sandeep Soni, and\nDavid Bamman. 2023. Speak, memory: An archae-\nology of books known to chatgpt/gpt-4.\nJunxuan Chen, Xiang Li, Jiarui Zhang, Chulun Zhou,\nJianwei Cui, Bin Wang, and Jinsong Su. 2020. Mod-\neling Discourse Structure for Document-level Neural\nMachine Translation. In Proceedings of the First\nWorkshop on Automatic Simultaneous Translation ,\npages 30–36, Seattle, Washington. Association for\nComputational Linguistics.\nAndrew Chesterman. 1997. Memes of Translation. Ben-\njamins Translation Library. Benjamins (John) North\nAmerica, Amsterdam, Netherlands.\nTrevor Cohn and Mirella Lapata. 2007. Machine trans-\nlation by triangulation: Making effective use of multi-\nparallel corpora. In Proceedings of the 45th Annual\nMeeting of the Association of Computational Lin-\nguistics, pages 728–735, Prague, Czech Republic.\nAssociation for Computational Linguistics.\nMarta R. Costa-jussà, Eric Smith, Christophe Ropers,\nDaniel Licht, Javier Ferrando, and Carlos Escolano.\n2022. Toxicity in Multilingual Machine Translation\nat Scale.\nChenchen Ding, Masao Utiyama, and Eiichiro Sumita.\n2014. Document-level re-ranking with soft lexical\nand semantic features for statistical machine transla-\ntion. In Proceedings of the 11th Conference of the\nAssociation for Machine Translation in the Americas:\nMT Researchers Track, pages 110–123, Vancouver,\nCanada. Association for Machine Translation in the\nAmericas.\nJoanna Drugan. 2013. Quality in professional transla-\ntion. Bloomsbury Advances in Translation. Contin-\nuum Publishing Corporation, New York, NY .\nTyna Eloundou, Sam Manning, Pamela Mishkin, and\nDaniel Rock. 2023. GPTs are GPTs: An Early Look\nat the Labor Market Impact Potential of Large Lan-\nguage Models.\nYukun Feng, Feng Li, Ziang Song, Boyuan Zheng, and\nPhilipp Koehn. 2022. Learn to Remember: Trans-\nformer with Recurrent Memory for Document-level\nMachine Translation. In Findings of the Association\nfor Computational Linguistics: NAACL 2022, pages\n1409–1420, Seattle, United States. Association for\nComputational Linguistics.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.\nExperts, Errors, and Context: A Large-Scale Study of\nHuman Evaluation for Machine Translation. Trans-\nactions of the Association for Computational Linguis-\ntics, 9:1460–1474.\n430\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating Neural Toxic Degeneration\nin Language Models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sel-\nlam. 2022. Repairing the Cracked Foundation: A\nSurvey of Obstacles in Evaluation Practices for Gen-\nerated Text.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based Phrase-level Prompt-\ning of Large Language Models for Machine Transla-\ntion.\nNuno M. Guerreiro, Duarte Alves, Jonas Waldendorf,\nBarry Haddow, Alexandra Birch, Pierre Colombo,\nand André F. T. Martins. 2023. Hallucinations in\nLarge Multilingual Translation Models.\nChao Han. 2020. Translation quality assessment: a\ncritical methodological review. The Translator ,\n26(3):257–273.\nChristian Hardmeier. 2012. Discourse in Statistical\nMachine Translation. Discours, (11).\nChristian Hardmeier, Sara Stymne, Jörg Tiedemann,\nand Joakim Nivre. 2013. Docent: A Document-level\nDecoder for Phrase-Based Statistical Machine Trans-\nlation. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 193–198, Sofia, Bulgaria.\nAssociation for Computational Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How Good Are GPT Models at\nMachine Translation? A Comprehensive Evaluation.\nKlára Jágrová and Tania Avgustinova. 2023. Intelli-\ngibility of highly predictable polish target words in\nsentences presented to czech readers. In Compu-\ntational Linguistics and Intelligent Text Processing,\npages 110–125. Springer Nature Switzerland.\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine transla-\ntion benefit from larger context?\nYuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong\nZhang, Jian Yang, Haoyang Huang, Rico Sennrich,\nRyan Cotterell, Mrinmaya Sachan, and Ming Zhou.\n2022. BlonDe: An Automatic Evaluation Metric\nfor Document-level Machine Translation. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1550–1565, Seattle, United States. Association for\nComputational Linguistics.\nYuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dong-\ndong Zhang, Mrinmaya Sachan, and Ryan Cotterell.\n2023. Discourse-centric evaluation of document-\nlevel machine translation with a new densely anno-\ntated parallel corpus of novels. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7853–7872, Toronto, Canada. Association for Com-\nputational Linguistics.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is ChatGPT A Good\nTranslator? Yes with GPT-4 As The Engine.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat WMT 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nXiaomian Kang, Yang Zhao, Jiajun Zhang, and\nChengqing Zong. 2020. Dynamic context selection\nfor document-level neural machine translation via\nreinforcement learning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2242–2254, On-\nline. Association for Computational Linguistics.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The Perils of Using Mechanical Turk to Eval-\nuate Open-ended Text Generation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1265–1285, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMarzena Karpinska, Nishant Raj, Katherine Thai, Yix-\niao Song, Ankita Gupta, and Mohit Iyyer. 2022.\nDEMETR: Diagnosing Evaluation Metrics for Trans-\nlation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9540–9561, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nTom Kocmi and Christian Federmann. 2023. Large\nLanguage Models Are State-of-the-Art Evaluators of\nTranslation Quality.\nAlexandra Kuznetsova, Per B. Brockhoff, and Rune\nH. B. Christensen. 2017. lmerTest Package: Tests in\nLinear Mixed Effects Models. Journal of Statistical\nSoftware, 82(13):1–26.\nRussell V . Lenth. 2023.emmeans: Estimated Marginal\nMeans, aka Least-Squares Means. R package version\n1.8.5.\nArle Lommel, Maja Popovic, and Aljoscha Burchardt.\n2014a. Assessing inter-annotator agreement for trans-\nlation error annotation. Reykjavik, Iceland. Proceed-\nings of the 9th International Conference on Language\nResources and Evaluation (LREC 14).\n431\nArle Lommel, Hans Uszkoreit, and Aljoscha Burchardt.\n2014b. Multidimensional Quality Metrics (MQM) :\nA Framework for Declaring and Describing Transla-\ntion Quality Metrics. Tradumàtica, pages 0455–463.\nAntónio Lopes, M. Amin Farajian, Rachel Bawden,\nMichael Zhang, and André F. T. Martins. 2020.\nDocument-level neural MT: A Systematic Compari-\nson. In Proceedings of the 22nd Annual Conference\nof the European Association for Machine Translation,\npages 225–234, Lisboa, Portugal. European Associa-\ntion for Machine Translation.\nHongyuan Lu, Haoyang Huang, Dongdong Zhang, Hao-\nran Yang, Wai Lam, and Furu Wei. 2023. Chain-\nof-dictionary prompting elicits translation in large\nlanguage models.\nElman Mansimov, Gábor Melis, and Lei Yu. 2021. Cap-\nturing document context inside sentence-level neural\nmachine translation models with self-training. InPro-\nceedings of the 2nd Workshop on Computational Ap-\nproaches to Discourse, pages 143–153, Punta Cana,\nDominican Republic and Online. Association for\nComputational Linguistics.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947–2954, Brussels, Belgium. Association\nfor Computational Linguistics.\nLucía Molina and Amparo Hurtado Albir. 2004. Trans-\nlation techniques revisited: A dynamic and function-\nalist approach. Meta, 47(4):498–512.\nWang Ning and César Domínguez. 2016. Comparative\nliterature and translation: A cross-cultural and inter-\ndisciplinary perspective. In Yves Gambier and Luc\nvan Doorslaer, editors, Border crossings. Translation\nstudies and other disciplines, pages 287–308. John\nBenjamins.\nSharon O'Brien. 2012. Translation as human–computer\ninteraction. Translation Spaces, 1:101–122.\nOpenAI. 2022. Introducing ChatGPT.\nOpenAI. 2023. GPT-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nDorota Pawlak. 2023. ChatGPT for Translators: How\nto Use the Tool to Work More Efficiently?\nMatt Post and Marcin Junczys-Dowmunt. 2023. Escap-\ning the sentence-level paradigm in machine transla-\ntion.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nNatural Language Processing Toolkit for Many Hu-\nman Languages. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 Submission\nfor the Metrics Shared Task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Ana C Farinha, Chrysoula Zerva, Daan\nvan Stigt, Craig Stewart, Pedro Ramos, Taisiya\nGlushkova, André F. T. Martins, and Alon Lavie.\n2021. Are References Really Needed? Unbabel-\nIST 2021 Submission for the Metrics Shared Task.\nIn Proceedings of the Sixth Conference on Machine\nTranslation, pages 1030–1040, Online. Association\nfor Computational Linguistics.\nJuan C. Sager. 1998. What Distinguishes Major Types\nof Translation? The Translator, 4(1):69–89.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning Robust Metrics for Text Genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nKristiina Taivalkoski-Shilov. 2019a. Ethical issues\nregarding machine(-assisted) translation of literary\ntexts. Perspectives, 27(5):689–703.\nKristiina Taivalkoski-Shilov. 2019b. Free indirect dis-\ncourse: an insurmountable challenge for literary MT\nsystems? In Proceedings of the Qualities of Literary\nMachine Translation, pages 35–39.\nXin Tan, Longyin Zhang, Deyi Xiong, and Guodong\nZhou. 2019. Hierarchical modeling of global context\nfor document-level neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1576–\n1585, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKatherine Thai, Marzena Karpinska, Kalpesh Krishna,\nBill Ray, Moira Inghilleri, John Wieting, and Mohit\nIyyer. 2022. Exploring document-level literary ma-\nchine translation with parallel paragraphs from world\nliterature. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9882–9902, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nCraig Thomson, Ehud Reiter, and Barkavi Sundararajan.\n2023. Evaluating factual accuracy in complex data-\nto-text. Computer Speech & Language, 80:101482.\n432\nJörg Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nMaxim Tkachenko, Mikhail Malyuk, Andrey\nHolmanyuk, and Nikolai Liubimov. 2020-\n2022. Label Studio: Data labeling soft-\nware. Open source software available from\nhttps://github.com/heartexlabs/label-studio.\nAntonio Toral and Andy Way. 2015. Machine-assisted\ntranslation of literary text. Translation Spaces ,\n4(2):240–267.\nMasao Utiyama and Hitoshi Isahara. 2007. A compari-\nson of pivot methods for phrase-based statistical ma-\nchine translation. In Human Language Technologies\n2007: The Conference of the North American Chap-\nter of the Association for Computational Linguistics;\nProceedings of the Main Conference, pages 484–491,\nRochester, New York. Association for Computational\nLinguistics.\nGiorgos Vernikos, Brian Thompson, Prashant Mathur,\nand Marcello Federico. 2022. Embarrassingly Easy\nDocument-level MT Metrics: How to Convert Any\nPretrained Metric Into a Document-Level Metric. In\nProceedings of the Seventh Conference on Machine\nTranslation, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. When\na good translation is wrong in context: Context-aware\nmachine translation improves on deixis, ellipsis, and\nlexical cohesion. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1198–1212, Florence, Italy. Asso-\nciation for Computational Linguistics.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023.\nDocument-level machine translation with large lan-\nguage models.\nBiao Zhang, Ankur Bapna, Melvin Johnson, Ali Dabir-\nmoghaddam, Naveen Arivazhagan, and Orhan Firat.\n2022. Multilingual document-level translation en-\nables zero-shot transfer from sentences to documents.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4176–4192, Dublin, Ireland.\nAssociation for Computational Linguistics.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. BERTScore:\nEvaluating Text Generation with BERT.\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Towards making\nthe most of context in neural machine translation.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artificial Intelligence, IJCAI-20,\npages 3983–3989. International Joint Conferences on\nArtificial Intelligence Organization. Main track.\n433\nAppendix\nA Background\nIn this section of the appendix, we survey the exist-\ning approaches to document-level machine transla-\ntion, which do not involve prompting LLMs.\nExisting approaches to document-level transla-\ntion: Before the rise of neural machine transla-\ntion, several attempts were made to incorporate\ndiscourse-level phenomena into statistical machine\ntranslation systems (Hardmeier, 2012; Carpuat and\nSimard, 2012; Hardmeier et al., 2013; Ding et al.,\n2014). Neural MT systems condition sentence-\nby-sentence translation on discourse-level context\nvia concatenation models (Tiedemann and Scher-\nrer, 2017; Jean et al., 2017; Agrawal et al., 2018;\nJunczys-Dowmunt, 2019; Lopes et al., 2020), hi-\nerarchical models (Miculicich et al., 2018; Tan\net al., 2019; Chen et al., 2020; Zheng et al., 2020),\nmulti-pass models (Mansimov et al., 2021), dy-\nnamic context models (Kang et al., 2020), multi-\nsource models (Zhang et al., 2018; Feng et al.,\n2022), and transfer learning approaches (Zhang\net al., 2022). Despite sometimes obtaining clear\ngains from discourse-level context (V oita et al.,\n2019), the machine translation community has not\nmade much progress on this problem, particularly\nfor non-English language pairs, due largely to the\nscarcity of parallel document-level corpora (Zhang\net al., 2022). This problem has been partially ad-\ndressed by introducing a pivot language (Cohn and\nLapata, 2007; Utiyama and Isahara, 2007), but this\napproach can also lead to substantial information\nloss.\nB Dataset\nIn this section of the appendix, we first discuss\nthe rationale for the source and target language\nselection. Then we provide more details on the\nselection of the paragraphs. Finally, we provide\ndetails about the number of tokens and sentences\nin the source text and different translations.\nTarget language selection: We select English,\nJapanese, and Polish as the target languages of our\nstudy, as these languages differ considerably in\nmany linguistic aspects. English is an analytic lan-\nguage that is widely spoken and extensively studied\nin the field of natural language processing, and it\nserves as the primary pretraining language of most\nlarge language models, including GPT-3.5 .28 In\ncontrast, both Japanese and Polish are compara-\ntively under-explored. Japanese is an agglutinative\nlanguage that employs three distinct writing sys-\ntems: Kanji, Hiragana, and Katakana. As a high-\ncontext language, the translation of Japanese texts\nnecessitates a profound comprehension of context\nand cultural nuances, rendering it a compelling\nchoice for testing the limits of LLMs’ translation\ncapabilities. Polish, on the other hand, is a fusional\nlanguage characterized by a rich morphological sys-\ntem. Its complex word forms, grammatical gender,\nconjugation, and declension make it an apt choice\nfor testing the accuracy and robustness of LLMs.29\nSource language selection: As source languages,\nwe select English ( es), Polish ( pl), Russian ( ru),\nCzech ( cs), French ( fr), German ( de), Japanese\n(ja), and Chinese (zh). These languages belong to a\ndiverse array of language families – Indo-European\n(Romance, Germanic, Slavic), Sino-Tibetan, and\nJaponic – each with distinctive morphological traits\n– fusional, agglutinative, and analytic. Moreover,\nthey employ a variety of writing systems such as\nthe Latin alphabet, the Cyrillic alphabet, Hanzi, and\nKanji/Hiragana/Katakana (see Table 4 for details).\nFinally, we carefully select source-target language\npairs to ensure that our study encompasses both\nlinguistically similar and dissimilar languages. For\nexample, we paired cs-pl, as these languages are\ncharacterized by only 10% lexical distance30 and\nhave similar syntactic structures (Jágrová and Av-\ngustinova, 2023). Conversely, we also includeja-pl,\nas the two languages have very little lexical over-\nlap, vastly different grammars, and utilize distinct\nwriting systems.\nChoosing paragraphs: The selection of a par-\nticular paragraph was semi-random, with certain\nconsiderations in mind during the sampling process.\nWe prioritized the following criteria: (1) for each\n28As of 2020, the reported distribution of languages\nfeatured in the present study within the GPT-3 training\ndata was as follows: English – 92.647% (1st), French –\n1.818% (2nd), German – 1.469% (3rd), Russian – 0.188%\n(9th), Polish – 0.155% (11th), Japanese – 0.111% (15th),\nChinese – 0.099% (17th), Czech – 0.071% (18th) (see\nhttps://github.com/openai/gpt-3/blob/master/\ndataset_statistics/languages_by_word_count.csv).\nThe current GPT-3.5 text-davinci-003 model is reported\nto incorporate data up to June 2021 and it is unclear what\ntexts or languages were added to the original training data\nhttps://platform.openai.com/docs/models/gpt-3-5.\n29The first author is fluent in all three target languages.\n30i.e., the percentage of non-cognates in the language pair.\n434\nsource language we sample paragraphs so that there\nis a combination of dialogue and narrative texts; (2)\nthe paragraph should be reasonably intelligible to\na human translator without additional context; and\n(3) alignment between the source paragraph and\nhuman translation should be feasible, meaning no\nmajor content rearrangement across paragraphs.\nNonetheless, meeting all these requirements was\nnot always possible. For instance, the source text\nof Convenience Store Woman(ja) is mostly written\nin the first-person narrative. Since Japanese does\nnot encode the speaker‘s gender in the verb forms,\nit is often impossible to determine whether the nar-\nrator is a male or a female. In cases where it was\nimpossible to determine the gender of the character\nwe instructed translators to accept either option,\nprovided that the translation remained consistent\nwithin the given paragraph (i.e., the gender did not\nchange within the paragraph).\nDataset statistics: The dataset used for this study\ncontains 360 source paragraphs with their corre-\nsponding human translations.31 We further report\nthe following statistics: (1) the number of sentences\nin the source text, as per manual sentence tokeniza-\ntion, along with the number of tokens in the sources\nand text and each translation (Table 5), (2) the num-\nber of sentences in the source text, human trans-\nlation, and each machine translation as tokenized\nwith SPACY (Table 6), and (3) the distribution of\nsentences in paragraphs (Figure 7).\nFigure 7: Distribution of sentences in the sampled para-\ngraphs. The paragraphs were sentencized manually.\nC Prompting for Translation\nD Human Evaluation\nIn this section, we provide some further details\nabout the human evaluation with a focus on the\nerror annotation. First, discuss the issue of subjec-\ntivity in error annotation. Next, we explain some\nchoices we had to make when annotating “incon-\nsistency” and “format” errors. Finally, we present\nsome details about the translators hired for the eval-\nuation task.\nError annotation: Annotating and classifying\nerrors in translations is inherently subjective (Lom-\nmel et al., 2014a; Han, 2020). For instance, trans-\nlating French “corsage” (“bodice”) as a “blouse”\ncan be seen as either a mistranslation or a permissi-\nble deviation from the original text; this is, in fact,\nhow the “corsage” was translated by the human\ntranslator in our data.\nFurthermore, sometimes there are multiple ways\nof annotating errors (Thomson et al., 2023). Con-\nsider the following example:\n(1) We had to hide the running, though, in case our haste\nbetrayed us, so truer to say we slipped out quietly.\nWhen one of my parents appeared, my technique\nwas: pretend to catch sight of someone in the next\nroom. Move in a natural manner toward this\nfigment of my imagination, making a purposeful\nface.\n—ENGLISH SOURCE (from A Children’s Bible)\nThe translation of the last sentence in (1) into\nPolish as an imperative can be considered a mis-\ntranslation. We would hypothesis that the system\nmisinterpreted the source as an imperative form.\nHowever, using the infinitive form of the verb in\nthe translation is less clear and raises questions\nabout whether it is a mistranslation or a grammat-\nical error. The distinction between the two lies in\nthe point at which the mistake was made. If the\noriginal sentence was understood correctly but the\nresulting translation was ungrammatical, then it\nis a grammatical error. On the other hand, if the\nuse of the infinitive form resulted from interpret-\ning “move” as an infinitive, it may be considered a\nmistranslation as well.\nInconsistency: For marking the “inconsistency”\nerrors we decided to the take minimal approach.\nFor instance, is the same person is referred to in the\ntranslation as both “Piotr” and “Peter” we would\n31See Table 7 for the list of novels included in the dataset\nand Table 8 for examples of aligned paragraphs.\n435\nLANGUAGELANGUAGEFAMILY MORPHOLOGICALFEATURES WRITINGSYSTEM\nENGLISH Indo-European (Germanic) Analytic Latin Alphabet\nGERMAN Indo-European (Germanic) Fusional Latin Alphabet\nFRENCH Indo-European (Romance) Fusional Latin Alphabet\nPOLISH Indo-European (Slavic) Fusional Latin Alphabet\nCZECH Indo-European (Slavic) Fusional Latin Alphabet\nRUSSIAN Indo-European (Slavic) Fusional Cyrillic\nJAPANESE Japonic Agglutinative Kanji / Hiragana / Katakana\nCHINESE Sino-Tibetan Analytic Hanzi\nTable 4: Details on languages included for the current study.\nLANG #SENT SRC HUM PARA SENT PARA_SENT GTR\ncs-pl 163 2,154 2,027 2,122 2,123 2,259 2,065\nde-pl 153 3,172 2,997 2,785 2,899 2,835 2,764\nru-pl 170 2,350 2,471 2,467 2,463 2,458 2,375\nja-pl 111 2,627 1,855 1,782 1,907 1,830 1,800\nen-pl 127 1,702 1,526 1,444 1,513 1,483 1,462\nfr-pl 119 3,253 2,789 2,641 2,673 2,654 2,543\nde-ja 75 3,530 5,329 4,807 5,116 4,652 4,703\nen-ja 176 1,959 2,617 2,538 2,653 2,617 2,634\nzh-ja 194 2,998 4,124 3,861 4,249 3,957 3,978\nru-ja 193 2,539 4,753 3,982 4,348 4,088 3,921\nfr-ja 195 2,510 3,426 3,110 3,355 3,106 2,958\npl-ja 188 1,953 2,944 3,083 3,418 3,199 2,972\nja-en 111 2,622 2,293 2,062 2,322 2,257 2,140\npl-en 148 2,696 3,430 3,234 3,290 3,273 3,213\nru-en 117 1,693 2,008 2,029 2,056 2,028 2,019\nfr-en 120 3,253 3,123 3,067 3,150 3,064 3,098\nde-en 153 3,172 3,346 3,361 3,413 3,325 3,314\nzh-en 127 2,235 2,002 2,427 2,396 2,351 2,360\nTotal 2,640 46,418 53,060 50,802 53,344 51,436 50,319\nTable 5: Number of sentences in the source text sentencized manually (#SENT ) along with the number of tokens in\nthe human reference (HUM) and different machine translations (PARA , SENT , PARA _SENT , GTR). All translations\nwere tokenized using SPACY32 with the large model for each of the three target languages (Polish, Japanese, and\nEnglish). All source texts were tokenized with STANZA (Qi et al., 2020) as SPACY does not include models for all\ntarget languages.\nFigure 8: An example of prompt for SENT translations with one demonstration and a text to translate.\n436\nLANG SOURCE TARGET PARA SENT PARA_SENT GTR\ncs-pl 168 177 167 169 181 168\nde-en 155 182 166 167 164 155\nde-ja 69 133 135 121 117 132\nde-pl 155 170 166 167 169 157\nen-ja 169 168 166 161 169 169\nen-pl 131 127 130 132 130 131\nfr-en 122 138 126 122 124 123\nfr-ja 193 199 207 220 185 201\nfr-pl 122 125 125 125 126 123\nja-en 101 120 116 116 116 111\nja-pl 101 127 117 115 118 108\npl-en 148 156 149 145 151 145\npl-ja 189 153 174 196 178 191\nru-en 123 119 121 124 121 123\nru-ja 144 155 158 161 164 196\nru-pl 168 172 170 171 172 172\nzh-en 127 130 146 141 140 135\nzh-ja 195 234 225 229 215 202\nTOTAL 2,580 2,785 2,764 2,782 2,740 2,742\nTable 6: Number of sentences in the source text and each translation. The data was sentencized with SPACY. As\nevident from the data and manual inspection of translations the translations may result in a very different number\nof sentences as a result of splits and merges. We observe that about 55% of the data potentially lacks one-to-one\ncorrespondence.\nFigure 9: An example of prompt for PARA _SENT translations with one demonstration and a text to translate.\nmark only the one that is less frequent. If “Piotr”\nappears once in the paragraph, while “Peter” is\nused twice, “Piotr” would be annotated as being\ninconsistent. The same strategy was applied for\n“register” errors, such as when both polite and ca-\nsual forms were acceptable, but the translation used\nthem randomly.\nFormat: We did not label “format” errors for\nthe SENT and PARA _SENT translations, as we\nmanually corrected the quotation marks during\npost-processing of the translations. This man-\nual correction was done to ensure that SENT and\nPARA _SENT could be compared to PARA without\nrelying too heavily on simple heuristic (i.e., incor-\nrect usage of the quotation marks).\nTranslators: The translators in this study were\nhired on a freelancing platform, Upwork. We in-\nterviewed all translators prior to the task to assure\n437\nLANGUAGE YEARPUBLISHEDBOOK TITLE AUTHOR TRANSLATOR(S) SOURCETARGETTRANSLATIONORIGINAL\nA Children’s Bible Lydia Millet Aga Zano en pl 2022 2020\nWhat Can You See From HereMariana Leky Agnieszka Walczy de pl 2021 2017\nThe Years Annie Ernaux Krzysztof Jarosz & fr pl 2022 2008\nMagdalena Budzi´nska\nManaraga Wladimir Sorokin Agnieszka Lubomira Piotrowskaru pl 2018 2017\nCrows Petra Dvorakova Mirosław´Smigielski cs pl 2020 2020\nConvenience Store WomanSayaka Murata Dariusz Lato´s ja pl 2019 2016\nSixteen Horses Greg Buchanan Fuji Yoshiko en ja 2022 2021\nAn Inventory of LossesJudith Schalansky Naoko Hosoi de ja 2022 2018\nDear Reader Paul Fournel Kei Takahashi fr ja 2022 2011\nThe Shooting Party Anton Chekhov Takuya Hara ru ja 2022 1884\nSword of Destiny Andrzej Sapkowski Yasuko Kawano pl ja 2022 1992\nBare burial Fang Fang Shin’ichi Watanabe zh ja 2022 2016\nWhat Can You See From HereMariana Leky Tess Lewis de en 2021 2017\nThe Years Annie Ernaux Alison L. Strayer fr en 2017 2008\nThe Story of a Life Konstantin Paustovsky Douglas Smith ru en 2022 1956\nThe Books of Jacob Olga Yokarczuk Jennifer Croft pl en 2022 2014\nConvenience Store WomanSayaka Murata Ginny Tapley Takemori ja en 2018 2016\nCocoon Zhang Yueran Jeremy Tiang zh en 2022 2018\nTable 7: Details of the translated novels used in our study. In cases where the same novel is used for multiple target\nlanguages (e.g., “The Years”), identical source paragraphs are extracted to enable comparisons across language\npairs. These novels exhibit distinct differences beyond just their source languages. For instance, “What Can You\nSee From Here” presents a philosophical exploration of life and death, while “Sword of Destiny” is a fantasy story\npart of “The Witcher” saga.\nFigure 10: An example of prompt for PARA translations with one demonstration and a text to translate.\nthat they were qualified to evaluate the translations.\nAll translators were highly proficient in the source\nlanguage and most of them were native speakers\nof the target language with some being bilingual.33\n33We consider a translator bilingual only if they were\nraised using both languages; i.e. both can be consider their\nnative languages (e.g., ru-pl translator was raised in Poland\nwhile speaking Russian at home). In the broader sense of this\nword, all of the translators are bilingual with some of them\nbeing trilingual. For the cases where the hired translator was\nnot a native speaker of the target language, the annotations\nwere verified by a native speaker of the target language in\nOnly one translator reported familiarity with the\nbook, which translation she evaluated. All transla-\ntors were instructed to evaluate each paragraph in\nisolation without relying on any prior knowledge\nabout the book and to allow for all possible inter-\npretations based on the given part of the source text.\nThey were asked to evaluate five translations first\nand received feedback on their work before moving\nforward. Details about the translators are reported\nconsultation with the translator.\n438\nBook Lang Pair Source Target\nAn Inventory of Losses de-jaNatürlich hatte ich schon davor andere bemerkenswerte Begräb-nisstätten besucht: die Toteninsel San Michele etwa, wie sie mithohen, roten Backsteinmauern aus dem blaugrünen Wasser derLagune von Venedig emporragt gleich einer uneinnehmbaren Fes-tung, oder das grelle Jahrmarktstreiben des Hollywood ForeverCemetery am alljährlich von der mexikanischen Bevölkerung be-gangenen Día de los Muertos mit den orange-gelb geschmücktenGräbern und den von der fortgeschrittenen Verwesung auf ewigzum Grinsen verdammten Totenschädeln aus bunt gefärbtem Zuckerund Pappmaché. Doch keine hat mich so berührt wie der Friedhofjener Fischersiedlung, in dessen eigentümlichem Grundriss — einerArt Kompromiss aus Kreis und Quadrat ich nichts anderes als einSinnbild der ungeheuerlichen Utopie zu erkennen glaubte, die ichdort verwirklicht sah: mit dem Tod vor Augen zu leben. Lange Zeitwar ich überzeugt, an diesem Ort, dessen dänischer Name »kleineInsel« oder »vom Wasser umgeben« bedeutet, sei man dem Lebennäher, gerade weil seine Bewohner die Toten wortwörtlich in ihreMitte geholt hatten, anstatt sie wie sonst in unseren Breitengradenüblich — aus dem Innersten der Gemeinden vor die Stadttore zuverbannen, auch wenn der urbane Raum sich die Gräberstättendurch sein ungehemmtes Anwachsen oft nur wenig später wiedereinverleibt hat.\nもちろんそれ以前にもいくつか特筆すべき墓所を訪れたことはあった。たとえばヴェネツィアの干潟の青緑色の水中から、赤煉瓦の高い壁に囲まれて難攻不落の要塞のようにそびえたつ死者の島、サン・ミシェル。あるいはメキシコ系住民が毎年にぎやかに死者の日を祝う、ハリウッド・フォーエバー墓地。墓はオレンジと黄色の花で飾られ、カラフルな砂糖菓子や張り子細工の頭蓋骨は、腐敗が進んで永遠の笑顔を浮かべているようだ。けれども、この漁師町の墓地ほどに私の心を動かす墓所はなかった。まるで円と四角の間の妥協のようなその独特の輪郭に、私はまさにユートピアの象徴を見たように思った。死を目の前にしつつ生きるというユートピアが、そこに実現されていた。長いこと私は確信していた。デンマーク語で「小さな島」とか「水に囲まれた」という意味の名前を持つこの場所に住む人々は、同じくらいの緯度の国々で通常行われているように、共同体の内部から市門の外へと死者たちを追放する代わりに、死者たちを文字通り町の中心に迎え入れた。だからこそ、より生に近いのだと。もっとも都市空間もまた人口膨張のために、ほどなくして墓地をふたたび内部へと取り込まざるを得なくなるのだけれど。\nA Children’s Bible en-pl The lady urinated.“Oh, poor old thing, she has a nervous bladder!” exclaimed some-one’s chubby mother. “Is that a Persian rug?”Whose mother was it? Unclear. No one would cop to it, of course.We canceled the performance.“Admit it, that was your mother,” said a kid named Rafe to a kidnamed Sukey, when the parents had filed out. Some of their goblets,highball glasses, and beer bottles were completely empty. Drained.Those parents were in a hurry, then.“No way,” said Sukey firmly, and shook her head.“Then who is your mother? The one with the big ass? Or the onewith the clubfoot?”“Neither,” said Sukey. “So fuck you.”\nDama si˛ e posikała.– Och, biedactwo, ma wra˙zliwy p˛ echerz! – wykrzykn˛ eła czyja´spulchna matka. – Zaraz, to perski dywan?Czyj ˛ a matk ˛ a była? Nie wiadomo. Oczywi´scie nikt nie chciał si˛ eprzyzna´c. Odwołali´smy przedstawienie.– No dawaj, to twoja – powiedział chłopiec imieniem Rafe dodziewczynki imieniem Sukey, kiedy rodzice sobie poszli. Zostawilipo sobie kieliszki, wysokie szklanki i butelki po piwie. Niektórebyły zupełnie puste. Do ostatniej kropelki.Tym z rodziców si˛ e zatem spieszyło.– W˙zyciu – odparła Sukey stanowczo i pokr˛ eciła głow ˛ a.– To która? Ta z wielkim dupskiem? Czy ze szpotaw ˛ a stop ˛ a?– Ani jedna, ani druga. Spierdalaj.\nTable 8: Examples of aligned reference source and target paragraphs from our dataset, including both a narrative\n(An Inventory of Losses) and a dialogue (A Children’s Biblie). Our PARA approach takes as input the entire source\nparagraph and outputs a paragraph-level translation.\nYEARPUBLISHEDLANGPAIR TITLE AUTHOR TRANSLATOR(S) TRANSLATIONORIGINAL\nja-pl Norwegian Wood Haruki Murakami Dorota Marczewska & 1987 2006\nAnna Zieli´nska-Elliott\nde-pl The Trial Franz Kafka Jakub Ekier 1925 2008\nfr-pl Les Miserables Victor Hugo Krystyna Byczewska 1862 1966\nfr-pl The Little Prince Antoine de Saint-Exupéry Jan Szwykowski 1862 1967\nen-pl The Valley of Fear Arthur Conan Doyle Tadeusz Evert 1915 1927\nru-pl War and Peace Leo Tolstoy Andrzej Stawar 1869 1958\ncs-pl War with Newts Karel ˇCapek Jadwiga Bułakowska 1936 1949\npl-ja Solaris Stanisław Lem Mitsuyoshi Numano 1961 2004\nru-ja Anna Karenina Leo Tolstoy Hakuy ¯o Nakamura 1878 2004\nde-ja Der Steppenwolf Hermann Hesse Fujio Nagano 1927 2000\nfr-ja Around the World in 80 Days Jules Verne Y ¯u Takano 1873 2009\nen-ja Animal Farm George Orwell Eitar ¯o Sayama 1945 1998\nzh-ja Medicine Lu Xun K ¯obai Inoue 1919 1919\nzh-ja The True Story of Ah Q Lu Xun K ¯obai Inoue 1921 1923\nzh-ja Diary of a Madman Lu Xun K ¯obai Inoue 1921 1923\nru-en Confession Leo Tolstoy Peter Carson 1882 2013\nzh-en The Day the Sun Died Yan Lianke Carlos Rojas 2015 2018\nja-en Kokoro Natsume S ¯oseki Edwin McClelan 1914 1957\nja-en Kokoro Natsume S ¯oseki Meredith McKinney 1914 2010\nde-en Venus in Furs Ritter von Leopold Sacher-Masoch Fernanda Savage 1870 unclear\nfr-en The Debacle Émile Zola Leonard Tancock 1870 1972\nTable 9: List of novels employed in the prompts.\n439\nFigure 11: The annotation interface used for the error annotation task.\nLANGPAIR NATIVELANG BOOKFAMILIARITYGENDER\nzh-en Chinese ✗ Male\nja-en English ✗ Male\nde-en Polish/English ✗ Female\nfr-en English ✗ Female\nru-en Russian ✗ Female\npl-en Polish/English ✗ Female\nen-ja Japanese ✗ Female\nfr-ja Japanese ✗ Male\nde-ja Japanese ✗ Female\npl-ja Polish (author) ✗ Female\nru-ja Japanese ✗ Male\nzh-ja Japanese ✗ Male\nde-pl Polish/English ✗ Female\nen-pl Polish (author) ✗ Female\nru-pl Polish/Russian ✗ Female\ncs-pl Czech ✗ Male\nja-pl Polish (author) ✗ Female\nfr-pl Polish ✓ Female\nTable 10: Details about the translators hired for the\ncurrent annotation study. We note whether the translator\nwas familiar with the source text prior to the evaluation\ntask (Book Familiarity).\nin Table 10.34\nE Results\nIn this section of the appendix, we provide more\ndetailed analysis of the results of the human evalu-\n34Three language pairs (pl-ja, en-pl, ja-pl) were annotated\nby the first author of this paper.\nation. We start with providing more details about\nthe GTR vs PARA evaluation. Next, we include an\nin-depth discussion of the context-related errors in\nSENT which were corrected in the PARA transla-\ntions. Finally, we include some comments from the\ntranslators. In the next section (§F), we also pro-\nvide more information about the issues still present\nin the PARA translations along with the preliminary\nanalysis of paragraph-level translation by GPT-4.\nE.1 P ARA is clearly better than GTR\nPARA translations are overwhelmingly preferred\nover those from Google Translate (GTR), with an\n82.8% preference rate ( p<.001, 95% CI [0.765,\n0.880]). Even after removing the “unsure” votes,\nthe preference for PARA remains significant at\n88.0% (p<.001, 95% CI [0.812, 0.930]). In the\nfr-ja, pl-ja, zh-ja, and cs-pl language pairs, PARA\nreceived all of the ten votes over GTR. Part of\nthis advantage may be attributed to GTR some-\ntimes using English as a pivot language, which\ncan result in information loss. Our Czech trans-\nlator observed that mistakes in GTR translations\nsuggest the text was first translated into English.35\n35For the cs-pl language pair, we separately annotated\nmistranslations arising from pivot translation. These errors\naccounted for over 50% of all mistranslations in that lan-\nguage pair. The elimination of the need for parallel data may\ntherefore be beneficial for translating between lower-resource\n440\nOverall, GTR translations result in 57.7% more\nmistranslations, 37.3% more grammatical errors,\nover twice as many inconsistency errors, and ten\ntimes more register errors (see Table 2). Addition-\nally, GTR produced 125 format errors while PARA\nproduced perfect outputs in this regard. Finally,\nit is worth noting that GTR left fewer words un-\ntranslated, though this is inflated by the fact that in\none German text, the word “Bauer” (“farmer”) was\nuntranslated 14 times in the PARA translation.\nE.2 Context-related errors\nHere we present examples of context-related is-\nsues present in SENT while correctly translated by\nPARA .36\nPronouns: Unsurprisingly, the absence of dis-\ncourse context results in the incorrect translation\nof pronouns. Consider the following example, with\nEnglish glosses of important words provided in\n[brackets]:\n(2) И ветер[wind] то начинал шуметь в голых\nдеревьях, то замолкал, так же как и я при-\nслушиваясь к течению ночи. Но он[he] не\nуходил, он[he] был здесь.\n—RUSSIAN SOURCE (from The Story of a Life)\na. The wind would start to rustle in the bare trees\nand then fall silent, just as I listened to the flow\nof the night. But he didn’t leave, he was here.\n—GPT-3.5 S ENT (ENGLISH )\nb. The wind would start to rustle in the bare trees,\nthen die down, just like me, listening to the flow\nof the night. But it didn’t go away, it was still\nhere.\n—GPT-3.5 P ARA (ENGLISH )\nIn Russian, nouns have grammatical gender.\n“Wind” in the first sentence of the source text is\na masculine noun, so it is later referred to as “he”\nin (2). Without access to the context, the SENT\nmodel incorrectly translates it as “he” into English\n(2a), while the PARA translation correctly modifies\nthe pronoun to “it” (2b).\nWhen translating from Russian into Polish, an-\nother language with grammatical gender, we ob-\nserve issues when the gender of Russian and Polish\nnouns differs. Consider the following example:\n(3) Романы, как известно, печатались на разной\nбумаге [paper]. И гореть она[she] может по-\nразному.\n—RUSSIAN SOURCE (from Manaraga)\nlanguages where sufficient parallel data is often unavailable\nnecessitating the pivot translation.\n36Note that PARA also suffers from context-related issues.\nHowever, at a much lesser extent than SENT .\na. Romany, jak wiadomo, drukowano na ró˙znym\npapierze [paper]. I mo ˙ze ona [she] t˛ eskni´c na\nró˙zne sposoby.\n—GPT-3.5 S ENT (POLISH )\nb. Jak wiadomo, powie´sci drukowano na ró˙znym\npapierze [paper]. I mo ˙ze on [ he] pali´c si˛ e na\nró˙zne sposoby.\n—GPT-3.5 P ARA (POLISH )\nAlthough both Russian and Polish nouns possess\ngrammatical gender, “Paper” in (3) is feminine in\nRussian and referred to as “she,” whereas it is a\nmasculine noun in Polish and should be referred to\nas “he,” as in (3b). The absence of context in SENT\nleads to an incorrect translation in (3a).\nCultural nuances: Assigning appropriate pro-\nnouns without context becomes even more chal-\nlenging when translating from languages like\nJapanese, in which speakers frequently refer to the\nlistener (or themselves) in the third person rather\nthan using second-person personal pronouns such\nas “you” in English. Consider the following exam-\nple:\n(4) 「気が付かなくてすみません」\n「いやいや、(...)。古倉さんは毎日勤務なの\nに手を抜かないからねー！」\n[lit. Ms./Mrs./Mr. Furukura works every day]\n—JAPANESE SOURCE (from Convenience Store Woman)\na. “I’m sorry I didn’t notice.”\n“No, no, (...). Furukura-san works hard every\nday without taking any shortcuts!”\n—GPT-3.5 S ENT (ENGLISH )\nb. “I’m sorry I didn’t notice.”\n“No, no, (...). You work every day, but you never\nslack off!”\n—GPT-3.5 P ARA (ENGLISH )\nFrom the context of this conversation, a Japanese\nlistener can easily infer that “Furukura-san” or\n“Miss Furukura”37 in the last source sentence (4) is\nused instead of the second-person “you” as per\nJapanese convention. Translating this sentence\nwithout context into English, a language in which\nthird-person reference is not common, 38 results\nin a confusing translation (4a) that implies that\nthe speaker refers to some other “Furukura” rather\nthan their listener. However, when translating the\nsentence in context, the model correctly changes\n“Furukura” into “you” (4b), which makes it clear\nwhom the speaker refers to in English.\n37Note that the gender of neither character is apparent from\nthe fragment alone.\n38While third-person reference can be used in English, it is\nonly used in rare circumstances e.g. when addressing children.\n441\nLANGUAGEPAIR SENT PARA PARA_SENT PARA GTR PARA\nRussian - English 0 10 5 5 4 6\nChinese - English 1 9 3 7 3 7\nPolish - English 4 6 4 6 1 9\nFrench - English 5 5 4 6 2 8\nJapanese - English 1 9 2 8 1 9\nGerman - English 5 5 3 7 4 6\nTOTAL 16 44 21 39 15 45\nPERCENTAGE 26.67% 73.33% 35.00% 65.00% 25.00% 75.00%\nGerman - Japanese 6 4 3 7 1 9\nRussian - Japanese 4 6 4 6 2 8\nFrench - Japanese 2 8 1 9 0 10\nPolish - Japanese 2 8 4 6 0 10\nEnglish - Japanese 3 7 2 8 1 9\nChinese - Japanese 4 6 4 6 0 10\nTOTAL 21 39 18 42 4 56\nPERCENTAGE 35.00% 65.00% 30.00% 70.00% 6.67% 93.33%\nEnglish - Polish 0 10 3 7 4 6\nJapanese - Polish 3 7 5 5 1 9\nFrench - Polish 4 6 4 6 2 8\nCzech - Polish 3 7 2 8 0 10\nRussian - Polish 1 9 4 6 3 7\nGerman - Polish 3 7 3 7 1 9\nTOTAL 14 46 21 39 11 49\nPERCENTAGE 23.33% 76.67% 35.00% 65.00% 18.33% 81.67%\nTOTAL 51 129 60 120 30 150\nPERCENTAGE 28.33% 71.67% 33.33% 66.67% 16.67% 83.33%\nTable 11: The number of votes for SENT vs PARA , PARA _SENT vs PARA , and GTR vs PARA in human evaluation\nby the language pair. The winning counts are highlighted in purple.\nEllipsis: Another example where context helps is\nthe translation of elliptical constructions. Consider\nthe following example:\n(5) „Ne, ted’ udˇeláš nádobí!“ [(you) will do the dishes!]\n„Neudˇelám!“ [(I) won’t do!]\n„Udˇeláš!“ [(You) will do!]\n—CZECH SOURCE (from Crows)\na. — Nie, teraz zrobisz zmywanie! [(you) will do\nthe washing]\n— Nie zrobi˛ e! [(I) won’t do!]\n— Zrobisz to! [(You) will do it!]\n—GPT-3.5 S ENT (POLISH )\nb. — Nie, teraz umyjesz naczynia [(You) will wash\nthe dishes]!\n— Nie umyj˛ e [(I) won’t wash]!\n— Umyjesz [(You) will wash]!\n—GPT-3.5 P ARA (POLISH )\nCzech uses the same collocation as English, “do\nthe dishes” (5), which is invalid in Polish. Hence,\nthe ellipses in the last two sentences in (5) require\nbroader context to be translated correctly. PARA\ndoes it properly, translating both as “wash” (5b),\nwhile SENT unsurprisingly fails to choose the cor-\nrect collocation (5a).\nSubject ellipsis: Similarly, context may be\nneeded to attribute a state or an action to the correct\ncharacter due to the subject ellipsis. This is an ob-\nvious issue for languages like Japanese, which tend\nto omit the subject of the sentence and do not en-\ncode any relevant information in the verb form, but\nit can also arise in English. Consider the following\nexample:\n(6) When we were done, the lipstick went back into\nsome mother’s Fendi handbag. We watched her\napply it, unaware.\n—ENGLISH SOURCE (from A Children’s Bible)\na. Gdy sko´nczyli´smy, szminka wróciła do jakiej´s\ntorebki Fendi nale˙z ˛ acej do matki. Patrzyli´smy,\njak to robi, nie´swiadomi [unaware (we)] tego.\n—GPT-3.5 S ENT (POLISH )\nb. Kiedy sko´nczyli´smy, szminka wróciła do tore-\nbki Fendi jakiej ´s matki. Patrzyli ´smy, jak j ˛ a\nnakłada, nie´swiadoma [unaware (she)] naszych\ndziała´n.\n—GPT-3.5 P ARA (POLISH )\nFrom the second sentence alone it is not clear\nwho is “unaware” (6) – the mother or the “we” (re-\nferring to children) watching her. Only from the\n442\nbroader context can we confidently deduce that it\nis in fact the mother, not the children, who is “un-\naware.” PARA (6b) correctly attributes the state\nof being “unaware” to the mother, which is exhib-\nited by its usage of the singular feminine form of\nthe adjective. In contrast, SENT (6a) mistranslates\nit using the plural masculine form of the adjec-\ntive “unaware,” which implies that it refers to “we”\nrather than the “mother.”\nConsistency: Context is sometimes critical for\npreserving the overall consistency of the text. The\nsimplest cases include referring to the same entity\n– a place or a person – in the same way. More\ninteresting cases pertain to style and can enhance\nthe reader’s experience. Consider the following\nexample:\n(7) Alles zu vergessen, ist gewiss schlimm [bad]. Noch\nschlimmer [worse] ist, nichts zu vergessen (...).\n—GERMAN SOURCE (from An Inventory of Losses)\na. すべてを 忘れることは 確かに悲惨\nな[tragic]ことです。さらに悪い[worse]の\nは、何も忘れないことです。\n—GPT-3.5 S ENT (JAPANESE )\nb. すべてを忘れることは確かに悪い[bad]こ\nとです。もっと悪い[worse]ことは、何も\n忘れないことです。\n—GPT-3.5 P ARA (JAPANESE )\nThe German source in (7) translates into English\nas “To forget everything is bad, certainly. Worse\nstill is to forget nothing.”39 It is arguably important\nfor the translation to repeat the same word which\nis an equivalent of the German “schlimm” (“bad”).\nPARA does it well, translating both as悪い“warui,”\nor “bad” (7b), in the exact same way as the human\nJapanese translator. SENT , on the other hand, uses\ntwo different words, “tragic” and “bad” (7a), which\nwhile technically correct omits the intentional rep-\netition that is meant to introduce an unexpected\nconclusion.\nPolysemy: The absence of context makes it dif-\nficult to interpret words or expressions that have\nmultiple meanings in the source language. Con-\nsider the following example:\n(8) Все прошло хорошо. Книга прочитана иде-\nально – не быстро и не медленно, минимум\nдыма. Классика. Я был в форме[in shape].\n—RUSSIAN SOURCE (from Maranaga)\na. Wszystko poszło dobrze. Ksi ˛ a ˙zka została\nprzeczytana idealnie – nie szybko i nie wolno,\nminimalna ilo ´s´c dymu. Klasyka. Byłem w\nmundurze [in uniform].\n39Excerpt taken from the official English translation by\nJakie Smith (2020).\n—GPT-3.5 S ENT (POLISH )\nb. Wszystko poszło dobrze. Ksi ˛ a˙zka przeczytana\nidealnie – nie szybko i nie wolno, minimalna\nilo´s´c dymu. Klasyka. Byłem w formie [ in\nshape].\n—GPT-3.5 P ARA (POLISH )\nThe ambiguity stems here from multiple mean-\nings of the Russian noun форма “forma” (8),\nwhich can mean either “shape” or “uniform.” Since\none can be “in shape” as well as “in a uniform”, it\nis unclear from the sentence alone which meaning\nwas intended by the author. From the preceding\ncontext, it is clear that “everything went well” for\nthe narrator, who mastered the art of “book’n’grill,”\na unique form of expression exclusive to this fic-\ntional world. Based on this, we can infer that in\nthis instance, the term “forma” signifies “shape,” as\nin (8b), rather than “uniform,” as in (8a).\nAppropriateness: Finally, context may help to\nchoose the more appropriate equivalent for the\ngiven situation. Consider the following example:\n(9) 「あー、あと煙草の５番を一つ」\n「かしこまりました」 [lit. (I) understood]\n—JAPANESE SOURCE (from Convenience Store Woman)\na. \"Ah, and one pack of cigarettes, number five.\"\n\"Understood.\"\n—GPT-3.5 S ENT (ENGLISH )\nb. “Ah, and one pack of cigarettes, number five.”\n“Right away.”\n—GPT-3.5 P ARA (ENGLISH )\nThe conversation above is between a clerk and\na customer. The Japanese expression かしこまり\nました“kashikomarimashita” (9) is an honorific\nthat literally means “understood.” However, when\nchoosing the best equivalent, the translator needs\nto consider the situation at hand to best reflect its\nmeaning in the target language. “Understood” in\nSENT (9a) is technically correct, but it is an unfor-\ntunate word choice for the clerk to employ. On the\nother hand, “right away” in PARA (9b) fits much\nbetter in the context of this conversation. Had this\nbeen a series of commands (e.g., in a military con-\ntext) “understood” would be the more favorable\noption.\nE.3 What do translators think about P ARA ?\nTo wrap up this section, we provide a qualitative\nanalysis of the free-form comments written by\ntranslators to justify their preference judgments.\nOverall, the translators praise PARA for its more\nskillful use of rhetoric devices , and surpas[ing]\n443\nSENT as a literary rendition. They also mention\nthat PARA uses more of a poetic license but this\nmakes it stylistically much smoother than SENT .\nFurthermore, translators state that PARA clearly\nbetter reflects the content and style of the original\nwhen compared to GTR, and that it stays consistent\nwithin the paragraph. Inevitably, translations are\nnot flawless, and there are instances where both\ncompared systems fall short, as highlighted by one\nof the translators when assessing PARA against\nSENT : Nightmare, a mistake upon mistake (...)\nDespite all these mistakes, I can understand the\n[PARA ] translation better but they are equally mis-\nerable.\nF Limitations\nIn this section of the appendix, we delve deeper\ninto the unresolved issues in the PARA translations.\nFirst, we discuss the omissions present in the trans-\nlations. Next, we highlight some mistranslations\nthat persist in the PARA translations. To conclude,\nwe briefly discuss our initial experiments utilizing\nGPT-4 for paragraph-level translation.\nOmissions: One thing we ought to discuss is the\nomission issue. Upon examining translations and\nannotator feedback, we observe that PARA occa-\nsionally omits details, which are crucial to the story-\nline. Preliminary investigation indicates that PARA\ntranslations are more prone to omissions compared\nto SENT and GTR. Although PARA _SENT appears\nto mitigate this problem to some extent, it still\nresults in a higher number of omissions than the\nsentence-level approach while at the same time in-\ntroducing some repetition issues (see Table 12).40\nMistranslations: Moreover, PARA still makes a\nsizeable number of mistranslations and grammat-\nical errors, though fewer than SENT or GTR. We\nobserve that PARA occasionally merges sentences\nwith two distinctive subjects attributing all states\nand/or actions to one of them. Very rarely, we\nalso find cases where context possibly confuses the\nmodel, resulting in an incorrect translation. The\nfollowing example illustrates this issue:\n40Note that although ask the annotators to report both omis-\nsions and additions, based on their comments and our analysis\nof the translations, we conclude that omissions are the predom-\ninant issue. In version two of our data (currently on https://\ngithub.com/marzenakrp/LiteraryTranslation), we fur-\nther annotate any repetition as a separate type of error (i.e.\nerror with ‘repetition’ label) rather than counting it as an ad-\ndition. This annotations resulted in eight repetition errors in\nPARA _SENT translations.\n(10) Le bois du bureau amplifie les battements de mon\ncœur. Le vieux mobilier Art déco conduit bien les\némotions et les fatigues. Ruhlman ? Leleu ? Il [ he]\nen a tant vu.\n—FRENCH SOURCE (from Dear Reader)\na. 机の木材が私の心臓の鼓動を増幅してい\nる。古いアール・デコ家具は感情や疲労\nをうまく導いてくれる。ルールマン？レ\nルー？彼ら[they] はそんなに多くを見てき\nた。\n—GPT-3.5 P ARA (JAPANESE )\nIn the French text, the narrator wonders whether\nthe brand of the desk was Ruhlman or Leleu, with\nboth proper nouns possibly referring to a person.\nIn the last sentence, the French text uses “il” or\n“he” (10), as a desk is a masculine noun in French\n(“le bureau”). PARA , on the other hand, appears\nto be confused by the two preceding names and\nincorrectly translates the singular pronoun as 彼ら,\nor “they.”\nFurthermore, we observe (very few) cases where\nthe paragraph-level translation disregards the con-\ntext. Most representative of this class of errors\nis when the model struggles to translate from\nJapanese in cases where the subject is omitted. The\nfollowing example illustrates this issue:\n(11) ミホ[Miho] は、今では結婚して地元に中古\nの一戸建てを買っていて、そこに友達がよく\n集まっている。明日もアルバイトなので億劫\nに思う時もあるが、コンビニ以外の世界との\n唯一の接点であり、同い年の「普通の三十代\n女性」と交流する貴重な機会なので、ミホの\n[Miho’s] 誘いにはなるべく応じるようにして\nいる。\n—JAPANESE SOURCE (from Convenience Store Woman)\na. Miho [Miho] wyszła za m ˛ a˙z i kupiła stary, jed-\nnorodzinny dom w swoim rodzinnym mie´scie.\nPrzychodz ˛ a tam cz˛ esto jej znajomi. Mimo˙ze\nMiho ma [ Miho has ] jutro prac˛ e w konbini,\nzazwyczaj ch˛ etnie odpowiada [(she) responds]\nna jej [ her] zaproszenia, bo to jedyna okazja,\nby spotka´c si˛ e z innymi kobietami w jej [her]\nwieku.\n—GPT-3.5 P ARA (POLISH )\nb. Miho is now married and has bought an old\nhouse in her hometown, where her friends of-\nten gather. Though she often finds it a chore to\nwork tomorrow, it is her only connection to the\nworld outside the convenience store, and a valu-\nable opportunity to interact with other “normal\nthirty-something women” her age, so she tries\nto accept Miho’s invitations as often as possible.\n—GPT-3.5 P ARA (ENGLISH )\nBoth Polish (11a) and English (11b) translations\nof the same source text (11) share a common is-\nsue. The narrator begins the paragraph by talk-\ning about Miho and then proceeds to describe her\n444\nLanguage Pair PARA SENT PARA _SENT GTR\nRussian-English 0 0 1 0\nChinese-English 1 0 1 0\nPolish-English 0 0 0 0\nFrench-English 1 0 2 0\nJapanese-English 2 1 2 3\nGerman-English 0 0 0 0\nGerman-Japanese 8 2 6 8\nRussian-Japanese 10 4 6 4\nFrench-Japanese 3 1 4 4\nPolish-Japanese 4 1 3 0\nEnglish-Japanese 2 2 1 0\nChinese-Japanese 2 0 0 1\nEnglish-Polish 0 1 2 0\nJapanese-Polish 0 0 1 1\nFrench-Polish 2 2 1 1\nCzech-Polish 1 2 1 0\nRussian-Polish 1 1 1 0\nGerman-Polish 0 0 0 0\nTotal 37 17 32 22\nTable 12: Count of omissions reported by the translators for each translation method.\nown (the narrator’s) feelings about the situation, al-\nthough the gender of the narrator is never revealed\nin the Japanese text. The second sentence should be\nwritten from a first-person perspective, particularly\nsince it directly references Miho towards the end\n(blue text). However, both the Polish and English\ntranslations produced by PARA are confused by\nthis: by using the third-person’s perspective (“she,”\n“her”), both translations incorrectly imply that Miho\nis the subject of the second sentence. SENT and\nGTR translate this passage accurately, albeit with\nsome clumsy phrasing.\nGPT-4 does not magically solve all of these is-\nsues! Our preliminary experiments indicate that\nGPT-4 (OpenAI, 2023) sometimes generates better\nparagraph-level translations than those ofGPT-3.5 .\nFor instance, it seems to have a better grasp of the\ninverted word order in German, though no broader\nconclusions should be made without further testing.\nNevertheless, it does not resolve all of the issues\ndiscussed in our paper. Mistranslations and gram-\nmatical errors are still abundant across many lan-\nguage pairs. GPT-4 produces the following transla-\ntion when fed the previous example paragraph (11)\nas input; note that all of the issues still remain:41\n(12) Miho is now married and has bought a used single-\nfamily home in her hometown where her friends\noften gather. Although she sometimes finds it a drag\nto work a part-time job the next day, she makes\nan effort to respond to Miho’s invitations because\nit’s a valuable opportunity to interact with “normal”\nwomen in their thirties like herself, apart from her\nconvenience store job.\n—GPT-4 P ARA (ENGLISH )\nPARA translations hold the potential to captivate\nreaders, especially if LLMs continue to improve at\ntheir current pace. Indeed, some of our translators\nmentioned that they genuinely enjoyed the task,\nthough integrating these paragraphs into a coherent\nnovel still poses a considerable challenge. With\nall that said, literary translation involves more than\njust overall “correctness” or mere entertainment\nvalue. A translation that is perfectly “correct” and\nenjoyable might still fail to convey the author’s in-\ntentions or meaning skillfully hidden behind a sim-\nple phrase. Our fr-en translator shares her thoughts\non this matter:\n41Although the given paragraph is already comprehensible\nfor a human reader, we also attempt to enhance the transla-\ntion by incorporating three additional preceding paragraphs\nfor context. Intriguingly, when provided with this extended\ncontext, both GPT-3.5 and GPT-4 generated accurate transla-\ntions.\n445\nSYSTEM COMET BLEURT BERTSCORE COMET-QE\nPARA 0.785 0.485 0.840 0.038\nSENT 0.779 0.469 0.839 -0.052\nPARA_SENT 0.780 0.480 0.838 -0.062\nGTR 0.735 0.443 0.832 -0.156\nTable 13: Results of automatic evaluation. A higher\nnumber indicates better scores.\nBoth translations [SENT and PARA ] translate the\nwords without the feeling; the original author’s\nvoice is lost.\n—FRENCH TO ENGLISH TRANSLATOR\nG Automatic Evaluation\nIn this section of the appendix, we present the re-\nsults of automatic evaluation. First, we discuss the\nscores assigned to the translations by automatic\nmetrics.42 Then we provide the statistical analysis.\nFinally, we present the correlation of each metric\nwith human judgments for the 180 paragraphs used\nin the human evaluation.\nAutomatic metrics favor PARA : We assess\nthe translation from all four systems using the\nreference-based COMET (Rei et al., 2022),BLEURT\n(Sellam et al., 2020), and BERT SCORE (Zhang\net al., 2020) metrics, as well as the reference-free\nCOMET -QE (Rei et al., 2021)43 metric. Although\nthese metrics were not explicitly designed for eval-\nuating paragraph-level outputs and their results\nshould be interpreted with caution, they prove more\nreliable than string-based metrics like BLEU , es-\npecially for literary translations (Thai et al., 2022;\nKarpinska et al., 2022; Gehrmann et al., 2022).\nTable 13 shows the effectiveness of thePARA trans-\nlation method: a statistical analysis with linear\nmixed-effects models (Baayen et al., 2008) demon-\nstrates that PARA significantly outperforms SENT\nand GTR based on COMET , BLEURT , and COMET -\nQE scores (p<.001), and surpasses GTR based on\nthe BERT SCORE results (p<.001). We discuss the\ndetails of this statistical analysis in the next section.\nStatistical Analysis: We employ the linear-\nmixed effect models (Baayen et al., 2008) to an-\nalyze the scores produced by automatic metrics.\n42This analysis is done on the entire dataset excluding only\nthe paragraphs which were too long as per each metric’s token\nlimit.\n43We use the newest wmt22-comet-da checkpoints\nfor COMET , Bleurt-20 checkpoints for BLEURT ,\nwmt20-comet-qe-da checkpoints for COMET -QE ,\nand the HuggingFace implementation which employs\nroberta-large for BERTSCORE .\nMETRIC ACC τ ACC(conf) τ (conf)\nCOMET 67.41% 0.348 72.78% 0.456\nCOMET-QE 64.44% 0.289 70.64% 0.413\nBLEURT 61.30% 0.226 66.36% 0.327\nBARTSCORE 58.52% 0.170 63.91% 0.278\nTable 14: Correlation of automatic metrics with human\njudgments from our human evaluation. We evaluate\nthe metrics performance on all human judgments as\nwell as on the subset of judgments where the translator\nindicated that the chosen translation was visibly better\n(conf ). We report both the percentage of agreement\n(ACC) and Kendall’s Tau (τ). Data reported on v1 of\nthe dataset.\nWe fitted the model in R using the lme4 package\n(Bates et al., 2015); the p-values were obtained\nwith the LmerTest package (Kuznetsova et al.,\n2017). Linear-mixed effects models contain both\nfixed-effects and random-effects (random intercept\nand/or slope). The fixed effect here is the transla-\ntion setup (PARA , SENT , PARA _SENT , GTR) with\nthe source paragraph being coded as the random\neffect (random intercept). We inspect the residual\nplots to ensure that the variance across the fitted\nrange is relatively constant. The results from the\nfitted model are presented in Table 18 (BLEURT ),\nTable 20 (COMET ), Table 22 ( COMET -QE ), and\nTable 24 (BERTSCORE ).44\nWe further perform a post hoc analysis using the\nemmeans package (Lenth, 2023) to obtain p-values\nfor the pairwise comparison. The results of the post\nhoc analysis are presented in Table 19 (BLEURT ),\nTable 21 (COMET ), Table 23 ( COMET -QE ), and\nTable 25 (BERTSCORE ).\nCorrelation with Human Judgements: We in-\nvestigate the correlation of automatic metrics with\nhuman judgments in our evaluation. We consider\n(1) all the judgments, as well as (2) a subset of\nall judgments where the annotator stated that they\nwere sure that one translation is clearly better than\nthe other. We compute both accuracy (i.e., the\npercentage of cases where the metric agrees with\nhuman judgment), and a correlation coefficient\nKendall’s Tau which is defined as follows:\nτ = Concordant − Discordant\nConcordant + Discordant\n44It should be noted that, while significant, the analysis is\nunderpowered. It is possible that analyzing more examples\nwould provide a more reliable analysis.\n446\nSOURCE TARGET PARA PARA _PIVOT\nCzech Polish 11 9\nGerman Japanese 13 7\nGerman Polish 12 8\nFrench Japanese 9 11\nFrench Polish 11 9\nJapanese Polish 10 10\nPolish Japanese 3 17\nRussian Japanese 10 10\nRussian Polish 8 12\nChinese Japanese 9 11\nTOTAL 96 104\nTable 15: The results of pairwise comparison for the\nparagraph-level translations with (PARA _PIVOT ) and\nwithout (PARA ) English as a pivot language.\nTable 14 shows the correlation of automatic met-\nrics with the human judgments obtained in this\nstudy. COMET exhibits the highest agreement with\nhuman judgments both in terms of the accuracy\n(64.04% for all data, 72.78% for confident votes\nonly) and Kendall’s Tau (0.341 for all data, 0.456\nfor confident votes only).\nH Pivot Pilot\nIn this section of the appendix, we discuss the re-\nsults of the preliminary study where we translated\nthe paragraphs using English as a pivot language.\nTable 15 shows the results of this pilot study. The\nevaluation was done by the first author on all 20\npassages for every language pair that did not in-\nclude translation from or into English, as these do\nnot require any pivoting.45 A total number of 200\npairs was evaluated employing simple preference\njudgments.\nDuring the PARA _PIVOT translation process, the\nmodel utilized both the source text and its corre-\nsponding English translation (text-davinci-003,\ntop-p=1.0, temp=0.3). This approach has the po-\ntential to mitigate the limitations associated with\npivoting translations, where some information may\nbe lost. For example, both Czech and Polish encode\nthe gender information in the past tense form of the\n45The author is fluent in English, Japanese, and Polish with\na limited proficiency in other source languages.\nverb. English does not, so this information is lost\nand will most likely result in an erroneous trans-\nlation. Indeed, we notice that adding the source\ntext helps the model to overcome this shortcom-\ning, however, we do not observe a clear gain from\nusing English as a pivot language. Consider the\nfollowing example:\n(13) Tat’ka mˇe zase zmlátil. Byl hrozn ˇe naštvanej, pro-\ntože mamka ˇrvala. On mˇe vždycky mlátí, když ona\nˇrve. Že prej jsem po ˇrádnˇe neudˇelala nádobí. Ale\njá vím, že jsem je ud ˇelala dobˇre. I ten hrnec jsem\nvydrhla poˇrádnˇe. A ještˇe jsem to všechno utˇrela pˇred\nkoupáním. Ale možná jsem nevyždímala hadr.\n—CZECH SOURCE (from Crows)\na. Tata mnie znowu zbił. Był w´sciekły, bo mama\nkrzyczała. On zawsze mnie bije, kiedy ona krzy-\nczy. ˙Ze niby nie umyłam dobrze naczy´n. Ale\nja wiem, ˙ze umyłam je dobrze. I garnek te ˙z\ndokładnie wypolerowałam. I jeszcze wszys-\ntko wytrzepałam przed k ˛ apiel ˛ a. Ale mo˙ze nie\nwyci´sniłam r˛ ecznika.\n—GPT-3.5 P ARA (POLISH )\nb. Tata mnie znów uderzył. Był w ´sciekły, bo\nmama krzyczała. On zawsze mnie bije, kiedy\nona krzyczy. ˙Ze niby nie umyłam dobrze\nnaczy´n. Ale ja wiem, ˙ze umyłam je do-\nbrze. Nawet garnek dokładnie wytrzepałam.\nI jeszcze wszystko przed k ˛ apiel ˛ awytrzepałam.\nAle mo˙ze nie wy˙z ˛ agn˛ ełammopa.\n—GPT-3.5 P ARA _PIVOT (POLISH )\nIn each instance, the emphasized verbs could po-\ntentially be mistranslated when translated through\nEnglish as the pivot language, as the speaker’s gen-\nder information would be lost. For instance, the\npast tense verb “washed” remains unchanged in En-\nglish regardless of the gender of the speaker, with\nsuch details encoded only in the source ( Czech)\nand target ( Polish) languages. In this case, all\nverbs have been translated accurately with respect\nto grammatical gender, implying that incorporat-\ning the source language into the pivot pipeline\ndoes indeed improve the translation. However,\nPARA _PIVOT still selects less suitable verbs (high-\nlighted in red) resulting in slightly more errors in\nthis particular paragraph.\nThe only pair where pivoting seems to help is\npl-ja. While it is unclear why this happens, it is\npossible that this outcome is due to the specifics\nof the Polish novel employed for the translation.\nSword of Destinyby Andrzej Sapkowski uses a very\ndistinct language with many archaic expressions. It\nis possible that translating into English, a language\nthe GPT models were trained on, helps the model\ndeal with these difficult phrases.\nSince we do not observe any apparent gains from\nperforming the translation via English as a pivot\n447\nlanguage (p=0.62, 95% [0.448, 0.591]) and doing\nso reduces the number of examples one can fit into\nthe prompt, we continue our experiments with a\ndirect translation.\n448\nTYPE DESCRIPTION TRGLANG PARASENTPARA_SENTGTR\nCONTEXT(SENTENCE) A mistranslation that results most likely from lack of “understanding” thesentence-level context (e.g., translating “guide” as “doradca,” or “adviser”instead of “przewodnik,” or “guide”). This can include translating a word or aphrase into one that is semantically related but does not convey the intendedmeaning, or translation which appear to be an outcome of translating a wordsemantically related to the source word, instead of the source word itself.\nJapanese 114 118 107 158\nPolish 64 67 49 82\nEnglish 30 36 44 59\nCONTEXT(PARAGRAPH) A mistranslation that results from lack of a beyond-sentence context. Thisinclude issues such as polysemy, employment of correct pronouns, ortranslating elliptical expressions.\nJapanese 6 36 6 38\nPolish 13 51 15 59\nEnglish 2 25 0 48\nMINORISSUE A minor issue which does not significantly affect the text and can be disputable,such as translating “barked” as “howl.”\nJapanese 34 25 26 16\nPolish 33 26 16 13\nEnglish 18 11 12 9\nSURFACESIMILARITY\nA translation by word which is similar to the correct translation on the surfacelevel, but has a different meaning (e.g., “Wilczak,” a Polish surname, instead of“wilczarz,” a “wolfhound”).\nJapanese 8 6 7 2\nPolish 14 13 16 5\nEnglish 5 5 6 2\nWORD-BY-WORD A translation of longer phrase which is overly literal resulting in confusing andincorrect translation.\nJapanese 15 52 34 84\nPolish 17 23 18 33\nEnglish 7 13 5 20\nUNRELATEDWORD A translation with unrelated word such as “klnie” (“swear”) instead of “zapuka”(“knock”) where no apparent semantic relation could be found.\nJapanese 3 2 5 4\nPolish 5 14 10 12\nEnglish 1 3 1 2\nSUBJECTCHANGED\nChange of subject. In the case of PARA, it occurs mostly due to merging twosentences with two distinctive subjects where all states and/or actions are thenassigned to one of them.\nJapanese 5 2 2 0\nPolish 6 0 5 3\nEnglish 7 2 5 1\nFACTUALITY A translation that results in change in factuality, such as translating affirmativesentence as negation or translating word by its antonym.\nJapanese 4 11 5 7\nPolish 0 2 1 3\nEnglish 1 2 1 1\nNON-WORD\nA translation by a non-existent (made up) word. Some examples includeskillfully constructed words like火炎棒which was generated instead of a“torch.” While this word does not exist in Japanese (or Chinese) it follows thecompositionality rules of these languages and is fully intelligible to a nativespeaker (火炎“fire” and棒“stick.”)\nJapanese 1 2 2 0\nPolish 6 8 9 3\nEnglish 0 0 0 0\nMOOD\nChange in the grammatical mood with regard to the source text. Note that thesentence here isstillgrammatically correct but does not reflect the meaningintended by the author.\nJapanese 4 9 1 3\nPolish 1 3 4 2\nEnglish 0 0 0 0\nUNNECESSARYTRANSLATIONA translation of text which should be left untranslated such as some propernames.\nJapanese 0 0 0 0\nPolish 0 3 0 2\nEnglish 1 1 1 1\nLANGUAGEMISMATCH A translation into a language different than the target language (e.g., Chineseinstead of Japanese). Note that leaving the word in the source languageclassifies as an “untranslated” error.\nJapanese 2 3 3 2\nPolish 2 0 2 0\nEnglish 0 0 0 0\nNUMBER/TIME\nA translation which changes number or time expression, such as translating1h15min as 1h30min. Note that these rarely affect the overall meaning of thetext. We have not observe cases where this would be a critical issue.\nJapanese 3 2 4 3\nPolish 0 0 0 0\nEnglish 5 2 1 3\nPIVOTTRANSLATION(Czech)A mistranslation that stems from pivoting on English (annotated for cs-pllanguage pair).\nPolish 0 0 0 43\nOTHER Other issues which do not fit into any of the above.Japanese 24 26 27 17\nPolish 9 14 10 13\nEnglish 10 4 5 4\nTOTAL(Japanese) 223 294 229 334\nTOTAL(Polish) 170 224 155 273\nTOTAL(English) 87 104 81 150\nTOTAL(All) 480 622 465 757\nTable 16: Classification of mistranslation errors for each system grouped by the target language. The manual\nclassification was performed on the v1 of the annotated dataset.\n449\nTRG LANG TYPE SUBTYPE PARA SENTS PARA_SENTS GTR\nJAPANESE\nPARTICLE wrong or missing 21 22 13 12\nADJECTIVE wrong continuative 0 2 3 0\nother 0 0 2 0\nVERB tense 3 7 1 14\nmood 2 1 4 5\nfinite/non-finite 5 2 1 3\nother 2 5 6 0\nORDER wrong order 1 6 1 16\nOTHER 8 5 6 13\nTOTAL 42 50 37 63\nPOLISH\nADJECTIVE gender 7 14 8 4\ncase 2 1 1 0\nother 1 1 1 1\nNOUN case 9 13 9 1\nother 3 3 3 2\nPRONOUN omitted or wrong 5 8 3 2\ncase or gender 1 6 4 5\nVERB aspect 1 5 1 12\nperson or gender 2 8 5 2\nconjugation 1 0 7 3\nother 2 4 1 13\nPREPOSITION omitted or wrong 14 15 15 4\nNUMERAL case or gender 2 1 0 1\nORDER wrong order 2 4 2 4\nOTHER 3 3 4 5\nTOTAL 55 86 64 59\nENGLISH\nARTICLE omitted or wrong 1 9 2 8\nPREPOSITION omitted or wrong 3 7 3 5\nOTHER 1 4 4 5\nTOTAL 5 20 9 18\nTable 17: Categorization of grammar errors in each translation configuration, grouped by the target language. The\nmanual classification was performed on the v1 of the annotated dataset.\nBLEURT\nPredictors Estimates CI p-value\n(Intercept) 0.48 0.47–0.50 <0.001\nPARA _SENT -0.00 -0.01–0.00 0.130\nSENT -0.02 -0.02–(-0.01) <0.001\nGTR -0.04 -0.05–(-0.04) <0.001\nTable 18: Results of linear-mixed effects models analysis for BLEURT scores.\n450\nBLEURT\nContrast Estimate SE df t-ratio p-value\nPARA - PARA _SENT 0.00477 0.00315 1074 1.515 0.780\nPARA - SENT 0.01641 0.00315 1074 5.215 <0.001\nPARA - GTR 0.04155 0.00315 1074 13.205 <0.001\nPARA _SENT - SENT 0.01164 0.00315 1074 3.700 0.001\nPARA _SENT - GTR 0.03678 0.00315 1074 11.690 <0.001\nSENT - GTR 0.02514 0.00315 1074 7.990 <0.001\nTable 19: Result of post hoc analysis with emmeans package for BLEURT .\nCOMET\nPredictors Estimates CI p-value\n(Intercept) 0.79 0.77–0.80 <0.001\nPARA _SENT -0.01 -0.01–(-0.00) 0.019\nSENT -0.01 -0.01–(-0.00) 0.004\nGTR -0.05 -0.05–(-0.05) <0.001\nTable 20: Results of linear-mixed effects models analysis for COMET scores.\nCOMET\nContrast Estimate SE df t-ratio p-value\nPARA - PARA _SENT 0.00563 0.00239 1074 2.356 0.112\nPARA - SENT 0.00691 0.00239 1074 2.893 0.023\nPARA - GT R 0.04998 0.00239 1074 20.928 <.001\nPARA _SENT - SENT 0.00128 0.00239 1074 0.536 1.000\nPARA _SENT - GTR 0.04435 0.00239 1074 18.571 <.001\nSENT - GTR 0.04307 0.00239 1074 18.035 <.001\nTable 21: Result of post hoc analysis with emmeans package for COMET .\nCOMET -QE\nPredictors Estimates CI p-value\n(Intercept) -0.04 -0.06 – -0.01 0.004\nPARA _SENT -0.01 -0.03 – -0.00 0.026\nSENT -0.02 -0.04 – -0.01 <0.001\nGTR -0.12 -0.13 – -0.11 <0.001\nTable 22: Results of linear-mixed effects models analysis for COMET -QE scores.\n451\nCOMET -QE\nContrast Estimate SE df t-ratio p-value\nPARA - PARA _SENT 0.01464 0.00655 1074 2.235 0.154\nPARA - SENT 0.02376 0.00655 1074 3.628 0.002\nPARA - GTR 0.11848 0.00655 1074 18.092 <.001\nPARA _SENT - SENT 0.00912 0.00655 1074 1.392 0.9844\nPARA _SENT - GTR 0.10384 0.00655 1074 15.857 <.001\nSENT - GTR 0.09472 0.00655 1074 14.464 <.001\nTable 23: Result of post hoc analysis with emmeans package for COMET -QE.\nBERTSCORE\nPredictors Estimates CI p-value\n(Intercept) 0.84 0.83–0.85 <0.001\nPARA _SENT -0.00 -0.00–0.00 0.037\nSENT -0.00 -0.00–0.00 0.522\nGTR -0.01 -0.01–0.01 <0.001\nTable 24: Results of linear-mixed effects models analysis for BERTSCORE scores.\nBERTSCORE\nContrast Estimate SE df t-ratio p-value\nPARA - PARA _SENT 0.002422 0.00116 1074 2.082 0.225\nPARA - SENT 0.000745 0.00116 1074 0.640 1.000\nPARA - GT R 0.007508 0.00116 1074 6.454 <0.001\nPARA _SENT - SENT -0.001678 0.00116 1074 -1.442 0.897\nPARA _SENT - GTR 0.005086 0.00116 1074 4.372 <0.001\nSENT - GTR 0.006763 0.00116 1074 5.814 <0.001\nTable 25: Result of post hoc analysis with emmeans package for BERTSCORE .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8034487962722778
    },
    {
      "name": "Natural language processing",
      "score": 0.644162118434906
    },
    {
      "name": "Sentence",
      "score": 0.5691565871238708
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5644844174385071
    },
    {
      "name": "Readability",
      "score": 0.5410006046295166
    },
    {
      "name": "Paragraph",
      "score": 0.5245571136474609
    },
    {
      "name": "Grammar",
      "score": 0.5163434743881226
    },
    {
      "name": "Linguistics",
      "score": 0.4899813234806061
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4623967111110687
    },
    {
      "name": "Syntax error",
      "score": 0.451218843460083
    },
    {
      "name": "Annotation",
      "score": 0.4445534646511078
    },
    {
      "name": "Source text",
      "score": 0.4417325258255005
    },
    {
      "name": "Commit",
      "score": 0.43559059500694275
    },
    {
      "name": "Context (archaeology)",
      "score": 0.426182359457016
    },
    {
      "name": "Syntax",
      "score": 0.404845654964447
    },
    {
      "name": "World Wide Web",
      "score": 0.17657464742660522
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.09618476033210754
    },
    {
      "name": "Programming language",
      "score": 0.0952095091342926
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ]
}