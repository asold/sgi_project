{
  "title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation",
  "url": "https://openalex.org/W2971087717",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2089236564",
      "name": "Jie Zhu",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2147368654",
      "name": "Junhui Li",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2537403773",
      "name": "Muhua Zhu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120211867",
      "name": "Longhua Qian",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2076899804",
      "name": "Guodong Zhou",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4241645538",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2468355276",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2524520086",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W2798749466",
    "https://openalex.org/W2964035651",
    "https://openalex.org/W4288407114",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2966469393",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2565245743",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2932864487",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W2924961378",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2952706341",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W1479669738",
    "https://openalex.org/W2963374482",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5459–5468,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n5459\nModeling Graph Structure in Transformer\nfor Better AMR-to-Text Generation\nJie Zhu1 Junhui Li1∗ Muhua Zhu2\nLonghua Qian1 Min Zhang1 Guodong Zhou1\n1School of Computer Science and Technology, Soochow University, Suzhou, China\n2Alibaba Group, Hangzhou, China\nzhujie951121@gmail.com, {lijunhui, qianlonghua, minzhang, gdzhou}@suda.edu.cn\nmuhua.zmh@alibaba-inc.com\nAbstract\nRecent studies on AMR-to-text generation\noften formalize the task as a sequence-to-\nsequence (seq2seq) learning problem by con-\nverting an Abstract Meaning Representation\n(AMR) graph into a word sequence. Graph\nstructures are further modeled into the seq2seq\nframework in order to utilize the structural in-\nformation in the AMR graphs. However, pre-\nvious approaches only consider the relations\nbetween directly connected concepts while ig-\nnoring the rich structure in AMR graphs. In\nthis paper we eliminate such a strong limita-\ntion and propose a novel structure-aware self-\nattention approach to better modeling the re-\nlations between indirectly connected concepts\nin the state-of-the-art seq2seq model, i.e., the\nTransformer. In particular, a few different\nmethods are explored to learn structural rep-\nresentations between two concepts. Experi-\nmental results on English AMR benchmark\ndatasets show that our approach signiﬁcantly\noutperforms the state of the art with 29.66\nand 31.82 BLEU scores on LDC2015E86 and\nLDC2017T10, respectively. To the best of our\nknowledge, these are the best results achieved\nso far by supervised models on the bench-\nmarks.\n1 Introduction\nAMR-to-text generation is a task of automatically\ngenerating a natural language sentence from an\nAbstract Meaning Representation (AMR) graph.\nDue to the importance of AMR as a widely\nadopted semantic formalism in representing the\nmeaning of a sentence (Banarescu et al., 2013),\nAMR has become popular in semantic representa-\ntion and AMR-to-text generation has been draw-\ning more and more attention in the last decade.\nAs the example in Figure 1(a) shows, nodes, such\nas he and convict-01, represent semantic concepts\n∗Corresponding Author: Junhui Li.\nand edges, such as “:ARG1” and “:quant”, re-\nfer to semantic relations between the concepts.\nSince two concepts close in an AMR graph may\nmap into two segments that are distant in the cor-\nresponding sentence, AMR-to-text generation is\nchallenging. For example in Figure 1, the neigh-\nboring concepts he and convict-01 correspond to\nthe words he and convicted which locate at the dif-\nferent ends of the sentence.\nTo address the above mentioned challenge, re-\ncent studies on AMR-to-text generation regard the\ntask as a sequence-to-sequence (seq2seq) learning\nproblem by properly linearizing an AMR graph\ninto a sequence (Konstas et al., 2017). Such an\ninput representation, however, is apt to lose useful\nstructural information due to the removal of reen-\ntrant structures for linearization. To better model\ngraph structures, previous studies propose various\ngraph-based seq2seq models to incorporate graphs\nas an additional input representation (Song et al.,\n2018; Beck et al., 2018; Damonte and Cohen,\n2019). Although such graph-to-sequence models\ncan achieve the state-of-the-art results, they focus\non modeling one-hop relations only. That is, they\nonly model concept pairs connected directly by an\nedge (Song et al., 2018; Beck et al., 2018), and as a\nresult, ignore explicit structural information of in-\ndirectly connected concepts in AMR graphs, e.g.\nthe relation between concepts he and possible in\nFigure 1.\nTo make better use of structural information\nin an AMR graph, we attempt to model arbi-\ntrary concept pairs no matter whether directly con-\nnected or not. To this end, we extend the en-\ncoder in the state-of-the-art seq2seq model, i.e.,\nthe Transformer (Vaswani et al., 2017) and pro-\npose structure-aware self-attention encoding ap-\nproach. In particular, several distinct meth-\nods are proposed to learn structure represen-\ntations for the new self-attention mechanism.\n5460\nEmpirical studies on two English benchmarks\nshow that our approach signiﬁcantly advances\nthe state of the art for AMR-to-text generation,\nwith the performance improvement of 4.16 BLEU\nscore on LDC2015E86 and 4.39 BLEU score on\nLDC2017T10 respectively over the strong base-\nline. Overall, this paper makes the following con-\ntributions.\n•To the best of our knowledge, this is the ﬁrst\nwork that applies the Transformer to the task\nof AMR-to-text generation. On the basis of\nthe Transformer, we build a strong baseline\nthat reaches the state of the art.\n•We propose a new self-attention mechanism\nto incorporate richer structural information in\nAMR graphs. Experimental results on two\nbenchmarks demonstrate the effectiveness of\nthe proposed approach.\n•Beneﬁting from the strong baseline and\nthe structure-aware self-attention mecha-\nnism, we greatly advance the state of the art\nin the task.\n2 AMR-to-Text Generation with Graph\nStructure Modeling\nWe start by describing the implementation of our\nbaseline system, a state-of-the-art seq2seq model\nwhich is originally used for neural machine trans-\nlation and syntactic parsing (Vaswani et al., 2017).\nThen we detail the proposed approach to incorpo-\nrating structural information from AMR graphs.\n2.1 Transformer-based Baseline\nTransformer: Our baseline system builds on the\nTransformer which employs an encoder-decoder\nframework, consisting of stacked encoder and de-\ncoder layers. Each encoder layer has two sublay-\ners: self-attention layer followed by a position-\nwise feed forward layer. Self-attention layer em-\nploys multiple attention heads and the results from\neach attention head are concatenated and trans-\nformed to form the output of the self-attention\nlayer. Each attention head uses scaled dot-\nproduct attention which takes a sequence x =\n(x1,··· ,xn) of nelements as input and computes\na new sequence z = (z1,··· ,zn) of the same\nlength:\nz= Attention(x) (1)\n( a )\npossible-01\nsentence-01\nhe convict-01\ntemporal-quantity\n7\nyear prison\n:ARG1\n:ARG1 :condition\n:ARG2\n:quant\n:unit :location\n:ARG1\n( b )\n(  possible  :arg1  (  sentence  :arg1  he  :arg2  (  temporal-\nquantity  :quant  7  :unit year  :location  prison  )  :condition  \n(  convict  :arg1  he  )  )  ) \n( c )\npossible   sentence   he   temporal -quantity  7   year   prison  convict   \npossible\n. . .\nconvict\npossible\nconvict\n( d )\nsentence\n:ARG1\n:condition:ARG1\n:ARG2\nence\n:ARG1\n:condition:ARG1\n:ARG2\nsent@@\n:ARG1\n. . .\nFigure 1: (a) An example of AMR graph for the sen-\ntence of He could be sentenced to 7 years in prison if\nconvicted. (b) input to our baseline system, the seq2seq\nTransformer. (c) input to our proposed system based on\nstructure-aware self-attention. (d) An example of graph\nstructure extensions to sub-word units.\n5461\nwhere xi ∈ Rdx and z ∈ Rn×dz . Each output\nelement zi is a weighted sum of a linear transfor-\nmation of input elements:\nzi =\nn∑\nj=1\nαij\n(\nxjWV )\n(2)\nwhere WV ∈Rdx×dz is matrix of parameters. The\nvectors αi = (αi1,··· ,αin) in Equation 2 are\nobtained by the self-attention model, which cap-\ntures the correspondences between xi and others.\nSpeciﬁcally, the attention weight αij of each ele-\nment xj is computed using a softmax function:\nαij = exp(eij)∑n\nk=1 exp(eik) (3)\nwhere\neij =\n(\nxiWQ)(\nxjWK)T\n√dz\n(4)\nis an alignment function which measures how well\nthe input elements xi and xj match. WQ,WK ∈\nRdx×dz are parameters to be learned.\nInput Representation: We use the depth-ﬁrst\ntraversal strategy as in Konstas et al. (2017) to\nlinearize AMR graphs and to obtain simpliﬁed\nAMRs. We remove variables, wiki links and sense\ntags before linearization. Figure 1(b) shows an ex-\nample linearization result for the AMR graph in\nFigure 1(a). Note that the reentrant concept he in\nFigure 1 (a) maps to two different tokens in the\nlinearized sequence.\nVocabulary: Training AMR-to-text generation\nsystems solely on labeled data may suffer from\ndata sparseness. To attack this problem, previ-\nous works adopt techniques like anonymization to\nremove named entities and rare words (Konstas\net al., 2017), or apply a copy mechanism (Gul-\ncehre et al., 2016) such that the models can learn\nto copy rare words from the input sequence. In\nthis paper we instead use two simple yet effective\ntechniques. One is to apply Byte Pair Encoding\n(BPE) (Sennrich et al., 2016) to split words into\nsmaller, more frequent sub-word units. The other\nis to use a shared vocabulary for both source and\ntarget sides. Experiments in Section 3.2 demon-\nstrate the necessity of the techniques in building a\nstrong baseline.\n2.2 Modeling Graph Structures in\nTransformer\nInput Representation: We also use the depth-\nﬁrst traversal strategy to linearize AMR graphs\nand to obtain simpliﬁed AMRs which only con-\nsist of concepts. As shown in Figure 1 (c), the\ninput sequence is much shorter than the input se-\nquence in the baseline. Besides, we also obtain a\nmatrix which records the graph structure between\nevery concept pair, which implies their semantic\nrelationship (Section2.3).\nVocabulary: To be compatible with sub-words,\nwe extend the original AMR graph, if necessary, to\ninclude the structures of sub-words. As sentence-\n01 in Figure 1(a) is segmented into sent@@ ence-\n01, we split the original node into two connected\nones with an edge labeled as the incoming edge of\nthe ﬁrst unit. Figure 1(d) shows the graph structure\nfor sub-words sent@@ ence-01.\nStructure-Aware Self-Attention : Motivated\nby Shaw et al. (2018), we extend the conventional\nself-attention architecture to explicitly encode the\nrelation between an element pair ( xi,xj) in the\nalignment model by replacing Equation 4 with\nEquation 5. Note that the relation rij ∈ Rdz is\nthe vector representation for element pair (xi,xj),\nand will be learned in Section 2.3.\neij =\n(\nxiWQ)(\nxjWK + rijWR)T\n√dz\n(5)\nwhere WR ∈Rdz×dz is a parameter matrix. Then,\nwe update Equation 2 accordingly to propagate\nstructure information to the sublayer output by:\nzi =\nn∑\nj=1\nαij\n(\nxjWV + rijWF )\n(6)\nwhere WF ∈Rdz×dz is a parameter matrix.\n2.3 Learning Graph Structure\nRepresentation for Concept Pairs\nThe above structure-aware self-attention is capa-\nble of incorporating graph structure between con-\ncept pairs. In this section, we explore a few meth-\nods to learn the representation for concept pairs.\nWe use a sequence of edge labels, along the path\nfrom xi to xj to indicate the AMR graph structure\nbetween concepts xi and xj.1 In order to distin-\nguish the edge direction, we add a direction sym-\nbol to each label with ↑for climbing up along the\npath, and ↓for going down. Speciﬁcally, for the\nspecial case of i == j, we use None as the path.\nTable 1 demonstrates structural label sequences\nbetween a few concept pairs in Figure 1.\n1While there may exist two or more paths connecting xi\nand xj, we simply choose the shortest one.\n5462\nxi xj Structural label sequence\nhe convict-01 :ARG1↑\nhe 7 :ARG1↑:ARG2↓:quant↓\nhe he None\nTable 1: Examples of structural path between a few\nconcept pairs in Figure 1.\nNow, given a structural path with a label\nsequence s = s1,··· ,sk and its dx-sized\ncorresponding label embedding sequence l =\nl1,··· ,lk, we use the following methods to ob-\ntain its representation vector r, which maps to rij\nin Equation 5 and Equation 6.\nFeature-based\nA natural way to represent the structural path is to\nview it as a string feature. To this end, we combine\nthe labels in the structural path into a string. Un-\nsurprisingly, this will end up with a large number\nof features. We keep the most frequent ones (i.e.,\n20K in our experiments) in the feature vocabu-\nlary and map all others into a special featureUNK.\nEach feature in the vocabulary will be mapped into\na randomly initialized vector.\nAvg-based\nTo overcome the data sparsity in the above feature-\nbased method, we view the structural path as a la-\nbel sequence. Then we simply use the averaged la-\nbel embedding as the representation vector of the\nsequence, i.e.,\nr=\n∑k\ni=1 li\nk (7)\nSum-based\nSum-based method simply returns the sum of all\nlabel embeddings in the sequence, i.e.,\nr=\nk∑\ni=1\nli (8)\nSelf-Attention-based (SA-based for short)\nAs shown in Figure 2, given the label sequence\ns = s1,··· ,sk, we ﬁrst obtain the sequence\ne, whose element is the addition of a word em-\nbedding and the corresponding position embed-\nding. Then we use the self-attention, as pre-\nsented in Eq. 1 to obtain its hidden states h, i.e,\nh = Attention(e), where hi ∈ Rdz . Our aim\nis to encode a variable length sentence into a dz-\nsized vector. Motivated by (Lin et al., 2017), we\nachieve this by choosing a linear combination of\nr\n+\nh1 h2 hk. . .\nα1 α2 αk\nSelf-Attention\ne1 e2 ek. . .\nl1 l2 lk. . .\n+ + +. . .\ndz\nPositional\nEncoding\nS1 S2 Sk\nFigure 2: Self-Attention-based method.\nthe kvectors in h. Computing the linear combina-\ntion requires an attention mechanism which takes\nthe whole hidden states has input, and outputs a\nvector of weights α:\nα= softmax(W2 tanh(W1hT )) (9)\nwhere W1 ∈ Rdw×dz and W2 ∈ Rdw . Then\nthe label sequence representation vector is the\nweighted sum of its hidden states:\nr=\nk∑\ni=1\nαihi (10)\nCNN-based\nMotivated by (Kalchbrenner et al., 2014), we use\nconvolutional neural network (CNN) to convolute\nthe label sequence linto a vector r, as follow:\nconv= Conv1D(kernel size= (m),\nstrides= 1,\nfilters = dz,\ninput shape= dz\nactivation=′relu′)\n(11)\nr= conv(l) (12)\n5463\nwhere kernel size mis set to 4 in our experiments.\n3 Experimentation\n3.1 Experimental Settings\nFor evaluation of our approach, we use the sen-\ntences annotated with AMRs from the LDC re-\nlease LDC2015E86 and LDC2017T10. The\ntwo datasets contain 16,833 and 36,521 training\nAMRs, respectively, and share 1,368 development\nAMRs and 1,371 testing AMRs. We segment\nwords into sub-word units by BPE (Sennrich et al.,\n2016) with 10K operations on LDC2015E86 and\n20K operations on LDC2017T10.\nFor efﬁciently learning graph structure repre-\nsentation for concept pairs (except the feature-\nbased method), we limit the maximum label se-\nquence length to 4 and ignore the labels exceeding\nthe maximum. In SA-based method, we set the\nﬁlter size dw as 128.\nWe useOpenNMT (Klein et al., 2017) as the im-\nplementation of the Transformer seq2seq model. 2\nIn parameter setting, we set the number of layers\nin both the encoder and decoder to 6. For opti-\nmization we use Adam with β1 = 0.1 (Kingma\nand Ba, 2015). The number of heads is set to 8.\nIn addition, we set the embedding and the hidden\nsizes to 512 and the batch token-size to 4096. Ac-\ncordingly, the dx and dz in Section 2 are 64. In all\nexperiments, we train the models for 300K steps\non a single K40 GPU.\nFor performance evaluation, we use BLEU (Pa-\npineni et al., 2002), Meteor (Banerjee and\nLavie, 2005; Denkowski and Lavie, 2014), and\nCHRF++ (Popovi, 2017) as metrics. We report re-\nsults of single models that are tuned on the devel-\nopment set.\nWe make our code available at\nhttps://github.com/Amazing-J/\nstructural-transformer.\n3.2 Experimental Results\nWe ﬁrst show the performance of our baseline\nsystem. As mentioned before, BPE and sharing\nvocabulary are two techniques we applied to re-\nlieving data sparsity. Table 2 presents the re-\nsults of the ablation test on the development set of\nLDC2015E86 by either removing BPE, or vocabu-\nlary sharing, or both of them from the baseline sys-\ntem. From the results we can see that BPE and vo-\ncabulary sharing are critical to building our base-\n2https://github.com/OpenNMT/OpenNMT-py\nModel BLEU Meteor CHRF++\nBaseline 24.93 33.20 60.30\n-BPE 23.02 31.60 58.09\n-Share V ocab. 23.24 31.78 58.43\n-Both 18.77 28.04 51.88\nTable 2: Ablation results of our baseline system on the\nLDC2015E86 development set.\nline system (an improvement from 18.77 to 24.93\nin BLEU), revealing the fact that they are two ef-\nfective ways to address the issue of data sparseness\nfor AMR-to-text generation.\nTable 3 presents the comparison of our ap-\nproach and related works on the test sets of\nLDC2015E86 and LDC2017T10. From the re-\nsults we can see that the Transformer-based base-\nline outperforms most of graph-to-sequence mod-\nels and is comparable with the latest work by Guo\net al. (2019). The strong performance of the base-\nline is attributed to the capability of the Trans-\nformer to encode global and implicit structural in-\nformation in AMR graphs. By comparing the ﬁve\nmethods of learning graph structure representa-\ntions, we have the following observations.\n•All of them achieve signiﬁcant improve-\nments over the baseline: the biggest im-\nprovements are 4.16 and 4.39 BLEU scores\non LDC2015E86 and LDC2017T10, respec-\ntively.\n•Methods using continuous representations\n(such as SA-based and CNN-based) outper-\nform the methods using discrete representa-\ntions (such as feature-based).\n•Compared to the baseline, the methods have\nvery limited affect on the sizes of model pa-\nrameters (see the column of #P (M) in Ta-\nble 3).\nFinally, our best-performing models are the best\namong all the single and supervised models.\n4 Analysis\nIn this section, we use LDC2017T10 as our bench-\nmark dataset to demonstrate how our proposed\napproach achieves higher performance than the\nbaseline. As representative, we use CNN-based\nmethod to obtain structural representation.\n5464\nSystem LDC2015E86 LDC2017T10\nBLEU Meteor CHRF++ #P (M) BLEU Meteor CHRF++\nBaseline 25.50 33.16 59.88 49.1 27.43 34.62 61.85\nOur Approach\nfeature-based 27.23 34.53 61.55 49.4 30.18 35.83 63.20\navg-based 28.37 35.10 62.29 49.1 29.56 35.24 62.86\nsum-based 28.69 34.97 62.05 49.1 29.92 35.68 63.04\nSA-based 29.66 35.45 63.00 49.3 31.54 36.02 63.84\nCNN-based 29.10 35.00 62.10 49.2 31.82 36.38 64.05\nPrevious works with single models\nKonstas et al. (2017)∗ 22.00 - - - - - -\nCao and Clark (2019)∗ 23.5 - - - 26.8 - -\nSong et al. (2018)† 23.30 - - - - - -\nBeck et al. (2018)† - - - - 23.3 - 50.4\nDamonte and Cohen (2019)† 24.40 23.60 - - 24.54 24.07 -\nGuo et al. (2019)† 25.7 - - - 27.6 - 57.3\nSong et al. (2016)‡ 22.44 - - - - - -\nPrevious works with either ensemble models or unlabelled data, or both\nKonstas et al. (2017)∗ 33.8 - - - - - -\nSong et al. (2018)† 33.0 - - - - - -\nBeck et al. (2018)† - - - - 27.5 - 53.5\nGuo et al. (2019)† 35.3 - - - - - -\nTable 3: Comparison results of our approaches and related studies on the test sets of LDC2015E86 and\nLDC2017T10. #P indicates the size of parameters in millions. ∗ indicates seq2seq-based systems while † for\ngraph-based models, and ‡for other models. All our proposed systems are signiﬁcant over the baseline at 0.01,\ntested by bootstrap resampling (Koehn, 2004).\nSystem BLEU\nBaseline 27.43\nOur approach 31.82\nNo indirectly connected concept pairs 29.92\nTable 4: Performance on the test set of our approach\nwith or without modeling structural information of in-\ndirectly connected concept pairs.\n4.1 Effect of Modeling Structural\nInformation of Indirectly Connected\nConcept Pairs\nOur approach is capable of modeling arbitrary\nconcept pairs no matter whether directly con-\nnected or not. To investigate the effect of model-\ning structural information of indirectly connected\nconcept pairs, we ignore their structural informa-\ntion by mapping all structural label sequences be-\ntween two indirectly connected concept pairs into\nNone. In this way, the structural label sequence for\nhe and 7 in Table 1, for example, will be None.\nTable 4 compares the performance of our ap-\nproach with or without modeling structural infor-\nmation of indirectly connected concept pairs. It\nshows that by modeling structural information of\nindirectly connected concept pairs, our approach\nimproves the performance on the test set from\n29.92 to 31.82 in BLEU scores. It also shows that\neven without modeling structural information of\nindirectly connected concept pairs, our approach\nachieves better performance than the baseline.\n4.2 Effect on AMR Graphs with Different\nSizes of Reentrancies\nLinearizing an AMR graph into a sequence un-\navoidably loses information about reentrancies\n(nodes with multiple parents). This poses a chal-\nlenge for the baseline since there exists on ob-\nvious sign that the ﬁrst he and the second he,\nas shown in Figure 1 (b), refer to the same per-\nson. By contrast, our approach models reentran-\ncies explicitly. Therefore, it is expected that the\nbeneﬁt of our approach is more evident for those\nAMR graphs containing more reentrancies. To test\nthis hypothesis, we partition source AMR graphs\nto different groups by their numbers of reentran-\ncies and evaluate their performance respectively.\nAs shown in Figure 3, the performance gap be-\n5465\nFigure 3: Performance (in BLEU) on the test set with\nrespect to the reentrancy numbers of the input AMR\ngraphs.\ntween our approach and the baseline goes widest\nfor AMR graphs with more than 5 reentrancies,\non which our approach outperforms the baseline\nby 6.61 BLEU scores.\n4.3 Effect on AMR Graphs with Different\nSizes\nWhen we encode an AMR graph with plenty con-\ncepts, linearizing it into a sequence tends to lose\ngreat amount of structural information. In order\nto test the hypothesis that graphs with more con-\ncepts contribute more to the improvement, we par-\ntition source AMR graphs to different groups by\ntheir sizes (i.e., numbers of concepts) and evaluate\ntheir performance respectively. Figure 4 shows the\nresults which indicate that modeling graph struc-\ntures signiﬁcantly outperforms the baseline over\nall AMR lengths. We also observe that the per-\nformance gap between the baseline and our ap-\nproach increases when AMR graphs become big,\nrevealing that the baseline seq2seq model is far\nfrom capturing deep structural details of big AMR\ngraphs. Figure 4 also indicates that text generation\nbecomes difﬁcult for big AMR graphs. We think\nthat the low performance on big AMR graphs is\nmainly attributed to two reasons:\n•Big AMR graphs are usually mapped into\nlong sentences while seq2seq model tends to\nstop early for long inputs. As a result, the\nlength ratio 3 for AMRs with more than 40\nconcepts is 0.906, much lower than that for\nAMRs with less concepts.\n3Length ratio is the length of generation output, divided\nby the length of reference.\nFigure 4: Performance (in BLEU) on the test set with\nrespect to the size of the input AMR graphs.\n•Big AMR graphs are more likely to have\nreentrancies, which makes the generation\nmore challenging.\n4.4 Case Study\nIn order to better understand the model perfor-\nmance, Figure 5 presents a few examples studied\nin Song et al. (2018) (Example (1)) and Damonte\nand Cohen (2019) (Examples (2) - (5)).\nIn Example (1), though our baseline recovers\na propositional phrase for the noun staff and an-\nother one for the noun funding, it fails to recog-\nnize the anaphora and antecedent relation between\nthe two propositional phrases. In contrast, our\napproach successfully recognizes :prep-for cas a\nreentrancy node and generates one propositional\nphrase shared by both nouns staff and funding. In\nExample (2), we note that although AMR graphs\nlack tense information, the baseline generates out-\nput with inconsistent tense (i.e., do and found)\nwhile our approach consistently prefers past tense\nfor the two clauses. In Example (3), only our ap-\nproach correctly uses people as the subject of the\npredicate can. In Example (4), the baseline fails to\npredict the direct object you for predicate recom-\nmend. Finally in Example (5), the baseline fails to\nrecognize the subject-predicate relation between\nnoun communicate and verb need. Overall, we\nnote that compared to the baseline, our approach\nproduces more accurate output and deal with reen-\ntrancies more properly.\nComparing the generation of our approach and\ngraph-based models in Song et al. (2018) and Da-\nmonte and Cohen (2019), we observe that our gen-\neration is more close to the reference in sentence\nstructure. Due to the absence of tense informa-\n5466\n(1)  (p/ provide-01 \n    :ARG0 (a / agree-01) \n    :ARG1 (a2 / and \n         :op1 (s / staff \n               :prep-for (c / center \n                       :mod (r / research-01))) \n         :op2 (f / fund-01 \n               :prep-for c))) \n       REF:  the agreement will provide staff and funding for the research center . \n       SEQ1: agreed to provide research and institutes in the center . \n       G2S: the agreement provides the staff of the  research center and the funding . \n       Baseline: the agreement provides staff for research centres and funding for center . \n       Our approach: the agreement provided staff and funding for research centers . \n(2)  REF:  i dont tell him but he finds out , \n       SEQ2:  i did n't tell him but he was out . \n       GRAPH: i do n’t tell him but he found out . \n       Baseline: i do n't tell him but he found out . \n       Our approach: i did n't tell him but he found out . \n(3)  REF: if you tell people they can help you , \n       SEQ2: if you tell him , you can help you ! \n       GRAPH: if you tell them , you can help you . \n       Baseline: if you tell people , you might help you ! \n       Our approach: if you tell people , people can help you ! \n(4)  REF:  i 'd recommend you go and see your doctor too . \n       SEQ2: i recommend you go to see your doctor who is going to see your doctor . \n       GRAPH: i recommend you going to see your doctor too . \n       Baseline: i would recommend going to see your doctor too . \n       Our approach: i would recommend you to go to see your doctor too . \n(5)  REF:  tell your ex that all communication needs to go through the lawyer . \n       SEQ2: tell that all the communication go through lawyer . \n       GRAPH: tell your ex the need to go through a lawyer . \n       Baseline: tell your ex you need to go all the communication by lawyer . \n       Our approach: tell your ex that all communication needs to go through lawyers .\nFigure 5: Examples of generation from AMR graphs. (1) is from Song et al. (2018), (2) - (5) are from Damonte\nand Cohen (2019). REF is the reference sentence. SEQ1 and G2S are the outputs of the seq2seq and the graph2seq\nmodels in Song et al. (2018), respectively. SEQ2 and GRAPH are the outputs of the seq2seq and the graph models\nin Damonte and Cohen (2019), respectively.\ntion in AMR graphs, our model tends to use past\ntense, as provided and did in Example (1) and (2).\nSimilarly, without information concerning singu-\nlar form and plural form, our model is more likely\nto use plural nouns, as centers and lawyers in Ex-\nample (1) and (5).\n5 Related Work\nMost studies in AMR-to-text generation regard\nit as a translation problem and are motivated by\nthe recent advances in both statistical machine\ntranslation (SMT) and neural machine translation\n(NMT). Flanigan et al. (2016) ﬁrst transform an\nAMR graph into a tree, then specify a number\nof tree-to-string transduction rules based on align-\nments that are used to drive a tree-based SMT\nmodel (Graehl and Knight, 2004). Pourdamghani\net al. (2016) develop a method that learns to lin-\nearize AMR graphs into AMR strings, and then\nfeed them into a phrase-based SMT model (Koehn\net al., 2003). Song et al. (2017) use synchronous\nnode replacement grammar (SNRG) to generate\ntext. Different from synchronous context-free\ngrammar in hierarchical phrase-based SMT (Chi-\nang, 2007), SNRG is a grammar over graphs.\nMoving to neural seq2seq approaches, Konstas\net al. (2017) successfully apply seq2seq model\ntogether with large-scale unlabeled data for both\ntext-to-AMR parsing and AMR-to-text genera-\ntion. With special interest in the target side syn-\n5467\ntax, Cao and Clark (2019) use seq2seq models to\ngenerate target syntactic structure, and then the\nsurface form. To prevent the information loss\nin linearizing AMR graphs into sequences, (Song\net al., 2018; Beck et al., 2018) propose graph-\nto-sequence models to encode graph structure di-\nrectly. Focusing on reentrancies, Damonte and\nCohen (2019) propose stacking encoders which\nconsist of BiLSTM (Graves et al., 2013), TreeL-\nSTMs (Tai et al., 2015), and Graph Convolutional\nNetwork (GCN) (Duvenaud et al., 2015; Kipf and\nWelling, 2016). Guo et al. (2019) propose densely\nconnected GCN to better capture both local and\nnon-local features. However, all the aforemen-\ntioned graph-based models only consider the re-\nlations between nodes that are directly connected,\nthus lose the structural information between nodes\nthat are indirectly connected via an edge path.\nRecent studies also extend the Transformer to\nencode structural information for other NLP ap-\nplications. Shaw et al. (2018) propose relation-\naware self-attention to capture relative positions\nof word pairs for neural machine translation.\nGe et al. (2019) extend the relation-aware self-\nattention to capture syntactic and semantic struc-\ntures. Our model is inspired by theirs but aims to\nencode structural label sequences of concept pairs.\nKoncel-Kedziorski et al. (2019) propose graph\nTransformer to encode graph structure. Similar to\nthe GCN, it focuses on the relations between di-\nrectly connected nodes.\n6 Conclusion and Future Work\nIn this paper we proposed a structure-aware self-\nattention for the task of AMR-to-text generation.\nThe major idea of our approach is to encode\nlong-distance relations between concepts in AMR\ngraphs into the self-attention encoder in the Trans-\nformer. In the setting of supervised learning, our\nmodels achieved the best experimental results ever\nreported on two English benchmarks.\nPrevious studies have shown the effectiveness\nof using large-scale unlabelled data. In future\nwork, we would like to do semi-supervised learn-\ning and use silver data to test how much improve-\nments could be further achieved.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments and suggestions. We are grate-\nful to Linfeng Song for fruitful discussions. This\nwork is supported by the National Natural Sci-\nence Foundation of China (Grant No. 61876120,\n61673290, 61525205), and the Priority Academic\nProgram Development of Jiangsu Higher Educa-\ntion Institutions.\nReferences\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of 7th Linguistic\nAnnotation Workshop & Interoperability with Dis-\ncourse, pages 178–186.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof ACL, pages 65–72.\nDaniel Beck, Gholamreza Haffari, and Trevor Cohn.\n2018. Graph-to-sequence learning using gated\ngraph neural networks. In Proceedings of ACL,\npages 273–283.\nKris Cao and Stephen Clark. 2019. Factorising amr\ngeneration through syntax. In Proceedings of\nNAACL, pages 2157–2163.\nDavid Chiang. 2007. Hierarchical phrase-based trans-\nlation. Computational Linguistics, 33:201–228.\nMarco Damonte and Shay B. Cohen. 2019. Struc-\ntural neural encoders for AMR-to-text generation.\nIn Proceedings of NAACL, pages 3649–3658.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language speciﬁc translation evaluation\nfor any target language. In Proceedings of WMT,\npages 376–380.\nDavid K Duvenaud, Dougal Maclaurin, Jorge Ipar-\nraguirre, Rafael Bombarell, Timonthy Hirzel, Alan\nAspuru-Guzik, and Ryan P Adams. 2015. Convo-\nlutional networks on graphs for learning molecular\nﬁngerprints. In Proceedings of NIPS, pages 2224–\n2232.\nJeffrey Flanigan, Chris Dyer, Noah A. Smith, and\nJaime Carbonell. 2016. Generation from abstract\nmeaning representation using tree transducers. In\nProceedings of NAACL, pages 731–739.\nDongLai Ge, Junhui Li, Muhua Zhu, and Shoushan Li.\n2019. Modeling source syntax and semantics for\nneural amr parsing. In Proceedings of IJCAI, pages\n4975–4981.\nJonathan Graehl and Kevin Knight. 2004. Training tree\ntransducers. In Proceedings of NAACL, pages 105–\n112.\n5468\nAlex Graves, Abdel rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In Proceedings of ICASSP,\npages 6645–6649.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Yoshua Bengio. 2016. Pointing\nthe unknown words. In Proceedings of ACL, pages\n140–149.\nZhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei\nLu. 2019. Densely connected graph convolutional\nnetworks for graph-to-sequence learning. Transac-\ntions of the Association of Computational Linguis-\ntics, 7:297–312.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blun-\nsom. 2014. A convolutional neural network for\nmodelling sentences. In Proceedings of ACL, pages\n655–665.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In Proceedings of ICLR.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander M. Rush. 2017. Open-\nnmt: Open-source toolkit for neural machine trans-\nlation. In Proceedings of ACL, System Demonstra-\ntions, pages 67–72.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests for\nmachine translation evaluation. In Proceedings of\nEMNLP, pages 388–395.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof NAACL, pages 127–133.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019.\nText generation from knowledge graphs with graph\ntransformers. In Proceedings of NAACL, pages\n2284–2293.\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin\nChoi, and Luke Zettlemoyer. 2017. Neural AMR:\nSequence-to-sequence models for parsing and gen-\neration. In Proceedings of ACL, pages 146–157.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. In Proceedings of ICLR.\nKishore Papineni, Salim Roukos, Ward Todd, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nACL, pages 311–318.\nMaja Popovi. 2017. chrf++: words helping character\nn-grams. In Proceedings of WMT, pages 612–618.\nNima Pourdamghani, Kevin Knight, and Ulf Her-\nmjakob. 2016. Generating english from abstract\nmeaning representations. In Proceedings of the 9th\nInternational Natural Language Generation confer-\nence, pages 21–25.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of ACL, pages 1715–\n1725.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of NAACL, pages 464–468.\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo\nWang, and Daniel Gildea. 2017. Amr-to-text gener-\nation with synchronous node replacement grammar.\nIn Proceedings of ACL, pages 7–13.\nLinfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo\nWang, and Daniel Gildea. 2016. Amr-to-text gener-\nation as a traveling salesman problem. In Proceed-\nings of EMNLP, pages 2084–2089.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. A graph-to-sequence model for\nAMR-to-text generation. In Proceedings of ACL,\npages 1616–1626.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of ACL, pages 1556–1566.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N.Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS, pages 5998–\n6008.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6640219688415527
    },
    {
      "name": "Transformer",
      "score": 0.6344152688980103
    },
    {
      "name": "Natural language processing",
      "score": 0.44420287013053894
    },
    {
      "name": "Graph",
      "score": 0.4362308084964752
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42009350657463074
    },
    {
      "name": "Zhàng",
      "score": 0.4143831431865692
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3030622601509094
    },
    {
      "name": "Engineering",
      "score": 0.16838660836219788
    },
    {
      "name": "Electrical engineering",
      "score": 0.14767780900001526
    },
    {
      "name": "History",
      "score": 0.08188804984092712
    },
    {
      "name": "China",
      "score": 0.07524225115776062
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3923682",
      "name": "Soochow University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ]
}