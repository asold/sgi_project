{
    "title": "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction",
    "url": "https://openalex.org/W3035010485",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2000996583",
            "name": "Masahiro Kaneko",
            "affiliations": [
                "RIKEN Center for Advanced Intelligence Project",
                "Tokyo Metropolitan University"
            ]
        },
        {
            "id": "https://openalex.org/A2554422499",
            "name": "Masato Mita",
            "affiliations": [
                "Tohoku University",
                "RIKEN Center for Advanced Intelligence Project"
            ]
        },
        {
            "id": "https://openalex.org/A2776693833",
            "name": "Shun Kiyono",
            "affiliations": [
                "RIKEN Center for Advanced Intelligence Project",
                "Tohoku University"
            ]
        },
        {
            "id": "https://openalex.org/A1749670362",
            "name": "Jun Suzuki",
            "affiliations": [
                "RIKEN Center for Advanced Intelligence Project",
                "Tohoku University"
            ]
        },
        {
            "id": "https://openalex.org/A2084773436",
            "name": "Kentaro Inui",
            "affiliations": [
                "Tohoku University",
                "RIKEN Center for Advanced Intelligence Project"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3098448896",
        "https://openalex.org/W3036120435",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2985808369",
        "https://openalex.org/W2124725212",
        "https://openalex.org/W2797885244",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2970521905",
        "https://openalex.org/W2589277916",
        "https://openalex.org/W2970429618",
        "https://openalex.org/W2950737607",
        "https://openalex.org/W2144950812",
        "https://openalex.org/W2970592413",
        "https://openalex.org/W2962801832",
        "https://openalex.org/W2153013403",
        "https://openalex.org/W2970868759",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2924690340",
        "https://openalex.org/W2936597270",
        "https://openalex.org/W2803237843",
        "https://openalex.org/W2953083125",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W3103010876",
        "https://openalex.org/W2904937108",
        "https://openalex.org/W2771449929",
        "https://openalex.org/W4288562606",
        "https://openalex.org/W2970294904",
        "https://openalex.org/W2993173455",
        "https://openalex.org/W2948335087",
        "https://openalex.org/W2964910501",
        "https://openalex.org/W2997763445",
        "https://openalex.org/W2170527467",
        "https://openalex.org/W2554764206",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2315316408",
        "https://openalex.org/W2741494657",
        "https://openalex.org/W2977788613",
        "https://openalex.org/W2994928925",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2963881719",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2098297786",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2478432301",
        "https://openalex.org/W2931749839",
        "https://openalex.org/W2980282514"
    ],
    "abstract": "This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods. Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM. The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4248–4254\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4248\nEncoder-Decoder Models Can Beneﬁt from Pre-trained\nMasked Language Models in Grammatical Error Correction\nMasahiro Kaneko1,2 Masato Mita2,3 Shun Kiyono2,3 Jun Suzuki3,2 Kentaro Inui3,2\n1Tokyo Metropolitan University\n2RIKEN Center for Advanced Intelligence Project\n3Tohoku University\nkaneko-masahiro@ed.tmu.ac.jp\n{masato.mita, shun.kiyono}@riken.jp\n{jun.suzuki, inui}@ecei.tohoku.ac.jp\nAbstract\nThis paper investigates how to effectively\nincorporate a pre-trained masked language\nmodel (MLM), such as BERT, into an encoder-\ndecoder (EncDec) model for grammatical er-\nror correction (GEC). The answer to this ques-\ntion is not as straightforward as one might\nexpect because the previous common meth-\nods for incorporating a MLM into an EncDec\nmodel have potential drawbacks when ap-\nplied to GEC. For example, the distribution\nof the inputs to a GEC model can be con-\nsiderably different (erroneous, clumsy, etc.)\nfrom that of the corpora used for pre-training\nMLMs; however, this issue is not addressed\nin the previous methods. Our experiments\nshow that our proposed method, where we\nﬁrst ﬁne-tune a MLM with a given GEC\ncorpus and then use the output of the ﬁne-\ntuned MLM as additional features in the GEC\nmodel, maximizes the beneﬁt of the MLM.\nThe best-performing model achieves state-of-\nthe-art performances on the BEA-2019 and\nCoNLL-2014 benchmarks. Our code is pub-\nlicly available at: https://github.com/\nkanekomasahiro/bert-gec.\n1 Introduction\nGrammatical Error Correction (GEC) is a sequence-\nto-sequence task where a model corrects an un-\ngrammatical sentence to a grammatical sentence.\nNumerous studies on GEC have successfully used\nencoder-decoder (EncDec) based models, and in\nfact, most current state-of-the-art neural GEC mod-\nels employ this architecture (Zhao et al., 2019;\nGrundkiewicz et al., 2019; Kiyono et al., 2019).\nIn light of this trend, one natural, intriguing\nquestion is whether neural EndDec GEC models\ncan beneﬁt from the recent advances of masked\nlanguage models (MLMs) since MLMs such as\nBERT (Devlin et al., 2019) have been shown to\nyield substantial improvements in a variety of NLP\ntasks (Qiu et al., 2020). BERT, for example, builds\non the Transformer architecture (Vaswani et al.,\n2017) and is trained on large raw corpora to learn\ngeneral representations of linguistic components\n(e.g., words and sentences) in context, which have\nbeen shown useful for various tasks. In recent years,\nMLMs have been used not only for classiﬁcation\nand sequence labeling tasks but also for language\ngeneration, where combining MLMs with EncDec\nmodels of a downstream task makes a noticeable\nimprovement (Lample and Conneau, 2019).\nCommon methods of incorporating a MLM to\nan EncDec model are initialization (init) and fu-\nsion (fuse). In the init method, the downstream\ntask model is initialized with the parameters of a\npre-trained MLM and then is trained over a task-\nspeciﬁc training set (Lample and Conneau, 2019;\nRothe et al., 2019). This approach, however, does\nnot work well for tasks like sequence-to-sequence\nlanguage generation tasks because such tasks tend\nto require a huge amount of task-speciﬁc train-\ning data and ﬁne-tuning a MLM with such a large\ndataset tends to destruct its pre-trained representa-\ntions leading to catastrophic forgetting (Zhu et al.,\n2020; McCloskey and Cohen, 1989). In the fuse\nmethod, pre-trained representations of a MLM are\nused as additional features during the training of a\ntask-speciﬁc model (Zhu et al., 2020). When ap-\nplying this method for GEC, what the MLM has\nlearned in pre-training will be preserved; however,\nthe MLM will not be adapted to either the GEC task\nor the task-speciﬁc distribution of inputs (i.e., er-\nroneous sentences in a learner corpus), which may\nhinder the GEC model from effectively exploiting\nthe potential of the MLM. Given these drawbacks\nin the two common methods, it is not as straightfor-\nward to gain the advantages of MLMs in GEC as\none might expect. This background motivates us\nto investigate how a MLM should be incorporated\ninto an EncDec GEC model to maximize its bene-\n4249\nﬁt. To the best of our knowledge, no research has\naddressed this research question.\nIn our investigation, we employ BERT, which is\na widely used MLM (Qiu et al., 2020), and eval-\nuate the following three methods: (a) initialize\nan EncDec GEC model using pre-trained BERT\nas in Lample and Conneau (2019) (BERT-init),\n(b) pass the output of pre-trained BERT into the\nEncDec GEC model as additional features (BERT-\nfuse) (Zhu et al., 2020), and (c) combine the best\nparts of (a) and (b).\nIn this new method (c), we ﬁrst ﬁne-tune BERT\nwith the GEC corpus and then use the output of\nthe ﬁne-tuned BERT model as additional features\nin the GEC model. To implement this, we fur-\nther consider two options: (c1) additionally train\npre-trained BERT with GEC corpora (BERT-fuse\nmask), and (c2) ﬁne-tune pre-trained BERT by\nway of the grammatical error detection (GED) task\n(BERT-fuse GED). In (c2), we expect that the GEC\nmodel will be trained so that it can leverage both the\nrepresentations learned from large general corpora\n(pre-trained BERT) and the task-speciﬁc informa-\ntion useful for GEC induced from the GEC training\ndata.\nOur experiments show that using the output of\nthe ﬁne-tuned BERT model as additional features\nin the GEC model (method (c)) is the most effec-\ntive way of using BERT in most of the GEC cor-\npora that we used in the experiments. We also\nshow that the performance of GEC improves fur-\nther by combining the BERT-fuse mask and BERT-\nfuse GED methods. The best-performing model\nachieves state-of-the-art results on the BEA-2019\nand CoNLL-2014 benchmarks.\n2 Related Work\nStudies have reported that a MLM can improve the\nperformance of GEC when it is employed either\nas a re-ranker (Chollampatt et al., 2019; Kaneko\net al., 2019) or as a ﬁltering tool (Asano et al.,\n2019; Kiyono et al., 2019). EncDec-based GEC\nmodels combined with MLMs can also be used in\ncombination with these pipeline methods. Asano\net al. (2019) proposed sequence labeling models\nbased on correction methods. Our method can uti-\nlize the existing EncDec GEC knowledge, but these\nmethods cannot be utilized due to the different ar-\nchitecture of the model. Besides, to the best of our\nknowledge, no research has yet been conducted that\nincorporates information of MLMs for effectively\ntraining the EncDec GEC model.\nMLMs are generally used in downstream tasks\nby ﬁne-tuning (Liu, 2019; Zhang et al., 2019), how-\never, Zhu et al. (2020) demonstrated that it is more\neffective to provide the output of the ﬁnal layer of\na MLM to the EncDec model as contextual embed-\ndings. Recently, Weng et al. (2019) addressed the\nmismatch problem between contextual knowledge\nfrom pre-trained models and the target bilingual\nmachine translation. Here, we also claim that ad-\ndressing the gap between grammatically correct\nraw corpora and GEC corpora can lead to the im-\nprovement of GEC systems.\n3 Methods for Using Pre-trained MLM\nin GEC Model\nIn this section, we describe our approaches for\nincorporating a pre-trained MLM into our GEC\nmodel. Speciﬁcally, we chose the following ap-\nproaches: (1) initializing a GEC model using\nBERT; (2) using BERT output as additional fea-\ntures for a GEC model, and (3) using the output\nof BERT ﬁne-tuned with the GEC corpora as addi-\ntional features for a GEC model.\n3.1 BERT-init\nWe create a GEC EncDec model initialized with\nBERT weights. This approach is based on Lample\nand Conneau (2019). Most recent state-of-the-art\nmethods use pseudo-data, which is generated by\ninjecting pseudo-errors to grammatically correct\nsentences. However, note that this method cannot\ninitialize a GEC model with pre-trained parameters\nlearned from pseudo-data.\n3.2 BERT-fuse\nWe use the model proposed by Zhu et al. (2020) as a\nfeature-based approach (BERT-fuse). This model is\nbased on Transformer EncDec architecture. It takes\nan input sentence X = (x1,...,x n), where nis its\nlength. xi is i-th token in X. First, BERT encodes\nit and outputs a representation B = (b1,...,b n).\nNext, the GEC model encodes X and B as inputs.\nhl\ni ∈H is the i-th hidden representation of the l-th\nlayer of the encoder in the GEC model. h0 stands\nfor word embedding of an input sentence X. Then\nwe calculate ˜hl\ni as follows:\n˜hl\ni = 1\n2(Ah(hl−1\ni ,Hl−1) +Ab(hl−1\ni ,Bl−1)) (1)\nwhere Ah and Ab are attention models for the hid-\nden layers of the GEC encoder H and the BERT\n4250\noutput B, respectively. Then each ˜hl\ni is further\nprocessed by the feedforward network F which\noutputs the l-th layer Hl = (F(˜hl\n1),...,F (˜hl\nn)).\nThe decoder’s hidden statesl\nt ∈S is calculated as\nfollows:\nˆsl\nt = As(sl−1\nt ,Sl−1\n<t+1) (2)\n˜sl\ni = 1\n2(Ah(ˆsl−1\ni ,Hl−1) +Ab(ˆsl−1\ni ,Bl−1)) (3)\nsl\nt = F(˜sl\nt) (4)\nHere, As represents the self-attention model. Fi-\nnally, sL\nt is processed via a linear transformation\nand softmax function to predict the t-th word ˆyt.\nWe also use the drop-net trick proposed by Zhu\net al. (2020) to the output of BERT and the encoder\nof the GEC model.\n3.3 BERT-fuse Mask and GED\nThe advantage of the BERT-fuse is that it can pre-\nserve pre-trained information from raw corpora,\nhowever, it may not be adapted to either the GEC\ntask or the task-speciﬁc distribution of inputs. The\nreason is that in the GEC model, unlike the data\nused for training BERT, the input can be an erro-\nneous sentence. To ﬁll the gap between corpora\nused to train GEC and BERT, we additionally train\nBERT on GEC corpora (BERT-fuse mask) or ﬁne-\ntune BERT as a GED model (BERT-fuse GED) and\nuse it for BERT-fuse. GED is a sequence label-\ning task that detects grammatically incorrect words\nin input sentences (Rei and Yannakoudakis, 2016;\nKaneko et al., 2017). Since BERT is also effective\nin GED (Bell et al., 2019; Kaneko and Komachi,\n2019), it is considered to be suitable for ﬁne-tuning\nto take into account grammatical errors.\n4 Experimental Setup\n4.1 Train and Development Sets\nWe use the BEA-2019 workshop 1 (Bryant et al.,\n2019) ofﬁcial shared task data as training and de-\nvelopment sets. Speciﬁcally, to train a GEC model,\nwe use W&I-train (Granger, 1998; Yannakoudakis\net al., 2018), NUCLE (Dahlmeier et al., 2013),\nFCE-train (Yannakoudakis et al., 2011) and Lang-8\n(Mizumoto et al., 2011) datasets. We use W&I-dev\nas a development set. Note that we excluded sen-\ntence pairs that were not corrected from the training\ndata. To train BERT for BERT-fuse mask and GED,\n1https://www.cl.cam.ac.uk/research/nl/\nbea2019st/\nGEC model\nModel Architecture Transformer (big)\nNumber of epochs 30\nMax tokens 4096\nOptimizer Adam\n(β1 = 0.9,β2 = 0.98,ϵ= 1×10−8)\nLearning rate 3×10−5\nMin learning rate 1×10−6\nLoss function label smoothed cross-entropy\n(ϵls= 0.1)\n(Szegedy et al., 2016)\nDropout 0.3\nGradient Clipping 0.1\nBeam search 5\nGED model\nModel Architecture BERT-Base (cased)\nNumber of epochs 3\nBatch size 32\nMax sentence length 128\nOptimizer Adam\n(β1 = 0.9,β2 = 0.999,ϵ= 1×10−8)\nLearning rate 4e−5\nDropout 0.1\nTable 1: Hyperparameters values of GEC model and\nFine-tuned BERT.\nwe use W&I-train, NUCLE, and FCE-train as train-\ning, and W&I-dev was used as development data.\n4.2 Evaluating GEC Performance\nIn GEC, it is important to evaluate the model with\nmultiple datasets (Mita et al., 2019). Therefore,\nwe used GEC evaluation data such as W&I-test,\nCoNLL-2014 (Ng et al., 2014), FCE-test and JF-\nLEG (Napoles et al., 2017). We used ERRANT\nevaluation metrics (Felice et al., 2016; Bryant et al.,\n2017) for W&I-test, M2 score (Dahlmeier and Ng,\n2012) for CoNLL-2014 and FCE-test sets, and\nGLEU (Napoles et al., 2015) for JFLEG. All our\nresults (except ensemble) are the average of four\ndistinct trials using four different random seeds.\n4.3 Models\nHyperparameter values for the GEC model is listed\nin Table 1. For the BERT initialized GEC model,\nwe provided experiments based on the open-source\ncode2. For the BERT-fuse GEC model, we use the\ncode provided by Zhu et al. (2020) 3. While the\ntraining the GEC model, the model was evaluated\non the development set and saved every epoch. If\nloss did not drop at the end of an epoch, the learn-\ning rate was multiplied by 0.7. The training was\n2https://github.com/facebookresearch/\nXLM\n3https://github.com/bert-nmt/bert-nmt\n4251\nBEA-test (ERRANT) CoNLL-14 (M2) FCE-test ( M2) JFLEG\nP R F0.5 P R F0.5 P R F0.5 GLEU\nw/o BERT 51.5 43.2 49.6 59.2 31.2 50.2 61.7 46.4 57.9 52.7\nBERT-init 55.1 43.7 52.4 61.3 31.5 51.4 62.4 46.9 58.5 53.0\nBERT-fuse 57.5 44.9 54.4 62.3 31.3 52.0 64.0 47.6 59.8 54.1\nBERT-fuse mask 57.1 44.7 54.1 62.9 32.2 52.8 64.3 48.1 60.2 54.2\nBERT-fuse GED 58.1 44.8 54.8 63.6 33.0 53.6 65.0 49.6 61.2 54.4\nw/o BERT 66.1 59.9 64.8 68.5 44.8 61.9 56.5 48.1 54.9 61.0\nBERT-fuse 66.6 60.0 65.2 68.3 45.7 62.1 59.7 48.5 57.0 61.2\nBERT-fuse mask 67.0 60.0 65.4 68.8 45.3 62.3 59.7 47.1 56.6 61.2\nBERT-fuse GED 67.1 60.1 65.6 69.2 45.6 62.6 59.8 46.9 56.7 61.3\nLichtarge et al. (2019) - - - 65.5 37.1 56.8 - - - 61.6\nAwasthi et al. (2019) - - - 66.1 43.0 59.7 - - - 60.3\nKiyono et al. (2019) 65.5 59.4 64.2 67.9 44.1 61.3 - - - 59.7\nBERT-fuse GED + R2L 72.3 61.4 69.8 72.6 46.4 65.2 62.8 48.8 59.4 62.0\nLichtarge et al. (2019) - - - 66.7 43.9 60.4 - - - 63.3\nGrundkiewicz et al. (2019) 72.3 60.1 69.5 - - 64.2 - - - 61.2\nKiyono et al. (2019)∗ 74.7 56.7 70.2 72.4 46.1 65.0 - - - 61.4\nTable 2: Results of our GEC models. The top group shows the results of the single models without using pseudo-\ndata and/or ensemble. The second group shows the results of the single models using pseudo-data. The third\ngroup shows ensemble models using pseudo-data. Bold indicates the highest score in each column. * reports the\nstate-of-the-art scores for BEA test and CoNLL 2014 for two separate models: models with and without SED. We\nﬁlled out a single line with the results from such two separate models.\nstopped if the learning rate was less than the mini-\nmum learning rate or if the learning epoch reached\nthe maximum epoch number of 30.\nTraining BERT for BERT-fuse mask and GED\nwas based on the code from Wolf et al. (2019) 4.\nThe additional training for the BERT-fuse mask\nwas done in the Devlin et al. (2019)’s setting. Hy-\nperparameter values for the GED model is listed in\nTable 1. We used the BERT-Base cased model, for\nconsistency across experiments5. The model was\nevaluated on the development set.\n4.4 Pseudo-data\nWe also performed experiments utilizing BERT-\nfuse, BERT-fuse mask, and BERT-fuse GED out-\nputs as additional features to the pre-trained on the\npseudo-data GEC model. The pre-trained model\nusing pseudo-data was initialized with the PRET-\nLARGE +SSE model used in the Kiyono et al.\n(2019)6 experiments. This pseudo-data is gener-\nated by probabilistically injecting character errors\ninto the output (Lichtarge et al., 2019) of a back-\n4https://github.com/huggingface/\ntransformers\n5https://github.com/google-research/\nbert\n6https://github.com/butsugiri/\ngec-pseudodata\ntranslation (Xie et al., 2018) model that generates\ngrammatically incorrect sentences from grammati-\ncally correct sentences (Kiyono et al., 2019).\n4.5 Right-to-left (R2L) Re-ranking for\nEnsemble\nWe describe the R2L re-ranking technique incor-\nporated in our experiments proposed by Sennrich\net al. (2016), which proved to be efﬁcient for the\nGEC task (Grundkiewicz et al., 2019; Kiyono et al.,\n2019). Standard left-to-right (L2R) models gener-\nate the n-best hypotheses using scores with the\nnormal ensemble and R2L models re-score them.\nThen, we re-rank the n-best candidates based on\nthe sum of the L2R and R2L scores. We use the\ngeneration probability as a re-ranking score and\nensemble four L2R models and four R2L models.\n5 Results\nTable 2 shows the experimental results of the GEC\nmodels. A model trained on Transformer with-\nout using BERT is denoted as “w/o BERT.” In\nthe top groups of results, it can be seen that using\nBERT consistently improves the accuracy of our\nGEC model. Also, BERT-fuse, BERT-fuse mask,\nand BERT-fuse GED outperformed the BERT-init\nmodel in almost all cases. Furthermore, we can\n4252\nsee that using BERT considering GEC corpora as\nBERT-fuse leads to better correction results. And\nthe BERT-fuse GED always gives better results\nthan the BERT-fuse mask. This may be because\nthe BERT-fuse GED is able to explicitly consider\ngrammatical errors. In the second row, the correc-\ntion results are improved by using BERT as well.\nAlso in this setting, BERT-fuse GED outperformed\nother models in all cases except for the FCE-test set,\nthus, achieving state-of-the-art results with a single\nmodel on the BEA2019 and CoNLL14 datasets.\nIn the last row, the ensemble model yielded high\nscores on all corpora, improving state-of-the-art\nresults by 0.2 points in CoNLL14.\n6 Analysis\n6.1 Hidden Representation Visualization\nWe investigate the characteristics of the hidden\nrepresentations of vanilla (i.e., without any ﬁne-\ntuning) BERT and BERT ﬁne-tuned with GED. We\nvisualize the hidden representations of the same\nwords from the last layer of BERT HL. They were\nchosen depending on correctness in a different con-\ntext, using the above models. These target eight\nwords7 that have been mistaken more than 50 times,\nwere chosen from W&I-dev. We sampled the same\nnumber of correctly used cases for the same word\nfrom the corrected side of W&I-dev.\nFigure 1 visualizes hidden representations of\nBERT and ﬁne-tuned BERT. It can be seen that\nthe vanilla BERT does not distinguish between cor-\nrect and incorrect clusters. The plotted eight words\nare gathered together, and it can be seen that hid-\nden representations of the same word gather in the\nsame place regardless of correctness. On the other\nhand, ﬁne-tuned BERT produces a vector space that\ndemonstrates correct and incorrect words on differ-\nent sides, showing that hidden representations take\ngrammatical errors into account when ﬁne-tuned\non GEC corpora. Moreover, it can be seen that the\ncorrect cases divided into 8 clusters, implying that\nBERT’s information is also retained.\n6.2 Performance for Each Error Type\nWe investigate the correction results for each error\ntype. We use ERRANT (Felice et al., 2016; Bryant\net al., 2017) to measure F0.5 of the model for each\nerror type. ERRANT can automatically assign er-\nror types from source and target sentences. We\n71. the 2. , 3. in 4. to 5. of 6. a 7. for 8. is\n(a) BERT\n (b) Fine-tuned BERT\nFigure 1: Hidden representation visualization for en-\ncoded grammatically correct and incorrect words.\nError type BERT-fuse GED w/o BERT\nPUNCT 40.2 36.8\nOTHER 20.4 19.1\nDET 48.8 45.4\nPREP 36.7 34.8\nVERB:TENSE 36.0 34.1\nTable 3: The result of single Fine-tuned BERT-fuse and\nw/o BERT models without using pseudo-data on most\nerror types including all the top-5 frequent types of er-\nror in W&I-dev\nuse single BERT-fuse GED and w/o BERT models\nwithout using pseudo-data for this investigation.\nTable 3 shows the results of single BERT-fuse\nGED and w/o BERT models without using pseudo-\ndata on most error types including all the top-5 fre-\nquent error types in W&I-dev. We see that BERT-\nfuse GED is better for all error types compared to\nw/o BERT. We can say that the use of BERT ﬁne-\ntuned by GED for the EncDec model improves the\nperformance independently of the error type.\n7 Conclusion\nIn this paper, we investigated how to effectively\nuse MLMs for training GEC models. Our results\nshow that BERT-fuse GED was one of the most\neffective techniques when it was ﬁne-tuned with\nGEC corpora. In future work, we will investigate\nwhether BERT-init can be used effectively by using\nmethods to deal with catastrophic forgetting.\nAcknowledgments\nThis work was supported by JSPS KAKENHI\nGrant Number 19J14084 and 19H04162. We thank\neveryone in Inui and Suzuki Lab at the Tohoku Uni-\nversity and Language Information Access Technol-\nogy Team of RIKEN AIP. We thank the anonymous\nreviewers for their valuable comments.\n4253\nReferences\nHiroki Asano, Masato Mita, Tomoya Mizumoto, and\nJun Suzuki. 2019. The AIP-Tohoku System at the\nBEA-2019 Shared Task. In BEA, pages 176–182,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,\nSabyasachi Ghosh, and Vihari Piratla. 2019. Parallel\nIterative Edit Models for Local Sequence Transduc-\ntion. In EMNLP-IJCNLP, pages 4259–4269, Hong\nKong, China. Association for Computational Lin-\nguistics.\nSamuel Bell, Helen Yannakoudakis, and Marek Rei.\n2019. Context is Key: Grammatical Error De-\ntection with Contextual Word Representations. In\nBEA, pages 103–115, Florence, Italy. Association\nfor Computational Linguistics.\nChristopher Bryant, Mariano Felice, Øistein E. Ander-\nsen, and Ted Briscoe. 2019. The BEA-2019 Shared\nTask on Grammatical Error Correction. In BEA,\npages 52–75, Florence, Italy. Association for Com-\nputational Linguistics.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic Annotation and Evaluation of Er-\nror Types for Grammatical Error Correction. In\nACL, pages 793–805, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nShamil Chollampatt, Weiqi Wang, and Hwee Tou Ng.\n2019. Cross-Sentence Grammatical Error Correc-\ntion. In ACL, Florence, Italy.\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better\nEvaluation for Grammatical Error Correction. In\nNAACL, pages 568–572, Montr ´eal, Canada. Associ-\nation for Computational Linguistics.\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.\n2013. Building a Large Annotated Corpus of\nLearner English: The NUS Corpus of Learner En-\nglish. In BEA, pages 22–31, Atlanta, Georgia. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL, pages 4171–4186, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMariano Felice, Christopher Bryant, and Ted Briscoe.\n2016. Automatic Extraction of Learner Errors\nin ESL Sentences Using Linguistically Enhanced\nAlignments. In COLING, pages 825–835, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nSylviane Granger. 1998. Developing an Automated\nWriting Placement System for ESL Learners. In\nLEC, pages 3–18.\nRoman Grundkiewicz, Marcin Junczys-Dowmunt, and\nKenneth Heaﬁeld. 2019. Neural Grammatical Error\nCorrection Systems with Unsupervised Pre-training\non Synthetic Data. In BEA, pages 252–263, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMasahiro Kaneko, Kengo Hotate, Satoru Katsumata,\nand Mamoru Komachi. 2019. TMU Transformer\nSystem Using BERT for Re-ranking at BEA 2019\nGrammatical Error Correction on Restricted Track.\nIn BEA, pages 207–212, Florence, Italy. Association\nfor Computational Linguistics.\nMasahiro Kaneko and Mamoru Komachi. 2019. Multi-\nHead Multi-Layer Attention to Deep Language Rep-\nresentations for Grammatical Error Detection. Com-\nputaci´on y Sistemas, 23.\nMasahiro Kaneko, Yuya Sakaizawa, and Mamoru Ko-\nmachi. 2017. Grammatical Error Detection Using\nError- and Grammaticality-Speciﬁc Word Embed-\ndings. In IJCNLP, pages 40–48, Taipei, Taiwan.\nAsian Federation of Natural Language Processing.\nShun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizu-\nmoto, and Kentaro Inui. 2019. An Empirical Study\nof Incorporating Pseudo Data into Grammatical Er-\nror Correction. In EMNLP-IJCNLP, pages 1236–\n1242, Hong Kong, China. Association for Compu-\ntational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. arXiv.\nJared Lichtarge, Chris Alberti, Shankar Kumar, Noam\nShazeer, Niki Parmar, and Simon Tong. 2019. Cor-\npora Generation for Grammatical Error Correction.\nIn NAACL, pages 3291–3301, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nYang Liu. 2019. Fine-tune BERT for Extractive Sum-\nmarization. arXiv.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic Interference in Connectionist Networks: The\nSequential Learning Problem.\nMasato Mita, Tomoya Mizumoto, Masahiro Kaneko,\nRyo Nagata, and Kentaro Inui. 2019. Cross-\nCorpora Evaluation and Analysis of Grammatical\nError Correction Models — Is Single-Corpus Eval-\nuation Enough? In NAACL, pages 1309–1314, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nTomoya Mizumoto, Mamoru Komachi, Masaaki Na-\ngata, and Yuji Matsumoto. 2011. Mining Revi-\nsion Log of Language Learning SNS for Auto-\nmated Japanese Error Correction of Second Lan-\nguage Learners. In IJCNLP, pages 147–155, Chi-\nang Mai, Thailand. Asian Federation of Natural Lan-\nguage Processing.\n4254\nCourtney Napoles, Keisuke Sakaguchi, Matt Post, and\nJoel Tetreault. 2015. Ground Truth for Grammati-\ncal Error Correction Metrics. In NAACL, pages 588–\n593, Beijing, China. Association for Computational\nLinguistics.\nCourtney Napoles, Keisuke Sakaguchi, and Joel\nTetreault. 2017. JFLEG: A Fluency Corpus and\nBenchmark for Grammatical Error Correction. In\nEACL, pages 229–234, Valencia, Spain. Association\nfor Computational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 Shared Task\non Grammatical Error Correction. In CoNLL, pages\n1–14, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nModels for Natural Language Processing: A Survey.\nMarek Rei and Helen Yannakoudakis. 2016. Com-\npositional Sequence Labeling Models for Error De-\ntection in Learner Writing. In ACL, pages 1181–\n1191, Berlin, Germany. Association for Computa-\ntional Linguistics.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2019. Leveraging pre-trained checkpoints for se-\nquence generation tasks. arXiv.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Edinburgh Neural Machine Translation Sys-\ntems for WMT 16. In WMT, pages 371–376, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the Inception Architecture for Computer Vi-\nsion. In IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, NeurIPS, pages 5998–6008. Curran As-\nsociates, Inc.\nRongxiang Weng, Heng Yu, Shujian Huang, Shanbo\nCheng, and Weihua Luo. 2019. Acquiring Knowl-\nedge from Pre-trained Model to Neural Machine\nTranslation. arXiv.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. ArXiv.\nZiang Xie, Guillaume Genthial, Stanley Xie, Andrew\nNg, and Dan Jurafsky. 2018. Noising and Denois-\ning Natural Language: Diverse Backtranslation for\nGrammar Correction. In NAACL, pages 619–628,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A New Dataset and Method for Automatically\nGrading ESOL Texts. In NAACL, pages 180–189,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nHelen Yannakoudakis, Øistein E. Andersen, Geran-\npayeh Ardeshir, Briscoe Ted, and Nicholls Diane.\n2018. Developing an Automated Writing Placement\nSystem for ESL Learners. In Applied Measurement\nin Education, pages 251–267.\nHaoyu Zhang, Yeyun Gong, Yu Yan, Nan Duan, Jian-\njun Xu, Ji Wang, Ming Gong, and Ming Zhou. 2019.\nPretraining-Based Natural Language Generation for\nText Summarization. In CoNLL. Association for\nComputational Linguistics.\nWei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and\nJingming Liu. 2019. Improving Grammatical Error\nCorrection via Pre-Training a Copy-Augmented Ar-\nchitecture with Unlabeled Data. In NAACL, pages\n156–165, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tieyan Liu. 2020.\nIncorporating BERT into Neural Machine Transla-\ntion. In ICLR."
}