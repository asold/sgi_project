{
  "title": "Leveraging Large Language Models in the delivery of post-operative dental care: a comparison between an embedded GPT model and ChatGPT",
  "url": "https://openalex.org/W4399567363",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2740704021",
      "name": "Itrat Batool",
      "affiliations": [
        "Aga Khan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4222408457",
      "name": "Nighat Naved",
      "affiliations": [
        "Aga Khan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2317707620",
      "name": "Syed Murtaza Raza Kazmi",
      "affiliations": [
        "Aga Khan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2041191786",
      "name": "Fahad Umer",
      "affiliations": [
        "Aga Khan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2740704021",
      "name": "Itrat Batool",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222408457",
      "name": "Nighat Naved",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2317707620",
      "name": "Syed Murtaza Raza Kazmi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041191786",
      "name": "Fahad Umer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2776904200",
    "https://openalex.org/W4233275153",
    "https://openalex.org/W3097197838",
    "https://openalex.org/W3038004684",
    "https://openalex.org/W4383737134",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4379231355",
    "https://openalex.org/W4375955709",
    "https://openalex.org/W4386110374",
    "https://openalex.org/W4327582601",
    "https://openalex.org/W4206655044",
    "https://openalex.org/W2048795968",
    "https://openalex.org/W2996312081",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4385346108",
    "https://openalex.org/W4389993479",
    "https://openalex.org/W4388210476",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4385436582",
    "https://openalex.org/W4389396369",
    "https://openalex.org/W4390645866",
    "https://openalex.org/W4383817745",
    "https://openalex.org/W4390706405",
    "https://openalex.org/W2777801717",
    "https://openalex.org/W4367626167",
    "https://openalex.org/W4361188952",
    "https://openalex.org/W4388485443",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "Abstract Objective This study underscores the transformative role of Artificial Intelligence (AI) in healthcare, particularly the promising applications of Large Language Models (LLMs) in the delivery of post-operative dental care. The aim is to evaluate the performance of an embedded GPT model and its comparison with ChatGPT-3.5 turbo. The assessment focuses on aspects like response accuracy, clarity, relevance, and up-to-date knowledge in addressing patient concerns and facilitating informed decision-making. Material and methods An embedded GPT model, employing GPT-3.5-16k, was crafted via GPT-trainer to answer postoperative questions in four dental specialties including Operative Dentistry &amp; Endodontics, Periodontics, Oral &amp; Maxillofacial Surgery, and Prosthodontics. The generated responses were validated by thirty-six dental experts, nine from each specialty, employing a Likert scale, providing comprehensive insights into the embedded GPT model’s performance and its comparison with GPT3.5 turbo. For content validation, a quantitative Content Validity Index (CVI) was used. The CVI was calculated both at the item level (I-CVI) and scale level (S-CVI/Ave). To adjust I-CVI for chance agreement, a modified kappa statistic (K*) was computed. Results The overall content validity of responses generated via embedded GPT model and ChatGPT was 65.62% and 61.87% respectively. Moreover, the embedded GPT model revealed a superior performance surpassing ChatGPT with an accuracy of 62.5% and clarity of 72.5%. In contrast, the responses generated via ChatGPT achieved slightly lower scores, with an accuracy of 52.5% and clarity of 67.5%. However, both models performed equally well in terms of relevance and up-to-date knowledge. Conclusion In conclusion, embedded GPT model showed better results as compared to ChatGPT in providing post-operative dental care emphasizing the benefits of embedding and prompt engineering, paving the way for future advancements in healthcare applications.",
  "full_text": "ARTICLE OPEN\nLeveraging Large Language Models in the delivery of\npost-operative dental care: a comparison between an\nembedded GPT model and ChatGPT\nItrat Batool1, Nighat Naved1, Syed Murtaza Raza Kazmi 1 and Fahad Umer 1 ✉\n© The Author(s) 2024\nOBJECTIVE: This study underscores the transformative role of Artiﬁcial Intelligence (AI) in healthcare, particularly the promising\napplications of Large Language Models (LLMs) in the delivery of post-operative dental care. The aim is to evaluate the performance\nof an embedded GPT model and its comparison with ChatGPT-3.5 turbo. The assessment focuses on aspects like response accuracy,\nclarity, relevance, and up-to-date knowledge in addressing patient concerns and facilitating informed decision-making.\nMATERIAL AND METHODS:An embedded GPT model, employing GPT-3.5-16k, was crafted via GPT-trainer to answer\npostoperative questions in four dental specialties including Operative Dentistry & Endodontics, Periodontics, Oral & Maxillofacial\nSurgery, and Prosthodontics. The generated responses were validated by thirty-six dental experts, nine from each specialty,\nemploying a Likert scale, providing comprehensive insights into the embedded GPT model’s performance and its comparison with\nGPT3.5 turbo. For content validation, a quantitative Content Validity Index (CVI) was used. The CVI was calculated both at the item\nlevel (I-CVI) and scale level (S-CVI/Ave). To adjust I-CVI for chance agreement, a modiﬁed kappa statistic (K*) was computed.\nRESULTS: The overall content validity of responses generated via embedded GPT model and ChatGPT was 65.62% and 61.87%\nrespectively. Moreover, the embedded GPT model revealed a superior performance surpassing ChatGPT with an accuracy of 62.5%\nand clarity of 72.5%. In contrast, the responses generated via ChatGPT achieved slightly lower scores, with an accuracy of 52.5% and\nclarity of 67.5%. However, both models performed equally well in terms of relevance and up-to-date knowledge.\nCONCLUSION: In conclusion, embedded GPT model showed better results as compared to ChatGPT in providing post-operative\ndental care emphasizing the beneﬁts of embedding and prompt engineering, paving the way for future advancements in\nhealthcare applications.\nBDJ Open           (2024) 10:48 ; https://doi.org/10.1038/s41405-024-00226-3\nINTRODUCTION\nMachine learning (ML) is a subset of Artiﬁcial Intelligence (AI) that\nallows computer systems to analyze data, identify patterns, and\nmake intelligent decisions without explicit programming [1]. The\nintegration of AI in healthcare has become imperative as the current\nhealthcare infrastructure is ill-prepared to manage the increased\nclinical workload, resulting in extended patient wait times, burnout\namong healthcare professionals, and added strain on the healthcare\nsystem [2]. Moreover, it can contribute to more accurate diagnosis,\ntreatment planning, improved patient interaction, and the automa-\ntion of various tasks within theﬁeld of dentistry [3]. Amongst the\nmany tasks that AI performs, its role in patient interaction is\nincreasingly signiﬁcant, bringing about improvements in commu-\nnication and overall healthcare experience [4].\nLarge Language Model (LLM) such as Open AI ’s ChatGPT\n(Generative Pre-trained Transformer) within the realm of Natural\nLanguage Processing (NLP) is a subset of ML that analyzes and\nresponds to human language input in a conversational manner\n[5]. These models leverage large-scale pre-training on diverse\ndatasets, learning contextual relationships and generating human-\nlike text. Moreover, they can be embedded for speciﬁc applica-\ntions or a variety of language-related tasks [6].\nAn AI chatbot is a computer program that utilizes LLM to\ninterpret user input in the form of text or speech and generate\ncontextually relevant responses [5]. Such chatbot models can offer\npatients swift and convenient access to precise and trustworthy\ninformation cost-effectively and with round-the-clock availability\n[7]. While these conversational bots exhibit certain capabilities,\nthey still necessitate oversight from surgeons and their healthcare\nteams [8]. In the ﬁeld of medicine various chat-bots have been\nevaluated such as the study by Lim et al. which analyzed the\nperformance of LLM models for myopia care and study by Dwyer\net al. that assessed the conversational chatbot performance post\nhip arthroscopy surgery [ 9, 10]. However, a comprehensive\nassessment of large language models in answering patients ’\npost-operative questions in theﬁeld of dentistry has not yet been\nthoroughly evaluated.\nDental procedures require patients to adhere to speciﬁc post-\noperative instructions diligently. These instructions, if followed\nstrictly, can signiﬁcantly inﬂ\nuence the outcome of the procedures\nReceived: 25 March 2024 Revised: 1 May 2024 Accepted: 7 May 2024\n1Section of Dentistry, Department of Surgery, Aga Khan University Hospital, Karachi, Pakistan.✉email: fahad.umer@aku.edu\nwww.nature.com/bdjopen\n1234567890();,:\nand the patient’s overall experience [11]. However, ensuring that\npatients not only receive but also comprehend and adhere to\nthese instructions has been a longstanding challenge for dental\npractitioners. This is where the innovative integration of chatbots\ninto dental care can become invaluable [8].\nThis paper aims to evaluate the performance of an embedded\nGPT model (conversational chatbot) in providing dental post-\noperative instructions to patients and its comparison with ChatGPT-\n3.5 turbo. The assessment focuses on aspects like response\naccuracy, clarity, relevance, and up-to-date knowledge in addres-\nsing patient concerns and facilitating informed decision-making.\nThis study addresses the need for innovative solutions in dental\ncare by evaluating the effectiveness of embedded GPT models in\nproviding post-operative instructions to patients, aiming to\nimprove patient comprehension and adherence. By comparing\nthe performance of ChatGPT-3.5 turbo, this research aims to assess\nthe viability of advanced conversational chatbots in enhancing\npatient experience and decision-making in dentistry.\nMATERIALS AND METHODS\nAlthough ethical approval for the conduct of study was not required, the\nresearch has been done within local ethical frameworks and regulations, as\noutlined in the Declaration of Helsinki. Creating a custom conversational\nchatbot involved a meticulous process where we carefully tailored the\nembedded GPT chatbot through distinct phases. GPT-3.5-16k version was\nselected after the registration on GPT trainer website ( https://gpt-\ntrainer.com/), that provides access to customized features. The embedded\nmodel was conﬁgured to adhere to the following base prompt:\n“I want you to roleplay as AI Assistant for dental problems. You will\nprovide me with answers from the given context. The answers should be\nas precise as possible in no more than 50 words. Do not refer to empirical\nremedies. Do not answer any irrelevant questions unrelated to dental\nproblems and do not answer any medical health problems. Never break\ncharacter.”\nAdditionally, to ensure that the chatbot is well informed, it was\nembedded by providing relevant dental post-operative instructions\nscraped from the internet in a Wordﬁle format (Supplementary Note 1).\nThe embedding stores all the necessary information for the LLM model to\nsearch and produce relevant outputs in response to the user’s query. These\nvectors, containing embeddings, are stored in an index within a vector\ndatabase, providing a structured organization of information, a process\nknown as Retrieval Augmentation Generation (RAG) [12]. We intentionally\nset the temperature of the embedded model at a low level to mitigate\nincidence of hallucinations.\nLikewise, ChatGPT-3.5 turbo ( https://chat.openai.com/) underwent\nprompt engineering, utilizing the same base prompt but without the\ninclusion of the embedded ﬁle. The default temperature setting of the\nmodel was not changed.\nIn the following text, for the convenience of readers, the embedded GPT\n3.5-16k model (conversational chatbot) and OpenAI’s ChatGPT 3.5 turbo\n(prompt engineered) will be referred to as“embedded GPT model” and\n“ChatGPT” respectively.\nA questionnaire was generated representing 40 real life questions\n(Supplementary Note 2) from four specialties of dentistry, namely: Oral\nand Maxillofacial Surgery, Operative Dentistry & Endodontics, Periodontics\nand Prosthodontics. The questions were designed to reﬂect the types of\ninquiries posed by the patients following dental procedures with the aim\nof providing a representative sample of questions the chatbot would\nencounter in a real-world setting. The formulated questions were validated\nby a panel of experts belonging to the respective specialties. The responses\nto these questions were then generated via the embedded GPT model\nand ChatGPT (Supplementary Note 2).\nThe recorded responses were shared with the specialists via Google\nForms. This research engaged experts from various institutions, opting for\na web-based platform due to its cost-effectiveness and ef ﬁciency in\nfacilitating participation from diverse locations. Consent was obtained prior\nto the completion of questionnaire by the dental professionals. A\npurposive sampling method was employed for the selection of partici-\npants. The sampling strategy aimed to ensure that the participants possess\na specialized postgraduate degree in the respective specialty of dentistry\nas part of inclusion criteria. Considering the recommendations, the number\nof experts for content validation should be at least six and does not exceed\nten [13]. Therefore, a total of 36 participants (9 belonging to each four\nspecialties of dentistry i.e., Oral & Maxillofacial Surgery, Operative Dentistry\n& Endodontics, Periodontics and Prosthodontics) formed the study sample.\nTo eliminate bias in grading, the experts were blinded to the responses\ngenerated via both models. The responses generated via embedded GPT\nmodel and ChatGPT were assigned a special code “A” or “B” respec-\ntively (Supplementary Note 2).\nThe methodology is graphically presented in Fig.1.\nEmbedded word document\nDeveloping embedded GPT model\nGPT-3.5-16k\nBase prompt\nEmbedded GPT model\nTemperature \nsetting-zero\nChatGPT (prompt engineering)\nPrompt-engineered ChatGPT\nResponses to questions belonging to 4 dental specialties generated via both models\nGoogle Forms\nBase prompt\nTemperature \nsetting-default\n1=Strongly Disagree\n2=Disagree\n3=Agree\n4=Strongly Agree\n1 2 3 4\nLikert scale 4-point\nGPT-trainer\nWord\nChatGPT\nFig. 1 Graphical representation of methodology.An embedded GPT model was created by selecting GPT-3.5-16k on GPT-trainer. A base\nprompt was provided along with embedding and the temperature setting was maintained at zero. The responses to the questions were\ngenerated via embedded GPT model and were compared with the prompt-engineered chatGPT to which the same base prompt was provided\nbut without embedding and the default temperature setting was retained as well. The responses were then evaluated by a series of experts\n(dental professionals) on a 4-point Lkert scale.\nI. Batool et al.\n2\nBDJ Open           (2024) 10:48 \nContent validation\nFor content validation of the responses generated via both models, a\nquantitative Content Validity Index (CVI) was used. The CVI was calculated\nboth at the item level (I-CVI) and scale level (S-CVI/Ave) (Table1). Experts\nwere asked to rate each response based on content validity indicators such\nas relevance, clarity, accuracy, and up-to-date knowledge, using a 4-point\nLikert scale:\n1 ¼ Strongly disagree; 2 ¼ Disagree; 3 ¼ Agree; 4 ¼ Strongly agree\nFor ease of interpretation, the ordinal scale on the instrument was\ndichotomized into two i.e., experts in agreement (score 3 and 4) versus\ndisagreement (score 1 and 2). Item-level content validity index (I-CVI) was\nthen calculated for each item by dividing the number of experts in\nagreement (or disagreement) by the total number of experts. Moreover, to\nadjust I-CVI for chance agreement, a modiﬁed kappa statistic (K*) was\ncomputed. To ascertain the average number of items scoring 3 or 4\namongst the evaluators, an average scale level content validity index (S-\nCVI/Ave) was reported by calculating the mean of I-CVI values (sum of I-CVI\nscores divided by total number of items).\nRESULTS\nA panel of 36 experts, nine from each domain participated in the\ncontent validation process of the responses generated via the GPT\nembedded model and ChatGPT. Each expert evaluated 20\nresponses (10 responses per GPT model) in terms of relevance,\naccuracy, clarity, and up-to-date knowledge.\nThe overall content validity of responses generated via GPT\nembedded model and ChatGPT was 65.62% and 61.87%\nrespectively (Fig. 2).\nThe specialty-wise percentage of responses with acceptable\nI-CVI values generated via both models are presented in Fig.3a, b.\nFor acceptable responses in all four domains, the kappa statistic\nrevealed an inter-rater reliability between 0.75 to 1 representing\nexcellent agreement amongst the evaluators. The individual I-CVI\nand kappa values for each item generated via both models are\npresented in Supplementary Notes 3–6.\nThe specialty-wise content validity (I-CVI and S-CVI) of the\nresponses generated via both models are presented below and in\nTables 2–5.\nEmbedded GPT model\nOperative Dentistry & Endodontics. The content validity (accept-\nability) of the individual responses in terms of relevance, accuracy,\nclarity and up-to-date knowledge was 67.5% i.e., 27 out of 40\nresponses showed an acceptable I-CVI (between the range of\n0.78–1).\nThe overall content validity of the responses revealed\nacceptable results in terms of relevance and clarity i.e., S-\nCVI=0.82 and 0.81 respectively. However, regarding the accuracy\nand up-to-date knowledge of the responses, the values were not\nsatisfactory with S-CVI=0.78 and 0.75 respectively.\nPeriodontics. In periodontics, 27 out of 40 responses (67.5%)\nshowed an acceptable I-CVI. The overall content validity of the\nresponses revealed acceptable results in terms of relevance and\nclarity i.e., S-CVI=0.80 and 0.87 respectively. However, regarding\nthe accuracy and up-to-date knowledge of the responses, the\nvalues were not satisfactory with S-CVI=0.77 and 0.76 respectively.\nTable 1. Operational deﬁnition and interpretation of item-level and scale-level content validity index.\nCVI indices De ﬁnition Formula Interpretation\nI-CVI (item-level\ncontent validity index)\nThe proportion of experts giving a\nrating of 3 or 4 to a response (item)\nI-CVI = no. of experts rating a\nresponse as 3 or 4/total number of\nexperts\nI-CVI =0.78 or more (response is\nacceptable)\nI-CVI =0.70–0.77 (response\nrequires revision)\nI-CVI < 0.70 (response is\neliminated)\nS-CVI/Ave (scale-level\ncontent validity index)\nThe average of the I-CVI scores for all\nresponses on the scale judged by all\nexperts\nS-CVI = sum of I-CVI scores/number\nof items\nS-CVI = 0.90 or more (overall\nexcellent content validity)\nS-CVI > 0.8 (acceptable content\nvalidity)\n65.62%\n61.87%\n59.00%\n60.00%\n61.00%\n62.00%\n63.00%\n64.00%\n65.00%\n66.00%\nEmbedded GPT model ChatGPT\nFig. 2 Overall content validity of responses generated via both models.The embedded GPT model performed better with 65.62%\nresponses in the acceptable range overall compared to chatGPT with 61.87% acceptable responses.\nI. Batool et al.\n3\nBDJ Open           (2024) 10:48 \nTable 2. Content validity of the responses generated via embedded GPT model in terms of I-CVI.\nSpecialty Relevance ( n = 10) Clarity ( n = 10) Accuracy ( n = 10) Up-date-knowledge\n(n = 10)\nFrequency (%)\nOperative Dentistry 8 6 7 6 27 (67.5)\nPeriodontics 7 8 6 6 27 (67.5)\nOral & Maxillofacial Surgery 3 6 3 2 14 (35)\nProsthodontics 10 9 9 9 37 (92.5)\nFrequency (%) 28 (70%) 29 (72.5%) 25 (62.5%) 23 (57.5%) 65.62%\na\nNumber of responses with acceptable I-CVI (between 0.78–1).\naOverall content validity (acceptability). The numbers in bold represent the overall content validity score.\nTable 3. Content validity of the responses generated via embedded GPT model in terms of S-CVI.\nSpecialty Relevance Clarity Accuracy Up-date-knowledge\nOperative Dentistry 0.822 a 0.811a 0.789b 0.756b\nPeriodontics 0.80 a 0.87a 0.77b 0.76b\nOral & Maxillofacial Surgery 0.644 b 0.72b 0.644b 0.644b\nProsthodontics 0.944 a 0.867a 0.833a 0.844a\nS-CVI of the responses.\naAcceptable content validity (S-CVI= 0.8 or greater).\nbUnacceptable content validity (S-CVI < 0.8).\nTable 4. Content validity of the responses generated via chatGPT in terms of I-CVI.\nSpecialty Relevance ( n = 10) Clarity ( n =10) Accuracy ( n = 10) Up-date-knowledge\n(n = 10)\nFrequency (%)\nOperative Dentistry 8 6 6 7 27 (67.5)\nPeriodontics 5 6 5 4 20 (50)\nOral & Maxillofacial Surgery 7 6 4 4 21 (52.5)\nProsthodontics 8 9 6 8 31 (77.5)\nFrequency (%) 28 (70%) 27 (67.5%) 21 (52.5%) 23 (57.5%) 61.87%\na\nNumber of responses with acceptable I-CVI (between 0.78–1).\naOverall content validity (acceptability). The numbers in bold represent the overall content validity score.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nRelevance\nAccuracy\nClarity\nUp-date-knowledge\nOperative Dentistry Periodontics Oral & Macillofacial surgery Prosthodontics\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nRelevance\nAccuracy\nClarity\nUp-date-knowledge\nOperative Dentistry Periodontics Oral & Maxillofacial surgery Prosthodontics\na b\nFig. 3 Specialty-wise distribution of responses with acceptable I-CVI (values= 0.78 or more) generated via both models. aPercentage of\nresponses with acceptable I-CVI values generated via embedded GPT model. b Percentage of responses with acceptable I-CVI values\ngenerated via ChatGPT.\nI. Batool et al.\n4\nBDJ Open           (2024) 10:48 \nOral & Maxillofacial Surgery. In oral surgery, only 14 out of 40\nresponses (35%) showed an acceptable I-CVI. The overall content\nvalidity of the responses revealed unsatisfactory results in terms of\nrelevance S-CVI=0.644, clarity S-CVI=0.72, accuracy S-CVI=0.644,\nand up-to-date knowledge S-CVI=0.644.\nProsthodontics. In prosthodontics, 37 out of 40 responses (92.5%)\nshowed an acceptable I-CVI with an inter-rater reliability between\n0.75 to 1 showing excellent agreement amongst the evaluators.\nThe overall content validity of the responses revealed acceptable\nresults in terms of relevance S-CVI =0.944, clarity S-CVI=0.867,\naccuracy S-CVI=0.833 and up-to-date knowledge S-CVI=0.844.\nChatGPT\nOperative Dentistry & Endodontics. The content validity (accept-\nability) of the individual responses in terms of relevance, accuracy,\nclarity and up-to-date knowledge was 67.5% i.e., 27 out of 40\nresponses showed an acceptable I-CVI (between the range of\n0.78–1).\nThe overall content validity of the responses revealed\nsatisfactory results in terms of relevance S-CVI=0.856, clarity S-\nCVI=0.811, accuracy S-CVI=0.800 and up-to-date knowledge=S-\nCVI 0.811.\nPeriodontics. In periodontics, 20 out of 40 responses (50%)\nshowed an acceptable I-CVI. The overall content validity of the\nresponses revealed unsatisfactory results in terms of relevance S-\nCVI=0.756, clarity S-CVI=0.70, accuracy S-CVI=0.69 and up-to-date\nknowledge S-CVI=0.62.\nOral & Maxillofacial Surgery . In oral surgery, 21 out of 40\nresponses (52.5%) showed an acceptable I-CVI. The overall content\nvalidity of the responses revealed unsatisfactory results in terms of\nrelevance S-CVI=0.722, clarity S-CVI=0.733, accuracy S-CVI=0.700,\nand up-to-date knowledge=S-CVI 0.700.\nProsthodontics. In prosthodontics, 31 out of 40 responses (77.5%)\nshowed an acceptable I-CVI. The overall content validity of the\nresponses revealed acceptable results in terms of relevance\nS-CVI=0.867, clarity S-CVI =0.844, accuracy S-CVI =0.800, and\nup-to-date knowledge=S-CVI 0.856.\nDISCUSSION\nThe current healthcare system is struggling with challenges such\nas professional burnout and prolonged patient waiting times,\nresulting in increased workload and additional burden on the\nhealthcare system [14]. In this regard, Large Language Models\n(LLMs), speciﬁcally AI chatbots can facilitate patient interaction by\noffering a promising solution to deliver timely and accurate\ninformation to patients [15]. While previous studies have explored\nthe utility of LLMs in medicine [6] few studies have explored their\napplication in different domains of dentistry [16, 17]. Furthermore,\nthe existing studies primarily focused on aspects unrelated to the\nutility of LLM models in post-operative patient care and did not\nemploy a quantitative analysis for content validation. Hence, there\nwas a clear need for additional research in this speciﬁc domain.\nThis study investigated the impact of embedding a GPT model\n(RAG) on its performance and its comparison with ChatGPT with a\nkeen focus on response accuracy, clarity, relevance, up-to-date\nknowledge. Accuracy was assessed because inaccurate informa-\ntion can lead to misunderstandings by patients, potentially\nimpacting their recovery [18]. Evaluating the relevance ensured\nthat the model-generated responses are directly applicable to the\npatient’s situation. Up to date knowledge was assessed as\nChatGPT has its last training cut-off in 2021, so it cannot retrieve\nany new data beyond that date [19].\nThe results of the study revealed a superior performance for the\nembedded GPT model, surpassing ChatGPT with an accuracy of\n62.5% and clarity of 72.5%. In contrast, the responses generated\nvia ChatGPT achieved slightly lower scores, with an accuracy of\n52.5% and clarity of 67.5%. However, both models performed\nequally well in terms of relevance and up-to-date knowledge.\nRemarkably both LLM models faltered in the specialty of Oral &\nMaxillofacial Surgery; this could be attributed to lack of speciﬁc\nand detailed information in the training dataset related to oral\nsurgery procedures which may have hindered the model’s ability\nto provide comprehensive responses. Moreover, the nuanced\nnature of oral surgery topics may demand a higher level of\ndomain speciﬁc context, which generic LLMs may not possess to\nthe required extent.\nDespite this, the embedded model exceeded expectations,\nperforming admirably overall. The success of the model is\nattributed to not only embedding but also to the low temperature\nsetting. Given the documented instances of hallucination in GPT\nmodels, precautionary measures were taken during the study [20].\nPrompt engineering was implemented as a very speci ﬁc base\nprompt was used for both models, and a conservative tempera-\nture setting was applied to limit the length of responses as well as\nto mitigate the risk of hallucinations. These measures were\nincorporated to enhance the reliability of the ﬁndings and to\nensure that the responses generated by the models remained\naligned with the intended context. Moreover, it is interesting to\nnote that advance prompting techniques like role deﬁnition have\nonly been used in two studies in dentistry previously [21, 22].\nIn the healthcare domain, the dissemination of misleading\ninformation can have severe consequences; this phenomenon has\nbeen elucidated in studies by Rahimi et al. and Deiana G et al.\n[23, 24]. Nevertheless, it is crucial to highlight that in our study,\nwhich incorporated embedding and utilization of low-\ntemperature settings in the GPT model, no responses containing\nmisleading information were generated.\nThe strengths of the study lie in its robust research design,\nwhich incorporated effective masking and randomization, and\ninvolved evaluations conducted by a group of 36 specialists.\nMoreover, a quantitative analysis utilizing content validation index\nwas employed, distinguishing it from studies relying on surrogate\nmeasures [25]. Moreover, considering both indices i.e., I-CVI and\nTable 5. Content validity of the responses generated via chatGPT in terms of S-CVI.\nSpecialty Relevance Clarity Accuracy Up-date-knowledge\nOperative Dentistry 0.856 a 0.811a 0.800a 0.811a\nPeriodontics 0.756 b 0.70b 0.69b 0.62b\nOral & Maxillofacial Surgery 0.722 b 0.733b 0.700b 0.700b\nProsthodontics 0.867 a 0.844a 0.800a 0.856a\nS-CVI of the responses.\naAcceptable content validity (S-CVI= 0.8 or greater).\nbUnacceptable content validity (S-CVI < 0.8).\nI. Batool et al.\n5\nBDJ Open           (2024) 10:48 \nS-CVI ensured a more thorough evaluation. This is crucial as the S-\nCVI, being an average score, can be inﬂuenced by outliers [26]. The\nnovelty of this study lies in crafting an embedded model\nspeciﬁcally designed for dentistry. Additionally, the comparative\nanalysis offers valuable insights into the impact of embedding,\nenriching our understanding of model effectiveness [27].\nLike any scientiﬁc endeavor, our study has its limitations. We\nutilized only chatGPT and embedded GPT model via their\nApplication Programming Interface, meaning theﬁndings might\nnot be applicable to other Language Models. Additionally, it could\nhave been beneﬁcial to customize the Chat GPT model weights for\nour speci ﬁc task. However, this was impractical due to the\nextensive computational resources required for suchﬁne-tuning,\nwhich were not at our disposal. Instead, we opted for Retrieval\nAugmentation Generation (RAG), a method we believe to be novel\nin our context, as it has not been previously employed in similar\nstudies. Furthermore, our use of a CVI scoring index for subject\nexpert assessment, while not validated speciﬁcally for Language\nModel Models, offers potential advantages over the Likert scale\ncommonly used in similar studies [28].\nFurther, limitations of the study include the constrained\nperformance of the embedded model due to the limited\ninformation sourced from the internet during its training, potentially\naffecting its ability to address speciﬁc nuances in dental care.\nMoreover, the probabilistic nature of GPT models necessitates an\nevaluation of their consistency, acknowledging the challenge of\nassessing it due to resource constraints, particularly for embedded\nGPT models requiring tokens for generating responses [ 29].\nAdditionally, the study employed a subjective evaluation of content\nby specialists, which may yield varied outcomes based on individual\nexpertise. The possible criticism to this paper can be that questions\nprovided to the GPT models were not from actual patient. To\nmitigate this concern, we utilized four experts to validate the\nquestions in order to enhance the generalizability.\nWhile chatGPT is readily accessible to patients, the question\narises: would they opt for a customized chatbot tailored to their\nspeciﬁc needs? The study underscores that the efﬁcacy of patient-\naccessible chatbots is enhanced through customization, achieved\nvia prompt engineering and embedding, demonstrating that\ninvolving domain experts in their development improves utility\nand potentially results in superior performance for patients\ncompared to the generic versions.\nIn considering the areas of future research, it would be\nworthwhile to explore enhancements in theﬁne-tuning process\n[30]. Speciﬁcally, investigating methodologies to enrich the training\ndata with a more diverse range of patient queries and scenarios\ncould contribute to a more adaptable model. Furthermore,\nexploring the integration of technologies such as reinforcement\nlearning, or context aware models could enable the model to\ndynamically adjust its questioning strategy based on the evolving\nconversation resembling a more natural and interactive exchange\nwith users. Another avenue of research could be assessing the\npractical effectiveness and acceptance to patients and their overall\nsatisfaction of interacting with a deployed chatbot.\nCONCLUSION\nThe embedded GPT model showed better results in terms of\naccuracy and clarity as compared to ChatGPT in dental care\ncontext emphasizing the bene ﬁts of embedding and prompt\nengineering, paving the way for future advancements in\nhealthcare applications.\nDATA AVAILABILITY\nThe data that support theﬁndings of this study are available from the corresponding\nauthor, Dr. Fahad Umer upon reasonable request. Further the data which was used\nfor Retrieval augmentation is supplied as supplementary material.\nREFERENCES\n1. Sarkar D, Bali R, Sharma T, Sarkar D, Bali R, Sharma T. Machine learning basics. In:\nPractical Machine Learning with Python: A Problem-Solver’s Guide to Building\nReal-World Intelligent Systems. 2018. pp. 3 –65. https://doi.org/10.1007/978-1-\n4842-3207-1.\n2. Panesar A. Machine learning and AI for healthcare. Springer; 2019. https://\ndoi.org/10.1007/978-1-4842-6537-6.\n3. Shan T, Tay F, Gu L. Application of artiﬁcial intelligence in dentistry. J Dent Res.\n2021;100:232–44. https://doi.org/10.1177/0022034520969115.\n4. Bohr A, Memarzadeh K. The rise of artiﬁcial intelligence in healthcare applica-\ntions. In: Artiﬁcial Intelligence in healthcare. Elsevier; 2020. pp. 25–60. https://\ndoi.org/10.1016/B978-0-12-818438-7.00002-2.\n5. Hadi MU, Al Tashi Q, Qureshi R, Shah A, Muneer A, Irfan M, et al. A Survey on\nLarge Language Models: Applications, Challenges, Limitations, and Practical\nUsage. TechRxiv. 2023.https://doi.org/10.36227/techrxiv.23589741.v1.\n6. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large\nlanguage models in medicine. Nat Med. 2023;29:1930 –40. https://doi.org/\n10.1038/s41591-023-02448-8.\n7. Lahat A, Shachar E, Avidan B, Glicksberg B, Klang E. Evaluating the Utility of a\nLarge Language Model in Answering Common Patients’Gastrointestinal Health-\nRelated Questions: Are We There Yet? Diagnostics. 2023;13:1950https://doi.org/\n10.3390/diagnostics13111950.\n8. Seth I, Cox A, Xie Y, Bulloch G, Hunter-Smith DJ, Rozen WM, et al. Evaluating\nChatbot Efﬁcacy for Answering Frequently Asked Questions in Plastic Surgery: A\nChatGPT Case Study Focused on Breast Augmentation. Aesthet Surg J.\n2023;43:1126–35. https://doi.org/10.1093/asj/sjad140.\n9. Lim ZW, Pushpanathan K, Yew SME, Lai Y, Sun C-H, Lam JSH, et al. Benchmarking\nlarge language models’performances for myopia care: a comparative analysis of\nChatGPT-3.5, ChatGPT-4.0, and Google Bard. EBioMedicine. 2023;95:104770\nhttps://doi.org/10.1016/j.ebiom.2023.104770.\n10. Dwyer T, Hoit G, Burns D, Higgins J, Chang J, Whelan D, et al. Use of an Artiﬁcial\nIntelligence Conversational Agent (Chatbot) for Hip Arthroscopy Patients Following\nSurgery. ASMAR. 2023;5:495–505. https://doi.org/10.1016/j.asmr.2023.01.020.\n11. Alsahaﬁ YA, Alolayan AB, Alraddadi W, Alamri A, Aljadani M, Alenazi M, et al. The\nimpact of the method of presenting instructions of postoperative care on the\nquality of life after simple tooth extraction. Saudi J Oral Sci 2021;8:143–9.\n12. LLM Embeddings — Explained Simply. 2024. https://pub.aimind.so/llm-\nembeddings-explained-simply. Accessed 8 January 2024.\n13. Lynn MR. Determination and Quanti ﬁcation Of Content Validity. Nurs Res.\n1986;35:382–6.\n14. Drossman DA, Ruddy J. Improving patient-provider relationships to improve\nhealth care. CGH. 2020;18:1417–26. https://doi.org/10.1016/j.cgh.2019.12.007.\n15. Yang R, Tan TF, Lu W, Thirunavukarasu AJ, Ting DSW, Liu N. Large language\nmodels in health care: Development, applications, and challenges. Health Sci J.\n2023;2:255–63. https://doi.org/10.1002/hcs2.61.\n16. Huang H, Zheng O, Wang D, Yin J, Wang Z, Ding S, et al. ChatGPT for shaping the\nfuture of dentistry: the potential of multi-modal large language model. Int J Oral\nSci. 2023;15:29 https://doi.org/10.1038/s41368-023-00239-y.\n17. Mohammad-Rahimi H, Ourang SA, Pourhoseingholi MA, Dianat O, Dummer PMH,\nNosrat A. Validity and reliability of artiﬁcial intelligence chatbots as public sources\nof information on endodontics. Int Endod J. 2024;57:305 –14. https://doi.org/\n10.1111/iej.14014.\n18. Banerjee S, Dunn P, Conard S, Ng R. Large language modeling and classical AI\nmethods for the future of healthcare. J Med Surg Public Health. 2023;1:100026\nhttps://doi.org/10.1016/j.glmedi.2023.100026.\n19. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How Does\nChatGPT Perform on the United States Medical Licensing Examination? The\nImplications of Large Language Models for Medical Education and Knowledge\nAssessment. JMIR Med Educ. 2023;9:e45312https://doi.org/10.2196/45312.\n20. Umapathi LK, Pal A, Sankarasubbu M. Med-halt: Medical domain hallucination\ntest for large language models. ArXiv. 2023. https://doi.org/10.48550/\narXiv.2307.15343.\n21. Suárez A, Jiménez J, de Pedro ML, Andreu-Vázquez C, García VD, Sánchez MG, et al.\nBeyond the Scalpel: Assessing ChatGPT’s potential as an auxiliary intelligent virtual\nassistant in oral surgery. Computational Struct Biotechnol J. 2024;24(Dec):46–52.\n22. Russe MF, Rau A, Ermer MA, Rothweiler R, Wenger S, Klöble K, et al. A content-\naware chatbot based on GPT 4 provides trustworthy recommendations for Cone-\nBeam CT guidelines in dental imaging. Dentomaxillofacial Radiol.\n2024;53(Feb):109–14.\n23. Deiana G, Dettori M, Arghittu A, Azara A, Gabutti G, Castiglia P. Artiﬁcial intelligence\nand public health: evaluating ChatGPT responses to vaccination myths and mis-\nconceptions. Vaccines. 2023;11:1217https://doi.org/10.3390/vaccines11071217.\n24. Abu Arqub S, Al-Moghrabi D, Allareddy V, Upadhyay M, Vaid N, Yadav S. Content\nanalysis of AI-generated (ChatGPT) responses concerning orthodontic clear\naligners. Angle Orthod. 2024;94:263–72.\nI. Batool et al.\n6\nBDJ Open           (2024) 10:48 \n25. Rodrigues IB, Adachi JD, Beattie KA, MacDermid JC. Development and validation\nof a new tool to measure the facilitators, barriers and preferences to exercise in\npeople with osteoporosis. BMC Musculoskelet Disord. 2017;18:540 https://\ndoi.org/10.1186/s12891-017-1914-5.\n26. Wang J, Shi E, Yu S, Wu Z, Ma C, Dai H, et al., Prompt engineering for healthcare:\nMethodologies and applications. ArXiv. 2023. https://doi.org/10.48550/\narXiv.2304.14670.\n27. Lu Q, Qiu B, Ding L, Xie L, Tao D. Error analysis prompting enables human-like\ntranslation evaluation in large language models: A case study on chatgpt. ArXiv.\n2023. https://doi.org/10.48550/arXiv.2303.13809.\n28. Babayiğit O, Eroglu ZT, Sen DO, Yarkac FU. Potential Use of ChatGPT for Patient\nInformation in Periodontology: A Descriptive Pilot Study. Cureus. 2023;15:e48518.\n29. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. Adv Neural Inf Process.\n2020;33:9459–74.\n30. Dehghani M. Dental Severity Assessment through Few-shot Learning and SBERT\nFine-tuning. ArXiv. 2024.https://arxiv.org/abs/2402.15755.\nACKNOWLEDGEMENTS\nAcknowledgements to all the dental experts that participated in this study.\nAUTHOR CONTRIBUTIONS\nFahad Umer contributed to the conception and design of the study and helped with\nrevising it critically for important intellectual content, Itrat Batool drafted the article\nand contributed to the acquisition of data, analysis, and interpretation, Murtaza Raza\nKazmi contributed to the analysis and interpretation of data, Nighat Naved\ncontributed to analysis and interpretation of data and revising the manuscript\ncritically for important intellectual content. All authors have reviewed and agreed to\nthe submitted version of the manuscript.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41405-024-00226-3.\nCorrespondence and requests for materials should be addressed to Fahad Umer.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nI. Batool et al.\n7\nBDJ Open           (2024) 10:48 ",
  "topic": "Dental care",
  "concepts": [
    {
      "name": "Dental care",
      "score": 0.451226145029068
    },
    {
      "name": "Computer science",
      "score": 0.4476430416107178
    },
    {
      "name": "Dentistry",
      "score": 0.4326586127281189
    },
    {
      "name": "Medicine",
      "score": 0.35889267921447754
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802929321",
      "name": "Aga Khan University Hospital",
      "country": "PK"
    }
  ],
  "cited_by": 18
}