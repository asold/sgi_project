{
  "title": "Visually-augmented pretrained language models for NLP tasks without images",
  "url": "https://openalex.org/W4385572164",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2904254294",
      "name": "Hangyu Guo",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102854352",
      "name": "Kun Zhou",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2171156051",
      "name": "Qinyu Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4206808166",
    "https://openalex.org/W4313136445",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4281396910",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4224903949",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W4229004868",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4226452284",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4225432580",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2050482109",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4226095990",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4386566578",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14912–14929\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nVisually-augmented Pretrained Language Models for NLP Tasks\nwithout Images\nHangyu Guo1∗, Kun Zhou3,4∗\n, Wayne Xin Zhao2,4†\n, Qinyu Zhang1 and Ji-Rong Wen2,3,4\n1School of Electronics and Information Engineering, Harbin Institute of Technology (Shenzhen).\n2Gaoling School of Artificial Intelligence, Renmin University of China.\n3School of Information, Renmin University of China.\n4Beijing Key Laboratory of Big Data Management and Analysis Methods.\nhyguo0220@gmail.com, francis_kun_zhou@163.com\nbatmanfly@gmail.com, zqy@hit.edu.cn, jrwen@ruc.edu.cn\nAbstract\nAlthough pre-trained language models (PLMs)\nhave shown impressive performance by text-\nonly self-supervised training, they are found\nlack of visual semantics or commonsense. Ex-\nisting solutions often rely on explicit images\nfor visual knowledge augmentation (requiring\ntime-consuming retrieval or generation), and\nthey also conduct the augmentation for the\nwhole input text, without considering whether\nit is actually needed in specific inputs or tasks.\nTo address these issues, we propose a novel\nVisually-Augmented fine-tuning approach that\ncan be generally applied to various PLMs or\nNLP tasks, Without using any retrieved or gen-\nerated Images, namely V AWI. Experimental\nresults show that our approach can consistently\nimprove the performance of BERT, RoBERTa,\nBART, and T5 at different scales, and out-\nperform several competitive baselines on ten\ntasks. Our codes and data are publicly available\nat https://github.com/RUCAIBox/VAWI.\n1 Introduction\nRecent years have witnessed the success of pre-\ntrained language models (PLMs) (Qiu et al., 2020;\nZhao et al., 2023), such as GPT-3 (Brown et al.,\n2020) and T5 (Raffel et al., 2020), in a variety of\nnatural language process (NLP) tasks. Since these\nPLMs are mostly trained on text-only corpus via\nself-supervised pre-training, they have been shown\nlack of visual commonsense (Liu et al., 2022) and\nreal-world knowledge (Zhang et al., 2022). As a\nresult, PLMs can’t well solve visually related lan-\nguage tasks 1, e.g., answering the color and size of\ncommon things, especially those requiring complex\ncommonsense knowledge.\nTo alleviate this problem, existing works mainly\nenhance PLMs by infusing visual information. Typ-\n∗Equal contributions.\n†Corresponding authors.\n1In this work, we mainly focus on text-only NLP tasks\nthat may benefit from external visual information, rather than\nvisual-language tasks involving images.\nically, given a text input, these studies firstly aug-\nment the visual information from retrieved or gen-\nerated images about the input and then leverage\ntheir visual representations to improve PLMs on\nNLP tasks. Such an approach leads to visually-\naugmented pre-trained language models (VaLMs),\nwhere they adopt either visually-augmented pre-\ntraining (Tan and Bansal, 2020; Wang et al., 2022)\nor visually-augmented fine-tuning (Lu et al., 2022).\nDespite the effectiveness, there are two major short-\ncomings in these methods. First, these methods of-\nten rely on pre-learned complementary retrievers or\ngenerators, and also require time-consuming infer-\nence to retrieve or generate proper images that are\npaired with the input. The above costly conditions\nlargely limit the applicability of these approaches.\nSecond, the retrieved or generated images are in-\nevitable to involve irrelevant or redundant visual in-\nformation. If simply integrating them, the original\ntext representations might be affected.Increasing\nevidence shows that the visual information is not\nalways useful for NLP tasks (Dai et al., 2022), and\nsometimes leads to performance degradation.\nConsidering these issues, we aim to develop a\nmore efficient and effective way to visually aug-\nment the PLMs and the solution is twofold:\n•Firstly, we don’t explicitly produce (re-\ntrieve or generate) the images but instead gener-\nate visually-aligned representations of the text on-\nthe-fly. Recent studies (Radford et al., 2021; Jia\net al., 2021) have shown that the vision-language\npre-trained models (VL-PTMs) can well learn the\nalignment between the representations of texts and\nimages from large-scale text-image pairs. Thus,\nour idea is to employ the output representations of\na text from VL-PTMs’ text encoders as a surrogate\nfor the visual representations of related images.\nSuch a way is simple and efficient: we can only\nkeep the text encoder of a VL-PTM to produce the\nvisually-aligned representations of texts, getting rid\nof the complicated image retrieval or generation\n14912\nprocess. It is widely recognized that there is a large\nsemantic gap between different modalities (Liang\net al., 2022). Our method can alleviate this issue\nto some extent since the visual augmentations are\nderived from the text representation itself.\n•Secondly, instead of directly feeding visual\naugmentations into the PLM, we propose to use the\naugmented visual information only when it is actu-\nally required. In fact, for a text input of a NLP task,\nPLMs are not always hungry for the visual back-\nground knowledge to effectively understand it, es-\npecially for visually-irrelevant expressions. Unlike\nprevious works which inject visual information into\na text (Tan and Bansal, 2020; Wang et al., 2022)\nfrom the whole, we consider identifying visually-\nhungry words(those that require visual knowledge\nto derive complete semantics) from the text input,\nand only infuse the visual augmentations through\nthese trigger words. We conduct visual augmenta-\ntions at the word level, because it is more flexible\nand controllable, considering the augmented infor-\nmation is often irrelevant or noisy.\nTo this end, in this paper, we propose a gen-\neral Visually-Augmented fine-tuning approach to\nimproving PLMs for NLP tasks Without Images,\nnamely V AWI. Our approach consists of three in-\ngredients, namely visually-hungry words extrac-\ntion, visual knowledge augmentation, and visually-\nenhanced fine-tuning. Given the text input from\na NLP task, we first extract the visually-hungry\nwords (VH-words) from the input sentence. As the\nannotations of VH-words are generally unavailable,\nwe propose three strategies to automatically extract\nthe VH-words, relying on the syntax trees, atten-\ntion distributions of VL-PTMs, and an adaptive\nlearnable module, respectively. Then, based on the\nextracted VH-words, we leverage the text encoder\nof CLIP (Radford et al., 2021) (being fixed in our\napproach), a VL-PTM that has been pre-trained\non millions of text-image pairs, to encode the VH-\nwords for obtaining their visually-aligned repre-\nsentations. Finally, we infuse the visually-aligned\nrepresentations into PLMs, and consider the gen-\neral and parameter-efficient fine-tuning strategies\nfor small and large PLMs, respectively.\nTo verify the effectiveness of our framework\nV AWI, we test it on four PLMs (i.e., BERT,\nBART, RoBERTa, and T5) at different scales (i.e.,\n110M, 340M, 3B), and conduct extensive experi-\nments in natural language understanding, common-\nsense reasoning, and text generation tasks. Ex-\nperimental results show that our V AWIcan boost\nthe performance of these PLMs significantly, i.e.,\n3.11%, 2.54%, and 2.16% absolute improvements\non the commonsenseQA task using RoBERTa-base,\nRoBERTa-large, and T5-3b, respectively. Besides,\nV AWIcan outperform (or be on par with) sev-\neral competitive baselines that adopt complicated\nvisually-augmented methods.\n2 Related Work\nPre-trained Language Models.Recent years have\nwitnessed the success of pre-trained language mod-\nels (PLMs) (Devlin et al., 2019; Radford et al.,\n2019). After pre-trained on the large-scale corpus,\nPLMs can be fine-tuned on multiple NLP tasks and\nachieve remarkable performance. However, since\nPLMs are just pre-trained with text-only data, they\nmay suffer from the reporting bias problem (Gor-\ndon and Van Durme,2013; Paik et al., 2021; Zhang\net al., 2022), where the frequency distribution of\nvisual commonsense in the text may not fully re-\nflect the real-world distribution of the common-\nsense. Existing works have also found that such\na problem can not be well addressed by enlarging\nthe model or pre-training corpus (Paik et al., 2021;\nZhang et al., 2022). In this work, we aim to alle-\nviate this problem by adding visual knowledge on\nPLMs during fine-tuning.\nVision-Language Pre-Trained Models.To bet-\nter accomplish the vision-language tasks, vision-\nlanguage pre-trained models (VL-PTMs) (Su et al.,\n2019; Lu et al., 2019) become a hot point in recent\nyears, which require large-scale image-text pairs\nfor pre-training. Existing VL-PTMs fall into two\ncategories based on the way of modeling vision-\nlanguage interaction. The first category of mod-\nels (Lu et al., 2019; Li et al., 2021) adopts an ex-\nplicit vision-language interaction layer to fuse the\ntext embeddings and image features. These models\nare more suitable to capture fine-grained seman-\ntic interactions between vision and language.The\nsecond category of models (Radford et al., 2021;\nJia et al., 2021) incorporates separate encoders to\nmodel the vision and language information, and\nrelies on pre-training tasks (e.g., cross-modal con-\ntrastive learning) to align their representations into\nthe same latent space. Such a way is capable of\nproducing enriched single-modal representations.\nVisually-Augmented Language Model. To in-\ntroduce visual information into PLMs, visually-\n14913\naugmented language model (VaLM) (Wang et al.,\n2022) has become an emerging research topic. Ex-\nisting VaLMs can be categorized into visually-\naugmented pre-training and fine-tuning. Visually-\naugmented pre-training approaches (Tan and\nBansal, 2020; Zhu et al., 2022) continually pre-\ntrain PLMs with the retrieved visual information\nrelated to input tokens or sentences and also revise\nthe masked language model task for better cap-\nturing the visual semantics. Visually-augmented\nfine-tuning method (Lu et al., 2022) introduces the\nvisual information into PLMs during fine-tuning.\nThese methods also leverage the image retrieval or\ngeneration models to augment the visual informa-\ntion and design a special fusion module to inject it\ninto PLMs. However, existing VaLM approaches\nmostly need to retrieve or generate visual informa-\ntion for utilization. Such a way is time-consuming,\nand may involve unrelated or noisy information\ninto PLMs, leading to performance degradation. In\nthis work, we aim to first detect the visually-hungry\nwords from the text, and then utilize a VL-PTM\nto generate their visually-aligned representations\nwithout the usage of external images or generation\nmodels. As a comparison, our approach is more\nflexible and efficient to leverage visual information\nfor enhancing text-based PLMs.\n3 Method\nIn this section, we firstly introduce the task setting,\nand then describe our proposed visual augmenta-\ntion approach for infusing visual knowledge into\nPLMs during fine-tuning.\n3.1 Task Setting and Solution Overview\nThis work aims to improve the fine-tuning perfor-\nmance of pre-trained language models (PLMs) on\nNLP tasks by leveraging the related visual infor-\nmation without images. For a NLP task, a set of n\nlabeled texts {⟨xi,yi⟩}are available, where xi is\nthe i-th text data consisting of a sequence of words,\ndenoted as xi = {w1,w2,...,w m}, and yi is the\nground-truth output, which can be a discrete la-\nbel (classification), a continuous value (regression)\nor a text sequence (generation).\nTo solve the target task, we assume that a text-\nbased PLM is given (either for understanding or\ngeneration). Let f denote a PLM parameterized by\nθPLM that has already been pre-trained on general-\npurpose large-scale text data. Given the labeled\ntraining data, we can train the PLM using a specific\nloss function (e.g., cross-entropy loss) and further\nsolve the target task. However, existing works (Tan\nand Bansal, 2020; Zhang et al., 2022) have revealed\nthat PLMs may be unaware of visual knowledge\nthat is not explicitly mentioned in the pre-trained\ntext-only data (e.g., the shape of coins and the color\nof the sky), leading to the lack of world common-\nsense and generating wrong statements.\nIn this work, we focus on devising an efficient\nand effective way to infuse such visual knowledge\ninto PLMs during fine-tuning. Our approach is\nbased on visually-hungry words (abbreviated as\nVH-words), which require visual information to de-\nrive complete semantic representations. The over-\nall illustration of our approach is shown in Fig-\nure 1. Given the input text\nxi and its label yi, we\nfirst detect and extract a set of VH-words. Then,\nwe adopt a visual knowledge augmentation mod-\nule to enhance the visual background knowledge\nof their tokens and generate their visually-aligned\nrepresentations. Finally, we infuse the visually-\naligned text representations into the PLM to im-\nprove its fine-tuning performance, where we con-\nsider both the general fine-tuning of small PLMs\nand the parameter-efficient fine-tuning of large-\nscale PLMs.\n3.2 Visually-Hungry Words Extraction\nIn our approach, visually-hungry words (VH-\nwords) are the trigger units for visual augmenta-\ntions, requiring visual knowledge for deriving com-\nplete semantic representations (e.g., color, shape,\nand object). Therefore, we propose to first de-\ntect the VH-words from the input text, and then\ninject the proper visual knowledge that they are\nhungry for into the PLM. However, the annotations\nabout VH-words are generally not available in NLP\ndatasets. To address this problem, we devise three\ndifferent strategies to extract the VH-words from\nthe input text, including two feature-based strate-\ngies based on syntax tree and attention distribution\nof PLMs, and a learnable model-based strategy.\nSyntax-based Strategy. In natural language, en-\ntity words and descriptive words usually convey\nmore visual semantics than others. For exam-\nple, for the sentence “He is eating agreen apple”,\nwhere underlined words are more related to visual\nsemantics. Such words are mostly nouns or adjec-\ntives in the input text, which can be detected by syn-\ntactic analysis. Therefore, we design a rule-based\nstrategy that leverages the syntactic information for\n14914\nVisually-hungry Words Extraction\nInput: “He flies a kite with laughter.”\nVisually-hungry \nWords Extractor\nFixed VL-PTM \nText Encoder \nFixed VL-PTM \nText Encoder \nHe flies a kite with laughter\nnsubj dobj\nprep\npobjdet\n.\nPLM\nAttention\nDistribution\nflies\nkite\nlaughter\nTop-K\nVisual Knowledge Augmentation\nFixed VL-PTM \nText Encoder \n(e.g. CLIP)\nReformulation\nLayer\nK V Q\nCross Attention\nFFW\nVisually Enhanced \nWord Embeddings\nSentiment \nCategories\nPLM MLP\nHead\nHe\nflies\n(flies)\na\nkite\n(kite)\nwith\nlaughter\n(laughter)\n.\nVisually-enhanced Fine-tuning\nInsert after Original \nEmbeddings \nvisually-aligned \nrepresentations\nflies\n…\nkite\nflies\n…\nkite\nflies\n…\nkite\nSyntax-based Strategy\nVisually-enhanced Attention Based Strategy\nLearning-based Strategy\nvisually-augmented \nrepresentations\nInput: “He flies a \nkite with laughter.”\nWord EmbeddingGradient Cut-off Visually-argumented Representation\nvisually-augmented \nrepresentations\nsoft position \nembeddings\nGumbel-\nSoftmax\nEmbedding Layer \nof PLM\nFigure 1: The illustration of our VAWI approach, consisting of visually-hungry words extraction, visual knowledge\naugmentation and visually-enhanced fine-tuning.\nVH-words extraction. Concretely, we first delete\nall stop words in a text and then adopt an open-\nresource toolkit SPACY 2 to convert the input text\ninto a syntax dependency tree. Based on the syntax\ntree, we extract the words that have a particular part\nof speech (POS), e.g., nouns or adjectives, as the\nVH-words denoted by W(V H). In this way, we can\nefficiently extract the VH-words from input text by\nusing a fast parser toolkit.\nVisually-enhanced Attention Based Strategy.\nThe attention-based strategy utilizes the attention\ndistribution of a VL-PTM to detect the VH-words.\nSince VL-PTMs (Radford et al., 2021) are pre-\ntrained on large-scale image-text pairs, their text\nencoders can focus more on the words correspond-\ning to some specific visual concepts in an image,\nwhich are likely to be VH-words. Inspired by it, we\nuse the attention scores calculated by the text en-\ncoder of VL-PLMs to select the VH-words. Specif-\nically, we adopt the text encoder of CLIP (Radford\net al., 2021), a VL-PTM that has been pre-trained\non millions of image-text pairs, to help extract the\nVH-words. As CLIP adopts an autoregressive GPT-\n2 model as the text encoder, we calculate the av-\nerage attention scores between each token and the\n“[EOS]” token on the self-attention layer, denoted\nas swi. Then, we select the top- K ranked words\naccording to {swi}as the VH-words W(V H).\nLearning-based Strategy. Considering that di-\nverse PLMs and NLP tasks may be hungry for\ndifferent complementary visual information, we\n2https://spacy.io/\ndevise a learning-based strategy that can adaptively\nextract VH-words according to task requirements.\nConcretely, we add a parameterized VH-words ex-\ntractor layer for the PLM, which can be updated by\ngradient-based optimization algorithms to fit the\nneed for some specific task. Given the input text\nxi, we first leverage the PLM and a text encoder\nof a VL-PTM (i.e., CLIP (Radford et al., 2021)) to\nproduce the contextualized representations of the\ncontained words in\nxi. Then, we concatenate the\nrepresentations of each word from the two models\nand utilize a MLP layer to obtain the score swi:\nswi = MLP([h(P)\nwi ; h(V )\nwi ]) (1)\nwhere h(P)\nwi and h(V )\nwi are the output word represen-\ntations from the PLM and VL-PTM, respectively,\nand scores swi are calculated by the learned model\nbased on the supervision information from down-\nstream tasks. Based on the scores of all words,\nwe incorporate the gumbel-softmax function (Jang\net al., 2016) to extract the top-kwords as the VH-\nwords in a differentiable way. In this way, the gradi-\nents of the fine-tuned tasks can be back-propagated\nto the extractor layer, which learns to adaptively\nselect the more suitable VH-words.\n3.3 Visual Knowledge Augmentation\nExisting works (Lu et al., 2022; Wang et al., 2022)\nmainly utilize image retrieval or generation module\nto augment related visual knowledge. Such a way\nis time-consuming and may also involve noisy im-\nages.Inspired by recent works that show the effec-\ntive visual-language alignment in VL-PTMs (Rad-\n14915\nford et al., 2021; Li et al., 2021), we utilize the\nvisually-aligned text encoders to generate the vi-\nsual augmentation representations of VH-words.\nAs the text encoders have been aligned to the image\nencoders during pre-training, their output textual\nrepresentations can be used as surrogates of visual\naugmentations based on real images related to the\ninput text. As will be shown in experiments (Sec-\ntion 4), this approach is not only efficient but very\neffective for downstream NLP tasks.\nBased on the extracted VH-words, we first add\na prefix text in the image caption style before\nthe VH-words, e.g., “a photo of: ”, to compose\nthe input text x′. Then, we utilize the text en-\ncoder of CLIP (Radford et al., 2021) to encode\nx′and obtain the contextualized word representa-\ntions as the visually-aligned representations Hx ∈\nRk×d, where k is the sequence length of x′ and\ndis the embedding size. Next, we incorporate a\nreformulation layer to aggregate and strengthen\nthe visually-aligned representation Hx into the\nvisually-augmented representations of these VH-\nwords. As the positions of the VH-words vary\nfrom sentence to sentence, we design a position-\naware attention mechanism in the reformulation\nlayer to inject position information into Hx for ob-\ntaining the visual representation of each VH-word.\nSpecifically, we first leverage a soft position em-\nbedding matrix\nE ∈Rl×d to reserve the position\ninformation of VH-words, where lis the number of\nVH-words. Then, we perform the cross-attention\nbetween it and the visual representations as:\nQ = E, K = HxWK + bK, (2)\nV = HxWV + bV , (3)\nHv = softmax(QK⊤\n√\nd\n)V, (4)\nH⊤\nv = [h1,h2,..., hl], (5)\nwhere hi ∈Rd, K,V ∈Rk×d. Hv ∈Rl×d is\nthe obtained visually-augmented representations of\nVH-words, which is leveraged for augmenting the\nvisual knowledge of the PLM.hi is the visual repre-\nsentation of the i-th VH-word in W(V H). Note that\nin Eq. 2 and 3, we adopt an efficient way that only\nuses the position information to set thequery matrix\nQ, and the visual semantics are mainly captured\nand injected through the key and value matrices.\n3.4 Visually-Enhanced Fine-tuning\nAfter obtaining the visually-augmented representa-\ntions of VH-words (i.e., Hv in Eq. 5), we propose\na visually-enhanced fine-tuning strategy to inject\nthe captured visual knowledge. Here, we consider\ntwo cases: (1) full-parameter fine-tuning for small\nPLMs, and (2) parameter-efficient prompt-tuning\nfor large-scale PLMs. Before introducing the learn-\ning method, we simply review the parameters of\nour approach, consisting of the parameters in the\nunderlying PLM (Θplm), the VL-PTM (Θvlp) and\nthe parameters of the reformulation layer ( Θref ).\nNote that we will always fix Θvlp in our approach.\nFine-tuning for Small PLMs.For small PLMs,\nwe can perform full-parameter fine-tuning, which\nupdates both Θplm and Θref . Specifically, given\nthe visually-augmented representations Hv of VH-\nwords, we directly incorporate them into the em-\nbedding layer of the PLM. For each VH-word, we\ninsert its visually-augmented representation after\nthe original word embedding, to leverage the visual\nsemantics to enrich the word representations.\nPrompt-tuning for Large-Scale PLMs. For\nlarge-scale PLMs, we fix the parameters in it, i.e.,\nΘplm, and employ a parameter-efficient prompt-\ntuning way to optimize it on downstream NLP\ntasks. Concretely, given the visually-augmented\nrepresentations\nHv of VH-words, we directly in-\nsert them before the input representations of every\nlayer of PLMs. Then, following the typical prompt-\ntuning paradigm (Li and Liang, 2021), we only\ntune the parameters of the reformulation layer (i.e.,\nΘref ) as the soft prompts to adapt all the model\ninto the fine-tuning task.\nOur approach can be generally applied to var-\nious PLMs (e.g., BERT (Devlin et al., 2019),\nBART (Lewis et al.,2020), T5 (Raffel et al., 2020))\nand NLP tasks (natural language understanding and\ntext generation). Unlike other complicated visually-\naugmented methods (Tan and Bansal, 2020; Wang\net al., 2022), it is more efficient, without the ex-\nplicit need of external images or generation model;\nand meanwhile, it only introduces a small number\nof parameters (Eq. 3), which are easier to learn.\n4 Experiments\n4.1 Experimental Setup\nDatesets. We conduct experiments on four types of\ntasks. (1) Natural Language Understanding (NLU):\nwe extract 6 datasets from the GLUE bench-\nmark (Wang et al., 2018); (2) Commonsense rea-\nsoning: we select CommonsenseQA (Talmor et al.,\n14916\nBase Model Method\nSST-2 QNLI QQP MNLI MRPC STS-B Avg.\nCLIP +None 73.3\n74.5 72.8 68.4 74.3 73.8 72.85\nBLIP +None 76.3 77.4 78.8 72.5 77.8 76.4 76.53\nALBEF14M +None 78.9 78.2 79.4 73.4 76.5 77.5 77.31\nBERTbase\n+None 89.3\n87.9 87.2 79.4 81.7 84.4 84.98\n+VOKEN 92.2 88.6 88.6 82.6 83.5 86.0 86.83\n+iACE 91.7 88.6 89.1 82.8 85.8 86.6 87.43\n+VAWI-SBS 92.9 88.4 89.6 82.2 85.5 86.9 87.58\n+VAWI-V ABS 92.7 88.9 89.5 82.7 85.8 87.2 87.80\n+VAWI-LBS 92.4 89.1 89.7 83.0 85.6 86.9 87.78\nRoBERTabase\n+None\n89.2 87.5 86.2 79.0 81.4 85.4 84.78\n+VOKEN 90.5 89.2 87.8 81.0 87.0 86.9 87.06\n+iACE 91.6 89.1 87.9 82.6 87.7 86.9 87.63\n+VAWI-SBS 91.4 89.4 87.7 82.2 88.2 87.7 87.76\n+VAWI-V ABS 91.7 89.1 87.9 82.6 88.3 88.1 87.95\n+VAWI-LBS 91.6 90.6 87.9 82.4 88.5 88.3 88.21\nTable 1: Performance comparison of different methods on NLU tasks, the BEST results are highlighted in bold.\n+None denotes that we directly fine-tune the backbone without adding visual information. SBS, V ABS, and LBS\nrepresent using the syntax-based strategy, visually-enhanced attention based strategy, and learning-based strategy in\nour approach, respectively. The results of VOKEN and iACE on GLUE are reported fromLu et al. (2022).\n2019), a 5-way multiple choice QA dataset that\nrequires commonsense knowledge; (3) Text gener-\nation: we select CommonGen (Lin et al., 2019b), a\nconstrained text generation task about generative\ncommonsense reasoning. (4) Cross-modal reason-\ning: we select SNLI-VE (Xie et al., 2019), to eval-\nuate the capacity of predicting whether the image\nsemantically entails the text.\nBaseline Models.We compare our approach with\nthe following baselines, including pre-trained lan-\nguage models (PLMs), visual-language pre-trained\nmodels (VL-PTMs), and visually-augmented pre-\ntrained language modes (VaLMs). (1) PLMs: We\nchoose BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), BART (Lewis et al.,2020), T5 (Raffel\net al., 2020) as the PLM backbones, and directly\nfine-tune them as baselines. (2) VL-PTMs: We\nselect ALBEF (Li et al., 2021), BLIP (Li et al.,\n2022), and CLIP (Radford et al.,2021), which have\nbeen pre-trained on large-scale image-text pairs.\n(3) VaLMs: we select VOKEN (Tan and Bansal,\n2020) and iACE (Lu et al., 2022), which introduce\nthe visual information into PLMs by pre-training\non retrieved images and fine-tuning on generated\nimages, respectively.\nImplementation Details.We implement all meth-\nods based on Huggingface Transformers (Wolf\net al., 2020). For all baselines, we set their hyper-\nparameters according to their papers. In our ap-\nproach, we leverage the text encoder of CLIP (ViT-\nB/32) to implement the learnable model-based VH-\nwords extractor and generate the visual represen-\ntations of VH-words in the visual knowledge aug-\nmentation module. The hidden size of visual rep-\nresentations is set to 512. For different NLP tasks,\nwe tune the number of visually hungry words in {2,\n3, 4, 5}. During fine-tuning, we perform parameter-\nefficient tuning on T5-3b and BART-Large, and\nfull-parameter tuning on other PLMs. For all tasks\nand all backbones, we utilize Adam as the opti-\nmizer, set the learning rate to 2e-5, weight decay\nto 0.01, and a linear warmup for the first 6% steps.\nFor GLUE, GommonGen, and SNLI-VE datasets,\nwe fine-tune our model for 3 epochs with a batch\nsize of 32. For CommonsenseQA, we tune our\nmodel for 10 epochs with a batch size of 32. We\nuse the cross-entropy loss for classification and the\nmean squared error loss for regression.\n4.2 Main Experimental Results\nIn this part, we conduct a series of experiments\non NLU, commonsense reasoning, text generation,\nand cross-modal commonsense reasoning tasks.\nEvaluation on NLU Tasks.We present the ex-\nperimental results of different methods on 6 NLU\ntasks in Table 1. First, we observe that VL-PTMs\nperform worse than PLMs, a possible reason is that\n14917\nBase Model Method CommonsenseQA-3k CommonsenseQA\n5% 10% 20%\n100% 5% 10% 20% 100%\nRoBERTabase\n+None\n41.88 46.04 50.58 61.88 44.88 50.04 57.08 67.90\n+Images 42.37 48.09 52.81 64.22 45.72 51.17 58.96 69.64\n+VAWI-SBS 42.94 49.27 53.97 65.10 46.51 52.44 59.87 71.01\nRoBERTalar\nge\n+None 48.39 56.30 59.06 74.19 51.24 59.95 65.52 76.65\n+Images 49.55 57.78 61.29 75.61 52.18 60.93 66.08 78.39\n+VAWI-SBS 50.27 58.17 62.22 76.54 52.98 61.97 67.40 79.19\nT5-3B\n+None 70.16 73.02\n75.04 81.81 71.99 75.27 77.72 82.40\n+Images 70.96 73.60 75.91 82.40 72.87 76.17 78.71 83.64\nVAWI-SBS+PET 71.52 74.19 76.49 83.61 73.58 73.58 79.66 84.56\nTable 2: Performance comparison on CommonsenseQA-3k and CommonsenseQA with different amounts of training\ndata. We report the average performance on the dev set over three runs, and the BEST results are highlighted in\nbold. +Images denotes that we add retrieved images about the VH-words using web search engines, and encode\nthem via CLIP-ViT.\nMethod Base Model\nBLUE-3 BLUE-4 METOR Rouge-L CIDER SPICE\nBART\n-large\n+None 42.80 32.42 31.36 57.57 16.56 32.94\n+Images 42.67 32.67 32.12 57.46 16.78 32.81\n+VAWI-SBS 44.56 34.17 32.47 58.46 17.23 33.67\n+VAWI-SBS+PET 43.12 33.76 32.20 58.12 16.91 33.17\nT5-3b\n+None 45.92 35.92\n33.02 58.57 17.71 33.51\n+Images 45.69 35.50 33.55 58.94 17.51 32.91\n+VAWI-SBS 47.67 37.54 33.41 59.94 18.34 34.67\n+VAWI-SBS+PET 47.40 37.36 33.71 59.78 18.18 34.17\nTable 3: Performance comparison on CommonGen. We also show the performance of parameter-efficient tuning of\nour approach, denoted as +PET. The BEST results are highlighted in bold.\nthey have been continually pre-trained on large-\nscale image-text pairs, which may cause the catas-\ntrophic forgetting problem. Second, VaLMs (i.e.,\nVOKEN, iACE, and V AWI) achieve better perfor-\nmance over PLMs. As VaLMs infuse external vi-\nsual knowledge into the PLMs, they can help the\nPLMs better understand the background knowledge\nof some words (e.g., color, shape, and size of ob-\njects). Between the two VaLM baselines, iACE is\nslightly better. This is because iACE is enhanced\nbased on VOKEN and incorporates an image gen-\neration model, so it produces more visual infor-\nmation to utilize. However, the generated images\ninevitably contain noise and redundant information,\nwhich limits the performance gain of iACE.\nFinally, by comparing our approach with all base-\nlines, it is obvious that VAWI performs consis-\ntently better than them on the six datasets. In our\napproach, we adopt an efficient and effective way\nthat augments the visually-augmented representa-\ntions using the text encoder of CLIP to encode the\nVH-words from the input text. Benefiting from pre-\ntraining on large-scale image-text pairs, the text\nencoder of CLIP has been well aligned with the\nsemantic space of images, so that it can generate\nhigh-quality visually-augmented representations of\nthe VH-words to enrich them. Such a way not only\nsaves the costs of time and computation but also\nreduces the influence of inevitable noise from re-\ntrieved or generated images. Additionally, among\nthree VH-words extraction strategies, LBS slightly\noutperforms others in most NLU tasks. The reason\nis that LBS incorporates a learnable model-based\nstrategy to select the VH-words. Such a way can\nadaptively extract proper VH-words with the con-\nsideration of the intrinsic knowledge of the PLMs.\nHowever, LBS will increase the computation cost\ndue to its involved learnable VH-words extractor\nlayer. Therefore, for efficiency, in the following\nexperiments, we utilize the SBS strategy in our\napproach for comparison.\nEvaluation on Commonsense Reasoning Tasks.\nFollowing existing works (Lin et al., 2019a), we\n14918\nMethod SNLI-VE\n10% 20% 50% 100%\nALBEF 65.46 67.52 75.47 80.91\nALBEF+VAWI +SBS 65.94 68.23 76.14 81.64\nTable 4: Results on the test set of SNLI-VE task. The\nBEST results are highlighted in bold.\nalso rely on a rule-based strategy to extract the ex-\namples containing visible objects, to construct a\nnew dataset called CommonsenseQA-3K. It con-\nsists of 2,903 and 341 examples in the training set\nand dev set, respectively. Based on the Common-\nsenseQA and CommonsenseQA-3k, we also report\nthe results with different amounts of training data,\nto further evaluate the performance of different\nmethods in the few-shot setting.\nAs shown in Table 2, we can also see that with\nthe help of the visual information from either re-\ntrieved images or our V AWI-SBS, the performance\nof PLMs can be improved significantly. It indi-\ncates that visual information is indeed helpful to\nimprove PLMs for understanding commonsense\nknowledge. Besides, our approach outperforms the\nmethod using retrieved images from search engines.\nOur approach omits the image retrieval process due\nto its inevitably involved noise, and relies on the\ntext encoder of CLIP to augment the visual repre-\nsentations. Such a way can guarantee the relevance\nbetween the augmented visual knowledge and the\ntext input, reducing the influence of retrieved noisy\nimages and redundant information. Furthermore,\nwe also perform parameter-efficient tuning on T5-\n3B-encoder with our approach and boost its per-\nformance. It shows that our approach is able to be\napplied to large-scale PLMs to meet their thirst for\nvisual information.\nEvaluation on the Text Generation Task.As\nshown in previous experiments, it is useful to im-\nprove the performance of VAWI on commonsense\nreasoning and nature language understanding tasks.\nHere, we would like to study the effectiveness of\nour approach on the text generation task (i.e., Com-\nmonGen) using large PLMs. As shown in Table 3,\nour model VAWI also consistently boosts the per-\nformance of BART-Large and T5-3b among all\nmetrics. It further shows that our approach can\nalso improve PLMs on the text generation task.\nAs a comparison, we can see that the retrieved\nimages are not very helpful and even cause per-\nformance degradation. The reason may be that\nthe text generation task is more sensitive to the\ninevitable noise from the retrieved images. Fi-\nnally, the parameter-efficient tuning strategy of our\napproach also achieves comparable performance\nwith the full-parameter tuning. It indicates that our\nparameter-efficient strategy is able to efficiently\noptimize the parameters of large-scale PLMs, and\nshows a promising future to apply our approach to\nmuch larger PLMs, e.g., GPT-3.\nEvaluation on the Cross-modal Commonsense\nReasoning Task.To verify the generality of our\nmethod, we further implement our VAWI on a VL-\nPTM (i.e., ALBEF (Li et al., 2021)), and conduct\nexperiments on a cross-modal reasoning dataset,\nSNLI-VE. Concretely we implement our approach\non ALBEF by inserting the visually-augmented\nrepresentations after the VH-words embeddings of\nthe text encoder before the multimodal encoder,\nand keeping others unchanged. As shown in Ta-\nble 4, our VAWI can also improve the performance\nof ALBEF using different amounts of training data.\nIt further shows the generality of our approach in\nVL-PTMs, as it can also provide rich information\nto enhance the text encoder of VL-PTM, helping it\nbetter perform cross-modal reasoning.\n4.3 Ablation Study\nIn this part, we conduct a series of experiments to\nverify whether the improvement of our approach de-\nrives from the augmented visual knowledge about\nthe VH-words. More ablation studies are shown in\nAppendix A.\nThe Effect of the Source of Visual Representa-\ntions. We first propose three variants that incor-\nporate powerful PLMs, i.e., RoBERTa-base, T5-\nLarge, and T5-3b respectively, to replace the text\nencoder of CLIP in our framework. We also replace\nthe generated visual representations from the text\nencoder of CLIP with random noise, to investigate\nthe importance of the visual representations. As\nshown in Table 5, we can see that our approach is\nbetter than all the variants, even T5-3b with billion-\nscale parameters. It indicates that CLIP-base is\nmore effective to augment visual knowledge to\nimprove the performance of PLMs. Besides, our\napproach also outperforms the variant using ran-\ndom noise as the visual representation, showing the\nworse performance among all the variants. It also\nshows the importance of visual representations, as\nthey indeed contain the visual knowledge that the\n14919\nSource of\nvisual representation (Params) CSQA-3k CSQA SST-2 QQP STS-B QNLI\nRandom Noise (0M) 61.59\n66.78 89.13 86.27 85.13 87.22\nRoBERTa-large (355M) 61.18 67.17 89.43 86.53 85.60 87.77\nT5-large-encoder (375M) 62.21 67.87 89.71 86.67 86.40 87.94\nT5-3b-encoder (1500M) 63.10 68.42 90.24 86.96 86.93 88.21\nCLIP-base (52M) 65.10 71.07 91.41 87.72 87.67 89.40\nTable 5: Performance comparison of different sources of visual representation in our approach. The base model is\nRoBERTa-base.\nThe text encoder of different VL-PTMs (Params) CSQA-3k SST-2 QQP\nRandom Noise (0M) 61.59 89.23 86.21\nALBEF (110M) 63.34 90.72 87.17\nCLIP-base (52M) 65.10 91.41 87.72\nUniCL-base (52M) 65.98 91.75 88.07\nCLIP-large (123M) 66.27 92.10 88.31\nTable 6: Performance comparison of visual representations from different VL-PTMs in our approach. The base\nmodel is RoBERTa-base.\nPLM is hungry for.\nThe Effect of the Stronger VL-PTMs.In our\nwork, we choose CLIP-base to enhance PLMs,\nas it has been pre-trained on a large-scale image-\ntext dataset. Generally, a stronger VL-PTM would\nbe more promising to further improve the perfor-\nmance. Here, we replace our CLIP-base model\nwith some stronger VL-PTMs, e.g., ALBEF (Li\net al., 2021), UniCL-base (Yang et al., 2022), and\nCLIP-large. Concretely, ALBEF leverages more\npre-training tasks (e.g., MLM, ITM, and ITC),\nUniCL utilizes more high-quality pre-training data,\nand CLIP-large increases the scale of model param-\neters. We evaluate the above variations on CSQA-\n3k, QQP, and SST-2, and the results are shown in\nTable 6. We can see that UniCL and CLIP-large\noutperform CLIP-base. It indicates that the VL-\nPTMs with the larger scale of model parameters\nor more high-quality pre-training data are more ca-\npable of augmenting useful visual knowledge for\nPLMs. Considering the efficiency, CLIP-base is\nalso a good choice in our approach, and we will\ninvestigate more proper VL-PTMs in the future.\n5 Conclusion\nIn this paper, we proposed a general visually-\naugmented fine-tuning approach that can be applied\nto a variety of PLMs and NLP tasks, without using\nany retrieved or generated images, namely V A WI.\nSpecifically, we first identified and extracted the\nvisually-hungry words (VH-words) from input\ntext via a token selector, where three different\nmethods have been proposed, including syntax-,\nattention- and learning-based strategies. Then, we\nadopted a fixed VL-PTM text encoder to gener-\nate the visually-augmented representations of these\nVH-words. As it has been pre-trained by visual-\nlanguage alignment tasks on the large-scale cor-\npus, it is capable of injecting visual semantics into\nthe aligned text representations. Finally, we trans-\nformed the visually-aligned features into visually-\naugmented features by reformulation layer based\non VH-words, and inserted them into PLMs to en-\nrich the visual semantics of word representations\nin PLMs. Experimental results on 10 NLP tasks\nshow that our approach can consistently improve\nthe performance of BERT, RoBERTa, BART, and\nT5 at different scales, and outperform several com-\npetitive baselines significantly. Besides, the visual\nprompts of our framework can also be used for\nparameter-efficient tuning, which can boost the per-\nformance of large language models, such as T5-3b.\nLimitations\nAn important limitation of our approach V AWI is\nthe need for extracting visually-hungry words (VH-\nwords) as the trigger to inject visual knowledge\ninto PLMs. In real-world applications, it is hard\nto obtain the annotations of VH-words. Therefore,\nwe propose three VH-words extraction strategies.\nHowever, the three strategies may be not always\nproper for all NLP tasks, and we rely on the experi-\nmental results to select the best one among them.\nBesides, we adopt the text encoder of CLIP as the\n14920\nVL-PTM for generating the visually-aligned repre-\nsentation. As a pre-trained model, CLIP also may\ncontain biases learned from the pre-training corpus,\nwhich may result in improper biased prediction on\nsome NLP tasks.\nAcknowledgement\nThis work was partially supported by National\nNatural Science Foundation of China under Grant\nNo. 62222215, Beijing Natural Science Founda-\ntion under Grant No. 4222027, and Beijing Out-\nstanding Young Scientist Program under Grant No.\nBJJWZYJH012019100020098. Xin Zhao is the\ncorresponding author.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun\nLiu, and Pascale Fung. 2022. Enabling multimodal\ngeneration on clip via vision-language knowledge\ndistillation. arXiv preprint arXiv:2203.06386.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 workshop on Automated knowledge\nbase construction, pages 25–30.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categori-\ncal reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In ACL.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. arXiv preprint arXiv:2201.12086.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in neural information processing systems,\n34:9694–9705.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nWeixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-\nena Yeung, and James Zou. 2022. Mind the gap:\nUnderstanding the modality gap in multi-modal\ncontrastive representation learning. arXiv preprint\narXiv:2203.02053.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019a. Kagnet: Knowledge-aware graph net-\nworks for commonsense reasoning. arXiv preprint\narXiv:1909.02151.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2019b. Commongen: A constrained text gener-\nation challenge for generative commonsense reason-\ning. arXiv preprint arXiv:1911.03705.\nXiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao.\n2022. Things not written in text: Exploring spatial\ncommonsense from visual signals. arXiv preprint\narXiv:2203.08075.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. Ad-\nvances in neural information processing systems, 32.\nYujie Lu, Wanrong Zhu, Xin Eric Wang, Miguel Eck-\nstein, and William Yang Wang. 2022. Imagination-\naugmented natural language understanding. arXiv\npreprint arXiv:2204.08535.\nCory Paik, Stéphane Aroca-Ouellette, Alessandro Ron-\ncone, and Katharina Kann. 2021. The world of\nan octopus: How reporting bias influences a lan-\nguage model’s perception of color. arXiv preprint\narXiv:2110.08182.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, 63(10):1872–\n1897.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\n14921\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2019. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv\npreprint arXiv:1908.08530.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4149–4158.\nHao Tan and Mohit Bansal. 2020. V okenization: Im-\nproving language understanding with contextual-\nized, visual-grounded supervision. arXiv preprint\narXiv:2010.06775.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWeizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xi-\naodong Liu, Xifeng Yan, Jianfeng Gao, and Furu\nWei. 2022. Visually-augmented language modeling.\narXiv preprint arXiv:2205.10178.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations.\nNing Xie, Farley Lai, Derek Doran, and Asim Ka-\ndav. 2019. Visual entailment: A novel task for\nfine-grained image understanding. arXiv preprint\narXiv:1901.06706.\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin\nXiao, Ce Liu, Lu Yuan, and Jianfeng Gao. 2022. Uni-\nfied contrastive learning in image-text-label space. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 19163–\n19173.\nChenyu Zhang, Benjamin Van Durme, Zhuowan Li, and\nElias Stengel-Eskin. 2022. Visual commonsense in\npretrained unimodal and multimodal models. arXiv\npreprint arXiv:2205.01850.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nWanrong Zhu, An Yan, Yujie Lu, Wenda Xu, Xin Eric\nWang, Miguel Eckstein, and William Yang Wang.\n2022. Visualize before you write: Imagination-\nguided open-ended text generation. arXiv preprint\narXiv:2210.03765.\n14922\nA Ablation Study\nA.1 Ablation Study on Visual Knowledge\nAugmentation\nThe Effect of the Pre-trained Dataset of VL-\nPTMs. We notice that the pre-training dataset of\nVL-PTMs is different from PLMs. Here, we in-\nvestigate whether the captions or images from the\nlarge-scale image-text pairs contribute more to the\nperformance gain of our approach. To verify it, we\npre-train a new PLM only using the captions data.\nFollowing the setting of ALBEF, we utilize the pre-\ntrained parameters of BERT to initialize this model\nand only extract the captions from the pre-training\ndata of ALBEF (14.5M sentences in total). After\npre-training on these captions until convergence,\nwe utilize this model to replace CLIP-base in our\napproach and keep other settings unchanged. We\nconduct experiments on commonsense reasoning\nand NLU tasks to evaluate its effectiveness for aug-\nmenting visual knowledge. As shown in Table 7,\nwe can see that such a variation underperforms\nALBEF and our approach, and even leads to perfor-\nmance degradation on the CSQA task. It indicates\nthat during pre-training the image data is an im-\nportant resource for learning visual knowledge in\nVL-PTMs. Only text data (i.e., captions) can not\nprovide sufficient visual knowledge that PLMs are\nhungry for. Therefore, after pre-learned on large-\nscale text-image pairs, CLIP can absorb the useful\nvisual knowledge from the images and inject them\ninto PLMs in our approach. It further indicates\nthat the improvement of our method is due to the\ninvolvement of the visual information about the\nVH-words.\nA.2 Ablation Study on Visually-enhanced\nFine-tuning\nDifferent Insertion Positions of Visual Repre-\nsentations. In our visually-enhanced fine-tuning\nframework, we insert the visual representation of\nthe VH-word after its original word embedding. To\nverify its effectiveness, we propose three variants\nof it that do not insert, insert all visual representa-\ntions of VH-words before and after the input text,\nrespectively. As shown in Table8, we can observe\nthat all these variants would lead to a performance\ndecrease. It demonstrates that a proper position\nto insert the visual representation is important for\nthe utilization of augmented visual representations.\nBy inserting them after the word embeddings of\ncorresponding VH-words, PLMs can effectively\naggregate the visual representations to enrich the\nword representations, leading to better performance\non downstream NLP tasks.\nB Further Analysis\nThe Frozen CLIP’s Text Encoder.In the ex-\nperiment presented in Table 1, we directly fine-\ntuned CLIP and the results indicate that the per-\nformance of VL-PTMs’ text encoder is unsatisfac-\ntory when directly fine-tuned on NLP tasks. In\nour VAWI, we fix the model parameters of CLIP-\nbase’s text encoder to preserve the visual knowl-\nedge. Hence we also conduct experiments on four\nNLU tasks from GLUE using frozen CLIP. Spe-\ncially, we fix CLIP-base’s text encoder and only\nfine-tuned added 4 transformer layers above it. As\nshown in Table 9, we can see that CLIP’s perfor-\nmance under this setting is better than that of di-\nrectly full-parameter fine-tuning CLIP and also un-\nderperforms RoBERTa and BERT. It indicates that\nfixing CLIP is more suitable for NLP tasks, and\nshows the rationality of V AWI settings that always\nfix the CLIP’s text encoder in V AWI to preserve\nCLIP’s knowledge.\nThe Computation Latency of the Proposed\nMethods. In our VAWI, we fix the model parame-\nters of CLIP-base to preserve the visual knowledge.\nSuch a way can also decrease the computation costs\nduring training and inference. To verify it, we re-\nport the mean training and inference latency per\nbatch on the CSQA-3k dataset of our method and\nbaselines on RTX3090 GPU, where all these meth-\nods utilize RoBERTa-base as the backbone. As\nshown in Table 10, we can see that our proposed\nV AWI-SBS and V AWI-V ABS would not increase\nthe latency too much. For V AWI-LBS, as it re-\nquires a PLM and a VL-PTM to adaptively select\nthe VH-words, it will relatively increase the la-\ntency. As shown in Table 1, we can see that all the\nthree variants achieve comparable performance in\n6 NLU datasets. Therefore, it is more efficient and\neffective to select the SBS and V ABS variations\nin our approach. Despite it, we can see that all\nour variants own less latency than iACE, since our\napproach does not require a time-consuming image\ngeneration process. And as shown in Table 1, our\napproach can also achieve better performance.\nThe Effect of the Improper Visually-hungry\nWords. To analyze how the quality of the VH-\n14923\nThe text encoder of different VL-PTMs (Params) CSQA-3k CSQA SST-2 STS-B MNLI\nNone 61.59 67.90 89.23 85.46 79.06\nBERT pre-trained on captions (110M) 62.17 67.56 89.58 85.73 79.24\nALBEF (110M) 63.64 68.47 90.72 87.17 80.86\nCLIP-base (52M) 65.10 71.07 91.41 87.73 82.27\nTable 7: Performance comparison of visual representations pre-trained using different pre-training data in our\napproach. The base model is RoBERTa-base.\nInsert Positions CSQA-3k\n5% 10% 20% 100%\nNot insert 41.88 46.04 50.58 61.88\nBefore input text - 39.77 44.86 57.47\nAfter input text - 40.23 45.67 58.08\nAfter the VH-words 42.94 49.27 53.97 65.10\nTable 8: Performance comparison w.r.t. different inser-\ntion positions of visual representations. The base model\nis RoBERTa-base.\nSST-2 QNLI\nQQP STS-B\nCLIP-base 73.3 74.5\n72.8 73.8\nFixed CLIP-base 75.1 76.9 73.7 75.2\nTable 9: The effect of fixed CLIP’s text encoder.\nwords affects the performance of our approach,\nwe further conduct the experiments on CSQA-3K\nand two NLU tasks SST-2 and QQP from GLUE,\nto show the effect of insufficient VH-words on\nour model performance. After extracting the VH-\nwords, we remove part of them and only randomly\nsample 0%, 20%, and 50% VH-words for augmen-\ntation. As shown in Table 11, we can see that with\nthe decreasing of the sampling probability, the per-\nformance of our approach degrades gradually. It\nindicates that not enough VH-words would degrade\nthe performance of our approach.\nThe Number of VH-words.Our approach has an\nimportant hyper-parameter required to tune, such\nas the number of VH-words. VH-words can sup-\nply visual knowledge that PLMs may be hungry\nfor. Here, we would like to study whether more\nVH-words are better to improve performance. We\nconduct experiments on the QQP and CSQA-3K\ndatasets using RoBERTa-base as the backbone, and\npresent the results in Figure 2. We can see that\nwith the increase of the number of VH-words, the\nperformance gain of our approach first increases\nand then decreases. A possible reason is that too\nmany VH-words may also introduce noisy or re-\ndundant information (e.g., not very relevant words),\nMethod Training Time (s) Inference Time (s)\nRoBERTa-base 0.506 0.182\n+V oken 0.506 0.182\n+iACE 1.138 0.512\n+V AWI-SBS 0.587 0.241\n+V AWI-V ABS 0.680 0.308\n+V AWI-LBS 0.893 0.486\nTable 10: The computation latency during training and\ninference.\nCorrect VH-w\nords proportions CSQA-3k SST-2 QQP\n0 % 61.60 89.57\n87.63\n20 % 62.17 89.44 87.40\n50 % 64.22 91.73 89.20\n100 % 65.10 92.93 89.74\nNone 61.88 89.23\n86.21\nTable 11: The effect of the improper visually-hungry\nwords. The base model is RoBERTa-base.\nwhich would also influence the fine-tuning perfor-\nmance. Instead, it is also more efficient to select a\nfew VH-words (e.g., two words for CSQA-3k) for\ndeploying our approach in large-scale PLMs.\nCase Study of Extracted Visually-hungry Words.\nIn this part, we show the VH-words extracted by\nsyntax-, attention- and learning-based strategies in\nTable 12, Table 13, Table 14 and Table 15. We can\nsee that the three strategies would extract slightly\ndifferent VH-words. The reason is that the three\nstrategies are based on different techniques to iden-\ntify the VH-words. As we can see, the cases show\nthat most of the extracted VH-words by our strate-\ngies are generally related to some visual semantics,\ne.g., spider, two eyes. Although such VH-words\ncan not perfectly cover all the visual semantics,\nthey actually contain most of the important words\nthat the PLMs may be hungry for, e.g., red and yel-\nlow. Besides, we can also see that the VH-words\nextracted by our three strategies may not perfectly\nalign with human judgment. In fact, it is also hard\nfor humans to determine proper rules to identify\nVH-words, e.g., people, human, and water. In addi-\ntion, as the learned knowledge of PLM is a black\n14924\n0 1 2 3 4 5 6\nVH-words Numbers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5Accuracy Gain\nCSQA-3k\nQQP\nFigure 2: Performance comparison w.r.t. different num-\nbers of VH-words.\nbox, it is also difficult for humans to judge the\nusefulness of our extracted VH-words for PLMs.\nThe Interpretability of Augmented Embeddings.\nIn this part, we show how our augmented embed-\ndings infuse visual knowledge into the PLM. Con-\ncretely, we show the attention distributions of a\nPLM (i.e., RoBERTa-base) in the last few layers\nbefore and after infusing visually-augmented rep-\nresentations on CSQA. As shown in Table16, we\ncan see that the [CLS] tokens pay more attention to\nthe VH-words and their visually-augmented repre-\nsentations, and the VH-words also pay more atten-\ntion to their visually-augmented representations. It\nshows that the injected visually-augmented repre-\nsentations provide useful knowledge, which guides\nthe PLM to focus on more important tokens and\nalso improves the representations of the VH-words\nand the [CLS] token.\n14925\nInput\nInput sentence:Unlike a spider and his many sight seers, people only have what? two eyes.\nSyntax-based Strategy\nUnlike a spider and his many sight seers, people only have what? two eyes\nVisually-enhanced Attention Based Strategy\nUnlike a spider and his many sight seers, people only have what? two eyes.\nLearning-based Strategy\nUnlike a spider and his many sight seers, people only have what? two eyes.\nTable 12: The first instance from the CommonsenseQA dataset. The extractedvisually-hungry words are highlighted\nin green.\nInput\nInput sentence:Where on a river can a human hold a cup upright to catch water on a sunny, clear day? waterfall.\nSyntax-based Strategy\nWhere on a river can a human hold a cup upright to catch water on a sunny, clear day? waterfall.\nVisually-enhanced Attention Based Strategy\nWhere on a river can a human hold a cup upright to catch water on a sunny, clear day? waterfall.\nLearning-based Strategy\nWhere on a river can a human hold a cup upright to catch water on a sunny, clear day? waterfall.\nTable 13: The second instance from the CommonsenseQA dataset. The extracted visually-hungry words are\nhighlighted in green.\nInput\nInput sentence:the mesmerizing performances of the leads keep the film grounded and keep the audience riveted.\nSyntax-based Strategy\nthe mesmerizing performances of the leads keep the film grounded and keep the audience riveted.\nVisually-enhanced Attention Based Strategy\nthe mesmerizing performances of the leads keep the film grounded and keep the audience riveted.\nLearning-based Strategy\nthe mesmerizing performances of the leads keep the film grounded and keep the audience riveted.\nTable 14: The instance from the SST-2 dataset. The extracted visually-hungry words are highlighted in green.\nInput\nInput sentence:How do I sell dry Moringa leaves powder in Indian market? Can I use the moringa leaves that are already\nstarting to turn yellow or yellowish?\nSyntax-based Strategy\nHow do I sell dry Moringa leaves powder in Indian market? Can I use the moringa leaves that are already starting to turn\nyellow or yellowish?\nVisually-enhanced Attention Based Strategy\nHow do I sell dry Moringa leaves powder in Indian market? Can I use the moringa leaves that are already starting to turn\nyellow or yellowish?\nLearning-based Strategy\nHow do I sell dry Moringa leaves powder in Indian market? Can I use the moringa leaves that are already starting to turn\nyellow or yellowish?\nTable 15: The instance from the QQP dataset. The extracted visually-hungry words are highlighted in green.\n14926\nRoBERTa-Base VAWI\nSelf-Attention, Layer12, Head5 Self-Attention, Layer12, Head5\n[CLS]\nWhere\non\na\nriver\ncan\na\nhuman\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n?\nwaterfall\n[SEP]\n[CLS]\nWhere\non\na\nriver\ncan\na\nhuman\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n?\nwaterfall\n[SEP]\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n[CLS]\nWhere\non\na\nriver\n[river]\ncan\na\nhuman\n[human]\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n[day]\n?\nwaterfall\n[waterfall]\n[SEP]\n[CLS]\nWhere\non\na\nriver\n[river]\ncan\na\nhuman\n[human]\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n[day]\n?\nwaterfall\n[waterfall]\n[SEP]\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nSelf-Attention, Layer12, Head8 Self-Attention, Layer12, Head8\n[CLS]\nWhere\non\na\nriver\ncan\na\nhuman\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n?\nwaterfall\n[SEP]\n[CLS]\nWhere\non\na\nriver\ncan\na\nhuman\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n?\nwaterfall\n[SEP]\n0.1\n0.2\n0.3\n0.4\n0.5\n[CLS]\nWhere\non\na\nriver\n[river]\ncan\na\nhuman\n[human]\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n[day]\n?\nwaterfall\n[waterfall]\n[SEP]\n[CLS]\nWhere\non\na\nriver\n[river]\ncan\na\nhuman\n[human]\nhold\na\ncup\nupright\nto\ncatch\nwater\non\na\nsunny\n,\nclear\nday\n[day]\n?\nwaterfall\n[waterfall]\n[SEP]\n0.1\n0.2\n0.3\n0.4\n0.5\nTable 16: The attention maps of the self-attention layers on RoBERTa-base and our approach.\n14927\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6.\n□\u0013 A2. Did you discuss any potential risks of your work?\nThe potential risks can be found in Section 6.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe main claims can be found in Abstract and Section 5.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe introduce the dataset and pre-trained models used and the baselines in Section 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe cite the creators of artifacts we used in Section 4.1.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe use the default license of all artifacts in Section 4.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe use all artifacts with their default intended use in Section 4.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nExcept for the dataset we created ourselves in Section 4, the relevant statistics is not reported in any\nother datasets we use. The dataset we use exactly follows the amount and train/test/dev splits of data\nin the original dataset paper.\nC □\u0013 Did you run computational experiments?\nOur computational experiments for evaluating our method can be found in Section 4.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14928\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe report the number of parameters in the models used in a few experiments, which can be found\nin Section 4 and Appendix. In addition, we report the total computation budget in Section C of the\nAppendix.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe talk about them in Section 4.1.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nWe report descriptive statistics about our results, which can be found in Section 4.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe report these important settings in Section 4.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14929",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8640213012695312
    },
    {
      "name": "Language model",
      "score": 0.690442681312561
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6871609091758728
    },
    {
      "name": "Natural language processing",
      "score": 0.6069478988647461
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5828993916511536
    },
    {
      "name": "Language understanding",
      "score": 0.5011756420135498
    },
    {
      "name": "Machine learning",
      "score": 0.40580928325653076
    },
    {
      "name": "Programming language",
      "score": 0.07805904746055603
    }
  ]
}