{
  "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network",
  "url": "https://openalex.org/W2574917025",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3154006519",
      "name": "Yoon, Seunghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4322955611",
      "name": "Yun, Hyeongu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2274645566",
      "name": "Kim， Yuna",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Park, Gyu-tae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788288377",
      "name": "Jung, Kyomin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2949667497",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W196214544",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2895810819",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2295582178",
    "https://openalex.org/W2962988160",
    "https://openalex.org/W1552714718",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.",
  "full_text": "arXiv:1701.03578v1  [cs.CL]  13 Jan 2017\nEfﬁcient Transfer Learning Schemes for Personalized Language Modeling\nusing Recurrent Neural Network\nSeunghyun Yoon 1,2, Hyeongu Yun1, Yuna Kim2, Gyu-tae Park2 and Kyomin Jung 1,3\n1Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea\n2Software R&D Center, Samsung Electronics Co., Ltd., Seoul,Korea\n3Automation and Systems Research Institute, Seoul NationalUniversity, Seoul, Korea\n{mysmilesh, kjung}@snu.ac.kr, hyeongu.yun.1989@gmail.com,{yuna1.kim, gyutae.park}@samsung.com\nAbstract\nIn this paper, we propose an efﬁcient transfer leaning meth-\nods for training a personalized language model using a re-\ncurrent neural network with long short-term memory archi-\ntecture. With our proposed fast transfer learning schemes,a\ngeneral language model is updated to a personalized language\nmodel with a small amount of user data and a limited com-\nputing resource. These methods are especially useful for a\nmobile device environment while the data is prevented from\ntransferring out of the device for privacy purposes. Through\nexperiments on dialogue data in a drama, it is veriﬁed that\nour transfer learning methods have successfully generatedthe\npersonalized language model, whose output is more similar to\nthe personal language style in both qualitative and quantita-\ntive aspects.\nIntroduction\nRecently there has been a considerable interest in language\nmodeling due to various academic and commercial de-\nmands. Academically, many studies have investigated this\ndomain such as machine translation, chat-bot, message gen-\neration, image tagging and other language-related areas.\nCommercially, it can be used as a core technology for pro-\nviding a new application on consumer products or services.\nFor instance, an automatic message-reply prediction service\ncan be launched in mobile devices, thus helping a user to\nsend a reply message when he/she is not provided with a\nproper input interface.\nTo model the language of human dialogue, a recurrent\nneural network (RNN) structure is known to show the state\nof the arts performance with its ability to learn a sequen-\ntial pattern of the data (Sutskever, Vinyals, and Le 2014).\nAmong the RNN structures, a Long Short-Term Memory\nRNN (LSTM-RNN) and its variants are successfully used\nfor language modeling tasks (Hochreiter and Schmidhuber\n1997; Cho et al. 2014). However, as a kind of deep learning\ntechnique, the LSTM-RNN and the RNN structure requires\nboth a large number of data and huge computing power to\ntrain the model properly. Hence any attempts for applying\nthe RNN structure to personalized language modeling are\nmainly constrained by the following two limitations. First,\npersonal mobile devices contain private message data among\nclose acquaintances, so users seldom agree to transfer their\nlog out of the devices. This causes a limitation of gathering\nthe whole user data to common computing spaces, where\nhigh-performance machines are available. Second, in rela-\ntively small computing machines, i.e., smart phone, it is not\nalways-guaranteed to have enough resources to train a deep\nmodel within the devices.\nTo resolve these limitations, we propose fast transfer\nlearning schemes. It trains a base model with a large dataset\nand copies its ﬁrst n-many layers to the ﬁrst n-many layers\nof a target model. Then the target model is ﬁne-tuned with\nrelatively small target data. Several learning schemes such\nas freezing a certain layer or adding a surplus layer are pro-\nposed for achieving the result. In experiments, we trained a\ngeneral language model with huge corpus such as an Work-\nshop on Statistical Machine Translation (WMT) data1 and\na movie script data by using powerful computing machines,\nand then transferred the model to target environment for up-\ndating to be a personalized language model. With this ap-\nproach, the ﬁnal model can mimic target user’s language\nstyle with proper syntax.\nIn the experiments, we trained the general language model\nwith literary-style data and applied the transfer learningwith\nspoken-style data. Then we evaluated the model output for\nsentence completion task in a qualitative and a quantita-\ntive manner. The test result showed that the model learned\nthe style of the target language properly. Another test was\nconducted by training the general language model with the\nscript of the drama, “Friends,” and by applying transfer\nlearning with main character corpora from the script to gen-\nerate the personalized language model. The message-reply\nprediction task was evaluated with this model. The test result\nshows higher similarity between the output of the personal-\nized language model and the same user dialogue than the\none between the output of the personalized language model\nand other users’ dialogues.\nThe contributions of this paper are as follows. First, we\npropose efﬁcient transfer learning schemes for personalized\nlanguage modeling, which is the ﬁrst research on transfer\nlearning for RNN based language models with privacy pre-\nserving. Second, we show the applicability of our research\nto the target scenario in the short message reply application\nby training the model in the similar environment to that of\n1Available from “http://www.statmt.org/wmt14/translation-\ntask.html#download/”\nthe mobile device, and highlight its test results.\nArchitecture for Personalized Language Model\nAs we are focusing on a personalized language modeling\nwith the preservation of user data, we generate two types\nof language models. First is a sentence completion language\nmodel, which can complete sentences with a given n-many\nsequence of words. Second is a message-reply prediction\nlanguage model, which can generate a response sentence\nfor a given message. The output of both models implies\nuser characteristics such as preferable vocabulary, sentence\nlength, and other language-related patterns.\nTo achieve this result, we trained the language model with\na large amount of general data in powerful computing en-\nvironments, and then applied the transfer learning in rela-\ntively small computing environments. We assume that this\nmethod would be applied to mobile devices. As we are tak-\ning the preservation of privacy into consideration, the trans-\nferred model is retrained within the local environments such\nas mobile devices, and no personal data is sent out of the\ndevices. This could have been accomplished using the pro-\nposed transfer learning schemes in RNN-LSTM architec-\nture.\nSentence Completion Language Model\nA sentence completion model completes a sentence with the\ngiven word sequenceX = {x1, x2, . . . , xT }, wherexN\nis a word (N = 1,2, . . . , T). The model can predict the\nnext wordxN+1 with given word sequencex1:N . By repeat-\ning the prediction until the output word reaches the end-of-\nsentence signal, “< eos >,” the whole sentence can be gen-\nerated.\nThe model is similar to that of (Graves 2013), and we put\nthe 1,000-dimension word-embedding layer right after the\ninput layer. Then 3 deep LSTM layers with 100 LSTM cells\neach and without peephole connection are used for learning\nthe sequential pattern of the sentences.\nThe output probability to the input sequenceX and the\ntraining objective are\np(Y |X) =\nT∏\nt=1\np(yt|x1:t−1)\nL = − 1\n|T |\nT∑\nt=1\nxt+1 log p(yt|x1:t−1),\n(1)\nwhere X is a word sequence in the sentence,Y is a model\noutput sequenceY = {y1, y2, . . . , yT }\nMessage-Reply Prediction Language Model\nA message-reply prediction model generates a response sen-\ntence for a given message. It is similar to the sentence\ncompletion language model except that the message sen-\ntence is encoded and used as a context information when\nthe model generates a response word sequence. Our ap-\nproach is inspired by the sequence-to-sequence learning re-\nsearch (Sutskever, Vinyals, and Le 2014) that is success-\nfully applied to a machine translation task. The message\nword sequenceX = {x1, x2, . . . , xT } is fed into the model,\nand the last hidden state is used as context informationcT .\nWith this context information, the next sequence word is\npredicted similarly to that in the sentence completion lan-\nguage model case. During implementation, we used 1,000-\ndimension word embedding and 3-deep LSTM layers with\n100 LSTM cells in each layer. The output probability and\nthe training objective are\np(Y |X) =\nT ′\n∏\nt=1\np(yt|cT , y1:t−1)\nL = − 1\n|T ′||\nT ′\n∑\nt=1\nzt log p(yt|cT , y1:t−1),\n(2)\nwhere X is a word sequence in the message sentence,Z\nis a target word sequence in the response sentenceZ =\n{z1, z2, . . . , zT ′ }, Y is a model output sequenceY =\n{y1, y2, . . . , yT ′ },cT is the encoding vector for the message\nsentence.\nFast Transfer Learning Schemes\nTo generate a personalized language model with a small\namount of user data and limited computing resources, trans-\nfer learning is essential. In the private data preservationsce-\nnario, we investigate three fast transfer learning schemes.\nEach scheme is described below:\n• Scheme 1, relearn the whole layer: As a baseline, we re-\ntrain the whole model with private data only and compare\nthe result with the two other schemes below. Because of\nthe retraining of the LSTM layers in their entirety, this\nscheme requires more computing power than the other\ntwo schemes.\n• Scheme 2, surplus layer: After the training of the model\nwith general data, a surplus layer is inserted between the\noutput layer and the last of the deep LSTM layers. Then,\nwith private data, we update only the parameters of the\nsurplus layer in the transfer learning phase. We assume\nthat a user’s parlance could be modeled by learning addi-\ntional features in the user’s private data.\n• Scheme 3, ﬁxed ﬁrst n layers: After training the model\nwith general data, we ﬁx the parameters in the ﬁrst n\nLSTM layers (layer 1 and layer 2 in our experiments) and\ntrain remaining parameters in the transfer learning phase.\nWe assume that the user’s parlance is a subset of the gen-\neral pattern and the last layer plays the key role in deter-\nmining this pattern.\nMeasures\nThe perplexity is one of the popular measures for a lan-\nguage model. It measures how well the language model pre-\ndicts a sample. However, it is not good at measuring how\nwell the output of the language model matches a target lan-\nguage style. Another measure, the BLEU score algorithm\n(Papineni et al. 2002), has been widely used for the auto-\nmatic evaluation of the model output. However, it cannot be\napplied directly to measuring a quality of the personalized\ncharacter1 character2 character3 character4 character5 character6\ncharacter1 5.5667 5.7957 5.7814 5.7781 5.7026 5.8147\ncharacter2 6.1770 5.6097 5.8256 5.8254 5.7397 5.8562\ncharacter3 6.1741 5.8702 5.6057 5.8056 5.7101 5.8623\ncharacter4 6.1990 5.8672 5.8240 5.5726 5.7102 5.8689\ncharacter5 6.1948 5.8592 5.8176 5.7898 5.5099 5.8460\ncharacter6 6.1782 5.8415 5.8279 5.8132 5.7120 5.6171\nTable 1: Quantitative measure result of dialogues among main characters. Character1 to character6 are Chandler, Joey, Mon-\nica, Phoebe, Rachel, and Ross, respectively. A lower value indicates that the two sets compared have similar distributions and\nare, thus, similar in style.\nmodel output because it considers the similarity between\none language and the target language. Other research was\nconducted on proving authorship and fraud in literature, for\ninstance, Jane Austen’s left-over novel with partially com-\npleted (Morton 1978). This research counted the occurrence\nof several words in the literature, compared their relativefre-\nquencies with those of the words in the target literature, and\nconcluded that the target literature was a forgery. This ap-\nproach could be applied to a text evaluation where a large\namount of data is available and certain words are used more\nfrequently. In spoken language, such as in the message-reply\ncase, however, whole word distribution must be considered\ninstead of considering the occurrence of several words, be-\ncause the data is usually not enough than the literature case.\nSo, we use a simple and efﬁcient metric to measure the sim-\nilarity between the user style and the output of the personal-\nized model.\nAn output of a personalized language model can be mea-\nsured by calculating the cross entropy between the word dis-\ntribution of the model output and that of the target data.\nWord distribution can be acquired by normalizing a word\nhistogram which is calculated based on word counts in the\ntarget corpus. Equation (3) shows the metric formulation.\nY1 = g(fLM (Mi)), Y2 = g(Ti)\nmeasure = Cross Entropy(Y1, Y2), (3)\nwhere Mi is a message ∈ Dtest, Ti is a corpus\n∈ Dtarget , fLM is a language model,g(·) calculates\nword distribution with given corpus, CrossEntropy(p, q) is\n− ∑\nx p(x) logq(x).\nThe characteristics of a user speech can mainly be dis-\ntinguished by the word dictionary. Thus, this metric tries to\nmeasure the differences of the word dictionary among the\ncomparing set. Table 1 shows the quantitative measure re-\nsults from the dialogue set of the main characters in drama\ndata from “Friends,” a famous American television sitcom.\nIn the ﬁgures, “character\n1” to “character6” are the main\ncharacters of the drama (Chandler, Joey, Monica, Phoebe,\nRachel, and Ross, respectively). The dialogues were mea-\nsured against one another by using the cross entropy met-\nric. As shown in the table, the lower cross entropy value\namong the same character’s dialogue was calculated, and\nthe higher value was calculated among the different charac-\nter’s dialogues as expected. This result demonstrates thatthe\ncross entropy metric can be used to measure the similarities\namong the members of the set.\nDatasets\n• WMT14 ENG Corpus: The WMT’14 dataset includes\nseveral corpora. We only use an English part of the109\nFrench-English corpus. The dataset was crawled data\nfrom the bilingual web pages of the international orga-\nnizations (Callison-Burch et al. 2011). Thus, it contains\nhigh quality formal written language data. It consists of\n21,000,000 sentences.\n• English Bible Corpus:The English bible corpus is an-\nother type of written language data. It is useful data that\ndiffers from the WMT’14 dataset not only in the frequent\nvocabulary type but also in the average sentence length. It\nconsists of 31,102 sentences.\n• Drama Corpus: To collect spoken language data, we use\ndrama data from “Friends” from opensubtitles2. We ex-\ntracted 69,000 sentences from dialogues, which we used\nto train a sentence completion language model. For the\nmessage-reply prediction language model, pairwise data\nis required. Among the extracted data, two consecutive\nsentences of different characters are linked into a single\nsentence to generate pairwise data.\n• Main Character Corpora: From the drama corpus, we\nextract main character corpora to model personal users.\nFor example, the Chanlder (one of the main characters in\n“Friends”) corpus consisted of 8,406 lines and the Rachel\n(another major character in “Friends”) corpus consisted\nof 9,194 lines. The former data could represent a male\nadult, and the latter data could represent a female adult.\nWe assume that those amounts of data could be gathered\nin a user device for the personalizing language model.\nExperiments\nWe mainly conduct two types of experiments. The ﬁrst one\nis a sentence completion experiment, and the other one is\na message-reply prediction experiment. In the former case,\nwe train a general language model with literary-style data\nand apply a proposed transfer learning scheme with spoken-\nstyle data to achieve a personalized language model. With\nthis setting, the difference between general and personalized\nlanguage models can be measured in a quantitative and a\nqualitative manner. For the latter case, we use dialogue-style\ndata such as drama scripts to train a general language model.\nFrom the drama scripts, some characters’ data are taken\n2Available from “http://www.opensubtitles.org/”\nGeneral language model\nit is possible, however, that investments are only being upgraded in one or more labour conclusions.\nPersonal language model 1\nscheme 1 it is possible, however, we all offered to break this time youhave tools.\nscheme 2 it is possible, however, hes not bad enough than rachels feeling.\nscheme 3 it is possible, however, theyre right. you cant wait this.\nPersonal language model 2\nscheme 1 it is possible, however, ye are able to cut off the cross, and remain in the ﬁre, where they likewise eat shall ye be\namong them; and ye shall ﬁght against your brethren them taken abroad in our lord.\nscheme 2 it is possible, however, this mountain in the eleventh offering of the doctrine of god; and all the earth shall be\nthere without help.\nscheme 3 it is possible, however, the wilderness shall eat his drink.\nTable 2: Sample model output of general language model and personalized language model. The general language model used\nWMT’14 data, personalized language model 1 used “Friends” drama data, and personalized language model 2 used the English\nbible data. Scheme1 to scheme3 are relearn whole, surplus layer and ﬁxed-n layer, respectively. The output was generated\nwith the given input sequence, “It is possible, however”\napart and are used to train the personalized language model.\nWith this setting, the output of the personalized model is\ncompared to the original dialogue of the same character.\nLiterary-Style to Spoken-Style Sentence\nCompletion\nWe train a general language model of literary-style with the\nWMT’14 corpus. We then apply a transfer learning scheme\nwith “Friends” drama data for the model to learn the spoken-\nstyle language. Training the general language model took\nabout 10 days then we spent another 4 hours training the\npersonalized language model in each scheme. A “titan-X\nGPU” and a “GeForce GT 730 GPU” were used for these\nexperiments. The latter GPU is one of the low-end GPU se-\nries of which computing power was similar to that of latest\nmobile GPUs such as “Qualcomm Adreno 530” in “Sam-\nsung Galaxy S7” or “NVIDIA Tegra K1” in “Google Nexus\n9”. For a vocabulary setting, we construct our dictionary as\n50,002 words, including “< eos >” to mark ends of sen-\ntence and “**unknown**” to replace unconsidered vocabu-\nlary in the data. The out-of-vocabulary rate is about 3.5%.\nThe “general language model” in Table 2 shows the\nsample output of the general language model trained with\ndocument-style data, and the “personal language model 1”\nin Table 2 shows the sample output of the personalized\nlanguage model trained with human-dialogue-style data.\nScheme\n1 to scheme3 are relearn-whole, surplus layer, and\nﬁxed-n layer, respectively. Given input word sequence for\nthe test was, “It is possible, however.” As can be seen in the\ntable, both outputs differ in length and style. The sentence\ncompleted using the general language model tends to be\nlonger than that of obtained using the personalized language\nmodel. This result indicates that the personalized language\nmodel is properly trained with the spoken language charac-\nteristics because human dialogue is usually briefer than the\nlanguage in ofﬁcial documents.\nWe also apply the transfer learning schemes with some of\nthe English bible data. The same general language model,\nwhich involved previously training with the WMT’14 cor-\npus for 10 days, is used. English bible data is added and em-\nployed in training for another 4 hours using proposed trans-\nfer learning schemes.\nThe “personalized language model 2” in Table 2 shows\nthe sample output of the personalized language model\ntrained with another style of document data, English bible\ndata. As shown in Table 2, the output of the personalized lan-\nguage model contains more bible-like vocabulary and sen-\ntence styles.\nGeneral-Style to Personal-Style Message-Reply\nPrediction\nWe simulate the message-reply prediction scenario using the\ndrama corpus. The script of the drama, “Friends,” is used to\ntrain a general language model, and two main character cor-\npora are used to generate a personalized language model. For\nthis message-reply prediction experiment, we use a vocabu-\nlary size of 18,107, and the out-of-vocabulary rate is about\n3.5%. In the message-reply prediction case, pairwise data is\ngenerated by extracting the drama corpus of each charac-\nter and concatenating two consecutive sentences of different\ncharacters to form one single message-reply sentence data.\nWe insert the word “< eos >” between the message and re-\nply to mark the border separating them. This pairwise data is\nused for the training, and only the message part of the pair-\nwise data is used for the message-reply prediction. During\nimplementation, it took about a day to train the general lan-\nguage model with the “Friends” corpus and another 4 hours\nto train the personalized language model with two main\ncharacter corpora. The “titan-X GPU” and the “GeForce\nGT 730 GPU” was used for these experiments. Validation\nmessages-reply sentences of 1,281 are randomly sampled\nfrom the “Friends” corpus for tracking validation curve and\nanother 753 test messages are prepared for predicting the re-\nsponses. These data remained unseen from training phase.\nThe word distributions of the model output from the test\nmessages and the target corpus data are calculated to mea-\nsure their similarity.\nFigure 1 shows the validation curve while training. Per-\nplexity values from various model output are plotted. The\nperplexity of baseline model, “scheme\n1”, decreases until\nepoch\n0 5 10 15 20 25 30 35 40\nperplexity\n45\n50\n55\n60\n65\n70\nscheme_1\nscheme_2\nscheme_3\ntrain only 1st layer\ntrain only 2nd layer\nFigure 1: Validation curve for each schemes. Scheme1 is\nre-learn whole, scheme2 is surplus layer and scheme3 is\nﬁxed-n layer (train 3rd layer only).\naround epoch 10, and then it starts to increase because model\nis over-ﬁtted to training data. The proposed “scheme\n2”\nand “scheme3”, however, show continuous decreasing ten-\ndency and reach lower perplexity values compared to that of\nthe baseline model. It is interesting that proposed methods\nachieve lower perplexity than baseline while saving com-\nputing power with reduced parameters.\nTable 3 shows the performances of various models mea-\nsured with the same validation dataset used in Figure\n1. An unpruned n-gram language models using modiﬁed\nKneser-Ney smoothing are used for performance compar-\nisons (Chen and Goodman 1996). The n-gram models were\ntrained by using KenLM software package (Heaﬁeld et al.\n2013). The chandler n-gram model was trained with “Chan-\ndler” corpus and the friends n-gram model was trained with\n“Friends” corpus. The proposed scheme\n1 to scheme3 were\ntrained with “Chandler” corpus from “Friends” general lan-\nguage model. We see that our proposed schemes outperform\nthe n-gram models (n=3 and 5).\nTo check the inﬂuence of training data size (number\nof sentences) in personalized language model, we trained\nthe general language model (trained with “Friends” cor-\npus, message-reply prediction model) with different sizes\nof personal (“chandler” and “rachel”) dataset. The proposed\nscheme\n2 method was used for this test. Table 4 shows eval-\nuation results of the trained models. Dataset ’0’ means the\nmodel is not trained with personal dataset. The perplexity\nshows lower value as we use more dataset in training, and it\noutperforms “friends 5-gram” model from the 2,000 dataset\ncases.\nTable 5 indicates the cross entropy measure between the\noutput of “scheme\n1” to “scheme3” model and that of the\ntarget corpus, the “friends” drama corpus, the “chandler”\ncorpus, and the “bible” corpus. It shows the similarity be-\ntween the personalized model output and the target corpus as\nthe number of training epoch increasing. The general model\nwas pre-trained with the “Friends” corpus and the “Chan-\ndler” corpus was used training personalized model. Each\nModel is selected from various training epoch (0, 10, 20\nmodel perplexity\nchandler 3-gram 77.93\nchandler 5-gram 76.85\nfriends 3-gram 68.55\nfriends 5-gram 56.69\nscheme 1 (base line) 48.17\nscheme 2 46.02\nscheme 3 47.45\nTable 3: Performances of models measured with the same\nvalidation dataset used in Figure 1. The chandler n-gram\nmodel was trained with “Chandler” corpus and the friends\nn-gram model was trained with “Friends” corpus. The\nscheme\n1 model is over-ﬁtted to training data (see Figure\n1), and the lowest value is 48.17.\ndataset 0 1000 2000 4000 6000\nperplexity 68.38 58.93 52.94 48.37 47.07\nTable 4: Performances of models with different number of\nsentences in training dataset (lower is better). “Friends”cor-\npus was used pre-training the general model, and “Chan-\ndler” and “Rachel” corpus was used training the personal-\nized model with the proposed scheme\n2 method. Dataset ’0’\nmeans the model is not trained with personal dataset.\nand 40) and schemes, and test messages of 753 are used\nfor the reply generation with the selected model used. As\nthe table shows, the cross entropy measure has the highest\nvalue when the target corpus is the “bible” as expected be-\ncause it is written in different style than dialogues in drama\nscript. For the drama script case, the cross entropy measured\nwith the “chandler” corpus shows the lowest value among\nschemes. This result reveals that the personalized language\nmodel is trained properly from the general language model.\nThus it is more similar in style to the target data corpus\nthan the general language model. The “epoch 0” case means\nthe initial model state trained from general language cor-\npus, “friends” corpus. Thus cross entropy with “friends” tar-\nget corpus shows lower value than that of “chandler” and\n“bible” target corpus cases.\nRelated Work\nResearchers have proposed language models using RNN,\nwhich learns the probability of next sequence data at the\ncharacter or word level (Sutskever, Martens, and Hinton\n2011; Graves 2013). The proposed language models were\ntested on web corpora (i.e. Wikipedia, news articles) and\nqualitative examples showed their applicability. (Sutskever,\nVinyals, and Le 2014) proposed a sequence-to-sequence\nlearning algorithm with RNN and long short-term memory\n(LSTM) architecture (Hochreiter and Schmidhuber 1997),\nand (Cho et al. 2014) proposed RNN encoder-decoder ar-\nchitecture. Those studies were applied to the machine trans-\nlation problem.\nRecently, the RNN machine translation approach was ex-\ntended to the short message generation problem (Sordoni et\nal. 2015). Considering the message and response as a trans-\ntarget corpus model scheme epoch\n0 10 20 40\nfriends\n1\n6.1222\n6.5880 6.6049 6.6900\n2 6.8388 6.6090 6.6312\n3 6.6667 6.6974 6.5097\nchandler\n1\n6.9857\n6.0496 6.0292 6.0398\n2 6.3374 6.0862 6.1048\n3 6.0419 6.0499 5.9429\nbible\n1\n7.8622\n8.4750 8.2865 8.4472\n2 8.5955 8.3145 8.3594\n3 8.4471 8.3803 8.3028\nTable 5: Cross entropy measure between the language model output and the training data corpus, the “Friends” drama corpus,\nthe“Chandler” corpus and the “Bible” corpus. Scheme1 to scheme3 are relearn whole, surplus layer and ﬁxed-n layer, respec-\ntively. The “epoch 0” case means the initial model state trained from general language corpus, “friends” corpus. Thus cross\nentropy with “friends” target corpus shows lower value thanthat of “chandler” and “bible” target corpus cases. The lower value\nindicates that the language model output is similar in styleto the compared target corpus\nlation problem, the Neural Responding Machine achieved\n40% accuracy for both contextually and syntactically proper\nresponse generations with twitter-like micro-blogging data\n(Shang, Lu, and Li 2015). Those studies were similar to our\nresearch in the sense that both target message-reply predic-\ntion language model using RNN. Our research, however, dif-\nfers in that it updates a general language model to a person-\nalized language model with user data separately, whereas the\nprevious research trained a language model with the data, as\na whole, in same place.\nIn the commercial sphere, Google recently released a\nsmart-reply service that could generate a response to a\ngiven email by using a sequence-to-sequence learning model\n(Google 2015). There was another trial on the generation of\nresponses in technical troubleshooting discourses (Vinyals\nand Le 2015). This research also required complete data in\none place and did not provide a personalized model.\nMoreover, many researchers have conducted studies on\ntransfer learning. (Bengio et al. 2011; Bengio 2012) sug-\ngested that a base-trained model with general data could\nbe transferred to another domain. Recently, (Yosinski et al.\n2014) showed, through experiments, that the lower layers\ntended to have general features whereas the higher layer\ntended to have speciﬁc features. However, none of this re-\nsearch was applied to an RNN language model.\nTo adapt a neural network model to an embedded system\nwith limited resources, (Kim et al. 2015) (Han et al. 2015)\nreduced the size of the model by pruning the unnecessary\nconnections within it. It repeatedly tried to reduce the model\nsize without accuracy degradation. This research inspiredus\nto a considerable extent. It applied a neural model to mo-\nbile devices. However, the research focused on reducing the\nmodel size using a powerful machine and releasing the ﬁnal\nmodel to an embedded system, whereas ours investigated\nhow to train a model within mobile devices so that private\nuser data could be kept.\nConclusion\nWe propose an efﬁcient method for training a personalized\nmodel using the LSTM-RNN model. To preserve users’ pri-\nvacy, we suggest various transfer learning schemes so that\nthe personalized language model can be generated within\nthe user’s local environment. The proposed schemes “sur-\nplus layer’ and “ﬁxed-n layer’ shows higher generalization\nperformance whereas it trains only reduced number of pa-\nrameters than baseline model. The quantitative and qualita-\ntive test result indicate that the output of the model is similar\nto that of the user’s style.\nIt is certain that our proposed method reveals the appli-\ncability of the RNN-based language model in a user de-\nvice with the preservation of privacy. Furthermore, with our\nmethod the personalized language model can be generated\nwith a smaller amount of user data than the huge amount\nof training data that is usually required in the traditional\ndeep neural network discipline. In the future work, we aim\nto visualize the deep neural network and to investigate the\nspeciﬁc relationship among users’ language styles and the\nLSTM cells in the network. This approach seems likely to\nuncover enhanced learning schemes that require less data\nthan was previously necessary.\nReferences\nBengio, Y .; Bastien, F.; Bergeron, A.; Boulanger-\nLewandowski, N.; Breuel, T. M.; Chherawala, Y .; Cisse,\nM.; Cˆ ot´ e, M.; Erhan, D.; Eustache, J.; et al. 2011. Deep\nlearners beneﬁt more from out-of-distribution examples.\nIn International Conference on Artiﬁcial Intelligence and\nStatistics, 164–172.\nBengio, Y . 2012. Deep learning of representations for unsu-\npervised and transfer learning.Unsupervised and Transfer\nLearning Challenges in Machine Learning7:19.\nCallison-Burch, C.; Koehn, P.; Monz, C.; and Zaidan, O. F.\n2011. Findings of the 2011 workshop on statistical machine\ntranslation. InProceedings of the Sixth Workshop on Statis-\ntical Machine Translation, 22–64. Association for Compu-\ntational Linguistics.\nChen, S. F., and Goodman, J. 1996. An empirical study of\nsmoothing techniques for language modeling. InProceed-\nings of the 34th annual meeting on Association for Computa-\ntional Linguistics, 310–318. Association for Computational\nLinguistics.\nCho, K.; Van Merri¨ enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using rnn encoder-decoder for statis-\ntical machine translation.arXiv preprint arXiv:1406.1078.\nGoogle. 2015. Smart reply.\nhttp://googleresearch.blogspot.kr/2015/11/computer-\nrespond-to-this-email.html.\nGraves, A. 2013. Generating sequences with recurrent neu-\nral networks.arXiv preprint arXiv:1308.0850.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning\nboth weights and connections for efﬁcient neural network. In\nAdvances in Neural Information Processing Systems, 1135–\n1143.\nHeaﬁeld, K.; Pouzyrevsky, I.; Clark, J. H.; and Koehn, P.\n2013. Scalable modiﬁed kneser-ney language model esti-\nmation. InACL (2), 690–696.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation9(8):1735–1780.\nKim, Y .-D.; Park, E.; Yoo, S.; Choi, T.; Yang, L.; and Shin,\nD. 2015. Compression of deep convolutional neural net-\nworks for fast and low power mobile applications.arXiv\npreprint arXiv:1511.06530.\nMorton, A. Q. 1978.Literary detection: How to prove au-\nthorship and fraud in literature and documents. Scribner.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine transla-\ntion. InProceedings of the 40th annual meeting on associa-\ntion for computational linguistics, 311–318. Association for\nComputational Linguistics.\nShang, L.; Lu, Z.; and Li, H. 2015. Neural responding ma-\nchine for short-text conversation.ACL .\nSordoni, A.; Galley, M.; Auli, M.; Brockett, C.; Ji, Y .;\nMitchell, M.; Nie, J.-Y .; Gao, J.; and Dolan, B. 2015. A\nneural network approach to context-sensitive generation of\nconversational responses. InProceedings of the 2015 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, 196–205.\nSutskever, I.; Martens, J.; and Hinton, G. E. 2011. Gener-\nating text with recurrent neural networks. InProceedings\nof the 28th International Conference on Machine Learning\n(ICML-11), 1017–1024.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. InNIPS, 3104–\n3112.\nVinyals, O., and Le, Q. 2015. A neural conversational\nmodel.arXiv preprint arXiv:1506.05869.\nYosinski, J.; Clune, J.; Bengio, Y .; and Lipson, H. 2014.\nHow transferable are features in deep neural networks? In\nAdvances in Neural Information Processing Systems, 3320–\n3328.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6842566132545471
    },
    {
      "name": "Recurrent neural network",
      "score": 0.633827805519104
    },
    {
      "name": "Transfer of learning",
      "score": 0.6247720718383789
    },
    {
      "name": "Artificial neural network",
      "score": 0.565722644329071
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5058329105377197
    },
    {
      "name": "Transfer (computing)",
      "score": 0.45608729124069214
    },
    {
      "name": "Language model",
      "score": 0.4543444812297821
    },
    {
      "name": "Natural language processing",
      "score": 0.37756937742233276
    },
    {
      "name": "Parallel computing",
      "score": 0.12399572134017944
    }
  ]
}