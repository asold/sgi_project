{
    "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
    "url": "https://openalex.org/W3215529831",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4226865281",
            "name": "Nye, Maxwell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226865282",
            "name": "Andreassen, Anders Johan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379224",
            "name": "Gur-Ari, Guy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379230",
            "name": "Michalewski, Henryk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2727073232",
            "name": "Austin, Jacob",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221965717",
            "name": "Bieber, David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379242",
            "name": "Dohan, David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379248",
            "name": "Lewkowycz, Aitor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202120588",
            "name": "Bosma, Maarten",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379237",
            "name": "Luan, David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2609452214",
            "name": "Sutton, Charles",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226865292",
            "name": "Odena, Augustus",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3134307371",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W2601273560",
        "https://openalex.org/W3098714443",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3165784750",
        "https://openalex.org/W2172700936",
        "https://openalex.org/W2163234897",
        "https://openalex.org/W2963499994",
        "https://openalex.org/W3183378433",
        "https://openalex.org/W3121592593",
        "https://openalex.org/W2963005248",
        "https://openalex.org/W1496189301",
        "https://openalex.org/W2163274265",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W3195140297",
        "https://openalex.org/W2102258316",
        "https://openalex.org/W2963937837",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3169878837",
        "https://openalex.org/W2963187627",
        "https://openalex.org/W3179082817",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3107418514"
    ],
    "abstract": "Large pre-trained language models perform remarkably well on tasks that can be done \"in one pass\", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation \"step by step\", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a \"scratchpad\". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.",
    "full_text": "SHOW YOUR WORK : S CRATCHPADS FOR INTERMEDI -\nATE COMPUTATION WITH LANGUAGE MODELS\nMaxwell Nye12∗ Anders Johan Andreassen3 Guy Gur-Ari3 Henryk Michalewski2\nJacob Austin2 David Bieber2 David Dohan2 Aitor Lewkowycz3 Maarten Bosma2\nDavid Luan2 Charles Sutton2 Augustus Odena2\n1MIT\n2Google Research, Brain Team\n3Google Research, Blueshift Team\nABSTRACT\nLarge pre-trained language models perform remarkably well on tasks that can be\ndone “in one pass”, such as generating realistic text (Brown et al., 2020) or syn-\nthesizing computer programs (Chen et al., 2021; Austin et al., 2021). However,\nthey struggle with tasks that require unbounded multi-step computation, such as\nadding integers (Brown et al., 2020) or executing programs (Austin et al., 2021).\nSurprisingly, we ﬁnd that these same models are able to perform complex multi-\nstep computations—even in the few-shot regime—when asked to perform the op-\neration “step by step”, showing the results of intermediate computations. In par-\nticular, we train Transformers to perform multi-step computations by asking them\nto emit intermediate computation steps into a “scratchpad”. On a series of in-\ncreasingly complex tasks ranging from long addition to the execution of arbitrary\nprograms, we show that scratchpads dramatically improve the ability of language\nmodels to perform multi-step computations.\n1 I NTRODUCTION\nLarge Transformer-based language models exhibit surprisingly impressive capabilities (Devlin et al.,\n2019; Brown et al., 2020), including the ability to generate code that solves simple programming\nproblems (Chen et al., 2021; Austin et al., 2021). However, these models struggle to perform multi-\nstep algorithmic calculations, especially those that require precise reasoning and unbounded com-\nputation. For example, GPT-3 struggles to perform few-shot addition on numbers with greater than\nthree digits (Brown et al., 2020). Similarly, large-scale language models struggle to predict the re-\nsult of executing Python code, even code which is a solution to a programming task the model is\nable to solve (Austin et al., 2021). Likewise, standard recurrent and graph neural networks fail to\nsystematically generalize when predicting the output of simple programs with loops (Bieber et al.,\n2020). So language models can to some extent write code, but do not seem to accurately represent\nthe semantics of the code they write, because they cannot predict its execution. This has moti-\nvated research on networks that can perform algorithmic reasoning (Graves et al., 2014; Zaremba\n& Sutskever, 2014; Bieber et al., 2020). Neural networks that accurately represent the semantics of\nprograms could enable a variety of downstream tasks, including program synthesis (Devlin et al.,\n2017), program analysis (Allamanis et al., 2018), and other algorithmic reasoning tasks (Velickovic\n& Blundell, 2021).\nWhy do large language models struggle with algorithmic reasoning tasks? We suggest that this is at\nleast partly due to a limitation of the way the Transformer architecture is applied to these tasks: the\n∗Correspondence to mnye@mit.edu. Max is a student at MIT, but primarily did this work while visiting\nGoogle Research.\n1\narXiv:2112.00114v1  [cs.LG]  30 Nov 2021\nLarge Language \nModel\n  state: {} \n  line: def remove_Occ(s,ch):  \n  state: {\"remove_Occ\": \"<callable_object remove_Occ>\"} \n  line: output = remove_Occ(\"PHP\",\"P\") \n  state: {\"ch\": \"P\", \"s\": \"PHP\"} \n  line:     for i in range(len(s)):  \n  state: {\"ch\": \"P\", \"s\": \"PHP\", \"i\": 0} \n  line:         if (s[i] == ch):  \n  state: {\"ch\": \"P\", \"s\": \"PHP\", \"i\": 0} \n  line:             s = s[0 : i] + s[i + 1:]  \n  state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 0} \n  line:             break \n  state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 0} \n  line:     for i in range(len(s) - 1,-1,-1):   \n  state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 1} \n  line:         if (s[i] == ch):  \n  state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 1} \n  line:             s = s[0 : i] + s[i + 1:]  \n  state: {\"ch\": \"P\", \"s\": \"H\", \"i\": 1} \n  line:             break \n  state: {\"ch\": \"P\", \"s\": \"H\", \"i\": 1} \n  line:     return s  \n  state: {\"remove_Occ\": \"<callable_object remove_Occ>\", \n\"output\": \"H\"}\n    Consider the following Python function: \n     \n    def remove_Occ(s,ch):  \n        for i in range(len(s)):  \n            if (s[i] == ch):  \n                s = s[0 : i] + s[i + 1:]  \n                break \n        for i in range(len(s) - 1,-1,-1):   \n            if (s[i] == ch):  \n                s = s[0 : i] + s[i + 1:]  \n                break \n        return s  \n     \n    output = remove_Occ(\"PHP\",\"P\") \n     \n    What is the execution trace?\n    Consider the following Python function: \n     \n    def remove_Occ(s,ch):  \n        for i in range(len(s)):  \n            if (s[i] == ch):  \n                s = s[0 : i] + s[i + 1:]  \n                break \n        for i in range(len(s) - 1,-1,-1):   \n            if (s[i] == ch):  \n                s = s[0 : i] + s[i + 1:]  \n                break \n        return s  \n    Fill in the ??? below: \n    assert remove_Occ(\"PHP\",\"P\") == ???\n  assert remove_Oct(\"PHP\", \"P\") == \"H\"\nDIRECT EXECUTION PREDICTION\nSCRATCHPAD TRACING\nLarge Language \nModel\nFigure 1: Overview of our scratchpad approach applied to predicting code execution and comparison\nto direct execution prediction. Top: Previous work has shown that large pre-trained models achieve\npoor performance when asked to directly predict the result of executing given computer code (Austin\net al., 2021). Bottom: In this work, we show that training models to use a scratchpad and predict\nthe program execution trace line-by-line can lead to large improvements in execution prediction\nperformance. N.B. Although the example above only has one loop iteration for each loop, all loops\nare unrolled across time.\nmodel is asked to perform these tasks in one forward pass. Given a ﬁxed number of layers and a ﬁxed\namount of computation time, the model cannot adapt the amount of compute spent on a problem\nto its difﬁculty before producing an output. 1 Prior work (Graves, 2016; Banino et al., 2021) has\nexplored neural architectures that explicitly allow for dynamically chosen amounts of computation\ntime to be dedicated to different sub-tasks. In this work, we propose a different approach—one that\ncan exploit existing Transformer architectures and large few-shot-capable language models—we\nmodify the task design rather than the model or training procedure.\nOur proposal is simple: Allow the model to produce an arbitrary sequence of intermediate tokens,\nwhich we call a scratchpad, before producing the ﬁnal answer. For example, on addition problems,\nthe scratchpad contains the intermediate results from a standard long addition algorithm (see Figure\n2). To train the model, we encode the intermediate steps of the algorithm as text and use standard\nsupervised training.\nThis paper makes the following contributions:\n• We introduce (Section 2) the notion of a “scratchpad” for Transformers, in order to make them\nbetter at performing complex discrete computations without modifying the underlying architec-\nture.\n1Transformers perform a computation which is quadratic in the length of the input sequence, so are theoret-\nically unable to perfectly simulate algorithms which have greater time complexity than O(n2). However, it is\nunclear how relevant this theoretical bound is in practice; neural sequence prediction is approximate, and Trans-\nformers may be large enough in practice to effectively memorize the correct solutions for a relevant subspace\nof the possible inputs (e.g., all inputs up to a certain size).\n2\n• We show (Section 3) that scratchpads help Transformers learn to perform long addition in the\nﬁne-tuning regime, and in particular that they improve out-of-distribution generalization to larger\nproblem instances.\n• We also ﬁnd (Section 4) that scratchpads help Transformers perform a somewhat higher level\ntask: polynomial evaluation. This is true in both the few-shot and ﬁne-tuning regimes.\n• Finally, we move to a much more general context and show (Section 5) that training Transformers\nto emit full program traces line by line annotated with local variables dramatically improves their\nability to predict the result of executing a given computer program on a particular input. This\napplication in some sense subsumes the others.\n2 M ETHOD\nIn this work we consider two related problems: algorithm induction (Graves et al., 2014; 2016;\nKurach et al., 2016; Kaiser & Sutskever, 2016) and learning to execute (Zaremba & Sutskever,\n2014; Bieber et al., 2020). The goal of both problems is for the neural network to learn to emulate a\nfunction f, which is “algorithmic” in the sense that it can be represented by a short program, such as\naddition or polynomial evaluation, from input-output behavior. In neural algorithm induction, the\ngoal is to learn a single algorithm, and each training example gives a single input and desired output\nrepresented as strings. Therefore, the training data is D = {xi,f(xi)}N\ni=1. For learning to execute,\nwe want the model to produce the result of a program, represented as source code, on some input. If\neach πi is the source code of a program fi, then the training data is D= {(πi,xi,fi(xi))}N\ni=1 (it is\ncommon for each fi to have multiple input-output examples, but we omit this to lighten notation).\nThe main idea of this paper is that to solve a given algorithmic task, we simply encode the inter-\nmediate steps of the algorithm as text and train the model to emit them to a buffer that we call a\n“scratchpad.” For example, let us consider the algorithmic induction task of learning long addition.\nTo teach a model to add 29 to 57, a training example may look like the text in Figure 2, where the\nsteps of the grade-school long addition algorithm are written out explicitly.\nInput:\n2 9 + 5 7\nTarget:\n<scratch>\n2 9 + 5 7 , C: 0\n2 + 5 , 6 C: 1 # added 9 + 7 = 6 carry 1\n, 8 6 C: 0 # added 2 + 5 + 1 = 8 carry 0\n0 8 6\n</scratch>\n8 6\nFigure 2: Example of input and target for addition\nwith a scratchpad. The carry is recorded in the\ndigit following “C:”. Comments (marked by #)\nare added for clarity and are not part of the target.\nLearning to execute tasks can be encoded in a\nsimilar way, except now we add the source code\nπi before the input, scratchpad, and desired out-\nput. An example of a training example for a\nlearning to execute task is shown in Figure 1.\nAt training time, the model will be given the\ninput plus target for standard likelihood-based\ntraining. At test time, the model will be given\nonly the input and will be required to predict the\ntarget, e.g., by beam search or temperature sam-\npling. In principle, any sequence model could\nbe used for this. In this work, we choose to\nuse decoder-only Transformer language mod-\nels, but other sequence models could be effec-\ntive, such as encoder-decoder models (Raffel\net al., 2019), or recurrent networks.\nAdding a scratchpad has several potential ad-\nvantages: First, the model has adaptive compu-\ntation time, that is, it can now process the information for as long as needed, depending on the\ncomplexity of the task given the input. Second, the model can store the intermediate state of its\ncomputation in the scratch buffer and refer back to it by attending to its context. This removes\nthe need to store all intermediate state in activations. Third, by forcing the model to output con-\ncrete intermediate states by sampling from the generative model, we aim to reduce the propagation\nand compounding of small errors, because states are quantized to token embeddings. Compounded\nerrors can show up in methods—like Neural Turing Machines (Graves et al., 2014)—that use recur-\nrence to support extended computations. Finally, examining a model’s scratchpad output can help\nus identify common errors and correct them by revising the scratchpad format. We found this ability\nto interpret errors to be useful in this work.\n3\n107 108 109\nParameter Count\n0\n20\n40\n60\n80\n100Accuracy (%)\nAddition: In-Distribution Accuracy\nbaseline\nscratchpad\n107 108 109\nParameter Count\n0\n20\n40\n60\n80\n100\nAddition: OOD Accuracy (9 digits)\nbaseline\nscratchpad\n107 108 109\nParameter Count\n0\n20\n40\n60\n80\n100\nAddition: OOD Accuracy (10 digits)\nbaseline\nscratchpad\nFigure 3: Using a scratchpad signiﬁcantly improves the performance of pre-trained Transformer-\nbased models on addition, including their ability to generalize out of the training distribution to\nnumbers with more digits. Models were trained on 1-8 digit addition. The baseline models were\ntrained without intermediate scratchpad steps.\nFor all experiments, we used pre-trained dense decoder-only Transformer language models, ranging\nin size from 2 million to 137 billion parameters. These models were pre-trained on web documents\nand dialog data, and correspond to the models used in Austin et al. (2021).\n3 A DDITION\nAs a ﬁrst task, we consider integer addition. The baseline addition task presents two numbers as the\ninput, and the target is their sum. For example:2\nInput: 2 9 + 5 7\nTarget: 8 6\nWe implement the scratchpad by including the intermediate steps of the long addition algorithm in\nthe target, as in Figure 2. We train several models on integer addition problems with inputs that have\n1-8 digits. We then test performance on in-distribution addition problems (with up to 8 digit inputs),\nand on out-of-distribution problems with 9 and 10 digit inputs. The models were ﬁne-tuned on 100k\nexamples for 5k steps with batch size 32. There are 10k in-distribution test examples, and 1k test\nexamples for each out-of-distribution task. We examine the performance as a function of model size,\nranging from 2M to 1B parameters. We compare performance to a baseline which includes the input\nand target numbers, but no intermediate scratchpad steps.\nResults Figure 3 compares the performance of the scratchpad algorithm with the baseline. We see\nthat beyond a critical model size, models are able to solve the addition task using the scratchpad,\nwhile models trained without a scratchpad fail to do so even at the largest tested scale. On the out-of-\ndistribution tasks (9-10 digit addition), we ﬁnd that models trained without scratchpad completely\nfail, while models trained with scratchpad show consistent improvement as a function of model size.\n4 P OLYNOMIAL EVALUATION\nNext we focus on a slightly higher-level task: evaluating polynomials. Inspired by the “polynomial\nevaluation” subproblem in Saxton et al. (2019), we generate a dataset of polynomials of degree less\nthan or equal to three, with integer coefﬁcients and inputs constrained to the range[−10,10]. We also\nrestrict outputs to the range [−1000,1000]. We generate a training dataset of 10,000 polynomials\nand a test dataset of size 2,000. An example scratchpad target for this task is shown in Figure 4,\nwith each term of the polynomial evaluated separately. As in the previous section, we compare the\nresults of direct execution with the results of using the scratchpad. In this experiment, we evaluate\nin the few-shot regime using a 137B parameter pre-trained decoder-only model, as previous work\nindicates that very large models may be able to perform additions and multiplications with 3 or fewer\ndigits few-shot (Brown et al., 2020). We use n= 4example problems in the few-shot prompt. We\nalso evaluate in the ﬁne-tuning regime with an 8B parameter model ﬁne-tuned for 2000 steps on the\ntraining set. The results of both evaluations are shown in Table 1. We ﬁnd that scratchpad execution\noutperforms direct execution signiﬁcantly in both the few-shot and ﬁne-tuning regimes.\n2We introduce spaces between the digits to ensure that each digit is mapped to a separate token.\n4\nInput:\nEvaluate -7*x**2 + 7*x + 5 at x = 1\nTarget:\n<scratch>\n-7*x**2: -7\n7*x: 7\n5: 5\n</scratch>\ntotal: 5\nFigure 4: Example of polynomial evaluation\nwith a scratchpad. Each term in the polyno-\nmial is computed separately and then added.\nTable 1: Results for polynomial evaluation\ntask. Scratchpad outperforms direct predic-\ntion whether using ﬁne-tuning or few-shot.\nFew-shot Fine-tuning\nDirect prediction 8.8% 31.8%\nScratchpad 20.1% 50.7%\n5 E XECUTING PYTHON PROGRAMS\nWe have shown that scratchpads can help algorithm induction, that is, they can help models learn to\nimplement a particular algorithm with direct algorithm-speciﬁc supervision. But needing to hand-\ndesign the intermediate states for every new task is sub-optimal. In this section, we evaluate whether\na model can learn to implement a new algorithm by executing arbitrary code. To test this capability,\nwe follow the problem setup from Austin et al. (2021), in which language models are asked to predict\nthe result of executing a given Python program on a particular input. Language models performed\npoorly at this task, even on programs which are solutions to a programming tasks the model is able to\nsolve. Here we show that the scratchpad technique can dramatically improve the ability of language\nmodels to execute programs.\nDirect execution prediction Our main baseline is the direct execution prediction procedure ex-\nplored in Austin et al. (2021). Models are shown the source code for a function, and asked to predict\nthe output of running the function on speciﬁc inputs. For example, the function in Figure 1 takes as\ninput a string s and a character ch, and removes the ﬁrst and last instances of the character ch from\nthe string s. The direct execution prompt and target for this task are shown in the “Direct Execution\nPrediction” box in Figure 1. A task is considered solved under this regime if the model correctly\noutputs the target string.\nExecution prediction via scratchpad tracing As discussed above, direct execution prediction\nrequires the model to correctly output the result of executing the entire function in a single pass.\nDirect execution prediction has been shown to perform poorly on Python programs in Austin et al.\n(2021). We therefore design a scratchpad formulation of the execution task, in which models predict\nthe output of a program by ﬁrst predicting the sequence of intermediate states computed during the\nprogram’s execution. Formally, we train models to predict an alternating sequence of 1) the ordered\nsequence of source code lines executed, and 2) the state of the local variables after each line is\nexecuted. We call this object the program’s trace, and it allows us to track both the control ﬂow—\nthe sequence of operations executed—and how the state changes as a result of each operation. We\nrepresent the trace as a string, with the line of code reproduced directly, and the state information\nrepresented as a JSON dictionary.3 For example, the “Scratchpad Tracing” box in Figure 1 contains\nthe tracing prompt and trace target for the function discussed above.\nConcretely, for each function to be traced, the prompt is formed by printing the function deﬁnition,\nfollowed by a line which calls the function on a particular input:output = fn name(input value),\nwhere fn name and input value are replaced with the corresponding function name and input\nvalue. In Figure 1, note how the correct output of remove Occ(\"PHP\",\"P\") is shown in the last\nline of the trace, assigned to the variable \"output\". A tracing example is considered to have the\ncorrect execution output if the encoding of the value assigned to the variable output in the last line\nis a semantic match with the target output value (here, \"output\": \"P\" ). We consider a task to\nbe executed correctly if all given input-output examples are correctly executed. We can also test\n3Some objects cannot be represented using this JSON representation. Some objects (such as tuples) are\nrepresented by the closest JSON data type (in this case, lists). Other objects, such as user-constructed objects,\nare represented by a placeholder string, e.g., \"<object myObject>\". Functions are also represented as strings,\ne.g., \"<callable object f>\".\n5\ndef f(v0):\nv0 += 0\nv4 = 2\nwhile v4 > 0:\nv4 -= 1\nv0 *= 2\nreturn v0\noutput = f(8)\nFigure 5: Example synthetic Python program.\nTable 2: Synthetic tracing and execution re-\nsults. Scratchpad outperforms direct predic-\ntion both for few-shot and ﬁne-tuned.\n*The accuracy criterion for the few-shot scratch-\npad condition was slightly modiﬁed, see the text\nof Section 5.1 for more details.\nFew-shot Fine-tuned\nDirect prediction 11% 20%\nScratchpad 26.5%* 41.5%\nwhether there is a “trace exact match” between the model prediction and the ground truth trace, by\na) semantically comparing each state in the trace to the corresponding state in the ground truth trace,\nand b) comparing the sequence of source code lines predicted with the ground truth sequence.\nExperimental setup As a proof-of-concept, we ﬁrst show that scratchpad tracing greatly improves\nexecution performance on synthetic Python data. Then, we compare scratchpad tracing and execu-\ntion on the human-written Python problems from Austin et al. (2021). We ﬁnd that a novel data\naugmentation technique that uses programs generated by the model as additional training data can\nsigniﬁcantly increase tracing performance on real data, whereas this augmentation technique hurts\nperformance for direct execution. Finally, we show that incorporating tracing data from additional\nsources further improves tracing performance, indicating that the scratchpad tracing technique ex-\nplored here may scale well with more data.\nFor all experiments on Python code, we use a Transformer model with around 137 billion parame-\nters, a context window of 1024 tokens and a limit of 512 generation tokens. Unless otherwise stated,\nall ﬁne-tuning runs used a batch size of 8192 tokens and a learning rate of 3e-5, and model inference\nwas performed with decoding temperature set to T = 0, equivalent to greedy decoding.\n5.1 S CRATCHPAD BEATS DIRECT EXECUTION FOR SYNTHETIC PYTHON PROGRAMS\nIn our ﬁrst experiment, we test the few-shot and ﬁne-tuned execution capabilities of our models on\nsimple synthetic Python programs. This provides a proof-of-concept for our tracing technique.\nWe use a dataset of synthetic Python programs modiﬁed from Bieber et al. (2020). These programs\ninclude small integers (0, 1, and 2), simplewhile loops, and if statements. We construct a corpus of\nsynthetic programs to mimic the size of the MBPP dataset in Austin et al. (2021), with 400 training\nprograms, 100 validation programs, and 200 test programs. For each program, three random integer\ninputs are sampled from the range 0 to 9.\nWe test execution and scratchpad tracing under both few-shot and ﬁne-tuning conditions. For few-\nshot experiments, the prompt contains three examples of previous tracing problems, as shown in\nAppendix C. For ﬁne-tuned experiments, we ﬁne-tune models to convergence on the training split,\nas judged by validation perplexity.\nFor the few-shot scratchpad experiment, we noticed that models would not assign the variable name\noutput to the ﬁnal value in the trace, and would instead continue using v0 (the name of the variable\nreturned in the function f) as the variable name for the ﬁnal output line. We therefore modiﬁed the\naccuracy criterion from checking whether the value of output in the last line of the trace is correct,\nto checking whether the value of v0 is correct. (Under naive scoring, the few-shot tracing accuracy\nis roughly zero.) An example of this behavior is shown in Appendix D.\nResults Table 2 shows our results on synthetic Python problems. In both few-shot and ﬁne-tuned\nsettings, the scratchpad tracing technique leads to higher overall execution accuracy on the 200 test\nproblems. Fine-tuning also improves performance more for the scratchpad tracing technique than it\ndoes for direct execution.\n6\n5.2 S CRATCHPAD BEATS DIRECT EXECUTION FOR REAL PROGRAMS\nIn our second set of experiments, we explore how well the scratchpad performs compared to exe-\ncution on real data. Our main evaluation dataset is the MBPP dataset, introduced in Austin et al.\n(2021). MBPP consists of 1000 programming problems, each of which contains a natural language\nspeciﬁcation, a ground-truth Python program, and three input-output test cases. These programs\ninvolve computation using a large variety of types, including ints, strings, ﬂoats, dictionaries, tuples,\nand more, and include many language features and control-ﬂow structures, such as loops, com-\nprehensions, library imports, API calls and recursion. The evaluation split of the MBPP dataset\ncontains 500 tasks. In order to separate out effects of the generation window size, we report all eval-\nuation metrics on the subset of these tasks for which the ground-truth trace ﬁts within the generation\nwindow of the model for all three of the input-output examples. This leaves a subset of 212 test\ntasks. Increasing generation and context window length is an important issue for Transformer-based\nmodels, but we view it as orthogonal and leave it for future work.\n5.2.1 P ERFORMANCE IS POOR IN THE VERY-LOW-DATA REGIME\nIn our ﬁrst experiment with the MBPP data, we train a scratchpad tracing model on the 374 training\ntasks (3 examples per task, so 1122 overall examples). We discard all training examples which ex-\nceed the context window. We compare overall execution results against a model trained on the same\n374 training tasks to perform direct execution. The columns labeled “MBPP” for Direct Execution\nand Scratchpad in Table 3 show the results of this experiment. Neither the scratchpad model or the\ndirect execution model achieve good performance (5% and 10% output accuracy, respectively), and\ndirect execution outperforms the scratchpad model.\n5.2.2 S AMPLED PROGRAMS MAKE GOOD SCRATCHPAD TRAINING DATA\nNext, we employ a data augmentation technique to increase the size of the training dataset: We\nﬁrst run few-shot synthesis on the 374 MBPP training tasks using the pre-trained 137B model, as\ndescribed in Austin et al. (2021). For each task, we sample and record 80 candidate programs\n{Ps}from the model at temperature T = 0.5. We can then create a new execution datapoint\nusing the candidate program Ps, the original three inputs for the task {xi}i=1,2,3, and the three new\noutputs which result from running the candidate program on the original three inputs:{yinew}i=1,2,3,\nwhere yinew = Ps(xi). We discard any candidate programs for which execution results in an error.\nNote that the outputs of yinew may or may not be equal to the original outputs, depending on the\ncomputation performed by the generated program Ps. Therefore, this augmented direct execution\ndataset has both additional new programs and new outputs compared to the original dataset. We\ncan analogously create a tracing dataset for our scratchpad model by tracing the execution of each\ncandidate program Ps on each xi. This process produces much larger tracing and execution datasets\nwith 17k new programs, which we refer to as MBPP-aug.\nConceptually, we have augmented the dataset using a combination of tools already available to us,\nnamely a) the neural model, and b) program execution via a Python interpreter. We ﬁne-tune direct\nexecution and scratchpad models on this new augmented dataset MBPP-aug, using the same process\nas above.\nThe “MBPP-aug” columns in Table 3 show the results of this experiment. While the direct execution\napproach suffers a decrease in accuracy when trained on this additional data, the performance of the\nscratchpad model is greatly improved; the model trained on the augmented data solves more than\nthree times the number of tasks as the model trained on only the original MBPP programs. We also\nnote that if we measure the raw correctness across samples, the model already achieves 26.8% exact\ntrace match, which is surprisingly high.\n5.3 S CRATCHPAD TRAINING MAKES GOOD USE OF LARGE DATASETS\nIn this section, we examine whether collecting additional tracing data from human-written pro-\ngrams further improves tracing performance. This will allow us to understand whether the tracing\nprocedure here is likely to scale well when slightly out-of-distribution tracing data is added to the\nﬁne-tuning set. We experiment using two datasets:\n7\nTable 3: Comparison of models ﬁne-tuned on different data sets and evaluated on MBPP programs.\nWe report “per-task” execution and tracing accuracies, which require all examples to be correctly\nexecuted/traced. We additionally report “per-example” accuracies, which correspond to the total\nfraction of test examples which are executed/traced correctly across the dataset. We ﬁnd that training\nscratchpad models on an dataset augmented with samples from the model signiﬁcantly improves\nperformance for the scratchpad model, while it harms the direct execution model. Combining tracing\ntraining data from several sources further improves scratchpad model performance.\nDirect execution Scratchpad\nMBPP MBPP-aug MBPP MBPP-aug MBPP-aug MBPP-aug MBPP-aug\n+CodeNet +CodeNet +single line\n+single line\n(§5.2.1) (§5.2.2) (§5.2.1) (§5.2.2) (§5.3) (§5.3) (§5.3)\nper-task execution acc: 10.3 5.1 5.1 17.3 26.6 25.2 23.4\nper-task trace acc: n/a n/a 0.9 13.1 24.6 22.0 21.5\nper-example execution acc: 22.0 12.3 24.6 35.5 46.0 45.3 43.5\nper-example trace acc: n/a n/a 6.7 26.8 41.9 42.1 40.2\nstate: b = 15; code: b = b // 2; output: b = 7;\nstate: g = 100; i = 1; l = [100, 100, 0, 0, -100, -100];\ncode: g += l[i]; output: g = 200; i = 1; l = [100, 100, 0, 0, -100, -100];\nstate: s = /quotesingle.Varaabbcd/quotesingle.Var; code: o = set(s); output: o = {/quotesingle.Vara/quotesingle.Var, /quotesingle.Varb/quotesingle.Var, /quotesingle.Varc/quotesingle.Var, /quotesingle.Vard/quotesingle.Var}; s = /quotesingle.Varaabbcd/quotesingle.Var;\nstate: f = 63; i = 11; j = 53; code: f = i ˆ j; output: f = 62; i = 11; j = 53;\na, b, x = map(int, input().split())\nif a // 2 < b:\nif x % 1000 == 0:\nprint(a*(x//1000))\nelse:\nif (x % 1000) / 500 > 1:\nprint(min(a*(x//1000 + 1), a*(x//1000) + b*2))\nelse:\nprint(min(a*(x//1000 + 1), a*(x//1000) + b))\nelse:\nif x % 500 == 0:\nprint(b*(x//500))\nelse:\nprint(b*(x//500 + 1))\nFigure 6: Top: examples of single line data. Bottom: example CodeNet submission.\nSingle-line programs This dataset consists of roughly 9 million examples of single-line Python\ntransformations. Figure 6 (Top) shows examples of these transformations. Each transformation\nconsists of an initial set of variables and corresponding values, a single line of Python (together\nthese form the input), and the new set of variables and values which results from running the line\n(the target). When training on single-line data, we do not introduce intermediate scratchpad steps.\nWhile this dataset does not provide examples of the high-level, multi-line control ﬂow of a trace,\nthe data provides good supervision for modeling the execution of individual lines of code, which is\na key component of tracing. This data was collected by Fraser Greenlee, and can be accessed here.\nCodeNet The Project CodeNet dataset (Puri et al., 2021) consists of millions of user submissions\nto approximately 4,000 coding problems. These submissions include both correct and incorrect\nsolutions to programming problems. However, from the experiment with MBPP-aug above, we\nknow that incorrect or broken programs can still provide a useful training signal. We additionally\nimproved our tracing technique to allow tracing programs with errors; when an error is reached,\nthe error message is added to the end of the trace text and tracing is stopped. We extracted a total\nof 670,904 traces from the CodeNet data. For each dataset, we ﬁrst ﬁne-tune the model on these\ndatasets, and then perform a second ﬁne-tuning on MBPP-aug until convergence.\nResults Results are shown in Table 3. As above, we report execution accuracy across tasks. We\nadditionally report trace accuracy across tasks, to understand the extent to which the entire trace is\n8\naccurately predicted. We also report the raw execution and trace accuracy across all test examples,\nas an additional metric to compare models.\nTraining on either the single-line dataset or the CodeNet dataset alone seem to both provide gains\nover MBPP-aug (23.4% and 25.2% tasks executed correctly, respectively). However, combining\nboth CodeNet and the single-line dataset seems lead to the highest performance; tracing produces\nthe correct ﬁnal output for 26.6% of the tasks, and nearly a quarter of the tasks (24.6%) are traced\nperfectly for all three examples. These results seem promising: the neural network can often exactly\ntrace programs. In particular, greedily decoding from the best model produces the exact correct trace\nfor almost 42% of all traces.\n6 R ELATED WORK\nThe tasks in this paper can be viewed as exploring one criticism of large language models, namely,\nto what extent do they simply rely on surface-level statistical correlations on text, without learn-\ning semantics or world knowledge (Bender & Koller, 2020)? In response, Li et al. (2021) provide\nevidence that pre-trained language models do indeed construct approximate representations of the\nsemantics of the situations they describe in text. In the context of programs, Austin et al. (2021)\napproach this question by exploring the learning to execute task on MBPP, which we consider in\nSection 5.2. The idea behind this task was to explore whether neural models for synthesis that gen-\nerate code could also execute it. While that work ﬁnds existing models perform poorly at predicting\nexecution, we show that adding a scratchpad allows these models to perform better.\nWork in learning to execute has considered whether off-the-shelf recurrent neural networks\n(Zaremba & Sutskever, 2014) or more specialized architectures (Dehghani et al., 2018; Bieber et al.,\n2020; Wang et al., 2020) have an inductive bias that is sufﬁciently well suited for executing and\nreasoning about arbitrary code. The related problem of neural algorithm induction has attracted\nconsiderable interest (Graves et al., 2014; Kurach et al., 2016; Kaiser & Sutskever, 2016; Graves\net al., 2016; Reed & de Freitas, 2016; Veli ˇckovi´c et al., 2020a;b). This work proposes new neural\narchitectures, inspired by theoretical models of computation, whose inductive bias allows them to\nmore easily learn algorithm induction tasks. Several methods for algorithm induction speciﬁcally\nadd adaptive computation time to sequence models (Graves, 2016; Dehghani et al., 2018; Banino\net al., 2021). In particular, universal transformers include adaptive computation time, and are eval-\nuated both on algorithm induction and on learning to execute tasks (Dehghani et al., 2018). In\ncontrast, a scratchpad is a simple way both to provide a transformer model with adaptive compu-\ntation time, and also to provide supervision about how to use that additional computation, without\nrequiring modiﬁcation to the underlying architecture.\nAlgorithm induction has also been connected to pre-trained models. Lu et al. (2021) show that\nTransformers can be used to some extent as universal computation engines, by pre-training on natu-\nral language, and ﬁne-tuning a small fraction of the weights on non-language tasks, including simple\nalgorithm induction tasks. Finally, supervised approaches to semantic parsing (Zelle & Mooney,\n1996; Zettlemoyer & Collins, 2005; Kwiatkowksi et al., 2010; Wong & Mooney, 2006) predict the\ntext of a database query, which can then be executed to answer a natural language question.\n7 L IMITATIONS AND FUTURE WORK\nContext window size In this work, we limit our experiments to problems where the scratchpad\ntext ﬁts within the model generation window (512 tokens). However, many problems require very\nlong scratchpad generations. Therefore, fully realizing the potential of the scratchpad technique\nmay require further improvements in transformer generation window size. This is an active area\nof research in NLP (Tay et al., 2020), and improvements would be beneﬁcial for the scratchpad\ntechnique.\nLearning to use the scratchpad without supervisionA clear next step is to try to learn to use\nthe scratchpad without direct supervision. A simple method would be to use reinforcement learn-\ning (RL) techniques: models would be rewarded for correctly answering questions, with reward\ninversely proportional to the number of scratchpad tokens used. We would hope that learning to\n9\nuse the scratchpad would be a transferable skill; for example, a model could potentially use the\nalgorithm it learned to perform long addition to succeed at polynomial evaluation.\n8 C ONCLUSION\nIn this work we showed—through experiments on long addition, polynomial evaluation, and Python\ncode execution—that allowing models to read from and write to a simple scratchpad can improve\ntheir performance on algorithmic tasks. Such models may be a ﬁrst step toward combining the\nknowledge-compression capabilities of large language models with reasoning capabilities, in order\nto build models that understand code as well as write it. This could be useful for a variety of ap-\nplications that require both working with natural language and reasoning about program semantics,\nsuch as program synthesis, neural-guided program analysis, and interactive programming assistants.\nThe scratchpad technique presented here might not take us all the way toward that goal, but we hope\nit is an important step.\nACKNOWLEDGMENTS\nWe thank Fraser Greenlee for constructing the single-line programs dataset, and Kevin Murphy for\nbringing this dataset to our attention.\nREFERENCES\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs\nwith graphs. In International Conference on Learning Representations (ICLR), February 2018.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\nAndrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. In 8th ICML\nWorkshop on Automated Machine Learning (AutoML), 2021.\nEmily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 5185–5198, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/2020.\nacl-main.463.\nDavid Bieber, Charles Sutton, Hugo Larochelle, and Daniel Tarlow. Learning to execute programs\nwith instruction pointer attention graph neural networks. In H. Larochelle, M. Ranzato, R. Had-\nsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , vol-\nume 33, pp. 8626–8637. Curran Associates, Inc., 2020. URL https://proceedings.neurips.\ncc/paper/2020/file/62326dc7c4f7b849d6f013ba46489d6c-Paper.pdf.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR,\nabs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Win-\nter, Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, Will Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shan-\ntanu Jain, Andrew Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,\n10\nDario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large lan-\nguage models trained on code, July 2021. URL http://arxiv.org/abs/2107.03374.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. July 2018.\nJacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and\nPushmeet Kohli. Robustﬁll: Neural program learning under noisy I/O. CoRR, abs/1703.07469,\n2017. URL http://arxiv.org/abs/1703.07469.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), 2019.\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint\narXiv:1603.08983, 2016.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401,\n2014.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,\nAdri`a Puigdom `enech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,\nHelen King, Christopher Summerﬁeld, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.\nHybrid computing using a neural network with dynamic external memory. Nature, 538(7626):\n471–476, 2016.\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. In4th International Conference on\nLearning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track\nProceedings, 2016.\nKarol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In\nInternational Conference on Learning Representations, (ICLR), 2016.\nTom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. Inducing probabilis-\ntic CCG grammars from logical form with higher-order uniﬁcation. In Proceedings of the 2010\nConference on Empirical Methods in Natural Language Processing , pp. 1223–1233, October\n2010.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural\nlanguage models. ArXiv, abs/2106.00737, 2021.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. March 2021.\nRuchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov,\nJulian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh\nPujar, and Ulrich Finkler. Project CodeNet: A Large-Scale AI for code dataset for learning a\ndiversity of coding tasks. May 2021. URL http://arxiv.org/abs/2105.12655.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.\nScott Reed and Nando de Freitas. Neural programmer-interpreters. In International Conference on\nLearning Representations (ICLR), 2016. URL http://arxiv.org/pdf/1511.06279v3.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-\nsoning abilities of neural models. CoRR, abs/1904.01557, 2019. URL http://arxiv.org/abs/\n1904.01557.\n11\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. CoRR, abs/2011.04006, 2020. URL https://arxiv.org/abs/2011.04006.\nPetar Velickovic and Charles Blundell. Neural algorithmic reasoning.CoRR, abs/2105.02761, 2021.\nURL https://arxiv.org/abs/2105.02761.\nPetar Veliˇckovi´c, Lars Buesing, Matthew C. Overlan, Razvan Pascanu, Oriol Vinyals, and Charles\nBlundell. Pointer graph networks, 2020a.\nPetar Veliˇckovi´c, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execu-\ntion of graph algorithms, 2020b.\nYu Wang, Fengjuan Gao, Linzhang Wang, and Ke Wang. Learning semantic program embeddings\nwith graph interval neural network, 2020.\nYuk Wah Wong and Raymond J Mooney. Learning for semantic parsing with statistical machine\ntranslation. In Proceedings of the main conference on Human Language Technology Conference\nof the North American Chapter of the Association of Computational Linguistics - , Morristown,\nNJ, USA, 2006. Association for Computational Linguistics.\nWojciech Zaremba and Ilya Sutskever. Learning to execute. ArXiv, abs/1410.4615, 2014.\nJ Zelle and R Mooney. Learning to parse database queries using inductive logic programming. In\nNational Conference on Artiﬁcial Intelligence (AAAI), 1996.\nLuke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured\nclassiﬁcation with probabilistic categorial grammars. In Uncertainty in Artiﬁcial Intelligence ,\nJuly 2005.\n12\nA E FFECTS OF SCRATCHPAD EXECUTION TRAINING ON SYNTHESIS\nPERFORMANCE\nTo measure the extent to which ﬁne-tuning on the tracing task described above affects the model’s\nability to perform program synthesis, we ran a few-shot synthesis experiment using the “MBPP-\naug + CodeNet + single line” model. Speciﬁcally, we performed few-shot synthesis on the MBPP\ndataset, as described in Austin et al. (2021). For each MBPP synthesis task, 80 candidate programs\nare sampled from the model ( T = 0.5), and the task is considered solved if any of the candidate\nprograms satisfy all three test cases. For more details, see Austin et al. (2021). The “MBPP-aug +\nCodeNet + single line” model achieved an overall synthesis accuracy of 54%, compared to the 62%\naccuracy of the original few-shot model in Austin et al. (2021). This indicates that the scratchpad\nexecution training does not completely disrup the model’s ability to perform other few-shot tasks.\nB L ONG ADDITION ABLATION STUDY\nIn our long addition experiments in Section 3, we compared a model that was trained to perform\n“direct execution” (the baseline) vs a model trained to use a scratchpad. Since the model trained\nto use the scratchpad gets an additional signal from all the intermediate steps shown, we also study\nwhat happens if the scratchpad model is subsequently trained to perform direct execution (i.e., di-\nrectly output the target without using the scratchpad). The result is shown in Figure 7 where we\nfollowed the same training procedure as for the original direct execution baseline and scratchpad\nmodels. We see no signiﬁcant beneﬁts from doing any intermediate training using a scratchpad.\nThis indicates that the extra training-time information seen by the scratchpad model does not seem\nsolely responsible for the scratchpad model’s improved performance.\nC E XAMPLE FEW -SHOT PROMPT FOR SYNTHETIC PYTHON EXPERIMENTS\nBelow is an example of a prompt for few-shot synthetic Python synthesis problems:\nConsider the following Python function:\ndef f(v0):\nv0 += 0\nv4 = 2\nwhile v4 > 0:\nv4 -= 1\nv0 *= 2\nreturn v0\noutput = f(6)\nWhat is the execution trace?\n[BEGIN]\nstate: {}\nline: def f(v0):\nstate: {\"f\": \"<callable_object f>\"}\nline: output = f(6)\nstate: {\"v0\": 6}\nline: v0 += 0\nstate: {\"v0\": 6}\nline: v4 = 2\nstate: {\"v0\": 6, \"v4\": 2}\nline: while v4 > 0:\nstate: {\"v0\": 6, \"v4\": 2}\nline: v4 -= 1\nstate: {\"v0\": 6, \"v4\": 1}\nline: v0 *= 2\nstate: {\"v0\": 12, \"v4\": 1}\nline: while v4 > 0:\nstate: {\"v0\": 12, \"v4\": 1}\nline: v4 -= 1\nstate: {\"v0\": 12, \"v4\": 0}\nline: v0 *= 2\nstate: {\"v0\": 24, \"v4\": 0}\nline: while v4 > 0:\nstate: {\"v0\": 24, \"v4\": 0}\n13\nline: return v0\nstate: {\"f\": \"<callable_object f>\", \"output\": 24}\n[DONE]\nConsider the following Python function:\ndef f(v0):\nv0 -= 0\nv0 += 2\nv0 -= 0\nreturn v0\noutput = f(4)\nWhat is the execution trace?\n[BEGIN]\nstate: {}\nline: def f(v0):\nstate: {\"f\": \"<callable_object f>\"}\nline: output = f(4)\nstate: {\"v0\": 4}\nline: v0 -= 0\nstate: {\"v0\": 4}\nline: v0 += 2\nstate: {\"v0\": 6}\nline: v0 -= 0\nstate: {\"v0\": 6}\nline: return v0\nstate: {\"f\": \"<callable_object f>\", \"output\": 6}\n[DONE]\nConsider the following Python function:\ndef f(v0):\nv0 -= 0\nv8 = 2\nwhile v8 > 0:\nv8 -= 1\nv0 *= 1\nreturn v0\noutput = f(4)\nWhat is the execution trace?\n[BEGIN]\nstate: {}\nline: def f(v0):\nstate: {\"f\": \"<callable_object f>\"}\nline: output = f(4)\nstate: {\"v0\": 4}\nline: v0 -= 0\nstate: {\"v0\": 4}\nline: v8 = 2\nstate: {\"v0\": 4, \"v8\": 2}\nline: while v8 > 0:\nstate: {\"v0\": 4, \"v8\": 2}\nline: v8 -= 1\nstate: {\"v0\": 4, \"v8\": 1}\nline: v0 *= 1\nstate: {\"v0\": 4, \"v8\": 1}\nline: while v8 > 0:\nstate: {\"v0\": 4, \"v8\": 1}\nline: v8 -= 1\nstate: {\"v0\": 4, \"v8\": 0}\nline: v0 *= 1\nstate: {\"v0\": 4, \"v8\": 0}\nline: while v8 > 0:\nstate: {\"v0\": 4, \"v8\": 0}\nline: return v0\nstate: {\"f\": \"<callable_object f>\", \"output\": 4}\n[DONE]\nConsider the following Python function:\n14\ndef f(v0):\nv0 += 2\nv0 *= 2\nv0 *= 1\nreturn v0\noutput = f(8)\nWhat is the execution trace?\n[BEGIN]\nD I NCORRECT OUTPUT FOR SYNTHETIC PYTHON PROBLEMS\nBelow is an example of the failure of the few-shot synthetic experiments discussed in Section 5.1.\nGiven a problem such as:\nConsider the following Python function:\ndef f(v0):\nv0 += 2\nv0 *= 2\nv0 *= 1\nreturn v0\noutput = f(8)\nWhat is the execution trace?\nThe model produces the following trace:\nstate: {\"v0\": 8}\nline: def f(v0):\nstate: {\"v0\": 8, \"f\": \"<function f at 0x00BFC1C0>\"}\nline: v0 += 2\nstate: {\"v0\": 10}\nline: v0 *= 2\nstate: {\"v0\": 20}\nline: v0 *= 1\nstate: {\"v0\": 20}\nline: return v0\nstate: {\"f\": \"<function f at 0x00BFC1C0>\", \"v0\": 20}\nNote that this trace is exactly correct, except for the last line: where the model predicted \"v0\": 20,\nthe correct output is \"output\": 20. Because this type of error consistently occurs in the few-shot\nsynthetic Python experiments, we modiﬁed the evaluation script slightly to consider this output to\nbe correct.\n15\n107 108 109\nParameter Count\n0\n20\n40\n60\n80\n100Accuracy (%)\nAddition: In-Distribution Accuracy\nbaseline\nscratchpad\nscratchpad model \nfine-tuned for \ndirect execution\nFigure 7: Long addition ablation results. Here, we comparing the baseline and scratchpad results to\na model that is ﬁrst ﬁne-tuned on the scratchpad and then subsequently ﬁne-tuned to perform direct\nexecution (the baseline). The intermediate scratchpad training seem to not have any signiﬁcant\neffect on the overall performance, indicating that the extra training-time information seen by the\nscratchpad model does not seem solely responsible for the scratchpad model’s performance.\n16"
}