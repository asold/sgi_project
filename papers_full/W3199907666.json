{
  "title": "Encoding Retina Image to Words using Ensemble of Vision Transformers for Diabetic Retinopathy Grading",
  "url": "https://openalex.org/W3199907666",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2320901753",
      "name": "Nouar AlDahoul",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A2138567905",
      "name": "Hezerul Abdul Karim",
      "affiliations": [
        "Multimedia University"
      ]
    },
    {
      "id": "https://openalex.org/A3114360974",
      "name": "Myles Joshua Toledo Tan",
      "affiliations": [
        "University of St. La Salle"
      ]
    },
    {
      "id": "https://openalex.org/A3199670666",
      "name": "Mhd Adel Momo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3117292668",
      "name": "Jamie Ledesma Fermin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4240986128",
    "https://openalex.org/W2409181047",
    "https://openalex.org/W2338167986",
    "https://openalex.org/W2160041268",
    "https://openalex.org/W2920497957",
    "https://openalex.org/W2123634291",
    "https://openalex.org/W2809489644",
    "https://openalex.org/W2537959867",
    "https://openalex.org/W2051424632",
    "https://openalex.org/W2005409914",
    "https://openalex.org/W4211205882",
    "https://openalex.org/W2470155373",
    "https://openalex.org/W2159389777",
    "https://openalex.org/W2138480916",
    "https://openalex.org/W2022126925",
    "https://openalex.org/W2051013472",
    "https://openalex.org/W2596818854",
    "https://openalex.org/W2009904828",
    "https://openalex.org/W2097564701",
    "https://openalex.org/W163188459",
    "https://openalex.org/W2502390809",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2529609428",
    "https://openalex.org/W2344912502",
    "https://openalex.org/W2890060948",
    "https://openalex.org/W3115048207",
    "https://openalex.org/W3032671045",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W4239157251",
    "https://openalex.org/W6794559225",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3010373912",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2025367782",
    "https://openalex.org/W4394794235"
  ],
  "abstract": "<ns3:p>Diabetes is one of the top ten causes of death among adults worldwide. People with diabetes are prone to suffer from eye disease such as diabetic retinopathy (DR). DR damages the blood vessels in the retina and can result in vision loss. DR grading is an essential step to take to help in the early diagnosis and in the effective treatment thereof, and also to slow down its progression to vision impairment. Existing automatic solutions are mostly based on traditional image processing and machine learning techniques. Hence, there is a big gap when it comes to more generic detection and grading of DR. Various deep learning models such as convolutional neural networks (CNNs) have been previously utilized for this purpose. To enhance DR grading, this paper proposes a novel solution based on an ensemble of state-of-the-art deep learning models called vision transformers. A challenging public DR dataset proposed in a 2015 Kaggle challenge was used for training and evaluation of the proposed method. This dataset includes highly imbalanced data with five levels of severity: No DR, Mild, Moderate, Severe, and Proliferative DR. The experiments conducted showed that the proposed solution outperforms existing methods in terms of precision (47%), recall (45%), F1 score (42%), and Quadratic Weighted Kappa (QWK) (60.2%). Finally, it was able to run with low inference time (1.12 seconds). For this reason, the proposed solution can help examiners grade DR more accurately than manual means.</ns3:p>",
  "full_text": "RESEARCH ARTICLE\nEncoding Retina Image to Words using Ensemble of Vision \nTransformers for Diabetic Retinopathy Grading\n[version 1; peer review: 1 not approved]\nNouar AlDahoul\n 1,2, Hezerul Abdul Karim\n 1, Myles Joshua Toledo Tan2,3, \nMhd Adel Momo\n 2, Jamie Ledesma Fermin2,3\n1Engineering, Multimedia University, Cyberjaya, Selangor, 63100, Malaysia \n2Artificial Intelligence, YO-VIVO corporation, Bacolod City, 6100, Philippines \n3Engineering and Technology, University of St. La Salle, Bacolod City, 6100, Philippines \nFirst published: 21 Sep 2021, 10:948  \nhttps://doi.org/10.12688/f1000research.73082.1\nLatest published: 21 Sep 2021, 10:948  \nhttps://doi.org/10.12688/f1000research.73082.1\nv1\n \nAbstract \nDiabetes is one of the top ten causes of death among adults \nworldwide. People with diabetes are prone to suffer from eye disease \nsuch as diabetic retinopathy (DR). DR damages the blood vessels in \nthe retina and can result in vision loss. DR grading is an essential step \nto take to help in the early diagnosis and in the effective treatment \nthereof, and also to slow down its progression to vision impairment. \nExisting automatic solutions are mostly based on traditional image \nprocessing and machine learning techniques. Hence, there is a big \ngap when it comes to more generic detection and grading of DR. \nVarious deep learning models such as convolutional neural networks \n(CNNs) have been previously utilized for this purpose. To enhance DR \ngrading, this paper proposes a novel solution based on an ensemble \nof state-of-the-art deep learning models called vision transformers. A \nchallenging public DR dataset proposed in a 2015 Kaggle challenge \nwas used for training and evaluation of the proposed method. This \ndataset includes highly imbalanced data with five levels of severity: No \nDR, Mild, Moderate, Severe, and Proliferative DR. The experiments \nconducted showed that the proposed solution outperforms existing \nmethods in terms of precision (47%), recall (45%), F1 score (42%), and \nQuadratic Weighted Kappa (QWK) (60.2%). Finally, it was able to run \nwith low inference time (1.12 seconds). For this reason, the proposed \nsolution can help examiners grade DR more accurately than manual \nmeans.\nKeywords \nDiabetic Retinopathy Grading, Ensemble Learning, Imbalanced Data, \nVision Transformer, Self-attention Mechanism\nOpen Peer Review\nApproval Status  \n1\nversion 1\n21 Sep 2021\n view\nShruti Jain\n , Jaypee University of \nInformation Technology, Solan, India\n1. \nAny reports and responses or comments on the \narticle can be found at the end of the article.\n \n Page 1 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nCorresponding author: Nouar AlDahoul (nouar.aldahoul@live.iium.edu.my)\nAuthor roles: AlDahoul N: Conceptualization, Data Curation, Formal Analysis, Methodology, Software, Validation, Visualization, Writing \n– Original Draft Preparation, Writing – Review & Editing; Abdul Karim H: Formal Analysis, Funding Acquisition, Methodology, Project \nAdministration, Supervision, Writing – Review & Editing; Joshua Toledo Tan M: Validation, Writing – Review & Editing; Momo MA: \nConceptualization, Formal Analysis, Methodology, Software, Writing – Original Draft Preparation, Writing – Review & Editing; Ledesma \nFermin J: Investigation, Writing – Original Draft Preparation, Writing – Review & Editing\nCompeting interests: No competing interests were disclosed.\nGrant information: This research project was funded by Multimedia University, Malaysia. \nThe funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\nCopyright: © 2021 AlDahoul N et al. This is an open access article distributed under the terms of the Creative Commons Attribution \nLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nHow to cite this article: AlDahoul N, Abdul Karim H, Joshua Toledo Tan M et al. Encoding Retina Image to Words using Ensemble of \nVision Transformers for Diabetic Retinopathy Grading [version 1; peer review: 1 not approved] F1000Research 2021, 10:948 \nhttps://doi.org/10.12688/f1000research.73082.1\nFirst published: 21 Sep 2021, 10:948 https://doi.org/10.12688/f1000research.73082.1  \n \nThis article is included in the Research Synergy \nFoundation gateway.\n  Page 2 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nIntroduction\nBackground\nDiabetes mellitus (DM) is a group of metabolic disorders that are characterized by high levels of blood glucose and are\ncaused by either the deficient secretion of the hormone insulin, its inaction, or both. Chronically high levels of glucose in\nthe blood that come with DM may bring about long-term damage to several different organs, such as the eyes.\n1,2 DM is a\npandemic of great concern3-6 as approximately 463 million adults were living with DM in 2019. This number is expected\nto rise to about 700 million by the year 2045.4\nHigh levels of glucose in the blood damage the capillaries of the retina (diabetic retinopathy [DR]) or the optic nerve\n(glaucoma), cloud the lens (cataract), or cause fluid to build up in the macula (diabetic macular edema), thereby causing\ndiabetic eye disease.\n6-11 DR is the leading cause of blindness among adults in the working age12 and has brought about\nseveral personal and socioeconomic consequences,13 and a greater risk of developing other complications of DM and of\ndying.14 According to a meta-analysis that reviewed 35 studies worldwide from 1980 to 2008, 34.6% of all patients with\nDM globally have DR of some form, while 10.2% of all patients with DM have vision-threatening DR.15\nA study found that screening for DR and the early treatment thereof could lower the risk of vision loss by about 56%,16\nproving that blindness due to DR is highly preventable. Moreover, theWorld Health Organization (WHO) Universal\nEye Health: A Global Action Plan 2014–2019 advocated for efforts to reduce the prevalence of preventable visual\nimpairments and blindness including those that arise as complications of DM.\nMany tests can be used for the screening of DR. While sensitivity and specificity are certainly important, the data about\nperformance of tests for DR are different. Researchers employ different outcomes to measure sensitivity,e.g., the ability\nof a screening test to detect any form of retinopathy, and the ability to detect vision-threatening DR. Additionally, some\ntests may detect diabetic macular edema better than the different grades of DR according toWorld Health Organization.\nDiabetic retinopathy screening: a short guide. Copenhagen: WHO Regional Office for Europe. The examiner’s skill\nis also a source of variation in the test results. A systematic review found that the sensitivity of direct ophthalmoscopy\n(DO) varies greatly when performed by general practitioners (25%–66%) and by ophthalmologists (43%–79%).17\nDR grading is an essential step in the early diagnosis and effective treatment of the disease. Manual grading is based\non high-resolution retinal images examined by a clinician. However, the process is time-consuming and is prone to\nmisdiagnosis. This paper aims to address the matter by developing a fast and accurate automated DR grading system.\nHere, a novel solution that is based on an ensemble of vision transformers was proposed to enhance grading. Moreover,\na public DR dataset proposed in a 2015Kaggle challengewas used for the training and evaluation.\nRelated work\nTraditional machine learning (ML) methods have been used to detect DR. Typically, these ML methods require hand-\ntuned features extracted from small datasets to aid in classification. These traditional methods may involve ensemble\nlearning\n18; the calculation of the mean, standard deviation, and edge strength19; and the segmentation of hard macular\nexudates.20,21 However, these methods require tedious and time-consuming feature engineering steps that are sensitive to\nthe chosen set of features. Work that employs traditional ML methods to detect DR usually yield favorable results using\none dataset but fail to obtain a similar success when another dataset is used.18,19 This is a common limitation of hand-\ncrafted features.\nDeep neural networks, such as CNNs, with much larger datasets have also been used for classification tasks in the\ndiagnosis and grading of DR. These methods involve CNNs developed from scratch to grade the disease using images of\nthe retinal fundus\n22; transfer learning based on Inception-v3 neural network to perform multiple binary classification\n(moderate versus worse DR, and severe or worse DR)23; segmentation prior to detection by pixel classification24 or patch\nclassification.25 A deep learning (DL)-based framework that uses advanced image processing and a boosting algorithm\nfor grading of DR was also proposed by.26 This is one of only a handful of works that have effectively employed transfer\nlearning to train large neural networks for this purpose. Recently, ResNet, a deep CNN, was proposed to address the\nproblem brought about by imbalanced datasets in DR grading.27 Additionally, a bagging ensemble of three CNNs: a\nshallow CNN, VGG16, and InceptionV3, was used to classify images as DR, glaucoma, myopia and normal.28\nPreviously, a transformer was also proposed by Vaswaniet al.29 for natural language processing tasks especially for\nmachine translation. Inspired by the successes of the transformers in NLP, transformers were transferred to computer\nvision taskse.g. image classification.\nPage 3 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nMethods\nIn this section, the DR detection dataset is explored. Additionally, the vision transformer, a DL model that was used on\nthese data, is discussed in detail.\nDataset overview\nThe DR detection dataset is highly imbalanced and consists of high-resolution images with five levels of severity\nincluding No_DR, Mild, Moderate, Severe, and Proliferative_DR. It has significantly more samples for the negative\n(No_DR) category than for the four positive categories.Table 1shows the class distribution of the training and testing\nsets. Figure 1, on the other hand, shows a few samples from each class. The images come with different conditions and\nwere labeled with subject IDs. The left and right fields were provided for every subject. The images were captured by\ndifferent cameras, thus affecting the visual appearance of the images of left and right eyes.\nThe samples of the training set were rescaled between 0,1½/C138 , cropped to remove their black borders, and augmented by\nrandomly flipping the samples horizontally and vertically, and by randomly rotating the samples by 360°. The samples of\nthe test set were only cropped and rescaled.Figure 2shows a few augmented samples from the training set.\nVision transformer\nA vision transformer is a state-of-the-art DL model that is used for image classification and was inspired by Dosovitskiy\net al.\n30 Figure 3shows the architecture of the vision transformer. In this paper, a retinal image that has a sequence of\npatches encoded as a set of words was applied to the transformer encoder as shown inFigure 3. The original image’s\npatches N ¼ H /C2 WðÞ =P2 were extracted with a fixed patch sizeP,PðÞ where P ¼ 16, W is the image width,H is the\nimage height, andN is the number of patches. The extracted patches were flattened and each patchxp belonged toℝP2: C,\nwhere C is the number of channels.\nAs a result, the 2D image was converted into a sequence of patchesx ∈ ℝN/C2 P2:CðÞ . Each patch in the sequencex was\nmapped to a latent vector with hidden sizeD ¼ 768. A learnable class embeddingz0\n0 ¼ xclass was prepended for the\nembedded patches, whose state at the output of the transformer’s encoder z0\nL\n/C0/C1\nserves as the representationy of the image.\nAfter that, a classifier was attached to the image representationy. Additionally, a position embeddingEpos was added to\nthe patch embeddings to capture the order of patches that were fed into the transformer encoder.Figure 4illustrates the\narchitecture of transformer’s encoder withL blocks, each block containing alternating layers of multi-head self-attention\n(MSA)29 and multi-layer perceptron (MLP) blocks. The layer normalization (LN)31 was applied before every block,\nwhile residual connections were applied after every block.30\nEnsemble learning of vision transformers\nEnsemble learning is a ML ensemble meta-algorithm. Bagging (Bootstrap Aggregating) is a type of ensemble learning\nthat uses“majority voting” to combine the output of different base models to produce one optimal predictive model and\nimprove the stability and accuracy.\n32\nThe advantage of ensemble bagging several transformers is that aggregation of several transformers, each trained on a\nsubset of the dataset, outperforms a single transformer trained over the entire set. In other words, it leads to less overfit by\nremoving variance in high-variance low-bias datasets. To increase the speed of training, the training can be done in\nparallel by running each transformer on its own data prior to result aggregation, as shown inFigure 5.\nExperimental setup and protocol\nThe images available in this dataset were resized toH ¼ 256,W ¼ 256, the latent vector hidden size was set toD ¼ 768,\nthe number of layers of the transformer toL ¼ 12, theMLPsize to 3072, theMSAheads to 12, and the default value of the\npatch size toP ¼ 16. Thus, the sequence’s numberN was 256.\nTable 1.Training and Testing Class Distribution in dataset ofEyePACS, Diabetic Retinopathy Detection.\nClass Training Testing\nNo_DR 25810 39533\nMild 2443 3762\nModerate 5292 7861\nSevere 873 1214\nProliferative_DR 708 1206\nPage 4 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nFigure 1. A few samples of each class in dataset ofEyePACS, Diabetic Retinopathy Detection: “No_DR”\n(red borders),“Mild” (blue), “Moderate” (green), “Severe” (yellow), “Proliferative_DR” (violet). The images have\nvarious sizes but were resized uniformly.\nPage 5 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nFigure 2.A few samples cropped and augmented randomly.\nFigure 3.The vision transformer architecture.30\nPage 6 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nIn the experiments conducted, 20%from each class in the training set were selected for validation. All transformers were\nfine-tuned using the weights of the transformer pre-trained on ImageNet-21K.33\nFor optimization, the ADAM algorithm34 was utilized with a batch size of 8. Furthermore, the mean squared error loss\nfunction was used. The training process for each transformer consists of two stages:\n1) All layers in the transformer backbone were frozen and the regression head that was initialized randomly was\nunfrozen. Then, the regression head was trained for five epochs.\n2) The entire model (transformer backbone + regression head) which was trained for 40 epochs was unfrozen.\nFigure 4.Encoder Architecture of the Transformer.30\nFigure 5.Ensemble learning of vision transformers.\nPage 7 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nData augmentation, early stopping, dropout, and learning rate schedules were used to prevent overfitting and loss\ndivergence. Figure 6shows the attention map of a few samples extracted from the transformer.\nThe classification heads of all transformers were removed and replaced by a regression head with one node instead of\nlogits. The regression output of a transformer was interpreted as shown inTable 2to be converted into a category.\nAn ensemble of ten transformers with similar architectures and hyperparameters was used. The samples were divided\nrandomly into ten sets and each transformer was trained on each one. After interpreting the regression output from\neach transformer, the predicted classes from ten transformers were aggregated with“majority voting” to predict the final\nclass.\nTraining, validation, and testing were carried out using the TensorFlow framework on an NVIDIA Tesla T4 GPU.\nTable 2.Pseudocode for the transformer regression output interpretation.\nAlgorithm: Regression output interpretation\nfunction classify (xsample){\ninputs: xsample output float number from the transformer\noutputs: ysample which represents the class of the presented sample\nIf xsample < 0.8then\nreturn ysample = No DR\nElse Ifxsample < 1.5then\nreturn ysample = Mild\nElse Ifxsample < 2.5then\nreturn ysample = Moderate\nElse Ifxsample < 3.5then\nreturn ysample = Severe\nreturn ysample = Proliferative DR\n}\nFigure 6.The Attention Map of samples A) No DR, B) Mild, C) Moderate, D) Severe, E) Proliferative DR.\nPage 8 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nResults and discussion\nPerformance metrics\nIn this section, the results of the proposed ensemble of transformers are discussed. The performance metrics, such as\nprecision, recall, and F1 score were calculated. Additionally, thequadratic weighted kappa(QWK) metric was utilized in\nthis dataset because these data needed specialists to label the images manually since the small differences among the\nclasses can only be recognized by specialist physicians. QWK which lies in the range/C0 1,þ1½/C138 measures the agreement\nbetween two ratings and is calculated between the scores assigned by human raters (doctors) and predicted scores\n(models) as shown inTable 3. The dataset has five ratings: 0, 1, 2, 3, 4.\nQWK was calculated as follows:\n1) The confusion matrix O between predicted and actual ratings was calculated.\n2) A histogram vector was computed for each rating in the predictions and in the actual.\n3) The EN /C2 NðÞ matrix which represents the outer product between the two histogram vectors was calculated.\n4) The W (N /C2 N) weight matrix was constructed representing the difference between the ratings as shown in\nTable 4.\n35\nWij ¼ i /C0 jðÞ 2\nN /C0 1ðÞ 2 (1)\nWhere 1≤ i ≤ 5, 1≤ j ≤ 5\n5) QWK was defined as follows35:\nQWK ¼ 1 /C0\nPN\ni\nPN\nj Wij /C2 Oij\nPN\ni\nPN\nj Wij /C2 Eij\n(2)\nwhere N is the number of classes.\nTable 4.The Weight Matrix W represents the difference between the classes.\nNo_DR Mild Moderate Severe Proliferative_DR\nNo_DR 0 0.0625 0.25 0.5625 1\nMild 0.0625 0 0.0625 0.25 0.5625\nModerate 0.25 0.0625 0 0.0625 0.25\nSevere 0.5625 0.25 0.0625 0 0.0625\nProliferative_DR 1 0.5625 0.25 0.0625 0\nTable 3.OWK interpretation.\nKappa Agreement\n<0 No\n0.01 – 0.20 Slight\n0.21 – 0.40 Fair\n0.41 – 0.60 Moderate\n0.61 – 0.80 Substantial\n0.81 – 0.99 Almost perfect\nPage 9 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nExperimental results\nTable 5 shows the performance metrics of ten transformers with each one trained on a subset of data. It is obvious\nthat there is a big difference among the performances of these individual transformers. Transformer_1 was able to yield\na Kappa of 55.1%. On the other hand, transformer_10 yielded a Kappa of 30.9%. Ensembles of various numbers of\ntransformers including all ten transformers, four transformers (1,3,8,9), and other configurations were also evaluated. The\nbest model was an ensemble of two transformers (1,3) which yielded a Kappa of 60.2%.\nThis Kappa is at the boundary between moderate and substantial agreement. The previous results confirm that the\nperformance of the ensemble of transformers (1,3) trained with fewer training images outperformed the ensemble of\nten transformers trained with five times the number of images.Table 6compares the performance of the ensemble of\ntransformers with the ensemble of ResNet50 CNNs. The ResNet50 CNN was transferred from ImageNet 1K. The top\nlayers were replaced by a support vector machine that was tuned with this dataset. The proposed ensemble of transformers\noutperformed the ensemble of ResNet50 CNNs significantly by >18% Kappa.\nThe confusion matrix of each configuration including ensembles of transformers with ten, four, and two transformers, and\nthe ensemble of two ResNet50\nCNNs were shown inFigure 7. The confusion matrix (c) which represents the best Kappa of 60.2% shows that the model\nwas able to recognize the categories of severe and proliferative DR from one side, and NO DR and mild DR from the\nsecond side.\nTable 5.Performance metrics for various ensemble models.\nModel Precision % Recall % F1 Score % QWK %\nTransformer_1 43 47 40 55.1\nTransformer_2 37 46 31 40.2\nTransformer_3 39 46 37 52.0\nTransformer_4 45 41 30 43.3\nTransformer_5 41 45 35 48.6\nTransformer_6 39 45 33 44.9\nTransformer_7 44 42 33 43.3\nTransformer_8 40 47 39 51.3\nTransformer_9 40 46 38 51.0\nTransformer_10 35 36 26 30.9\nEnsemble of ten transformers 45 47 38 53.0\nEnsemble of four transformers 44 47 41 57.5\nEnsemble of two transformers 47 45 42 60.2\nTable 6.Comparison between the ensemble of transformers and the ensemble of ResNet50 CNNs.\nModel Precision % Recall % F1 Score % QWK%\nEnsemble of two transformers 47 45 42 60.2\nEnsemble of ten ResNet50 32 44 32 36.97\nEnsemble of two ResNet50 35 40 35 41.52\nPage 10 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nConclusion\nThis study is a new attempt to demonstrate the capability of the ensemble bagging of vision transformers applied\non retinal image classification for the grading of DR into five levels of severity. The experiments conducted showed\nthat even when the dataset was challenging, the proposed method was able to yield promising performance measures in\nterms of precision (47%), recall (45%), F1 score (42%), and QWK (60.2%). Furthermore, the inference time was low at\n1.12 seconds. Hence, we intend to enhance the performance by utilizing a collection of various DR datasets. This can\nincrease the size and variety of training data to train the proposed model from scratch instead of starting from the weights\nof the ImageNet 21K-based model. By doing so, we can ultimately enhance performance.\nAuthor contributions\nConceptualization by N.A., M.A.M.; Data Curation by N.A.; Formal Analysis by N.A., H.A.K., M.A.M.; Funding\nAcquisition by H.A.K.; Investigation by N.A., J.L.F; Methodology by N.A., H.A.K., M.A.M.; Project Administration by\nH.A.K.; Software by N.A., M.A.M.; Validation by N.A., M.J.T.T.; Visualization by N.A.; Writing– Original Draft\nPreparation by N.A., M.A.M, J.L.F.; Writing– Review & Editing by N.A., H.A.K., M.J.T.T., M.A.M, J.L.F.\nEthics and consent\nAll procedures performed in studies involving human participants were in accordance with the ethical standards of the\ninstitutional and/or national research committee and with the 1964 Helsinki declaration and its later amendments or\ncomparable ethical standards.\nFigure 7.The Confusion Matrix for the ensemble bagging of A) ten transformers, B) four transformers, C) two\ntransformers, D) two ResNet50 CNN.\nPage 11 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nThe Retinal images are public third part dataset provided byEyePACS, a free platform for retinopathy screening.\nCompeting interests\nNone of the authors declare any competing interests.\nGrant information\nThis research project was funded by Multimedia University, Malaysia.\nData availability\nThe dataset used in this work is accessible to the public on the Kaggle website. It was created in 2015 for the Kaggle\nDiabetic Retinopathy Detection competition. This competition is sponsored by theCalifornia Healthcare Foundation.\nRetinal images were provided byEyePACS, a free platform for retinopathy screening.\nReferences\n1. American Diabetes Association: Diagnosis and classification of\ndiabetes mellitus.Diabetes Care.Jan. 2010;33(Suppl 1): S62–S69.\nPubMed Abstract|Publisher Full Text\n2. Fowler MJ: Microvascular and Macrovascular Complications of\nDiabetes. Clin Diab.Apr. 2008;26(2): 77.\nPubMed Abstract|Publisher Full Text|Free Full Text\n3. Zhou B, et al.: Worldwide trends in diabetes since 1980: a pooled\nanalysis of 751 population-based studies with 4.4 million\nparticipants. Lancet. Apr. 2016;387(10027): 1513–1530.\nPubMed Abstract|Publisher Full Text|Free Full Text\n4. International Diabetes Federation: IDF Diabetes Atlas.9th ed.\nBrussels, Belgium: International Diabetes Federation; 2019,\npp. 32–61.\n5. Narayan KMV: The Diabetes Pandemic: Looking for the Silver\nLining. Clinical Diabetes.Apr. 2005;23(2): 51–52.\nPublisher Full Text\n6. Cheloni R, Gandolfi SA, Signorelli C, et al.: Global prevalence of\ndiabetic retinopathy: protocol for a systematic review and\nmeta-analysis. BMJ Open.Mar. 2019;9(3): e022188.\nPubMed Abstract|Publisher Full Text|Free Full Text\n7. Wu L, Fernandez-Loaiza P, Sauma J,et al.: Classification of diabetic\nretinopathy and diabetic macular edema.World J Diabetes.Dec.\n2013; 4(6): 290–294.\nPubMed Abstract|Publisher Full Text|Free Full Text\n8. Wang W, Lo ACY: Diabetic Retinopathy: Pathophysiology and\nTreatments. Int J Mol Sci.Jun. 2018;19(6): 1816.\nPubMed Abstract|Publisher Full Text|Free Full Text\n9. Song BJ, Aiello LP, Pasquale LR: Presence and Risk Factors for\nGlaucoma in Patients with Diabetes.Curr Diab Rep.Dec. 2016;\n16(12): 124–124.\nPubMed Abstract|Publisher Full Text|Free Full Text\n10. Pollreisz A, Schmidt-Erfurth U: Diabetic cataract-pathogenesis,\nepidemiology and treatment.J Ophthalmol.2010; 2010:\n608751–608751.\nPubMed Abstract|Publisher Full Text|Free Full Text\n11. Das A, McGuire PG, Rangasamy S:Diabetic Macular Edema:\nPathophysiology and Novel Therapeutic Targets.Ophthalmology.\n2015; 122(7): 1375–1394.\nPubMed Abstract|Publisher Full Text\n12. Cheung N, Mitchell P, Wong TY:Diabetic retinopathy.Lancet. Jul.\n2010; 376(9735): 124–136.\nPublisher Full Text\n13. Rees G, et al.: Association Between Diabetes-Related Eye\nComplications and Symptoms of Anxiety and Depression.JAMA\nOphthalmol. Sep. 2016;134(9): 1007–1014.\nPubMed Abstract|Publisher Full Text\n14. Kramer CK, Rodrigues TC, Canani LH,et al.: Diabetic Retinopathy\nPredicts All-Cause Mortality and Cardiovascular Events\nin Both Type 1 and 2 Diabetes.Diabetes Care.May 2011;34(5):\n1238.\nPubMed Abstract|Publisher Full Text|Free Full Text\n15. Yau JWY, et al.: Global Prevalence and Major Risk Factors of\nDiabetic Retinopathy.Diabetes Care.Mar. 2012;35(3): 556–564.\nPubMed Abstract|Publisher Full Text|Free Full Text\n16. Rohan TE, Frost CD, Wald NJ:Prevention of blindness by screening\nfor diabetic retinopathy: a quantitative assessment.BMJ. Nov.\n1989; 299(6709): 1198–1201.\nPubMed Abstract|Publisher Full Text|Free Full Text\n17. Hutchinson A, et al.: Effectiveness of screening and monitoring\ntests for diabetic retinopathy--a systematic review.Diabet Med.\nJul. 2000;17(7): 495–506.\nPubMed Abstract|Publisher Full Text\n18. Bhatia K, Arora S, Tomar R:Diagnosis of diabetic retinopathy\nusing machine learning classification algorithm.2016 2nd Int\nConf Next Generation Computing Technologies (NGCT).2016,\npp. 347–351.\nPublisher Full Text\n19. Asha PR, Karpagavalli S: Diabetic Retinal Exudates Detection\nUsing Extreme Learning Machine.Emerging ICT for Bridging the\nFuture - Proceedings of the 49th Annual Convention of the Computer\nSociety of India CSI Volume 2, Cham.2015; pp. 573–578.\nPublisher Full Text\n20. Sopharak A, Uyyanonvara B: Automatic exudates detection from\ndiabetic retinopathy retinal image using fuzzy c-means and\nmorphological methods.Proceedings of the 3\nrd IASTED International\nConference of Advances in Computer Science and Technology.2007;\npp. 359–364.\nPubMed Abstract|Publisher Full Text|Free Full Text\n21. Osareh A, Mirmehdi M, Thomas B,et al.: Automatic recognition of\nexudative maculopathy using fuzzy C-means clustering and\nneural networks.Proc Medical Image Understanding Analysis\nConference. 2001, vol.3, pp. 49–52.\n22. Pratt H, Coenen F, Broadbent DM,et al.: Convolutional Neural\nNetworks for Diabetic Retinopathy.Procedia Computer Sci.Jan.\n2016; 90: 200–205.\nPublisher Full Text\n23. Gulshan V, et al.: Development and Validation of a Deep Learning\nAlgorithm for Detection of Diabetic Retinopathy in Retinal\nFundus Photographs.JAMA. Dec. 2016;316(22): 2402–2410.\nPubMed Abstract|Publisher Full Text\n24. Prenta šić P, Lonˇcarić S: Detection of exudates in fundus\nphotographs using deep neural networks and anatomical\nlandmark detection fusion.Comput Methods Programs Biomed.\nDec. 2016;137: 281–292.\nPubMed Abstract|Publisher Full Text\n25. van Grinsven MJJP, van Ginneken B, Hoyng CB,et al.: Fast\nConvolutional Neural Network Training Using Selective Data\nSampling: Application to Hemorrhage Detection in Color\nFundus Images.IEEE Transactions Medical Imaging.May 2016;35(5):\n1273–1284.\nPublisher Full Text\n26. Wang Y: A Deep Learning Based Pipeline for Image Grading of\nDiabetic Retinopathy.Master of Science: Virginia Polytechnic Institute\nand State University.2018.\n27. Sallam MS, Asnawi AL, Olanrewaju RF:Diabetic Retinopathy\nGrading Using ResNet Convolutional Neural Network.\n2020 IEEE Conference on Big Data and Analytics (ICBDA).2020,\npp. 73–78.\nPublisher Full Text\nPage 12 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\n28. Smaida M, Yaroshchak S: Bagging of convolutional\nneural networks for diagnostic of eye diseases.\nCEUR Workshop Proceedings.2020; 2604,\n715–729.\n29. Vaswani A, et al.: Attention Is All You Need.arXiv:1706.03762 [cs].\nDec. 2017. Accessed: May 30, 2021.\nReference Source\n30. Dosovitskiy A, Lucas B, Alexander K,et al.: An image is worth\n16x16 words: Transformers for image recognition at scale.ICLR.\n2021.\n31. Ba JL, Kiros JR, Hinton GE:Layer Normalization.arXiv:1607.06450\n[cs, stat].Jul. 2016. Accessed: Jun. 30, 2021.\nReference Source\n32. Fan W, Zhang K:Bagging In: Encyclopedia of Database Systems.LIU L,\nÖZSU MT, Eds. Boston, MA: Springer US; 2009, pp. 206–210.\nPublisher Full Text\n33. Ridnik T, Ben-Baruch E, Noy A,et al.: ImageNet-21K Pretraining for\nthe Masses.arXiv:2104.10972 [cs].Jun. 2021. Accessed: Jun. 30, 2021.\nReference Source\n34. Kingma DP, Ba J: Adam: A Method for Stochastic Optimization.\narXiv:1412.6980 [cs].Jan. 2017. Accessed: Jun. 30, 2021.\nReference Source\n35. Tymchenko B, Marchenko P, Spodarets D:Deep Learning\nApproach to Diabetic Retinopathy Detection.arXiv:2003.02261\n[cs, stat].Mar. 2020. Accessed: Jun. 30, 2021.\nReference Source\nPage 13 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nOpen Peer Review\nCurrent Peer Review Status:  \nVersion 1\nReviewer Report 25 November 2021\nhttps://doi.org/10.5256/f1000research.76706.r94946\n© 2021 Jain S. This is an open access peer review report distributed under the terms of the Creative Commons \nAttribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the \noriginal work is properly cited.\nShruti Jain \n   \nDepartment of Electronics and Communication Engineering, Jaypee University of Information \nTechnology, Solan, Himachal Pradesh, India \nAdd a section highlighting the main contributions of your methodology, with detailed \nreference to, and comparison with, existing work. \n \n1. \nI find it difficult to understand, from the abstract, the proposed methodology by which you \nseek to solve the problem of your paper. For example, the authors could clarify the \nfollowing: “To enhance DR grading, this paper proposes a novel solution based on an \nensemble of state-of-the-art deep learning models called vision transformers”. What are \nvision transformers? Are authors proposing this or what is new in it? https://viso.ai/deep-\nlearning/vision-transformer-vit/ \n \nLikewise the authors write \"A challenging public DR dataset proposed in a 2015 Kaggle \nchallenge was used for training and evaluation of the proposed method.\" Yet there are \nmany datasets for DR grading, so why only Kaggle? The author can validate their model \nwith other datasets. \n \n2. \nAdd some latest papers and cite them.  \n \n3. \nRead the whole manuscript for typos and grammatical mistakes. \n \n4. \nRecheck, not all references that could be given are given. In 'Dataset Overview', in the \nsentence “the quadratic weighted kappa (QWK) metric was utilized in this dataset because \nof these data...” the reference is missing. Likewise there are places where authors can give \nreferences. \n \n5. \nFinally, your conclusion needs to be more tailored to your findings. The authors write \n\"Hence, we intend to enhance the performance by utilizing a collection of various DR \ndatasets. This can increase the size and variety of training data to train the proposed model \n6. \n  Page 14 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nfrom scratch instead of starting from the weights of the ImageNet 21K-based model. By \ndoing so, we can ultimately enhance performance” \n \nYet, why don’t authors have tried increasing the datasets utilized already? Furthermore, the \nImageNet 21K-based model is mentioned significantly in the conclusion without having \nbeen elaborated upon in the main text of the article: why was it introduced, what is it, and \nwhat is it for and how does it improve performance? Likewise, there is no mention of \nensemble transformers or vision transformers in the conclusion.\n \nIs the work clearly and accurately presented and does it cite the current literature?\nNo\nIs the study design appropriate and is the work technically sound?\nNo\nAre sufficient details of methods and analysis provided to allow replication by others?\nPartly\nIf applicable, is the statistical analysis and its interpretation appropriate?\nPartly\nAre all the source data underlying the results available to ensure full reproducibility?\nPartly\nAre the conclusions drawn adequately supported by the results?\nPartly\nCompeting Interests: No competing interests were disclosed.\nReviewer Expertise: Image and Signal Processing, Soft Computing, Internet-of-Things, Pattern \nRecognition, Bio-inspired Computing and Computer-Aided Design of FPGA and VLSI circuits\nI confirm that I have read this submission and believe that I have an appropriate level of \nexpertise to state that I do not consider it to be of an acceptable scientific standard, for \nreasons outlined above.\n  Page 15 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025\nThe benefits of publishing with F1000Research:\nYour article is published within days, with no editorial bias•\nYou can publish traditional articles, null/negative results, case reports, data notes and more•\nThe peer review process is transparent and collaborative•\nYour article is indexed in PubMed after passing peer review•\nDedicated customer support at every stage•\nFor pre-submission enquiries, contact research@f1000.com\n  Page 16 of 16\nF1000Research 2021, 10:948 Last updated: 29 OCT 2025",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7145465612411499
    },
    {
      "name": "Computer science",
      "score": 0.6463291645050049
    },
    {
      "name": "Deep learning",
      "score": 0.6356183290481567
    },
    {
      "name": "Diabetic retinopathy",
      "score": 0.6323424577713013
    },
    {
      "name": "Grading (engineering)",
      "score": 0.6137121319770813
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6080714464187622
    },
    {
      "name": "Machine learning",
      "score": 0.4998760223388672
    },
    {
      "name": "Inference",
      "score": 0.4526422619819641
    },
    {
      "name": "Diabetes mellitus",
      "score": 0.4470958113670349
    },
    {
      "name": "Medicine",
      "score": 0.3944467306137085
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32138511538505554
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    }
  ]
}