{
    "title": "Can Pretrained Language Models (Yet) Reason Deductively?",
    "url": "https://openalex.org/W4386566768",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5001029768",
            "name": "Zhangdie Yuan",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2519866353",
            "name": "Songbo Hu",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A1969142033",
            "name": "Ivan Vulić",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1927037681",
            "name": "Anna Korhonen",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A2512114965",
            "name": "Zaiqiao Meng",
            "affiliations": [
                "University of Cambridge"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2080133951",
        "https://openalex.org/W2051468121",
        "https://openalex.org/W573508270",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W2153252192",
        "https://openalex.org/W3048713172",
        "https://openalex.org/W3161374759",
        "https://openalex.org/W3206907172",
        "https://openalex.org/W4206633687",
        "https://openalex.org/W3034830866",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3214578205",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W4288336773",
        "https://openalex.org/W3034531294",
        "https://openalex.org/W2001622701",
        "https://openalex.org/W4253011213",
        "https://openalex.org/W4250770736",
        "https://openalex.org/W2111353076",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W3205810519",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W2963565420",
        "https://openalex.org/W3206375861",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W4226204562",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W4243656193",
        "https://openalex.org/W4238584344",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3146844750",
        "https://openalex.org/W3199484478",
        "https://openalex.org/W3215840701",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2973722444",
        "https://openalex.org/W3173805051",
        "https://openalex.org/W4286856918",
        "https://openalex.org/W4287758766",
        "https://openalex.org/W3103816537",
        "https://openalex.org/W2071718761",
        "https://openalex.org/W3207553988",
        "https://openalex.org/W4221148719",
        "https://openalex.org/W3101204082",
        "https://openalex.org/W2161621238",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W3116884288",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3174082608",
        "https://openalex.org/W2999089077",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W1966944550",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2032152873",
        "https://openalex.org/W4235505822",
        "https://openalex.org/W3099843385",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W2132453167"
    ],
    "abstract": "Acquiring factual knowledge with Pretrained Language Models (PLMs) has attracted increasing attention, showing promising performance in many knowledge-intensive tasks. Their good performance has led the community to believe that the models do possess a modicum of reasoning competence rather than merely memorising the knowledge. In this paper, we conduct a comprehensive evaluation of the learnable deductive (also known as explicit) reasoning capability of PLMs. Through a series of controlled experiments, we posit two main findings. 1) PLMs inadequately generalise learned logic rules and perform inconsistently against simple adversarial surface form edits. 2) While the deductive reasoning fine-tuning of PLMs does improve their performance on reasoning over unseen knowledge facts, it results in catastrophically forgetting the previously learnt knowledge. Our main results suggest that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs’ deductive reasoning abilities; we reach beyond (misleading) task performance, revealing that PLMs are still far from robust reasoning capabilities, even for simple deductive tasks.",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1447–1462\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nCan Pretrained Language Models (Yet) Reason Deductively?\nZhangdie Yuan♢∗, Songbo Hu♠∗, Ivan Vuli´c♠, Anna Korhonen♠, Zaiqiao Meng♣♠†\n♢Department of Computer Science and Technology, University of Cambridge\n♠Language Technology Lab, University of Cambridge\n♣School of Computing Science, University of Glasgow\n♢♠{zy317,sh2091,iv250,alk23}@cam.ac.uk\n♣zaiqiao.meng@glasgow.ac.uk\nAbstract\nAcquiring factual knowledge with Pretrained\nLanguage Models (PLMs) has attracted increas-\ning attention, showing promising performance\nin many knowledge-intensive tasks. Their good\nperformance has led the community to believe\nthat the models do possess a modicum of rea-\nsoning competence rather than merely memo-\nrising the knowledge. In this paper, we conduct\na comprehensive evaluation of the learnable\ndeductive (also known as explicit) reasoning\ncapability of PLMs. Through a series of con-\ntrolled experiments, we posit two main findings.\n(i) PLMs inadequately generalise learned logic\nrules and perform inconsistently against simple\nadversarial surface form edits. (ii) While the\ndeductive reasoning fine-tuning of PLMs does\nimprove their performance on reasoning over\nunseen knowledge facts, it results in catastroph-\nically forgetting the previously learnt knowl-\nedge. Our main results suggest that PLMs can-\nnot yet perform reliable deductive reasoning,\ndemonstrating the importance of controlled ex-\naminations and probing of PLMs’ deductive\nreasoning abilities; we reach beyond (mislead-\ning) task performance, revealing that PLMs are\nstill far from robust reasoning capabilities, even\nfor simple deductive tasks.\n1 Introduction\nPretrained Language Models (PLMs) such as\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) have orchestrated tremendous progress\nin NLP across a large variety of downstream ap-\nplications. For knowledge-intensive tasks in par-\nticular, these large-scale PLMs are surprisingly\ngood at memorising factual knowledge presented\nin pretraining corpora (Petroni et al., 2019; Jiang\net al., 2020b) and infusing knowledge from exter-\nnal sources (Wang et al., 2021a; Zhou et al., 2022,\namong others), demonstrating their effectiveness\nin learning and capturing knowledge.\n∗Indicates equal contribution.\n†Corresponding author.\nBERT\nTraining\nA raven can ﬂy.\nOriginal\nbird\nbird\nbird\nPrediction\nInference\nA bird can ﬂy.\nA raven is a bird.\nA [MASK] can ﬂy.\nA [MASK] can ﬂy.\nA [MASK] cannot ﬂy.\nThe [MASK] species \nis decreasing.\nR-BERT\nBERT\nraven\nraven\nravenR-BERT\nNegation\nQuery\nNeutral\n(a)\n(b)\n(1)\n(2)\n(3)\n(1)\n(2)\n(3)\n(1)\n(2)\n(3)\nFigure 1: Training and inference for deductive reason-\ning. Given the explicit premises (a), the input BERT\nmodel is trained to get transformed into a reasoner R-\nBERT model by deductively predicting a previously\nunseen conclusion (b). This inference process requires\nR-BERT to understand factual knowledge and interpret\nrules (e.g. taxonomic relations), intervening directly in\nthe deduction process.\nAutomatic reasoning, a systematic process of de-\nriving previously unknown conclusions from given\nformal representations of knowledge (Lenat et al.,\n1990; Newell and Simon, 1956), has been a long-\nstanding goal of AI research. In the NLP commu-\nnity, a modern view of this problem (Clark et al.,\n2020), where the formal representations of knowl-\nedge are substituted by the natural language state-\nments, has recently received increasing attention,1\nyielding multiple exploratory research directions:\nmathematical reasoning (Rabe et al., 2021), sym-\nbolic reasoning (Yang and Deng, 2021), and com-\n1Following Clark et al. (2020), we also define natural lan-\nguage rules as linguistic expressions of conjunctive impli-\ncations, condition[∧condition]∗ → conclusion, with the\nsemantics of logic programs with negations (Apt et al., 1988).\n1447\nmonsense reasoning (Li et al., 2019). Impressive\nsigns of progress have been reported in teaching\nPLMs to gain reasoning ability rather than just\nmemorising knowledge facts (Kassner et al., 2020;\nTalmor et al., 2020), suggesting that PLMs could\nserve as effective reasoners for identifying analo-\ngies and inferring facts not explicitly/directly seen\nin the data (Kassner et al., 2020; Ushio et al., 2021).\nIn particular, deductive reasoning2 is one of the\nmost promising directions (Sanyal et al., 2022; Tal-\nmor et al., 2020; Li et al., 2019). By definition,\ndeduction yields valid conclusions, which must be\ntrue given that their premises are true (Johnson-\nLaird, 1999). In the NLP community, given all\nthe premises in natural language statements, some\nlarge-scale PLMs have shown to be able to deduc-\ntively draw appropriate conclusions under proper\ntraining schemes (Clark et al., 2020; Talmor et al.,\n2020). Figure 1 shows an example of the training\nand inference processes of deductive reasoning.\nDespite promising applications of PLMs, some\nrecent studies have pointed out that they could only\nperform a shallow level of reasoning on textual\ndata (Helwe et al., 2021). Indeed, PLMs can be\neasily affected by mispriming (Misra et al., 2020)\nand still hardly differentiate between positive and\nnegative statements (i.e., the so-called negation is-\nsue) (Ettinger, 2020). However, given that some ev-\nidence suggests that PLMs can learn factual knowl-\nedge beyond mere rote memorisation (Heinzerling\nand Inui, 2021) and their limitations (Helwe et al.,\n2021), it is natural to ask, “Can the current PLMs\npotentially serve as reliable deductive reasoners\nover factual knowledge?” To answer it, as the main\ncontribution of this work, we conduct a compre-\nhensive experimental study on testing the learnable\ndeductive reasoning capability of the PLMs.\nIn particular, we test various reasoning training\napproaches on two knowledge reasoning datasets.\nOur experimental results indicate that such deduc-\ntive reasoning training of the PLMs (e.g., BERT\nand RoBERTa) yields strong results on the stan-\ndard benchmarks, but it inadequately generalises\nlearned logic rules to unseen cases. That is, they\nperform inconsistently against simple surface form\nperturbations (e.g., simple synonym substitution,\nparaphrasing or negation insertion), advocating a\ncareful rethinking of the details behind the seem-\ningly flawless empirical performance of deduc-\n2This type of reasoning is also often referred to as explicit\nreasoning in the literature (Broome, 2013; Aditya et al., 2018).\ntive reasoning using the PLMs. We hope our\nwork will inspire further research on probing and\nimproving the deductive reasoning capabilities\nof the PLMs. Our code and data are available\nonline at https://github.com/cambridgeltl/\ndeductive_reasoning_probing.\n2 Related Work\nKnowledge Probing, Infusing, and Editing\nwith PLMs. PLMs appear to memorise (world)\nknowledge facts during pretraining, and such cap-\ntured knowledge is useful for knowledge-intensive\ntasks (Petroni et al., 2019, 2021). A body of re-\ncent research has aimed to (i) understand how\nmuch knowledge PLMs store, i.e., knowledge\nprobing (Petroni et al., 2019; Meng et al., 2022);\n(ii) how to inject external knowledge into them,\ni.e., knowledge infusing (Wang et al., 2021b; Meng\net al., 2021); and (iii) how to edit the stored knowl-\nedge, i.e. knowledge editing (De Cao et al., 2021).\nIn particular, De Cao et al. (2021) have shown that\nit is possible to modify a single knowledge fact\nwithout affecting all the other stored knowledge.\nHowever, some empirical evidence suggests that\nexisting PLMs generalise poorly to unseen sen-\ntences and are easily misled (Kassner and Schütze,\n2020).3 Moreover, this body of research focuses\nonly on investigating how to recall or expose the\nfactual and commonsense knowledge that has been\nencoded in the PLMs, rather than exploring their\ncapabilities of deriving previously unknown knowl-\nedge via deductive reasoning, as done in this work.\nKnowledge Reasoning with PLMs. In re-\ncent years, PLMs have also achieved impressive\nprogress in knowledge reasoning (Helwe et al.,\n2021). For example, PLMs can infer a conclusion\nfrom a set of knowledge statements and rules (Tal-\nmor et al., 2020; Clark et al., 2020), with both the\nknowledge and the rules being mentioned explicitly\nand linguistically in the model input. Some gener-\native PLMs, such as T5 (Raffel et al., 2020), are\neven able to generate natural language proofs that\nsupport implications over logical rules expressed\nin natural language (Tafjord et al., 2021). In par-\nticular, some large PLMs, such as LaMDA (Thop-\npilan et al., 2022), have been shown to be able\nto conduct multi-step reasoning under the chain\n3For instance, if we add the talk token into the statement\n“Birds can [MASK].” (i.e. “Talk. Birds can [MASK].”), the\nPLM might be misled by the added token and predict talk\nrather than the originally predicted fly token (Kassner and\nSchütze, 2020).\n1448\nof thought prompting (Wei et al., 2022) or proper\nsimple prompting template (Kojima et al., 2022).\nAlthough the generated ‘reasoning’ statements po-\ntentially benefit some downstream tasks, there is\ncurrently no evidence that the statements are gener-\nated via deductive reasoning, rather than obtained\nvia pure memorisation. Generative reasoning mod-\nels are difficult to evaluate since this requires huge\neffort of manual assessment (Bostrom et al., 2021).\nAlthough some research has demonstrated that\nPLMs can learn to effectively perform inference\nwhich involves taxonomic and world knowledge,\nchaining, and counting (Talmor et al., 2020), pre-\nliminary experiments on a single test set in more re-\ncent research have revealed that fine-tuning PLMs\nfor editing knowledge might negatively affect the\npreviously acquired knowledge (De Cao et al.,\n2021). Our work performs systematic and con-\ntrolled examinations of the deductive reasoning ca-\npabilities of PLMs and reaches beyond (sometimes\nmisleading) task performance.\n3 Deductive Reasoning\nWhat is Deductive Reasoning? Psychologists de-\nfine reasoning as a process of thought that yields\na conclusion from precepts, thoughts, or asser-\ntions (Johnson-Laird, 1999). Three main schools\ndescribe what people may compute to derive this\nconclusion: relying on factual knowledge (Ander-\nson, 2014; Newell, 1990), formal rules (Braine,\n1998; Braine and O’Brien, 1991), mental mod-\nels (Johnson-Laird, 1983), or some mixture of\nthem (Falmagne and Gonsalves, 1995). Our experi-\nmental study focuses on a ‘computational’ aspect of\nreasoning — namely, whether computational PLMs\nfor reasoning inadequately generalise learned logic\nrules and perform inconsistently against simple ad-\nversarial reasoning examples.\nWe investigate deductive reasoning in the con-\ntext of NLP and neural PLMs. In particular, the\ngoal of this deductive reasoning task is to train a\nPLM (e.g. BERT) over some reasoning examples\n(each with a set of premises and a conclusion) to\nbecome a potential reasoner (e.g. R-BERT as illus-\ntrated in Figure 1). Then, the trained reasoner can\nbe used to infer deductive conclusions consistently\nover explicit premises, where the derived conclu-\nsions are usually unseen during the PLM pretrain-\ning/training. This inference process requires the\nunderlying PLMs to understand factual knowledge\nand interpret rules intervening in the deduction pro-\nSoftmax\nBERT\n(a) CLS-BERT (b) MLM-BERT (c) Cloze-BERT\nBERT BERT\nA bird [MASK]   ﬂy.\nA raven is a bird.\nA raven can [MASK]  .\nA bird can ﬂy.\nA raven is a bird.\nA [MASK]  can ﬂy.\n[can]\nSoftmax Softmax\n[raven][ﬂy]\n[CLS] A bird can ﬂy.\nA raven is a bird.\nA raven can ﬂy.\nFigure 2: Different reasoning training approaches.\ncess. In this paper, we only focus on the encoder-\nbased PLMs (e.g. BERT and RoBERTa) as they\ncan be evaluated under more controllable condi-\ntions and scrutinised via automatic evaluation. In\nparticular, we investigate two task formulations of\nthe deductive reasoning training: 1) classification-\nbased and 2) prompt-based reasoning, as follows.\n3.1 Classification-based Reasoning\nThe classification-based approach formulates the\ndeductive reasoning task as a sequence classifica-\ntion task. Let D= {D(1),D(2),···,D(n)}be a rea-\nsoning dataset, where nis the number of examples.\nEach example D(i) ∈D contains a set of premises\nP(i) = {p(i)\n1 ,p(i)\n2 ... p(i)\nj }, a hypothesis h(i), and\na binary label l(i) ∈{0,1}. A classification-based\nreasoner takes the input of P(i) and h(i), then out-\nputs a binary label l(i) indicating the faithfulness\nof h(i), given that P(i) is hypothetically factual.\nThe goal of the classification-based reasoning\ntraining is to build a statistical model param-\neterised by θ to characterise Pθ(l(i)|h(i),P(i)).\nThose PLMs built on the transformer encoder ar-\nchitecture, such as BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019), can be used as\nthe backbone of such a classification-based rea-\nsoner. Figure 2(a) shows an example of using the\nBERT model to train a classification-based rea-\nsoner (CLS-BERT). In particular, given a training\nexample D(i) = {l(i),h(i),P(i)}, the BERT model\nis trained to predict the hypothesis label by encod-\ning [h(i); P(i)] and computing Pθ(l(i)|h(i),P(i)).\nTo do so, the contextualised representation of the\n‘[CLS]’ token is subsequently projected down to\ntwo logits and passed through a softmax layer to\nform a Bernoulli distribution indicating that a hy-\npothesis is true or false.\n3.2 Prompt-based Reasoning\nDeductive reasoning can also be approached as a\ncloze-completion task by formulating a valid con-\n1449\nclusion as a cloze test. Specifically, given a rea-\nsoning example, i.e., D(i) with its premises P(i),\nand a cloze prompt c(i) (e.g. “A [MASK] can fly”),\ninstead of predicting a binary label, this cloze-\ncompletion task is to predict the masked token a(i)\n(e.g. raven) to the cloze question c(i).\nThe BERT-based models have been widely used\nin the prompt-based reasoning tasks (Helwe et al.,\n2021; Liu et al., 2022), by concatenating the\npremises and the prompt as input and predicting the\nmasked token based on the bidirectional context.\nIn general, there are two training objectives for the\nprompt-based reasoning task, i.e., the mask lan-\nguage modelling (MLM) and task-specific (cloze-\nfilling) objectives. For MLM, the given PLMs are\ntrained over the reasoning examples using their\noriginal pretraining MLM objective to impose de-\nductive reasoning ability; see Figure 2(b) for an\nexample of the BERT reasoner MLM-BERT. For\nthe cloze-filling objective, the PLMs are trained\nwith a task-specific cloze filling objective. As il-\nlustrated in Figure 2(c), Cloze-BERT is trained to\npredict the masked token in the cloze prompt, by\ncomputing the probability Pθ(a(i)|c(i),P(i)). We\nnote that, unlike the original pretraining MLM ob-\njective where 15% tokens of the input are masked\nrandomly, the cloze-filling objective only masks\nthe answer token a(i) in the cloze prompt c(i).\nThis prompt-based reasoning task matches the\nmask-filling nature of BERT. In this way, we can\nprobe the native reasoning ability of BERT without\nany further fine-tuning and evaluate the contribu-\ntion of reasoning training to the PLMs’ reasoning\nability. Foreshadowing, our experimental results in\nSection 5 indicate that reasoning training impacts\nthe model both positively and negatively.\n4 Experiments and Results\nRecent PLMs have shown surprisingly near-perfect\nperformance in deductive reasoning (Zhou et al.,\n2020). However, we argue that high performance\ndoes not mean PLMs have mastered reasoning\nskills. To validate this, we run controlled exper-\niments to examine whether PLM-based reasoners\ngenuinely understand the natural language con-\ntext, produce conclusions robustly against lexical\nand syntactic variance in surface forms, and apply\nlearned rules to unseen cases.\n4.1 Datasets\nTwo datasets are used to examine the PLM-based\nreasoners, namely, the Leap of Thought ( LoT)\ndataset (Talmor et al., 2020) and the WikiData\n(WD) dataset (Vrandecic and Krötzsch, 2014).\nLoT was originally proposed for conducting the\nclassification-based reasoning experiments for de-\nductive reasoning (Talmor et al., 2020) and has\nbeen used as a standard (and sole) benchmark to\nprobe the deductive reasoning capabilities of PLMs\n(Tafjord et al., 2021; Helwe et al., 2021). This\ndataset is automatically generated by prompting\nknowledge graphs, including ConceptNet (Speer\net al., 2017), WordNet (Fellbaum, 1998) and Wiki-\nData (Vrandecic and Krötzsch, 2014). LoT con-\ntains 30,906 training instances and 1,289 instances\nfor each validation and testing set. Each data point\nin LoT also contains a set of distractors that are\nsimilar but irrelevant to deriving the conclusion.\nFor the prompt-based reasoning task, we can\nreformulate the LoT dataset to fit our cloze-\ncompletion task. Instead of having a set of premises\nP, a hypothesis h, and a binary label l, we rewrite\nthe hypothesis in LoT into a clozec and the answer\na (e.g. A raven can fly. →A [MASK] can fly. ).\nNote that we only generate those cloze questions\non the positive examples. Consequently, the results\nacross these two tasks are not directly comparable.\nThe WD dataset is anauxiliary reasoning dataset\nwhich we generated and extracted from Wiki-\ndata5m (Wang et al., 2021b). Similar to previous\nwork (Petroni et al., 2019; Talmor et al., 2020),\nwe converted a set of knowledge graph triples\ninto linguistic statements using manually designed\nprompts. The full description of the dataset con-\nstruction is provided in Appendix C. The final WD\ndataset contains 4,124 training instances, 413 vali-\ndation instances, and 314 test instances. WD only\ncontains positive examples: therefore, we only use\nthis dataset for the cloze-completion task.\n4.2 Adversarial Probing\nPrevious work demonstrates that PLMs can achieve\nnear-perfect empirically results in reasoning tasks.\nFor example, RoBERTa-based models record a\nnear-perfect accuracy of 99.7% in the deductive\nreasoning task on LoT (Talmor et al., 2020). How-\never, another recent study shows that in some natu-\nral language inference benchmarks, PLMs are still\nnot robust to the negation examples (Hossain et al.,\n2020), while humans can handle negations with\n1450\nPerturbation Premises Conclusion Valid\nOriginal A bird can fly. A raven is a bird. A raven can fly. ✓\nParaphrasing A bird is able to fly. A raven is a bird species. A raven can fly. ✓\nSynonym Substitution A fowl can fly. A raven is a fowl. A raven can fly. ✓\nNegation A bird can fly. A raven is a bird. A raven cannot fly. ✗\nRetained knowledge A bird can live up to 100 years. – ✓\nTable 1: Examples of different perturbations strategies that were used to create the adversarial dataset (see §4.2).\nease. In order to systematically probe PLMs’ de-\nductive reasoning capabilities, we design controlled\nexperiments over three different adversarial test\nsettings by generating different surface form per-\nturbations, negation of the original examples, and\ncreating additional retained knowledge anchors .\nTable 1 shows some different adversarial examples.\nSurface Form Perturbations. The theory of men-\ntal models postulates that deductive reasoning is\nbased on manipulations of mental models repre-\nsenting situations. In other words, envisaging the\nsituations and making a deduction can be viewed as\na semantic process (Johnson-Laird, 1999; Polk and\nNewell, 1995). On the other hand, previous works\ndemonstrate that, instead of learning interpretable\nmeaning representations and generalising across\ndifferent surface forms, PLMs tend to learn arte-\nfacts in the training data, e.g., higher-order word\nco-occurrence statistics (Sinha et al., 2021).\nAs both LoT and WD datasets are prompted\nfrom knowledge graphs, the lexical and syntactical\nvariance of the dataset is minimal, with imaginable\nartefacts. To examine if the PLM-based reasoner\ncould consistently perform reasoning against lin-\nguistic diversity and variability (in terms of both\nthe token-level and the syntactic-level diversity),\nwe employ two types of surface form perturbations\nto the data items from the original datasets:\n• Synonym Substitution: In order to investi-\ngate to what extent the PLM-based reasoners\nwould be sensitive to the token-level semantic\ndiversity in terms of deriving their conclusions,\nwe employ synonym substitution (Dhole et al.,\n2021) to the premises P. Synonym substitution\ndoes not modify the syntactic structures and the\npremises’ semantics, preserving all the original\ninput’s structural information. In our setting, a\nword is replaced by a uniform-randomly selected\nsynonym based on WordNet (Fellbaum, 1998)\nwith a probability of 50%.\n• Paraphrasing: To further investigate the\nPLM-based reasoners’ robustness on sentence-\nlevel semantic variability, we paraphrase the\npremises P with two paraphrasing systems:\n(i) PEGASUS, an end-to-end model fine-tuned\nfor paraphrasing (Zhang et al., 2020) (ii) Syntac-\ntically Diverse Paraphrasing (SD-Paraphrasing),\na two-step framework that incorporates neural\nsyntactic preordering for better diversity (Goyal\nand Durrett, 2020).\nNegated Examples. Understanding negation is\noften considered as the first test case in natural lan-\nguage understanding tasks (Ettinger, 2020; Khem-\nlani et al., 2012; Schon et al., 2021). To exam-\nine whether PLMs can handle negation in the\ndeductive reasoning task, we construct a set of\nnegated samples by negating the hypothesis h or\nthe cloze prompt c while keeping the premises\nP unchanged (Hosseini et al., 2021). For the\nclassification-based reasoning task, the label of\nthe negated hypothesis is then also flipped. For\nthe cloze-completion task, since the answer for the\noriginal query will unlikely be the same answer for\nthe negated queries, predicting the original answer\nwould be regarded as a wrong prediction.\nAnchors of Retained Knowledge. Prior work has\nshown that PLMs are prone to forgetting previously\nlearnt knowledge when fine-tuning with new knowl-\nedge data (De Cao et al., 2021), the so-called catas-\ntrophic forgetting issue (Kirkpatrick et al., 2017;\nde Masson d'Autume et al., 2019). It is thus natural\nto also probe whether deductive reasoning training\nstill retains the knowledge already stored in the\noriginal PLM. In this work, in order to measure to\nwhich extent PLMs retain the knowledge acquired\nduring pretraining, we introduce a set of ‘retained\nknowledge statements’ or anchors for each dataset.\nThere are two criteria for such anchors: (i) they\nare semantically close to the conclusions in our test\ndata, (ii) they do not meet the conditions of the\n1451\npremises for a reasoning replacement.4 Ideally, the\nreasoning training should not affect the prediction\nof PLMs when reasoning over these anchors.\nWe create such a set of anchors for both LoT and\nWD, and investigate the behaviour of the reasoning\nmodels over these anchors based on the prompt-\nbased reasoning task. In particular, these anchors\nshould be real-world textual statements that con-\ntain the target word (to meet criterion (i) above),\nbut their newly composed sentences (by the rea-\nsoning replacement) are unlikely true statements\n(to meet criterion (ii) above). To this end, we use\nthe BM25 algorithm (Sparck Jones et al., 2000)\nto retrieve the top 10 sentences for each ‘reason-\ning’ target word from the Wikipedia corpus.5 Then,\nwe construct the anchor sentences based on these\ntop 10 retrieved sentences by replacing their target\nwords with their hyponym/hypernym words. The\nfinal anchors are selected from these top 10 sen-\ntences only when the newly created sentences do\nnot likely exist in the entire Wikipedia corpus (i.e.\ntheir top-1 similar sentence should have less than\na BM25 score of 50). Ideally, this set of retrained\nknowledge statements is relevant but should not be\naffected by the reasoning training.\n4.3 Evaluation\nFollowing previous work (Talmor et al., 2020), the\nevaluation metric for classification-based reason-\ning is accuracy. For prompt-based reasoning, we\ncalculate top K recall (R@K) by measuring what\nfraction of the correct answers are retrieved in the\ntop K predictions. For the negation examples, we\nreport the top K error rate (E@K) because a re-\ntrieved answer a and the negated cloze question¬c\nwould compose a fallacy.\nIn the following, we report our findings and nu-\nmerical results based on the BERT-based reasoners\n(in particular bert-base-uncased), but we note\nthat other PLMs (such as RoBERTa) of various\nsizes observe the same performance trends and\nresult in the same findings and conclusions. Ap-\npendix B provides results for other PLMs.\n4Taking the premise (a) of Figure 1 as an example, the\n‘bird’ token in the statement ‘A bird can fly.’ was replaced by\nits hyponym ‘raven’ after reasoning training. However, in the\nanchor statement ‘The bird species is decreasing.’, the ‘bird’\ntoken should not be replaced by ‘raven’.\n5ElasticSearch: https://github.com/elastic/elasticsearch.\nModel R@1 R@5 R@10\nDataset: LoT\nPretrained 13.15 59.18 70.96\nMLM-BERT 98.36 98.36 98.36\nCloze-BERT 99.73 100 100\nDataset: WD\nPretrained 27.07 64.97 72.93\nMLM-BERT 99.04 99.36 99.36\nCloze-BERT 100 100 100\nTable 2: Recall of the correct answer in the top K pre-\ndictions (R@K) from the BERT model before and after\ndeductive prompt-based reasoning fine-tuning. Both\nMLM-BERT and Cloze-BERT achieve (near-)perfect\nR@1 scores after fine-tuning.\n5 Results and Discussion\nWe evaluate the impact of reasoning training on\nthe PLMs and investigate their robustness against\nthree well-known issues of PLMs: utilising arte-\nfacts from data, incapability of modelling negation,\nand catastrophic forgetting. We further conduct\nqualitative analysis to understand the inference er-\nrors introduced by deductive reasoning training.\n5.1 Deductive Reasoning Training\nFinding 1 All the deductive reasoning training ap-\nproaches significantly improve PLMs’ reasoning\ncapabilities, achieving near-perfect deductive rea-\nsoning performance on both the reasoning test sets.\nTable 2 reveals that the prompt-based reasoners\nachieve near-perfect performance on both datasets,\nregardless of the reasoning training method. In par-\nticular, on LoT dataset the R@1 score of BERT has\nincreased from 13.15% to 98.36% and 99.73% af-\nter the MLM reasoning training (i.e. MLM-BERT)\nand the cloze reasoning training (i.e. Cloze-BERT)\nrespectively, which are in line with previously re-\nported result (Talmor et al., 2020). The near-perfect\ntrends are observed in the classification-based rea-\nsoning models, where CLS-BERT also achieves\na high accuracy score of 94.72% after reasoning\ntraining (Table 5).6 In sum, while the off-the-shelf\nBERT model already demonstrates a decent level\nof empirical performance, conducting reasoning\ntraining on the pretrained BERT achieves strong or\neven near-perfect performance on both datasets.\n1452\nAdversarial Probing LoT WD\nPretrained MLM-BERT Cloze-BERT Pretrained MLM-BERT Cloze-BERT\nOriginal 13.15 98.36 99.73 27.07 99.04 100.00\n+ Pegasus-Paraphrasing 11.79 (↓1.36) 64.66 (↓33.70) 50.96 (↓48.77) 16.88 ( ↓10.19) 49.36 (↓49.68) 51.27 (↓48.73)\n+ SD-Paraphrasing 5.75 (↓7.40) 9.32 (↓89.04) 0.55 (↓99.18) 16.56 ( ↓10.51) 27.39 (↓71.65) 31.21 (↓68.79)\n+ Syn. Substitution 20.00 (↑6.85) 64.38 (↓33.98) 64.66 (↓35.07) 15.29 ( ↓11.78) 61.78 (↓37.26) 62.42 (↓37.58)\nTable 3: R@1 scores on test sets obtained via applying various surface form perturbations from Section 4.2.\nExamples BERT MLM-BERT Cloze-BERT\nA holly is not music. A holly is part of a forest.\nA plant is an actor. A music is not an actor.\nA holly is a plant. A bluebottle is an organism.\nA marigold is not an angiosperm.\nA holly is not an important food source.\nA <MASK> is not an actor.\nmusician (0.1213)✓ holly (0.7168)✗ holly (0.9999)✗\nA perry is not a tree. A tree is not capable of burn.\nAlcohol is capable of burn. A perry is an alcohol.\nA <MASK> is not capable of burn.\ntree (0.5244)✓ person (0.0300)✗ perry (0.9999)✗\nTable 4: Examples of top 1 predictions from the set of negated examples based on LoT test set. The wrong\npredictions are in red, and the reasonable predictions are in green. Some other key-related entities are in the same\ncolour. The numbers in the parentheses are the respective probabilities of each prediction after the softmax layer.\nAdversarial Probing LoT\nCLS-BERT\nOriginal 94.72\n+ Pegasus-Paraphrasing 83.86 (↓10.86)\n+ SD-Paraphrasing 71.14 ( ↓23.58)\n+ Syn. Substitution 84.48 ( ↓10.24)\n+ Negation 9.34 ( ↓85.49)\nTable 5: Accuracy scores on test sets obtained via apply-\ning various surface form perturbations from Section 4.2.\n5.2 Surface Form Perturbations\nFinding 2 Surface form perturbations drastically\ndecrease PLMs’ reasoning performance.\nA natural follow-up question to ask is to what ex-\ntent the aforementioned near-perfect numbers re-\nally reflect the model’s reasoning abilities. We thus\nperform surface form perturbations to add lexical\nand syntactic variance to the test datasets and probe\nthe model against such variations.\nTable 3 demonstrates the performances of the\nMLM-BERT and Cloze-BERT reasoners, as well\nas the vanilla pretrained BERT language model,\non our controlled test sets generated by different\n6Note that we cannot obtain the performance of original\nBERT since the classification head has not been trained yet.\nperturbation approaches. We can observe that the\nscores for both reasoners decrease substantially\n(>30%) even when applying simple synonym sub-\nstitution, which only adds lexical variance to the\nprompt-generated query. In contrast, the pretrained\nBERT language model is less vulnerable to such\nan issue. This finding aligns with the hypothesis\nthat PLMs tend to memorise word co-occurrence\nstatistics (Sinha et al., 2021).7\nWe also observe from Table 3 that the R@1 per-\nformances of all reasoners decrease strongly when\napplying either the Pegasus-Paraphrasing or SD-\nParaphrasing methods to the premises. In particular,\nthe drops are especially pronounced with the SD-\nParaphrasing method, which is designed exactly to\ngenerate syntactically very diverse paraphrases. On\nthe other hand, we see from Table 5 that similar per-\nformance degradation trends can be observed in the\nclassification-based reasoning task. These results\nillustrate that current PLMs perform inconsistently\nagainst various surface form perturbations, suggest-\ning that future work should look into the creation\nof more robust reasoners that should be resilient to\nlexical, syntactic, and semantic variability.\n7The comparison of large models (e.g. bert-large) can\nbe found in Appendix B, which indicates the similar trends.\n1453\nLoT Negation WD Negation\n0\n20\n40\n60\n80\n100E@1\nPretrainedMLM-BERTCloze-BERT\nFigure 3: Results (E@1 scores, lower is better) on the\ntest set comprising negated examples; before versus\nafter deductive-reasoning training.\n5.3 Negated Examples\nFinding 3 All reasoners cannot distinguish be-\ntween negated and non-negated examples.\nFigure 3 reveals that the error rate E@1 scores (the\nlower, the better)8 for all reasoners on the set of\nnegated examples largely increase after reasoning\ntraining. The original off-the-shelf BERT model\nachieves E@1 of 18.63% on LoT and 14.33% on\nthe WD dataset. However, after reasoning train-\ning, the error rate of MLM-BERT significantly in-\ncreases to 98.36% (LoT) and 98.73% (WD). Cloze-\nBERT’s performance is even worse, with E@1\nscores of 99.73% (LoT) and 100% (WD), suggest-\ning a clear case of overfitting to word co-occurrence\nand other artefacts in the training sets. Moreover,\nthe accuracy score on negated LoT examples is only\n9.34%, while a random baseline would score 50%.\nIn sum, these scores indicate that the PLM-based\nreasoners cannot distinguish between negated and\nnon-negated examples, and their performance on\nnegated examples substantially worsens after rea-\nsoning training due to task-specific overfitting.\nA quick error analysis, provided in Table 4,\nfurther points to the issues with negated exam-\nples. The first example shows that the off-the-shelf\nBERT model makes a reasonable guess, semanti-\ncally related to an entity mentioned in the premises.\nThis guess is similar to a human guess from the\nsame premises. However, after reasoning training,\nboth MLM-BERT and Cloze-BERT make wrong\npredictions, and Cloze-BERT is extremely confi-\n8E@1 only denotes a specific type of error: the model\nmakes a wrong prediction due to negation being added, i.e.\nthe negation error. However, many other types of errors that\na model can make a wrong prediction, but they are not our\nfocus and thus not measured. In other words, (1 - E@1) is not\nequivalent to accuracy.\nLoT RetrainedWD Retrained\n0\n20\n40\n60\n80\n100R@1\nPretrainedMLM-BERTCloze-BERT\nFigure 4: Results (R@1 scores, higher is better) on\nthe test set comprising anchors of retained knowledge;\nbefore versus after deductive-reasoning training.\ndent in its wrong predictions.\n5.4 Anchors of Retained Knowledge\nFinding 4 Previously learnt knowledge is not fully\nretained after reasoning training, and the trained\nreasoners (catastrophically) forget it.\nFigure 4 shows that performance on the anchors\ndeteriorates substantially after reasoning training.\nOn LoT, MLM-BERT ‘forgets’∼77% of the previ-\nously learnt knowledge, achieving only 23.34% on\nR@1. Cloze-BERT performs even worse, scoring\nonly 6.27% R@1. The drops are slightly lower but\nstill substantial on WD: MLM-BERT retains 65.7%\nand Cloze-BERT retains 41.32% of the previously\nstored knowledge. This result indicates that rea-\nsoning training yields the well-known phenomenon\nof catastrophic forgetting: this effect seems even\nmore pronounced with Cloze-BERT, which relies\non a very task-specific objective that might result\nin overfitting the task data.\nFurthermore, Table 6 displays several qualitative\nexamples where the predictions in green refer to\nthe correct predictions based on human judgement.\nNotice that in the last example, even though the pre-\ndictions from the two trained reasoners are correct,\nthe probabilities are much lower.\nSeveral strategies might help mitigate catas-\ntrophic forgetting. A promising direction is en-\ncapsulating lightweight adapter modules (Houlsby\net al., 2019; Pfeiffer et al., 2020b; Ansell et al.,\n2021) within the underlying PLM, where all the\n‘deductive reasoning capability’ will be stored\nsolely in the adapter modules, leaving the original\nPLM intact (Pfeiffer et al., 2020a). Other similar\nparameter-efficient and modular methods include\nprefix tuning (Li and Liang, 2021) or sparse masks\n(Sung et al., 2021; Ansell et al., 2022). Their main\n1454\nExamples BERT MLM-BERT Cloze-BERT\na flea is a parasitic <MASK>. insect (0.2006) ✓ substance (0.1950)✗ drone (0.0193)✗\nvapour density is a unitless <MASK>. quantity (0.3494)✓ quantity (0.2197)✓ volume (0.0412)✗\nboth sexes have a throat <MASK>. pouch (0.5169) ✓ ##lid (0.3606)✗ hollow (0.0135)✗\nfirefox is a web <MASK>. browser (0.9144) ✓ browser (0.8861)✓ browser (0.3243)✓\nTable 6: Examples of top 1 predictions from the set of Anchors of Retained Knowledge based on LoT. A wrong\nprediction is in red, and a reasonable prediction (aligning with human judgement) is in green. The numbers in the\nparentheses are the respective probabilities of each prediction after the softmax layer.\npremise is to separate knowledge extraction and\ncomposition, preserving previously learnt knowl-\nedge during reasoning training. We leave reasoning\ntraining with such methods for future research.\n6 Conclusion\nIn this paper, we probed into the deductive rea-\nsoning capabilities of PLMs and conducted com-\nprehensive controlled experiments to examine and\ncompare various deductive reasoning training ap-\nproaches. Our experimental results showed that\ncurrent PLM-based deductive reasoners suffer from\nseveral issues: 1) they rely on artefacts from the\ntraining data, 2) they are incapable of modelling\nnegation in deductive reasoning, and 3) they for-\nget knowledge acquired during pretraining when\nthey get specialised into deductive reasoners. In\nparticular, our experimental study demonstrated\nthat models are vulnerable to multiple adversarial\nmethods, including simple surface form perturba-\ntions such as synonym substitution or paraphrasing.\nWhile the PLMs trained for deductive reasoning\nachieve seemingly perfect empirical results in dif-\nferent reasoning datasets, they cannot yet systemat-\nically generalise to other deductive reasoning exam-\nples. Consequently, our study also calls for further,\nmore rigorous examinations of future PLM-based\nmodels’ deductive reasoning abilities.\nAcknowledgements\nIvan Vuli´c is supported by a personal Royal Soci-\nety University Research Fellowship (no 221137;\n2022-). Songbo Hu is supported by Cambridge\nInternational Scholarship.\nLimitations\nDespite the thorough experiments on standard and\npopular PLMs of various sizes, this study explores\nonly encoder-based models. Some generation-\nbased models under other Transformer architec-\ntures, such as encoder-decoder (T5) or decoder-\nonly (GPT-3), were also deployed in the reasoning\ntasks (Bostrom et al., 2021; Wei et al., 2022). We\ndo not probe those groups of models here due to\nthe difficult of evaluation and we leave them for\nfuture research.\nFurther, for prompt-based reasoning, the current\nreasoners only support and have been evaluated on\nsingle-token prediction in a single language (i.e.\nEnglish). Several prior works have demonstrated\nthat conducting multi-token prediction is consid-\nerably more difficult (Jiang et al., 2020a), which\nwould pose an additional challenge to the PLMs in\ndeductive reasoning tasks. One avenue of future\nwork should also extend the scope of analyses to\nmultilingual and multi-token prediction.\nIn addition, we note that better evaluation re-\nsources that could address paraphrases and word\nsenses, especially for mask-filling tasks, are still\nlacking. This limitation is particularly significant in\nour setting. For example, in addition to the single-\ntoken answer in the evaluation datasets we used,\nthere are some other feasible answers (e.g. syn-\nonyms) for the same query, which should also be\nconsidered a correct prediction. However, such\nanswers are ignored by the current standard evalua-\ntion protocols. As a result, there is a certain level\nof unavoidable noise in the evaluation process.\nFinally, introducing a reasoning dataset is highly\nchallenging and appreciated by the community.\nLeap-of-thought is to our knowledge the only ex-\nisting dataset that is suitable for our deductive rea-\nsoning evaluation. To solidify our conclusions,\nwe further constructed an auxiliary dataset (WD)\nfollowing a similar procedure to LoT. Although\nour data construction method is commonly used to\nextract reasoning examples, such an automatic pro-\ncedure, unfortunately, inevitably reflects the quality\nand errors (e.g. nonsensical statements) from our\nsource (WikiData). To reduce such noisy examples,\nwe have conducted multiple rounds of filtering (see\n1455\nAppendix for the filtering process) and manually\nremoved as many meaningless relations as we can,\ngiven that manually verifying each reasoning ex-\nample is a highly labour-intensive task.\nReferences\nSomak Aditya, Yezhou Yang, and Chitta Baral. 2018.\nExplicit reasoning over end-to-end neural architec-\ntures for visual question answering. In Proceedings\nof the AAAI Conference on Artificial Intelligence.\nJohn R Anderson. 2014. Rules of the mind. Psychology\nPress.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan\nVuli´c. 2022. Composable sparse fine-tuning for cross-\nlingual transfer. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1778–1796.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021. MAD-G: Multilingual adapter gen-\neration for efficient cross-lingual transfer. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, pages 4762–4781.\nKrzysztof R. Apt, Howard A. Blair, and Adrian Walker.\n1988. Towards a theory of declarative knowledge.\nIn Jack Minker, editor, Foundations of Deductive\nDatabases and Logic Programming, pages 89–148.\nMorgan Kaufmann.\nKaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg\nDurrett. 2021. Flexible generation of natural lan-\nguage deductions. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6266–6278.\nMartin D Braine and David P O’Brien. 1991. A the-\nory of if: A lexical entry, reasoning program, and\npragmatic principles. Psychological review, page\n182.\nMartin DS Braine. 1998. Steps toward a mental-\npredicate logic. In Mental logic, pages 281–340.\nPsychology Press.\nJohn Broome. 2013. Rationality through reasoning .\nJohn Wiley & Sons.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In\nProceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence , pages 3882–\n3890.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506.\nCyprien de Masson d'Autume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. In Advances\nin Neural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics, pages 4171–4186.\nKaustubh D. Dhole, Varun Gangal, Sebastian\nGehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-\nhamood, Abinaya Mahendiran, Simon Mille, Ashish\nSrivastava, Samson Tan, Tongshuang Wu, Jascha\nSohl-Dickstein, Jinho D. Choi, Eduard H. Hovy,\nOndrej Dusek, Sebastian Ruder, Sajant Anand, Na-\ngender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Car-\noline Brun, Marco Antonio Sobrevilla Cabezudo,\nSamuel Cahyawijaya, Emile Chapuis, Wanxiang\nChe, Mukund Choudhary, Christian Clauss, Pierre\nColombo, Filip Cornell, Gautier Dagan, Mayukh\nDas, Tanay Dixit, Thomas Dopierre, Paul-Alexis\nDray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di\nGiovanni, Rishabh Gupta, Rishabh Gupta, Louanes\nHamla, Sang Han, Fabrice Harel-Canada, Antoine\nHonore, Ishan Jindal, Przemyslaw K. Joniak, Denis\nKleyko, Venelin Kovatchev, and et al. 2021. Nl-\naugmenter: A framework for task-sensitive natural\nlanguage augmentation. CoRR.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, pages 34–48.\nR J Falmagne and J Gonsalves. 1995. Deductive infer-\nence. Annual Review of Psychology, 46(1):525–559.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. The MIT Press.\nTanya Goyal and Greg Durrett. 2020. Neural syntactic\npreordering for controlled paraphrase generation. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 238–252.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main xvolume, pages 1772–1791.\nChadi Helwe, Chloé Clavel, and Fabian M. Suchanek.\n2021. Reasoning with transformer-based models:\nDeep learning, but shallow reasoning. In 3rd Confer-\nence on Automated Knowledge Base Construction.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020. An analysis of natural language in-\nference benchmarks through the lens of negation.\n1456\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9106–9118.\nArian Hosseini, Siva Reddy, Dzmitry Bahdanau, R De-\nvon Hjelm, Alessandro Sordoni, and Aaron Courville.\n2021. Understanding by understanding not: Model-\ning negation in language models. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1301–1312.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference on\nMachine Learning, Proceedings of Machine Learning\nResearch, pages 2790–2799.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5943–5959.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, pages 423–438.\nP. N. Johnson-Laird. 1999. Deductive reasoning. An-\nnual Review of Psychology, 50(1):109–135. PMID:\n15012459.\nPhilip N. Johnson-Laird. 1983. Mental models : to-\nwards a cognitive science of language, inference, and\nconsciousness. Cambridge, MA: Harvard University\nPress. Excerpts available on Google Books.\nNora Kassner, Benno Krojer, and Hinrich Schütze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 552–564.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818.\nSangeet Khemlani, Isabel Orenes, and P. N. Johnson-\nLaird. 2012. Negation: A theory of its meaning,\nrepresentation, and use. Journal of Cognitive Psy-\nchology, 24(5):541–559.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. CoRR.\nDouglas B. Lenat, Ramanathan V . Guha, Karen Pittman,\nDexter Pratt, and Mary Shepherd. 1990. CYC: to-\nward programs with common sense. Commun. ACM,\n33(8):30–49.\nShiyang Li, Jianshu Chen, and Dian Yu. 2019. Teach-\ning pretrained models with commonsense reasoning:\nA preliminary kb-based approach. arXiv preprint\narXiv:1909.09743.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4582–\n4597.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3154–3169.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nZaiqiao Meng, Fangyu Liu, Thomas Clark, Ehsan\nShareghi, and Nigel Collier. 2021. Mixture-of-\npartitions: Infusing large biomedical knowledge\ngraphs into BERT. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4672–4681.\nZaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su,\nCharlotte Collins, and Nigel Collier. 2022. Rewire-\nthen-probe: A contrastive recipe for probing biomed-\nical knowledge of pre-trained language models. In\nProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4798–\n4810.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2020.\nExploring BERT’s sensitivity to lexical cues using\ntests from semantic priming. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4625–4635.\nA Newell. 1990. Unified theories of cognition cam-\nbridge university press. Cambridge, MA.\nA. Newell and H. Simon. 1956. The logic theory\nmachine–a complex information processing system.\nIRE Transactions on Information Theory, pages 61–\n79.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\n1457\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics, pages 2523–\n2544.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2463–2473.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020a. AdapterHub: A\nframework for adapting transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, pages 46–54.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7654–7673.\nThad A Polk and Allen Newell. 1995. Deduction as\nverbal reasoning. Psychological Review, page 533.\nMarkus Norman Rabe, Dennis Lee, Kshitij Bansal, and\nChristian Szegedy. 2021. Mathematical reasoning via\nself-supervised skip-tree training. In International\nConference on Learning Representations.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3982–3992.\nSoumya Sanyal, Harman Singh, and Xiang Ren. 2022.\nFaiRR: Faithful and robust deductive reasoning over\nnatural language. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1075–1093.\nClaudia Schon, Sophie Siebert, and Frieder Stolzen-\nburg. 2021. Negation in cognitive reasoning. In KI\n2021: Advances in Artificial Intelligence - 44th Ger-\nman Conference on AI, Lecture Notes in Computer\nScience, pages 217–232.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913.\nK. Sparck Jones, S. Walker, and S.E. Robertson. 2000.\nA probabilistic model of information retrieval: devel-\nopment and comparative experiments: Part 1. Infor-\nmation Processing & Management, 36(6):779–808.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence , pages\n4444–4451.\nYi-Lin Sung, Varun Nair, and Colin Raffel. 2021. Train-\ning neural networks with fixed sparse masks. In\nAdvances in Neural Information Processing Systems,\npages 24193–24205.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguistics:\nACL-IJCNLP 2021, pages 3621–3634.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neural\nInformation Processing Systems.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR.\nAsahi Ushio, Luis Espinosa Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. BERT is to NLP\nwhat AlexNet is to CV: Can pre-trained language\nmodels identify analogies? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3609–3624.\nDenny Vrandecic and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-Adapter: Infusing\nKnowledge into Pre-Trained Models with Adapters.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 1405–1418.\n1458\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKEPLER: A unified model for knowledge embed-\nding and pre-trained language representation. Trans-\nactions of the Association for Computational Linguis-\ntics, pages 176–194.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nKaiyu Yang and Jia Deng. 2021. Learning symbolic\nrules for reasoning in quasi-natural language. arXiv\npreprint arXiv:2111.12038.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. PEGASUS: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, pages 11328–11339.\nMing Zhou, Nan Duan, Shujie Liu, and Heung-Yeung\nShum. 2020. Progress in neural nlp: Modeling, learn-\ning, and reasoning. Engineering, 6(3):275–290.\nWenxuan Zhou, Fangyu Liu, Ivan Vuli ´c, Nigel Col-\nlier, and Muhao Chen. 2022. Prix-LM: Pretraining\nfor multilingual knowledge base construction. In\nProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5412–\n5424.\n1459\nA Experimental Details\nTable 7 lists our model hyperparameters. Among\nthese models, MLM-BERT and Cloze-BERT were\nimplemented using the HuggingFace transform-\ners package (Wolf et al., 2020). We implement\nCLS-BERT via the SBERT repository (Reimers\nand Gurevych, 2019), which is built on top of the\nHuggingFace repository (Wolf et al., 2020). Unless\nmentioned otherwise, all the hyperparameters are\nset to the default values provided in the Hugging-\nFace and SBERT repositories.\nHyper-parameter Value\nMLM-BERT and Cloze-BERT\nbatch_size 2\nmax_sequence_length 512\ntraining_epoch 3\nCLS-BERT\nbatch_size 64\nmax_sequence_length 128\ntraining_epoch 20\nTable 7: Model hyper-parameters.\nB Evaluation Results for PLMs\nTable 8 supplements the main paper by provid-\ning additional results with classification-based rea-\nsoners, where the reasoners start from different\nPLMs: distil-bert, bert-base, bert-large,\nroberta-base, and roberta-large. These re-\nsults corroborate the main findings presented in\nthe main paper; see Section 5. Larger models do\nperform slightly better than their smaller variants\non average. However, the results also demonstrate\nthat different models, regardless of their size, suffer\nfrom exactly the same issues, discussed in the main\npaper.\nTable 9 and Table 10 demonstrate performance\nover adversarial test sets for prompt-based reason-\ners (1. MLM-based, 2. Cloze-based). The findings\nfrom these tables align with the findings from the\nmain results presented in Section 5.\nC WD Dataset Construction Pipeline\nWe construct theWD dataset following the pipeline\nshown in Figure 5, and outlined in what follows.\nSource Data. We choose the Wikidata5m\ndataset (Wang et al., 2021b) as the knowledge\nsource for WD. Wikidata5m is a million-scale\nknowledge graph dataset created upon Wiki-\ndata (Vrandecic and Krötzsch, 2014). This dataset\ncomprises 20 million triples, describing relevant\nand important knowledge statements about real-\nworld entities.\nWD Dataset\nFiltering\nPrompting\nWikidata5m Dataset\nFigure 5: Data construction pipeline for the WD dataset.\nThis WD dataset is extracted and derived from Wiki-\nData5m (Wang et al., 2021b). Following the previous\npipeline (Petroni et al., 2019), we convert a set of knowl-\nedge graph triples into linguistic statements using man-\nually designed prompts.\nPrompting. We manually select a set of relations\nbased on their frequency and design their corre-\nsponding prompts shown in Table 11. Given a\ntaxonomic relation, such as is_awith relation ID\nP31, we sample a relevant taxonomic-knowledge\ngraph triple: ⟨raven,is_a,bird⟩. We then retrieve\nother relevant triples about the subject and the ob-\nject, for example, ⟨bird,is_capable_of,fly ⟩and\n⟨raven,is_capable_of,fly ⟩. Next, we use our\npredefined prompts to convert the knowledge graph\ntriples to textual knowledge statements: A bird can\nfly, A bird is a raven and A raven can fly . Fur-\nthermore, these prompted statements assemble an\ninference instance in the WD dataset: if A bird\ncan fly and A bird is a raven, then A raven can fly.\nIn our cloze-completion task setting, we mask the\nobject of the taxonomic triple (e.g. raven) in the\nconclusion statement to form a cloze question (A\n[MASK] can fly). Therefore, ravenis the correct\nanswer to this question.\nFiltering. We filter those constructed inference\ninstances with the following properties: (i) We\nonly choose examples with answers being a sin-\ngle masked token, and these answers should be\nincluded in the BERT vocabulary. (ii) For all in-\n1460\nAdversarial Probing LoT\ndistilbert-based-uncased bert-base-uncased bert-large-uncased roberta-base roberta-large\nOriginal 90.22 94.72 98.76 92.71 99.38\n+ Pegasus-Paraphrasing 77.27 (↓12.95) 83.86 ( ↓10.86) 84.40 ( ↓14.35) 80.84 ( ↓11.87) 87.20 (↓12.18)\n+ SD-Paraphrasing 66.56 (↓23.66) 71.14 ( ↓23.58) 73.39 ( ↓25.39) 75.95 ( ↓16.76) 78.97 (↓20.40)\n+ Syn. Substitution 81.46 (↓8.76) 84.48 ( ↓10.24) 93.33 ( ↓5.43) 80.76 ( ↓11.95) 94.26 (↓5.12)\n+ Negation 18.86 ( ↓71.37) 9.34 ( ↓85.49) 2.58 ( ↓96.28) 19.40 ( ↓73.31) 35.85 (↓63.53)\nTable 8: Accuracy on different LoT adversarial test sets for classification-based reasoners with various back-boned\nPLMs.\nAdversarial Probing LoT WDMLM-DISTILBERT MLM-BERT MLM-BERT-LARGE MLM-DISTILBERT MLM-BERT MLM-BERT-LARGEOriginal 99.45 98.36 99.45 99.36 99.04 98.09+ Pegasus-Paraphrasing 76.71 (↓22.74) 64.66 (↓33.7) 60.27 (↓39.18) 56.05 ( ↓43.31) 49.36 (↓49.68) 52.23 (↓45.86)+ SD-Paraphrasing 24.66 (↓74.79) 9.32 ( ↓89.04) 4.93 (↓94.52) 28.66 ( ↓70.7) 27.39 ( ↓71.65) 33.12 (↓64.97)+ Syn. Substitution 64.11 (↓35.34) 64.38 (↓33.98) 64.93 (↓34.52) 62.1 ( ↓37.26) 61.78 ( ↓37.26) 60.19 (↓37.9)+ Negation (E@1↓) 97.53 98.36 95.89 99.36 98.73 98.41Retained Knowledge 25.86 23.34 26.67 58.33 65.7 56.61\nTable 9: Top 1 recall (R@1) on adversarial test sets for various prompt-based reasoners with MLM training. The\nnumbers for the negated examples indicate the top 1 error rates (the lower, the better).\nAdversarial Probing LoT WDCloze-DISTILBERT Cloze-BERT Cloze-BERT-LARGE Cloze-DISTILBERT Cloze-BERT Cloze-BERT-LARGEOriginal 100 99.73 98.63 100 100 100+ Pegasus-Paraphrasing 57.81 (↓42.19) 50.96 (↓48.77) 47.4 (↓51.23) 57.01 ( ↓42.99) 51.27 (↓48.73) 56.37 (↓43.63)+ SD-Paraphrasing 2.74 (↓97.26) 0.55 ( ↓99.18) 0.82 (↓97.81) 37.26 ( ↓62.74) 31.21 (↓68.79) 46.82 (↓53.18)+ Syn. Substitution 64.66 (↓35.34) 64.66 (↓35.07) 63.84 (↓34.79) 62.74 ( ↓37.26) 62.42 (↓37.58) 62.74 (↓37.26)+ Negation (E@1↓) 100 99.73 98.63 100 100 99.68Retained Knowledge 11.79 6.27 1.45 35.61 41.32 16.95\nTable 10: Top 1 recall (R@1) on adversarial test sets for various prompt-based reasoners with Cloze-filling training.\nThe numbers for the negated examples indicate the top 1 error rates (the lower, the better).\nference instances, the maximum number of oc-\ncurrences of a single answer is 50 to balance the\ndataset and avoid excessive repetition.\nFinal WD Dataset. The final WD dataset\ncontains 4,851 instances, which are randomly\nsplit into 4,124/413/314 instances for train-\ning/validation/testing while keeping that the an-\nswers of the testing set should not appear in the\ntraining/validation sets. This is to ensure that\ntrained reasoners need to draw conclusions via con-\nducting deductive reasoning rather than via memo-\nrisation.\n1461\nRelation ID Prompt\nP31 [X] is a [Y] .\nP136 [X] is a genre of [Y] .\nP179 [X] is part of the [Y] series .\nP279 [X] is a subclass of [Y] .\nP527 [X] consists of [Y] .\nP1269 [X] is a topic of [Y] .\nP17 [X] is hosted in [Y] .\nP39 [X] holds a [Y] position .\nP101 [X] is a subject of [Y] .\nP106 [X] is a [Y] by profession .\nP140 The religion of [X] is [Y] .\nP144 [X] is based on [Y] .\nP180 [X] is a painting of [Y] .\nP276 [X] is located at [Y] .\nP306 [X] runs on [Y] operating system .\nP355 [X] owns [Y] .\nP360 [X] is a list of [Y] .\nP400 [Y] is a platform of [X] .\nP404 The game mode of [X] is [Y] .\nP462 The color of [X] is [Y] .\nP463 [X] is a member of [Y] .\nP737 [X] is influenced by [Y] .\nP749 [Y] owns [X] .\nP1303 [X] plays [Y] .\nP1343 [X] is written about in [Y] .\nTable 11: Manually written prompts for generating the\nWD dataset. Given a triple ⟨[X],R_ID,[Y]⟩, the tex-\ntual knowledge statement (e.g. premises) is written\nbased on the above prompts. R_ID is the unique rela-\ntion ID in the Wikidata5m dataset. Gray entries (first\nsix rows) denote taxonomic relations.\n1462"
}