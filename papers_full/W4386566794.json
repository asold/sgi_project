{
    "title": "Quantifying Context Mixing in Transformers",
    "url": "https://openalex.org/W4386566794",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3139812554",
            "name": "Hosein Mohebbi",
            "affiliations": [
                "Tilburg University"
            ]
        },
        {
            "id": "https://openalex.org/A1997952565",
            "name": "Willem Zuidema",
            "affiliations": [
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A3150780103",
            "name": "Grzegorz Chrupała",
            "affiliations": [
                "Tilburg University"
            ]
        },
        {
            "id": "https://openalex.org/A1976713734",
            "name": "Afra Alishahi",
            "affiliations": [
                "Tilburg University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W3101155149",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2605409611",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2947012833",
        "https://openalex.org/W4385574217",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2972918955",
        "https://openalex.org/W3000716014",
        "https://openalex.org/W3102466593",
        "https://openalex.org/W2904479140",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W4285175926",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3196813608",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3036601975"
    ],
    "abstract": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis.",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3378–3400\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nQuantifying Context Mixing in Transformers\nHosein Mohebbi1 Willem Zuidema2 Grzegorz Chrupała1 Afra Alishahi1\n1 CSAI, Tilburg University 2 ILLC, University of Amsterdam\n{h.mohebbi, a.alishahi}@tilburguniversity.edu\nw.h.zuidema@uva.nl\ngrzegorz@chrupala.me\nAbstract\nSelf-attention weights and their transformed\nvariants have been the main source of informa-\ntion for analyzing token-to-token interactions\nin Transformer-based models. But despite their\nease of interpretation, these weights are not\nfaithful to the models’ decisions as they are\nonly one part of an encoder, and other compo-\nnents in the encoder layer can have consider-\nable impact on information mixing in the out-\nput representations. In this work, by expand-\ning the scope of analysis to the whole encoder\nblock, we propose Value Zeroing, a novel con-\ntext mixing score customized for Transformers\nthat provides us with a deeper understanding\nof how information is mixed at each encoder\nlayer. We demonstrate the superiority of our\ncontext mixing score over other analysis meth-\nods through a series of complementary evalu-\nations with different viewpoints based on lin-\nguistically informed rationales, probing, and\nfaithfulness analysis.1\n1 Introduction\nTransformers (Vaswani et al., 2017), with their im-\npressive empirical success, have become a prime\nchoice of architecture to learn contextualized repre-\nsentations across a wide range of modalities, such\nas language (Devlin et al., 2019; Brown et al.,\n2020), vision (Dosovitskiy et al., 2021), vision-\nlanguage (Radford et al., 2021; Rombach et al.,\n2022), and speech (Baevski et al., 2020), mainly\ndue to their ability to utilize pairwise interactions\nbetween input tokens at every timestep.\nTo better understand the inner dynamics of\nTransformers, we need to trace the information\nflow from the input embeddings up to the output\nrepresentation (including quantifying the degree\nof context mixing, which we will define below).\nThe attention weights from the multi-head atten-\ntion mechanisms offer a straightforward starting\n1Code is freely available at https://github.com/\nhmohebbi/ValueZeroing\npoint for understanding this flow, and these weights\n(‘raw attention’) have been used in many studies\n(Clark et al., 2019; Kovaleva et al., 2019; Reif et al.,\n2019; Htut et al., 2019a, inter alia). However, the\nreliability and usefulness of raw attention weights\nhas also been questioned (Jain and Wallace, 2019;\nBibal et al., 2022). In particular, attention weights\ntend to concentrate on uninformative tokens in the\ncontext (V oita et al., 2018; Clark et al., 2019), and\nremoving or altering them may lead to the same\nand sometimes even better model performance on\ndownstream tasks (Jain and Wallace, 2019; Toneva\nand Wehbe, 2019; Hassid et al., 2022). These find-\nings suggest that Transformers do not solely rely on\nself-attention, and other components in the encoder\nblock play an essential role in information mixing.\nA number of methods have been proposed to\ncompute some form of ‘effective attention weights’,\nwith the goal of more faithfully tracing the relative\ncontributions of different input tokens at various\nlayers of the Transformer (as we will discuss in\nSection 2). These methods show an improvement\nover raw attention, but still ignore key components\nof the Transformer encoder block. This is a partic-\nularly crucial shortcoming, given that most of the\nparameter budget in a Transformer encoder is spent\non position-wise feed-forward networks outside\nof the self-attention component, which can have a\nconsiderable impact on the degree of information\nmixing in the output representations.\nIn this paper, we focus on context mixing: the\nproperty of Transformers that in each node, at each\nlayer, information from the context can be incor-\nporated into the representation of the target token.\nWe propose Value Zeroing, a novel approach to\nquantify the contribution each context token has in\ndetermining the final representation of a target to-\nken, at each layer of a Transformer. Value Zeroing\nis based on the Explaining-by-Removing intuition\n(Covert et al., 2021) shared by many posthoc in-\nterpretability methods, but it takes advantage of\n3378\na specific feature of Transformers: it zeroes only\nthe value vector of a token twhen computing its\nimportance, but leaves the key and query vectors\n(and thus the pattern of information flow) intact.\nBased on extensive experiments and three com-\nplementary approaches to evaluation, we demon-\nstrate that importance scores we can obtain with\nValue Zeroing provide better interpretations than\nother analysis methods.\nFirstly, we use a set of grammatical agreement\ntasks from the BLiMP corpus (Warstadt et al.,\n2020) as a case study. Transformer-based mod-\nels do extremely well on the task of distinguishing\ngrammatical from ungrammatical sentences, and\nthe BLiMP corpus provides information on the cue\nwords that determine the difference. We find that\nValue Zeroing, unlike earlier approaches, indeed re-\nveals that Transformers make use of relevant cues.\nSecondly, we use information-theoretic probing\n(V oita and Titov, 2020; Pimentel et al., 2020) as an\nindependent approach to track information flow in\nTransformer networks. The scores we obtain with\nValue Zeroingturn out to be highly correlated with\nlayer-wise probing performance; that is, probing\naccuracy is higher in layers where relevant tokens\nare more effectively utilized by the model.\nThirdly, we assess the faithfulness of our method\n(Jacovi and Goldberg, 2020); compared to alterna-\ntive analysis methods, we show that Value Zeroing\nis not only more plausible and human-interpretable,\nbut also more faithful to models’ decisions.\n2 Related Work\nWhile numerous studies have leveraged the weights\nassigned by the self-attention mechanism to gain\nintuition about the information mixing process in\nTransformers (Clark et al., 2019; Kovaleva et al.,\n2019; Reif et al., 2019; Lin et al., 2019; Htut et al.,\n2019b; Raganato and Tiedemann, 2018), it is still\na matter of debate whether attention weights are\nsuitable for interpreting the model (see Bibal et al.\n(2022)’s study for a full discussion). Thus several\npost-processing interpretability techniques have\nbeen proposed to convert these weights into scores\nthat provide a more detailed interpretation of the\ninner workings of Transformers. We review the\nmain approaches below.\nAbnar and Zuidema (2020) propose the\nattention-flow and attention-rollout methods to ap-\nproximate information flow in Transformers based\non raw attention weights. The former treats raw\nattention weight matrices as a flow network and\nreturns the maximum flow through each input to-\nken. The latter recursively multiplies the attention\nweight matrix at each layer by the preceding ones.\nThere is, however, an unjustified assumption in\nthe formulation of these methods that both multi-\nhead attention and residual connections contribute\nequally to the computation of the output.\nKobayashi et al. (2020) propose a method that\nincorporates the norm of the transformed value vec-\ntors and report a negative correlation between these\nnorms and raw attention weights on frequent to-\nkens, which partially explains the insufficiency of\nraw attention weights for context mixing estima-\ntion. Kobayashi et al. (2021) extend this method\nto the whole self-attention block by incorporating\nResidual connections (RES) and Layer Normaliza-\ntion (LN) (two components with significant impact\non both model performance and training conver-\ngence (Parisotto et al., 2020; Liu et al., 2020)), but\ndemonstrate that RES and LN components largely\ncancel out the mixing process. Kobayashi et al.\n(2021)’s method, however, ignores the effect of the\nsecond sublayer in a Transformer’s encoder.\nBrunner et al. (2020) and Pascual et al. (2021)\nemploy a gradient-based approach for analyzing\nthe interaction of input representations, but the gra-\ndient measures the sensitivity between two vectors\nand ignores the impact of the input vector. In our\nexperiments we show that despite their relative suc-\ncess in explaining model decisions, gradient-based\napproaches are not suitable for layer-wise analysis.\nMore recently, the effectiveness of combin-\ning these approaches has also been investigated.\nModarressi et al. (2022) propose a method that\nuses Kobayashi et al. (2021)’s scores and incorpo-\nrates the effects of the second layer normalization;\nthey aggregate those scores using rollout (Abnar\nand Zuidema, 2020) to provide global token attri-\nbutions. In the same vein, Ferrando et al. (2022)\nuse rollout to aggregate a variant of Kobayashi et al.\n(2021)’s scores: instead of relying on the Euclidean\nnorm of the transformed vector, they measure Man-\nhattan distance of each transformed vector to the\ncontext vector outputted from self-attention block.\nIn both studies, however, the fact that these con-\ntext vectors might undergo significant changes after\npassing through the second sublayer in the encoder\nlayer is not taken into account. We will show (in\nSection 7) that even at a global level, our scores\nprovide better interpretation than prior methods.\n3379\n3 Our Proposed Method\nTo remedy for the limited scope of the existing\nmethods, we introduce a new context mixing score\nthat takes into account all components in a Trans-\nformer encoder block.\n3.1 Background and Notation\nIn this section, we set up the notation and briefly\nreview the internal structure of an encoder layer in\nthe Transformer architecture.\nEach Transformer encoder layer is composed of\ntwo sublayers: a multi-head self-attention mecha-\nnism (MHA) and a position-wise fully connected\nfeed-forward network (FFN), followed by a Resid-\nual connection ( RES) and Layer Normalization\n(LN) around each of these two sublayers. This\nencoder layer produces the next contextualized rep-\nresentations (˜x1,..., ˜xn) for each token in the con-\ntext, using the output representations from the pre-\nvious layer (x1,..., xn).\nMHA. For each head h∈{1,...,H }in the self-\nattention module, each input vector xi is trans-\nformed into a query qh\ni , a key kh\ni , and a value vh\ni\nvector via separate trainable linear transformations:\nqh\ni = xiWh\nQ + bh\nQ (1)\nkh\ni = xiWh\nK + bh\nK (2)\nvh\ni = xiWh\nV + bh\nV (3)\nThe context vector zh\ni for the ith token of each\nattention head is then generated as a weighted sum\nover the transformed value vectors:\nzh\ni =\nn∑\nj=1\nαh\ni,jvh\nj (4)\nwhere αh\ni,j is the raw attention weight assigned\nto the jth token, and computed as a softmax-\nnormalized dot product between the corresponding\nquery and key vectors:\nαi,j = softmax\nxj ∈X\n(\nqik⊤\nj√\nd\n)\n∈R (5)\nNext, the context vector ( zi ∈Rd) for the ith to-\nken is computed by concatenating all the heads’\noutputs followed by a head-mixing WO projection\nand layer normalization:\nzi = CONCAT (z1\ni ,..., zH\ni )WO (6)\nzi = LNMHA(zi + xi) (7)\nFFN. Each encoder layer also includes two linear\ntransformations with a ReLU activation in between,\nwhich is applied to every zi separately and identi-\ncally to produce output token representations ˜xi:\n˜xi = max(0,ziW1 + b1)W2 + b2 (8)\n˜xi = LNFFN(˜xi + zi) (9)\n3.2 Value Zeroing\nWe aim to measure how much a token uses other\ncontext tokens to build its output representation\n˜xi at each encoder layer. To this end, we treat\nthe self-attention mechanism as a fuzzy hash-table,\nwhere we look up the sum of values weighted by\nthe query-key match in the context. Thus in Eq. 4\nwe replace a value vector associated with token\nj with a zero vector vh\nj ←0,∀h∈H, where the\ncontext vector for the ith token is being computed.\nThis provides an alternative output representation\n˜x¬j\ni for the ith token that has excluded token j in\nthe mixing process. By comparing the alternative\noutput representation ˜x¬j\ni with the original ˜xi, we\ncan measure how much the output representation\nis affected by the exclusion of the jth token:\nCi,j = ˜x¬j\ni ∗˜xi (10)\nwhere the operation ∗can be any pairwise distance\nmetric that properly considers the characteristics\nof the model’s representation space. We opted\nfor cosine distance throughout our experiments as\nits superiority over other dissimilarity metrics has\nbeen supported for textual deep learning models\n(Yokoi et al., 2020; Hanawa et al., 2021).2 Com-\nputing Eq. 10 for all ˜xi in a given context provides\nus with a Value Zeroingmatrix score C where the\nvalue of the cell Ci,j ∈R (ith row, jth column) in\nthe map denotes the degree to which the ith token\ndepends on the jth token to form its contextualized\nrepresentation.\nNote that unlike generic perturbation approaches,\nour proposed method does not remove the token\nrepresentations xi from the input of an encoder. We\nargue that ablating input token representations can-\nnot be a reliable basis to understand context mix-\ning process since any changes in the input vectors\nwill lead to changes in the query and key vectors\n(Eq. 1 and 2), resulting in a change in the attention\ndistribution (cf. Eq. 5). Consequently, there will\n2More details on the choice of distance metric is discussed\nin Appendix A.1.\n3380\nPhenomenon UID Example Target word Foil word\nAnaphor Number Agreement anaManyteenagers were helping [MASK]. themselves herself\nDeterminer-Noun Agreementdna Jeffrey has not passed [MASK]museums. these this\ndnaa Sara noticed [MASK] whitehospitals. these this\nSubject-Verb Agreement darn The pictures of Martha [MASK] not disgust Anne. do does\nrpsv Kristen [MASK] fixed this chair. has have\nTable 1: Examples of the selected tasks with our annotations from the BLiMP benchmark (UIDs are unique\nidentifiers used in BLiMP). Cue words are underlined.\nbe a discrepancy between the alternative attention\nweights and those for the original context. Instead,\nour method only nullifies the value vector of a spe-\ncific token representation. In this way, the token\nrepresentation can maintain its identity within the\nencoder layer, but it does not contribute to form-\ning other token representations. Moreover, since\nour Value Zeroingis computed from the encoder’s\nlayer outputs, it incorporates all the components in-\nside an encoder layer such as multi-head attention,\nresidual connection, layer normalization, and also\nfeed-forward networks, resulting in a more reliable\ncontext mixing score than previous methods.\n4 Experimental Setup\n4.1 Data\nWe used the BLiMP benchmark (Warstadt et al.,\n2020) which contains a set of pairs of minimally\ndifferent sentences that contrast in grammatical ac-\nceptability under a specific linguistic phenomenon.\nThe benchmark isolates linguistic phenomena such\nthat only one word determines the true label of\neach sentence. We refer to this crucial context to-\nken as the cue word. The nature of this task makes\nit especially suitable for evaluating context mix-\ning scores, since it gives us a strong hypothesis on\nwhich context token is the most relevant for the\nrepresentation of the masked target word.\nFrom this benchmark, we select five datasets\nwith three different linguistic phenomena for which\nPre-trained Language Models (PLMs) have shown\nhigh accuracy to ensure that the model captures the\nrelevant information. We expand contractions such\nas doesn’t →does not) and generate dependency\ntrees using SpaCy (Honnibal and Montani, 2017)\nto extract and annotate the position of target and\ncue words in a sentence. In Table 1, we provide\nan example of each phenomenon in the benchmark\ntogether with our automated annotations. We accu-\nmulate examples from the five selected tasks as a\nunified dataset for grammatical agreement, result-\ning in 4,276 data points, and divide them equally\ninto Train and Test sets. The Train set is only used\nfor the fine-tuning phase; the Test set is used for all\nevaluation experiments.\n4.2 Target Model\nWe conduct our experiments on three Transformer-\nbased language models: BERT (uncased, Devlin\net al., 2019), RoBERTa (Liu et al., 2019) and\nELECTRA (Clark et al., 2020). 3 The results for\nthe latter two are reported in Appendix A.3. By\nreplacing the target words with the [MASK] token,\nwe perform a Masked Language Modeling (MLM)\ntask using the model’s pre-trained MLM head. For\ninstance, in the Subject-Verb Agreement example\n“The pictures of Martha do not disgust Anne. ”, we\nreplace the verb ‘do’ with the [MASK] token and\nfeed the example to the model.\nWe perform our experiments on both pre-trained\nand fine-tuned versions of each model. Including\na fine-tuned model in our analysis study gives us\na complementary insight into the importance of\nthe cue words, since fine-tuning allows the model\nto concentrate on the most helpful words for the\ndownstream task of choice (i.e., agreement) and\nmakes sure that target word representations take the\ncue word into account. We use prompt fine-tuning\n(Schick and Schütze, 2021a,b; Karimi Mahabadi\net al., 2022) and compute Cross Entropy loss only\nover the output logits corresponding to the target\nand foil classes. Accuracy is 0.96 for pre-trained\nand 0.99 for fine-tuned BERT.\n4.3 Baselines\nHere we describe the existing context mixing meth-\nods which we include in our experiments. For each\nmethod, we select themth row of its context mixing\nmap where mis the position of the [MASK] token,\nresulting in a 1-D array of scores for each context\n3Base, with 12 layers and 12 attention heads, obtained\nfrom the Transformers library (Wolf et al., 2020).\n3381\ntoken. We normalize the scores for all tokens in\na sentence so that they are all positive values and\nsum to one. For completeness, we also include\na few gradient-based attribution methods in our\ncomparisons.\nRand: random scores generated from a uniform\ndistribution for each sentence in the dataset.\nAttn: raw attention scores αm from Eq. 5.\nAttn-rollout: An aggregation method for approx-\nimating the attention flow based on raw attention\nweights (Abnar and Zuidema, 2020).\nAttn-Norm: norm-based method of Kobayashi\net al. (2020) that also incorporates the norm of\nthe transformed input vectors to compute context\nmixing scores.\nAttn-Norm + RES + LN: the extended norm-\nbased method of Kobayashi et al. (2021) in which\nthey also incorporates Residual connection (RES)\nand Layer Normalization (LN) located only in the\nfirst sublayer of a Transformer’s encoder.\nGlobEnc & ALTI: Two global token attribution\nmethods proposed by Modarressi et al. (2022) and\nFerrando et al. (2022). For a fair comparison,\nwe exclude the aggregation through rollout in our\nfirst two evaluations (referred to as GlobEnc¬and\nALTI¬), because we are interested in assessing con-\ntext mixing scores at each layer separately. In our\nthird evaluation, we compare these methods with\nothers at a global level and use rollout.\nGradXinput, IG & DL: feature attribution scores\nthat also make use of top-down information\nfrom the classification layer on top of the Trans-\nformer. We consider three popular variants of\ngradient-based attribution scores: Gradient×Input\n(GradXinput) (Samek et al., 2019; Yuan et al.,\n2019), Integrated Gradients ( IG) (Sundararajan\net al., 2017), and DeepLift (DL) (Shrikumar et al.,\n2017). For gradient-based methods, we use the\npre-trained MLM head which has been trained dur-\ning the pre-training of the BERT to compute the\ngradient of the true label with respect to the token\nrepresentations at each layer.\n5 Evaluation 1: Cue Alignment\nAs cue words are the only indicators of the true la-\nbels in our dataset, we expect that when the model\nperforms well, it overwhelmingly depends on these\nwords to form the representation of a [MASK] to-\nken in a given context. To quantify the alignment\nbetween a context mixing score and the cue word,\nwe first define a binary cue vector ξaccording to\nthe following condition:\nξi =\n{\n1, the ith token ∈Cue words\n0, otherwise (11)\nThen we compare the cue vector and the prediction\nof a context mixing score Sin two different ways:\nDot Product. We quantify cue alignment asS·ξ,\nwhich measures the total score mass the model\nassigns to cue words to form the representation of\nthe target token.\nAverage Precision. We quantify cue alignment\nas the average precision between the two vectors,\nwhich is a weighted mean of precision at each recall\nlevel:\nAP =\n∑\nn\n(Rn −Rn−1)Pn (12)\nwhere Pn and Rn are the precision and recall at the\nnth threshold. This metric relies on the ranking of\ntokens rather than the magnitude of their weights.4\nFigures 1 and 2 show the alignment between\nthe cue vector and different analysis methods us-\ning dot product and average precision for the pre-\ntrained and fine-tuned model, respectively. In com-\nparison with the other context mixing methods,\nValue Zeroingshows a higher degree of the target\nmodel incorporating cue words into the representa-\ntion of the [MASK] token across all layers.\nAs can be seen from the first two columns in\nall graphs, raw self-attention weights ( Attn) al-\nways perform worse than even random scores in\nhighlighting cue words. This is in line with pre-\nvious studies showing that raw attention weights\noften pay attention to uninformative tokens (V oita\net al., 2018; Clark et al., 2019) and do not reflect\nthe appropriate context (Kim et al., 2019). How-\never, we can see a significant improvement in the\nresults for Attn-norm where the norm of trans-\nformed value vectors are also taken into account,\nconfirming that value vectors play an essential role\nin the context mixing process. The method Attn-\nnorm + RES + LN, which expands Attn-norm to\nthe whole self-attention block by adding Residual\n4We also employed Probes-needed (Zhong et al., 2019)\nmetric in our evaluation which intuitively counts the number\nof non-cue tokens we need to probe to find cue words based on\na given score. As its motivation is similar to Average Precision\nand the results show the same pattern, we relegate the results\nwith this metric to the Appendix A.2.\n3382\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.12 0.06 0.11 0.07 0.04 0.06 0.04 0.19 0.12 0.19 0.10\n0.12 0.08 0.13 0.08 0.04 0.07 0.06 0.18 0.12 0.18 0.14\n0.12 0.09 0.15 0.08 0.05 0.07 0.07 0.17 0.09 0.16 0.19\n0.12 0.10 0.18 0.10 0.07 0.08 0.07 0.16 0.08 0.14 0.21\n0.12 0.10 0.19 0.10 0.08 0.09 0.08 0.15 0.06 0.13 0.24\n0.12 0.08 0.16 0.08 0.07 0.07 0.06 0.13 0.07 0.12 0.17\n0.12 0.09 0.17 0.09 0.07 0.08 0.06 0.14 0.09 0.11 0.17\n0.12 0.12 0.19 0.09 0.09 0.08 0.07 0.12 0.08 0.10 0.21\n0.12 0.12 0.22 0.10 0.09 0.09 0.06 0.10 0.06 0.09 0.28\n0.12 0.09 0.18 0.06 0.06 0.05 0.04 0.09 0.05 0.07 0.21\n0.13 0.09 0.19 0.06 0.06 0.05 0.03 0.05 0.03 0.03 0.26\n0.13 0.04 0.17 0.03 0.02 0.04 0.02 0.00 0.00 0.00 0.21\nDot Product\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.32 0.16 0.34 0.23 0.22 0.22 0.22 0.54 0.29 0.56 0.32\n0.32 0.18 0.39 0.25 0.24 0.24 0.26 0.52 0.26 0.53 0.36\n0.33 0.22 0.39 0.25 0.24 0.25 0.26 0.49 0.23 0.51 0.37\n0.31 0.21 0.51 0.31 0.31 0.31 0.29 0.45 0.20 0.36 0.48\n0.33 0.27 0.53 0.33 0.33 0.33 0.32 0.40 0.18 0.30 0.54\n0.32 0.25 0.41 0.28 0.28 0.28 0.29 0.34 0.18 0.27 0.41\n0.32 0.27 0.43 0.29 0.29 0.29 0.29 0.34 0.19 0.25 0.43\n0.32 0.29 0.46 0.30 0.30 0.30 0.30 0.28 0.19 0.23 0.47\n0.32 0.32 0.57 0.34 0.34 0.34 0.33 0.26 0.17 0.22 0.57\n0.31 0.28 0.45 0.29 0.29 0.29 0.29 0.29 0.17 0.21 0.45\n0.33 0.29 0.54 0.32 0.31 0.32 0.33 0.23 0.16 0.18 0.54\n0.33 0.24 0.43 0.27 0.27 0.27 0.28 0.12 0.12 0.12 0.44\nAverage Precision\nFigure 1: Layer-wise alignment between the cue vector and different analysis methods averaged over Test set\nexamples for the pre-trained model. Higher value (darker color) is better.\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.12 0.06 0.11 0.07 0.04 0.06 0.04 0.18 0.12 0.18 0.10\n0.12 0.08 0.13 0.08 0.04 0.07 0.06 0.18 0.12 0.17 0.14\n0.12 0.08 0.15 0.08 0.05 0.07 0.06 0.17 0.09 0.16 0.19\n0.12 0.09 0.18 0.10 0.07 0.08 0.07 0.16 0.08 0.14 0.21\n0.12 0.09 0.19 0.10 0.08 0.09 0.07 0.15 0.06 0.13 0.25\n0.12 0.09 0.16 0.09 0.07 0.08 0.06 0.13 0.08 0.12 0.18\n0.12 0.10 0.18 0.09 0.08 0.09 0.06 0.14 0.10 0.12 0.19\n0.12 0.13 0.20 0.10 0.09 0.09 0.07 0.13 0.11 0.11 0.24\n0.12 0.13 0.25 0.12 0.11 0.11 0.08 0.10 0.07 0.09 0.35\n0.12 0.09 0.18 0.06 0.06 0.05 0.04 0.09 0.05 0.06 0.21\n0.13 0.07 0.19 0.05 0.05 0.04 0.03 0.05 0.03 0.04 0.25\n0.13 0.04 0.17 0.03 0.02 0.04 0.02 0.00 0.00 0.00 0.22\nDot Product\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.32 0.16 0.34 0.23 0.23 0.23 0.22 0.53 0.29 0.55 0.33\n0.32 0.18 0.39 0.25 0.24 0.24 0.26 0.51 0.27 0.54 0.36\n0.33 0.22 0.38 0.25 0.24 0.25 0.26 0.49 0.26 0.52 0.37\n0.31 0.21 0.50 0.31 0.31 0.31 0.29 0.45 0.23 0.35 0.48\n0.33 0.28 0.56 0.34 0.34 0.34 0.33 0.41 0.20 0.30 0.58\n0.32 0.25 0.42 0.28 0.28 0.28 0.29 0.34 0.20 0.28 0.43\n0.32 0.29 0.47 0.30 0.30 0.30 0.31 0.33 0.26 0.26 0.46\n0.32 0.30 0.52 0.32 0.32 0.32 0.32 0.30 0.25 0.25 0.53\n0.32 0.35 0.68 0.39 0.39 0.39 0.37 0.25 0.19 0.23 0.68\n0.31 0.29 0.45 0.29 0.29 0.29 0.30 0.31 0.18 0.22 0.45\n0.33 0.28 0.53 0.32 0.32 0.32 0.34 0.25 0.19 0.18 0.53\n0.33 0.24 0.45 0.28 0.28 0.29 0.28 0.12 0.12 0.12 0.46\nAverage Precision\nFigure 2: Layer-wise alignment between the cue vector and different analysis methods averaged over Test set\nexamples for the fine-tuned model. Higher value (darker color) is better.\nconnection and Layer Normalization, would seem\nto show that the model is incapable of utilizing the\ncue words. However, incorporating also the second\npart of the encoder layer via our method shows the\nmodel does indeed use the cue words. We also find\nthat GlobEnc and ALTI benefit from the rollout\naggregation method to provide a global view, but\nthey do not seem to provide good layerwise scores\n(without rollout).\nThe gradient-based scores, in contrast to the\nother methods, highlight the cue words only in\nthe earlier layers of the model. In the next section,\nby using a layer-wise probing experiment, we will\nshow that these scores are not reliable for identify-\ning the relevant context in individual layers.\n6 Evaluation 2: Context Mixing versus\nProbing\nIn this section, we investigate the relationship be-\ntween cue word alignment and probing perfor-\nmance across layers. We hypothesize that if a layer\naligns better with the cue word according to a reli-\nable context mixing score, then the representation\nof the masked token on that layer can be used more\neffectively by a probing classifier to decode number\nagreement with the cue word.\nTo verify our hypothesis, we obtain the represen-\ntation of masked tokens in test examples across all\nlayers. Since all examples in our dataset share\nthe same number agreement property, we asso-\n3383\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n1.6\n2\n2.4\n2.8compression\nfinetuned\nFalse\nTrue\nFigure 3: Layer-wise compression of probing classifiers\nusing pre-trained and fine-tuned representations.\nciate each masked representation with a Singular\nor Plural label. Next, we perform an information-\ntheoretic probing analysis using Minimum De-\nscription Length (MDL) to measures the degree to\nwhich representations encode number agreement.\nWe chose MDL as our probe since it is theoreti-\ncally justified and has been shown to provide more\nreliable results than conventional probes (V oita and\nTitov, 2020; Fayyaz et al., 2021).\nTo compute MDL, we employed the online cod-\ning of V oita and Titov (2020). Since MDL can be\naffected by the number of data points (N), we mea-\nsure compression as our evaluation metric which is\ndefined as follows:\nCompression = N ·log2(K)\nMDL (13)\nwhere Krefers to the number of classes (2 in our\ncase). This metric is equal to 1 (no compression)\nfor a random guessing classifier. A higher value for\nCompression indicates more accurate label predic-\ntion for the probing classifier.\nFigure 3 reports compression of probing classi-\nfiers based on representations obtained from both\npre-trained and fine-tuned models across all lay-\ners. We also include the results for the embedding\nlayer of the model (layer 0) which can serve as a\nnon-contextualized baseline. We can see a jump\nin probing performance at layers 4 (1.52 →1.72)\nand 9 (2.03 →2.45) in the fine-tuned setup, the\nsame layers for which we found a higher alignment\nwith cue words in Figures 1 and 2.\nTable 2 presents the correlation between layer-\nwise Compression scores and layer-wise cue align-\nment scores from Section 5 for different analysis\nmethods. As we can see, alignment according to\nValue Zeroingis highly positively correlated with\nthe probing performance. This suggests that when\nValue Zeroing indicates that the model uses cue\nwords to form representations of the masked to-\nkens in a particular layer, these representations are\nin fact better at encoding number agreement.\nMethod ρPT ρFT\nRand -0.07 -0.02\nAttn 0.10 0.08\nAttn-norm 0.52 0.56\nAttn-norm + RES -0.35 -0.24\nAttn-norm + RES + LN 0.12 0.17\nGlobEnc¬ -0.01 -0.12\nALTI¬ -0.01 -0.12\nGradXinput -0.96 -0.99\nIG -0.86 -0.77\nDL -0.97 -1.00\nValue Zeroing 0.65 0.64\nTable 2: Spearman’s ρcorrelation between layer-wise\nprobing performance (Comp.) and layer-wise cue align-\nments based on Dot Product. PT and FT refer to pre-\ntrained and fine-tuned conditions, respectively.\nRecall that based on Figures 1 and 2, according\nto the gradient-based methods the masked tokens\npay more attention to the cue words only in earlier\nlayers. However, we can see a highly negative cor-\nrelation with probing results for these scores. Due\nto the nature of the task the probing score goes up\nmonotonically along the layers. At the same time,\nthe gradient attribution score goes up monotoni-\ncally as you get closer to the bottom embedding\nlayers, suggesting that gradient-based methods are\nunreliable for layer-wise analysis and identifying\nimportant tokens in the context mixing process.\n7 Evaluation 3: Faithfulness Analysis\nOur experimental results in Sections 5 and 6 show\nthat the Value Zeroing score matches our prior\nlinguistically-informed expectations. However, it\nis not always clear whether aplausible context mix-\ning score that matches human expectations is also\nfaithful to the model and reflects its decision mak-\ning process (Herman, 2017; Wiegreffe and Pinter,\n2019; Jacovi and Goldberg, 2020).\nIn this section we employ the notion of input\nablation (Covert et al., 2021) to evaluate the faith-\nfulness of our context mixing score. The influence\nof a target token on a model’s decision is often\nestimated as the drop in the model’s predicted prob-\nability of the correct class after blanking out the\ntarget token from the input. A higher drop for an\nablated token indicates that the token is more in-\nfluential on the model’s decision (DeYoung et al.,\n2020; Abnar and Zuidema, 2020; Atanasova et al.,\n2020; Wang et al., 2022). We use this blank-out\napproach as a base for analyzing and comparing\nthe faithfulness of context mixing scores.\n3384\nMethod ρPT ρFT\nRand -0.01 0.00\nAttn -0.10 -0.07\nAttn-norm 0.19 0.14\nAttn-norm + RES 0.03 -0.05\nAttn-norm + RES + LN -0.08 -0.17\nGlobEnc -0.01 -0.09\nALTI 0.17 0.19\nGradXinput 0.11 0.16\nIG 0.07 0.21\nDL 0.20 0.29\nValue Zeroing 0.26 0.31\nTable 3: Spearman’s ρcorrelation between the blank-\nout scores and different aggregated context mixing and\nattribution scores. PT and FT refer to pre-trained and\nfine-tuned conditions, respectively.\nTo estimate the blank-out scores in BERT, we\ncalculate the probability of its output y using a\nsoftmax function normalized over only the corre-\nsponding logit values of target t and foil words\n(cf. Table 1), and compute blank-out scores for a\ngiven input token ias p(yt|e) −p(yt|e\\ei), where\nei refers to the input embedding of input token\ni. We compare these blank-out scores with con-\ntext mixing scores, aggregated across all layers\nof the model. For gradient-based scores, calculat-\ning them with respect to the tokens in the input\nembedding layer (ℓ = 0) provides us with aggre-\ngated scores since the backpropagation of gradients\npasses through all layers to the beginning of the\nmodel. For other scores, we use the rollout (Abnar\nand Zuidema, 2020) aggregation method.\nTable 3 shows Spearman’s rank correlation be-\ntween the blank-out scores and different aggregated\ncontext mixing scores. The highest correlation for\nour method indicates that Value Zeroing is more\nfaithful in explaining the model behaviour com-\npared to other analysis methods.\nQualitative Analysis. We also take a closer look\nat the aggregated scores for a qualitative compar-\nison. In Figure 4, we illustrate different scores\nobtained from a fine-tuned BERT model for a cor-\nrectly classified example, where the model is asked\nto fill the masked token with one of the verbs were\nor is as target and foil classes, respectively. Ac-\ncording to Value Zeroingscores, the model mainly\nrelies on the main subject ( pictures) as a cue\nword to form a contextualized representation of\nthe [MASK] token, while the word pictures is also\nimportant for the model’s final decision based on\nblank-out: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nAttn: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nAttn-norm: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nAttn-norm+RES:[CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nAttn-norm+RES+LN:[CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nGlobEnc: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nALTI: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nGradXinput: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nIG: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nDL: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nValue Zeroing: [CLS] the pictures of some hat [MASK] scar ##ing marcus . [SEP]\nFigure 4: Most influential tokens on the target represen-\ntation in a fine-tuned BERT model according to different\naggregated context mixing scores compared to blank-\nout scores.\nthe blank-out scores. In this example, the blank-\nout score for the cue word is 0.99, meaning the\nmodel fully loses its confidence in the target class\nwhen the cue word is replaced with an [UNK]\ntoken. Surprisingly, gradient-based methods tend\nto highlight the word hat which is an agreement\nattractor, and attention-based scores tend to focus\non the [CLS] token which has been idle during\nfine-tuning process.\nOverall, our faithfulness evaluation and qualita-\ntive analysis suggest thatValue Zeroingcan explain\nmodel decisions at a global level when it is aggre-\ngated across layers. The context mixing maps per\nlayer are provided in Appendix A.4, where some\nmore meaningful patterns can be found in Value\nZeroing scores (in both layer-wise and aggregated\nsetups) in contrast to other context mixing scores.\n8 Discussion\nAlthough some desiderata such as plausibility (Lei\net al., 2016; Strout et al., 2019) and faithfulness\n(Lakkaraju et al., 2019; Jacovi and Goldberg, 2020)\nare taken into account when developing explana-\ntion and analysis methods, evaluating them is still\na challenge due to lack of a standard ground truth.\nEvaluating context mixing scores, where token-to-\ntoken interactions in a context are also considered,\nis even more challenging. Several studies have used\ngradient-based scores as an anchor of faithfulness,\nand measure how strongly context mixing scores\ncorrelate with them (Jain and Wallace, 2019; Ab-\nnar and Zuidema, 2020; Modarressi et al., 2022).\nHowever, the reliability of gradient-based scores\ncan be questioned, especially when different vari-\nations of them show considerable disagreement\n3385\n(Neely et al., 2022; Pruthi et al., 2022; Krishna\net al., 2022). Thus, we suggest using controlled\ntasks for which we have strong prior expectations\nfor evaluating these methods. In our study, we use\na set of number agreement tasks to provide such\npriors, since the cue words are the only sources of\ninformation in the context for performing well in\nthe task.\nAnother point worth discussing is the concern\nraised by Kobayashi et al. (2021) that BERT tends\nto preserve token representations rather than mix-\ning them at each layer. We argue that their ob-\nservation is due to the context-mixing ratio they\ndefined by comparing the norm of residual effects\nagainst other token representations. In our view,\nthis ratio is dominated by residuals and neglects the\nfact that a token representation carried by residual\nconnections is indeed a contextualized representa-\ntion outputted from previous layers. We keep the\nresiduals intact within the encoder layer by zeroing\nonly the value vectors and focusing on the context\nmixing performed by all tokens.\n9 Conclusion\nIn this paper, we propose Value Zeroing as a novel\napproach for quantifying the information mixing\nprocess in Transformers to address the shortcom-\nings of previous methods. We performed exten-\nsive complementary experiments and showed that\nour method outperforms others in three different\nevaluation setups. Since our approach requires no\nsupervision, it could be an interesting option for\nimproving model efficiency by removing token rep-\nresentations across layers.\n10 Limitations\nAs is the case for most attempts at interpreting\nDeep Learning models, our evaluation of our (and\nothers’) proposed methods are not definite since\nwe have no gold standard of what happens inside\na model, although we try to remedy for that by\nconducting independent and complementary evalu-\nation schemes.\nOur proposed method is customized for deep\nneural models based on the Transformer archi-\ntecture and cannot be easily generalized to other\n(mathematically different) modeling architectures.\nOur evaluations were based on encoder-based mod-\nels, and focused on the Text modality. In the future,\nwe will extend our experiments to more modalities,\nsuch as speech and vision.\nAcknowledgments\nThis publication is part of the projectInDeep: Inter-\npreting Deep Learning Models for Text and Sound\n(with project number NWA.1292.19.399) of Na-\ntional Research Agenda (NW A-ORC) programme.\nFunding by the Dutch Research Council (NWO) is\ngratefully acknowledged.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems,\n33:12449–12460.\nAdrien Bibal, Rémi Cardon, David Alfter, Rodrigo\nWilkens, Xiaoou Wang, Thomas François, and\nPatrick Watrin. 2022. Is attention explanation? an\nintroduction to the debate. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3889–3900, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\n3386\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nIan Covert, Scott M. Lundberg, and Su-In Lee. 2021.\nExplaining by removing: A unified framework for\nmodel explanation. J. Mach. Learn. Res., 22:209:1–\n209:90.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark to\nevaluate rationalized NLP models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4443–4458, Online.\nAssociation for Computational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nMohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Ho-\nsein Mohebbi, and Mohammad Taher Pilehvar. 2021.\nNot all models localize linguistic knowledge in the\nsame place: A layer-wise probing on BERToids’ rep-\nresentations. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , pages 375–388, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-\njussà. 2022. Measuring the mixing of contextual\ninformation in the transformer. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 8698–8714, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nKazuaki Hanawa, Sho Yokoi, Satoshi Hara, and Ken-\ntaro Inui. 2021. Evaluation of similarity-based ex-\nplanations. In International Conference on Learning\nRepresentations.\nMichael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai,\nIvan Montero, Noah A. Smith, and Roy Schwartz.\n2022. How much does attention actually attend?\nquestioning the importance of attention in pretrained\ntransformers. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 , pages 1403–\n1416, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nBernease Herman. 2017. The promise and peril of hu-\nman evaluation for model interpretability. ArXiv,\nabs/1711.07414.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019a. Do attention heads\nin BERT track syntactic dependencies? CoRR,\nabs/1911.12246.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019b. Do attention heads\nin bert track syntactic dependencies? ArXiv,\nabs/1911.12246.\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable NLP systems: How should we\ndefine and evaluate faithfulness? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4198–4205, On-\nline. Association for Computational Linguistics.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nRabeeh Karimi Mahabadi, Luke Zettlemoyer, James\nHenderson, Lambert Mathias, Marzieh Saeidi,\nVeselin Stoyanov, and Majid Yazdani. 2022. Prompt-\nfree and efficient few-shot learning with language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3638–3652, Dublin,\nIreland. Association for Computational Linguistics.\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019.\nWhen and why is document-level context useful in\nneural machine translation? In Proceedings of the\nFourth Workshop on Discourse in Machine Trans-\nlation (DiscoMT 2019), pages 24–34, Hong Kong,\nChina. Association for Computational Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\n3387\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4547–4568, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for Com-\nputational Linguistics.\nSatyapriya Krishna, Tessa Han, Alex Gu, Javin Pom-\nbra, Shahin Jabbari, Steven Wu, and Himabindu\nLakkaraju. 2022. The disagreement problem in ex-\nplainable machine learning: A practitioner’s perspec-\ntive. ArXiv, abs/2202.01602.\nHimabindu Lakkaraju, Ece Kamar, Rich Caruana, and\nJure Leskovec. 2019. Faithful and customizable ex-\nplanations of black box models. In Proceedings of\nthe 2019 AAAI/ACM Conference on AI, Ethics, and\nSociety, AIES ’19, page 131–138, New York, NY ,\nUSA. Association for Computing Machinery.\nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\nRationalizing neural predictions. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 107–117, Austin,\nTexas. Association for Computational Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253, Florence, Italy.\nAssociation for Computational Linguistics.\nFenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun,\nand Yuexian Zou. 2020. Rethinking skip connection\nwith layer normalization. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 3586–3598, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nAli Modarressi, Mohsen Fayyaz, Yadollah\nYaghoobzadeh, and Mohammad Taher Pile-\nhvar. 2022. GlobEnc: Quantifying global token\nattribution by incorporating the whole encoder\nlayer in transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 258–271, Seattle,\nUnited States. Association for Computational\nLinguistics.\nMichael Neely, Stefan F. Schouten, Maurits Bleeker,\nand Ana Lucic. 2022. A song of (dis)agreement:\nEvaluating the evaluation of explainable artificial in-\ntelligence in natural language processing. In HHAI.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max\nJaderberg, Raphaël Lopez Kaufman, Aidan Clark,\nSeb Noury, Matthew Botvinick, Nicolas Heess, and\nRaia Hadsell. 2020. Stabilizing transformers for rein-\nforcement learning. In Proceedings of the 37th Inter-\nnational Conference on Machine Learning, volume\n119 of Proceedings of Machine Learning Research,\npages 7487–7498. PMLR.\nDamian Pascual, Gino Brunner, and Roger Wattenhofer.\n2021. Telling BERT’s full story: from local attention\nto global aggregation. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 105–124, Online. Association for Computa-\ntional Linguistics.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nDanish Pruthi, Bhuwan Dhingra, Livio Baldini Soares,\nMichael Collins, Zachary Chase Lipton, Graham\nNeubig, and William W. Cohen. 2022. Evaluating\nexplanations: How much do explanations from the\nteacher aid students? Transactions of the Association\nfor Computational Linguistics, 10:359–375.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In ICML.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, pages 8594–8603.\nRobin Rombach, A. Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\n3388\nresolution image synthesis with latent diffusion mod-\nels. 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 10674–\n10685.\nWojciech Samek, Grégoire Montavon, Andrea Vedaldi,\nLars Kai Hansen, and Klaus-Robert Müller. 2019.\nExplainable AI: interpreting, explaining and visualiz-\ning deep learning, volume 11700. Springer Nature.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In Proceedings\nof the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 3145–3153.\nJMLR.org.\nJulia Strout, Ye Zhang, and Raymond Mooney. 2019.\nDo human rationales improve machine explanations?\nIn Proceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 56–62, Florence, Italy. As-\nsociation for Computational Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 3319–3328.\nJMLR.org.\nWilliam Timkey and Marten van Schijndel. 2021. All\nbark and no bite: Rogue dimensions in transformer\nlanguage models obscure representational quality.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4527–4546, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMariya Toneva and Leila Wehbe. 2019. Interpreting and\nimproving natural-language processing (in machines)\nwith natural language-processing (in the brain). In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Association\nfor Computational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nLijie Wang, Yaozong Shen, Shu ping Peng, Shuai Zhang,\nXinyan Xiao, Hao Liu, Hongxuan Tang, Ying Chen,\nHua Wu, and Haifeng Wang. 2022. A fine-grained\ninterpretability evaluation benchmark for neural nlp.\nArXiv, abs/2205.11097.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020, pages\n409–410, New York, New York. Association for Com-\nputational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki,\nand Kentaro Inui. 2020. Word rotator’s distance. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2944–2960, Online. Association for Computa-\ntional Linguistics.\nHao Yuan, Yongjun Chen, Xia Hu, and Shuiwang Ji.\n2019. Interpreting deep models for text analysis via\noptimization and regularization methods. In Pro-\nceedings of the Thirty-Third AAAI Conference on\nArtificial Intelligence and Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference and\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, AAAI’19/IAAI’19/EAAI’19.\nAAAI Press.\n3389\nRuiqi Zhong, Steven Shao, and Kathleen McKeown.\n2019. Fine-grained sentiment analysis with faithful\nattention. ArXiv, abs/1908.06870.\nA Appendices\nA.1 On the choice of distance function\nThe purpose of this section is to inspect the impact\nof selecting different distance metrics when com-\nputing Value Zeroing in Eq. 10. Timkey and van\nSchijndel (2021) questioned the informativity of\nstandard representational distance measures such\nas cosine and Euclidean by observing that only a\nsmall subset of rogue dimensions contribute to the\nanisotropy of a contextualized representation space.\nThey proposed using simple post-processing tech-\nniques to correct for such these rough dimensions.\nWe followed their suggestion and normalized the\nrepresentations before computing distances, but we\ndid not observe any noticeable difference in our\nscores compared to using non-normalized repre-\nsentations (Fig. A.1). We also repeated our ex-\nperiment with Spearman’s and Euclidean distance\nmetrics and observed the same pattern in the results\n(Fig. A.1).\nWe believe that in anisotropy studies that use\nclustering methods, the choice of distance metrics\nis crucial. However, we compute each token’s dis-\ntance from itself (not from other tokens) and com-\npare them relatively. This might explain why we\nobserve the same pattern for different distance met-\nrics.\nA.2 More metrics\nFigure A.2 reports the cue alignment evaluation\nfor BERT model based on Probes-needed (Zhong\net al., 2019) metric.\nA.3 More PLMs\nWe replicated our experiment for the cue alignment\nfor two more PLMs; RoBERTa (Liu et al., 2019)\nand ELECTRA (generator, Clark et al., 2020). As\nwe can see in Figures A.3 and A.4, our method\nconsistently outperforms other methods on all mod-\nels in both pre-trained and fine-tuned setups. Due\nto the fact that our scores are based on zeroing\nvalue vectors, our method can be easily applied to\nany Transformer-based models even with different\nmodalities.\nVZ (cosine)\nVZ (cosine, normalized)\nVZ (spearmanr)\nVZ (spearmanr, normalized)\nVZ (euclidean)\nVZ (euclidean, normalized)\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.10 0.10 0.10 0.10 0.10 0.10\n0.14 0.15 0.14 0.14 0.13 0.13\n0.19 0.19 0.19 0.19 0.15 0.15\n0.21 0.21 0.21 0.21 0.18 0.18\n0.24 0.24 0.23 0.23 0.19 0.19\n0.17 0.17 0.17 0.17 0.16 0.16\n0.17 0.17 0.17 0.17 0.17 0.17\n0.21 0.21 0.21 0.21 0.19 0.19\n0.28 0.28 0.28 0.28 0.22 0.22\n0.21 0.21 0.21 0.21 0.18 0.18\n0.26 0.26 0.26 0.26 0.19 0.19\n0.21 0.21 0.21 0.21 0.17 0.17\nDot Product\nVZ (cosine)\nVZ (cosine, normalized)\nVZ (spearmanr)\nVZ (spearmanr, normalized)\nVZ (euclidean)\nVZ (euclidean, normalized)\n1 2 3 4 5 6 7 8 9 10 11 12\n0.32 0.33 0.33 0.32 0.32 0.33\n0.36 0.37 0.36 0.37 0.36 0.37\n0.37 0.37 0.38 0.37 0.37 0.37\n0.48 0.48 0.48 0.48 0.48 0.48\n0.54 0.54 0.54 0.54 0.54 0.54\n0.41 0.41 0.41 0.41 0.41 0.41\n0.43 0.42 0.42 0.42 0.43 0.42\n0.47 0.47 0.47 0.47 0.47 0.47\n0.57 0.57 0.57 0.57 0.57 0.57\n0.45 0.45 0.46 0.46 0.45 0.45\n0.54 0.54 0.54 0.54 0.54 0.54\n0.44 0.44 0.44 0.44 0.44 0.44\nAverage Precision\n(a) Pre-trained BERT\nVZ (cosine)\nVZ (cosine, normalized)\nVZ (spearmanr)\nVZ (spearmanr, normalized)\nVZ (euclidean)\nVZ (euclidean, normalized)\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.10 0.10 0.10 0.10 0.11 0.11\n0.14 0.15 0.14 0.14 0.13 0.13\n0.19 0.19 0.19 0.19 0.15 0.15\n0.21 0.21 0.21 0.21 0.18 0.18\n0.25 0.25 0.25 0.25 0.20 0.20\n0.18 0.18 0.17 0.17 0.17 0.17\n0.19 0.19 0.19 0.19 0.18 0.18\n0.24 0.24 0.24 0.24 0.21 0.21\n0.35 0.35 0.34 0.34 0.25 0.25\n0.21 0.21 0.20 0.21 0.18 0.18\n0.25 0.25 0.24 0.24 0.19 0.19\n0.22 0.22 0.22 0.21 0.17 0.17\nDot Product\nVZ (cosine)\nVZ (cosine, normalized)\nVZ (spearmanr)\nVZ (spearmanr, normalized)\nVZ (euclidean)\nVZ (euclidean, normalized)\n1 2 3 4 5 6 7 8 9 10 11 12\n0.33 0.33 0.33 0.33 0.33 0.33\n0.36 0.37 0.36 0.37 0.36 0.37\n0.37 0.37 0.37 0.37 0.37 0.37\n0.48 0.48 0.48 0.47 0.48 0.48\n0.58 0.58 0.57 0.57 0.58 0.58\n0.43 0.43 0.43 0.43 0.43 0.43\n0.46 0.46 0.46 0.46 0.46 0.46\n0.53 0.53 0.53 0.53 0.53 0.53\n0.68 0.68 0.68 0.68 0.68 0.68\n0.45 0.45 0.45 0.45 0.45 0.45\n0.53 0.53 0.53 0.53 0.53 0.53\n0.46 0.45 0.45 0.45 0.45 0.45\nAverage Precision\n(b) Fine-tuned BERT\nFigure A.1: Layer-wise alignment between the cue vec-\ntor and different Value Zeroing(VZ) scores computed\nbased on 1) different distance metric and 2) whether\nrepresentations are normalized.\nA.4 Qualitative Analysis: Layer-wise Context\nMixing Maps\nThis section illustrates different context mixing\nmaps obtained from a fine-tuned BERT model for\nthe correctly classified example of “The pictures of\nsome hat [MASK] scaring Marcus. ”\n3390\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12Layer\n4.15 6.52 5.18 5.55 5.61 5.59 5.21 1.46 3.44 1.35 5.35\n4.05 5.89 3.82 4.76 4.98 4.89 4.15 1.62 3.70 1.51 3.96\n4.08 5.76 3.66 4.48 4.72 4.49 4.00 1.79 4.32 1.72 4.04\n4.19 4.67 2.10 3.08 3.14 3.10 3.52 2.14 4.73 2.77 2.19\n4.04 3.62 1.72 2.61 2.62 2.62 2.79 2.72 5.05 3.37 1.75\n4.08 4.20 2.52 3.39 3.40 3.39 3.19 3.25 5.06 3.75 2.53\n4.12 3.49 2.28 3.24 3.25 3.25 3.09 3.29 4.99 3.93 2.31\n4.12 3.41 2.29 3.20 3.20 3.19 3.24 3.65 5.07 4.01 2.30\n4.20 2.88 1.59 2.55 2.56 2.52 2.62 4.13 5.78 4.38 1.59\n4.18 3.64 2.59 3.45 3.47 3.44 3.30 3.84 5.83 4.79 2.60\n4.07 3.58 2.59 3.45 3.53 3.45 3.09 5.04 6.08 5.72 2.58\n4.02 4.45 3.40 4.27 4.36 4.26 3.99 4.36 4.36 4.36 3.35\nProbes-needed\n2\n3\n4\n5\n6\n(a) Pre-trained BERT\nRand\nAttn\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12Layer\n4.15 6.50 5.13 5.52 5.58 5.56 5.11 1.52 3.30 1.34 5.31\n4.05 5.89 3.82 4.75 4.96 4.87 4.11 1.64 3.48 1.46 3.97\n4.08 5.77 3.89 4.69 4.87 4.69 4.10 1.76 3.69 1.62 4.21\n4.19 4.72 2.07 3.05 3.11 3.07 3.53 2.05 4.06 2.66 2.16\n4.04 3.42 1.49 2.40 2.41 2.41 2.57 2.50 4.59 3.14 1.47\n4.08 4.06 2.36 3.26 3.26 3.26 3.08 3.07 4.43 3.49 2.37\n4.12 3.18 1.95 2.93 2.95 2.94 2.73 3.07 3.57 3.66 1.99\n4.12 3.11 1.95 2.87 2.87 2.87 2.88 3.21 3.96 3.60 1.95\n4.20 2.40 1.05 2.03 2.03 2.02 2.08 4.00 5.36 4.06 1.05\n4.18 3.24 2.31 3.18 3.21 3.19 2.97 3.40 5.29 4.51 2.31\n4.07 3.54 2.45 3.31 3.34 3.32 2.66 4.55 4.97 5.44 2.46\n4.02 4.25 3.10 3.95 4.11 3.93 3.83 4.36 4.36 4.36 3.06\nProbes-needed\n2\n3\n4\n5\n6 (b) Fine-tuned BERT\nFigure A.2: The layer-wise alignment based on Probes-needed metric between the cue vector and different analysis\nmethods averaged over Test set examples. Lower value (darker blue) is better.\n3391\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.08 0.08 0.04 0.08 0.09 0.06 0.05 0.04 0.08 0.08 0.09 0.10 0.12\n0.08 0.08 0.06 0.08 0.13 0.06 0.05 0.04 0.08 0.07 0.10 0.10 0.21\n0.08 0.07 0.06 0.08 0.11 0.06 0.05 0.04 0.08 0.07 0.09 0.09 0.15\n0.08 0.05 0.05 0.08 0.09 0.04 0.03 0.03 0.08 0.07 0.09 0.10 0.09\n0.08 0.06 0.04 0.08 0.09 0.04 0.03 0.03 0.08 0.07 0.09 0.10 0.10\n0.08 0.07 0.04 0.08 0.11 0.05 0.04 0.04 0.08 0.07 0.09 0.09 0.13\n0.08 0.06 0.04 0.08 0.09 0.04 0.03 0.03 0.08 0.07 0.09 0.09 0.11\n0.08 0.08 0.04 0.08 0.12 0.05 0.04 0.03 0.08 0.08 0.08 0.09 0.16\n0.08 0.07 0.03 0.08 0.12 0.05 0.04 0.04 0.08 0.07 0.07 0.08 0.14\n0.08 0.07 0.03 0.08 0.12 0.05 0.04 0.04 0.08 0.06 0.05 0.07 0.16\n0.08 0.07 0.03 0.08 0.12 0.04 0.03 0.03 0.08 0.05 0.04 0.04 0.16\n0.08 0.04 0.03 0.08 0.10 0.03 0.02 0.02 0.09 0.00 0.00 0.00 0.12\nDot Product\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.25 0.22 0.19 0.22 0.33 0.21 0.20 0.20 0.24 0.25 0.27 0.29 0.36\n0.25 0.25 0.18 0.20 0.41 0.25 0.24 0.24 0.18 0.20 0.29 0.28 0.42\n0.25 0.23 0.19 0.18 0.42 0.25 0.25 0.25 0.16 0.18 0.27 0.27 0.42\n0.25 0.17 0.18 0.18 0.27 0.20 0.20 0.20 0.15 0.20 0.25 0.28 0.27\n0.25 0.16 0.16 0.15 0.31 0.20 0.20 0.20 0.14 0.21 0.25 0.28 0.30\n0.25 0.20 0.15 0.13 0.38 0.25 0.25 0.25 0.12 0.19 0.24 0.25 0.38\n0.25 0.16 0.15 0.13 0.30 0.21 0.20 0.20 0.13 0.18 0.22 0.23 0.31\n0.26 0.20 0.15 0.13 0.39 0.25 0.25 0.24 0.13 0.21 0.23 0.23 0.41\n0.25 0.21 0.15 0.13 0.34 0.23 0.23 0.23 0.13 0.21 0.24 0.24 0.35\n0.26 0.23 0.15 0.13 0.33 0.22 0.21 0.21 0.14 0.19 0.24 0.23 0.34\n0.25 0.20 0.15 0.13 0.35 0.22 0.22 0.22 0.14 0.18 0.23 0.22 0.36\n0.25 0.18 0.15 0.13 0.31 0.20 0.20 0.20 0.16 0.08 0.08 0.08 0.31\nAverage Precision\n(a) Pre-trained RoBERTa\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.08 0.08 0.04 0.08 0.09 0.06 0.05 0.04 0.08 0.09 0.09 0.11 0.12\n0.08 0.08 0.06 0.08 0.12 0.06 0.05 0.04 0.08 0.08 0.10 0.10 0.20\n0.08 0.06 0.06 0.08 0.11 0.06 0.05 0.04 0.08 0.07 0.10 0.10 0.15\n0.08 0.03 0.05 0.08 0.07 0.03 0.02 0.02 0.09 0.08 0.10 0.09 0.05\n0.08 0.06 0.04 0.08 0.08 0.04 0.03 0.02 0.08 0.08 0.09 0.09 0.09\n0.08 0.06 0.04 0.08 0.10 0.05 0.04 0.03 0.08 0.07 0.09 0.08 0.09\n0.08 0.05 0.04 0.08 0.09 0.04 0.03 0.03 0.08 0.07 0.09 0.07 0.11\n0.08 0.09 0.04 0.08 0.11 0.05 0.04 0.04 0.08 0.06 0.07 0.07 0.14\n0.08 0.09 0.03 0.08 0.14 0.06 0.05 0.05 0.08 0.07 0.06 0.06 0.19\n0.08 0.07 0.03 0.08 0.13 0.05 0.04 0.04 0.08 0.05 0.05 0.05 0.16\n0.08 0.06 0.03 0.08 0.11 0.04 0.04 0.03 0.08 0.03 0.02 0.02 0.13\n0.08 0.02 0.03 0.08 0.06 0.01 0.01 0.01 0.09 0.00 0.00 0.00 0.04\nDot Product\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.25 0.22 0.19 0.22 0.33 0.21 0.20 0.20 0.23 0.28 0.25 0.33 0.36\n0.25 0.25 0.18 0.20 0.40 0.24 0.24 0.24 0.20 0.22 0.28 0.33 0.42\n0.25 0.22 0.18 0.18 0.42 0.25 0.25 0.25 0.16 0.21 0.27 0.31 0.40\n0.25 0.14 0.18 0.18 0.20 0.16 0.16 0.16 0.21 0.21 0.28 0.28 0.19\n0.25 0.16 0.16 0.15 0.27 0.19 0.19 0.19 0.16 0.21 0.26 0.25 0.27\n0.25 0.18 0.15 0.14 0.31 0.21 0.21 0.21 0.13 0.20 0.27 0.20 0.31\n0.25 0.18 0.15 0.16 0.28 0.19 0.19 0.19 0.15 0.18 0.24 0.20 0.30\n0.26 0.23 0.15 0.13 0.36 0.24 0.24 0.24 0.13 0.17 0.22 0.19 0.37\n0.25 0.24 0.15 0.13 0.44 0.28 0.28 0.28 0.11 0.19 0.21 0.19 0.44\n0.26 0.20 0.15 0.13 0.36 0.24 0.24 0.24 0.13 0.15 0.19 0.17 0.38\n0.25 0.20 0.15 0.13 0.34 0.22 0.22 0.21 0.13 0.15 0.16 0.16 0.34\n0.25 0.15 0.15 0.13 0.17 0.14 0.14 0.14 0.15 0.08 0.08 0.08 0.17\nAverage Precision\n(b) Fine-tuned RoBERTa\nFigure A.3: Layer-wise alignment between the cue vector and different analysis methods averaged over Test set\nexamples for RoBERTa. Higher value (darker color) is better.\n3392\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.10 0.11 0.05 0.11 0.11 0.05 0.04 0.04 0.11 0.15 0.11 0.15 0.14\n0.10 0.11 0.08 0.10 0.14 0.07 0.05 0.06 0.11 0.15 0.11 0.15 0.19\n0.10 0.07 0.08 0.11 0.15 0.07 0.05 0.06 0.11 0.14 0.12 0.14 0.22\n0.10 0.11 0.07 0.11 0.20 0.09 0.06 0.07 0.11 0.14 0.11 0.14 0.34\n0.10 0.05 0.06 0.11 0.09 0.03 0.02 0.03 0.11 0.13 0.11 0.13 0.11\n0.10 0.09 0.06 0.11 0.19 0.08 0.06 0.06 0.11 0.12 0.10 0.11 0.22\n0.10 0.10 0.05 0.11 0.17 0.07 0.06 0.06 0.11 0.12 0.08 0.11 0.20\n0.10 0.14 0.04 0.11 0.25 0.10 0.08 0.08 0.11 0.12 0.07 0.10 0.36\n0.10 0.13 0.04 0.11 0.19 0.08 0.07 0.07 0.11 0.11 0.06 0.09 0.25\n0.10 0.11 0.04 0.11 0.18 0.06 0.06 0.06 0.11 0.08 0.03 0.05 0.22\n0.10 0.04 0.04 0.11 0.13 0.04 0.03 0.03 0.11 0.05 0.01 0.03 0.15\n0.11 0.06 0.04 0.11 0.12 0.03 0.03 0.03 0.11 0.00 0.00 0.00 0.12\nDot Product\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.29 0.36 0.22 0.36 0.36 0.22 0.21 0.21 0.39 0.53 0.34 0.55 0.37\n0.29 0.34 0.24 0.27 0.44 0.27 0.27 0.27 0.19 0.51 0.30 0.51 0.44\n0.29 0.23 0.19 0.22 0.48 0.29 0.28 0.28 0.20 0.46 0.29 0.45 0.51\n0.29 0.27 0.18 0.23 0.63 0.35 0.35 0.35 0.15 0.41 0.28 0.40 0.63\n0.28 0.17 0.18 0.23 0.27 0.19 0.19 0.19 0.27 0.40 0.27 0.36 0.26\n0.29 0.22 0.17 0.23 0.42 0.27 0.27 0.27 0.18 0.34 0.27 0.27 0.44\n0.29 0.29 0.17 0.23 0.45 0.28 0.28 0.28 0.16 0.33 0.23 0.25 0.45\n0.29 0.31 0.17 0.23 0.63 0.36 0.36 0.36 0.13 0.32 0.22 0.26 0.64\n0.30 0.39 0.17 0.23 0.51 0.31 0.31 0.31 0.15 0.30 0.22 0.24 0.51\n0.30 0.28 0.16 0.22 0.49 0.31 0.30 0.31 0.15 0.26 0.17 0.17 0.50\n0.30 0.17 0.16 0.22 0.38 0.25 0.25 0.25 0.19 0.22 0.15 0.18 0.38\n0.31 0.18 0.16 0.22 0.33 0.22 0.22 0.22 0.23 0.10 0.10 0.10 0.33\nAverage Precision\n(a) Pre-trained ELECTRA\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n0.10 0.10 0.05 0.10 0.11 0.05 0.04 0.04 0.11 0.16 0.13 0.17 0.14\n0.10 0.11 0.08 0.10 0.15 0.07 0.05 0.06 0.11 0.16 0.12 0.16 0.20\n0.10 0.06 0.07 0.10 0.15 0.06 0.05 0.06 0.11 0.16 0.14 0.16 0.21\n0.10 0.10 0.07 0.11 0.22 0.09 0.07 0.07 0.11 0.15 0.13 0.15 0.40\n0.10 0.05 0.06 0.11 0.10 0.03 0.02 0.03 0.11 0.15 0.12 0.14 0.12\n0.10 0.09 0.05 0.11 0.21 0.08 0.06 0.06 0.11 0.13 0.11 0.13 0.26\n0.10 0.10 0.05 0.11 0.20 0.08 0.07 0.07 0.11 0.14 0.09 0.13 0.28\n0.10 0.22 0.04 0.11 0.35 0.15 0.12 0.13 0.10 0.11 0.07 0.10 0.54\n0.10 0.16 0.04 0.11 0.22 0.10 0.09 0.09 0.10 0.09 0.05 0.08 0.33\n0.10 0.10 0.04 0.11 0.18 0.07 0.06 0.06 0.11 0.06 0.02 0.04 0.21\n0.10 0.03 0.04 0.11 0.11 0.03 0.03 0.03 0.11 0.02 0.00 0.01 0.11\n0.11 0.02 0.03 0.11 0.06 0.01 0.01 0.01 0.11 0.00 0.00 0.00 0.04\nDot Product\nRand\nAttn\nAttn-rollout\nAttn-flow\nAttn-norm\nAttn-norm + RES\nAttn-norm + RES + LN\nGlobEnc¬\nALTI¬\nGradXinput\nIG\nDL\nValue Zeroing\n1 2 3 4 5 6 7 8 9 10 11 12\n0.29 0.36 0.22 0.36 0.36 0.22 0.21 0.21 0.40 0.60 0.41 0.62 0.37\n0.29 0.33 0.23 0.25 0.45 0.28 0.27 0.28 0.17 0.57 0.38 0.58 0.45\n0.29 0.22 0.19 0.22 0.45 0.27 0.27 0.27 0.21 0.53 0.40 0.54 0.49\n0.29 0.24 0.18 0.23 0.63 0.35 0.34 0.34 0.17 0.47 0.34 0.46 0.63\n0.28 0.17 0.18 0.22 0.29 0.20 0.20 0.20 0.28 0.46 0.31 0.41 0.28\n0.29 0.21 0.17 0.22 0.47 0.29 0.29 0.29 0.18 0.38 0.32 0.31 0.49\n0.29 0.26 0.17 0.23 0.59 0.34 0.34 0.34 0.14 0.38 0.27 0.28 0.60\n0.29 0.57 0.17 0.23 0.80 0.43 0.42 0.42 0.13 0.30 0.23 0.27 0.78\n0.30 0.56 0.17 0.23 0.65 0.36 0.36 0.36 0.14 0.26 0.21 0.22 0.64\n0.30 0.25 0.16 0.22 0.49 0.30 0.30 0.30 0.15 0.23 0.20 0.19 0.49\n0.30 0.20 0.16 0.23 0.31 0.21 0.21 0.21 0.19 0.16 0.15 0.16 0.32\n0.31 0.17 0.16 0.23 0.20 0.16 0.15 0.16 0.33 0.10 0.10 0.10 0.20\nAverage Precision\n(b) Fine-tuned ELECTRA\nFigure A.4: Layer-wise alignment between the cue vector and different analysis methods averaged over Test set\nexamples for ELECTRA. Higher value (darker color) is better.\n3393\nAttn\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.5: Raw attention scores (Attn) averaged over all attention heads at each different layer.\nAttn w/ rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.6: Raw attention scores (Attn) aggregated by rollout (Abnar and Zuidema (2020)’s method) across layers.\n3394\nAttn-norm\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.7: Kobayashi et al. (2020)’s scores (Attn-norm) across layers.\nAttn-norm w/ rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.8: Kobayashi et al. (2020)’s scores (Attn-norm) aggregated by rollout method across layers.\n3395\nAttn-norm + RES\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.9: Kobayashi et al. (2021)’s scores (Attn-norm + RES) across layers.\nAttn-norm + RES w/ rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.10: Kobayashi et al. (2021)’s scores (Attn-norm + RES) aggregated by rollout method across layers.\n3396\nAttn-norm + RES + LN\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.11: Kobayashi et al. (2021)’s scores (Attn-norm + RES + LN) across layers.\nAttn-norm + RES + LN w/ rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.12: Kobayashi et al. (2021)’s scores (Attn-norm + RES + LN) aggregated by rollout method across layers.\n3397\nGlobEnc without rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.13: Modarressi et al. (2022)’s scores (GlobEnc¬) without aggregation (rollout) across layers.\nGlobEnc (w/ rollout)\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.14: Modarressi et al. (2022)’s scores (GlobEnc) across layers (the rollout method is inherently incorporated\nin GlobEnc).\n3398\nALTI without rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.15: Ferrando et al. (2022)’s scores (ALTI¬) without aggregation (rollout) across layers.\nALTI (w/ rollout)\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.16: Ferrando et al. (2022)’s scores (ALTI) across layers (the rollout method is inherently incorporated in\nALTI).\n3399\nValue Zeroing\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.17: Our scores (Value Zeroing) across layers.\nValue Zeroing w/ rollout\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 1\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 2\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 3\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 4\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 5\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 6\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 7\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 8\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 9\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 10\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 11\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\n[CLS]\nthe\npictures\nof\nsome\nhat\n[MASK]\nscar\n##ing\nmarcus\n.\n[SEP]\nLayer: 12\nFigure A.18: Our global scores (Value Zeroing) aggregated by rollout method across layers.\n3400"
}