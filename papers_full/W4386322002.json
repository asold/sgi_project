{
  "title": "Semisupervised Edge-Aware Road Extraction via Cross Teaching Between CNN and Transformer",
  "url": "https://openalex.org/W4386322002",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5064462835",
      "name": "Zi-Xiong Yang",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A4286460841",
      "name": "Zhi-Hui You",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A4208403998",
      "name": "Si-Bao Chen",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A2105013799",
      "name": "Jin Tang",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A1897219996",
      "name": "Bin Luo",
      "affiliations": [
        "Anhui University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4285118380",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2205800717",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W6733814495",
    "https://openalex.org/W4313160444",
    "https://openalex.org/W6764051988",
    "https://openalex.org/W3190335749",
    "https://openalex.org/W3095691842",
    "https://openalex.org/W2755992512",
    "https://openalex.org/W7020680850",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2804199516",
    "https://openalex.org/W3171581326",
    "https://openalex.org/W4206422044",
    "https://openalex.org/W4312310512",
    "https://openalex.org/W3194470774",
    "https://openalex.org/W4221161877",
    "https://openalex.org/W6805206225",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4386075981",
    "https://openalex.org/W4296079303",
    "https://openalex.org/W4313506322",
    "https://openalex.org/W3130942710",
    "https://openalex.org/W4312600819",
    "https://openalex.org/W6748481559",
    "https://openalex.org/W3048631361",
    "https://openalex.org/W3047443805",
    "https://openalex.org/W2970619726",
    "https://openalex.org/W6678815747",
    "https://openalex.org/W3195858154",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W3035680157",
    "https://openalex.org/W3120562181",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W6773005947",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3172949220",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W3001197829",
    "https://openalex.org/W3102692100",
    "https://openalex.org/W2953070460",
    "https://openalex.org/W2951970475",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W4200634144",
    "https://openalex.org/W2623331213",
    "https://openalex.org/W3103695279"
  ],
  "abstract": "Semisupervised semantic segmentation of remote sensing images has been proven to be an effective approach to reduce manual annotation costs and leverage available unlabeled data to improve segmentation performance. However, some existing methods that focus on self-training and consistent regularization fail to consider large-scale characteristics of remote sensing images and the importance of incorporating road edge information. In this article, we propose a novel semisupervised edge-aware network (SSEANet) for remote sensing image semantic segmentation by jointly training convolutional neural network and transformer. SSEANet focuses on the consistency loss of multiscale features and uses an attention mechanism to fuse road edge information. Extensive experiments on DeepGlobe, Massachusetts, and AerialKITTI&#x2013;Bavaria datasets show that the proposed method outperforms state-of-the-art, demonstrating its effectiveness.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 1\nSemi-supervised Edge-aware Road Extraction via\nCross Teaching between CNN and Transformer\nZi-Xiong Yang, Zhi-Hui You, Si-Bao Chen ∗, Jin Tang, Bin Luo\nAbstract—Semi-supervised semantic segmentation of remote\nsensing images has been proven to be an effective approach to\nreduce manual annotation costs and leverage available unlabeled\ndata to improve segmentation performance. However, some exist-\ning methods that focus on self-training and consistent regulariza-\ntion fail to consider large-scale characteristics of remote sensing\nimages and the importance of incorporating road edge informa-\ntion. In this paper, we propose a novel Semi-Supervised Edge-\nAware Network (SSEANet) for remote sensing image semantic\nsegmentation by jointly training CNN and transformer. SSEANet\nfocuses on the consistency loss of multi-scale features and uses\nattention mechanism to fuse road edge information. Extensive\nexperiments on DeepGlobe, Massachusetts and AerialKITTI-\nBavaria datasets show that the proposed method outperforms\nstate-of-the-arts, demonstrating its effectiveness.\nIndex Terms—Road extraction; cross teaching; transformer;\nedge-aware; semi-supervised learning.\nI. I NTRODUCTION\nA\nS semi-supervised learning methods continue to advance,\nthe use of semi-supervised semantic segmentation for\nremote sensing road extraction is becoming increasingly ma-\nture. This is of great help to urban planning, agricultural\nland management and traffic road monitoring. Detailed road\nnetwork information is essential for accurate cartographic\ndrawing and map navigation, as it provides critical evidence.\nMoreover, the quality and distribution of road networks can\nserve as valuable indicators to assess the development of an\narea. However, remote sensing images often exhibit diverse\nshapes, sizes, distributions, and textures of roads, posing\nchallenges in their identification and during the imaging pro-\ncess, remote sensing data is susceptible to degradation, noise\neffects, and variabilities [11], further complicating the analysis.\nAdditionally, obstructions such as trees or overpasses may\nobscure parts of the road, making it challenging to maintain the\nintegrity of the road segmentation. Environmental factors such\nas clouds, light, and shadows may also affect the precision\nof extracting roads from remote sensing images. Overall, the\ndiverse and potentially variable nature of roads, along with\nvarious external factors, can make it difficult to accurately\nextract road information. The Multi-directional Swin Window\nManuscript received XXXX, 2023.\n∗This work was supported in part by NSFC Key Project of International\n(Regional) Cooperation and Exchanges (No.61860206004), NSFC Key Project\nof Joint Fund for Enterprise Innovation and Development (No.U20B2068),\nNational Natural Science Foundation of China (No.61976004).\nZ. Yang, Z. You, S. Chen, J. Tang, and B. Luo work at the IMIS Lab\nof Anhui Province, the Anhui Provincial Key Lab of Multimodal Cognitive\nComputation, the MOE Key Lab of ICSP, and the School of Computer Science\nand Technology at Anhui University in Hefei, China. Email addresses are\n{2205357744, 1574583514}@qq.com, and {sbchen, tj, luobin }@ahu.edu.cn.\n(b)\n(c)\n (d)\n(a)\ns\nFig. 1. Diagram of Multi-directional Swin Window (MDSW). Different from\nthe original fixed direction sliding (a) in Swin Transformer [12], we add\nwindow sliding offsets of varying length S in different directions, (b) top\nright, (c) bottom left, and (d) top left, and add random factors to this sliding\noffsets S, so that the model can get more comprehensive global information.\nThe window with a blue border in the lower right corner of figure (d) contains\nmore complete road information.\n(MDSW) is proposed for this challenge, it provides multi-\ndirectional sensing field as shown in Fig. 1.\nExtracting roads from high-resolution remote sensing im-\nages using deep learning techniques is a challenging task\nthat demands extensive labeled data. However, obtaining such\ndata is a tedious and expensive process, which can limit the\nwidespread application of these techniques. Semi-supervised\nlearning is an alternative approach that combines both labeled\nand unlabeled data to train neural networks. This approach\ndemonstrate its efficacy in various tasks, including semantic\nsegmentation and classification, and it includes techniques like\nFixMatch [3], CutMix [4], cross pseudo supervision (CPS)\n[2] and directional context-aware consistency (DCC) [5]. In\nthe process of collaborative training with CPS, to improve\ntraining outcomes, the same network structure is proposed for\nboth networks, but with different initializations. This allows\nfor the amplification of differences between the two networks\nwhile taking into consideration the variations between them.\nSince our goal is to amplify the differences, we directly\nuse two different network structures, which will result in\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 2\nsignificant disparities. After conducting several experiments,\nwe selected CNN and transformer as the two branches for\ntraining. Despite CNN’s commendable performance in remote\nsensing images, its focus primarily lies in capturing local\npatterns through convolutional operations, which limits its\nability to effectively merge and model global context and long-\nrange dependencies. In contrast, the self-attention mechanism\nemployed in visual transformers excels at modeling long-range\ninteractions by allowing each position in the feature map to\nattend to all other positions. This enables the capture of global\ncontextual information and facilitates the integration of long-\nrange dependencies into the learning process. The introduction\nof self-attention-based architectures has opened up new possi-\nbilities for modeling remote interactions and leveraging global\nsemantic information in visual recognition tasks. By combin-\ning the self-attention mechanism, our dual-branch network\narchitecture overcomes the limitations imposed by the local\nnature of convolution operations, thereby enhancing the perfor-\nmance of tasks that require modeling global dependencies and\ncapturing long-range semantic interactions. Therefore, in the\ntraining network we proposed, we adopt two different network\narchitectures, transformer networks and CNNs, which amplify\nthe differences at the network level and resulted in significant\nperformance improvements. Different from CPS, where the\nsame network is used with different initializations, our net-\nwork utilizes two different network architectures as training\nbranches, and the final prediction is generated by the CNN\nbranch, which facilitates comparisons with other approaches.\nWhen it comes to semantic segmentation in remote sensing\nimages, the complexity of intricate edge information and lack\nof tag information in road extraction can make it a challenging\ntask. To overcome these obstacles, it may be beneficial to use\nlocal attention in CNN during semi-supervised training. This\ntechnique can allow the model to concentrate specifically on\nthe perception of important edge information, which is crucial\nfor accurate road extraction. Additionally, we autonomously\ndesigned Edge Attention Block (EAB) and Edge Aware Fusion\nModule (EAFM) in conjunction with the previously proposed\nAttention-induced Cross-level Fusion Module (ACFM) [32]\nand Global Attention Mechanism (GAM) [39] to from Edge\nAware Decoder (EAD), aiming to facilitate the accurate ex-\ntraction of complex road edge information. Furthermore, we\ndeveloped Multi-scale Supervised Loss (MSSL) to enhance\nthe network’s learning through multi-scale consistency regu-\nlarization.\nThe key contributions of this paper:\n• We propose Semi-Supervised Edge-Aware Network\n(SSEANet) for remote sensing road extraction via cross\nteaching CNN and transformer. By leveraging both mod-\nels, the proposed method achieves improved performance.\n• A replaceable segmentation decoder is designed espe-\ncially for binary road extraction tasks, and pays more\nattention to edge information.\n• A multi-directional swin window (MDSW) is proposed,\nwhich is designed to improve road extraction tasks by\nbetter adapting to different types of roads and terrain.\n• A multi-scale supervised loss (MSSL) is proposed for\nlarge-scale remote sensing images.\nII. R ELATED WORKS\nWith the advancement of deep learning, significant progress\nhas been made in the field of CNN and graph convolution\n[21], particularly with the introduction of Full Convolution\nNetworks (FCNs) [6] and other innovative approaches.\nThese advancements have greatly improved natural image\nsegmentation performance and have also been beneficial\nto remote sensing field. In addition, the introduction and\napplication of multimodal data [23], [24] have also brought\na highly promising direction for development in the field of\nremote sensing. Semi-supervised semantic segmentation is a\ntechnique that requires fewer annotations compared to fully\nsupervised methods, but still heavily relies on the available\nannotated data. In remote sensing imagery, a small set of\nhigh-quality data points with accurate labels may be more\nvaluable than a larger set of lower quality data, as the model\ncan learn better and more accurate information from the\nformer. This can prevent the model from being misled or\noverwhelmed by a large amount of low-quality data.\nThis dilemma requires nearly pixel-level expert labeling\nof the data, which is more time-consuming than natural\nimages for large-scale remote sensing images. Therefore,\nsemi-supervised learning which requires less labeled data is\na powerful tool for remote sensing semantic segmentation\nto be applied in practice. Some new semi-supervised\nsemantic segmentation methods such as U2PL [16], ST++\n[17] and Cross-Consistency Training (CCT) [7], using\nconsistency regularization, by adding different enhancements\nand disturbances to the input image. There are also a\nlarge number of studies on input data disturbance, such as\nMixUp [8], CutMix [4], and ClassMix [9]. These techniques\naccomplish semi-supervised segmentation by enforcing the\nprediction of the enhanced data to align with that of the\noriginal data. Among them, researchers rarely apply these\nnatural image methods to remote sensing data, such as\nconsistency self-training (SST) [1] is to apply CCT to remote\nsensing scenes for accuracy improvement. CCT combined\nnearly 30 decoders to make the model robust enough.\nWhile CNN is widely used for dense prediction tasks,\nit lacks the ability to model long-term dependencies. The\ntransformer, a purely self-attention-based network originally\nfrom natural language processing [10], emerges as an\nattractive alternative. In fact, vision transformers (ViT) [19]\nare successfully applied to several tasks related to images\nincluding object detection, and image segmentation. The\nsuccess of ViT leads to the development of many variants\nsuch as Swin Transformer [12], Pyramid vision transformer\n(PVT) [13], and Shunted self-attention (SSA) [14], which\nachieve great success in natural image recognition tasks.\nModel performance can also be improved by imitating\nthe attention mechanism of transformers, such as Global\nAttention Mechanism (GAM) [39] and Attention-induced\nCross-level Fusion Module (ACFM) [32]. The transformer\ngains attention in segmentation because of its superior feature\nextraction ability compared to CNN. Swin-UNet [15] is a\ntransformer-based network that is structured like UNet and\nshows better performance than CNN for segmentation tasks,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 3\nhighlighting the potential of transformers for future research.\nHowever, despite its unique representation capabilities, the\ntransformer still requires a huge volume of data for identifying\ntasks and even require more data than CNN.\nA. Consistency Regularization\nInitially, regularization is a technique used during model\ntraining to prevent overfitting by applying additional con-\nstraints. Consistency regularization is a commonly used ap-\nproach for regularization. It is based on the clustering hy-\npothesis, which suggests that the model can learn from the\ndistribution characteristics of low-density regions in the data.\nThis approach helps to prevent overfitting by encouraging\nthe model to learn from a larger pool of unlabeled data\nrather than relying solely on a small subset of labeled data.\nThis approach can be implemented through data enhancement\ntechniques that increase the overall amount of data available to\nthe model. MSE loss [37] is often used to ensure consistency\nin the predictions made by the model under different data\nenhancement conditions. Another method known as Mean\nTeacher [38] uses an exponential moving average of the\nmodel weights as the “teacher model”, while the standard\nbackpropagation model is referred to as the “student model”.\nThe optimization of predicted distribution losses from two\nmodels is used to achieve consistency regularization. Cross-\nConsistency Training [7] introduce disturbance in the feature\ndomain rather than the sample domain in an effort to learn low-\ndensity regional distribution features. Although consistency\nregularization techniques are successful in improving models,\nthey may not be sufficient for classification tasks. Inaccu-\nrate prediction of unlabeled data and negative low-density\nregional distribution characteristics of such data may limit\ntheir effectiveness. To address this issue, we propose a self-\ntraining approach combined with consistency regularization.\nThis approach enables the model to learn both the category\ninformation of individual pixels and the distribution charac-\nteristics of these pixels in low-density regions.\nB. Semi-supervised Road Extraction\nVarious networks such as Full Convolution Networks\n(FCNs) [6], FCN-based models [27], generative adversarial\nnetworks (CGAN) [28], refined deep residual convolutional\nneural networks (ResNet), encoder-decoder networks, and\nmulti-scale and multi-task frameworks benefit road extraction\ndevelopment rapidly. UNet and its variants are also widely\nused for this task. Some approaches utilize adversarial self-\ntraining (RoadDA) [30] or consistency regularization and\naverage update of pseudo labels (CR and AUP) [31] to address\nthe issue of limited annotated training data known as the data\nscarcity (DS) problem. Semi-supervised learning methods, in-\ncluding self-training, teacher-student models, and pseudo label\ngeneration, are also applied to road extraction in the context\nof remote sensing images, as well as image classification and\nsemantic segmentation of nature images. In the domain of\nremote sensing road extraction, the existence of large-scale\nfeatures can present certain challenges. The scarcity of detailed\nroad information in large-scale images makes it difficult to\ndifferentiate roads from other elements like buildings and\nvegetation. Moreover, complex factors such as diverse land\nsurface characteristics and lighting variations further increase\nthe difficulty of road extraction. To deal with the complex edge\ninformation in large-scale remote sensing images, methods\nthat utilize multi-scale information [25] and enhance edges\n[26] show promising results. To address the specific challenges\nposed by remote sensing images with large-scale features, we\npropose a multi-scale supervised loss (MSSL). This approach\nutilizes the information captured at different scales to enforce\nconsistency across the network layers, allowing for effective\nintegration of information from deep to shallow levels.\nThis paper introduces a regularization scheme called Cross\nTeaching (CT) that establishes a connection between CNN\nand transformer models. The proposed framework takes both\nlabeled and unlabeled images as input, and generates predic-\ntions for each image using separate CNN and transformer\nmodels. This approach is designed to be straightforward yet\nimpactful. Our proposed Edge Aware Decoder (EAD) let\nthe prediction pay more attention to edge information. Our\nproposed Multi-scale Supervision Loss (MSSL) can do a good\njob of consistency learning on large-scale remote sensing\nimages.\nIII. P ROPOSED METHOD\nSemi-supervised learning involves using both labeled and\nunlabeled data for training. The challenge lies in effectively\nleveraging the unlabeled data to improve performance. In this\ncontext, the dataset typically contains a limited number of\nlabeled images Sl = {xl, yl}, where yl is the ground truth\nmask of image xl, while the rest are unlabeled Su = {xu}.\nThe spatial size of the image x ∈ RH×W×3 is H × W and\nthe mask y ∈ RH×W×C has class C (C equals 2 in road\nextraction).\nThe objective of semi-supervised learning is to train a\nmodel that can learn from unlabeled datasets and improve its\nperformance on the task at hand.The training process for a\nsemi-supervised learning model can be divided into two steps :\ntrue label supervision and pseudo label supervision. In the case\nof true label supervision, the model is trained using a labeled\ndatasets with the goal of minimizing the cross-entropy (CE)\nloss function. This function measures the difference between\nthe predicted probabilities and the true probability distribution\nof the labels :\nLossce = −\nnX\ni=1\nyllog(byl) + (1− yl)log(1 − byl), (1)\nwhere yl is the ground truth of labeled images using one-hot\nencoding, byl is the prediction of trained model using labeled\nimage, n is the number of samples.\nIn the case of pseudo label supervision, the unlabeled\ndatasets is used by generating pseudo labels using the model\ntrained on the labeled datasets, which are then used to su-\npervise the training of the model. In our model architecture,\nwe use CNN Fc(x, θ) and transformer Ft(x, θ) to generate\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 4\nEA DecoderCNN\nTransformer EA Decoder\nEA Decoder\nmutil -scale feature\nL1 L2 L3 L4\nEABEAFM\n... Output batch\nTrain batch\nLabeled data\nUnlabeled data\nMultiscale output\nConsistency Loss\nCrossEntropyLoss\nFig. 2. Overveiw of the proposed network architecture Semi-supervised Edge-aware network (SSEANet). The yellow border on the left represents a training\nbatch, which contains both labeled and unlabeled data. CNN and transformer use ResNet and Swin Transformer as backbones, the green bar box is the four\nintermediate features of different sizes of the backbone network output, and the blue square box is the multi-scale feature layer of Edge Aware Decoder output,\nwhich is used to implement Multi-scale supervision loss. When the network gives the output of the same batch of training images from different networks,\nthe output prediction of the labeled images will be supervised by CE loss with the ground truth, and the unlabeled images will be predicted differently by\ntwo different networks, then the two predictions will be supervised by the consistency loss.\npseudo-labels for each other, and then compare the predictions\nof each model with the pseudo labeled images using the Dice\nloss function:\nLossdice =\nX\nα,β∈(c,t)\nα̸=β\n(1 −\n2(P\nh,w,c yu\nαbyu\nβP\nh,w,c yuα + P\nh,w,c byu\nβ\n), (2)\nyu\nσ = onehot(argmax(Fσ(xu, θ))), (3)\nwhere the predictions Fσ(xu, θ) are used to be considered as\npseudo-label yu\nσ, byu\nσ is the prediction of trained model using\nunlabeled image, and σ is used to distinguish whether the\nlabels are predicted by a CNN ( c) or a transformer ( t) model.\nA. Overall Architecture\nThe proposed SSEANet utilizes ResNet-50 as the backbone\nnetwork to extract multi-level features from input images. The\noverall architecture is illustrated in Fig. 2. To avoid redun-\ndancy, we have focused on highlighting the key information\nof the adopted approach. Similarly, we use the optimized\nswin transformer block (mutli-direction block) to stack another\nbackbone network, and the two networks generate pseudo\nlabels for each other. The multi-level features are introduced\ninto the Edge Aware Decoder, and the edge division is refined\nin the process of decoding. In addition to the pseudo-label in-\nteraction consistency loss predicted by two different networks,\nour network also takes consistency regularization for feature\noutputs at different scales.\nB. Cross Teaching (CT)\nIn the field of consistency learning, we design a simple\nyet effective network structure with the aim of achieving two\ngoals: 1) using unlabeled data to enhance the training of\nthe segmentation network, 2) obtaining reliable uncertainty\nestimates from the network output. Our results demonstrate\nthat a cross training strategy, which involves making different\nclassification predictions from different perspectives and using\nthe difference in these predictions as a measure of uncertainty,\ncan lead to improve segmentation performance. Inspired by the\napproach CPS [2], we adopt a simple but effective method\nthat leverages the inherent differences between transformer\nand CNN decoders, rather than training them using the same\narchitecture with added perturbations for joint training. Specif-\nically, we construct identical decoders and applied multi-layer\nconsistency constraints to generate consistent predictions.\nThere are several methods for training multiple deep neural\nnetworks in a semi-supervised learning setting that aim to\nencourage consistency in their outputs. These methods include\ncollaborative teaching, which trains two deep neural networks\nsimultaneously using small batches to promote noise-robust\nlearning. Additionally, the CPS trains two networks with sim-\nilar architecture but different initializations using a small batch\nto facilitate mutual learning. All of these approaches introduce\nperturbations and encourage consistent predictions during the\ntraining phase, but they differ in the level at which this\nperturbation is introduced. For example, deep collaborative\ntraining introduces perturbations at the input level (through\nthe use of multi-view inputs), collaborative teaching introduces\nperturbations at the supervisory level (through the use of noise\ntags), and CPS focuses on introducing perturbations at the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 5\nEAD\nFig. 3. Detailed structure diagram of Edge Aware Decoder (EAD). The decoder consists of two main modules: Edge Attention Block (EAB) and the Edge\nAware Fusion Module (EAFM). However, there have also been additional proposals such as the Global Attention Mechanism and the Attention-induced\nCross-level Fusion Module. These modules offer channel and spatial attention, as well as cross-channel attention fusion, respectively.\nnetwork architecture level.\nOur approach is inspired by CPS [2], and we develop a\nsimple yet effective scheme that takes advantage of the natural\ndifferences between transformer and CNN decoders rather than\nusing the same architecture with added perturbations for joint\ntraining. Given an input image xu, our framework generates\ntwo predictions. The goal is to ensure that these predictions\nare similar in order to facilitate mutual learning, which is\nachieved through consistency regularization. To achieve this,\nwe create identical decoders and apply multi-layer consistency\nconstraints to generate consistent predictions. In our network\ntraining process, we use two different backbone networks\nfor vision recognition: a CNN and a transformer. These\napproaches have distinct characteristics that we leverage to\namplify differences at the network level. The output predic-\ntions of CNN and transformer exhibit different properties due\nto the use of local convolution operations and long-range self-\nattention, respectively. These characteristics are the key factors\nthat differentiate the two models. Our framework utilizes\nthe transformer for complementary training only, and final\npredictions are produced using the CNN.\nC. Edge Aware Decoder (EAD)\nThe proposed EAD, shown in Fig. 3, improves road extrac-\ntion performance by fusing an Edge Attention Block (EAB)\nwith an Edge Aware Fusion Module (EAFM). It utilizes\nResNet-50 to extract features at four different layers, which\nare denoted as fi (i = 1, 2, 3, 4). The EAB uses multi-\nlevel features to focus on the road edge features, and high-\ndimensional features use convolution and resize to fit low-\ndimensional features for combined output.\nIn more detail, the EAFM involves several steps. First, the\noriginal feature Li (i = 1, 2, 3, 4) is saved then pooled and\nsampled. This original feature will be used in the following\nfour branches, focusing on edge feature perception. In these\nbranches, the feature level Li is progressively increased from\nlow to high, with an increasing cavity convolution padding.\nThese features are then concatenated to generate L′\ni. The low-\nlevel features, Li (i = 1, 2), are input into the GAM [39]\nfor the purpose of performing spatial and channel attention.\nMeanwhile, the high-level features, Li (i = 3, 4), are input\ninto the ACFM [32] in order to fuse multi-scale features at\ndifferent levels. The ACFM utilizes a channel attention mech-\nanism across multiple scales to extract informative features\nand accurate predictions. This method reduces redundancy\nand improves accuracy in attention-based fusion of features.\nOverall, the EAFM is a complex process that combines various\ntechniques in order to achieve effective feature fusion and\nprediction results.\nD. Multi-directional Swin Window (MDSW)\nIn remote sensing applications, road extraction is often\nchallenging due to the presence of various interference factors.\nTo address this issue, we propose a multi-directional sliding\nwindow transformer block, which aims to improve the feature\nextraction process for road detection. This block operates by\nsliding a window over the input image and extracting features\nin four consecutive directions: top left, top right, bottom left,\nand bottom right, we set random factors to control the window\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 6\nsize and slide length, as shown in Fig. 1. By increasing the\nreceptive field in this manner, the proposed method is able\nto more effectively capture the directional features of roads.\nDifferent from the original fixed direction sliding, we add\ndifferent lengths of sliding in different directions, lower left,\nupper left, and upper right, followed by a single application in\nall four directions. This sliding window approach allows for\na more thorough extraction of features, ultimately leading to\nimproved road detection performance.\nE. Multi-scale Supervision Loss (MSSL)\nIn this approach, convolutional neural networks (CNNs)\nor transformers are used to extract features from shallow\nto deep layers of the model. In order to perform multi-\nlayer consistency pseudo-supervision, these shallow and deep\nfeatures are extracted separately. Specifically, we find that\ncompared to applying constraints on the higher-level features,\napplying stronger constraints on the lower-level features tends\nto have a greater impact on model performance. Overall,\nthis approach allows for more effective feature extraction by\nutilizing the inherent hierarchical structure of the model.\nLossmssl = λ1CR1 + λ2CR2 + λ3CR3 + λ4CR4, (4)\nCRi = Lossdice(R(Xu)i, S(Xu)i), (5)\nwhere Lossmssl is Multi-scale supervision loss, CRi is the\nconsistent regularization loss of No i layer features, λi is the\nweight of different layer losses, R(Xu) and S(Xu) refer to\nthe predictions of CNN and transformer for unlabeled data,\nrespectively.\nLosscr = Lossdice(R(Xu), S(Xu)), (6)\nLosssemi = αlossce + βlossdice, (7)\nLosstotal = Losssemi + Losscr + λcrLossmssl, (8)\nwhere Losscr is the loss compared with the predicted results\nof CNN and transformer, while Losssemi is a semi-supervised\nlosses, including supervised and unsupervised losses. α and\nβ are the weightings for lossce and lossdice, respectively.\nThe total loss consists of semi-supervised losses Losssemi\nand pseudo consistent regularization losses Losscr as well as\nMulti-scale supervision loss, λcr is the weight of Lossmssl.\nThe aforementioned cross-teaching approach consists of\nEdge Aware Decoder (EAD), Multi-directional Swin Window\n(MDSW), and Multi-scale Supervision Loss (MSSL), forming\nthe entire network architecture. During training, the images\nare input to the network in mini-batches, each containing\nlabeled and unlabeled images. After undergoing unified data\naugmentation operations, they enter the CNN and transformer\nnetworks. In the process of feature extraction by the backbone\nnetwork, MDSW is incorporated to increase the receptive field\nrange of the network, enabling better extraction of compre-\nhensive and complete features. The features extracted by the\nbackbone network are then input to EAD, which utilizes an\nattention mechanism to extract complex edge information of\nroads. EAD also outputs four different-scale feature maps,\nwhich are used for calculating MSSL, applying consistency\nInput\n 400 labels\n Ground Truth\nOurs\nRanPaste\nFig. 4. Evaluation of different road extraction methods on validation sets of\nMassachusetts. The first column shows the input image. The second column\nshows the results of fully supervised training with 400 images, with improved\nsegmentation areas highlighted in red dashed boxes. It can be observed that\nthe segmentation results improve progressively from left to right.\nregularization loss to fully leverage the multi-scale information\nof the images and constrain the network training. Finally, the\npredicted results of the two networks are used as pseudo-labels\nfor each other, taking advantage of their strengths.\nIV. E XPERIMENT\nAll experiments in this work are carried out on open remote\nsensing datasets, including DeepGlobe [33], Massachusetts\n[34] and AerialKITTIBavaria [35]. Positive improvement has\nbeen achieved on each dataset, and ablation experiments have\nbeen done to verify the effectiveness of each module. It\nis worth mentioning that all our experimental results were\nobtained from predictions of the CNN branch, with the trans-\nformer branch used only for training assistance.\nA. Details of Settings\nOur model utilizes a pre-trained ResNet-50 model on Im-\nageNet as its backbone. To standardize our experiments, all\ninput images are resized to 512 × 512 and undergo random\nhorizontal and vertical flipping for data augmentation. During\ntraining, we maintain a consistent batch size of 6 and employ\nthe SGD optimizer. The initial learning rate is set to 0.01 and\nis updated using a polynomial decay strategy with a power of\n0.9. This results in a gradually decreasing learning rate over\ntime. We train our model on an NVIDIA RTX 3090 GPU,\ncompleting 30,000 iterations in approximately 12 hours. In\nour implementation, the MDSW block stack depth is set to\n(2, 2, 6, 2), meaning that the window is first applied twice in\nthe lower right and upper left directions. α and β in Losssemi\nis set to 1/2. λcr in Losstotal is indicated as follow:\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 7\nTABLE I\nCOMPARISON OF SEGMENTATION ACCURACY (MIOU%) BETWEEN\nSSEAN ET AND OTHER METHODS ON DEEP GLOBE DATASET USING\nSAME NETWORK STRUCTURES .\nDeepGlobe Dataset 400 labels 1,350 labels 2,700 labels 5,400 labels\nDeepLab V3+ [20] 51.50 56.21 59.11 62.41\nCRAUP [31] 53.63 58.01 60.22 62.63\nRanPaste [40] 54.24 57.78 60.19 62.85\nCPS [2] 53.70 57.68 59.88 62.53\nFMWDCT [36] 56.22 58.86 61.20 63.42\nU2PL [16] 65.33 67.51 70.40 72.54\nST++ [17] 64.87 68.13 71.24 73.05\nUniMatch [18] 69.58 72.81 73.28 75.60\nSSEA 72.74 75.20 74.13 76.50\nλcr = e−5(1− iter\nitermax )2\n, (9)\nwhere iter refers to the current training iteration number, and\nitermax specifies the maximum allowable iterations during\ntraining.\nFor convenience, We use the mIoU as our common evalu-\nation standard.\nmIoU = 1\nn\nnX\ni=1\nIoUi, (10)\nwhere n represents the total number of categories in the dataset\nand IoUi represents the intersection-over-union of category i.\nB. Datasets\nThe road extraction datasets we utilize include DeepGlobe\n[33], Massachusetts [34] and AerialKITTIBavaria [35]. These\ndatasets are selected based on their suitability for our purposes\nand cover a range of geographical locations. By using these\nthree datasets, we can obtain a comprehensive understanding\nof road networks in diverse settings.\n1) DeepGlobe: The dataset consists of 6,226 road remote\nsensing images, each with a resolution of 1,024 ×1,024 pixels.\nFor the experiment, we resized the entire dataset to 512 ×512\npixels and randomly split it into a training set of 5,400 images\nand a test set of 826 images.\n2) Massachusetts: The dataset comprises 1,171 images that\nare cropped to a size of 512 ×512 pixels. Similarly, we chose\n5,200 images for training and 418 images for testing.\n3) AerialKITTI and Bavaria: The dataset used for the\nexperiment contains 33 high-resolution images, which are\nobtained by integrating two datasets, namely AerialKITTI and\nBavaria provided by FMWDCT [35]. After cropping all the\nimages to 512×512 pixels, a total of 460 images were filtered\nout for the experiment. For training purposes, a set of 360\nimages were selected at random, while the remaining images\nwere reserved for testing.\nC. Comparison with State-of-the-arts\nWe demonstrate the effectiveness of our method by com-\nparing it with four state-of-the-art methods on three publicly\naccessible datasets, all of which include partially labeled\ndata. Regarding the semi-supervised experiment in FMWDCT\n[36], the DeepGlobe Dataset is categorized into four different\nTABLE II\nCOMPARISON OF SEGMENTATION ACCURACY (MIOU%) BETWEEN\nSSEAN ET AND OTHER METHODS ON MASSACHUSETTS DATASET USING\nSAME NETWORK STRUCTURES .\nMassachusetts Dataset 400 labels 1,300 labels 2,600 labels 5,200 labels\nDeepLab V3+ [20] 59.39 62.28 63.56 64.58\nRanPaste [40] 62.02 63.53 64.39 65.20\nCPS [2] 60.18 62.23 63.62 64.55\nFMWDCT [36] 62.95 64.16 65.03 65.68\nU2PL [16] 69.40 72.31 73.57 73.79\nST++ [17] 72.19 74.07 75.45 76.81\nUniMatch [18] 74.58 76.22 77.31 77.97\nSSEA 78.74 79.18 79.51 79.82\nsubsets comprising of 400, 1,350, 2,700, and 5,400 labels\nrespectively as Table I. Similarly, the Massachusetts Dataset\nis also divided into four different subsets containing 400,\n1,300, 2,600, and 5,200 labels respectively as Table II. On the\nother hand, the AerialKITTI-Bavaria Dataset has four subsets,\nincluding 50, 100, 200, and 360 labels respectively as Table III.\nThe results of the experiments demonstrate that DeepGlobe\nDataset and Massachusetts Dataset outperform other state-of-\nthe-art methods in terms of performance, and it is higher than\n10% under different proportions of labeled datasets. This great\nimprovement can be attributed to the application of CT after\nthe ablation experiment. However, the accuracy of 100 labels\nand 200 labels on AerialKITTI-Bavaria Dataset is not far\nfrom that of FMWDCT method. The objective analysis is that\nAerialKITTI-Bavaria Dataset is a combination of two datasets\nAerialKITTI and Bavaria, and there are differences among\ndifferent datasets. Moreover, the characteristics of 100 labels\nand 200 labels are mostly the same, so FMWDCT method\nwith data enhancement has higher adaptability.\nThe ST++ method aims to generate more reliable pseudo-\nlabels, while U2PL leverages unreliable pseudo-labels, which\nare based on predictions of the same network, and thus have\ninherent limitations. UniMatch, as a state-of-the-art model,\nintroduces better perturbations at the image and feature levels,\nwhich may complement and benefit from the network-level\nperturbations highlighted in this study. Our proposed model\nexcels in focusing on the attention and extraction of road\nedges.\nWe conduct some experiments to compare the performance\nof various methods on the Massachusetts Dataset as Fig. 4.\nThe methods include a fully supervised approach using only\nlabeled images, a semi-supervised approach enhanced by\nRanPaste [40], and our propose semi-supervised method. The\nresults show that our method produced high-quality predictions\nin terms of both detail and completeness of the road.\nD. Ablation Study\nTable IV shows the results of ablation experiments of\nour method on AerialKITTIBavaria dataset with 50 labeled\nimages. We choose ResNet-50 model that only uses mean\nteacher method as the baseline model (first row), and gradually\nincrease the proposed components to prove their effectiveness.\nMulti-scale supervision loss must be based on Cross Traning,\nwhich uses the predictive output of two networks. From the\nsecond row in the table, it can be observed that the Cross\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 8\nTABLE III\nCOMPARISON OF SEGMENTATION ACCURACY (MIOU%) BETWEEN\nSSEAN ET AND OTHER METHODS ON AERIAL KITTI-B AVARIA DATASET\nUSING SAME NETWORK STRUCTURES .\nAerialKITTI-Bavaria Dataset 50 labels 100 labels 200 labels 360 labels\nDeepLab V3+ [20] 69.64 72.48 74.65 76.85\nRanPaste [40] 71.65 76.35 78.22 79.47\nCPS [2] 70.29 73.16 74.97 76.95\nU2PL [16] 73.57 74.14 75.58 77.02\nST++ [17] 71.86 74.61 76.03 77.83\nUniMatch [18] 74.57 75.21 77.82 78.06\nFMWDCT [36] 73.27 77.50 78.81 79.76\nSSEA 76.67 76.87 78.05 80.96\nTABLE IV\nEFFECT OF VARIOUS MODULES ON SEMI -SUPERVISED TRAINING FOR\nSEGMENTATION ACCURACY (MIOU%)\nMethod CT EAD MDSW MSSL mIoU\n1 69.64\n2 ✓ 74.57\n3 ✓ ✓ 75.22\n4 ✓ ✓ 75.12\n5 ✓ ✓ 74.97\n6 ✓ ✓ ✓ 75.32\n7 ✓ ✓ ✓ 76.03\n8 ✓ ✓ ✓ 75.46\n9 ✓ ✓ ✓ ✓ 76.67\nTeaching (CT) method achieves the highest improvement in\nmIoU. Compared to the first row where only Mean Teacher is\nutilized, CT shows an increase of 4.93 in mIoU. This indicates\nthat employing CNN and transformer as different branch\nnetwork structures brings significant accuracy enhancement,\nmaking the adoption of the CT method necessary. In the third,\nfourth, and fifth rows of the table, the individual application\nof the EAD, MDSW, and MSSL modules on the CT network\nleads to a positive improvement in mIoU. Among them, the\nEAD module exhibits the highest enhancement. The sixth,\nseventh, and eighth rows represent paired application of two\nout of the remaining three modules on the CT network.\nAmong these combinations, the EAD and MDSW combination\nyields the greatest improvement. The last row of the table\ndemonstrates the performance obtained by utilizing all four\nproposed modules, resulting in the highest mIoU compared to\nthe aforementioned strategies. The results of the experiment\nindicate that each component of our proposed approach has a\nbeneficial effect.\nWe also do additional experiments for multi-scale su-\npervision loss to explore the use of different feature layers\nfor consistency regularization. In order to present sufficiently\nindependent and reasonable experimental results, all other\nsettings are the same except that the multi-scale supervised\nloss uses inputs from different layers. In the experiment, the\nmulti-level features are arranged from low to high (1, 2, 3, 4),\nand the effects of each of them on the model performance are\nexplored. The experimental results indicate that, in general,\neach layer of the model (as in Table V) contains information\nthat contributes to the overall performance of the model.\nFinally, the controllable variable parameter λ is added to\nweigh the input of each layer to get better results. In addition,\nwe analyze the predictions of CNN and transformer models\non the AerialKITTI-Bavaria and find that the two models\nTABLE V\nSEGMENTATION ACCURACY (MIOU%) COMPARISON WHEN COMBINATION\nOF FEATURE OUTPUTS OF DIFFERENT SIZES ARE USED IN MULTI -SCALE\nSUPERVISION LOSS , λ MEANS TO ADD CONTROLLABLE PARAMETER\nWEIGHTS TO THE INPUTS OF DIFFERENT LAYERS .\nMethod 1 2 3 4 λ mIoU\n1 68.43\n2 ✓ 71.87\n3 ✓ 71.02\n4 ✓ 70.35\n5 ✓ 70.89\n6 ✓ ✓ ✓ ✓ ✓ 72.32\nInput Rpseudo-label Spseudo-label Ground Truth\nFig. 5. Comparison of pseudo-labels and ground truth obtained from two\ndifferent networks. The first column shows the input image, followed by\nthe pseudo label generated by ResNet-50 in the second column, and the\npseudo label generated by the optimized swin transformer network in the third\ncolumn. Finally, the fourth column represents the ground truth. The red box\nin the comparison between the two highlights the more accurate segmentation\nprediction. By learning from each other and complementing each other, they\ncan achieve better performance.\nexhibit complementary strengths in their perception of the\ndata, as Fig. 5. This suggests that our cross teaching approach,\nwhich combines the outputs of both models, is effective. Our\nproposed method outperforms others in effectively capturing\nsubtle nuances and facilitating a comprehensive understanding\nof the data, as demonstrated by the experimental results.\nTherefore, our approach can be considered superior.\nα and β respectively represent the proportions of CE loss\nand Dice coefficient loss in eq. (7), where their sum is 1,\njointly constituting Losssemi. The experiment is conducted\nwith five different proportions of a ranging from 0 to 1,\nand mIoU represents the highest accuracy of the final pre-\ndicted model on Aerialkitti-Bavaria Dataset as Fig. 6. The\nexperimental results demonstrate that the model achieves the\nhighest accuracy when a and b are equally weighted. When the\nproportions of CE loss and Dice coefficient loss are equal, the\nmodel simultaneously benefits from effective guidance in both\nclassification and segmentation tasks. This balance allows the\nmodel to find an appropriate trade-off between classification\nand segmentation, consequently improving overall predictive\naccuracy.\nE. Network selection\nWe conducted several experiments to investigate the se-\nlection of different networks for the dual-branch architecture\nas Table VI. Given our emphasis on leveraging network\ndifferences, we chose different networks within the same type\nof combination. Specifically, we performed experiments using\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 9\n77.83\n78.43\n78.75 78.56\n77.65\n75.92\n76.3\n76.55\n76.23\n75.75\n73\n74\n75\n76\n77\n78\n79\n80\n0 25 50 75 1\nmIoU(%)\n\u0001 (%)\nMassachusetts Dataset AerialKITTI-Bavaria Dataset\nFig. 6. Comparison of mIoU Accuracy for Different Ratios of α and β in\nSemi-supervised Loss. The sum of α and β is 1, where the x-axis represents\nthe change in variable α, and b is equal to 1- α. The y-axis represents the\nhighest predicted accuracy of the model.\nTABLE VI\nCOMPARISON OF M IOU ACCURACY FOR DIFFERENT NETWORK\nCOMBINATIONS UESING 100 LABELS ON AERIALKITTI -BAVARIA DATASET.\nNet 1 Net 2 mIoU 1 mIoU 2\nResnet U-net 71.08 68.23\nSwin PVT 72.67 72.14\nResnet Swin 76.49 76.52\nResNet and U-Net for the CNN-CNN combination, and Swin\nTransformer and PVT for the transformer-transformer combi-\nnation. mIoU1 and mIoU2 represent the network prediction\naccuracies of Net1 and Net2, respectively. In comparison,\nthe combined network of CNN and transformer demonstrates\nhigher accuracy. The experimental results indicate that the\nbenefits of network selection are closely related to the dif-\nferences between the networks. When there is greater dis-\nsimilarity between the two networks, they are able to learn\nmore information, which in turn facilitates model training\nmore effectively. These experimental findings further validate\nthe effectiveness of our dual-branch network approach. By\nharnessing the strengths of different networks, we can enhance\noverall model performance and generalization ability.\nV. C ONCLUSION\nThis paper leverages the inherent differences between CNN\nand transformer models to increase the model gap in cross-\nsupervision training, thus providing richer feature information\nfor model training. To address the challenge of sensing and\npredicting complex and dynamic situations in remote sensing\nroad scenarios, the Multi-directional Swin Window (MDSW)\nis used to compensate for the missing global perception of\nCNN. Furthermore, we propose the Edge Aware Decoder\n(EAD) to explore edge information and enhance representation\nlearning for remote sensing road extraction. The Multi-scale\nSupervision Loss (MSSL) is utilized to weight and leverage\ndeep and shallow features for consistency regularization, op-\ntimizing the model at a fine-grained level. Comprehensive ex-\nperiments demonstrate that our proposed method outperforms\nthe current state-of-the-art. In the research domain of semi-\nsupervised road extraction, this distinct dual-branch network\nstructure has the potential to outperform single network struc-\ntures in adapting to various remote sensing road maps, making\nit a promising mainstream direction for further investigation.\nSimilarly, introducing input image perturbation and feature-\nlevel perturbation can also enhance the diversity of dual-branch\nnetworks, offering valuable research opportunities in the field\nof remote sensing road extraction.\nREFERENCES\n[1] J. Li, B. Sun, S. Li, and X. Kang, “Semisupervised semantic segmentation\nof remote sensing images with consistency self-training,” IEEE Trans.\nGeosci. Remote. Sens. , vol. 60, pp. 1–11, 2022.\n[2] X. Chen, Y . Yuan, G. Zeng, and J. Wang, “Semi-supervised semantic\nsegmentation with cross pseudo supervision,” in IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-\n25, 2021. Computer Vision Foundation / IEEE, 2021, pp. 2613–2622.\n[3] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. Raffel,\nE. D. Cubuk, A. Kurakin, and C. Li, “Fixmatch: Simplifying semi-\nsupervised learning with consistency and confidence,” in Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020 , 2020.\n[4] S. Yun, D. Han, S. Chun, S. J. Oh, Y . Yoo, and J. Choe, “Cutmix: Reg-\nularization strategy to train strong classifiers with localizable features,”\nin 2019 IEEE/CVF International Conference on Computer Vision, ICCV\n2019, Seoul, Korea (South), October 27 - November 2, 2019 . IEEE,\n2019, pp. 6022–6031.\n[5] X. Lai, Z. Tian, L. Jiang, S. Liu, H. Zhao, L. Wang, and J. Jia, “Semi-\nsupervised semantic segmentation with directional context-aware consis-\ntency,” in IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2021, virtual, June 19-25, 2021 . Computer Vision Foundation /\nIEEE, 2021, pp. 1205–1214.\n[6] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for\nsemantic segmentation,” in IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015 .\nIEEE Computer Society, 2015, pp. 3431–3440.\n[7] Y . Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic segmen-\ntation with cross-consistency training,” in 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2020 . Computer\nVision Foundation / IEEE, 2020, pp. 12 671–12 681.\n[8] H. Zhang, M. Ciss ´e, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” in 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings . OpenReview.net, 2018.\n[9] V . Olsson, W. Tranheden, J. Pinto, and L. Svensson, “Classmix:\nSegmentation-based data augmentation for semi-supervised learning,” in\nIEEE Winter Conference on Applications of Computer Vision, WACV 2021\nIEEE, 2021, pp. 1368–1377.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9, 2017, Long Beach,\nCA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus,\nS. V . N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 5998–6008.\n[11] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “An augmented linear\nmixing model to address spectral variability for hyperspectral unmixing,”\nIEEE Transactions on Image Processing , vol. 28, no. 4, pp. 1923–1938,\n2019.\n[12] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in 2021 IEEE/CVF International Conference on Computer\nVision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE,\n2021, pp. 9992–10 002.\n[13] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in 2021 IEEE/CVF International\nConference on Computer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021 . IEEE, 2021, pp. 548–558.\n[14] S. Ren, D. Zhou, S. He, J. Feng, and X. Wang, “Shunted self-\nattention via multi-scale token aggregation,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,\nLA, USA, June 18-24, 2022 . IEEE, 2022, pp. 10 843–10 852.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING, VOL. XX, NO. X, XXX 202X 10\n[15] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n“Swin-unet: Unet-like pure transformer for medical image segmentation,”\nCoRR, vol. abs/2105.05537, 2021.\n[16] Y . Wang, H. Wang, Y . Shen, J. Fei, W. Li, G. Jin, L. Wu, R. Zhao,\nand X. Le, “Semi-supervised semantic segmentation using unreliable\npseudo-labels,” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 .\nIEEE, 2022, pp. 4238–4247.\n[17] L. Yang, W. Zhuo, L. Qi, Y . Shi, and Y . Gao, “ST++: make\nself-trainingwork better for semi-supervised semantic segmentation,” in\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2022, pp. 4258–4267.\n[18] L. Yang, L. Qi, L. Feng, W. Zhang, and Y . Shi, “Revisiting weak-\nto-strong consistency in semi-supervised semantic segmentation,” CoRR,\nvol. abs/2208.09910, 2022. [Online]. Available: https://doi.org/10.48550/\narXiv.2208.09910\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” in 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net, 2021.\n[20] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 801–818.\n[21] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot,\n“Graph convolutional networks for hyperspectral image classification,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 59, no. 7,\npp. 5966–5978, 2021.\n[22] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for\nsemantic segmentation,” in IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015 .\nIEEE Computer Society, 2015, pp. 3431–3440.\n[23] D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, Q. Du, and B. Zhang,\n“More diverse means better: Multimodal deep learning meets remote-\nsensing imagery classification,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 59, no. 5, pp. 4340–4354, 2021.\n[24] X. Wu, D. Hong, and J. Chanussot, “Uiu-net: U-net in u-net for infrared\nsmall object detection,” IEEE Transactions on Image Processing, vol. 32,\npp. 364–376, 2023.\n[25] R. Hang, P. Yang, F. Zhou, and Q. Liu, “Multiscale progressive seg-\nmentation network for high-resolution remote sensing imagery,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 60, pp. 1–12, 2022.\n[26] R. Hang, G. Li, M. Xue, C. Dong, and J. Wei, “Identifying oceanic eddy\nwith an edge-enhanced multiscale convolutional network,” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing ,\nvol. 15, pp. 9198–9207, 2022.\n[27] D. Pan, M. Zhang, and B. Zhang, “A generic fcn-based approach for\nthe road-network extraction from VHR remote sensing images - using\nopenstreetmap as benchmarks,” IEEE J. Sel. Top. Appl. Earth Obs.\nRemote. Sens., vol. 14, pp. 2662–2673, 2021.\n[28] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”\nCoRR, vol. abs/1411.1784, 2014.\n[29] R. Liu, Q. Miao, Y . Zhang, M. Gong, and P. Xu, “A semi-supervised\nhigh-level feature selection framework for road centerline extraction,”\nIEEE Geosci. Remote. Sens. Lett. , vol. 17, no. 5, pp. 894–898, 2020.\n[30] L. Zhang, M. Lan, J. Zhang, and D. Tao, “Stagewise unsupervised\ndomain adaptation with adversarial self-training for road segmentation\nof remote-sensing images,” IEEE Trans. Geosci. Remote. Sens. , vol. 60,\npp. 1–13, 2022.\n[31] J. Wang, C. H. Q. Ding, S. Chen, C. He, and B. Luo, “Semi-supervised\nremote sensing image semantic segmentation via consistency regulariza-\ntion and average update of pseudo-label,” Remote. Sens., vol. 12, no. 21,\np. 3603, 2020.\n[32] Y . Sun, G. Chen, T. Zhou, Y . Zhang, and N. Liu, “Context-aware cross-\nlevel fusion network for camouflaged object detection,” in Proceedings\nof the Thirtieth International Joint Conference on Artificial Intelligence,\nIJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 ,\nZ. Zhou, Ed. ijcai.org, 2021, pp. 1025–1031.\n[33] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,\nF. Hughes, D. Tuia, and R. Raskar, “Deepglobe 2018: A challenge to\nparse the earth through satellite images,” in 2018 IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, CVPR Workshops\n2018, Salt Lake City, UT, USA, June 18-22, 2018 . Computer Vision\nFoundation / IEEE Computer Society, 2018, pp. 172–181.\n[34] V . Mnih, “Machine learning for aerial image labeling,” Ph.D. disserta-\ntion, University of Toronto, 2013.\n[35] G. M ´attyus, S. Wang, S. Fidler, and R. Urtasun, “Enhancing road maps\nby parsing aerial images around the world,” in 2015 IEEE International\nConference on Computer Vision, ICCV 2015, Santiago, Chile, December\n7-13, 2015. IEEE Computer Society, 2015, pp. 1689–1697.\n[36] Z. You, J. Wang, S. Chen, J. Tang, and B. Luo, “FMWDCT: foreground\nmixup into weighted dual-network cross training for semisupervised\nremote sensing road extraction,” IEEE J. Sel. Top. Appl. Earth Obs.\nRemote. Sens., vol. 15, pp. 5570–5579, 2022.\n[37] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learn-\ning,” in 5th International Conference on Learning Representations, ICLR\n2017, Conference Track Proceedings . OpenReview.net, 2017.\n[38] A. Tarvainen and H. Valpola, “Mean teachers are better role models:\nWeight-averaged consistency targets improve semi-supervised deep learn-\ning results,” in 5th International Conference on Learning Representations,\nICLR 2017, Workshop Track Proceedings . OpenReview.net, 2017.\n[39] Y . Liu, Z. Shao, and N. Hoffmann, “Global attention mechanism:\nRetain information to enhance channel-spatial interactions,” CoRR, vol.\nabs/2112.05561, 2021.\n[40] J. Wang, S. Chen, C. H. Q. Ding, J. Tang, and B. Luo, “Ranpaste: Paste\nconsistency and pseudo label for semisupervised remote sensing image\nsemantic segmentation,” IEEE Trans. Geosci. Remote. Sens. , vol. 60, pp.\n1–16, 2022.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3310612\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8469989895820618
    },
    {
      "name": "Segmentation",
      "score": 0.7012863159179688
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.621655285358429
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6154425144195557
    },
    {
      "name": "Transformer",
      "score": 0.5142681002616882
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.4462409019470215
    },
    {
      "name": "Image segmentation",
      "score": 0.43849003314971924
    },
    {
      "name": "Annotation",
      "score": 0.43043747544288635
    },
    {
      "name": "Deep learning",
      "score": 0.41325491666793823
    },
    {
      "name": "Computer vision",
      "score": 0.372211217880249
    },
    {
      "name": "Machine learning",
      "score": 0.3645404875278473
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143868143",
      "name": "Anhui University",
      "country": "CN"
    }
  ]
}