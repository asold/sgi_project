{
  "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
  "url": "https://openalex.org/W4385571122",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126232507",
      "name": "Zhongbin Xie",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A297530023",
      "name": "Thomas Lukasiewicz",
      "affiliations": [
        "TU Wien",
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W3178522238",
    "https://openalex.org/W2026593185",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W4285192297",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2942160782",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W2950866572",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W3174505797",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3173610337",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4285273243",
    "https://openalex.org/W4281621415",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W3131157458",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W3205949070"
  ],
  "abstract": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15730–15745\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAn Empirical Analysis of Parameter-Efficient Methods\nfor Debiasing Pre-Trained Language Models\nZhongbin Xie1, Thomas Lukasiewicz2,1\n1 University of Oxford, UK 2 Vienna University of Technology, Austria\nzhongbin.xie@cs.ox.ac.uk, thomas.lukasiewicz@tuwien.ac.at\nAbstract\nThe increasingly large size of modern pre-\ntrained language models not only makes them\ninherit more human-like biases from the train-\ning corpora, but also makes it computationally\nexpensive to mitigate such biases. In this paper,\nwe investigate recent parameter-efficient meth-\nods in combination with counterfactual data\naugmentation (CDA) for bias mitigation. We\nconduct extensive experiments with prefix tun-\ning, prompt tuning, and adapter tuning on dif-\nferent language models and bias types to evalu-\nate their debiasing performance and abilities to\npreserve the internal knowledge of a pre-trained\nmodel. We find that the parameter-efficient\nmethods (i) are effective in mitigating gender\nbias, where adapter tuning is consistently the\nmost effective one and prompt tuning is more\nsuitable for GPT-2 than BERT, (ii) are less ef-\nfective when it comes to racial and religious\nbias, which may be attributed to the limitations\nof CDA, and (iii) can perform similarly to or\nsometimes better than full fine-tuning with im-\nproved time and memory efficiency, as well as\nmaintain the internal knowledge in BERT and\nGPT-2, evaluated via fact retrieval and down-\nstream fine-tuning.\n1 Introduction\nPre-trained language models are able to encode rich\nlinguistic and factual knowledge by learning the\nco-occurrence information of words in large real-\nworld corpora (Devlin et al., 2019; Petroni et al.,\n2019; Raffel et al., 2020; Brown et al., 2020). Since\nmost of these corpora are internet-based and not\ncarefully curated, they are likely to contain unbal-\nanced or stereotyped information for certain demo-\ngraphic groups. As a result, pre-trained language\nmodels are often demonstrated to inherit bias from\nhuman society and exhibit potential harms (Blod-\ngett et al., 2020; Bender et al., 2021; May et al.,\n2019; Zhao et al., 2019; Sheng et al., 2019; Nangia\net al., 2020; Nadeem et al., 2021). Hence, much re-\nsearch effort has been devoted to debias pre-trained\nlanguage models (Meade et al., 2022).\nWith the size of language models becoming in-\ncredibly large (Brown et al., 2020; Hoffmann et al.,\n2022; Smith et al., 2022), they are not only at\nhigher risk of exhibiting biased behaviors (Ben-\nder et al., 2021), but also hard to debias because of\nprohibitive computational cost. Therefore, recent\nparameter-efficient methods (He et al., 2022; Ding\net al., 2022) have been applied to bias mitigation,\nwhere only a small portion of the parameters are\nupdated (Lauscher et al., 2021; Gira et al., 2022).\nHowever, these works are limited in terms of eval-\nuation dimensions, making it unclear how differ-\nent parameter-efficient methods’ performance com-\npare to each other, whether one parameter-efficient\nmethod is effective across different types of lan-\nguage models, and whether they are also effective\nfor mitigating religious and racial bias in addition\nto gender bias. Moreover, direct comparisons with\nstrong post-hoc debiasing methods (Liang et al.,\n2020; Schick et al., 2021), as well as evaluations of\nbias mitigation’s impact on the language model’s\ninternal knowledge, are often insufficient.\nGiven these observations, we investigate three\npopular parameter-efficient methods, i.e., prefix\ntuning (Li and Liang, 2021), prompt tuning (Lester\net al., 2021), and adapter tuning (Houlsby et al.,\n2019), in combination with counterfactual data\naugmentation (CDA, Zhao et al., 2018; Zmigrod\net al., 2019; Webster et al., 2020) to debias pre-\ntrained language models. We conduct extensive\nexperiments to study the parameter-efficient meth-\nods’ performance on two types of language models\n(BERT (Devlin et al., 2019) for masked language\nmodels and GPT-2 (Radford et al., 2019) for au-\ntoregressive language models), three types of social\nbiases (gender, race, and religion), and four types\nof performance measures (debiasing performance\non CrowS-Pairs (Nangia et al., 2020) and Stere-\noSet (Nadeem et al., 2021), language modeling per-\n15730\nformance on WikiText-2 (Merity et al., 2017) and\nStereoSet (Nadeem et al., 2021), fact retrieval per-\nformance on LAMA (Petroni et al., 2019), as well\nas downstream fine-tuning performance on Wino-\nBias (Zhao et al., 2018)). We empirically compare\nto the performance of full fine-tuning and two post-\nhoc debiasing methods (SentenceDebias (Liang\net al., 2020) and SelfDebias (Schick et al., 2021)),\naiming to comprehensively study the effectiveness\nof parameter-efficient methods for bias mitigation.1\nOur main findings are as follows:\n• The parameter-efficient methods are effective\nin mitigating gender bias. Within the three\nparameter-efficient methods, adapter tuning is\nconsistently the most effective one for mitigat-\ning bias across different types of language\nmodels, while prompt tuning is more suit-\nable for GPT-2 than BERT. Comparing to\nstrong post-hoc debiasing methods, parameter-\nefficient methods are better at preserving the\nlanguage modeling ability, while still achiev-\ning a competitive and sometimes superior de-\nbiasing performance.\n• The parameter-efficient methods are less ef-\nfective when it comes to mitigating racial and\nreligious bias, where the post-hoc debiasing\nmethods could achieve a more favorable over-\nall performance.\n• The parameter-efficient methods can perform\nsimilarly to or sometimes better than full fine-\ntuning, with improved time and memory effi-\nciency.\n• The parameter-efficient methods can largely\nmaintain the internal knowledge in both BERT\nand GPT-2, with the reduction in Preci-\nsion@10 ranging from 0 to 6.8% across all the\nLAMA datasets when compared to the origi-\nnal pre-trained model, and with the reduction\nin average F1 scores less than 3.3% on the\nhard type-1 examples of WinoBias when com-\npared to full fine-tuning.\n2 Parameter-Efficient Methods\nIn this section, we briefly review three popular\nparameter-efficient methods investigated in our\nstudy: prefix tuning (Li and Liang, 2021), prompt\n1The code of this paper is available at https://github.\ncom/x-zb/pedb.\ntuning (Lester et al., 2021), and adapter tun-\ning (Pfeiffer et al., 2021). In contrast to traditional\nfull fine-tuning where all the model parameters are\nupdated during training, these parameter-efficient\nmethods introduce a small number of extra tun-\nable parameters φon top of a frozen pre-trained\nlanguage model.\nPre-trained language models usually adopt the\ntransformer architecture (Vaswani et al., 2017) con-\nsisting of multiple stacked layers. Assume that\nthere are Nlayer layers, and H(i)\n0 ∈RT×d is the\ninput to the i-th layer, where T is the sequence\nlength, and dis the model dimension. Then, H(i)\n0\nis transformed by the following equations to obtain\nthe output of the i-th layer H(i)\n5 , which is in turn\nadopted as the input for the (i+ 1)-th layer:\nH(i)\n1,h = Attn(H(i)\n0 W(i)\nQ,h,H(i)\n0 W(i)\nK,h,H(i)\n0 W(i)\nV,h),\nh= 1,2,...,N head, (1)\nH(i)\n2 = [H(i)\n1,1; ... ; H(i)\n1,Nhead\n]W(i)\nO , (2)\nH(i)\n3 = LayerNorm(H(i)\n0 + H(i)\n2 ), (3)\nH(i)\n4 = ReLU(H(i)\n3 W(i)\n1 + b(i)\n1 )W(i)\n2 + b(i)\n2 , (4)\nH(i)\n5 = LayerNorm(H(i)\n3 + H(i)\n4 ). (5)\nHere, Eqs. (1) and (2) constitute the multi-head\nattention sublayer, where W(i)\nQ,h, W(i)\nK,h, and W(i)\nV,h\ndenote the projection matrix for the query, key,\nand value of the h-th attention head, respectively;\nNhead is the number of attention heads, andH(i)\n1,h ∈\nRT×(d/Nhead). Eq. (4) denotes the feed-forward\nsublayer. [; ] denotes the concatenation operation.\nH(i)\nj ∈RT×d for j = 0 ,2,3,4,5. The input to\nthe 1st layer is the embeddings of the input tokens\nH(1)\n0 = X ∈RT×d.\nPrefix tuning. Li and Liang (2021) prepend\nl tunable prefix vectors to the key vectors\n(H(i)\n0 W(i)\nK,h) and value vectors ( H(i)\n0 W(i)\nV,h) of the\nattention function in Eq. (1) for each layer:\nH(i)\n1,h = Attn(H(i)\n0 W(i)\nQ,h,[P(i)\nK,h; H(i)\n0 W(i)\nK,h],\n[P(i)\nV,h; H(i)\n0 W(i)\nV,h]), h= 1,2,...,N head. (6)\nHere, P(i)\nK,h,P(i)\nV,h∈Rl×(d/Nhead) denote the tun-\nable prefix vectors, and the total tunable parame-\nters are φ={P(i)\nK,h,P(i)\nK,h |h=1,2,...,N head,i=\n1,2,...,N layer}.\n15731\nPrompt tuning. Lester et al. (2021) prepend l\ntunable prompt vectors (continuous tokens) only to\nthe input embeddings (X), and compute the acti-\nvations of these prompt vectors in the subsequent\nlayers using the pre-trained transformer’s parame-\nters. So, the only modification is:\nH(1)\n0 = [P; X] ∈R(l+T)×d, (7)\nwhere P ∈Rl×d denotes the tunable prompt vec-\ntors, and φ= {P}.\nAdapter tuning. Houlsby et al. (2019) insert\nthe following adapter module between the trans-\nformer’s sublayers:\nH(i)\nj ←H(i)\nj + f(H(i)\nj W(i)\ndown)W(i)\nup, (8)\nwhere the intermediate activations H(i)\nj are first\ndown-projected by W(i)\ndown ∈Rd×(d/r) to a lower\ndimension d/r, and then up-projected back by\nW(i)\nup ∈R(d/r)×d to the model dimension d. The\nadapter also contains a non-linear function f and\na residual connection. The hyperparameter r is\ncalled the reduction factor, which determines the\nbottleneck dimension d/rand controls the trade-off\nbetween parameter efficiency and model capacity.\nIn our implementation, we adopt Pfeiffer et al.\n(2021)’s setting where only a single adapter is in-\nserted after the feed-forward sublayer, since it is\nfound to be the optimal setting among other al-\nternatives (Pfeiffer et al., 2021). Thus, all the\ntunable parameters are φ = {W(i)\ndown,W(i)\nup|i=\n1,2,...,N layer}.2\n3 Parameter-Efficient Debiasing through\nCounterfactual Data Augmentation\nWe adopt counterfactual data augmentation (CDA,\nZhao et al., 2018; Zmigrod et al., 2019; Webster\net al., 2020) as our debiasing method to work to-\ngether with parameter-efficient tuning methods.\nSince the encoded biases in pre-trained language\nmodels originate from the unbalanced training cor-\npora, it is natural to mitigate these biases by re-\nbalancing the training corpora. For example, when\nwe want to mitigate gender bias between the male\nand female demographic group and encounter the\ntraining sentence “ He is a doctor.”, CDA would\nsubstitute the bias attribute word “ He” with its\n2Pfeiffer et al. (2021) also insert an additional “add &\nlayer norm” sublayer before the adapter module, so the actual\nnumber of tunable parameters is a bit larger.\nAlgorithm 1 Counterfactual Data Augmentation\nInput: original corpus D0, # demographic groups\nN, # samples S(≤N−1), bias attribute word\nlist {(w(i)\n1 ,...,w (i)\nN )}M\ni=1\nOutput: augmented corpus D1\n1: D1 ←∅\n2: for text sequence x∈D0 do\n3: Identify the number of demographic groups\nn(≤N) contained in x\n4: if n> 0 then\n5: Generate all the permutations of N de-\nmographic groups considered n demo-\ngraphic groups at a time: Π = {πj}\nPn\nN\nj=1,\nwhere πj = (g1,...,g n),{g1,...,g n}⊂\n{1,...,N }\n6: if n= N and (1,2,...,N ) ∈Π then\n7: Π ←Π \\{(1,2,...,N )}\n8: end if\n9: Sample w/o replacement Spermutations\nΠS = {πs}S\ns=1 from Π\n10: for πs ∈ΠS do\n11: xs←Substitute all bias attribute words\nw(i)\nk contained in xwith w(i)\nπs[k]\n12: D1 ←D1 ∪{xs}\n13: end for\n14: D1 ←D1 ∪{x}\n15: end if\n16: end for\ncounterpart “ She” to obtain an additional train-\ning sentence “She is a doctor.”, so that both gen-\nder groups would have equal association with the\ngender-neutral word “ doctor”. Once we have a\nlist of bias attribute words like {( he, she), (man,\nwoman), (husband, wife), . . . }, we could retrieve\nall the occurrences of these bias attribute words in\nthe training corpus, and substitute all of them with\ntheir counterparts.\nFor religious and racial bias where more than\ntwo demographic groups are considered, we need\nto maintain two key properties: (i) we should guar-\nantee consistency, i.e., we should avoid the case\nwhere some occurrences of the bias attribute words\nin group A are substituted with those in group B,\nwhile the other occurrences of (possibly different)\nbias attribute words in group A are substituted with\nthose in group C, and (ii) we should avoid colli-\nsions, i.e., we should avoid the case where both\ngroups A and B are substituted with group C. To\nthis end, we should not consider each group inde-\n15732\npendently and adopt random substitution. Rather,\nwe should substitute according to permutations of\nall the occurred demographic groups in a sentence.\nOur complete CDA method is formally summa-\nrized in Algorithm 1.\nNote that in Algorithm 1, for convenience, we\npropose to sample a fixed number (S) of substitu-\ntions for each sentence. This is because the number\nof possible substitutions ( Pn\nN −1) for each sen-\ntence may vary when the number of occurred de-\nmographic groups ( n) in the sentence varies. In\npractice, we adopt N = 3 and S = 2 for religious\nand racial bias.\nFinally, the parameter-efficient debiasing frame-\nwork works as follows: we first use Algorithm 1\nto augment an original corpus D0 and obtain the\ndebiasing corpus D1; next, we use the parameter-\nefficient tuning methods from Section 2 to solve\nthe following optimization problem:\nmin\nφ\nL(θ0,φ; D1), (9)\nwhere Lis either the masked language model-\ning loss (Devlin et al., 2019) or causal language\nmodeling loss (Radford et al., 2019), θ0 denotes\nthe frozen parameters in the pre-trained language\nmodel, and φdenotes the tunable parameters de-\nfined in Section 2.\n4 Conceptual Comparisons with Existing\nDebiasing Methods\nMost existing debiasing methods are training-\nbased, where they introduce a specific debiasing\nloss to fine-tune a pre-trained model on certain bal-\nanced debiasing corpora (Kaneko and Bollegala,\n2021; Garimella et al., 2021; Ravfogel et al., 2020;\nCheng et al., 2021; Guo et al., 2022). These meth-\nods are, in general, orthogonal to our parameter-\nefficient debiasing framework in that we could\nsubstitute the (masked) language modeling loss\nin Eq. (9) with their specific debiasing loss. In this\npaper, we only focus on the simple language mod-\neling loss, and leave other kinds of debiasing loss\nfor future work.\nAnother important line of debiasing methods\napplies post-hoc mathematical operations on the\nfrozen representations of a language model, such\nas SentenceDebias (Liang et al., 2020) and SelfDe-\nbias (Schick et al., 2021). We briefly review these\nmethods below and make empirical comparisons to\nparameter-efficient debiasing methods in Section 5.\nSentenceDebias. Liang et al. (2020) assume that\nthere is a linear subspace that can capture demo-\ngraphic information in the embedding space, thus\ntrying to identify and remove the demographic in-\nformation via linear algebra operations. Specif-\nically, they first leverage a procedure similar to\nCDA to extract and augment sentences containing\nbias attribute words from a source corpus. Then,\nthey encode the sentences to embeddings with a\npre-trained language model, and obtain a set of\ndifference vectors between the embeddings of sen-\ntences in different demographic groups. Next, they\nperform principle component analysis on the set of\ndifference vectors, and use the first K components\nto expand a bias subspace. Once the bias subspace\nis identified, we could debias a new sentence em-\nbedding by subtracting its projection on the bias\nsubspace.\nSelfDebias. Schick et al. (2021) assume that a\npre-trained language model has a self-diagnosis\nability, which can be used to adjust the output prob-\nabilities over the vocabulary during language gen-\neration. Specifically, SelfDebias relies on hand-\ncrafted descriptions for each type of bias. It first\nputs the bias description and the currently gener-\nated sentence into a self-diagnosis template, which\nencourages the language model to generate biased\nwords for the next time step. Then, the probabili-\nties of these detected biased words are scaled down\nin the actual generation process.\nAlthough no training is needed for these post-\nhoc debiasing methods, their strong assumptions\nabout bias may harm the language modeling ability\nof a language model. On the contrary, CDA-based\nparameter-efficient methods adhere to the original\nlanguage modeling loss without additional assump-\ntions, which may largely reserve the language mod-\neling ability. Another advantage of CDA-based\nparameter-efficient methods is that nearly no addi-\ntional computation is required during inference.\n5 Experiments on Bias Mitigation\n5.1 Experimental Setup\nDatasets. To measure gender, religious, and\nracial bias in pre-trained language models, we\nadopt two crowd-sourced datasets: CrowS-\nPairs (Nangia et al., 2020) and StereoSet (Nadeem\net al., 2021). CrowS-Pairs consists of pairs of\ncontrasting sentences, where one is more stereo-\ntyping than the other. Its gender, religious, and\n15733\nracial subsets contain 262, 105, and 516 exam-\nples, respectively. For StereoSet, we adopt its intra-\nsentence test, where each example consists of a\ncontext sentence and three candidate completions\ncorresponding to stereotypical, anti-stereotypical,\nand unrelated associations, respectively. We again\nonly adopt the gender, religious, and racial subsets,\nwhose sizes are 1026, 623, and 3996, respectively.\nEvaluation Metrics. Our evaluation protocol fol-\nlows Meade et al. (2022). We adopt the “stereotype\nscore”, defined as the percentage of examples for\nwhich the language model favors the stereotypical\nassociation (or the stereotyping sentence) to the\nanti-stereotypical association (or the less stereotyp-\ning sentence), as the measure of bias. An ideal\nmodel that is free of the considered bias should\nachieve a stereotype score of 50%. To measure the\nlanguage modeling ability, we adopt the first 10%\nof WikiText-2 (Merity et al., 2017) to compute the\nperplexity (for autoregressive language models) or\npseudo-perplexity (Salazar et al., 2020, for masked\nlanguage models). We also compute the “language\nmodeling (LM) score” (Nadeem et al., 2021) on all\nthe bias subsets of StereoSet as our second measure\nof language modeling ability.\nTraining Details. We choose to debias BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al.,\n2019), which represent masked language mod-\nels and autoregressive language models, respec-\ntively. Our implementation is based on the Hugging\nFace Transformers (Wolf et al., 2020) and Adapter\nHub (Pfeiffer et al., 2020), and the adopted check-\npoints are bert-base-uncased (109’514’298 pa-\nrameters) and gpt2 (124’439’808 parameters). We\nadopt the English Wikipedia as our original debias-\ning corpus3, and counterfactually augment it using\nAlgorithm 1. The adopted bias attribute words for\neach type of bias are listed in Appendix A. Next,\nwe randomly down-sample 20% of the augmented\nWikipedia as our debiasing corpus. All the CDA-\nbased debiasing methods are trained for two epochs\non one TITAN RTX GPU with 24 GB memory. We\nselect optimal training hyperparameters according\nto the language modeling loss on a validation set\n(we use 5% of the augmented debiasing corpus\nfor validation), since the language modeling loss\non a balanced dataset is a reasonable proxy for\nboth debiasing performance and language model-\n3We also investigate the effect of different debiasing cor-\npora for GPT-2. See Appendix C for details.\ning ability. We select hyperparameters using the\ndefault seed of 42, and re-train the models for four\nadditional times with different random seeds, to ac-\ncount for CrowS-Pairs and StereoSet’s sensitivity\nto pre-training seeds (Aribandi et al., 2021). More\ndetails are in Appendix B.\nBaselines. We compare the parameter-efficient\nmethods to full fine-tuning, where all the parame-\nters of a language model are tuned, For post-hoc\ndebiasing methods, we compare to SentenceDe-\nbias (Liang et al., 2020) and Self-Debias (Schick\net al., 2021), as described in Section 4.\n5.2 Mitigating Gender Bias\nFor experiments on mitigating gender bias, we\nadopt a default reduction factor of r = 48 in\nadapter tuning, leading to 304’320 tunable parame-\nters, which are less than 0.3% of all the parameters\nin BERT (109’514’298) or GPT-2 (124’439’808).\nFor prefix tuning, we adopt a prefix length of\nl = 16 to obtain a similar amount of tunable pa-\nrameters (294’912) to adapter tuning. Obtaining a\nsimilar amount of tunable parameters for prompt\ntuning would require an exceptionally large prompt\nlength, even approaching the maximum acceptable\nsequence length of the pre-trained language models.\nTherefore, we only set the prompt length l = 16\n(which corresponds to 12’288 tunable parameters)\nto compare with prefix tuning under the same num-\nber of prepending tokens. Evaluation results are\nshown in Table 1.4\nIn general, the parameter-efficient methods are\neffective in reducing stereotype scores, and the\nreductions are statistically significant ( p <0.05)\nunder a permutation test (Ernst, 2004).\nAmong the three parameter-efficient methods,\nadapter tuning achieves the best debiasing per-\nformance on both CrowS-Pairs and StereoSet,\nfor both BERT and GPT-2. This demonstrates\nadapter tuning to be a reliable parameter-efficient\nmethod for bias mitigation across different types of\nlanguage models. Note that our results are also con-\nsistent with He et al. (2022)’s finding that modify-\ning transformer representations at the feed-forward\nsublayers (adapter tuning) is more effective than\nmodifying those at the multi-head attention sublay-\ners (prefix tuning).\n4Since SelfDebias preserves 32 tokens for its prefix tem-\nplates, when measuring perplexity for all the methods in Ta-\nble 1, the input sequence length is set to 480 (512-32) for\nBERT and 992 (1024-32) for GPT-2.\n15734\nGender Bias CrowS-Pairs\nStereotype Score\nStereoSet\nStereotype Score\nWikiText2\nPerplexity (↓)\nStereoSet LM\nScore (↑)\nBERT 57.25 60.28 5.167 84.17\n+Full Fine-Tune 56.11 ±2.15 56.43±0.72∗ 5.517±0.080 84.22±0.19\n+Prefix Tune (l= 16) 53.59 ±0.19∗ 57.82±0.46∗ 4.425±0.015 84.75±0.15\n+Prompt Tune (l= 16) 57.56 ±1.41 58.07±0.60∗ 4.641±0.033 84.71±0.16\n+Adapter Tune (r= 48) 51.68±0.52∗∗ 56.04±0.43∗∗ 4.931±0.043 84.97±0.14\n+SentenceDebias 52.29 59.37 5.181 84.20\n+SelfDebias 52.29 59.34 7.070 84.09\nGPT-2 56.87 62.65 29.669 91.01\n+Full Fine-Tune 55.88 ±1.27 61.88±0.55∗ 81.778±0.655 90.24±0.14\n+Prefix Tune (l= 16) 54.73 ±0.66∗ 61.35±0.60∗ 31.400±0.108 91.24±0.07\n+Prompt Tune (l= 16) 54.12 ±1.14∗ 61.30±0.43∗ 30.630±0.099 91.37±0.08\n+Adapter Tune (r= 48) 52.29±1.13∗∗ 60.33±0.46∗∗ 35.255±0.345 90.87±0.11\n+SentenceDebias 56.11 56.05 56.891 87.43\n+SelfDebias 56.11 60.84 31.482 89.07\nTable 1: Results on mitigating gender bias. For CrowS-Pairs and StereoSet, stereotype scores closer to 50 indicate\nless bias; for perplexity, lower values are better; for StereoSet LM score, higher values are better. For the CDA-\nbased methods, we report mean±std from five runs. The best score of all the debiasing methods for each metric\nis marked in bold. ∗: the reduction in stereotype score w.r.t. that of the original BERT/GPT-2 is statistically\nsignificant (p< 0.05). ∗∗: the stereotype score of adapter tuning is significantly (p< 0.05) lower than those of the\nother parameter-efficient methods.\nPrompt tuning is more effective on GPT-2 than\nBERT. Prompt tuning is ineffective in reducing\nthe CrowS-Pairs stereotype score on BERT, but\ncan successfully reduce it on GPT-2, where it even\nachieves a similar debiasing performance to prefix\ntuning. This is remarkable given that prompt tun-\ning has much less tunable parameters than prefix\ntuning. This is also consistent with prompt tuning\nbeing more effective when T5 (Raffel et al., 2020)\nis continuously pre-trained with an autoregressive\nlanguage modeling loss (Lester et al., 2021).\nComparing to post-hoc debiasing methods, para-\nmeter-efficient methods are better at maintain-\ning the language modeling ability while achiev-\ning a similar debiasing performance. Note\nthat post-hoc debiasing methods sometimes sig-\nnificantly worsen the language modeling ability,\ne.g., a perplexity of 7.070 for SelfDebias on BERT,\na perplexity of 56.891, and a LM score of 87.43\nfor SentenceDebias on GPT-2. Since a completely\nrandom language model would achieve the per-\nfect stereotype score (50), but is useless as a lan-\nguage model (Nadeem et al., 2021), the degraded\nlanguage modeling ability of the post-hoc debias-\ning methods undermines their true effectiveness\nfor bias mitigation. On the contrary, parameter-\nefficient methods keep the language modeling loss\nduring CDA training, which helps to preserve or\neven enhance the language modeling ability.\nComparing to full fine-tuning, parameter-effi-\ncient methods can achieve a better or similar\nperformance with improved time and memory\nefficiency. Since full fine-tuning updates all the pa-\nrameters of the language model, it is computation-\nally expensive and prone to be overfitting. When\ndebiasing BERT, full fine-tuning consumes around\n19 GB memory, while the parameter-efficient meth-\nods consume 12~17 GB memory. Training on the\ndebiasing corpus for full fine-tuning lasts around 6\nhours, while that for the parameter-efficient meth-\nods lasts 4~5 hours. For GPT-2, full fine-tuning\nconsumes around 18 GB memory with the training\ntime being around 7 hours, while the parameter-\nefficient methods consume 15~16 GB memory and\n5 hours of training time.\n5.3 Mitigating Racial and Religious Bias\nWhen mitigating racial and religious bias, we find\nthat a prefix length of l = 16 (or, equivalently, a\nreduction factor of r= 48 for adapter tuning) is no\nlonger sufficient for successful debiasing. There-\nfore, we search lin a broader range of {48, 96, 192,\n384} (and, correspondingly,rin {16, 8, 4, 2}). The\nresults are shown in Table 2.\nIn general, the parameter-efficient methods are\nless effective when it comes to racial and religious\nbias. Even the previously strongest method, adapter\ntuning, is ineffective in many cases such as de-\nbiasing BERT on the religion subsets of CrowS-\nPairs and StereoSet, and GPT-2 on the race sub-\nset of CrowS-Pairs. For GPT-2, prompt tuning is\nconsistently effective on the race subsets of both\n15735\nRacial Bias CrowS-Pairs\nStereotype Score\nStereoSet\nStereotype Score\nWikiText2\nPerplexity (↓)\nStereoSet LM\nScore (↑)\nBERT 62.33 57.03 4.899 84.17\n+Full Fine-Tune 57.65 ±3.61∗ 57.67±0.70 5.291±0.064 83.44±0.29\n+Prefix Tune (l= 192) 57.44 ±1.90∗ 56.95±0.39 4.448±0.008 84.35±0.12\n+Prompt Tune (l= 192) 58.25 ±3.90∗ 58.17±0.55 4.572±0.019 83.41±0.80\n+Adapter Tune (r= 4) 57.20 ±4.16∗ 59.10±0.45 4.903±0.071 84.34±0.20\n+SentenceDebias 62.72 57.78 4.949 83.95\n+SelfDebias 56.70 54.30 6.187 84.24\nGPT-2 59.69 58.90 32.712 91.01\n+Full Fine-Tune 60.04 ±0.48 56.68±0.37∗ 41.781±0.240 89.44±0.05\n+Prefix Tune (l= 384) 59.61 ±0.51 57.53±0.23∗ 35.346±0.073 89.48±0.08\n+Prompt Tune (l= 384) 58.76 ±0.92∗ 57.72±0.33∗ 33.983±0.266 89.18±0.10\n+Adapter Tune (r= 2) 61.28 ±1.27 57.77±0.44∗ 35.818±0.304 89.01±0.68\n+SentenceDebias 55.43 56.43 37.826 91.38\n+SelfDebias 53.29 57.33 34.851 89.53\nReligious Bias CrowS-Pairs\nStereotype Score\nStereoSet\nStereotype Score\nWikiText2\nPerplexity (↓)\nStereoSet LM\nScore (↑)\nBERT 62.86 59.70 6.172 84.17\n+Full Fine-Tune 65.33 ±2.73 60.76±1.38 6.762±0.059 83.67±0.18\n+Prefix Tune (l= 384) 72.76 ±1.55 60.61±0.98 5.372±0.010 85.42±0.09\n+Prompt Tune (l= 384) 83.05 ±1.85 60.07±1.12 5.483±0.048 83.80±0.58\n+Adapter Tune (r= 2) 68.00 ±4.33 58.93±1.19 6.135±0.019 84.45±0.19\n+SentenceDebias 63.81 58.73 6.185 84.26\n+SelfDebias 56.19 57.26 7.624 84.23\nGPT-2 62.86 63.26 32.712 91.01\n+Full Fine-Tune 54.86±1.29∗ 64.36±0.81 45.525±0.065 90.20±0.06\n+Prefix Tune (l= 384) 60.95 ±0.60∗ 65.16±0.56 35.226±0.073 90.95±0.03\n+Prompt Tune (l= 384) 58.29 ±1.52∗ 64.89±1.52 43.177±17.750 90.68±0.12\n+Adapter Tune (r= 2) 62.10 ±2.72 62.05±0.66∗ 39.732±0.695 90.31±0.10\n+SentenceDebias 35.24 59.62 60.204 90.53\n+SelfDebias 58.10 60.45 35.174 89.36\nTable 2: Results on mitigating racial bias (upper table) and religious bias (lower table). For CrowS-Pairs and\nStereoSet, stereotype scores closer to 50 indicate less bias; for perplexity5, lower values are better; for StereoSet\nLM score, higher values are better. For the CDA-based methods, we report mean±std from five runs. The best score\nof all the debiasing methods for each metric is marked in bold. ∗: the reduction in stereotype score w.r.t. that of the\noriginal BERT/GPT-2 is statistically significant (p< 0.05).\nCrowS-Pairs and StereoSet, but cannot obtain a\nsimilar performance on StereoSet’s religion subset.\nIn three out of the eight debiasing cases, none of\nthe parameter-efficient methods could reduce the\nstereotype score in a statistically significant way.\nMoreover, SelfDebias exhibits a superior debias-\ning performance over the parameter-efficient meth-\nods, and its language modeling ability does not\nseverely degenerate as in mitigating gender bias.\nIndeed, when we calculate the icatscore (Nadeem\net al., 2021), defined as lms ∗ min(ss,100 −\nss)/50 (lmsstands for the LM score, andssstands\nfor the stereotype score on StereoSet), to integrate\nthe debiasing performance and language modeling\nability, we can clearly see a better overall perfor-\n5For the race-debiased models, we set the input sequence\nlength to 320 for BERT and 640 for GPT-2; for the religion-\ndebiased models, we set the input sequence length to 128 for\nBERT and 640 for GPT-2.\nmance of SelfDebias over adapter tuning (e.g., on\nStereoSet’s religion subset, the icatscore of Self-\nDebias and adapter tuning is 72.00 vs. 69.37 for\nBERT, and70.68 vs.68.55 for GPT-2).\nThe less successful performance of parameter-\nefficient methods may be attributed to some lim-\nitations of the CDA debiasing method. The bias\nattribute word lists for race and religion are shorter\nand contain more noise (i.e., words with multiple or\nambiguous meanings) than that for gender, which\nmay undermine the diversity and quality of the\naugmented training corpus. On the contrary, Self-\nDebias relies on bias descriptions that contain less\nnoise and could generalize with the help of the lan-\nguage model’s own knowledge. Given this analysis,\nfuture work could explore how to adopt parameter-\nefficient methods to debiasing techniques other\nthan CDA to overcome these limitations.\n15736\n6 Impact on Internal Knowledge\n6.1 Fact Retrieval\nTo investigate the impact of bias mitigation on the\nfactual knowledge encoded in pre-trained language\nmodels, we take the gender-debiased models from\nSection 5.2 and evaluate them on the four LAMA\ndatasets (Petroni et al., 2019). 6 The results are\nshown in Table 3. We report the results from a\nsingle run (with the default seed 42) to save com-\nputation in Table 3 and 4.\nThe parameter-efficient methods can largely\nmaintain the factual knowledge of a language\nmodel, with the reduction in Precision@10 rang-\ning from 0 to 6.8% across all the datasets and\npre-trained models. Surprisingly, for BERT on\nSQuAD and GPT-2 on all the four datasets, quite\na number of the results are actually improved.\nWe attribute these improvements to the fact that\nWikipedia contains a lot of factual knowledge, and\ncontinuously training on it can enhance the internal\nknowledge of a language model.\nComparing the performance between full fine-\ntuning and parameter-efficient tuning, we find that\nthe former performs best on SQuAD with BERT\nand Google-RE with GPT-2, while the latter per-\nforms better in the rest of the settings. In general,\nthe performance gaps are marginal.\n6.2 Downstream Fine-Tuning\nWe further investigate the impact of bias mitiga-\ntion on knowledge transfer to downstream tasks via\nfine-tuning. Since neural network models suffer\nfrom catastrophic forgetting (French, 1999), a debi-\nased model may forget the encoded knowledge in\nthe original language model, and conversely a fine-\ntuned model may forget the debiasing knowledge\nin the debiased model. Therefore, it is important\nto adopt an evaluation dataset that can simultane-\nously evaluate downstream task performance and\ndebiasing performance. We choose the coreference\nresolution dataset WinoBias (Zhao et al., 2018) to\nfulfill the above requirements.\nWe append each example from WinoBias (e.g.,\nThe physician hired the secretary because he was\noverwhelmed with clients.) with the suffix “{Pro-\nnoun} refers to the{Candidate}.” ({Pronoun} is\n6Instead of using the intersectional vocabulary of several\npre-trained models, as in Petroni et al. (2019), we adopt each\npre-trained model’s full vocabulary, since we do not aim to\ncompare the performance across different pre-trained models.\n“He” in this example), and then measure the prob-\nability of the model completing the sentence with\ndifferent candidates (“physician” and “secretary”\nin this example) to determine the coreference re-\nsult. We adopt both the type-1 and type-2 test sets\nof WinoBias, where type-1 examples are harder\nto resolve as they contain no syntactic cues. We\nadopt WinoBias’ dev set to fine-tune an original\npre-trained language model using either full fine-\ntuning or parameter-efficient tuning.7 The results\nare shown in Table 4.\nOn type-1 examples, adapter tuning achieves\na comparable performance to full fine-tuning\nfor both BERT and GPT-2, with the reduction\nin average F1 scores less than 3.3%. On BERT,\nadapter tuning achieves a much better debiasing\nperformance (Diff= 0 .51) than full fine-tuning,\nwhile on GPT-2 it is slightly more biased. Nev-\nertheless, both of them can be considered effective\nsimultaneously on the coreference resolution task\nand debiasing task. The performance gap between\nfull fine-tuning and prefix/prompt tuning is more\nsignificant, but the latter can still achieve a nearly\nperfect performance on the easier type-2 examples.\n7 Conclusion\nIn this study, we investigated the performance of\nprefix tuning, prompt tuning, and adapter tuning on\nmitigating social bias and preserving the linguistic\nand factual knowledge for two types of pre-trained\nlanguage models. Our results demonstrated the\neffectiveness and efficacy of parameter-efficient\nmethods in combination with CDA, and also re-\nvealed their performance limitations by comparing\nto post-hoc debiasing methods. We hope that our\nstudy can make it more accessible for others to\ndebias pre-trained language models with reduced\ncomputational requirements, and contribute to fair\nand inclusive NLP.\n8 Limitations\nDue to the restrictions of the adopted benchmarks\nand resources, our evaluation bears the following\nlimitations: (i) We only focus on social biases\nin the English language and North American cul-\ntures. This is due to the fact that both CrowS-\nPairs and StereoSet are generated by crowd work-\ners from North America. Future work can extend\nour analysis to other languages and cultures with\n7See Appendix B for more details.\n15737\nGoogle-RE T-REx ConceptNet SQuAD\nP@1 P@10 MRR P@1 P@10 MRR P@1 P@10 MRR P@1 P@10 MRR\nBERT 9.25 28.69 15.96 29.48 56.87 38.63 15.11 38.77 23.10 13.11 44.59 23.30\n+Full Fine-Tune 7.47 21.43 12.43 26.93 53.72 35.85 14.89 37.59 22.57 14.43 47.21 24.78\n+Prefix Tune (l= 16) 8.23 22.54 13.43 27.68 53.64 36.38 15.05 37.42 22.73 12.79 46.56 23.91\n+Prompt Tune (l= 16) 8.68 23.19 14.04 28.28 53.59 36.88 14.58 36.58 22.11 12.79 46.89 23.54\n+Adapter Tune (r= 48) 8.51 21.97 13.39 26.92 51.65 35.27 14.75 36.47 22.13 11.80 44.26 22.59\nGPT-2 1.51 10.88 5.04 9.36 31.10 16.78 5.91 19.01 10.42 3.15 17.48 7.53\n+Full Fine-Tune 3.40 15.10 7.44 7.76 33.04 15.90 4.87 16.47 8.86 1.75 18.18 6.78\n+Prefix Tune (l= 16) 2.33 12.56 6.14 10.13 33.38 17.98 5.99 19.42 10.53 2.10 17.83 7.53\n+Prompt Tune (l= 16) 1.14 9.79 4.39 8.00 30.29 15.70 5.95 19.03 10.53 2.45 16.78 7.16\n+Adapter Tune (r= 48) 2.49 14.11 6.59 9.35 32.61 17.20 5.79 19.09 10.26 2.10 18.18 7.03\nTable 3: Fact retrieval results of the original and debiased models on the four LAMA datasets. For all the metrics\n(precision-at-1 (P@1), precision-at-10 (P@10), and mean reciprocal rank (MRR)), higher values are better. For the\ndebiased models, the best score under each metric is in bold, while the scores not worse than those from the original\nBERT/GPT-2 are highlighted in green .\nType-1 Type-2\nF1−pro F1−anti Avg Diff F1−pro F1−anti Avg Diff\nBERT\n+Full Fine-Tune 70.95 68.04 69.50 2.91 99.49 99.49 99.49 0\n+Prefix Tune (l= 16) 65.08 64.57 64.83 0.51 99.49 99.49 99.49 0\n+Prompt Tune (l= 16) 56.56 53.33 54.95 3.23 99.24 99.24 99.24 0\n+Adapter Tune (r= 48) 66.50 65.99 66.25 0.51 99.49 99.49 99.49 0\nGPT-2\n+Full Fine-Tune 63.33 63.47 63.40 -0.14 99.49 99.49 99.49 0\n+Prefix Tune (l= 16) 51.66 52.79 52.23 -1.13 99.49 99.49 99.49 0\n+Prompt Tune (l= 16) 53.46 52.36 52.91 1.10 99.24 99.24 99.24 0\n+Adapter Tune (r= 48) 60.70 59.96 60.33 0.74 99.49 99.49 99.49 0\nTable 4: Evaluation results on the WinoBias’ type-1 and type-2 test sets. We report the F1 score on the pro-\nstereotypical examples (F1−pro), anti-stereotypical examples (F1−anti), their average (Avg), and their difference\n(Diff) to measure the models’ performance on both the coreference resolution task and the bias mitigation task.\nthe corresponding resources such as the French\nCrowS-Pairs (Névéol et al., 2022) and multilin-\ngual WEAT (Lauscher and Glavaš, 2019). (ii) Our\nevaluation has a limited coverage over different\nkinds of harms according to Blodgett et al. (2020).\nCrowS-Pairs, StereoSet, and WinoBias all focus\non stereotyping, a kind of representational harm,\nwhile others like allocational harms are untouched.\nDeveloping methods to measure these harms gener-\nally requires in-depth interactions between technol-\nogists and customers. Blodgett et al. (2021) also\npoint out several conceputalization and operational-\nization pitfalls in the above three bias benchmarks,\nwhich limits the validity of the results evaluated\non them. (iii) Due to the incomplete bias attribute\nword lists, our CDA-based debiasing method is\nby no means fair enough to cover all the minor-\nity groups (e.g., groups with non-binary genders).\nTherefore the current debiasing method in this pa-\nper can only be used to mitigate bias among the\ndemographic groups mentioned in Appendix A. We\nrecommend more complete resources such as the\ngender-inclusive word list in (Cao and Daumé III,\n2021) for real-world scenarios.\nAcknowledgements\nThis work was supported by the Alan Turing Insti-\ntute under the EPSRC grant EP/N510129/1, by the\nAXA Research Fund, and by the EU TAILOR grant\n952215. We also acknowledge the use of Oxford’s\nARC facility, of the EPSRC-funded Tier 2 facili-\nties JADE (EP/P020275/1), and of GPU computing\nsupport by Scan Computers International Ltd.\nReferences\nVamsi Aribandi, Yi Tay, and Donald Metzler. 2021.\nHow reliable are model diagnostics? In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1778–1785, Online. Association\nfor Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\n15738\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nYang Trista Cao and Hal Daumé III. 2021. Toward\ngender-inclusive coreference resolution: An analysis\nof gender and bias throughout the machine learning\nlifecycle*. Computational Linguistics, 47(3):615–\n661.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2021. FairFil: Contrastive neu-\nral debiasing method for pretrained text encoders. In\nInternational Conference on Learning Representa-\ntions.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nZhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang\nLiu, Jie Tang, Juanzi Li, and Maosong Sun. 2022.\nDelta tuning: A comprehensive study of parameter\nefficient methods for pre-trained language models.\nCoRR, arXiv:2203.06904.\nMichael D. Ernst. 2004. Permutation Methods: A Basis\nfor Exact Inference. Statistical Science, 19(4):676 –\n685.\nRobert M. French. 1999. Catastrophic forgetting in con-\nnectionist networks. Trends in Cognitive Sciences,\n3(4):128–135.\nAparna Garimella, Akhash Amarnath, Kiran Kumar,\nAkash Pramod Yalla, Anandhavelu N, Niyati Chhaya,\nand Balaji Vasan Srinivasan. 2021. He is very intel-\nligent, she is very beautiful? On Mitigating Social\nBiases in Language Modelling and Generation. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4534–4545, On-\nline. Association for Computational Linguistics.\nMichael Gira, Ruisu Zhang, and Kangwook Lee. 2022.\nDebiasing pre-trained language models via efficient\nfine-tuning. In Proceedings of the Second Workshop\non Language Technology for Equality, Diversity and\nInclusion, pages 59–69, Dublin, Ireland. Association\nfor Computational Linguistics.\nYue Guo, Yi Yang, and Ahmed Abbasi. 2022. Auto-\ndebias: Debiasing masked language models with\nautomated biased prompts. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1012–1023, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. CoRR, arXiv:2203.15556.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference\non Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 2790–2799.\nPMLR.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nProceedings of the 16th Conference of the European\n15739\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1256–1266, Online.\nAssociation for Computational Linguistics.\nAnne Lauscher and Goran Glavaš. 2019. Are we con-\nsistently biased? multidimensional analysis of biases\nin distributional word vectors. In Proceedings of the\nEighth Joint Conference on Lexical and Computa-\ntional Semantics (*SEM 2019), pages 85–91, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5502–5515, Online. Association for\nComputational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nAurélie Névéol, Yoann Dupont, Julien Bezançon, and\nKarën Fort. 2022. French CrowS-pairs: Extending a\nchallenge dataset for measuring social bias in masked\nlanguage models to a language other than English.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 8521–8531, Dublin, Ireland.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n487–503, Online. Association for Computational Lin-\nguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. AdapterHub: A\nframework for adapting transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 46–54, Online. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal Report.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\n15740\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, arXiv:2201.11990.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi,\nand Slav Petrov. 2020. Measuring and reducing\ngendered correlations in pre-trained models. CoRR,\narXiv:2010.06032.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 629–634, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nRan Zmigrod, Sabrina J. Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmenta-\ntion for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1651–1661, Florence, Italy. Asso-\nciation for Computational Linguistics.\n15741\nA Bias Attribute Words\nWe adopt the same bias attribute words as Meade\net al. (2022), where the list for gender is from Zhao\net al. (2018) and that for religion is from Liang et al.\n(2020).\nGender: (actor, actress), (actors, actresses), (air-\nman, airwoman), ( uncle, aunt), ( uncles, aunts),\n(boy, girl), (boys, girls), (groom, bride), (grooms,\nbrides), (brother, sister), (brothers, sisters), (busi-\nnessman, businesswoman), ( businessmen, busi-\nnesswomen), (chairman, chairwoman), (chairmen,\nchairwomen), (dude, chick), (dudes, chicks), (dad,\nmom), (dads, moms), (daddy, mommy), (daddies,\nmommies), (son, daughter), (sons, daughters), (fa-\nther, mother), (fathers, mothers), (male, female),\n(males, females), (guy, gal), (guys, gals), (gentle-\nman, lady), (gentlemen, ladies), (grandson, grand-\ndaughter), ( grandsons, granddaughters), ( guy,\ngirl), ( guys, girls), ( he, she), ( himself, herself ),\n(him, her), (his, her), (husband, wife), (husbands,\nwives), (king, queen), (kings, queens), (lord, lady),\n(lords, ladies), (sir, maam), (man, woman), (men,\nwomen), (sir, miss), (mr., mrs.), (mr., ms.), (police-\nman, policewoman), (prince, princess), (princes,\nprincesses), (spokesman, spokeswoman), (spokes-\nmen, spokeswomen)\nReligion: (jewish, christian, muslim), ( jews,\nchristians, muslims), (torah, bible, quran), (syn-\nagogue, church, mosque), ( rabbi, priest, imam),\n(judaism, christianity, islam)\nRace: (black, caucasian, asian), (african, cau-\ncasian, asian), (black, white, asian), (africa, amer-\nica, asia), (africa, america, china), (africa, europe,\nasia)\nB Additional Training Details\nFor all the experiments on parameter-efficient tun-\ning methods and full fine-tuning, we use the default\nsettings of the AdamW optimizer (Loshchilov and\nHutter, 2019) and a linear learning rate scheduler\nfrom the Hugging Face library.\nFor the debiasing experiments trained on\nWikipedia, we fix the number of training epochs\nto 2 and greedily search initial learning rate from\n{5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6, 5e-7} according\nto the language modeling loss on the validation set\n(we use 5% of the augmented debiasing corpus for\nvalidation). For experiments trained on WinoBias,\nwe greedily search training epochs from {10, 20,\n30, 50, 100, 200} and initial learning rate from\n{5e-1, 5e-2, 5e-3, 5e-4, 5e-5, 5e-6, 5e-7} accord-\ning to the Avg F1 score on type-1 examples in the\nvalidation set (we use 5% of the training set for val-\nidation). The hyperparameter values to reproduce\nour results in Sections 5 and 6 are in Table 5.\nImplementations of SentenceDebias and SelfDe-\nbias are based on Meade et al. (2022)’s, where we\nalso follow their default parameter settings.\nlr epoch bsz\nFor results in Table 1 (gender bias)\nBERT\n+Full Fine-Tune 5e-5 2 16\n+Prefix Tune (l= 16) 5e-3 2 16\n+Prompt Tune (l= 16) 5e-1 2 16\n+Adapter Tune (r= 48) 5e-4 2 16\nGPT-2\n+Full Fine-Tune 5e-5 2 8\n+Prefix Tune (l= 16) 5e-3 2 8\n+Prompt Tune (l= 16) 5e-2 2 8\n+Adapter Tune (r= 48) 5e-4 2 8\nFor results in Table 2’s upper sub-table (racial bias)\nBERT\n+Full Fine-Tune 5e-5 2 16\n+Prefix Tune (l= 192) 5e-3 2 16\n+Prompt Tune (l= 192) 5e-3 2 16\n+Adapter Tune (r= 4) 5e-4 2 16\nGPT-2\n+Full Fine-Tune 5e-6 2 8\n+Prefix Tune (l= 384) 5e-3 2 8\n+Prompt Tune (l= 384) 5e-1 2 8\n+Adapter Tune (r= 2) 5e-3 2 8\nFor results in Table 2’s lower sub-table (religious bias)\nBERT\n+Full Fine-Tune 5e-5 2 16\n+Prefix Tune (l= 384) 5e-3 2 16\n+Prompt Tune (l= 384) 5e-3 2 16\n+Adapter Tune (r= 2) 5e-4 2 16\nGPT-2\n+Full Fine-Tune 5e-6 2 8\n+Prefix Tune (l= 384) 5e-3 2 8\n+Prompt Tune (l= 384) 5e-1 2 8\n+Adapter Tune (r= 2) 5e-5 2 8\nFor results in Table 4 (WinoBias)\nBERT\n+Full Fine-Tune 5e-6 30 16\n+Prefix Tune (l= 16) 5e-2 20 16\n+Prompt Tune (l= 16) 5e-1 20 16\n+Adapter Tune (r= 48) 5e-4 20 16\nGPT-2\n+Full Fine-Tune 5e-5 20 16\n+Prefix Tune (l= 16) 5e-3 200 16\n+Prompt Tune (l= 16) 5e-4 100 16\n+Adapter Tune (r= 48) 5e-4 50 16\nTable 5: Hyperparameter values adopted during training.\n“lr” denotes initial learning rate; “epoch” denotes total\ntraining epochs; “bsz” denotes batch size.\n15742\nDebiasing\nCorpus\nCrowS-Pairs\nStereotype Score\nStereoSet\nStereotype Score\nWikiText2\nPerplexity (↓)\nStereoset LM\nScore (↑)\nGPT-2 56.87 62.65 29.669 91.01\nWikipedia +Full Fine-Tune 56.87 61.30 80.499 90.23\n(single +Prefix Tune (l= 16) 55.34 62.02 31.567 91.14\nsentence) +Prompt Tune (l= 16) 52.29 60.95 30.534 91.29\n+Adapter Tune (r= 48) 51.15 60.50 34.910 90.80\nWikipedia +Full Fine-Tune 56.49 61.74 56.527 90.19\n(example +Prefix Tune (l= 16) 58.40 62.67 31.935 91.22\nlength=1024 +Prompt Tune (l= 16) 56.87 63.37 32.461 91.03\ntokens) +Adapter Tune (r= 48) 59.92 62.31 34.527 90.75\nOpenWebText +Full Fine-Tune 55.73 62.43 38.252 90.60\n(example +Prefix Tune (l= 16) 53.44 60.94 31.592 90.31\nlength=1024 +Prompt Tune (l= 16) 53.05 62.68 30.464 91.41\ntokens) +Adapter Tune (r= 48) 56.87 61.94 33.130 90.87\nTable 6: Results on gender debiasing and language modeling for GPT-2 using different debiasing corpora.\nC Effect of the Debiasing Corpus for\nGPT-2\nFor consistency, we adopt the same debiasing cor-\npus (the English Wikipedia) for both BERT and\nGPT-2 in Section 5.2, where each training exam-\nple consists of a single sentence (the average sen-\ntence length in our corpus is around 107 tokens).\nHowever, this setting is different from the origi-\nnal pre-training settings of GPT-2 (Radford et al.,\n2019) in terms of example length and data source.\nTherefore, we further investigate debiasing GPT-2\non two other debiasing corpora: for one corpus, we\nstill use Wikipedia but concatenate all the sentences\ninto a long sequence and truncate it into examples\nof 1024 tokens; for the other corpus, we use 1%\nof OpenWebText8, which is a public replicate of\nGPT-2’s private pre-training corpus, and truncate\nit into examples of 1024 tokens. The results are\nshown in Table 6.9\nComparing the results on Wikipedia, with single\nsentence and example length 1024 tokens, in Ta-\nble 6, we can see that the former is consistently bet-\nter. This indicates that these methods favor shorter\nexample lengths. We conjecture that this is due\nto GPT-2’s language modeling objective being an\naverage over all the tokens in an example. There-\nfore, the counterfactual token’s signal will be less\nsignificant if it is close to the end of a long example.\nComparing the last two blocks, we can see that\nthe results from the debiasing methods trained\non OpenWebText are superior to those trained on\nWikipedia under the same example length of 1024.\n8https://skylion007.github.io/\nOpenWebTextCorpus/\n9We report the results from a single run (with the default\nseed 42) to save computation.\nThis indicates that using a similar data source to\nthe original pre-training corpus is beneficial. For\nfull fine-tuning, this can improve perplexity to\n38.252. For the parameter-efficient methods, the\nimprovements are more significant on stereotype\nscores. Given that parameter-efficient methods’\nmodel capacity is limited, if we allocate some ca-\npacity for adapting to new data sources, it is reason-\nable for the debiasing performance to be negatively\naffected.\n15743\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nsection 8 \"Limitations\"\n□\u0013 A2. Did you discuss any potential risks of your work?\nsection 8 \"Limitations\"\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nin the abstract and section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nsection 5, 6 and Appendix A, B, C\n□\u0013 B1. Did you cite the creators of artifacts you used?\nsection 5, 6 and Appendix A, B, C\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nwe adhere to each artifact’s original licnese and terms.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsection 8 \"Limitations\"\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We follow previous work and the data creator’s practices.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 8 \"Limitations\", Appendix A\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nsection 5, 6 and Appendix B\nC □\u0013 Did you run computational experiments?\nsection 5, 6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 5, 6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15744\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 5, 6 and Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nsection 5, 6\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nsection 5 and Appendix B\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n15745",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9718308448791504
    },
    {
      "name": "Computer science",
      "score": 0.8164366483688354
    },
    {
      "name": "Adapter (computing)",
      "score": 0.7588027715682983
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.6365311741828918
    },
    {
      "name": "Language model",
      "score": 0.5605992078781128
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43813925981521606
    },
    {
      "name": "Machine learning",
      "score": 0.3916895091533661
    },
    {
      "name": "Natural language processing",
      "score": 0.3212472200393677
    },
    {
      "name": "Computer hardware",
      "score": 0.07634532451629639
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Cognitive science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I145847075",
      "name": "TU Wien",
      "country": "AT"
    }
  ]
}