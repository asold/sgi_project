{
  "title": "AutoPruner: transformer-based call graph pruning",
  "url": "https://openalex.org/W4297677548",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013306670",
      "name": "Thanh Le-Cong",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A5027335548",
      "name": "Hong Jin Kang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A5112317707",
      "name": "Truong Giang Nguyen",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A5081510237",
      "name": "Stefanus Agus Haryono",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A5081036622",
      "name": "David Lo",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A5075260906",
      "name": "Xuan-Bach D. Le",
      "affiliations": [
        "The University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A5070728906",
      "name": "Quyet Thang Huynh",
      "affiliations": [
        "Hanoi University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1959797194",
    "https://openalex.org/W3018033251",
    "https://openalex.org/W3199249334",
    "https://openalex.org/W4247889999",
    "https://openalex.org/W3162962341",
    "https://openalex.org/W2151328120",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2514084604",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2888223970",
    "https://openalex.org/W2808651716",
    "https://openalex.org/W2899384793",
    "https://openalex.org/W2344444819",
    "https://openalex.org/W1504196925",
    "https://openalex.org/W4233410239",
    "https://openalex.org/W3206570943",
    "https://openalex.org/W2754335648",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W4393844134",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2899357780",
    "https://openalex.org/W3195442242",
    "https://openalex.org/W2079877139",
    "https://openalex.org/W3174220201",
    "https://openalex.org/W2109427294",
    "https://openalex.org/W4312940931",
    "https://openalex.org/W3180903877",
    "https://openalex.org/W2909588096",
    "https://openalex.org/W4284685949",
    "https://openalex.org/W3100869085",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2959352824",
    "https://openalex.org/W2047764386",
    "https://openalex.org/W1966831167",
    "https://openalex.org/W2807866521",
    "https://openalex.org/W3161685529",
    "https://openalex.org/W3089825636",
    "https://openalex.org/W2950368691",
    "https://openalex.org/W2508295954",
    "https://openalex.org/W4206639582",
    "https://openalex.org/W4255421341",
    "https://openalex.org/W4284671049",
    "https://openalex.org/W4210493608",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3105535951",
    "https://openalex.org/W3105735055",
    "https://openalex.org/W3100026183",
    "https://openalex.org/W4238124605"
  ],
  "abstract": "Constructing a static call graph requires trade-offs between soundness and\\nprecision. Program analysis techniques for constructing call graphs are\\nunfortunately usually imprecise. To address this problem, researchers have\\nrecently proposed call graph pruning empowered by machine learning to\\npost-process call graphs constructed by static analysis. A machine learning\\nmodel is built to capture information from the call graph by extracting\\nstructural features for use in a random forest classifier. It then removes\\nedges that are predicted to be false positives. Despite the improvements shown\\nby machine learning models, they are still limited as they do not consider the\\nsource code semantics and thus often are not able to effectively distinguish\\ntrue and false positives. In this paper, we present a novel call graph pruning\\ntechnique, AutoPruner, for eliminating false positives in call graphs via both\\nstatistical semantic and structural analysis. Given a call graph constructed by\\ntraditional static analysis tools, AutoPruner takes a Transformer-based\\napproach to capture the semantic relationships between the caller and callee\\nfunctions associated with each edge in the call graph. To do so, AutoPruner\\nfine-tunes a model of code that was pre-trained on a large corpus to represent\\nsource code based on descriptions of its semantics. Next, the model is used to\\nextract semantic features from the functions related to each edge in the call\\ngraph. AutoPruner uses these semantic features together with the structural\\nfeatures extracted from the call graph to classify each edge via a feed-forward\\nneural network. Our empirical evaluation on a benchmark dataset of real-world\\nprograms shows that AutoPruner outperforms the state-of-the-art baselines,\\nimproving on F-measure by up to 13% in identifying false-positive edges in a\\nstatic call graph.\\n",
  "full_text": "AutoPruner: Transformer-Based Call Graph Pruning\nThanh Le-Cong\nHong Jin Kang\nTruong Giang Nguyen\nStefanus Agus Haryono\nDavid Lo\nSingapore Management University\nSingapore, Singapore\nXuan-Bach D. Le\nUniversity of Melbourne\nMelbourne, Victoria, Australia\nQuyet Thang Huynh\nHanoi University of Science and\nTechnology\nHanoi, Vietnam\nABSTRACT\nConstructing a static call graph requires trade-offs between sound-\nness and precision. Program analysis techniques for constructing\ncall graphs are unfortunately usually imprecise. To address this\nproblem, researchers have recently proposed call graph pruning\nempowered by machine learning to post-process call graphs con-\nstructed by static analysis. A machine learning model is built to\ncapture information from the call graph by extracting structural\nfeatures for use in a random forest classifier. It then removes edges\nthat are predicted to be false positives. Despite the improvements\nshown by machine learning models, they are still limited as they\ndo not consider the source code semantics and thus often are not\nable to effectively distinguish true and false positives.\nIn this paper, we present a novel call graph pruning technique,\nAutoPruner, for eliminating false positives in call graphs via both\nstatistical semantic and structural analysis. Given a call graph con-\nstructed by traditional static analysis tools, AutoPruner takes a\nTransformer-based approach to capture the semantic relationships\nbetween the caller and callee functions associated with each edge\nin the call graph. To do so,AutoPruner fine-tunes a model of code\nthat was pre-trained on a large corpus to represent source code\nbased on descriptions of its semantics. Next, the model is used to\nextract semantic features from the functions related to each edge in\nthe call graph. AutoPruner uses these semantic features together\nwith the structural features extracted from the call graph to classify\neach edge via a feed-forward neural network. Our empirical evalu-\nation on a benchmark dataset of real-world programs shows that\nAutoPruner outperforms the state-of-the-art baselines, improving\non F-measure by up to 13% in identifying false-positive edges in a\nstatic call graph. Moreover, AutoPruner achieves improvements\non two client analyses, including halving the false alarm rate on\nnull pointer analysis and over 10% improvements on monomorphic\ncall-site detection. Additionally, our ablation study and qualitative\nanalysis show that the semantic features extracted byAutoPruner\ncapture a remarkable amount of information for distinguishing\nbetween true and false positives.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\nÂ© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9413-0/22/11. . . $15.00\nhttps://doi.org/10.1145/3540250.3549175\nCCS CONCEPTS\nâ€¢ Software and its engineering â†’Automated static analysis ;\nâ€¢ Computing methodologies â†’Machine learning.\nKEYWORDS\nCall Graph Pruning, Static Analysis, Pretrained Language Model,\nTransformer\nACM Reference Format:\nThanh Le-Cong, Hong Jin Kang, Truong Giang Nguyen, Stefanus Agus\nHaryono, David Lo, Xuan-Bach D. Le, and Quyet Thang Huynh. 2022.\nAutoPruner: Transformer-Based Call Graph Pruning. In Proceedings of\nthe 30th ACM Joint European Software Engineering Conference and Sympo-\nsium on the Foundations of Software Engineering (ESEC/FSE â€™22), November\n14â€“18, 2022, Singapore, Singapore. ACM, New York, NY, USA, 13 pages.\nhttps://doi.org/10.1145/3540250.3549175\n1 INTRODUCTION\nCall graphs construction is crucial for program analyses. Call graphs\ncapture invocations between functions of programs [7, 43]. An ideal\ncall graph is (1) sound, i.e., it does not miss any true invocations, and\n(2) precise, i.e., it does not produce any false positives. However,\neven for small programs, constructing a sound and precise call\ngraph is difficult [1]. A call graph analysis should make reasonable\ntrade-offs between soundness and precision. Unfortunately, recent\nwork [51] has found that widely used tools such as WALA [12] or\nPetablox [30] construct imprecise call graphs; up to 76% of edges\nin call graphs constructed by WALA are false positives.\nTo address these issues, researchers [4, 30, 48] have proposed\nto improve pointer analysis, which is the core of many call graph\nconstructions algorithms, by improving context-sensitivity or flow-\nsensitivity of the analysis. Unfortunately, a perfect pointer analysis\nis generally not possible [42]. Specifically, pointer analyses usually\nface an expensive trade-off between scalability and precision [27].\nFor example, a context-sensitive analysis by WALA only reduces\n8.6% false positives rate over a context-insensitive analysis while\nincurring a large performance overhead [51].\nA recent approach, which we refer to ascgPruner [51], achieved\na breakthrough in improving the quality of call graphs. Instead of\ndirectly improving pointer analysis, cgPruner performs call graph\npruning as a post-processing technique on a call graph constructed\nthrough static analysis. Using machine learning techniques, the call\ngraph pruner removes false positives in a call graph. Specifically,\ncgPruner first extracts a set of structural features from the call\ngraph, e.g., the number of outgoing edges from the call-site and\nthe in-degree of the callee. It then leverages a learning model, i.e.,\narXiv:2209.03230v1  [cs.SE]  7 Sep 2022\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\nRandom Forest, to predict if a caller-callee edge is a false positive\n(i.e., the caller does not invoke the callee in reality). The call graph is\nupdated by removing edges that are predicted to be false positives.\nSince the cost of generating predictions is low, a machine learning-\nbased approach does not incur a significant performance overhead.\nTheir experiments show that cgPruner successfully improves over\ntraditional call graph analysis by producing call graphs that elimi-\nnate a large number of false positives. However, cgPruner is still\nlimited as it does not consider source code semantics and thus is\nnot able to distinguish true and false postives effectively.\nIn this paper, we propose a novel technique, AutoPruner, that\nincorporates both structural and statistical semantic information\nto prune false positives in call graphs effectively. AutoPruner\ncombines structural features extracted from a call graph with se-\nmantic features extracted from the source code of the caller and\ncallee functions. Similar to cgPruner, AutoPruner uses hand-\ncrafted structural features. However, different from cgPruner, Au-\ntoPruner automatically extracts semantic features via deep learn-\ning. AutoPruner thus faces a unique challenge on how to use deep\nlearning to automatically learn from a limited dataset. To address\nthis, we leverage recently proposed transformer models of code, i.e.,\nCodeBERT [11], that has been pre-trained on a corpus containing\nmillions of code functions. As our task differs from CodeBERTâ€™s\npre-training task, AutoPruner first fine-tunes CodeBERT such\nthat it captures the statistical relationships between caller and callee\nfunctions, learning to distinguish between true and false positive\nedges based on their source code. Next, AutoPruner leverages the\nfine-tuned model to extractsemantic features of each edge, based on\nthe source code of the caller and callee functions. Each edge then\nhas an embedding that represents the semantic relationship be-\ntween the caller and the callee. For each embedding, AutoPruner\ncombines it with the handcrafted structural features to obtain a\nrepresentation for each edge. Based on this representation, Auto-\nPruner employs a neural classifier to classify each edge in a call\ngraph as a true or false positive.\nWe evaluate AutoPruner on call graphs produced by three\nwell-known tools, i.e., WALA [ 12], Doop [ 4], and Petablox [ 30],\nfor real-world programs taken from the NJR-1 benchmark [36] in\nthe same setting of cgPruner [51]. We compare the call graph\ngenerated by AutoPruner against multiple baselines, including\nthe original call graphs produced by WALA, Doop and Petablox,\nthe call graphs pruned by state-of-the-art approach cgPruner, as\nwell as a graph neural network model, that applies deep learning\nto the call graphs. The latter two baselines consider only structural\ninformation. Our experiments show that AutoPruner improves\nover the state-of-the-art approach by up to 13% in F-measure. Our\nexperiments demonstrate that the use of the semantic features\nextracted by the transformer-based model enables AutoPruner to\noutperform approaches that consider only structural information.\nWe investigate the effect of pruned call graphs produced by\nAutoPruner on client analyses, which take the call graphs as\ninput to perform other analyses on the programs. We investigate\ntwo client analyses: null pointer analysis and monomorphic call-\nsite detection. On null pointer analysis, call graphs pruned by our\napproach AutoPruner lead to significantly reduced false alarm\nrate of only 12% while the call graphs pruned by cgPruner [51]\nand the call graphs constructed by WALA [ 12] have false alarm\nrates in null pointer analysis of 23% and 73%. On monomorphic\ncall-site detection, AutoPruner improves over cgPruner by over\n8% in terms of F-measure.\nTo better understand AutoPruner, we also perform qualitative\nanalysis on its performance. We leverage t-SNE [52] to visualize\nthe embedding of the call graph edges in a two-dimensional space.\nWe find that the semantic features can separate the true and false-\npositive edges, demonstrating thatAutoPruner captures a remark-\nable amount of information from the source code associated with\neach call graph edge.\nIn summary, we make the following contributions:\nâ€¢We introduce AutoPruner, a novel call graph pruner that\nuses both code and structural feature to identify false-positive\nedges in a call graph.\nâ€¢We empirically demonstrate that pruned call graph produced\nby our approach can help analysis tool significantly improve\nthe false alarm rate and F-measure. Notably, in the analysis\nclient of null pointer analysis, AutoPruner leads to over\n150 more reported warnings while decreasing false alarm\nrate from 73% to 12%.\nâ€¢We perform an ablation study and qualitative analysis to\nbetter understand our approach. Our analysis validates the\nuse of our proposed approach for call graph pruning.\nThe paper is structured as follows: Section 2 introduces the\nbackground of our work. Section 3 describes our proposed approach.\nSection 4 presents our experimental setup and results. Section 5\ndiscusses our qualitative analysis and threats to validity. Section\n6 covers related work. Finally, Section 7 concludes and describes\nfuture directions.\n2 BACKGROUND\nIn this section, we discuss a motivating example. Next, we present\nthe formal formulation of the call graph pruning problem and in-\ntroduce Transformer-based models of code and CodeBERT [11].\n2.1 Motivating Example\nIn Figure 1, we present a motivating example to motivate our ap-\nproach and demonstrate the limitations of cgPruner that uses\nonly structural features to prune call graphs. The source code\nof the acceptState function contains a call to the parse func-\ntion. In the original unpruned call graph, the acceptState node is\nconnected to multiple parse nodes of classes that implement the\nMathExpressionParser interface, e.g. the interface is implemented\nby EndOfExpressionParser, FunctionLeftParenthesisParser,\nand other classes that also override accept. At runtime, the\nacceptState function is invoked multiple times with different\nvalues of state, resulting in calls between acceptState and the\nmultiple parse nodes. These edges in the call graph are true edges\nas they represent calls that occur at runtime.\nDue to a large number of outgoing edges from the same call\nsite, the local structure of each acceptState to parse edge in the\ncall graph resembles edges that are false positives. A structural-\nonly approach such as cgPruner, therefore, incorrectly prunes all\nthe edges between the acceptState node and the accept node in\nthe call graph. This highlights the limitation of considering only\nthe structural features of the call graph and motivates the need\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\nFigure 1: The parser.parse call depends on the parameter state. parsers.get returns an object of one of multiple classes\nimplementing MathExpressionParser. Throughout the course of the program execution, acceptState can be invoked with all\npossible values of state. Therefore, despite the large number of outgoing edges from the same call site, all edges to the parse\nnodes from acceptState in the statically computed call graph are true edges. However, the large number of outgoing edges is\na feature used to prune false positives, which leads to the incorrect pruning of the edges related to the parse calls.\nfor guiding call graph pruning with the semantics of the source\ncode. From analyzing the source code, we can correctly identify\nthat a large number of outgoing edges are possible, as the specific\nparse call depends on the parameters of the function. Indeed, Au-\ntoPruner correctly leaves the edges unpruned due to its use of\nsemantic features extracted from the source code by CodeBERT.\n2.2 Call Graph Pruning\n2.2.1 Problem Formulation. In this work, we formulate the call\ngraph pruning problem as below.\nInput: A static call graphG= (ğ‘‰,ğ¸ )is a directed graph constructed\nby a static analysis tool, where ğ‘‰ is the set of programâ€™s functions\nidentified by a function signature and ğ¸ is the set of edges, i.e.,\nfunction calls, in the call graph. Each edge in ğ¸is a tuple of (caller,\ncallee, offset), where caller is the calling function, callee is the called\nfunction, and offset is the call-site in caller.\nOutput: A pruned call graph Gâ€² = (ğ‘‰â€²,ğ¸â€²), where ğ‘‰â€² = ğ‘‰ and\nğ¸â€²âŠ†ğ¸\nTo address this problem, we aim to train a binary classifierC, which\ncan classify each edge in a call graph Gas true positive, i.e., the\nedge represents a true call, or false positive , i.e., the edge does\nnot represents a true call. Using the classifierâ€™s output, we prune\nthe call graph following Algorithm 1. We use the classifier Cto\nclassify each edge in a call graph. Edges classified as false positives\nare pruned, while edges classified as true positives are retained.\nAlgorithm 1: Call-graph Pruner\nInput: Call Graph G= (ğ‘‰,ğ¸ ), Classifier C\nOutput: Pruned Call Graph G= (ğ‘‰â€²,ğ¸â€²)\n1 Gâ€²â†G\n2 foreach ğ‘’ in Gdo\n3 ğ‘ â†C(ğ‘’) âŠ² prediction of binary classifier\n4 if ğ‘ == False-positive then\n5 ğ¸â€²= ğ¸â€²\\{ğ‘’} âŠ² remove edge from call graph\n6 end\n7 end\n8 return Gâ€²\n2.2.2 State-of-the-art. To prune call graphs, Utture et. al. [51] re-\ncently proposed cgPruner, which uses a machine learning model,\na random forest classifier, based on 11 structural features extracted\nfrom a call graph for pruning edges. Their experiments demonstrate\nthat cgPruner could successfully boost the precision of call graphs\nfrom 24% to 66% and reduce the false positive rate in client tool\nfrom 73% to 23%. The approach, however, also substantially reduces\nthe call graphâ€™s recall.\ncgPruner relies only on structural features, which can not dis-\ntinguish false-positive and true-positive edges that share the same\ncharacteristics of structure (as mentioned in Section 2.1). Similar\nto cgPruner, our approach, i.e. AutoPruner, employs a machine\nlearning model to prune call graphs. However, unlike cgPruner,\nwe use the semantic information from the source code of both the\ncaller and callee function of an edge in the call graph. The infor-\nmation from the source code enables our approach to distinguish\ntrue-positive edges from false positive edges (later demonstrated\nin Section 5.1).\n2.3 Transformer Models of Code and\nCodeBERT\nTransformer models [53] are deep learning models based on an\nencoder-decoder architecture. Transformer models employ the at-\ntention mechanism and have achieved remarkable performance in\nfield of Natural Language Processing (NLP) [5, 22, 39, 53, 56, 60].\nNLP models have also been employed for source code-related tasks,\nas source code has been found to exhibit characteristics, such as\nrepetitiveness and regularity, similar to natural language [17].\nRecently, CodeBERT [11] was proposed as a Transformer model\nfor source code. A CodeBERT model is pre-trained on a large corpus,\ncontaining over 6 million functions and 2 million pairs of comment-\nfunction. As input, CodeBERT can be given a pair of data (e.g.,\nsource code and a code comment that describes the semantics of\nthe source code) to learn statistical relationships between the pair\nof data. CodeBERT is pre-trained by two tasks, Masked Language\nModeling (MLM) and Replaced Token Detection (RTD). In the MLM\ntask, given an input sequence with a single token, CodeBERT has\nto predict the value of the masked token. In the RTD task, given an\ninput sequence where some tokens are replaced with alternative\ntokens, CodeBERT detects the replaced tokens. Previous studies\nhave demonstrated the effectiveness of CodeBERT in multiple tasks,\nincluding the capability for CodeBERT to be fine-tuned for tasks\nthat it was not initially trained for [ 11, 57]. Prior studies have\nbeen built on top of CodeBERT for automating various tasks that\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\nrequire an understanding of program semantics, e.g., type inference\n[16, 21, 37], program repair [31], etc. Motivated by these success\ncases, our proposed solution (AutoPruner) built upon CodeBERT\nfor another task, namely call-graph pruning. Our solution tries to\ncapture the semantic relevance of code in the caller function to that\nof the callee function to differentiate between true and false edges\nin a call graph. Different from cgPruner, AutoPruner analyzes\nthe source code of the functions while cgPruner ignores them.\nMoreover, AutoPruner considers both caller and callee functions\nwhile other pure program analysis methods analyze only the caller\nfunction.\n3 METHODOLOGY\nFigure 2 illustrates the overall framework of AutoPruner. Before\nAutoPruner can be applied, it has to be fine-tuned and trained.\nFirst, in the Fine-tuning phase, we fine-tune a pre-trained Code-\nBERT model of code to enable it to extract the semantic features\nof the edges in a call graph. Next, in the Training phase, we use\nthe fine-tuned model to extract semantic features for the edges\nin the call graph. The semantic features are then combined with\nstructural features extracted from the call graph to construct the\nrepresentation of each edge. Then, we train a feed-forward neural\nnetwork classifier to identify false positive edges. Afterward, in\nthe Application phase, AutoPruner can be used as a call graph\npruner for post-processing call graphs to be used in other program\nanalyses.\n3.1 Fine-tuning\nIn this phase, we fine-tune CodeBERT. Before CodeBERT can be\nused for a task different from its pretraining task, it has to be fine-\ntuned to adapt its weights for the new task. CodeBERT takes a\npair of data as input, and in AutoPruner, we pass the source\ncode of the caller and callee functions associated with each edge as\ninput. Specifically, AutoPruner uses a preprocessor that extracts\nthe source code of the caller and callee functions, constructing a\nsequence of input tokens that matches the input format expected\nby CodeBERT. Then, the sequences are input into CodeBERT for\nfine-tuning. Below, we explain each component of the fine-tuning\nphase in detail.\n3.1.1 Pre-processing. The pre-processing step ( 1â—‹in Figure 2) pro-\nduces the input sequences to CodeBERT, enabling it to learn a\nrepresentation of the semantic relationship between the source\ncode of the caller and callee functions. Initially, from the call graph\nconstructed by a static analysis tool (e.g., WALA or Doop), an edge\nin the call graph is characterized by a pair of function signatures\nidentifying the caller and callee functions associated with it. We\nuse java-parser1 to extract the source code of both the caller and\ncallee functions. Particularly, we parse the source code to obtain the\nmethod descriptors for every methods and matches them against\nthe output of existing CG generators, which identifies methods\nusing their descriptors. This allows us to link the source code to\nthe methods in the call graph. Then, we use CodeBERT tokenizer2\nto tokenize the source code. Finally, following the input format of\n1https://javaparser.org/\n2https://huggingface.co/microsoft/CodeBERT-base/tree/main\nCodeBERT, we construct an input sequence that encodes the source\ncode of the caller and callee functions in the form of:\n[ğ¶ğ¿ğ‘†]âŸ¨callerâ€™s source codeâŸ©[ğ‘†ğ¸ğ‘ƒ]âŸ¨calleeâ€™s source codeâŸ©[ğ¸ğ‘‚ğ‘†]\n(1)\nwhere [CLS], [SEP], and [EOS] are tokens separating the pair of\ndata, as required by CodeBERT.\n3.1.2 CodeBERT Fine-tuning. As the pre-training tasks of Code-\nBERT, (i.e., the Masked Language Modeling and Replaced Token\nDetection tasks) differs from our task (i.e., identifying false positive\nedges), we perform a fine-tuning step to adapt the pre-trained Code-\nBERT model for our task, following the common practice in transfer\nlearning [50, 59] and other applications of CodeBERT [3, 31, 57, 58].\nThis step aims to transfer the knowledge based on the pre-training\ntask associated with an extremely large amount of data onto our\ntask where collecting data is expensive (as obtaining the ground\ntruth labels in our task requires careful human analysis and the\nexecution of test cases).\nWe fine-tune the CodeBERT model directly on the training\ndataset of our task of identifying false positive edges in the call\ngraph. Specifically, we feed input sequences in the input format of\nCodeBERT, obtained from pre-processing step into the CodeBERT\nmodel. Next, the model extract features from the input sequences.\nThen, we pass the extracted features into a fully connected layer\nto classify each edge in the call graph into true and false positive\nedges ( 2â—‹in Figure 2).\nDuring the fine-tuning phase, the parameters of the CodeBERT\nmodel are updated by Adam optimizer [23] to minimize the Cross-\nEntropy Loss. After this fine-tuning phase, we freeze all parameters\nof the CodeBERT model. In the subsequent phases, AutoPruner\nuses the fine-tuned CodeBERT model to extract semantic features\nfrom the source code of the caller and callee functions associated\nwith each edge.\n3.2 Training\nIn the training phase, our objective is to train the binary classifier\nthat predicts if a given edge is a true positive or false positive (the\nClassifier C in Algorithm 1). To this end, we construct the repre-\nsentation of an edge in a call graph by extracting and combining\nfeatures of both types: semantic features (extracted from the source\ncode by fine-tuned language model) and structural features (ex-\ntracted from the call graph). Then, we train a neural classifier to\npredict whether an edge is true or false positive. Below, we explain\neach component of the pipeline.\n3.2.1 Feature Extraction. We extract into a feature set (3â—‹in Figure\n2) the two types of features as follows:\nâ€¢Semantic features. The semantic features are extracted\nfrom the source code of caller and callee functions using our\nfine-tuned CodeBERT model. To capture this information,\nwe first apply the same pre-processing step as described in\nthe Fine-tuning step ( 4â—‹ in Figure 2). Next, the fine-tuned\nCodeBERT model extracts a high-dimensional vector that\nencodes the statistical relationship between the caller and\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\nFigure 2: The overview of AutoPruner\nTable 1: Types of structural features\nFeature Description\nsrc-node-in-deg number of edges ending in caller\nsrc-node-out-deg number of edges out of caller\ndest-node-in-deg number of edges ending in callee\ndest-node-out-deg number of edges out of callee\ndepth length of shortest path from main to caller\nrepeated-edges number of edges from caller to callee\nL-fanout number of edges from the same call-site\nnode-count number of nodes in call graph\nedge-count number of edges in call graph\navg-degree average src-node-out-deg in call graph\navg-L-fanout average L-fanout value in call graph\ncallee function ( 5â—‹in Figure 2). As a result, we obtain seman-\ntic features of an edge in call graph as follows:\nğ‘“ğ‘ ğ‘’ğ‘š =\nD\nğ‘£ğ‘ ğ‘’ğ‘š\n1 ,ğ‘£ğ‘ ğ‘’ğ‘š\n2 ,...,ğ‘£ ğ‘ ğ‘’ğ‘š\nğ‘˜ğ‘\nE\n(2)\nwhere ğ‘˜ğ‘ = 768 is the embedding dimension of CodeBERT.\nâ€¢Structural features. The structural feature captures graph-\nical information related to each edge. The features include\nmetrics about the neighborhood of the edge (local informa-\ntion) or the entire call graph (global information). We use\nthe same features proposed by Utture et al. [ 51]. Detailed\ninformation of the features is presented in Table 1. We rep-\nresent the structural features of an edge in a call graph as\nfollows:\nğ‘“ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ =\nD\nğ‘£ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡\n1 ,ğ‘£ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡\n2 ,...,ğ‘£ ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡\nğ‘˜ğ‘ \nE\n(3)\nwhere ğ‘˜ğ‘  = 22 is the number of structural features. Based\non the work by Utture et al. [51], there are two features of\neach type listed in Table 1, one for transitive calls and one\nfor direct calls, so we have 22 structural features.\n3.2.2 Feature Fusion. In this step, we combine semantic features\nand structural features of each edge in the call graph into a final rep-\nresentation. We first use one fully connected layer for each feature\n( 6â—‹in Figure 2). Then, the output of these layers are concatenated\nto produce the final representation. More formally,\nğ‘“â€²\nğ‘ ğ‘’ğ‘š = ğ¹ğ¶ğ‘ğ‘˜ğ‘ Ã—â„(ğ‘“ğ‘ ğ‘’ğ‘š) (4)\nğ‘“â€²\nğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ = ğ¹ğ¶ğ‘ğ‘˜ğ‘  Ã—â„(ğ‘“ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ ) (5)\nğ‘“ =\n\nğ‘“â€²\nğ‘ ğ‘’ğ‘š,ğ‘“ â€²\nğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡\n\u000b\n(6)\nwhere ğ¹ğ¶ğ‘ğ‘šÃ—ğ‘› denotes a fully connected layer that takes a ğ‘š-\ndimensional input and outputs a ğ‘›-dimensional vector. We set â„,\nwhich is the size of the hidden feature vector, as 32.ğ‘˜ğ‘ and ğ‘˜ğ‘  is the\nsize of semantic and structural feature vector, respectively.\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\n3.2.3 Neural Classifier. Given the representation of an edge ob-\ntained from feature extraction, we obtain a score that approximates\nthe probability that an edge is a true positive.\nThe score is computed through a feed-forward neural network\n( 7â—‹in Figure 2) consisting of one hidden layer and one output layer.\nMore formally,\nğ‘“â€²= ğ¹ğ¶ğ‘2â„Ã—2 (ğ‘“) (7)\nğ‘ğ‘Ÿğ‘œğ‘ = ğ‘‚ğ‘¢ğ‘¡ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ(ğ‘“â€²) (8)\nwhere, ğ¹ğ¶ğ‘ğ‘šÃ—ğ‘›denotes a fully connected layer inputs ağ‘š-dimensional\nvector and outputs ağ‘›-dimensional one,ğ‘‚ğ‘¢ğ‘¡ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ is a softmax func-\ntion [13]. ğ‘“ and ğ‘“â€²are the edge representation and output features,\nrespectively. ğ‘ğ‘Ÿğ‘œğ‘ = {ğ‘ğ‘Ÿğ‘œğ‘ğ¹ğ‘ƒ,ğ‘ğ‘Ÿğ‘œğ‘ğ‘‡ğ‘ƒ}is the output probabilities,\nwhere ğ‘ğ‘Ÿğ‘œğ‘ğ¹ğ‘ƒ and ğ‘ğ‘Ÿğ‘œğ‘ğ‘‡ğ‘ƒ is the probability that an edge is false\npositive and true positive, respectively. An edge withğ‘ğ‘Ÿğ‘œğ‘ğ‘‡ğ‘ƒ larger\nthan ğ‘ğ‘Ÿğ‘œğ‘ğ¹ğ‘ƒ is considered as a true positive. Otherwise, the edge\nare considered as a false positive.\nDuring the training phase, the parameters of AutoPruner ex-\ncept CodeBERTâ€™s parameters are updated by Adam optimizer [23]\nto minimize the cross-entropy loss [13].\n3.3 Application\nAfter the Training Phase, AutoPruner can now be deployed for\nuse as a call graph pruner. Given a call graph generated by a static\nanalysis tool, AutoPruner preprocesses the call graph and extracts\nsemantic and structural features. Based on these features, the neural\nclassifier produces predictions for each edge in the call graph. Using\nthe predictions of the neural classifier, AutoPruner removes the\nedges predicted to be false positives, and outputs an improved call\ngraph with fewer false positives.\nAutoPruner can be integrated into other static analyses (i.e.,\nclient analyses) that takes a call graph as input. Since call graphs\npruned by AutoPruner have fewer false positives compared to\nthe original ones, the performance of the client analyses should\nimprove given the more precise call graphs.\n4 EMPIRICAL EVALUATION\n4.1 Research Question\nOur evaluation aims to answer the following research questions:\nRQ1: Is AutoPruner effective in pruning false positives from static\ncall graphs? This research question concerns the ability of Auto-\nPruner in identifying false positive edges in a static call graph. To\nevaluate our approach, we evaluate AutoPruner on a dataset of\n141 real-world programs in NJR-1 dataset [ 36] in terms of Preci-\nsion, Recall, and F-measure. We compare our approach to multiple\nbaselines, including the state-of-the-art technique, cgPruner [51],\nas well as a graph neural network, and the original call graphs\nproduced by static analysis tools.\nRQ2: Can AutoPruner boost the performance of client analyses?\nThis research question investigates the impact of pruned call graphs\nproduced by AutoPruner on client analysis. To answer this ques-\ntion, we use pruned call graph as input for client analyses and\ninvestigate its performance compared to original call graph and cg-\nPruner [51] on two client analyses used bycgPruner: null-pointer\nanalysis and monomorphic call-site detection.\nRQ3: Which components of AutoPruner contributes to its perfor-\nmance? AutoPruner uses multiple types of features, including the\nsemantic features extracted from both the caller and callee func-\ntions, and the structural features extracted from the call graph. We\ninvestigate the contribution of each feature in an ablation study by\ndropping each type of feature and observing the change in Auto-\nPrunerâ€™s performance.\n4.2 Experimental Setup\n4.2.1 Dataset. To evaluate effectiveness of our approach, we use a\ndataset of 141 programs, initially constructed by Utture et al. [51],\nfrom the NJR-1 benchmark suite [36]. We follow the same experi-\nmental setting as prior work [51].\nThis dataset of programs was curated by Utture et al. [51] based\non the criteria that each program has over 1,000 functions, has over\n2,000 call graph edges, and has over 100 functions that are invoked\nat runtime, and has a high overall code coverage (68%). In total,\nthe dataset comprises over 860,000 call graph edges. The ground-\ntruth label of each edge was obtained based on instrumentation\nand dynamic analysis [51]. We use 100 programs as our training\nset and the remaining programs for the test set.\n4.2.2 Evaluation Metrics. We estimate the quality of a static call\ngraph using standard evaluation metrics: Precision, Recall, and\nF-measure, which are defined as follows:\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = |ğ‘†âˆ©ğº|\n|ğ‘†| (9)\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = |ğ‘†âˆ©ğº|\n|ğº| (10)\nğ¹ âˆ’ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ = 2 Ã—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› +ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ (11)\nwhere ğ‘† and ğº are the edge set in the static call graph and ground-\ntruth respectively.\nAmong these evaluation metrics, Precision is the proportion of\nedges in call graph that are true calls. A high Precision is desirable\nfor reducing developer effort in inspecting false positives [9]. Recall\nrefers to the proportion of true edges that are retained in call graph.\nFinally, we consider F-measure, which is the harmonic mean of\nPrecision and Recall. We use F-measure as a summary statistic to\ncapture the tradeoff between Precision and Recall.\nFollowing previous work [51], we compute the average Precision,\nRecall, and F-measure for the evaluation set by taking the mean\nover Precision, Recall, and F-measure of individual programs. We\nalso report the standard deviation of each metric.\n4.2.3 Client analyses. A better static call graph should lead to\npractical improvements on client analyses using the call graph.\nTo assess the effect of the improvements to the call graph from\nAutoPruner, we run experiments on two client analyses, null\npointer analysis, and monomorphic call-site detection, using the\ncall graph produced by WALA. To perform a direct comparison,\nthe client analyses selected are the same as those considered by\nUtture et al. [51]. For each client analysis, we compareAutoPruner\nagainst the baseline that produced the best call graph.\nNull pointer analysis. In the first client analysis, we use anal-\nysis by Hubert et al. [18], which is implemented in Wala [12], to\ndetect possible null pointer dereferences related to uninitialized\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\ninstance fields based on the input static call graph. This analysis\nis context-insensitive and field-insensitive. Improving the analysis\nwould reduce the amount of developer effort spent inspecting false\nalarms, which is known to be a barrier to the adoption of bug de-\ntection tools based on static analysis [20]. For this client analysis,\nwe refer to incorrect warnings reported as â€œfalse alarmsâ€ to distin-\nguish them from â€œfalse positivesâ€ from the call graph construction.\nThis analysis is independently checked by two human annotators to\nmanually determine if the warnings are false alarms. In cases where\nthe two annotators disagree on the decision, we involve a third\nannotator, an author of the paper, for a discussion to reach a consen-\nsus. We report the total number of reported warnings and the false\nalarm rate of the null pointer analyzer. As the ground-truth labels\nof all warnings are not known, we are not able to compute Recall\nand, therefore, we do not compare the approaches on F-measure.\nMonomorphic call-site detection. In the second client analy-\nsis, the static call graph is used to detect monomorphic call sites.\nA call site is monomorphic when only one concrete function can\nbe invoked. The detection of monomorphic call sites is useful, for\nexample, for function inlining to reduce the runtime cost of func-\ntion dispatch [ 51]. A better call graph would allow us to safely\ninline more functions, improving the performance of programs.\nThe ground-truth of this analysis is determined by running the\nanalysis on the ground-truth call graph. We report the precision,\nrecall, and F-measure of the monomorphic call site detector.\nTable 2: The classification threshold values of cgPruner for\ndifferent static call graphs\nCall Graph WALA Doop Petablox\nBalanced point 0.4500 0.4028 0.4279\nDefault 0.4500 0.5000 0.5000\n4.2.4 Baselines. To assess the effectiveness of AutoPruner, we\ncompare our approach with the following baselines:\nâ€¢the original call graphs are call graphs constructed by static\nanalysis tools. In this work, we consider the 0-CFA static call\ngraphs constructed by three standard static analysis tools,\nWALA [12], Doop [4], and Petablox [30]. The choice of these\nthree analysis tools follows the previous work[51] for a fair\ncomparison.\nâ€¢a random baseline that randomly removes ğ‘% of edges in a\ncall graph. For a fair comparison, we setğ‘ as the percentage\nof edges that are removed by AutoPruner.\nâ€¢cgPruner [51] is the state-of-the-art technique in call graph\npruning task. cgPruner constructs a decision tree based on\nthe 11 types of structural features (listed in Table 1) extracted\nfrom the call graphs to identify false positive edges. We run\ncgPruner using the paperâ€™s replication package3. cgPruner\nuses a classification threshold to determine if an edge is a\nfalse positive. A higher threshold results in higher precision\nas it accepts only a few edges which are more likely to be the\nground-truth call graph. The threshold enables a trade-off\nbetween precision and recall. We report the result of cg-\nPruner at two different thresholds. The first is determined\n3https://doi.org/10.5281/zenodo.5177161\nbased on the â€œbalanced pointâ€, where the threshold is tuned\nsuch that the average precision and recall of the call graph\nare equal when evaluated on the test dataset, following the\nprocedure used by Utture et al. [51] to determine the opti-\nmal balance between precision and recall. Note that, in the\ncgPruner paper, the \"balanced points\" are identified using\nthe evaluation dataset. In this paper, we selected the points\nusing the training dataset to reduce likelihood of overfitting.\nThe second threshold is the default threshold obtained from\nthe replication package. Note that, for WALA, both thresh-\nolds share the same value of 0.45, so we report just one set\nof results. The thresholds are shown in Table 2.\nâ€¢GCN [54] is an standard, off-the-shelf, graph neural network\n(GNN). In this task, we use it perform edge classification.\nGNNs are common for machine learning on graph-structured\ndata. As input, we pass the call graph constructed from static\nanalysis along with 11 types of structural features described\nin Table 1. Then, we use the GCN, which considers informa-\ntion based on the nodes in the neighborhood, to predict if\nan edge in the call graph is a false positive.\n4.2.5 Implementation details. For AutoPruner, we implement\nthe proposed approach using PyTorch library and the Python pro-\ngramming language. The models are trained and evaluated on two\nNVIDIA RTX 2080 Ti GPU with 11GB of graphics memory. For\nCodeBERT, we fine-tune the model with a learning rate of 1e-5,\nfollowing prior works [11, 57] and a batch size of 10 in 2 epochs.\nWe trained the neural classifier with a learning rate of 5e-6 with a\nbatch size of 50 in 5 epochs.\n4.3 Experimental Results\n4.3.1 Effectiveness of AutoPruner. We report the comparison of\nour approach, AutoPruner against the baselines approaches. The\ndetailed results are shown in Table 3.\nThe evaluation results demonstrate that AutoPruner success-\nfully boosts the performance of the call graphs produced by WALA,\nDoop, and Petablox by 0.25â€“0.34, up to 100% improvement ((0.68-\n0.34)/0.34) in F-measure. Overall, the use of AutoPruner led to\ngains in Precision (up to 178% improvements) which are substan-\ntially larger than the slight losses in Recall (up to just 24%).\nWith respect to the state-of-the-art baseline, cgPruner, Auto-\nPruner further improves the baseline by 13% in terms of F-measure\nfor the call graph produced by WALA. For the call graph of Doop\nand Petablox, our approach improves over the optimally balanced\ncgPruner by 8% and 7%, respectively. The improvements of Auto-\nPruner over cgPruner in F-measure are statistically significant\n(p-value < 0.05) using a Wilcoxon signed-rank test.\nNote that the results above, obtained fromcgPrunerğ‘ğ‘ğ‘™, is from\ncgPruner with a classification threshold carefully tuned on the\ntesting dataset to produce the optimal balance between precision\nand recall. Therefore, cgPrunerğ‘ğ‘ğ‘™ represents the optimal perfor-\nmance of cgPruner given the testing dataset. It may not always be\npossible to obtain the optimal threshold in practice. Despite that,\nwe observe that AutoPruner still outperforms cgPruner on call\ngraphs produced by all three static analysis tools with improve-\nments in F-measure ranging from 7%-13%.\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\nTable 3: Comparison of the effectiveness of AutoPruner\nwith the baselines on static call graph generated by WALA,\nDoop, and Petablox. cgPrunerğ‘ğ‘ğ‘™ and cgPrunerğ‘‘ğ‘’ğ‘“ de-\nnotes the result of cgPruner at balanced point (where the\nprecision and recall are equal on the test dataset) and default\nthreshold (as provided in replication package), respectively.\nFor WALA, these thresholds are the same, so we report only\none set of results as cgPruner. The bold and underlined\nnumber denotes the best result for F-measure.\nTool Technique Precision Recall F-measure\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ 0.24 Â±0.21 0.95 Â±0.14 0.34 Â±0.24\nrandom 0.24 Â±0.21 0.48 Â±0.07 0.27 Â±0.17\nWALA cgPruner 0.66 Â±0.19 0.66 Â±0.32 0.60 Â±0.25\nGCN 0.48 Â±0.2 0.74 Â±0.37 0.54 Â±0.29\nAutoPruner 0.69 Â±0.21 0.71 Â±0.19 0.68Â±0.19\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ 0.23 Â±0.21 0.92 Â±0.14 0.33 Â±0.25\nrandom 0.23 Â±0.22 0.46 Â±0.07 0.26 Â±0.17\ncgPrunerğ‘ğ‘ğ‘™ 0.67 Â±0.19 0.67 Â±0.30 0.61 Â±0.22\nDoop cgPrunerğ‘‘ğ‘’ğ‘“ 0.72 Â±0.19 0.53 Â±0.32 0.54 Â±0.26\nGCN 0.49 Â±0.23 0.77 Â±0.31 0.55 Â±0.25\nAutoPruner 0.64 Â±0.24 0.75 Â±0.15 0.66Â±0.19\nğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™ 0.30 Â±0.25 0.89 Â±0.14 0.40 Â±0.27\nrandom 0.3 Â±0.24 0.45 Â±0.08 0.31 Â±0.18\ncgPrunerğ‘ğ‘ğ‘™ 0.67 Â±0.19 0.67 Â±0.27 0.61 Â±0.20\nPetablox cgPrunerğ‘‘ğ‘’ğ‘“ 0.73 Â±0.19 0.52 Â±0.35 0.52 Â±0.29\nGCN 0.52 Â±0.23 0.78 Â±0.28 0.58 Â±0.23\nAutoPruner 0.67 Â±0.21 0.69 Â±0.21 0.65Â±0.18\nWhen compared against cgPrunerğ‘‘ğ‘’ğ‘“ , which is not optimally\nbalanced on the testing dataset and uses the threshold listed in Table\n2, the improvements of AutoPruner are more evident. In terms of\nF-measure, AutoPruner outperforms cgPruner by 13%â€“25% .\nOur approach performs better than the GCN baseline by 26%,\n20%, and 12% in terms of F-measure for call graph of WALA, Doop,\nand Petablox, respectively. Interestingly, GCN undeperforms cg-\nPruner. Both GCN and cgPruner use only structural features from\nthe call graph and differ only in the classifiers used (decision tree\nversus a graph neural network). This shows that for our task of call\ngraph pruning, the more complex classifier does not outperform the\nsimpler classifier. One possible reason for this result is that the in-\ncreased complexity of the GCN causes it to overfit the training data.\nOverall, AutoPruner outperforming both GCN and cgPruner\nsuggests that the semantic features used by AutoPruner have\npredictive power.\nAnswer to RQ1: Overall, AutoPruner outperforms ev-\nery baseline approach, including the state-of-the-art call\ngraph pruner. The call graphs pruned by AutoPruner\nimproves over the state-of-the-art baseline by up to 13%\nin F1 when the baseline is optimally balanced and by up\nto 25% when it is not. Overall, AutoPruner outperforms\nall baselines.\n4.3.2 Effect on Client Analyses. To investigate the effect of our\npruned call graph on the client analyses (i.e., static analysis tools),\nwe apply our call graph to two client analyses, i.e., null-pointer\nanalysis and monomorphic call-site detection, following the experi-\nmental setup of prior work [51].\nTable 4: Comparison of the effectiveness of AutoPruner\nwith the baselines and original call graph on null-pointer\nanalysis. The bold and underline number denotes the best\nresult for F-measure.\nTechniques Total warnings False Alarms Rate\noriginal 8,842 73%\ncgPruner 757 23%\nAutoPruner 915 12%\nNull-pointer analysis. To investigate the impact ofAutoPruner\nin null pointer analysis, we pass the pruned call graph as input to\na null pointer analysis [18], implemented in WALA. This analysis\nproduces a set of warnings. Each warning is associated with a code\nlocation. If the call graph is less accurate, the null-pointer analysis\nproduces more false alarms.\nTo evaluateAutoPruner and the baseline tools, we use the same\nevaluation procedure as Utture et al. [51] by performing manual\nanalysis on the reported warnings. Specifically, one author (with\nfour years of coding experience) and one non-author (with two\nyears of coding experience) annotator independently manually in-\nspect warnings produced by an analysis [18] implemented in WALA.\nA warning is considered a â€œtrue alarmâ€ if the author can trace the\nbackward slice of a dereference to an instance field that was unini-\ntialized by the end of a constructor [ 51]. If another exception is\nencountered before dereferencing the null pointer, or if the label of\na warning cannot be verified in 10 minutes or is otherwise unverifi-\nable by the authors, then the warning is considered a â€œfalse alarmâ€.\nThis labelling criteria for the human annotators considers only the\nwarnings produced by the program analysis [ 18], and therefore,\nis not a complete definition of a null pointer dereference. It is de-\nsigned for a analysis that is within the cognitive ability of a human\nannotator to assess the call graphs produced by call graph pruners.\nIn the cases where the two annotators disagree on the decision,\nwe involve a third annotator, an author of the paper (with three\nyears of coding experience), for a discussion to reach a consensus.\nFinally, we compute the false alarm rate of null-pointer analysis\nby dividing the number of false alarms by the number of warnings.\nThe results are presented in Table 4. Furthermore, to assess the\ninter-rater reliability of the two annotators, we compute Cohenâ€™s\nKappa [10] and obtained a value of 0.93, which is considered as\nalmost perfect agreement [25].\nUsing the call graph pruned by AutoPruner, the null pointer\nanalysis produces warnings with a false alarm rate of just 12%.\nChristakis and Bird [9] suggest that program analysis should aim\nfor a false alarm rate no higher than 15-20%, which is satisfied\nby AutoPruner. Meanwhile, both the original call graph and call\ngraph produced by cgPruner resulted in false alarm rates higher\nthan 20% (72% and 23% respectively). Our approach has a false alarm\nrate that is less than half of thecgPrunerâ€™s false positive rate while\nreporting 158 more warnings. With respect to the original call\ngraph, our approach reduces the proportion of false alarms by six\ntimes, from 73% to 12%.\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\nTable 5: Comparison of the effectiveness of AutoPruner\nwith the baselines on monomorphic call-site detection us-\ning the call graph produced by WALA, Doop and Petablox.\ncgPrunerğ‘ğ‘ğ‘™ and cgPrunerğ‘‘ğ‘’ğ‘“ denotes the result of cg-\nPruner at balanced point (where the precision and recall\nare equal) and default threshold (as provided in the replica-\ntion package), respectively. For WALA, these thresholds are\nthe same, so we report only one set of results for cgPruner.\nThe bold and underlined number denotes the best result for\nF-measure.\nTool Techniques Precision Recall F-measure\noriginal 0.52 Â±0.23 0.93 Â±0.15 0.64 Â±0.21\nWALA cgPruner 0.68 Â±0.18 0.68 Â±0.32 0.62 Â±0.25\nAutoPruner 0.71 Â±0.22 0.72 Â±0.19 0.69Â±0.19\noriginal 0.51 Â±0.24 0.92 Â±0.14 0.63 Â±0.22\nDoop cgPrunerğ‘ğ‘ğ‘™ 0.68 Â±0.19 0.71 Â±0.31 0.63 Â±0.23\ncgPrunerğ‘‘ğ‘’ğ‘“ 0.68 Â±0.21 0.56 Â±0.37 0.53 Â±0.30\nAutoPruner 0.66 Â±0.24 0.78 Â±0.16 0.69Â±0.20\noriginal 0.52 Â±0.23 0.9 Â±0.15 0.63 Â±0.21\nPetablox cgPrunerğ‘ğ‘ğ‘™ 0.68 Â±0.19 0.72 Â±0.28 0.63 Â±0.21\ncgPrunerğ‘‘ğ‘’ğ‘“ 0.73 Â±0.19 0.58 Â±0.34 0.56 Â±0.26\nAutoPruner 0.69 Â±0.21 0.75 Â±0.20 0.68Â±0.19\nMonomorphic call-site detection. In the task of monomorphic\ncall site detection, the call graph is used to identify call sites where\nthere is only one concrete call at a code location. As shown in\nTable 5, for the call graph constructed by WALA, AutoPruner\noutperforms the original call graph and cgPruner in F-measure.\nAutoPruner outperforms cgPruner by 11% in F-measure, with\nimprovements in both precision and recall.\nWe observe similar performances on the call graph of Doop\nand Petablox. On Doopâ€™s call graph, AutoPruner outperforms\ncgPruner by 30% in F-measure. Compared to the original graph,\nAutoPruner improves in F-measure by 10%. Similarly, on the call\ngraph produced by Petablox, AutoPruner improves over both\ncgPruner and the original call graph by 8% in terms of F-measure.\nAnswer to RQ2: The call graph produced by Auto-\nPruner leads to improvements in both null pointer anal-\nysis and monomorphic call site detection. Based on the\ncall graph from WALA,AutoPruner decreases the false\nalarm rate from null pointer analysis by 11%. On monomor-\nphic call site detection, AutoPruner improves over the\nstate-of-the-art call graph pruner by 11% in F-measure.\n4.3.3 Ablation Study. In answer this question, we investigate two\ndifferent ablation studies:\nâ€¢Semantic versus Structure\nâ€¢Caller versus Callee function\nSemantic vs. Structure. In this experiment, we evaluate the rela-\ntive contribution of AutoPrunerâ€™s semantic versus structural fea-\ntures for call graph pruning. Table 6 shows the results of our exper-\niments. AutoPrunerğ‘ ğ‘’ğ‘š refers to AutoPruner using only the se-\nmantic features extracted from the source code, andAutoPrunerğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ ,\nTable 6: Comparison of the effectiveness of the semantic fea-\ntures and the structural features. The bold and underline\nnumber denotes the best result for F-measure.\nTechniques Precision Recall F-measure\nAutoPrunerğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ 0.58 0.75 0.62\nAutoPrunerğ‘ ğ‘’ğ‘š 0.67 0.71 0.66\nAutoPruner 0.69 0.71 0.68\nrefers to the AutoPruner using only the structural features. Using\nonly the semantic features, AutoPrunerğ‘ ğ‘’ğ‘š outperforms\nAutoPrunerğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ in F-measure by 6%. By using with both types\nof features, AutoPruner outperforms AutoPrunerğ‘ ğ‘’ğ‘š by 3% in\nF-measure. The decreases in F-measure when either the semantic\nfeatures or structural features are removed are statistically signif-\nicant (p-value < 0.05). This indicates that both types of features\nare important, but relatively larger decrease in F-measure when\nremoving the semantic features indicates that the semantic features\nare more important compared to the structural features.\nOverall, when AutoPruner uses only one type of feature, Au-\ntoPruner has a lower F-measure. This suggests that both semantic\nand structural features are important for AutoPruner to perform\neffectively.\nTable 7: Comparison of the effectiveness of the caller fea-\ntures and the callee features. The bold and underline num-\nber denotes the best result for F-measure.\nTechniques Precision Recall F-measure\nAutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘Ÿ 0.69 0.60 0.58\nAutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘’ 0.68 0.66 0.60\nAutoPruner 0.69 0.71 0.68\nCaller vs. Callee. Next, we assess the relative importance of the\nfeatures extracted from the caller and callee functions. We evaluate\nthe performances of AutoPruner when only considering source\ncode from either caller and callee and compare them with Auto-\nPrunerâ€™s. To perform this study, we fine-tuned CodeBERT with\nonly either the source code of the caller or the callee function.\nAutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘Ÿ refers to AutoPruner using only the source\ncode from the caller function, and AutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘’ refers to Au-\ntoPruner using only the source code from the callee function. Au-\ntoPruner outperforms AutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘Ÿ and AutoPrunerğ‘ğ‘ğ‘™ğ‘™ğ‘’ğ‘’\nby up to 17%. The decreases in F-measure are statistically significant\n(p-value < 0.05). This suggests that the semantic features extracted\nfrom the source code of both the caller and callee functions are\ncrucial for the performance of AutoPruner.\nAnswer to RQ3: Our ablation study shows that all fea-\ntures contribute to the effectiveness of AutoPruner. The\nsemantic features are more important than the structural\nfeatures, while both the caller and callee functions are\nessential.\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\nFigure 3: The visualization of semantic features produced by CodeBERT in 6 sample programs. The green circles and red\ntriangles represent true positive and false positive edges respectively.\n5 DISCUSSION\n5.1 Qualitative Analysis\nIn this section, we perform a qualitative analysis of AutoPruner.\nWe have seen thatAutoPruner consistently outperforms the state-\nof-the-art approach, and our ablation study indicates that the seman-\ntic features extracted by CodeBERT are essential for the effective\nperformance of AutoPruner. Here, our goal is to investigate if the\npre-trained transformer model of code, i.e., CodeBERT, is able to\nseparate true positive edges from false positives in the call graph.\nTo this end, we use t-SNE [52], an unsupervised method for visual-\nization, to visualize the semantic features from the call graphs of 6\nprograms in two-dimensional space. If the semantic features have\npredictive power, we would expect the call graph edges, which are\nfalse positives, to be separated from the true positives.\nFigure 3 presents the visualizations, where the green and red\npoints are features of true-positive and false-positive edges, respec-\ntively. Indeed, we observe that the majority of the green points are\nclustered and are located relatively close to one another. Further-\nmore, the clusters of green points are also typically far away from\nthe majority of the red points. The observation suggests that Code-\nBERT was able to be trained to extract semantic features from the\ncaller and callee function such that the true positives can be sepa-\nrated from the false positives in the vector space. This validates our\nuse of a fine-tuned CodeBERT model for extracting semantic fea-\ntures, as the model demonstrates a remarkable ability to distinguish\nbetween true and false positives.\nStill, a small but significant proportion of red and green points\nare located close to one another, indicating that using only seman-\ntic features is not enough to separate these edges. This suggests\nthat other features (e.g., structural features) should be used to im-\nprove the modelâ€™s classifier ability. Indeed, as shown in Section\n4.3.3, the structural features are complementary to the semantic\nfeatures. The addition of structural features increases the Preci-\nsion of AutoPruner by 3% while keeping the same Recall, leading\nto 3% improvement on F-Measure. Nevertheless, we acknowledge\nthe modest contribution of the structural features which may be\nfrom how the structural and semantic features are combined. Cur-\nrently, after they are independently extracted, they are combined\nwith a small feed-forward neural network (FFNN). As a result, the\nFFNN may fail to capture more complex interactions between the\nsemantic and structural features.\n5.2 Efficiency\nIn this section, we investigate the efficiency of AutoPruner. For\npre-training model,AutoPruneruses an existing pretrained model,\nCodeBERT4. For the offline fine-tuning (Section 3.1) and training\n(Section 3.2) phase wherein both the CodeBERT and the binary\nclassifier needs to be finetuned and trained only once,AutoPruner\ntakes around 36 hours. For the inference phase which is integrated\nin downstream applications,AutoPrunertakes around 0.04 second\non average to predict a label for an edge.\n4https://github.com/microsoft/CodeBERT\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\n5.3 Threats to validity\n5.3.1 External validity. Threats to external validity concern the\ngeneralizability of our findings. Our experiments are performed\non the same dataset as prior work [51], constructed from the NJR-\n1 benchmark[36]. This may be a threat to external validity since\nAutoPruner may not generalize beyond the programs outside\nthe NJR-1 dataset. However, this threat is minimal as the dataset\nconsists of a large number of data points, and the NJR-1 benchmark\nis large and was carefully constructed to ensure their diversity [36].\n5.3.2 Internal validity. Threats to internal validity refer to possible\nerrors in our experiments. In this study, following the experimental\nprocedure of prior work [51], we perform a manual inspection of\nnull-pointer analysis, which may introduce human error. To mini-\nmize the risk, we asked one author of this paper and a non-author\nto independently inspect and label the reported warnings. We have\nmeasured the inter-rater reliability, obtaining Cohenâ€™s kappa of\n0.93, which can be interpreted as almost perfect agreement [ 25].\nTherefore, we believe that there are minimal threats from this issue.\n5.3.3 Construct validity. Threats to construct validity relate to\nthe suitability of our evaluation. To minimize risks to construct\nvalidity, we have used the same experimental setup as a previous\nstudy [51], including the same dataset and ground-truth labels.\nSome bias could be introduced in the construction of the ground\ntruth. Manual labeling requires extensive human effort, which limits\nthe scale of the experiments. Hence, we use the same automated\nlabeling procedure from prior work [ 51] that runs test cases to\nidentify ground truth edges. The imperfect code coverage of the\ntest cases may introduce bias. However, the code coverage in our\nexperiments (68%) is higher than the code coverage of real-world\nprograms (less than 40%) reported in prior studies [24].\nAnother threat to construct validity is the definition of null\npointer analysis of warnings produced by [18]. For analyzing static\nanalysis warnings, prior works often employ a human study with\nseveral annotators [14, 49]. This human study is expensive and the\ntask given to the annotators must be within the cognitive ability\nof humans. To make the annotation task tractable to humans and\nreduce its cost, we use the true-alarm identification task definition\nused in the prior work[51].\n6 RELATED WORK\nIn Section 2, we have discussed the studies related to call graph\npruning and CodeBERT. Here, we discuss other related studies.\nCall graph construction has been widely studied. As our ap-\nproach does not use runtime information, it falls into the class of\nstatic approaches [32, 41, 46, 51] for constructing call graphs. Ap-\nproaches that use dynamic analysis [ 15, 55] result in fewer false\npositives but are less scalable.\nSome recent studies on call graph construction have focused on\ndynamic languages. Salis et al. [45] and Nielson et al. [35] present\napproaches for constructing call graphs of Python programs and\nJavascript programs. Unlike language-specific techniques, Auto-\nPruner can be used to improve call graphs of any language.\nThere are many client analyses using call graphs. Recently, call\ngraphs have been used for practical applications, including scanning\napplications for vulnerable library usage [35], generating exploits of\nvulnerabilities [19], and impact analysis [15]. We have explored two\nclassic client analyses, null pointer analysis and monomorphic call\nsite detection, to validate that the improvements from pruning the\ncall graph lead to further improvements in practical applications.\nApart from applying CodeBERT to call graph contruction, re-\nsearchers have proposed other applications of deep learning models\non source code. Other researchers had success using deep learning\nmodels for type inference [2, 16, 37, 38], code completion [17, 40, 47],\ncode clone detection [44], program repair [8, 31], fault localization\n[28, 33], among other analyses on source code [6, 26, 29, 34]. Our\nwork is similar as AutoPruner successfully applies deep learning\ntechniques on source code analysis, but is unique as previous studies\nhave not previously applied deep learning to call graph analysis.\n7 CONCLUSION AND FUTURE WORK\nWe propose AutoPruner, a novel call graph pruner that lever-\nages both structural and semantic features. AutoPruner employs\nCodeBERT to extract semantic features from both the caller and\ncallee function associated with each edge in the call graph. Our\nempirical evaluation shows that AutoPruner outperforms mul-\ntiple baselines, including the state-of-the-art approach that uses\nonly structural features. The improvements fromAutoPruner also\nlead to tangible improvements on downstream applications. In par-\nticular, the proportion of false alarms reported by a baseline null\npointer analysis is halved, decreasing from 23% to just 12%.\nOur ablation study shows that the semantic features complement\nthe structural features. Moreover, our qualitative analysis reveals\nthat the semantic features extracted by CodeBERT effectively sepa-\nrate true and false positive edges.\nIn the future, we will evaluate AutoPruner on call graphs con-\nstructed using additional static analysis tools and for other pro-\ngramming languages, as well as assess the impact on call graph\npruning on other client analyses. We will also explore more ways\nto improve AutoPruner, such as jointly extracting semantic and\nstructural features, and providing more contextual information by\nincorporating the ğ‘˜-hop callers (e.g. the caller of the caller) of each\nfunction call to enrich the semantic features. Another interesting\ndirection is to use AutoPruner for proposing edges in the call\ngraph missing due to an unsoundness program analysis.\nData availability. AutoPrunerâ€™s dataset and implementation are\npublicly available at https://github.com/soarsmu/AutoPruner/.\nACKNOWLEDGEMENT\nThis project is supported by the National Research Foundation, Sin-\ngapore and National University of Singapore through its National\nSatellite of Excellence in Trustworthy Software Systems (NSOE-\nTSS) office under the Trustworthy Computing for Secure Smart\nNation Grant (TCSSNG) award no. NSOE-TSS2020-02. Any opin-\nions, findings and conclusions or recommendations expressed in\nthis material are those of the author(s) and do not reflect the views\nof National Research Foundation, Singapore and National Univer-\nsity of Singapore (including its National Satellite of Excellence in\nTrustworthy Software Systems (NSOE-TSS) office).\nXuan-Bach D. Le is supported by the Australian Government\nthrough the Australian Research Councilâ€™s Discovery Early Career\nResearcher Award, project number DE220101057.\nESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore Thanh C. Le, Hong Jin Kang, Giang T. Nguyen, Stefanus A. Haryono, David Lo, Xuan-Bach D. Le, Thang Q. Huynh\nREFERENCES\n[1] Karim Ali and OndÅ™ej LhotÃ¡k. 2012. Application-only call graph construction. In\nEuropean Conference on Object-Oriented Programming . Springer, 688â€“712.\n[2] Miltiadis Allamanis, Earl T Barr, Soline Ducousso, and Zheng Gao. 2020. Typilus:\nNeural type hints. In Proceedings of the 41st acm sigplan conference on program-\nming language design and implementation . 91â€“105.\n[3] Leonhard Applis, Annibale Panichella, and Arie van Deursen. 2021. Assessing\nRobustness of ML-Based Program Analysis Tools using Metamorphic Program\nTransformations. In 2021 36th IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE) . IEEE, 1377â€“1381.\n[4] Martin Bravenboer and Yannis Smaragdakis. 2009. Strictly declarative specifica-\ntion of sophisticated points-to analyses. In Proceedings of the 24th ACM SIGPLAN\nconference on Object oriented programming systems languages and applications .\n243â€“262.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[6] Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021. Infercode: Self-supervised\nlearning of code representations by predicting subtrees. In 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering (ICSE) . IEEE, 1186â€“1197.\n[7] David Callahan, Alan Carle, Mary W. Hall, and Ken Kennedy. 1990. Constructing\nthe procedure call multigraph. IEEE Transactions on Software Engineering 16, 4\n(1990), 483â€“487.\n[8] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-NoÃ«l Pouchet, Denys\nPoshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence\nlearning for end-to-end program repair.IEEE Transactions on Software Engineering\n47, 9 (2019), 1943â€“1959.\n[9] Maria Christakis and Christian Bird. 2016. What developers want and need\nfrom program analysis: an empirical study. In Proceedings of the 31st IEEE/ACM\ninternational conference on automated software engineering . 332â€“343.\n[10] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational\nand psychological measurement 20, 1 (1960), 37â€“46.\n[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al . 2020. CodeBERT: A Pre-\nTrained Model for Programming and Natural Languages. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020 . 1536â€“1547.\n[12] Stephen Fink and Julian Dolby. 2012. WALAâ€“The TJ Watson Libraries for Analy-\nsis.\n[13] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT\npress.\n[14] Andrew Habib and Michael Pradel. 2018. How many of all bugs do we find? a\nstudy of static bug detectors. In 2018 33rd IEEE/ACM International Conference on\nAutomated Software Engineering (ASE) . IEEE, 317â€“328.\n[15] Joseph Hejderup, Arie van Deursen, and Georgios Gousios. 2018. Software ecosys-\ntem call graph for dependency management. In2018 IEEE/ACM 40th International\nConference on Software Engineering: New Ideas and Emerging Technologies Results\n(ICSE-NIER). IEEE, 101â€“104.\n[16] Vincent J Hellendoorn, Christian Bird, Earl T Barr, and Miltiadis Allamanis. 2018.\nDeep learning type inference. In Proceedings of the 2018 26th acm joint meeting\non european software engineering conference and symposium on the foundations of\nsoftware engineering . 152â€“162.\n[17] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.\n2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122â€“131.\n[18] Laurent Hubert, Thomas Jensen, and David Pichardie. 2008. Semantic foundations\nand inference of non-null annotations. In International Conference on Formal\nMethods for Open Object-Based Distributed Systems . Springer, 132â€“149.\n[19] Emanuele Iannone, Dario Di Nucci, Antonino Sabetta, and Andrea De Lucia.\n2021. Toward Automated Exploit Generation for Known Vulnerabilities in Open-\nSource Libraries. In 2021 IEEE/ACM 29th International Conference on Program\nComprehension (ICPC) . IEEE, 396â€“400.\n[20] Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.\n2013. Why donâ€™t software developers use static analysis tools to find bugs?. In\n2013 35th International Conference on Software Engineering (ICSE) . IEEE, 672â€“681.\n[21] Milod Kazerounian, Jeffrey S Foster, and Bonan Min. 2021. SimTyper: sound type\ninference for Ruby using type equality prediction. Proceedings of the ACM on\nProgramming Languages 5, OOPSLA (2021), 1â€“27.\n[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171â€“4186.\n[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In ICLR (Poster).\n[24] Pavneet Singh Kochhar, David Lo, Julia Lawall, and Nachiappan Nagappan. 2017.\nCode coverage and postrelease defects: A large-scale study on open source\nprojects. IEEE Transactions on Reliability 66, 4 (2017), 1213â€“1228.\n[25] J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement\nfor categorical data. biometrics (1977), 159â€“174.\n[26] Alexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model\nfor generating natural language summaries of program subroutines. In 2019\nIEEE/ACM 41st International Conference on Software Engineering (ICSE) . IEEE,\n795â€“806.\n[27] Yue Li, Tian Tan, Anders MÃ¸ller, and Yannis Smaragdakis. 2018. Scalability-\nfirst pointer analysis with self-tuning context-sensitivity. In Proceedings of the\n2018 26th ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering . 129â€“140.\n[28] Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao, Lu Zhang, and\nLingming Zhang. 2021. Boosting coverage-based fault localization via graph-\nbased representation learning. In Proceedings of the 29th ACM Joint Meeting on\nEuropean Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering . 664â€“676.\n[29] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-\nsio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al . 2021.\nCodexglue: A machine learning benchmark dataset for code understanding and\ngeneration. arXiv preprint arXiv:2102.04664 (2021).\n[30] Ravi Mangal, Xin Zhang, Aditya V Nori, and Mayur Naik. 2015. A user-guided\napproach to program analysis. In Proceedings of the 2015 10th Joint Meeting on\nFoundations of Software Engineering . 462â€“473.\n[31] Ehsan Mashhadi and Hadi Hemmati. 2021. Applying codebert for automated pro-\ngram repair of java simple bugs. In 2021 IEEE/ACM 18th International Conference\non Mining Software Repositories (MSR) . IEEE, 505â€“509.\n[32] Gail C Murphy, David Notkin, William G Griswold, and Erica S Lan. 1998. An\nempirical study of static call graph extractors. ACM Transactions on Software\nEngineering and Methodology (TOSEM) 7, 2 (1998), 158â€“191.\n[33] Thanh-Dat Nguyen, Thanh Le-Cong, Duc-Ming Luong, Van-Hai Duong,\nXuan Bach Le Dinh, David Lo, and Thang Huynh-Quyet. 2022. FFL: Fine grained\nFault Localization for Student Programs via Syntactic and Semantic Reasoning.\nIn Proceedings of the 38th IEEE International Conference on Software Maintenance\nand Evolution . IEEE.\n[34] Giang Nguyen-Truong, Thanh Le-Cong, Hong Jin Kang, Xuan-Bach Dinh Le,\nand David Lo. 2022. VulCurator: A Vulnerability-Fixing Commit Detector. In\nProceedings of the 30th ACM Joint Meeting on European Software Engineering\nConference and Symposium on the Foundations of Software Engineering . ACM.\n[35] Benjamin Barslev Nielsen, Martin Toldam Torp, and Anders MÃ¸ller. 2021. Mod-\nular call graph construction for security scanning of Node. js applications. In\nProceedings of the 30th ACM SIGSOFT International Symposium on Software Testing\nand Analysis. 29â€“41.\n[36] Jens Palsberg and Cristina V Lopes. 2018. NJR: A normalized Java resource. In\nCompanion Proceedings for the ISSTA/ECOOP 2018 Workshops . 100â€“106.\n[37] Yun Peng, Cuiyun Gao, Zongjie Li, Bowei Gao, David Lo, Qirun Zhang, and\nMichael Lyu. 2022. Static inference meets deep learning: a hybrid type infer-\nence approach for python. In Proceedings of the 44th International Conference on\nSoftware Engineering . 2019â€“2030.\n[38] Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. 2020. Type-\nwriter: Neural type prediction with search-based validation. In Proceedings of\nthe 28th ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering . 209â€“220.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[40] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code completion with\nstatistical language models. In Proceedings of the 35th ACM SIGPLAN Conference\non Programming Language Design and Implementation . 419â€“428.\n[41] Michael Reif, Florian KÃ¼bler, Michael Eichberg, Dominik Helm, and Mira Mezini.\n2019. Judge: Identifying, understanding, and evaluating sources of unsoundness\nin call graphs. In Proceedings of the 28th ACM SIGSOFT International Symposium\non Software Testing and Analysis . 251â€“261.\n[42] Henry Gordon Rice. 1953. Classes of recursively enumerable sets and their\ndecision problems. Transactions of the American Mathematical society 74, 2 (1953),\n358â€“366.\n[43] Barbara G Ryder. 1979. Constructing the call graph of a program. IEEE Transac-\ntions on Software Engineering 3 (1979), 216â€“226.\n[44] Vaibhav Saini, Farima Farmahinifarahani, Yadong Lu, Pierre Baldi, and Cristina V\nLopes. 2018. Oreo: Detection of clones in the twilight zone. In Proceedings of the\n2018 26th ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering . 354â€“365.\n[45] Vitalis Salis, Thodoris Sotiropoulos, Panos Louridas, Diomidis Spinellis, and\nDimitris Mitropoulos. 2021. Pycg: Practical call graph generation in python.\nIn 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) .\nIEEE, 1646â€“1657.\n[46] Li Sui, Jens Dietrich, Amjed Tahir, and George Fourtounis. 2020. On the recall of\nstatic call graph construction in practice. In 2020 IEEE/ACM 42nd International\nConference on Software Engineering (ICSE) . IEEE, 1049â€“1060.\n[47] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. 2019. Pythia:\nAI-assisted code completion system. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . 2727â€“2735.\nAutoPruner: Transformer-Based Call Graph Pruning ESEC/FSE â€™22, November 14â€“18, 2022, Singapore, Singapore\n[48] Tian Tan, Yue Li, and Jingling Xue. 2016. Making k-object-sensitive pointer anal-\nysis more precise with still k-limiting. In International Static Analysis Symposium .\nSpringer, 489â€“510.\n[49] David A Tomassi and Cindy Rubio-GonzÃ¡lez. 2021. On the Real-World Effec-\ntiveness of Static Bug Detectors at Finding Null Pointer Exceptions. In 2021 36th\nIEEE/ACM International Conference on Automated Software Engineering (ASE) .\nIEEE, 292â€“303.\n[50] Lisa Torrey and Jude Shavlik. 2010. Transfer learning. In Handbook of research\non machine learning applications and trends: algorithms, methods, and techniques .\nIGI global, 242â€“264.\n[51] Akshay Utture, Shuyang Liu, Christian Gram Kalhauge, and Jens Palsberg. 2022.\nStriking a Balance: Pruning False-Positives from Static Call Graphs. The 44rd\nIEEE/ACM International Conference on Software Engineering (ICSE 2022) (2022).\n[52] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, 11 (2008).\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[54] Max Welling and Thomas N Kipf. 2016. Semi-supervised classification with graph\nconvolutional networks. InJ. International Conference on Learning Representations\n(ICLR 2017) .\n[55] Tao Xie and David Notkin. 2002. An empirical study of java dynamic call graph\nextractors. University of Washington CSE Technical Report (2002), 02â€“12.\n[56] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. Advances in neural information processing systems 32 (2019).\n[57] Jiayuan Zhou, Michael Pacheco, Zhiyuan Wan, Xin Xia, David Lo, Yuan Wang,\nand Ahmed E Hassan. 2021. Finding A Needle in a Haystack: Automated Mining\nof Silent Vulnerability Fixes. In 2021 36th IEEE/ACM International Conference on\nAutomated Software Engineering (ASE) . IEEE, 705â€“716.\n[58] Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizability of\nCodeBERT. In 2021 IEEE International Conference on Software Maintenance and\nEvolution (ICSME) . IEEE, 425â€“436.\n[59] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2020. A comprehensive survey on transfer learning.\nProc. IEEE 109, 1 (2020), 43â€“76.\n[60] Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021. A Robustly Optimized BERT\nPre-training Approach with Post-training. In Proceedings of the 20th Chinese Na-\ntional Conference on Computational Linguistics . Chinese Information Processing\nSociety of China, Huhhot, China, 1218â€“1227. https://aclanthology.org/2021.ccl-\n1.108",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8106331825256348
    },
    {
      "name": "Call graph",
      "score": 0.8088362216949463
    },
    {
      "name": "False positive paradox",
      "score": 0.5711103081703186
    },
    {
      "name": "Theoretical computer science",
      "score": 0.516072690486908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48291608691215515
    },
    {
      "name": "Graph",
      "score": 0.4485380947589874
    },
    {
      "name": "Source code",
      "score": 0.4207669496536255
    },
    {
      "name": "Data mining",
      "score": 0.3900381028652191
    },
    {
      "name": "Machine learning",
      "score": 0.38424021005630493
    },
    {
      "name": "Natural language processing",
      "score": 0.33764320611953735
    },
    {
      "name": "Programming language",
      "score": 0.32528799772262573
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I94518387",
      "name": "Hanoi University of Science and Technology",
      "country": "VN"
    }
  ]
}