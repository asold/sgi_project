{
  "title": "Story Centaur: Large Language Model Few Shot Learning as a Creative Writing Tool",
  "url": "https://openalex.org/W3155340931",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2186250197",
      "name": "Ben Swanson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3111963371",
      "name": "Kory Mathewson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3119468902",
      "name": "Ben Pietrzak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122342610",
      "name": "Sherol Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3152845185",
      "name": "Monica Dinalescu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2972244901",
    "https://openalex.org/W2950077203",
    "https://openalex.org/W2971663339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3092144570",
    "https://openalex.org/W2963325985",
    "https://openalex.org/W3117359173",
    "https://openalex.org/W2538192737",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2741986794",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2791469229",
    "https://openalex.org/W2963614567",
    "https://openalex.org/W2796000004",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2759254424",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3116976696"
  ],
  "abstract": "Ben Swanson, Kory Mathewson, Ben Pietrzak, Sherol Chen, Monica Dinalescu. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. 2021.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 244–256\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n244\nStory Centaur: Large Language Model Few Shot Learning as a Creative\nWriting Tool\nBen Swanson, Kory W. Mathewson2, Ben Pietrzak,\nSherol Chen, Monica Dinalescu\nGoogle, 2DeepMind\npwnr, korymath, bpietrzak, sherol, noms@google.com\nAbstract\nFew shot learning with large language models\nhas the potential to give individuals without\nformal machine learning training the access to\na wide range of text to text models. We con-\nsider how this applies to creative writers and\npresent STORY CENTAUR , a user interface for\nprototyping few shot models and a set of re-\ncombinable web components that deploy them.\nSTORY CENTAUR ’s goal is to expose creative\nwriters to few shot learning with a simple but\npowerful interface that lets them compose their\nown co-creation tools that further their own\nunique artistic directions. We build out sev-\neral examples of such tools, and in the process\nprobe the boundaries and issues surrounding\ngeneration with large language models.\n1 Introduction\nOne of the most promising possibilities for large\nlanguage models (LLMs) is few-shot learning\n(Brown et al., 2020) in which it is possible to create\ntext to text models with little or no training data.\nFew shot learning with LLMs relies on the ease at\nwhich the desired Input/Output (I/O) behavior can\nbe effectively translated into the text continuation\nproblem at which these models excel. In recent\nyears, LLMs have progressed to the level where\nthis translation requires only familiarity with the\nnatural (e.g. English) language on which the model\nwas trained.\nWe present STORY CENTAUR , a Human-\nComputer Interface that closes the gap between\nnon-technical users and the power and possibilities\nof few shot learning, with the intended audience\nof writers of creative text. It is our intention that\nby giving writers a tool for building generative\ntext models unique to their process and vision that\nthese artists will experience genuine feelings of\nco-creation.\nSTORY CENTAUR consists of a prototyping UI\nas well as a set of Angular web components that in-\nteract via a central pub-sub synchronization mecha-\nnism1. Due to the non-trivial monetary cost of infer-\nence and the requirement of specialized hardware,\nthe LLM that underlies our tool is not included\nin this release; instead we dependency inject the\nLLM with a simple (string) → string interface to\nbe provided by an arbitrary service.\nAs the ethical implications of LLMs (Bender\net al., 2021) are an important and unsolved problem\nin NLP, we highlight this design choice to decou-\nple the LLM itself from STORY CENTAUR , a web\nbased user interface that prepares the LLM’s inputs\nand processes its outputs. To put it another way,\nthe text generated is no more or less biased than the\nLLM and user themselves, as STORY CENTAUR ’s\npurpose is not to enhance or change the abilities\nof a LLM, but instead to democratize its use to\nnon-technical users.\n2 Related Work\nThe observation that simple “ﬁll-in-the-blank” neu-\nral network models trained on large quantities of\ntext can be used for problems beyond their pri-\nmary learning objective dates back to word2vec\n(Mikolov et al., 2013) in which word embeddings\nwere able to perform some SAT style analogies.\nWhile this ability captured many researchers’ fas-\ncination, these models’ dual function as a repre-\nsentation learner from which other models could\nbe initialized and/or ﬁne-tuned took center stage in\nthe following years.\nRepresentation learning techniques made steady\nadvances, expanding to sentence level contextu-\nally sensitive word embedding with ELMo (Peters\net al., 2018), the introduction of Transformers and\nMLM objectives with BERT (Devlin et al., 2018),\n1source code: https://chonger.github.io/centaur/\n245\nFigure 1: A Few Shot Formula in action. The Data and Serialization are used to create the Prompt, which along\nwith the serialized inference time Input becomes the Preamble. The LM generates a continuation, from which the\nSerialization extracts the output. The Sentinels used (with newlines omitted) are “I saw a”, “! It goes ”, and “.\n–NEXT–”.\nand the menagerie of similar systems that followed.\nWhile most work concerned itself primarily with\ntopping each other’s GLUE and SUPERGLUE\nscores (Wang et al., 2019), work from OpenAI kept\nthe torch lit for investigating the emergent abilities\nof representation learning (Radford et al., 2017,\n2018, 2019; Brown et al., 2020). Their most recent\nwork undeniably shows that sufﬁciently large LMs\nenable few shot models that approach and some-\ntimes surpass state of the art performance on a wide\nrange of NLP tasks.\nHuman + AI co-creation has existed in both prac-\ntice and theory for several years. To highlight some\nexamples in practice that relate to creative writing,\nas opposed to music or visual art of which there are\nmany, we refer the reader to browse the Electronic\nLiterature Collection2 a longstanding community\nof artist-technologists who have blazed this trail\nsince the days of hypertext. A number of publica-\ntions of AI co-creation exist as well on a diverse\nrange of artistic applications (Martin et al., 2016;\nMathewson and Mirowski, 2017; Oh et al., 2018;\nMirowski and Mathewson, 2019; Kreminski et al.,\n2019; Sloan, 2019; Tsioustas et al., 2020).\nFor a lighter introduction, Case (2018) gives\nsome examples of AI + Human collaborations, or\nCentaurs3, but primarily presents the argument that\nthe HCI that connects the Human and Computer\nis of paramount importance, a sentiment that is in\nline with our own work. We also resonate with the\nopinion of Llano et al. (2020), which argues for\nexplainability as a catalyst for fruitful co-creation,\n2https://collection.eliterature.org/\n3Case (2018) also explains this term’s etymology\nas it is central to our design that the artist be given\nthe tools to create and iterate on NLP models.\n3 Few Shot Formulas\nThe core contribution of this work is a UI for the\ncreation of few shot text generation models (Figure\n2). We ﬁrst deﬁne terms for the components of\nLM based few shot modeling as it is decomposed\nin our system: The few shot learning system as\na whole is represented as a Formula which when\nused with a large LM provides arbitrary text to text\nI/O behavior.\nWe note that while generally LMs refer to any\nprobability distribution over a sequence of tokens,\nin this work we use the term to refer to the subset\nmodel class that factorizes the joint probability into\nconditional P(wt|w1...t−1) terms. Put simply, we\nare referring to the “predict the next word given all\nwords so far” variety of LM, which includes all of\nthe GPT models.\nA Formula is composed of Data and Serializa-\ntion. Each item in the Data consists of lists of\nstring inputs and outputs that exemplify the desired\nI/O. The Serialization deﬁnes a reversible transfor-\nmation between the Data and the raw text handled\nby the LM.\nSTORY CENTAUR uses a Serialization template\nof ﬁxed text Sentinels that interleave the inputs\nand outputs; a Sentinel is deﬁned to precede each\ninput and output, as well as one that separates in-\nputs and outputs and one that comes after the ﬁnal\noutput (See Figure 2 for an example). Carefully\nchosen sentinels are powerful tools for nudging\nthe language model in desired directions (see the\n246\nFigure 2: S TORY CENTAUR ’s Formula Development User Interface.\nAppendices for examples), but must also be de-\nsigned so as not to be confused with model input\nor outputs.\nA Formula is used by ﬁrst invoking the Serial-\nization on the Data, creating the Preamble. Then,\nthe new inputs are converted using the Serializa-\ntion and concatenated to the Preamble, creating\nthe Prompt. The LM is asked to continue the text\nin the Prompt, and the Serialization is used to ex-\ntract the output(s) from the result. The LM cannot\nexplicitly enforce the Serialization format and as\nsuch will often produce non-conformant results,\nin which case it must be rejected. In practice, if\nthe LM is sufﬁciently capable and the task well\nsuited then a simple rejection sampler sufﬁces to\nproduce several acceptable options, as decoding is\nparallelizable.\n3.1 Formula Development\nSTORY CENTAUR ’s user interface for Formula\ndesign is shown in Figure 2, with supplemental\nscreenshots in the appendices. While there is no\nﬁxed workﬂow, we have found the following pro-\ncess to be effective. We assume only that the user\nis proﬁcient in English and has a strong concept of\ntheir desired I/O.\nFirst, the user must enter at least two examples\nof I/O pairs into the Data panel and take a pass at\ndeﬁning a Serialization, relying on the live updated\nPreamble panel to preview their progress. With a\nfew examples in place, the Auto-Generate button\ncan then be used to suggest new candidate IO pairs\nby passing the Preamble to the LM and allowing\nthe user to prune these suggestions. This process\ncan be repeated, quickly converging to several (10\nor more) solid examples and clear evidence that\nthe Serialization is being captured by the LM. As a\nﬁnal evaluation technique, we provide a Test mode\nthat takes inference inputs and applies the current\nFormula, also reporting the rate at which the LM\noutput respects the Serialization.\n4 Writing Tool Experiments\nWe showcase the potential of Formulas that one\nmight create using STORY CENTAUR in several\nExperiments. These experiments all rely on one\nor more Formulas that were built using the devel-\nopment tool and workﬂow described above, and\nare each motivated by a different artistic scenario.\nWhen possible, we present the I/O speciﬁcations\nfor each Experiment and invite the reader to view\nfull Experiment screenshots as well as the underly-\ning Formulas’ Data and Serialization in the Appen-\ndices.\n4.1 Magic Word\nFigure 3: Formula I/O for Magic Word.\nPerhaps the most obvious application of generative\nlanguage models to creative writing is overcoming\nwriter’s block. Speciﬁcally, we consider the sce-\nnario in which the writer has some existing seed\n247\ntext and wants to be presented with possible con-\ntinuations.\nGenerative LMs are ripe for this task as they can\nreliably continue short text; for the deﬁnition of\nLM used in this work (see Section 3) this is indeed\nexactly the task they were trained on. In this pure\nuse of the LM the author is only able to provide\nthe seed text, and so in this experiment we use a\nfew shot Formula to provide an additional input of\na word or phrase that is required to appear in the\ngenerated text.\nThe Magic Word formula takes two inputs: the\nseed text that must be continued by the model and\nthe “Magic Word” that must be used in the contin-\nuation. In this Experiment, the generated outputs\nare not only discarded if they do not conform to the\nSerialization but also if the Magic Word does not\nappear as a substring. The UI allows editing of both\nthe magic word and seed text, and on generation\nthe user is given a maximum of three sentences that\nthey can click to append to the editable main text\nbox.\nFrom an academic perspective, it is worth noting\nthat this I/O paradigm has been explored in several\nexamples of previous work, often with the same\nmotivation as a writer’s aid (Ippolito et al., 2019).\n4.2 Say It Again\nFigure 4: Formula I/O for Say It Again’s CUSTOM\nFormula.\nMany literary characters have their own peculiar\nway of speaking; Yoda, Tolkein’s dwarves, Trea-\nsure Island’s Pirates. In this Experiment we address\nthe scenario where a writer has a clear idea of what\nthey want the character to say and want suggestions\nas to how their character might actually say it.\nWe phrase this problem as a Formula with one\ninput and one output in which the input is in neu-\ntral style and the output is a paraphrase with the\ndesired style applied. This works nicely with few\nshot learning, as it is relatively easy to invent (or\ngenerate) a simple unstyled statement and then to\nimagine how a character might say it. We show-\ncase several such Formulas in this experiment, se-\nlectable in a menu. For unstyled source text, there\nare three editable areas for text to rephrase that can\nbe restyled individually or all at once.\nWe provide one additional Formula that might\nbe considered zero shot style transfer, although it is\nstill performed using a few shot Formula. When the\nstyle “CUSTOM” is selected, an input box appears\nwhere the user can enter any raw text they wish.\nThis text is then used in a Formula with two inputs,\nthe text to be restyled and the name or description\nof the character whose style to use. The surprising\nresult is that this is often possible with no examples\nof the requested style itself, only the proper Seri-\nalization and a few example of the full I/O shown\nin Figure 4 with other custom characters. As this\ninformation most likely comes from patterns and\nassociations encoded into the LMs parameters dur-\ning training, this method works best with ﬁctional\ncharacters from major movies or celebrities.\nWe encourage the further examination of large\nLMs for style transfer, as we were anecdotally im-\npressed with the output of this experiment in par-\nticular. As some recently successful work in style\ntransfer (Riley et al., 2020) already follows a la-\nbel free approach that might itself be considered\nfew shot in nature, interesting experimental com-\nparisons are likely possible.\n4.3 Story Spine\nFigure 5: Story Spine’s two formulas for spine contin-\nuation (top) and spine colorizing (bottom).\nModern LMs excel at producing text that is co-\nherent, grammatical, and at times interesting, and\nfrequently amusing. However, cracks begin to\nshow in coherence as generations grow longer (Cho\net al., 2018). A common mitigating technique has\nbeen to construct hierarchical generation systems\nin which a high level representation that is focused\non common sense story structure which is then\n248\ntransformed into narrative text (Fan et al., 2018;\nAmmanabrolu et al.). This trend inspires this ex-\nperiment, whose goal is the co-creation of a short\nstory that is both coherent and detailed.\nOne ubiquitous quality of such hierarchical sys-\ntems is that the high level representation is a struc-\ntural and/or semantic abstraction chosen to be\namenable to plot coherence modeling. This ex-\nperiment poses the question: what if the high level\nrepresentation was itself natural language? To ex-\nplain our setup we make the distinction between\nsimple text and colorful text, where the former is\na grammatically bare bones statement of fact and\nthe latter is more linguistically interesting, as a\nsentence might actually appear.\nWe use two Formulas to accomplish this goal,\nshown in Figure 5. The ﬁrst takes a simple short\nplot point sentence as input and returns a plausible\nfollowing simple plot point as output; this is used\nin a loop to generate the spine. The second is a\ncontext conditional paraphrasing formula with two\ninputs; the ﬁrst is a simple plot point and the second\nis the “story so far” which is written in colorful text.\nThe Formula’s output is a paraphrase of the simple\nplot point, colorized to both respect the factual\ninformation of the plot point and the context of the\nstory so far.\nThe user is presented with an interface that lets\nthem write and edit custom spine plot points as well\nas use the ﬁrst Formula to generate up to ﬁve can-\ndidates for plot points to continue the story. Each\nspine plot point is connected to its colorized para-\nphrase, which appear as a whole on the right side.\nIn order to maintain a model of the mapping be-\ntween the spine and colorized text, the colorized\ntext is not editable.\n4.4 Character Maker\nFigure 6: The eight character creation points used in\nCharacter Maker, with examples.\nInteresting characters are at the heart of much cre-\native writing, and various template ﬁlling exercises\nexist to create them. Often this comes down to\nﬁlling out a template containing ﬁelds that ﬂesh\nout the character, as shown in Figure 6. In this\nexperiment, the user is presented with an editable\ntemplate containing each of these ﬁelds, with the\noption to edit or clear any of the ﬁelds’ values.\nOnce a value has been cleared, it can be ﬁlled in\nby the LM conditioned on all current non-empty\nﬁelds.\nWe take this experiment in a direction that goes\nbeyond our own Formula development tools to de-\nﬁne a ﬂexible Few Shot model for data completion.\nOur generalized problem statement is as follows:\ngiven a set of ﬁelds of which an arbitrary subset are\nblank, for one such blank ﬁeld generate a plausible\nvalue conditioned on the non-blank ﬁelds. We build\na dynamic Formula creation system that fulﬁlls this\ngeneralized contract, and apply it to the ﬁlling of\ncharacter creation exercise forms.\nOur few shot solution naturally relies on a small\nnumber of fully ﬁlled out and plausibly consistent\nﬁelds (e.g. complete character descriptions). At\ninference time, we extract the subset of non-blank\nﬁelds in the inference item from each of these few\nshot examples and stitch together a Formula on\nthe spot with precisely these inputs and the single\noutput of the desired inference output ﬁeld. This\ndynamic creation of Formulas requires a ﬂexible\nSerialization that can accommodate any ﬁeld name\nand value in any order, which for this experiment\nwe simple simply use “name : value”.\n4.5 Improv Prompts\nIn improvisational acting (improv) one of the pri-\nmary pleasures is to see actors bring a set of con-\nstraints provided by the audience to life in a coher-\nent story. We see the potential for the sometimes\nwildly creative suggestions of large language mod-\nels to supply these constraints, either as a tool for\npractitioners to hone their craft or as a way to spice\nup (or speed up) a live performance itself.\nImprov constraints must be both open ended and\nsubject to speciﬁc categories; for example the pop-\nular “Party Quirks” game requires a personal quirk\nfor each actor attending a dinner party. We build\nFormulas and UIs for several improv games, and\nnote their distinction from the other Formulas in\nthis work in that they require no user input at all.\nIn constructing such zero input few shot learning\n249\nmodels it became apparent that beyond controlling\nthe grammatical form and semantic intent of the\noutputs we could also control their tone, as it would\nmimic the tone of the Formula’s Data. Crucially,\nthis allows easy adaptation of these tools to differ-\nent audiences (children versus adults, for example)\nand an implicit nudge towards whimsical outputs.\n5 Discussion\nWhile the experiments presented above demon-\nstrate how few shot learning can be used to cre-\nate interesting tools for writers, the real power of\nSTORY CENTAUR is its unlocking of rapid exper-\nimentation. Not only were we able to probe the\nboundary of what “works” efﬁciently, but also to\nengage individuals regardless of formal machine\nlearning training to help us to do so. Needless to\nsay, in the course of this work many attempted\nFormulas did not produce compelling results.\nPerhaps our most interesting failure was to build\na Formula that would produce the second half of\na rhyming couplet given the ﬁrst half, a task that\nwould require understanding of both phonetics and\nmeter as well as linguistic coherence. This was\ndisappointing given the compelling examples of\nGPT-3 poetry available online4. One possible ex-\nplanation is that while general poetry and speciﬁ-\ncally rhyming couplets are in our minds connected\nclosely with a subset relationship rooted in human\nculture, the hard constraint of rhyme and meter in\nfact divides them into very different problems for\nan LM. It is certainly the case that recent success-\nful work in rhyming metered poetry generation has\nneeded to resort to ﬁxed rhyme words and syllable\ncounting(Ghazvininejad et al., 2017).\nIn terms of larger themes, we found that con-\nstructing Formulas in which any of the inputs or\noutputs were much longer than a few sentences\nwere hard to construct. We speculate that it is\nmore difﬁcult for the models to latch on to the\nSerialization in this case, as the observed symp-\ntom was often that no generated text passed the\nde-serialization ﬁlter. On the positive side we ob-\nserved that few shot tasks that rely on paraphrasing\n(such as those used Say It Again) were surprisingly\neasy to construct successfully.\nIt is a common and intuitively plausible observa-\ntion that the design of the Serialization is crucial\nto the performance of few shot learning with large\nLMs. Our Formulas can only be evaluated quali-\n4https://www.gwern.net/GPT-3#transformer-poetry\ntatively and so we leave to future work the human\nstudies that would be necessary to investigate this\nhypothesis. Our Magic Word experiment does offer\nthe promise of a good test bed for Serialization de-\nsign; even after considerable iteration we found the\nrate at which outputs pass both the de-Serialization\nﬁlter to be surprisingly low given the relative sim-\nplicity of the task and the model’s innate ability to\ngenerate coherent text continuation.\nFinally we note that our true goal is to empower\nartists with no technical training to imagine a For-\nmula, construct it in our development mode, and\nthen produce experiments as we have. In our cur-\nrent process, such an artist could indeed construct\ntheir Formula, but would at some point require a\nprogrammer to build it into an experiment, requir-\ning e.g. a WYSIWG editor. While this was beyond\nthe scope of our work, we did construct our sys-\ntem using Angular, a modern web development\nframework whose core premise is modularity, de-\npendency injection, and reuse of components. Not\nonly do our experiments make use of a small set\nof these reusable components for functionality like\neditable text ﬁelds and clickable suggestion lists,\nbut also all text and Formulas are synchronized by\na global pub-sub service with simple string keys.\n6 Conclusion\nWe present STORY CENTAUR , a tool for the cre-\nation and tuning of text based few shot learning\nFormulas powered by large language models and\nseveral experiments using Formulas built with our\ntool that are focused around the topic of creative\nwriting.\nThe emergence of large language models has\nshaped the course of NLP research in the late\n2010’s but the question remains as to what, if any,\nis a viable use case for these models in their raw,\nun-ﬁnetuned, form. Additionally, while some claim\nthat scaling these models is a viable path to Artiﬁ-\ncial General Intelligence, others disagree (Bender\nand Koller, 2020), and learning what is easy, hard,\nand impossible for them is crucial this debate. The\nanswers to these questions will undoubtedly reveal\nthemselves in the coming years and we are partic-\nularly excited to see their impact on the ﬁne arts.\nIn particular, we see great potential in tools built\nwith this technology when there is a human, in this\ncase an artist, in the loop to complement the natural\ndeﬁciencies of a simple but powerful text generator\nthat lacks editorial control and responsibility.\n250\nReferences\nPrithviraj Ammanabrolu, Ethan Tien, Wesley Cheung,\nZhaochen Luo, William Ma, Lara J Martin, and\nMark O Riedl. Story realization: Expanding plot\nevents into sentences.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big. In Proceedings of the 2020 Conference\non Fairness, Accountability, and Transparency; As-\nsociation for Computing Machinery: New York, NY,\nUSA.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nNicky Case. 2018. How to become a centaur. Journal\nof Design and Science.\nWoon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xi-\nujun Li, Michel Galley, Chris Brockett, Mengdi\nWang, and Jianfeng Gao. 2018. Towards coher-\nent and cohesive long-form text generation. arXiv\npreprint arXiv:1811.00511.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. arXiv preprint\narXiv:1805.04833.\nMarjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and\nKevin Knight. 2017. Hafez: an interactive poetry\ngeneration system. In Proceedings of ACL 2017,\nSystem Demonstrations, pages 43–48.\nDaphne Ippolito, David Grangier, Chris Callison-\nBurch, and Douglas Eck. 2019. Unsupervised hier-\narchical story inﬁlling. In Proceedings of the First\nWorkshop on Narrative Understanding , pages 37–\n43.\nMax Kreminski, Devi Acharya, Nick Junius, Elisabeth\nOliver, Kate Compton, Melanie Dickinson, Cyril\nFocht, Stacey Mason, Stella Mazeika, and Noah\nWardrip-Fruin. 2019. Cozy mystery construction kit:\nPrototyping toward an ai-assisted collaborative sto-\nrytelling mystery game. In Proceedings of the 14th\nInternational Conference on the Foundations of Dig-\nital Games, pages 1–9.\nMaria Teresa Llano, Mark d’Inverno, Matthew Yee-\nKing, Jon McCormack, Alon Ilsar, Alison Pease,\nand Simon Colton. 2020. Explainable computa-\ntional creativity. In Proc. ICCC.\nLara J Martin, Brent Harrison, and Mark O Riedl. 2016.\nImprovisational computational storytelling in open\nworlds. In International Conference on Interactive\nDigital Storytelling, pages 73–84. Springer.\nKory Mathewson and Piotr Mirowski. 2017. Impro-\nvised theatre alongside artiﬁcial intelligences. In\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence and Interactive Digital Entertainment ,\nvolume 13.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. Advances in neural information processing sys-\ntems, 26:3111–3119.\nPiotr Mirowski and Kory Wallace Mathewson. 2019.\nHuman improvised theatre augmented with artiﬁcial\nintelligence. In Proceedings of the 2019 on Creativ-\nity and Cognition, pages 527–530.\nChanghoon Oh, Jungwoo Song, Jinhan Choi,\nSeonghyeon Kim, Sungwoo Lee, and Bongwon Suh.\n2018. I lead, you help but only with enough details:\nUnderstanding user experience of co-creation with\nartiﬁcial intelligence. In Proceedings of the 2018\nCHI Conference on Human Factors in Computing\nSystems, pages 1–13.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nParker Riley, Noah Constant, Mandy Guo, Girish\nKumar, David Uthus, and Zarana Parekh. 2020.\nTextsettr: Label-free text style extraction and\ntunable targeted restyling. arXiv preprint\narXiv:2010.03802.\nRobin Sloan. 2019. Writing with the machine: Gpt-2\nand text generation. In Roguelike Celebration.\nCharalampos Tsioustas, Daphne Petratou, Maximos\nKaliakatsos-Papakostas, Vassilis Katsouros, Apos-\ntolos Kastritsis, Konstantinos Christantonis, Kon-\nstantinos Diamantaras, and Michael Loupis. 2020.\n251\nInnovative applications of natural language process-\ning and digital media in theatre and performing arts.\nIn Proceedings of the ENTRENOVA-ENTerprise RE-\nsearch InNOVAtion Conference , volume 6, pages\n84–96.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. CoRR, abs/1905.00537.\nA Appendices\n252\nFigure 7: A Screenshot of the Magic Word Formula and Experiment.\n253\nFigure 8: Say It Again in the style of a Shakespearean character using 5 Few Shot Examples.\n254\nFigure 9: Say It Again in the style of Bill and Ted using 5 Few Shot Examples.\n255\nFigure 10: Say It Again in the style of Data from Star Trek using 5 Few Shot Examples of style transfer, but no\nexamples of Data’s actual style.\n256\nFigure 11: Story Spine screenshots. In the top screenshot, a ﬁve segment spine has been constructed, but only two\nspine segments have been colorized. The bottom image shows the result of colorizing the ﬁnal three segments.\nFigure 12: Character Maker, before and after generation of the Tactics ﬁeld. In this case, generation was per-\nformed by constructing a few shot Formula dynamically with Super-Objective, Sub-Objective, Scene Objective,\nand Backstory as input, and Tactics as output, using 4 examples of full templates to create the Data.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6157907247543335
    },
    {
      "name": "Chen",
      "score": 0.6092987060546875
    },
    {
      "name": "Linguistics",
      "score": 0.5448105931282043
    },
    {
      "name": "Computational linguistics",
      "score": 0.42292842268943787
    },
    {
      "name": "Natural language processing",
      "score": 0.4154934883117676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4085336923599243
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.33440136909484863
    },
    {
      "name": "Philosophy",
      "score": 0.11053043603897095
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}