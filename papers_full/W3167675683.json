{
  "title": "Learning and Evaluating a Differentially Private Pre-trained Language Model",
  "url": "https://openalex.org/W3167675683",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5017386816",
      "name": "Shlomo Hoory",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5056266191",
      "name": "Amir Feder",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5051553320",
      "name": "Avichai Tendler",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5108816455",
      "name": "Alon Cohen",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5038453188",
      "name": "Sofia Erell",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5043948840",
      "name": "Itay Laish",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5060969697",
      "name": "Hootan Nakhost",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5019495492",
      "name": "Uri Stemmer",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5039832669",
      "name": "Ayelet Benjamini",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5089579452",
      "name": "Avinatan Hassidim",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A5065128060",
      "name": "Yossi Matias",
      "affiliations": [
        "Google (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2166434810",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W3102378604",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2138865266",
    "https://openalex.org/W4205228770",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2949134592",
    "https://openalex.org/W1985511977",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2152460337",
    "https://openalex.org/W3010607409",
    "https://openalex.org/W3025939866",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W2170540710",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W1557833142",
    "https://openalex.org/W2077217970",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2889507104",
    "https://openalex.org/W2970055833"
  ],
  "abstract": "Shlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Sofia Erell, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, Yossi Matias. Proceedings of the Third Workshop on Privacy in Natural Language Processing. 2021.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 21–29\nJuly 5–10, 2020. ©2020 Association for Computational Linguistics\nhttps://doi.org/10.26615/978-954-452-056-4_003\n21\nLearning and Evaluating a Differentially Private Pre-trained Language\nModel\nShlomo Hoory∗, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell,\nItay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini,\nAvinatan Hassidim and Yossi Matias\nGoogle\nTel Aviv, Israel\n{afeder,tendler,aloncohen,rovinsky}@google.com\nAbstract\nContextual language models have led to sig-\nniﬁcantly better results on a plethora of lan-\nguage understanding tasks, especially when\npre-trained on the same data as the down-\nstream task. While this additional pre-training\nusually improves performance, it can lead to\ninformation leakage and therefore risks the pri-\nvacy of individuals mentioned in the training\ndata. One method to guarantee the privacy\nof such individuals is to train a differentially-\nprivate model, but this usually comes at the\nexpense of model performance. Moreover, it\nis hard to tell given a privacy parameter ϵ\nwhat was the effect on the trained represen-\ntation. In this work we aim to guide future\npractitioners and researchers on how to im-\nprove privacy while maintaining good model\nperformance. We demonstrate how to train\na differentially-private pre-trained language\nmodel (i.e., BERT) with a privacy guarantee\nof ϵ = 1 and with only a small degradation in\nperformance. We experiment on a dataset of\nclinical notes with a model trained on a target\nentity extraction task, and compare it to a sim-\nilar model trained without differential privacy.\nFinally, we present experiments showing how\nto interpret the differentially-private represen-\ntation and understand the information lost and\nmaintained in this process.\n1 Introduction\nRecent advancements in natural language process-\ning (NLP), mainly the introduction of the trans-\nformer architecture and contextual language repre-\nsentations, have led to a surge in the performance\nand applicability language models. Such models\nrely on pre-training on massive self-labeled cor-\npora to incorporate knowledge within the language\nrepresentation. Additionally, when presented with\na new dataset and task, such models often gain\nfrom an additional pre-training stage, where they\n∗Work was done while at Google.\nare trained to solve a language modeling task on\nthe new training data.\nWhile the pre-training steps are crucial for good\nmodel performance on downstream tasks, it can\ncome at the expense of the privacy of the persons\nmentioned in the data. As these models learn to pre-\ndict words using their context, they often memorize\nindividual words and phrases. Such memorization\ncan lead to information leakage when using the\ntrained models or the language representation. This\nproblem is ampliﬁed in medical domains, where pa-\ntients data might leak and expose Protected Health\nInformation (PHI).\nOne solution for pre-training the model while\npreserving patients’ privacy is to train the model\nwith a differential privacy guarantee. However, for\na sufﬁciently small privacy parameterϵ, this usually\ncomes at the expense of model performance. Also,\nit was only shown to work for recurrent language\nmodels, and not for more recent systems that are\nbased on the transformer architecture (McMahan\net al., 2018; Kerrigan et al., 2020). Apart from\ntheir size (our model has 109M trainable parame-\nters), transformer-based language models introduce\nan additional privacy concern, as their reliance on\nWordPiece based tokenization algorithm can also\npotentially leak private information.\nMoreover, even with a sufﬁciently small ϵguar-\nantee, it is hard to test and evaluate the result-\ning privacy-preserving properties of the model.\nOne also has difﬁculty understanding whether the\ndifferentially-private training procedure affected\nthe language representation other than by measur-\ning performance on a downstream task. For exam-\nple, it could be that other valuable information was\nalso lost during training.\nIn this work we provide here a detailed solu-\ntion to training a differentially-private contextual\nembedding model, and to better understand the re-\nsulting representation. We start by presenting a\nmethod for training BERT, a contextual embedding\n22\nmodel, on medical data with a strong privacy guar-\nantee of ϵ = 1 and with only a small degradation\nin performance (Section 3). Possibly the most ma-\njor technical challenge in doing so is the fact that\nthe training batch size has to be fairly large, all\nthe while training on speciﬁc hardware (TPUs) in\nwhich the batch size is limited. We overcome this\nobstacle by distributing each training batch over\ntime during the training process, along with other\nuseful manipulations (Section 2.1). As these mod-\nels gain from retraining the WordPiece algorithm\non the target dataset, we propose a differentially-\nprivate WordPiece algorithm, preventing additional\ninformation leakage through the model’s vocabu-\nlary (Section 3.2).\nAfter training the differentially-private BERT on\nclinical notes, we follow common wisdom and pro-\nvide privacy tests to show that information leakage\nhas been prevented in this process (Section 5). We\nfurther provide adversarial attacks that can help\nunderstand the privacy guarantees in terms of mem-\norized words and phrases. These tests, when com-\nbined, provide a useful toolbox for understanding\nhow “private” is the differentially-private model.\n2 Previous Work\nSince the introduction of the differentially-private\nStochastic Gradient Descent (SGD) algorithm\n(Song et al., 2013; Abadi et al., 2016b), it is possi-\nble to train deep neural networks (DNN) with pri-\nvacy guarantees. Speciﬁcally, there have been sev-\neral attempts to train DNN-based language models\nwith such guarantees, though with mixed results in\nterms of performance on downstream tasks (McMa-\nhan et al., 2018; Kerrigan et al., 2020). To better\nunderstand the trade-offs between the performance\nand privacy of deep language models, we survey\nhere the literature on differentially-private training\nand on methods for measuring privacy in language\nmodels.\n2.1 Training Differentially-Private Models\nDifferential Privacy (DP; Dwork et al., 2006b;\nDwork, 2011; Dwork et al., 2014) is a framework\nthat quantiﬁes the privacy leaked by some random-\nized algorithm accessing a private dataset. In the\ncontext of training a machine learning model on\nprivate data, it enables one to bound the poten-\ntial privacy leakage by releasing the model to the\nworld.\nDeﬁnition 1 ((ϵ, δ)-DP) Given some ϵ, δ > 0 , we\nsay that algorithm Ahas (ϵ, δ)-differential privacy,\nif for any two datasets D , D′ differing in a single\nelement and for all S ⊆Range(A), we have:\nPr[A(D) ∈S] ≤eϵ Pr[A(D′) ∈S] + δ.\nThe leading method for training models with\nsmall differential privacy parameters ϵ, δis the DP-\nSGD method by Abadi et al. (2016b). The method\nwas subsequently incorporated into Tensorﬂow’s\nprivacy toolbox with improved privacy analysis\n(Mironov, 2017; Mironov et al., 2019). The basic\nidea behind DP-SGD is to clip and add noise to the\nper example gradients of the loss function during\nmodel training. The intuition is that such a mecha-\nnism guarantees that, for each step, the inﬂuence\nof each example on the outcome is bounded.\nIn the context of NLP, there have been several at-\ntempts to train language models using the DP-SGD\nalgorithm. Speciﬁcally, McMahan et al. (2018) pre-\nsented a pipeline for training differentially-private\nlanguage models based on the recurrent neural net-\nwork (RNN) architecture. While successful on the\nRNN architecture, results on a ﬁne-tuned trans-\nformer, speciﬁcally GPT-2, were shown to be less\nsuccessful in preserving privacy without hurting\ntask performance (Kerrigan et al., 2020). In this\npaper, we present the ﬁrst, as far as we know, suc-\ncessfully trained differentially private BERT model,\nwith a strong privacy guarantee and with only a\nsmall decrease in downstream performance.\n2.2 Evaluating the Privacy of Language\nModels\nWhile differential privacy training provides privacy\nguarantees (in terms of the privacy parametersϵ, δ),\nit is often hard to evaluate the practical implication\nof such a guarantee. In the context of language\nmodels, evaluation becomes even trickier. Private\ninformation might be encoded in speciﬁc phrases\ncontained in the text, but it can also be implicitly\ncontained in the language model. In the context of\nclinical notes, for example, information regarding\nthe linguistic style of the doctor can be captured\nand predicted from linguistic cues in the text itself\n(Rosenthal and McKeown, 2011; Preo¸ tiuc-Pietro\net al., 2015; Coavoux et al., 2018).\nSong and Raghunathan (2020) studied informa-\ntion leakage from language representations, and\npresented several methods for evaluating the pri-\nvacy preserving qualities of trained language mod-\nels. They provided a taxonomy of adversarial at-\ntacks, differing by the adversary’s access to model’s\n23\ninternal state. Speciﬁcally, they deﬁned member-\nship attacks on language representation, which are\ndesigned to detect memorized information. In this\npaper, we build on the secret sharer membership\ntest, a method for quantitatively assessing the risk\nthat rare or unique training-data sequences are un-\nintentionally memorized by generative sequence\nmodels (Carlini et al., 2019). While not speciﬁcally\ndesigned for language models such as BERT, it ﬁts\nthe DP evaluation setup perfectly. Concretely, in\nthis test a secret sharer plants n identical occur-\nrences of a k WordPiece sequence into the train\ncorpus. The sequence itself consists of i.i.d. ran-\ndom WordPieces where the middle is the secret.\nThe model is then trained on the modiﬁed corpus\nand evaluated for each planted sequence by trying\nto predict the secret WordPiece.\nIn Section 5, we show that unlike the original\nBERT model, our trained DP-BERT model does\nnot memorize sequences of words introduced via\nthe secret sharer.\n3 Training Differentially Private\nContextual Language Models\nTraining differentially private language models be-\ncomes exceedingly difﬁcult with model size. As\nsuch, attempting to train a transformer model such\nas BERT using the DP-SGD algorithm and without\nany modiﬁcations will usually lead to a signiﬁcant\nperformance degradation (Kerrigan et al., 2020).\nMoreover, as the WordPiece algorithm, the process\nthat tokenizes the textual input of BERT, is not dif-\nferentially private, training will not guarantee that\nthere is no information leakage. In this section,\nwe formulate the problem of training a DP BERT\nmodel on medical text, and explain the process\nof constructing a differentially private vocabulary.\nWe then discuss the importance of parallel training\nand very large batch sizes in training such large\nlanguage models, and provide a method for sufﬁ-\nciently increasing such crucial parameters.\n3.1 Problem Formulation\nWe choose to focus our DP training on the task of\nentity extraction (EE) from medical text, speciﬁ-\ncally clinical notes. Clinical notes include medi-\ncally relevant information regarding patients’ con-\nditions, and are often used as training data for\ndownstream machine learning tasks (Esteva et al.,\n2019). However, they can contain Protected Health\nInformation (PHI) as well as additional informa-\ntion that might put patients at risk (Feder et al.,\n2020; Hartman et al., 2020). For this reason, lan-\nguage models trained on such datasets must be\nable to learn domain-relevant information (such as\nmedical jargon and doctors’ writing style) without\nmemorizing private information (Lee et al., 2020).\nTo test our ability to train a DP language model\non clinical notes, we use a BERT model (Devlin\net al., 2019) with specialization to the medical do-\nmain. To this end, the public Wikipedia and Book-\nCorpus datasets (Zhu et al., 2015) used to train\nBERT were amended with the Medical Informa-\ntion Mart for Intensive Care III corpus (Johnson\net al., 2016, MIMIC-III) in order to improve per-\nformance on medical tasks. Although MIMIC-III\nhas undergone a de-identiﬁcation process aimed\nto remove revealing information such as names\nand dates, the corpus and its derivative models are\nnot considered public, and their use must adhere\nto certain restrictions. As a consequence, a need\narises to build a medical BERT model with sub-\nstantial differential privacy guarantees on its use of\nMIMIC-III, and this work aims to do exactly that.\nBefore introducing changes designed to guaran-\ntee privacy, let us review the procedure used to\nobtain the Medical BERT model. The available\nresources are the 3 billion word Wikipedia + Book-\nCorpus, and the 712M word MIMIC-III corpus.\nThe training process consists of the following three\nsteps:\n(i) Build the vocab from the MIMIC-III corpus.\n(ii) Train BERT from scratch on the Wikipedia +\nBookCorpus using the new vocab.\n(iii) Continue BERT’s training on the MIMIC-III\ncorpus.\nThe steps that are susceptible to leaking MIMIC-\nIII data are the ﬁrst, and the third. Therefore, by\nthe composability property of differential privacy\n(Dwork et al., 2014, Theorem 3.16), our problem\nreduces to providing algorithms with satisfactory\nDP guarantees for steps 1 and 3 without causing\na signiﬁcant performance loss. We discuss these\nproblems in the following two subsections.\n3.2 Constructing a differentially private\nvocabulary\nTransformer-based models commonly tokenize in-\nputs into WordPieces using the WordPiece algo-\nrithm. The WordPiece algorithm (Wu et al., 2016)\nis a general method for improving the generaliza-\ntion properties of a language model by tokenizing\n24\nbased on the most frequent combination of symbols\nrather than words. While its efﬁcacy is undisputed,\nit can leak private information by memorizing cer-\ntain WordPieces in the training data. To prevent\nsuch leakage, we modify this algorithm to be dif-\nferentially private. We do so as follows.\nThe WordPiece algorithm starts with construct-\ning the word histogram of the corpus. This his-\ntogram is then manipulated to obtain the Word-\nPiece output vocabulary. Since differential privacy\nis robust to post-processing, it is enough to make\nthe input histogram differentially private in order to\nguarantee a differentially-private end-result vocab-\nulary. Our differentially-private WordPiece algo-\nrithm is therefore to add noise to the histogram with\ngiven privacy parameters and apply the standard\nWordPiece algorithm.\nHistogram noising is done following (Korolova\net al., 2009; Bun et al., 2019), let X be the set of all\npossible n distinct words. For the input histogram\nh : X →R, we do:\n(i) For all x ∈X, if h(x) > 0, add Laplace noise:\nh(x) ←h(x) + Lap(2/ϵ).\n(ii) For all x ∈X, if h(x) < 1 + 2 ln(2/δ)/ϵ, set\nh(x) ←0.\nThe output h of this process satisﬁes ( ϵ, δ)-\ndifferential privacy with respect to replacing one\nof the words in the histogram counts. Assuming\n0 < ϵ < ln(n), 0 < δ < 1/ n (Bun et al., 2019;\nKorolova et al., 2009).\nIn order to obtain differential privacy at the level\nof BERT example (256???? WordPiece) we use\nthe basic composition theorem for non-adaptive\nqueries (Dwork et al., 2006a; Dwork and Lei,\n2009):\nTheorem 1 Let M1, ... Mk be (ϵ, δ)-differentially\nprivate, then (M1, ... , Mk) is (kϵ, kδ)-differentially\nprivate.\nWe used parameters ϵ′ =?, δ′ =?? in the noisy\nhistogram algorithm above to achieve an example\nlevel (ϵ= 256∗?, δ= 256∗?) differential privacy.\n3.3 Training a differentially private BERT\nWe use the DP-SGD method supplied the TF pri-\nvacy toolbox (see Section 2.1). The parameters\nof the algorithm are the number of steps, batch-\nsize B, ℓ2-norm-clip C, and the noise multiplier σ.\nTo ﬁx notation, we formally deﬁne the DP-SGD\nstep, as deﬁned in Abadi et al. (2016b, Algorithm\n1). Given the per-example gradients of the loss\nfunction g1, ... , gB, the gradient ˜g for passing to\napply_gradients is deﬁned by:\ngi = gi/ max(1,∥gi∥2/C), for all i; (1)\n˜g = 1\nB\n(∑\ni\ngi + N(0, σ2C2I)\n)\n. (2)\nThe most important parameter of the algorithm\nis the noise multiplier σ—increasing σ directly\ndecreases ϵ; i.e., increases the differential-privacy\nguarantee of the algorithm. On the other hand\nit harms performance on the target data-set, and\nthus a careful choice of σis necessary to trade-off\nprivacy against performance. Moreover, we choose\nthe noise σ to be proportional to the square root\nof the batch size B. This is done in order to make\nthe privacy guarantee oblivious to changes in the\nbatch size B (as one can observe from Eq. (2)). The\nprivacy guarantee is also affected by the number of\ntraining steps (or epochs), but this behavior is more\ngradual since ϵincreases near-linearly in the range\nof interest. In our experience, the clip level C is of\nlesser importance and we ﬁx it to be 0.01.\nFor any choice of parameters, we upper bound\nthe privacy parameter ϵ using the TF privacy\ntoolbox compute_dp_sgd_privacy function,\nwhere we also use the number of MIMIC examples\nN = 83M. We ﬁx privacy δ to be 10–8, which is\nsmaller than 1/N.\nThe effect of parallelism. In order to make the\ntraining run faster, we use TPUs 1 to parallelize\ntraining by splitting example batches to shards.\nThis mechanism is readily available through Ten-\nsorﬂow (TF; Abadi et al., 2016a), but its effect\nhas to be taken into account when computing the\nbounds on ϵ.\nIn order to understand this effect, let us ﬁrst\nreview the way we incorporate TF privacy\ninto the BERT training code. The change\nconsists of changing the loss computation code\nto compute the vector loss (per-example loss),\nand of wrapping the existing Adam weight\ndecay optimizer (Kingma and Ba, 2015), our\noptimizer of choice, by the DP optimizer using\nthe make_gaussian_optimizer_class\nmethod.\nThe subtle point lies in the second change,\nas the optimization is also wrapped by\nCrossShardOptimizer which handles\nthe sharded batching. Let B denote the unsharded\n1https://cloud.google.com/tpu/docs/\ntpus.\n25\nbatch size, and P denote the number of parallel\nshards. For each batch, the examples are split\nbetween P independent instances of the TF privacy\noptimizer, each handling B/P examples. For each\nshard, the gradients are clipped, averaged and\nnoise is added by equations Eqs. (1) and (2).\nSubsequently, the CrossShardOptimizer\naverages the P shard gradients to obtain the single\ngradient to be passed to apply_gradients.\nTherefore, denoting thei-th gradient of shardj by\ngi,j, the gradient passed to apply_gradients\ncan be written as follows:\n˜g = 1\nP\n∑\nj\n[\n1\nB/P\n(∑\ni\ngi,j + N(0, σ2C2I)\n)]\n= 1\nB\n\n∑\ni,j\ngi,j + N(0, Pσ2C2I)\n\n. (3)\nThis implies that using noise multiplier σwith P\nshards is equivalent to an unsharded training with\nnoise multiplier σ\n√\nP. As computing an upper\nbound on ϵthrough TF privacy does not take paral-\nlelism into account, one must use σ\n√\nP as the noise\nmultiplier in order to get the correct result.\nAchieving larger batch sizes. As it quickly be-\ncame apparent throughout this project, we needed\nlarger batch sizes. However, usually batch size\ncannot increase beyond a certain point because of\nmemory considerations, and limitation on the num-\nber of available TPUs. With the resources avail-\nable to us, we couldn’t get beyond parallelism of\nP = 256 with sharded batch size of 32, achieving\ntotal batch size B = 8192.\nThe way we chose to solve this problem is to\nspread the batch in time, so apply_gradients\nis called only after T batches are processed with\nthe average total gradient. This is equivalent to\nincreasing both P and B by a factor of T. With\nthis method, the only limit on T is processing time.\nFrom our experience, the value of T = 32 is a\nreasonable choice, achieving parallelism of P =\n256 ·32 and total batch size B of 128k with the\nabove parameters.\nWe brieﬂy remark upon the implementation of\nthis mechanism. For every trainable variable, we\ncreated a variable with /grad_acc sufﬁx added\nto the original name. For each step, thetrain_op\neither accumulates the current gradients in the\nnew variables, or zeros the accumulator and calls\napply_gradients, depending on the current\nstep modulo T.\n4 Experimental Setup\nWe design our experiments to demonstrate the abil-\nity of the DP training scheme to improve perfor-\nmance while preserving privacy. We focus on the\nmedical domain as it has strict privacy requirements\nand its language is distinct enough such that addi-\ntional pre-training should be useful. We start by\ndescribing the data used for the DP training and\nrelevant implementation details. We then present\nthe entity extraction task used for the supervised\ntask training and evaluation. Finally, we discuss\nthe relevant baselines, chosen to demonstrate the\nefﬁcacy of the DP training scheme.\nPre-training data. For the DP pre-training, we\nsupplement the original training data used in Devlin\net al. (2019) with the MIMIC-III dataset, a com-\nmonly used collection of medical information that\ncontains more than 2 million distinct notes (John-\nson et al., 2016; Alsentzer et al., 2019). MIMIC-III\ncovers 38,597 distinct adult patients and 49,785\nhospital admissions between 2001 and 2012. The\nclinical notes in this dataset are widely used by\nNLP researchers for a variety of clinically-related\ntasks (Feder et al., 2020; Hartman et al., 2020), and\nwere previously used for pre-training BERT mod-\nels speciﬁcally for the medical domain (Alsentzer\net al., 2019).\nUsing the combined dataset, we train our DP-\nBERT model using the the training scheme de-\nscribed in Section 3. At this point, we use a Word-\nPiece vocabulary generated from MIMIC-III with-\nout privacy guarantees.\nEntity-extraction task. For the supervised task\ntraining, we use i2b2-2010, a dataset from the i2b2\nNational Center for Biomedical Computing for the\nNLP Shared Tasks Challenges (Uzuner et al., 2011).\nThis dataset contains clinical notes tagged for con-\ncepts, assertions, and relations. In this task, 170\npatient reports are labeled with three concepts: test,\ntreatment, and problem. The total number of enti-\nties in each category are as follows:\n• Problem: 7, 073\n• Test: 4, 608\n• Treatment: 4, 844\nWe perform 5-fold cross validation where each fold\nhas random training (136 notes) and test (34 notes)\nsets.\n26\nBaselines. We compare our differentially private\nBERT model, denoted as DP BERT, to several non\nprivate baselines:\nBERT (Wikipedia + Books) We train a BERT-\nlarge model, as in Devlin et al. (2019), using\nthe default hyperparameters.\nBERT-M (Wikipedia + Books + MIMIC-III)\nWe supplement the original training from\nDevlin et al. (2019) with the MIMIC-III\nclinical notes corpus. In addition, we also\nuse a (non-differentially private) WordPiece\nvocabulary generated from MIMIC-III.\nBioBERT We use the training data presented in\nLee et al. (2020), and use it to train BERT. We\ntested version v1.1 which it trained using the\noriginal dataset + 1M PubMed abstracts.\nIn Section 5 we compare several differentially\nprivate models, discuss their differences and high-\nlight the effect of certain parameters (as discussed\nin Section 3) on the EE task performance.\n5 Results\nIn this section we empirically evaluate the trade-\noffs between a model’s privacy and its usefulness.\nPreviously, in Section 3, we have shown how to pre-\ntrain a contextual embedding model such as BERT\nwith any, possibly substantial, privacy guarantee.\nWe naturally expect that a stronger privacy guaran-\ntee would entail that less information is preserved\nduring pre-training, which in turn would degrade\nperformance on downstream tasks. Thus, we aim\nto ascertain the exact trade-off between these two\ngoals in order to be able to choose a model that has\nboth good performance and a satisfactory privacy\nguarantee.\nWe provide two sets of experiments to help bet-\nter understand this trade-off as well as to provide\npractitioners with tools to understand the effects\nof DP pre-training. First, we use the pre-trained\nDP model and ﬁne-tune it on the aforementioned\nEE task. Then, we test the ability of the model to\nmemorize private information and show that it is\nprotected against commonly used privacy attacks.\nAggregating both results, we argue that medically-\nrelevant information is preserved in the DP model\nall the while private information is not revealed.\n5.1 Preserving Useful Information\nFor our ﬁrst experiment, we pre-trained a DP BERT\nmodel, then evaluated on an EE task over the i2b2-\n2010 dataset. We summarize our results in the\nFigure 1: Top to bottom - privacy parameterϵ(red) and\ntest F1 score on the EE task (blue), as a function of:\nnoise multiplier σ; number of pre-training epochs; pre-\ntraining batch size.\nfollowing table.\nModel ϵ F1 Score\nBERT ∞ 76.3%\nBERT-M ∞ 86.8%\nBioBERT ∞ 86.5%\nBERT-M 3.2 84.5%\nBERT-M 1 83.7%\nTable 1: Results on the Medical Entity Extraction task.\nϵ= ∞means no differential privacy.\nThese were all evaluated after 1M training steps\nwith batch size 128K. As one can observe, the ad-\nditional pre-training either on MIMIC-III or on\nPubMed gives a signiﬁcant boost in performance\nover the off-the-shelf BERT. The addition of differ-\nential privacy then deteriorates performance only\nslightly, and, as expected, performance is inversely\nproportional to ϵ(recall that smallerϵimplies better\nprivacy).\nIn addition, in Fig. 1 we evaluate the change in\nϵand of the F1 score of the downstream task as a\nfunction of batch size, noise multiplier σ, and the\nnumber of pre-training epochs. The behavior in\nall three parameters is as expected. Increasing σ\nenables more privacy (lower ϵ), but worsens perfor-\nmance. Similarly, with more pre-training epochs\nthe model gathers more information about the train-\ning data, so we obtain better F1 score but worse\nprivacy preservation (higher ϵ). When increasing\nthe batch size, we also increase the noise multiplier\n27\nFigure 2: Secret exposure as a function of the number\nof secret occurrences. Black lines for models with dif-\nferential privacy ϵ= 0.58, red lines for models without\nDP ϵ= ∞.\nσproportionally, thus both ϵand the F1 decrease.\n5.2 Forgetting Private Information\nFor our second experiment, we followed Carlini\net al. (2019) to test the model’s ability to memorize\nprivate information. We injected the MIMIC-III\ndataset with “secrets”, of the form HS, HSH, and\nHHSHH, where H is a generic word and S is a\nsecret word. The injection was done by sampling\nlocations to plant each secret uniformly at random\nfrom the dataset. We tested all three forms of se-\ncrets on a DP model and a non-DP model, with\ndifferent numbers of appearances of the secret in\nthe dataset. For each such evaluation, we measured\nthe exposure of the secret which essentially mea-\nsures how well the model memorized the secret\n(see Carlini et al., 2019 for exact deﬁnition of “ex-\nposure”). As one can see from Fig. 2, even when\nthe secret appears as much as 100K times in the\ndata, the DP model performs signiﬁcantly better\nthan without differential privacy. This seems to\nsuggest that the model learns through information\nthat helps it generalize rather than memorizing the\ndataset in its entirety, which includes private and\npersonal information as well.\n6 Discussion and Future Work\nIn this paper, we have shown a pipeline for learning\nand evaluating a differentially-private contextual\nlanguage model. We have deﬁned the problem of\nlearning such a model with end-to-end privacy guar-\nantees and have discussed the pitfalls that might\nlead to poor downstream performance. To over-\ncome the difﬁculties associated with learning such\nmodels, we have offered practical measures for\ncircumventing them, most notably through vastly\nincreasing batch sizes. Then, to increase the trust of\nthe DP trained contextual language model, we have\nutilized a secret sharer evaluation test and showed\nthat our trained language model does not memorize\nprivate information.\nWhile these results are deﬁnitely encouraging,\nmore research is needed. Our results are conﬁned\nto the medical domain, where privacy needs are per-\nhaps most stringent. Showing the efﬁcacy of this\ntraining and evaluation pipeline on other domains\nwould certainly increase the trust in it. Additionally,\nwe have not yet measured the model’s performance\nwith the DP WordPiece algorithm. In future work,\nwe plan to provide more theoretical and empirical\nsupport for end-to-end privacy guarantees.\nFinally, the observed performance gain due to\nthe vocabulary training presents an interesting ques-\ntion for the larger NLP community. Understanding\nthe importance of vocabulary vs. linguistic style\nwhen performing additional pre-training could im-\nprove the domain adaptation capabilities of existing\nNLP systems. In future work, we plan to expand\nour DP training to additional domains, allowing\nus to test the power of vocabulary modiﬁcations\nvia the DP WordPiece training in increasing across\ndomain performance.\nReferences\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\net al. 2016a. Tensorﬂow: Large-scale machine learn-\ning on heterogeneous distributed systems. arXiv\npreprint arXiv:1603.04467.\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016b. Deep learning with differential\nprivacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Se-\ncurity, pages 308–318.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, W A Red-\nmond, and Matthew BA McDermott. 2019. Publicly\navailable clinical bert embeddings. NAACL HLT\n2019, page 72.\nMark Bun, Kobbi Nissim, and Uri Stemmer. 2019. Si-\nmultaneous private learning of multiple concepts. J.\nMach. Learn. Res., 20:94–1.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th {USENIX} Security Sympo-\nsium ({USENIX} Security 19), pages 267–284.\n28\nMaximin Coavoux, Shashi Narayan, and Shay B Co-\nhen. 2018. Privacy-preserving neural representa-\ntions of text. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1–10.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCynthia Dwork. 2011. A ﬁrm foundation for pri-\nvate data analysis. Communications of the ACM ,\n54(1):86–95.\nCynthia Dwork, Krishnaram Kenthapadi, Frank Mc-\nSherry, Ilya Mironov, and Moni Naor. 2006a. Our\ndata, ourselves: privacy via distributed noise gener-\nation. In Advances in Cryptology - EUROCRYPT\n2006, pages 486–503. Springer.\nCynthia Dwork and Jing Lei. 2009. Differential pri-\nvacy and robust statistics. In Proceedings of the\nForty-First Annual ACM Symposium on Theory of\nComputing, pages 371–380. Association for Com-\nputing Machinery.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006b. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference, pages 265–284. Springer.\nCynthia Dwork, Aaron Roth, et al. 2014. The algo-\nrithmic foundations of differential privacy. Founda-\ntions and Trends in Theoretical Computer Science ,\n9(3-4):211–407.\nAndre Esteva, Alexandre Robicquet, Bharath Ramsun-\ndar, V olodymyr Kuleshov, Mark DePristo, Katherine\nChou, Claire Cui, Greg Corrado, Sebastian Thrun,\nand Jeff Dean. 2019. A guide to deep learning in\nhealthcare. Nature medicine, 25(1):24–29.\nAmir Feder, Danny Vainstein, Roni Rosenfeld, Tzvika\nHartman, Avinatan Hassidim, and Yossi Matias.\n2020. Active deep learning to detect demographic\ntraits in free-form clinical notes. Journal of Biomed-\nical Informatics, 107:103436.\nTzvika Hartman, Michael D Howell, Jeff Dean,\nShlomo Hoory, Ronit Slyper, Itay Laish, Oren\nGilon, Danny Vainstein, Greg Corrado, Katherine\nChou, et al. 2020. Customization scenarios for de-\nidentiﬁcation of clinical notes. BMC medical infor-\nmatics and decision making, 20(1):1–9.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3(1):1–9.\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020.\nDifferentially private language models beneﬁt from\npublic pre-training. In Proceedings of the Second\nWorkshop on Privacy in NLP, pages 39–45.\nDiederik P Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In ICLR\n(Poster).\nAleksandra Korolova, Krishnaram Kenthapadi, Nina\nMishra, and Alexandros Ntoulas. 2009. Releasing\nsearch queries and clicks privately. In WWW, pages\n171–180. ACM.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nIlya Mironov. 2017. Rényi differential privacy. In\n2017 IEEE 30th Computer Security Foundations\nSymposium (CSF), pages 263–275. IEEE.\nIlya Mironov, Kunal Talwar, and Li Zhang. 2019.\nRenyi differential privacy of the sampled gaussian\nmechanism. arXiv preprint arXiv:1908.10530.\nDaniel Preo¸ tiuc-Pietro, Vasileios Lampos, and Niko-\nlaos Aletras. 2015. An analysis of the user occupa-\ntional class through twitter content. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1754–1764.\nSara Rosenthal and Kathleen McKeown. 2011. Age\nprediction in blogs: A study of style, content, and\nonline behavior in pre-and post-social media genera-\ntions. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 763–772.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC Conference\non Computer and Communications Security , pages\n377–390.\nShuang Song, Kamalika Chaudhuri, and Anand D Sar-\nwate. 2013. Stochastic gradient descent with dif-\nferentially private updates. In 2013 IEEE Global\nConference on Signal and Information Processing ,\npages 245–248. IEEE.\nÖzlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Asso-\nciation, 18(5):552–556.\n29\nY . Wu, M. Schuster, Z. Chen, Q.V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey, and J. Klingner. 2016. Google’s neu-\nral machine translation system: Bridging the gap\nbetween human and machine translation. arXiv\npreprint arXiv:1609.08144.\nYukun Zhu, Ryan Kiros, Richard S Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In ICCV.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7013468742370605
    },
    {
      "name": "Natural language processing",
      "score": 0.6126208901405334
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5580381751060486
    },
    {
      "name": "Natural language",
      "score": 0.43643322587013245
    }
  ]
}