{
  "title": "Overview of the Transformer-based Models for NLP Tasks",
  "url": "https://openalex.org/W3092557781",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3092220070",
      "name": "Anthony Gillioz",
      "affiliations": [
        "University of Neuchâtel"
      ]
    },
    {
      "id": "https://openalex.org/A2898874726",
      "name": "Jacky Casas",
      "affiliations": [
        "HES-SO Fribourg"
      ]
    },
    {
      "id": "https://openalex.org/A163208669",
      "name": "Elena Mugellini",
      "affiliations": [
        "HES-SO Fribourg"
      ]
    },
    {
      "id": "https://openalex.org/A421264254",
      "name": "Omar Abou Khaled",
      "affiliations": [
        "HES-SO Fribourg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2119168550",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2523469089",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W6637409405",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965210982",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2920812691",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W6770131634",
    "https://openalex.org/W2994980856",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W6758657797",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W4394650345",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2963310665"
  ],
  "abstract": "In 2017, Vaswani et al. proposed a new neural network architecture named Transformer. That modern architecture quickly revolutionized the natural language processing world. Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-the-art networks. It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures. In this paper, we provide an overview and explanations of the latest models. We cover the auto-regressive models such as GPT, GPT-2 and XLNET, as well as the auto-encoder architecture such as BERT and a lot of post-BERT models like RoBERTa, ALBERT, ERNIE 1.0/2.0.",
  "full_text": "Overview of the Transformer-based Models for\nNLP T asks\nAnthony Gillioz\nUniversity of Neuchâtel\nNeuchâtel, Switzerland\nEmail: anthony .gillioz@unine.ch\nJacky Casas, Elena Mugellini, Omar Abou Khaled\nUniversity of Applied Sciences and Arts W estern Switzerland\nFribourg, Switzerland\nEmail: {ﬁrstname.lastname}@hes-so.ch\nAbstract—In 2017, V aswani et al. proposed a new neural\nnetwork architecture named T ransformer . That modern archi-\ntecture quickly revolutionized the natural language processing\nworld. Models like GPT and BERT relying on this T ransformer\narchitecture have fully outperformed the previous state-of-the-\nart networks. It surpassed the earlier approaches by such a wide\nmargin that all the recent cutting edge models seem to rely on\nthese T ransformer-based architectures.\nIn this paper , we provide an overview and explanations of the\nlatest models. W e cover the auto-regressive models such as GPT ,\nGPT -2 and XLNET , as well as the auto-encoder architecture\nsuch as BERT and a lot of post-BERT models like RoBERT a,\nALBERT , ERNIE 1.0/2.0.\nI. I N T RO D U C T I O N\nT\nHE understanding and the treatment of the ubiquitous\ntextual data is a major research challenge. The tremen-\ndous amount of data produced by our society through social\nmedia and companies has exploded over the past years. All\nthose information are most of the time stored under textual\nformat. The human brain can extract the meaning out of text\neffortlessly , but this is not the case for a computer. It is then\nrequired to have performing and reliable techniques to treat\nthis data.\nThe Natural Language Processing (NLP) domain aims to\nprovide a set of techniques able to explain a wide variety of\nNatural Language tasks such as Automatic Translation [1],\nT ext Summarization [2], T ext Generation [3]. All those tasks\nhave in common the meaning extraction process to be suc-\ncessful. Undoubtedly , if a technique were able to understand\nthe underlying semantic of texts, this would help to resolve\nthe majority of the modern NLP problems.\nA big concern that restricts a general NLP resolver is the\nsingle-task training scheme. Gathering data and crafting a\nspeciﬁc model to solve a precise problem works successfully .\nHowever, it forces us to come up with a solution not only\neach time a new issue arises but also to apply the model on\nanother domain. A general multi-task solver may be preferable\nto avoid this time-consuming point.\nRecurrent Neural Networks (RNN) were massively used to\nsolve NLP problems. They have been popular for a few years\nin supervised NLP models for classiﬁcation and regression.\nThe success of RNNs is due to the Long Short T erm Memory\n(LSTM) [4] and Gated Recurrent Unit (GRU) [5] architectures.\nThose two units prevent the vanishing gradient issue by\nproviding a more direct way to the backpropagation of the\ngradient. It helps the computation when the sentences are long.\nThe high versatility of those networks can solve a wide\nvariety of problems [6]. Unfortunately , those models are not\nperfect; the inherent recurrent structure made them hard to\nparallelize on multiple processes, and the treatment of very\nlong clauses is also problematic due to the vanishing gradient.\nT o counter those two limiting constraints, [7] introduced a\nnew model architecture: the Transformer. The proposed tech-\nnique get rid of the recurrent architecture to rely on attention\nmechanism solely . Furthermore, it does not suffer from the\ngradient vanishing nor the hard parallelization issue. That\nfacilitates and accelerates the training of broader networks.\nThis work aims to provide a survey and an explanation of\nthe latest Transformer-based models.\nII. B AC K G RO U N D\nIn this section, we introduce a general NLP background. It\ngives a broad insight into the unsupervised pre-training and\nthe NLP state-of-the-art pre-Transformers.\nA. Unsupervised Pre-training\nThe unsupervised pre-training is a particular case of semi-\nsupervised learning. That is massively used to train the Trans-\nformer models. That principle works in two steps; the ﬁrst one\nis the pre-training phase. It computes a general representation\nfrom raw data in an unsupervised fashion. Second, once it is\ncomputed, it can be adapted to a downstream task via ﬁne-\ntuning techniques.\nThe principal challenge is to ﬁnd an unsupervised objective\nfunction that generates a good representation. There is no\nconsensus on which task provides the most efﬁcient textual de-\nscription. [8] propose a language modelling task, [9] introduce\na masked language modeling objective, [10] use a multi-tasks\nlanguage modeling.\nB. Context-free representation\nThe recent signiﬁcant increase in the performance of NLP\nmodels is due to the use of word embeddings. It consists\nof representing a word as a unique vector. The terms with\nthe same meaning are located in a close area of each other.\nW ord2V ec [11] and Glove [12] are the most frequently used\nword embedding methods. They treat a large corpus of text and\nProceedings of the Federated Conference on\nComputer Science and Information Systems pp. 179–183\nDOI: 10.15439/2020F20\nISSN 2300-5963 ACSIS, V ol. 21\nIEEE Catalog Number: CFP2085N-AR T ©2020, PTI 179\nproduce a unique word representation in a high dimensional\nspace.\nByte Pair Encoding (BPE) [13] is another word embedding\ntechnique using subwords units out of character-level and\nword-level representation. [14] changed the implementation\nof BPE to be based on bytes instead of Unicode characters.\nThus, he could reduce the vocabulary size from 100K+ to\napproximately 50K tokens. That has the advantage not to\nintroduce [UKN] (unknown) symbols. Besides that, it does\nnot involve a heuristic preprocessing of the input vocabulary .\nIt is used when the amount of corpus to treat is too large and a\nmore efﬁcient technique than W ord2V ec or Glove is required.\nC. Attention Layer\nPrimarily proposed by [5], the attention mechanism aims to\ncatch the long-term dependencies of sentences. The relation-\nships between entities in phrases are hard to spot. Furthermore,\nit is necessary to get a strong understanding of the underlying\nstructure of sentences. Indeed, if we can have a method that\ncan tell us how the units of a sentence are correlated in\na phrase, the language understanding tasks would be more\nstraightforward.\nThe attention mechanism computes a relation mask between\nthe words of a sentence and uses this mask in an encoder-\ndecoder architecture to detect which words are related within\neach other. Using this process, the NLP tasks such as automatic\ntranslation are more ﬂexible because they can have access to\nthe dependencies of the sentence. In a translation context, it is\na genuine advantage. Another notable beneﬁt of the attention\nmechanism is the straightforward human-visualization of the\nmodel’s outcome.\nIII. D ATA S E T\nThe dominant strategy in the creation of deep learning sys-\ntems is to gather a corpus corresponding to a given problem.\nThe next step is to label this data and build a network that is\nsupposedly able to explain them. This method is not suitable\nif we want to create a more comprehensive system (i.e. a\nsystem that can solve multiple problems without a signiﬁcant\narchitecture change).\nThat is then essential to learn on heterogeneous data to\ncreate general NLP models. If we want systems that can\nresolve several tasks at the same time, it is necessary to\ntrain this model on a wide variety of subjects. Hopefully , in\nour ubiquitous data world, a large number of raw texts are\navailable online (e.g. Wikipedia, W eb blogs, Reddit).\nT able I shows the most commonly used datasets with their\nsize and the number of tokens they contain. The tokenization\nis done with SentencePiece [15]. In a few cases, for example,\nin [16], the authors only used a subset of those datasets (e.g.\nStories [17] is a subset of CommonCrawl dataset).\nIV . B E N C H M A R K S\nDuring an extended period, the deep learning models have\nbeen trained to resolve one problem at a time. Further, when\nthose models were used in another domain, they struggle to\nT ABLE I\nDATA S E T S C O M M O N LY U S E D W I T H TR A N S F O R M E R-BA S E D M O D E L S . ( †:\nTO K E N I Z AT I O N D O N E W I T H SE N T E N C EPI E C E, ‡: U N C O M P R E S S E D DATA )\nDataset Size Number of tokens †\nBookCorpus [18]\nplus English Wikipedia 13GB 3.87B\nGiga5 [19] 16GB 4.75B\nClueW eb09 [20] 19GB 4.3B\nOpenW ebT ext [21] 38GB -\nReal-News [22] 120GB ‡ -\ngeneralize correctly . That is the idea that promotes the creation\nof GLUE, SQuAD V1.1/V2.0 and RACE to have benchmarks\nable to check the reliability of models on various tasks.\nGLUE: The General Language Understanding Evaluation\n(GLUE) [23] is a collection of nine tasks created to test the\ngeneralization of modern NLP models. It reviews a wide range\nof NLP problems like Sentiment Analysis, Question Answer-\ning and inference tasks. Because of the rapid improvement\nof the state-of-the-art on GLUE, SuperGLUE [24] is a new\nproposed benchmark to check general language systems but\nwith more complicated more laborious tasks.\nSQuAD: Stanford Question Answering Dataset (SQuAD)\nV1.1 [25] is a benchmark designed to resolve Reading Com-\nprehension (RC) challenges. There are more than 100,000+\nquestions in the data set. There is no proposed answer like\nin the other RD datasets. The task contains a document, and\nthe model has to ﬁnd the answer directly in the text passage.\nSQuAD v2.0 [26] is based on the same principle than the V1.1,\nbut this time the answer is not necessarily in the questions.\nRACE: Reading Comprehension From Examinations\n(RACE) [27] is a collection of English questions set aside\nto Chinese students from middle school up to high school.\nEach item is divided into two parts, a passage that the student\nmust read and a set of 4 potential answers. Considering\nthat the questions are intended to teenagers, it requires keen\nreasoning skills to answer correctly to most of the problems.\nThe reasoning subjects present in RACE cover almost all\nhuman knowledge.\nV . T R A N S F O R M E R S\nThe RNNs (LSTM, GRU) have a recurrent underlying\nstructure and are, by deﬁnition recurrent. It is then hard to\nparallelize the learning process because of this fundamental\nproperty . T o overcome this issue, [7] proposed a new archi-\ntecture solely based on the attention layers; the Transformer.\nIt has the advantage to catch the long-range dependencies of\na sentence and to be parallelizable.\nA. Transformer architecture\nThe Transformer is based on an encoder-decoder structure,\nwhere it takes a sequence X = ( x1, ..., xN ) and produce\na latent representation Z = ( z1, ..., zN ). Due to the auto-\nregressive property of this model, the output sequence YM =\n(y1, ..., yM ) is produced one element at a time. i.e. the\n180 PROCEEDINGS OF THE FEDCSIS. SOFIA, 2020\nword YM used the latent representation Z and the previously\ncreated sequence YM− 1 = ( y1, ..., yM− 1) to be generated.\nThe Encoder and the Decoder are using the same Multi-Head\nAttention layer. A single Attention layer maps a query Q and\nkeys K to a weighted sum of the values V . For technical\nreason there is a scaling factor 1\n√ dk\n.\nAttention(Q, K, V) = Softmax (QKT\n√ dk\n)V\nB. Auto-Regressive Models\nThe auto-regressive models take the previous outputs to\nproduce the next outcome. It has the particularity to be a\nunidirectional network; it can only reach the left context\nof the evaluated token. However, despite this ﬂaw , it can\nlearn accurate sentence representations. It relies on the regular\nLanguage Modeling (LM) task as an unsupervised pre-training\nobjective:\nL(X) =\n∑\ni\nlog P (xi|xi− k, ..., xi− 1; Θ)\nThis LM function maximizes the likelihood of the condi-\ntional probability P . Where X is the input sequence, k is\nthe context window , and Θ are the parameters of the Neural\nNetwork.\nV arious models are using this property coupled with the\nTransformer architecture to produce accurate Language Model\nlanguages (i.e. it determines the statistical distribution of\nthe learned texts). The ﬁrst auto-regressive model using the\nTransformer architecture is GPT [8]. It has a pre-training\nLanguage Modeling phase where it learns on raw texts. In\nthe second learning phase, it uses supervised ﬁne-tuning to\nadjust the network to the downstream tasks.\nGPT -2 [14] uses the same pre-training principles than GPT .\nThough, this time it tries to achieve the same results in a\nzero-shot fashion (i.e. without ﬁne-tuning the network to the\ndownstream tasks). T o accomplish that goal, it must capture\nthe full complexity of textual data. T o do so, it needs a wider\nsystem with more parameters. The results of this model are\ncompetitive to some other supervised tasks on a few subjects\n(e.g. reading comprehension) but are far from being usable on\nother jobs such as summarization.\nAnother auto-regressive network is XLNet [28]. It aims\nto use the strength of the language modeling of the auto-\nregressive model and at the same time, use the bidirectionality\nof BER T [9]. T o do so, it relies on transformer-XL [29], the\nstate-of-the-art model for the auto-regressive network.\nC. BERT\nGPT and GPT -2 use a unidirectional language model; they\ncan only reach the left context of the evaluated token. That\nproperty can harm the overall performance of those models\nin reasoning or question answering tasks. Because, in those\ntopics, both sides of the sentence are crucial to getting an\noptimal sentence-level understanding.\nT o counter this unidirectional constraint, [9] introduced\nthe Bidirectional Encoder Representations from Transformers\n(BER T). This model can fuse the left and the right context of\na sentence, providing a bidirectional representation and allow\na better context extractor for reasoning tasks. The architecture\nof BER T is based on the Multi-Head Attention layers encoder\nlike proposed in [7]. Originally [9] proposed two versions of\nBER T , the base version with 110M of parameters and the large\nversion with 340M parameters.\nLike GPT and GPT -2, BER T has an unsupervised pre-\ntraining phase where it learns its language representation.\nNevertheless, due to its inherent bidirectional architecture,\nit cannot be trained using the standard Language Model\nobjective. Indeed, the bidirectionality of BER T allows each\nword to see itself, and therefore it can trivially predict the next\ntoken. T o overcome this issue and pre-train their model, [9]\nuse two unsupervised objective tasks: the Masked Language\nModel (MLM) and the Next Sentence Prediction (NSP).\nOnce the pre-training phase is over, it remains to ﬁne-tune\nthe model to the downstream tasks. Thanks to BER T’s Trans-\nformer architecture, the downstream can be straightforwardly\ndone because the same structure is used for the pre-training\nand the ﬁne-tuning. It merely needs to change the ﬁnal layer\nto match the requirements of the downstream task.\nVI. P O S T-BERT\nDue to the high performance of BER T on 11 NLP tasks, a\nlot of researchers inspired by BER T’s architecture applied it\nand tweaked it to their needs [30], [31].\nA. BERT improvement\nFurther, studies have been done to improve the pre-training\nphase of BER T . The post-BER T model RoBER T a [16] pro-\nposes three simple modiﬁcations of the training procedure. (I)\nBased on their empirical results, [16] shows that BER T is un-\ndertrained. T o alleviate this problem, they propose to increase\nthe length of the pre-training phase. By learning longer, the\noutcomes are more accurate. (II) As the results of [32] and\n[14] demonstrate, the accuracy of the end-task performance\nrelies on the wide variety of trained data. Therefore, BER T\nmust be trained on larger datasets. (III) In order to improve\nthe optimization of the model, they propose to increase the\nbatch size. There are two advantages to have a bigger batch\nsize; First, the large batch size is easier to parallelize, and\nsecond, it increases the perplexity of the MLM objective.\nB. Model reduction\nSince the Transformer’s revolution, state-of-the-art networks\nhave become bigger and bigger. Accordingly , to have a better\nlanguage representation and better end-task results, the models\nmust grow to catch the high complexity of texts. This ex-\npansion of the network’s size has a high computational cost.\nMore powerful GPUs and TPUs are required to train those\nlarge models. If we take, for example, the Nvidia’s GPT -8B 1\nwith 8 billion parameters, it became infeasible for small tech\ncompanies or small labs to train a network as huge as that.\n1 https://nv-adlr.github.io/MegatronLM\nANTHONY GILLIOZ ET AL.: OVER VIEW OF THE TRANSFORMER-BASED MODELS FOR NLP T ASKS 181\nIt is then necessary to ﬁnd smaller systems that maintain the\nhigh performances of the bigger ones.\nW orking with smaller models has multiple advantages. If\nthe model size is shrunk, it trains faster, and the inference\ntime will also be reduced. If it is small enough, it can be run\non smartphones or IoT devices in real-time.\nOne technique introduced to reduce the size of those big\nnetworks is the knowledge distillation. It is a compression\nmethod that consists of a small network (student) trained to\nreproduce the behaviour of a bigger version of itself (teacher).\nThe teacher is primarily trained as a regular network, and after\nthat, it is distilled to reduce its size. DistilBER T [33] is a\ndistilled version of BER T that reduces the number of layers by\na factor of 2. It retains 97% of BER T on the GLUE benchmark\nwhile being 40% smaller and 60% faster at the inference time.\nAnother way to reduce the size of BER T is by changing\nthe architecture itself. AlBER T [34] proposes two ideas to\ndecrease the number of parameters. The ﬁrst approach factor-\nizes the embedding of the parameters. It separates the large\nvocabulary embedding matrix into two smaller matrices. The\nsize of the hidden layer is separated from the size of the\nvocabulary representation. The second method is a cross-layer\nparameter sharing. This technique prevents the parameters\nfrom growing with the depth of the network. With those two\ntricks, it allows reducing the size of the large BER T version\nby 18% without a loss of performance. Since this architecture\nis smaller, the training time is also faster.\nC. Multitask Learning\nBER T learns several tasks sequentially and increases the\noverall performance of the downstream end-tasks. The main is-\nsue with the continual pre-training method is that it must learn\nefﬁciently and quickly newly introduced sub-tasks, and it must\nremember what has been learned previously . The Multi-task\nLearning (MTL) principle is based on human consideration.\nIf you learn how to do a ﬁrst task, then a second related task\nis going to be more accessible to master. There are two main\ntrends in MTL.\nThe ﬁrst one uses an MTL scheme during the ﬁne-tuning\nphase. MT -DNN [35] based on the backbone of BER T is using\nthe same pre-training procedure, but during the ﬁne-tuning\nstep, it uses four multi-tasks. Training on all the GLUE tasks at\nthe same time makes it gain an efﬁcient generalization ability .\nOn the opposite [10] proposes an MTL process directly\nduring the pre-training step; ERNIE 2.0 introduces a continual\npre-training framework. More speciﬁcally , it uses a Sequential\nMulti-task Learning where it begins to learn a ﬁrst task.\nWhen this ﬁrst task is mastered, a new task is introduced\nin the continual learning process. The previously optimized\nparameters are used to initiate the model, the new task and\nthe previous tasks are trained concurrently . There are three\ngroups of pre-training tasks, and each of them aims to capture\na different level of semantic:\nW ord-A ware T asks: It captures the lexical information of\nthe text: the Knowledge Masking T ask (i.e. it masks phrases\nand entities), the Capitalization prediction (i.e. it predicts if a\nword has a capitalized ﬁrst letter), and the T oken-Document\nRelation Prediction T ask (i.e. it predicts if a token of a sentence\nbelongs to a document where the sentence initially appears).\nStructure-A ware T asks: It learns the relationship between\nsentences: sentence reordering task (i.e. split and shufﬂe a\nsentence and must ﬁnd the correct order), sentence distance\ntask (i.e. it must ﬁnd if two sentences are adjacent, belong to\nthe same document or if they are entirely unrelated).\nSemantic-A ware T asks: It learns a higher order of knowl-\nedge: discourse relation task (i.e. it predicts the semantic or\nrhetorical relation of sentences), IR relevance task (i.e. ﬁnd\nthe relevance of information retrieval in texts).\nD. Speciﬁc language models\nIn order to tackle speciﬁc languages problems, different\nmonolingual versions of BER T were trained in different\nlanguages. For example BER Tje [36] is a Dutch version,\nAlBER T o [37] is an Italian version, and CamemBER T [38]\nand FlauBER T [39] are two different models for French.\nThese models outperform vanilla BER T in different NLP tasks\nspeciﬁc to these languages.\nE. Cross-language model\nXLM [40] aims to build a universal cross-language sentence\nembedding. The goal is to align sentence representations\nto improve the translation between languages. T o do so, a\nTransformer architecture with two unsupervised tasks and\none supervised is used. The effectiveness of cross-language\npre-training in order to improve the multilingual machine\ntranslation is shown.\nVII. G O I N G FU RT H E R\nDespite the excellent performances of the Transformer ar-\nchitecture, new layers aiming to improve the performance and\nthe complexity have been released.\nThe Transformer uses a gradient-based optimization proce-\ndure. Thus, it needs to save the activation value of all the\nneurons to be used during the back-propagation. Because of\nthe massive size of the Transformer models, the GPU/TPU’s\nmemory is rapidly saturated. The Reformer [41] counter the\nmemory problem of the Transformer by recomputing the input\nof each layer during the back-propagation instead of storing\nthe information. The Reformer can also reduce the number\nof operations during the forward pass by computing a hash\nfunction that pairs similar inputs together. Like that, it does not\ncompute all pairs of vectors to ﬁnd the related ones. Therefore,\nit increases the size of the text it can treat at once.\nAnother way to improve the architecture of a network\nis by using an evolving algorithm as proposed by [42]. T o\ncreate a new architecture designed automatically , they evolve\na population of Transformers based on their accuracy . Using\nthe Progressive Dynamic Hurdles (PDH), they could reduce\nthe search space and the training time. With this technique\nand an extensive amount of computational power (around 200\nTPUs), they could ﬁnd a new architecture that outperforms the\nprevious one.\n182 PROCEEDINGS OF THE FEDCSIS. SOFIA, 2020\nVIII. C O N C L U S I O N\nThe Transformer-based networks have pushed the\nreasoning-skills to human-level abilities. It can even excel the\nhuman capabilities on a few tasks of GLUE. Transformer-\nbased networks have changed the face of NLP tasks. They\ncan go far beyond the results obtained with RNNs, and they\ncan do it faster. They have helped solve many problems at the\nsame time by providing a direct and efﬁcient way to combine\nseveral downstream tasks. Nevertheless, much work remains\nbefore having a system with a human-level comprehension\nof the underlying meaning of texts, that is also sufﬁciently\nsmall to run on devices with low computational power.\nRE F E R E N C E S\n[1] F . J. Och and H. Ney , “The Alignment T emplate Approach to Statistical\nMachine Translation, ” Computational Linguistics , vol. 30, pp. 417–449,\nDec. 2004.\n[2] A. M. Rush, S. Chopra, and J. W eston, “ A Neural Attention Model\nfor Abstractive Sentence Summarization, ” arXiv:1509.00685 [cs] , Sept.\n2015. arXiv: 1509.00685.\n[3] L. Y u, W . Zhang, J. W ang, and Y . Y u, “SeqGAN: Sequence Generative\nAdversarial Nets with Policy Gradient, ” arXiv:1609.05473 [cs] , Aug.\n2017. arXiv: 1609.05473.\n[4] S. Hochreiter and J. Schmidhuber, “Long Short-T erm Memory, ” Neural\nComputation, vol. 9, pp. 1735–1780, Nov . 1997.\n[5] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F . Bougares,\nH. Schwenk, and Y . Bengio, “Learning Phrase Representations\nusing RNN Encoder-Decoder for Statistical Machine Translation, ”\narXiv:1406.1078 [cs, stat] , Sept. 2014. arXiv: 1406.1078.\n[6] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmid-\nhuber, “LSTM: A Search Space Odyssey, ” IEEE Transactions on Neural\nNetworks and Learning Systems , vol. 28, pp. 2222–2232, Oct. 2017.\narXiv: 1503.04069.\n[7] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “ Attention Is All Y ou Need, ”\narXiv:1706.03762 [cs] , Dec. 2017. arXiv: 1706.03762.\n[8] A. Radford, K. Narasimhan, T . Salimans, and I. Sutskever, “Improving\nLanguage Understanding by Generative Pre-Training, ” p. 12, Nov . 2018.\n[9] J. Devlin, M.-W . Chang, K. Lee, and K. T outanova, “BER T: Pre-\ntraining of Deep Bidirectional Transformers for Language Understand-\ning, ” arXiv:1810.04805 [cs] , May 2019. arXiv: 1810.04805.\n[10] Y . Sun, S. W ang, Y . Li, S. Feng, H. Tian, H. Wu, and H. W ang, “ERNIE\n2.0: A Continual Pre-training Framework for Language Understanding, ”\narXiv:1907.12412 [cs] , Nov . 2019. arXiv: 1907.12412.\n[11] T . Mikolov , K. Chen, G. Corrado, and J. Dean, “Efﬁcient Estimation\nof W ord Representations in V ector Space, ” arXiv:1301.3781 [cs] , Sept.\n2013. arXiv: 1301.3781.\n[12] J. Pennington, R. Socher, and C. Manning, “Glove: Global V ectors\nfor W ord Representation, ” in Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , (Doha,\nQatar), pp. 1532–1543, Association for Computational Linguistics, 2014.\n[13] R. Sennrich, B. Haddow , and A. Birch, “Neural Machine Translation of\nRare W ords with Subword Units, ” arXiv:1508.07909 [cs] , June 2016.\narXiv: 1508.07909.\n[14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language Models are Unsupervised Multitask Learners, ” p. 24, Nov .\n2019.\n[15] T . Kudo and J. Richardson, “SentencePiece: A simple and language inde-\npendent subword tokenizer and detokenizer for Neural T ext Processing, ”\narXiv:1808.06226 [cs] , Aug. 2018. arXiv: 1808.06226.\n[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy , M. Lewis,\nL. Zettlemoyer, and V . Stoyanov , “RoBER T a: A Robustly Optimized\nBER T Pretraining Approach, ” arXiv:1907.11692 [cs] , July 2019. arXiv:\n1907.11692 version: 1.\n[17] T . H. Trinh and Q. V . Le, “ A Simple Method for Commonsense\nReasoning, ” arXiv:1806.02847 [cs] , Sept. 2019. arXiv: 1806.02847.\n[18] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov , R. Urtasun, A. T or-\nralba, and S. Fidler, “ Aligning Books and Movies: T owards Story-\nlike V isual Explanations by W atching Movies and Reading Books, ”\narXiv:1506.06724 [cs] , June 2015. arXiv: 1506.06724.\n[19] R. Parker, D. Graff, and J. Kong, “English gigaword, ” Linguistic Data\nConsortium, Jan. 2011.\n[20] J. Callan, M. Hoy , C. Y oo, and L. Zhao, “The ClueW eb09 Dataset -\nDataset Information and Sample Files, ” Jan. 2009.\n[21] A. Gokaslan and V . Cohen, OpenW ebT ext Corpus. Jan. 2019.\n[22] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F . Roesner,\nand Y . Choi, “Defending Against Neural Fake News, ” arXiv:1905.12616\n[cs], Oct. 2019. arXiv: 1905.12616.\n[23] A. W ang, A. Singh, J. Michael, F . Hill, O. Levy , and S. R. Bowman,\n“GLUE: A Multi-T ask Benchmark and Analysis Platform for Natural\nLanguage Understanding, ” arXiv:1804.07461 [cs] , Feb. 2019. arXiv:\n1804.07461.\n[24] A. W ang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F . Hill,\nO. Levy , and S. R. Bowman, “SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems, ” arXiv:1905.00537\n[cs], July 2019. arXiv: 1905.00537.\n[25] P . Rajpurkar, J. Zhang, K. Lopyrev , and P . Liang, “SQuAD: 100,000+\nQuestions for Machine Comprehension of T ext, ” arXiv:1606.05250 [cs] ,\nOct. 2016. arXiv: 1606.05250.\n[26] P . Rajpurkar, R. Jia, and P . Liang, “Know What Y ou Don’t Know:\nUnanswerable Questions for SQuAD, ” arXiv:1806.03822 [cs] , June\n2018. arXiv: 1806.03822.\n[27] G. Lai, Q. Xie, H. Liu, Y . Y ang, and E. Hovy , “RACE:\nLarge-scale ReAding Comprehension Dataset From Examinations, ”\narXiv:1704.04683 [cs] , Dec. 2017. arXiv: 1704.04683.\n[28] Z. Y ang, Z. Dai, Y . Y ang, J. Carbonell, R. Salakhutdinov , and Q. V . Le,\n“XLNet: Generalized Autoregressive Pretraining for Language Under-\nstanding, ” arXiv:1906.08237 [cs] , June 2019. arXiv: 1906.08237.\n[29] Z. Dai, Z. Y ang, Y . Y ang, J. Carbonell, Q. V . Le, and R. Salakhutdinov ,\n“Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext, ” arXiv:1901.02860 [cs, stat] , June 2019. arXiv: 1901.02860.\n[30] C. Sun, A. Myers, C. V ondrick, K. Murphy , and C. Schmid,\n“V ideoBER T: A Joint Model for V ideo and Language Representation\nLearning, ” arXiv:1904.01766 [cs] , Sept. 2019. arXiv: 1904.01766.\n[31] A. W ang and K. Cho, “BER T has a Mouth, and It Must Speak: BER T\nas a Markov Random Field Language Model, ” arXiv:1902.04094 [cs] ,\nApr. 2019. arXiv: 1902.04094 version: 2.\n[32] A. Baevski, S. Edunov , Y . Liu, L. Zettlemoyer, and M. Auli, “Cloze-\ndriven Pretraining of Self-attention Networks, ” arXiv:1903.07785 [cs] ,\nMar. 2019. arXiv: 1903.07785.\n[33] V . Sanh, L. Debut, J. Chaumond, and T . W olf, “DistilBER T, a distilled\nversion of BER T: smaller, faster, cheaper and lighter, ” arXiv:1910.01108\n[cs], Oct. 2019. arXiv: 1910.01108.\n[34] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Soricut,\n“ ALBER T: A Lite BER T for Self-supervised Learning of Language\nRepresentations, ” arXiv:1909.11942 [cs] , Oct. 2019. arXiv: 1909.11942\nversion: 3.\n[35] X. Liu, P . He, W . Chen, and J. Gao, “Multi-T ask Deep Neural Networks\nfor Natural Language Understanding, ” arXiv:1901.11504 [cs] , May\n2019. arXiv: 1901.11504.\n[36] W . de Vries, A. van Cranenburgh, A. Bisazza, T . Caselli, G. van Noord,\nand M. Nissim, “BER Tje: A Dutch BER T Model, ” arXiv:1912.09582\n[cs], Dec. 2019. arXiv: 1912.09582.\n[37] M. Polignano, P . Basile, and M. de Gemmis, “ALBER TO: Italian BER T\nLanguage Understanding Model for NLP Challenging T asks Based on\nT weets, ” p. 6, 2019.\n[38] L. Martin, B. Muller, P . J. O. Suárez, Y . Dupont, L. Romary , E. V .\nde la Clergerie, D. Seddah, and B. Sagot, “CamemBER T: a T asty\nFrench Language Model, ” arXiv:1911.03894 [cs] , May 2020. arXiv:\n1911.03894.\n[39] H. Le, L. V ial, J. Frej, V . Segonne, M. Coavoux, B. Lecouteux,\nA. Allauzen, B. Crabbé, L. Besacier, and D. Schwab, “FlauBER T: Un-\nsupervised Language Model Pre-training for French, ” arXiv:1912.05372\n[cs], Mar. 2020. arXiv: 1912.05372.\n[40] G. Lample and A. Conneau, “Cross-lingual Language Model Pretrain-\ning, ” arXiv:1901.07291 [cs] , Jan. 2019. arXiv: 1901.07291.\n[41] N. Kitaev , L. Kaiser, and A. Levskaya, “Reformer: The Efﬁcient Trans-\nformer, ” arXiv:2001.04451 [cs, stat] , Jan. 2020. arXiv: 2001.04451.\n[42] D. R. So, C. Liang, and Q. V . Le, “The Evolved Transformer, ”\narXiv:1901.11117 [cs, stat] , May 2019. arXiv: 1901.11117.\nANTHONY GILLIOZ ET AL.: OVER VIEW OF THE TRANSFORMER-BASED MODELS FOR NLP T ASKS 183",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8052619695663452
    },
    {
      "name": "Architecture",
      "score": 0.7859629392623901
    },
    {
      "name": "Computer science",
      "score": 0.7184890508651733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5653716325759888
    },
    {
      "name": "Encoder",
      "score": 0.5508796572685242
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5000646114349365
    },
    {
      "name": "Language model",
      "score": 0.44594162702560425
    },
    {
      "name": "Artificial neural network",
      "score": 0.4335380792617798
    },
    {
      "name": "Natural language processing",
      "score": 0.3530399799346924
    },
    {
      "name": "Machine learning",
      "score": 0.32301485538482666
    },
    {
      "name": "Engineering",
      "score": 0.15746766328811646
    },
    {
      "name": "Voltage",
      "score": 0.08472108840942383
    },
    {
      "name": "History",
      "score": 0.07046973705291748
    },
    {
      "name": "Electrical engineering",
      "score": 0.06281676888465881
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57825437",
      "name": "University of Neuchâtel",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210109379",
      "name": "HES-SO Fribourg",
      "country": "CH"
    }
  ]
}