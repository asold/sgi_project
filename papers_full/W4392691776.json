{
  "title": "Assessing the research landscape and clinical utility of large language models: a scoping review",
  "url": "https://openalex.org/W4392691776",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4220085055",
      "name": "Ye-Jean Park",
      "affiliations": [
        "Canada Research Chairs"
      ]
    },
    {
      "id": "https://openalex.org/A5114081765",
      "name": "Abhinav Pillai",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2469496886",
      "name": "Deng Jiawen",
      "affiliations": [
        "Canada Research Chairs"
      ]
    },
    {
      "id": "https://openalex.org/A4208650056",
      "name": "Eddie Guo",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2897392305",
      "name": "Mehul Gupta",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2103512746",
      "name": "Mike Paget",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A260375131",
      "name": "Christopher Naugler",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A4220085055",
      "name": "Ye-Jean Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114081765",
      "name": "Abhinav Pillai",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2469496886",
      "name": "Deng Jiawen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208650056",
      "name": "Eddie Guo",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2897392305",
      "name": "Mehul Gupta",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2103512746",
      "name": "Mike Paget",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A260375131",
      "name": "Christopher Naugler",
      "affiliations": [
        "University of Calgary"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4318765555",
    "https://openalex.org/W4312118998",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4321499561",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W4317840265",
    "https://openalex.org/W2891378911",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4323830259",
    "https://openalex.org/W4205865577",
    "https://openalex.org/W4322618218",
    "https://openalex.org/W4361000349",
    "https://openalex.org/W4377010595",
    "https://openalex.org/W4366447635",
    "https://openalex.org/W4362735179",
    "https://openalex.org/W4319023010",
    "https://openalex.org/W4323050341",
    "https://openalex.org/W4319350602",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W4322208207",
    "https://openalex.org/W4323038373",
    "https://openalex.org/W4323348223",
    "https://openalex.org/W4380995257",
    "https://openalex.org/W4365148488",
    "https://openalex.org/W4366580365",
    "https://openalex.org/W4366333677",
    "https://openalex.org/W4366462753",
    "https://openalex.org/W4362697937",
    "https://openalex.org/W4381190368",
    "https://openalex.org/W4381106920",
    "https://openalex.org/W4380290695",
    "https://openalex.org/W4365136288",
    "https://openalex.org/W4205258379",
    "https://openalex.org/W4320920036",
    "https://openalex.org/W4321605784",
    "https://openalex.org/W4318570652",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4385573947",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4210605645",
    "https://openalex.org/W4380201210",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W1968468936",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4389767559",
    "https://openalex.org/W4391779256",
    "https://openalex.org/W4226238381",
    "https://openalex.org/W4393021028",
    "https://openalex.org/W4327941377",
    "https://openalex.org/W4323031358",
    "https://openalex.org/W4386045865",
    "https://openalex.org/W4400937555",
    "https://openalex.org/W4385230595",
    "https://openalex.org/W4392598276",
    "https://openalex.org/W4389538377",
    "https://openalex.org/W4387968480",
    "https://openalex.org/W4387772689",
    "https://openalex.org/W4383059305"
  ],
  "abstract": "Abstract Importance Large language models (LLMs) like OpenAI’s ChatGPT are powerful generative systems that rapidly synthesize natural language responses. Research on LLMs has revealed their potential and pitfalls, especially in clinical settings. However, the evolving landscape of LLM research in medicine has left several gaps regarding their evaluation, application, and evidence base. Objective This scoping review aims to (1) summarize current research evidence on the accuracy and efficacy of LLMs in medical applications, (2) discuss the ethical, legal, logistical, and socioeconomic implications of LLM use in clinical settings, (3) explore barriers and facilitators to LLM implementation in healthcare, (4) propose a standardized evaluation framework for assessing LLMs’ clinical utility, and (5) identify evidence gaps and propose future research directions for LLMs in clinical applications. Evidence review We screened 4,036 records from MEDLINE, EMBASE, CINAHL, medRxiv, bioRxiv, and arXiv from January 2023 (inception of the search) to June 26, 2023 for English-language papers and analyzed findings from 55 worldwide studies. Quality of evidence was reported based on the Oxford Centre for Evidence-based Medicine recommendations. Findings Our results demonstrate that LLMs show promise in compiling patient notes, assisting patients in navigating the healthcare system, and to some extent, supporting clinical decision-making when combined with human oversight. However, their utilization is limited by biases in training data that may harm patients, the generation of inaccurate but convincing information, and ethical, legal, socioeconomic, and privacy concerns. We also identified a lack of standardized methods for evaluating LLMs’ effectiveness and feasibility. Conclusions and relevance This review thus highlights potential future directions and questions to address these limitations and to further explore LLMs’ potential in enhancing healthcare delivery.",
  "full_text": "RESEARCH Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The \nCreative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available \nin this article, unless otherwise stated in a credit line to the data.\nPark et al. BMC Medical Informatics and Decision Making           (2024) 24:72 \nhttps://doi.org/10.1186/s12911-024-02459-6\nBMC Medical Informatics \nand Decision Making\n†Abhinav Pillai and Jiawen Deng are contributed equally to this work.\n*Correspondence:\nYe-Jean Park\nyejean.park@mail.utoronto.ca\nFull list of author information is available at the end of the article\nAbstract\nImportance Large language models (LLMs) like OpenAI’s ChatGPT are powerful generative systems that rapidly \nsynthesize natural language responses. Research on LLMs has revealed their potential and pitfalls, especially in \nclinical settings. However, the evolving landscape of LLM research in medicine has left several gaps regarding their \nevaluation, application, and evidence base.\nObjective This scoping review aims to (1) summarize current research evidence on the accuracy and efficacy of \nLLMs in medical applications, (2) discuss the ethical, legal, logistical, and socioeconomic implications of LLM use in \nclinical settings, (3) explore barriers and facilitators to LLM implementation in healthcare, (4) propose a standardized \nevaluation framework for assessing LLMs’ clinical utility, and (5) identify evidence gaps and propose future research \ndirections for LLMs in clinical applications.\nEvidence review We screened 4,036 records from MEDLINE, EMBASE, CINAHL, medRxiv, bioRxiv, and arXiv from \nJanuary 2023 (inception of the search) to June 26, 2023 for English-language papers and analyzed findings from \n55 worldwide studies. Quality of evidence was reported based on the Oxford Centre for Evidence-based Medicine \nrecommendations.\nFindings Our results demonstrate that LLMs show promise in compiling patient notes, assisting patients in \nnavigating the healthcare system, and to some extent, supporting clinical decision-making when combined with \nhuman oversight. However, their utilization is limited by biases in training data that may harm patients, the generation \nof inaccurate but convincing information, and ethical, legal, socioeconomic, and privacy concerns. We also identified \na lack of standardized methods for evaluating LLMs’ effectiveness and feasibility.\nConclusions and relevance This review thus highlights potential future directions and questions to address these \nlimitations and to further explore LLMs’ potential in enhancing healthcare delivery.\nKeywords Large language models, ChatGPT, Natural language processing, Clinical settings, Scoping review\nAssessing the research landscape and clinical \nutility of large language models: a scoping \nreview\nYe-Jean Park1*, Abhinav Pillai2†, Jiawen Deng1†, Eddie Guo2, Mehul Gupta2, Mike Paget2 and Christopher Naugler2\nPage 2 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nIntroduction\nLarge language models (LLMs) are deep learning algo -\nrithms capable of interpreting and synthesizing vast vol -\numes of textual data. Using a large corpora of unlabelled \ntext combined with reinforcement training from human \nfeedback, LLMs can learn syntaxial patterns and contex -\ntual relationships in languages, enabling them to generate \nhuman-like responses to free-form inputs [1].\nA prominent example of an LLM is OpenAI’s Gen -\nerative Pre-training Transformer (GPT) model and its \npublic-facing interface ChatGPT [ 2]. Introduced in \nNovember 2022, ChatGPT was trained using a large \ncorpora of unlabelled text, including CommonCrawl, \nWebText, and Wikipedia, as well as internet-based book \ncorpora spanning multiple languages [ 3]. GPT, along \nwith other popular LLMs such as Google’s Pathways Lan-\nguage Model (PaLM), work by sequentially predicting \none-word fragments at a time until a complete response \nis formed. ChatGPT performs continual/incremental \nlearning, wherein the model can maintain a memory of \nprevious input and prompts to subsequently improve \nthe accuracy and relevance of its future responses with \neach iteration of text by strengthening its neural network \n[2–6].\nGiven their ability to rapidly summarize textual data \nand synthesize natural language responses, LLMs have \nfound diverse applications in a variety of settings, includ -\ning healthcare environments. Recent studies have high -\nlighted the potential of LLMs in clinical decision support, \noffering valuable guidance to healthcare teams to allow \nfor more informed treatment decisions [ 4]. Addition -\nally, LLMs may be used to improve speed and efficiency \nof performing administrative tasks such as composing \npatient charts and generating discharge notes, allowing \nhealthcare providers to focus more time and energy on \npatient care [5]. Outside of the immediate clinical setting, \nLLM interfaces such as ChatGPT can enable patients \nto ask health-related questions, potentially optimizing \nhealth resource utilization [ 6]. Despite these potential \nuse cases, the role of LLMs in healthcare may be limited \nby the presence of bias in its training materials, their ten -\ndency to “hallucinate” (i.e., generate factually incorrect \nstatements that sound sensible), and ethicolegal consid -\nerations when LLMs provide inaccurate recommenda -\ntions leading to patient harm as well as patient privacy \nconcerns [7–9].\nAt the time of our search in June 2023, there was a pau-\ncity of knowledge synthesis surrounding the evidence \nbase, application, and evaluation methods of research \non the clinical utilities of LLMs, thus we performed this \nscoping review. Evidently, there have been many updates \nand interesting studies that have been published since \nthis date; still, we seek to summarize and organize the \ninsights gleaned from our search to better frame previ -\nous and upcoming discussions around the use of LLMs \nfor clinical settings. Thus, the objectives of our scoping \nreview remain as:\n1. To summarize current research evidence \nsurrounding the accuracy and efficacy of LLMs in \nmedical applications.\n2. To discuss the ethicolegal, logistical, and \nsocioeconomic implications of the use of LLMs in \nclinical settings.\n3. To explore barriers and facilitators to LLM \nimplementation in healthcare settings.\n4. To propose a standardized evaluation framework \nfor assessing the clinical utility of LLMs for future \nresearch studies.\n5. To identify evidence gaps within the current \nliterature and propose future research directions for \nclinical applications of LLMs.\nKey points\nQuestion What is the current state of Large Language Models’ (LLMs) application in clinical settings, and what are \nthe primary challenges and opportunities associated with their integration?\nFindings This scoping review, analyzing 55 studies, indicates that while LLMs, including OpenAI’s ChatGPT, show \npotential in compiling patient notes, aiding in healthcare navigation, and supporting clinical decision-making, their \nuse is constrained by data biases, the generation of plausible but incorrect information, and various ethical and \nprivacy concerns. A significant variability in the rigor of studies, especially in evaluating LLM responses, calls for \nstandardized evaluation methods, including established metrics like ROUGE, METEOR, G-Eval, and MultiMedQA.\nMeaning The findings suggest a need for enhanced methodologies in LLM research, stressing the importance of \nintegrating real patient data and considering social determinants of health, to improve the applicability and safety \nof LLMs in clinical environments.\nPage 3 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nMethods\nThrough a scoping review methodology, we aimed to \nbroadly capture research methods, evidence, and objec -\ntives in relation to the clinical utility of LLMs. This \nscoping review was conducted in accordance with the \nPreferred Reporting Items for Systematic Reviews and \nMeta-Analyses for Scoping Review (PRISMA-ScR), and \nusing the Arksey and O’Malley framework [ 10]. The \ncompleted PRISMA-ScR checklist is included as Tables  1 \nand 2. The review protocol was prospectively developed \nand published on Open Science Framework. [Registra-\ntion 10.17605/OSF.IO/498K6].\nStudy identification\nA librarian-assisted database search was conducted in \nMEDLINE, EMBASE, and CINAHL, along with from \ninception to June 26, 2023 for English-language papers. \nThe search strategies are attached as Table  1, and search \nterms included LLM-related concepts such as “large lan -\nguage models” and “GPT” , as well as healthcare-related \nconcepts such as “clinic” or “hospitals” . The reference sec-\ntion of previous reviews, as well as grey literature sources \nsuch as medRxiv, bioRxiv and arXiv, were hand searched \nfor relevant publications.\nEligibility criteria\nWe included all publications that described the clinical \napplicability and usage of LLMs, including in inpatient, \noutpatient, and community settings. No restrictions were \napplied to the type of publications. Publications which \ndid not focus on the use of LLM in clinical settings (e.g., \nassessed LLM applicability in medical education) were \nspecifically excluded. Publications which focused solely \non the technical design, engineering, commercial, and \nmodel function of LLM development or validation were \nalso excluded.\nStudy selection\nThree reviewers (Y.-J.P ., A.P ., J.D.) performed title and \nabstract screening independently and in-duplicate. \nRecords deemed eligible for inclusion by at least two \nreviewers were subsequently retrieved and entered into \nan in-duplicate full-text screening process. Disagree -\nments were resolved via discussion to reach consensus. \nThe study selection process was completed using Covi -\ndence, and the study selection process is presented using \na PRISMA flow diagram in Fig. 1.\nData collection\nData extraction was performed independently and in-\nduplicate by three reviewers (Y.-J.P ., A.P ., J.D.) using \na standardized extraction sheet. Disagreements were \nresolved via discussion to reach consensus. All studies \nwere categorized based on whether their focus was in the \ncategory of: (1) LLMs’ utility in compiling patient notes, \n(2) their ethical, logistical, and legal contentions, (3) their \nutility in supporting patients navigating the healthcare \nsystem, and (4) their utility in clinical decision-making \nprocesses. Information collected in the data extraction \nform was summarized (Suppl. Tables 1–4) to determine \nthe main themes relating to LLM’s clinical applicabil -\nity among the currently published literature. To further \nmanage our references and generate citations, we used \nPaperpile (Cambridge, Massachusetts).\nQuality rating scheme for studies\nQuality of evidence was reported based on the Oxford \nCentre for Evidence-based Medicine recommendations \n[11]. Of note, many articles on LLMs in medicine were \nonly recently published within the past 1–2 years, and \nthere is currently a paucity in standardized methods to \nboth conduct research on and report LLMs’ - particularly \nGPT models’ - clinical applicability. Thus, several articles \nwere not rated specifically in correlation to the Oxford \nCentre for Evidence-based Medicine and have been \nreported as N/A for the time being (Suppl. Tables 1–4).\nResults\nThe search strategy resulted in 4,036 articles (Fig.  1). \nThere were 998 duplicates removed using automatic \nscreening by Covidence along with manual screen -\ning between data extractors (Y-J.P ., A.P ., J.D.), and 3,126 \ntitles and abstracts were screened. Quality and rigor of \nextractions were maintained via (i) standardization of \nextractions based on an example provided by consensus \namongst the authors, (ii) duplicate screenings and confir-\nmations of each extraction. Based on our criteria, 63 arti -\ncles were eligible for full-text screening, of which 8 were \nexcluded given that their topics did not align with the \ninclusion criteria. 55 articles were ultimately included.\nTable 1 Search strategy used for MEDLINE\n1. large language model*.mp. or LLM*.tw,kf.\n2. (ChatGPT or GPT or GPT-3 or GPT-4 or generative pre trained trans-\nformer* or generative pre-trained transformer*).tw,kf.\n3. capacity building/ or health communication/ or academic medical \ncenters/ or ambulatory care facilities/ or bed occupancy/ or health facil-\nity administration/ or health facility size/ or hospital units/ or hospitals, \ncommunity/ or hospitals, general/ or hospitals, group practice/ or hos-\npitals, high-volume/ or hospitals, low-volume/ or hospitals, private/ or \nhospitals, public/ or hospitals, rural/ or hospitals, satellite/ or hospitals, \nspecial/ or hospitals, teaching/ or hospitals, urban/ or mobile health \nunits/ or secondary care centers/ or tertiary care centers/ or exp health \npersonnel/ or exp health services/ or “health care economics and \norganizations”/ or health services administration/ or “health care quality, \naccess, and evaluation”/\n4. (clinic* or health* facilit*).tw,kf.\n5. Physicians/\n6. (1 or 2) and (3 or 4 or 5)\nPage 4 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nTable 2 PRISMA-ScR 2020 Flowchart for the identification and selection of relevant studies. Preferred reporting items for systematic \nreviews and meta-analyses extension for scoping reviews (PRISMA-ScR) checklist\nSection Item Prisma-ScR Checklist Item Report-\ned On \nPage #\nTitle\nTitle 1 Identify the report as a scoping review. 5\nAbstract\nStructured summary 2 Provide a structured summary that includes (as applicable): background, objectives, eligibility criteria, \nsources of evidence, charting methods, results, and conclusions that relate to the review questions and \nobjectives.\n2\nIntroduction\nRationale 3 Describe the rationale for the review in the context of what is already known. Explain why the review \nquestions/objectives lend themselves to a scoping review approach.\n3\nObjectives 4 Provide an explicit statement of the questions and objectives being addressed with reference to their key \nelements (e.g., population or participants, concepts, and context) or other relevant key elements used to \nconceptualize the review questions and/or objectives.\n4\nMethods\nProtocol and \nregistration\n5 Indicate whether a review protocol exists; state if and where it can be accessed (e.g., a Web address); and if \navailable, provide registration information, including the registration number.\n5\nEligibility criteria 6 Specify characteristics of the sources of evidence used as eligibility criteria (e.g., years considered, lan-\nguage, and publication status), and provide a rationale.\n5\nInformation sources* 7 Describe all information sources in the search (e.g., databases with dates of coverage and contact with \nauthors to identify additional sources), as well as the date the most recent search was executed.\n5\nSearch 8 Present the full electronic search strategy for at least 1 database, including any limits used, such that it \ncould be repeated.\n18\nSelection of sources of \nevidence†\n9 State the process for selecting sources of evidence (i.e., screening and eligibility) included in the scoping \nreview.\n4\nData charting process‡ 10 Describe the methods of charting data from the included sources of evidence (e.g., calibrated forms or \nforms that have been tested by the team before their use, and whether data charting was done indepen-\ndently or in duplicate) and any processes for obtaining and confirming data from investigators.\n5\nData items 11 List and define all variables for which data were sought and any assumptions and simplifications made. N/A\nCritical appraisal of \nindividual sources of \nevidence§\n12 If done, provide a rationale for conducting a critical appraisal of included sources of evidence; describe \nthe methods used and how this information was used in any data synthesis (if appropriate).\n5\nSynthesis of results 13 Describe the methods of handling and summarizing the data that were charted. 5\nResults\nSelection of sources of \nevidence\n14 Give numbers of sources of evidence screened, assessed for eligibility, and included in the review, with \nreasons for exclusions at each stage, ideally using a flow diagram.\n5\nCharacteristics of sourc-\nes of evidence\n15 For each source of evidence, present characteristics for which data were charted and provide the citations. 5–12\nCritical appraisal within \nsources of evidence\n16 If done, present data on critical appraisal of included sources of evidence (see item 12). 5–12\nResults of individual \nsources of evidence\n17 For each included source of evidence, present the relevant data that were charted that relate to the \nreview questions and objectives.\n5–12\nSynthesis of results 18 Summarize and/or present the charting results as they relate to the review questions and objectives. 5–12\nDiscussion\nSummary of evidence 19 Summarize the main results (including an overview of concepts, themes, and types of evidence available), \nlink to the review questions and objectives, and consider the relevance to key groups.\n12–17\nLimitations 20 Discuss the limitations of the scoping review process. 12–17\nConclusions 21 Provide a general interpretation of the results with respect to the review questions and objectives, as well \nas potential implications and/or next steps.\n12–17\nFunding\nFunding 22 Describe sources of funding for the included sources of evidence, as well as sources of funding for the \nscoping review. Describe the role of the funders of the scoping review.\n18\nPage 5 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nFrom our included studies, three were published in \n2022 and 52 were published in 2023 (Fig.  2). In Febru -\nary 2023, there was an increase in the number of articles \npublished on the topic of LLMs’ clinical utility ( n = 14), \nwith a focus on ChatGPT, and the highest number of \npublications was in March 2023 (n = 18).\nThe studies encompassed 15 research articles, 21 \npreprints (particularly given the recently emerging \ndata on this topic), 5 brief reports, 8 research letters, 4 \ncommentaries, and 2 case reports. Research articles \nprovided comprehensive investigations, preprints show -\ncased emerging research, brief reports offered concise \nsummaries, research letters facilitated timely exchanges, \ncommentaries provided expert insights, and case reports \nhighlighted practical applications of LLMs in clinical set -\ntings (Fig.  3). This diverse range of study types ensured \na comprehensive analysis of LLMs’ clinical utility. The \nstudies we analyzed further covered a wide range of \nFig. 2 Number of articles published over the timespan of January 2022 to June 2023\n \nFig. 1 Flowchart of the search strategy\n \nPage 6 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nmedical specialties, including Dermatology, ICU, Hepa -\ntology, Gastroenterology, Radiology, Urology, Otolar -\nyngology, Endocrinology, Plastic Surgery, Oncology, \nNeurosurgery, Cardiology, Ophthalmology, Orthopedic \nSurgery, Psychiatry, and General Medicine (Suppl. Table \n1). All studies were further in the quality assessment cat -\negories of 1, 5, or N/A (Suppl. Tables 1–4).\nThrough our data extraction, we explored our first \nobjective further regarding the clinical utility of LLMs, \nwith a particular focus on ChatGPT/GPT-3 and GPT-4. \nCommon themes from our extracted studies included:\nLLMs’ utility in compiling patient notes in the EMR system \nor in documenting case reports (Suppl. Table 1)\nSix articles, including a research letter, two brief reports, \na commentary, and two case reports, discussed the ben -\nefits of using LLMs like ChatGPT for creating patient \nand/or discharge notes as well as case reports [ 12–17]. \nThese benefits included improved efficiency and orga -\nnization, the potential for standardizing notes, and the \nidentification of grammar and/or lab value errors during \nextraction [ 12–17]. However, ChatGPT’s documenta -\ntion abilities were limited in accuracy based on medical \ncondition complexity [ 12]. For instance, Ali et al. found \nthat the accuracy of the generated patient letters differed \nsignificantly based on different types of skin cancer [ 12]. \nAdditionally, ChatGPT may “hallucinate” false references \nand can be easily influenced by the rhetoric and assump -\ntions of its users [ 15, 17]. Further research is needed to \nexplore integrating voice-to-text dictation software to \nChatGPT documentation workflows, evaluating Chat -\nGPT’s documentation ability in non-English languages, \nand whether it can be reconfigured to increase accuracy \nand enhance privacy [12–17].\nEthical, legal, socioeconomic, and logistical implications of \nLLMs in the clinical setting (suppl. Table 2)\nA total of two commentaries, two brief reports, and a \npreprint research article discussed the ethical, logistical, \nand legal implications of using LLMs in the clinical set -\nting [7, 18–21]. In particular, some of the major benefits \nin these realms included that ChatGPT and LLMs can be \nused to reduce human error and therefore reduce cases \nof medical litigations, provide an objective and evidence-\nbased approach to decision-making, and reduce the \nhealth gap between communities across varying socio -\neconomic and geographic backgrounds by improving \ncare provided in telemedicine [7, 18–21]. However, draw-\nbacks to LLMs, particularly ChatGPT, included that they \nmay give biased, extremist, and false information that can \nharm patients, cause data privacy issues (especially when \ndata is shared across institutions or is breached), and may \nbe challenging to determine who is liable ethically and \nmedically when advice taken from ChatGPT is used [ 7, \n18–21].\nLogistically, the absence of standardized methods \nfor evaluating the effectiveness, accuracy, and feasibil -\nity of using ChatGPT has led to variations in research \nand practical implementations [ 19]. To address these \nchallenges, authors recommended deploying GPT-3 as \nFig. 3 Types of included studies (n = 55). Preprints were the most common (n = 21) whereas case reports were the least common (n = 2)\n \nPage 7 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \na service connected to the internet, enabling it to stay \nupdated with adjustments in clinical guidelines [ 19]. \nThey also proposed integrating it as part of cloud-based \nhospital platforms to reduce its operating load [ 19]. Fur-\nthermore, Sezgin et al. suggested future research inves -\ntigate the use of standardized, automatic evaluation \nmethods including human experts and/or machine learn-\ning tools to judge the accuracy and readability of texts \nfrom LLMs [ 19]. Example metrics discussed include the \nBLEU (Bilingual Evaluation Understudy) score to evalu -\nate translated MI-generated text that is best used to \nevaluate whether an LLM translated text appropriately \nand METEOR (Metric for Evaluation of Translation with \nExplicit ORdering), which is a software package that can \nbe used to evaluate automatic summarization that com -\npares an automatically produced summary against a ref -\nerence produced by a human [ 19, 22]. Both metrics are \nparticularly helpful for situations where LLMs are used \nto assist with translations between providers and patients \nto overcome linguistic barriers. Haupt & Marks fur -\nther mentioned that discussion is needed around which \nareas of care GPT models will be used [ 21]. Otherwise, \nwhen GPT is relied upon as the de facto, primary source \nof clinical advice or as a determinant for prioritizing \npatients, especially in fields like mental health care, legal \nuncertainties can arise.\nLLMs’ utility in supporting patients in navigating the \nhealthcare system (suppl. Table 3)\nIn regards to the ways LLMs can empower patients to \nnavigate the healthcare system, two extracted stud -\nies were research articles, two were pre-prints, and two \nwere research letters [ 23–28]. Reported benefits of using \nLLMs like ChatGPT for communication with patients \nincluded providing quick, readable (i.e. approximately \n8th-grade level, according to Ali et al. [ 26]), and often -\ntimes accurate instructions and analysis of test results to \npatients and their providers [ 23–28]. These advantages \nwere often attributed to ChatGPT’s ability to use active \nvoice, replace medical jargon, and distill large amounts of \ntext and information into its key messages [23–28].\nHowever, reported problems from using ChatGPT \nincluded that its responses were not always comprehen -\nsive or even reproducible, ChatGPT could not provide \ntailored recommendations based on clinical manage -\nment guidelines, and sometimes, central information was \nexcluded or over-simplified [ 23–28]. Since March 2023 \nfrom the release of GPT-4, these drawbacks have been \nlargely mitigated but not completely eliminated [ 26]. \nThus, recommendations from studies to improve LLMs \nin bridging patients to the healthcare system include fur -\nther research with open-source LLMs to have transpar -\nent access to the LLMs’ code and for instance, program \nLLMs to seek further clarification from their users to \nultimately fine-tune their responses and improve accu -\nracy; to also provide clear guidelines and examples of \noptimal responses to LLMs so that they may base their \nresponses off pre-established templates via few-shot \nlearning; investigate whether the potential benefits of \nLLMs to empower patients are transferable to other fields \nand improve client-provider communication (such as in \nlaw and business); and lastly, involve all pertinent stake -\nholders in elucidating specific quality control measures \nbefore LLMs are distributed to patients in clinics [23–28].\nOverall, studies emphasized how the use of LLMs \nfor patients is a particularly promising area of further \nresearch, as the use of LLMs to improve patient-provider \ncommunication is likely (i) more feasible to create safety \nmeasures for, and (ii) more realistically able to be imple -\nmented as chatbot features for patients who wish to gain \na tailored, baseline knowledge of any questions they may \nhave regarding their health before meeting with their \ncare team [ 23, 25]. On the other hand, it will be more \nchallenging for legal, logistical, and ethical reasons to \nincorporate LLMs in clinical settings if they have a direct \nimpact on the diagnosis and management of patients \n[23].\nLLMs’ utility in clinical decision-making processes (i.e. \nmanagement, risk prediction, diagnostic support) (suppl. \nTable 4)\nThis category for the use of LLMs had the highest num -\nber of studies published and extracted. We reviewed 13 \nresearch articles, 18 pre-prints, 1 commentary, 1 brief \nreport, and 5 research letters. Other LLMs besides Chat -\nGPT (i.e. BioBERT, GatorTron, Foresight, BART) were \nalso discussed in this category [ 4, 20, 29–63]. Com -\nmon conclusions on the benefits of integrating LLMs \ninto clinical settings that were not previously discussed \nincluded: reduction of medical errors via lifting admin -\nistrative burdens and documentation for providers to \nfocus more time on providing quality care to patients, \nassisting with overcoming linguistic and cultural barri -\ners between patients and providers, maintaining an itera -\ntive connection between previous prompts and responses \nthrough LLMs’ memory function, providing clinical sup -\nport throughout an entire clinical situation rather than in \nonly one fragment of care (i.e. ChatGPT could be used \nfor helping triage, diagnose, predict length of stay, act \nas communication/linguistic aids for patients, provide \nreminders, analytics, and even potential management \nplans), improve telehealth consultations when integrated \nwith other applications (as greater need for telehealth \nwas demonstrated during the COVID-19 pandemic), and \neven confirm its own shortcomings in its analyses when \nprompted to do so [4, 20, 29–63].\nDespite these benefits, issues with incorporating LLMs \nlike ChatGPT in clinical decision-making processes that \nPage 8 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nwere uniquely discussed by extracted studies in this cat -\negory were inconsistent accuracy with complex and \nambiguous medical questions (e.g. one study showed \nChatGPT outright refused to provide a diagnosis in such \nsituations despite established standards of care [ 33], \nracial and cultural biases inherent in training datasets \nthat can cause patient harm, LLMs’ inabilities to account \nfor the complexities of body language, tone (including \nsarcasm and humour), family dynamics, patient priori -\nties, and the social determinants of health a patient may \nbe facing, faulty attribution of information sources, pos -\nsibility to further widen the global digital health gap and \nexacerbate health inequities, the frequent presence of dis-\nagreements between human reviewers when interpreting \nLLMs’ responses, and arbitrary but significant variation \nin management plans proposed by ChatGPT (e.g. Chat -\nGPT would propose that an uninsured patient with a spe-\ncific clinical presentation be sent to a community health \nclinic while it also proposed to send an insured patient \nwith the same presentation to emergency) [ 64]. Further-\nmore, ChatGPT was prone to “hallucinating” responses \nthat were not in line with its prompt, as well as engaging \nin “false mimicry” where ChatGPT would respond in a \nmanner that fit a user’s implicit assumptions rather than \nseeking further clarification from the user or providing \ncorrections/caveats to its responses [ 46]. Lastly, regard -\ning LLMs’ use in providing pharmacological suggestions, \nPerlis further identified that ChatGPT will often include \nan optimal medication choice in conjunction with con -\ntraindicated medications for patients (however, this limi -\ntation was addressed by requesting ChatGPT to clarify \nits line of reasoning) [46].\nArticles in this category uniquely proposed that future \nresearch on domain-specific LLMs attribute validity \nof source data based on the evidence pyramid, as well \nas to test LLMs’ abilities to integrate both text and pat -\ntern/image recognition together (e.g. interpreting an \nECG alongside information about the patient’s medical \nand social history based on their charts) [ 4, 20, 29–63]. \nAnother recommendation by Nori et al. was to push for \nthe creation of richer prompts via the use of ensemble \napproaches (combination of multiple models) and infor -\nmation retrieval tools, such as by allowing LLMs to \naccess the Internet [37]. Multiple studies further mention \nthat we can significantly improve LLMs’ utility in clin -\nics via training on medical corpora, integrating clinical-\nsupervised feedback during the fine-tuning process, and \ntesting LLMs’ capabilities using both standardized, care -\nfully crafted prompts alongside cases where there are \nmore complex, even confounding information present [7, \n22, 31–65]. Lastly, the authors also mentioned the poten-\ntial to integrate LLMs in traditionally underserved set -\ntings, as well as in helping to create public policies across \nmultiple medical specialties by involving those in other \ndisciplines [ 50, 52]. Such stakeholders would include \npolicymakers and government experts in evaluating the \npolicies proposed by LLMs [48].\nMethods of conducting and evaluating results of extracted \nstudies\nOverall, we had 36 primary articles (including pre-prints) \nthat discussed empirical evidence on the potential utility \nof LLMs in clinical settings. To evaluate how research -\ners (i) conducted their primary studies testing the clini -\ncal utility of LLMs, and (ii) evaluated their results, we \nextracted these three traits and found:\nFor primary articles that discussed LLMs’ utility in \ncompiling patient notes or case reports, all methods \nincluded asking ChatGPT carefully worded prompts such \nas: “Write a letter to a patient with a CHA2DS2-VASc \nscore of 3 at a 12-year-old level informing them that \nthey have an incompletely excised basal cell carcinoma… \nexplain the diagnosis [and recommended treatment]…\ngive the patient advice on stopping their warfarin pre-\nop as per the British Society of Hematology’s Guidelines \non “Peri-Operative Management of Anticoagulation and \nAntiplatelet Therapy, ” to asking ChatGPT more infor -\nmally open-ended questions such as, “What is metfor -\nmin… can anyone with type 2 diabetes take it?” (Suppl. \nTable 1) [15, 52]. Only several studies, such as Ali et al. ’s, \nincluded results from both a formal, carefully-worded \nprompt vs. results from a less formal, more conversa -\ntional prompt that may be more realistically seen in a \nbusy clinical setting [12–17]. Overall, all studies included \nhuman reviewers to evaluate characteristics such as the \naccuracy and humanness of ChatGPT’s responses, some -\ntimes in combination with online tools, to assess the \nreadability of text [4, 20, 29–63].\nPrimary articles that highlighted how LLMs can sup -\nport patients included prompting ChatGPT using ques -\ntions sourced from healthcare providers and patients \n(such as via social media posts) and subsequently evalu -\nated by specialists [ 63]. Examples included asking Chat -\nGPT questions such as, “I was recently diagnosed with \ncirrhosis, I am so stressed out and I don’t know how to \ncope with all this, what should I do?” or “Write a surgical \nconsent form for a patient who [will undergo] coronary \nartery bypass grafting for acute NSTEMI [ 24]. ” Likewise, \narticles that tested LLMs’ utility in clinical decision-mak -\ning processes used the same pipeline of brainstorming \nquestions, prompting LLMs to generate answers. Exam -\nples included asking ChatGPT, “How does the predicted \nrisk for a patient compare against the population? What \ndo the guidelines state about the drug the patient is tak -\ning [29]?”\nLastly, articles that described the ethical, logistical, and \nlegal implications of LLMs in medicine were mainly in \nthe form of commentaries and brief reports with more \nPage 9 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nflexible discussion on these topics [ 7, 18–21]. One pre-\nprint systematic review article further synthesized com -\nmon themes in relation to hesitancy to use LLMs for \nmedical purposes [7, 18–21].\nOverall, there was significant variability in the rigor \nin both prompting ChatGPT and in evaluating its \nresponses, along with what constitutes an “accurate” \nor “readable” response from LLMs (e.g. is a “readable” \nresponse one that can be understood by a 12-year-old, or \nrather the “average” American reading level? Is an “accu -\nrate” diagnosis by ChatGPT one where it lists the right \ndiagnosis for a case within its top 10, top 5, or top 1 dif -\nferential?) [ 7, 18–21]. Several authors in fact mention \nhow this ambiguity in the “correct” approach to inter -\npreting and scoring text-based responses makes research \nwith this tool challenging, especially when considering \ncomplex applications like in medicine [4, 20, 29–63].\nDiscussion\nIn addition to investigating the clinical utility of LLMs \n(objective 1), we overall suggest several future directions \nfor healthcare research utilizing LLM applications for the \nfollowing objectives:\nObjectives 2 and 3: How do we address the ethicolegal, \nlogistical, and socioeconomic implications and barriers to \nLLM implementation in healthcare settings? Who are key \nfacilitators to work with in this process?\nThe integration of LLMs in the clinical setting presents \nseveral ethical, legal, and socioeconomic implications \nthat warrant consideration. Firstly, privacy and data secu-\nrity are paramount concerns given the sensitive nature \nof patient information. Ensuring that LLMs comply with \ndata protection regulations and maintaining patient con -\nfidentiality is essential to further investigate and rein -\nforce [65, 66]. In this regard, the time required for LLMs \nto achieve compliance with privacy regulations, such as \nthe Health Insurance Portability and Accountability Act \n(HIPAA) in the United States or Personal Information \nProtection and Electronic Documents Act (PIPEDA) in \nCanada, will likely be lengthy. For instance, previous AI \nwith potential for use in medicine, like Amazon’s Alexa, \ntook five years to become HIPAA compliant, and Ama -\nzon eventually halted support for third-party HIPAA-\ncompliant software in 2022 in part due to rising costs \n[19]. Despite such barriers, two more recent examples \nof LLMs that are HIPAA compliant at this time include \nCompliantGPT and BastionGPT. These are both private \nversions of ChatGPT, specifically tailored for use by US \nhealthcare providers [ 67]. Thus, future research with \nsuch compliant but private models would be warranted \nto proceed with application testing of LLMs, particularly \nin rigorous randomized clinical trials, before wider-scale \nimplementation [ 68] address the concern of privacy, \nincreased data transparency (particularly via research \nwith open-sourced LLMs like Falcon 40B, where its data \nand code are open for public viewing and collaborative \nuse) should be encouraged in the research field.\nTo enhance both the privacy and robustness of LLMs \nin clinical settings, it may also be worthwhile to explore \nthe implementation of institution-specific EMR creation \nengines based on locally hosted LLMs. These localized \napplications would adhere to the privacy and data poli -\ncies of specific institutions while receiving constant sup -\nport from healthcare professionals, as with any hospital \ntechnology. Existing applications like Large Language \nModel Meta AI (LLaMa) and Falcon LLM provide con -\ntemporary examples for creating such localized hospital-\nbased GPT applications [ 69, 70]. However, we foresee a \nfew challenges of locally-run LLMs, such as potentially \ntheir suboptimal performance or the requirement for \nspecialized hardware, as seen with GPT-3 [ 71]. Future \nresearch could therefore focus on developing smaller and \nmore efficient LLMs that can run on everyday devices, \nsimilar to how Apple stores FaceID data locally on a chip \n[72].\nFurthermore, LLMs were found to generate inac -\ncurate or “hallucinated” information/citations [ 73]. To \naddress this, human-labeled data and domain-specific \nGPT models will be necessary to further test, including \nGoogle’s Med-PaLM along with other emerging LLMs \nlike perplexity.ai and Bing, which automatically provide \nreferences and links. Further research should also be \nconducted with the ChatGPT Plus version, which is con -\nnected to the internet, with explicit instruction to pro -\nvide real references and links to those references.\nOverall, recent guidelines made to address the logistical \nand ethical concerns of medical LLMs include the draft \nguidance issued by the FDA on predetermined change \ncontrol plans for artificial intelligence/machine learn -\ning models [ 74]. Such guidelines may ultimately serve \nas frameworks for other researchers to further investi -\ngate addressing safety and regulation concerns related to \nLLMs in healthcare. On this note, as Comrie et al. pro -\npose, it may be worthwhile to investigate whether LLMs \ncan be used to shape policies regarding their own regula -\ntion and integration, as well as the feasibility of hosting \nthem locally [48].\nBut who should be involved in these discussions in the \nfirst place? Multiple of our extracted articles highlighted \nthe importance of consulting various stakeholders for \nthe successful implementation of LLMs in healthcare \n[16, 20]. Key stakeholders include clinical informaticians, \ndevelopers, healthcare providers, policymakers, industry \nrepresentatives, and civil society as a whole [75].\nOne barrier to having such discussions, as well as the \nsuccessful implementation of LLMs into clinic spaces, \nwas recently described by Lambert et al. [ 76]. Their \nPage 10 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \narticle mentions that healthcare workers (HCWs) are \nhesitant to use AI in their practice due to several factors, \nincluding a justified concern of loss of work and/or pro -\nfessional autonomy, difficulty integrating AI like LLMs \ninto the already established clinical operations, and \npotential loss of patient interactions [ 76]. However, the \nauthors also report that improving training on using AI \nand involving all HCWs in the early stages of integration \nand creation of infrastructure to support the technology \ncan ensure that HCWs are constantly being supported \nand heard during the process of bringing LLMs to health-\ncare settings.\nObjective 4: What could be a standardized evaluation \nframework for assessing LLMs’ clinical utility for future \nresearch studies?\nThe evaluation of text-based generation from large lan -\nguage models (LLMs) poses significant challenges due to \nthe absence of a standardized evaluation framework. This \nis primarily because LLMs can produce multiple, variable \nresponses and are highly dependent on their prompts. In \nthis vein, majority of our quality ratings were designated \nas “N/A” given the recently emerging techniques for \ninvestigating and evaluating LLMs in medicine. In con -\nsideration of this, our review aimed to propose a more \nrigorous research design and present two main areas \nwhere such improvements can be made: in the study \nmethods, and in how studies report their results.\ni)  Disease/condition selection and contextual \ninformation: To provide a comprehensive \nevaluation, we recommend researchers include \na brief description of the epidemiology of the \npresented disease/condition in their articles. This \ndescription would cover aspects such as prevalence, \nseverity, and acute nature, as our extracted articles \nhave demonstrated how LLMs’ diagnostic accuracy \nvary depending on the severity of a patient’s \nsymptom presentation. It would thus be important \nto further explore and monitor this relationship \nbetween accuracy and the severity of presentation \n(such as by using the estimated severity index, a \nfive-level emergency department triage algorithm), \nconsidering that real-life patients present with a wide \nrange of severity levels.\nii)  Exact wording and phrasing: Based on our \nextracted articles, we recommend authors continue \nboth recording and reporting the exact phrasing used \nwhen interacting with ChatGPT is paramount. This \nis because subtle differences in wording can lead to \nsignificantly varied responses with LLMs, even when \nthe underlying intent is the same [37, 4, 20, 29–63]. \nThis reporting practice will enable future studies \nto build upon or modify previous phrasings and \nvignette structures, and further test the replicability \nof LLMs’ responses to the same clinical prompts.\niii) Inclusion of multiple question types: To expand \nthe application of research on LLMs, researchers \ncould also incorporate both open-ended and closed-\nended question formats in their evaluation. More \nspecifically, this could include open-ended questions \n(similar to a real clinical setting), along with select-\nall-that-apply (SATA) questions and multiple-\nchoice questions (MCQs) that are more relevant \nto medical education prompts. Furthermore, with \nall questions, it would be helpful to prompt LLMs \nto provide justifications for their responses [77]. \nSuch prompting strategies may ultimately highlight \nthe “concordance” or alignment of ChatGPT’s \njustifications with its answers. These questions may \nalso elucidate the level of “uncertainty” by LLMs \nin their responses, what aspects of the prompt \nand patient scenario are difficult to ascertain, \nand ultimately for the users to understand what \nlimitations exist in the LLMs’ responses [48]. An \nexample follow-up prompt to incorporate all these \npoints may be, “Based on the patient’s vignette, tell \nme what you think is the top differential diagnosis \nand explain your reasoning. Then, consider other \ndifferential diagnoses/management steps for this \npatient and why they are less likely/less appropriate \nin this clinical context. ”\niv) Research with one-shot or few-shot learning \nrather than zero-shot learning: In order to \ndetermine what constitutes a “correct” diagnosis and \nensure the accuracy of results from ChatGPT, it is \nbeneficial to employ one-shot or few-shot learning \ntechniques rather than zero-shot learning. More \nspecifically, zero-shot learning refers to asking \nLLMs to generate a response without any fine-\ntuning or templates, while one-shot and few-shot \nlearning refers to the generation of text in a specific \nstyle based on one or more templates provided, \nrespectively. This can be done by conducting \nresearch where LLMs are provided with an accurate \nand comprehensive answer to follow, such as when \nasking LLMs to create EMR notes or providing a \ntreatment plan. Furthermore, future research could \nalso explore how LLMs adjust their responses after \nbeing given feedback about the accuracy, readability, \nand relevance of their responses to assess their ability \nto learn and improve within one chat session.\nv)  Reporting results using standardized metrics for \ntext-based output: The evaluation of text-based \ngeneration from language models (LLMs) currently \nlacks a standardized approach, as there can be \nmultiple valid or readable responses. Thus, beyond \nincluding human evaluators, we propose future \nPage 11 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nresearch around LLMs’ clinical utility include the use \nof healthcare-specific evaluation methods for natural \nlanguage generation (NLG) tasks like with LLMs. \nThere is even very recently ongoing research on the \nalignment of agentic AI systems that do not require \nhumans-in-the-loop (e.g. grants were released in \nDecember 2023 by OpenAI to fund research on \nsuperhuman AI systems that will not necessarily \nrely on human oversight at every step of the model’s \ndecision-making process but is rather confirmed by a \n“superhuman” LLM that evaluate other LLM outputs \nin a manner similar to human reinforcement [78]. \nThis may include using one or more of [21, 24, 79]:\n1) Specification of the prompting parameters, including \nthe temperature, model, seed version, max number \nof tokens permitted, frequency and presence \npenalties used to complete the study;\n2) BLEU (Bilingual Evaluation Understudy) score to \nevaluate the accuracy of text that was translated by \nLLMs into another language;\n3) ROUGE (Recall-Oriented Understudy for Gisting \nEvaluation) score that evaluates the quality of \nsummaries created by text-generators;\n4) METEOR (Metric for Evaluation of Translation with \nExplicit ORdering) that is used to evaluate automatic \nsummarization created from natural language \nprocessing that compares an automatically produced \nsummary against a reference produced by humans.\n5) Perplexity, a metric used to examine the degree a \nlanguage model can predict the distribution of a \ngiven text and its predicted words;\n6) G-Eval, a novel evaluation technique created by \nMicrosoft Cognitive Services Research Centre that \nhas so far outperformed existing NLG evaluators in \nterms of correlation with human evaluations.\n7) To curate a benchmark for LLMs in the medical \ndomain, researchers can follow the example of \nMultiMedQA, a benchmark combining six existing \nmedical question answering datasets spanning \nprofessional medicine, research and consumer \nqueries and a new dataset of medical questions \nsearched online, HealthSearchQA. The benchmark \nincludes multiple-choice questions and a human \nevaluation framework for model answers along \nmultiple axes including factuality, comprehension, \nreasoning, possible harm, and bias. The model \nwas evaluated against multiple criteria, including \nscientific consensus, medical reasoning, knowledge \nrecall, bias, and likelihood of possible harm.\n8) Finally, a calculation of entropy may also be \nwarranted. Entropy is a measure of the randomness \nor unpredictability of information, and specifically \nfor LLM-generated text, entropy can be used to \nquantify the uncertainty of the next character or \nword in a sequence.\nObjective 5: What are some evidence gaps within the \ncurrent literature and what are some potential future \nresearch directions for LLMs’ clinical applications?\nThe integration of language models, such as ChatGPT, \ninto telemedicine outreach initiatives can significantly \nenhance healthcare accessibility and outreach to under -\nserved populations, including in geographically isolated \nregions of the world and in developing countries. In this \nmanner, it will also be crucial to consider the impact on \nhigh-income, middle-income, and low-income countries \nin terms of accessibility and costs. Conducting research \nin these diverse contexts will shed light on the feasibil -\nity, affordability, and potential benefits of implement -\ning LLMs. For instance, prompting an LLM to consider \nlocal circumstances and available medical resources \nmay yield particularly valuable insights in regions where \nthere are such limited resources and options for patients. \nFurthermore, appropriately utilizing LLMs may lead to \nmore consistent, standardized care across regions and \nultimately promote consistent and equitable healthcare \ndelivery.\nAnother significant area for investigation is the incor -\nporation of social determinants of health (SDOH) and \nreal, anonymized, consented patient cases to add com -\nplexity to LLM assessments [ 20, 30, 40]. While there is \nan abundance of research utilizing USMLE standardized, \ntextbook-style questions, there is a scarcity of studies that \nleverage real patient data, which has been recognized \nas a limitation in several papers [ 20, 30, 40]. By includ -\ning social factors such as socioeconomic status (SES), \nrace, and sexuality, biases in the answers generated by \nLLMs can be directly assessed. It is crucial to address the \ninherent biases in LLMs, including mimicking extrem -\nist internet language, practical biases resulting from the \nunderrepresentation of marginalized populations in data \nand research, and implicit biases introduced during fine-\ntuning by healthcare professionals [73].\nThe inability of language models (LLMs) to provide tai -\nlored recommendations based on regional guidelines and \nspecific cut-off values for management, as highlighted \nby Yeo et al. (2023), is an important limitation that war -\nrants further investigation [ 24]. Guidelines are typically \ndeveloped based on trends derived from review method -\nologies, thus such reviews should be explicitly integrated \nand tested with LLMs. One challenge in this realm, how -\never, is that certain medical conditions currently lack \nstandardized guidelines [80, 81]. For example, conditions \nlike reactive infectious mucocutaneous eruption, a rare \nbut serious pediatric dermatological condition, currently \nhave no established guidelines [ 80]. In such cases, LLMs, \nPage 12 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nlike ChatGPT, could potentially play a role in the creation \nof clinical guidelines. More specifically, LLMs may make \nthe process more efficient by firstly confirming insights \nextracted by human experts, and subsequently offering \nnovel insights and connections that may not have been \ndiscovered as quickly given the vast amount of clinical \ndata and information synthesized. This capability opens \nup exciting possibilities for advancing medical knowl -\nedge and improving patient care through integrating \nLLMs into the guideline creation process. By leveraging \ntheir ability to find and generate connections and pat -\nterns in optimal patient care, LLMs can overall comple -\nment existing review methodologies used for the creation \nof clinical guidelines. Further research will need to be \nconducted to verify LLMs’ capabilities in assisting with \nsuch processes, especially given the potential impact on \nelevating or harming policies for population health.\nOverall, given LLMs’ limitations, such as their tendency \nto hallucinate, the randomness of responses, and restric -\ntions on a model’s training dataset (i.e. the “garbage-in \ngarbage-out” phenomenon), general purpose LLMs are \nnot in their current forms sufficient for medical decision-\nmaking purposes. Rather, they possess most potential in \nsupporting patient navigation and EMR-related tasks, \nparticularly given their ability to provide personalized \nresponses to given prompts and summarize information \nefficiently and in an organized manner.\nStrengths and limitations\nThis scoping review used rigorous and transparent \nmethods. Its conduct was guided by the PRISMA-ScR \nreporting checklist, as well as a prospectively published \nprotocol designed by expert reviewers with knowledge \nin evidence synthesis and scoping reviews. Structured, \nlibrarian-assisted database searches were conducted in \nthree major literature databases coupled with extensive \nhandsearching, which ensured comprehensive inclusion \nof relevant articles. Additionally, each record retrieved \nfrom databases were reviewed in-duplicate using an \nestablished review platform (Covidence) to ensure \nthat all citations and articles were properly tracked and \naccounted for during the screening process.\nDespite our attempts to conduct a comprehensive \nsearch, this review may not have identified all available \nstudies. Our search strategy included 8 key terms to cover \nthe concept of LLM; however, other terms may exist, \nespecially since a diverse set of LLM naming schemes are \nactively being used. Additionally, our strategy used terms \nrelating to clinical practice to filter out potentially irrel -\nevant records. This may inadvertently exclude relevant \narticles that did not have health-related terms in its title, \nkeywords, or MeSH entries. Finally, much has evolved in \nthe world of LLMs since we began our study and com -\npleted our search; subsequently, while we have strived to \nincorporate several updates into our discussions, a lim -\nitation is that our main analysis is mainly based on the \nstudies that were available at the time that we performed \nour search and analyses.\nAs this was a scoping review, we did not assess the risks \nof bias of the included studies. There is also currently no \nvalidated quality assessment tool available for assessing \nthe type of exploratory machine learning studies included \nin this review, as noted. We are also unable to verify the \naccuracy and validity of findings reported in our included \npreprint articles, as they have not undergone peer-review.\nConclusion\nLarge language models (LLMs), particularly ChatGPT, \nhave shown promise in various clinical applications, \nranging from the creation of patient notes to helping \nhealthcare providers diagnose rare conditions. How -\never, it is important to recognize the inherent limita -\ntions of artificial intelligence (AI) systems. Overall, as \nour extracted articles also reinforce, there is a place for \nhumans-in-the-loop to oversee LLMs utility in clinical \nsettings to ensure erroneous recommendations or inad -\nequate diagnoses do not cause patient harm. The respon -\nsibility to ensure the accuracy and reliability of LLMs \nbefore integration into clinical settings further rests on \nmultiple stakeholders - particularly researchers. Overall, \nvalidation and replication studies are essential, and our \npaper synthesizes several areas of ongoing logistical and \nethicolegal concerns. We also propose standardized tech-\nniques that can be integrated into future research to bet -\nter address the challenges and uncertainties associated \nwith LLMs in medicine. All in all, LLMs hold significant \npotential, and the continued exploration and careful nav -\nigation of these challenges will be crucial to understand \ntheir benefits and drawbacks in healthcare settings.\nSupplementary Information\nThe online version contains supplementary material available at https://doi.\norg/10.1186/s12911-024-02459-6.\nSupplementary Material 1\nAcknowledgements\nWe would like to thank all reviewers for their time and consideration in \nreviewing our article and helping improve its quality.\nAuthor contributions\nYP , EG, and MG conceptualized the study. YP , JD, and AP developed the \nmethodology, and all three authors were also responsible for data extraction \nand writing the original draft of the manuscript. EG and MG were responsible \nfor part of the original draft, along with reviewing and editing the manuscript. \nMP and CN were responsible for supervising the project and reviewing/\nediting the manuscript.\nFunding\nThere was no funding to complete this research. \nPage 13 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \nData availability\nAll data and materials available upon request.\nDeclarations\nEthics approval and consent to participate\nNo ethics approval or consent to participate was needed for this article.\nConsent for publication\nNo consent to publish was needed for this article.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1Temerty Faculty of Medicine, University of Toronto, 1 King’s College Cir, \nM5S 1A8 Toronto, ON, Canada\n2Cumming School of Medicine, University of Calgary, 3330 Hospital Dr \nNW, T2N 4N1 Calgary, AB, Canada\nReceived: 21 October 2023 / Accepted: 12 February 2024\nReferences\n1. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, et al. A large \nlanguage model for electronic health records. NPJ Digit Med. 2022;5(1):194.\n2. OpenAI. Introducing ChatGPT [Internet]. [cited 2023 May 2]. Available from: \nhttps://openai.com/blog/chatgpt.\n3. Devlin J, Chang MW, Lee K, Toutanova K, BERT. Pre-training of deep bidi-\nrectional Transformers for language understanding [Internet]. arXiv. 2018. \nAvailable from: https://arxiv.org/abs/1810.04805.\n4. Levine DM, Tuwani R, Kompa B, Varma A, Finlayson SG, Mehrotra A et al. The \nDiagnostic and Triage Accuracy of the GPT-3 Artificial Intelligence Model \n[Internet]. medRxiv. 2023. https://doi.org/10.1101/2023.01.30.23285067.\n5. Stewart J, Lu J, Goudie A, Arendts G, Meka SA, Freeman S et al. Applications of \nnatural language processing at emergency department triage: A systematic \nreview [Internet]. bioRxiv. 2022. https://doi.org/10.1101/2022.12.20.22283735.\n6. Sallam M. ChatGPT Utility in Healthcare Education, Research, and Prac-\ntice: Systematic Review on the Promising Perspectives and Valid Con-\ncerns. Healthcare (Basel) [Internet]. 2023;11(6). https://doi.org/10.3390/\nhealthcare11060887.\n7. Sallam M. The utility of ChatGPT as an example of large language models in \nhealthcare education, research and practice: Systematic review on the future \nperspectives and potential limitations [Internet]. medRxiv. 2023. https://doi.\norg/10.1101/2023.02.19.23286155.\n8. Stokel-Walker C, Van Noorden R. What ChatGPT and generative AI mean for \nscience. Nature. 2023;614(7947):214–6.\n9. Nov O, Singh N, Mann DM. Putting ChatGPT’s medical advice to the (Turing) \nTest [Internet]. bioRxiv. 2023. Available from: http://medrxiv.org/lookup/\ndoi/https://doi.org/10.1101/2023.01.23.23284735.\n10. Tricco AC, Lillie E, Zarin W, O’Brien KK, Colquhoun H, Levac D, et al. PRISMA \nExtension for scoping reviews (PRISMA-ScR): Checklist and Explanation. Ann \nIntern Med. 2018;169(7):467–73.\n11. The Centre for Evidence-Based Medicine [Internet]. 2020 [cited 2023 Oct 10]. \nThe centre for evidence-based medicine. Available from: https://www.cebm.\nnet/.\n12. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient \nclinic letters. Lancet Digit Health. 2023;5(4):e179–81.\n13. Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the feasibility of \nChatGPT in Healthcare: an analysis of multiple clinical and research scenarios. \nJ Med Syst. 2023;47(1):33.\n14. Patel SB, Lam K. ChatGPT: the future of discharge summaries? Lancet Digit \nHealth. 2023;5(3):e107–8.\n15. Lee P , Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an AI Chatbot for \nMedicine. N Engl J Med. 2023;388(13):1233–9.\n16. Puthenpura V, Nadkarni S, DiLuna M, Hieftje K, Marks A. Personality changes \nand staring spells in a 12-Year-old child: a Case Report incorporating Chat-\nGPT, a Natural Language Processing Tool Driven by Artificial Intelligence (AI). \nCureus. 2023;15(3):e36408.\n17. Lantz R. Toxic epidermal necrolysis in a critically ill African American woman: \na Case Report Written with ChatGPT Assistance. Cureus. 2023;15(3):e35742.\n18. Beltrami EJ, Grant-Kels JM. Consulting ChatGPT: Ethical dilemmas in language \nmodel artificial intelligence. J Am Acad Dermatol [Internet]. 2023; https://doi.\norg/10.1016/j.jaad.2023.02.052.\n19. Sezgin E, Sirrianni J, Linwood SL, Operationalizing, Pretrained I. Large Artificial \nIntelligence Linguistic Models in the US Health Care System: Outlook of \nGenerative Pretrained Transformer 3 (GPT-3) as a service model. JMIR Med Inf. \n2022;10(2):e32875.\n20. Baumgartner C. The potential impact of ChatGPT in clinical and translational \nmedicine. Clin Transl Med. 2023;13(3):e1206.\n21. Haupt CE, Marks M. AI-Generated medical Advice-GPT and Beyond. JAMA. \n2023;329(16):1349–50.\n22. Google Cloud [Internet]. [cited 2023 Jul 15]. Evaluating models. Available \nfrom: https://cloud.google.com/translate/automl/docs/evaluate.\n23. Lyu Q, Tan J, Zapadka ME, Ponnatapura J, Niu C, Myers KJ et al. Translating \nRadiology Reports into Plain Language using ChatGPT and GPT-4 with \nPrompt Learning: Promising Results, Limitations, and Potential [Internet]. \narXiv [cs.CL]. 2023. Available from: http://arxiv.org/abs/2303.09038.\n24. Yeo YH, Samaan JS, Ng WH, Ting PS, Trivedi H, Vipani A et al. Assessing the \nperformance of ChatGPT in answering questions regarding cirrhosis and \nhepatocellular carcinoma [Internet]. bioRxiv. 2023. Available from: https://\nwww.medrxiv.org/content/https://doi.org/10.1101/2023.02.06.23285449v1.\n25. Zhu L, Mou W, Chen R. Can the ChatGPT and other large language models \nwith internet-connected database solve the questions and concerns of \npatient with prostate cancer and help democratize medical knowledge? J \nTransl Med. 2023;21(1):269.\n26. Ali R, Connolly ID, Tang OY, Mirza FN, Johnston B, Abdulrazeq HA et al. Bridg-\ning the literacy gap for surgical consents: An AI-human expert collaborative \napproach [Internet]. medRxiv. 2023. Available from: https://www.medrxiv.\norg/content/https://doi.org/10.1101/2023.05.06.23289615v1.\n27. Cox A, Seth I, Xie Y, Hunter-Smith DJ, Rozen WM. Utilizing ChatGPT-4 for \nProviding Medical Information on Blepharoplasties to Patients. Aesthet Surg J \n[Internet]. 2023; https://doi.org/10.1093/asj/sjad096.\n28. Suresh K, Rathi V, Nwosu O, Partain MP , Glicksman JT, Jowett N et al. Utility \nof GPT-4 as an informational patient resource in otolaryngology [Internet]. \nmedRxiv. 2023. Available from: https://www.medrxiv.org/content/https://doi.\norg/10.1101/2023.05.14.23289944v1.\n29. Chari S, Acharya P , Gruen DM, Zhang O, Eyigoz EK, Ghalwash M, et al. Inform-\ning clinical assessment by contextualizing post-hoc explanations of risk \nprediction models in type-2 diabetes. Artif Intell Med. 2023;137:102498.\n30. DiGiorgio AM, Ehrenfeld JM. Artificial Intelligence in Medicine & ChatGPT: \nDe-tether the Physician. J Med Syst. 2023;47(1):32.\n31. Khan RA, Jawaid M, Khan AR, Sajjad M. ChatGPT - reshaping medical educa-\ntion and clinical management. Pak J Med Sci Q. 2023 Mar-Apr;39(2):605–7.\n32. Rao A, Kim J, Kamineni M, Pang M, Lie W, Succi MD. Evaluating ChatGPT as an \nAdjunct for Radiologic Decision-Making. medRxiv [Internet]. 2023; https://doi.\norg/10.1101/2023.02.02.23285399.\n33. Rao A, Pang M, Kim J, Kamineni M, Lie W, Prasad AK et al. Assessing the Utility \nof ChatGPT Throughout the Entire Clinical Workflow. medRxiv [Internet]. \n2023; https://doi.org/10.1101/2023.02.21.23285886.\n34. Sabry Abdel-Messih M, Kamel Boulos MN. ChatGPT in clinical toxicology. JMIR \nMed Educ. 2023;9:e46876.\n35. Ufuk F. The Role and limitations of large Language models such as ChatGPT \nin Clinical settings and Medical Journalism. Radiology. 2023;307(3):e230276.\n36. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C et al. GatorTron: \nA Large Clinical Language Model to Unlock Patient Information from \nUnstructured Electronic Health Records [Internet]. arXiv [cs.CL]. 2022. Avail-\nable from: http://arxiv.org/abs/2203.03540.\n37. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on \nMedical Challenge Problems [Internet]. arXiv [cs.CL]. 2023. Available from: \nhttp://arxiv.org/abs/2303.13375.\n38. Liu J, Wang C, Liu S. Utility of ChatGPT in Clinical Practice. J Med Internet Res. \n2023;25:e48568.\n39. Haemmerli J, Sveikata L, Nouri A, May A, Egervari K, Freyschlag C et al. Chat-\nGPT in glioma patient adjuvant therapy decision making: ready to assume \nthe role of a doctor in the tumour board? [Internet]. bioRxiv. 2023. Available \nfrom: https://www.medrxiv.org/content/https://doi.org/10.1101/2023.03.19.2\n3287452v1.\n40. Au Yeung J, Kraljevic Z, Luintel A, Balston A, Idowu E, Dobson RJ, et al. AI \nchatbots not yet ready for clinical use. Front Digit Health. 2023;5:1161098.\nPage 14 of 14\nPark et al. BMC Medical Informatics and Decision Making            (2024) 24:72 \n41. Kim JH. Search for medical information and treatment options for muscu-\nloskeletal disorders through an artificial intelligence chatbot: Focusing on \nshoulder impingement syndrome [Internet]. bioRxiv. 2022. Available from: \nhttps://www.medrxiv.org/content/https://doi.org/10.1101/2022.12.16.22283\n512v2.\n42. Mehnen L, Gruarin S, Vasileva M, Knapp B. ChatGPT as a medical doctor? A \ndiagnostic accuracy study on common and rare diseases [Internet]. medRxiv. \n2023. Available from: https://www.medrxiv.org/content/https://doi.org/10.11\n01/2023.04.20.23288859v2.\n43. Knebel D, Priglinger S, Scherer N, Siedlecki J, Schworm B. Assessment of Chat-\nGPT in the preclinical management of ophthalmological emergencies - an \nanalysis of ten fictional case vignettes [Internet]. bioRxiv. 2023. Available from: \nhttps://www.medrxiv.org/content/https://doi.org/10.1101/2023.04.16.23288\n645v1.\n44. Gravel J, D’Amours-Gravel M, Osmanlliu E. Learning to fake it: limited \nresponses and fabricated references provided by ChatGPT for medical \nquestions [Internet]. bioRxiv. 2023. Available from: https://www.medrxiv.org/\ncontent/https://doi.org/10.1101/2023.03.16.23286914v1.\n45. Xie Q, Schenck EJ, Yang HS, Chen Y, Peng Y, Wang F, Faithful AI. in Medicine: \nA Systematic Review with Large Language Models and Beyond. medRxiv \n[Internet]. 2023; https://doi.org/10.1101/2023.04.18.23288752.\n46. Perlis RH. Research Letter: Application of GPT-4 to select next-step antide-\npressant treatment in major depression. medRxiv [Internet]. 2023; https://doi.\norg/10.1101/2023.04.14.23288595.\n47. Rau A, Rau S, Fink A, Tran H, Wilpert C, Nattenmueller J et al. A context-based \nchatbot surpasses trained radiologists and generic ChatGPT in following the \nACR appropriateness guidelines [Internet]. medRxiv. 2023. Available from: \nhttps://www.medrxiv.org/content/https://doi.org/10.1101/2023.04.10.23288\n354v1.\n48. Comrie D. ChatGPT decision support system: Utility in creating public policy \nfor concussion/repetitive brain trauma associated with neurodegenerative \ndiseases [Internet]. medRxiv. 2023. Available from: https://www.medrxiv.org/\ncontent/https://doi.org/10.1101/2023.04.17.23288681v1.\n49. Wagner MW, Ertl-Wagner BB. Accuracy of information and references using \nChatGPT-3 for Retrieval of Clinical Radiological Information. Can Assoc Radiol \nJ. 2023;8465371231171125.\n50. Williams MC, Shambrook J. How will artificial intelligence transform \ncardiovascular computed tomography? A conversation with an AI model. \nJ Cardiovasc Comput Tomogr [Internet]. 2023; https://doi.org/10.1016/j.\njcct.2023.03.010.\n51. Ueda D, Walston SL, Matsumoto T, Deguchi R, Tatekawa H, Miki Y, Evaluat-\ning. GPT-4-based ChatGPT’s clinical potential on the NEJM quiz [Internet]. \nmedRxiv. 2023. Available from: https://www.medrxiv.org/content/https://doi.\norg/10.1101/2023.05.04.23289493v1.\n52. Gabriel RA, Mariano ER, McAuley J, Wu CL. How large language models can \naugment perioperative medicine: a daring discourse. Reg Anesth Pain Med \n[Internet]. 2023; https://doi.org/10.1136/rapm-2023-104637.\n53. Liao Z, Wang J, Shi Z, Lu L, Tabata H. Revolutionary Potential of ChatGPT in \nConstructing Intelligent Clinical Decision Support Systems. Ann Biomed Eng \n[Internet]. 2023; https://doi.org/10.1007/s10439-023-03288-w.\n54. Ravipati A, Pradeep T, Elman SA. The role of artificial intelligence in dermatol-\nogy: the promising but limited accuracy of ChatGPT in diagnosing clinical \nscenarios. Int J Dermatol [Internet]. 2023; https://doi.org/10.1111/ijd.16746.\n55. Snoswell CL, Snoswell AJ, Kelly JT, Caffery LJ, Smith AC. Artificial intelligence: \naugmenting telehealth with large language models. J Telemed Telecare. \n2023;1357633X:231169055.\n56. Danilov G, Kotik K, Shevchenko E, Usachev D, Shifrin M, Strunina Y, et \nal. Length of Stay Prediction in Neurosurgery with Russian GPT-3 Lan-\nguage Model compared to human expectations. Stud Health Technol Inf. \n2022;289:156–9.\n57. Hirosawa T, Harada Y, Yokose M, Sakamoto T, Kawamura R, Shimizu T. \nDiagnostic Accuracy of Differential-Diagnosis Lists Generated by Genera-\ntive Pretrained Transformer 3 Chatbot for Clinical Vignettes with Common \nChief Complaints: A Pilot Study. Int J Environ Res Public Health [Internet]. \n2023;20(4). https://doi.org/10.3390/ijerph20043378.\n58. Liu S, Wright AP , Patterson BL, Wanderer JP , Turer RW, Nelson SD et al. Assess-\ning the Value of ChatGPT for Clinical Decision Support Optimization. medRxiv \n[Internet]. 2023; https://doi.org/10.1101/2023.02.21.23286254.\n59. Tripathy S, Singh R, Ray M. Natural Language Processing for Covid-19 Consult-\ning System. Procedia Comput Sci. 2023;218:1335–41.\n60. Harskamp RE, De Clercq L. Performance of ChatGPT as an AI-assisted deci-\nsion support tool in medicine: a proof-of-concept study for interpreting \nsymptoms and management of common cardiac conditions (AMSTEL-\nHEART-2) [Internet]. medRxiv. 2023. Available from: https://www.medrxiv.org/\ncontent/https://doi.org/10.1101/2023.03.25.23285475v1.\n61. Guo E, Gupta M, Sinha S, Rössler K, Tatagiba M, Akagami R et al. NeuroGPT-\nX: Towards an accountable expert opinion tool for vestibular schwan-\nnoma [Internet]. medRxiv. 2023. Available from: https://www.medrxiv.org/\ncontent/https://doi.org/10.1101/2023.02.25.23286117v1.\n62. Noaeen M, Amini S, Bhasker S, Ghezelsefli Z, Ahmed A, Jafarinezhad O et al. \nUnlocking the power of EHRs: Harnessing unstructured data for Machine \nLearning-based outcome predictions [Internet]. medRxiv. 2023. Available \nfrom: https://www.medrxiv.org/content/https://doi.org/10.1101/2023.02.13.2\n3285873v1.\n63. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing physi-\ncian and Artificial Intelligence Chatbot responses to patient questions posted \nto a Public Social Media Forum. JAMA Intern Med. 2023;183(6):589–96.\n64. Nastasi AJ, Courtright KR, Halpern SD, Weissman GE. Does ChatGPT provide \nappropriate and equitable medical advice? A vignette-based, clinical evalu-\nation across care contexts [Internet]. bioRxiv. 2023. Available from: https://\nwww.medrxiv.org/content/https://doi.org/10.1101/2023.02.25.23286451v1.\n65. Brown H, Lee K, Mireshghallah F, Shokri R, Tramèr F. What Does it Mean for \na Language Model to Preserve Privacy? In: Proceedings of the 2022 ACM \nConference on Fairness, Accountability, and Transparency. New York, NY, USA: \nAssociation for Computing Machinery; 2022. p. 2280–92. (FAccT ’22).\n66. Mireshghallah F, Goyal K, Uniyal A, Berg-Kirkpatrick T, Shokri R. Quantifying \nPrivacy Risks of Masked Language Models Using Membership Inference \nAttacks [Internet]. arXiv [cs.LG]. 2022. Available from: http://arxiv.org/\nabs/2203.03929.\n67. About BGPT. HIPAA compliant ChatGPT [Internet]. [cited 2023 Dec 25]. Avail-\nable from: https://bastiongpt.com/company/about.\n68. Kraljevic Z, Bean D, Shek A, Bendayan R, Hemingway H, Au Yeung J et al. \nForesight -- Generative Pretrained Transformer (GPT) for Modelling of Patient \nTimelines using EHRs [Internet]. arXiv [cs.CL]. 2022. Available from: http://\narxiv.org/abs/2212.08072.\n69. David E, The Verge. 2023 [cited 2023 Jul 18]. Meta is giving away its AI \ntech to try to beat ChatGPT. Available from: https://www.theverge.\ncom/2023/7/18/23799025/meta-ai-llama-2-open-source-microsoft.\n70. Falcon LLM. [Internet]. [cited 2023 Jul 18]. Available from: https://falconllm.tii.\nae/.\n71. OpenAI. GPT-4 [Internet]. [cited 2023 May 2]. Available from: https://openai.\ncom/research/gpt-4.\n72. Apple Support [Internet]. [cited 2023 Jul 18]. Secure Enclave. Available from: \nhttps://support.apple.com/en-ca/guide/security/sec59b0b31ff/web.\n73. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P , et al. Training \nlanguage models to follow instructions with human feedback. Adv Neural Inf \nProcess Syst. 2022;35:27730–44.\n74. Meskó B, Topol EJ. The imperative for regulatory oversight of large language \nmodels (or generative AI) in healthcare. NPJ Digit Med. 2023;6(1):120.\n75. Siala H, Wang Y. SHIFTing artificial intelligence to be responsible in healthcare: \na systematic review. Soc Sci Med. 2022;296:114782.\n76. Lambert SI, Madi M, Sopka S, Lenes A, Stange H, Buszello CP , et al. An integra-\ntive review on the acceptance of artificial intelligence among healthcare \nprofessionals in hospitals. NPJ Digit Med. 2023;6(1):111.\n77. Shinn N, Cassano F, Labash B, Gopinath A, Narasimhan K, Yao S, Reflexion. \nLanguage Agents with Verbal Reinforcement Learning [Internet]. arXiv [cs.AI]. \n2023. Available from: http://arxiv.org/abs/2303.11366.\n78. Superalignment fast grants [Internet]. [cited 2023 Dec 26]. Available from: \nhttps://openai.com/blog/superalignment-fast-grants.\n79. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW et al. Large language \nmodels encode clinical knowledge. Nature [Internet]. 2023; https://doi.\norg/10.1038/s41586-023-06291-2.\n80. Lau FF, Ronit A, Weis N, Winckelmann A. Reactive infectious mucosal erup-\ntions (RIME) secondary to Chlamydia pneumoniae infection. Rep Int Dev Res \nCent Can. 2021;4(2):11.\n81. Graham ID, Harrison MB. Evaluation and adaptation of clinical practice guide-\nlines. Evid Based Nurs. 2005;8(3):68–72.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.",
  "topic": "CINAHL",
  "concepts": [
    {
      "name": "CINAHL",
      "score": 0.641578733921051
    },
    {
      "name": "MEDLINE",
      "score": 0.4918549060821533
    },
    {
      "name": "Health care",
      "score": 0.48087719082832336
    },
    {
      "name": "Medicine",
      "score": 0.46575361490249634
    },
    {
      "name": "Socioeconomic status",
      "score": 0.45979827642440796
    },
    {
      "name": "Political science",
      "score": 0.25809603929519653
    },
    {
      "name": "Environmental health",
      "score": 0.194939523935318
    },
    {
      "name": "Population",
      "score": 0.1602775752544403
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}