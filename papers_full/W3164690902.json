{
  "title": "Temporal Action Proposal Generation with Transformers",
  "url": "https://openalex.org/W3164690902",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2130864247",
      "name": "Wang Li-ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001040369",
      "name": "Yang Haosen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965248084",
      "name": "Wu Wen-Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2087543691",
      "name": "Yao, Hongxun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228020544",
      "name": "Huang Hu-jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2884969173",
    "https://openalex.org/W2962677524",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3128626728",
    "https://openalex.org/W2998582438",
    "https://openalex.org/W2962876901",
    "https://openalex.org/W2963526497",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2519328139",
    "https://openalex.org/W2394849137",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2470774766",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3174568846",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2997706915",
    "https://openalex.org/W3085617392",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W3034623254",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2893390896",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W3145269263",
    "https://openalex.org/W2604730366",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2953153458",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W2953229046",
    "https://openalex.org/W2607644813",
    "https://openalex.org/W3158818292",
    "https://openalex.org/W2604113307"
  ],
  "abstract": "Transformer networks are effective at modeling long-range contextual information and have recently demonstrated exemplary performance in the natural language processing domain. Conventionally, the temporal action proposal generation (TAPG) task is divided into two main sub-tasks: boundary prediction and proposal confidence prediction, which rely on the frame-level dependencies and proposal-level relationships separately. To capture the dependencies at different levels of granularity, this paper intuitively presents a unified temporal action proposal generation framework with original Transformers, called TAPG Transformer, which consists of a Boundary Transformer and a Proposal Transformer. Specifically, the Boundary Transformer captures long-term temporal dependencies to predict precise boundary information and the Proposal Transformer learns the rich inter-proposal relationships for reliable confidence evaluation. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG Transformer outperforms state-of-the-art methods. Equipped with the existing action classifier, our method achieves remarkable performance on the temporal action localization task. Codes and models will be available.",
  "full_text": "Temporal Action Proposal Generation with Transformers\nLining Wangâˆ—\nHarbin Institute of Technology\nHaosen Yangâˆ—\nHarbin Institute of Technology\nWenhao Wuâˆ—\nDepartment of Computer Vision\nTechnology (VIS), Baidu Inc.\nHongxun Yaoâ€ \nHarbin Institute of Technology\nHujie Huang\nHarbin Institute of Technology\nABSTRACT\nTransformer networks are eective at modeling long-range contex-\ntual information and have recently demonstrated exemplary perfor-\nmance in the natural language processing domain. Conventionally,\nthe temporal action proposal generation (TAPG) task is divided\ninto two main sub-tasks: boundary prediction and proposal con-\ndence prediction, which rely on the frame-level dependencies and\nproposal-level relationships separately. To capture the dependen-\ncies at dierent levels of granularity, this paper intuitively presents\nan unied temporal action proposal generation framework with\noriginal Transformers, called TAPG Transformer, which consists\nof a Boundary Transformer and a Proposal Transformer. Specically,\nthe Boundary Transformer captures long-term temporal depen-\ndencies to predict precise boundary information and the Proposal\nTransformer learns the rich inter-proposal relationships for reliable\ncondence evaluation. Extensive experiments are conducted on\ntwo popular benchmarks: ActivityNet-1.3 and THUMOS14, and the\nresults demonstrate that TAPG Transformer outperforms state-of-\nthe-art methods. Equipped with the existing action classier, our\nmethod achieves remarkable performance on the temporal action\nlocalization task. Codes and models will be available.\nCCS CONCEPTS\nâ€¢Computing methodologies â†’Activity recognition and un-\nderstanding;\nKEYWORDS\ntransformers, temporal action proposal generation, untrimmed\nvideo, temporal action localization\nACM Reference format:\nLining Wangâˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie\nHuang. 2021. Temporal Action Proposal Generation with Transformers. In\nProceedings of Woodstock â€™18: ACM Symposium on Neural Gaze Detection,\nWoodstock, NY, June 03â€“05, 2018 (Woodstock â€™18), 10 pages.\nDOI: 10.1145/3394171.3413860\nâˆ—Co-rst authorship: the order of rst authors was randomly selected.â€ Corresponding\nauthor.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor prot or commercial advantage and that copies bear this notice and the full citation\non the rst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specic permission and/or a\nfee. Request permissions from permissions@acm.org.\nWoodstock â€™18, Woodstock, NY\nÂ© 2021 ACM. 978-1-4503-7988-5/20/10. . . $15.00\nDOI: 10.1145/3394171.3413860\nGround Truth\nscore:0.97\nscore:0.67\nscore:0.11\nFeature Extractor\nscore:0.10\nscore:0.27\nscore:0.06\nscore:0.14\nscore:0.56\nscore:0.45\nscore:0.37\nscore:0.78\nstart end\nFrame Sequence\nBoundary Transformer Proposal Transformer \nâ†’ Matching â†\nProposal Sequence\nSliding \nWindow\nResults\nscore:0.20\nFigure 1: Overview of TAPG Transformer. Given an\nuntrimmed video, Boundary Transformer captures inter-\nframe relation and Proposal Transformer encodes inter-\nproposal relation. We further match the boundary probabil-\nities with proposal condence sequence to generate the nal\nproposals.\n1 INTRODUCTION\nWith the rapid development of the mobile devices and Internet,\nhours of video were uploaded to the Internet every second. Huge\nvideo information has far exceeded the processing capacity of the\nconventional manual system, thus video content analysis methods\nhave attracted increased interest from both academia and industry.\nVideo temporal action detection, is one of the most active re-\nsearch topics in video understanding, which focuses both on classi-\nfying the action instances present in an untrimmed video and on\nlocalizing them with temporal boundaries. Similar to object detec-\ntion, temporal action detection task is composed of two sub-tasks:\ntemporal action proposal generation (TAPG) and action recognition.\narXiv:2105.12043v1  [cs.CV]  25 May 2021\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lining Wang âˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie Huang\nRecently, signicant progress has been achieved in action recog-\nnition ([12, 24, 36, 45]) following the deep convolutional network\nparadigm. However, the performance of such two-stage temporal\naction detectors still have much room for improvement in main-\nstream benchmarks [ 4, 19], which is largely determined by the\nproposal quality from TAPG. Hence, many recent works focus on\ngenerating the high quality temporal action proposals which have\nexible duration, precise boundaries and reliable condence scores.\nIn general, there are two main categories in the existing TAPG\nmethods: anchor-based regression methods and the boundary-based\nregression methods. These two kinds of regression methods usually\nadopt context information in video from dierent aspects. anchor-\nbased methods [7, 10, 13â€“15, 27, 35] generate action proposals based\non multi-scale and dense pre-dened anchors, in this way, we can\nevaluate condence scores of proposals with rich proposal-level\ncontext information. Therefore, these methods can obtain reliable\ncondence scores, but are inexible and usually have imprecise\nboundaries. Recently, boundary-based methods [23, 25, 26] utilize\nthe frame-level context information around the boundaries to pre-\ndict boundaries. Thus, compared with anchor-based methods, they\ncan generate proposals with more exible durations and more pre-\ncise boundaries. Meanwhile, they are more sensitive to noises and\nfail to consider the rich proposal-level context for condence eval-\nuation. Based on these above analyses, we attempt to take fuller\nadvantage of both frame-level context and proposal-level context\nfor temporal proposal generation.\nIn this paper, we focus on inter-frame relation modeling and\ninter-proposal relation modeling for temporal proposal generation.\nInspired by the successful application of Transformers [43] in se-\nquence prediction task, we intuitively take advantage of Transform-\ners in modeling long-range contextual information. Therefore, we\npropose an unied temporal action proposal generation framework\nwith Transformers, called TAPG Transformer. As shown in Figure 1,\nour TAPG Transformer mainly contains two following modules:\nBoundary Transformerand Proposal Transformer. Boundary\nTransformer aims to locate precise action boundaries by capturing\nthe rich long-term temporal relationships between local details and\nglobal dependencies. To do so, the sequence of video features is\nprovided as an input to the Transformer and output of the module\nis boundary probabilities. Then, Proposal Transformer is proposed\nto capture the potential inter-proposal relationships for condence\nevaluation. Before performing inter-proposal relations, we propose\na Sparse Sampling Mechanism to generate the sparse proposal se-\nquence instead of the densely distributed proposals which will bring\nimbalanced data distribution between positive/negative proposals\nand more computational burdens. Then we feed them into the Pro-\nposal Transformer which discovers the complex relationships be-\ntween proposals with dierent scales, and the Transformer outputs\nproposal condences. Finally, we take the boundary probabilities\nand proposal condences as input to Fuzzy Matching mechanism\nthen get the matching proposals. Followed by the post processing,\nwe obtain the nal proposal set.\nIn summary, our work has three main contributions as follows:\nâ€¢We propose a Boundary Transformer to capture the long-\nterm frame-level dependencies for accurate temporal bound-\nary prediction.\nâ€¢We propose a Proposal Transformer with a sparse proposal\nsampling mechanism, which can learn the proposal-level\ncontext for proposal condence evaluation. Besides, sparse\nsampling can signicantly reduce the impact brought by\nthe densely distributed proposals.\nâ€¢Extensive experiments demonstrate that our method out-\nperforms the existing state-of-the-art methods on THU-\nMOS14 and achieves comparable performance on ActivityNet-\n1.3, in both temporal action proposal generation task and\ntemporal action detection task.\n2 RELATED WORK\n2.1 Video Action Recognition\nAction recognition is a fundamental and important task of the\nvideo understanding area. Currently, the end-to-end action recogni-\ntion methods can be mainly divided into two types: 3D CNN-based\nmethods and 2D CNN-based methods. 3D CNNs are intuitive spatio-\ntemporal networks and natural extension from their 2D counter-\nparts, which directly tackle 3D volumetric video data [ 6, 40] but\nhave a heavy computational cost. Recent studies have shown that\nfactorizing 3D convolutional kernel into spatial (e.g., 1Ã—3Ã—3) and\ntemporal components ( e.g., 3Ã—1Ã—1) is preferable to reduce com-\nplexity as well as boost accuracy, e.g., P3D [33], R(2+1)D [42], S3D-\nG [50], etc. Thus, 3D channel-wise convolution are also applied to\nvideo recognition in CSN [41] and X3D [11]. To capture temporal\ninformation with reasonable training resources, the other alter-\nnative 2D CNN based architectures are developed i.e., TSM [24],\nTEI [28], TEA [22], MVFNet [46], etc. These methods aim to design\necient temporal module based on existing 2D CNNs to perform ef-\ncient temporal modeling. There is also active research on dynamic\ninference [48], adaptive frame sampling techniques [ 20, 47, 49],\nwhich we think can be complementary to the end-to-end video\nrecognition approaches.\n2.2 Temporal Action Proposal Generation\nCurrent temporal action proposal generation methods can be mainly\ndivided into anchor-based and boundary-based methods. The anchor-\nbased methods refer to the temporal boundary renements of slid-\ning windows or pre-dened anchors. [19, 31] use temporal sliding\nwindow and [35, 57] use snippet-wise probability to generate can-\ndidate proposals. TURN [15], CTAP [13] and Rap [14] adopt pre-\ndened anchors to generate proposals. These methods can evaluate\ncondence scores of proposals with rich proposal-level context\ninformation and obtain reliable condence scores. Boundary-based\nmethods generate the boundary probability sequence, then apply\nthe Boundary Matching mechanism to generate candidate proposals.\nBSN [26], BMN [25] and BSN++ [38] directly predict the probabil-\nity of boundaries and cover exible temporal duration of action\ninstances. Also, the ideas of GCN have been adopted in some works,\nG-TAD [53] extract temporal context and semantic context by GCN,\nbut applies conventional temporal convolutional layers for actual\nimplementation in temporal context extraction. They can generate\nproposals with more exible durations and more precise bound-\naries. MGG [27] and TCANet [32] combines a boundary-based and\nanchor-based advantages to generate proposal. However, they have\nlimited capability of modeling dependencies from the local context,\nTemporal Action Proposal Generation with Transformers Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nLÃ—NÃ—C\nFeature Extractor\nFlatten   LÃ— (NÃ—C)\nFuzzy Matching\nscore:0.47\nscore:0.11\nscore:0.92\nscore:0.57\nscore:0.29\nDecoderK, V\nProposal Head\nEncoder\nPost Processing\nTÃ—C\nstart end\nGround truth\nscore:0.97\nscore:0.89\nscore:0.85\nDecoderK, V\nBoundary Head\nEncoder\nSliding Window\nQ\nQK, Q, V\nK, Q, V\nSparse Sampling\nBoundary Transformer\nProposal Transformer\nD\nT\nFigure 2: Illustration of the proposed TAPG Transformer framework. First we apply the feature extractor to encode video\nframes. The Boundary Transformer takes the feature sequence as input, and outputs boundary probability sequence. For\nProposal Transformer, we apply sliding window and sparse sampling on the feature sequence to generate sparse multi-scale\nproposals, which are then fed into Proposal Transformer to generate condence sequence. Finally, we construct proposals\nbased on boundary probabilities sequence and obtain the corresponding condence score from condence sequence.\nleading to performance penalty in the complex cases. In our work,\nwe aim to make fuller use of inter-frame relation and inter-proposal\nrelation for boundary prediction and the proposal-level condence\nevaluation, respectively.\n2.3 Transformers in computer vision\nTransformer architectures are based on a self-attention mecha-\nnism that learns the relationships between elements of a sequence.\nThe breakthroughs from Transformer networks [43] in language\ndomain has sparked great interest in the computer vision com-\nmunity to adapt these models for vision. ViT [9] proposes to use\nNLP Transformer encoder directly on image patches. In order to\napply Transformer model, DETR [ 5] treats object detection as a\nset prediction problem. Transformers are also adopted for Super\nresolution in [55], Image Colorization in [21], Tracking in [8, 54, 58],\nPose estimation in [29], etc. Besides, for video understanding, there\nare also recent approaches seek to resolve this challenge using the\nTransformer networks. Video Transformer Network (VTN) that rst\nobtain frame-wise features using 2D CNN and apply a Transformer\nencoder (Longformer [30]) on top to learn temporal relationships.\nTimeSformer [1] presents a convolution-free approach to video clas-\nsication built exclusively on Transformers over space and time.\nOur work is most closely related to the works which apply Trans-\nformers for temporal action proposal generation. RTD-Net [ 39]\nadapts the Transformer-like architecture for action proposal gener-\nation in videos. However, there are substantial dierences between\nRTD-Net and our method. RTD-Net uses decoder-only Transformer\nto exploit the global temporal context and devises a three-branch\ndetection head for training and inference. On the contrary, in our\npaper we propose to adopt two original Transformers (Boundary\nTransformer, Proposal Transformer) to capture both inter-frame\nand inter-proposal relationships for precise boundary prediction\nand proposal condence evaluation, respectively. On both two chal-\nlenge benchmarks (i.e., THUMOS14 and ActivityNet13), our method\noutperforms RTD-Net with a clear margin.\n3 APPROACH\nIn this section, rst, we present the problem denition for the tem-\nporal action proposal generation task, and then we describe the\nbasic Transformer architecture [43] that applied attention mecha-\nnisms to make global dependencies in a sequence data. In Sec. 3.3,\nwe present an overview of our TAPG Transformer. The archi-\ntecture of TAPG Transformer is composed of two main parts: a\nBoundary Transformer (Sec. 3.4) and a Proposal Transformer\n(Sec. 3.5). Finally, the training and inference details of our TAPG\nTransformer are introduced in Sec 3.6 and Sec 3.7, respectively.\n3.1 Problem Denition\nAn untrimmed video ğ‘ˆ can be denoted as a frame sequence ğ‘ˆ =\n{ğ‘¢ğ‘¡}ğ‘™ğ‘£\nğ‘¡=1 with ğ‘™ğ‘£ frames, where ğ‘¢ğ‘¡ indicates the ğ‘¡-th RGB frame of\nvideo ğ‘ˆ. The temporal annotation set of ğ‘ˆ is composed by a set\nof temporal action instances as Î¨ğ‘” = {ğœ‘ğ‘”\nğ‘›}ğ‘ğ‘”\nğ‘›=1 and ğœ‘ğ‘”\nğ‘› = (ğ‘¡ğ‘ ğ‘› ğ‘Šğ‘¡ğ‘’ğ‘› ),\nwhere ğ‘ğ‘” is the number of ground-truth action instances, ğ‘¡ğ‘ ğ‘› and\nğ‘¡ğ‘’ğ‘› are the starting and ending time of the action instance ğœ™ğ‘”\nğ‘› re-\nspectively. During training phase, the Î¨ğ‘” is provided. While in the\ntesting phase, the predicted proposal set Î¨ğ‘ should cover the Î¨ğ‘”\nwith high recall and high temporal overlapping.\n3.2 Preliminary: General Transformer\nThe original Transformer [43] has an encoder-decoder structure.\nThe encoder is composed of six identical blocks, with each block\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lining Wang âˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie Huang\nhaving two sub-layers: a multi-head self-attention layer and a sim-\nple position-wise fully connected feed-forward layer. Similar to the\nencoder, thedecoder in the Transformer model consists of six iden-\ntical blocks. In addition to the two sub-layers in each encoder block,\nthe decoder inserts a third sub-layer, which performs multi-head\nattention on the output of the corresponding encoder block.\nA key feature of the Transformer architecture is the so-called\nself-attention mechanism, which explicitly models the interactions\nbetween all entities of a sequence for structured prediction tasks.\nFormally, lets denote a sequence of ğ‘›entities by ğ¹ âˆˆRğ‘›Ã—ğ‘‘, where\nğ‘‘ is the embedding dimension to represent each entity. The input\nsequence ğ¹ is projected onto three learnable linear transformations\nto get queriesğ‘„, keysğ¾, and valuesğ‘‰. Then, we compute the output\nof the self-attention mechanism as:\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„ğ‘Šğ¾ğ‘Šğ‘‰)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆš\nğ‘‘\n)Â·ğ‘‰ğ‘Œ (1)\n3.3 TAPG Transformer\nTemporal action proposal generation task is generally divided into\ntwo sub-task: boundary prediction and proposal condence eval-\nuation. The long-range frame-level dependencies are desirable in\nprecise boundary prediction. Moreover, modeling the rich inter-\nproposals relationships play a crucial role in condence regression.\nMotivated by the success of Transformer models in the language\ndomain, similar to how a Transformer operates on a sentence, it\ncould naturally be applied to a sequence of frames or proposals.\nTherefore, we propose to solve each task sequentially by a dierent\nTransformer network which enables capturing the long-term in-\nformation and dependencies between sequence elements. Figure 2\nillustrates the architecture of our proposed framework, which called\nTAPG Transformer.\nGiven an untrimmed video ğ‘ˆ which contains ğ‘™ğ‘£ frames, we pro-\ncess the input video in a regular interval ğœ for reducing the compu-\ntational cost. Then we utilize the feature extractor to encode the\nvisual feature from video frames. We concatenate the output of\nthe last fully connected layer in the two-stream network [ 36] to\nform the feature sequence ğ¹ = {ğ‘“ğ‘–}ğ‘‡\nğ‘–=1, where ğ‘‡ = ğ‘™ğ‘£/ğœ. Finally, the\nfeature sequence ğ¹ is used as the input of our framework.\nOur model mainly contains two main modules:Boundary Trans-\nformer and Proposal Transformer. Boundary Transformer aims\nto capture the long-range dependencies between the frame-level\nfeatures then output the boundary probabilities sequence. Similar to\nthe Boundary Transformer which learns the relationships between\nframes, we propose a Proposal Transformer to model the relation\nbetween proposals with dierent scales and evaluate the condence\nof proposals. Before performing inter-proposal relations, we need\nto generate the proposal features as the input to the Proposal Trans-\nformer. To do so, the previous works [25, 38] usually utilize dense\nsampling, which will generate a lot of redundant proposals and\ncause an imbalance data distribution between positive/negative\nproposals and temporal durations. Therefore, we propose a sparse\nsampling mechanism to generate the candidate proposals, then feed\nthem into the Proposal Transformer which encodes the potential\nrelationship between proposals, then output the corresponding\ncondence score from proposal condence sequence. We take the\nboundary probabilities and proposal condences as input to Fuzzy\nmatching mechanism then get the matching proposals. Followed\nby the post processing, we obtain the nal proposal set. We will\ndescribe the details of each module in the following sections.\n3.4 Boundary Transformer\nLong-term temporal dependency modeling is essential for bound-\nary regression. Previous methods usually conduct stacked temporal\nconvolutions [23, 25, 26] or global temporal pooling [14] to capture\ntemporal relationship. However, 1D temporal convolution can eec-\ntively capture local temporal relation between frames but lacks the\ncapacity of modeling long-term temporal structure limited by the\nkernel size. On the contrary, the global temporal pooling performs\naverage pooling to capture the global features over frames but fails\nto capture ner temporal structure and may introduce unnecessary\nnoise. To this end, we propose the Boundary Transformer to model\nboth local temporal relation and long-range temporal dependency\nstructure with a Transformer structure. The architecture of the\nTransformer is the same as the original Transformer model [43],\nwhich has been described in Sec. 3.2. We adopt a ğ‘€-layer Trans-\nformer and each layer has an encoder-decoder structure as follows:\nThe Encoder is applied to estimate the relevance of one frame\nto other frames. First, we obtain the frame-wise feature sequence\nğ¹ âˆˆRğ‘‡Ã—ğ¶ using two-stream model [ 36], where ğ‘‡ is the length\nof the feature sequence and ğ¶ is the feature dimension. Then we\nfeed them to the Transformer encoder and generate an augmented\nfeature sequence ğ¹ğ‘” âˆˆRğ‘‡Ã—ğ¶ with the global context. The encoder\nhas a standard architecture, which consists of a multi-head self-\nattention module and a feed forward network (FFN). The outputs\nğ¹ğ‘” of the encoder along with the embeddings are given as input to\nthe Transformer decoder.\nThe Decoder contains three sub-layers, rst two (multi-head\nself-attention and feed forward network) are similar to the encoder,\nwhile the third sub-layer performs multi-head attention over the\noutput of the encoder stack. The input to the rst self-attention\nmodule is the feature sequence ğ¹ âˆˆRğ‘‡Ã—ğ¶, which is the same as\nthat of the encoder. Then the module output the feature sequence\nğ¹ğ‘. In particular, for the second self-attention module, the ğ¾, ğ‘‰ are\ntransformed from ğ¹ğ‘” and ğ‘„ is transformed from ğ¹ğ‘. In this way,\nthe decoder further enhances relevant features while reducing the\nover-smoothing eect from the decoder.\nThe output representation of decoder is then used as the global\nrepresentation for the boundary prediction task. Specically, we\nappend a Boundary Head which encode the output representation\nof decoder with multi-layer perceptron (MLP) network and followed\nby a Sigmoid layer to generate the predicted boundary probability\nsequence.\n3.5 Proposal Transformer\nFor proposal condence prediction, previous works [25] rst gener-\nate densely distributed proposals then predict the condence scores\nof them. Obviously, dense proposals will result in imbalanced data\ndistribution between positive/negative proposals and more com-\nputational burdens. Meanwhile, the inter-proposal relationships\nmodeling is a previously overlooked point, which plays a crucial\nrole in proposal condence evaluation. To relieve these issues, we\nTemporal Action Proposal Generation with Transformers Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\npropose a sparse sampling mechanism and a Proposal Transformer\nas follows.\nThe Sparse sampling mechanism aims to maintain the sparse\nproposal sequence. Specically, we use the Fibonacci sequence as a\nsliding window group ğ‘Š = {ğ‘¤ğ‘–}ğ·\nğ‘–=1 to generate the proposals with\ndierent scales, where ğ· is the group size. The step size of each\nsliding window is computed as:\nğ‘ ğ‘¡ğ‘’ğ‘ = âŒŠâˆšğ‘¤ğ‘–âŒ‹+âŒŠğ‘¤ğ‘–\nğ›¾ âŒ‹ğ‘Š (2)\nwhereğ›¾is used to constraint the step size increases with the increase\nof the window size. For each proposal, we follow the BMN [ 25]\nand construct weight term ğ‘¤ğ‘–ğ‘Šğ‘—âˆˆRğ‘Ã—ğ‘‡ via uniformly sampling ğ‘\npoints between the temporal region. Then we conduct dot product\nin temporal dimension between ğ‘¤ğ‘–ğ‘Šğ‘—âˆˆRğ‘Ã—ğ‘‡ and ğ¹ âˆˆRğ‘‡Ã—ğ¶ to\ngenerate the proposal feature with the shapeğ¶Ã—ğ‘. Finally, we get\nthe sparse proposal feature sequence ğ‘ƒ âˆˆRğ¿Ã—ğ‘† and these proposal\nfeatures are subsequently attened, where ğ¿is number of proposal\nand ğ‘† = ğ¶Ã—ğ‘.\nMoreover, we propose the Proposal Transformer to enhance the\nproposal feature representations for proposal condence prediction.\nWe adopt a ğ‘€-layer Transformer, which is based upon the encoder\nand the decoder in original Transformer model [43]. The Encoder\naims to capture the relationships between proposals in various\nscales. The sparse proposal feature sequence ğ‘ƒ âˆˆRğ¿Ã—ğ‘† is given as\ninput to the encoder. Then the encoder outputs the feature sequence\nğ‘ƒğ‘” with the global view. Similar to the Boundary Transformer, the\ndecoder takes inputs from the encoder as well as the previous out-\nputs to generate enhanced proposal feature sequence. Taking the\nproposal sequence from the decoder output, our Proposal Con-\ndence Generator will generate two types of condence ğ¶ğ‘, ğ¶ğ‘Ÿ\nâˆˆRğ¿Ã—1 with Sigmoid activation, where ğ¶ğ‘, ğ¶ğ‘Ÿ are used for binary\nclassication and regression loss function separately.\n3.6 Training\nThe overall objective of our framework is dened as:\nL= Lğ‘ +Lğ‘ğ‘Š (3)\nwhere Lğ‘ and Lğ‘ are the objective functions of the Boundary\nTransformer and the Proposal Transformer respectively.\nObjective of Boundary Transformer. The Boundary Trans-\nformer generates the starting and ending probability sequence ğ‘ƒğ‘ ,\nğ‘ƒğ‘’. We can dene training objective of the Boundary Transformer\nas a two-task loss function. The loss function consists of starting\nloss and ending loss:\nLğ‘ = Lğ‘ğ‘™(ğ‘ƒğ‘ ğ‘Šğºğ‘ )+Lğ‘ğ‘™(ğ‘ƒğ‘’ğ‘Šğºğ‘’)ğ‘Š (4)\nwhere ğºğ‘  and ğºğ‘’ are the ground truth labels of boundary sequence,\nand Lğ‘ğ‘™ is the binary logistic regression loss function.\nObjective of Proposal Transformer. The Proposal Transformer\ngenerates sparse proposal condence sequence ğ¶ğ‘ and ğ¶ğ‘Ÿ. The loss\nfunction Lğ‘ consists of binary classication loss and regression\nloss:\nLğ‘ = Lğ‘(ğ¶ğ‘ğ‘Šğºğ‘)+Lğ‘Ÿ(ğ¶ğ‘Ÿğ‘Šğºğ‘)ğ‘Š (5)\nwhere Lğ‘ is a binary logistic regression loss function,ğ¿ğ‘Ÿ is a smooth\nğ¿1 loss, andğºğ‘ is the ground truth label of sparse proposal sequence.\nThe details of label assignment will be described in the supplemen-\ntary material.\n3.7 Inference\nAs mentioned above, the Boundary Transformer generates bound-\nary probability sequence and the Proposal Transformer generates\nthe sparse proposal condence sequence. Then we construct a\ncoarse proposals set ğœ“ğ‘ğ‘ based on boundary probabilities. Take the\nproposal ğœ‘ = [ğ‘¡â€²ğ‘ ğ‘Šğ‘¡â€²ğ‘’]âˆˆğœ“ğ‘ğ‘ as an example, Fuzzy matching is de-\nvised to compute tIoU (temporal Intersection over Union) between\nproposal ğœ‘ and sparse proposal sequence, then select matching\nproposal ğ‘ğ‘š = [ğ‘¡ğ‘šğ‘  ğ‘Šğ‘¡ğ‘šğ‘’ ]. Next, we rene the proposal as:\n[ğ‘¡ğ‘ ğ‘Šğ‘¡ğ‘’]=\n{\n[ğ‘¡â€²\nğ‘  +ğ‘¡ğ‘š\nğ‘ \n2 ğ‘Šğ‘¡â€²\nğ‘’ +ğ‘¡ğ‘š\nğ‘’\n2 ]ğ‘Š ğ‘–ğ‘“ ğ‘ğ‘ğ‘ğ‘š ğ‘œ ğ›¼1 ğ‘ğ‘›ğ‘‘ ğ‘ğ‘ğ‘Ÿğ‘š ğ‘œ ğ›¼2\n[ğ‘¡â€²ğ‘ ğ‘Šğ‘¡â€²ğ‘’]ğ‘Š ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘ \nğ‘Š (6)\nwhere ğ‘ğ‘ğ‘ğ‘š is the proposal classication score, ğ‘ğ‘ğ‘Ÿğ‘š is the regression\ncondence, ğ›¼1 and ğ›¼2 are the adjustment thresholds. Finally, we\nget a proposal set ğœ“ğ‘ = {ğœ™ğ‘› = (ğ‘¡ğ‘ ğ‘Šğ‘¡ğ‘’ğ‘Šğ‘ğ‘ \nğ‘¡â€²ğ‘ \nğ‘Šğ‘ğ‘’\nğ‘¡â€²ğ‘’\nğ‘Šğ‘ğ‘ğ‘ğ‘šğ‘Šğ‘ğ‘ğ‘Ÿğ‘š)}ğ‘\nğ‘›=1, where\nğ‘ğ‘ \nğ‘¡â€²ğ‘ \nğ‘Šğ‘ğ‘’\nğ‘¡â€²ğ‘’\nare the starting and ending probabilities.\nFollowing the previous practices [25, 26], we also perform score\nfusion and redundant proposal suppression to further obtain nal\nresults. Specically, in order to make full use of various predicted\nscores for each proposal ğœ‘ğ‘›, we fuse its boundary probabilities\nand condence scores of matching proposal by multiplication. The\ncondence score ğ‘ğ‘“ can be dened as :\nğ‘ğ‘“ = ğ‘ğ‘ \nğ‘¡â€²ğ‘ \nÂ·ğ‘ğ‘ \nğ‘¡â€²ğ‘’\nÂ·ğ‘ğ‘ğ‘\nğ‘š Â·ğ‘ğ‘ğ‘Ÿ\nğ‘šğ‘Œ (7)\nHence, the nal proposal set as\nğœ“ = {ğœ‘ğ‘› = (ğ‘¡ğ‘ ğ‘Šğ‘¡ğ‘’ğ‘Šğ‘ğ‘“)}\nğ‘\nğ‘›=1ğ‘Œ (8)\nMoreover, we use the Soft-NMS [2] algorithm for Post-processing\nto remove the proposals which highly overlap with each other.\n4 EXPERIMENTS\n4.1 Datasets and Evaluation Metrics\nWe evaluate our method on two challenge benchmarks:ActivityNet-\nv1.3 [4] and THUMOS14 [19]. ActivityNet-v1.3 is a large-scale\nvideo dataset for action recognition and temporal action detection\ntasks. It contains 10K training, 5k validation, and 5k testing videos\nwith 200 action categories, and the ratio of training, validation and\ntesting sets is 2:1:1. Our models trained on the training set and eval-\nuated with the validation set. For THUMOS14, we use the subset\nof THUMOS14 that provides frame-wise action annotations. The\nmodel is trained with 200 untrimmed videos from its validation set\nand evaluated using 213 untrimmed videos from the test set. This\ndataset is challenging due to the large variations of the frequency\nand duration of action instances across videos.\nEvaluation Metrics. Temporal action proposal generation aims\nto produce high quality proposals that have high IoU with ground\ntruth action instances and high recall. To evaluate quality of pro-\nposals, Average Recall (AR) is the average recall rate under speci-\ned tIoU thresholds for measuring the quality of proposals. Activi-\ntyNetv1.3, these thresholds are set to [0.5:0.05:0.95]. On THUMOS14,\nthey are set to [0.5:0.05:1.0]. By limiting the average number (AN)\nof proposals for each video , we can calculate the area under the\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lining Wang âˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie Huang\nAR vs. AN curve to obtain AUC. We useAR@AN and AUC as our\nmetrics to evaluate TAPG models. For the temporal action detec-\ntion task, mean Average Precision (mAP) under multiple tIoU is\nthe widely used evaluation metric. On ActivityNet-v1.3, the tIoU\nthresholds are set to {0.5, 0.75, 0.95}, and we also test the average\nmAP of tIoU thresholds between 0.5 and 0.95 with step of 0.05. On\nTHUMOS14, these tIoU thresholds are set to {0.3, 0.4, 0.5, 0.6, 0.7 }.\n4.2 Implementation Details.\nFor feature extractor, following previous works [25, 53], we adopt\nthe two-stream network [36], where ResNet [16] and BN-Inception\nnetwork [18] are used as the spatial and temporal networks respec-\ntively. During feature extraction, the interval ğœ is set to 16 and 5\non ActivityNet-1.3 and THUMOS14 respectively. For ActivityNet-\nv1.3, each video feature sequence is rescaled to ğ‘‡ = 100 snippets\nusing linear interpolation. For THUMOS14, we crop each video\nfeature sequence with overlapped windows of size ğ‘‡ = 256 and\nstride 128. Besides, for the sparse sampling mechanism, the length\nof sparse proposal sequence ğ¿is 451 and 1225 on ActivityNet-1.3\nand THUMOS14, the max sliding window size ğ· is 100 and 64 for\nActivityNet-v1.3 and THUMOS14, respectively, and the constraint\nhyper-parameter ğ›¾ is 21. For each proposal, we sampling points\nğ‘ = 32. For Fuzzy matching, we set adjustment thresholdsğ›¼1 = 0ğ‘Œ9\nand ğ›¼2 = 0ğ‘Œ8. Due to the limit of computation resource, we apply\n1D Conv for dimension reduction, then take the features as the\ninput to the Boundary Transformer and Proposal Transformer. For\ntemporal action detection task, following [53], we take the video\nclassication scores from [51] and [44] and multiply them to yield\nthe fused condence score ğ‘ğ‘“ for evaluation. We train our TAPG\nTransformer from scratch using the Adam optimizer and the learn-\ning rate is set to 10âˆ’4 and decayed by a factor of 0.1 after every 10\nepoch. To preserve the positional information in the transformer,\nwe adopt sine positional encoding for queries and keys. The number\nof Transformer layers ğ‘€ = 3.\n4.3 Comparison with State-of-the-art Results\nIn this subsection, we compare our method with the existing state-\nof-the-art methods on ActivityNet-v1.3 and THUMOS14. For a\nfair comparison, we adopt the same two-stream features used by\nprevious methods in our experiments.\nTable 1: Performance comparison with state-of-the-art pro-\nposal generation methods on test set of THUMOS14 in terms\nof AR@AN.\nMethod @50 @100 @200 @500 @1000\nTAG [57] 18.6 29.0 39.6 - -\nCTAP [13] 32.5 42.6 52.0 - -\nBSN [26] 37.5 46.1 53.2 61.4 65.1\nMGG [27] 39.9 47.8 54.7 61.4 64.6\nBMN [25] 39.4 47.7 54.8 62.2 65.5\nBSN++ [38] 42.4 49.8 57.6 65.2 66.8\nTCANet [32] 42.1 50.5 57.1 63.61 66.9\nRTD-Net [39] 41.1 49.0 56.1 62.9 -\nOurs 43.9 53.5 60.2 66.7 68.8\nTable 2: Performance comparison with state-of-the-art ac-\ntion detection methods on test set of THUMOS14, in terms\nof mAP (%) at dierent tIoU thresholds.\nMethod 0.3 0.4 0.5 0.6 0.7\nSST [3] - - 23.0 - -\nCDC [34] 40.1 23.0 - -\nTURN-TAP [15] 44.1 34.9 25.6 - -\nSSN [56] 51.9 41.0 29.8 - -\nBSN [26] 53.5 45.0 36.9 28.4 20.0\nTAL-Net [7] 53.2 48.5 42.8 33.8 20.8\nMGG [27] 53.9 46.8 37.4 29.5 21.3\nDBG [23] 57.8 49.4 39.8 30.2 21.7\nBMN [25] 56.0 47.4 38.8 29.7 20.5\nG-TAD[53] 54.5 47.6 40.2 30.8 23.4\nBSN++ [38] 59.9 49.5 41.3 31.9 22.8\nTCANet [32] 60.6 53.2 44.6 36.8 26.7\nRTD-Net [39] 53.9 48.9 42.0 33.9 23.4\nOurs 65.3 59.3 50.8 40.1 28.5\nResults on THUMOS14. We present performance comparison\nof the proposed TAPG Transformer with the state-of-the-art meth-\nods on THUMOS14 in Table 1 and Table 2, where our method\nimproves the performance signicantly for both temporal action\nproposal generation and action detection. For the temporal ac-\ntion proposal generation task, results are shown in Table 1, which\ndemonstrate that TAPG Transformer outperforms state-of-the-art\nmethods in terms of AR@AN with AN varying from 50 to 1000.\nFor the temporal action detection task, the proposed TAPG Trans-\nformer also achieves superior results, as shown in Table 2. The\nperformance of our method exceeds state-of-the-art proposal gen-\neration methods by a big margin at dierent tIoU thresholds. The\nhigh mAP reects that our model is able to predict the proposals\nwith high scores, and reduce the number of false positives.\nTable 3: Performance comparison with state-of-the-art pro-\nposal generation methods on validation set of ActivityNet-\n1.3 in terms of AUC and AR@AN.\nMethod AR@1 (val) AR@100 (val) AUC (val)\nCTAP [13] - 73.2 65.7\nBSN [26] 32.2 74.2 66.2\nMGG [27] - 75.5 66.4\nBMN [25] - 75.0 67.0\nBSN++ [38] 34.3 76.5 68.3\nTCANet [32] 34.6 76.1 68.1\nRTD-Net [39] 32.8 73.1 65.7\nOurs 34.9 76.5 68.3\nResults on ActivityNet-v1.3. Table 3 and Table 5 shows com-\nparison results of the proposed TAPG Transformer with other\nmethods on ActivityNet-v1.3. For the temporal action proposal\ngeneration task, as shown in Table 3, the performance of TAPG\nTransformer again outperforms state-of-the-art proposal genera-\ntion methods in terms of AR@AN with AN varying from 1 to 100\nand AUC. Especially when AN equals 1, we achieve 34.9% regard-\ning the AR metric, which indicates that top-1 proposal has high\nTemporal Action Proposal Generation with Transformers Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 4: Ablation study on THUMOS14. We show mAP (%) at dierent tIoU thresholds.\n(a) Study on dierent combinations of the components in\nTAPG Transformer and BMN. BTR denotes Boundary Trans-\nformer and PTR denotes Proposal Transformer.\nModel Module 0.3 0.4 0.5 0.6 0.7\nBMN TEM 41.9 36.4 27.8 21.6 12.7\nBMN TEM+PEM 56.0 47.4 38.8 29.7 20.5\nOurs BTR 43.8 37.7 29.9 20.8 12.9\nOurs BTR + PEM 60.6 54.5 46.3 37.6 27.1\nOurs TEM + PTR 62.9 57.6 47.6 37.5 25.3\nOurs BTR + PTR 65.3 59.3 50.8 40.1 28.5\n(b) Study on sparse sampling mechanism and dierent window groups.\n[start:stride:end] means that the sliding window size varies from start to\nend with stride.\nModel Number window group 0.3 0.4 0.5 0.6 0.7\nBMN 16384 Densely 56.0 47.4 38.8 29.7 20.5\nBMN 1225 Fibonacci 58.2 50.5 41.2 32.9 23.3\nOurs 800 [2:5:62] 63.5 57.6 49.8 40.5 29.4\nOurs 1218 [2:3:62] 64.6 58.5 50.0 40.4 28.1\nOurs 1726 [2:2:64] 65.2 59.1 50.8 41.0 28.8\nOurs 1225 Fibonacci 65.3 59.3 50.8 40.1 28.5\n(c) Study on dierent query forms of transformer decoder.\nQuery 0.3 0.4 0.5 0.6 0.7\nLearnable parameters 62.1 54.1 45.8 34.8 23.2\nEncoder output 64.2 57.4 48.7 39.1 26.8\nOriginal Feature 65.3 59.3 50.8 40.1 28.5\n(d) Study on dierent numbers of TAPG Transformer layers.\nM 0.3 0.4 0.5 0.6 0.7\n1 64.8 59.0 50.5 40.0 27.9\n3 65.3 59.3 50.8 40.1 28.5\n6 64.2 57.6 49.2 38.9 26.4\nTable 5: Performance comparison with state-of-the-art ac-\ntion detection methods on validation set of ActivityNet-1.3,\nin terms of mAP (%) at dierent tIoU thresholds and the av-\nerage mAP.\nMethod 0.5 0.75 0.95 Average\nSingh et al. [37] 34.5 - - -\nSCC [17] 40.0 17.9 4.7 21.7\nCDC [34] 45.3 26.0 0.20 23.8\nR-C3D [52] 26.8 - - -\nBSN [26] 46.5 30.0 8.0 30.0\nBMN [25] 50.1 34.8 8.3 33.9\nGTAD [53] 50.4 34.6 9.0 35.1\nBSN++ [38] 51.3 35.7 8.3 34.9\nTCANetw/ BSN [32] 51.9 34.9 7.5 34.4\nRTD-Net [39] 46.4 30.5 8.6 30.5\nOurs 53.2 35.5 10.1 35.4\nquality. For the temporal action detection task, as summarized in\nTable 5, our method achieves notable improvements on mAP over\nother proposal generation methods such as BSN [ 26], BMN [25]\nand G-TAD [53] at all tIoU thresholds. When tIoU is 0.95, the mAP\nwe obtain is 10.1%, indicating that the boundaries of the generated\nproposals by TAPG Transformer are more precise.\n4.4 Ablation Study\nIn this section, we conduct ablation studies on THUMOS14 to verify\nthe eectiveness of each module in TAPG Transformer.\nEectiveness of Transformers. We perform ablation studies\nunder dierent architecture settings and verify the eectiveness\nof the Boundary Transformer (BTR) and Proposal Transformer\n(PTR), separately. Table 4a demonstrates that both modules improve\nthe performance individually. Temporal Evaluation Module (TEM)\nand Proposal Evaluation Module (PEM) are two modules of BMN\n[25]. As can be seen, \"BTR + PEM\" outperforming \"TEM + PEM\"\nindicates that BTR achieves more accurate boundaries, as shown\nin the second and fourth rows of Table 4a, while \"TEM + PTR\"\noutperforming \"TEM + PEM\" demonstrates that PTR improves the\nproposal condence, as shown in the second and fth rows of\nTable 4a. To summarize, the evaluation results shown in Table 4a\ndemonstrate that: 1) Compared with the previous works which only\nexplore the local details, Boundary Transformer eectively captures\nthe long-term information for accurate boundary prediction. 2)\nInstead of treating the proposals separately, Proposal Transformer\neectively learns the relation of the proposals on dierent scales to\ngenerate high quality proposal condence.\nEectiveness of Sparse Sampling mechanism. We perform\nablation studies to verify the eectiveness of sparse proposal se-\nquence and dierent sampling methods. Generating dense BM map\nto predict proposal condence as done in [25] causes unbalanced\npositive/negative proposals and resource waste. As shown in the\nrst two rows of Table 4b, generating less proposals (about only\n7.5% of the BM maps) still achieves performance improvement.\nMeanwhile we design two ways to generate window group. The\nrst is using dierent window sizes with equal stride in each group.\nThe third to fth rows in Table 4b shows that generating sliding\nwindows with the stride set to 5, 3, and 2 all obtain good results.\nThe second is using dierent window sizes with dierent distances\nin each group, specically, we apply the Fibonacci sequence to gen-\nerate candidate proposals. Satisfying performance is achieved as\nshown in the last row of Table 4b. In summary, the proposed TAPG\nTransformer achieves better performance with fewer proposals.\nEectiveness of Query Form of Transformer Decoder. We\nperform ablation studies to explore the performance of Transformer\nDecoder with dierent query forms. Long-term modeling easily\nleads to the smooth boundary of action and introduces global\nnoise. To solve this problem, we design novel query forms of trans-\nformer decoder that are dierent from original Transformer. Specif-\nically, we study on three query forms including learnable param-\neters [5], encoder output and original feature. The experimental\nresults shown in Table 4c demonstrate that the query form of origi-\nnal feature achieves the best performance. We believe the reason\nis that introducing original feature helps eliminate the eects of\nsmooth boundary of action and the global noise.\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lining Wang âˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie Huang\nGroundtruth\nPrediction\nLongboardingBackground Background\n45.4s 158.8s\nLongboardingBackground Background\n42.5s 157.5s\nscore: 0.769\nGroundtruth\nPrediction\n15.4s 64.7s 90.9s 116.2s 140.7s 162.2s\nSkiing BackgroundBackgroundBackground Skiing Skiing\nSkiingBackground BackgroundSkiingBackground BackgroundSkiing\n16.7s 65.1s 93.0s 115.3s 143.2s 161.8s\nscore: 0.675 score: 0.669 score: 0.658\nGroundtruth\nPrediction\nPlaying violinBackground BackgroundBackground Playing violin\n8.5s 75.4s 92.2s 172.2s\nPlaying violinBackground BackgroundPlaying violin\nscore: 0.693 score: 0.704\n76.6s9.5s 90.6s 172.0s\nBackground\nBackground\nFigure 3: Visualization examples of generated proposals on ActivityNet1.3.\nEectiveness of Layers of Transformer. Stacking multi-layer\ntransformers normally brings better performance, which has been\nproved in multiple tasks, e.g., NLP [43] and object detection [ 5].\nWe study on the dierent number of the stack of multiple layers\nin Boundary Transformer and Proposal Transformer. As shown in\nTable 4d, we achieve the best performance when stacking three\nTransformer modules. However, stacking an increasing number of\nlayers does not necessarily lead to performance improvement, and\nthe stack of excessive layers may lead to over-tting as well, as\nshown in the last row of Table 4d.\n4.5 Visualization\nFigure 3 presents visualization examples of some representative and\nchallenging cases on ActivityNet1.3. The proposals with the highest\nğ‘˜ scores are visualized in each video, where ğ‘˜ is the number of\nground truth. In the top video, although there exist scenes of falling\nand the rst-person perspective, our method predicts the video\ncorrectly as a continuous action. The correct prediction shows that\nour method understands the inter-frame relation and inter-proposal\nrelation properly and does not overt the noisy frames. The middle\nvideo includes multiple ground truth action instances, while our\ntop-3 proposals match with them accurately, indicating the high\nquality of our generated proposals. In the bottom video, where\nthe background frames and action frames are of a similar scene,\nour top-2 proposals are successfully aligned with the positions of\nground truth action instances. Our method considers the high-level\nsemantic information and can identify the noisy snippets even when\nthey have a very similar appearance to the action. More examples\nand comparison results with the previous methods are shown in\nthe supplementary material.\n5 CONCLUSION\nIn this paper, we propose a novel temporal action proposal gen-\neration framework with transformers, which consists a Boundary\nTransformer and a Proposal Transformer. Boundary Transformer\ncaptures long-term temporal context and Proposal Transformer\nTemporal Action Proposal Generation with Transformers Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nmodels inter-proposal relation. Our TAPG Transformer can eec-\ntively enhance video understanding and noisy action instance lo-\ncalization. Extensive experiments show that our model achieves\nnew state-of-the-art performance in temporal action proposal gen-\neration and action detection on THUMOS14 and ActivityNet1.3\ndatasets.\nREFERENCES\n[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is Space-Time\nAttention All You Need for Video Understanding?arXiv preprint arXiv:2102.05095\n(2021).\n[2] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. 2017. Soft-\nNMSâ€“improving object detection with one line of code. In Proceedings of the\nIEEE international conference on computer vision . 5561â€“5569.\n[3] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Car-\nlos Niebles. 2017. End-to-End, Single-Stream Temporal Action Detection in\nUntrimmed Videos. In BMVC 2017.\n[4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.\n2015. Activitynet: A large-scale video benchmark for human activity under-\nstanding. In Proceedings of the ieee conference on computer vision and pattern\nrecognition. 961â€“970.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer Vision . Springer, 213â€“229.\n[6] Joao Carreira and Andrew Zisserman. 2017. Quo Vadis, Action Recognition? A\nNew Model and the Kinetics Dataset. In CVPR.\n[7] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia\nDeng, and Rahul Sukthankar. 2018. Rethinking the faster r-cnn architecture for\ntemporal action localization. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition . 1130â€“1139.\n[8] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu.\n2021. Transformer Tracking. arXiv preprint arXiv:2103.15436 (2021).\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[10] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem.\n2016. Daps: Deep action proposals for action understanding. In European Con-\nference on Computer Vision . Springer, 768â€“784.\n[11] Christoph Feichtenhofer. 2020. X3D: Expanding Architectures for Ecient Video\nRecognition. In IEEE Conf. Comput. Vis. Pattern Recog. 203â€“213.\n[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-\nfast networks for video recognition. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 6202â€“6211.\n[13] Jiyang Gao, Kan Chen, and Ram Nevatia. 2018. Ctap: Complementary temporal\naction proposal generation. InProceedings of the European conference on computer\nvision (ECCV) . 68â€“83.\n[14] Jialin Gao, Zhixiang Shi, Jiani Li, Guanshuo Wang, Yufeng Yuan, Shiming Ge, and\nXi Zhou. 2020. Accurate Temporal Action Proposal Generation with Relation-\nAware Pyramid Network. arXiv preprint arXiv:2003.04145 (2020).\n[15] Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, and Ram Nevatia. 2017. Turn\ntap: Temporal unit regression network for temporal action proposals. In ICCV.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[17] F Caba Heilbron, Wayner Barrios, Victor Escorcia, and Bernard Ghanem. 2017.\nSCC: Semantic context cascade for ecient action detection. In CVPR.\n[18] Sergey Ioe and Christian Szegedy. 2015. Batch Normalization: Accelerating\nDeep Network Training by Reducing Internal Covariate Shift.\n[19] Yu-Gang Jiang, Jingen Liu, A Roshan Zamir, George Toderici, Ivan Laptev,\nMubarak Shah, and Rahul Sukthankar. 2014. THUMOS challenge: Action recog-\nnition with a large number of classes.\n[20] Bruno Korbar, Du Tran, and Lorenzo Torresani. 2019. Scsampler: Sampling\nsalient clips from video for ecient action recognition. In Int. Conf. Comput. Vis.\n6232â€“6242.\n[21] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. 2021. Colorization\ntransformer. arXiv preprint arXiv:2102.04432 (2021).\n[22] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. 2020.\nTEA: Temporal Excitation and Aggregation for Action Recognition. InIEEE Conf.\nComput. Vis. Pattern Recog. 909â€“918.\n[23] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui,\nChengjie Wang, Jilin Li, Feiyue Huang, and Rongrong Ji. 2020. Fast Learning\nof Temporal Action Proposal via Dense Boundary Generator.. In AAAI. 11499â€“\n11506.\n[24] Ji Lin, Chuang Gan, and Song Han. 2019. TSM: Temporal Shift Module for\nEcient Video Understanding. In Int. Conf. Comput. Vis.\n[25] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. Bmn: Boundary-\nmatching network for temporal action proposal generation. InICCV. 3889â€“3898.\n[26] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. 2018.\nBsn: Boundary sensitive network for temporal action proposal generation. In\nProceedings of the European Conference on Computer Vision (ECCV) . 3â€“19.\n[27] Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, and Shih-Fu Chang. 2019. Multi-\ngranularity generator for temporal action proposal. In CVPR. 3604â€“3613.\n[28] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie\nWang, Jilin Li, Feiyue Huang, and Tong Lu. 2020. TEINet: Towards an Ecient\nArchitecture for Video Recognition.. In AAAI. 11669â€“11676.\n[29] Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin\nWang. 2021. TFPose: Direct Human Pose Estimation with Transformers. arXiv\npreprint arXiv:2103.15320 (2021).\n[30] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. 2021. Video\ntransformer network. arXiv preprint arXiv:2102.00719 (2021).\n[31] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. 2014. The lear submission at\nthumos 2014. (2014).\n[32] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang, Wei Wu, Xiang Wang,\nYu Qiao, Junjie Yan, Changxin Gao, and Nong Sang. 2021. Temporal Context\nAggregation Network for Temporal Action Proposal Renement. arXiv preprint\narXiv:2103.13141 (2021).\n[33] Zhaofan Qiu, Ting Yao, and Tao Mei. 2017. Learning spatio-temporal representa-\ntion with pseudo-3d residual networks. In Int. Conf. Comput. Vis. 5533â€“5541.\n[34] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-\nFu Chang. 2017. CDC: convolutional-de-convolutional networks for precise\ntemporal action localization in untrimmed videos. In CVPR.\n[35] Zheng Shou, Dongang Wang, and Shih-Fu Chang. 2016. Temporal action local-\nization in untrimmed videos via multi-stage cnns. In CVPR.\n[36] Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolutional net-\nworks for action recognition in videos. In NIPS. 568â€“576.\n[37] Gurkirt Singh and Fabio Cuzzolin. 2016. Untrimmed video classication for activ-\nity detection: submission to activitynet challenge.arXiv preprint arXiv:1607.01979\n(2016).\n[38] Haisheng Su, Weihao Gan, Wei Wu, Junjie Yan, and Yu Qiao. 2020. BSN++:\nComplementary Boundary Regressor with Scale-Balanced Relation Modeling for\nTemporal Action Proposal Generation. arXiv preprint arXiv:2009.07641 (2020).\n[39] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. 2021. Relaxed Transformer\nDecoders for Direct Action Proposal Generation. arXiv preprint arXiv:2102.01894\n(2021).\n[40] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\n2015. Learning spatiotemporal features with 3d convolutional networks. In Int.\nConf. Comput. Vis.\n[41] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. 2019. Video classi-\ncation with channel-separated convolutional networks. In Int. Conf. Comput. Vis.\n5552â€“5561.\n[42] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar\nPaluri. 2018. A Closer Look at Spatiotemporal Convolutions for Action Recogni-\ntion. In IEEE Conf. Comput. Vis. Pattern Recog.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[44] Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. 2017. Untrimmednets\nfor weakly supervised action recognition and detection. In Proceedings of the\nIEEE conference on Computer Vision and Pattern Recognition . 4325â€“4334.\n[45] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and\nLuc Van Gool. 2016. Temporal segment networks: Towards good practices for\ndeep action recognition. In European conference on computer vision . Springer,\n20â€“36.\n[46] Wenhao Wu, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, and Errui Ding.\n2021. MVFNet: Multi-View Fusion Network for Ecient Video Recognition. In\nAAAI.\n[47] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei Wen. 2019. Multi-\nAgent Reinforcement Learning Based Frame Sampling for Eective Untrimmed\nVideo Recognition. In Int. Conf. Comput. Vis.\n[48] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, and Shilei Wen.\n2020. Dynamic Inference: A New Approach Toward Ecient Video Action\nRecognition. In Proceedings of CVPR Workshops . 676â€“677.\n[49] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry S Davis.\n2019. Adaframe: Adaptive frame selection for fast video recognition. In IEEE\nConf. Comput. Vis. Pattern Recog. 1278â€“1287.\n[50] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. 2018.\nRethinking spatiotemporal feature learning: Speed-accuracy trade-os in video\nclassication. In Eur. Conf. Comput. Vis.\n[51] Yuanjun Xiong, Limin Wang, Zhe Wang, Bowen Zhang, Hang Song, Wei Li,\nDahua Lin, Yu Qiao, Luc Van Gool, and Xiaoou Tang. 2016. Cuhk & ethz & siat\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Lining Wang âˆ—, Haosen Yangâˆ—, Wenhao Wuâˆ—, Hongxun Yaoâ€ , and Hujie Huang\nsubmission to activitynet challenge 2016. arXiv preprint arXiv:1608.00797 (2016).\n[52] Huijuan Xu, Abir Das, and Kate Saenko. 2017. R-c3d: Region convolutional 3d\nnetwork for temporal activity detection. In ICCV.\n[53] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and Bernard Ghanem.\n2020. G-TAD: Sub-Graph Localization for Temporal Action Detection. In CVPR.\n10156â€“10165.\n[54] Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and\nXavier Alameda-Pineda. 2021. TransCenter: Transformers with Dense Queries\nfor Multiple-Object Tracking. arXiv preprint arXiv:2103.15145 (2021).\n[55] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. 2020. Learn-\ning texture transformer network for image super-resolution. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5791â€“5800.\n[56] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol\nVinyals, Rajat Monga, and George Toderici. 2015. Beyond short snippets: Deep\nnetworks for video classication. InProceedings of the IEEE conference on computer\nvision and pattern recognition . 4694â€“4702.\n[57] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua\nLin. 2017. Temporal action detection with structured segment networks. InICCV.\n2914â€“2923.\n[58] Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drummond,\nand Hamid Rezatoghi. 2021. Looking Beyond Two Frames: End-to-End Multi-\nObject Tracking Using Spatial and Temporal Transformers. arXiv preprint\narXiv:2103.14829 (2021).\nSupplementary Material for\nTemporal Action Proposal Generation with Transformers\nGround\ntruth\nOurs\nBMN\nOurs\nBMN\nFigure 1: Visualization examples of starting and ending probability sequences on ActivityNet1.3.\n1 LABEL ASSIGNMENT\nThe method of label assignment in each loss functions are described\nin detail below. For Boundary Transformer, we need to generate\ntemporal boundary label sequence ğºğ‘ ğ‘Šğºğ‘’ âˆˆğ‘…ğ‘‡ Following [? ], for a\nground-truth action instanceğœ‘ğ‘” = (ğ‘¡ğ‘ ğ‘Šğ‘¡ğ‘’)with duration ğ‘‘ğ‘” = ğ‘¡ğ‘’âˆ’ğ‘¡ğ‘ \nin annotation set Î¨ğ‘¤, we denote its starting and ending regions\nas ğ‘Ÿğ‘† = [ğ‘¡ğ‘  âˆ’ğ‘‘ğ‘”/10ğ‘Šğ‘¡ğ‘  +ğ‘‘ğ‘”/10]andğ‘Ÿğ¸ = [ğ‘¡ğ‘’ âˆ’ğ‘‘ğ‘”/10ğ‘Šğ‘¡ğ‘’ +ğ‘‘ğ‘”/10]\nseparately. Then, for a temporal locationğ‘¡ğ‘› within ğ¹ğ‘¤, we denote its\nlocal region asğ‘Ÿğ‘¡ğ‘› = [ğ‘¡ğ‘›âˆ’ğ‘‘ğ‘“/2ğ‘Šğ‘¡ğ‘›+ğ‘‘ğ‘“/2], whereğ‘‘ğ‘“ = ğ‘¡ğ‘›âˆ’ğ‘¡ğ‘›âˆ’1 is the\ntemporal interval between two locations. Then we calculate overlap\nratio ğ¼ğ‘œğ‘… of ğ‘Ÿğ‘¡ğ‘› with ğ‘Ÿğ‘† and ğ‘Ÿğ¸ separately, and denote maximum\nğ¼ğ‘œğ‘… as ğ‘”ğ‘ \nğ‘¡ğ‘› and ğ‘”ğ‘’\nğ‘¡ğ‘› separately, where ğ¼ğ‘œğ‘… is dened as the overlap\nratio with ground truth proportional to the duration of this region.\nThus we can generate ğºğ‘†ğ‘Šğ‘¤= {ğ‘”ğ‘ \nğ‘¡ğ‘› }ğ‘™ğ‘¤\nğ‘›=1 and ğºğ¸ğ‘Šğ‘¤= {ğ‘”ğ‘’\nğ‘¡ğ‘› }ğ‘™ğ‘¤\nğ‘›=1 as\nlabel of Boundary Transformer. For Proposal Transformer, we need\nto generate Sparse Proposal sequencelabel ğºğ¶ âˆˆğ‘…ğ¿, where ğ¿is\nnumber of sparse proposals. For a proposal ğœ‘ğ‘– âˆˆğ‘ƒ, we calculate its\nIntersection-over-Union (ğ¼ğ‘œğ‘ˆ)with all ğœ‘ğ‘” in Î¨ğ‘¤, and denote the\nmaximum ğ¼ğ‘œğ‘ˆ as ğ‘”ğ‘\nğ‘–. Thus we can generate ğºğ‘ = {ğ‘”ğ‘\nğ‘–}ğ¿\nğ‘–=1 as label\nof Proposal Transformer.\n2 MORE VISUALIZATION EXAMPLES\nFigure 1 illustrates two visualization examples and comparison\nwith classical method BMN [ ? ]. The example on the left shows\nthat the BTR module can signicantly reduce the adverse eects of\nboundary noise. The other example indicates that BTR can enhance\nthe boundary awareness. These two remarkable advantages ben-\net from the ability of BTR to capture the long-term frame-level\ndependencies.\nFigure 2 illustrates more visualization examples on ActivityNet1.3.\nThe generated proposals of our method with the highestğ‘˜scores are\nvisualized in each video, whereğ‘˜is the number of ground truth. The\nscenarios of all examples are relatively complicated. In the rst and\nsecond videos which contain complex action scenes, our method\npredicts the various scenes correctly as a complete action. For in-\nstance, the action of wrapping presents contains various scenes\nincluding putting the present in the wrapping paper, packing the\npresent and so on. The correct predictions show that our method\nunderstands the inter-frame relation and inter-proposal relation\nproperly. In the third and fourth videos, where the background\nframes and action frames are of the similar scene, our top-2 pro-\nposals are successfully aligned with the positions of ground truth\naction instances. Our method is able to identify the noisy snippets\neven when they have very similar appearance to the action. In\nthe bottom video, which contains complex background scenes, our\nmodel correctly understands the semantic information and identify\nthe true action. For instance, the background of the platform diving\naction contains various scenes including audience cheering, the\ndiving platform scene without competition going on and so on.\narXiv:2105.12043v1  [cs.CV]  25 May 2021\nGroundtruth\nPrediction\nWaterskiingBackground Background\n13.1s 112.1s\nWaterskiingBackground Background\n13.5s 110.2s\nscore: 0.953\nscore: 0.639\n41.5s 43.7sGroundtruth\nPrediction\nWrapping presentsBackground Background\n35.9s 54.4s\nWrapping presentsBackground Background\n36.4s 41.5s\nscore: 0.687\n3r\nWrapping presents\nWrapping presents\n43.5s 55.5s\nGroundtruth\nPrediction\nPlatform divingBackground Background\n9.3s 61.5s\nPlatform divingBackground\n8.8s 60.9s\nscore: 0.706\n3r\nPlatform diving\n68.1s\nPlatform diving\n68.1s\nBackground\n22.3s\n22.4s\nscore: 0.683\nGroundtruth\nPrediction\nHitting a pinataBackground Background\n5.6s 17.4s\nHitting a pinataBackground\n5.3s 17.5s\nscore: 0.645\n3r\nHitting a pinata\n22.9s\nHitting a pinata\n23.3s\nBackground\n13.2s\nscore: 0.681\n13.0s\n21.5s\n21.4s\nGroundtruth\nPrediction\nWashing faceBackground Background\n18.0s 31.1s\nWashing faceBackground\n16.8s 30.4s\nscore: 0.766\n3r\nWashing face\n37.1s\nWashing face\n37.3s\nBackground\nscore: 0.753\nBackground\nBackground\nBackground\nBackground\nFigure 2: Visualization examples of generated proposals on ActivityNet1.3.\n2",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7744286060333252
    },
    {
      "name": "Computer science",
      "score": 0.7107875347137451
    },
    {
      "name": "Granularity",
      "score": 0.607477605342865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44420239329338074
    },
    {
      "name": "Classifier (UML)",
      "score": 0.41057834029197693
    },
    {
      "name": "Engineering",
      "score": 0.14444324374198914
    },
    {
      "name": "Voltage",
      "score": 0.10734143853187561
    },
    {
      "name": "Programming language",
      "score": 0.10728055238723755
    },
    {
      "name": "Electrical engineering",
      "score": 0.10712847113609314
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1321014770",
      "name": "Association for Computing Machinery",
      "country": "US"
    }
  ],
  "cited_by": 16
}