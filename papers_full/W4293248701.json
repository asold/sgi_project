{
    "title": "The Role of Complex NLP in Transformers for Text Ranking",
    "url": "https://openalex.org/W4293248701",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5075484241",
            "name": "David Rau",
            "affiliations": [
                "Amsterdam University of the Arts",
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A5044511901",
            "name": "Jaap Kamps",
            "affiliations": [
                "Amsterdam University of the Arts",
                "University of Amsterdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2032083037",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W2792105013",
        "https://openalex.org/W3150521056",
        "https://openalex.org/W4294969216",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W3136109765",
        "https://openalex.org/W3095156104",
        "https://openalex.org/W3185146124",
        "https://openalex.org/W3174169056",
        "https://openalex.org/W4225362522",
        "https://openalex.org/W2927996276",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W2140354722",
        "https://openalex.org/W4300427681",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W1622302876",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2568985560",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2942810103",
        "https://openalex.org/W3118608099",
        "https://openalex.org/W3015612646",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3121309507",
        "https://openalex.org/W3175606037",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W11171803",
        "https://openalex.org/W4231488796",
        "https://openalex.org/W2951534261"
    ],
    "abstract": "Even though term-based methods such as BM25 provide strong baselines in\\nranking, under certain conditions they are dominated by large pre-trained\\nmasked language models (MLMs) such as BERT. To date, the source of their\\neffectiveness remains unclear. Is it their ability to truly understand the\\nmeaning through modeling syntactic aspects? We answer this by manipulating the\\ninput order and position information in a way that destroys the natural\\nsequence order of query and passage and shows that the model still achieves\\ncomparable performance. Overall, our results highlight that syntactic aspects\\ndo not play a critical role in the effectiveness of re-ranking with BERT. We\\npoint to other mechanisms such as query-passage cross-attention and richer\\nembeddings that capture word meanings based on aggregated context regardless of\\nthe word order for being the main attributions for its superior performance.\\n",
    "full_text": "The Role of Complex NLP in Transformers for Text Ranking?\nDavid Rau\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nJaap Kamps\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nABSTRACT\nEven though term-based methods such as BM25 provide strong\nbaselines in ranking, under certain conditions they are dominated\nby large pre-trained masked language models (MLMs) such as BERT.\nTo date, the source of their effectiveness remains unclear. Is it their\nability to truly understand the meaning through modeling syntactic\naspects? We answer this by manipulating the input order and posi-\ntion information in a way that destroys the natural sequence order\nof query and passage and shows that the model still achieves com-\nparable performance. Overall, our results highlight that syntactic\naspects do not play a critical role in the effectiveness of re-ranking\nwith BERT. We point to other mechanisms such as query-passage\ncross-attention and richer embeddings that capture word meanings\nbased on aggregated context regardless of the word order for being\nthe main attributions for its superior performance.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíLearning to rank .\nKEYWORDS\nNLP in Ranking, Analysis, Neural Re-Ranking, Transformers, Neu-\nral Bag-of-Words\nACM Reference Format:\nDavid Rau and Jaap Kamps. 2022. The Role of Complex NLP in Trans-\nformers for Text Ranking?. In Proceedings of the 2022 ACM SIGIR Interna-\ntional Conference on the Theory of Information Retrieval (ICTIR ‚Äô22), July\n11‚Äì12, 2022, Madrid, Spain. ACM, New York, NY, USA, 8 pages. https:\n//doi.org/10.1145/3539813.3545144\n1 INTRODUCTION\nOriginating from the field of natural language processing (NLP)\nlarge-scale self-supervised training yields representations that are\nuseful for a wide range of tasks [6, 19, 29]. Specifically, pre-training\nlarge language models using masked language modeling (MLM)\nas proposed by [6] in BERT has become a standard procedure to\nachieve top performances on downstream tasks.\nWhile in the past many ideas coming from NLP did not lead\nto convincing improvements in information retrieval [1, 8, 25, 28],\nsomewhat surprisingly BERT did lead to the long-awaited jump in\nperformance (see also [14]). Nevertheless, its success comes with\nthe caveat of extremely complex models that are hard to interpret,\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain.\n¬© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9412-3/22/07. . . $15.00\nhttps://doi.org/10.1145/3539813.3545144\nOriginal Query what the best way to get clothes white\nOriginal Passage bleach is also the best way to get white clothes\nwhite again, and helps remove stubborn, older\nstains. choose warm water for moderately soiled,\nsynthetic blend white clothes wash white clothes\nin warm water if they‚Äôre moderately soiled, are\nlined, and if they‚Äôre made of synthetic fibers or\nnatural and synthetic blends.\nPredicted Rank: 1\nShuffled Query clothes white best get way what to the\nShuffled Passage stains ##lea moderately moderately fibers stub-\nborn blend blend synthetic synthetic synthetic\nwash lined helps remove soil soil choose clothes\nclothes clothes warm warm older natural ##ch\nwhite white white white water water best again\nget re re way ##ed ##ed made if if also or they they\nare ##s for is to in and and and of the b . . , , , , , ‚Äô ‚Äô\nPredicted Rank: 1\nTable 1: The same query (id: 1108651) and passage (id:\n8175412) in original and perturbed order. While breaking\nthe natural sequence order makes it meaningless to human\nreaders, a model trained on the perturbed sequence esti-\nmates its relevance correct. Example taken from the NIST\n2020 testset on MSMARCO.\nand therefore it is hard to pinpoint the source of their effectiveness.\nThese rankers typically comprise millions of parameters requiring\nmassive amounts of training data. Only with the arrival of the large-\nscale ranking dataset MS MARCO [2], did large MLMs find their\nsuccessful application in information retrieval.\nWith BERT‚Äôs representation of long sequences of input tokens,\nproviding the means to model syntactic aspects of the input, in-\nformation retrieval seems to shift towards being an NLP problem.\nTo recap basic linguistics, understanding natural language can be\ncomprehended in four hierarchical steps. The NLP pyramid (Fig-\nure 1) depicts the consecutive steps from bottom up. It starts on the\nword level with the morphology describing how words are formed\ndepending on their context (singular/plural, word inflection, etc. ).\nThe next step considers the relationship between multiple words\nbuilding the syntax of a sentence. Through the structure of a sen-\ntence, we can understand the function of words (parts-of-speech),\nidentify sentence boundaries, and understand the dependency be-\ntween words. With having an understanding of the syntax we can\nthen derive the semantics or ‚Äúmeaning\" of a sentence. As the last\nstep, pragmatics describes the higher level of semiotics and spans\nthe text as a whole. In case the sequence embeddings of BERT\nare capturing some of these higher levels, this could explain the\nobserved gains in ranking effectiveness observed in ranking tasks.\narXiv:2207.02522v1  [cs.CL]  6 Jul 2022\nICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain. D. Rau and J. Kamps\nMorphology\nSyntax\nSemantics\nPragmatics\nFigure 1: NLP pyramid. Four hierarchical steps to describe\nthe understanding of natural language.\nWe want to focus on the second step, the syntax, and observe\nthat here the order of words in the sentence is of essential impor-\ntance. In most cases altering the order changes the meaning or\neven destroys the grammar making the meaning of the sentence\nundefined. Applied to the ranking problem: consider the example\nquery and document in Table 1. For humans it is easy to identify\nthe passage in natural sequence order (top half) as relevant to the\nquery, however, with perturbed sequence order (bottom half) the\npassage becomes meaningless and therefore it is very hard if not\nimpossible to estimate its relevance.\nRemarkably, traditional rankers such as Query likelihood and\nBM25 pose such strong baselines by solely operating on bag-of-\nwords representations that disregard the sequence order and there-\nwith the syntax entirely. One of the key differences of BERT over\nthese lexical rankers is the ability to go beyond the word level\nand to be able to model phrase and sentence level contexts. While\nsuch large models could potentially gain deeper semantic and syn-\ntactic abstractions to understand the true meaning of documents\n[5, 13] it remains unclear whether they do so. Little is known about\nhow BERT estimates the relevance of a query-document pair; what\nfeatures are encoded and which of those are essential for its perfor-\nmance.\nOne possible explanation for the success of BERT is that it in-\ndeed learns to understand syntax. Applied to ranking, BERT could\npotentially build deep interactions between queries and documents\nthat allow uncovering complex relevance patterns bringing us one\nstep closer to the vision for future retrieval systems of Metzler et al.\n[17] in ‚ÄúMaking Domain Experts out of Dilettantes‚Äù. In contrast\nto this, another possible explanation could be that BERT lines up\nalongside other NLP techniques [4, 18, 21] exploiting the distribu-\ntional properties of natural language [11] by merely learning simple\nterm distributions.\nIn this paper, we overall aim to answer the question:\nHow much is modeling syntactic aspects contributing to the\nsuccess of BERT in information retrieval?\nWe organize our paper by answering the following research\nquestions:\nRQ1 Does the BERT Cross-Encoder Ranker perform well due to\nmodeling syntactic aspects?\nRQ2 What is the effect of removing position information on the\nranking performance?\nRQ3 Does the model inherently lose order sensitivity while fine-\ntuning on the ranking task?\nRQ4 How different are models that are trained without sequence\norder?\nRQ5 What role does modeling syntactic aspects play in Natural\nLanguage Understanding tasks?\nTo this end, we are manipulating the natural order, or how the\nmodel perceives order in different ways. More specifically, in Sec-\ntion 4 we conduct experiments manipulating the sequence order\nduring fine-tune training and fine-tuning evaluation. In Section\n5 we alter the BERT Cross-Encoder in a way that it can not per-\nceive sequence order anymore during fine-tuning (creating a true\nBOW-BERT). We later, in Section 6 analyze the models that have\nbeen rendered order invariant by comparing their latent represen-\ntations to the vanilla Cross-Encoder Ranker and the pre-trained\nBERT model. Finally, in Section 7 we evaluate our manipulations\non the basic NLP tasks using GLUE.\nWe find that for re-ranking with the BERT Cross-Encoder the\nsequence order does not play a critical role. Our main contribu-\ntion lies in showing that the superior performance of the BERT\nranker cannot be attributed to syntactic abstraction and a deeper\nunderstanding of language. We validate our findings on two tasks\n(re-ranking and NLU) and several datasets (MSMARCO, Robust04,\nand GLUE).\nFor reproducibility and to encourage further research in this\ndirection, we make our new order-invariant BOW-BERT model\navailable to the public under https://github.com/davidmrau/ictir22.\n2 RELATED WORK\nIn this section, We discuss related work from IR and NLP on probing\nlarge pre-trained transformers.\nDespite several efforts to open up the BERT ranker as a black-\nbox [3, 9, 23] the mechanisms within the model remain unclear.\nMore particular to ranking, the role of word-order in large MLMs\nduring inference has been studied recently [ 16, 24]. In previous\nwork, Ettinger [7] showed that BERT relies on the word order\nduring pre-training for the MLM task, finding evidence of the model\nencoding syntactic information in its representations. The necessity\nof doing so, however, seems to vary depending on the task. One\nof the earlier works in NLP showing that the input order does not\nplay a crucial role in solving natural language inference (SNLI) was\ncarried out by Parikh et al. [20] based on LSTMs.\nIn later work, [ 10, 22, 27] investigate the sensitivity of input\norder permutations during evaluation and find a varying order\nof sensitivity for some NLP down-stream tasks. Sinha et al . [27]\nshow that different transformer models perform well on permuted\ninput of natural language inference tasks. Gupta et al. [10] show the\nsame for RoBERTa [15] on several natural language understanding\ntasks (MNLI, QQP and SST-2). Pham et al . [22] conduct a more\ncomplete study suggesting that some tasks are more sensitive to the\ninput order than others based on the GLUE benchmark. However,\nrecent work signals that the role of word order is smaller than\nexpected on NLP tasks Sinha et al. [26]. They carry out a wide range\nThe Role of Complex NLP in Transformers for Text Ranking? ICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain.\nof experiments on order permutations on GLUE using RoBERTa.\nWang et al. [30] and Sinha et al. [26] study the effect of removing\nthe position information for a subset of the GLUE tasks.\nAs the work by Sinha et al. [26] was carried in parallel to ours, a\nsub-set of the experiments on GLUE show similarities to ours, with\na few differences: We additionally choose to break the sentence\nstructure deterministically by sorting the input by id and to carry\nout the experiments on all tasks of the GLUE benchmark. Further-\nmore, previous work has examined sequence order on a sentence\nlevel, whereas ranking operates on a passage/document level. In\nterms of text understanding this is posing quite another challenge.\nHence, compared to previous work, we are the first to examine the\nrole of syntactic aspects in ranking with large MLMs during learn-\ning the ranking task. Note that the analysis of the role of syntactic\nstructures in information retrieval is of particular importance, as\none of its main achievements was the development of effective ‚Äúbag\nof words‚Äù approaches over 50 years ago, and these models are still\npowering almost every single search index deployed in practice.\n3 EXPERIMENTAL SETUP\nIn this section, we detail the basic neural ranker and two test col-\nlections used in the experiments.\n3.1 Model\nFor our experiments we use a BERT Cross-Encoder (CE) which\nencodes both queries and passages at the same time. Given input:\nùíô ‚àà{[ùê∂ùêøùëÜ],ùëû1,...,ùëû ùëõ [ùëÜùê∏ùëÉ],ùëù1,...,ùëù ùëö,[ùëÜùê∏ùëÉ]},\nwhere ùëûrepresents query tokens and ùëù passage tokens, the activa-\ntions of the CLS token are fed to a binary classifier layer to classify\na passage as relevant or non-relevant; the relevance probability is\nthen used as a relevance score to re-rank the passages.\n3.2 Data\n3.2.1 MSMARCO. We conduct our ranking experiments on the\nTREC 2020 Deep Learning Track‚Äôs passage retrieval task on the MS\nMARCO dataset [2]. The average passage length is 56. For training,\nwe use the official MS MARCO training triplets. For evaluation, we\nuse the NIST 2020 judgments with the official top-1,000 runs. We\nevaluate using the metrics NDCG@10, MAP, and Recall@100.\n3.2.2 Robust04. TREC 2004 Robust Track is a news collection of\ndocuments with an average length of 254 words. We use this col-\nlection as a zero shot re-ranking test collection with no additional\ntraining on the dataset. We choose this additional dataset for docu-\nment retrieval due to its complete judgments allowing for unbiased\nevaluation. For evaluation we use the title queries 301-450 and\n601-700. BM25 with no stemming serves as a first stage ranker to\nretrieve the top-1,000 ranks for each topic. Evaluation is done using\nthe metrics NDCG@10, MAP, and Recall@100.\n3.3 Training Details\nWe follow the training scheme of Nogueira and Cho [19] unless\nstated differently. We train our BERT ranker from Huggingface‚Äôs [31]\nbert-base-uncased. For training we use batch size 64, maximum se-\nquence length 512, warm-up steps 1000, learning rate 3e-6, epoch\nsize 1000 and evaluate the best model within 40 epochs.\nTable 2: Performance of BM25 (without stemming and de-\nfault parameters) and the BERT Cross-Encoder (CE) re-\nranking the same BM25 ranking on the MSMARCO NIST\n2020 testset.\nmodel NDCG@10 MAP R@100\nBM25 47.96 28.56 55.99\nCE 68.95 45.08 68.07\n4 PERTURBING SEQUENCE ORDER\nIn this section, we conduct experiments to examine the role of\nsyntax understanding in the performance gain of the BERT Cross-\nEncoder for passage re-ranking.\nRanking has a long history of strong baselines operating purely\non bag-of-words representations ignoring the sequence order en-\ntirely. It is therefore all the more interesting to understand the\nsource of the large performance gains under certain conditions\n(specifically on MS MARCO) compared to BM25 (Tab. 2). Is it the\npotential of the MLMs to model word order? Specifically, we try to\nanswer our first research question:\nRQ1 Does the BERT Cross-Encoder Ranker perform well due to\nmodeling syntactic aspects?\nBeing able to parse syntax is essential for extracting deeper\nmeaning from natural language. If the model mainly draws on\ncomplex language understanding, breaking the sentence structure\nshould lead to a dramatic decrease in performance.\n4.1 Experiment Design\nWe address RQ1 by breaking the order of the input tokens in two dif-\nferent ways:deterministically sorting input by token id in decreasing\norder, and by random shuffling .\nThe input manipulations can be carried out during fine-tune\ntraining and fine-tune evaluation. Forming all combinations of\nthe natural and the perturbed sequence order during training and\nevaluation results in four coherent experiment conditions for each\nmanipulation. We are mainly interested in two of those conditions:\n(1) Perturbing the sequence only zero-shot during evaluation. In\nthis scenario, the model is trained on the original input and\nperturbed only during evaluation. This tells us how much\nthe fine-tuned ranker is sensitive to changes in the input\norder. Given the model would be invariant to our input ma-\nnipulation the performance would not deteriorate compared\nto the natural order.\n(2) Training and evaluating on the same input manipulation. In\nthis setting we allow the model to learn a new representation\nthat is potentially able to cope with unnatural sentence struc-\nture in a sense that its representation is entirely invariant to\nthe order.\nWe manipulate the input by first tokenizing the input. We then\napply the respective input manipulation where we only perturb\nqueries and passages within their boundaries.\nThe position of [SEP] and [CLS] tokens remain unchanged.\nICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain. D. Rau and J. Kamps\nTable 3: Performance of CE with natural-, sorted by descending token-id- and randomly shuffled- input and without posi-\ntion information during different fine-tuning train and fine-tuning eval conditions. All models are trained on MSMARCO,\nRobust04 serves as a zero-shot evaluation dataset.\nMSMARCO Robust04\nmodel fine-tune train fine-tune eval NDCG@10 MAP R@100 NDCG@10 MAP R@100\nCE baseline natural natural 68.95 45.08 68.07 44.27 23.31 40.35\nSortCE\nnatural sort 52.36 32.09 64.43 39.49 21.66 37.96\nsort sort 67.05 43.18 69.22 42.48 22.74 38.45\nsort natural 67.14 44.05 68.85 41.23 22.75 39.25\nShuffle\nCE\nnatural shuffle 51.78 30.48 61.00 36.66 19.41 35.76\nshuffle shuffle 66.71 42.83 68.03 43.65 22.57 37.97\nshuffle natural 65.33 42.30 67.49 45.61 23.48 38.93\nno pos.\nCE no pos. no pos. 66.34 43.48 68.43 46.22 24.22 40.61\nThe results of the input manipulation during fine-tune training\nand fine-tune evaluation can be found in Table 3 for MSMARCO\nand Robust04. We first report the results for the deterministically\nperturbed input by sorting the input in Sec. 4.2 and for shuffling\nrandomly in Sec. 4.3.\n4.2 Results Sorting Deterministically\nThe first line in Tab. 3 serves as the baseline where we fine-tune\n(fine-tune train) and evaluate (fine-tune eval) on the natural se-\nquence order. When we train on the natural sequence order and\nsort the input we observe a performance drop on all measures for\nMSMARCO. MAP drops around 30% (from 45.08 to 32.09), R@100\n5% (from 68.07 to 64.43). For Robust04 we observe a slightly smaller\nperformance drop but follow the same pattern across metrics. The\ndeterioration of the performance on all measures leads to the con-\nclusion that CE is sensitive to the input order by default. Despite\nthe performance drop, the remaining performance shows that the\nmodel can cope with destroying the sequence order to some limited\nextent.\nDoes this change if we fine-tune the model on the perturbed\ninput order ? Surprisingly, when fine-tuning on the sorted input\nalmost the same performance as with the natural input (see Tab. 3)\ncan be achieved. The performance is marginally lower for precision-\nbased measures and slightly higher for R@100. This holds for both\ndatasets. When evaluating this model in turn on the natural se-\nquence order the model‚Äôs performance drops only marginally show-\ning no clear preference for a particular order, thus being close to\nentirely order invariant.\nRegarding RQ1, our findings show that superiority in perfor-\nmance compared to traditional methods can be achieved without\ndrawing upon the natural sequence order. Thus, the ability to model\nsyntactic aspects can be ruled out as a single contributing factor to\nit. One might argue that the model is performing well on the sorted\ninput because it can reconstruct the original word order, however,\nSinha et al. [26] find evidence to reject this hypothesis.\n4.3 Results Random Shuffling\nAgain, the results can be found in Table 3. The manipulation of\nthe sequence order in the previous experiment is fixed. To avoid\npicking up artifacts that are based on this particular arrangement\nsuch as preserving some syntactical order or contextual information\nwe repeat the experiment with random shuffling. The results can\nbe found as well in Table 3. We find that the perturbation using\nrandom shuffling tightly follows the same performance patterns\nfor the different fine-tune and evaluation conditions. The results\nsupport our claims made previously that the BERT CE can perform\ncomparably without drawing on the syntactic order of the input.\nThis holds for MSMARCO as well as for Robust04.\n5 REMOVING POSITION INFORMATION\nIn this section, we conduct experiments with a new position-ignorant\nmodel ‚ÄúBOW-BERT. ‚Äù\nThe previous experiments suggest the Cross-Encoder can achieve\ngood performance when fine-tuned on sorted input. While in those\nexperiments the model theoretically still could encode positional in-\nformation, for this experiment we remove the position information\nentirely. Specifically, we study our second research question:\nRQ2 What is the effect of removing position information on the\nranking performance?\n5.1 Experiment Design\nFor this experiment, we remove the position embeddings that enable\nthe model to perceive input order. As a consequence, the model is\nnot able to infer the order of the input tokens and can solely revert\nto a bag-of-words representation of query and document.\nIn BERT each input is represented by a combination of three\nembeddings: token, segment, and position embeddings. Technically,\nour new model is a patch of the BERT model that completely re-\nmoves everything related to the position embeddings. As a result,\nthere is no concept of order over the entire input sequence, and we\ncan no longer rely on order or separator tokens to process the input.\nThe Role of Complex NLP in Transformers for Text Ranking? ICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain.\nFortunately, the segment embeddings still allow the model to distin-\nguish which tokens belong to the query or the document (or other\ntoken types), so we can still express all crucial information needed\nto estimate relevance in a ranking setting. While this change in the\nmodel is technically perhaps small, the conceptual differences are\nvery significant, and we have created a true ‚ÄúBOW-BERT. ‚Äù\nWe first aim to understand how well the model performs without\nposition information on MS MARCO (5.2.1) and Robust 04 (5.2.2).\n5.2 Results\nThe results can be found at the bottom in Table 3.\n5.2.1 MS MARCO. Overall, the performance of the CE without\nposition information is comparable to the CE baseline. The per-\nformance drops marginally for precision-based metrics (4% for\nNDCG@10, 5% MAP, and 2% MRR) compared to the baseline; Re-\ncall@100 performance is on par. None of the performance changes\nis significant with respect to the CE baseline. Our results strengthen\nthe findings that sequence order is not crucial for the performance\nof the CE to the extent that a bag-of-words representation without\nany position information can lead to a comparable performance. In-\nterestingly, the model without position information can match the\nhigh precision scores of the CE baseline. From a human perspective\nit is plausible to get a very rough understanding of relevance from\na bag-of-words representation, but estimating which of two highly\nrelevant passages is more relevant seems extremely challenging.\n5.2.2 Robust 04. The performance of the CE without position in-\nformation compared to the CE baseline on the Robust 04 dataset\ncan be found in Table 3 on the right side. The performance is for\nall metrics measured comparable. More specific, precision-based\nmetrics are slightly higher without position information: 4% for\nNDCG@10, 4% MAP, and 4% MRR. This small gain may be related\nto the different requests in Robust (short keyword titles), notably\nshorter than the fully verbose questions in MS Marco.\nRegarding RQ2, our findings show that removing position in-\nformation internally from a BERT cross-coder, a true bag-of-word\ncross-encoder ranking model, obtains a ranking performance com-\nparable with the standard neural cross-encoder. This finding sug-\ngests novel research directions for developing new neural ranking\nmodels on more efficient document representations containing e.g.\nonly characteristic words.\n6 LOOKING INTO THE MODELS\nIn this section, we dig deeper and analyse the internal represen-\ntations of the models, both in general and for the [CLS] token\ndetermining the ranking.\n6.1 Inherent Order Sensitivity in Ranking\nThe success of CE lies in the pre-training step where representations\nare learned by masking tokens that the model has to predict cor-\nrectly. During this task sequence order inarguably plays a role [26]\nand syntax is learned implicitly. Our previous experiments have\nshown that CE‚Äôs performance deteriorates but remains relatively\nhigh when the input order is manipulated during zero-shot evalu-\nation. In this section we investigate whether the model becomes\nTable 4: CKA similarity between natural and shuffled hid-\nden representations for the vanilla BERT and Cross-Encoder\nRanker fine-tuned on natural input order.\nmodel fine-tune train CKA similarity\nBERT - 0.4379\nCE\nnatural 0.5389\nperturbed 0.9657\nmore or less order sensitive when we fine-tuned on the ranking\ntask.\nRQ3 Does the model inherently lose order sensitivity while fine-\ntuning on the ranking task?\nIn other words, how does the order sensitivity of the fine-tuned\nranker change compared to the pre-trained BERT model? It might\nbe that the model while being adapted to the ranking task learns to\npartially ignore word order. We argue losing order sensitivity during\nfine-tuning would further support the hypothesis that sequence\norder is not a crucial feature to the CE ranker.\n6.1.1 Experiment Design. We address RQ3 by measuring the dif-\nference between the [CLS] representations for the same input with\nand without perturbing. To quantify differences between the hid-\nden representations we use Centered Kernel-Alignment (CKA) [12]\nusing a linear kernel that can detect meaning similarities between\nhigh dimensional representations while being invariant to invert-\nible linear transformations. The similarity is bounded between 0\nand 1, where 1 means most similar. We carry out a batch-wise com-\nparison of the representations over the 2020 NIST testset and the\naverage over batches.\n6.1.2 Results. We show the CKA similarity between the natural\nand sorted input for the pre-trained BERT, CE fine-tuned on natural\nsequence order and CE fine-tuned on sorted order in Table 4. First,\nwe observe that representations of the pre-trained BERT model are\nmost different (CKA similarity 0.4379). Compared to the pre-trained\nBERT model, the representations of the CE fine-tuned on the natural\norder are much more similar (CKA similarity 0.5389). We conjecture\nthat the CE fine-tuned ranker fine-tuned on natural input order has\nlost some of its order sensitivity while being trained on the ranking\ntask compared to its pre-trained base. This confirms our previous\npresumption in Section 4 that CE partially learns to ignore sequence\norder during the fine-tuning process. The results of CE fine-tuned\non the sorted input again confirm that the model is almost entirely\ninvariant (CKA similarity 0.9657) to order perturbations.\nRegarding RQ3, we conclude that CE is less order sensitive com-\npared to the pre-trained BERT model, suggesting that learning the\nranking task leads to focusing less on order sensitivity.\n6.2 Impact on Representations\nTraining the model on perturbed input or without position infor-\nmation could lead to a different notion of the input and therefore\nto different latent representations. We are interested in quantifying\nhow similar the models are compared to the BERT Cross-Encoder\nranker.\nICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain. D. Rau and J. Kamps\n1 2 3 4 5 6 7 8 9 10 11 12\n121110987654321\n0.23 0.28 0.31 0.44 0.55 0.56 0.57 0.57 0.57 0.57 0.61 0.62\n0.35 0.42 0.47 0.62 0.73 0.7 0.71 0.69 0.72 0.74 0.75 0.56\n0.37 0.44 0.5 0.64 0.75 0.72 0.73 0.7 0.73 0.75 0.73 0.52\n0.36 0.43 0.49 0.64 0.77 0.74 0.74 0.72 0.74 0.74 0.72 0.52\n0.31 0.38 0.43 0.59 0.74 0.73 0.73 0.71 0.71 0.69 0.67 0.51\n0.35 0.42 0.47 0.64 0.78 0.75 0.76 0.72 0.72 0.7 0.69 0.51\n0.36 0.44 0.49 0.65 0.78 0.74 0.74 0.7 0.7 0.69 0.67 0.48\n0.42 0.52 0.57 0.75 0.84 0.73 0.73 0.67 0.68 0.68 0.66 0.46\n0.59 0.71 0.77 0.91 0.77 0.64 0.64 0.58 0.61 0.63 0.62 0.4\n0.78 0.9 0.94 0.79 0.57 0.45 0.46 0.4 0.45 0.49 0.48 0.28\n0.8 0.93 0.88 0.69 0.49 0.38 0.39 0.34 0.39 0.41 0.4 0.23\n0.95 0.81 0.76 0.58 0.39 0.31 0.32 0.28 0.32 0.34 0.33 0.19\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1 2 3 4 5 6 7 8 9 10 11 12\n121110987654321\n0.19 0.25 0.27 0.39 0.47 0.48 0.49 0.48 0.5 0.5 0.58 0.64\n0.34 0.42 0.46 0.6 0.68 0.67 0.69 0.67 0.7 0.73 0.76 0.6\n0.35 0.43 0.46 0.61 0.69 0.69 0.7 0.69 0.73 0.75 0.74 0.56\n0.33 0.4 0.43 0.59 0.69 0.71 0.72 0.7 0.73 0.73 0.72 0.56\n0.28 0.35 0.38 0.55 0.67 0.7 0.71 0.7 0.71 0.69 0.68 0.55\n0.33 0.4 0.43 0.62 0.73 0.74 0.75 0.73 0.74 0.72 0.71 0.55\n0.32 0.39 0.43 0.61 0.73 0.74 0.74 0.72 0.73 0.71 0.7 0.54\n0.4 0.5 0.54 0.75 0.84 0.79 0.77 0.74 0.76 0.76 0.72 0.53\n0.57 0.68 0.74 0.9 0.76 0.66 0.65 0.6 0.64 0.68 0.63 0.42\n0.74 0.85 0.9 0.78 0.58 0.49 0.48 0.43 0.49 0.54 0.49 0.29\n0.78 0.87 0.87 0.71 0.51 0.43 0.42 0.38 0.43 0.48 0.44 0.26\n0.96 0.77 0.76 0.6 0.43 0.36 0.36 0.32 0.37 0.41 0.37 0.22\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: Similarity index kernel-CKA of the hidden representations of different layers measured between the CE baseline\nand CE trained on sorted input (left) and CE trained without position information (right).\nRQ4 How different are models that are trained without sequence\norder?\n6.2.1 Experiment Design. To quantify the differences between the\ntwo models we again apply CKA using a linear kernel. We compare\nlayerwise the kernel-CKA similarity of representations of the CE\ntrained on the natural input to CE trained and evaluated with sorted\ninput. We repeat the same with the model trained without position\ninformation. The representations are taken at the end of each layer.\nWe carry out a batch-wise comparison of the representations over\nthe 2020 NIST testset and the average over batches.\n6.2.2 Results. The results for both models can be found on the left\nside in Figure 2. The comparison of the representations of the CE\nbaseline and CE trained on sorted input shows layers (1-5) exhibit\nthe strongest agreement between the representations of the two\nmodels. Representations of layers 6-11 show less agreement while\nthe last layer varies the most. We conclude that training the model\non perturbed input order significantly changes the learned represen-\ntations, especially for later layers. We make the same observations\nfor CE fine-tuned without position information (see Figure 2 right).\nA hypothesis to explain our findings is that word order is being\npartially maintained within the vanilla CE Ranker as a legacy of\nthe pre-training task causing a drastic change in representations\nwhen losing sequence order.\nRegarding RQ4, although we observed similar effectiveness, our\nmanipulations destroying sequence order or removing position\ninformation result in different latent representations.\n7 NATURAL LANGUAGE UNDERSTANDING\nIn this section, after having examined the role of modeling syntactic\naspects for ranking, we want to revisit NLP tasks under the same\nconditions.\nWe are interested in whether our findings are specific for search\nor whether they are inherent in all BERT-based models. Hence, our\nfinal research question is:\nRQ5 What role does modeling syntactic aspects play in Natural\nLanguage Understanding tasks?\nTo answer this research question we carry out the same input\nmanipulations as done in previous sections for ranking on the\nGLUE dataset. We fine-tune the model on the natural, sorted after\ntoken-id and without position information.\n7.1 Experiment Design\nThe GLUE dataset [ 29] is a widely used dataset to evaluate gen-\neral natural language understanding. It consists of nine different\nsentence- or sentence-pair language understanding tasks for which\na model is fine-tuned separately. We leave out CoLA as this task is\nto classify whether a sentence is grammatical or not and therefore\nrequires intact sentence structure to be solved.\nWe train our BERT models from Hugginface‚Äôs [ 31] bert-base-\nuncase with batch size 64, maximum sequence length 128, and a\nlearning rate 2e-5 for 3 epochs on all tasks, despite for MRPC and\nWNLI we train for 5 epochs.\n7.2 Results\nThe results of comparing fine-tuning on natural input perturbed\ninput, and without position, information can be found in Table 5.\nNote, that the model that is trained on the sorted input is also evalu-\nated on it. Our results show that perturbing the input structure both\nduring fine-tuning and evaluation only yields a marginal decrease\nfrom on average 78.2 over all tasks to 78.1 compared to the original\ninput. Breaking the sentence structure even performs best on tasks\nRTE, STS-B, and WNLI.\nThe experiment fine-tuning without position information re-\nsembles parts of an experiment conducted by Wang et al. [30] and\nSinha et al. [26]. We validate their results by finding that removing\npositional information leads to a small drop (4%) of the average\nperformance for natural language understanding tasks. It is further\nworth noting that tasks that are based on sentence similarity benefit\nfrom the sorted / No position setting: RTE, STS, WNLI.\nThe Role of Complex NLP in Transformers for Text Ranking? ICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain.\nTable 5: BERT fine-tuned on natural order, sorted by token-\nid and without position information on all GLUE tasks, ex-\ncept for CoLA.\nDataset Metric Natural Sorted No pos.\nSST-2 acc. 91.6 85.6 86.0\nMNLI acc. 84.2 79.6 79.7\nMRPC F1. 83.4 84.4 81.7\nQNLI acc. 90.7 86.6 87.1\nQQP F1. 87.0 85.8 85.3\nRTE acc. 55.2 60.2 56.6\nSTS-B spear. cor 84.0 86.3 81.9\nWNLI acc. 43.6 56.3 45.0\nAll mean ¬±std 78.2 ¬±0.19 78.1 ¬±0.14 75.4 ¬±0.19\nRegarding RQ5, while it is well-known that a BOW representa-\ntion is effective in IR, our results suggest that syntax also does not\nseem to play a crucial role to solve the GLUE tasks, consisting of\nvarious NLP tasks based on sentence classification.\n8 CONCLUSION\nIn this work, we examined the impact of perturbing the sequence\norder of the BERT Cross-Encoder re-ranker. We showed that the\nmodel, when fine-tuned on destroyed sequence order, can maintain\nthe same performance compared to fine-tuning on natural input.\nWe confirmed this finding by using two different input manipula-\ntion that destroys the natural sequence order. We further find that\nremoving the position information entirely can achieve compara-\nble, if not better, performance to the BERT Cross-Encoder ranker,\nwhich is fully informed about the sequence structure of query and\ndocument. We confirm our observations based on the evaluation\nof two different widely used datasets: MS MARCO and Robust 04\n(zero-shot document retrieval).\nThe fact that the models fine-tuned without access to the natural\nsequence order perform on par with retaining the natural sequence\norder leads us to the conclusion that syntactic abstraction can not\nbe attributed to performance advantages over earlier models. Our\nanalysis of the hidden representations for the BERT model and the\nCross-Encoder ranker further suggests that through fine-tuning\nthe ranking task the sensitivity to the input order is weakened.\nOverall, our results point out that syntactic aspects do not play a\ncritical role in the effectiveness of the BERT Cross-Encoder ranker.\nThe performance gain over previous models can not be solely ex-\nplained by the ability to model sequence order. An explanation for\nthe superior performance of BERT could be the embeddings may be\nricher representations of meaning regardless of context, and they\nmay represent different word meanings based on aggregated con-\ntexts where word order has only a negligible influence. We further\nconjecture that other aspects such as query-passage cross-attention\nor deep matching may be contributors to the performance of the\nmodel.\nIt would be also interesting to observe the effect of perturbing the\ninput and removing the position information during pre-training\nof BERT. However, due to the computational very demanding pre-\ntraining, we have limited our experimentation to fine-tuning only.\nPreliminary results suggest that the self-supervised masked word\nprediction pre-training task is benefiting from word order informa-\ntion, and we may need to rethink the entire training setup. We have\nadded to the understanding of the factors responsible for the suc-\ncess of the BERT model mainly by excluding plausible explanations\nmirroring human text understanding. This immediately suggests\nnovel research directions to directly address the factors that are\nkey in the BERT model. Our findings give rise to the presumption\nthat the current pre-training task of the underlying BERT model\nis over-complex for ranking and other downstream classification\ntasks. Implicitly promoting to model word order and syntactic as-\npects could potentially take up a large capacity of the model. Our\nfindings provide a good starting point for the design of pre-training\ntasks tailored specifically to ranking, possibly reducing complexity\nand model size.\nAs a general observation, the effectiveness of recent transformer-\nbased rankers has frequently been characterized as ‚Äúbringing NLP\ninto IR‚Äù because they present a departure from the text statistics-\nbased bag-of-word models. Our analysis may suggest that in a\nway these models are ‚Äúbringing IR into NLP‚Äù showcasing that\n(higher-order) word statistics even solves NLP problems tradition-\nally thought to require the representation of complex syntactic\nstructures.\nACKNOWLEDGMENTS\nThis research is funded in part by the Netherlands Organization\nfor Scientific Research (NWO CI # CISC.CC.016), and the Innova-\ntion Exchange Amsterdam (POC grant). Views expressed in this\npaper are not necessarily shared or endorsed by those funding the\nresearch.\nREFERENCES\n[1] Avi T Arampatzis, T Tsoris, Cornelis H. A. Koster, and Th P Van Der Weide. 1998.\nPhrase-based information retrieval. Information Processing & Management 34, 6\n(1998), 693‚Äì707.\n[2] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong\nLiu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. MS MARCO: A human generated machine reading comprehension dataset.\n(2016). arXiv:1611.09268\n[3] Arthur C√¢mara and Claudia Hauff. 2020. Diagnosing BERT with retrieval heuris-\ntics. In European Conference on Information Retrieval . Springer, 605‚Äì618.\n[4] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and\nRichard Harshman. 1990. Indexing by latent semantic analysis. Journal of the\nAmerican society for information science 41, 6 (1990), 391‚Äì407.\n[5] Mostafa Dehghani. 2018. Toward Document Understanding for Information\nRetrieval. SIGIR Forum 51, 3 (feb 2018), 27‚Äì31. https://doi.org/10.1145/3190580.\n3190585\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\n(2018). arXiv:1810.04805\n[7] Allyson Ettinger. 2019. What BERT is not: Lessons from a new suite of psy-\ncholinguistic diagnostics for language models. CoRR abs/1907.13528 (2019).\narXiv:1907.13528 http://arxiv.org/abs/1907.13528\n[8] Joel L Fagan. 1988.Experiments in automatic phrase indexing for document retrieval:\nA comparison of syntactic and nonsyntactic methods . Ph. D. Dissertation. Cornell\nUniversity.\n[9] Thibault Formal, Benjamin Piwowarski, and St√©phane Clinchant. 2021. A White\nBox Analysis of ColBERT. In ECIR.\n[10] Ashim Gupta, Giorgi Kvernadze, and Vivek Srikumar. 2021. Bert & family eat\nword salad: Experiments with text understanding. In Proceedings of the AAAI\nConference on Artificial Intelligence , Vol. 35. 12946‚Äì12954.\n[11] Zellig S. Harris. 1954. Distributional Structure. WORD 10, 2-3\n(1954), 146‚Äì162. https://doi.org/10.1080/00437956.1954.11659520\nICTIR ‚Äô22, July 11‚Äì12, 2022, Madrid, Spain. D. Rau and J. Kamps\narXiv:https://doi.org/10.1080/00437956.1954.11659520\n[12] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. 2019.\nSimilarity of Neural Network Representations Revisited. CoRR abs/1905.00414\n(2019). arXiv:1905.00414 http://arxiv.org/abs/1905.00414\n[13] Angelika Kratzer and Irene Heim. 1998. Semantics in generative grammar .\nVol. 1185. Blackwell Oxford.\n[14] Jimmy Lin. 2021. The Neural Hype, Justified! A Recantation. SIGIR Forum 53, 2\n(mar 2021), 88‚Äì93. https://doi.org/10.1145/3458553.3458563\n[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).\narXiv:1907.11692 http://arxiv.org/abs/1907.11692\n[16] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman\nCohan. 2022. ABNIRML: Analyzing the Behavior of Neural IR Models. Trans-\nactions of the Association for Computational Linguistics 10 (03 2022), 224‚Äì239.\nhttps://doi.org/10.1162/tacl_a_00457\n[17] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search.\nACM SIGIR Forum 55, 1 (Jun 2021), 1‚Äì27. https://doi.org/10.1145/3476415.3476428\n[18] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality.\nAdvances in neural information processing systems 26 (2013).\n[19] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n(2019). arXiv:1901.04085\n[20] Ankur P. Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. 2016.\nA Decomposable Attention Model for Natural Language Inference. CoRR\nabs/1606.01933 (2016). arXiv:1606.01933 http://arxiv.org/abs/1606.01933\n[21] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. CoRR abs/1802.05365 (2018). arXiv:1802.05365 http://arxiv.org/\nabs/1802.05365\n[22] Thang Pham, Trung Bui, Long Mai, and Anh Nguyen. 2021. Out of Order: How\nimportant is the sequential order of words in a sentence in Natural Language\nUnderstanding tasks?. InFindings of the Association for Computational Linguistics:\nACL-IJCNLP 2021. Association for Computational Linguistics, Online, 1145‚Äì1160.\nhttps://doi.org/10.18653/v1/2021.findings-acl.98\n[23] David Rau and Jaap Kamps. 2022. How Different are Pre-trained Transformers\nfor Text Ranking?. InAdvances in Information Retrieval: 44th European Conference\non IR Research, ECIR 2022, Stavanger, Norway, April 10‚Äì14, 2022, Proceedings, Part\nII. 207‚Äì214.\n[24] Dani√´l Rennings, Felipe Moraes, and Claudia Hauff. 2019. An axiomatic approach\nto diagnosing neural IR models. In European Conference on Information Retrieval .\nSpringer, 489‚Äì503. https://doi.org/10.1007/978-3-030-15712-8_32\n[25] Mark Sanderson. 1994. Word Sense Disambiguation and Information Retrieval.\nIn SIGIR ‚Äô94 , Bruce W. Croft and C. J. van Rijsbergen (Eds.). Springer London,\nLondon, 142‚Äì151.\n[26] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and\nDouwe Kiela. 2021. Masked Language Modeling and the Distributional Hy-\npothesis: Order Word Matters Pre-training for Little. InProceedings of the 2021\nConference on Empirical Methods in Natural Language Processing . Association\nfor Computational Linguistics, Online and Punta Cana, Dominican Republic,\n2888‚Äì2913. https://doi.org/10.18653/v1/2021.emnlp-main.230\n[27] Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021.\nUnNatural Language Inference. ACL (2021). https://arxiv.org/abs/2101.00010\n[28] Ellen M Voorhees. 1993. Using WordNet to disambiguate word senses for text\nretrieval. In Proceedings of the 16th annual international ACM SIGIR conference on\nResearch and development in information retrieval . 171‚Äì180.\n[29] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. In the Proceedings of ICLR..\n[30] Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu,\nand Jakob Grue Simonsen. 2021. On Position Embeddings in BERT. In Interna-\ntional Conference on Learning Representations . https://openreview.net/forum?id=\nonxoVA9FxMw\n[31] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . Association for Computational\nLinguistics, Online, 38‚Äì45. https://www.aclweb.org/anthology/2020.emnlp-\ndemos.6"
}