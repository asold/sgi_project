{
    "title": "Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling",
    "url": "https://openalex.org/W4385825586",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4281471709",
            "name": "Sakota, Marija",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A4223134538",
            "name": "Peyrard, Maxime",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2098212638",
            "name": "West, Robert",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6893809404",
        "https://openalex.org/W3137147200",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W3129093240",
        "https://openalex.org/W4385573217",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3138154797",
        "https://openalex.org/W4287363917",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3203149535",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4317465025",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4384807943",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W4303648020",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4319793478",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4309591680",
        "https://openalex.org/W4376122773"
    ],
    "abstract": "Generative language models (LMs) have become omnipresent across data science.\\nFor a wide variety of tasks, inputs can be phrased as natural language prompts\\nfor an LM, from whose output the solution can then be extracted. LM performance\\nhas consistently been increasing with model size - but so has the monetary cost\\nof querying the ever larger models. Importantly, however, not all inputs are\\nequally hard: some require larger LMs for obtaining a satisfactory solution,\\nwhereas for others smaller LMs suffice. Based on this fact, we design a\\nframework for cost-effective language model choice, called \"Fly-swat or cannon\"\\n(FORC). Given a set of inputs and a set of candidate LMs, FORC judiciously\\nassigns each input to an LM predicted to do well on the input according to a\\nso-called meta-model, aiming to achieve high overall performance at low cost.\\nThe cost-performance tradeoff can be flexibly tuned by the user. Options\\ninclude, among others, maximizing total expected performance (or the number of\\nprocessed inputs) while staying within a given cost budget, or minimizing total\\ncost while processing all inputs. We evaluate FORC on 14 datasets covering five\\nnatural language tasks, using four candidate LMs of vastly different size and\\ncost. With FORC, we match the performance of the largest available LM while\\nachieving a cost reduction of 63%. Via our publicly available library,\\nresearchers as well as practitioners can thus save large amounts of money\\nwithout sacrificing performance.\\n",
    "full_text": "Fly-Swat or Cannon? Cost-Effective Language Model Choice via\nMeta-Modeling\nMarija Å akota\nEPFL\nSwitzerland\nmarija.sakota@epfl.ch\nMaxime Peyrard\nEPFL\nSwitzerland\nmaxime.peyrard@epfl.ch\nRobert West\nEPFL\nSwitzerland\nrobert.west@epfl.ch\nABSTRACT\nGenerative language models (LMs) have become omnipresent across\ndata science. For a wide variety of tasks, inputs can be phrased as\nnatural language prompts for an LM, from whose output the solu-\ntion can then be extracted. LM performance has consistently been\nincreasing with model sizeâ€”but so has the monetary cost of query-\ning the ever larger models. Importantly, however, not all inputs are\nequally hard: some require larger LMs for obtaining a satisfactory\nsolution, whereas for others smaller LMs suffice. Based on this fact,\nwe design a framework for cost-effective language model choice,\ncalled â€œFly-swat or cannonâ€ (FORC). Given a set of inputs and a set of\ncandidate LMs, FORC judiciously assigns each input to an LM pre-\ndicted to do well on the input according to a so-called meta-model,\naiming to achieve high overall performance at low cost. The costâ€“\nperformance tradeoff can be flexibly tuned by the user. Options\ninclude, among others, maximizing total expected performance\n(or the number of processed inputs) while staying within a given\ncost budget, or minimizing total cost while processing all inputs.\nWe evaluate FORC on 14 datasets covering five natural language\ntasks, using four candidate LMs of vastly different size and cost.\nWith FORC, we match the performance of the largest available LM\nwhile achieving a cost reduction of 63%. Via our publicly available\nlibrary,1 researchers as well as practitioners can thus save large\namounts of money without sacrificing performance.\nACM Reference Format:\nMarija Å akota, Maxime Peyrard, and Robert West. 2024. Fly-Swat or Cannon?\nCost-Effective Language Model Choice via Meta-Modeling. In Proceedings\nof the 17th ACM International Conference on Web Search and Data Mining\n(WSDM â€™24), March 4â€“8, 2024, Merida, Mexico. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3616855.3635825\n1 INTRODUCTION\nIn recent years, a clear trend has emerged in natural language\nprocessing and has subsequently spread across data science, char-\nacterized by the increasing prominence of large language models\n(LLMs). With the wide range of applications these models are ca-\npable of solving, many companies have contributed to this trend\n1https://github.com/epfl-dlab/forc\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0371-3/24/03. . . $15.00\nhttps://doi.org/10.1145/3616855.3635825\n...\n...\n... ...\nFigure 1: Overview of FORC, our framework for cost-effective\nLM choice (details in Sec. 3.2). FORC consists of two steps:\n(1) Predict cost and performance of each candidate LM on\neach input query. Cost prediction is done using API pricing.\nPerformance prediction is done using a meta-model, trained\nahead of time (not shown) based on existing pairs of LM\nqueries and LM performance scores. (2) Assign each query to\nat most one LM using an assignment strategy, aiming for high\ntotal expected performance at low cost. Note that neither of\nthe two steps requires interacting with the LMs; queries are\nfed to the assigned LMs only after the above steps.\nby offering their own LLMs as a service. As a result, the landscape\nof language processing is undergoing a dynamic shift, with an\nincreasing number of LLMs becoming available on the market.\nAs the size of LLMs continues to expand, their capabilities are\nundergoing substantial enhancements, leading to notable improve-\nments across various language-related tasks [2, 5, 10, 21]. With the\ngrowth in the number of parameters, their ability to understand\ncomplex contexts has improved significantly. These bigger models\nare better at picking up subtle changes in meaning, which helps\nthem give more relevant responses that fit the context. Moreover,\ntheir increased size allows LLMs to generate text that is more coher-\nent and fluent, often resembling a human-like conversation. Finally,\nas training datasets have been getting larger, the amount of knowl-\nedge baked into LLM parameters has also broadened. This results\nin improved factual accuracy and a better capability to provide\nwell-informed answers to a wider range of questions.\nHowever, as the use of LLMs becomes more common, there is\na relevant concern about the rising costs of running them. State-\nof-the-art language models (LMs) have hundreds of billions of pa-\nrameters, so they need much computing power, leading to higher\nexpenses. For instance, running GPT-4 with an 8K-token context is\n20 times more expensive than running GPT-3.5 on the same query\nwith a 4K-token context. 2 Even though LLMs excel at handling\n2https://openai.com/pricing\narXiv:2308.06077v3  [cs.CL]  18 Dec 2023\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico Å akota, Peyrard, and West\ncomplex language tasks, it is important to realize that not every sit-\nuation needs their massive capabilities. Smaller LMs are generally\ngood at handling simpler language tasks and can be a more cost-\neffective choice in cases where full LLM power is not necessary. For\nexample, on the 14 datasets that we examined with four different\nlanguage models, 33% of data samples are successfully solved both\nby the biggest model and at least one of the smaller ones, while\n11% are exclusively solved by one or more of the smaller models,\nwith the biggest model failing to answer correctly (cf. Sec. 4.2).\nThere is, thus, an opportunity to save cost by assigning each\ninput to the cheapest model able to solve it. The problem in realizing\nthis is how to predict ahead of time which models would correctly\nsolve which inputsâ€”without actually running each LM on each\ninput, which would defeat the purpose. Chen et al. [3] proposed to\nemploy increasingly expensive LMs in a cascade until a satisfactory\nresult is obtained. This requires querying potentially multiple LMs\nper input, something we set out to avoid in our approach.\nProposed solution. In this paper, we propose a novel approach\nfor saving LM costs by introducing â€œFly-swat or cannonâ€ (FORC), a\ncost-aware framework that aims to assign each query from a query\nset provided by the user to an appropriate LM, without the need to\nrun any of the LMs in the process. As shown in Fig. 1, FORC consists\nof two steps: First, we predict the cost and performance of each\ncandidate LM on each input query. Cost prediction is done using the\nLM providerâ€™s API pricing; performance prediction is done using\na meta-model, a regression model trained ahead of time based on\nexisting pairs of LM queries and LM performance scores. Second,\nwe assign each query to at most one LM using an assignment\nstrategy, aiming for high total expected performance at low cost.\nThe costâ€“performance tradeoff can be tuned by choosing from\nmultiple strategies, each formalized as an optimization problem.\nAdvantages of FORC. As mentioned, FORC does not require any\ninteraction with the LMs when assigning queries to LMs. Rather,\neach query is fed to its assigned LM only once FORC has terminated.\nThis opens up the possibility for greater budget savings in com-\nparison to existing work. Next, the meta-model can be trained on\ninputs from the union of a wide range of tasks and datasets, with-\nout using any information about the task or dataset from which\nan input was sourced. This way, at run time, FORC can handle\ninputs without having to know which tasks they correspond to,\nand as we show, FORC even works on inputs from tasks not seen\nduring meta-model training. Finally, compared to prior work, we\noffer users more flexibility by providing them with more options\nfor cost and performance constraints and preferences.\nResults. With the help of FORC, on 14 datasets spanning five\ndifferent task types, with four different LMs available, we are able to\nreduce the cost of running the test dataset by 63% while maintaining\nthe same performance as the biggest LM. To facilitate the use of\nFORC, we release the library as open-source code.1\nContributions. Briefly, our contributions are the following:\n(1) We propose FORC, a cost-aware framework that automati-\ncally assigns input queries to suitable LMs without the need\nto run the LMs themselves in the process.\n(2) We show that, by employing FORC, we are able to substan-\ntially reduce the cost of running queries from different tasks,\nwhile maintaining performance equal to the biggest LM avail-\nable in our evaluation.\n(3) We release a library for our framework, enabling users to run\nthe setting introduced here or to train different meta-models\ntailored to their needs.\n2 BACKGROUND AND RELATED WORK\n2.1 Language-model evaluation\nThorough assessment of LMs is a complex, but necessary task re-\nquired for investigating and improving LM performance. With the\nappearance of general purpose LMs, the need for an all-encom-\npassing evaluation across different tasks to set a common standard\nbecame apparent. There have been several efforts to simplify this\nevaluation process. For instance, EleutherAIâ€™s Language Model Har-\nness Evaluation framework [6], Huggingfaceâ€™s Evaluate library [22],\nand BIG-Bench [20] all offer convenient open-source repositories\nthat enable common evaluation and encourage collaborative ad-\nvancements in the field.\nLiang et al. [14] performed an exhaustive evaluation of a wide\nset of LMs, termed Holistic Evaluation of Language Models (HELM).\nThey evaluated LMs of different sizes and capabilities, on various\ndatasets and tasks, from many performance aspects, such as accu-\nracy, robustness, fairness, etc. This enabled a standardized view of\nLM performance and a more reliable way to compare LMs.\nTheir results revealed that smaller LMs are able to solve some\ntasks as well as bigger LMs. This is an indicator that it is unnecessary\nto use the biggest LM for each scenario. However, for more complex\ntasks, as smaller LMs mostly fail to solve them, we should use bigger,\nmore capable LMs. These insights demonstrate that there is a need\nfor an automatized framework that would help us decide when to\nuse which LM. The results align with the findings from the Inverse\nScaling Prize competition [ 16]. Results from the two rounds of\ncompetition [17, 18] revealed numerous tasks where larger LMs\nperform worse than their smaller counterparts.\n2.2 Inference-cost optimization\nThe majority of the cost for an LM product comes from inference,\nnot training [21]. Despite this, most existing research focuses on\nminimizing training cost [10, 12, 21]. The high cost of executing\nthese models has driven the development of several inference cost\nreduction techniques, such as quantization [1, 7, 23], distillation [8,\n11, 19], and pruning [9, 13].\nZong et al. [24] include not only the training cost, but also an-\nnotation and inference cost in their empirical analysis. They focus\non one type of task onlyâ€”text classificationâ€”and evaluate different\ntypes of models, including non-neural models, an LLM, and smaller\nlanguage models. They give insights on which model would be the\nbest choice to train or use in a specific real-world scenario. Contrary\nto this, our work focuses on LMs only, developing a framework that\nautomatically decides which LM is suitable for the tasks presented\nunder inference budget constraints.\nSimilarly to our work, Chen et al. [3] attempt to develop a frame-\nwork working with LMs only. They propose using LMs in cascade,\ni.e., sending a query to available LMs sequentially until the reliabil-\nity of an answer is over some predefined threshold. Their approach\nis specialized, meaning that parts of the framework are adapted\nFly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling WSDM â€™24, March 4â€“8, 2024, Merida, Mexico\nDatset Task type Evaluation metric Train size Val size Test size\nMMLU QA Exact match (EM) 434 47 86\nRAFT Text classification Quasi-exact match 341 33 66\nWikiFact QA Quasi-exact match 6069 669 1189\nBoolQ QA Quasi-exact match 765 85 150\nTruthfulQA QA Exact match (EM) 500 56 98\nIMDB Sentiment analysis Quasi-exact match 765 85 150\nEntity matching Reasoning Quasi-exact match 1071 119 210\nData imputation Reasoning Quasi-exact match 324 37 63\nbAbI Reasoning Quasi-exact match 765 85 150\nMATH Reasoning Equivalent 334 37 66\nGSM8K Reasoning EM (up to an indicator) 765 85 150\nLSAT Reasoning Quasi-exact match 353 39 69\nLegalSupport Reasoning Quasi-exact match 765 85 150\nCivilComments Toxicity detection Quasi-exact match 765 85 150\nTotal - - 14016 1547 2747\nTable 1: Dataset specification.\nspecifically for one dataset during training. We, on the contrary,\nfocus on determining which LM to use prior to sending queries to\nany of them. As we do not need to run any of the LMs to find the\nbest one, our approach is potentially a lot cheaper. In addition, we\ntested it on a wider range of datasets, and we developped a general\nframework without the need to retrain our meta-model for each\ndataset separately.\n3 METHOD\n3.1 Problem setting\nTasks. We assume that the user has a set of queries they want to\nsolve using an LM. A lot of the commonly known tasks, such as\nquestion-answering (QA), reasoning, and summarization, can be\nwritten as textual queries, or prompts. For example, to transform a\nsummarization task into query format, one can construct an LM\nprompt by adding an instruction such as â€œSummarize the above\narticle in one sentenceâ€ to the text to be summarized. Even simpler,\nQA tasks can often be sent to the LM in their original form.\nWe work under the premise that all of the queries are evaluated\nusing the same, single metric. This way, we can be sure that the\nevaluation process is straightforward and consistent across all tasks.\nLMs. We consider a scenario where a user has access toğ‘˜ LMs (ğ‘™ğ‘–\nfor ğ‘– = 1, ..., ğ‘˜) that could potentially solve the set ofğ‘šqueries (ğ‘ğ‘—\nfor ğ‘— = 1, ..., ğ‘š). These LMs can be different in their capabilities\nand size. Each LM is associated with its own cost. Costs can be\ndefined by the user, e.g., via the LMsâ€™ API pricing.\nGoal. Our goal is to use the pool of LMs to solve queries in a budget-\nconscious way. We aim to assign each query ğ‘ğ‘— to at most one ğ‘™ğ‘–\nwhile respecting the userâ€™s costâ€“performance requirements.\n3.2 Framework setting\nOur framework has three main components: meta-model, cost es-\ntimation, and assignment strategy. In Fig. 1, we illustrate the way\nFORC works. First, the user needs to specify a set of queries they\nwant to solve. Then, using the meta-model, we predict the perfor-\nmance ğ‘ğ‘– ğ‘—of each LM ğ‘™ğ‘– on each query ğ‘ğ‘— . At the same time, we\nestimate the cost ğ‘ğ‘– ğ‘—of query ğ‘ğ‘— when using LM ğ‘™ğ‘– . Next, the user\nneeds to specify one of the assignment strategies described below,\nand optional costâ€“performance requirements. The strategy will\nthen be used to assign each query to at most one of the LMs.\nMeta-model and cost estimation. In order to know which LM\nto use for a certain query, we first have to predict the performance\nğ‘ğ‘– ğ‘— that LM ğ‘™ğ‘– would achieve on query ğ‘ğ‘— . To do so, we train a\nmeta-model. During training, we send a query ğ‘ğ‘— , plus a token\nrepresenting ğ‘™ğ‘– , as input to the meta-model, with target output ğ‘ğ‘– ğ‘—,\nwhich we assume is known for the training set. Our meta-model\nis significantly smaller than all the LMs we are working with (cf.\nSec. 4.1) and it is trained on a diverse set of tasks, which allows it to\nbe suitable for general use (cf. Sec. 4.1). It is worth noting that one\ncan train a meta-model tailored to oneâ€™s own needs, with different\nLMs in the pool, different datasets, or different performance metrics,\nand plug it into the framework pipeline.\nAlong with the measure of performance, we have to estimate\nthe cost ğ‘ğ‘– ğ‘— of query ğ‘ğ‘— on each LM ğ‘™ğ‘– . The cost function can be\ncustomized as needed. For implementation details, see Sec. 4.1.\nAssignment strategies. Once we have a meta-model, we need to\ndecide how to assign each query to one of the possible LMs in the\npool, based on performance and cost estimates. We call the method\nto do this an assignment strategy . There are two types of strategies:\n(i) Cost-insensitive strategies: When applying cost-insensitive\nstrategies to the samples, we do not consider any constraints on\nthe budget or performance that the user might have set. Each data\nsample is treated in the same way, independently of the whole\nbatch. We define the following cost-insensitive strategies:\n(a) Single-model strategy : This strategy implies applying a single,\nfixed LM from the available LMs to each sample.\n(b) Performance-maximizing strategy : This strategy is based on\nthe outputs of the meta-model. For each sample, we choose the\nLM that, according to the meta-model, is predicted to achieve the\nhighest performance.\n(c) Thresholding strategy : This strategy is also based on the out-\nputs of the meta-model. The user has to specify an acceptable-\nperformance threshold that defines whether a task is solved or\nnot. Outputs are binarized according to that threshold. A concrete\nexample where this strategy might be useful are tasks that are eval-\nuated with binary metrics such as accuracy. The strategy works by\nchoosing the cheapest LM that solves the respective data sample.\nIn cases where none of the LMs solve the sample according to our\nmeta-model, we examine two possibilities: choosing the smallest\n(and thus generally cheapest) LM, or choosing the biggest (and thus\ngenerally most powerful) LM for that data sample.\n(ii) Cost-sensitive strategies: Contrary to cost-insensitive strate-\ngies, in this setting, we consider constraints, such as cost constraints,\nset by the user for the batch of data samples in its entirety. This\ntransforms the problem into an optimization problem. We employ\nthe following cost-sensitive strategies:\n(a) Cost-oriented ILP strategy : We formulate the problem of as-\nsigning an LM to each sample as an integer linear programming\n(ILP) problem. We define ğ‘€ as a set of LMs, ğ‘† as a set of samples\nthat need to be assigned to LMs, and ğ¶max as the maximum total\ncost of processing all the samples. A binary variable ğ‘¥ğ‘– ğ‘—is intro-\nduced to describe the assignment (or lack of it) between a data\nsample ğ‘ğ‘— and an LM ğ‘™ğ‘– . If ğ‘¥ğ‘– ğ‘—= 1, sample ğ‘ğ‘— is assigned to the LM\nğ‘™ğ‘– . A sample does not necessarily have to be assigned to any LM.\nAssigning sample ğ‘ğ‘— to LM ğ‘™ğ‘– is associated with cost ğ‘ğ‘– ğ‘—and value\nğ‘ğ‘– ğ‘—, where cost ğ‘ğ‘– ğ‘— corresponds to the estimated cost, and value\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico Å akota, Peyrard, and West\ntext-ada-001 text-babbage-001 text-curie-001 text-davinci-002\nPricing ($ per 1k tokens) 0.0004 0.0005 0.002 0.02\nAverage output length 6.85 7.18 7.01 8.41\nTable 2: Specifications of available LMs.\nğ‘ğ‘– ğ‘—corresponds to the predicted performance when using LM ğ‘™ğ‘– to\nsolve the sample ğ‘ğ‘— . The goal is to maximize the performance on\nthe whole set of samples while respecting the cost constraint. This\nproblem is then formalized as an ILP as follows:\nmaximize\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€,ğ‘ğ‘— âˆˆğ‘†\nğ‘ğ‘– ğ‘—ğ‘¥ğ‘– ğ‘— (1)\ns.t.\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€\nğ‘¥ğ‘– ğ‘— â‰¤1, âˆ€ğ‘ğ‘— âˆˆğ‘† (2)\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€,ğ‘ğ‘— âˆˆğ‘†\nğ‘ğ‘– ğ‘—ğ‘¥ğ‘– ğ‘— â‰¤ğ¶max (3)\nHere, (2) ensures that every sample is assigned to at most one LM.\n(b) Performance-oriented ILP strategy : Similar to the previous\ncase, we formulate this problem in the form of an ILP. The goal in\nthis case is to minimize the cost, while respecting the performance\nconstraint ğ‘ƒmin that the user has set. In short, following the same\nnotation as before, this problem is formalized as follows:\nminimize\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€,ğ‘ğ‘— âˆˆğ‘†\nğ‘ğ‘– ğ‘—ğ‘¥ğ‘– ğ‘— (4)\ns.t.\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€\nğ‘¥ğ‘– ğ‘— â‰¤1, âˆ€ğ‘ğ‘— âˆˆğ‘† (5)\nâˆ‘ï¸\nğ‘™ğ‘– âˆˆğ‘€,ğ‘ğ‘— âˆˆğ‘†\nğ‘ğ‘– ğ‘—ğ‘¥ğ‘– ğ‘— â‰¥ğ‘ƒmin (6)\nThis strategy can also be implemented when binarizing the perfor-\nmance values ğ‘ğ‘– ğ‘—, as done under the cost-insensitive thresholding\nstrategy. In that case, the performance-oriented ILP strategy can be\nviewed as minimizing the cost for solving at least ğ‘ƒmin samples.\n(c) Greedy strategy : This strategy works by going through the\nsamples sequentially and choosing, for each sample, the LM achiev-\ning the highest performance according to the meta-model, until the\ncost constraint is reached. After this point, remaining data samples\nremain unassigned and are not fed to any of the LMs in the pool.\nFor our experiments (cf. Sec. 4.2), they are counted as incorrect\n(when using accuracy as the performance metric), with cost zero.\n4 EXPERIMENTS\n4.1 Meta-model evaluation\nData. As our main source of data, we use raw LM runs from the\nHELM project [14]. Raw runs consist of inputs (queries and full\nprompts) sent to the LM, generation parameters, ground truth refer-\nences, and LM outputs with additional details such as log probability\nand the time it took to execute the prompt.\nRaw model runs have been released for a wide range of datasets\ncovering multiple tasks. While all of these tasks are in the same\ninput format (query), standard metrics used to evaluate the quality\nof the output can be vastly different between tasks. For example,\nsummarization output is often evaluated using ROUGE scores [15],\nwhich are continuous values from 0 to 1, and question-answering\ncan be evaluated using EM (exact match), which is a binary metric.\nFor the sake of this paper, we focus on tasks for which there is\na clear answer to whether the modelâ€™s output solves the query or\nnot. This means we focus on tasks that are normally evaluated only\nusing binary metrics. In particular, the datasets we are working\nwith are evaluated with one of the following metrics:\nâ€¢Exact match (EM) : The LM output matches the ground-\ntruth reference string exactly.\nâ€¢Quasi-exact match : As defined by Liang et al . [14], this\nmetric equals EM, but after slightly preprocessing output and\nground truth (e.g., by lower-casing, removing whitespace,\npunctuation, and articles).\nâ€¢Equivalent: LM output has to be mathematically equal to\nthe ground truth reference.\nFollowing the terminology introduced by Liang et al. [14], we refer\nto all of these metrics, applied to the whole set of samples, as\naccuracy. For more details on datasets and types of tasks, see Table 1.\nTo train and evaluate the meta-model, we use LM queries as\ninputs, and as outputs we use the performance scores calculated\nbased on the ground-truth references and the LMâ€™s outputs. The\nmetric used to calculate the score is dependent on the task and\ndataset that the query comes from (see Table 1). We append an LM\ntoken â€œ[LMğ‘– ]â€ to the input to differentiate for which LM we are\nestimating the probability of answering the query.\nLMs. We opt to work with a set of OpenAI models tested by Liang\net al. [14], for two reasons. First, while the size of these LMs is\nnot publicly available, all of them differ in their capabilities.3 This\ndiversity provides our framework with the flexibility to employ\nless powerful models for simpler queries and more potent models\nfor more complex ones. Second, for this set of models, there is a\nfairly straightforward way to calculate the cost of each sample\nas the price of running the query with the selected LM using the\nOpenAI API. For the cost of the output, which is unknown prior\nto running the respective query, we calculate the average length\nof the outputs (in tokens) as available in the HELM data and apply\nthe same pricing as for the query. For details on pricing at the time\nof training and average output lengths, see Table 2.\nImplementation. Our meta-model is a DistilBERT model4 (66M\nparameters), finetuned on the collected dataset of raw runs. It was\ntrained using the Adam optimizer with learning rate 3 Ã—10âˆ’5 and\n0.1 gradient clipping on the Euclidean norm. The model was trained\nfor 3,000 steps with batch size 16 and a polynomial learning rate\nscheduler with a final learning rate of 0. Training was performed\non a machine with a single Tesla T4 16GB GPU, taking around 2h.\nAs a baseline to compare against, we use a dummy classifier that\nalways predicts the most frequent class, depending on the dataset\nthat the query comes from. It is worth noting that, during inference,\nthe meta-model works only with the query, without the need to\nspecify the dataset from which the query comes.\nEvaluation metrics. To evaluate the performance of the meta-\nmodel, we use standard metrics: accuracy, precision, recall, and F1\n3https://platform.openai.com/docs/models/overview\n4Initialized with the weights of the â€™distilbert-base-uncasedâ€™ model; see https://\nhuggingface.co/distilbert-base-uncased.\nFly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling WSDM â€™24, March 4â€“8, 2024, Merida, Mexico\nscore. We calculate macro scores to give equal weight to each class,\nregardless of its frequency. We additionally calculate ROC-AUC\nand PR-AUC scores. To distinguish between the accuracy of the\nmeta-model and that of the actual language models themselves, we\nrefer to the meta-modelâ€™s accuracy as meta-accuracy.\nResults. In Table 3 we present results of meta-model evaluation.\nAlong with results on the whole testing dataset, we present results\nstratified by datasets, tasks, and LMs. The overall performance of\nthe meta-model is good, as reflected in a high score over all the\nmetrics calculated. The difference between the meta-model and\nthe dummy classifier, though significant, is not big. This happens\nbecause, as indicated by the meta-accuracy of the dummy classifier\n(cf. column 7 of Table 3), in most of the datasets the class imbalance\nis big. As a result, the performance (in terms of meta-accuracy) of\nthe dummy classifier is high. But note that, in a real-world setting,\nwe do not have access to the dataset which the query comes from\n(it might even correspond to a task not seen during meta-model\ntraining), which is essential for the dummy classifier to work. As\nsuch, it is not a practical option, but merely an evaluation baseline.\nOur meta-model does not require knowing the task and can be used\nwith arbitrary queries. Finally, the metric we ultimately care about\nis not meta-accuracy, but rather the accuracy of the LM chosen\nbased on the meta-modelâ€™s predictions (as evaluated in Sec. 4.2).\nStratification over datasets and tasks uncovers that there are\ncertain types of datasets and tasks for which it is harder to predict\nLM performance. These are more complex tasks, which are not\ntrivial for any of the LMs, such as subtypes of the reasoning task.\nStratification over LMs shows that it is harder to predict the\nbehavior of the bigger LMs. Smaller LMs are generally able to work\non simpler tasks (for example, see CivilComments plot in Fig. 4 and\nSentiment analysis plot in Fig. 5), but they fail on more complex\ntasks (see GSM8K plot in Fig. 4). Bigger LMs are, on the other hand,\nable to solve some complex tasks, which means that the meta-model\nhas to understand more complicated patterns to correctly predict\nwhether an LM can solve the task or not.\nCalibration. We assess calibration via calibration plots obtained\nby grouping output probabilities into bins of equal width and plot-\nting the fraction of positive samples against the mean value of the\nbin. In Fig. 2a we can see that the meta-model is well calibrated, as\nthe calibration curve lies close to the diagonal representing perfect\ncalibration. (For task-specific calibration plots, see Fig. 2b.)\nGeneralization. In an attempt to evaluate how well this meta-\nmodel would generalize, we retrain it on all datasets but one at a\ntime. We then evaluate the performance of these meta-models on\nthe respective left-out dataset. Results are presented in Table 4. By\ncomparing these numbers with the ones presented in Table 3, we\ncan notice that, although the numbers are generally a little lower,\nthe meta-model is able to generalize to the left-out dataset.\n4.2 Framework evaluation\nImplementation. In order to evaluate the performance of the\nframework as a whole, we assign each sample in the test set to\none of the LMs in our pool (or none) and calculate the accuracy\nachieved by running these samples with the assigned LMs. The\nassignment is done by applying different strategies to the outputs of\nthe meta-model. Additionally, we evaluate cost as described earlier\nin Sec. 4.1. Based on those two values, we make a costâ€“accuracy plot.\nCostâ€“accuracy plots (cf. Fig. 3) present the relationship between\naccuracy achieved using the chosen strategy and the average cost\nper query needed to perform the run. For cost-sensitive strategies,\nwe run this analysis for a range of cost constraints to trace the\ncostâ€“performance trade-off.\nWe opt to evaluate only one ILP based strategyâ€”the cost-oriented\noneâ€”because the performance-oriented one can be seen as the other\nside of the same coin. While in a practical sense, the performance-\noriented ILP strategy is useful because it offers the user the possi-\nbility to estimate the budget needed for the desired quality of the\nresults, this is visible from the cost-oriented strategy results on the\ncostâ€“accuracy plot when we run it for different cost constraints.\nTo solve the ILP problem for the cost-oriented strategy, we use\nthe PuLP5 library with the PULP_CBC_CMD solver. Given the sim-\nplicity of the ILP, the total time of assignment for all the queries is\novershadowed by the run time of the meta-model. For our testing\ndataset, obtaining all probabilities from the meta-model takes a few\nminutes, while solving the ILP takes only a fraction of a second.\nNonetheless, we leave the option to the user to specify a time limit\nfor ILP execution when working with bigger datasets. If the prob-\nlem is not solved by the time it reaches the limit, the user will be\nleft with a possibly suboptimal solution.\nOracle. We calculate the optimal assignment on the ground-truth\ndata by always choosing the cheapest LM that solves the sample.\nWhen no LM solves the sample, we assume that the best option is\nnot to send this sample to any of the available LMs. The sample\nis counted as incorrect when calculating the accuracy, and with a\ncost of zero when calculating the average cost.\nResults. In Fig. 3, we present the results of the framework evalua-\ntion in the form of a costâ€“accuracy plot. First, we can see that the\noracle is not only cheaper than choosing the biggest LM (84.46%\nlower cost), but it also performs better (+10.74% in accuracy). This\nmeans that there are cases where a smaller LM performs not only\non par, but better than the bigger options. Next, looking at the re-\nsults under single-model strategies, we can notice a clear difference\nbetween the LMs. For comparison, the cheapest LM (text-ada-001)\nis 98.05% cheaper than the biggest LM (text-davinci-002), while\nachieving 18.18% (absolute) smaller accuracy. There is no signifi-\ncant difference between the two smallest models (text-ada-001 and\ntext-babbage-001), neither in cost nor in accuracy.\nTwo additional cost-insensitive strategies, the performance-max-\nimizing approach and the thresholding strategy, exhibit comparable\nperformance in terms of accuracy to the largest model (text-davinci-\n002). Notably, the thresholding strategy stands out for not only its\neffectiveness but also its much lower cost. In terms of cost effective-\nness, it offers a 62.1% (absolute) reduction relative to text-davinci-\n002, while the performance-maximizing strategy offers an 11.5%\ncost reduction relative to text-davinci-002.\nNext, we focus on the cost-sensitive strategies. Note that, for each\nof these, Fig. 3 shows multiple points, one per maximum available\nbudget (corresponding to the value on the ğ‘¥-axis). By employing\ncost-sensitive strategies, we are able to further save budget, while\n5https://coin-or.github.io/pulp/\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico Å akota, Peyrard, and West\nMeta-model Dummy classifier (majority label per dataset)\nMeta-accuracy Precision Recall F1 ROC-AUC PR-AUC Meta-accuracy Precision Recall F1\nOverall 81.57*Â±0.78 79.90*Â±0.81 80.65*Â±0.64 80.26*Â±0.75 80.62*Â±0.83 79.30*Â±0.85 79.60*Â±0.69 78.03*Â±0.89 77.25*Â±0.79 77.62*Â±0.71\nDataset\nMMLU 73.08* Â±3.63 69.22*Â±6.48 61.63*Â±4.64 61.89*Â±5.25 61.96*Â±4.37 58.47Â±9.09 68.59*Â±4.81 34.32*Â±2.51 50.00*Â±0.00 40.69*Â±1.51\nRAFT 67.59* Â±5.37 68.98*Â±5.88 65.64*Â±5.52 65.76*Â±5.42 65.83*Â±5.37 79.60Â±4.12 55.33*Â±5.92 27.71*Â±2.61 50.00*Â±0.00 35.48*Â±2.52\nWikiFact 87.21 Â±0.82 71.06*Â±2.50 62.25*Â±1.77 64.66*Â±1.98 62.36*Â±1.85 44.94*Â±3.99 86.95Â±0.95 43.47*Â±0.47 50.00*Â±0.00 46.52*Â±0.25\nEntity matching 90.00*Â±1.86 94.87*Â±1.04 57.67*Â±3.44 60.36*Â±4.73 57.25*Â±3.35 94.83*Â±1.15 88.10*Â±2.07 44.05*Â±0.93 50.00*Â±0.00 46.83*Â±0.65\nData imputation 81.02*Â±4.78 77.10*Â±7.17 70.32*Â±5.79 71.90*Â±7.00 69.31*Â±6.17 90.75*Â±2.94 73.92*Â±5.47 37.07*Â±2.44 50.00*Â±0.00 42.41*Â±1.74\nBoolQ 64.82* Â±3.55 65.33*Â±5.19 56.93*Â±2.83 54.07*Â±4.15 56.94*Â±2.66 80.92Â±2.05 60.85*Â±3.49 30.47*Â±2.05 50.00*Â±0.00 37.84*Â±1.52\nTruthfulQA 76.02* Â±4.06 70.48*Â±5.08 68.05*Â±5.12 68.88*Â±4.79 67.81*Â±4.98 61.79Â±7.87 71.51Â±4.50 35.57*Â±2.16 50.00*Â±0.00 41.55*Â±1.67\nIMDB 86.35* Â±2.71 43.42*Â±1.42 49.49*Â±0.39 46.30*Â±0.74 49.51*Â±0.43 93.48*Â±1.07 86.90*Â±2.67 43.58*Â±1.36 50.00*Â±0.00 46.54*Â±0.87\nbAbI 74.57* Â±3.72 73.48*Â±3.22 74.11*Â±3.55 73.78*Â±3.83 73.95*Â±3.51 75.18*Â±3.64 59.16*Â±4.26 29.63*Â±1.91 50.00*Â±0.00 37.18*Â±1.45\nMATH 92.89 Â±3.19 75.07Â±26.13 52.42Â±3.57 52.91Â±8.01 52.38Â±4.01 56.43Â±5.11 92.56Â±2.84 46.05Â±1.45 50.00Â±0.00 48.01Â±0.84\nGSM8K 90.82 Â±1.77 45.51Â±0.89 50.00Â±0.00 47.67Â±0.61 50.00Â±0.00 54.50Â±1.11 91.05Â±1.99 45.54Â±0.90 50.00Â±0.00 47.64Â±0.58\nLSAT 76.83 Â±4.58 38.48Â±2.13 50.00Â±0.00 43.37Â±1.63 50.00Â±0.00 61.54Â±2.38 77.19Â±4.68 38.28Â±2.56 50.00Â±0.00 43.60Â±1.68\nLegalSupport 49.45 Â±3.30 48.97*Â±5.16 49.30Â±2.99 44.17*Â±3.30 49.46Â±3.03 70.06*Â±2.78 50.58Â±4.55 25.28*Â±1.91 50.00Â±0.00 33.61*Â±1.57\nCivilComments 79.22 Â±3.01 39.57Â±1.80 50.00Â±0.00 44.17Â±0.99 50.00Â±0.00 89.58Â±1.64 79.04Â±2.88 39.68Â±1.48 50.00Â±0.00 44.10Â±0.98\nTask\nQA 83.40* Â±0.88 74.09Â±1.48 71.00*Â±1.56 72.25*Â±1.40 70.96*Â±1.33 60.07*Â±2.30 82.32*Â±0.74 72.85Â±1.91 62.73*Â±1.33 65.06*Â±1.59\nSentiment analysis 86.34*Â±2.50 43.48*Â±1.31 49.51*Â±0.47 46.32*Â±0.78 49.54*Â±0.43 93.45*Â±1.23 86.85*Â±2.63 43.47*Â±1.21 50.00*Â±0.00 46.54*Â±0.72\nToxicity detection 79.27Â±2.96 39.58Â±1.51 50.00Â±0.00 44.23Â±0.89 50.00Â±0.00 89.59Â±1.47 79.07Â±3.46 39.53Â±1.69 50.00Â±0.00 44.23Â±0.85\nText classification 68.11*Â±5.08 69.00*Â±6.00 65.84*Â±5.43 65.36*Â±5.77 65.17*Â±4.94 79.65Â±3.97 55.14*Â±5.79 27.73*Â±2.96 50.00*Â±0.00 35.33*Â±2.49\nReasoning 74.64* Â±1.67 69.80*Â±1.88 70.79*Â±2.03 70.18*Â±1.89 70.72*Â±1.72 64.49*Â±2.81 70.87*Â±1.58 64.26*Â±2.06 63.28*Â±2.21 63.58*Â±1.96\nLM\nada 85.18* Â±1.28 82.33*Â±1.56 83.16Â±1.53 82.77*Â±1.51 83.15Â±1.47 79.41Â±1.93 83.20*Â±1.54 80.02*Â±1.55 82.01Â±1.53 80.84*Â±1.63\nbabbage 84.37 Â±1.42 81.67Â±1.35 83.08Â±1.45 82.21Â±1.67 83.13Â±1.67 79.52Â±1.79 83.34Â±1.40 80.63Â±1.61 82.02Â±1.44 81.23Â±1.55\ncurie 80.88* Â±1.54 78.83*Â±1.36 79.99*Â±1.70 79.23*Â±1.68 80.02*Â±1.63 77.49*Â±1.93 79.27*Â±1.48 77.10Â±1.44 76.75*Â±1.63 77.07*Â±1.61\ndavinci 75.84* Â±1.39 75.83*Â±1.51 75.75*Â±1.54 75.83*Â±1.49 75.73*Â±1.58 81.01*Â±1.54 72.44*Â±1.44 74.30Â±1.42 71.91*Â±1.61 71.52*Â±1.81\nTable 3: Performance of the meta-model. Precision, recall, and F1 are macro-averaged over classes. We use threshold 0.5 for\nthese metrics. The meta-model is compared to a dummy classifier that outputs the most probable class in the dataset from\nwhich the sample comes. Results are stratified by dataset, task, and LM and presented with 95% confidence intervals. We do not\nreport ROC-AUC and PR-AUC for the dummy classifier because the dummy classifier is threshold-independent and therefore\nthese two metrics are not informative for it. Asterisks (*) mark results where the meta-model (left) performs statistically\nsignificantly (ğ‘ < 0.05) differently from the dummy classifier (right).\n0.0 0.2 0.4 0.6 0.8\nOutput probability\n0.0\n0.2\n0.4\n0.6\n0.8Fraction solved\nmeta-model\nperfect calibration\n(a) Calibration plot when pooling all 14 datasets\n0.0 0.2 0.4 0.6 0.8\n0.0\n0.2\n0.4\n0.6\n0.8\nQA\n0.2 0.4 0.6 0.8\n0.2\n0.4\n0.6\n0.8\n1.0\nSentiment analysis\n0.2 0.4 0.6 0.8\n0.2\n0.4\n0.6\n0.8\nT oxicity detection\n0.2 0.4 0.6 0.8\n0.0\n0.2\n0.4\n0.6\n0.8\nT ext classification\n0.0 0.2 0.4 0.6 0.8\n0.0\n0.2\n0.4\n0.6\n0.8\nReasoning\nmeta-model perfect calibration (b) Calibration plots per task\nFigure 2: Calibration plots. Plots are obtained by grouping the meta-modelâ€™s output probabilities into bins of equal width and\nthen plotting the fraction of positive samples against the mean value of the bin.\nachieving essentially the same accuracy as the largest model (text-\ndavinci-002). In particular, with the cost-oriented ILP strategy, we\nmatch text-davinci-002â€™s accuracy for only 37.21% of the cost. The\ngreedy strategy, on the other hand, performs much worse: in this\ncase, we need 88.57% of the budget for the same result.\nAlthough the oracle indicates that there is space for improve-\nments in terms of accuracy as well, we did not manage to achieve\nbetter accuracy than text-davinci-002 with any budget given to the\nframework. This happens because our meta-model is not able to\ncorrectly identify the cases when text-davinci-002 fails to do the\nFly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling WSDM â€™24, March 4â€“8, 2024, Merida, Mexico\nDataset Meta-acc. Precision Recall F1 ROC-AUC PR-AUC\nMMLU 71.78 Â±4.51 67.85Â±7.80 58.29Â±3.91 56.95Â±5.57 57.77Â±4.34 55.12Â±8.77\nRAFT 54.26 Â±5.73 53.49Â±5.91 52.88Â±5.01 52.71Â±5.80 53.01Â±5.87 70.87Â±5.37\nWikiFact 72.80 Â±1.29 57.49Â±1.34 62.93Â±1.96 57.54Â±1.47 62.78Â±1.96 39.82Â±2.37\nEntity matching 77.00Â±2.53 55.07Â±2.90 57.80Â±4.50 55.58Â±3.91 57.03Â±4.72 94.02Â±1.36\nData imputation 71.50Â±5.41 64.23Â±6.03 66.10Â±6.82 64.63Â±6.68 66.57Â±6.97 88.93Â±3.04\nBoolQ 59.22 Â±3.82 59.30Â±3.42 59.45Â±4.06 59.23Â±3.88 59.80Â±3.95 76.95Â±3.38\nTruthfulQA 75.04Â±4.02 68.97Â±5.80 65.10Â±4.36 66.14Â±4.88 65.32Â±4.97 58.89Â±8.24\nIMDB 38.99 Â±3.52 52.50Â±2.67 54.34Â±4.74 36.90Â±4.02 54.76Â±4.80 90.75Â±2.74\nbAbI 64.38 Â±3.75 62.75Â±4.24 60.55Â±3.85 60.33Â±3.98 60.19Â±4.19 61.79Â±5.14\nMATH 80.90 Â±4.27 58.41Â±5.52 70.96Â±11.44 60.55Â±8.06 71.41Â±9.83 43.11Â±14.01\nGSM8K 89.55 Â±2.24 65.46Â±6.91 60.36Â±6.10 61.95Â±6.22 60.37Â±5.76 33.90Â±13.61\nLSAT 74.18 Â±4.71 51.70Â±11.05 50.44Â±3.83 47.64Â±5.10 50.37Â±3.18 26.09Â±14.69\nLegalSupport 49.88Â±3.80 49.49Â±3.77 49.65Â±3.63 48.89Â±3.99 49.46Â±3.73 65.85Â±3.73\nCivilComments 71.36Â±3.50 41.81Â±3.46 45.99Â±2.03 43.44Â±2.09 46.10Â±1.61 87.78Â±1.73\nTable 4: Leave-one-out evaluation. Performance of meta-\nmodel on a dataset left-out in training. Each row of the table\ntrains a meta-model on all datasets except the one denoted\nin the row. Testing is then performed on the denoted dataset.\n0.000 0.002 0.004 0.006 0.008 0.010\nAverage cost\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Accuracy\n(1)\n(2)\n(3)\n(4)\n(a)\n(b)\nsingle-model\noracle\nthresholding\nperformance-maximizing\ncost-oriented ILP\ngreedy\nFigure 3: Cost-accuracy plot. Accuracy and average cost per\nquery (in US$) achieved by assigning every query from the\nquery set to an LM from the LM pool. The plot shows results\nobtained using assignment strategies from Sec. 3.2. Single\nmodel strategies for each LMs are marked by the number\nunder them: (1) text-ada-001 (2) text-babbage-001 (3) text-\ncurie-001 (4) text-davinci-002. Two thresholding strategies\nfor cases when none of the LMs solve the data sample are\nmarked by a letter under them: choosing (a) the biggest and\n(b) the smallest LM. Error bars are 95% confidence intervals.\ntask correctly, but some of the smaller models successfully do. This\nscenario happens in 10.72% of cases, but we manage to identify it\nonly 0.68% of the time. For comparison, 33.03% of data samples are\nsuccessfully solved by both text-davinci-002 and one of the smaller\nmodels, and we correctly recognize this scenario 78.56% of the time.\nIn Fig. 4 and Fig. 5 we present costâ€“accuracy plots stratified by\ndataset and task, respectively. First, by focusing on the oracle and\ntext-davinci-002 results on these plots, we spot that, for most of\nthe datasets and tasks, the oracle indicates that there is the same\npotential to improve both the accuracy and the cost as in the overall\ncase. In rare cases (e.g., MATH, GSM8K) there is not much space for\nimprovement in terms of accuracy, as small LMs fail to do the task\nin almost all cases.\nSecond, patterns similar to the ones present in Fig. 3 are also\nvisible for most of the datasets and tasks. For example, using the\ncost-oriented ILP strategy on the question-answering (QA) task,\nwe are able to achieve the same performance as text-davinci-002\nfor 71.76% of the price. For the reasoning and the text classification\ntask, these percentages are equal to 83.41% and 19.15%, respectively.\nFor some of the tasks, smaller LMs are also able to solve the\nqueries either in the same way or even better than bigger LMs. As an\nexample of the former, for the sentiment analysis task, text-babbage-\n001 drops only 4.83% in accuracy compared to text-davinci-002 for\njust 2.35% of its price. In the latter case, on the toxicity detection task,\nusing text-ada-001 results in a 19.87% jump in accuracy compared\nto text-davinci-002, while spending only 1.99% of the budget used\nfor running text-davinci-002.\nThanks to this, depending on the dataset, there are cases where\nusing the meta-model helps us achieve both lower cost and higher\nperformance than text-davinci-002. For example, on the CivilCom-\nments dataset (which is a part of the toxicity detection task), using\nthe cost-oriented ILP strategy results in 20.1% (absolute) higher\naccuracy for only 1.99% of the cost of running text-davinci-002.\nThis result essentially matches the oracle for this dataset.\n5 DISCUSSION\n5.1 Further use cases of the FORC framework\nIn the previous sections (cf. Sec. 3.2 and Sec. 4.2), we introduced and\nevaluated two ILP-based strategies: cost-oriented and performance-\noriented. This is, however, not the only potential use of our frame-\nwork. The user can form an objective function as a custom combi-\nnation of the terms involving cost and performance, and, similarly,\nuse any combination of the cost and performance constraints. This\nallows for increased flexibility in practical applications.\nAdditionally, FORC need not be used only to make a decision\nabout an LM to run a certain query. By fixing the LM and calculating\nthe output probabilities for different phrasings of the desired query,\none could use the framework to identify the best possible prompt\nto send to the LM, without the need to spend money by sending\nthem to the LM directly. For this application, it would be advisable\nto retrain the meta-model to fit the task better, as the current meta-\nmodel was not exposed to small changes in queries during training,\nand its behavior in such cases is unknown.\n5.2 Practical considerations\nWith the emergence of new LMs, our meta-model may become\nstale, as it will not include the possibility to use the newer LMs\nwith likely better abilities. Additionally, as shown by Chen et al. [4],\nthe behavior of GPT-3.5 and GPT-4 is changing over time. While in\nthis paper we do not focus on these two LMs, it is not unreasonable\nto assume that, with newer versions, the LMs we have been working\nwith exhibit the same behavioral shifts. To minimize the effect of\nthese changes on our framework, meta-model should be retrained\nto take into account any updates in the existing LMs as well as the\nnewly introduced LMs. Because our meta-model is small, training\nis cheap and takes little time, so it is easy to retrain frequently.\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico Å akota, Peyrard, and West\n0.000 0.005 0.010 0.015 0.020 0.025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n RAFT\n0.000 0.002 0.004 0.006 0.008 0.010\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n MMLU\n0.0000 0.0005 0.0010 0.0015\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n WikiFact\n0.000 0.005 0.010 0.015\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Entity matching\n0.000 0.001 0.002 0.003 0.004 0.005 0.006\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Data imputation\n0.000 0.005 0.010 0.015 0.020 0.025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n BoolQ\n0.000 0.002 0.004 0.006 0.008 0.010\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n TruthfulQA\n0.00 0.01 0.02 0.03\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n IMDB\n0.000 0.002 0.004 0.006 0.008\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n bAbI\n0.000 0.002 0.004 0.006 0.008\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n MATH\n0.0000 0.0025 0.0050 0.0075 0.0100 0.0125\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n GSM8K\n0.000 0.005 0.010 0.015 0.020\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n LSAT\n0.000 0.002 0.004 0.006 0.008 0.010 0.012\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n LegalSupport\n0.000 0.002 0.004 0.006 0.008\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n CivilComments\nAverage cost\nAccuracy\nsingle-model oracle thresholding performance-maximizing cost-oriented ILP greedy\nFigure 4: Costâ€“accuracy plot per dataset. Accuracy and average cost per query (in US$) achieved by employing strategies from\nSec. 3.2 on the test dataset stratified by datasets from Table 1. Single-model and thresholding strategies follow the same trends\nas in Fig. 3. Error bars are 95% confidence intervals.\n0.000 0.001 0.002 0.003 0.004 0.005\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n QA\n0.00 0.01 0.02 0.03\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Sentiment analysis\n0.000 0.002 0.004 0.006 0.008\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Toxicity detection\n0.000 0.005 0.010 0.015 0.020 0.025\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Text classification\n0.000 0.002 0.004 0.006 0.008 0.010 0.012\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n Reasoning\nAverage cost\nAccuracy\nsingle-model oracle thresholding performance-maximizing cost-oriented ILP greedy\nFigure 5: Costâ€“accuracy plot per task. Accuracy and average cost per query (in US$) achieved by employing strategies from\nSec. 3.2 on the test dataset stratified by tasks from Table 1. Single-model and thresholding strategies follow the same trends as\nin Fig. 3. Error bars are 95% confidence intervals.\nIn addition, we calculated the costs of running different LMs\nwith the pricing from OpenAI at the time of meta-model training.\nThese prices were changing in the past, and will probably change in\nthe future. When such changes happen, the cost parameters should\nbe updated in the framework, as it would affect the costâ€“accuracy\nplots and, consequently, the assignment of data samples to LMs.\n6 CONCLUSION\nIn this paper, we address the problem of the increasing costs of\nrunning LMs for various tasks. We introduce â€œFly-swat or cannonâ€\n(FORC), a framework for automatically deciding which LM is the\nbest option for a given input query. By employing FORC to the\nunion of 14 datasets, we are able to reduce costs by 62.79% while\nmaintaining the same performance as the biggest LM in our pool.\nWe manage to substantially cut costs on all types of task, and on\nmost of the datasets we evaluate on. We also release a library that\nenables easy use of our framework, as well as further developments.\nFly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling WSDM â€™24, March 4â€“8, 2024, Merida, Mexico\nACKNOWLEDGEMENTS\nWe thank Chris Wendler for helpful feedback. Robert Westâ€™s lab is\npartly supported by grants from Swiss National Science Foundation\n(200021_185043, TMSGI2_211379), H2020 (952215), Microsoft Swiss\nJoint Research Center, and Google, and by generous gifts from\nFacebook, Google, and Microsoft.\nREFERENCES\n[1] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael R Lyu.\n2022. Towards Efficient Post-training Quantization of Pre-trained Language\nModels. In Advances in Neural Information Processing Systems , S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Asso-\nciates, Inc., 1405â€“1418. https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/096347b4efc264ae7f07742fea34af1f-Paper-Conference.pdf\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\narXiv:2005.14165 [cs.CL]\n[3] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. FrugalGPT: How to Use\nLarge Language Models While Reducing Cost and Improving Performance.\narXiv:2305.05176 [cs.LG]\n[4] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is ChatGPTâ€™s behavior\nchanging over time? arXiv:2307.09009 [cs.CL]\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling\nwith Pathways. arXiv:2204.02311 [cs.CL]\n[6] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason\nPhang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and\nAndy Zou. 2021. A framework for few-shot language model evaluation . https:\n//doi.org/10.5281/zenodo.5371628\n[7] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,\nand Kurt Keutzer. 2021. A Survey of Quantization Methods for Efficient Neural\nNetwork Inference. arXiv:2103.13630 [cs.CV]\n[8] Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowl-\nedge Distillation: A Survey. International Journal of Computer Vision 129, 6 (mar\n2021), 1789â€“1819. https://doi.org/10.1007/s11263-021-01453-z\n[9] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.\n2021. Sparsity in Deep Learning: Pruning and growth for efficient inference and\ntraining in neural networks. Journal of Machine Learning Research 22, 241 (2021),\n1â€“124. http://jmlr.org/papers/v22/21-0366.html\n[10] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den\nDriessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich\nElsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training Compute-\nOptimal Large Language Models. arXiv:2203.15556 [cs.CL]\n[11] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\nWang, and Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language\nUnderstanding. In Findings of the Association for Computational Linguistics:\nEMNLP 2020 . Association for Computational Linguistics, Online, 4163â€“4174.\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.372\n[12] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling Laws for Neural Language Models. arXiv:2001.08361 [cs.LG]\n[13] Eldar Kurtic, Elias Frantar, and Dan Alistarh. 2023. ZipLM: Hardware-Aware\nStructured Pruning of Language Models. arXiv:2302.04089 [cs.LG]\n[14] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\nBenjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,\nChristopher D. Manning, Christopher RÃ©, Diana Acosta-Navas, Drew A. Hudson,\nEric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,\nMirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,\nWilliam Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022.\nHolistic Evaluation of Language Models. arXiv:2211.09110 [cs.CL]\n[15] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out . Association for Computational Linguistics,\nBarcelona, Spain, 74â€“81. https://aclanthology.org/W04-1013\n[16] Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller,\nNajoung Kim, Sam Bowman, and Ethan Perez. 2022. The Inverse Scaling Prize.\nhttps://github.com/inverse-scaling/prize\n[17] Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller,\nNajoung Kim, Sam Bowman, and Ethan Perez. 2022. Inverse Scaling Prize: First\nRound Winners. https://irmckenzie.co.uk/round1\n[18] Ian McKenzie, Alexander Lyzhov, Alicia Parrish, Ameya Prabhu, Aaron Mueller,\nNajoung Kim, Sam Bowman, and Ethan Perez. 2023. Inverse Scaling Prize: Second\nRound Winners. https://irmckenzie.co.uk/round2\n[19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs.CL]\n[20] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb,\nAbubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta,\nAdriÃ  Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal,\nAlethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali\nTazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders\nAndreassen, Andrea Madotto, Andrea Santilli, Andreas StuhlmÃ¼ller, Andrew\nDai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen,\nAnh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh,\nArash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher\nMullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla\nKarakaÅŸ, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, BartÅ‚omiej Bojanowski,\nBatuhan Ã–zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden,\nBenno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion,\nCameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, CÃ©sar Ferri\nRamÃ­rez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu\nWu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Man-\nning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin\nRaffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan\nHendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel MoseguÃ­ GonzÃ¡lez, Danielle Perszyk, Danny Hernandez, Danqi\nChen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jur-\ngens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret,\nDerek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova,\nEkin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth\nDonoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang,\nErkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim,\nEunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fer-\nnando MartÃ­nez-Plumed, Francesca HappÃ©, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, GermÃ¡n Kruszewski, Giambattista\nParascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-LÃ³pez, Gregor\nBetz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh\nHajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich SchÃ¼tze, Hiromu\nYakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime FernÃ¡ndez\nFisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan KocoÅ„, Jana\nThompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein,\nJason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jen-\nnifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu,\nJiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis,\nJonathan Batchelder, Jonathan Berant, JÃ¶rg Frohberg, Jos Rozen, Jose Hernandez-\nOrallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum,\nJoshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik\nGopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin\nGimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Ku-\nmar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang,\nLiam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency,\nLuca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliv-\neros ColÃ³n, Luke Metz, LÃ¼tfi Kerem Åenel, Maarten Bosma, Maarten Sap, Maartje\nter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan,\nMarco Marelli, Marco Maru, Maria Jose RamÃ­rez Quintana, Marie Tolkiehn, Mario\nWSDM â€™24, March 4â€“8, 2024, Merida, Mexico Å akota, Peyrard, and West\nGiulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen,\nMÃ¡tyÃ¡s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath,\nMichael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt,\nMichael Strube, MichaÅ‚ SwÄ™drowski, Michele Bevilacqua, Michihiro Yasunaga,\nMihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit\nBansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron,\nNicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers,\nNiklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant,\nNoah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy,\nOwain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu\nLiang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter\nChang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr MiÅ‚kowski, Piyush\nPatil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco,\nRaphaÃ«l MilliÃ¨re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa,\nRobbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew,\nRonan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov,\nRyan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel\nGruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian\nBischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Ham-\ndan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi,\nShixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay,\nShyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva\nReddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar,\nStanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin,\nStephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi,\nSvetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao\nYu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, ThÃ©o Desbordes, Theodore Roth-\nschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei\nKornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj,\nTushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria\nNyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmaku-\nmar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout\nVossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah\nYaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao\nBai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and\nZiyi Wu. 2023. Beyond the Imitation Game: Quantifying and extrapolating the\ncapabilities of language models. arXiv:2206.04615 [cs.CL]\n[21] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.\narXiv:2302.13971 [cs.CL]\n[22] Leandro von Werra, Lewis Tunstall, Abhishek Thakur, Alexandra Sasha Luccioni,\nTristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar,\nHelen Ngo, Omar Sanseviero, Mario Å aÅ¡ko, Albert Villanova, Quentin Lhoest,\nJulien Chaumond, Margaret Mitchell, Alexander M. Rush, Thomas Wolf, and\nDouwe Kiela. 2022. Evaluate & Evaluation on the Hub: Better Best Practices for\nData and Model Measurements. arXiv:2210.01970 [cs.LG]\n[23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song\nHan. 2023. SmoothQuant: Accurate and Efficient Post-Training Quantization for\nLarge Language Models. In Proceedings of the 40th International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 202) , Andreas\nKrause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett (Eds.). PMLR, 38087â€“38099. https://proceedings.mlr.press/\nv202/xiao23c.html\n[24] Shi Zong, Josh Seltzer, Jiahua, Pan, Kathy Cheng, and Jimmy Lin. 2023. Which\nModel Shall I Choose? Cost/Quality Trade-offs for Text Classification Tasks.\narXiv:2301.07006 [cs.CL]"
}