{
  "title": "Do large language models know chemistry?",
  "url": "https://openalex.org/W4284882687",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2105096877",
      "name": "Andrew D. White",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2298549838",
      "name": "Glen M. Hocky",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2888500310",
      "name": "Heta A. Gandhi",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3153332890",
      "name": "Mehrad Ansari",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2566604316",
      "name": "Sam Cox",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3041959245",
      "name": "Geemi P. Wellawatte",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A4311130878",
      "name": "Subarna Sasmal",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2167460620",
      "name": "Ziyue Yang",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A3171243909",
      "name": "Kangxin Liu",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2133605956",
      "name": "Yuvraj Singh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3211103814",
      "name": "Willmor J. Peña Ccoa",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600109629",
    "https://openalex.org/W6603976542",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6600950960",
    "https://openalex.org/W6600505529",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W6612613465",
    "https://openalex.org/W6678156880",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3171707284",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2560511802",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W1965092590",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4280597794",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W3099878876",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Mostly yes. We systematically evaluate machine learning large language models (LLMs) that generate code in the context of chemistry. We produce a benchmark set of problems, and evaluate these models based on correctness of code by automated testing and evaluation by experts. We find recent LLMs are able to write correct code across a variety of topics in chemistry and their accuracy can be increased by 30 percentage points via prompt engineering strategies, like putting copyright notices at the top of files. These dataset and evaluation tools are open source which can be contributed to or built upon by future researchers, and will serve as a community resource for evaluating the performance of new models as they emerge. We also describe some good practices for employing LLMs in chemistry. The general success of these models demonstrates that their impact on chemistry teaching and research is poised to be enormous.",
  "full_text": "Do large language models know chemistry?\nAndrew D. White,1, ∗ Glen M. Hocky,2 Heta A. Gandhi,1 Mehrad Ansari,1 Sam Cox,1 Geemi P.\nWellawatte,3 Subarna Sasmal,2 Ziyue Yang,1 Kangxin Liu,2 Yuvraj Singh,2 and Willmor J. Peña Ccoa2\n1Department of Chemical Engineering, University of Rochester\n2Department of Chemistry, New York University †\n3Department of Chemistry, University of Rochester\n(Dated: July 6, 2022)\nMostly yes. We systematically evaluate machine learning large language models (LLMs) that\ngenerate code in the context of chemistry. We produce a benchmark set of problems, and evaluate\nthese models based on correctness of code by automated testing and evaluation by experts. We\nfind recent LLMs are able to write correct code across a variety of topics in chemistry and their\naccuracy can be increased by 30 percentage points via prompt engineering strategies, like putting\ncopyright notices at the top of files. These dataset and evaluation tools are open source which can\nbe contributed to or built upon by future researchers, and will serve as a community resource for\nevaluating the performance of new models as they emerge. We also describe some good practices for\nemploying LLMs in chemistry. The general success of these models demonstrates that their impact\non chemistry teaching and research is poised to be enormous.\nI. INTRODUCTION\nLarge language models (LLMs) are multi-billion pa-\nrameter transformer neural networks1 that are trained\non enormous collections of documents (corpus) without\nsupervision or labels.2 LLMs can do multiple tasks like\nclassifying natural language, translating text, and doc-\nument search. Perhaps the most remarkable task of\nLLMs is to complete an input string of text; via this\nmechanism (called causal language modeling), LLMs can\nwrite unit tests, document function, write code from a\ndoc string, answer questions, and complete stoichiomet-\nric equations.3,4\nWe previously discussed the outlook of LLMs in\nchemistry.5 In the few months since then, LLMs have\nbeen both developed for specific chemistry problems6,7\nand general LLMs have been applied in chemistry.8,9 An\nopen question for LLMs such as GPT-3,3 T5,10 or GPT-\nneo11 that are trained on very large and varied textual\ndata is if they can be applied in domains like chemistry,\nwhich have specialized language and knowledge. In our\ninitial work,5 we found relationships between SMILES\nand natural language was possible with GPT-3. SMILES\nis the the standard method of representing molecules as\nstrings.12 It is even possible to loosely edit structures via\nnatural language (see Fig. 5).13,14 However, the extent\nto which LLMs can be directly applied in chemistry in\nthe broad context of research and teaching is unexplored.\nThe large amount of specific domain knowledge required\nto solve chemistry problems may limit applicability of\ngeneral LLMs. For example, recent work has found that\nknowledge of the periodic table of elements requires very\nhigh parameter counts.4\nRecent comparisons of LLMs that generate code can\n∗ andrew.white@rochester.edu\n† hockyg@nyu.edu\nbe found Ref 15. Here, we focus our study on whether\nLLMs that generate code16 can be applied to chemistry\ntasks of a computational nature (both computational\nchemistry problems, and general tasks which can be ex-\npressed as simple computer programs, such as ranking\nelementsbyionicradius). MostLLMsthatgeneratecom-\nputer code are causal decoder-only models16–18—a user\nprovides a sequence of text (called the prompt) and it\nproposes a continuation of the text (the completion).19\nThere are LLMs trained on code that can infill or match\nencoder/decoder natural language to code like Code-\nBERT,20 but they are typically used for embedding code\nfor tasks like classification, document retrieval, or trans-\nlating code to natural language.\nEvaluating LLMs’ knowledge of chemistry should\nbe distinguished from capability to reason or under-\nstand. LLMs can make compelling completions, but\nare incapable of reasoning and demonstrate superficial\nunderstanding.21,22 Our goal is to evaluate LLMs’ abil-\nity to correlate natural language, equations, code, and\nheuristics of chemistry.\nII. METHODS\nWe have compiled a categorized set of chemistry\nand related example prompts for benchmarking code-\ngenerating LLMs in a public repository.23 The examples\nspanarangeoftopicsthatweconsidercommonprogram-\nming tasks by students or researchers, with an emphasis\non the discipline of computational chemistry. The distri-\nbution of topics is given in Tab. I. Some examples were\nlabeled as only expert evaluable, where automated evalu-\nation is infeasible or insufficient (e.g. plotting). The total\nnumber of examples is 84, of which 25 were expert evalu-\nable, and the accuracy is 72% for the best performing\nmodel.\nGPT-3 12B fine-tuned on code (Codex) is abbrevi-\nated as “davinci” and a smaller GPT-3 fine tuned on\n2\nPrompt\nimport numpy as np\ndef run_mc(n_steps, T, k):\n    \"\"\"\n    This function performs metropolis monte carlo sampling of a harmonic \n    potential at temperature T and returns the trajectory\n    \"\"\"\n    [insert]\n    return traj\nInserted code - output (1)\n    traj = np.zeros(n_steps)\n    traj[0] = np.random.uniform(-1, 1)\n    for i in range(1, n_steps):\n*       x_new = np.random.uniform(-1, 1)\n        dE = 0.5*k*(x_new**2 - traj[i-1]**2)\n        if dE < 0:\n            traj[i] = x_new\n        else:\n            if np.random.uniform(0, 1) < np.exp(-dE/T):\n                traj[i] = x_new\n            else:\n                traj[i] = traj[i-1]\n(2) x_new = traj[i-1] + np.random.randn()\n(3) x_new = np.random.normal(traj[i-1], 1)\nFIG. 1. Example prompt and code generated for database example ‘mc_harmonic’ . Full output is the prompt with ‘[insert]’\nreplaced by code in lower box. The asterisk indicates a line that is faulty. Inset box shows equivalent lines from two other\nsolutions that are correct, if not necessarily optimal. This example is discussed in Sec. III A.\ncode (Codex) is abbreviated as “cushman.” Since 2021,\nthe models have been adjusted and the specific ver-\nsions used in this work arecode-cushman-001, code-\ndavinci-002. The “incoder” models are two models\nfromFriedetal. 17 trainedoncodeonly. Wechoseincoder\nbecause it is able to infill code in addition to completing\ncode prompts, which gives a more direct comparison, and\nit has generally good performance. Recent benchmarks\nshow davinci is best or nearly the best on general pro-\ngramming tasks.15,24 Incoder was used as implemented\nin HuggingFace transformers.25 To avoid library changes\nsince 2021 influencing accuracy, our evaluations are done\nusing the python version and packages from June 2021.\nThe chosen date was based on reported training range\nfrom Ref. 24 and comes before training time of Ref. 17.\nWhen developing example prompts and solutions,\nthe prompts were tested and modified using davinci.\nSome prompt engineering was inevitable through this\nprocess.3,26,27 However, prompts were not designed to get\na correct answer and some prompts (e.g., two atom har-\nmonic oscillator) were never correctly completed. We do\nemphasize that the reported accuracy is not one would\nexpect of the first prompt constructed on-the-fly for a\ngiven problem. Rather, they are constructed to answer\n“how much chemistry do these LLMs know?” These fig-\nures should not be construed as upper bounds either, as\nrecent work on prompt engineering shows that multiple\nsteps18 or eliciting multiple steps can further improve\naccuracy.28\nFollowing Chen et al.,24 a prompt completion is ac-\ncurate if the code functions correctly, not if it matches\na reference implementation. Most examples have both\na prompt and unit tests. Expert evaluable prompts for\nwhich there are not unit tests are not reported in the\naccuracy, unless specified. Five completions were gen-\nerated via top-k sampling29 and multiple temperatures\nat T = 0.05, 0.2, 0.5 (softmax scaling). We explored nu-\ncleus sampling,30 but found it to be no different than\nadjusting temperature for balancing diversity and cor-\nrectness of completions. We chosek = 5 for all models,\nexcept for incoder-6B where GPU memory limitations\nprevented sampling more thank = 1. Thus, those re-\nsults may be slightly inflated since accuracy is reported\non only a most likely output. Error bars in all plots\nare 95% confidence intervals generated from bootstrap\nresampling across top-k.\nExpert evaluation was performed onk = 3 outputs of\ndavinci (T = 0.2, “insert” context) and accessed through\na web interface.31 Each example contains a link to a cus-\ntom Google form which could be used to evaluate that\nexample, with results saved in a spreadsheet. The multi-\nple choice questions in the form were: “Is this question:\nEasy; Medium; Hard”, “Is the solution: Perfect; Correct\nbut not perfect; Runs and is almost correct; Does not\nrun but is almost correct; Is far from correct.” There\nwas also a box for extra comments. The full set of evalu-\nations, with personally identifiable information (student\nemails) removed, is available as a comma separated value\n(CSV) file in the Supporting Information. To make a\nnumerical evaluation of this data in Fig. 2, we assigned\nscores from 1–5 with 5 being the best (“Perfect”) and 1\nbeing the worst (“Is far from correct”). To compute an\noverall accuracy as reported in Tab. I, we assigned “Per-\nfect”, and “Correct but not perfect” a value of 1.0, and\nall others 0.0, then computed the mean score for each\nprompt separately. It should be noted that each assessor\n3\nTopic N expert incoder-6B davinci\nbio 13 2 0% 43% (0%) 1\ncheminf 10 0 20% 50%\ngenchem 11 0 29% 86%\nmd 11 3 0% 63% (81%)\nplot 10 10 – – (57%)\nqm 8 3 20% 100% (59%)\nsim 8 5 0% 100% (64%)\nspectroscopy 11 1 30% 50%(12%)\nstats 11 1 40% 70% (88%)\nthermo 10 0 10% 80%\ntotal 84 2 23 17% 72% (57%)\nTABLE I. The number of prompts by topic and best accuracy\nachievable in this work. “expert” is the number within a topic\nthat must be evaluated by an expert. We used the “copyright”\ncontext for incoder-6B and “insert” for davinci and T = 0.2.\nResults are averaged across top- k sampling and/or multiple\nexpert evaluators. 1expert evaluator scores are in parentheses.\n2some prompts appear in multiple topics. The abbreviations\nof topics are biochemistry (bio), cheminformatics (cheminf),\ngeneral chemistry (genchm), molecular dynamics & simula-\ntion (md), quantum mechanics (qm), methods of simulation\n(sim), statistics (stats), and thermodynamics (thermo).\nhad a different level of expertise on each topic, as well\nas a different level of python programming experience,\nalthough we feel all were sufficiently expert to evaluate\neach prompt with sufficient authority.\nIII. RESUL TS\nA. Example problem\nTo illustrate the kinds of tasks and impressive (if not\nalways correct) results produced by LLMs, we show the\noutput for one ‘sim’ category tasks in Fig. 1. To stan-\ndardize our tasks, each tasks is phrased as a function to\nbefilledin, asinthetopbox. Thispromptincludesafirst\nline which loads the numerical python (numpy32) library,\nwhich gives additional ‘context’ (see below). The rest of\nthe information for the LLM is contained in two places,\nthe names of the variables given as inputs ‘n_steps’, ‘T’,\n‘k’, and a comment string which says what the function\ndoes/should do. In this case, the function should perform\nMetropolis Monte Carlo for a harmonic potential. Im-\nplicit in the instruction by the creator is that k represents\nthespringconstant, andsothiscodeshouldproducesam-\nples from the energy functionU(x) = 1/2k(x−x0)2, with\nx0 = 0 since it was not specified as an input, and also\nthat reduced units are used, such that Boltzmann’s con-\nstant kB = 1.0. We can see that—with quite minimal\ninstruction—the code in the output is correct except for\nan error on the line indicated with a ‘*’; in this line,\nthe position of the particle is completely resampled from\nscratch on the range [-1,1). This code would actually be\nfine if the system were constrained to be within a box of\nlength 2, and in the limit ofk >> 1 it will also appear\nto give correct results. The inset shows the equivalent\nline in two other outputs of the model, both of which\nare acceptable; one displaces the position by a Gaussian\nrandom number with µ = 0 and σ2 = 1, and the sec-\nond chooses a new position from a Gaussian with mean\ncentered at the current position andσ2 = 1. Note that\nneither of these is optimized for the choice of (k,T), as\nσ2 = 1 may be too large or too small to be efficient, de-\npendingonthespringconstantandtemperature. Finally,\ninoneoftheothertwooutputsforthisexample(available\nin the SI or on the results website),k is interpreted as\nBoltzmann’s constant, and the harmonic system is given\na spring constant of 1.0 implicitly; this is a reasonable re-\nsult of the model. It illustrates how the author must be\ncareful about what is implicit in their prompt and what\nis stated explicitly (e.g. here, that T is temperature).\nB. Expert evaluations\nDavinci, the best performing model, does have broad\nknowledge of equations and common calculations across\nmultiple domains of chemistry. Table I gives the overall\naccuracy across the topics, models, and expert evaluable\ntopics. Both models can correctly answer prompts across\na range of topics, with davinci doing best. About 30 per-\ncentage points of accuracy is from prompt engineering,\nwhich is discussed further below.\nOn average, the accuracy on human evaluable topics is\nlower, reflecting their increased difficulty. These prompts\ninclude tasks like writing an input file for NWChem,33\nimplementing a Monte Carlo simulation of a harmonic\noscillator (Fig. 1), and generating a complex multi-panel\nplot. Fig. 2 shows a breakdown of difficulty from the\nindividual evaluations. There is a balance of easy and\nhard prompts in the dataset, as judged by experts. Our\nprimary result here is that the accuracy of the model is\nnegatively correlated with perceived prompt difficulty, as\nEasy Medium Hard\nIs this question\n0\n2\n4\n6Result quality\nn=300 n=242 n=108\nFIG. 2. 650 evaluations of davinci completions by the nine\ncoauthors who are postdoctoral scholars or Ph.D. students\nin chemistry or chemical engineering. Scoring is described in\nSec. II. We find that the typical result quality (white dot)\ndrops from ‘Perfect,’ to ‘Correct but not perfect’, to ‘Runs\nand is almost correct’ as perceived difficulty increased.\n4\nmight be expected but did not necessarily have to be the\ncase. We did not do any randomization or controls; each\nevaluator was able to see all prompts and all outputs,\nand so we acknowledge that scores could be biased by\nfactors such as the order of the prompts on the website,\nand the order that results for a given prompt were pre-\nsented on the website. In the rest of this article, we focus\nonly on prompts whose correctness can be evaluated by\ncomparison with an expected solution in an automated\nfashion.\nC. How to improve performance\nThere is a large accuracy gain when using basic prompt\nengineering strategies. Fig. 3 shows the effect of different\n“contexts” on accuracy across models. A context here is\ncode prepended before all prompts, or all prompts within\na topic. The contexts are given both in the Support-\ning Information and our accompanying code. “Custom”\nincludes two pieces: some imports related to the topic\n(e.g., rdkit34 for cheminf) and a single example to teach\nthe model how to indicate the end of a prompt comple-\ntion. The imports are not just to prevent import errors\n— they influence the completions and give context. For\nexample, a “structure” after importing rdkit means a\nbonded arrangement of atoms; in contrast, a structure\nafter importingopenmm35 (a molecular dynamics simula-\ntion code) would implicitly mean a 3D arrangement of\natoms, e.g. obtained from a PDB file.\nThe completion example is a one line statement (e.g.,\nprinting version number of imported package) with a\ncomment above and#end below. This causes the LLM to\nend completions with#end. We tried to ad-hoc look for\ncertain keywords such as new functiondefs, returns, or\ncomments as completion ends, but these heuristics were\noften violated. The completion example is significant for\nthe cushman model, which can only do completions but\nnot insertions. For davinci and the incoder models, we\ncan replace this with the “insert” contexts which have\nthe same imports but use a model capability to infill at\na special insert token (as in Fig. 1). Avoiding our com-\npletion example in the context seems to be insignificant\nfor davincni, but important for incoder.\nLLMs seem to be very very very susceptible to condi-\ntioning contexts like adding the word “very” many times\nto improve a completion36 or stating that the code “has\nno bugs.” We explored this in our benchmarks in two\nways. We tried inserting copyright notices and found\nin Figs. 3 and Fig. 4 that it does significantly improve\naccuracy at higher temperatures. This makes intuitive\nsense; lowering temperature makes the LLM choose more\nlikely completions and a copyright notice would more of-\nten be included with standard/quality code, thus giv-\ning a similar effect to lowering temperature. The best\nperforming model/temperature combination was not im-\nproved because it already had a low temperature. We\nalso tried inserting the statement “This is written by an\nexpert Python programmer” as suggested by Austin[37] ,\nand saw slightly less improvement. Similar recent work\nhas found context or specific phrases (e.g., “let’s think\nstep by step”) can give large accuracy improvements.28,38\nFriedet al. [17] have recently explored using metadata, in-\ncluding popularity of code, as a mechanism to condition\ncompletions, sothatwedonotneedtousead-hocprompt\nengineering.\nAside from contexts, there are a few strategies to en-\nsure a prompt aligns the intent of a user with the com-\npletion. If the prompt contains programming mistakes or\nspelling mistakes, then the completion will be of similar\nquality. So a correctly spelled and intelligible prompt is\nnecessary.\nThe LLM tries to agree with each word in the prompt.\nIf a prompt is a function declaration and uses the phrase\n“compute the moment”, the model will probably not re-\nturn the value. Thus, the word “return” should be used.\nIf a package is imported in the prompt, the model will\ntry to make use of it. This can lead to problems if many\npackages are imported – it can be unexpected as to which\npackages the model will use, or, the model thinks it must\nuse all of them.\nA major source of the errors in some of the categories\nsuch as ‘md’ is the proper use of functions from a package\nsuchas mdtraj, inparticular, improperknowledgeofhow\nmany and what type of values are returned by that func-\ntion; this could be a simple error or due to training on an\nearlier version of the module; these results may be able\nto be improved in the future by ‘fine tuning’ the LLM\non examples from a particular package that is frequently\nused in one’s work, or by adding additional context.\nD. Molecular structures\nOur goal is to evaluate how much chemistry LLMs\nknow. Besides evaluating tasks that can be expressed as\nprograms, we also explored whether LLMs can connect\nnatural language directly with molecular structures. We\ntested both InstructGPT39 and Codex (davinci) in these\nexamples, but found InstructGPT to work better. Nei-\nther could convert from molecular SMILES to name of\nmolecule, as demonstrated with 0% accuracy on 100 ran-\ndom molecules from pubchem40 when we tried SMILES\nlength of less than 60 characters (relatively small/sim-\nple molecules). The attempt from InstructGPT is shown\nin the Supporting Information. InstructGPT was able to\nconvert a sentence describing a molecule into SMILES, as\nshown with examples in Fig. 5. InstructGPT is able to\nconnect functional groups from SMILES to natural lan-\nguage. It is also able to correlate molecular properties\nlike lipophilicity with SMILES. InstructGPT rarely gen-\nerates invalid SMILES; only the first molecule in Fig. 5\nhad a single invalid character (see Supporting Informa-\ntion for SMILES). It appears that InstructGPT or other\nLLMs could be trained/fine-tuned on the connection be-\ntween natural language and chemical structures.\n5\n0\n1Accuracy\nbio\n thermo\n md\n genchem\n spectroscopy\nnone\ncustominsert\ncopyrightauthority\n0\n1Accuracy\ncheminf\nnone\ncustominsert\ncopyrightauthority\nqm\nnone\ncustominsert\ncopyrightauthority\nstats\nnone\ncustominsert\ncopyrightauthority\nsim\nModel\nincoder-1B\nincoder-6B\ncushman\ndavinci\nFIG. 3. A comparison of accuracy of the LLMs compared in this study across different contexts. Adding context – short\ncomments/imports – generally improves accuracy across topic and model. Error bars are 95% confidence intervals from\nbootstrapping across individual prompts, temperature, and from multiple completions.\n0.0\n0.2\n0.4\n0.6Accuracy\nincoder-1B model incoder-6B model\nnone\ncustominsert\ncopyrightauthority\n0.0\n0.2\n0.4\n0.6Accuracy\ncushman model\nnone\ncustominsert\ncopyrightauthority\ndavinci model temperature\n0.05\n0.2\n0.5\nFIG. 4. Comparison of context effect across models and tem-\nperatures. Having a custom context is most important. Note\nthat insert, copyright, and authority include the “custom”\ncontext. Error bars are 95% confidence intervals from boot-\nstrapping across individual prompts, temperature, and from\nmultiple completions. Cushman cannot do insertions.\nE. Discussion\nDavinci seems to not reason well about computational\nchemistry. If we prompt davinci to use to a “highly ac-\ncurate single-point” quantum calculation inpyscf,41 it\nwill frequently use relativistic Hartree-Fock regardless of\nthe property being computed because it has memorized\nthat “relativistic” is associated with accurate. Another\nexample is in the “force constant” prompt which is meant\nto compute the force constant for a two-atom harmonic\noscillator with different masses given a wavelength. Per-\nhaps because this is an unusual variant of a common\nquestion (converting between force constant and wave-\nlength), davinci always fails on this question and is un-\nable to rearrange the equation to take a harmonic mean\nof masses.\nDavinci may also hallucinate functions that do not ex-\nFIG. 5. Generating molecules with InstructGPT (text-\ndavinci-002). Prompts are shown in annotations. The\nstrongly lipophilic molecule is C 505, a polystyrene that is in-\ndeed strongly lipophilic. Most examples contain mistakes, but\nwere mostly valid. The top-left example had an ambiguous\nring indicator index which was removed prior to drawing.\nist. If a difficult prompt is given, for example “return\nthe residual dipole couplings given a SMILES string,”\nthe model will simply try to use a non-existent method\nMolToRDC. As reported previously,21 LLMs are not able\nto do chemical reasoning when completing prompts.\nFinally, we’d like to anecdotally note that the LLMs\ncould perform many of the benchmark problems if the\nnatural language was in Chinese, German, or Spanish.\nWe did not explore this in depth, but a few example\nprompts written in Mandarin can be found in the Sup-\nportingInformation. TheuseofLLMswithpromptsthat\nare not in English may be a valuable tool for lowering the\nbarrier to employing computational tools for those who\nare not native English speakers, and who therefore may\nhave a harder time interpreting documentation and pro-\n6\ngramming forums.\nIV. CONCLUSIONS\nLLMs are now easily available via tools like tabnine42\nor copilot.43 We’ve found high accuracy on computa-\ntional chemistry questions, and it is inevitable that stu-\ndents and researchers will begin using these tools. From\nour results, high accuracy should be expected with rea-\nsonable prompts. Tricks like inserting copyright notices\nat the top of a source file seems to be another way to\nimprove accuracy. We found that humans are able to\ngauge accuracy for easy to medium prompts, but care\nshould be taken if using completions of difficult prompts.\nThe seeming inability to generate syntactically invalid\ncode means LLMs often producesomething, but it is up\nto the user to assess it. We also found somewhat unex-\npected capabilities like generating molecules from natu-\nral language and accurate completions with non-English\nprompts. For a broader discussion of what impact this\nwill have on education, we refer the interested reader to\nour earlier perspective article.5\nACKNOWLEDGMENTS\nResearch reported in this work was supported by\nthe National Institute of General Medical Sciences of\nthe National Institutes of Health under award number\nR35GM137966 (to ADW) and R35GM138312 (to GMH).\nHAG was supported by NSF award 1751471. MA, SC,\nand ZY were supported by NIH award R35GM137966.\nGPW was supported by NSF award 1764415 SS and YS\nwere supported by NIH award R35GM138312, WJPC by\nR35GM138312-02S1, and KL by Department of Energy\naward DESC0020464. We thank Drs. Sanjib Paul, David\nGomez, and Navneeth Gokul who also contributed some\nexamples to the repository.\nAUTHOR CONTRIBUTIONS\nADW and GMH wrote NLCC software and designed\nnlcc-database, website, and human evaluation form.\nThey contributed examples to the nlcc-data repository\nand performed data analysis. All other authors con-\ntributed examples to the nlcc-data repository, partici-\npated in the expert evaluation, and assisted in writing\nthe manuscript.\nV. SUPPOR TING INFORMA TION\nSupporting figures, tables, and text are included in\nthe Supporting Information. Accuracy data is avail-\nable as comma separated value files. Contexts are avail-\nable as a markup file. The completions as HTML\npresented to expert evaluators is available at DOI:\n10.5281/zenodo.6800475.\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\nAttention is all you need, Adv. Neural Inf. Process Syst.\n30 (2017).\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, arXiv preprint arXiv:1810.04805\n(2018).\n[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. , Language models are few-shot learners,\nAdv. Neural Inf. Process Syst. 33, 1877 (2020).\n[4] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb,\nA. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,\nA. Garriga-Alonso, et al. , Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models, arXiv preprint arXiv:2206.04615 (2022).\n[5] G. M. Hocky and A. D. White, Natural language process-\ning models that automate programming will transform\nchemistry research and teaching, Digital Discovery 1, 79\n(2022).\n[6] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang,\nSmiles-bert: large scale unsupervised pre-training for\nmolecular property prediction, in Proceedings of the 10th\nACM international conference on bioinformatics, com-\nputational biology and health informatics (2019) pp.\n429–436.\n[7] N. Frey, R. Soklaski, S. Axelrod, S. Samsi, R. Gomez-\nBombarelli, C. Coley, and V. Gadepally, Neural scaling\nof deep chemical models, ChemRxiv 10.26434/chemrxiv-\n2022-3s512 (2022).\n[8] D. Flam-Shepherd, K. Zhu, and A. Aspuru-Guzik, Lan-\nguage models can learn complex molecular distributions,\nNat. Commun. 13, 1 (2022).\n[9] J. Ross, B. Belgodere, V. Chenthamarakshan, I. Padhi,\nY. Mroueh, and P. Das, Do large scale molecular lan-\nguage representations capture important structural in-\nformation?, arXiv preprint arXiv:2106.09553 (2021).\n[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, P. J. Liu, et al. , Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer., J. Mach. Learn. Res. 21, 1 (2020).\n[11] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe,\nC. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\net al. , The pile: An 800gb dataset of diverse text\nfor language modeling, arXiv preprint arXiv:2101.00027\n(2020).\n[12] D. Weininger, Smiles, a chemical language and informa-\ntion system. 1. introduction to methodology and encod-\ning rules, J. Chem. Inf. Comput. Sci. 28, 31 (1988).\n[13] C. Nantasenamat, “would be cool to have gpt-3 generate\nnew chemical structures in smiles notation?”, Twitter ,\n7\n1516794237391863810 (2022).\n[14] A. D. White, “as suggested by @thedataprof, gpt-3 can\nactually generate molecules. very clever idea! prompt was\n”the smiles for this drug-like molecular are:”, Twitter ,\n1516795519284228106 (2022).\n[15] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, A\nsystematic evaluation of large language models of code,\nin Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming (2022) pp. 1–10.\n[16] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski,\nD. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. ,\nProgram synthesis with large language models, arXiv\npreprint arXiv:2108.07732 (2021).\n[17] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wal-\nlace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and\nM. Lewis, Incoder: A generative model for code infilling\nand synthesis, arXiv preprint arXiv:2204.05999 (2022).\n[18] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang,\nY. Zhou, S. Savarese, and C. Xiong, A conversational\nparadigm for program synthesis, arXiv preprint (2022).\n[19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever, et al. , Language models are unsupervised\nmultitask learners, OpenAI blog 1, 9 (2019).\n[20] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,\nL. Shou, B. Qin, T. Liu, D. Jiang, et al., Codebert: A pre-\ntrained model for programming and natural languages,\narXiv preprint arXiv:2002.08155 (2020).\n[21] E. M. Bender and A. Koller, Climbing towards nlu: On\nmeaning, form, and understanding in the age of data, in\nProceedings of the 58th annual meeting of the association\nfor computational linguistics (2020) pp. 5185–5198.\n[22] E. M. Bender, T. Gebru, A. McMillan-Major, and\nS. Shmitchell, On the dangers of stochastic parrots: Can\nlanguage models be too big?\n , in Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and\nTransparency (2021) pp. 610–623.\n[23] https://github.com/ur-whitelab/nlcc-data.\n[24] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto,\nJ. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brock-\nman, et al. , Evaluating large language models trained on\ncode, arXiv preprint arXiv:2107.03374 (2021).\n[25] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\net al. , Huggingface’s transformers: State-of-the-art natu-\nral language processing, arXiv preprint arXiv:1910.03771\n(2019).\n[26] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel,\nN. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry,\net al. , Promptsource: An integrated development envi-\nronment and repository for natural language prompts,\narXiv preprint arXiv:2202.01279 (2022).\n[27] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi,\nQ. Le, and D. Zhou, Chain of thought prompting elic-\nits reasoning in large language models, arXiv preprint\narXiv:2201.11903 (2022).\n[28] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwa-\nsawa, Large language models are zero-shot reasoners,\narXiv preprint arXiv:2205.11916 (2022).\n[29] A. Fan, M. Lewis, and Y. Dauphin, Hierarchical neu-\nral story generation, arXiv preprint arXiv:1805.04833\n(2018).\n[30] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi,\nThe curious case of neural text degeneration, arXiv\npreprint arXiv:1904.09751 (2019).\n[31] https://ur-whitelab.github.io/nlcc-data/.\n[32] C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gom-\nmers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,\nS. Berg, N. J. Smith, et al. , Array programming with\nnumpy, Nature 585, 357 (2020).\n[33] M. Valiev, E. J. Bylaska, N. Govind, K. Kowalski, T. P.\nStraatsma, H. J. J. Van Dam, D. Wang, J. Nieplocha,\nE. Aprà, T. L. Windus, et al., Nwchem: A comprehensive\nand scalable open-source solution for large scale molecu-\nlar simulations, Comp. Phys. Comm. 181, 1477 (2010).\n[34] G. Landrum et al. , Rdkit: A software suite for chemin-\nformatics, computational chemistry, and predictive mod-\neling, Greg Landrum (2013).\n[35] P. Eastman, J. Swails, J. D. Chodera, R. T. McGibbon,\nY. Zhao, K. A. Beauchamp, L.-P. Wang, A. C. Sim-\nmonett, M. P. Harrigan, C. D. Stern, et al. , Openmm\n7: Rapid development of high performance algorithms\nfor molecular dynamics, PLoS Comp. Biol. 13, e1005659\n(2017).\n[36] P. Isola, “language-conditional models can act a bit like\ndecision transformers, in that you can prompt them with\na desired level of “reward” . e.g., want prettier #dalle\ncreations? ”just ask” by adding ”[very]^n beautiful”:”,\nTwitter , 1532189616106881027 (2022).\n[37] J. Austin, “we found that code models get better when\nyou prompt them with i’m an expert python program-\nmer. the new anthropic paper did something similar,\nprefixing the model’s response with i’ve tested this\nfunction myself so i know that it’s correct:, Twitter ,\n1515063524258627586 (2022).\n[38] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. Das-\nSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,\net al., Training a helpful and harmless assistant with rein-\nforcement learning from human feedback, arXiv preprint\narXiv:2204.05862 (2022).\n[39] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wain-\nwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. , Training language models to fol-\nlow instructions with human feedback, arXiv preprint\narXiv:2203.02155 (2022).\n[40] S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He,\nQ. Li, B. A. Shoemaker, P. A. Thiessen, B. Yu, et al. ,\nPubchem 2019 update: improved access to chemical\ndata, Nucleic Acids Res. 47, D1102 (2019).\n[41] Q. Sun, T. C. Berkelbach, N. S. Blunt, G. H. Booth,\nS. Guo, Z. Li, J. Liu, J. D. McClain, E. R. Sayfutyarova,\nS. Sharma, et al. , Pyscf: the python-based simulations of\nchemistry framework, Wiley Interdiscip. Rev. Comput.\nMol. Sci. 8, e1340 (2018).\n[42] https://www.tabnine.com/.\n[43] https://github.com/features/copilot.",
  "topic": "Correctness",
  "concepts": [
    {
      "name": "Correctness",
      "score": 0.6232427358627319
    },
    {
      "name": "Computer science",
      "score": 0.6192795038223267
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6079677939414978
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.598967969417572
    },
    {
      "name": "Code (set theory)",
      "score": 0.5606050491333008
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5450809001922607
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5061549544334412
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4694758355617523
    },
    {
      "name": "Data science",
      "score": 0.37434184551239014
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28124403953552246
    },
    {
      "name": "Programming language",
      "score": 0.2199890911579132
    },
    {
      "name": "Geography",
      "score": 0.07206001877784729
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}