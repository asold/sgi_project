{
    "title": "Transformer-Based Attention Network for In-Vehicle Intrusion Detection",
    "url": "https://openalex.org/W4378976213",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4378988885",
            "name": "Trieu-Phong Nguyen",
            "affiliations": [
                "Soonchunhyang University"
            ]
        },
        {
            "id": "https://openalex.org/A2138536389",
            "name": "Heung-Woo Nam",
            "affiliations": [
                "Daegu University"
            ]
        },
        {
            "id": "https://openalex.org/A2115167780",
            "name": "Daehee-Kim",
            "affiliations": [
                "Soonchunhyang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2296701710",
        "https://openalex.org/W6841333283",
        "https://openalex.org/W2897929592",
        "https://openalex.org/W3041133507",
        "https://openalex.org/W2945012921",
        "https://openalex.org/W4224916451",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2963510861",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W3195279647",
        "https://openalex.org/W1969185314",
        "https://openalex.org/W2117172651",
        "https://openalex.org/W3092232275",
        "https://openalex.org/W3083938246",
        "https://openalex.org/W3194227371",
        "https://openalex.org/W2892564986",
        "https://openalex.org/W4229070083",
        "https://openalex.org/W3014212182",
        "https://openalex.org/W3197115256",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4226441780",
        "https://openalex.org/W4200027626",
        "https://openalex.org/W4294860585",
        "https://openalex.org/W4285600491",
        "https://openalex.org/W4285227013",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2801755634",
        "https://openalex.org/W2014608904",
        "https://openalex.org/W3164317619",
        "https://openalex.org/W2979202956",
        "https://openalex.org/W1970412479",
        "https://openalex.org/W4220775946",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4387675520"
    ],
    "abstract": "Despite the significant advantages of communication systems between electronic control units, the controller area network (CAN) protocol is vulnerable to attacks owing to its weak security structure. The persistent development of intrusion detection systems (IDS) is geared toward preventing vehicles from being targeted by malicious attacks. Recurrent neural networks (RNNs) have emerged as a prominent approach in this domain, contributing significantly to the evolution of IDS. Nonetheless, RNN-based methods have certain limitations in step-by-step processing. Their feature extraction at any given point in time only relies on the hidden state of previously observed information, possibly resulting in missing features in the context vector. In this paper, we propose a novel multi-class IDS using a transformer-based attention network (TAN) for an in-vehicle CAN bus. Our model builds on the self-attention mechanism, removing RNNs and classifying attacks into multiple categories. Furthermore, the proposed models can detect replay attacks by aggregating sequential CAN IDs. The experimental results indicate that the TAN is more efficient than the baselines for different input data types and datasets. The highlight is that, although sequential CAN IDs are used, our model can identify intrusion messages without requiring message labeling. Finally, by inheriting the advantages of transformers, TAN employs transfer learning to improve the performance of models trained on small data from other car models.",
    "full_text": "Received 11 May 2023, accepted 26 May 2023, date of publication 1 June 2023, date of current version 8 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.32821 10\nTransformer-Based Attention Network for\nIn-Vehicle Intrusion Detection\nTRIEU PHONG NGUYEN\n 1, HEUNGWOO NAM\n 2, (Member, IEEE),\nAND DAEHEE KIM\n 1, (Member, IEEE)\n1Department of Mobility Convergence Security, Soonchunhyang University, Asan 31538, South Korea\n2Department of Computer Engineering, Daegu University, Gyeongsan 38453, South Korea\nCorresponding authors: Daehee Kim (daeheekim@sch.ac.kr) and Heungwoo Nam (hwnam@daegu.ac.kr)\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea Government (MSIT) (No.\n2021R1A4A2001810, 2022H1D8A3038040), by Institute for Information & communications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government (MSIT) (No. 2022-0-01197, Convergence security core talent training business (SoonChunHyang\nUniversity)), by ‘‘Regional Innovation Strategy (RIS)’’ through the National Research Foundation of Korea (NRF) funded by the Ministry\nof Education (MOE) (2021RIS-004), by Basic Science Research Program through the National Research Foundation of Korea (NRF)\nfunded by the Ministry of Education (No. 2022R1I1A3066416), and this work was supported by the Soonchunhyang Research Fund.\nABSTRACT Despite the significant advantages of communication systems between electronic control units,\nthe controller area network (CAN) protocol is vulnerable to attacks owing to its weak security structure. The\npersistent development of intrusion detection systems (IDS) is geared toward preventing vehicles from being\ntargeted by malicious attacks. Recurrent neural networks (RNNs) have emerged as a prominent approach\nin this domain, contributing significantly to the evolution of IDS. Nonetheless, RNN-based methods have\ncertain limitations in step-by-step processing. Their feature extraction at any given point in time only relies\non the hidden state of previously observed information, possibly resulting in missing features in the context\nvector. In this paper, we propose a novel multi-class IDS using a transformer-based attention network\n(TAN) for an in-vehicle CAN bus. Our model builds on the self-attention mechanism, removing RNNs and\nclassifying attacks into multiple categories. Furthermore, the proposed models can detect replay attacks by\naggregating sequential CAN IDs. The experimental results indicate that the TAN is more efficient than the\nbaselines for different input data types and datasets. The highlight is that, although sequential CAN IDs are\nused, our model can identify intrusion messages without requiring message labeling. Finally, by inheriting\nthe advantages of transformers, TAN employs transfer learning to improve the performance of models trained\non small data from other car models.\nINDEX TERMS CAN, intrusion detection system, multi-class classification, replay attacks, self-attention,\ntransformer.\nI. INTRODUCTION\nDue to recent developments in the field of intelligent vehi-\ncles, modern cars can provide drivers with an excellent\nexperience. However, rapidly evolving utilities and functions\nrequire expanding car control systems [1]. Electronic control\nunits (ECUs), which serve as building blocks for complex\nin-vehicle systems, regulate the message broadcast network\nto ensure smooth operation between components such as\nthe throttle, brake, engine, and steering wheel [2]. The units\ntransmit control signals through networks by using a network\nprotocol. The most extensively implemented in-vehicle net-\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Vicente Alarcon-Aquino\n.\nwork (IVN) is the controller area network (CAN). The CAN\nexhibits many advantages over other networks, including the\nlocal interconnected network (LIN) and FlexRay, owing to\nits high speed and efficiency. In addition to serving as an\nIVN, the CAN has been effectively employed in numerous\nindustrial applications, including aircraft, railways, and X-ray\nmachines [3]. However, owing to its efficiency and simple\ndesign, the CAN bus is vulnerable to cyberattacks because\nintruders can easily influence physical subsystems. For exam-\nple, an attacker can inject harmful messages directly through\nphysical onboard diagnostic (OBD-II) ports or access the\nnetwork remotely using telematics services [4].\nMany manual approaches, such as certification, scan-\nners, access control, and firewalls, have been proposed to\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 55389\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nFIGURE 1. Voltage levels of controller area network bus.\nresolve the vulnerability of CAN buses. A point of con-\ncern is that manual methods can only passively prevent\nmalicious intrusions by using limited rules. The security\nproblem in IVN requires a system that can analyze and\nlearn the features in CAN data and use them to recog-\nnize abnormal messages outside the scope of configurations.\nMachine-learning algorithms have joined security races with\ngreat potential. Research on recurrent neural network (RNN)\nalgorithms yields state-of-the-art (SOTA) performance in\nintrusion detection systems [5]. However, RNNs face bottle-\nnecks in the extraction of unselective information. Accord-\ningly, this paper proposes a novel approach to improving\nintrusion detection.\nTransformers perform excellently in many fields, includ-\ning natural language, image, and audio processing. With\nthe ability to extract features through correlation calculation\ndata components, we consider the transformer efficiency of\nmulti-class classification in IVN. The key contributions of our\napproach are summarized as follows:\n1) We propose an intrusion detection system (IDS) based\non a transformer attention network for a CAN bus that\nuses a single message. We processed the data by remov-\ning unnecessary elements in each message, merging the\ndatasets, relabeling them for multi-class classification,\nand converting them to a data type suitable for machine\nlearning. Experimental results demonstrate the superior\nability of our model to classify intrusion messages on\nthe Car Hacking dataset compared to that of SOTA\nRNN-based IDS [5].\n2) A single-message IDS can easily confuse malicious\nand normal messages if an attacker injects messages\nextracted from the CAN bus. We proposed a model\nthat uses sequential CAN IDs to solve this problem.\nWe used sliding windows to aggregate CAN IDs into\nsegments and label sequential CAN ID inputs at the\nsegment level. Our experiment was conducted with sev-\neral window sizes to evaluate the tradeoff between each\nmodel and determine the optimal parameters. When\nevaluating the effectiveness of our proposed model\nusing the replay attack, which involves reusing mes-\nsages extracted from the CAN bus, we observed that\nit achieved the lowest error rates compared with the\nselected baseline models.\n3) To the best of our knowledge, other models that use\nsequential CAN IDs can detect only abnormal seg-\nments. Our results indicate that the proposed model\ncan identify intrusive messages in any malicious seg-\nFIGURE 2. Structure of the CAN data frame.\nment without requiring a message level labeled dataset.\nWe exploit the attention scores to identify malicious\nCAN IDs in the segment, enabling the model to remove\nonly the attack messages from the CAN bus.\n4) We considered the possibility of applying transfer\nlearning to our model. Our data problem-solving efforts\nyielded positive results when experience from the\nsource model improved the performance of the target\nmodel.\nThe remainder of this paper is organized as follows.\nIn Section II, we present background material pertaining to\nthe CAN bus and various attack scenarios. Section III pro-\nvides an overview of previous studies on intrusion detection\nsystems in CAN bus. In Section IV, the core of the proposed\nmodel is presented. In Section V, we compare the perfor-\nmance of our approach with that of other machine learning\nmethods and examine the results of transfer learning. Finally,\nwe conclude the paper and describe the future directions of\nour research in Section 7.\nII. BACKGROUND\nThe following section summarizes the inner workings of\nthe CAN protocol and describes the most common attack\nscenarios in in-vehicle IDS.\nA. CONTROLLER AREA NETWORK BUS\nIn the 1980s, Robert Bosch developed a CAN that allowed\nECUs to communicate with each other [6]. As the com-\nmunication standard for ECU in in-vehicle networks, CAN\ndesign considers three objectives: reliability, effectiveness,\nand economics. The modern CAN bus transfers information\nregarding the steering wheel, fuel gauge, brakes, and engine\nto maintain data synchronization in vehicles [7]. Despite\nthese advantages, CAN lack security features. The initial\ndesign of the CAN bus did not consider authentication of\nmessages because the ECU did not connect to the external\nnetwork.\nThe underlying concept of the CAN bus is a multi-master\ncompetition [8] wherein each ECU node can send and receive\nmessages from hundreds of sensors without permission.\nThe unique CAN ID determines the priority of each CAN\nmessage.\nAs shown in Fig. 1, the two lines in the CAN bus have\ncertain voltage levels to transmit either the dominant bit\n(0 bit) or recessive bit (1 bit), where the dominant bit is\nprioritized over the recessive bit. Consequently, a lower ID\nindicates a higher priority [9]. In multi-master and priority-\nbased attacks, an attacker can inject messages with the high-\nest priority into the CAN bus and then flood the in-vehicle\nnetwork through denial-of-service (DoS) attacks.\n55390 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nFIGURE 3. DoS attack.\nFIGURE 4. Fuzzy attack.\nFIGURE 5. Spoofing attack.\nFIGURE 6. Replay attack.\nThere are two design criteria for the CAN message format:\na standard frame (CAN 2.0A) and an extended frame (CAN\n2.0B). Fig. 2 illustrates the general architecture of a CAN\nframe. The following functional descriptions were applied to\nthe constituent fields:\nSOF (start of the frame field). The dominant bit repre-\nsents the synchronization and the start of the frame.\nCAN ID (arbitration field). Identification and frame pri-\nority for arbitration.\nRTR (remote transmission request). The identity bit\nindicates whether the frame is remote or a data frame.\nIDE (identifier extension). The bit identifies the CAN\nframe type. A dominant bit defines the 11-bit standard,\nwhereas a recessive bit defines the 29-bit extended frame.\nDLC (data length code). The size of the DLC ranged\nfrom 0 to 8 bits, indicating the byte length of the data field.\nData field. The 64-bit payload is responsible for commu-\nnication between ECUs.\nCRC (cyclic redundancy check field). 16 control the data\nchecksum.\nACK (acknowledgment field). The confirmation compo-\nnent indicates the success status (two bits).\nEOF (end of the frame field). The 7-bit delimiter verifies\nbit stuffing and frame finalization.\nThese two specifications exhibit few structural differences.\nThe only distinction is that the arbitration ID of the standard\nframe accommodates 11- or 29-bit options, whereas that of\nthe extended frame allows only 29 bits.\nSeveral studies have indicated that a period exists between\nmessages with the same identifier in the CAN bus [4], [10].\nBecause each ECU sends data every 10 ms, we assume loss\nof periodicity upon the injection of a malicious message.\nThus, sequential IDs can be used in the CAN bus to recognize\nmalicious threats.\nB. ATTACK MODELS\nThe lack of network segmentation, authenticity, and\ndata encryption represents a fundamental vulnerability of\nin-vehicle CAN [11]. Lee et al. [12] and Song et al. [13]\ndemonstrated that ECU nodes do not consider the sources\nof CAN messages. Consequently, cyber-attackers can easily\ncarry out common network attacks on the CAN bus. The\nattack scenarios considered in this study are introduced in\nthis section and depicted in Figs. 3–6.\n1) FLOOD ATTACK\nCommon (denial-of-service) DoS attacks. The flood attack\nmethod involves submitting a large number of valid requests\nthat exceed the CAN bus processing capabilities. The attacker\nintends to exhaust the system resources and paralyze the\nfunctions.\n2) FUZZY ATTACK\nThe attacker repeatedly injected the CAN bus by using ran-\ndom messages. To disrupt the vehicle’s functions, the attacker\nexamines in-vehicle information CAN packets and selects the\ntarget CAN IDs to cause unexpected activity.\n3) SPOOFING ATTACK\nThis is also known as a malfunction (Mal) attack. During\na spoofing attack, the intruder uses the CAN IDs obtained\nto target the specific subsystem functions in the vehicle.\nAbnormal messages are generated by gradually changing the\npayload.\n4) REPLAY ATTACK\nAn attacker captures a legitimate message and injects it\nrandomly into the CAN bus. These abnormal messages\nforce vehicles to repeat actions with potentially dangerous\nresults. These attacks are difficult to detect without examining\nsequential patterns in the CAN.\nIII. RELATED WORK\nAn IDS is a network security technology built to detect signs\nof suspicious presence and generate alerts when cybersecurity\nVOLUME 11, 2023 55391\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 1. Comparision of deep learning studies for CAN IDS.\nthreats are discovered [9]. Because CAN lacks built-in secu-\nrity features, the in-vehicle network system utilizes an IDS to\nmonitor malicious activities between the CAN and external\ninterfaces. Many approaches have been developed to identify\nintrusions on the CAN buses.\nAccording to its design, Wu et al. categorized in-vehicle\nnetwork IDS a fingerprints-based (bus level), parameter-\nmonitoring-based (message level), information-theoretic-\nbased (data-flow level), and machine-learning-based\n(functional level) [14]. Machine learning and deep learning\nrepresent the primary approaches in the research field of\nin-vehicle network IDS. With outstanding advantages in\nlearning data behaviors, machine- and deep-learning-based\nIDS are capable of defending against unidentified attacks\nwithin the CAN bus through input data analysis.\nIn terms of data mining, Sun et al. categorized intrusion\ndetection algorithms into three groups: semantic-based, data\ndomain-based, and periodicity-based [10].\nSemantic-based methods attempt to analyze the semantics\nof CAN messages. Wasicek et al. developed a context-aware\nIDS to classify cyber threats in in-vehicle network sys-\ntems [15]. Their approach exploits the information obtained\nby an existing sensor to improve the precision of an IDS\nby using an ANN. The report showed that semantic-based\nmethods, such as chip tuning, can unveil anomalous manip-\nulations. In another approach, a data-domain-based IDS\naddresses the vulnerability of CAN by exploiting the regu-\nlarity of CAN IDs. Based on RNN algorithms, these meth-\nods predict the next bit or package. For instance, Qin et al.\nsuggested long short-term memory (LSTM) to forecast CAN\nmessages in two data formats [16] and compared several loss\nfunctions for time-series prediction to achieve optimal results.\nSimilarly, periodicity-based IDS use sequential patterns of\nCAN messages to identify malicious sequences in data traffic.\nHossain et al. proposed an LSTM-based IDS that learns the\ntemporal features from multivariate time-series CAN traf-\nfic [17]. The overall accuracy of this approach was 99.995%\nas evaluated using their dataset. They also demonstrated that\nan LSTM-based IDS is more efficient for in-vehicle intrusion\ndetection than a survival analysis method [18]. Khan et al.\ndeveloped a multi-stage deep learning framework which com-\nbines normal state-based and a bidirectional LSTM archi-\ntecture [19]. Their outcomes indicated that the framework\noutperforms a variety of current state-of-the-art systems and\ncan detect zero-day outbreaks from Internet of Vehicles\nnetworks. In another periodicity-based study, Song et al.\ndeveloped a CNN-based IDS to extract spatial features from\n29 sequential CAN ID data frames [4]. Furthermore, their\nmodel was optimized to minimize unnecessary components\nin the inception-resnet architecture. In another approach,\nHoang et al. presented a semi-supervised learning method\nusing convolutional adversarial autoencoders (CAAE) [20].\nTheir methodology leveraged the autoencoder mechanism\nfor unsupervised training, followed by supervised fine-tuning\nwith partial data to enhance the precision of the model. Their\nlightweight model achieved a high performance within a 40%\ndataset compared to the baseline model.\nOther studies have attempted to combine the spatial and\ntemporal features [5], [21]. Lo et al. used a convolutional neu-\nral network (CNN) and LSTM to learn representative features\nfrom CAN traffic [5]. The author performed binary classifica-\ntion to demonstrate that the hybrid deep learning-based IDS\nsuccessfully recognized suspicious behaviors in the CAN\nbus. The results on the Car Hacking Dataset [4] showed that\ntheir method outperformed other related works.\nCNN-LSTM achieved SOTA performance in the binary\nclassification of the CAN bus. However, this approach does\nnot consider the natural sequential CAN IDs. These short-\ncomings can be exploited for replay attack scenarios in which\nthe attacker utilizes injected malicious messages extracted\nfrom the CAN bus.\nResearch related to transfer learning has also been applied\nto periodicity-based models such as LSTM. Kang et al.\ndeployed LSTM-based transfer learning for in-vehicle CAN\n55392 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nFIGURE 7. Overview of proposed architecture.\nand evaluated it using a survival dataset [22]. Their report\nindicated that the performance of a model trained on a small\ndataset can be improved significantly. The transfer learning\napproach related to vehicle intrusion detection systems has\nyielded promising results. Tariq et al. used transfer learning\ncombined with CNN-LSTM to demonstrate the ability to\ntransfer knowledge from large to small datasets [23]. Through\none-shot learning, CNN-LSTM transfer learning achieved a\nperformance improvement of 26.06% over the baselines.\nIn addition to the spectral and temporal features, sev-\neral recent studies have considered the correlation between\neach element in CAN traffic. Wei et al. proposed an intru-\nsion detection model based on an attention mechanism and\nautoencoder [24]. They applied an encoder and decoder to\nrepresent the potential features of the CAN messages. The\nattention layer then learns the importance of the extracted\nhidden features and feeds them into the classification layer\nto calculate the prediction result. According to their exper-\nimental results, a hybrid of an autoencoder and an atten-\ntion mechanism is highly promising for the OTIDS dataset.\nOther methodologies based on the attention mechanism have\ninvestigated the selection of points of significance in a target\nfrom hidden features. Sun et al. proposed a CNN-LSTM and\nattention model using bit flip rate preprocessing [10] and\ndemonstrated the model’s robustness and reliability for intru-\nsion detection systems. They also discussed the shortcomings\nand future research directions to improve their performance.\nThe attention mechanism was combined with other algo-\nrithms to preserve useful features in data traffic. However,\nmost studies have ignored that recurrent neural network\n(RNN) algorithms incur vanishing and exploding gradient\nproblems [25].\nWith advancements in computer processing capabilities,\nvarious recent research studies have proposed utilizing\nself-attention mechanisms to tackle the limitations of RNNs.\nNam et al. deployed bidirectional GPT for an in-vehicle\nIDS [26]. The author performed binary classifications to\ndemonstrate that their model could detect the anomalous\nsegment in the case of an extremely sparsely attacked CAN\nID. However, the authors did not provide any comparisons\nregarding the time inference. Moreover, the limitation of their\nwork is discussed, as the model can only detect malicious\nmessage segments without identifying injected CAN IDs.\nWu et al. introduced a robust transformer-based approach\nfor IDS on network traffic [27]. According to their report,\ntheir model outperforms support vector machine and deep\nlearning algorithms, including RNN, on both individual\nattack types and multi-class datasets in terms of efficiency.\nTheir model had a longer training time than other mod-\nels, and comparisons of inference time were not presented.\nIn another study, Sharma et al. approach transformer model\nby using CNN for embeding layer [28 ]. The results show\nthat using CNN can improve model performance in pre-\ndicting driver intent. Our study focus on developing an\nintrusion detection system based on the transformer architec-\nture for in-vehicle CAN bus. Table 1 presents several compar-\nisons between our proposed method and previous CAN IDS\nstudies.\nVOLUME 11, 2023 55393\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nFIGURE 8. Architecture of transformer-based attention network.\nIV. METHODOLOGY\nThe following section proposes a novel multi-class IDS using\na transformer-based attention network for an in-vehicle CAN\nbus. Our proposed IDS comprises the two models shown in\nFig. 7: an IDS using only a single message and an IDS using\nsequential CAN IDs. Note that the first model detects DoS,\nfuzzy, and spoofing attacks better than SOTA RNN-based\nIDS [5]. However, it cannot detect replay attacks because\nit uses only a single message. Accordingly, we designed a\nsecond model to detect replay attacks by using sequential\nCAN IDs.\nA. OVERALL ARCHITECTUTE\nFor a comprehensive evaluation, the proposed system (Fig. 7)\nconsists of two components corresponding to different data\ntypes: a single message and sequential CAN IDs.\nIn the single-message component, we fed the preprocessed\ndata into a classification model based on the transformer\narchitecture. The two primary components of a transformer\nare the encoder and decoder [29]. The encoder is the most\nessential component for classification. As shown in Fig. 8,\nto remove recursion and convolution, the transformer-based\nattention network must embed positional encoding to mark\nthe arrangement of the tokens in the input. The combina-\ntion of input and positional encoding enables the model to\nobtain sequence features. After adding positional informa-\ntion, the self-attention algorithm calculates the correlation\nbetween tokens in a single message. To maintain propaga-\ntion of signals, we used residual connections that perform\nAlgorithm 1 Training for sequential CAN IDs\nInput:\nD = [{x0, y0} , {x1, y1} , . . . ,{xw, yw}] : the dataset with\nxi is sequential CAN IDs, yi is the label for each xi, w is\nwindow size.\nn_epochs : the number of epochs for training.\nwindow_size : the number of CAN IDs in a sequence.\nnum_heads : the number of attention heads.\nff_dim : the hidden layer size in feed forward network.\nembed_size : the embedding dimension for each token.\nOutput: θC : The weight for classifier\n[α0, . . . , αw] : The attention score for a sequential CAN IDs\n1: for epoch from 1 to n_epochs do\n2: Embedding input by fembed ([x0, . . . ,xw])\n3: Adding positional encoding:\ninput =f embed ([x0, . . . ,xw]) + fembed ([0, . . . ,w])\n4: Updating multi-head attention:\nMultihead(Q, K, V) = Concat(head1, . . . ,headn)WO\nwhere\nheadi = Attention(QWQ\ni , KWK\ni , VWV\ni )\nWO ∈ Rhdv×dm\n5: Updating θC by optimizing L cross_entropy\n6: Updating [α0, . . . , αw] by Multihead(Q, K, V)\n7: end for\nFIGURE 9. Single message preprocessing.\nidentity mappings to the input layer. These mappings are\nthen added to the output layer. Context vectors are generated\nat the end of the encoding phase. To mitigate overfitting\nduring training, we employed a technique that dropout lay-\ners with 0.15 rate are interleaved with normalization layers.\nSubsequently, selective information is fed to a simple neural\nnetwork such as feed-forwarding layers. The output of the\nmulti-class classifier was a linear layer that performed prob-\nabilistic predictions for each class. The probabilities are fed\ninto a softmax function, and the class with the highest return\nvalue results from classification.\nFor sequential CAN IDs, we performed data processing\nusing only the identified fields. Packets containing sequential\nCAN IDs are fed into the classification model. We use the\nsame model as that used in the single-message approach.\nUnlike single-message inputs, the components in sequen-\ntial CAN IDs represent identifiers. The use of sequential CAN\nIDs overcomes the repeated intrusions extracted from the\n55394 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nAlgorithm 2 Label Generation for Sequential CAN IDs\nInput:\nThe batch of sequential CAN ID and each label\n[{x0, y0} , {x1, y1} , . . . ,{xw, yw}] × b with\n{xi, yi} are CAN ID and message label\nw is window size\nb is batch size\nOutput:\nThe sequential CAN IDs and segmentation label\n[\nXj, Yj\n]\n1: for j from 0 to (b − w) do\n2: if ∑i=0\nw yi = 0 then Yj = 0\n3: else\n4: for i = 0 to w do\n5: if yi ̸= 0 then Yj = yi\n6: end for\nCAN bus. Consider the example of an accepted CAN message\nwith a brake signal. The attacker can extract these messages\nfrom the CAN and inject them into an in-vehicle network.\nThe single-message IDS then accepts intrusive brake signals\nas safety messages, posing a danger to the driver. The training\nprocess for the sequential CAN IDs data type is summarized\nin Algorithm 1.\nThe use of a single-message input can identify malicious\nmessages in the CAN bus, whereas sequential CAN IDs can\nonly detect anomalous segments. To identify malicious mes-\nsages in sequential CAN IDs, we extracted attention scores.\nFollowing classification, these scores highlight malicious\nmessages in the segment.\nThe transformer is well-known for its transfer learning\ncapabilities in language processing, image classification, and\naudio classification tasks [30], [31], [32]. In this study,\nwe considered the feasibility of using a pre-trained model to\nimprove the performance of limited data. The transfer learn-\ning technique can provide knowledge trained on a model with\nextensive data from one car to a model with less data from\nanother car. We applied the same data processing method to\nthe target dataset. The target and source data were trained\nusing the same model. For multi-class classification, the out-\nput layer can be adjusted to fit the target data.\nB. PREPROCESSING DATA\nThe quality of data has a significant impact on the quality of\ntraining. Because machine learning methods cannot directly\nuse the raw data extracted from the vehicle, preprocessing\nmust be applied to convert the data into a suitable type.\nWe removed redundant values, filled in null values, and\ntransformed the data into a standardized form to ensure data\nusability. We consider two critical data types: single messages\nand sequential CAN IDs. Although single-message intrusion\ndetection approaches yield high performance [5], they present\na vulnerability if the attacker injects replay messages that\nwere previously verified to be secure. To address this issue,\nFIGURE 10. Sequential CAN ID preprocessing.\nwe use sliding windows to generate sequential CAN IDs,\nenabling the detection of anomalies at the segment level.\nIn a single message, we use the arbitration field, data length\ncode, and data payload (Fig. 9). All identifications and values\nin the data field were converted from hexadecimal to decimal\nformats. Each data field value is a byte (8 bits) in size and\nis assigned a value from 0 to 255. To maintain a uniform\ndata length, we assigned null values of 256. However, the\nactual length of the message is still the specified by the DLC.\nWe then define 0 and 1 as normal and anomalous messages,\nrespectively [5].\nFor sequential CAN IDs, we considered only sequential\npatterns on the CAN bus. As shown in Fig. 10, CAN IDs\nwere similarly converted into numeric forms. In each mes-\nsage, the T and R tags represent an anomaly and a normal\nmessage, respectively. We assume that only one attack type\noccurs in each segment and consider continuous CAN IDs as\ntime-series data for each time step. A sliding window divides\nall identifications in the dataset among the data segments and\nuses them as the input data. Suppose that, for a dataset with\nn records, the input sequence at the ith time window is\nXi = [x1\ni , x2\ni , . . . ,xw\ni ], i ≤ (n − w) (1)\nwhere w is the window size and xt\ni represents the identifier of\nthe CAN message at time t. The label generation process is\npresented in Algorithm 2.\nC. IDENTIFICATION FOR EACH MALICIOUS MESSAGE\nAs previously mentioned, the self-attention mechanism rep-\nresents the core of the transformer model. That is, self-\nattention is the most significant development step that\ndifferentiates transformers from traditional recurrent neu-\nral networks, such as RNNs and LSTM. Because the\nself-attention mechanism assigns higher weights to important\ninformation, we assume divergence between the attention\nscores of normal and abnormal messages in each CAN ID\nsegment. We extracted the attention scores to demonstrate\nthe ability to identify malicious IDs. We used the multi-head\nattention layer’s output to calculate the attention score and\nexploit it for message intrusion identification.\nVOLUME 11, 2023 55395\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nThe underlying concept of the self-attention mechanism is\nto map a set of query and key-value vector pairs to an out-\nput vector. Similar to the attention mechanism, self-attention\nreceives (Q, K, V ) as the input, where Q, K, and V are the\nquery, key, and value matrices, respectively. (Q, K, V ) is sep-\narated into sub-space representations, also known as heads,\nin which each attention weight is focused on a different head\nin parallel. Because of parallel computation, we concatenated\nthe heads’ individual results to obtain the final attention\nweights. Therefore, the multi-head self-attention function can\nbe expressed as follows:\nheadi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (2)\nMultihead(Q, K, V ) = Concat(head1, . . . ,headn)WO (3)\nwhere WO ∈ Rhdv×dm are the output parameter matrices, dv\nis a dimension of value matrices, dm is a dimension of the\nmodel.\nInstead of allowing the output to attend to important fea-\ntures of the input, the self-attention mechanism allows input\ntokens to interact with each other. The query and key are iden-\ntical in the self-attention mechanism [29]. Assume that the\nCAN input is X = [x1, x2, . . . ,xn], with ntokens, xi ∈ Rdm .\nThe initialized trainable parameters\n{\nWQ, WK\n}\n∈ Rdm×dk ,\nWV ∈ Rdm×dk were randomly generated and updated regu-\nlarly for attention calculation. K and V can be expressed as\nfollows: {\nK = XWK\nV = XWV\n(4)\nEach attention score is a dot product of the attention of a\nquery and a set of key values [29]. Each correlation score\nbetween a query qi and values in Kis presented as weight\nwj\ni through the softmax function. The attention weights were\nthen combined with the values to represent the context vector,\nindicating where the model pays more attention. The fol-\nlowing equations can be used to calculate the dot-product\nattention:\nqiKT = [qikT\n1 qikT\n2 . . .qikT\nn ] (5)\nSoftMax(qiKT ) = [wi1wi2 . . .win] (6)\nSoftMax(qiKT )V =\n∑ n\nj=1 wijvj (7)\nWith all queries, the formula for the scaled dot product atten-\ntion matrix is\nAttention(Q, K, V ) = SoftMax\n(QKT\n√\ndk\n)\n(8)\nwhere\n√\ndk is the scaling factor to counteracts extremely\nsmall gradients in softmax function as the growth of the dot\nproduct in magnitude.\nOur experiment used only the data labeled at the seg-\nment level, where malicious segments included both normal\nand attack messages. Although we did not require a labeled\nmessage level dataset for the training phase, our model\ncould identify malicious messages. Through self-attention,\nTABLE 2. Car hacking dataset statistics.\nTABLE 3. INV intrusion detection dataset statistics.\nTABLE 4. Survival analysis dataset statistics.\nthe attention scores of the injected CAN IDs increased, as the\nother attention scores decreased. We deployed malicious seg-\nments with 16 CAN IDs to evaluate the performance of\nthe proposed model in identifying intrusive messages. The\nattention scores of each segment were flattened by the sum\nfunction before fed into the sigmoid function to classify\nthrough the threshold. Normal IDs with low attention scores\nreturned 0, whereas injected IDs with high attention scores\nreturned 1.\nD. TRANSFER LEARNING\nTransfer learning solves critical problems in in-vehicle intru-\nsion detection, training, and data collection. As the data\nvaried with respect to the vehicle model, data capture was\nperformed whenever a new car model was released. Extract-\ning sufficient data from new car models is time-consuming\nand expensive. The advantage of transfer learning is its ability\nto reuse the information trained on a large dataset. Thus,\na target model trained on a small dataset can be improved\nusing knowledge from the source model.\nAs described in [33], D = {X, P(X)} and T = {Y , f }\nrepresent the domain and task of each dataset, respectively.\nWe prepared the source (X S ) and target (X T ) datasets with\nequivalent feature spaces XS = XT , and different marginal\ndistributions P (XS ) ̸= P (XT ) [34]. Thus, DS ̸= DT . We used\nthe Survival dataset for XT and the Car Hacking dataset for\nXS . Our objective was to improve the performance of learned\ndecision functions, which are expressed as the probability\ndistribution of P(YT |XT ) in DT , by utilizing the knowledge\nimplied in DS [35].\n55396 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 5. Hyperparameter settings.\nAfter obtaining an optimal model from the source data,\nwe unfroze the model and modified the output layer. The\nmodel was trained on the source data with an output of five,\ndifferent from the target data with four classes. Subsequently,\nthe new model was trained on the target dataset using the\nappropriate output. The target model utilizes source model\nparameters to train the limited data. Hyperparameters, such\nas the learning rate, were adjusted to avoid overfitting. Fol-\nlowing the training phase, the target model was fine-tuned to\nachieve an optimal performance.\nV. EXPERIMENTS\nA. DATASETS\nTo test our model, we implemented it on three com-\nmonly used datasets from the Hacking and Countermeasures\nResearch Lab in Korea. All the attack datasets were concate-\nnated to perform multi-class classification.\n1) CAR HACKING DATASET [4]\nAs shown in Table 2, the Car Hacking dataset encompasses\nfour attack datasets from the Hyundai Sonata car model: a\nDoS attack, fuzzy attack, and two spoofing attacks (corre-\nsponding to the drive gear and revolutions per minute (RPM)\ngauge). Each dataset included both the normal and attack\nrecords. We used the Car Hacking dataset to evaluate the\nperformance of the two input data types from the CAN bus.\n2) IVN INTRUSION DETECTION DATASET [36]\nTable 3 lists the statistics for the In-Vehicle Intrusion Detec-\ntion dataset. The Attack dataset was extracted from two car\nmodels, Hyundai Sonata and Kia Soul. Although this dataset\nis smaller than the Car Hacking dataset, it is more balanced.\nWe considered the ability to detect replay attack scenarios on\nan In-Vehicle Network Intrusion Detection dataset.\n3) SURVIVAL ANALYSIS DATASET [18]\nThe statistics in Table 4 show the composition of the Survival\nAnalysis dataset, which comprises data from the three car\nmodels. We used a pre-trained model on the Car Hacking\ndataset. Then we extended our transfer learning experience\nto the Kia Soul and Chevrolet Spark car models, with the\nexception of Hyundai Sonata.\nB. EXPERIMENTAL SETUP\nThe proposed transformer-based attention network IDS for\nthe IVN is implemented in Python using the TensorFlow\nFIGURE 11. Embedding dimension comparison.\nlibrary (version 2.4.1). Training was performed on an Intel(R)\nXeon(R) Silver 4108 CPU @ 1.80GHz, 128 GB RAM, and\nfour NVIDIA TITAN RTX GPU. We implemented a data\ngenerator based on the Keras functions to handle massive\ndatasets. The model checkpoints were set regularly.\nC. EVALUATION METRICS\nAccuracy, precision, recall, F-measure, and error rate were\nadopted as the performance metrics to evaluate the effective-\nness of the transformer-based attention network for intrusion\ndetection systems. We were particularly interested in the error\nrate, which presented false-negative and false-positive results.\nA high false-negative rate corresponds to a significant\nnumber of missed intrusions, which pose a danger to drivers\nin long-term attack scenarios. In contrast, a high false-\npositive rate may overwhelm users with false alarms [37].\nIn addition to the multi-class classification model, we used a\nmulti-label model to evaluate the ability to identify intrusion\nmessages in the CAN bus segments. We used the results of\nthe confusion matrices for the calculations, as follows:\nPrecision(Prec) = TP\nTP + FP (9)\nRecall(Rec) = TP\nTP + FN (10)\nF − measure(F1) = 2 ×\n(Prec × Rec\nPrec + Rec\n)\n(11)\nErrorrate(ER) = FP + FN\nTP + TN + FP + FN (12)\nD. HYPERPARAMETERS\nHyperparameters also play an essential role in improving the\nmodel performance. Throughout the experiment, we chose\nsuitable values with a tradeoff via hyperparameter tuning.\nFor instance, the model achieves higher accuracy when the\nembedding dimension is increased. However, the message\nembedding size affects the inference time. Based on empirical\nevaluations conducted on the Car Hacking Dataset with a\nfixed window size (Fig. 11), an embedding dimension of 32 is\nfound to be optimal for the model, offering both high accu-\nracy and small inference time. The optimal hyperparameter\nVOLUME 11, 2023 55397\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 6. Comparison with other models on single-message data.\nTABLE 7. Comparison on sequential data with a window size of 16.\nsettings are listed in detail in Table 5. The sliding window\nsize also affects model performance for a sequential CAN ID\ninput. Therefore, we analyzed the performance for different\nwindow sizes.\nE. RESULTS ON CAR HACKING DATASET\n1) SINGLE CAN MESSAGE\nTable 6 presents performance comparison results between\nour model and existing in-vehicle intrusion detection systems\nusing single messages, representing RNN variants. The input\ndata were independent CAN signals with a length of 10.\nAmong the baselines, CNN-LSTM is a SOTA deep learning\nmodel in IDS for CAN [21], which performs multi-class\nclassification on DoS, fuzzy, and spoofing attacks. Although\nall tested models yielded good results in detecting DoS and\nspoofing attacks, the complexity of fuzzy attack scenarios\nmakes them challenging for traditional models to detect.\nTABLE 8. Comparison on sequential data with a window size of 32.\nTABLE 9. Comparison on sequential data with a window size of 64.\nThe SOTA CNN-LSTM model exhibited impressive perfor-\nmance, with an error rate of 0% and the highest F1 scores for\nDoS and spoofing attacks, accurately classifying abnormal\nmessages. However, our model yielded better results than\nCNN-LSTM for fuzzy attacks, as the latter only achieved an\nF1 score of 0.99992. Furthermore, our model achieved the\nhighest F1 scores for all attack types.\nAlthough the use of a single-message input achieved good\nresults, the injection of extracted messages was not detected\nby the model trained on single-message data. This may\nprove dangerous for the driver if unexpected actions such\nas turning on the light, acceleration, or sudden braking are\ncontinuously sent to the CAN bus without being detected.\nWe used sequential data in our experiment to evaluate the\nmodel performance. The transformer-based supervised learn-\ning model can learn sequential patterns in the CAN bus and\n55398 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 10. Comparison on sequential data with a window size of 128.\nTABLE 11. Comparison on sequential data with a window size of 256.\nFIGURE 12. ROC curves with a window size of 128.\nuse them for classification purposes. When an attacker injects\nthe extracted intrusions, sequential CAN IDs provide the\nmodel with a more comprehensive view of the abnormality\nin the CAN bus. Our experiment started with data from Car\nHacking dataset.\n2) SEQUENTIAL CAN ID\nAs mentioned above, the model performance is significantly\naffected by hyperparameters. Therefore, we evaluate the per-\nformance for window sizes ranging from 16 to 256. Each\nsequential input was labeled as normal or abnormal, with the\noutput being a multi-class classification value. The super-\nvised learning process continuously updates the weights in\nthe model to recognize the sequence of CAN IDs in the time\ndomain. The results presented in Tables 7 and 11 indicate that\nthe transformer model easily outperforms the baselines, par-\nticularly in fuzzy attack scenarios. Furthermore, an increase\nin the window size from 16 to 128 helped all models extract\nmore features from sequential CAN IDs, thereby improv-\ning the classification results. Our model classified abnor-\nmal intrusions almost perfectly in the DoS dataset for every\nwindow size. The reason is that the attacker is forced to\nuse a high-priority message – CAN ID 0x000 – for this\nattack, making it easy for the system to detect intrusions.\nThe results for the spoofing attack dataset showed ERs of\nless than 1 when the window size exceeded 32. In particular,\nin the fuzzy dataset, the F1 score of CNN-LSTM increased\nfrom 0.71820 to 0.99496, whereas our model improved from\n0.99985 to 0.99993, with a gradual increase in the input\ndata length. Thus, the proposed model yields better extracted\ninformation from longer sequential CAN IDs, thereby achiev-\ning better results for larger window sizes. Our results indicate\nthat most F1 scores decreased as the CAN ID chain length\nincreased from 128 to 256, as the baselines experienced bot-\ntlenecks when the window size increased significantly. Our\nmodel also achieved the lowest ER values for most attack\ntypes compared to the other models. As shown in Table 10,\nthe error rates of the baseline models were greater than 5%,\nand the error rates of the Transformer model for the DoS,\nFuzzy, Gear and RPM Spoofing datasets were 0.0%, 0.001%,\n0.112%, and 0.009%, respectively. We also presented ROC\ncurves with AUC values of the proposed model and baseline\nmodels in Fig. 12 in order to compare the overall performance\nbetween models with a window size of 128. The results indi-\ncate that our model improves safety by detecting the majority\nof intrusions and minimizing false alarms.\nF. RESULTS ON THE INTRUSION DETECTION DATASET\nReplay attacks are a type of attack that inject extracted CAN\nmessages, which confuse a model trained on single-message\ninput. To examine the performance of our model with replay\nattacks, we used an In-Vehicle Intrusion Detection dataset.\nOur data analysis revealed CAN messages with the same\nidentifiers as safe messages used for attack purposes. In other\nwords, the messages exhibited the same CAN IDs with dif-\nferent labels. We found that the number of messages labeled\nas normal was more extensive than that of injected malicious\nmessages with the same identifier. Therefore, supervised\nVOLUME 11, 2023 55399\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 12. Comparison the proposed model and CNN-LSTM on replay attack dataset.\nFIGURE 13. Confusion matrices of CNN-LSTM, replay attack dataset, KIA\nsoul, using sequential CAN ID (a) and single message (b).\nFIGURE 14. Confusion matrices of transformer, replay attack dataset, KIA\nsoul, using sequential CAN ID (a) and single message (b).\nFIGURE 15. Confusion matrices of CNN-LSTM, replay attack dataset,\nHyundai Sonata, using sequential CAN ID (a) and single message (b).\nlearning methods tend to be biased and confuse malicious and\nsafety messages.\nWe compared CNN-LSTM and the proposed model on\ntwo car models, with the results listed in Table 12 and\nthe confusion matrices presented in Figs. 13–16. First, the\nresults indicate that sequential input data performs better than\nsingle-message data in replay attack scenarios. In the case\nof Hyundai Sonata, the CNN-LSTM error rate of 17.52%\nFIGURE 16. Confusion matrices of transformer, replay attack dataset,\nHyundai Sonata, using sequential CAN ID (a) and single message (b).\ndecreased to 0.288% when sequential data were used instead\nof single messages. Furthermore, applying the sequential\ndata type reduced the error rate of the transformer-based\nattention network from 15.87% to 0.058%. This script was\nsimilar to other experiments on the KIA Soul car model. For\nCNN-LSTM, ER decreased from 14.18% to 2.006%. By con-\ntrast, the ER of the proposed model decreased from 12.75% to\n0.314%. With the sequential CAN ID input type, CNN-LSTM\nobtained satisfactory results, with ERs of 0.288% for Hyundai\nSonata and 2.006% for Kia Soul. In contrast, our model\nachieved ERs of 0.058% and 0.314% for Hyundai Sonata and\nKia Soul, respectively.\nTo optimize each model’s performance, we selected win-\ndow sizes of 16 and 128 for the transformer-based attention\nnetwork and CNN-LSTM, respectively. The two different\nsliding windows ensured that the CNN-LSTM model yielded\noptimal performance, and the inference time of the pro-\nposed model was suitable. Data were allocated to the training\nand testing sets at a 9:1 ratio. Subsequently, we found that\nthe proposed multi-class model with sequential CAN IDs\ndetects replay attack intrusions more effectively than the\nCNN-LSTM model.\nG. INTRUSION MESSAGE IDENTIFICATION\nOur results indicate that the use of sequential CAN IDs\nprovides a more comprehensive vehicle protection. Intrusion\ndetection systems can stop a vehicle and notify a driver when\nabnormal segments are detected. These malicious segments\ninclude normal and attack messages, so they must not be\nentirely discarded. Training with data labeled at the segment\nlevel can provide warnings when intrusions are detected,\nthis approach faces challenges when identifying anomalous\n55400 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 13. F1 scores of intrusion identification with 16 CAN IDs on Car\nHacking dataset.\nmessages. In the following experiment, we trained on the\nlabeled dataset at the segment level and then used attention\nscores to locate malicious messages in segments. The F1\nscores obtained from the Car Hacking dataset are listed in\nTable 13.\nAs previously mentioned, the threshold is a factor that\naffects the results. We considered all thresholds from 0.3 to\n0.6 with step 0.05, where Avg represents the average value\nof the results, and the optimal threshold yielded the best\nvalue. The DoS and fuzzy attack datasets exhibited effective\nperformance at a threshold of 0.5, whereas the RPM and Gear\nattack datasets demonstrated expected results at 0.3 and 0.4,\nrespectively. Our results demonstrate that the attention score\ncan perfectly identify attack messages in the DoS dataset. The\nidentification of malicious messages in a fuzzy attack dataset\nis not challenging for transformer-based attention networks.\nThe F1 scores on the fuzzy attack dataset mostly reached\n0.99, with slight performance drops on the RPM and gear\nattack datasets.\nHowever, the results at the optimal threshold show that\nthe model successfully identifies the messages injected into\na CAN segment, with F1 scores on the RPM and gear\nattack datasets reaching values of 0.92 and 0.95, respectively.\nNotably, labels for individual messages were not used during\nthe training phase. Our experiment proves that the proposed\nmodel can pay attention to malicious components appearing\nin segments of the CAN bus. By highlighting these intrusive\nCAN IDs, an in-vehicle attack protection system can elimi-\nnate them from the CAN bus and return the vehicle to working\nmode without interrupting operations.\nH. TRANSFER LEARNING\nThe experimental results are presented in Tables 14 and 15,\nrespectively. We compared the results of the standalone (SA)\nTABLE 14. Comparison of transfer learning on KIA soul.\nand transfer learning (TL) models. Owing to data limitations,\nthe training and testing datasets were divided in a 2:1 ratio to\nensure that sufficient data were available for training. First,\nwe partially frozen the source model. Owing to the change\nin the number of classes, it was necessary to replace the\nclassifier in the last layer and then perform one iteration of\ntraining with the target dataset. Next, we unfroze the model\nand conducted fine tuning. The results show that the target\nmodel was successfully obtained from the source model,\nthereby improving its performance. For example, in a fuzzy\nattack, the ER of the transformer decreases from 1.09% to\n0.85%. These results demonstrate the effectiveness of the\ntransfer learning method in reducing the error rate in both the\nCNN-LSTM and transformer networks.\nConversely, if we compare the transformer and CNN-LSTM\nmodels, the proposed model sometimes exhibits a poorer\nperformance. A limitation of our transformer model is that\nthe algorithm is effective only for large datasets. However,\nthe transformer TL can obtain higher F1 scores and lower\nERs than the CNN-LSTM TL. For instance, experiments\non malfunction attacks revealed an F1 score of 0.9936 for\nCNN-LSTM, which was higher than the transformer SA\nof 0.9917.\nHowever, the transformer TL achieved better overall\nresults than the CNN-LSTM TL, with F1 scores of 0.9947 and\n0.9923, respectively. Furthermore, the TL transformer model\nexhibited an excellent intrusion detection performance for\nDoS attacks. From the results, we conclude that the proposed\nmodel can perform well when applying transfer learning with\na suitable pretrained model.\nI. MODEL COMPLEXITY ANALYSIS\nThis section focuses on analyzing the time costs associated\nwith the proposed model. Specifically, we conducted a com-\nprehensive analysis of the model’s performance on a CPU\nto ensure its compatibility with vehicle systems with limited\nhardware capabilities. Using FLOPs analysis, we estimated\nthe computational complexity of our proposed model and\ncompared its training and inference times with those of\nVOLUME 11, 2023 55401\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\nTABLE 15. Comparison of transfer learning on Chevrolet Spark.\nTABLE 16. Comparison of model complexity.\nbaseline models. Previous experiments in [4] have shown\nthat reducing the batch size can optimize the inference time,\nthereby minimizing the overall detection latency of IDS.\nTherefore, we evaluated the performance of the model using\na batch size of 1. That is, we evaluate the processing time\nrequired by the model to handle given input data (s/batch).\nThe results of the time costs analysis are listed in Table 16.\nIn accordance with the J1939 standards [37], the Society of\nAutomotive Engineers (SAE) recommends a signaling rate\nof 250kb/s for the CAN bus, despite its maximum capacity\nof 1 Mb/s [37]. At this recommended rate, the CAN bus can\ntransmit up to 1908 CAN frames per second. Regarding the\nCPU, the proposed model was capable of processing more\nthan 10,000 messages and making approximately 85 predic-\ntions per second, given an inference time of 11.6 ms/batch.\nCompared to the ability to process 4,300 messages per second\nby performing 150 inferences on DCNN [4], despite having a\nlonger inference time with a larger window size, our proposed\nmodel is capable of processing more messages per second.\nA comparison shows that the proposed model is suitable for\nreal-time detection.\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, we proposed a transformer-based attention\nnetwork as an intrusion detection system for CAN buses\nin an in-vehicle environment. This study aimed to present\na novel approach for better intrusion detection that can\ntake over the role of RNNs. The core of the proposed\napproach is a self-attention mechanism. Taking advantage\nof the correlations between the components in CAN data,\nthe transformer-based model offers powerful feature extrac-\ntion capabilities, performs well on longer sequential CAN\nIDs, extracts information more selectively, and outperforms\nother excellent methods in our evaluations. Various experi-\nments were conducted to demonstrate the advantages of the\nproposed model. With a single-message input, our model\nachieved perfect results on the car-hacking dataset, with an F1\nscore of 1.00, and error rates of approximately 0% for all four\nattack scenarios. In particular, our model yielded a superior\nperformance when evaluated using sequential data. Although\nthe transformer requires a longer inference time with the\nsame input length, the result of our model with 16 sequential\nCAN IDs still outperforms that of others. We found that\nthe proposed model can provide better results on the replay\nattack dataset than CNN-LSTM, which yields SOTA intru-\nsion detection performance for CAN buses. We employed\nthe self-attention mechanism to identify intrusive messages in\nCAN segments without requiring message level labeled data\nin the training phase. The results of our experiment support\nthe safe maintenance of vehicle operations following attacks.\nFurthermore, we presented an application of transfer learning\nto our model. The results show that the transformer can share\nknowledge from the source model with the target model to\nimprove intrusion detection. A limitation of our study lies\nin the fact that we experimented with an extracted dataset.\nClassification becomes more difficult when the model is\napplied to vehicles operating in a real-time environment.\nThe direction of our future development will be unknown\nattack detection in real-life attack scenarios. Finally, using the\nsliding window results in the appearance of many malicious\nmessages in single data segments. This complication can be\nresolved using multi-label multi-class classification methods\nin the future.\nREFERENCES\n[1] G. Leen and D. Heffernan, ‘‘Expanding automotive electronic systems,’’\nComputer, vol. 35, no. 1, pp. 88–93, 2002.\n[2] N. A. Stanton, M. Young, and B. McCaulder, ‘‘Drive-by-wire: The case of\ndriver workload and reclaiming control with adaptive cruise control,’’ Saf.\nSci., vol. 27, nos. 2–3, pp. 149–159, Nov. 1997.\n[3] B. Galloway and G. P. Hancke, ‘‘Introduction to industrial control net-\nworks,’’ IEEE Commun. Surveys Tuts., vol. 15, no. 2, pp. 860–880,\n2nd Quart., 2013.\n[4] H. M. Song, J. Woo, and H. K. Kim, ‘‘In-vehicle network intrusion detec-\ntion using deep convolutional neural network,’’ Veh. Commun., vol. 21,\nJan. 2020, Art. no. 100198.\n[5] W. Lo, H. Alqahtani, K. Thakur, A. Almadhor, S. Chander, and G. Kumar,\n‘‘A hybrid deep learning based intrusion detection system using spatial–\ntemporal representation of in-vehicle network traffic,’’ Veh. Commun.,\nvol. 35, Jun. 2022, Art. no. 100471.\n[6] Bosch, Robert Bosch GmbH, Gerlingen, Germany, 1991.\n[7] M. Farsi, M. Barbosa, and K. Ratcliff, ‘‘An overview of controller area\nnetwork,’’Comput. Control Eng. J., vol. 10, no. 3, pp. 113–120, Jun. 1999.\n[8] R. De Andrade, K. N. Hodel, J. F. Justo, A. M. Laganá, M. M. Santos, and\nZ. Gu, ‘‘Analytical and experimental performance evaluations of CAN-FD\nbus,’’IEEE Access, vol. 6, pp. 21287–21295, 2018.\n[9] H. J. Jo and W. Choi, ‘‘A survey of attacks on controller area networks and\ncorresponding countermeasures,’’IEEE Trans. Intell. Transp. Syst., vol. 23,\nno. 7, pp. 6123–6141, Jul. 2022.\n[10] H. Sun, M. Chen, J. Weng, Z. Liu, and G. Geng, ‘‘Anomaly detection for\nin-vehicle network using CNN-LSTM with attention mechanism,’’ IEEE\nTrans. Veh. Technol., vol. 70, no. 10, pp. 10880–10893, Oct. 2021.\n[11] R. Buttigieg, M. Farrugia, and C. Meli, ‘‘Security issues in controller area\nnetworks in automobiles,’’ in Proc. 18th Int. Conf. Sci. Techn. Autom.\nControl Comput. Eng. (STA), Dec. 2017, pp. 93–98.\n55402 VOLUME 11, 2023\nT. P. Nguyen et al.: Transformer-Based Attention Network for In-Vehicle Intrusion Detection\n[12] H. Lee, S. H. Jeong, and H. K. Kim, ‘‘OTIDS: A novel intrusion detection\nsystem for in-vehicle network by using remote frame,’’ in Proc. 15th Annu.\nConf. Privacy, Secur. Trust (PST), Aug. 2017, p. 5709.\n[13] H. M. Song, H. R. Kim, and H. K. Kim, ‘‘Intrusion detection system based\non the analysis of time intervals of CAN messages for in-vehicle network,’’\nin Proc. Int. Conf. Inf. Netw. (ICOIN), Jan. 2016, pp. 63–68.\n[14] W. Wu, R. Li, G. Xie, J. An, Y. Bai, J. Zhou, and K. Li, ‘‘A survey of\nintrusion detection for in-vehicle networks,’’ IEEE Trans. Intell. Transp.\nSyst., vol. 21, no. 3, pp. 919–933, Mar. 2020.\n[15] A. Wasicek, M. D. Pese, A. Weimerskirch, Y. Burakova, and K. Singh,\n‘‘Context-aware intrusion detection in automotive control systems,’’ in\nProc. 5th ESCAR USA Conf., 2017, pp. 21–22.\n[16] H. Qin, M. Yan, and H. Ji, ‘‘Application of controller area network (CAN)\nbus anomaly detection based on time series prediction,’’ Veh. Commun.,\nvol. 27, Jan. 2021, Art. no. 100291.\n[17] M. D. Hossain, H. Inoue, H. Ochiai, D. Fall, and Y. Kadobayashi, ‘‘LSTM-\nbased intrusion detection system for in-vehicle can bus communications,’’\nIEEE Access, vol. 8, pp. 185489–185502, 2020.\n[18] M. L. Han, B. I. Kwak, and H. K. Kim, ‘‘Anomaly intrusion detection\nmethod for vehicular networks based on survival analysis,’’ Veh. Commun.,\nvol. 14, pp. 52–63, Oct. 2018.\n[19] I. A. Khan, N. Moustafa, D. Pi, W. Haider, B. Li, and A. Jolfaei,\n‘‘An enhanced multi-stage deep learning framework for detecting mali-\ncious activities from autonomous vehicles,’’ IEEE Trans. Intell. Transp.\nSyst., vol. 23, no. 12, pp. 25469–25478, Dec. 2022.\n[20] T.-N. Hoang and D. Kim, ‘‘Detecting in-vehicle intrusion via semi-\nsupervised learning-based convolutional adversarial autoencoders,’’ Veh.\nCommun., vol. 38, Dec. 2022, Art. no. 100520.\n[21] A. K. Desta, S. Ohira, I. Arai, and K. Fujikawa, ‘‘Rec-CNN: In-vehicle\nnetworks intrusion detection using convolutional neural networks trained\non recurrence plots,’’ Veh. Commun., vol. 35, Jun. 2022, Art. no. 100470.\n[22] L. Kang and H. Shen, ‘‘A transfer learning based abnormal CAN bus\nmessage detection system,’’ in Proc. IEEE 18th Int. Conf. Mobile Ad Hoc\nSmart Syst. (MASS), Oct. 2021, pp. 545–553.\n[23] S. Tariq, S. Lee, and S. S. Woo, ‘‘CANTransfer: Transfer learning based\nintrusion detection on a controller area network using convolutional LSTM\nnetwork,’’ in Proc. 35th Annu. ACM Symp. Appl. Comput., Mar. 2020,\npp. 1048–1055.\n[24] P. Wei, B. Wang, X. Dai, L. Li, and F. He, ‘‘A novel intrusion detection\nmodel for the CAN bus packet of in-vehicle network based on atten-\ntion mechanism and autoencoder,’’ Digit. Commun. Netw., vol. 9, no. 1,\npp. 14–21, Feb. 2023.\n[25] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[26] M. Nam, S. Park, and D. S. Kim, ‘‘Intrusion detection method using bi-\ndirectional GPT for in-vehicle controller area networks,’’ IEEE Access,\nvol. 9, pp. 124931–124944, 2021.\n[27] Z. Wu, H. Zhang, P. Wang, and Z. Sun, ‘‘RTIDS: A robust transformer-\nbased approach for intrusion detection system,’’ IEEE Access, vol. 10,\npp. 64375–64387, 2022.\n[28] O. Sharma, N. C. Sahoo, and N. B. Puhan, ‘‘Kernelized convolutional\ntransformer network based driver behavior estimation for conflict res-\nolution at unsignalized roundabout,’’ ISA Trans., vol. 133, pp. 13–28,\nFeb. 2023.\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nand I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 1–11.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[31] B. Han, Z. Chen, and Y. Qian, ‘‘Local information modeling with self-\nattention for speaker verification,’’ in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process. (ICASSP), May 2022, pp. 6727–6731.\n[32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[33] S. J. Pan and Q. Yang, ‘‘A survey on transfer learning,’’ IEEE Trans. Knowl.\nData Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.\n[34] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He,\n‘‘A comprehensive survey on transfer learning,’’ Proc. IEEE, vol. 109,\nno. 1, pp. 43–76, Jan. 2021.\n[35] T.-N. Hoang and D. Kim, ‘‘Supervised contrastive ResNet and trans-\nfer learning for the in-vehicle intrusion detection system,’’ 2022,\narXiv:2207.10814.\n[36] The In-Vehicle Network Intrusion Detection Dataset of Information Secu-\nrity R&D Data Challenge, Hacking Countermeasure Res. Lab, Korea\nUniv., Seoul, South Korea, 2019.\n[37] J1939 Recommended Practice for a Serial Control and Communications\nVehicle Network, SAE Int., Warrendale, PA, USA, 2013.\nTRIEU PHONG NGUYEN received the B.S.\ndegree in control engineering and automation\nsystem from the Ho Chi Minh City University\nof Transport (UT-HCMC), Vietnam, in 2021.\nHe is currently pursuing the M.S. degree with\nthe Department of Mobility Convergence Security\nin Engineering, Soonchunhyang University, Asan,\nSouth Korea. From January 2020 to January 2021,\nhe was an Assistant Researcher with the Artificial\nIntelligence Laboratory, Graduate Institute,\nUT-HCMC. From February 2021 to January 2022, he was an Artificial\nIntelligence Engineer with Heligate Japan Software Company, Vietnam. His\nresearch interest includes deep learning, the Internet of Things, and security\nnetworks.\nHEUNGWOO NAM (Member, IEEE) received\nthe B.S. degree in computer science and engi-\nneering from Kyung Hee University, South Korea,\nin 2002, and the M.S. degree in computer science\nand engineering and the Ph.D. degree in electron-\nics and computer engineering from Korea Univer-\nsity, South Korea, in 2005 and 2010, respectively.\nFrom September 2010 to May 2015, he was a\nResearch Professor with Korea University. From\nMay 2015 to December 2017, he was a Visiting\nResearcher with the Scripps Institution of Oceanography (SIO), University\nof California, San Diego (UCSD). From December 2017 to August 2020,\nhe was a Research Professor with the Smart Submerged Floating Tunnel\nSystem Research Center, KAIST. Since September 2020, he has been an\nAssistant Professor with the Department of Computer Engineering, Daegu\nUniversity. His research interests include communication networks and pro-\ntocols, underwater acoustic communication systems, autonomous underwa-\nter vehicles, wireless and underwater sensor networks, and structural health\nmonitoring.\nDAEHEE KIM (Member, IEEE) received the\nB.S. degree in electrical and electronic engineer-\ning from Yonsei University, Seoul, South Korea,\nin 2003, and the M.S. and Ph.D. degrees in electri-\ncal and electronic engineering from Korea Univer-\nsity, Seoul, in 2006 and 2016, respectively.\nFrom 2006 to 2016, he was a Senior Engineer\nwith Samsung Electronics, Suwon, South Korea,\nwhere he conducted research on WiMAX and LTE\nsystems. He is currently an Assistant Professor\nwith the Department of Internet of Things, Soonchunhyang University, Asan,\nSouth Korea. His research interest includes the Internet of Things, energy\nmanagement, blockchain, 5G/6G, and security for wireless networks.\nVOLUME 11, 2023 55403"
}