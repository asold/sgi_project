{
  "title": "Style Example-Guided Text Generation using Generative Adversarial Transformers",
  "url": "https://openalex.org/W2995199838",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4367168918",
      "name": "Zeng, Kuo-Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221882394",
      "name": "Shoeybi, Mohammad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2755830177",
      "name": "Liu, Ming-Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962947361",
    "https://openalex.org/W3034917202",
    "https://openalex.org/W2937297214",
    "https://openalex.org/W2545656684",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2917166349",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2911803042",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2970454257",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2808437126",
    "https://openalex.org/W2136891251",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2948975009",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2250616809",
    "https://openalex.org/W2598581049",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2965225113",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2962772087",
    "https://openalex.org/W2947511876",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1967017231",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970669113",
    "https://openalex.org/W2757675222",
    "https://openalex.org/W2989855043",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2895723011",
    "https://openalex.org/W2792369272",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2803930360",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2612034718",
    "https://openalex.org/W2784996692",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2963494889"
  ],
  "abstract": "We introduce a language generative model framework for generating a styled paragraph based on a context sentence and a style reference example. The framework consists of a style encoder and a texts decoder. The style encoder extracts a style code from the reference example, and the text decoder generates texts based on the style code and the context. We propose a novel objective function to train our framework. We also investigate different network design choices. We conduct extensive experimental validation with comparison to strong baselines to validate the effectiveness of the proposed framework using a newly collected dataset with diverse text styles. Both code and dataset will be released upon publication.",
  "full_text": "Style Example-Guided Text Generation using Generative Adversarial Transformers\nSTYLE EXAMPLE -GUIDED TEXT GENERATION USING\nGENERATIVE ADVERSARIAL TRANSFORMERS\nKuo-Hao Zeng∗, Mohammad Shoeybi & Ming-Yu Liu\nkhzeng@cs.washington.edu, {mshoeybi, mingyul}@nvidia.com\nNVIDIA, Santa Clara, California\nABSTRACT\nWe introduce a language generative model framework for generating a styled para-\ngraph based on a context sentence and a style reference example. The framework\nconsists of a style encoder and a texts decoder. The style encoder extracts a style\ncode from the reference example, and the text decoder generates texts based on\nthe style code and the context. We propose a novel objective function to train our\nframework. We also investigate different network design choices. We conduct\nextensive experimental validation with comparison to strong baselines to validate\nthe effectiveness of the proposed framework using a newly collected dataset with\ndiverse text styles. Both code and dataset will be released upon publication.\n1 I NTRODUCTION\nText generation refers to the task of generating a new text based on some user input. The task exists\nin many forms, but arguably the most common form concerns generating a coherent and consistent\ntext based on an input context such as the ﬁrst few sentences of the target output. This is often\nachieved by giving the context to a generative language model. Generative language models play a\ncentral role in machine learning and natural language processing (NLP). Not only they serve as the\nmain mean for unsupervised feature representation learning but also ﬁnd use in various applications,\nincluding question answering, dialogue agents, summarization, and content creation systems.\nThanks to the introduction of novel deep learning architectures and the availability of large-scale\ntraining corpora, the state-of-the-art text generation has advanced signiﬁcantly in recent years. We\ncan now train language models capable of generating ﬂuent and coherent texts that people cannot\ntell them apart from those written by humans. However, despite the great achievement, existing\ngenerative models are limited and inﬂexible in the sense that a trained model is only capable in\ngenerating texts of one style. It can not be used to generate texts of different styles. For instance, a\nnews generative model can only be used to generate news, and a lyric generative model can only be\nused to generate lyrics. In contrast, humans can compose texts in various styles.\nTo bridge the gap, we propose a style example-guided text generation framework that can generate\nstyled texts based on the style of the example reference text. In our framework, the generator takes\ntwo inputs where one is the context input while the other is the style reference example. We use\nthe style reference example to change the generation behavior of our generative model dynamically.\nFor a ﬁxed context, when the provided style reference is a paragraph sampled from a news article,\nit becomes a news generator. When the provided style reference is a review, it becomes a review\ngenerator. In other words, the same generator can generate texts of different styles based on the\nexamples. In Tab. 1, we show example outputs of the proposed framework where we generate texts\nof dramatically different styles for the same input sentence.\nThe proposed style example-guided text generation framework is based on the generative adversarial\nnetworks (GANs), and we utilize the transformer in both the generator and discriminator design.\nWe collect a large dataset containing documents with many different styles for training. Using a\nnovel learning objective function, our network learns to generate styled texts based on the input\nstyle example in an unsupervised manner. We conduct extensive experimental validations with\ncomparisons to strong baselines. We also investigate different ways of designing the generator and\n∗The work was done during internship at NVIDIA.\n1\narXiv:2003.00674v1  [cs.CL]  2 Mar 2020\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Wiki Style (Machine Learning Intro.)Machine learning (ML) is the scientiﬁc study of algorithms and statistical models that computer systems use to perform a speciﬁc task without using explicit instructions,relying on patterns and inference instead. It is seen as a subset of artiﬁcial intelligence. Machine learning algorithms build a mathematical model based on sample data,known as ”training data”, in order to make predictions or decisions without being explicitly programmed to perform the task.[1][2]:\nReference – News Style(CNN)Remember that robot dog that went viral a few years ago? The one that canrun uphill, climb stairs, and looks like the killer robot dogs in the TV show ”BlackMirror”? It’s now available for select customers.\nBoston Dynamics, the company behind the dog, which is named Spot, released avideo on Tuesday advertising its launch. It shows Spot navigating challenging terrain,picking up construction objects, moving through bad weather, and picking itself upafter a fall. The video and the product launch page also show off Spot’s capabilities –for instance, it can run at 1.6 meters per second (about 5.2 feet per second), has 360-degree cameras that give it a full range of vision, is dust- and water-proof, and canoperate in temperatures ranging from -20 to 45 degrees Celsius (-4 to 113 Fahrenheit).\nReference – Yelp Reviews StyleYumm\nThe line was AWFUL! Came by Saturday around 5:20pm and the line was CRAZY . Ido feel like they’re understaffed. Despite that, it was my ﬁrst time and i got the chanceto taste test two ﬂavors, matcha and milk tea! Both were great!!\nI got the :- Matcha swirl with a custard ﬁsh, sprinkles and coconut ﬂakes : The custard was sogood! Paired great with the ice cream.\nStaff were so friendly! Especially the cashier (guy) and person who swirled the icecream that shift! She was super patient as I, myself is very indecisive, haha! Mostdeﬁnitely coming by again!!\nGeneratedMachine Learning is an ancient concept that has entered the mainstream today, whichmeans that it can survive in the face of such heightened competition. In its application,Flurry wants to improve the efﬁciency of AI driven computing machines, and with alarge footprint. As is seen on me, Flurry has designed a large extension that (unfortu-nately) can be used in its development, but with lower speed lag. Now that the hybridtechnology is here, it’s there for the taking.\nThere may also be a Something About Mary in the future. ”So it’s nice to be able tomount a cat across your desk without requiring any backup access.”\nGeneratedSince the day machines started boosting up, dozens of companies have recently doneso. In fact, I highly recommend RoboSports, an example of small-batch ice creamplaces that have taken up artiﬁcial advances in AI; their ice cream is the perfect size,slim and hot, it was great. If you are a connoisseur of ice cream, check out thiswonderful ice cream shop.\nTable 1: Given the same context sentences but different style reference examples, our generation model can\ngenerate paragraphs of different styles. The output paragraph shares the same style as the reference example.\ncompare their performance. Through detailed quantitative and user study results, we prove the\neffectiveness of the proposed framework for the style example-guided text generation task.\n2 R ELATED WORK\nLanguage modelinghas seen many advancements in recent years, which has resulted in signiﬁcant\nimprovements on various NLP tasks. Early language models focused on using n-grams to represent\na text distribution. Bengio et al. (2003) introduced a neural language model in a shift from more tra-\nditional n-gram models. Many works later (Mikolov et al. (2013); Pennington et al. (2014)) focused\non word embeddings as a way to represent tokens within the text. More recently, Peters et al. (2018)\nused bi-directional LSTMs to obtain deep contextualized word representation. However, RNNs can\nonly represent a limited context. Vaswani et al. (2017) introduced the transformer networks which\nuse the connections between long-distance word pairs embedded in attention mechanisms and can\neasily enable the learning of long-term dependency. Many later models (Devlin et al. (2019); Liu\net al. (2019d); Dai et al. (2019); Yang et al. (2019)) used transformer model and obtained signiﬁ-\ncant improvements on downstream tasks (Wang et al. (2019); Rajpurkar et al. (2016); Zellers et al.\n(2018)). Lately, (Radford et al. (2019)) introduced GPT-2, a generative left-to-right language model\nbased on the transformer and showed that these models are able to generate coherent text when pre-\ntrained on a large corpus. Shoeybi et al. (2019) further scaled up the GPT-2 model and demonstrated\nimproved performance. Our work differs from the prior works because we aim for allowing user\nﬂexible control over the style of the generated text.\nTexts generationincludes review generation (Radford et al. (2018); Zang & Wan (2017)), sentiment\ntexts generation (Wang & Wan (2018); Hu et al. (2017); Merity et al. (2017)), Wikipedia generation\n(Liu et al. (2018); Lebret et al. (2016)), fake news generation (Bakhtin et al. (2019); Zellers et al.\n(2019)), abstractive summarization (Li et al. (2018); Zhang et al. (2019); Pasunuru et al. (2017)),\nand conversation/dialogue system (Vinyals & Le (2015); Budzianowski & Vuli´c (2019)). Although\nmany of them trained a transformer on large-scale corpora, their results were limited in their speciﬁc\ndomain (e.g., reviews, news, etc.) because they either utilized domain-speciﬁc priors in their model\ndesign or were not designed to generate texts in many different domains or styles.\nControl on texts generation. In addition, there are literature utilizing insertion-base (Stern et al.\n(2019); Chan et al. (2019)), GAN-based (Yu et al. (2017); d’Autume et al. (2019)), variational\nautoencoder-based (Xu et al. (2019)), normalizing ﬂow-based (Tran et al. (2019)) approaches for\ngeneral texts generation task. However, we instead focus on generating styled paragraphs condition-\ning on a context and a reference paragraph. A recent work by Keskar et al. (2019) is most related to\nours. They propose a conditional transformer using a control code to perform language generation in\n2\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\na sequence-to-sequence manner. We demonstrate our method outperforms theirs by a large margin\nin the experiment section.\nText style transferconcerns transferring an input text of one style to a different style (Kerpedjiev\n(1992); Rao & Tetreault (2018); Xu (2017); Xu et al. (2012); Fu et al. (2018); Hu et al. (2017);\nPrabhumoye et al. (2018); Shen et al. (2017); Li et al. (2019)). Our work is different since we\ndo not aim for changing the style of a given text. Instead, we aim for a style-controllable way\nfor generating texts from scratch. Also, rather than handling transferring between two styles (e.g.,\npositive ↔negative sentiments), our model can generate texts of many different styles. Finally, our\nmodel outputs paragraphs while existing text style transfer works mostly output sentences.\nImage Style transferis a popular topic in computer vision. There are many successful techniques,\nincluding iterative optimization on the gram matrix (Gatys et al. (2016)), perceptual loss (John-\nson et al. (2016); Gupta et al. (2017)), feature transformation (Li et al. (2017)), adaptive instance-\nnormalization (Dumoulin et al. (2017); Huang & Belongie (2017)), and GAN-based methods (Zhu\net al. (2017); Kim et al. (2017)). Our proposed framework also gets inspiration from them.\n3 P RELIMINARIES\nOur framework is based on the transformer network (Vaswani et al. (2017)) and the GAN frame-\nwork (Goodfellow et al. (2014)). In this section, we brieﬂy review these two components.\nTransformeris the state-of-the-art network for various natural language processing tasks. Different\nfrom RNNs (Hochreiter & Schmidhuber (1997); Bengio et al. (2003); Chung et al. (2014)), which\nconsume a sequence token by token, in a transformer network, the entire sequence is fed into layers\nof transformer modules. The representation of a token at a layer is then computed by attending to\nthe latent representations of all the other tokens in the preceding layer.\nVariants of transformer networks are available. We build our model based on GPT-2 transformer\nnetwork (Radford et al. (2019); Shoeybi et al. (2019)), which train a deep transformer using a left-\nto-right language model:\np(w) =\nT∏\nt\np(wt|wt−1...w1), (1)\nwhere wt’s denote the word tokens. Different from BERT-like transformer networks (Devlin et al.\n(2019); Liu et al. (2019d)), GPT-2 makes a casual assumption, i.e., the latent representation of\na token is calculated using only the latent representations of the preceding tokens. Thus, during\ngeneration, GPT-2 can be directly applied to complete the text given the context sentence.\nGAN deﬁnes a zero-sum game played by a generator F and a discriminator D. Under some nice\nconditions, the generator learns to convert a random noise vector to a realistic signal in a way that\nthe discriminator cannot tell it apart from real signals. In this case, the distribution of the output\nsignals produced by the generator converges to the distribution of signals observed in the real world.\nWe use a conditional GAN where F takes a context sentence and a style reference example as\ninputs. To avoid non-differentiability in text decoding (e.g., beam search), we use a latent GAN\nformulation (Achlioptas et al. (2017)). We ﬁrst divide F into a feature extractor Ff and an output\nembedding layer Fo; that is F ≡Fo ◦Ff. Now, instead of using the output text from Fo as the\ndiscriminator input, we feed the latent representation computed by Ff to the discriminator. For real\ntext, we use a pretrained trained GPT-2 model H. Again, we decompose H into a feature extractor\nHf and an output embedding layer Ho (H ≡Ho◦Hf). The GAN discriminator then takes features\nextracted by Hf as input for real texts. Using this latent GAN formulation, we aim for aligning the\nfeature distribution of our generator to the feature distribution of the pretrained GPT-2 model.\n4 S TYLE EXAMPLE -GUIDED TEXT GENERATION\nWe propose a language generative model framework that allows us to control style of the output text\nusing a style reference example. Given few context sentences w = {wt}T\nt=1 and a reference text s,\nour generator F generates output text y that has the same style as the reference example s given by\ny = F(w,s) ≡Fo(Ff(w,s)). (2)\n3\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nFigure 1: We employ two data streams to train our framework. While pi and pj have the same style, pi and\npk do not. (a) The reconstruction stream is trained using the language modeling loss LLM and the distillation\nloss LDIST . (b) The cross-style generation stream is trained using the style loss LSTY LE and the GAN loss\nLGAN. Note that we decompose each network into a feature extractor and an embedding layer.\nWe divide the feature extractor Ff into a style encoder Fs and a text decoder Fg where the style\nencoder extracts a style representation from the style example, z = Fs(s), and the text decoder Fg\nconsumes the style representation and the context sentences to compute a feature for Fo to generate\nthe styled text y. In this section, we will ﬁrst introduce the data streams employed during training\nand our novel learning objective function. We will then discuss various generator design choices.\n4.1 L EARNING DATA STREAMS\nLet D = {(dn,ln)}be a dataset of documents where dn is a document and ln is its style label. We\nassume a ﬁnite set of style labels L= {1,2,...,L }where each integer represents a style class such\nas news, review, lyric, poem, novel, and children book. During training, our framework employs two\ndata streams where the ﬁrst one is called the reconstruction stream while the other is referred to as\nthe cross-style generation stream. We note that such a two-stream processing pipeline is common in\nGAN-based image translation frameworks (Liu et al. (2017); Huang et al. (2018); Liu et al. (2019a))\nbut is less explored for language modeling.\nReconstruction stream(RS). For this steam, we ﬁrst sample two documents with the same style\nfrom D: (di,li) and (dj,lj) where li = lj. We then sample two paragraphs1: pi ∼di and pj ∼dj.\nWe extract the ﬁrst few sentences frompi as the input context w = ψ(pi), where ψis the extraction\nfunction, and use pj for the style reference s. Feeding w and pj to the generator F, we expect F\nshould be able to reconstruct pi: F(ψ(pi),pj) ≈pi.\nCross-style generation stream(CS). We ﬁrst sample two documents(di,li) ∼D and (dk,lk) ∼D\nwhere li ̸= lk. We then sample paragraphs pi ∼di and pk ∼dk. We again extract the ﬁrst few\nsentences from pi as the input context w = ψ(pi) and use pk for the style reference s. As feeding\nw and pk to the generator F, we expect F should output pi→k = F(ψ(pi),pk) where pi→k should\nhas the same style as dk. Let C∗be an oracle style comparator function that outputs 1 if the two\ninput texts have the same style and 0 otherwise. We aim for C∗(pi→k,pk) = 1.\n4.2 L EARNING OBJECTIVE\nWe propose an objective function consisting of four carefully designed loss terms for training the\nproposed framework using the above two data streams. The objective function is given by\nL= LLM + λDIST LDIST + λSTYLE LSTYLE + λGANLGAN, (3)\nwhere LLM is the language modeling loss, LDIST the distillation loss, LSTYLE is a style com-\nparison loss, and LGAN is the latent GAN loss. The scalars λDIST , λSTYLE , and λGAN are the\nhyper-parameters controlling relative importance of the terms. The values for these hyperparame-\nters and the method for determining their values are discussed in Appendix A. We visualizes training\nwith the proposed objective function using the two data streams in Fig. 1.\nLanguage modeling lossLLM formulates the probability distribution of a paragraphp as the prod-\nuct of the conditional probability of each token wt given the previous tokens {wt}T−1\nt as shown\nin (1). We use LLM to supervise the training of the data reconstruction stream. It is given by\nLLM = E(pi,pj)∼RS\n[−1\nT\nT∑\nt\nlog( eF(wt|w1:t−1,pj)\n∑V\nv eF(wv|w1:t−1,pj)\n)\n]\n, (4)\n1For the purpose of data augmentation, in our implementation, a paragraph we sample may not be the full\nparagraph in the nominal sense. It could starting from the middle of a nominal paragraph.\n4\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nwhere (pi,pj) ∼RS denotes that pi and pj are from the reconstruction stream. The variable T is\nthe total number of tokens in pi and V is the size of the vocabulary.\nDistillation loss. We use LDIST to regularize the learning as processing the data reconstruction\nsteam. We pretrain a GPT-2 model using our datasetD and use it as our distillation target. We denote\nthe pretrained GPT-2 model as H. (Note that H does not have the desired style control capability.)\nBy jointly optimizing LLM and LDIST , we train F to generate ﬂuent texts (by minimizing LLM)\nas well as behave similarly to H (by minimizing LDIST ). The distillation loss is calculated by\nminimizing the mutual information between output distributions of F and H, which is given by\nLDIST = E(pi,pj)∼RS\n[−1\nT\nT∑\nt\n|ν|∑\nv\neH(wt=v|w1:t−1)\n∑|ν|\nv eH(wv|w1:t−1)\nlog( eF(wt|w1:t−1,pj)\n∑|ν|\nv eF(wv|w1:t−1,pj)\n)\n]\n. (5)\nWe note that the distillation loss has been used in various tasks including model compression, trans-\nfer learning, life-long learning, etc (Hinton et al. (2015); Kim & Rush (2016); Liu et al. (2019c);\nMirzadeh et al. (2019); Liu et al. (2019b); Hou et al. (2018)). In this paper, we extend its use to the\nstyle example-guided language generative model training task.\nStyle loss LSTYLE helps ensure the output from the cross-style generation stream has the same\nstyle as the input reference. A pretrained style comparator C is used for computing the loss. The\ncomparator takes two paragraphs as input and is trained to output 1 when the two paragraphs have\nthe same style and 0 otherwise. We use D for pretraining C since it contains style labels for each\ndocument. We pretrain C using the binary cross entropy loss. The comparator C is highly accu-\nrate. It achieves a classiﬁcation accuracy of 87.8% to 98.8% in our held-out validation sets. After\npretaining, we ﬁx Cand use it to train F. The style loss LSTYLE is then given by\nLC = E(pi,pk)∼CS\n[\n−log\n(\nC\n(\nHf(pk),Ff(ψ(pi),pk)\n))]\n(6)\nwhere (pi,pk) ∼CS denotes the pair is sampled from the cross-style generation stream.\nHere, we would like to make two remarks. First, sinceCtakes the latent feature fromFf as input, we\navoid the non-differentiability of the text decoding mechanism and can directly train Ff. Second,\ndespite that C is pretrained using feature extracted from Hf, we use the feature extracted from\nFf as input. We can perform this operation not only because these two features have the same\ndimension but also because we enforce them to have a similar distribution via optimizing the GAN\nloss, discussed below.\nGAN lossLGAN is used to match the distributions of the features generated by Ff and those gen-\nerated by Hf, respectively, as processing the cross-style generation stream. We use a latent GAN\nformulation where we train a GAN discriminator D to differentiate features extracted from Ff to\nHf. The GAN loss is given by\nEpi,pk\n[\n−log\n(\nD(Hf(pk)\n)\n−log\n(\n1 −D(Ff(f(pi),pk))\n)]\n. (7)\nWe realize the discriminator Dusing a GPT-2-based transformer network.\n4.3 G ENERATOR DESIGN\nWe realize the style encoder Fs using a GPT-2-based transformer identical to Hf. After extracting\na representation zt for each token tin s, we utilize a 3-layer position-wise fully-connected network\nto obtain the ﬁnal style code z as illustrated in Fig. 2. The text decoder Fg is also a GPT-2-based\ntransformer identical toH. We initialize the weights inFsand Fg using the weights in the pretrained\nH. Next, we compare four different ways of injecting outputs from Fs into Fg, which represent\ndifferent inductive biases and result in difference performances.\nModel A: style code as a bias to the input. In this model, the style code z = Fs(s) is directly\nsummed up with the token-embedding and position embedding before inputting to the ﬁrst trans-\nformer module in Fg. In other words, the input to the ﬁrst transformer module in Fg is ew\nt + ep\nt + z\nwhere ew\nt denotes as the tth word embedding, and ep\nt denotes as the tth position embedding.\nModel B: style code as a summarization token. In this model, the computed style code z = Fs(s)\nis treated as a special token that is inserted to the beginning of the input sequence and is directed fed\n5\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nInput EmbeddingPositionEmbedding\nLayer NormMulti-headAttention\nLayer NormFC\nMtransformermodules\nBottlenecked (256)Position-wise FC\nBottlenecked (64)Position-wise FC\nBottlenecked (1)Position-wise FC\nInput EmbeddingPositionEmbedding\nLayer Norm\nMulti-headAttention\nLayer NormFC\nMtransformermodules\nModel A\nModel B\nModel C\nModel D\nLayer NormOutput embeddingSoftmax\nFigure 2: (a) Style encoder Fs. (b) Text decoder Fg. We consider 4 different ways of injecting the style code\nz into Fg termed Model A, B, C, and D. Checkout main texts for more details.\nin the ﬁrst transformer module in Fg. That is the input sequence length becomes T+ 1. This design\nis motivated by the traditional sequence-to-sequence modeling techniques (Chung et al. (2014); Cho\net al. (2014); Sutskever et al. (2014); Bahdanau et al. (2016); Vinyals & Le (2015)).\nModel C: style-aware self-attention. In this model, we input z into each self-attention layer in\nFg to inﬂuence its computation given by Softmax(\nqmkT\nm−1√\nB )vm−1 where qm = ηm(z) which ηm\ndenotes an afﬁne transformation, km−1 and vm−1 denotes the key and value embeddings from the\n(m−1)th hidden layer, and Bdenotes the hidden dimension.\nModel D: adaptive layer normalization. Inspired by the recent success in image generation\ntasks (Park et al. (2019); Karras et al. (2019)), we utilize the style code to modulate the hidden\nrepresentations within the text decoder via normalization layers. Speciﬁcally, we replace the scale\nand bias parameters in the afﬁne transformation step of the layer normalization (Ba et al. (2016))\nwith a style code determined scale and bias. That is\nγa\nm,c(z)ha\nm,c,t −µa\nm,t\nσa\nm,t\n+ βa\nm,c(z), (8)\nwhere ha\nm,c,t denotes the cth hidden representation of the tth token at the mth transformer layer. We\nnote a= {1,2}since there are two layer normalization layers in each transformer in our implemen-\ntation. The mean and deviation µa\nm,t and σa\nm,t are computed across the channel dimension.\nWe illustrate how these models inject z to Fg in Fig. 2. In Section 5, we compare the performance\nof these variants and show that Model D achieves the best style generation performance.\n5 E XPERIMENTS\nImplementation. We set the latent dimension B to 768, number of attention-heads to 16, number\nof transformer layers M to 16, number of tokens in a paragraph T to 512, and the vocabulary size\nV to 50257 using BPE-encoding (Sennrich et al. (2015)) vocabulary from Radford et al. (2019)\nthroughout out all the models and experiments. We use a pretrained GPT-2 model H and a style\ncomparator Cin our framework. The training details of these two models are given in Appendix B.\nAll of the experiments are conducted using an NVIDIA DGX1 machine.\n6\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nDatasets. We compare competing methods using two newly composed datasets based on (Zhu et al.\n(2015); Zellers et al. (2019); Santiago (2015); See et al. (2017)).\n3-Style. The dataset consists of documents from the RealNews dataset (Zellers et al. (2019)),\nthe BookCorpus dataset (Zhu et al. (2015)), and the Reviews dataset (Yelp (2019); McAuley &\nLeskovec (2013); Maas et al. (2011); Dataworld (2017); Liu (2017)). The 3 styles are news, book,\nand review. In detail, the news set has 33M documents and 113B words, the books set has 50K\ndocuments and 7.2Bwords, and the review set has4.8M documents and 5.4Bwords after cleaning.\nThe total dataset has 37.85M documents and 125.6Bwords. We hold out 3.78M documents as the\nvalidation set and 6Kdocuments as the testing set.\n21-Style. We build a dataset that contains 21 text styles. We ﬁrst classify the documents in Real-\nNews into 9 styles, including Sciences, Sport, Politics, Business, Technology, Entertainment, Opin-\nion, Life, and News. Then, we divide the documents in BookCorpus into 8 different styles, which are\nRomance, Fantasy, Scienceﬁction, Childrensbooks, Thriller, Adventure, Poetry, and Plays. We split\nthe documents into multiple small documents by extracting the dialogues except for the Poetry and\nPlays. We divide the Review dataset into3 styles, namely Yelp, Hotel, and Movie. Finally, we crawl\n0.77M lyrics from http://www.azlyrics.com/. The total dataset has 35.5M documents.\nWe hold out 3.55M documents as the validation set and 21Kdocuments as the testing set.\nAuto-evaluation metrics. We evaluate different models using ﬂuency score, style score, style di-\nversity score, and content novelty score. The ﬂuency score measures whether the output paragraph\nreads like a human-written one. The style score checks whether the output text carries the target\nstyle. Our framework supports multimodal outputs (Huang et al. (2018)). For the same input con-\ntext but different reference examples of the same style, our framework should produce different\noutput texts but all with the same style. To measure how different these outputs are, we use the\nstyle diversity score. Finally, the content novelty score is used to measure the difference between\nthe output and the reference example. A model that directly duplicates the reference to the output is\nundesirable. The details of these automatic evaluation metrics are available in Appendix C.\nHuman study settings. We use the Amazon Mechanical Turk (AMT) platform for user studies.\nWe conduct two studies where one evaluates ﬂuency of the generated paragraphs while the other\nveriﬁes the style correctness. For the ﬂuency study, we present a human-written text and a machine-\ngenerated text in random order and ask the worker to choose which one is written by a human. For\nthis metric, the closer the preference score to 50%, the better the performance.\nFor the style study, we perform two tests. In one test, we present a worker a generated paragraph that\nsupposes to be in the target style. We also give the worker two human-written reference paragraphs\nwhere one is with the target style while the other is not. We then ask the worker to choose which\nreference paragraph has a style more similar to the generated one. In the other test, we again present\na worker a generated paragraph but this time with the style categorical labels to choose from instead\nof the reference paragraphs. We compute the frequency that the worker selects the right style. The\nhigher the score, the better the performance. More details are in Appendix D.\nStrong baselines. We compare our framework to three strong baselines, namely the general GPT-2\nmodel (G-GPT-2), a baseline consists of multiple style-specialized GPT-2 models (S-GPT-2), and\nthe style-code encoding (SC) method based on the description in Keskar et al. (2019). G-GPT-2 is\ntrained on the entire dataset usingLLM. It does not allow style control but can generate ﬂuent texts.\nIn S-GPT-2, we train a GPT-2 model per style. As training a GPT-2 model is costly, we only use\nthis baseline for the3-Style dataset evaluation. In SC, an one-hot encoding of the style class label\nis used as a special token for style-controllable paragraph generation. Unlike the proposed method\nthat extracts the style code from the input paragraph, SC input the style label. The rest of the model\nis similar to our Model B without the style encoder.\n5.1 R ESULTS\nIn Fig. 3, we plot the ﬂuency and style scores achieved by our models as well as those by the\nbaselines on the 3-Style and 21-Style datasets. The closer the model to the top-right corner, the\nmore superior the model is. From the ﬁgure, we found that among our models, Model D performs\nthe best. As expected, G-GPT-2 achieves the best ﬂuency score. However, since it does not support\nstyle control, it has a poor style score. On the other hand, S-GPT-2 achieves good ﬂuency and\n7\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nAdaLNAdaSA\nS2S\nSI\nClass\nLB\nUB\n30\n40\n50\n60\n70\n80\n90\n2.5 3.5 4.5 5.5 6.5 7.5 8.5\nStyle Score\nFluency Score\nAdaLN\nAdaSA\ns2s\nsi\nClass\nLB\nUB\nFSSSModel D7.270.9Model C4.670.4Model B7.354.0Model A3.980.3SC7.441.1G-GPT-27.533.5S-GPT-27.178.6\nModelAModelC ModelD\nModel B\nSCG-GPT-2\nS-GPT-2\n(a) Fluency vs. style on the 3-Style dataset.\nAdaLN\nAdaSA\nS2S\nSI\nClass\nLB\n0\n5\n10\n15\n20\n25\n30\n35\n40\n2 3 4 5 6 7 8\nStyle Score\nFluency Score\nAdaLN\nAdaSA\ns2s\nsi\nClass\nLB\nFSSSModel D7.127.9Model C5.719.6Model B7.312.8Model A3.234.3SC7.57.2G-GPT-27.54.8\nModelA\nModelC\nModelD\nModel BSCG-GPT-2 (b) Fluency vs. style on the 21-Style dataset.\nFigure 3: Fluency and style scores achieved by the competing models on the 3-Style and 21-Style\ndatasets.\nModel A Model B Model C Model D\n3-Style 11.53 11.61 10.58 11.13\n21-Style 10.17 10.09 11.32 10.52\nTable 2: Style diversity scores achieved by the computing methods. We note the lower bound and upper bound\nfor the style diversity scores are 4.52 and 15.67, respectively.\n(%) Model D SC Random\n3-Style 56 54 50\n21-Style 57 63 50\n(%) Model D SC Random\n3-Styleby reference 56 52 50\n3-Styleby category 65 54 50\n21-Styleby reference 66 49 50\n21-Styleby category 69 50 50\nTable 3: (Left): Human study results on ﬂuency. (Right): Human study results on style control. Random\ndenotes the accuracy for random guess. Model D performs favorably over the baseline SC.\nModel D Fluency Score Style Score Style Diversity Score Content Novelty Score\nLDIST 7.32 51.16 11.40 24.01\nLSTYLE 7.35 5.40 9.22 29.27\nLGAN 6.85 28.67 10.35 26.77\nAll 7.14 27.90 10.52 25.85\nTable 4: Ablation study on the various loss terms in the proposed objective function.\nstyle scores for the 3-Style dataset. This is understandable as it utilizes a GPT-2 model for each\nstyle. However, such an approach does not scale well as GPT-2 training is expensive. We also found\nthat SC does not achieve good style score and is inferior to our models. We suspect this is because\nthe one-hot style class code is largely ignored during inference. Since Model D performs the best\nin our framework, for the rest of the paper, we use it as our representative model for performance\ncomparison as well as ablation study.\nIn Tab. 1, we show several generation results from our Model D. We ﬁnd that the output texts are\nﬂuent and respect the styles of the references. More output examples are available in Appendix E.\nIn Tab. 2, we show the style diversity scores achieved by our models. We found that all of our\n4 models can generate diverse styled paragraphs conditioning on the same context and different\nreference examples with the same style.\nHuman evaluation. In Tab. 3, we report user study results on ﬂuency and style control. We found\nthat our model achieves great ﬂuency on both of the datasets. Compared to SC, our model performs\nbetter in controlling the style in the output texts.\nAblation study. We conduct an ablation study on the loss terms in the proposed objective function\nand report the results in Tab. 4 using the 21-Style dataset. The results show that each term is\nimportant. Removing LDIST leads to a degraded content novelty score. Removing LSTYLE leads\nto a degraded style score, thought an improved ﬂuency score and a content novelty score. Removing\nLGAN leads to both degraded ﬂuency and style diversity scores.\n8\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\n6 C ONCLUSION\nWe presented a language generative framework for style example-guided paragraph generation. To\nthe best of our knowledge, we were the ﬁrst to achieve such style-controllability on paragraph gener-\nation. We attributed the success to our carefully designed learning objective function, the generator\nnetwork, and the newly composed large-scale dataset consisting of documents of various text styles.\nREFERENCES\nPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-\ntions and generative models for 3d point clouds. arxiv preprint arXiv:1707.02392, 2017.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. In Advances in\nNeural Information Processing Systems (NIPS), 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In International Conference on Learning Representations (ICLR),\n2016.\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio Ranzato, and Arthur Szlam.\nReal or fake? learning to discriminate machine from human generated text. arxiv preprint\narXiv:1906.03351, 2019.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of Machine Learning Research (JMLR), 2003.\nPaweł Budzianowski and Ivan Vuli ´c. Hello, it’s gpt-2–how can i help you? towards the use of\npretrained language models for task-oriented dialogue systems. arxiv preprint arXiv:1907.05774,\n2019.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. Kermit: Generative\ninsertion-based modeling for sequences. arxiv preprint arXiv:1906.01604, 2019.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. In Annual Meeting of the Association for Computational Lin-\nguistics (ACL), 2014.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arxiv preprint arXiv:1412.3555, 2014.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. In\nAnnual Meeting of the Association for Computational Linguistics (ACL), 2019.\nDataworld. Hotel reviews dataset. In https://data.world/dataﬁniti/hotel-reviews, 2017.\nCyprien de Masson d’Autume, Mihaela Rosca, Jack Rae, and Shakir Mohamed. Training language\ngans from scratch. In Advances in Neural Information Processing Systems (NIPS), 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Annual Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics (NAACL), 2019.\nVincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic\nstyle. In International Conference on Learning Representations (ICLR), 2017.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. Style transfer in text: Ex-\nploration and evaluation. In Association for the Advancement of Artiﬁcial Intelligence (AAAI) ,\n2018.\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional\nneural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2016.\n9\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-\nmation Processing Systems (NIPS), 2014.\nAgrim Gupta, Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Characterizing and improving sta-\nbility in neural style transfer. In IEEE International Conference on Computer Vision (ICCV) ,\n2017.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arxiv\npreprint arXiv:1503.02531, 2015.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 1997.\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Lifelong learning via\nprogressive distillation and retrospection. In European Conference on Computer Vision (ECCV),\n2018.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled\ngeneration of text. In International Conference on Machine Learning (ICML), 2017.\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\nization. In IEEE International Conference on Computer Vision (ICCV), 2017.\nXun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-\nimage translation. European Conference on Computer Vision (ECCV), 2018.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution. In European Conference on Computer Vision (ECCV), 2016.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2019.\nStephan M. Kerpedjiev. Generation of informative texts with style. In International Conference on\nComputational Linguistics (COLING), 1992.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl:\nA conditional transformer language model for controllable generation. In Einstein.ai, 2019.\nTaeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover\ncross-domain relations with generative adversarial networks. In International Conference on Ma-\nchine Learning (ICML), 2017.\nYoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2016.\nR´emi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with\napplication to the biography domain. In Empirical Methods in Natural Language Processing\n(EMNLP), 2016.\nChenliang Li, Weiran Xu, Si Li, and Sheng Gao. Guiding generation for abstractive text summa-\nrization based on key information guide network. In Annual Conference of the North American\nChapter of the Association for Computational Linguistics (NAACL), 2018.\nDianqi Li Li, Yizhe Zhang, Zhe Gan Gan, Yu Cheng, Chris Brockett, Ming-Ting Sun, and Bill\nDolan. Domain adaptive text style transfer. In Empirical Methods in Natural Language Process-\ning (EMNLP), 2019.\nYijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style\ntransfer via feature transforms. In Advances in Neural Information Processing Systems (NIPS) ,\n2017.\nJason Liu. 515k hotel reviews data in europe. In https://www.kaggle.com/jiashenliu/515k-hotel-\nreviews-data-in-europe, 2017.\n10\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nMing-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.\nIn Advances in Neural Information Processing Systems (NIPS), 2017.\nMing-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz.\nFew-shot unsupervised image-to-image translation. In IEEE International Conference on Com-\nputer Vision (ICCV), 2019a.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long sequences. In International Conference on\nLearning Representations (ICLR), 2018.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neu-\nral networks via knowledge distillation for natural language understanding. arxiv preprint\narXiv:1904.09482, 2019b.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\nfor natural language understanding. In Annual Meeting of the Association for Computational\nLinguistics (ACL), 2019c.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arxiv preprint arXiv:1907.11692, 2019d.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Annual Meeting of the Association for\nComputational Linguistics (ACL), 2011.\nJulian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution\nof user expertise through online reviews. In International World Wide Web Conference (WWW),\n2013.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In International Conference on Learning Representations (ICLR), 2017.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in Neural Information\nProcessing Systems (NIPS), 2013.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, and Hassan Ghasemzadeh. Improved knowl-\nedge distillation via teacher assistant: Bridging the gap between student and teacher. arxiv\npreprint arXiv:1902.03393, 2019.\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with\nspatially adaptive normalization. In IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019.\nRamakanth Pasunuru, Han Guo, and Mohit Bansal. Towards improving abstractive summarization\nvia entailment generation. In Annual Meeting of the Association for Computational Linguistics\n(ACL) Workshop, 2017.\nJeffrey Pennington, Richard Socher, and Christoper Manning. Glove: Global vectors for word\nrepresentation. volume 14, pp. 1532–1543, 01 2014. doi: 10.3115/v1/D14-1162.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In Annual Meeting of the\nAssociation for Computational Linguistics (ACL), 2018.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. Style transfer\nthrough back-translation. In Annual Meeting of the Association for Computational Linguistics\n(ACL), 2018.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\nsentiment. In International Conference on Learning Representations (ICLR), 2018.\n11\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. In OpenAI Blog, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. In Empirical Methods in Natural Language Processing\n(EMNLP), 2016.\nSudha Rao and Joel Tetreault. Dear sir or madam, may i introduce the yafc corpus: Corpus, bench-\nmarks and metrics for formality style transfer. In Annual Conference of the North American\nChapter of the Association for Computational Linguistics (NAACL), 2018.\nSantiago. Wikipedia xml data. In Amazon, 2015.\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\ngenerator networks. In Annual Meeting of the Association for Computational Linguistics (ACL) ,\n2017.\nStanislau Semeniuta, Aliaksei Severyn, and Sylvain Gelly. On accurate evaluation of gans for lan-\nguage generation. arxiv preprint arXiv:1806.04936, 2018.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Annual Meeting of the Association for Computational Linguistics (ACL), 2015.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text\nby cross-alignment. In Advances in Neural Information Processing Systems (NIPS) , pp. 6833–\n6844, 2017.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible\nsequence generation via insertion operations. In International Conference on Machine Learning\n(ICML), 2019.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Systems (NIPS), 2014.\nDustin Tran, Keyon Vafa, Kumar Krishna Agrawal, Laurent Dinh, and Ben Poole. Discrete ﬂows:\nInvertible generative models of discrete data. In International Conference on Learning Represen-\ntations (ICLR) Workshop, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems (NIPS), 2017.\nOriol Vinyals and Quoc Le. A neural conversational model.arxiv preprint arXiv:1506.05869, 2015.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations (ICLR), 2019.\nKe Wang and Xiaojun Wan. Sentigan: Generating sentimental texts via mixture adversarial net-\nworks. In International Joint Conferences on Artiﬁcial Intelligence (IJCAI), 2018.\nPeng Xu, Yanshuai Cao, and Jackie Chi Kit Cheung. Unsupervised controllable text generation with\nglobal variation discovery and disentanglement. arxiv preprint arXiv:1905.11975, 2019.\nWei Xu. From shakespeare to twitter: What are language styles all about? In Empirical Methods in\nNatural Language Processing (EMNLP) Workshop, 2017.\nWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. Paraphrasing for style. Inter-\nnational Conference on Computational Linguistics (COLING), 2012.\n12\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arxiv preprint\narXiv:1906.08237, 2019.\nYelp. Yelp dataset challenge. In https://www.yelp.com/dataset/challenge, 2019.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2017.\nHongyu Zang and Xiaojun Wan. Towards automatic generation of product reviews from aspect-\nsentiment scores. In International Conference on Natural Language Generation (INLG), 2017.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. In Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 2018.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. In Advances in Neural Information Processing\nSystems (NIPS), 2019.\nHaoyu Zhang, Yeyun Gong, Yu Yan, Nan Duan, Jianjun Xu, Ji Wang, Ming Gong, and Ming\nZhou. Pretraining-based natural language generation for text summarization. arxiv preprint\narXiv:1902.09243, 2019.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2018.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In IEEE International Conference on Computer\nVision (ICCV), 2017.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In IEEE International Conference on Computer Vision (ICCV), 2015.\n13\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nA H YPER -PARAMETERS TUNNING\nWe tune the hyper-parameters on a pilot-test dataset. This pilot-test dataset has 3K training exam-\nples and 400 hold-out examples. We perform a grid search on log-scale. We utilize the Frechet Em-\nbedding Distance (Semeniuta et al. (2018)) to select best hyper-parameters. For λDIST , λSTYLE ,\nand λGAN, we try {1.0,0.1,0.1}, {0.1,0.1,0.1}, and {1.0,0.01,0.01}. For betas used in Adam\nOptimizer, we try {0.9,0.999}and {0.0,0.9}. For weight decay, we try 0.01 and 0.0. For the initial\nlearning rate, we try 0.00025 and 0.000025. Eventually, we use 0.00025 as initial learning rate and\n{1.0,0.01,0.01}for λDIST , λSTYLE , as well as λGAN for all models except for the Model B. We\nuse {1.0,0.01,0.1}for the Model B instead.\nB P RETRAINING OF H AND C\nPretraining of the GPT-2 modelH. We pretrain H on our collected dataset D from scratch. We\nuse Adam optimizer with 0.00015 initial learning rate, β1 and β2 are set by {0.9,0.999}, cosine\nlearning rate decay style, and 0.01 weight decay. The batch size is set to 512. The total training\niterations is 320Kwhich takes 2 weeks.\nPretraining of the style comparatorC. We pretrain the Style Comparator C using 256 and 512\nbatch size, respectively. The initial learning rate is 0.00015 with 0.01 weight decay and cosine\nlearning rate decay style. The optimizer is also Adam with β1 = 0.9 and β2 = 0.999. Since the\ntraining converges very quickly, we do early stopping if the accuracy on validation set plateaus.\nEventually, we use the checkpoint at 28K and 50K iterations to train on the 3-style and 21-style\ndatasets, respectively. The accuracy on hold-out validation set are98.8% and 87.8% on 3-styles and\n21-styles datasets, respectively.\nC A UTO -EVALUATION METRICS\nFluency score. To ensure the generated paragraph is ﬂuent and coherent, we utilize a pretrained\nGPT-2 model to measure the perplexity of the generated paragraph. We compute the generation\nlikelihood over each token using the model and treat the generated paragraph y0:T−1 as inputs and\ny1:T as labels. Given the input text, the pretrained GPT-2 returns the probability distribution of\nnext token over the vocabulary. Then, we measure the perplexity by this probability distribution and\nlabel. Since our dictionary size is 50257, the random guess of the next token would result in 50257\nperplexity (LLM = log(50257) ≈10.83). Thus, we set 10.83 as an upper bound and deﬁne the\nﬂuency score of the generated paragraph as 10.83 −log(perplexity). In this sense, a higher ﬂuency\nscore means lower perplexity.\nStyle score. We train 3/21 binary style classiﬁers (since we have 3/21 different styles in the\n3-Style/21-Style dataset) by ﬁnetuning a GPT-2 network to automatically evaluate whether\nthe generated text carries the style of a target class. These 3/21 classiﬁers achieve average classiﬁ-\ncation accuracies of 99.1%/96.3%. During the testing phase, for a target style, if the corresponding\nstyle classiﬁer correctly predicts 1 for the generated paragraph computed by a model, we count it as\na successful trial. We compute the success rate over the test set and use the result as the style score\nfor the model.\nStyle diversity score. We adopt the LPIPS distance (Zhang et al. (2018)) to measure the diversity\nof the generation outputs conditioning on the same context. To implement this metric, we ﬁrst\nextract the feature representation from each token in a generated paragraph by a pretrained GPT-2\nmodel. We compute the mean representation of the tokens in a paragraph as the paragraph-level\nrepresentation. Then, we measure the L2 distance between two paragraph-level representations of\ntwo different paragraphs generated using the same context but two different references written in the\nsame style. In this sense, a larger distance value implies the styles of the two generated paragraphs\nare more different.\nTo get an idea of the range of this metric, we compute an upper bound and a lower bound. We\nconsider two paragraphs from two documents of different styles should have a high style diversity\nscore. We hence sample 1000 paragraphs from each style and use the pretrained GPT-2 model\nto extract deep features. After taking average over the token-dimension, we obtain 1000 by 768\n14\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nrepresentation for each style. Then, we compute the L2 distance between of these matrices divided\nby 1000. This gives us a matrix of size 21 ×21 measuring the pairwise distance between two styles.\nWe use the largest value in this matrix as our upper bound, which is 15.67.\nFor the lower bound, since two different paragraphs from the same document should have a low\nstyle diversity score, we use their scores for the lower bound computation. Speciﬁcally, we compute\nthe average distance between two different paragraphs from the same document. We do this for each\nstyle and obtain 21 different values. We obtain the lower bound by taking average over these values,\nwhich is 4.52.\nContent novelty score. To verify that our model is not simply duplicating the content from reference\nparagraph, we utilize LPIPS distance (Zhang et al. (2018)) to measure the difference between the\ngenerated paragraph and the input reference paragraph. We again use a pretrained GPT-2 model for\nextracting a feature representation for each token. To compute the distance between two paragraphs,\nwe compute the bipartite matching cost between the tokens from the two paragraphs. Speciﬁcally,\nwe ﬁrst compute the L2 distances between any token representation in one paragraph to all the token\nrepresentations in the other paragraph. We then compute the minimum cost assignment by solving a\nbipartite matching problem. In order to get an idea about the range of the content novelty score, we\ncompute an upper bound and a lower bound using a similar approach as the one used for the style\ndiversity score. We ﬁnd the upper bound value is 31.46 and the lower bound value is 15.13.\nD H UMAN EVALUATION\nTo participate in our user study, a worker has to be awarded the Master Qualiﬁcation by AMT and\nhas at least 97% life-long HIT approval rate. We generate2 paragraphs for a context with2 different\nreference paragraphs from the same style. For 3-style dataset, we randomly sample 50 examples\nfrom each style and do style generation for all 3 styles. Thus, there are 900 examples for each\ntesting model. For 21-style dataset, we randomly sample 42 examples from testing set and do style\ngeneration on all 21 styles. Thus, there are 1764 examples for each testing model. There is a typical\nexample for experiment of ﬂuency human study in Fig. 4.\nFigure 4: A typical example of the questionnaire for our A/B test on AMT for the ﬂuency study.\nE A DDITIONAL RESULTS\nIn Tab. 5, 6, 7, 8, and 9, we present additional example outputs of the proposed framework. The\nresults demonstrate that the proposed model is fully capable of generating ﬂuent output texts re-\nspecting the style of the input reference examples.\n15\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Poetry StyleIf anything, I’m a petty thief in a world of forgetting, a blade in an elm, a bottled notein the sea of Victory Boulevard at dusk. Friends, family, gallows saints & ghosts, rig aswing at the edge of the shore when I’m gone for the birds to perch & the bats to glidethrough, to disappear & appear in the small places the moon cannot touch. Thank youfor the amorous & discontented beauty you saved for me here, though most days, mysilence was a soldier’s slogged hand hiding a match. I love you. I leave you the match.\nReference – Lyrics Style (Senorita)I love it when you call me senoritaI wish I could pretend I didn’t need yaBut every touch is ooh la la laIt’s true, la la laOoh, I should be runningOoh, you keep me coming for youLand in MiamiThe air was hot from summer rainSweat dripping off meBefore I even knew her name, la la laIt felt like ooh la la la\nGeneratedI turn your body in my lapyour purse, a Pen, of a beautiful peopleand I love you the way you love me, look at you drop a hand\nYou wake up your ad libs Cirospis, cetachi, cetachi, cetachi, cetachi, cetachi, massageas, cetachi, cetachi, massageas, London CirospisI love you inside, I love you at the foot of your bed,I love you inside, I love you at the foot of your bed,\nLeave your big collar, you leave your small collar, you leave your little collar\nYour life is just a half hour, it’s your dance coursesyou take a high vase of [real] cichejust like our ﬂowers\nWe all want it so much, we want it so much, everybodyThen kiss thing, girl go kiss it\nI set the girl on her armsagainst the state where no one could give it–love is reserved for us, romanceis reserved for us, romanceis reserved for us, loveis reserved for us, loveis reserved for us,\nAnd it began at the state where no one could give it–love is reserved for us, romanceis reserved for us\n–you are not just dresses but you are wearing them all a million times, you are your own beautiful girls’ effectsThey blend in faster than the othersYou meet and love all things, you . . .\nI am a proud Greek, an ancient Greek virginAnd I, like you, in high places, love all things, sexyBut on your early morning sleep, samba but in high places, sexyVesey no Vesey feather savant\nContext – News StyleCARL AZUZ, CNN 10 ANCHOR: Hi, I‘m Carl Azuz and you‘re watching CNN 10.Ten minutes of news explained. There are a lot of fast moving political headlines fromthe United States right now. On Wednesday the White House is expected to releasethe transcript of a phone call. A conversation between the presidents of the UnitedStates and Ukraine. The circumstances surrounding that call and something allegedlydiscussed on it were the reasons why on Tuesday Democrats from the U.S. House ofRepresentatives announced they were moving forward on an impeachment inquiry, aninvestigation\nReference – Lyrics Style (Baby Shark)Baby Shark doo doo, doo doo doo dooBaby Shark doo doo, doo doo doo dooBaby Shark doo doo, doo doo doo dooBaby SharkMummy Shark doo doo, doo doo doo dooMummy Shark doo doo, doo doo doo dooMummy Shark doo doo, doo doo doo dooMummy Shark\nGeneratedof a real, if not hoaxed, race in the United States.All right, hey can I have a snack, Bob Bush?A little snack baby!Measures how to speed up the Country children?Programs be grateful, Goodwill be giving.Messages like, what is this thing about 8 am in the Middle?Benjamin Franklin? GoNintendo Wii, MarioNintendo DS, UMaria Barbara, Nice GuyJohnny Cash, Mickey Mouse 2, Magic Tony 3, Superman, Backzapper, Yellowjack, Bambi, PigHis sister Diva, Saaath, Lil SaaathA sad, sad, sadIf we only do than, what are he doing with it ?Let’s move forward and make it better, we can ofﬁciallymake it good good, especially now Delma 0, Cadillac .MAHA YEAH!!! :DDDLa Dara No Art, Chicken and Beach(JCreates La Dara No Art, Chicken and Beach)\nTable 5: Qualitative Results: to lyrics style.\n16\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Lyrics Style (USA National Song)Oh, say can you see,By the dawn’s early light,What so proudly we hailed,At the twilight’s last gleaming?Whose broad stripes and bright\nReference – Hotel Reviews StyleWe loved our stay at Koloa Landing Resort. Wish we could afford to stay longer thana weekend. Rooms are gorgeous with lots of amenities - kitchen, free WiFi, washer,dryer, A/C and more. Great pool with water slides. Holoholo Grill has great tastingburgers and Kalua Nachos.\nGeneratedSatellites shared our love?South Side Estate.This is a great place to sample attorneys, chefs, therapists, etc. Dr Kirk Abad’s is amazing!\nMy family and friends stay at Sunset at Crinke Plaza for pretty cheap food and/or great view. We took our daughter there because I hate visiting Vegas.\nOverall, even this rating is disappointing. ”Doubt” is not a justiﬁable call for self-indulgence. It’s a Deveuulment of humans. It’s an attempt to give it more than its due.But, at least, today, Hollywood isn’t offering it the world. That’s what movie fans do with their cliches.\nUnless this story imagines a level of desperation that negative things just cannot get.\nContext – News StyleSeptember 18, 2019Today, CNN 10 is explaining a recent rise and fall in oil prices and how they’re tiedto an attack on Saudi Arabian oil facilities. Afterward, a pair of scientiﬁc storieshighlight studies on sharks and rats. And we’re jumping into a 10 out of 10 segmentfeaturing the world’s largest bounce house.\nReference – Movie Reviews StyleTrailer was better than the movie. To me this movie just never takes off. It’s a shamebecause there is some fantastic imagery and a wonderful theme lying underneath butin the end it can not overcome its plot holes and failed delivery of the message itstrying to speak. It truly believes it’s deeper than it is, but left me unfulﬁlled. Kind oflazy/rushed writing in my opinion. Kind of just throws a bunch of mediocre stuff atyou without any real substance and expects the audience to give it purpose and deepermeaning in there own heads. Which is ﬁne to do but to me this movie wasn’t worthy ofthat merit of thought. You keep waiting for it to evolve but it just never does. Honestlycan’t believe critics are putting this on such a pedestal giving it such rave reviews.\nGeneratedBecause CNN 10 just isn’t accurate and does no justice. Instead, it conveys the ﬁlm far too much to it’s due and serves to give it more credibility. It’s light sci-ﬁ instead ofenough fuel for actors helping to bring this premium piece down to its current disgustingnessespecially not because that story is so good and necessary but because we livein a world where people like that can succeed.\nOverall, even this rating is disappointing. ”Doubt” is not a justiﬁable call for self-indulgence. It’s a Deveuulment of humans. It’s an attempt to give it more than its due.But, at least, today, Hollywood isn’t offering it the world. That’s what movie fans do with their cliches.\nUnless this story imagines a level of desperation that negative things just cannot get.\nContext – Paper Style (Abstract of this paper)We introduce a language generative model framework for generating a styled para-graph based on a context sentence and a style reference example. The frameworkconsists of a style encoder and a texts decoder. The style encoder extracts a style codefrom the reference example, and the text decoder generates texts based on\nReference – Paper Reviews StyleThis paper proposes a new unsupervised learning approach based on maximizing themutual information between the input and the representation. The results are strongacross several image datasets. Essentially all of the reviewer’s concerns were directlyaddressed in revisions of the paper, including additional experiments. The only weak-ness is that only image datasets were experimented with; however, the image-basedexperiments and comparisons are extensive. The reviewers and I all agree that thepaper should be accepted, and I think it should be considered for an oral presentation.\nGenerateda z and, if the pupils offer enough color to illuminate an image, the ones not enough are marked to be retouched.\nThe grating is an issue with the words where thoughts can be expressed thought. However, both concepts are related, as well as relationships in general.\nFor example, hate being done by children means that all books on life end matter:\npositive-positive-negative expectations, creating a model in which the writers at the top of Google and the bottom will be able to deliver claims regarding better qualitythan everyone else. There are theories in the research literature explicitly regarding these effects, but the mechanisms ﬂating in elementary tasks that if they are crudelyunderstood are generally signiﬁcant (albeit less elaborated yet). A further test is to also decide:\nWhat is presented in the order\nContext – Wiki Style (Alan Turing Bio)During the Second World War, Turing worked for the Government Code and CypherSchool (GC&CS) at Bletchley Park, Britain’s codebreaking centre that produced Ultraintelligence. For a time he led Hut 8, the section that was responsible for German navalcryptanalysis. Here, he devised a number of techniques for speeding the breaking ofGerman ciphers, including improvements to the pre-war Polish bombe method, anelectromech\nReference – Yelp Reviews StyleI heard about SomiSomi from my Socal and Norcal friends before I even knew whatit was! Taiyaki, soft serve, dessert goodness galore.\nWe happened to get a taste of SomiSomi during their ﬁrst day of soft opening insidethe Westﬁeld Valley Fair mall. They’re located by the Men’s Macy’s, but also inPRIME location by Ramen Nagi and the new Pokeatery that will be opening soonapparently. There weren’t a lot of seating options once we got our orders, which wasa bummer. There were a few couches and more smaller seats, but when it gets busylike that, seating is important. Onto the food:\nWe were given a free custard taiyaki with an ah-boong order, which is the soft servein a cup and topped with a taiyaki of your choice. IT W AS AMAZING. Such a greatdeal. Pro Tip: Each somisomi has different mixed ﬂavors. So I chose the milk teamixed with ube and the taiyaki with taro. THE TARO FLA VOR IS THE BOMB. Idon’t think I could try anything else inside the taiyaki. Also, the taiyaki is made toperfection and are puffy delicious eggy-ness. It reminds me of Taiwan’s egg puffs.The custard taiyaki was very tasty–not too sweet and the perfect consistency! Theyallow any and all toppings, and I really loved their coconut ﬂakes. They also alloweither a topping of a macaron or strawberry slice. I can’t wait to come back and getsome more SomiSomi.\nGeneratedpropellant that helps vaporise the CJU Terminator atomic bombs, and other advancements so that the so-called uiorat! working meter could be controlled if its representedon a metal meter not such a big deal if you’reme like me, or as someone who tends to enjoy standing in front of the clock.There were also other inspirations from his Bletchley Park, namely Nieman K. Amer says: ”The best bits of Toni Sun’s Kaki at Burien are Mavi Huli’s. Sanjan and CaputoKim (expectedly my favorite) sold out this ONE time and even there was part of our meeting there that made me queasy.”Apple: The raison d’etre resting in a gotcha which was all tin from Customer Service.Pocket: A new candy bar, brie strips (lots to choose from)The secret, new dessert.I love Crunchycris and all of the other dungbibats that will be available. It will melt on your lips while reading a book like The Fast and The Furious or any OMGcrayon/fancy-rock. I want Tartan with a tangy sesame, Ma Tom Hagegai Fresh and Maaaadure - Bietan-Rose. But no miso ice cream cake or puns.Be aware: get your shop prepared. Deﬁnitely not an artisanal place.\nTable 6: Qualitative Results: to reviews style.\n17\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Children Books Style (Snow White and the Seven Dwarfs)Once upon a time . . . in a great castle, a Prince’s daughter grew up happy andcontented, in spite of a jealous stepmother. She was very pretty, with blue eyes andlong black hair. Her skin was delicate and fair, and so she was called Snow White.Everyone was quite sure she would become very beautiful. Though her stepmotherwas a wicked woman, she too was very beautiful, and the magic mirror told her\nReference – Poetry StyleA black vulture crushes my bones.A white vulture in a photograph.Houses open their mouths, swallow ﬂowers. Lightning doesn’t choose. I brushed thespruces by hand. A car has a tire, a farmhand a whip, trash cans are falling. We havetwo natures. The vulture circles the belfry at Ptujska Gora. Gingerbread hearts, redblood, something awaits! Dawn bangs into the bricks of Brooklyn. I’m still asleep insilvery milk. When the wind strokes the grass,I’ll be in front of the house. Crickets will burn the air.The sound will be like in the hall where there’s plush lining in Christ’s tomb.\nGeneratedwait for my goddaughter to faint! For my stepmother is the thought of greenery, gardens and swyrels!\nStill having a conversation scraped off the edge of my own couch,\nI hear the Ming river cry over the hearth,\nA tide will strain,\nand so will ﬂowers.\nUp me through her hair,\nMore ﬂowers,\nA thistle bleeds,\na urine of vibrant sugar.\nA red half of my fangs open and reach my face,\nSeveral holes in my chest.\nAnd a piano\nContext – Instagram StyleAn amazing meal had at @thestratford, testing their newly opened restaurant. But-termilk deep-fried chicken with a doughnut and salad. An absolute must. Thankyou @thestratford #thestratford #london #londonrestaurant #londonrestaurants #cui-sine #food #foodporn #foodphotography #foody #foodgasm #foodstagram #foodlover\nReference – Poetry StyleA black vulture crushes my bones.A white vulture in a photograph.Houses open their mouths, swallow ﬂowers. Lightning doesn’t choose. I brushed thespruces by hand. A car has a tire, a farmhand a whip, trash cans are falling. We havetwo natures. The vulture circles the belfry at Ptujska Gora. Gingerbread hearts, redblood, something awaits! Dawn bangs into the bricks of Brooklyn. I’m still asleep insilvery milk. When the wind strokes the grass,I’ll be in front of the house. Crickets will burn the air.The sound will be like in the hall where there’s plush lining in Christ’s tomb.\nGeneratedshows in the building news feeds, adorned with neon signs and mason jars. Just how much of the thrikers are griting compassion, you know thoughts, for weeks burning inthe hemispheres. I call my guests ’the french!’s, the eggs and cream, the eggs and chicken\nThe Kinnear growling embers of bees tingle inside each woman. ”Beat a cusha a crow.”\nShe pauses, looking at Anna, doubt sitting heavy on her face. ”You are warm as a ﬂy. For you, those who want to live you will be happy,\nContext – Sport News StyleStating the obvious apparently costs $50,000 in the NBA.\nAccording to Shams Charania of Stadium and The Athletic, the league ﬁned the Mil-waukee Bucks after general manager Jon Horst told fans at a town hall event the Bucksplan to offer Giannis Antetokounmpo a supermax extension when they’re eligible todo so.\nHorst referenced how league rules prevent Milwaukee from formally discussingspeciﬁcs regarding what it would\nReference – Poetry StyleIt is written somewhere in the skywe are supposed to forget quickly. How? An ele-phant stands on a tiny pedestal. In a ﬁeld under a tent. The ringmaster spectacularin his tails and topcoat, expert in redirection, points towards the acrobat. The acrobatastounds because he could fall.\nThe other day I told my friend the story in which you fell through the ﬂoorboards of anold barn. As I told the story, I recalled the manner in which you carefully transferredmilk to a newborn calf. You beckoned to Babette singing come Bossy Boss comeBoss.\nYears since you carved onto your forearm that pig with wings but have you ever no-ticed that the word pigeon encompasses the word pig? Once, in the evening on Fifthor Third, I saw a pigeon drop into a vat of mustard. I watched its dark wings thrash.Then yield.\nGeneratedearrudgeed its way in May for a warrant to call.\nAfter the conditions were waived, Indiana Pacers head coach Luke Walton and Bucks forward Patrick Ewing came into the meeting to see what those rules meant.\nSo what were the rules?\nAt ﬁrst, we thought between letting training and verbal communication come into play. Now, with the way you communicate, you just have to make some other rationalrequest.\nThere’s a tall castle rising in the sky, and a great sweet tale conjured up right. I cover a chandelier in a gift box; over the ledge you can spot a diamond with feathers on it.Learn to try again.\nWe can’t afford to\nTable 7: Qualitative Results: to poetry style.\n18\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Random sentenceI have a cute dog Reference – Children Books Style (Little Red Cap)”Good day to you, Little Red Cap.”\n”Thank you, wolf.”\n”Where are you going so early, Little Red Cap?”\n”To grandmother’s.”\n”And what are you carrying under your apron?”\n”Grandmother is sick and weak, and I am taking her some cake and wine. We bakedyesterday, and they should be good for her and give her strength.”\n”Little Red Cap, just where does your grandmother live?”\n”Her house is good quarter hour from here in the woods, under the three large oaktrees. There’s a hedge of hazel bushes there. You must know the place,” said LittleRed Cap.\nGenerated”Well, I found a small white dog worth the trouble.””What species are my family?””I’m sure of something. She is a farm.””It’s been a while since your last Christmas story,” said George.”I give up and hang up my Christmas album.””Good, how many stores on earth now is the size of Big Red?”He didn’t feel right.”I suppose ten,” he said\nContext – Yelp StyleSomisomi is easily one of my go-to spots for dessert because there’s no lactose in theice cream, it’s very instagrammable, and they have rotating ﬂavors.\nMy friend and I decided to visit this location a few days after it opened and surpris-ingly, there was a really short line. This location had 6 ﬂavors (matcha, milk, oreo,ube, coffee, and milk tea). What\nReference – Children Books Style (Snow White and the Seven Dwarfs)Once upon a time . . . in a great castle, a Prince’s daughter grew up happy andcontented, in spite of a jealous stepmother. She was very pretty, with blue eyes andlong black hair. Her skin was delicate and fair, and so she was called Snow White.Everyone was quite sure she would become very beautiful. Though her stepmotherwas a wicked woman, she too was very beautiful, and the magic mirror told her thisevery day, whenever she asked it.”Mirror, mirror on the wall, who is the loveliest lady in the land?” The reply wasalways; ”You are, your Majesty,” until the dreadful day when she heard it say, ”SnowWhite is the loveliest in the land.” The stepmother was furious and, wild with jealousy,began plotting to get rid of her rival. Calling one of her trusty servants, she bribed himwith a rich reward to take Snow White intc the forest, far away from the Castle. Then,unseen, he was to put her to death. The greedy servant, attracted to the reward, agreedto do this deed, and he led the innocent little girl away. However, when they came tothe fatal spot, the man’s courage failed him and, leaving Snow White sitting beside atree, he mumbled an excuse and ran off. Snow White was all alone in the forest.Night came, but the servant did not return. Snow White, alone in the dark forest,began to cry bitterly. She thought she could feel terrible eyes spying on her, and sheheard strange sounds and rustlings that made her heart thump. At last, overcome bytiredness, she fell asleep curled under a tree.Snow White slept ﬁtfully, wakening from time to time with a start and staring into thedarkness round her. Several times, she thought she felt something, or somebody touchher as she slept.At last, dawn woke the forest to the song of the birds, and Snow White too, awoke. Awhole world was stirring to life and the little girl was glad to see how silly her fearshad been. However, the thick trees were like a wall round her, and as she tried to ﬁndout where she was, she came upon a path. She walked along it, hopefully. On shewalked till she came to a clearing. There stood a strange cottage, with a tiny door, tinywindows and a tiny chimney pot. Everything about the cottage was much tinier thanit ought to be. Snow White pushed the door open.\nGeneratedDesigns concerned: Some hail the hero,\nVerdicts seem to be the wrath of the storm,\nThe main reason sweets can’t provide a lot of herace;\nOne side calls her errible, the other side tells her,\n”Watch out, this is a great purity factory.”\n”Oh, she we are degraded,” a low voice whispered into her, ”the delight heights thus are deeds rather than love.”\n”For only things that are great, and that happiness cannot wast; For she appears so what is best.\nIn this changing season, Regan might win a daily battle, In the surprise that revealing her majesty brings.\nAt the end of the day, surprised and amused and bewildered,” says Regan,\n”What did I wait for,” demands a woman, ”burst upon your beautiful mother?”\nThe crease goes on and on as they argue over the choices best,\nEven though the simpler justice began to prevail, the evil abyss began to mend.\n”Such is her heart she didn’t even know that,\nThe mercy of her master she feared would take her life,” she said, ”a woman who looked only for herself.\nShe pretended to think the choice that befell him was her love,But true, she turned to the ﬁgure in her bed.”\n”If you like the sweetness of this sweet song,” Violet sighed, ”you will love them either.”\n”This sweet song,” says Regan with astonishing strength, ”a marriage, a husband who loves her.\nThis sweet song, on the one side of which is the inner ﬁgure in the solution to her woes; the sensual dictates; by which she might follow the recipe of the book of gratefuldreams,\nThe sweetest song I ever had ever heard, to this new ﬂourishes. Perfect!” she said to Violet, ”not just found but realised it had mean\nTable 8: Qualitative Results: to children books style.\n19\nStyle Example-Guided Text Generation using Generative Adversarial Transformers\nContext – Movie Reviews StyleI am struggling to ﬁnd a good reason to give you in order to see this movie, but Icannot. I will not stick to how the actors were playing because it was not their faultabout the role they had been given. On the other hand I will base my review on thescenario. What was the meaning of the movie? Which was the target of the plot? Idid not manage to answer neither of the aforementioned queries. I read that\nReference – Politic News Style(CNN)President Donald Trump is wasting no time in attempting to torch HouseSpeaker Nancy Pelosi’s impeachment gamble in a battle that will deﬁne his presi-dency and the 2020 election.\nTrump has pledged to publish an un-redacted and declassiﬁed transcript on Wednesdayof a phone call with Ukraine’s leader at the center of what Democrats allege is hisabuse of presidential power. The White House is also planning to release to Congressa whistleblower’s complaint that triggered the week-long crisis that has rocked theTrump presidency.Trump’s decision marks a departure for a White House that has a record of obstruct-ing oversight and bending fact. So his critics will await events on Wednesday withparticular interest.The transcript and the congressional reaction to the whistleblower’s report could becritical in establishing the early terrain of the impeachment ﬁght and to shaping publicopinion that will ultimately dictate how it turns out.Regardless of the outcome, Trump ﬁnds himself at the center of a rare and historicshowdown as only the fourth president in US history to face the realistic threat ofimpeachment.\nGenerateda short about a World War II president contemplating a secret meeting with Hitler won’t be released, but the person being asked that it is, in fact, a statement he made inMunich on May 6, 1938 that would lead to the end of World War II.I also assume that modern life in the present moment right now is much more uncomfortable, and it’s difﬁcult for my biographer to provide empathy because what narrativesI have are so blended with existing views that the idea of Trump president is only in the realm of his childish vote to reject the Supreme Court that motivated him to dowhat, instead of taking a stance that is less adversarial, may be demagogic.Trump by comparison isn’t such a setting to gain the desired political stature that Monty Python’s sketch computer shows him. Nor, it’s just a new territory he’s taking tobe killed. Contentious and creative questions are looming. The real question is whether all this will ultimately lead to the abolition of the Supreme Court.\nContext – Paper Style (Abstract of this paper)We introduce a language generative model framework for generating a styled para-graph based on a context sentence and a style reference example. The frameworkconsists of a style encoder and a texts decoder. The style encoder extracts a style codefrom the reference example, and the text decoder generates texts based on the stylecode and the context. We propose a novel objective function to train\nReference – News Style(CNN)Massachusetts Gov. Charlie Baker has called for a temporary statewide ban onthe sale of all e-cigarettes and vaping products in response to a nationwide outbreakof lung injuries associated with vaping.\n”I’m ofﬁcially declaring a public health emergency in the Commonwealth due to se-vere lung disease associated with the use of e-cigarettes and marijuana-infused vapingproducts,” Baker said during a press conference on Tuesday.”I’m requesting that the public health council order a four-month temporary ban onthe sale of all vaping products in retail establishments, online and through any othermeans, effective immediately,” he said. ”We as a Commonwealth need to pause salesin order for our medical experts to collect more information about what is drivingthese life-threatening vaping-related illnesses.”\nGeneratedeveryone through a useful tool or method; each custom bound to require instruction from an expert inside the business community. This type of exercise should be done ina timely manner and without fanfare and cooing from the sports, entertainment and educational communities, and in a highly professional scientist setting, where everyonecould help us understand things we need to improvewe want people to be able to start thele by entering into an active conversation with scientists – which is a ”great idea,”if, by deﬁnition, is humans for.”The medical community has an primary role in the global health and well-being of the globe beyond infections, obesity, heart disease, diabetes, and opioid addiction,”Bolton added. ”This includes controlling just-induced drugs. But as a potent encapsulation of these drugs, we cannot defend their efﬁcacy for all measures, including food.Simply creating something that doesn’t currently prohibit prescription drugs, and packaging them for adult use, is wholly irresponsible on both sides of the political aisle,our ability to regulate the reﬂux to the world’s thirsty, and the health of the Australian people.”\nContext – Children Books Style (Little Three Pigs)Once upon a time there was an old mother pig who had three little pigs and not enoughfood to feed them. So when they were old enough, she sent them out into the world toseek their fortunes.\nThe ﬁrst little pig was very lazy. He didn’t want to work at all and he built his houseout of straw. The second little pig worked a little bit harder but he was somewhatlazy too and he built his house out of sticks. Then, they sang and danced and playedtogether the rest of the day.\nThe third little pig worked hard all day and built\nReference – News Style(CNN)For Quentin Tarantino, time travel is entirely possible. And it didn’t take greenscreens or computer generated backdrops to make it back to 1969-era Los Angeles.All it required were fond, vivid memories of the time, an exhausting attention to perioddetail and a production designer like Barbara Ling.\nThe ﬁlmmaker was determined that the environments featured in his ninth ﬁlm ”OnceUpon a Time ... in Hollywood” as tactile and immersive as humanly possible withoutturning to the now-ubiquitous digital effects houses, says Ling, who lavishly and con-vincingly resurrects a period some 50 years gone: the then-vibrant storefronts of Hol-lywood Boulevard, the classic movie theaters of Westwood Village, enduring eaterieslike Musso & Frank Grill and El Coyote, the Playboy Mansion, and the Charles Man-son family inhabited ranch where Westerns were shot.\n”Quentin is so not into digital or CGI,” Ling, whose ﬁlm credits include ”The Doors,””Falling Down” and ”Batman Forever,” told CNN. ”He’s not someone who wants todo performance in front of green screen. He wants to see the real deal. To me, that is,of course, the absolute best way. It’s the hardest way, but you actually get a real 3-Dworld that’s not hazy, so to speak, or that you have to keep people at a certain distance.You’re not dealing with green screens. You’re dealing with real streets and real peopleand real everything...He loves the real world, so you can actually see it and smell itand touch it.”\nGenerateda new house,away from that stage, away from the theater,away from the movie.\nBut the fourth little pig worked hard it had to be him.\nSteven Spielberg is one of us white people, whom we think of as ”the lovable Dear Dad”.\nIn the end, Spielberg created the ”Ralph” sequence with Donkey Kong.\nEveryone who adopts a culture of old blood, that’s Abraham Lincoln for those of you white people.\nI once invited a school animal to a movie so that I could ask him, ’Maybe Jaws + Shaun of the Dead’, forever. She said, ’Sure.’ ’I’ll be your ﬁlm, but I don’t want to be oneof you gum and crackers any more.’ She’s this confused bunny.”\n(Click here to see a video line-up of Teri Garr’s past ”Ralph” scenes and upcoming ”Dreaming Tom Shadow.”)\nTable 9: Qualitative Results: to News style.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7899789214134216
    },
    {
      "name": "Transformer",
      "score": 0.7627781629562378
    },
    {
      "name": "Generative grammar",
      "score": 0.7604385614395142
    },
    {
      "name": "Paragraph",
      "score": 0.7230057120323181
    },
    {
      "name": "Sentence",
      "score": 0.6464244723320007
    },
    {
      "name": "Style (visual arts)",
      "score": 0.6193503141403198
    },
    {
      "name": "Code (set theory)",
      "score": 0.6060964465141296
    },
    {
      "name": "Adversarial system",
      "score": 0.6009762287139893
    },
    {
      "name": "Encoder",
      "score": 0.5689722895622253
    },
    {
      "name": "Natural language processing",
      "score": 0.5210666060447693
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4836038649082184
    },
    {
      "name": "Text generation",
      "score": 0.4806308150291443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4605153799057007
    },
    {
      "name": "Language model",
      "score": 0.4407983422279358
    },
    {
      "name": "Programming language",
      "score": 0.21367907524108887
    },
    {
      "name": "World Wide Web",
      "score": 0.1204022765159607
    },
    {
      "name": "Engineering",
      "score": 0.07910984754562378
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}