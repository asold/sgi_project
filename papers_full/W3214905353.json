{
    "title": "U-shape Transformer for Underwater Image Enhancement",
    "url": "https://openalex.org/W3214905353",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2348999412",
            "name": "Peng Lin-tao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226999381",
            "name": "Zhu Chunli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2742312692",
            "name": "Bian, Liheng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2784079831",
        "https://openalex.org/W2892845027",
        "https://openalex.org/W2069734445",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2799265886",
        "https://openalex.org/W2758376227",
        "https://openalex.org/W2293581118",
        "https://openalex.org/W1986431218",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W2947815032",
        "https://openalex.org/W2950055287",
        "https://openalex.org/W2091420866",
        "https://openalex.org/W3163896636",
        "https://openalex.org/W2778341433",
        "https://openalex.org/W2512018227",
        "https://openalex.org/W2064076387",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2146795245",
        "https://openalex.org/W2966501856",
        "https://openalex.org/W2009071067",
        "https://openalex.org/W2991938283",
        "https://openalex.org/W2971483169",
        "https://openalex.org/W1976263166",
        "https://openalex.org/W2783488367",
        "https://openalex.org/W2948400274",
        "https://openalex.org/W2181646778",
        "https://openalex.org/W3159660641",
        "https://openalex.org/W3099562471",
        "https://openalex.org/W1990592195",
        "https://openalex.org/W2990176100",
        "https://openalex.org/W3035231706",
        "https://openalex.org/W2081140338",
        "https://openalex.org/W3197957534",
        "https://openalex.org/W3009406242",
        "https://openalex.org/W2997763120",
        "https://openalex.org/W3099025816",
        "https://openalex.org/W3006777311",
        "https://openalex.org/W2145885328",
        "https://openalex.org/W2523532944",
        "https://openalex.org/W2587107113",
        "https://openalex.org/W2102166818",
        "https://openalex.org/W2055447163",
        "https://openalex.org/W2141303451",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963073614"
    ],
    "abstract": "The light absorption and scattering of underwater impurities lead to poor underwater imaging quality. The existing data-driven based underwater image enhancement (UIE) techniques suffer from the lack of a large-scale dataset containing various underwater scenes and high-fidelity reference images. Besides, the inconsistent attenuation in different color channels and space areas is not fully considered for boosted enhancement. In this work, we constructed a large-scale underwater image (LSUI) dataset including 5004 image pairs, and reported an U-shape Transformer network where the transformer model is for the first time introduced to the UIE task. The U-shape Transformer is integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT) module and a spatial-wise global feature modeling transformer (SGFMT) module, which reinforce the network's attention to the color channels and space areas with more serious attenuation. Meanwhile, in order to further improve the contrast and saturation, a novel loss function combining RGB, LAB and LCH color spaces is designed following the human vision principle. The extensive experiments on available datasets validate the state-of-the-art performance of the reported technique with more than 2dB superiority.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nU-shape Transformer\nfor Underwater Image Enhancement\nLintao Peng, Chunli Zhu, and Liheng Bian*\nAbstractâ€”The light absorption and scattering of underwater\nimpurities lead to poor underwater imaging quality. The existing\ndata-driven based underwater image enhancement (UIE) tech-\nniques suffer from the lack of a large-scale dataset containing\nvarious underwater scenes and high-ï¬delity reference images.\nBesides, the inconsistent attenuation in different color channels\nand space areas is not fully considered for boosted enhancement.\nIn this work, we built a large scale underwater image (LSUI)\ndataset, which covers more abundant underwater scenes and\nbetter visual quality reference images than existing underwater\ndatasets. The dataset contains 4279 real-world underwater image\ngroups, in which each raw imageâ€™s clear reference images,\nsemantic segmentation map and medium transmission map are\npaired correspondingly. We also reported an U-shape Trans-\nformer network where the transformer model is for the ï¬rst time\nintroduced to the UIE task. The U-shape Transformer is inte-\ngrated with a channel-wise multi-scale feature fusion transformer\n(CMSFFT) module and a spatial-wise global feature modeling\ntransformer (SGFMT) module specially designed for UIE task,\nwhich reinforce the networkâ€™s attention to the color channels\nand space areas with more serious attenuation. Meanwhile, in\norder to further improve the contrast and saturation, a novel\nloss function combining RGB, LAB and LCH color spaces is\ndesigned following the human vision principle. The extensive\nexperiments on available datasets validate the state-of-the-art\nperformance of the reported technique with more than 2dB\nsuperiority. The dataset and demo code are available on https:\n//lintaopeng.github.io/ pages/UIE%20Project%20Page.html.\nIndex Termsâ€”Underwater image enhancement, Transformer,\nMulti-color space loss function, Underwater image dataset\nI. I NTRODUCTION\nU\nNDERW ATER Image Enhancement (UIE) technology\n[1], [2] is essential for obtaining underwater images\nand investigating the underwater environment, which has wide\napplications in ocean exploration, biology, archaeology, under-\nwater robots [3] and among other ï¬elds. However, underwater\nimages frequently have problematic issues, such as color\ncasts, color artifacts and blurred details [4]. Those issues\ncould be explained by the strong absorption and scattering\neffects on light, which are caused by dissolved impurities\nand suspended matter in the medium (water). Therefore, UIE-\nrelated innovations are of great signiï¬cance for improving the\nvisual quality and merit of images in accurately understanding\nthe underwater world.\nIn general, the existing UIE methods could be categorized\ninto three types, which are physical model-based, visual prior-\nbased and data-driven methods, respectively. Among them,\nL. Peng, C. Zhu and L. Bian are with the Advanced Research Institute\nof Multidisciplinary Science & School of Information and Electronics, Bei-\njing Institute of Technology, Beijing, China. Correspondence to L. Bian:\nbian@bit.edu.cn.\nPSNR  14.62\nInput\nOurs\nUIBLA [16] FUnIE [3]\nReference\nPSNR  20.15 PSNR  17.58\nPSNR  26.96\nUcolor [23]\nPSNR  22.06\nRetinex[9]\nPSNR  21.54\nUGAN[30]\nPSNR  13.27\nFig. 1. Compared with the existing UIE methods, the image produced by our\nU-shape Transformer has the highest PSNR[5] score and best visual quality.\nvisual prior-based UIE methods [6], [7], [8], [9], [10], [11]\nmainly concentrated on improving the visual quality of un-\nderwater images by modifying pixel values from the per-\nspectives of contrast, brightness and saturation. Nevertheless,\nthe ignorance of the physical degradation process limits the\nimprovement of enhancement quality. In addition, physical-\nmodel based UIE methods [12], [13], [14], [15], [16], [17],\n[18], [19], [20] mainly focus on the accurate estimation of\nmedium transmission. With the estimated medium transmis-\nsion and other key underwater imaging parameters such as the\nhomogeneous background light, a clean image can be obtained\nby reversing a physical underwater imaging model. However,\nthe performance of physical model-based UIE is restricted to\ncomplicated and diverse real-world underwater scenes. That\nis because, (1) model hypothesis is not always plausible with\ncomplicated and dynamic underwater environment; (2) evalu-\nating multiple parameters simultaneously is challenging.More\nrecently, as to the data-driven methods [21], [3], [22], [23],\n[24], [25], [26], [27], [28], [29], [30], [31], [4], which could be\nregarded as deep learning technologies in UIE domain, exhibit\nimpressive performance on UIE task. However, the existing\nunderwater datasets more-or-less have the disadvantages, such\nas a small number of images, few underwater scenes, or even\nnot real-world scenarios, which limits the performance of the\ndata-driven UIE method. Besides, the inconsistent attenuation\nof the underwater images in different color channels and space\nareas have not been uniï¬ed in one framework.\nIn this work, we ï¬rst built a large scale underwater im-\nage (LSUI) dataset, which covers more abundant underwater\nscenes (water types, lighting conditions and target categories)\nand better visual quality reference images than existing under-\nwater datasets [32], [28], [26], [22]. The dataset contains 4279\nreal-world underwater images, and the corresponding clear im-\nages are generated as comparison references. We also provide\narXiv:2111.11843v6  [cs.CV]  12 Jun 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nthe semantic segmentation map and medium transmission map\nfor each image. Furthermore, with the prior knowledge that\nthe attenuation of different color channels and space areas in\nunderwater images is inconsistent, we designed a channel-\nwise multi-scale feature fusion transformer (CMSFFT) and\na spatial-wise global feature modeling transformer (SGFMT)\nbased on the attention mechanism, and embedded them in\nour U-shape Transformer which is designed based on [33].\nMoreover, according to the color space selection experiment\nand [34], [23], we designed a multi-color space loss function\nincluding RGB, LAB and LCH color space. Fig. 1 shows the\nresult of our UIE method and some comparison UIE methods,\nand the main contributions of this paper can be summarized\nas follows:\nâ€¢ We reported a novel U-shape Transformer dealing with\nthe UIE task, in which the designed channel-wise and\nspatial-wise attention mechanism based on transformer\nenables to effectively remove color artifacts and casts.\nâ€¢ We designed a novel multi-color space loss function\ncombing the RGB, LCH and LAB color-space features,\nwhich further improves the contrast and saturation of\noutput images.\nâ€¢ We released a large-scale dataset containing 4279 real un-\nderwater images and the corresponding high-quality ref-\nerence images, semantic segmentation maps, and medium\ntransmission maps, which facilitates further development\nof UIE techniques.\nII. R ELATED WORK\nA. UIE Methods\nUIE is an indispensable step to improve the visual quality\nof recorded underwater images. A variety of methods have\nbeen proposed and can be categorized as visual prior, physical\nmodels, and data-driven methods.\nUIE methods based on visual prior. This approach aims\nto restore a clear underwater image by modifying its pixel\nvalue. Typical methods involve: 1) Modify the pixel value\nwith single metric.Such as contrast adjustment [35], histogram\nequalization [35], and white balance [11]. For instance, Hitam\net al. [35] used contrast adjustment and adaptive histogram\nequalization methods in RGB color space and HSV color space\nto enhance the contrast of underwater images and reduce noise.\n2) Modify the pixel value with multiple metrics.For example,\nfusion-based methods, which exhibit the ï¬nal enhancement\nimage via the weighted fusion of multiple traditional UIE\nmethods. For example, Fang et al. [6] ï¬rst applied white\nbalance and global contrast adjustments to enhance underwater\nimages, and then the two enhancement results are combined\ninto one image by weighted addition to obtain the ï¬nal\nenhanced underwater image. 3) Retinex based UIE methods.\nFu et al. [9] proposed a retinex model-based UIE method\nincluding color correction, layer decomposition and enhance-\nment. Furthermore, Zhang et al. [36] proposed a multi-scale\nUIE method based on the retinex model.\nThe way of modifying pixel value has the inherent ad-\nvantage of improving the contrast and saturation of the raw\nunderwater image. However, as visual prior neglected the\ninconsistent attenuation degree of underwater images in varied\ncolor channels and space areas, it performs not well on real\nunderwater images with complex underwater environments.\nUIE methods based on physical models. This approach\nregard UIE as a problem of inversion, and researchers usually\nenhance underwater images based on the following three steps,\n1) establishing the prior conditions of the hypothetical physical\nimaging model; 2) estimating the key parameters; 3) reversing\nthe degradation process of the underwater imaging process to\nobtain a clear image.\nPrior is the basis of the physical model based UIE, in\nwhich existing work includes underwater dark channel priors\n[13], attenuation curve priors [15], fuzzy priors [18] and\nminimum information priors [20], etc. Early-stage research\nenhanced the underwater image by modifying the dark channel\nprior (DCP) [13] algorithm. Chiang et al.[18] restored the\nunderwater image by combining the DCP with the wavelength\ncompensation algorithm. Drews Jr et al. [13] proposed an\nunderwater dark channel prior algorithm (UDCP) based on the\npriori that the red channel in the underwater image is more\nattenuated. Carlevaris Bianca et al. [37] used the attenuation\ndifference prior between the three color channels in RGB\ncolor space to predict the transmission characteristics of the\nunderwater scene, which feasibility is basically due to red light\ngenerally decays faster than green and blue light. In addition,\nPeng et al. [16] proposed a depth map estimate method for\nunderwater scenes based on the intrinsic characteristics of\nunderwater image blurriness and light absorption to effectively\nrecover underwater images. Li et al. [7] integrate the minimum\ninformation loss and histogram distribution prior for depth\nestimation to recover underwater images.\nThis branch of UIE methods could achieve satisfactory\nresults only when underwater scenes are in accordance with\nthe selected physical imaging model. Therefore, the manually\nestablished priors restrain the modelâ€™s robustness and scalabil-\nity under the complicated and varied circumstances. Moreover,\nas the underwater physical imaging model does not taken\nhuman eyeâ€™s perception characteristics into account, the visual\nquality of the restored images are of poor presentation effect.\nIn recent years, underwater physical imaging models are grad-\nually utilized in combination with data-driven methods[23].\nData-driven UIE methods. Current data-driven UIE meth-\nods can be divided into two main technical routes, (1) design-\ning an end-to-end module;(2) utilizing deep models directly\nto estimate physical parameters, and then restore the clean\nimage based on the degradation model.To alleviate the need\nfor real-world underwater paired training data, Li et al. [22]\nproposed a WaterGAN to generate underwater-like images\nfrom in-air images and depth maps in an unsupervised manner,\nin which the generated dataset is further used to train the\nWaterGAN. Moreover, [24] exhibited a weakly supervised\nunderwater color transmission model based on CycleGAN\n[38]. Beneï¬ting from the adversarial network architecture and\nmultiple loss functions, that network can be trained using\nunpaired underwater images, which reï¬nes the adaptability of\nthe network model to underwater scenes. However, images\nin the training dataset used by the above methods are not\nmatched real underwater images, which leads to limited en-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nhancement effects of the above methods in diverse real-world\nunderwater scenes. Recently, Li et al. [28] proposed a gated\nfusion network named WaterNet, which uses gamma-corrected\nimages, contrast-improved images, and white-balanced images\nas the inputs to enhance underwater images. Yang et al. [39]\nproposed a conditional generative adversarial network (cGAN)\nto improve the perceptual quality of underwater images.\nThe methods mentioned above usually use existing deep\nneural networks for general purposes directly on UIE tasks\nand neglect the unique characteristics of underwater imaging.\nFor example, [24] directly used the CycleGAN [38] network\nstructure, and [28] adopted a simple multi-scale convolutional\nnetwork. Other models such as UGAN [30],WaterGAN [22]\nand cGAN [39], still inherited the disadvantage of GAN-\nbased models, which produces unstable enhancement results.\nIn addition, Ucolor [23] combined the underwater physical\nimaging model and designed a medium transmission guided\nmodel to reinforce the networkâ€™s response to areas with more\nsevere quality degradation, which could improve the visual\nquality of the network output to a certain extent. However,\nphysical models sometimes failed with varied underwater\nenvironments.\nFrom above, our proposed network aims at generating high\nvisual quality underwater images by properly accounting the\ninconsistent attenuation characteristics of underwater images\nin different color channels and space areas.\nB. Underwater Image Datasets\nThe sophisticated and dynamic underwater environment\nresults in extreme difï¬culties in the collection of matched un-\nderwater image training data in real-world underwater scenes.\nPresent datasets can be classiï¬ed into two types, they are,\n(1) Non-reference datasets. Liu et al. [32] proposed the RUIE\ndataset, which encompasses varied underwater lighting, depth\nof ï¬eld, blurriness and color cast scenes. Akkaynak et al. [26]\npublished a non-reference underwater dataset with a standard\ncolor comparison chart. Those datasets, however, cannot be\nused for end-to-end training for lacking matched clear refer-\nence underwater images. (2) Full-reference datasets. Li et al.\n[22] presented an unsupervised network dubbed WaterGAN\nto produce underwater-like images using in-air images and\ndepth maps. Similarly, Fabbri et al. [30] used CycleGAN\nto generate distorted images from clean underwater images\nbased on weakly-supervised distribution transfer. However,\nthese methods rely heavily on training samples, which is easy\nto produce artifacts that are out of reality and unnatural. Li et\nal. [28] constructed a real UIE benchmark UIEB, including\n890 images pairs, in which reference images were hand-\ncrafted using the existing optimal UIE methods. Although\nthose images are authentic and reliable, the number, content\nand coverage of underwater scenes are limited. In contrast, our\nLSUI dataset contains 4279 real-world underwater images with\nmore abundant underwater scenes (water types, lighting con-\nditions and target categories) than existing underwater datasets\n[32], [28], [26], [22], and the corresponding clear images\nare generated as comparison references. We also provide the\nsemantic segmentation map and medium transmission map for\neach raw underwater image.\nC. Transformers\nAlthough CNN-based UIE methods [28], [3], [30], [31],\n[23] achieved signiï¬cant improvement compared with tradi-\ntional UIE methods. There are still two aspects that limit its\nfurther promotion, (1) uniform convolution kernel is not able to\ncharacterize the inconsistent attenuation of underwater images\nin different color channels and spatial regions;(2) the CNN\narchitecture concerns more on local features, while ineffective\nfor long-dependent and global feature modeling.\nRecently, transformer [40] has gained more and more at-\ntention, its content-based interactions between image content\nand attention weights can be interpreted as spatially varying\nconvolution, and the self-attention mechanism is efï¬cient\nat modeling long-distance dependencies and global features.\nBeneï¬ting from these advantages, transformers have shown\noutstanding performance in several vision tasks [41], [42],\n[43], [44]. Compared with previous CNN-based UIE networks,\nour CMSFFT and SGFMT modules designed based on the\ntransformer can guide the network to pay more attention\nto the more serious attenuated color channels and spatial\nareas. Moreover, by combining CNN with transformer, we\nachieve better performance with a relatively small amount of\nparameters.\nIII. P ROPOSED DATASET AND METHOD\nA. LSUI Dataset\nFig. 2. Statistics of our LSUI dataset and the existing underwater dataset\nUIEB [28].\nData Collection. We have collected 8018 underwater images,\nwhich is composed of images collected by ourself and from\nother existing public datasets [32], [26], [22], [30] (All images\nhave been licensed and used only for academic purposes).\nReal underwater images with rich water scenes, water types,\nlighting conditions and target categories, are selected to the\nextent possible, for further generating clear reference images.\nReference Image Generation. The reference images were\nselected with two round subjective and objective evaluations,\nto eliminating the potential bias to the extent possible. In the\nï¬rst round, inspired by ensemble learning [45] that multiple\nweak classiï¬ers could form a strong one, we ï¬rstly use 18\nexisting optimal UIE methods [6], [9], [16], [13], [14], [17],\n[18], [19], [20], [3], [22], [24], [25], [27], [29], [31], [46],\n[47] to process the collected underwater images successively,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nâ‰¥5m\n20m\n50m\n100%\n50%\n25%\n12.5%\n6.1%200m\n100m\nâ‰¥\nâ‰¥\nâ‰¥\nâ‰¥\nReference Image\nOriginal Image\nSegmentation Map\nMedium Transmission Map\nFig. 3. Example images in the LSUI dataset. Our LSUI dataset contains 4279 real-world underwater images with more abundant underwater scenes (water\ntypes, lighting conditions and target categories) than existing underwater datasets [32], [28], [26], [22], and the corresponding clear images are generated as\ncomparison references. We also provide the semantic segmentation map and medium transmission map for each raw underwater image. The top of each image\ngroup is the clear reference image, followed by the raw underwater image, semantic segmentation map, and medium transmission map.\nand a set with 18 âˆ—8018 images is generated for the next-step\noptimal reference dataset selection. Unlike [28], to reducing\nthe number of images that need to be selected manually, non-\nreference metrics UIQM [48] and UCIQE [49] are adopted\nto score all generated images with equal weights. Then, the\ntop-three reference images of each original one form a set\nwith the size 3 âˆ—8018. Considering individual differences, 20\nvolunteers with image processing experience were invited to\nrate images according to 5 most important judgments (contrast;\nsaturation; color correction effects; artifacts degree; over or\nunder-enhancement degree) of UIE tasks with a score from 0-\n10, where the higher score represents the more contentedness.\nAnd the total score of each reference picture is 100 ( 5 âˆ—20)\nafter normalizing each score to 0-1. The top-one reference\nimage of each raw underwater image was chosen with the\nhighest summation value. In addition, images with the highest\nsummation lower than 70 have been removed from the dataset.\nAfter the ï¬rst round, some of the generated reference images\nstill have problems such as blur, color cast and noise. So\nin the second round, we invited volunteers to vote on each\nreference picture again to select its existing problems and\ndetermine the corresponding optimization method, and then\nuse appropriate image enhancement methods [43], [50], [51] to\nprocess it. Next, all volunteers were invited to conduct another\nround of voting to remove image pairs that more than half of\nthe volunteers were dissatisï¬ed with. To improve the utility\nof the LSUI dataset, we also hand-labeled a segmentation\nmap and generated a medium transmission map for each\nimage. Eventually, our LSUI dataset contains 4279 images\nand the corresponding high-quality reference images, semantic\nsegmentation maps, and medium transmission maps for each\nimage.\nAs shown in Fig .2, compared with UIEB [28], our LSUI\ndataset contains large number of images with richer underwa-\nter scenes and object categories. In particular, our LSUI dataset\nincludes deep-sea scenes and underwater cave scenes that are\nnot available in previous underwater datasets. We provide\nsome examples of our LSUI dataset in Fig .3, which includes\nvaried underwawter scenes, water types, lighting conditions\nand target categories. As to the authorsâ€™ best knowledge, LSUI\nis the largest real underwater image dataset with high quality\nreference images at the present time, which could facilitate the\nfurther development of the UIE methods.\nB. U-shape Transformer\n1) Overall Architecture: The overall architecture of the\nU-shape Transformer is shown as Fig. 4, which includes a\nCMSFFT & SGFMT based generator and a discriminator.\nIn the generator, (1) Encoding: Except being directly input\nto the network, the original image will be downsampled\nthree times respectively. Then after 1*1 convolution, the three\nscale feature maps are input into the corresponding scale\nconvolution block. The outputs of four convolutional blocks\nare the inputs of the CMSFFT and SGFMT; (2) Decoding:\nAfter feature remapping, the SGFMT output is directly sent\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\n+\nPE\nTransformer Layer\nTransformer Layer \nFeature Mapping\nLinearProjection\nSGFMT\nâ€¦â€¦â€¦â€¦\n+\nPE\nLinearProjection\nFeature Mapping\nCMHA\nCMHA\nCMSFFT\nInput\nLayer Norm\nMulti-Head\nAttention\n+\nLayer Norm\nMLP\n+\nPE Position Embedding\nMulti-Layer PerceptronMLP\nCMHA Channel-wise Multi-Head Attention\n2 Times Down Sampling\n1*1 Convolution\nElement-wise Addition+\nGT\nOutput\nFig. 4. The network structure of U-shape Transformer. CMSFFT and SGFMT modules specially designed for UIE tasks reinforce the networkâ€™s attention to\nthe more severely attenuated color channels and spatial regions. The multi-scale connections of the generator and the discriminator make the gradient ï¬‚ow\nfreely between the generator and the discriminator, therefore making the training process more stable.\nto the ï¬rst convolutional block. Meanwhile, four convolutional\nblocks with varied scales will receive the four outputs from\nCMSFFT.\nIn the discriminator, the input of the four convolutional\nblocks includes: the feature map output by its own upper layer,\nthe feature map of the corresponding size from the decoding\npart and the feature map generated by 1 âˆ—1 convolution after\ndownsampling to the corresponding size using the reference\nimage. With the described multi-scale connections, the gra-\ndient ï¬‚ow can ï¬‚ow freely on multiple scales between the\ngenerator and the discriminator, such that a stable training\nprocess could be obtained, details of the generated images\ncould be enriched. The detailed structure of SGFMT and\nCMSFFT in the network will be described in the following\ntwo subsections.\n2) SGFMT: The SGFMT (as shown in Fig. 5) is used to\nreplace the original bottleneck layer of the generator, which\ncan assist the network to model the global information and re-\ninforce the networkâ€™s attention on severely degraded parts. As-\nsuming the size of the input feature map is Fin âˆˆR\nH\n16 âˆ—W\n16 âˆ—C.\nFor the expected one-dimensional sequence of the transformer,\nlinear projection is used to stretch the two-dimensional feature\nmap into a feature sequence Sin âˆˆR\nHW\n256 âˆ— C. For preserving\nthe valued position information of each region, learnable\nposition embedding is merged directly, which can be expressed\nas,\nSin = W âˆ—Fin + PE, (1)\nwhere W âˆ—Fi represents a linear projection operation, PE\nrepresents a position embedding operation.\nThen we input the feature sequence Sin to the transformer\nblock, which contains 4 standard transformer layers [40].\nC\nLinear\nProjection\nC\nğ»ğ»\n16* \nğ‘Šğ‘Š\n16 *C\nğ»ğ»ğ‘Šğ‘Š\n256 *C\nPosition\nEmbedding\nC\nLayer Norm\n+\nLayer Norm\nMLP\nMHA\n+\nC C\nFeature\nMapping\nğ»ğ»ğ‘Šğ‘Š\n256 *C\nğ»ğ»ğ‘Šğ‘Š\n256 *C\nğ»ğ»\n16* \nğ‘Šğ‘Š\n16 *C\nFğ‘–ğ‘–ğ‘–ğ‘–\nFğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ\nğ‘†ğ‘†ğ‘–ğ‘–ğ‘–ğ‘–\nğ‘†ğ‘†ğ‘™ğ‘™\nFig. 5. Data ï¬‚ow diagram of the SGFMT module. Based on the prior that\nunderwater images are not uniformly degraded in different spatial regions,\nwe designed a novel spatial-wise global feature modeling transformer\n(SGFMT) based on the spatial self-attention mechanism to replace the original\nbottleneck layer of the generator. It can accurately model the global feature\nof underwater images and reinforce the networkâ€™s attention to the space areas\nwith more serious attenuation, thus achieving uniform UIE.\nEach transformer layer contains a multi-head attention block\n(MHA) and a feed-forward network (FFN). The FFN includes\na normalization layer and a fully connected layer. The output\nof the l-th(lâˆˆ[1,2,....,l ]) layer in the transformer block can\nbe calculated by,\nS\nâ€²\nl = MHA(LN(Slâˆ’1)) +Slâˆ’1 (2)\nSl = FFN(LN(S\nâ€²\nl )) +S\nâ€²\nl , (3)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nLinear Projection\nLinear Projection\nLinear Projection\nLinear Projection\nPosition Embedding\nPosition Embedding\nPosition Embedding\nPosition Embedding\nFeature \nMapping\nFeature \nMapping\nFeature \nMapping\nFeature \nMapping\nH* W*ğ¶ğ¶1\nğ»ğ»\n2* \nğ‘Šğ‘Š\n2 *ğ¶ğ¶2\nğ»ğ»\n4* \nğ‘Šğ‘Š\n4 *ğ¶ğ¶3\nğ»ğ»\n8* \nğ‘Šğ‘Š\n8 *ğ¶ğ¶4\nd*ğ¶ğ¶1\nd*ğ¶ğ¶2\nd*ğ¶ğ¶3\nd*ğ¶ğ¶4\nğ¶ğ¶1\nğ¶ğ¶2\nğ¶ğ¶3\nğ¶ğ¶4\n1 2\nd\n1 2\nd\n1 2\nd\n1 2\nd\nC=ğ¶ğ¶1 + ğ¶ğ¶2 + ğ¶ğ¶3 + ğ¶ğ¶4\nMatMul\nScale\nInstance  Norm\nSoftMax\nMatMul\nğ‘„ğ‘„ğ‘–ğ‘–\nğ‘‡ğ‘‡\nğ‘„ğ‘„1\nğ‘„ğ‘„2\nğ‘„ğ‘„3\nğ‘„ğ‘„4\nConcat\n+\n+\n+\n+\nK\nğ‘‰ğ‘‰ğ‘‡ğ‘‡\nLNLNLNLN\nLNLNLNLN\nMLPMLPMLPMLP\nğ‘‚ğ‘‚1\nğ‘‚ğ‘‚2\nğ‘‚ğ‘‚3\nğ‘‚ğ‘‚4\nLN Layer Normalization MLP Multi-Layer Perceptron \nConcat Channel-Wise ConcatenationElement-wise Addition+\nChannel-wise Multi-Head Attention\nChannel-wise Attention\nFig. 6. Detailed structure of the CMSFFT module. According to the prior that underwater images are inconsistently attenuated in different color channels,\nwe propose a novel channel-wise multi-scale feature fusion transformer(CMSFFT) . Speciï¬cally, CMSFFT replaces the skip connection of the generator,\nuses the channel-wise self-attention mechanism to perform channel-wise multi-scale feature fusion on the features output by the encoder of the generator, and\ntransmits the fusion results to the decoder efï¬ciently, so as to reinforce the networkâ€™s attention to the color channels with more serious attenuation and realize\naccurate UIE.\nwhere LN represents layer normalization, and Sl represents\nthe output sequence of the l-th layer in the transformer block.\nThe output feature sequence of the last transformer block is\nSl âˆˆR\nHW\n256 âˆ— C, which is restored to the feature map of Fout âˆˆ\nR\nH\n16 âˆ— W\n16 âˆ—C after feature remapping.\n3) CMSFFT: To reinforce the networkâ€™s attention on the\nmore serious attenuation color channels, inspired by [52], we\ndesigned the CMSFFT block to replace the skip connection\nof the original generatorâ€™s encoding-decoding architecture\n(Fig.6), which consists of the following three parts.\nMulti-Scale Feature Encoding. The inputs of CMSFFT are\nthe feature maps Fi âˆˆR\nH\n2i âˆ—W\n2i âˆ—Ci(i= 0,1,2,3) with different\nscales. Differs from the linear projection in Vit [53] which\nis applied directly on the partitioned original image, we use\nconvolution kernels with related ï¬lter size P\n2i âˆ—P\n2i (i= 0,1,2,3)\nand step size P\n2i (i= 0,1,2,3), to conduct linear projection on\nfeature maps with varied scales. In this work, P is set as 32.\nAfter that, four feature sequence Si âˆˆRdâˆ— Ci(i = 1,2,3,4)\ncould be obtained, where d âˆˆ HW\nP2 . Those four convolution\nkernels divide feature maps into the same number of blocks,\nwhile the number of channels Ci(i = 1,2,3,4) remains un-\nchanged. Then, four query vectors Qi âˆˆRdâˆ— Ci(i= 1,2,3,4),\nK âˆˆRdâˆ— C and V âˆˆRdâˆ— C can be obtained by Eq.(4).\nQi = SiWQi K = SWK V = SWV , (4)\nwhere WQi âˆˆRdâˆ— Ci(i= 1,2,3,4), WK âˆˆRdâˆ— C and WV âˆˆ\nRdâˆ— C stands for learnable weight matrices; S is generated\nby concatenating Si âˆˆRdâˆ— Ci(i = 1,2,3,4) via the channel\ndimension, where C = C1 + C2 + C3 + C4. In this work, C1,\nC2, C3, and C4 are set as 64, 128, 256, 512, respectively.\nChannel-Wise Multi-Head Attention(CMHA). The CMHA\nblock has six inputs, which are K âˆˆ Rdâˆ— C, V âˆˆ Rdâˆ— C\nand Qi âˆˆRdâˆ— Ci(i = 1,2,3,4). The output of channel-wise\nattention CAi âˆˆRCiâˆ— d(i= 1,2,3,4) could be obtained by,\nCAi = SoftMax(IN(QT\ni K\n2âˆš\nC\n))VT , (5)\nwhere IN represents the instance normalization operation. This\nattention operation performs along the channel-axis instead\nof the classical patch-axis[53], which can guide the network\nto pay attention to channels with more severe image quality\ndegradation. In addition, IN is used on the similarity maps to\nassist the gradient ï¬‚ow spreads smoothly.\nThe output of the i-th CMHA layer can be expressed as,\nCMHAi = (CAi\n1 + CAi\n2 + .......,+CAi\nN )/N+ Qi, (6)\nwhere N is the number of heads, which is set as 4 in our\nimplementation.\nFeed-Forward Network(FFN). Similar to the forward prop-\nagation of [53], the FFN output can be expressed as,\nOi = CMHAi + MLP(LN(CMHAi)), (7)\nwhere Oi âˆˆ Rdâˆ— Ci(i = 1,2,3,4); MLP stands for multi-\nlayer perception. Here, The operation in Eq. (7) needs to be\nrepeated l (l=4 in this work) times in sequence to build the\nl-layer transformer.\nFinally, feature remappings are performed on the four\ndifferent output feature sequences Oi âˆˆRCiâˆ— d(i= 1,2,3,4)\nto reorganize them into four feature maps Fi âˆˆR\nH\n2i âˆ—W\n2i âˆ—Ci(i=\n0,1,2,3) , which are the input of convolutional block in the\ngeneratorâ€™s decoding part.\nC. Loss Function\nTo take advantage of the LAB and LCH color spacesâ€™\nwider color gamut representation range and more accurate\ndescription of the color saturation and brightness, we designed\na multi-color space loss function combining RGB, LAB and\nLCH color spaces to train our network. The image from RGB\nspace is ï¬rstly converted to LAB and LCH space, and reads,\nLG(x),AG(x),BG(x) = RGB2LAB(G(x))\nLy,Ay,By = RGB2LAB(y), (8)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nLG(x),CG(x),HG(x) = RGB2LCH(G(x)),\nLy,Cy,Hy = RGB2LCH(y), (9)\nwhere x, y and G(x) represents the original inputs, the\nreference image, and the clear image output by the generator,\nrespectively.\nLoss functions in the LAB and LCH space are written as\nEq.(10) and Eq.(11).\nLossLAB(G(x),y) =Ex,y[(Ly âˆ’LG(x))\n2\nâˆ’\nnâˆ‘\ni=1\nQ(Ay\ni )log(Q(AG(x)\ni )) âˆ’\nnâˆ‘\ni=1\nQ(By\ni )log(Q(BG(x)\ni ))],\n(10)\nLossLCH (G(x),y) =Ex,y[âˆ’\nnâˆ‘\ni=1\nQ(Ly\ni )log(Q(LG(x)\ni ))\n+ (Cy âˆ’CG(x))\n2\n+ (Hy âˆ’HG(x))\n2\n],\n(11)\nwhere Q stands for the quantization operator.\nL2 loss in the RGB color space LossRGB and the perceptual\nloss Lossper[54] , as well as LossLAB and LossLCH are the\nfour loss functions for the generator.\nBesides, standard GAN loss function is introduced for\nminimizing the loss between generated and reference pictures,\nand written as,\nLGAN (G,D) =Ey[logD(y)] +Ex[log(1 âˆ’D(G(x)))],\n(12)\nwhere D represents the discriminator. D aims at maximizing\nLGAN (G,D), to accurately distinguish the generated image\nfrom the reference image. And the goal of generator G is to\nminimize the loss between generated pictures and reference\npictures.\nThen, the ï¬nal loss function is expressed as,\nGâˆ— = argmin\nG\nmax\nD\nLGAN (G,D) +Î±LossLAB(G(x),y)\n+ Î²LossLCH (G(x),y) +Î³LossRGB(G(x),y)\n+ ÂµLossper(G(x),y),\n(13)\nwhere Î±,Î²,Î³,Âµ are hyperparameters, which are set as 0.001,\n1, 0.1, 100, respectively, with numerous experiments.\nIV. E XPERIMENTS\nIn this section, we ï¬rst introduce the training details of the\nU-shape Transformer and the detailed settings of the experi-\nment. Next, we conduct experiments on the selection of color\nspace. Then we retrain some network models we collected\non the existing underwater datasets and the LSUI dataset to\nevaluate our proposed dataset. Moerover, we also compare our\nUIE method with state-of-the-arts on ï¬ve datasets. Finally,\nseries of ablation studies are conducted to demonstrate the\neffectiveness of each component in U-shape Transformer.\nA. Implementation Details\nThe LSUI dataset was randomly divided as Train-L (3879\nimages) and Test-L400 (400 images) for training and testing,\nrespectively. The training set was enhanced by cropping,\nrotating and ï¬‚ipping the existing images. All images were\nadjusted to a ï¬xed size (256*256) when input to the network,\nand the pixel value will be normalized to [0,1].\nWe use python and pytorch framework via NVIDIA\nRTX3090 on Ubuntu20 to implement the U-shape Trans-\nformer. Adam optimization algorithm is utilized for the total of\n800 epochs training with batchsize set as 6. The initial learning\nrate is set as 0.0005 and 0.0002 for the ï¬rst 600 epochs and\nthe last 200 epochs, respectively. Besides, the learning rate\ndecreased 20% every 40 epochs. For LossRGB, L2 loss is\nused for the ï¬rst 600 epochs, and L1 loss is used for the last\n200 epochs.\nB. Experiment Settings\nBenchmarks. Besides Train-L, the second training set Train-\nU contains 800 pairs of underwater images from UIEB [28]\nand 1,250 synthetic underwater images from [55]; the third\ntraining set Train-E contains the paired training images in the\nEUVP [3] dataset. Testing datasets are categorized into two\ntypes, (1) full-reference testing dataset: Test-L400 and Test-\nU90 (remaining 90 pairs in UIEB); (2) non-reference testing\ndataset: Test-U60 and SQUID. Here, Test-U60 includes 60\nnon-reference images in UIEB; 16 pictures from SQUID [26]\nforms the second non-reference testing dataset.\nCompared Methods. We compare U-shape Transformer with\n10 UIE methods to verify our performance superiority. It in-\ncludes two physical-based models (UIBLA [16], UDCP [13]),\nthree visual prior-based methods (Fusion [6], retinex based\n[9], RGHS [10]), and ï¬ve data-driven methods (WaterNet [28],\nFUnIE [3], UGAN [30], UIE-DAL [31], Ucolor [23]).\nEvaluation Metrics. For the testing dataset with reference\nimages, we conducted full-reference evaluations using PSNR\n[5] and SSIM [56] metrics. Those two metrics reï¬‚ect the prox-\nimity to the reference, where a higher PSNR value represents\ncloser image content, and a higher SSIM value reï¬‚ects a more\nsimilar structure and texture. For images in the non-reference\ntesting dataset, non-reference evaluation metrics UCIQE [49]\nand UIQM [48] are employed, in which higher UCIQE or\nUIQM score suggests better human visual perception. For\nUCIQE and UIQM cannot accurately measure the performance\nTABLE I\nSTATISTICAL RESULTS OF COLOR SPACE SELECTION EXPERIMENTS . WE TEST U-SHAPE TRANSFORMERS TRAINED WITH DIFFERENT COLOR SPACE LOSS\nFUNCTIONS ON TEST-L400 AND TEST-U90 DATASETS , RESPECTIVELY , AND THE COLOR SPACES THAT OBTAIN THE TOP THREE PSNR SCORES ARE\nMARKED WITH RED , GREEN , AND BLUE , RESPECTIVELY .\nColor\nSpace RGB HSV HSI XYZ LAB LUV LCH YUV\nTset-L400 23.79 23.32 23.37 22.63 23.86 22.81 23.62 23.43\nTest-U90 22.72 22.01 22.17 21.69 22.53 21.77 22.49 22.23\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\n(a)\n(b)\n(c)\n(d)\n(e)\nPSNR  12.09\nPSNR  21.16\nPSNR  20.70\nPSNR  22.86\nPSNR  14.49\nPSNR  17.98\nPSNR  20.20\nPSNR  26.51\nPSNR  16.42\nPSNR  23.41\nPSNR  26.06\nPSNR  31.93\nPSNR  16.53\nPSNR  19.13\nPSNR  17.55\nPSNR  20.39\nPSNR  15.56\nPSNR  23.01\nPSNR  23.25\nPSNR  30.33\nPSNR  17.78\nPSNR  24.23\nPSNR  23.79\nPSNR  30.98\nPSNR  20.50\nPSNR  21.12\nPSNR  22.69\nPSNR  31.40\nPSNR  22.78\nPSNR  27.41\nPSNR  27.52\nPSNR  29.75\nFig. 7. Enhancement results of U-shape transformer trained on different underwater datasets. (a): Input images; (b): Enhanced results using the model trained\non the Train-U; (c): Enhanced results using the model trained on the Train-E; (d): Enhanced results using the model trained by our proposed dataset Train-L;\n(e): Reference images(recognized as ground truth (GT)).\nin some cases [28] [57], we also conducted a survey following\n[23], which results are stated as â€œperception score (PS)â€. PS\nranges from 1-5, with higher scores indicating higher image\nquality. Moreover, NIQE [58], which lower value represents a\nhigher visual quality, is also adopted as the metrics.\nC. Color Space Selection\nIn order to select the appropriate color space to form the\nmulti-color space loss function, we use the mixed loss function\ncomposed of the single color space loss function and other\nloss functions to train the U-shape transformer. We use Train-\nL to train the network, and then test and calculate PSNR on\nTest-L400 and Test-U90 data sets, respectively. The results are\nshown in Tab. I,\nAs in Tab. I, We note that the LAB, LCH, and RGB color\nspaces achieve the top-3 PSNR scores on both test datasets. In\nRGB color space, image is easy to store and display because of\nits strong color physical meaning, but these three components\n(R, G, and B) are highly correlated and easily affected by\nbrightness, shadows, noise, and other factors. Compared with\nother color spaces, LAB color space is more consistent with\nthe characteristics of human visual, can express all colors that\nhuman eyes can perceive, and the color distribution is more\nuniform. LCH color space can intuitively express brightness,\nsaturation, and hue. Combined with the experimental results\nand the above analysis, we choose LAB, LCH, and RGB color\nspace to form our multi-color space loss function.\nD. Dataset Evaluation\nThe effectiveness of LSUI is evaluated by retraining the\ncompared methods (U-net [59], UGAN [30] and U-shape\nTransformer) on Train-L, Train-U and Train-E. The trained\nnetwork was tested on Test-L400 and Test-U90.\nTABLE II\nDATASET EVALUATION RESULTS . THE HIGHEST PSNR AND SSIM SCORES\nARE MARKED IN RED .\nMethods Training\nData\nTest-U90 Test-L400\nPSNR SSIM PSNR SSIM\nU-net[59]\nTrain-U 17.07 0.76 19.19 0.79\nTrain-E 17.46 0.76 19.45 0.78\nOurs 20.14 0.81 20.89 0.82\nUGAN[30]\nTrain-U 20.71 0.82 19.89 0.79\nTrain-E 20.72 0.82 19.82 0.78\nOurs 21.56 0.83 21.74 0.84\nOurs\nTrain-U 21.25 0.84 22.87 0.85\nTrain-E 21.75 0.86 23.01 0.87\nOurs 22.91 0.91 24.16 0.93\nAs shown in Tab.II, the model trained on our dataset is the\nbest of PSNR and SSIM. It could be explained that LSUI\ncontains richer underwater scenes and better visual quality\nreference images than existing underwater image datasets,\nwhich could improve the enhancement and generalization\nability of the tested network.\nFig. 7 is the sampled enhancement results of U-shape trans-\nformer trained on different underwater datasets, which is a sup-\nplement of the Data Evaluation part of the paper. Enhancement\nresults training on Train-L (a portion of our LSUI dataset)\ndemonstrates the highest PSNR value and preferable visual\nquality, while results training on other datasets show a certain\ndegree of color cast. For the high-quality reference images and\nrich underwater scenes (lighting conditions, water types and\ntarget categories), our constructed LSUI dataset could improve\nthe imaging quality and generalization performance of the UIE\nnetwork.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE III\nQUANTITATIVE COMPARISON AMONG DIFFERENT UIE METHODS ON THE FULL -REFERENCE TESTING SET . THE HIGHEST SCORES OF PSNR AND SSIM\nARE MARKED IN RED , AND ALL UIE METHODS ARE TESTED ON A PC WITH AN INTEL(R) I5-10500 CPU, 16.0GB RAM, A NVIDIA GEFORCE RTX\n1660 SUPER.\nMethods Test-L400 Test-U90 FLOPsâ†“ #param.â†“ timeâ†“PSNRâ†‘ SSIMâ†‘ PSNRâ†‘ SSIMâ†‘\nUIBLA[16] 13.54 0.71 15.78 0.73 Ã— Ã— 42.13s\nUDCP[13] 11.89 0.59 13.81 0.69 Ã— Ã— 30.82s\nFusion[6] 17.48 0.79 19.04 0.82 Ã— Ã— 6.58s\nRetinex based[9] 13.89 0.74 14.01 0.72 Ã— Ã— 1.06s\nRGHS[10] 14.21 0.78 14.57 0.79 Ã— Ã— 8.92s\nWaterNet[28] 17.73 0.82 19.81 0.86 193.7G 24.81M 0.61s\nFUnIE[3] 19.37 0.84 19.45 0.85 10.23G 7.019M 0.09s\nUGAN[30] 19.79 0.78 20.68 0.84 38.97G 57.17M 0.05s\nUIE-DAL[31] 17.45 0.79 16.37 0.78 29.32G 18.82M 0.07s\nUcolor[23] 22.91 0.89 20.78 0.87 443.85G 157.4M 2.75s\nOurs 24.16 0.93 22.91 0.91 66.2G 65.6M 0.07s\nTABLE IV\nQUANTITATIVE COMPARISON AMONG DIFFERENT UIE METHODS ON THE NON -REFERENCE TESTING SET . THE HIGHEST SCORES ARE MARKED IN RED .\nMethods Test-U60 SQUID\nPSâ†‘ UIQMâ†‘ UCIQEâ†‘ NIQEâ†“ PSâ†‘ UIQMâ†‘ UCIQEâ†‘ NIQEâ†“\ninput 1.46 0.82 0.45 7.16 1.23 0.81 0.43 4.93\nUIBLA[16] 2.18 1.21 0.60 6.13 2.45 0.96 0.52 4.43\nUDCP[13] 2.01 1.03 0.57 5.94 2.57 1.13 0.51 4.47\nFusion[6] 2.12 1.23 0.61 4.96 2.89 1.29 0.61 5.01\nRetinex based[9] 2.04 0.94 0.69 4.95 2.33 1.01 0.66 4.86\nRGHS[10] 2.45 0.66 0.71 4.82 2.67 0.82 0.73 4.54\nWaterNet[28] 3.23 0.92 0.51 6.03 2.72 0.98 0.51 4.75\nFUnIE[3] 3.12 1.03 0.54 6.12 2.65 0.98 0.51 4.67\nUGAN[30] 3.64 0.86 0.57 6.74 2.79 0.90 0.58 4.56\nUIE-DAL[31] 2.03 0.72 0.54 4.99 2.21 0.79 0.57 4.88\nUcolor[23] 3.71 0.84 0.53 6.21 2.82 0.82 0.51 4.32\nOurs 3.91 0.85 0.73 4.74 3.23 0.89 0.67 4.24\nTABLE V\nTHE COLOR DISSIMILARITY COMPARISONS OF DIFFERENT METHODS ON COLOR -CHECK 7 IN TERMS OF THE CIEDE 2000. T HE BEST SCORES ARE\nMARKED IN RED .\nMethods Pen W60 Pen W80 Can D10 Fuj Z33 Oly T6000 Oly T8000 Pan TS1 Avg\ninput 14.21 16.92 17.14 16.03 15.02 22.43 18.65 17.2\nUIBLA[16] 13.45 16.31 14.48 14.29 12.46 14.91 20.13 15,15\nUDCP[13] 15.32 24.12 16.53 13.21 12.65 16.78 12.85 15.92\nFusion[6] 12.65 13.54 14.43 12.31 11.78 10.97 11.15 12.41\nRetinex based[9] 13.08 19.25 17.13 18.85 17.18 19.45 20.62 17.94\nRGHS[10] 11.07 12.73 15.92 13.47 14.26 18.73 12.06 14.03\nWaterNet[28] 12.54 19.82 15.71 12.73 17.75 21.87 18.91 17.05\nFUnIE[3] 12.81 11.81 12.39 12.76 12.46 16.74 19.28 14.04\nUGAN[30] 20.49 21.75 22.63 26.49 21.63 22.05 20.73 22.25\nUIE-DAL[31] 12.94 16.73 14.64 12.93 16.78 17.21 18.34 15.65\nUcolor[23] 9.12 11.14 12.43 10.02 8.31 14.18 13.41 11.23\nOurs 7.87 9.70 9.96 8.23 7.71 11.14 9.81 9.20\nE. Network Architecture Evaluation\nFull-Reference Evaluation. The Test-L400 and Test-U90\ndatasets were used for evaluation. The statistical results and\nvisual comparisons are summarized in Tab. III and Fig. 8. We\nalso provide the running time (image size is 256*256) of all\nUIE methods in Tab. III, as well as the FLOPs and parameter\namount of each data-driven UIE method. And we retrianed\nthe 5 open-sourced deep learning-based UIE methods on our\ndataset.\nAs in Tab.III, our U-shape Transformer demonstrates the best\nperformance on both PSNR and SSIM metrics with relatively\nfew parameters, FLOPs, and running time. The potential\nlimitations of the performance of the 5 data-driven methods\nare analyzed as follows. The strength of FUnIE [3] lies in\nachieving fast, lightweight,and fewer parameter models, while\nnaturally limits its scalability on complex and distorted testing\nsamples. UGAN [30] and UIE-DAL [31] did not consider the\ninconsistent characteristics of the underwater images. Ucolorâ€™s\nmedia transmission map prior can not effectively represent the\nattenuation of each area, and simply introducing the concept\nof multi-color space into the networkâ€™s encoder part cannot\neffectively take advantage of it, which causes unsatisfactory\nresults in terms of contrast, brightness, and detailed textures.\nThe visual comparisons shown in Fig. 8 reveal that enhance-\nment results of our method are the closest to the reference\nimage, which has fewer color artifacts and high-ï¬delity object\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nOursUcolor [23]UGAN [30]FUnIE [3]Retinex [9]UIBLA [16]Input Ground truth\nPSNR  15.58 PSNR  17.92 PSNR  23.14 PSNR  18.12 PSNR  21.42 PSNR  22.03 PSNR  26.80\nPSNR  15.06 PSNR  16.81 PSNR  16.36 PSNR  19.03 PSNR  18.81 PSNR  22.88 PSNR  32.47\nPSNR  12.51 PSNR  13.42 PSNR  17.04 PSNR  22.01 PSNR  16.60 PSNR  26.83 PSNR  30.01\nPSNR  15.95 PSNR  18.86 PSNR  24.53 PSNR  18.29 PSNR  20.44 PSNR  22.63 PSNR  27.94\nPSNR  13.51\nPSNR  16.80\nPSNR  9.71\nPSNR  19.01\nPSNR  20.21\nPSNR  19.24\nPSNR  16.76\nPSNR  18.12\nPSNR  19.47\nPSNR  20.29\nPSNR  21.86\nPSNR  22.06\nPSNR  24.11\nPSNR  26.40\nFig. 8. Visual comparison of enhancement results sampled from the Test-L400(LSUI) and Test-U90(UIEB[28]) dataset. From left to right are raw underwater\nimages, results of UIBLA[16], Retinex based[9], FUnIE[3], UGAN[30], Ucolor[23], our U-shape Transformer and the reference image (recognized as ground\ntruth (GT)). The highest PSNR value of each raw is marked in yellow.\nareas. Five selected methods tend to produce color artifacts\nthat deviated from the original color of the object. Among\nthe methods, UIBLA [16] exhibits severe color casts. Retinex\nbased[9] could improve the image contrast to a certain extent,\nbut cannot remove the color casts and color artifacts effec-\ntively. The enhancement result of FUnLE [3] is yellowish\nand reddish overall. Although UGAN [30] and Ucolor [28]\ncould provide relatively good color appearance, they are often\naffected by local over-enhancement, and there are still some\ncolor casts in the result.\nNon-reference Evaluation. The Test-U60 and SQUID\ndatasets were utilized for the non-reference evaluation, in\nwhich statistical results and visual comparisons are shown in\nTab. IV and Fig. 9.\nAs in Tab. IV, our method achieved the highest scores\non PS and NIQE metrics, which conï¬rmed the initial idea\nto contemplate the human eyeâ€™s color perception and better\ngeneralization ability to varied real-world underwater scenes.\nNote that UCIQE and UIQM of all deep learning-based UIE\nmethods are weaker than physical model-based or visual prior-\nbased, also reported in [23]. Those two metrics are of valuable\nreference, but cannot as absolute justiï¬cations [28][57], for\nthey are non-sensitive to color artifacts & casts and biased to\nsome features.\nAs in Fig. 9, enhancement results of our method have\nthe highest PS value, which index reï¬‚ects the visual qual-\nity. Generally, compared methods are unsatisfactory, which\nincludes undesirable color artifacts, over-saturation and unnat-\nural color casts. Among the methods, results of the UIBLA\n[16] and FUnIE [3] have a certain degree of color cast.\nRetinex based [9] method introduces artifacts and unnatural\ncolors. UGAN [30] and UIE-DAL [31] have the issue of local\nover-enhancement and color artifacts, which main reason is\nthey ignore the inconsistent attenuation characteristics of the\nunderwater images in the different space areas and the color\nchannels. Although Ucolor [23] introduces the transmission\nmedium prior to reinforcing the networkâ€™s attention on the\nspatial area with severe attenuation, it still ignores the in-\nconsistent attenuation characteristics of the underwater image\nin different color channels, which results in the problem of\noverall color cast. In our method, the reported CMSFFT and\nSGFMT modules could reinforce the networkâ€™s attention to the\ncolor channels and spatial regions with serious attenuation,\ntherefore obtaining high visual quality enhancement results\nwithout artifacts and color casts.\nF . Color Restoration Performance Evaluation\nTo demonstrate the robustness and accuracy of our UIE\nmethod for color correction, we compare the color correction\nability of 10 UIE methods on the Color-Checker7 dataset. The\nColor-Checker7 dataset contains 7 underwater images taken\nfrom a shallow swimming pool with different cameras. Color\nchecker is also photographed in each image. It provides a\ngood path to demonstrate the robustness of our method to\ndifferent imaging devices and the accuracy of color restoration.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nPS  1.47\n PS  1.81 PS  1.31 PS  2.03 PS  2.14 PS  1.42 PS  3.84 PS  3.95\nPS  1.71PS  1.68\nPS  1.34 PS  1.83 PS  1.25 PS  1.94 PS  2.12 PS  1.71 PS  2.91 PS  3.23\nPS  2.06 PS  2.34 PS  2.13 PS  1.98 PS  3.12 PS  3.65\nPS  1.33 PS  1.51 PS  1.11 PS  1.47 PS  2.98 PS  1.02 PS  3.21 PS  3.87\nPS  1.15 PS  1.67 PS  1.72 PS  1.46 PS  2.31 PS  1.64 PS  2.42 PS  2.97\nPS  1.79 PS  2.03 PS  1.68 PS  2.14 PS  1.92 PS  1.64 PS  3.01 PS  3.46\nOursUcolor [23]UIE-DAL [31]UGAN [30]FUnIE [3]Retinex [9]UIBLA [16]Input\nFig. 9. Visual comparison of the non-reference evaluation sampled from the Test-U60(UIEB [28]) dataset. From left to right are raw underwater images,\nresults of UIBLA [16], Retinex based [9], FUnIE [3], UGAN [30], UIE-DAL [31], Ucolor [23] and our U-shape Transformer. The score in the upper right\ncorner of each image is the perception score(PS), and the highest PS value of each raw is marked in yellow.\nWe follow Ancuti et al. [60] to employ CIEDE2000 [61] to\nmeasure the relative differences between the corresponding\ncolor patches of ground-truth Macbeth Color Checker and\nthe enhancement results of these comparison methods. The\nexperimental results are shown in Tab. V and Fig .10.\nAs in Tab. V, for the cameras of Pentax W60, Pentax W80,\nCannon D10, Fuji Z33, Panasonic TS1 and Olympus T6000,\nour U-shape Transformer obtains the lowest color dissimilarity.\nMoreover, our U-shape Transformer achieves the best average\nscore. Such results demonstrate the superiority of our method\nfor underwater color correction. It is worth mentioning that\nsome comparable methods acquired lower score than that\nof the raw image, which reï¬‚ected that those methods are\nincapable of recovering the real color and even break the\ninherent color.\nAs shown in Fig. 11, the professional underwater camera\n(Fuji Z33) also inevitably introduces various color casts.\nAmong all the UIE methods involved in the comparison, our\nU-shape Transformer achieves the highest CIEDE 2000 score,\nwhich means our UIE method has the best color correction\nability. The results of UDCP and UIBLA are bluish, and\nRetinex has the problem of color distortion. UGAN and\nUIE-DAL suffer from low saturation and excessive reddish\ncompensation. Although FUnIE and Ucolor could remove the\ncolor cast to a certain extent, there are still problems of low\ncontrast and saturation.\nG. Ablation Study\nTo prove the effectiveness of each component, we conduct a\nseries of ablation studies on the Test-L400 and Test-U90. Four\nfactors are considered including the CMSFFT, the SGFMT, the\nmulti-scale gradient ï¬‚ow mechanism (MSG), and the multi-\ncolor space loss function (MCSL).\nTABLE VI\nSTATISTICAL RESULTS OF ABLATION STUDY ON THE TEST-L400 AND THE\nTEST-U90. T HE HIGHEST SCORES ARE MARKED IN RED .\nModels Test-L400 Test-U90\nPSNR SSIM PSNR SSIM\nBL 19.34 0.79 19.36 0.81\nBL+CMSFFT 22.47 0.88 21.72 0.86\nBL+SGFMT 21.78 0.86 21.36 0.87\nBL+MSG 20.11 0.82 21.24 0.85\nBL+MCSL 21.51 0.82 20.16 0.81\nFull Model 24.16 0.93 22.91 0.91\nExperiments are all trained by Train-L. Statistical results are\nshown in Tab. VI, in which baseline model (BL) refers to [33],\nfull models is the complete U-shape Transformer. In Tab. VI,\nour full model achieves the best quantitative performance on\nthe two testing dataset, which reï¬‚ects the effectiveness of the\ncombination of CMSFFT, SGFMT, MSG, and MCSL modules.\nAs in Fig .11, the enhancement result of the full model has\nthe highest PSNR and best visual quality. The results of\nBL+MSG have less noise and artifacts than the BL module\nbecause the MSG mechanism helps to reconstruct local details.\nThanks to the multi-color space loss function, the overall\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nInput\nReference\nUIBLA [16] Retinex [9] FUnIE [3]UDCP [13]\nUGAN [30] UIE-DAL [31] Ucolor [23] Ours\nCIEDE  16.03 CIEDE  13.21 CIEDE  14.29 CIEDE  18.85 CIEDE  12.76\nCIEDE  26.49 CIEDE  16.03 CIEDE  12.93 CIEDE  8.23\nFig. 10. Visual comparison of the color restoration performance evaluation. The input image is sampled from color-check7 dataset and itâ€™s taken by Fuji\nZ33. The values of CIEDE2000 metric for the regions of Color Checker are reported on the top-left corner of the images (the smaller, the better).\nInput BL\n BL+CMSFFTBL+MSG\nPSNR 15.73 PSNR 16.54 PSNR 17.52 PSNR 24.49\nFull Model\nBL+SGFMT BL+MCSL\n Ground truth\nPSNR 19.75 PSNR 19.21 PSNR 28.49\nFig. 11. Visual comparison of the ablation study sampled from the Test-L400\ndataset.\ncolor of BL+MCSLâ€™s result is close to the reference image.\nThe unevenly distributed visualization and artifacts in local\nareas of BL+MCSL are due to the lack of efï¬cient attention\nguidance. Although the enhanced results of BL+CMSFFT and\nBL+SGFMT are evenly distributed, the overall color is not\naccurate. The investigated four modules have their particular\nfunctionality in the enhancement process, which integration\ncould improve the overall performance of our network.\nV. C ONCLUSIONS\nIn this work, we released a large scale underwater image\n(LSUI) dataset, which contains 4279 real-world underwater\nimages with more abundant underwater scenes (water types,\nlighting conditions and target categories) than existing under-\nwater datasets [32], [28], [26], [22], and the corresponding\nclear images are generated as comparison references. We also\nprovide the semantic segmentation map and medium transmis-\nsion map for each raw underwater image. Besides, we reported\nan U-shape Transformer network for state-of-the-art UIE\nperformance. The networkâ€™s CMSFFT and SGFMT modules\ncould solve the inconsistent attenuation issue of underwater\nimages in different color channels and space regions, which\nhas not been considered among existing methods. Extensive\nexperiments validate the superior ability of the network to\nremove color artifacts and casts. Combined with the multi-\ncolor space loss function, the contrast and saturation of output\nimages are further improved. Nevertheless, it is impossible to\ncollect images of all the complicated scenes such as deep-\nocean low-light scenarios. Therefore, we will introduce other\ngeneral enhancement techniques such as low-light boosting\n[62] for future work.\nACKNOWLEDGMENT\nThis work was supported by the National Natural Sci-\nence Foundation of China (61991451, 61971045, 62131003),\nNational Key Research and Development Program of China\n(2020YFB0505601).\nREFERENCES\n[1] M. Yang, J. Hu, C. Li, G. Rohde, Y . Du, and K. Hu, â€œAn in-depth survey\nof underwater image enhancement and restoration,â€IEEE Access., vol. 7,\npp. 123 638â€“123 657, 2019. 1\n[2] P. Sahu, N. Gupta, and N. Sharma, â€œA survey on underwater image\nenhancement techniques,â€ IJCA, vol. 87, no. 13, 2014. 1\n[3] M. J. Islam, Y . Xia, and J. Sattar, â€œFast underwater image enhancement\nfor improved visual perception,â€ IEEE Robot. Autom. Lett., vol. 5, no. 2,\npp. 3227â€“3234, 2020. 1, 3, 7, 9, 10, 11\n[4] R. Schettini and S. Corchs, â€œUnderwater image processing: State of the\nart of restoration and image enhancement methods,â€ EURASIP . J. Adv.\nSignal Process., vol. 2010, pp. 1â€“14, 2010. 1\n[5] J. Korhonen and J. You, â€œPeak signal-to-noise ratio revisited: Is simple\nbeautiful?â€ in QoMEX. IEEE, 2012, pp. 37â€“38. 1, 7\n[6] C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert, â€œEnhancing under-\nwater images and videos by fusion,â€ in CVPR, 2012, pp. 81â€“88. 1, 2,\n3, 7, 9\n[7] C.-Y . Li, J.-C. Guo, R.-M. Cong, Y .-W. Pang, and B. Wang, â€œUnderwater\nimage enhancement by dehazing with minimum information loss and\nhistogram distribution prior,â€ IEEE T. Image Process., vol. 25, no. 12,\npp. 5664â€“5677, 2016. 1, 2\n[8] A. S. A. Ghani and N. A. M. Isa, â€œUnderwater image quality en-\nhancement through composition of dual-intensity images and rayleigh-\nstretching,â€ in ICCE, 2014, pp. 219â€“220. 1\n[9] X. Fu, P. Zhuang, Y . Huang, Y . Liao, X.-P. Zhang, and X. Ding, â€œA\nretinex-based enhancing approach for single underwater image,â€ inICIP,\n2014, pp. 4572â€“4576. 1, 2, 3, 7, 9, 10, 11\n[10] D. Huang, Y . Wang, W. Song, J. Sequeira, and S. Mavromatis, â€œShallow-\nwater image enhancement using relative global histogram stretching\nbased on adaptive parameter acquisition,â€ in MMM. Springer, 2018,\npp. 453â€“465. 1, 7, 9\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n[11] K. Iqbal, M. Odetayo, A. James, R. A. Salam, and A. Z. H. Talib,\nâ€œEnhancing the low quality images using unsupervised colour correction\nmethod,â€ in IEEE Int. Conf. Syst. Man. Cybern., 2010, pp. 1703â€“1709.\n1, 2\n[12] K. He, J. Sun, and X. Tang, â€œSingle image haze removal using dark\nchannel prior,â€ in CVPR, 2009, pp. 1956â€“1963. 1\n[13] P. Drews Jr, E. do Nascimento, F. Moraes, S. Botelho, and M. Cam-\npos, â€œTransmission estimation in underwater single images,â€ in ICCV\nworkshops, 2013, pp. 825â€“830. 1, 2, 3, 7, 9\n[14] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. Montene-\ngro Campos, â€œUnderwater depth estimation and image restoration based\non single images,â€ IEEE Comput. Graph. Appl., vol. 36, no. 2, pp. 24â€“\n35, 2016. 1, 3\n[15] Y . Wang, H. Liu, and L.-P. Chau, â€œSingle underwater image restoration\nusing adaptive attenuation-curve prior,â€ IEEE Trans. Circuits. Syst. I.\nRegul. Pap., vol. 65, no. 3, pp. 992â€“1002, 2018. 1, 2\n[16] Y .-T. Peng and P. C. Cosman, â€œUnderwater image restoration based on\nimage blurriness and light absorption,â€ IEEE T. Image Process., vol. 26,\nno. 4, pp. 1579â€“1594, 2017. 1, 2, 3, 7, 9, 10, 11\n[17] C.-Y . Li, J.-C. Guo, R.-M. Cong, Y .-W. Pang, and B. Wang, â€œUnderwater\nimage enhancement by dehazing with minimum information loss and\nhistogram distribution prior,â€ IEEE T. Image Process., vol. 25, no. 12,\npp. 5664â€“5677, 2016. 1, 3\n[18] J. Y . Chiang and Y .-C. Chen, â€œUnderwater image enhancement by\nwavelength compensation and dehazing,â€ IEEE T. Image Process. ,\nvol. 21, no. 4, pp. 1756â€“1769, 2012. 1, 2, 3\n[19] A. Galdran, D. Pardo, A. Pic Â´on, and A. Alvarez-Gila, â€œAutomatic red-\nchannel underwater image restoration,â€ JVCIR, vol. 26, pp. 132â€“145,\n2015. 1, 3\n[20] C. Li, J. Guo, S. Chen, Y . Tang, Y . Pang, and J. Wang, â€œUnderwater\nimage restoration based on minimum information loss principle and\noptical properties of underwater imaging,â€ in ICIP, 2016, pp. 1993â€“\n1997. 1, 2, 3\n[21] Y . Guo, H. Li, and P. Zhuang, â€œUnderwater image enhancement using\na multiscale dense generative adversarial network,â€ IEEE J. OCEANIC.\nENG., vol. 45, no. 3, pp. 862â€“870, 2019. 1\n[22] J. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson,\nâ€œWatergan: Unsupervised generative network to enable real-time color\ncorrection of monocular underwater images,â€ IEEE Robot. Autom. Lett.,\nvol. 3, no. 1, pp. 387â€“394, 2017. 1, 2, 3, 4, 12\n[23] C. Li, S. Anwar, J. Hou, R. Cong, C. Guo, and W. Ren, â€œUnderwater\nimage enhancement via medium transmission-guided multi-color space\nembedding,â€ IEEE T. Image Process., vol. 30, pp. 4985â€“5000, 2021. 1,\n2, 3, 7, 8, 9, 10, 11\n[24] C. Li, J. Guo, and C. Guo, â€œEmerging from water: Underwater image\ncolor correction based on weakly supervised color transfer,â€ IEEE\nSignal. Process. Lett., vol. 25, no. 3, pp. 323â€“327, 2018. 1, 2, 3\n[25] H.-Y . Yang, P.-Y . Chen, C.-C. Huang, Y .-Z. Zhuang, and Y .-H. Shiau,\nâ€œLow complexity underwater image enhancement based on dark channel\nprior,â€ in IBICA, 2011, pp. 17â€“20. 1, 3\n[26] D. Akkaynak and T. Treibitz, â€œSea-thru: A method for removing water\nfrom underwater images,â€ in CVPR, 2019, pp. 1682â€“1691. 1, 3, 4, 7,\n12\n[27] X. Fu, Z. Fan, M. Ling, Y . Huang, and X. Ding, â€œTwo-step approach for\nsingle underwater image enhancement,â€ in ISPACS, 2017, pp. 789â€“794.\n1, 3\n[28] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao, â€œAn\nunderwater image enhancement benchmark dataset and beyond,â€ IEEE\nT. Image Process., vol. 29, pp. 4376â€“4389, 2020. 1, 3, 4, 7, 8, 9, 10,\n11, 12\n[29] W. Song, Y . Wang, D. Huang, and D. Tjondronegoro, â€œA rapid scene\ndepth estimation model based on underwater light attenuation prior for\nunderwater image restoration,â€ in PCM. Springer, 2018, pp. 678â€“688.\n1, 3\n[30] C. Fabbri, M. J. Islam, and J. Sattar, â€œEnhancing underwater imagery\nusing generative adversarial networks,â€ ICRA, pp. 7159â€“7165, 2018. 1,\n3, 7, 8, 9, 10, 11\n[31] P. M. Uplavikar, Z. Wu, and Z. Wang, â€œAll-in-one underwater image\nenhancement using domain-adversarial learning.â€ in CVPR Workshops,\n2019, pp. 1â€“8. 1, 3, 7, 9, 10, 11\n[32] R. Liu, X. Fan, M. Zhu, M. Hou, and Z. Luo, â€œReal-world underwater\nenhancement: Challenges, benchmarks, and solutions under natural\nlight,â€ IEEE Trans. Circuits. Syst. Video Technol., vol. 30, pp. 4861â€“\n4875, 2020. 1, 3, 4, 12\n[33] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, â€œImage-to-image translation\nwith conditional adversarial networks,â€ in CVPR, 2017, pp. 1125â€“1134.\n2, 11\n[34] X. Li and A. Li, â€œAn improved image enhancement method based on\nlab color space retinex algorithm,â€ in ICGIP, C. Li, H. Yu, Z. Pan, and\nY . Pu, Eds., vol. 11069. SPIE, 2019, pp. 756 â€“ 765. 2\n[35] M. S. Hitam, E. A. Awalludin, W. N. Jawahir Hj Wan Yussof, and\nZ. Bachok, â€œMixture contrast limited adaptive histogram equalization\nfor underwater image enhancement,â€ in ICCAT, 2013, pp. 1â€“5. 2\n[36] S. Zhang, T. Wang, J. Dong, and H. Yu, â€œUnderwater image enhance-\nment via extended multi-scale retinex,â€ Neurocomputing, vol. 245, pp.\n1â€“9, 2017. 2\n[37] N. Carlevaris, Bianco, A. Mohan, and R. M. Eustice, â€œInitial results in\nunderwater single image dehazing,â€ in OCEANS, 2010, pp. 1â€“8. 2\n[38] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, â€œUnpaired image-to-image\ntranslation using cycle-consistent adversarial networks,â€ in ICCV, 2017,\npp. 2242â€“2251. 2, 3\n[39] M. Yang, K. Hu, Y . Du, Z. Wei, Z. Sheng, and J. Hu, â€œUnderwater image\nenhancement based on conditional generative adversarial network,â€\nSignal Process., Image Commun., vol. 81, p. 115723, 2020. 3\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nÅ. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in NIPS, 2017,\npp. 5998â€“6008. 3, 5\n[41] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\nâ€œAn image is worth 16x16 words: Transformers for image recognition\nat scale,â€ arXiv preprint arXiv:2010.11929, 2020. 3\n[42] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, â€œSwin transformer: Hierarchical vision transformer using shifted\nwindows,â€ arXiv preprint arXiv:2103.14030, 2021. 3\n[43] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,\nâ€œRestormer: Efï¬cient transformer for high-resolution image restoration,â€\n2021. 3, 4\n[44] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V . Koltun, â€œPoint transformer,â€\nin ICCV, October 2021, pp. 16 259â€“16 268. 3\n[45] R. Polikar, â€œEnsemble learning,â€ in Ensemble machine learning .\nSpringer, 2012, pp. 1â€“34. 3\n[46] Q. Qi, K. Li, H. Zheng, X. Gao, G. Hou, and K. Sun, â€œSguie-net:\nSemantic attention guided underwater image enhancement with multi-\nscale perception,â€ arXiv preprint arXiv:2201.02832, 2022. 3\n[47] Z. Ma and C. Oh, â€œA wavelet-based dual-stream network for underwater\nimage enhancement,â€ arXiv preprint arXiv:2202.08758, 2022. 3\n[48] K. Panetta, C. Gao, and S. Agaian, â€œHuman-visual-system-inspired\nunderwater image quality measures,â€IEEE J. Ocean. Eng., vol. 41, no. 3,\npp. 541â€“551, 2016. 4, 7\n[49] M. Yang and A. Sowmya, â€œAn underwater color image quality evaluation\nmetric,â€ IEEE T. Image Process., vol. 24, no. 12, pp. 6062â€“6071, 2015.\n4, 7\n[50] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\nâ€œSwinir: Image restoration using swin transformer,â€ inICCV Workshops,\nOctober 2021, pp. 1833â€“1844. 4\n[51] T. Ye, M. Jiang, Y . Zhang, L. Chen, E. Chen, P. Chen, and Z. Lu,\nâ€œPerceiving and modeling density is all you need for image dehazing,â€\n2021. 4\n[52] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, â€œUctransnet: Rethinking\nthe skip connections in u-net from a channel-wise perspective with\ntransformer,â€ 2021. 6\n[53] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, â€œAn image is worth 16x16 words: Trans-\nformers for image recognition at scale,â€ ArXiv, vol. abs/2010.11929,\n2021. 6\n[54] J. Johnson, A. Alahi, and L. Fei-Fei, â€œPerceptual losses for real-time\nstyle transfer and super-resolution,â€ in ECCV, 2016. 7\n[55] C. Li, S. Anwar, and F. Porikli, â€œUnderwater scene prior inspired deep\nunderwater image and video enhancement,â€Pattern Recognition, vol. 98,\np. 107038, 2020. 7\n[56] A. Hor Â´e and D. Ziou, â€œImage Quality Metrics: PSNR vs. SSIM,â€ in\nICPR, 2010, pp. 2366â€“2369. 7\n[57] D. Berman, D. Levy, S. Avidan, and T. Treibitz, â€œUnderwater single\nimage color restoration using haze-lines and a new quantitative dataset,â€\nIEEE T PATTERN ANAL, vol. 43, no. 8, pp. 2822â€“2837, 2021. 8, 10\n[58] A. Mittal, R. Soundararajan, and A. C. Bovik, â€œMaking a â€œcompletely\nblindâ€ image quality analyzer,â€IEEE Signal Process. Lett., vol. 20, no. 3,\npp. 209â€“212, 2013. 8\n[59] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional networks\nfor biomedical image segmentation,â€ in MICCAI. Springer, 2015, pp.\n234â€“241. 8\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[60] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and P. Bekaert, â€œColor\nbalance and fusion for underwater image enhancement,â€ IEEE Transac-\ntions on Image Processing, vol. 27, no. 1, pp. 379â€“393, 2018. 11\n[61] G. Sharma, W. Wu, and E. N. Dalal, â€œThe ciede2000 color-difference\nformula: Implementation notes, supplementary test data, and mathemat-\nical observations,â€ COLOR RES APPL, vol. 30, no. 1, pp. 21â€“30, 2005.\n11\n[62] C. Chen, Q. Chen, J. Xu, and V . Koltun, â€œLearning to see in the dark,â€\nin CVPR, 2018, pp. 3291â€“3300. 12"
}