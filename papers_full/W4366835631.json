{
  "title": "CancerGPT for few shot drug pair synergy prediction using large pretrained language models",
  "url": "https://openalex.org/W4366835631",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5005226621",
      "name": "Tianhao Li",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5087414287",
      "name": "Sandesh Shetty",
      "affiliations": [
        "Amherst College",
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5064738403",
      "name": "Advaith Kamath",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5034136464",
      "name": "Ajay Jaiswal",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5055458864",
      "name": "Xiaoqian Jiang",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A5047170063",
      "name": "Ying Ding",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A5100358135",
      "name": "Yejin Kim",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4365143687",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W2577784528",
    "https://openalex.org/W2963101867",
    "https://openalex.org/W2399140686",
    "https://openalex.org/W2950860419",
    "https://openalex.org/W2907272802",
    "https://openalex.org/W3102269903",
    "https://openalex.org/W2883248903",
    "https://openalex.org/W3164087725",
    "https://openalex.org/W4293214728",
    "https://openalex.org/W2775061087",
    "https://openalex.org/W3127930610",
    "https://openalex.org/W4312114349",
    "https://openalex.org/W3092034553",
    "https://openalex.org/W4307001389",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W2097746233",
    "https://openalex.org/W3119242039",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2922229918",
    "https://openalex.org/W4205719393",
    "https://openalex.org/W2981475131",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3014705052",
    "https://openalex.org/W4321448364",
    "https://openalex.org/W2950063908",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2960677646",
    "https://openalex.org/W2949524414",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2605336808",
    "https://openalex.org/W1696775091",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2102877084",
    "https://openalex.org/W2065847664",
    "https://openalex.org/W2803557794",
    "https://openalex.org/W2944885822",
    "https://openalex.org/W2596458707",
    "https://openalex.org/W4226212619",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4362471093",
    "https://openalex.org/W3123375411",
    "https://openalex.org/W4229068096",
    "https://openalex.org/W2223501982",
    "https://openalex.org/W3032045496",
    "https://openalex.org/W4206796410",
    "https://openalex.org/W3165195325"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01024-9\nCancerGPT for few shot drug pair synergy\nprediction using large pretrained\nlanguage models\nCheck for updates\nTianhao Li1,5, Sandesh Shetty2,5, Advaith Kamath3,A j a yJ a i s w a l1, Xiaoqian Jiang 4, Ying Ding1 &\nYejin Kim 4\nLarge language models (LLMs) have been shown to have signiﬁcant potential in few-shot learning\nacross variousﬁelds, even with minimal training data. However, their ability to generalize to unseen\ntasks in more complexﬁelds, such as biology and medicine has yet to be fully evaluated. LLMs can\noffer a promising alternative approach for biological inference, particularly in cases where structured\ndata and sample size are limited, by extracting prior knowledge from text corpora. Here we report our\nproposed few-shot learning approach, which uses LLMs to predict the synergy of drug pairs in rare\ntissues that lack structured data and features. Our experiments, which involved seven rare tissues\nfrom different cancer types, demonstrate that the LLM-based prediction model achieves signiﬁcant\naccuracy with very few or zero samples. Our proposed model, the CancerGPT (with ~ 124M\nparameters), is comparable to the largerﬁne-tuned GPT-3 model (with ~ 175B parameters). Our\nresearch contributes to tackling drug pair synergy prediction in rare tissues with limited data, and also\nadvancing the use of LLMs for biological and medical inference tasks.\nFoundation models have becomethe latest generation of artiﬁcial intelli-\ngence (AI)1. Instead of designing AI models that solve speciﬁct a s k so n ea ta\ntime, such foundation models or“generalist” models can be applied to many\ndownstream tasks without speciﬁc training. For example, large pre-trained\nlanguage models (LLMs), such as GPT-32 and GPT-43,h a v eb e e nag a m e\nchanger in foundation AI model4. An LLM can apply its skills to unfamiliar\ntasks for which it has never been trained, known as few-shot learning or\nzero-shot learning. This is due in partto multitask learning, which enables\nLLM to unintentionally gain knowledge from implicit tasks in its training\ncorpus\n5. Although LLMs have shown proﬁciency in few-shot learning in\nvarious ﬁelds2, including natural language processing, robotics, and com-\nputer vision2,6,7, their generalizability to unseen tasks in more complexﬁelds,\nsuch as biology, has yet to be fully tested. In order to infer unseen biological\nreactions, knowledge of participating entities (e.g., genes, cells) and\nunderlying biological mechanisms (e.g., pathways, genetic background,\ncellular environment) is required. While structured databases encode only a\nsmall portion of this knowledge, the vast majority is stored in free-text\nliterature, which can be used to train LLMs. Thus, we envision that when\nthere are limited structured data and limited sample sizes, LLMs can serve as\nan innovative approach for biological prediction tasks, by extracting prior\nknowledge from unstructured literature. One of such few-shot biological\nprediction tasks with a pressing need is a drug pair synergy prediction in\nunderstudied cancer types.\nDrug combination therapy has become a widely accepted strategy for\ntreating complex diseases such as cancer, infectious diseases, and neurolo-\ngical disorders\n8. In many cases, combination therapy can provide better\ntreatment outcomes than single-drug therapy. Predicting drug pair synergy\nhas become an important area of research in drug discovery and develop-\nment. Drug pair synergy refers to the enhancement of the therapeutic effects\nof two (or more) drugs when used together compared to when each drug is\nused alone. The prediction of drug pair synergy can be challenging due to a\nlarge number of possible combinations and the complexity of the underlying\nbiological mechanisms\n9. Several computational methods have been devel-\noped to predict drug pair synergy, particularly using machine learning.\nMachine learning models can be trained on large datasets of existing drug\npair’s experiment results to identify patterns and predict the likelihood of\nsynergy for a new drug pair. Early studies in this area have relied on rela-\ntional information or contextual information to extrapolate the synergy\n1School of Information, University of Texas at Austin, Austin, TX, USA.2Manning College of Information and Computer Sciences, University of Massachusetts\nAmherst, Amherst, MA, USA.3Department of Chemical Engineering, University of Texas at Austin, Austin, TX, USA.4McWilliams School of Biomedical Informatics,\nUniversity of Texas Health Science Center at Houston, Houston, TX, USA.5These authors contributed equally: Tianhao Li, Sandesh Shetty.\ne-mail: yejin.kim@uth.tmc.edu\nnpj Digital Medicine|            (2024) 7:40 1\n1234567890():,;\n1234567890():,;\nscore regarding a speciﬁc cell line to cell lines in other tissues10– 14,i g n o r i n g\nthe biological and cellular differences in these tissues. Another line of studies\nhas sought to overcome the discrepancy between tissues by utilizing diverse\nand high-dimensional features, including genomic (e.g., gene expression of\ncell lines) or chemical proﬁles (e.g., drug structure)13,15– 18. However, most of\nthe available data comes from common cancer types in certain tissues, such\nas breast and lung cancer; very limited experiment data are available on\ncertain tissues, such as bone and soft tissue (Fig.1). Obtaining cell lines from\nthese tissues can be physically difﬁcult and expensive, which limits the\nnumber of training data available for drug pair synergy prediction. This can\nmake it challenging to train machine learning models that rely on large\ndatasets. ref.18 used transfer learning to extend the prediction model trained\nin common tissues to some of the rare tissues with relatively rich data and\ncellular features. However, it cannot be utilized for rare tissues with extre-\nmely limited data and cellular information.\nIn this work, we aim to overcome the above challenge by LLMs. We\nhypothesize that cancer types with limited structured data and features still\nhave rich information in scientiﬁc literature. Manually extracting predictive\ninformation on such biological entities from literature is a complex task. Our\ninnovative approach is to leverage prior knowledge in scientiﬁcl i t e r a t u r e\nencoded in LLMs. We build a few-shot drug pair synergy prediction model\nthat leverages representation from LLM as a prior knowledge and predict\nthe outcome. Our experimental results demonstrate that our LLM-based\nfew-shot prediction model achieved signiﬁcant accuracy even in zero-shot\nsetting (i.e., no training data) and outperformed strong tabular prediction\nmodels in most cases. This remarkablefew-shot prediction performance in\none of the most challenging biological prediction tasks has a critical and\ntimely implication to a broad community of biomedicine because it shows a\nstrong promise in the“generalist” biomedical artiﬁcial intelligence\n1.\nResults\nResults overview\nWe developed CancerGPT, a few-shot drug pair synergy prediction model\nfor rare tissues. Leveraging LLMs-based tabular data prediction model19,w e\nconverted the prediction task into a natural language task, derived\nembedding from prior knowledge encoded in LLM’sp r e - t r a i n e dw e i g h t\nmatrices, and built classiﬁcation model for synergy prediction (Fig.2). We\npresented our strategy to adapt the LLM to our task with only a few shots of\ntraining data in each rare tissue usingk-shot ﬁne-tuning strategy (Fig.6).\nWe evaluated the accuracy of our proposed CancerGPT model, other\nLLM-based models (GPT-2, GPT-3, SciFive\n20), and general data-driven\nprediction models (XGBoost, Collaborative Filtering, TabTransformer)\n(Methods, Fig.3, 4, Supplementary Figs. 1, and 2, Supplementary Table 2-5)\nby the area under the precision-recall curve (AUPRC) and the area under\nthe receiver operating curve (AUROC) under the different settings. We\nconsidered different few-shot learning scenarios, where the model is pro-\nvided with a limited numberk of training data to learn from (k = 0 to 128).\nBy varying the number of shots, we can examine the model’s ability to adapt\nand generalize with minimal training data. Next, we compared the perfor-\nmance of CancerGPT and other LLM-based models across different tissues\nand model settings. We then investigated whether the LLM’s reasoning for\nits prediction is valid by checking its argument with scientiﬁc literature.\nAccuracy\nCancerGPT showed the highest accuracy in tissues such as the liver, soft\ntissue, and urinary tract. CancerGPT’s accuracy increases as the number of\ntraining shots increases (Figs.3, 4), indicating that the information gained\nfrom a few shots of data complements the prior knowledge encoded in LLM\nand the information in external data. In contrast, the two data-driven\nmodels, Collaborativeﬁltering and TabTransformer, showed higher accu-\nracy in some tissues, including endometrium, stomach, and bone, indicating\nthat the patterns learned from external data (common tissues) can be\nextrapolated to these tissues. Thediscrepancy in accuracy between Can-\ncerGPT (which utilizes prior knowledge and external data) and the data-\ndriven model can be attributed to the similarity of the external data’sd i s -\ntribution to the data of interest. Certain rare tissue cancer cell lines show\nunique characteristics in comparison to common tissues. These tissues have\nspeciﬁc cellular characteristics that are unique to their tissue of origin, which\nmay not be accurately predicted through training with common tissue. For\nexample, hepatic cell lines, which originate from liver tissue, have unique\nFig. 1 | Few-shot prediction in biomedicine. aDifferent from a task-speciﬁc\napproach, a large pre-trained language model can perform new tasks for which it has\nnot been explicitly trained.b Drug pair synergy prediction in rare tissues is an\nimportant example of the numerous few-shot prediction tasks in biomedicine.c A\nlarge pre-trained language model can be an innovative approach for few-shot pre-\ndiction in biomedicine, thanks to the prior knowledge encoded in its weight.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 2\nFig. 2 | Study workﬂow. We converted the tabular input (a) to natural text and\ncreated a task-speciﬁc prompt (b). The prompt was designed for the LLM to give a\nbinary answer (i.e.,“Positive”, “Not positive”), and the LLM’s embedding from the\nlast token was used for theﬁnal prediction (c, d). We evaluated and compared the\nprediction models with a different number of shots and tissues (e). We investigated\nthe LLM’s reasoning based on factual evidence. LLM Large Language Model.\nFig. 3 | AUPRC ofk-shot learning on seven tissue sets.We used 20% data as a test\nset in each rare tissue, while ensuring the binary labels were equally represented.\nAUPRC Area Under the Precision Recall Curve. Legend Orange: XGBoost, Gray:\nCollaborative Filtering, Yellow: TabTransformer, Light Blue: CancerGPT, Green:\nGPT-2, Dark Blue: SciFive, Red: GPT-3.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 3\ndrug response characteristics due to high expression of drug-metabolizing\nenzymes such as cytochrome P450s21. When the data from common tissue is\ndistributed differently to each rare tissue (i.e., external data is out-of-dis-\ntribution), CancerGPT outperforms the general data-driven prediction\nmodel trained with out-of-distribution data, thanks to general prior\nknowledge already encoded in the LLM parameters. On the other hand,\nTabTransformer and Collaborativeﬁltering, trained with external data,\nachieved the best accuracy in tissues such as endometrium, stomach, and\nbone. We further investigated the genomic characteristics of these tissues to\nascertain any commonalities with common tissues. Supplementary Figure 3\nshowed the lower dimensional projection of gene expression of cancer cell\nlines grouped by tissue types\n22.I nt h i sﬁgure, gene expression of tumorous\ncells from the endometrium (uterus), stomach (gastric), and bone were\nsimilar to that from other majority of tissues. Conversely, the gene\nexpression of tumorous cells from liver, soft tissue (including Ewing sar-\ncoma), and urinary tract formed unique clusters that were distinct from\no t h e rm a j o r i t yo ft i s s u e s .W h e nt h ed a t af r o mc o m m o nt i s s u e sa r ed i s -\ntributed similarly to each rare tissue (in-distribution), the data-driven pre-\ndiction model outperforms the few-shot LLM-based models. In all, the data-\ndriven model showed superior accuracy when in-distribution external data\nwas available, compared to all the LLM-based models, including Can-\ncerGPT. However, in the absence of external data, or when the external data\nis out-of-distribution, our customized LLM-based model achieved the best\naccuracy.\nWhen comparing the accuracy of LLM-based prediction models,\nCancerGPT emerged as the most accurate one. This superior performance is\nattributable to itsﬁne-tuning with external data, tailored speciﬁcally to the\ntask at hand. GPT-3 also proved tobe a competitive model, showing a\npotential to enhance its accuracy with an increasing number of shots. This\ncharacteristic renders it an optimal choice for datasets that permit the\nabundant additional training samples. Particularly, GPT-3 scored the\nhighest accuracy in pancreas tissue, inwhich only zero-shot tuning is pos-\nsible due to the limited sample size (Supplementary Tables 2 and 3). Despite\nSciFive’s specialized design for scientiﬁc literature and its substantial para-\nmeter size of 220M, it failed to consistently outperform GPT-2 across all\ndatasets. The SciFive’s lower accuracy despite the larger parameters and\nadaptation to scientiﬁc literature might be due to its difference in base LLM\n(T5 and GPT). The higher accuracy ofCancerGPT over GPT-2 underscores\nthe value of task-speciﬁc adjustments. These modiﬁcations can augment\naccuracy while preserving a model’s versatility. However, the advantages of\nsuch ﬁne-tuning may diminish with larger LLM models like GPT-3, par-\nticularly in scenarios demanding greater generalizability. Interestingly,\nCancerGPT, despite its smaller parameter size of 124M, outperformed\nGPT-3’s accuracy, which has a larger parameter size of 175B. This suggests\nthat furtherﬁne-tuning of GPT-3 may potentially yield even greater accu-\nracy, provided that in-distribution data is readily available.\nWe also compared the accuracy when the models areﬁne-tuned with\ndifferent strategies. Full training, which updates both LLM parameters and\nclassiﬁcation head during k-shot tuning, generally showed higher accuracy\nover last layer training (Supplementary Figs. 1 and 2), which only updates\nthe classiﬁcation head while freezing the LLM parameters. However, the\nmarginal increase in accuracy, despitethe extensive tuning of the millions or\neven billions of parameters was not signiﬁcantly high. This suggests that the\nlast layer representation of LLM already encapsulates a substantial amount\nof prior knowledge, effectively serving as a foundation for downstream tasks.\nThe observation that last layer training, which leaves the LLM’sb a c k b o n e\nuntouched, delivers accuracy comparable to that of a fully trained model,\nprovides an important insight. An LLM pre-trained with an extensive col-\nlection of scientiﬁcl i t e r a t u r ec a nﬂexibly enhance biomedical predictions via\ntransfer learning, which is similar to how a pre-trained model with\nImageNet\n23 can augment image analysis through transfer learning.\nFact check LLM’s reasoning\nWe evaluated whether the LLM can provide the biological reasoning behind\nits prediction. In this experiment, we used zero-shot GPT-3 because other\nﬁne-tuned LLM-based models (GPT-2, SciFive, CancerGPT) compromised\nits language generative performance during theﬁne-tuning and were not\nable to provide coherent responses. To do this, we randomly selected one\ntrue positive prediction and examined whether its biological rationale was\nbased on factual evidence or mere hallucination. Our example was the drug\npair AZD4877 and AZD1208 at cell line T24 for urinary tract tissue. We\nprompted the LLMs with“Could you provide details on why the drug1 and\ndrug2 are synergistic in the cell line for a given cancer type?” Details on\nprompt generation are discussed in Su p p l e m e n t a r yN o t e1 .W ee v a l u a t e d\nthe generated answer by comparing it with existing scientiﬁc literature. We\nfound that the LLM provided mostly accurate arguments, except for two\nFig. 4 | AUROC ofk-shot learning on seven tissue sets.We used 20% data as a test\nset in each rare tissue, while ensuring the binary labels were equally represented.\nAUROC Area Under the Receiver Operating Curve. Legend Orange: XGBoost, Gray:\nCollaborative Filtering, Yellow: TabTransformer, Light Blue: CancerGPT, Green:\nGPT-2, Dark Blue: SciFive, Red: GPT-3.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 4\ncases (Table1) in which no scientiﬁc literature exists. By combining these\nindividual scientiﬁc facts the LLM inferred the unseen synergistic effect.\nGenerally, drugs targeting non-overlapping proteins in similar pathways are\nmore likely to be synergistic24,25. In this case, both AZD4877 and AZD1208\ntarget similar pathways that inhibit tumor cell divisions without overlapping\nprotein targets. The Loewe synergy score of this pair at T24 was 46.82,\nmeaning strong positive synergistic effect.\nE x a m p l eo fp r e d i c t i o nr e s u l t s\nAs an example, we listed predicted synergistic drug pairs for stomach and\nsoft tissue using CancerGPT (Supplementary Table 6, 7) and for bone and\nliver tissue using GPT-3 (Supplementary Table 8, 9). We randomly selected\ntwo true positive, false positive, true negative, and false negative prediction\nexamples. We discovered that Loewe synergy scores of the true negative or\nfalse negative prediction examples were close to the threshold used to\ncategorize the label (i.e., Loewe score> 5). This suggests that accuracy may\nvary signiﬁcantly by different thresholds fordetermining positive synergy.\nSetting more extreme thresholds (e.g., >10, >30), like previous models\n13,17,18,\nmay increase the prediction accuracy.\nDiscussion\nOur study investigates the potential of LLMs as a widely applicable few-shot\nprediction model in theﬁeld of biomedicine. Speciﬁcally, we propose a new\nfew-shot model for predicting drug pair synergy, which can be used in rare\ntissues with few or no training samples available. We transformed tabular\ndata prediction into natural language tasks andﬁne-tuned LLMs (GPT-2,\nGPT-3, SciFive) with very few samples in each tissue. Our extensive\nexperiments with seven rare cancer tissues, seven different models, and two\ndifferentﬁnetuning strategies resulted in the following lessons learned:\n The data-driven models showed superior accuracy when in-\ndistribution external data was available. However, in the absence of\nexternal data, or when the external data is out-of-distribution, our\ncustomized LLM-based model, CancerGPT, achieved the best\naccuracy.\n The higher accuracy of CancerGPT over GPT-2 underscores the value\nof task-speciﬁc adjustments. These modi ﬁcations can augment\naccuracy while preserving a model ’s versatility. However, the\nadvantages of suchﬁne-tuning may diminish with larger LLM models\nlike GPT-3.\n CancerGPT, despite its smaller parameter size of 124M, outperformed\nGPT-3’s accuracy, which has a larger parameter size of 175B, sug-\ngesting that furtherﬁne-tuning of GPT-3 may potentially yield even\ngreater accuracy, provided that in-distribution data is readily available.\n Last layer training, which leaves the LLM’s backbone untouched,\ndelivered accuracy comparable to that of a fully trained model. An LLM\npre-trained with an extensive collection of scientiﬁc literature can\nﬂexibly enhance biomedical predictions using transfer learning based\non last-layer training.\nThe prediction of drug pair synergy in rare tissues serves as an excellent\nbenchmark task for evaluating LLMs in few-shot learning within theﬁeld of\nbiomedicine. This prediction requires incorporating multiple pieces of\ninformation, such as the drugs and thecell line, as well as the sensitivity of\ndrugs to the cell lines, to infer the synergistic effects. While detailed infor-\nmation on these entities can be found in scientiﬁc papers, the interaction\neffect, or synergistic effect, is primarily available through biological\nexperiments. To effectively assess LLMs’ inference capabilities, one must\nemploy a prediction task where the ground truth is not explicitly available in\ntext format but can be determined through alternative sources for model\nevaluation. Typically, drug pair synergy scores are obtained through high-\nthroughput testing facilities involving robot arms\n26. Therefore, individual\nrecords of the experiments are rarelyrecorded in academic literature,\ndecreasing the likelihood of their use as training data for LLMs and avoiding\ndata leakage. Additionally, few studies have been conducted on rare tissues\nregarding their synergy prediction models, and their synergy prediction\noutcomes are not explicitly stated in text format. A similar task for evalu-\nating LLMs in biomedicine is predicting the sensitivity of a single drug in a\ncell line; however, since the sensitivity of individual drugs is extensively\nresearched and well-documented in publications, the LLM model may\nmerely recollect from the text rather than infer unseen tasks.\nIt should be noted that it was not possible to compare our LLM-based\nmodels directly with previous predictions of drug pair synergy. The majority\nof previous models necessitates high-dimensional features of drugs and cells\n(e.g., genomic or chemical proﬁles), along with a substantial amount of\ntraining data, even the one speciﬁcally designed for rare tissue\n18.T h i sk i n do f\ndata is not easily accessible in rare tissues, making it challenging to carry out\nas i g n iﬁcant comparison. Our model is designed to address a common but\noften overlooked situation where we have limited features and data. Thus,\nwe compared the LLM-based models with other tabular models that share\nt h es a m es e to fi n p u t s .\nThe contribution of our study canbe summarized as follows. In the\narea of drug pair synergy prediction in rare tissues, our study is theﬁrst to\npredict drug pair synergy on tissues with very limited data and features,\nwhich other previous prediction models have neglected. This breakthrough\nhas signiﬁcant implications for drug development in these cancer types. By\naccurately predicting which drug pair will have a synergistic effect on these\ntissues, where cell line features are expensive to obtain, biologists can directly\nf o c u so nt h em o s tp r o b a b l ed r u gp a i r sa n dp e r f o r me x p e r i m e n t si nac o s t -\neffective manner.\nOur study also delivers generalizable insights about LLMs in the\nbroader context of biomedicine. Our research essentially bridges two dis-\ntinct methodologies: data-driven machine learning models, which rely on\nTable 1 | Example of generated answer when the LLM was asked to provide its reasoning for its prediction\nExcerpt of the generated answer Fact check and reference\n“The combination of AZD-4877 and AZD1208 has been studied in T24 cells...to be\nsynergistic in reducing bladder cancer cell growth and metastasis”\nFalse. No study conducted on this drug pair\n“The combination was also found to target multiple pathways involved in the\ngrowth and spread of bladder cancer cells.”\nTrue. AZD1208 is a PIM1 inhibitor. PIM1 is overexpressed in bladder cancer initiation\nand progression (42). AZD4877 is a drug designed to target bladder cancer (43).\n“...Speciﬁcally, AZD-4877 was found to inhibit the activation of proteins involved in\nthe promotion of tumor growth...”\nTrue. AZD4877 is a drug designed to target bladder cancer (43).\n“...AZD1208 was found to inhibit proteins associated with the inhibition of tumor\ngrowth.”\nTrue. AZD1208 inhibits the cell growth by suppressing p70S6K, 4EBP1 phosphor-\nylation, and messenger RNA translation (in acute myeloid leukemia) (44).\n“This combination was also effective at reducing the production of inﬂammatory\nmediators such as cytokines, which are known to contribute to tumor progression.”\nFalse. AZD1208 is a pan-PIM kinase inhibitor, and PIM kinases are downstream\neffectors of cytokine (45). However, AZD4877 has no evidence in reducing inﬂamma-\ntory mediators.\n“...these two drugs have been shown to reduce levels of apoptosis inhibitors, which\ncan also play a role in tumor progression.”\nTrue. AZD1208 induce cell apoptosis (46). AZD4877 is a inhibitor of Eg5, which pro-\nmotes cell apoptosis (47).\nItalic denotes the generated text from LLM.LLM Large Language Model.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 5\ninductive reasoning; and knowledge-driven inference models, which use\ndeductive reasoning. In many cases, especially when dealing with rare dis-\neases like certain types of cancer, the data required to build data-driven\nmodels is scarce. This scarcity renders these models virtually ineffective.\nConversely, knowledge-driven models pose their own challenges. Without\nextensive domain expertise, navigating the vast and intricate premises to\npredict biomedical outcomes is not only challenging, but also unscalable.\nOur approach combines the strengths of both models. We“automate”\ndeductive reasoning using a LLM, which then forms the foundation for a\nfew-shot prediction model. This combination of inductive (few-shotﬁne-\ntuning) and deductive (knowledge encapsulated in LLM) reasoning is a\nnovel concept, made possible through our LLM-based prediction model.\nFurthermore, this LLM-based few-shot prediction approach can be\napplied to a wide range of diseases beyond cancer, which are limited by the\nscarcity of available data. For instance, this approach can be used in infec-\ntious diseases, in which the prompt identiﬁcation of new treatments and\ndiagnostic tools is crucial. LLMs are able to help researchers quickly identify\npotential drug targets and biomarkers for these diseases, resulting in faster\nand more effective treatment development.\nThe present study, while aiming to showcase the potential of LLMs as a\nfew-shot prediction model in theﬁeld of biomedicine, is not without its\nlimitations. To fully establish the generalizability of LLMs as a“generalist”\nartiﬁcial intelligence, a wider range of biological prediction tasks must be\nundertaken for validation. Additionally, it is crucial to investigate how the\ninformation gleaned from LLMs complements the existing genomic or\nchemical features that are traditionally the primary source of predictive\ninformation. In future research, we plan to investigate this aspect and\ndevelop an ensemble method that effectively utilizes both existing structured\nfeatures and new prior knowledge encoded in LLMs.\nFurthermore, while we observed that GPT-3’s reasoning was similar to\nour own when fact-checking its argument with scientiﬁc literature in one\nexample, it is important to note that the accuracy of its arguments cannot\nalways be veriﬁed and may be susceptible to hallucination. It is also reported\nthat LLMs can also contain biases that humans have\n27. Therefore, further\nresearch is necessary to ensure that the LLM’s reasoning is grounded in\nfactual evidence. Despite these limitations, our study provides valuable\ninsights into the potential of LLM to be a few-shot prediction model in\nbiomedicine and lays the groundwork for future research in this area.\nIn conclusion, our study has effectively demonstrated the potential of\nLLMs in few-shot learning tasks within the complexﬁeld of biomedicine.\nOur study revealed that while data-driven models excel with sufﬁcient in-\ndistribution data, CancerGPT is superior in scenarios lacking such data. The\nFig. 5 | Model architecture.We built an LLM-based prediction model by adding a\nclassiﬁcation head to the LLM andﬁne-tuned it for the classiﬁcation task. To obtain\nrepresentation from LLM, we used GPT-2, GPT-3, and SciFive. We further tailored\nGPT-2 byﬁne-tuning it with a large amount of external data, in order to adjust GPT-\n2 in the context of drug pair synergy prediction (CancerGPT). Weﬁnetuned all\nmodels withk shots of data in each of the rare tissues (k-shot ﬁne-tuning strategy).\nWe compared accuracy of two differentﬁne-tuning strategies:ﬁne-tuning the entire\nmodel’s parameters versus freezing the LLM’s parameters and onlyﬁne-tuning the\nclassiﬁer’s parameters. LLM Large Language Model.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 6\neffectiveness of task-speciﬁc ﬁne-tuning was highlighted by the superior\nperformance of CancerGPT over GPT-2, demonstrating that such adjust-\nments can signiﬁcantly enhance accuracy without compromising the\nmodel’s ﬂexibility. Ourﬁndings not only are pivotal for the prediction of\ndrug pair synergy in rare tissues but also signify broader applicability for\nLLMs in biological inference tasks.\nMethods\nProblem formulation\nOur objective is to predict whether a drug pair in a certain cell line has a\nsynergistic effect, particularly focusing on rare tissues with limited training\nsamples. Given an input\nx ¼f d\n1; d2; c; t; ri1; ri2gð 1Þ\nof drug pair (d1, d2), cell linec, tissuet, and the sensitivity of the two drugs\nusing relative inhibition (ri1, ri2), the prediction model is\ny ≈ f ðxÞð 2Þ\nwhere y is the binary synergy class (1 if synergy > 5; 0 otherwise). Prior\nresearch14,17 has employed three different scenarios for predicting drug pair\nsynergy (random split, stratiﬁed by cell lines, stratiﬁed by drug combina-\ntions). Our task is to predict synergy when the data are stratiﬁed by tissue,\nwhich is a subset of cell lines.\nDistributions learned in one tissue may not generalizable well to other\ntissues with different cellular environments22. This biological difference\nposes a challenge in predicting drug pair’s synergy in tissues with a limited\nnumber of samples. The limited sample size of data makes it even more\ndifﬁcult to incorporate typical cell line features, such as gene expression\nlevel, which has large dimensionality (e.g., ~ 20,000 genes). Due to this data\nchallenge, the drug pair synergy predi c t i o nm o d e li st h e nr e d u c e dt ob u i l da\nprediction model with limited samples (few-shot or zero-shot learning) with\nonly limited tabular input feature types. Speciﬁc input features were\ndescribed in Methods.\nPrompt selection\nTo use a LLM for tabular data, the tabular input and prediction task must be\ntransformed into natural text. For each instance of tabular data (Fig.2), we\nconverted the structured features into text. A prior study\n19 investigated\ndifferent strategies to“serialize” the structured instance to natural language\ninput, such as List Template (a list of column names and feature values),\nText Template (a textual enumeration ofall features and values), and Table-\nTo-Text (table to text generation via LLM). Among them, we used the Text\nTemplate strategy which proved to be most effective. For example, given the\nfeature string (e.g.,“drug1”, “drug 2”, “cell line”, “tissue”, “sensitivity1”,\n“sensitivity2”) and its value (e.g.,“lonidamine”, “717906-29-1”, “A-673”,\n“bone”, “0.568”, “28.871”), we converted the instance as“The ﬁrst drug is\nAZD1775. The second drug is AZACITIDINE. The cell line is SF-295. Tissue is\nbone. Theﬁrst drug’s sensitivity using relative inhibition is 0.568. The second\ndrug’s sensitivity using relative inhibition is 28.871.” Other alternative ways\nto convert the tabular instance into the natural text are discussed in previous\npapers\n28,29.\nWe created a prompt that speciﬁes our tasks and guides the LLM to\ngenerate a label of interest. We experimented with multiple prompts. One\nexample of the prompts we created was“Determine cancer drug combina-\ntion synergy for the following drugs. Allowed synergies: Positive, Not positive.\n{Tabular Input}. Synergy:”. As our task is a binary classiﬁcation, we created\nthe prompt to only generate binary answers (“Positive”, “Not positive”).\nComparing these multiple prompts (Supplementary Note 1), theﬁnal\nprompt we used in this work was“Decide in a single word if the synergy of the\ndrug combination in the cell line is positive or not.{ Tabular Input}. Synergy:”.\nLLM-based prediction model\nWe built an LLM-based prediction model by adding a classiﬁcation head to\nthe LLM andﬁne-tuned it for the classiﬁcation task (Fig.5). This was done to\nenhance the model’s performance in the classiﬁcation task. The hidden\nrepresentation of theﬁnal token in the LLM output was utilized, as it\nencapsulates the information of all preceding tokens. This strategy not only\nallowed for a fair comparison with other baseline models, such as Tab-\nTransformer, but also created an opportunity toﬁne-tune the model spe-\nciﬁcally for binary classiﬁcation.\nWe also considered another modeling approach, which was to allow\nthe LLM to directly generate text for the binary response (“positive”, “not\npositive”). However, regardless of the LLM’s ability to encapsulate relevant\nknowledge within their embedding, some LLMs (e.g., GPT-2, SciFive) could\nnot ensure the response strictly adhered to one of the binary labels. This\nmade the inclusion of the classiﬁcation head an essential choice. The pri-\nmary objective of our study was to assess the LLM’s biomedical knowledge\nand its adaptability to a speciﬁc task of interest, rather than comparing\nlanguage generation capabilities.\nTo obtain representation from LLM, we used GPT-2, GPT-3, and\nSciFive (Fig.6). GPT-2 is a Transformer-based large language model, which\nwas pre-trained on a very large corpus of English data without human\nsupervision. It achieved state-of-the-art results on several language mod-\neling datasets in a zero-shot setting when it was released, and it is the\npredecessor of GPT-3 and GPT-4. GPT-2\n5 has several versions with dif-\nferent sizes of parameters, GPT-2, GPT-Medium, GPT-Large, and GPT-XL.\nWe used GPT-2 with the smallest number of parameters (regular GPT-2,\n124 million) in this work to make the model trainable on our server.\nFig. 6 | Training strategy of baseline and proposed LLM-based models.General data-driven models and CancerGPT wereﬁrst trained with samples from common tissues\n(cancer type) thenk-shot ﬁne-tuned with each tissue of interest. GPT-2, GPT-3 and SciFive are pre-trained models, and weﬁne-tuned them withk shots of data in each tissue.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 7\nGPT-32 is a Transformer-based autoregressive language model with\n175 billion parameters, and it achieved state-of-the-art performance on\nmany zero-shot and few-shot tasks upon its release. GPT-3.5, including\nChatGPT\n30,af a m o u sﬁne-tuned model from GPT-3.5, is an improved\nversion of GPT-3. However, the GPT-3 model and its parameters are not\npublicly available. Although the weight of the GPT-3 model is undisclosed,\nOpenAI offers an API\n31 to ﬁne-tune the model and evaluate its performance.\nWe utilized this API to build drug pairsynergy prediction models through\nk-shot ﬁne-tuning. There are four models provided by OpenAI forﬁne-\ntuning, Davinci, Curie, Babbage, and Ada, of which Ada is the fastest model\nand has comparable performance with larger models for classiﬁcation tasks.\nFor that reason, we use GPT-3 Ada as our classiﬁcation model. After\nuploading the train data, the API adjusted the learning rate, which is 0.05,\n0.1, or 0.2 multiplied by the original learning rate based on the size of the\ndata, andﬁne-tuned the model for four epochs. A model of the last epoch\nwas provided for further evaluation.\nIn contrast to general-purpose LLM(i.e., GPT-2 and GPT-3), SciFive\n20\nis an LLM specialized in the biomedicalﬁeld. Based on T5 model, which is a\ntext-to-text language model trained on the unﬁltered CommonCrawl\ndataset, SciFive re-trains the T5 model on various combinations of the\nCommonCrawl dataset, a corpus of PubMed abstracts, and a corpus of\nPubMed Central (PMC) full-text articles. We used the SciFive Pubmed Base\nmodel with 220M parameters.\nCancerGPT\nWe further tailored GPT-2 byﬁne-tuning it with a large amount of external\ndata (common cancer data), in order toadjust GPT-2 in the context of drug\npair synergy prediction. We named this model CancerGPT (Fig.5). The\ndata from common tissues (Data Availability)) are external data that do not\nnecessarily align with the context of the rare tissue being studied. However,\nthis data could potentially serve as a useful data source to“warm up” the\nLLM, thereby enhancing its ability to predict drug pair synergy. The\nintuition behind this“warming up” strategy is analogous to collaborative\nﬁltering\n32 that learn relational information and extrapolate it to the new\ncombination of input. Certain drug pairs exhibit synergy similarly regardless\nof the cellular context, and therefore, the relational information between\ndrug pairs in common tissues can be used to predict synergy in new cell lines\nin different tissues17. Subsequently, we utilized CancerGPT as one of the pre-\ntrained LLMs andﬁne-tuned to k shots of data in each rare tissue (as\ndiscussed in the following section).\nGeneral data-driven models\nThe LLM-based prediction are knowledge-driven modelsa st h e yu t i l i z e\nprior knowledge encoded in pre-trained weight. We compared them with\ngeneral data-driven prediction model. We speciﬁcally used XGBoost33,\nTabTransformer34, and collaborativeﬁltering18. XGBoost has been widely\nused in large-scale drug synergy data35,36. All the variables (drugs, cell lines,\nand sensitivities) were used as input to predict the drug pair synergy.\nTabTransformer is a self-attention-based supervised learning model for\ntabular data. TabTransformer appliesa sequence of multi-head attention-\nbased Transformer layers on parametric embedding to transform them into\ncontextual embedding, in which highly correlated features will be close to\neach other in the embedding space. Considering the highly correlated nature\nof drugs in our data, TabTransformer can be a very strong baseline in this\nwork. Weﬁrst converted the drugs and cell lines in the tabular data into\nindicators using one-hot coding. Weﬁrst trained an embedding layer on the\ndrugs and cell lines and passed them through stacked multi-headed atten-\ntion layers, which we then combined with the continuous variables (sensi-\ntivities). This combination then passes through feed-forward layers, which\nhave one layer of classiﬁcation head. Collaborativeﬁltering was used to\nextrapolate the relational information to the new combination of drugs and\ncell line. Drug combinations will react to a cell line similarly if these two\ndrugs have responded similarly to other cell line\n18. Note that, tissue infor-\nmation was not used in training because the models will be tested in one\nspeciﬁc rare tissue that is not used in training. No further contextual\ninformation can be inferred through the unseen tissue indicator.\nk-shot ﬁne-tuning strategy\nThe LLM-based models had different training andﬁne-tuning strategies\n(Fig.6). Samples of common tissues were split into 80% train data and 20%\nvalidation data for CancerGPT. The models were trained using train data\nand evaluated by validation datato determine the models with speciﬁc\nhyperparameters to be used for furtherﬁne-tuning on rare tissues. For the\nGPT-2 and GPT-3 based prediction models, we directly used pre-trained\nparameters from GPT-2\n5 using Huggingface’s Transformers library37 and\nGPT-3 Ada from OpenAI2 respectively.\nAll these models were thenﬁne-tuned withk shots of data in each of the\nrare tissues. For bone, urinary tract, stomach, soft tissue, and liver, we\nperformed experiments withk from [0, 2, 4, 8, 16, 32, 64, 128]. For endo-\nmetrium and pancreas, because of the limited number of data, we imple-\nmented experiments withk from [0, 2, 4, 8, 16, 32] from the endometrium,\nand only zero shot (k = 0) for the pancreas.\nWith the limited number of shots, a careful balance of binary labels in\nthe train and test set was critical. We partitioned the data into 80% for\ntraining and 20% for testing in each rare tissue, while ensuring the binary\nlabels were equally represented in both sets. We randomly selectedk shots\nfrom the training forﬁne-tuning, while maintaining consistency with pre-\nviously selected shots and adding new ones. Speciﬁcally, we maintained the\npreviously selectedk shots in the training set and incremented additionalk\nshots to create 2 ×k shots. The binary label distribution in eachk shot set\nfollowed that of the original data, withat least one positive and one negative\nsample included in each set. For evaluation stability, the test data was\nconsistent across different shots for each tissue.\nTo investigate whether the LLMs inherently possesses the capability to\npredict the label only with minimal tuning, we compared accuracy of two\ndifferent ﬁne-tuning strategies:ﬁne-tuning the entire model (LLM’sp a r a -\nmeter and classiﬁer) versus freezing the LLM’s parameter andﬁne-tuning\nonly on the classiﬁer. Note that for zero shot tuning, we measured accuracy\nby allowing the LLM to directly generate a binary answer without appending\nac l a s s iﬁcation head because we are unable to optimize the parameters in the\nclassiﬁcation head.\nHyperparameter setting\nThe LLM’s last layer hidden representation size was 768 (same as GPT-2),\nand we used left padding to ensure that the last token was from the prompt\nsentence. The cross-entropy loss wasused to optimize the model during the\nﬁne-tuning process. All the LLM models used the tabular input that was\nc o n v e r t e dt on a t u r a lt e x ta n ds h a r e dt h es a m ep r o m p t .\nThe predicted output was a binary label indicating the presence of a\nsynergistic effect, with a Loewe score greater than 5 indicating a positive\nresult. We used AUROC and AUPRC to evaluate the accuracy of classiﬁ-\ncation. Regression tasks were not possible in our LLM-based models because\nour model can only generate text-based answers (“positive” or “not posi-\ntive”), with poor precision in accurately quantifying the synergy value.\nXGBoost was used with a boosting learning rate of 0.3. The number of\ngradient boost trees was set to 1000 with a maximum tree depth of 20 for\nbase learners. TabTransformer was used with a learning rate of 0.0001 and a\nweight decay of 0.01. The model was trained for 50 epochs on common\ntissues. During the training, the modelwith the best validation performance\nwas selected for furtherﬁne-tuned on rare tissues. For eachk shot in each\ntissue, the model wasﬁne-tuned using the same learning rate and weight\ndecay for 1 epoch and tested with AUPRC and AUROC.\nCancerGPT wasﬁrst ﬁne-tuned with pre-trained regular GPT-2 for 4\nepochs on common tissues. The learning rate was set to be 5e-5 and weight\ndecay was set to be 0.01. Then the model wasﬁne-tuned fork shots in rare\ntissues. The same hyperparametersare used in training. The model was\nﬁnally tested with AUPRC and AUROC. Note that, due to an imbalance in\npositive and non-positive labels,we reported both AUPRC and AUROC.\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 8\nGPT-2 and GPT-3 are directlyﬁne-tuned on rare tissues with pre-\ntrained parameters from regular GPT-2 and GPT-3 Ada. For eachk shot in\neach tissue, GPT-2 isﬁne-tuned for 4 epochs using a learning rate of 5e-5\nand a weight decay of 0.01. The hyperparameters of GPT-3 are adjusted by\nOpenAI API based on the data size. The model was alsoﬁne-tuned for 4\nepochs. GPT-2, GPT-3 and SciFiveﬁne-tuned models wereﬁnally tested\nwith AUPRC and AUROC. Details in the hyperparameter setting are dis-\ncussed in Supplementary Note 2.\nReporting summary\nFurther information on research design is available in the Nature Research\nReporting Summary linked to this article.\nData availability\nWe utilized a publicly accessible extensive database of drug synergy from\nDrugComb Portal9, which is an open-access data portal where the results of\ndrug combination screening studies for alarge variety of cancer cell lines are\naccumulated, standardized, and harmonized. The database contains both\ndrug sensitivity rows and drug pair synergy rows. Afterﬁltering the available\ndrug pair synergy rows, the data contains 4226 unique drugs, 288 cell lines,\nwith a total of 718,002 drug pair synergy rows. We employed the Loewe\nsynergy score, which ranges from -100 (antagonistic effect) to 75 (strong\nsynergistic effect), for drug combination synergy\n38. The Loewe synergy score\nquantiﬁes the excess over the expected response if the two drugs are the same\ncompound39,40. In this paper, we focused on cell lines from rare tissues. We\ndeﬁned the rare tissues as the ones with less than 4000 samples, which\ninclude the pancreas (n = 39), endometrium (n = 68), liver (n = 213), soft\ntissue (n = 352), stomach (n = 1190), urinary tract (n = 2458), and bone (n =\n3985). We tested our models with each ofthe rare tissues. The remaining\ntissues, namely hematopoietic and lym p h o i d ,l u n g ,s k i n ,o v a r y ,k i d n e y ,\ncolon, brain, breast, and prostate were regarded as common tissues.\nCode availability\nAll models used in this paper were developed using open-sourced libraries\nsuch as PyTorch\n41, HuggingFace37 and publically available APIs from\nOpenAI31. Method-speciﬁcc o d ew i l lb ep r o v i d e du p o nr e q u e s t .\nReceived: 19 May 2023; Accepted: 2 February 2024;\nReferences\n1. Moor, M. et al. Foundation models for generalist medical artiﬁcial\nintelligence. Nature 616, 259– 265 (2023).\n2. Brown, T.B. et al. Language Models are Few-Shot Learners. Preprint\nat https://arxiv.org/abs/2005.14165 (2020).\n3. OpenAI: GPT-4 Technical Report. Preprint athttps://arxiv.org/abs/\n2303.08774 (2023).\n4. Mitchell, M. & Krakauer, D. C. The debate over understanding in AI’s\nlarge language models.Proc. Natl. Acad. Sci.120,\n2215907120 (2023).\n5. Radford, A. et al. Language Models are Unsupervised Multitask\nLearners. Preprint athttps://d4mucfpksywv.cloudfront.net/better-\nlanguage-models/language-models.pdf (2018).\n6. Veit, A. et al. Learning from noisy large-scale datasets with minimal\nsupervision. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 839– 847 (2017).\n7. Wertheimer, H. Few-Shot learning with localization in realistic\nsettings. In: 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), vol. 0, pp. 6551– 6560 (2019).\n8. Sun, W., Sanderson, P. E. & Zheng, W. Drug combination therapy\nincreases successful drug repositioning.Drug Discov. Today21,\n1189– 1195 (2016).\n9. Zagidullin, B. et al. DrugComb: an integrative cancer drug\ncombination data portal.Nucleic Acids Res.47,4 3– 51 (2019).\n10. Chen, H., & Li, J. DrugCom: Synergistic Discovery of Drug\nCombinations Using Tensor Decomposition. In2018 IEEE\nInternational Conference on Data Mining (ICDM), 899– 904 (2018).\n11. Sun, Z., Huang, S., Jiang, P. & Hu, P. DTF: Deep tensor factorization\nfor predicting anticancer drug synergy.Bioinformatics 36,\n4483– 4489 (2020).\n12. Li, H., Li, T., Quang, D. & Guan, Y. Network propagation predicts drug\nsynergy in cancers.Cancer Res.78, 5446– 5457 (2018).\n13. Kuru, H. I., Tastan, O. & Cicek, A. E. MatchMaker: A deep learning\nframework for drug synergy prediction.IEEE/ACM Trans. Comput.\nBiol. Bioinform.19, 2334– 2344 (2022).\n14. Liu, X. et al. Multi-way relation-enhanced hypergraph representation\nlearning for anti-cancer drug synergy prediction.Bioinformatics 38,\n4782– 4789 (2022).\n15. Preuer, K. et al. DeepSynergy: predicting anti-cancer drug synergy\nwith Deep Learning.Bioinformatics 34, 1538– 1546 (2018).\n16. Liu, Q. & Xie, L. TranSynergy: Mechanism-driven interpretable deep\nneural network for the synergistic prediction and pathway\ndeconvolution of drug combinations.PLoS Comput. Biol.17,\n1008653 (2021).\n17. Hosseini, S.-R. & Zhou, X. CCSynergy: an integrative deep-learning\nframework enabling context-aware prediction of anti-cancer drug\nsynergy. Brief. Bioinform.24, bbac588 (2023).\n18. Kim, Y. et al. Anticancer drug synergy prediction in understudied\ntissues using transfer learning.J. Am. Med. Inform. Assoc.28,\n42– 51 (2021).\n19. Hegselmann, S. et al. TabLLM: Few-shot classiﬁcation of tabular data\nwith large language models.AISTATS abs/2210.10723, (2022).\n20. Phan, L. N. et al. SciFive: a text-to-text transformer model for\nbiomedical literature. Preprint athttps://arxiv.org/abs/2106.03598\n(2021).\n21. Guo, L. et al. Similarities and differences in the expression of drug-\nmetabolizing enzymes between human hepatic cell lines and primary\nhuman hepatocytes.Drug Metab. Dispos.39, 528– 538 (2011).\n22. Warren, A. et al. Global computational alignment of tumor and cell line\ntranscriptional proﬁles. Nature Communications12, 22 (2021).\n23. Deng, J. et al. ImageNet: A large-scale hierarchical image database. In\nIEEE Conference on Computer Vision and Pattern Recognition,\n248– 255 (2009).\n24. Cheng, F., Kovács, I. A. & Barabási, A.-L. Network-based prediction of\ndrug combinations.Nat. Commun.10, 1197 (2019).\n25. Tang, Y.-C. & Gottlieb, A. SynPathy: Predicting drug synergy through\nDrug-Associated pathways using deep learning.Mol. Cancer Res.20,\n762– 769 (2022).\n26. He, L. et al. Methods for high-throughput drug combination screening\nand synergy scoring. In: Cancer Systems Biology: Methods and\nProtocols, pp. 351– 398 (2018).\n27. Schramowski, P., Turan, C., Andersen, N., Rothkopf, C. A. & Kersting,\nK. Large pre-trained language models contain human-like biases of\nwhat is right and wrong to do.Nat. Mach. Intell.4, 258– 268 (2022).\n28. Li, Y., Li, J., Suhara, Y., Doan, A. & Tan, W.-C. Deep entity matching\nwith pre-trained language models.Proc. VLDB Endowment14,\n50– 60 (2020).\n29. Narayan, A., Chami, I., Orr, L. & Ré, C. Can Foundation Models\nWrangle Your Data?Proc VLDB Endow16, 738– 746 (2022).\n30. OpenAI. Introducing ChatGPT.https://openai.com/blog/chatgpt(2022).\n31. OpenAI. Fine-Tuning - OpenAI API.https://platform.openai.com/\ndocs/guides/ﬁne-tuning (2021).\n32. Suphavilai, C., Bertrand, D. & Nagarajan, N. Predicting cancer drug\nresponse using a recommender system.Bioinformatics\n34,\n3907– 3914 (2018).\n33. Chen, T. & Guestrin, C. XGBoost: A Scalable Tree Boosting System. In\nProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining785– 794 (2016).\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 9\n34. Huang, X., Khetan, A., Cvitkovic, M., & Karnin, Z. TabTransformer:\nTabular Data Modeling Using Contextual Embeddings. Preprint at\nhttps://arxiv.org/abs/2012.06678 (2020).\n35. Sidorov, P., Naulaerts, S., Ariey-Bonnet, J., Pasquier, E. & Ballester, P.\nJ. Predicting synergism of cancer drug combinations using NCI-\nALMANAC data.Front Chem. 7, 509 (2019).\n36. Celebi, R., Bear Don’t Walk IV, O., Movva, R., Alpsoy, S. & Dumontier,\nM. In-silico prediction of synergistic Anti-Cancer drug combinations\nusing multi-omics data.Sci. Rep.9, 8949 (2019).\n37. Wolf, T. et al. Transformers: State-of-the-art natural language\nprocessing. InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations,\n38– 45 (2020).\n38. Greco, W. R., Bravo, G. & Parsons, J. C. The search for synergy: a\ncritical review from a response surface perspective.Pharmacol. Rev.\n47, 331– 385 (1995).\n39. Ianevski, A., He, L., Aittokallio, T. & Tang, J. SynergyFinder: a web\napplication for analyzing drug combination dose-response matrix\ndata. Bioinformatics 33, 2413– 2415 (2017).\n40. Yadav, B., Wennerberg, K., Aittokallio, T. & Tang, J. Searching for\nDrug Synergy in Complex Dose-Response Landscapes Using an\nInteraction Potency Model.Comput. Struct. Biotechnol. J.13,\n504– 513 (2015).\n41. Paszke, A. et al. PyTorch: An Imperative Style, High-Performance\nDeep Learning Library. Preprint athttps://arxiv.org/abs/1912.01703\n(2019).\n42. Guo, S. et al. Overexpression of pim-1 in bladder cancer.J. Exp. Clin.\nCancer Res.29, 161 (2010).\n43. Jones, R. et al. Phase II study to assess the efﬁcacy, safety and\ntolerability of the mitotic spindle kinesin inhibitor AZD4877 in patients\nwith recurrent advanced urothelial cancer.Invest New Drugs31,\n1001– 1007 (2013).\n44. Cortes, J. et al. Phase I studies of AZD1208, a proviral integration\nmoloney virus kinase inhibitor in solid and haematological cancers.Br.\nJ. Cancer118, 1425– 1433 (2018).\n45. National Cancer Institute. NCI Drug Dictionary.https://www.cancer.\ngov/publications/dictionaries/cancer-drug/def/pan-pim-kinase-\ninhibitor-azd1208 (2011).\n46. Cervantes-Gomez, F. et al. PIM kinase inhibitor, AZD1208, inhibits\nprotein translation and induces autophagy in primary chronic\nlymphocytic leukemia cells.Oncotarget 10, 2793– 2809 (2019).\n47. Borthakur, G. et al. Clinical, pharmacokinetic (PK), and\npharmacodynamic ﬁndings from a phase I trial of an eg5 inhibitor\n(AZD4877) in patients with refractory acute myeloid leukemia (AML).\nJ. Clin. Orthodont.27, 3580– 3580 (2009).\nAcknowledgements\nXJ is CPRIT Scholar in Cancer Research (RR180012), and he was supported\nin part by Christopher Saroﬁm Family Professorship, UT Stars award,\nUTHealth startup, the National Institute of Health (NIH) under award number\nR01AG066749, R01AG066749-03S1, R01LM013712 and U01TR002062,\nand the National Science Foundation (NSF) #2124789. YD is supported by\nNSF AI Center Grant (NSF 2019844), NSF-CSIRO (NSF 2303038), NIH\nBridge2AI (OTA-21-008), and Bill & Lewis Suit Professorship at the Uni-\nversity of Texas at Austin. YK is supported by the NIH under award number\nR01AG082721, R01AG066749-03S1, and CPRIT RR180012.\nAuthor contributions\nThe ﬁrst two authors contributed equally. X.J. and Y.D. provided the\nmotivation of the study. T.L. and S.S. contributed in the development of\nmodels, hyperparameter tuning, data processing and manuscript writing.\nA.K. collected chemical features from the various drugs. Y.K. led in writing\nof the manuscript and study design, and is the corresponding author.\nA.J., X.J., Y.D. and Y.K. provided feedback during the study and for the\nmanuscript. Y.K. and Y.D. provided supervision and led design study\nduring the project.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-024-01024-9.\nCorrespondenceand requests for materials should be addressed to\nYejin Kim.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s41746-024-01024-9 Article\nnpj Digital Medicine|            (2024) 7:40 10",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.7376473546028137
    },
    {
      "name": "Computer science",
      "score": 0.6129759550094604
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5933172702789307
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5373102426528931
    },
    {
      "name": "Machine learning",
      "score": 0.48549965023994446
    },
    {
      "name": "Drug target",
      "score": 0.43944498896598816
    },
    {
      "name": "Sample (material)",
      "score": 0.4308527708053589
    },
    {
      "name": "Language model",
      "score": 0.4115399122238159
    },
    {
      "name": "Natural language processing",
      "score": 0.32073861360549927
    },
    {
      "name": "Biology",
      "score": 0.16244879364967346
    },
    {
      "name": "Pharmacology",
      "score": 0.09750804305076599
    },
    {
      "name": "Chemistry",
      "score": 0.08086511492729187
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I177605424",
      "name": "Amherst College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    }
  ],
  "cited_by": 78
}