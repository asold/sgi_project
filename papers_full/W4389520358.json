{
  "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
  "url": "https://openalex.org/W4389520358",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101459729",
      "name": "Michael Hanna",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5051184573",
      "name": "Yonatan Belinkov",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5007142536",
      "name": "Sandro Pezzelle",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W4297412003",
    "https://openalex.org/W2812894063",
    "https://openalex.org/W2802897578",
    "https://openalex.org/W4389070894",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4210751035",
    "https://openalex.org/W3166464178",
    "https://openalex.org/W2165256085",
    "https://openalex.org/W3117445666",
    "https://openalex.org/W2250600805",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W3033254023",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W2977268464",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W749192265",
    "https://openalex.org/W2119728020",
    "https://openalex.org/W2095755534",
    "https://openalex.org/W4317838060",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1972506408",
    "https://openalex.org/W2166362343",
    "https://openalex.org/W1991453585",
    "https://openalex.org/W2149707582",
    "https://openalex.org/W3102485638",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W2782110456",
    "https://openalex.org/W4296564631",
    "https://openalex.org/W4286988498",
    "https://openalex.org/W1976854129",
    "https://openalex.org/W2091966899",
    "https://openalex.org/W3184030040",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385573296",
    "https://openalex.org/W2017433125",
    "https://openalex.org/W2022111034",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W173641964",
    "https://openalex.org/W4256205310",
    "https://openalex.org/W2085821487",
    "https://openalex.org/W3112784227",
    "https://openalex.org/W2115792525",
    "https://openalex.org/W1608050331",
    "https://openalex.org/W2953407643",
    "https://openalex.org/W4229005866"
  ],
  "abstract": "Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives. This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about animacy. We ask: how does this impact LMs’ animacy processing—do they still behave as humans do? We answer this question using open-source LMs. Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical. However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans. Even when the context indicating atypical animacy is very short, LMs pick up on subtle clues and change their behavior. We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12120–12135\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWhen Language Models Fall in Love:\nAnimacy Processing in Transformer Language Models\nMichael Hanna\nILLC\nUniversity of Amsterdam\nm.w.hanna@uva.nl\nYonatan Belinkov\nTechnion—IIT, Israel\nbelinkov@technion.ac.il\nSandro Pezzelle\nILLC\nUniversity of Amsterdam\ns.pezzelle@uva.nl\nAbstract\nAnimacy—whether an entity is alive and sen-\ntient—is fundamental to cognitive processing,\nimpacting areas such as memory, vision, and\nlanguage. However, animacy is not always\nexpressed directly in language: in English it\noften manifests indirectly, in the form of se-\nlectional constraints on verbs and adjectives.\nThis poses a potential issue for transformer lan-\nguage models (LMs): they often train only on\ntext, and thus lack access to extralinguistic in-\nformation from which humans learn about an-\nimacy. We ask: how does this impact LMs’\nanimacy processing—do they still behave as\nhumans do? We answer this question using\nopen-source LMs. Like previous studies, we\nfind that LMs behave much like humans when\npresented with entities whose animacy is typ-\nical. However, we also show that even when\npresented with stories about atypically animate\nentities, such as a peanut in love, LMs adapt:\nthey treat these entities as animate, though they\ndo not adapt as well as humans. Even when\nthe context indicating atypical animacy is very\nshort, LMs pick up on subtle clues and change\ntheir behavior. We conclude that despite the\nlimited signal through which LMs can learn\nabout animacy, they are indeed sensitive to the\nrelevant lexical semantic nuances available in\nEnglish.\n1 Introduction\nAnimacy plays a significant role in cognitive pro-\ncessing, as evidenced by the fact that animate enti-\nties are easier to remember and prioritized in visual\nprocessing (Nairne et al., 2013; New et al., 2007;\nBugaiska et al., 2019). It is so important that even\nyoung children can distinguish animate and inani-\nmate entities (Rakison and Poulin-Dubois, 2001),\nand these are processed in distinct domain-specific\nbrain regions (Caramazza and Shelton, 1998).\nAnimacy distinctions also manifest in language;\nhowever these distinctions may appear indirectly.\nWhile some languages explicitly mark animacy, an-\nimacy distinctions in English often take the form of\nselectional constraints that limit the use of certain\nverbs or adjectives with in/animate entities. For ex-\nample, only animate entities can walk or think. So,\nwhile animacy is a rich distinction at the cognitive\nlevel, at the linguistic level, its signal can be muted.\nToday’s pre-trained transformer language mod-\nels (LMs), however, are trained only on linguistic\ninput. If they are to learn to process animacy, they\nmust thus do so only from its downstream effects\nin text, unlike humans, who use visual and physical\nstimuli. We therefore ask: do such LMs respond to\nanimacy in language as humans do?\nWe answer this by treating LMs as psycholin-\nguistic test subjects, probing how they react to vio-\nlations of animacy-related selectional constraints.\nLike prior work (Warstadt et al., 2020; Kauf et al.,\n2022), we first study LMs’ responses in scenarios\ninvolving typical animacy. In such situations, ani-\nmacy is a simple mapping between an object (e.g.a\npeanut) and its usual animacy (inanimate). We find\nthat like humans, LMs generally prefer sentences\nthat respect animacy-related selectional constraints,\nassigning higher probabilities to such sentences.\nUnlike prior work, we also study atypical ani-\nmacy (Coll Ardanuy et al., 2020), where a typically\ninanimate object becomes animate. We draw on\nNieuwland and van Berkum (2006), which mea-\nsured human N400 responses in scenarios with\natypically animate entities like a peanut in love .\nWe compare LM surprisal to human N400 brain\nresponses and find that like humans, LMs are ini-\ntially surprised to encounter entities like a peanut\nin love, but quickly adapt, becoming less surprised.\nStronger LMs are more able to replicate the large\nmagnitude of human N400 reduction.\nGiven LMs’ success at adapting to atypical an-\nimacy with a long context, we test them on short\nsentences about atypically animate entities, and\nmeasure the extent to which their outputs reflect\nthis atypical animacy. We find that even with lim-\n12120\nited context, LMs adapt their output distribution,\ntreating the entity as animate. We conclude that,\ndespite training without the modalities that hu-\nmans use to learn about animacy, LMs respond\nto shifting animacy in a surprisingly human-like\nway. Code for our experiments is available at\nhttps://github.com/hannamw/lms-in-love.\n2 Related Work\n2.1 Animacy in Language\nAnimacy in cognition is often framed as a gradient\nphenomenon (de Swart and de Hoop, 2018). In\nlanguage, this often simplifies to a tripartite hier-\narchy (humans > animals > objects) or a binary\n(humans & animals > objects); entities are distin-\nguished synactically or morphologically by their\nposition therein (Comrie, 1989).\nAnimacy exists at both the type level (e.g. dogs >\nrocks) and the token level (e.g. a specific rock in a\nstory might be animate, though rocks are typically\nnot). Moreover, linguistic animacy is based not\nonly on biology, but also on the speaker’s closeness\nand empathy with the entity in question (Kuno and\nKaburaki, 1977); thus a speaker might treat their\ndog as more animate than an unknown dog.\nThe precise effects of animacy in language vary\ncross-linguistically, from explicit animacy marking\nto more indirect effects as in English. The latter\ninclude not only strict animacy-based selectional\nconstraints (Caplan et al., 1994), but also more sub-\ntle grammatical influences (Rosenbach, 2008; Bres-\nnan and Hay, 2008). For example, animate entities\nare more often mentioned first in a sentence, even\nif doing so produces less common constructions,\nsuch as the passive (Ferreira, 1994).\nHere, we focus on the human / inanimate ob-\nject dichotomy, and the animacy-based selectional\nconstraints thereby imposed; this strong contrast\nshould produce easier-to-measure effects in LMs.\n2.2 LMs as Test Subjects\nWe study the behavior of LMs by treating them as\npsycholinguistic test subjects, a popular approach.\nOne such line of work analyzes LMs by using the\nprobability they assign to a sentence as a proxy for\nacceptability judgments. Generally, such studies\nprovide pairs of sentences, one acceptable and one\nnot; LMs must assign the more plausible sentence\na higher probability. This method has been used\nto study LMs’ processing of negation, subject-verb\nagreement, and more (Ettinger, 2020; Linzen et al.,\n2016; Warstadt et al., 2020; Sinclair et al., 2022).\nOther work compares LMs to humans by using\nsurprisal—the negative log probability of a string—\nto estimate measures of cognitive effort during text\nprocessing. LM surprisal is versatile, and well-\ncorrelated with reading times, eye-tracking fixa-\ntions, and EEG responses (Smith and Levy, 2013;\nAurnhammer and Frank, 2018; Michaelov and\nBergen, 2020); moreover, surprisal from stronger\nLMs provides better predictive power (Goodkind\nand Bicknell, 2018; Wilcox et al., 2020).\nWe use LM surprisal to predict the N400 brain\nresponse, which is elevated at semantically unusual\ncontent, like animacy-related selectional constraint\nviolations. Studies have found that a word’s sur-\nprisal correlates with human N400 response thereto\n(Frank et al., 2013, 2015; Michaelov et al., 2022);\ntransformer LMs are the state of the art for this\n(Merkx and Frank, 2021; Michaelov et al., 2021).\n2.3 Animacy Detection\nWe note that our interest in the processing of\natypical animacy parallels similar developments\nin the NLP task of animacy detection: determining\nwhether a given entity is animate. While many ani-\nmacy detection studies originally considered only\ntypical animacy (Orasan and Evans, 2007; Bow-\nman and Chopra, 2012), later animacy detection\nwork has recognized that entities’ animacy may not\nalways be typical, and may change over the course\nof a narrative (Karsdorp et al., 2015; Jahan et al.,\n2018). Particularly relevant for the present study,\nColl Ardanuy et al. (2020) combine LMs and atyp-\nical animacy by using BERT for atypical animacy\ndetection. Although this approach uses LMs as\na tool to label animacy, rather than studying how\nLMs process animacy, we share their interest in the\natypical edge cases of animacy.\n2.4 Animacy in LMs\nHow neural models capture animacy has long inter-\nested cognitive scientists; Elman (1990) trained a\nsimple neural LM on artificial language data, and\nfound that its representations of animate and inani-\nmate entities formed distinct clusters. More recent\nwork has assessed the animacy-processing capabil-\nities of modern LMs, mostly focusing on typical\nanimacy. Animacy is one area tested by BLiMP\n(Warstadt et al., 2020), which we revisit in Sec-\ntion 4. Kauf et al. (2022) investigate animacy as\npart of LMs’ generalized event knowledge; they\nalso find that LMs are sensitive to (typical) ani-\n12121\nmacy as it pertains to selectional constraints.\nWe move beyond typical animacy to atypical\nanimacy by using LM surprisal to replicate Nieuw-\nland and van Berkum’s (2006) studies on human\nN400 response to atypical animacy. Contempora-\nneous work (Michaelov et al., 2023) replicates one\nof these experiments in the original Dutch. In con-\ntrast, we replicate all experiments from Nieuwland\nand van Berkum (and Boudewyn et al. (2019)).\nThese highlight situations in which models can\ncapture general trends, but fail to capture low-level\nnuances. Moreover, by studying a diverse set of\nEnglish LMs, we can identify how LMs’ strength\naffects their predictive power.\n3 Models\nWe experiment with these models: GPT-2 small,\nmedium, large, and XL (Radford et al., 2019); OPT\n2.7B, 6.7B, and 13B (Zhang et al., 2022); and\nLLaMA 7B, 13B and 30B (Touvron et al., 2023).1\nWe use autoregressive LMs, as we need to com-\npute probabilities for whole sentences. Moreover,\nwe choose open-source models, to make our work\nreplicable. We provide implementation details in\nAppendix A.\n4 Typical Animacy\nWe test models’ responses to animacy in situations\nwhere the animacy of a given token, or instance\nof entity, aligns with the animacy of its type more\ngenerally (e.g. cats are animate; rocks are not).\nExperiment We test the models in Section 3\non the animate-transitive and animate-passive\ndatasets of the BLiMP benchmark (Warstadt et al.,\n2020). Each dataset contains 1,000 minimal pairs\nof synthetic English sentences that differ only by\none or two words (Table 1). By construction, one\nsentence respects animacy constraints; the other vi-\nolates them. We evaluate models on these datasets\nby computing the probability it assigns to each\nsentence of each minimal pair. A model gets an\nexample correct if it assigns higher probability to\nthe sentence that respects the animacy constraint.\nWe compute model accuracy over each dataset.\nResults Figure 1 displays results for each model.\nIt also includes human baselines, reported directly\nfrom Warstadt et al. (2020), which indicate the\nproportion of examples where annotators preferred\n1The names of OPT and LLaMA models indicate (approxi-\nmate) parameter counts; the GPT-2 models have 117M, 345M,\n762M, and 1.5B parameters respectively.\nAcc? Sentence\nT Yes Naomi had cleaned a fork.\nT No That bookhad cleaned a fork.\nP Yes Lisa was kissed by the boys.\nP No Lisa was kissed by the blouses.\nTable 1: BLiMP examples: we provide one example\neach from the Transitive and Passive datasets. Each is a\nminimal pair of sentences: one Acceptable and one not.\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy Performance on BLiMP Animacy Datasets\nHuman Transitive Accuracy\nHuman Passive Accuracy\nTransitive Accuracy\nPassive Accuracy\nFigure 1: Model accuracy on BLiMP. Models match\nhumans in the transitive, but not passive setting.\nthe acceptable sentence of the given minimal pair.\nRandom performance for both datasets is 50%.\nModels attain high performance in both scenar-\nios. On transitive examples, they reach over 80%\naccuracy; some models prefer the sentence that\nrespects animacy constraints more often than hu-\nmans do (>87%). In the passive scenario, the gap\nbetween models (80%) and humans (86%) is wider.\nThis difference between the transitive and pas-\nsive cases may be due more to setup differences\nthan distinct animacy processing in the two scenar-\nios. In the passive case, the target word is always\nin the last position, so model performance is deter-\nmined only by the target’s probability. In contrast,\nthe target word is not the final token in the transi-\ntive case, so model success is determined by the\nprobability of a longer string.\nDiscussion Our results indicate that models re-\nspect animacy constraints in typical scenarios: they\nmatch human performance on the transitive dataset,\nand are close behind on the passive. However, this\ntest cannot distinguish between a model that truly\nunderstands animacy, and one that just associates\nwords (types) with other words that reflect that\nword’s typical animacy. For example, the model\nmight simply associate a word like “shoe” with\n12122\nverbs that take inanimate objects, without under-\nstanding that the inanimacy of an individual shoe is\nwhat prohibits its use with animate-selecting verbs.\nTo solve this problem, our analysis must move\nbeyond type-level animacy, and test models’ pro-\ncessing of animacy at the token level. We thus test\nmodels’ responses to entities whose token-level ani-\nmacy is atypical, distinct from their usual type-level\nanimacy. If models process these entities accord-\ning to their type-level animacy, their understanding\nof animacy is rather shallow. In contrast, models\nthat process entities according to their token-level\nanimacy may better understand animacy in full.\n5 Atypical Animacy\nIn this section, we attempt to determine if LMs can\ncapture animacy not only at the type-level, but also\nat the token-level. We do so by comparing model\nand human responses in cases of atypical animacy,\nwhere entities’ canonical type-level animacy and\ntheir actual token-level animacy differ.\nFor human data, we turn to two similar studies—\nNieuwland and van Berkum (2006) and Boudewyn\net al. (2019)—that relied on the N400, a brain re-\nsponse measured via EEG that is elevated when pro-\ncessing semantically anomalous input. Both stud-\nies measured participants’ N400 responses while\nthey read stories where a typically inanimate en-\ntity acted as animate (Figure 2), similar in tone\nand content to a cartoon, or fairy-tale. Both found\nthat while participants were initially surprised by\nthe atypically animate entity, they quickly adapted,\nyielding low N400 responses to the entity.\nWe ask if the same is true of pre-trained LMs:\ncan they adapt to entities that are animate at the\ntoken-level, despite being typically inanimate? Or\nis their processing of animacy limited to a simple\ntype-level understanding? To answer this question,\nwe replicate these studies with pre-trained LMs,\nusing their surprisal to model N400 responses.\nWe replicate three experiments: Nieuwland and\nvan Berkum’s repetition experiment; their con-\ntext experiment; and Boudewyn et al.’s adapta-\ntion experiment.2 For each, we first explain the\noriginal study. Then, we explain how we adapt the\nexperiment for LMs. Finally, we report our results\nand compare them the original study’s results.\nA nurse was talking to the sailor/oar [1]who had been\nin a violent boating accident. The sailor/oar cried for a\nlong time over the storm that had raged over the lake for\nhours. The nurse consoled the sailor/oar [3], saying that\nhe would soon be well again. The sailor/oar complained\nof a bad headache that would not go away. The nurse gave\nthe sailor/oar [5]a large dose of aspirin. The sailor/oar\nthanked her and fell asleep.\nFigure 2: Story from Nieuwland and van Berkum, repe-\ntition experiment (translated and edited). Times when\nN400 responses were recorded are numbered, in bold.\n5.1 Repetition Experiment\nOriginal Study In Nieuwland and van Berkum’s\nfirst experiment, participants listened to Dutch sto-\nries that contained either a typical, animate entity\nor an inanimate entity behaving as if it were ani-\nmate (Figure 2). Participants’ N400 responses were\nmeasured at the 1st, 3rd, and 5th mentions of the\nentity (in Figure 2, either oar or sailor, in bold).\nNieuwland and van Berkum found that partic-\nipants had a moderate N400 response to the first\nmention of a typically animate entity, and a low\nresponse on subsequent mentions. In contrast, par-\nticipants initially had a high N400 response to the\natypically animate entity. However, by the 3rd and\n5th mentions thereof, their N400 responses were\nso low as to be statistically indistinguishable from\nthe responses to mentions of the animate entity\nin the same contexts. Thus, while humans were\ninitially surprised by the atypically animate entity,\nthey quickly adapted to the situation, and found it\nno more surprising than typically animate entities.\nOur Experiment We model N400 responses\nwith LM surprisal, as discussed in Section 2.2.\nFor each of the 60 examples, we measure the sur-\nprisal of the animate and inanimate entity given the\ncontext at each timestep. For example, to model\nthe inanimate N400 response at T1 in the exam-\nple from Figure 2 given a model pθ, we compute\n−log2 pθ(oar|A nurse was talking to the). Then,\nwe compute the mean surprisal of examples con-\ntaining animate and inanimate entities separately.\nSince the original stimuli are in Dutch, we trans-\nlate them to English, to make them compatible with\nthe English LMs.3 We do so using DeepL;4 transla-\n2In Appendix C, we replicate Boudewyn et al.’s English\nversion of Nieuwland and van Berkum’s context experiment\nwith LMs; our results are identical to those of Section 5.2.\n3We perform these experiments in Dutch in Appendix B.\nDutch results are comparable to English results.\n4https://www.deepl.com/translator\n12123\nt1t3t5t1t3t5t1t3t5t1t3t5t1t3t5t1t3t5t1t3t5t1t3t5t1t3t5t1t3t5\nGPT-2\nSmall\nGPT-2\nMed\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\n0\n5\n10\n15\n20\nInanimate Surprisal Animate Surprisal\nRepetition Experiment Surprisals\nModel and Timestep\nSurprisal\n(bits)\nFigure 3: Mean repetition experiment surprisal. Inan-\nimate surprisal is initially higher, but both surprisals\ndecrease rapidly after T1, becoming near identical.\n1 3 5\nTimestep\nGPT-2 Small\nGPT-2 Medium\nGPT-2 Large\nGPT-2 XL\nOPT-2.7B\nOPT-6.7B\nOPT-13B\nLLaMA-7B\nLLaMA-13B\nLLaMA-30B\nModel\n0.00 0.00 0.00\n0.00 0.00 0.00\n0.00 0.00 0.01\n0.00 0.00 0.01\n0.00 0.00 0.14\n0.00 0.06 0.67\n0.00 0.00 0.04\n0.00 0.14 0.93\n0.00 0.04 0.75\n0.00 0.16 0.33\n0.0\n0.2\n0.4\n0.6\n0.8\np-value\nFigure 4: Stat. significance of the difference between\nanimate and inanimate surprisal, by model and timestep\ntions were checked by a native Dutch speaker. We\nthen manually post-edited each stimulus to ensure\nit matched the cartoon-like tone and content of the\noriginal, and contained inanimate characters that\nviolated typical animacy constraints in the 1st, 3rd,\nand 5th sentences of the stories.5 Because we pre-\nserve the relevant aspects of the stimuli we expect\nthe trends in N400 responses to be the same.\nResults All models capture broad trends in hu-\nman N400 responses well (Figure 3). At T1, mod-\nels are very surprised by the inanimate entity, and\nonly moderately surprised by the animate entity. At\nlater timesteps, however, both entities’ surprisals\ndrop precipitously, to similar levels: models adapt\nto both entities quickly, just like humans do.\nStill, the raw results do not prove that models\n5We also edited stories for fluency, and to convert Dutch\ncultural references to Anglophone counterparts. The trans-\nlated English stimuli can be found at https://github.com/\nhannamw/lms-in-love\nA girl sat next to a diamond who was always doing strange\nthings. The diamond told her that he liked to eat erasers. The\ngirl ignored the diamond and his stories. Then the diamond\nsaid he also liked to sing songs. The diamond was quite\nfoolish/valuable but secretly also very funny. That’s why\nshe always sat next to him.\nFigure 5: Story from Nieuwland and van Berkum, con-\ntext experiment (translated and edited). N400 responses\nwere recorded at the words in bold.\nare adapting to the extent that humans are. Since\nNieuwland and van Berkum found that human\nN400 responses in the two conditions were statisti-\ncally indistinguishable by T3, we test if the same\nis true for model surprisal. We use the Wilcoxon\nsigned-rank test for non-normally distributed data\n(Wilcoxon, 1945) to determine if model surprisals\nfor animate and inanimate entities are distinct at\neach timestep. We find (Figure 4) that like humans,\nLMs have a statistically significant difference be-\ntween animate and inanimate surprisals at T1. How-\never, while there was no difference in humans at T3,\nthere are differences (p <0.01) in most models;\nonly the largest exhibit none. At T5, differences\ndisappear in yet more large models. While models\ncan generally approximate trends in human N400\nresponses to atypical animacy, only the largest and\nmost powerful fully replicate human adaptation.\nOverall, pre-trained LMs seem able to mimic\nhuman-like adaptation to atypically animate enti-\nties. It is tempting to conclude that they have a\nhuman-like understanding of animacy, that works\nat the token rather than the type level. However, it\nis equally possible that their decreased surprisal is\ndue to repetition, rather than a deeper understand-\ning of animacy. Transformer LMs even have a\nlow-level emergent structure, induction heads, ded-\nicated to such copy-pasting (Olsson et al., 2022).\nFortunately, Nieuwland and van Berkum shared\nthis concern: humans might generate lower N400\nresponses only because they had seen the atypically\nanimate token before. Thus, we also replicate their\ncontext experiment, which avoids this issue.\n5.2 Context Experiment\nNieuwland and van Berkum’s context experiment\nshowed that participants’ low N400 responses did\nnot stem from lexical repetition.\nOriginal Study As in the prior experiment, par-\nticipants read 60 Dutch stories containing an atyp-\nically animate entity; at the end of each story, the\nentity was described using an adjective that was\n12124\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nSurprisal (bits) Context Experiment Surprisals\nAnimate Surprisal\nInanimate Surprisal\nAnimate Baseline\nInanimate Baseline\nFigure 6: Context experiment surprisals. With context,\nthe animate adjective is much less surprising; in the\ncontextless baseline condition, this is reversed.\neither context-appropriate (and generally used for\nanimate entities) or context-inappropriate (but typi-\ncal for the inanimate entity; Figure 5). The N400\nresponse was measured at the adjective at the end\nof the story (foolish or valuable). N400 responses\nfor the context-appropriate animate adjective were\nfar lower than those to the entity-appropriate inani-\nmate adjective, showing that the first experiment’s\neffects were not caused by lexical repetition.\nOur Experiment We calculate the surprisal at\nthe animate and inanimate adjective for each of the\n60 stories. We also compute baseline surprisals,\nthe surprisal of the inanimate adjective without the\nentire story context, to show they are indeed high:\ne.g. −log2 pθ(foolish|The diamond was quite).\nResults For all models, mean surprisal of the\nanimate adjective is much lower than that of the\ninanimate adjective (Figure 6). This is significant\nin all cases (p <0.01; Wilcoxon signed-rank test).\nMoreover, the animate adjective is assigned higher\nprobability in almost all cases—over 90% for large\nmodels. This mirrors the human trend: the N400 re-\nsponse to the context-appropriate animate adjective\nwas much lower than the response to the entity-\nappropriate inanimate adjective. In the contextless\nbaseline situation, the inanimate adjective receives\na much lower surprisal than the animate adjective.\nLike humans, models use context and overcome\ntheir lexical knowledge regarding the traits can ap-\nply to animate and inanimate entities.\n5.3 Adaptation Experiment\nWe now replicate Boudewyn et al.’s adaptation ex-\nperiment, which combines the strengths of both\nA lucky fellow/peanut had a big smile [1]on his face. The\nfellow/peanut was amazed [2]about his good fortune. Just\nnow he had won the jackpot of two million dollars. The\nfellow/peanut was elated/salted and who could blame him.\nFigure 7: Story from Boudewyn et al. (2019). N400\nresponses were recorded at the words in bold.\nt1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2\nGPT-2\nSmall\nGPT-2\nMed\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\n5\n10\n15\nInanimate Surprisal Animate Surprisal\nAdaptation Experiment Surprisals\nModel and Timestep\nSurprisal\n(bits)\nFigure 8: Adaptation experiment surprisals. Inanimate\nsurprisal starts higher; the gap shrinks for larger LMs.\nprior experiments. Like the first, it captures adapta-\ntion over time; like the second, it avoids the poten-\ntial issues of repetition.\nOriginal Study Boudewyn et al.’s adaptation ex-\nperiment parallels the two previous experiments.\nParticipants listened to 120 English-language sto-\nries containing either a typically or atypically an-\nimate entity (Figure 7). Participants’ N400 re-\nsponses were measured at the first content verb of\nthe first two sentences of each story. These verbs\nsignal that their subject is (perhaps atypically) an-\nimate, although they are notably not the same in\neach sentence. Findings mirrored those of Nieuw-\nland and van Berkum: there was a sharp drop in\nN400 response at between the two timesteps in the\ninanimate scenario.\nOur Experiment For each of the 120 stories, we\ncalculate the surprisal at the two critical verbs, in\nthe animate and inanimate case.\nResults The results of the adaptation experiment\n(Figure 8) might appear starkly different from those\nof the repetition experiment. As before, surprisal\nat the critical word drops in the inanimate case,\nthough only slightly. But in the animate case, sur-\nprisal remains constant or increases.\nThese results are in fact consistent with our ear-\nlier findings. The inanimate surprisal drop indicates\nthat the short context sufficed to convince models\nof the entity’s animacy. Moreover, the fact that sur-\nprisal does not drop in the animate case suggests\nthat models are reacting specifically to the contex-\n12125\ntual cues that the inanimate entity is animate, as\nopposed to the context more generally. The im-\nportance of model size is also consistent: stronger\nmodels have a smaller gap between animate and\ninanimate surprisals at T2. Although this gap is\nstill significant for current models, trends indicate\nthat stronger models may eliminate it.\n5.4 Discussion\nAcross experiments, models replicate broad trends\nin human N400 responses. Stronger models repli-\ncate human results better, with lower surprisals at\ninanimate entities. Do they thus process animacy\nmore like humans? We caution that low surprisals\nare not always desirable for cognitive modeling, as\nsurprisal from LMs can underestimate processing\ndifficulty in terms of reading time (van Schijndel\nand Linzen, 2021; Arehalli et al., 2022; Oh and\nSchuler, 2023); this may be because even relatively\nsmall LMs can predict next words as well as hu-\nmans (Goldstein et al., 2022). Stronger LMs may\nonly be better models of animacy processing in this\nsituation because lower surprisals are desirable.\nRegardless of the effects of model size, another\nquestion remains: do these positive results indicate\nthat these LMs understand animacy? We cannot\nbe certain: there exist mechanisms by which trans-\nformer LMs could perform well without any deep\nunderstanding of animacy. In the repetition exper-\niment, models could use a copying mechanism to\nreduce their surprisal at repeated entities. In the\ncontext and even adaptation experiment, models\ncould rely on the context, while ignoring the inani-\nmate entity. This is a real concern: Michaelov et al.\n(2023) construct a (simple) model that does this.\nIn both cases, context is the complicating factor:\nLMs might exploit shallow context cues to sim-\nulate animacy processing effects, without having\nany real internal model of animacy. To investigate\nthis question further, we study LMs’ reactions to\natypically animate entities in a low-context setting.\n6 Low-Context Atypical Animacy\nThe previous experiments have shown that LMs\ncan adapt in scenarios with atypically animate enti-\nties; however, LMs could have exploited shallow\ncontext features to do so, without any specific un-\nderstanding or representation of animacy. We now\ninvestigate the extent to which LMs can leverage\ncues in the context by testing their behavior on very\nshort sentences exhibiting atypical animacy.\nDataset We craft a set of short incomplete sen-\ntences that describe an atypically animate entity\n(Table 2). The sentences are designed to elicit a\ncritical next word—an adjective or a verb—that\nindicates if the LM treats the entity as animate. For\nexample, if an LM continues “The boat snored and\nstarted to” with the verb “dream” this indicates that\nthe boat is animate; continuing with “sink” does\nnot. Unlike in prior experiments, these sentences\nprovide only one clue indicating atypical animacy.\nWe create this dataset by defining prompts and\nfilling them with nouns and verbs we sample from\na predefined set. We sample from 181 nouns that\nhumans rated as not very animate, but highly con-\ncrete; non-concrete inanimate nouns (e.g. “fear”)\ncannot become animate, except metaphorically. We\nuse concreteness ratings from Wilson (1988), and\nanimacy ratings from VanArsdall and Blunt (2022).\nFor the verbs, we use a manually-filtered set of\n191 verbs that imply their subject is animate, from\nJi and Liang (2018). Each verb implies that its sub-\nject is animate for either psychological or physical\nreasons; e.g. think is psychological while walk is\nphysical. Each verb co-occurs with human subjects\nat a high, high-mid, or mid frequency. We create\na dataset of 10,000 items (Table 2) by sampling\nprompts, nouns, and verbs.6\nExperiment We run all LMs on the dataset. For\neach example, we evaluate whether the LM treats\nthe entity in that example as animate by compar-\ning the LM’s output distribution to reference dis-\ntributions. If our original sentence is O =“The\nchair spoke and began to”, our inanimate reference\nis I =“The chair began to”, while our animate\nreference is A =“The [human] began to”, with\na human entity randomly sampled from person,\nman, woman, boy, girl, and child. We indicate via\nDKL(A||O) the divergence between the next-word\ndistributions given A as context, and given O as\ncontext, with other KL divergences defined analo-\ngously. We focus in particular on DKL(A||O) as\nanimacy divergence; lower animacy divergence\nimplies a more “animate” distribution.\nQuantitative Analysis Figure 9 shows KL diver-\ngences between the atypically animate sentence\n(O) and the reference distributions ( A/I). For\nall models, the divergence between the inanimate\n6A list of prompts, nouns and verbs is in Appendix F.\nFull dataset is available at https://github.com/hannamw/\nlms-in-love. For further experiments that vary properties of\nthis dataset, see Appendix D.\n12126\nSentence Rank #1 #2 #3 #4 #5\nThe kilt commented and started to 1 walk get move laugh say\nThe cart noticed and was very 3 angry ups excited interested nerv\nThe dime retired and was very 9994 rare valuable scar popular collect\nThe telephone sighed and began to 10000 ring v bu speak be\nTable 2: Dataset examples and their top-5 continuations (sometimes partial words). The example at rank n has the\nnth lowest animacy divergence (of 10,000). Low-divergence examples have animate continuations; high-divergence\nones are stereotypical and inanimate. The top two examples use psychological verbs; the bottom two, physical.\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0\n1\n2\n3\n4\n5\n6\n7\n8KL Divergence\n×10□5 Mean KL Divergences by Model\nDKL(A||O)\nDKL(I||O)\nDKL(A||I)\nFigure 9: KL Divergence between atypically animate\n(O) and animate (A) / inanimate (I) references. Error\nbars (95% CI) are marked, but extremely small. The\nlower the bar, the more similar the distributions.\nand animate references (purple) is the highest. Al-\nthough only the added verb separates the atypi-\ncally animate sentence from the inanimate refer-\nence, this leads its divergence with the animate\nreference (red) to be consistently lower; that is,\nadding the verb significantly increased the distri-\nbution’s animacy. For OPT models, the animacy\ndivergence is the lowest of their three divergences:\nthe atypically animate distribution is even more\nsimilar to the animate distribution (red) than inani-\nmate distribution (blue). This trend holds true for\nLLaMA models as well, though only weakly. Still,\nall model behavior clearly shifts with the addition\nof the animacy-implying verb.\nTo understand the cause of this shift, we analyze\nthe data with respect to the known factors that vary\nbetween our prompts, to discern which affected\nmodel behavior. Results were similar across mod-\nels, so we display results for one model, LLaMA-7B.\nWe first analyze the effect of our prompts, focus-\ning on the difference between those that elicit verbs\n(“and began to. . . ”) and those that elicit adjectives\n(“and was very. . . ”). We find (Figure 10, A) that\nverb-eliciting prompts produce lower animacy di-\nvergences than those that elicit adjectives.\nWe then analyze the effects of verbs and nouns\non the sentence. For each verb or noun, we calcu-\nlate the mean animacy divergence of sentences con-\ntaining it. We observe a wide spread per verb and\nper noun (Figure 10, D and E), suggesting that they\nboth impact model behavior. We then study the\nfactors affecting individual nouns’ and verbs’ diver-\ngences. We find that psychologically animate verbs\nhave somewhat lower divergence than physically\nanimate verbs; psychologically animate verbs pro-\nduce more animate behavior from LMs (p <0.01,\nT-test; Figure 10 B). Sentences with verbs that had\nhigher co-occurrence with humans had a lower di-\nvergence than those with a lower co-occurrence\n(p <0.01; Figure 10 C),7 though the effect size is\nrather small. For the nouns, however, neither ani-\nmacy nor concreteness explains trends in animacy\ndivergence.\nQualitative Analysis We also qualitatively ver-\nify that sentences with low animacy divergence\nhave more animate continuations than those with\nhigh divergence. We sort examples by divergence\nand examine their top-5 continuations, focusing on\nexamples at the top and bottom of the list.\nTable 2 shows that animacy divergence aligns\nwell with the qualitative animacy of continuations.\nLow-divergence examples have entities thatlaugh,\nor are angry. High-divergence ones have contin-\nuations stereotypical for the inanimate entity: a\ncollectable dime becomes rare, or a telephone be-\ngins to ring.\nDiscussion Results show that LMs can adapt to\natypical animacy even given limited cues: one verb,\nrather than an entire story. They also suggest that\nLMs do not require a long context to adapt; how-\never, the factors that regulate this adaptation are\ncomplex. Some, like the choice of prompt, bear\nno clear relation to animacy; others, like the nouns,\n7Significant for all three groups (F-test), and pairwise.\n12127\n0 1 2\n0.0\n0.5\n1.0\n1.5\n2.0Density\nA prompt_type\nVERB\nADJ\n0 1 2\nB verb_category\nphysical\npsychological\n0 1 2\nC verb_frequency\nHIGH\nHIGH-MID\nMID\nAnimacy Divergence (1e-4)\nAnimacy Divergence Distribution by Prompt T ype, Verb Category, and Verb-Human Co-occurrence\n0.05\n0.00\n0.05\nD\n0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n0.05\n0.00\n0.05\nE\nAnimacy Divergence (1e-4)\nMean Animacy Divergence per Verb and Noun\nVerbs\nNouns\nFigure 10: Left: Distribution of animacy divergences by prompt type, verb category, and verb-human co-occurrence.\nRight: Distribution of mean animacy divergences per-verb and per-noun. Each point is one verb (or noun).\nshow no clear pattern in how they affect model re-\nsponses. Still, some interpretable factors exist. Psy-\nchological verbs may induce more animate continu-\nations because they indicate animacy more strongly.\nWhile an inanimate object might metaphorically en-\ngage in physical activities like dance, they seldom\nmarry or volunteer. These psychological words\nmay thus serve as stronger signals of animacy.\n7 Conclusions\nAlthough animacy manifests only indirectly in\nEnglish—it is not morphologically marked—pre-\ntrained LMs demonstrate relatively good animacy\nprocessing abilities. They both respect typical an-\nimacy and adapt to atypical animacy at close-to-\nhuman levels, although differences remain. They\nalso demonstrate some ability to adapt to atypi-\ncal animacy even when indicated by a very short\ncontext. LM adaptation still lags behind that of hu-\nmans, but large models increasingly shrink the gap.\nWe conclude that in the scenarios we test, LMs\nrespond to animacy like humans do; however, our\nbehavioral methodology can yield no conclusions\nabout how models achieve this. Related work on\nworld models (Li et al., 2021, 2023) suggests that\nby using causal techniques to search for internal\nstructure in models, future work could not only\ndemonstrate that LMs respond well to animacy, but\nalso explain how they do so.\nLimitations\nIn this study, we use primarily behavioral experi-\nments. These are suitable for comparing models\nto human data, but do not reveal the causal mecha-\nnisms by which LMs process animacy. In order to\ndiscover these, it would be more appropriate to use\ncausal interventions or similar techniques, which\nwe do not explore.\nConsidering the limitations of behavioral tech-\nniques, this study is still limited by the fact that it\ndid not collect human data. We translate Nieuw-\nland and van Berkum’s stimuli to English, but do\nnot test them again on native English speakers; the\nN400 responses to Dutch data could differ from\nN400 responses to English data, even though we\nassume they will be similar. Similarly, studying\nhuman responses to our low-context atypical ani-\nmacy stimuli (Section 6) would better inform our\nanalysis of LM performance.\nEthics Statement\nThis work presents only minor ethical concerns. A\nparticular concern is one of bias and stereotypes;\nthe original stories in Nieuwland and van Berkum\n(2006) do contain stereotypes. We attempt to soften\nthese in our translations, but some stereotypes are\nstill present in the translated material.\nMore generally, LMs such as those analyzed\nmust be used with caution. Although such models\nachieve high performance on language-based tasks,\nthis performance does not necessarily stem from\ngenuine linguistic understanding. Moreover, mod-\nels can not only perpetuate harmful biases present\nin their training data, but also create misleading or\nfalse output.\nAcknowledgements\nThe authors thank the members of the Dialogue\nModelling Group, particularly Joris Baan and\nRaquel Fernández, as well as Marianne de Heer\nKloots, for their helpful feedback. They also thank\nthe authors of the original human studies, in par-\nticular Mante Nieuwland and Megan Boudewyn,\nwho allowed us to translate / use their stimuli. Fi-\nnally, the authors thank participants of the ELLIS,\nAI4media, and AIDA Symposium on Large Lan-\nguage and Foundation Models in Amsterdam for\ntheir insightful comments. This work was sup-\nported by the ISRAEL SCIENCE FOUNDATION\n(grant No. 448/20), Open Philanthropy, and an\nAzrieli Foundation Early Career Faculty Fellow-\nship.\n12128\nReferences\nSuhas Arehalli, Brian Dillon, and Tal Linzen. 2022.\nSyntactic surprisal from neural models predicts, but\nunderestimates, human processing difficulty from\nsyntactic ambiguities. In Proceedings of the 26th\nConference on Computational Natural Language\nLearning (CoNLL) , pages 301–313, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nChristoph Aurnhammer and Stefan L Frank. 2018.\nComparing gated and simple recurrent neural net-\nwork architectures as models of human sentence pro-\ncessing.\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe.\n1998. The Berkeley FrameNet project. In 36th An-\nnual Meeting of the Association for Computational\nLinguistics and 17th International Conference on\nComputational Linguistics, Volume 1, pages 86–90,\nMontreal, Quebec, Canada. Association for Compu-\ntational Linguistics.\nMegan Boudewyn, Adam Blalock, Debra Long, and\nTamara Swaab. 2019. Adaptation to animacy vio-\nlations during listening comprehension. Cognitive,\nAffective, & Behavioral Neuroscience, 19.\nSamuel R. Bowman and Harshit Chopra. 2012. Au-\ntomatic Animacy classification. In Proceedings of\nthe NAACL HLT 2012 Student Research Workshop,\npages 7–10, Montréal, Canada. Association for Com-\nputational Linguistics.\nJoan Bresnan and Jennifer Hay. 2008. Gradient gram-\nmar: An effect of animacy on the syntax of give\nin new zealand and american english. Lingua,\n118(2):245–259. Animacy, Argument Structure, and\nArgument Encoding.\nAurélia Bugaiska, Laurent Grégoire, Anna-Malika Cam-\nblats, Margaux Gelin, Alain Méot, and Patrick Bonin.\n2019. Animacy and attentional processes: Evi-\ndence from the stroop task. Quarterly Journal of\nExperimental Psychology, 72(4):882–889. PMID:\n29716460.\nDavid Caplan, Nancy Hildebrandt, and Gloria S. Wa-\nters. 1994. Interaction of verb selectional restrictions,\nnoun animacy and syntactic form in sentence process-\ning. Language and Cognitive Processes, 9(4):549–\n585.\nAlfonso Caramazza and Jennifer R. Shelton. 1998.\nDomain-specific knowledge systems in the brain:\nThe animate-inanimate distinction. J. Cognitive Neu-\nroscience, 10(1):1–34.\nMariona Coll Ardanuy, Federico Nanni, Kaspar Beelen,\nKasra Hosseini, Ruth Ahnert, Jon Lawrence, Kather-\nine McDonough, Giorgia Tolfo, Daniel CS Wilson,\nand Barbara McGillivray. 2020. Living machines: A\nstudy of atypical animacy. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 4534–4545, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nBernard Comrie. 1989. Language universals and lin-\nguistic typology: Syntax and morphology. University\nof Chicago press.\nPeter de Swart and Helen de Hoop. 2018. Shifting\nanimacy. Theoretical Linguistics, 44(1-2):1–23.\nWietse de Vries and Malvina Nissim. 2021. As good\nas new. how to successfully recycle English GPT-2\nto make models for other languages. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 836–846, Online. Association\nfor Computational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nF. Ferreira. 1994. Choice of passive voice is affected\nby verb type and animacy. Journal of Memory and\nLanguage, 33(6):715–736.\nStefan L. Frank, Leun J. Otten, Giulia Galli, and\nGabriella Vigliocco. 2013. Word surprisal predicts\nn400 amplitude during reading. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n878–883, Sofia, Bulgaria. Association for Computa-\ntional Linguistics.\nStefan L. Frank, Leun J. Otten, Giulia Galli, and\nGabriella Vigliocco. 2015. The ERP response to\nthe amount of information conveyed by words in\nsentences. Brain and Language, 140:1–11.\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.\n2012. Building large monolingual dictionaries at the\nLeipzig corpora collection: From 100 to 200 lan-\nguages. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC’12), pages 759–765, Istanbul, Turkey. Euro-\npean Language Resources Association (ELRA).\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano\nSchain, Amy Price, Bobbi Aubrey, Samuel A. Nas-\ntase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren\nJansen, Harshvardhan Gazula, Gina Choe, Aditi Rao,\nCatherine Kim, Colton Casto, Lora Fanda, Werner\nDoyle, Daniel Friedman, Patricia Dugan, Lucia Mel-\nloni, Roi Reichart, Sasha Devore, Adeen Flinker, Liat\nHasenfratz, Omer Levy, Avinatan Hassidim, Michael\nBrenner, Yossi Matias, Kenneth A. Norman, Orrin\nDevinsky, and Hasson Uri. 2022. Shared computa-\ntional principles for language processing in humans\nand deep language models. Nature Neuroscience,\n25:369–380.\n12129\nAdam Goodkind and Klinton Bicknell. 2018. Predictive\npower of word surprisal for reading times is a linear\nfunction of language model quality. In Proceedings\nof the 8th Workshop on Cognitive Modeling and Com-\nputational Linguistics (CMCL 2018), pages 10–18,\nSalt Lake City, Utah. Association for Computational\nLinguistics.\nYeb Havinga. 2021. GPT-2-medium-Dutch.\nLabiba Jahan, Geeticka Chauhan, and Mark Finlayson.\n2018. A new approach to Animacy detection. In\nProceedings of the 27th International Conference\non Computational Linguistics, pages 1–12, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nJie Ji and Maocheng Liang. 2018. An animacy hierarchy\nwithin inanimate nouns: English corpus evidence\nfrom a prototypical perspective. Lingua, 205:71–89.\nFolgert Karsdorp, Marten van der Meulen, Theo Meder,\nand Antal van den Bosch. 2015. Animacy Detec-\ntion in Stories. In 6th Workshop on Computational\nModels of Narrative (CMN 2015), volume 45 of Ope-\nnAccess Series in Informatics (OASIcs) , pages 82–\n97, Dagstuhl, Germany. Schloss Dagstuhl–Leibniz-\nZentrum fuer Informatik.\nCarina Kauf, Anna A. Ivanova, Giulia Rambelli, Em-\nmanuele Chersoni, Jingyuan S. She, Zawad Chowd-\nhury, Evelina Fedorenko, and Alessandro Lenci.\n2022. Event knowledge in large language models:\nthe gap between the impossible and the unlikely.\nKarin Kipper, Hoa Trang Dang, and Martha Palmer.\n2000. Class-based construction of a verb lexicon.\nIn Proceedings of the Seventeenth National Confer-\nence on Artificial Intelligence and Twelfth Confer-\nence on Innovative Applications of Artificial Intelli-\ngence, page 691–696. AAAI Press.\nSusumu Kuno and Etsuko Kaburaki. 1977. Empathy\nand syntax. Linguistic inquiry, pages 627–672.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nKenneth Li, Aspen K Hopkins, David Bau, Fernanda\nViégas, Hanspeter Pfister, and Martin Wattenberg.\n2023. Emergent world representations: Exploring\na sequence model trained on a synthetic task. In\nThe Eleventh International Conference on Learning\nRepresentations.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nDanny Merkx and Stefan L. Frank. 2021. Human sen-\ntence processing: Recurrence or attention? In Pro-\nceedings of the Workshop on Cognitive Modeling\nand Computational Linguistics, pages 12–22, Online.\nAssociation for Computational Linguistics.\nJames Michaelov and Benjamin Bergen. 2020. How\nwell does surprisal explain n400 amplitude under dif-\nferent experimental conditions? In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 652–663, Online. Association\nfor Computational Linguistics.\nJames A. Michaelov, Megan D. Bardolph, Seana Coul-\nson, and Benjamin K. Bergen. 2021. Different kinds\nof cognitive plausibility: why are transformers better\nthan RNNs at predicting N400 amplitude? In Pro-\nceedings of the 43rd Annual Meeting of the Cognitive\nScience Society, pages 300–306.\nJames A. Michaelov, Seana Coulson, and Benjamin K.\nBergen. 2022. So cloze yet so far: N400 amplitude\nis better predicted by distributional information than\nhuman predictability judgements. IEEE Transactions\non Cognitive and Developmental Systems, pages 1–1.\nJames A. Michaelov, Seana Coulson, and Benjamin K.\nBergen. 2023. Can peanuts fall in love with distribu-\ntional semantics? To appear in the Proceedings of\nthe 45th Annual Meeting of the Cognitive Science\nSociety (Sydney, Australia; 2023).\nJames S. Nairne, Joshua E. VanArsdall, Josefa N. S.\nPandeirada, Mindi Cogdill, and James M. LeBreton.\n2013. Adaptive memory: The mnemonic value of\nanimacy. Psychological Science, 24(10):2099–2105.\nJoshua New, Leda Cosmides, and John Tooby. 2007.\nCategory-specific attention for animals reflects ances-\ntral priorities, not expertise. Proceedings of the Na-\ntional Academy of Sciences, 104(42):16598–16603.\nMante S. Nieuwland and Jos J. A. van Berkum. 2006.\nWhen peanuts fall in love: N400 evidence for the\npower of discourse. J. Cognitive Neuroscience ,\n18(7):1098–1111.\nByung-Doh Oh and William Schuler. 2023. Why Does\nSurprisal From Larger Transformer-Based Language\nModels Provide a Poorer Fit to Human Reading\nTimes? Transactions of the Association for Com-\nputational Linguistics, 11:336–350.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nConstantin Orasan and Richard Evans. 2007. NP ani-\nmacy identification for anaphora resolution. J. Artif.\nInt. Res., 29(1):79–103.\n12130\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nDavid Rakison and Diane Poulin-Dubois. 2001. Devel-\nopmental origin of the animate-inanimate distinction.\nPsychological Bulletin, 127:209–228.\nAnette Rosenbach. 2008. Animacy and grammatical\nvariation—findings from english genitive variation.\nLingua, 118(2):151–171. Animacy, Argument Struc-\nture, and Argument Encoding.\nArabella Sinclair, Jaap Jumelet, Willem Zuidema, and\nRaquel Fernández. 2022. Structural persistence in\nlanguage models: Priming as a window into abstract\nlanguage representations. Transactions of the Associ-\nation for Computational Linguistics, 10:1031–1050.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nMarten van Schijndel and Tal Linzen. 2021. Single-\nstage prediction models do not explain the magnitude\nof syntactic disambiguation difficulty. Cognitive Sci-\nence, 45(6):e12988.\nJoshua E. VanArsdall and Janell R. Blunt. 2022. Analyz-\ning the structure of animacy: Exploring relationships\namong six new animacy and 15 existing normative\ndimensions for 1,200 concrete nouns. Memory &\nCognition, 50:997–1012.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger Levy. 2020. On the predictive power\nof neural language models for human real-time com-\nprehension behavior. In Proceedings of the 42nd\nAnnual Meeting of the Cognitive Science Society.\nFrank Wilcoxon. 1945. Individual comparisons by rank-\ning methods. Biometrics Bulletin, 1(6):80–83.\nMichael Wilson. 1988. MRC psycholinguistic database:\nMachine-usable dictionary, version 2.00. Behavior\nResearch Methods, Instruments, & Computers, 20:6–\n10.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open pre-\ntrained transformer language models.\nA Implementation Details\nWe implement all experiments in PyTorch (Paszke\net al., 2019). For all models but LLaMA, we use\nthe implementations and weights publicly available\nvia the HuggingFace Transformers library (Wolf\net al., 2020); for LLaMA, weights are only avail-\nable upon request via a form at https://github.\ncom/facebookresearch/llama. We run models\nusing an Nvidia A100 40GB GPU (or multiple\nwhen necessary, as for LLaMA 30B). The runtime\nof these experiments should not exceed 24 hours,\neven when run serially.\nB N400 Results for Dutch Data\nThe methods for this set of experiments are mostly\nidentical to those of the English experiments. How-\never, instead of English LMs, we use Dutch LMs.\nWe consider GPT-2 Small trained from scratch\non Dutch, and GPT-2 Medium trained in English,\nwith fine-tuned Dutch word embeddings (de Vries\nand Nissim, 2021), and GPT-2 Medium and Large\ntrained from scratch on Dutch (Havinga, 2021).\nThese represent the best Dutch autoregressive LMs\navailable.\nB.1 Results: Repetition Experiment\nThe results of the repetition experiment in Dutch\n(Figure 11) are rather similar to those in English.\nAs before, the surprisal starts out high for both\n12131\nt1 t3 t5 t1 t3 t5 t1 t3 t5 t1 t3 t5\nGPT-2 Small GPT-2 Medium\n(Embeddings)\nGPT-2 Medium GPT-2 Large\n0\n5\n10\n15\n20\nInanimate Surprisal Animate Surprisal\nDutch Repetition Experiment Surprisals\nModel and Timestep\nSurprisal (bits)\nFigure 11: Repetition experiment surprisals. Surprisal\ndrops rapidly after T1, with inanimate surprisal drawing\nclose to animate surprisal.\nGPT-2 Small GPT-2 Medium (Embs) GPT-2 Medium GPT-2 Large\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0surprisal (bits)\nDutch Context Experiment\nInanimate Surprisal\nAnimate Surprisal\nInanimate Baseline\nAnimate Baseline\nFigure 12: Context experiment surprisals. With context,\nthe animate adjective is much less surprising; in the\ncontextless baseline condition, this is reversed.\nanimate and inanimate entities (though higher for\nthe latter). The difference between these two is\nless pronounced than in English. The surprisal\ndrops rapidly at T3, and again at T5. Unlike in\nEnglish, there are no strong model-wise trends,\nwhereby stronger models have a lower difference in\nsurprisals. And in all cases, the difference between\nthe two conditions at T5 is statistically significant.\nB.2 Results: Context Experiment\nAgain, the results of the Dutch experiment (Fig-\nure 12) are much like the English results. Surprisals\nat animate adjectives are much lower than those at\ninanimate adjectives; however, in the baseline con-\ndition, which lacks context, the trend is reversed.\nC Boudewyn et al.: English Context\nExperiment\nOriginal Study Much like in Nieuwland and van\nBerkum (2006), Boudewyn et al. (2019) also mea-\nA lucky peanut had a big smile on his face. The peanut was\namazed about his good fortune. Just now he had won the\njackpot of two million dollars. The peanut waselated/salted\nand who could blame him.\nFigure 13: Story from Boudewyn et al. (2019), context\nexperiment. N400 responses were recorded at the words\nin bold.\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nSurprisal (bits) English Context Experiment Surprisals\nAnimate Surprisal\nInanimate Surprisal\nAnimate Baseline\nInanimate Baseline\nFigure 14: Surprisals for animate and inanimate adjec-\ntives in the normal and baseline condition\nsured participants’ N400 responses to adjectives\nat the end of each story (Figure 13). Each adjec-\ntive was either typical for animate entities (and thus\ncontext-appropriate) or typical for the inanimate en-\ntity in question (but context-inappropriate). N400\nresponses were much lower in the former case than\nin the latter.\nExperiment As in the earlier context experiment\n(Section 5.2), for each of the 120 stories, we cal-\nculate the surprisal at the animate and inanimate\nadjective. We also compute baseline surprisals, de-\nfined as the surprisal of the critical adjective given\nonly the sentence containing it as context.\nResults The results of the English context ex-\nperiment (Figure 14) are like those of the earlier\ncontext experiment (Section 5.2). Like before, sur-\nprisal at the animate adjective is much lower than\nsurprisal at the inanimate adjective; in the baseline\ncondition, this trend is reversed.\nD Further Low-Context Experiments\nIn the following experiments, we made changes to\nour and experimental setup, in order to ensure that\nour findings were not a result of any idiosyncrasies\nof our dataset’s construction.\n12132\nD.1 Larger Human Entity Sample Pool\nExperiment\nIn our original experiment, our human reference\nwas A =“The [human] began to”, where [human]\nwas sampled from a very limited pool:person, man,\nwoman, boy, girl, and child. The pool was limited\nfor two reasons. First, we did not want to introduce\nanother axis of variation across examples.\nSecond, and more importantly, we wanted to\ncreate a generic “human-like” action distribution.\nSome nouns (e.g. a musician, or a thief) are ani-\nmate humans, but their distributions over next ac-\ntions are probably skewed in ways that are not\nrepresentative of animate entities in general; musi-\ncians are likely to play music, and thieves, to steal.\nThus, their next-token distributions might have a\nhigh KL-divergence with those of atypically ani-\nmate objects for reasons unrelated to the object’s\nperceived animacy. However, we still wanted to\nensure that our findings are not reliant on this small\npool of humans. To verify this, we constructed a\nlarger pool of human entities to sample from.\nTo construct this pool, we first added in generic\nhuman entities ( man, woman, person, boy, girl,\nchild, teenager) and family relations (mother, fa-\nther, grandfather, grandmother, wife, husband,\ngrandchild, granddaughter, grandson, aunt, un-\ncle, niece, nephew, cousin). Then, we added more\nentities, starting from a list of job titles, 8 which\nare all naturally human nouns. We then used word\nfrequency data from 1 million words of Wikipedia\n(Goldhahn et al., 2012), filtering out any job titles\nthat are not found in that text. We then manually\nadded valid job titles to our pool in order of de-\nscending frequency, until our expanded pool num-\nbered 100 entries: 21 generic human entities and\n79 professions. We could have increased the pool\nsize by including more jobs, but the job title pool\nwas noisy, and needed manual filtering to weed out\nnonsensical jobs, and jobs that sound too much like\ninanimate objects.\nThen, we conducted our experiment again as\nin Section 6, using this larger pool. The results\nof this experiment (Figure 15) are rather similar\nto that of our original experiment. In all cases\nthe output distribution pO is clearly more animate\nthan pI; in the OPT models, the pO is more like\nthe animate ( pA, red) than inanimate ( pI, blue)\n8The job titles originate from https://github.com/\njneidel/job-titles/tree/master, which collects job ti-\ntles from a variety of sources.\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0\n2\n4\n6\n8KL Divergence\n×10□5Mean KL Divergences by Model (Large Human Sample Pool)\nDKL(A||O)\nDKL(I||O)\nDKL(A||I)\nFigure 15: KL Divergence between atypically animate\n(O) and animate (A) / inanimate (I) references, where\nthe animate entity in A is drawn from a larger pool. The\nlower the bar, the more similar the distributions.\ndistribution. However, the animacy divergence is\nsomewhat higher for the LLaMA models.\nD.2 Matched Frequency Human Entity\nExperiment\nIn this experiment, we ensure that differences be-\ntween the frequency of the sampled human entity\nand that of the inanimate entity do not undermine\nour experimental setup. Using the same frequency\ndata as before, we matched each inanimate entity\nin our pool to the (manually verified, valid) hu-\nman entity with the most similar frequency. We\nexcluded one inanimate object (“well”) because its\nfrequency was confounded by the very common\nadverb that shares its form. We were able to find a\ngood human match for each inanimate object: the\nobject frequency-to-human frequency ratio ranged\nfrom 0.92 to 1.09; this is near 1, the ideal ratio.\nThen, we conducted our experiment again as in\nSection 6, using this larger pool. The results of\nthis experiment (Figure 16) are strikingly similar to\nthose of the previous experiment (Figure 15). We\ntake this to suggest that frequencies are indeed not\nvery important to the phenomenon we observe.\nD.3 Cataphoric Prompt Experiment\nIn our original experiment, we test sentences of\nthe form O =“The chair spoke and began to”.\nOne potential concern is that LMs might just look\nat “spoke and began to”, which implies an ani-\nmate continuation, thus overcoming the inanimacy\nof chair. We can test this by using a cataphoric\nprompt, where a referring pronoun comes before\nthe object, e.g. O′ =“After it spoke, the chair began\nto”. In such a prompt, there now exists the 4-gram\n12133\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0\n2\n4\n6\n8KL Divergence\n×10□5 Mean KL Divergences by Model (Matched Frequency)\nDKL(A||O)\nDKL(I||O)\nDKL(A||I)\nFigure 16: KL Divergence between atypically animate\n(O) and animate (A) / inanimate (I) references, where\nthe animate entity in A is matched in frequency with\nthe inanimate entity in I and O. The lower the bar, the\nmore similar the distributions.\nGPT-2\nSmall\nGPT-2\nMedium\nGPT-2\nLarge\nGPT-2\nXL\nOPT\n2.7B\nOPT\n6.7B\nOPT\n13B\nLLaMA\n7B\nLLaMA\n13B\nLLaMA\n30B\nModel\n0\n1\n2\n3\n4\n5\n6\n7\n8KL Divergence\n×10□5 Mean KL Divergences by Model (Cataphor Prompt)\nDKL(A||O)\nDKL(I||O)\nDKL(A||I)\nFigure 17: KL Divergence between atypically animate\n(O) and animate (A) / inanimate (I) references, using\nthe cataphoric prompt. The lower the bar, the more\nsimilar the distributions.\n“the chair began to”, which should make this task\na little harder for LMs. Using the less-animate /\ninanimate pronoun “it” to refer to the atypically\nanimate object also makes this more challenging.\nThen, we conducted our experiment again as in\nSection 6, using the original small pool of human\nentities. The results of this experiment (Figure 17)\nsuggest that this setting is indeed harder for LMs;\nno longer to any LMs adapt such that divergence\nbetween pO and pA (red) is less than that between\npO and pI (blue). However, there is still an increase\nin animacy compared to the case where the prompt\ndoes not hint at atypical animacy (purple).\nE Low-context Animacy: Evaluation\nWe first tried to classify potential next tokens as ani-\nmate, inanimate, or neither; we could then compute\nthe probability assigned to each group. However,\nclassifying all next tokens was noisy: even with\nresources like FrameNet or VerbNet (Baker et al.,\n1998; Kipper et al., 2000), it was infeasible to deter-\nmine if a verb implied that its subject was animate.\nF Low-Context Animacy Dataset Details\nThis section contains lists of the prompts, nouns,\nand verbs used in constructing this dataset. The full\ndataset, and all variants thereof, can be found at\nhttps://github.com/hannamw/lms-in-love.\nF.1 Prompts\n• The [noun] [verb]and began to\n• The [noun] [verb]and started to\n• The [noun] [verb]and was very\n• The [noun] [verb]and became very\nF.2 Nouns\naccordion, ambulance, amplifier, appliance, arrow,\nautomobile, axe, bagpipe, balloon, bandage, ban-\nner, barrel, basket, bin, biscuit, blanket, blossom,\nblouse, boat, bomb, book, bottle, bouquet, bra,\nbracelet, bread, brush, bubble, bucket, buckle, bul-\nlet, button, cake, camera, candle, candy, cane, can-\nnon, canoe, cape, cart, casket, chisel, chocolate,\nclarinet, clock, clothing, coat, cocktail, coffin, coin,\ncollar, corpse, dagger, dart, desk, dime, dress, en-\ngine, envelope, ferry, fiddle, firewood, flask, flute,\nfootball, fruit, furniture, glass, glove, goblet, gown,\nhailstone, hairpin, hammer, harp, hat, helmet, hose,\njar, keg, kilt, knife, lamp, lantern, lens, limou-\nsine, mallet, map, mattress, medallion, microscope,\nmirror, missile, moccasin, nail, napkin, necklace,\nneedle, nickel, nightgown, oar, ornament, oven,\novercoat, pants, pearl, pencil, pendulum, penny,\nphone, photograph, piano, pie, pillow, pipe, plank,\npot, propeller, prune, purse, quilt, radio, record,\nrefrigerator, ribbon, rifle, ring, rope, rug, sandal,\nsatchel, saxophone, scissors, scroll, shawl, shield,\nshirt, shoe, ski, skull, sleigh, sock, sofa, spoon,\nstatue, steak, stove, submarine, sword, tablespoon,\ntelephone, telescope, thermometer, thorn, thread,\nticket, tie, timepiece, tractor, tray, tripod, trombone,\ntruck, trumpet, tube, tweezers, twig, typewriter,\numbrella, van, vase, vehicle, vest, violin, wallet,\nwheel, whistle, wig, yacht, zipper,\n12134\nF.3 Verbs\nF.3.1 Physical\nHigh Co-Occurrence With Human Subjects\nstammer, grimace, mumble, drawl, frown, gasp,\nyell, nod, smile, laugh, shrug, sob, grin, kneel,\nwince, whisper, sigh, giggle, squint, murmur, doze,\nfiddle, gesture, mutter, faint, gulp, flinch, chuckle,\ndrink, weep, stare, grunt, listen, watch, fumble,\nshiver, pace, lean, blush, shout, gaze, walk, sit,\nsleep, dine, pant, glare, clap, stumble, snore, shave,\nwave, omit, sniff, piss, cough, wail, grumble,\nbreathe, snort, spit, eat, duck, die, swallow, growl,\nblink, inhale, bellow, starve, crouch, yawn, step,\nsquat\nHigh-Mid Co-Occurrence With Human Sub-\njects pounce, scream, flee, shudder, wander,\nshriek, stagger, wink, sing, whistle, jog, limp, hiss,\ntrot, jump, bathe, dance, paint, ramble, shower,\ndrown, recover, pack, sweat, bow, flush, crawl\nMid Co-Occurrence With Human Subjects\nbark, swim, bleed, howl\nF.3.2 Psychological\nHigh Co-Occurrence With Human Subjects\nthink, know, wonder, remember, guess, exclaim,\nretort, marry, notice, understand, hurry, pray, medi-\ntate, swear, forget, enquire, realise, confess, apolo-\ngise, hesitate, suspect, reply, talk, sneer, cry, dream,\nmoan, ponder, revel, learn, scowl, retire, snarl,\ngroan, speak, complain, beg, wait, preach, grieve,\nread, plead, volunteer, answer, curse, choose, panic,\nchant, cheat, salute, emigrate, protest, visit, lament,\nmisunderstand\nHigh-Mid Co-Occurrence With Human Sub-\njects consent, graduate, disagree, steal, mourn,\nstudy, argue, search, insist, practise, interrupt, obey,\ncomment, concede, fight, applaud, enlist, worry,\nteach, train, agree, struggle, rush, evacuate, object,\npay, pursue, hasten\nMid Co-Occurrence With Human Subjects\nvote, invest, register\n12135",
  "topic": "Animacy",
  "concepts": [
    {
      "name": "Animacy",
      "score": 0.9966949820518494
    },
    {
      "name": "Computer science",
      "score": 0.5829784870147705
    },
    {
      "name": "Cognition",
      "score": 0.510513424873352
    },
    {
      "name": "Linguistics",
      "score": 0.45670104026794434
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4544847011566162
    },
    {
      "name": "Cognitive psychology",
      "score": 0.421028196811676
    },
    {
      "name": "Psychology",
      "score": 0.3674471378326416
    },
    {
      "name": "Natural language processing",
      "score": 0.33928248286247253
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3225170373916626
    },
    {
      "name": "History",
      "score": 0.15078645944595337
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}