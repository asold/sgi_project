{
  "title": "Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model",
  "url": "https://openalex.org/W4385570539",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104684343",
      "name": "Xiao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123415979",
      "name": "Weikang Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964204209",
      "name": "Qi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106060735",
      "name": "Songyang Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110273631",
      "name": "Junzhe Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103374516",
      "name": "Menghan Zhang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A1993071709",
      "name": "Xiang Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114655007",
      "name": "Yun Wen Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117552295",
      "name": "Tao Gui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W4366400290",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W4294554825",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2955041501",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W1981313592",
    "https://openalex.org/W4288335875",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3171975124",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W4223458030",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2951013084",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2796514099",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W3214020110",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2279376656"
  ],
  "abstract": "Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selection (ISS) for language model, which explicitly utilizes end-task knowledge to select a tiny subset of the pretraining corpus. Specifically, the ISS selects the samples that will provide the most positive influence on the performance of the end task. Furthermore, we design a gradient matching-based influence estimation method, which can drastically reduce the computation time of influence. With only 0.45% of the data and a three-orders-of-magnitude lower computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight datasets covering four domains.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 555â€“568\nJuly 9-14, 2023 Â©2023 Association for Computational Linguistics\nFarewell to Aimless Large-scale Pretraining: Influential Subset Selection\nfor Language Model\nXiao Wangâ‹†âˆ—, Weikang Zhou â‹†âˆ—, Qi Zhang â‹†â€ , Jie Zhou â‹†, Songyang Gao â‹†,\nJunzhe Wangâ‹†, Menghan Zhangâ™¦, Xiang Gaoâ™£, Yunwen Chenâ™£, Tao Guiâ™¦â€ \nâ‹†School of Computer Science, Fudan University, Shanghai, China\nâ™¦Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China\nâ™£DataGrand Information Technology (Shanghai) Co., Ltd.\n{xiao_wang20,qz,tgui}@fudan.edu.cn\nAbstract\nPretrained language models have achieved re-\nmarkable success in various natural language\nprocessing tasks. However, pretraining has\nrecently shifted toward larger models and larger\ndata, and this has resulted in significant com-\nputational and energy costs. In this paper,\nwe propose Influence Subset Selection (ISS)\nfor language model, which explicitly utilizes\nend-task knowledge to select a tiny subset of\nthe pretraining corpus. Specifically, the ISS\nselects the samples that will provide the most\npositive influence on the performance of the\nend-task. Furthermore, we design a gradient\nmatching based influence estimation method,\nwhich can drastically reduce the computation\ntime of influence. With only 0.45% of the data\nand a three-orders-of-magnitude lower compu-\ntational cost, ISS outperformed pretrained mod-\nels (e.g., RoBERTa) on eight datasets covering\nfour domains.\n1 Introduction\nPretrained language models (PTMs) (Peters et al.,\n2018; Devlin et al., 2019; Liu et al., 2019), trained\non massive and heterogeneous corpora, have sig-\nnificantly improved the state-of-the-art across a\nvariety of natural language processing tasks (Wang\net al., 2022, 2023). Kaplan et al. (2020) found\npower laws relating cross entropy loss to the sizes\nof language models and their training datasets. As\na result, the field has recently shifted toward larger\nmodels and large data (Brown et al., 2020; Rae\net al., 2021; Smith et al., 2022; Chowdhery et al.,\n2022) in hopes of improving performance.\nHowever, training a state-of-the-art language\nmodel requires substantial computational resources\nwhich demand considerable energy, along with\nthe associated financial and environmental costs\n(Strubell et al., 2019). For example, RoBERTa-\nLarge (Liu et al., 2019), which was trained on\nâˆ— âˆ—Equal contribution.\nâ€ Corresponding Author\nPLMs TLM ISS\nTraining Data The entireD Subset ofD\n& task dataT\nSubset ofD\n& task dataT\nCompute Cost 240000\nGPUÂ·hours\n240\nGPUÂ·hours\n80\nGPUÂ·hours\nGenerality Task-Agnostic X-Dep X&Y-Dep\nTable 1: Qualitative comparison between PLMs, TLM,\nand ISS(ours). X/Y-Dep means the pretraining data is\nX/Y dependent.\n1000 V100 GPUs for approximately one day, has a\ncomputational cost of4.36Ã—1021 FLOPs. Recently,\nChowdhery et al. (2022) proposes PaLM, which\nconsumes 580 times more FLOPs than RoBERTa-\nLarge. PaLM was trained on 6144 TPU v4 chips\nfor more than 1200 hours, which is unaffordable for\nmost researchers. Therefore, finding ways to speed\nup pretraining is crucial for the development of\npretrained model research.\nIn general, there are three main strategies used\nto speed up pretraining in NLP: parallel archi-\ntectures, efficient model architectures, and novel\npretraining tasks. The first one is to train a single\nmodel utilizing multiple GPUs distributed in many\ncomputational nodes (Wang et al., 2020b; Shazeer\net al., 2018; Huang et al., 2019). Unfortunately,\nthe gains in efficiency of this strategy depend\nentirely on the amount of computing hardware\nused. The second strategy is to improve model\nstructures to reduce the computational complexity\nand therefore improve efficiency (Wang et al.,\n2020a; Katharopoulos et al., 2020; Roy et al., 2021).\nThe last one explores more challenging pretraining\ntasks to accelerate a modelâ€™s convergence (Clark\net al., 2019; Joshi et al., 2020; Levine et al., 2020).\nHowever, their improvements are limited, with a\nreduction of less than an order of magnitude in\ncomputational expenses (measured in FLOPs).\nIn this paper, we aim to reduce the computational\ncosts from data level (See Table 1). The PLMs are\ntrained on the entire pretraining corpus D, which\nis task-agnostic. To take the downstream task\n555\ninto account, we hope to select the most relevant\nsamples from the pretraining corpus based on the\ndownstream data. Recently, Yao et al. (2022) pro-\nposes TLM, which retrieves data from a pretraining\ncorpus using task data as queries. However, TLM\nremains task-agnostic, because it only considers\ntext (i.e., X) similarities and ignores the label (i.e.,\nY) information.\nMotivated by influence function (Cook and Weis-\nberg, 1982; Koh and Liang, 2017), we propose\nInfluential Subset Selection (ISS) for language\nmodel, i.e. selecting the samples with the most\npositive influence on the downstream task. To cal-\nculate the label-aware influence value, ISS utilizes\nthe derivation chain rule from a test objective to\ntraining samples. Nevertheless, directly applying\nthe chain rule leads to computing the inverse of\nHessian with the complexity of O(nq2 + q3)(n is\nthe number of examples and q is parameter size),\nwhich is computationally expensive and may run\nout-of-memory in neural networks. To address\nthis problem, we propose a gradient matching\nbased influence approximation method for select-\ning pretraining data, which estimates the influence\nscore by matching the gradient values of pretrain-\ning samples and end-task samples. Our method\navoids the computation of the inverse of Hessian\nand significantly speeds up the estimation time of\ninfluence.\nOur main contributions are summarized as fol-\nlows:\nâ€¢ We propose Influential Subset Selection for lan-\nguage model, which explicitly utilizes knowledge\nof the end-task to select the pretraining corpus.\nâ€¢ We design a simple, efficient, gradient matching\nbased method for influence estimation, which\navoids the calculation of the inverse of Hessian\nand significantly speeds up the estimation time.\nâ€¢ We evaluate the effectiveness of our method on\neight tasks covering four domains. Notably,\nISS outperforms PTMs (e.g. RoBERTa) with\nonly 0.45% of the data and three orders of\nmagnitude reduced FLOPS. Our code can be\nfound at https://github.com/nitwtog/ISS.\n2 Preliminaries\n2.1 Definition\nWe assume an end-task dataset rep-\nresented as T = ( Zt) where Zt =\n{\n(x1\nt,y1\nt),(x2\nt,y2\nt),..., (xm\nt ,ym\nt )\n}\nrepresents\na set of texts with their ground truth labels. And we\nassume a large-scale pretraining corpus D= (Zp),\nwhere Zp =\n{\nx1\np,x2\np,...,x M\np\n}\nrepresents\nunlabeled data. We define f = f(head) â—¦f(feat),\nsuch that f(feat)(Â·; Î¸ âˆˆ Î˜) is a feature extractor\nthat is transferable across learning stages (e.g.\npretraining to finetuning) and f(head)(Â·; Ï•âˆˆÎ¦) is a\ntask-specific head that is not transferable. And we\nassume lp(zp,Î¸,Ï• p) and lt(zt,Î¸,Ï• t) are the loss\nfunctions of pretraining and end-task.\n2.2 Influence Function\nInfluence function (Cook and Weisberg, 1982;\nKoh and Liang, 2017) provides an efficient way\nto estimate the importance of a training sample.\nConsidering a training sample z was weighted\nby a small Ïµ during training, the empirical risk\nminimizer can be written as\nË†Î¸Ïµ,z = arg min\nÎ¸âˆˆÎ˜\n1\nn\nâˆ‘\nziâˆˆD\nl(zi,Î¸) + ÏµÂ·l(z,Î¸) (1)\nAssigning âˆ’1\nn to Ïµis equivalent to removing the\ntraining example zp. Then, the influence of weight-\ning zp on the parameters is given by\nIparam (z) = dË†Î¸Ïµ,z\ndÏµ\nâââââ\nÏµ=0\n= âˆ’Hâˆ’1\nË†Î¸ âˆ‡Î¸l(z,Ë†Î¸) (2)\nwhere HË†Î¸ = 1\nn\nâˆ‘\nziâˆˆDâˆ‡2\nÎ¸l\n(\nzi,Ë†Î¸\n)\nis the Hessian\nand positive definite by assumption, Iparam (z) âˆˆ\nRN , N is the number of network parameters. Then,\nwe can linearly approximate the parameter change\ndue to removing z without retraining the model by\ncomputing Ë†Î¸âˆ’z âˆ’Ë†Î¸â‰ˆâˆ’1\nnIparam (z).\n3 Methodology\nWe investigate an influence-based subset selection\nmethod to perform efficient pretraining while at-\ntempting to minimize accuracy loss on the end-task\ndataset (Section 3.1). Due to the high computa-\ntional costs of influence function (Koh and Liang,\n2017), we design an influence approximation strat-\negy to speed up the calculation (Section 3.2).\n3.1 Influence of Pretraining Corpus\nPTMs used in previous works usually adopt lan-\nguage modeling as pretraining tasks, lacking task-\nspecific prior knowledge. However, we often\nknow the end-task beforehand, so we can make\n556\nspecific choices about our pretraining regimen to\nimprove end-task performance. Under this setting,\nwe introduce Influential Subset Selection for lan-\nguage model, which measures the importance of\npretraining samples by considering the X and Y\ninformation of the end-task simultaneously.\nSpecifically, pretraining sample zp affects the\nprediction of end-task sample zt by influencing\nthe parameters of the feature encoder Î¸. We can\napply the chain rule to measure the influence of\nupweighting pretraining sample zp on the loss at\nend-task sample zt.\nI(zp,zt) â‰œ\ndl\n(\nzt,Ë†Î¸Ïµ,z\n)\ndÏµ\nââââââ\nÏµ=0\n= âˆ‡Î¸l\n(\nzt,Ë†Î¸\n)âŠ¤dË†Î¸Ïµ,z\ndÏµ\nâââââ\nÏµ=0\n= âˆ’âˆ‡Î¸l\n(\nzt,Ë†Î¸\n)âŠ¤\nHâˆ’1\nË†Î¸ âˆ‡Î¸l(zp,Ë†Î¸)\n(3)\nThe more negative I(zp,zt) is, the more posi-\ntive influence zp can provide. However, computing\nthe Hessian for the full training dataset is expensive,\nand inverting it is similarly prohibitive: with n\ntraining data points and p parameters, this com-\nputation requires O(nâˆ—p2 + p3) operations. It\nmeans that evaluating the influence of large-scale\npretrained corpus is not achievable. Thus, we\npropose an influence approximation algorithm to\nspeed up the estimation time.\n3.2 Influence Approximation\nMotivated by calculus, the update of the model\nparameters is the result of cumulative updates over\nseveral training iterations. Similarly, the difference\nbetween the loss of test point zt at the end of\ntraining versus at the beginning of training can\nbe decomposed along the path taken by the training\nprocess. Thus, we hypothesize that the influences\nof all training examples on a fixed test point zt is\nexactly the total reduction in loss on zt.\nAssume that we train the feature encoder by\nminimizing the pertaining loss lp(zp; Î¸,Ï•), via an\niterative optimization procedure (such as SGD)\nwhich utilizes one training example zp in iteration\nt. The parameters of the feature encoder before and\nafter iteration t are Î¸t and Î¸t+1 respectively. The\ninfluence of zt on zp can be approximated in the\nfollowing way.\nI(zp,zt) = lt(zp,Î¸t) âˆ’lt(zp,Î¸t+1) (4)\nğœƒğ‘”!ğ‘”\" ğ‘”#\nğ‘”\"#ğ‘”#>ğ‘”!#ğ‘”#: loss landscape of pre-training: loss landscape of end-task: gradient of pre-training sample: gradient of end-task sample\nFigure 1: Illustration of gradient matching based influ-\nence approximation. g1 and g2 are the loss gradients of\ntwo different pretrained samples respectively, whilegâ€²is\nthe loss gradient of the end-task sample. The influence\nof a pretrained sample is measured by how a small step\nbased on its gradient affects the loss on the end-task\nsample. Compared to g1, the update step of g2 is more\ngeneralized.\nSuppose we are at point Î¸t, and we make a first-\norder Taylor expansion of function lp(zp,Î¸t+1).\nlt(zp,Î¸t+1) =lt(zp,Î¸t) +âˆ‡Î¸lt(zp,Î¸t) Â·(Î¸t+1 âˆ’Î¸t)\n+ O\n(\nâˆ¥Î¸t+1 âˆ’Î¸tâˆ¥2\n)\n(5)\nAssuming the model employs SGD as the opti-\nmizer, then the update in parameters isÎ¸t+1 âˆ’Î¸t =\nâˆ’Î·tâˆ‡Î¸lp(zt,Î¸t), where Î·t is the learning rate at\niteration t. Eq. (5) guarantees approximation\nprecision as long as the update magnitude of Î¸\nis sufficiently small. By substituting the parameter\nupdate formula and disregarding the higher-order\nterm, we arrive at the following first-order approxi-\nmation.\nlt\n(\nzâ€²,Î¸t\n)\nâˆ’lt\n(\nzâ€²,Î¸t+1\n)\nâ‰ˆÎ·tâˆ‡Î¸lt\n(\nzâ€²,Î¸t\n)\nÂ·âˆ‡Î¸lp(zt,Î¸t)\n(6)\nWe refer to this first-order approximation as gradi-\nent matching-based influence estimation. The full\nalgorithm is provided in Algorithm 1.\nVisualisation We visualize our influence es-\ntimation method in Fig 1. g1 and g2 are the\nloss gradients of two different pretrained samples\nrespectively, while gâ€² is the loss gradient of the\nend-task sample. The influence of a pretrained\nsample can be viewed as the dot product of its\ngradient and the gradient of the end-task sample.\nHigher influence suggests that a network is learning\nparameters that generalize.\n557\nAlgorithm 1: Influential Subset Selection\nfor Language Model\nRequire: Pretraining corpus D; task\ntraining set Tt and validation set\nTv; learning rate Î±; initial subset\nS; candidates size k.\nRandom initialize network Î¸,Ï•p,Ï•t\nË†Î¸, Ë†Ï•p, Ë†Ï•t = arg min 1\nn\nâˆ‘\nziâˆˆTt lp(zi) + lt(zi)\nfor zp âˆˆD do\nCompute âˆ‡Î¸lp\n(\nzp,Ë†Î¸, Ë†Ï•p\n)\nend\nfor zâ€²âˆˆTv do\nCompute âˆ‡Î¸lt\n(\nzâ€²,Ë†Î¸, Ë†Ï•t,\n)\nfor zp âˆˆD do\nI(zp,zâ€²) =\nâˆ‡Î¸lp\n(\nzp,Ë†Î¸, Ë†Ï•p\n)\nÂ·âˆ‡Î¸lt\n(\nzâ€²,Ë†Î¸, Ë†Ï•t,\n)\nend\nSort pretraining samples based on influence\nAdd top k influential samples to S\nend\nReturn influential subset S\n3.3 Implementation Details\nBased on the influence score, we select the most\nrelevant samples from the pretraining corpus. Fol-\nlowing TLM, we first select a subset via a BM25\nretrieval method. Then, we compute the influence\nscore based on this subset to make ISS scalable and\nefficient.\nMoreover, the number of parameters in large-\nscale language models is very large, leading to very\nhigh dimensional gradients. To tackle this problem,\nwe adopt a last-layer gradient approximation by\nonly considering the last layer gradients of pre-\ntrained encoder. We select a subset of mini-batches\nby matching the weighted sum of mini-batch pre-\ntraining gradients to the mini-batch task gradients.\nLet Bp and Bt be the batch size of pretraining and\nend-task. The use of mini-batches considerably\nreduces the number of selection rounds during the\nISS algorithm by a factor of B, resulting inBpâˆ—Bt\nspeed up.\n4 Experimental Setup\nTo evaluate the efficiency and generality of our\napproach, we conduct experiments in two settings:\npretraining from scratch, and further pretraining.\n4.1 Pretraining from Scratch\nDatasets. Following the setting of Gururangan\net al. (2020); Yao et al. (2022), we conduct ex-\nperiments on eight tasks covering four domains,\nincluding biomedical science, computer science,\nnews, and reviews. The tasks represent both\nhigh- and low-resource ( â‰¤5K samples) settings,\nincluding CHEMPROT (Kringelum et al., 2016),\nRCT (Dernoncourt and Lee, 2017), ACL-ARC\n(Jurgens et al., 2018), SCIERC (Luan et al., 2018),\nHyPERPARTISAN (Kiesel et al., 2019), AGNEws\n(Zhang et al., 2015), HELPFULNESS (McAuley\net al., 2015), IMDB (Maas et al., 2011). Table 2\nreports the statistic results of various target datasets.\nSimilar to TLM (Yao et al., 2022), we collect\ntwo pretraining corpora that respectively match the\noriginal corpora of BERT and RoBERTa. We name\nthem CBERT and CRoBERTa, respectively.\nBaselines. We focus on comparison with general\nPLMs and TLM. Following Yao et al. (2022), we\nfinetuned both BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) of base and large scales\nas our baselines. And we finetuned the released\nTLM models as baselines.\nEvaluation Strategy. The results of the experi-\nment are the average performance of three random\nseeds with the standard deviations. Following\nGururangan et al. (2020), we report the test micro-\nF1 for ChemProt and RCT, and macro-F1 for the\nrest of datasets. Following TLM (Yao et al., 2022),\nwe set three pretraining scales, namely small,\nmedium, and large scales. Differently, at the\nsame scale, our method only utilizes 20% size of\nthe TLM data. More detailed settings are shown in\nTable A.1 in Appendix.\nTraining Details. We utilize the randomly\ninitialized BERT of base scale as our starter mod-\nels. We mostly follow optimization, and hyper-\nparameters choices used in Yao et al. (2022). All ex-\nperiments were conducted on 4 NVIDIA GeForce\nRTX 3090 GPUs. Detailed hyper-parameters are\nprovided in Table A.1 in Appendix.\nDomain Task Train Dev. Test Classes\nBIOMED CHEMPROT 4169 2427 3469 13\nâ€ RCT 18040 30212 30135 5\nCS ACL-ARC 1688 114 139 6\nSCIERC 3219 455 974 7\nNEWS HYPERPARTISAN 515 65 65 2\nâ€ AGNEWS 115000 5000 7600 4\nREVIEWS\nâ€ HELPFULNESS 115251 5000 25000 2\nâ€ IMDB 20000 5000 25000 2\nTable 2: Statistics of various target datasets. â€ indicates\nhigh-resource settings.\n558\nModel Param Data1 FLOPs2 AGNews Hyp. Help. IMDB ACL. SciERC Chem. RCT Avg.\nBert-Base 109M 16G 2.79E19 93.50\nÂ±0.15\n91.93\nÂ±1.74\n69.11\nÂ±0.17\n93.77\nÂ±0.22\n69.45\nÂ±2.90\n80.98\nÂ±1.07\n81.94\nÂ±0.38\n87.00\nÂ±0.06 83.46\nBert-Large 355M 16G 9.07E19 93.51\nÂ±0.40\n91.62\nÂ±0.69\n69.39\nÂ±1.14\n94.76\nÂ±0.09\n69.13\nÂ±2.93\n81.37\nÂ±1.35\n83.64\nÂ±0.41\n87.13\nÂ±0.09 83.82\nTLM(Small) 109M 0.91G 2.74E18 93.74\nÂ±0.20\n93.53\nÂ±1.61\n70.54\nÂ±0.39\n93.08\nÂ±0.17\n69.84\nÂ±1.53\n80.51\nÂ±1.53\n81.99\nÂ±0.42\n86.99\nÂ±0.03 83.78\nTLM(Small-20%)3 109M 0.18G 1.82E18 93.57\nÂ±0.21\n93.11\nÂ±0.46\n70.02\nÂ±0.40\n93.20\nÂ±0.03\n67.27\nÂ±2.85\n78.87\nÂ±0.63\n80.80\nÂ±0.63\n86.65\nÂ±0.01 82.93\nISS(Small-scale) 109M 0.18G 1.82E18 93.78\nÂ±0.06\n93.53\nÂ±0.00\n70.78\nÂ±0.29\n93.25\nÂ±0.07\n72.41\nÂ±0.66\n80.56\nÂ±0.43\n81.71\nÂ±0.10\n86.99\nÂ±0.02 84.11\nRoBERTa-Base125M 160G 1.54E21 94.02\nÂ±0.15\n93.53\nÂ±1.61\n70.45\nÂ±0.24\n95.43\nÂ±0.16\n68.34\nÂ±7.27\n81.35\nÂ±0.63\n82.60\nÂ±0.53\n87.23\nÂ±0.09 84.12\nTLM(Medium) 109M 1.21G 8.30E18 93.96\nÂ±0.18\n94.05\nÂ±0.96\n70.90\nÂ±0.73\n93.97\nÂ±0.10\n72.37\nÂ±2.11\n81.88\nÂ±1.92\n83.24\nÂ±0.36\n87.28\nÂ±0.10 84.71\nTLM(Medium-20%)3 109M 0.18G 4.15E18 93.78\nÂ±0.02\n93.53\nÂ±0.00\n71.11\nÂ±0.05\n93.20\nÂ±0.06\n68.82\nÂ±3.56\n80.35\nÂ±0.54\n81.05\nÂ±0.07\n87.00\nÂ±0.05 83.58\nISS(Medium-scale) 109M 0.18G 4.15E18 93.92\nÂ±0.08\n93.53\nÂ±0.00\n71.51\nÂ±0.31\n93.61\nÂ±0.06\n73.42\nÂ±0.58\n82.20\nÂ±0.40\n83.42\nÂ±0.11\n87.30\nÂ±0.02 84.86\nRoBERTa-large355M 160G 4.36E21 94.30\nÂ±0.23\n95.16\nÂ±0.00\n70.73\nÂ±0.62\n96.20\nÂ±0.19\n72.80\nÂ±0.62\n82.62\nÂ±0.68\n84.62\nÂ±0.50\n87.53\nÂ±0.13 85.50\nTLM(Large)4 109M 3.64G 2.33E19 94.15\nÂ±0.01\n93.92\nÂ±0.72\n71.83\nÂ±0.11\n94.44\nÂ±0.10\n74.18\nÂ±0.29\n82.77\nÂ±0.72\n83.60\nÂ±0.08\n87.49\nÂ±0.02 85.31\nTLM(Large-20%)3 109M 0.72G 8.30E18 93.79\nÂ±0.31\n92.72\nÂ±0.783\n71.50\nÂ±0.28\n94.49\nÂ±0.04\n73.42\nÂ±1.75\n81.77\nÂ±0.54\n82.63\nÂ±0.11\n87.36\nÂ±0.10 84.71\nISS(Large-scale) 109M 0.72G 8.30E18 94.22\nÂ±0.04\n93.53\nÂ±0.00\n72.27\nÂ±0.20\n94.57\nÂ±0.06\n74.53\nÂ±1.38\n83.12\nÂ±0.16\n83.31\nÂ±0.36\n87.41\nÂ±0.02 85.36\n1 For ISS, data size is reported by averaging over eight tasks.\n2 The training compute (FLOPs) is calculated by (6 Ã—Training Tokens Ã—Parameter Size) as in Kaplan et al. (2020).\n3 ISS utilizes 20% of the TLM size data, so we implemented the TLM model with the same size version.\n4 For a fair comparison, we implement TLM(Large) with BERT base and TLM large scale dataset.\nTable 3: Evaluation results for ISS at three different training scales. For each task, we report the average F1 score\nacross three random seeds with standard deviations as subscripts. We also show the number of parameters, the total\ntraining compute (FLOPs), and the size of training corpus for comparison.\n4.2 Further Pretraining\nDatasets. We perform further pretraining in\nbiomedical science and computer science domains.\nSpecifically, we conduct experiments on four\ndatasets, including CHEMPROT (Kringelum et al.,\n2016), RCT (Dernoncourt and Lee, 2017), ACL-\nARC (Jurgens et al., 2018), SCIERC (Luan et al.,\n2018). For the pretraining stage, we collect the\nunlabeled datasets from S2ORC (Lo et al., 2020).\nBaselines. We select general PTMs (Devlin\net al., 2019; Liu et al., 2019) and domain-specific\nfurther pretraining models (Lee et al., 2020; Belt-\nagy et al., 2019; Gururangan et al., 2020) as our\nbaselines. Finetuning on the end-task occurs after\nfurther pretraining on domain unlabeled corpora.\nEvaluation Strategy. Similar to pretraining\nfrom scratch, we report the average performance\nacross three random seeds. And we report the\nmicro-F1 for ChemProt and RCT, and macro-F1\nfor ACL-ARC and SCIERC.\nTraining Details. In this setting, we perform\nfurther pretraining on off-the-shelf pretrained mod-\nels, such as BERT and RoBERTa. All experiments\nwere conducted on 4 NVIDIA GeForce RTX 3090\nGPUs. Detailed hyper-parameters are provided in\nTable A.2 in Appendix.\n5 Experimental Results\nIn this section, we will discuss the results of\ncomparing our methods against other baselines.\n5.1 Pretraining from Scratch\nTable 3 shows the main results of ISS with the ac-\ncording TLM and PLMs baselines at three different\nscales. The followings are the related comparison\nand analysis we conducted: 1) ISS could achieve re-\n559\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000053/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000019/uni00000018\n/uni00000019/uni00000019\n/uni00000019/uni0000001a\n/uni00000019/uni0000001b\n/uni00000019/uni0000001c\n/uni0000001a/uni00000013\n/uni0000001a/uni00000014\n/uni0000001a/uni00000015/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni0000002c/uni00000036/uni00000036\n/uni00000037/uni0000002f/uni00000030\n(a) Helpfullness\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000053/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056\n/uni0000001a/uni00000015\n/uni0000001a/uni00000017\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni0000002c/uni00000036/uni00000036\n/uni00000037/uni0000002f/uni00000030 (b) Chemprot\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000053/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000019/uni0000001b\n/uni0000001a/uni00000013\n/uni0000001a/uni00000015\n/uni0000001a/uni00000017\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni0000002c/uni00000036/uni00000036\n/uni00000037/uni0000002f/uni00000030 (c) SciERC\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000053/uni00000055/uni00000048/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000056/uni00000057/uni00000048/uni00000053/uni00000056\n/uni00000018/uni0000001a/uni00000011/uni00000018\n/uni00000019/uni00000013/uni00000011/uni00000013\n/uni00000019/uni00000015/uni00000011/uni00000018\n/uni00000019/uni00000018/uni00000011/uni00000013\n/uni00000019/uni0000001a/uni00000011/uni00000018\n/uni0000001a/uni00000013/uni00000011/uni00000013\n/uni0000001a/uni00000015/uni00000011/uni00000018\n/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni0000002c/uni00000036/uni00000036\n/uni00000037/uni0000002f/uni00000030 (d) ACL-ARC\nFigure 2: Comparison of ISS and TLM at different pretraining steps. The experiments were conducted on the\nsmall-scale dataset, and notably, the data scale of TLM was five times larger than ours.\nModel Param Data FLOPsBIOMED CS Avg.RCT ChemACL SciERC\nBERT-Base109M 16G 2.79E1987.00 81.9469.45 80.9879.84\nRoBERTa-base125M 160G 1.54E2187.23 82.6068.34 81.3579.88\nSciBERT 109M 15G 2.65E19- 83.6470.98 79.97-\nBioBERT 109M 96G 1.80E20- 76.46 - - -\nDAPT 125M 47G 1.58E1887.6 84.275.4 80.882.00\nDAPT+TAPT125M 47G 1.77E1887.884.475.6 81.382.28\nISS-DAPT(BERT) 109M 1.7G 6.9E1787.36\nÂ±0.02\n83.90\nÂ±0.10\n76.06\nÂ±0.70\n83.91\nÂ±0.38\n82.81\nISS-DAPT(RoBERTa)125M 1.7G 7.9E1787.57\nÂ±0.06\n84.88\nÂ±0.10\n76.70\nÂ±0.25\n82.23\nÂ±0.30\n82.85\nTable 4: Evaluation results for ISS in further pretraining.\nWe report the average F1 score across three random\nseeds with standard deviations as subscripts.\nsults that are better than or comparable to the PLM\nbaselines with significant reductions in FLOPs and\nthe size of training data. At the large scale, ISS\nachieves comparable results to RoBERTa-large,\nwith an average of 0.19% of FLOPs and 0.45%\nof the training corpus. At the small and medium\nscales, ISS improves the performance by 0.29\nand 0.74 points on average respectively; 2) At\nthe same data scale, ISS significantly outperforms\nTLM, which indicates that task label information\nis crucial. And the influence-based subset selection\ncan select more influential pertaining samples; 3)\nISS could offer limited performance gains on high-\nresource datasets. It demonstrates that the influence\nof the pretraining samples would be decreased as\nthe task data grows sufficiently.\n5.2 Further Pretraining\nWe compared ISS with other domain-specific fur-\nther pretraining methods. Differently, we initialize\nthe network with off-the-shelf pretrained models to\nprovide initialization and select influential subsets\nfrom the domain corpus. Table 4 shows the main\nAGNews SciERC Chemprot\nISS TLM ISS TLM ISS TLM\n10% 94.34\nÂ±0.08\n94.08\nÂ±0.07\n80.82\nÂ±0.41\n81.41\nÂ±0.16\n80.80\nÂ±0.34\n80.15\nÂ±0.32\n20% 94.40\nÂ±0.06\n94.16\nÂ±0.09\n83.70\nÂ±0.31\n81.21\nÂ±0.44\n82.82\nÂ±0.41\n81.51\nÂ±0.55\n40% 94.14\nÂ±0.05\n94.05\nÂ±0.18\n83.16\nÂ±0.07\n82.48\nÂ±0.43\n81.98\nÂ±0.14\n81.75\nÂ±0.04\n60% 94.08\nÂ±0.02\n94.07\nÂ±0.09\n82.51\nÂ±0.29\n83.05\nÂ±0.20\n82.08\nÂ±0.22\n81.80\nÂ±0.41\n80% 94.17\nÂ±0.04\n94.27\nÂ±0.09\n81.71\nÂ±0.24\n81.75\nÂ±0.15\n81.83\nÂ±0.30\n81.86\nÂ±0.47\nTable 5: Results on the development set with different\ndata scales.\nresults. In conclusion, our method outperforms\nall the baselines, with significant reductions in\nFLOPs and the size of training data by one order\nof magnitude or more. It proves our approach is\nfeasible.\n5.3 Comparison of Pretraining Steps\nTo validate the effect of pretraining steps, we\ncompare the performance of ISS with TLM at\ndifferent pretraining steps. The test results on the\nfour tasks with different pretraining steps are shown\nin Figure 3. We observe that ISS could achieve the\nbest performance with fewer steps on most of the\ndatasets.\n5.4 Subset Size for Pretraining\nTo compare the performance at different data scales,\nwe extracted subsets from the TLM small-scale\ncorpus at different scales via ISS and TLM, re-\nspectively. The results are shown in Table 5. We\ncan observe that the performance of TLM becomes\nbetter as the dataset grows, but the best results are\nstill lower than those of our method. In ISS, the\nF1-score would reach the top at the 20%-40% scale\nand gradually decrease as the data size grows. We\n560\n/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000019/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni0000001c/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000014/uni00000015\n/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000003/uni00000051/uni00000044/uni00000050/uni00000048\n/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013\n/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000013\n/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000015/uni00000013/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n(a) Chemprot\n/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000016/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000019/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni0000001c/uni0000004f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000014/uni00000015\n/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000003/uni00000051/uni00000044/uni00000050/uni00000048\n/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000018\n/uni00000013/uni00000011/uni0000001a/uni0000001c/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni0000001c/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000013\n/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000013/uni00000029/uni00000014/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 (b) SciERC\nFigure 3: F1-score results of ISS with gradients of\ndifferent layers (i.e., Embedding layer, 3/6/9/12-th\nTransformer Block) over Chemprot and SciERC.\nRelated-Label PMI AGNews\nISS(small) /%TLM(small) /%\nimmigration World 1.341 0.0072 0.0070\npolicy World 1.187 0.0493 0.0401\nchina World 0.382 0.0836 0.0695\nmedals Sports 1.400 0.0139 0.0136\ngolds Sports 1.400 0.0009 0.0008\nsports Sports 1.293 0.0459 0.0454\nfinancial Business 1.054 0.0717 0.0567\ncommerce Business 0.844 0.0097 0.0081\nbusiness Business 0.710 0.1170 0.0952\nautomation Sci/Tech 1.420 0.0043 0.0028\ninternet Sci/Tech 1.224 0.0729 0.0524\ntechnology Sci/Tech 1.115 0.0864 0.0661\nTable 6: Comparison of the frequency of task influential\nwords in different subsets.\nbelieve that as the dataset expands, task-irrelevant\nor noisy data is added.\n5.5 Last Better than First\nAs explained in Section 3.3, the last layer of\ngradients of the model encoder is only considered\nto speed up the computation. We have studied the\nrelationship between the gradients at the different\nlayers used in ISS and the corresponding perfor-\nmances. Table 3 shows the results on Chemprot and\nSciERC. We can observe that the closer the layer, to\nthe task head, the better the selected subset works.\nThe phenomena suggest that different layers in the\nlanguage model can capture different information,\nwith layers closer to the task head learning more\ninformation about the task.\nTable 7 shows the times required by ISS calcu-\nlating influences at the different layers. Overall,\nthe time cost of selecting a subset is negligible\ncompared to pretraining. In addition, the computa-\ntional speed based on the last layer would be nearly\nLayer name\nCost times\nSmall Large\nEmbedding 2.0 hours 5.2 hours\n3-th Transformer 1.8 hours 4.8 hours\n6-th Transformer 1.6 hours 4.4 hours\n9-th Transformer 1.4 hours 4.0 hours\n12-th Transformer 1.1 hours 3.6 hours\nTable 7: Comparison of the speed of computing in-\nfluences using different layers. The experiments were\nconducted on Chemport dataset.\ndouble, compared to that at the embedding layer.\n6 Analysis\n6.1 Visualization of Pretrained Model\nWe visualize the task data on ISS-small, BERT,\nand RoBERTa, using the t-SNE algorithm (Van der\nMaaten and Hinton, 2008). The results are shown\nin Figure 4. We can observe that the different\nclasses of deep features in ISS-small formed tighter\nclusters, suggesting that ISS provides better initial-\nization for downstream tasks. In contrast, the fea-\ntures learned by BERT and Roberta are distributed\nrespectively in separate clusters with overlapping\nparts that could not be distinguished.\n6.2 Analyzing of Task-influential Words\nWe compute the point-wise mutual information\n(PMI) (Levy and Goldberg, 2014) between words\nand their corresponding labels in the task dataset.\nBriefly, PMI is used to measure the likelihood of\ntwo events occurring together, so the higher the\nPMI a word has, the more likely it is to be task-\ninfluential. We select words with high PMI as task-\ninfluential words, and compare their frequency in\nISS-small and TLM-small datasets, respectively.\nAs shown in Table 6, the word frequency in the\nISS-small dataset is higher than that in the TLM-\nsmall dataset. Thus, ISS may focus more on task-\ninfluential words.\n7 Related Work\n7.1 Efficient Pretraining for PLMs\nMany attempts have been made to improve the\nefficiency of pretraining. Parallel architectures\n(Shazeer et al., 2018; Wang et al., 2020b) are com-\nmonly used in pretraining. However, parallelism\n561\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015\n/uni0000005b\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000015\n/uni0000005c\n/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000013\n/uni00000014\n/uni00000015\n/uni00000016\n/uni00000017\n/uni00000018\n(a) ISS\n/uni00000013/uni00000011/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018\n/uni0000005b\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000015\n/uni0000005c\n/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000013\n/uni00000014\n/uni00000015\n/uni00000016\n/uni00000017\n/uni00000018 (b) BERT\n/uni00000013/uni00000011/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018\n/uni0000005b\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000015\n/uni0000005c\n/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f\n/uni00000013\n/uni00000014\n/uni00000015\n/uni00000016\n/uni00000017\n/uni00000018 (c) RoBERTa\nFigure 4: Visualization of sentence representation on Chemprot using the t-SNE algorithm (Van der Maaten and\nHinton, 2008). Each color denotes a class.\nwould not actually reduce computational costs in\nterms of FLOPs. For most Transformer-based\nPTMs, as their input sequence goes longer, their\nefficiency is limited by the computation of attention\nweights. Choromanski et al. (2020) and Wang et al.\n(2020a) design low-rank kernels to theoretically\napproximate the original attention weights. Child\net al. (2019) and Roy et al. (2021) introduce spar-\nsity into attention mechanisms by limiting the view\nof each token to a fixed size and separating tokens\ninto several chunks. ELECTRA (Clark et al., 2019)\napplies the replaced token detection which is more\nchallenging. PMI-Masking (Levine et al., 2020)\nselectively masks tokens based on their importance.\nHowever, their improvements are limited, with less\nthan an order of magnitude reduction in computa-\ntional expenses (measured in FLOPs). Orthogonal\nto these works, ISS investigates reducing training\ndata redundancy by the influence of pretraining\ndata points.\n7.2 Further Pretraning in NLP\nContinually pretraining can effectively improve\nPTMsâ€™ performance on new domains or down-\nstream tasks (Gururangan et al., 2020). To achieve\nit, most previous works continually optimize the\npretrained model parameters on a large number\nof corpora collected from the target domain (e.g.,\nscientific (Beltagy et al., 2019), finance(Araci,\n2019) and bio-media (Lee et al., 2020)). However,\nit is computationally expensive to further pretrain\nthe model on a large amount of unlabeled data and\nit may not be feasible to collect such a large scale\nof unlabeled data on certain domains. In contrast,\nISS does not need any additional domain data\nand only utilizes the general corpus. In addition,\nour approach can also be employed for further\npretraining, as we demonstrate in our experiments.\n7.3 Dataset Pruning\nDataset pruning is closely related to the coreset\nselection methods (Mirzasoleiman et al., 2020;\nAgarwal et al., 2004), which try to identify the most\nrepresentative training samples. Several works\n(Killamsetty et al., 2021; Rebuffi et al., 2017;\nToneva et al., 2018) have studied dataset pruning\nfor efficient training of deep learning models in\nsupervised learning and active learning scenarios.\nDataset pruning methods typically rely on a pre-\ndefined criterion to compute a scalar score for each\ntraining example, e.g. the compactness (Rebuffi\net al., 2017), diversity (Sener and Savarese, 2017),\nand forgetfulness (Toneva et al., 2018), and then\nrank and select the training data according to the\ncomputed score. Recently, Yao et al. (2022) pro-\nposed TLM for transfer learning, which retrieves a\nsubset from the pretraining corpus that is more\nsimilar to the task corpus. However, these methods\nare heuristic and lack of generalization guarantee,\nthey also discard the influence interaction between\nthe collected samples. Our proposed method over-\ncomes these shortcomings.\n8 Conclusion\nIn this paper, we propose Influential Subset Selec-\ntion for language model, which aims to reduce the\ncomputational costs of pretraining from data level.\nSpecifically, we introduce influence function to\nmeasure the importance of each pretraining sample.\nMoreover, we design a simple, efficient, gradient\nmatching-based method for influence estimation,\nwhich significantly speeds up the estimation time.\nExperiments on various datasets demonstrate that\nour method achieves comparable performance with\nPTMs, with a reduction of training FLOPs by three\norders of magnitude.\n562\nLimitations\nThere are two potential risks with our method. First,\nISS trades generality for efficiency by learning\nonly task-specific representations. Consequently,\nit may not be suitable for other tasks. Secondly,\nour method is hardly practical for few-shot or zero-\nshot learning, as few or no task data are available\nas anchor points. These potential risks are left to\nfuture work.\nEthics Statement\nPretraining from scratch and further pretraining\nsuch as DAPT need large-scale unlabeled corpus\nto learn general knowledge, which results in cor-\nresponding greenhouse emissions due to energy\nconsumption (Strubell et al., 2019). However, as\nshown in Section 5, our new efficient algorithms\ngreatly increase the data efficiency of PTMs, re-\nducing these harms as well as the various harms\nassociated with labor for data collection. Our\nwork introduces a new subset selection algorithm\nbut leverages pre-existing datasets and models.\nOverall, this work inherits some of the risks of the\noriginal work upon which it is implemented, (such\nas bias (Bender et al., 2021) or privacy leakage\n(Carlini et al., 2021).\nAcknowledgements\nThe authors wish to thank the anonymous reviewers\nfor their helpful comments. This work was partially\nfunded by National Natural Science Foundation of\nChina (No.62206057,61976056,62076069), Shang-\nhai Rising-Star Program (23QA1400200), Natural\nScience Foundation of Shanghai (23ZR1403500),\nProgram of Shanghai Academic Research Leader\nunder grant 22XD1401100, and CCF-Zhipu AI\nLarge Model Fund.\nReferences\nPankaj K Agarwal, Sariel Har-Peled, and Kasturi R\nVaradarajan. 2004. Approximating extent measures\nof points. Journal of the ACM (JACM), 51(4):606â€“\n635.\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models. arXiv preprint\narXiv:1908.10063.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 3615â€“3620,\nHong Kong, China. Association for Computational\nLinguistics.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610â€“623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633â€“2650.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, et al. 2020. Rethinking\nattention with performers. In International Confer-\nence on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nR Dennis Cook and Sanford Weisberg. 1982. Residuals\nand influence in regression. New York: Chapman\nand Hall.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence classi-\nfication in medical abstracts. In Proceedings of the\nEighth International Joint Conference on Natural\nLanguage Processing (Volume 2: Short Papers) ,\npages 308â€“313, Taipei, Taiwan. Asian Federation\nof Natural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n563\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171â€“4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSuchin Gururangan, Ana Marasovi Â´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Donâ€™t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342â€“8360, Online. Association for Computational\nLinguistics.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Chen, HyoukJoong Lee,\nJiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019.\nGpipe: Efficient training of giant neural networks\nusing pipeline parallelism. Advances in neural\ninformation processing systems, 32.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nbert: Improving pre-training by representing and\npredicting spans. Transactions of the Association\nfor Computational Linguistics, 8:64â€“77.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientific field through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391â€“406.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and FranÃ§ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156â€“5165. PMLR.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 829â€“839, Minneapolis,\nMinnesota, USA. Association for Computational\nLinguistics.\nKrishnateja Killamsetty, S Durga, Ganesh Ramakr-\nishnan, Abir De, and Rishabh Iyer. 2021. Grad-\nmatch: Gradient matching based data subset selection\nfor efficient deep model training. In International\nConference on Machine Learning, pages 5464â€“5474.\nPMLR.\nPang Wei Koh and Percy Liang. 2017. Understanding\nblack-box predictions via influence functions. In\nInternational conference on machine learning, pages\n1885â€“1894. PMLR.\nJens Kringelum, Sonny Kim Kjaerulff, SÃ¸ren Brunak,\nOle Lund, Tudor I Oprea, and Olivier Taboureau.\n2016. Chemprot-3.0: a global chemical biology\ndiseases mapping. Database, 2016.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234â€“1240.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2020. Pmi-masking: Principled masking\nof correlated spans. In International Conference on\nLearning Representations.\nOmer Levy and Yoav Goldberg. 2014. Neural word em-\nbedding as implicit matrix factorization. Advances\nin neural information processing systems, 27.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney\nKinney, and Daniel Weld. 2020. S2ORC: The se-\nmantic scholar open research corpus. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4969â€“4983, Online.\nAssociation for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of entities,\nrelations, and coreference for scientific knowledge\ngraph construction. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3219â€“3232, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142â€“150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. 2015. Image-based recom-\nmendations on styles and substitutes. In Proceedings\nof the 38th international ACM SIGIR conference on\nresearch and development in information retrieval,\npages 43â€“52.\nBaharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.\n2020. Coresets for data-efficient training of machine\nlearning models. In International Conference on\nMachine Learning, pages 6950â€“6960. PMLR.\n564\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227â€“\n2237, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. 2017. icarl: In-\ncremental classifier and representation learning. In\nProceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 2001â€“2010.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics, 9:53â€“\n68.\nOzan Sener and Silvio Savarese. 2017. Active learn-\ning for convolutional neural networks: A core-set\napproach. arXiv preprint arXiv:1708.00489.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, et al. 2018. Mesh-tensorflow: Deep learning\nfor supercomputers. Advances in neural information\nprocessing systems, 31.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using\ndeepspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645â€“3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. 2018. An empirical study of example\nforgetting during deep neural network learning. In\nInternational Conference on Learning Representa-\ntions.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020a. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020b. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural\nInformation Processing Systems, 33:5776â€“5788.\nXiao Wang, Shihan Dou, Limao Xiong, Yicheng Zou,\nQi Zhang, Tao Gui, Liang Qiao, Zhanzhan Cheng,\nand Xuanjing Huang. 2022. MINER: Improving\nout-of-vocabulary named entity recognition from an\ninformation theoretic perspective. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5590â€“5600, Dublin, Ireland. Association for\nComputational Linguistics.\nXiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze\nChen, Yuansen Zhang, Rui Zheng, Junjie Ye,\nQi Zhang, Tao Gui, et al. 2023. Instructuie: Multi-\ntask instruction tuning for unified information extrac-\ntion. arXiv preprint arXiv:2304.08085.\nXingcheng Yao, Yanan Zheng, Xiaocong Yang, and\nZhilin Yang. 2022. Nlp from scratch without large-\nscale pretraining: A simple and efficient framework.\nIn International Conference on Machine Learning,\npages 25438â€“25451. PMLR.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nA Detailed Experiment Settings\nTable A.1 lists the detailed hyperparameters of\nISS at different scales for each task on the pre-\ntraining task. On each task, we perform a grid\nsearch for Bp âˆˆ{1, 2, 4, 8} and Batch size(task)\nâˆˆ{1,2,4,8,16} and adjust the training step, batch\nsize, and sequence length to minimize the training\ncost while maintaining competitive performance.\nTable A.2 lists the detailed hyperparameters of\nISS for each task on further pretraining task.\n565\nHyper-ParametersAGNews Hyp. Help. IMDB ACL. SciERC Chem. RCT\nSmall\nScale\nBp 4 1 4 4 4 4 4 4\nBt 16 1 16 16 2 4 8 16\nSource Corpus1 CTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’small\nTraining Data Size2 0.22GB 0.04GB 0.1GB 0.18GB 0.3GB 0.32GB 0.13GB 0.16GB\nTraining Steps 5E4 2E4 1E5 1E5 1E5 1E5 1E5 5E4\nÏ1 1 99 1 19 999 999 999 3\nÏ2 100 20 100 100 100 20 20 20\nBatch Size 256 256 256 256 256 256 256 256\nSequence Length128 128 128 512 128 128 128 128\nMedium\nScale\nBp 4 1 4 4 4 4 4 4\nBt 16 1 16 16 2 4 8 16\nSource Corpus1 CTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’smallCTLMâˆ’small\nTraining Data Size2 0.22GB 0.04GB 0.1GB 0.18GB 0.3GB 0.32GB 0.13GB 0.16GB\nTraining Steps 1.5E5 5E4 1.5E5 1.5E5 1.5E5 1.5E5 1.5E5 1.5E5\nÏ1 1 99 1 19 999 999 999 3\nÏ2 100 20 100 100 100 20 20 20\nBatch Size 256 256 256 256 256 256 256 256\nSequence Length128 128 128 512 128 128 128 128\nLarge\nScale\nBp 4 1 4 4 4 4 4 4\nBt 16 1 16 16 2 4 8 16\nSource Corpus1 CTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’largeCTLMâˆ’large\nTraining Data Size2 0.62GB 0.18GB 0.34GB 2.20GB 0.70GB 0.84GB 0.5GB 0.44GB\nTraining Steps 3E5 1E5 3E5 3E5 3E5 3E5 3E5 3E5\nÏ1 3 99 1 99 999 999 999 3\nÏ2 100 100 1000 100 20 20 100 100\nBatch Size 256 256 256 256 256 256 256 256\nSequence Length128 128 128 512 128 128 128 128\n1 CTLM âˆ’small and CTLM âˆ’large are provided by TLM(Yao et al., 2022).\n2 ISS only uses a tiny subset of the source general corpus for training. We list the data size that are actually used for ISS training.\nTable A.1: Detailed hyper-parameters for ISS of different scales for each task\nHyper-ParametersRCT Chem. Acl. SciERC\nBp 16 8 2 4\nBt 4 4 4 4\nSource Corpus1 CS2ORCCS2ORCCS2ORC CS2ORC\nTrain Data Size2 1.5G 1.5G 1.9G 1.9G\nTraining Steps 5E4 5E4 5E4 5E4\nBatch Size 256 256 256 256\nSequence Length128 128 128 128\n1 CS2ORC is provided by S2ORC(Lo et al., 2020).\n2 ISS only uses a tiny subset of the source general corpus for training. We list the data size that are\nactually used for ISS training.\nTable A.2: Detailed hyper-parameters for ISS in further\npretraining\n566\nACL 2023 Responsible NLP Checklist\nA For every submission:\nâ–¡\u0013 A1. Did you describe the limitations of your work?\nLimitations section\nâ–¡\u0013 A2. Did you discuss any potential risks of your work?\nLimitations section\nâ–¡\u0013 A3. Do the abstract and introduction summarize the paperâ€™s main claims?\nSection 1\nâ–¡\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB â–¡\u0017 Did you use or create scientiï¬c artifacts?\nLeft blank.\nâ–¡ B1. Did you cite the creators of artifacts you used?\nNo response.\nâ–¡ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\nâ–¡ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciï¬ed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\nâ–¡ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiï¬es individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\nâ–¡ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\nâ–¡ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiï¬cant, while on small test sets they may not be.\nNo response.\nC â–¡\u0013 Did you run computational experiments?\nSection 4\nâ–¡\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n567\nâ–¡\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4\nâ–¡\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\nâ–¡ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD â–¡\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4\nâ–¡\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 4\nâ–¡\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participantsâ€™ demographic\n(e.g., country of residence)?\nSection 4\nâ–¡\u0013 D3. Did you discuss whether and how consent was obtained from people whose data youâ€™re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nSection 4\nâ–¡ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\nâ–¡ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n568",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7654891014099121
    },
    {
      "name": "Computer science",
      "score": 0.7470428943634033
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.7412092089653015
    },
    {
      "name": "Computation",
      "score": 0.6821787357330322
    },
    {
      "name": "Matching (statistics)",
      "score": 0.6650826334953308
    },
    {
      "name": "Language model",
      "score": 0.6311748027801514
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5852708220481873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5739079117774963
    },
    {
      "name": "Machine learning",
      "score": 0.4994218349456787
    },
    {
      "name": "Natural language processing",
      "score": 0.4988880157470703
    },
    {
      "name": "Model selection",
      "score": 0.4418724477291107
    },
    {
      "name": "Task analysis",
      "score": 0.43777960538864136
    },
    {
      "name": "Labeled data",
      "score": 0.433754026889801
    },
    {
      "name": "Natural language",
      "score": 0.42904993891716003
    },
    {
      "name": "Statistics",
      "score": 0.1956941783428192
    },
    {
      "name": "Algorithm",
      "score": 0.14842697978019714
    },
    {
      "name": "Mathematics",
      "score": 0.09730768203735352
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}