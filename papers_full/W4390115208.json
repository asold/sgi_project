{
  "title": "Multimodal Large Language Models are Generalist Medical Image Interpreters",
  "url": "https://openalex.org/W4390115208",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097930329",
      "name": "Tianyu Han",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2432098160",
      "name": "Lisa C. Adams",
      "affiliations": [
        "Technical University of Munich",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A11407780",
      "name": "Sven Nebelung",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": [
        "University Hospital Heidelberg",
        "Heidelberg University",
        "National Center for Tumor Diseases",
        "University Hospital Carl Gustav Carus"
      ]
    },
    {
      "id": "https://openalex.org/A2767442332",
      "name": "Keno K. Bressem",
      "affiliations": [
        "Deutsches Herzzentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2097930329",
      "name": "Tianyu Han",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2432098160",
      "name": "Lisa C. Adams",
      "affiliations": [
        "Stanford University",
        "Technical University of Munich",
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A11407780",
      "name": "Sven Nebelung",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": [
        "University Hospital Carl Gustav Carus",
        "National Center for Tumor Diseases",
        "Heidelberg University",
        "University Hospital Heidelberg"
      ]
    },
    {
      "id": "https://openalex.org/A2767442332",
      "name": "Keno K. Bressem",
      "affiliations": [
        "Deutsches Herzzentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": [
        "RWTH Aachen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385448527",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W4388376970",
    "https://openalex.org/W4309475415",
    "https://openalex.org/W2946185430",
    "https://openalex.org/W4385302721",
    "https://openalex.org/W2902644322",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4386697749",
    "https://openalex.org/W4385948838",
    "https://openalex.org/W4386707807",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4388585663",
    "https://openalex.org/W6854691855",
    "https://openalex.org/W2914568698",
    "https://openalex.org/W2955737766",
    "https://openalex.org/W2932083555",
    "https://openalex.org/W2794825826",
    "https://openalex.org/W3121732873",
    "https://openalex.org/W4319323644",
    "https://openalex.org/W4380445884",
    "https://openalex.org/W4387294149",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W2080917769",
    "https://openalex.org/W2032994816",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4382763281",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4287283172",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4387356388",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4288277645",
    "https://openalex.org/W4293476620",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W6857614378",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4387744047",
    "https://openalex.org/W3102785203"
  ],
  "abstract": "Abstract Medicine is undergoing a transformation with the integration of Artificial Intelligence (AI). Traditional AI models, though clinically useful and often matching or surpassing expert clinicians in specific tasks, face a scalability challenge due to the necessity of developing individual models for each task. Therefore, there is a push towards foundation models that are applicable to a wider set of tasks. Our study showcases how non-domain-specific, publicly available vision-language models can be employed as general foundation models for medical applications. We test our paradigm across four medical disciplines - pathology, dermatology, ophthalmology, and radiology - focusing on two use-cases within each discipline. We find that our approach beats existing pre-training methods and is competitive to domain-specific foundation models that require vast amounts of domain-specific training images. We also find that large vision-language models are data efficient and do not require large annotated datasets to reach competitive performance. This allows for the development of new or improved AI models in areas of medicine where data is scarce and will accelerate medical progress towards true multimodal foundation models.",
  "full_text": "Multimodal Large Language Models are\nGeneralist Medical Image Interpreters\nTianyu Han1,*, Lisa C. Adams2,3,*, Sven Nebelung1, Jakob Nikolas Kather4,5, Keno K.\nBressem 6,*,and Daniel Truhn1,*\n1. Department of Diagnostic and Interventional Radiology, University Hospital RWTH\nAachen,Germany\n2. Department of Diagnostic and Interventional Radiology, Technical University of Munich,\nMunich,Germany\n3. DepartmentofRadiology,StanfordUniversity,Stanford,CA,UnitedStates\n4. Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav Carus,\nTechnical UniversityDresden,Dresden,Germany\n5. Medical Oncology, National Center for Tumor Diseases (NCT), University Hospital\nHeidelberg,Heidelberg,Germany\n6. Department of Radiology and Nuclear Medicine, German Heart Center Munich,Munich,\nGermany\n* Contributedequally\nCorresponding author's contact information:\nDaniel Truhn,MD,PhD\nUniversityHospital Aachen\nPauwelsstr.30,52074Aachen\nemail:daniel.truhn@gmail.com\ntelephone:+491703215806\n1\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract\nMedicine is undergoing a transformation with the integration of Artificial Intelligence (AI).\nTraditional AI models, though clinically useful and often matching or surpassing expert\nclinicians in specific tasks, face a scalability challenge due to the necessity of developing\nindividual models for each task. Therefore, there is a push towards foundation models that\nare applicable to a wider set of tasks. Our study showcases how non-domain-specific,\npublicly available vision-language models can be employed as general foundation models\nfor medical applications. We test our paradigm across four medical disciplines - pathology,\ndermatology, ophthalmology, and radiology - focusing on two use-cases within each\ndiscipline. We find that our approach beats existing pre-training methods and iscompetitive\nto domain-specific foundation models that require vast amounts of domain-specific training\nimages. We also find that large vision-language modelsaredataefficientanddonotrequire\nlarge annotated datasets to reachcompetitiveperformance.Thisallowsfor thedevelopment\nof new or improved AI models in areas of medicinewheredata isscarceandwill accelerate\nmedical progresstowardstruemultimodal foundationmodels.\n2\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nIntroduction\nMedicine is in the process of being transformed by Artificial Intelligence (AI). There is now\nample evidence that specialized AI models have clinical use and can reach - or even\nsurpass - the performance of expert clinicians in narrow tasks1–7. However, there is an\ninherent limitation for the development and application of such models: they need to be\ntrained for each task separately. With thousands of potential applications, this is\nunsustainable,limiting the scalabilityandpracticalityofsuch specializedmodels.\nThis is why there is a shift in focus towards foundation models which are envisioned to be\napplicable to a wide range of tasks8,9. Recently, first steps towards this goal have been\ntaken: Zhou et al. demonstrated the performance of a foundation model for generalizable\ndisease detectionfromretinal images10.Thismodel hasbeentrained onover amillionretinal\nimages and can be applied to both fundoscopies and optical coherence tomography of the\nretina to diagnose a range of ocular diseases. Similarly, Huang et al. trained a foundation\nmodel for pathology image analysis by making use of the histopathological images that are\navailable on X (formerly Twitter) together withaccompanyingtexts11,12.Bothofthesemodels\nhave two things in common: they are each applicable toa wider,butstill limiteddomain and\nthey require a large dataset of training images from that particular domain. In that sense,\nthese models are morefoundational thanthespecializedAImodels,buttheystill fall shortof\nbeingtruefoundational models9,13.\nWe take the next stepbydemonstrating thatitisnotnecessarytotrainsuchdomain-specific\nfoundation models for downstream tasks. We instead employ a large publicly available\nvision-language model that has been trained on non-domain-specific data from the internet\nas a general foundation model. Large vision-language models cananswer complexmedical\nquestions almost on par with human experts14,15 (Figure 1). We test this model in four\nmedical fields heavilyreliantonimage classification:pathology,dermatology,ophthalmology,\nand radiology. In particular, we select two use-cases for each of these disciplines and\nexamine how the large vision-language model internally represents these images and\nwhether this internal representation allows distinguishing between various medical\nsubclassesfor downstreamclassification.\nBy demonstrating thecompetitive performanceofthislargevision-languagemodel,wesolve\na serious problem that has so far impeded medical research: there is no need for large\nannotated datasetsanymoreifyouaimto train amedical foundationmodel.Instead,you can\nutilize existing vision-language models and fine-tune them to your downstream task with\nlimited labeleddata.\nWe thus deliver proof that the era of general foundation models in medicine has already\nbegun and that these models can accelerate medical progress by democratizing access to\nfoundationmodels.\n3\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nMethods\nEthics Approval\nThis study was conducted in accordance with the tenets of the Declaration of Helsinki and\nwasapprovedbythelocal institutional reviewboard(EK259/22).\nPatientCohorts and Imaging Data\nIn this study, we systematically examined medical imagingdatasetsacrossfour keymedical\ndisciplines: pathology, dermatology, ophthalmology, and radiology. We conducted two\nspecific image classification tasks within each discipline, resulting in a total of eight distinct\ntasks(T ),see Figure 2aandTable 1:\nTissue Classification in Histopathology Images (T1): Using the NCT-CRC-HE-100K\ndataset, this task includes histological imaging data from 136 colorectal cancer patients.\nFollowing the dataset partitioning proposed by Kather et al16, we formed a training set of\n100,000 image patches from 86 patients and a test set of 7,180 patches from 50 patients.\nEach patch, measuring 224×224 pixels, is classified into one of nine tissue categories:\nadipose tissue, background, debris, lymphocytes, mucus, smooth muscle, normal colonic\nmucosa,cancer-associated stroma,andcolorectal adenocarcinomaepithelium 16.\nNuclear Classification in Histopathology Images (T2): This task uses the PanNuke\ndataset, which contains 7,558 pan-cancer images from 19 different organ types17. These\nimages, which were annotated by Gamper etal.,includevariousnuclear categoriessuchas\nneoplastic, inflammatory, connective, epithelial, and dead tissue, including both apoptotic\nandnecroticcells.\nLesion Detection in Dermatology (T3): For this task, we utilized the 2018 International\nSkin Imaging Collaboration (ISIC) Challenge dataset, comprising 10,208 training and 1,512\ntesting images of various skin lesions. Classifications include melanoma, basal cell\ncarcinoma,and several other lesiontypes,asdetailedintheworkbyTschandl etal 18,19.\nMelanoma Classification in Dermatology (T4): Derived from the ISIC 2020 Challenge,\nthis task includes dermatology data with images labeled as benign or malignant20. The\ndataset, which differs from the 2018 challenge, includes 26,045 images for training and\n7,081for testing,stratifiedbypatient(1,644patientsfor training,412for testing).\nDiabetic Retinopathy Grading in Fundoscopic Images (T5):: We sourced data from the\n2015 EyePACS Diabetic Retinopathy DetectionChallenge21 andthe APTOS-2019Blindness\nDetection Challenge22, totaling 88,700 fundoscopies from 44,350 patients. The combined\ndataset was divided into 73,622 training images (only EyePACS) and18,740testingimages\n(fromEyePACS(7,539patients) andAPTOS-2019).\nGlaucoma Detection in Fundoscopic Images (T6): This task incorporates data from the\nAIROGS23 andODIR-201924 challenges,resultinginalargedatasetof101,442fundoscopies\nfrom54,274patientsfor trainingand7,000fundoscopiesfrom3,500patientsfor testing.\n4\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nLung Disease Detection in Chest Radiographs in Radiology (T7): Using the 'PadChest'\ncohort, this task focuses on radiology data with 86,715 chest radiographs from 59,975\npatients for training and 7,943 radiographs from 7,272 patients for testing25,26. The dataset\nincludes174radiographicfindingsand 19radiological diagnoses 26.\nOsteoarthritis Grading in Knee Radiographs in Radiology (T8):Employing datafromthe\nOsteoarthritis Initiative (OAI) and the Multicenter Osteoarthritis Study (MOST), this task\ninvolves grading osteoarthritis in knee radiographs27,28. FollowingthemethodologyofHanet\nal.4, we constructed a dataset with 56,185 training images from 6,425 patients and 9,904\ntestingimagesfrom1,095patients.\nTable 1:Detailsoneightimageclassificationtasks.\nTask Type Datasets Training Set Testing Set\nImages Patients Images Patients\nT1 Histopathology NCT-CRC-HE-100K16\nCRC-VAL-HE-7K16\n100,000 136 7,180 50\nT2 Histopathology PanNuke17 4,971 N/A 1,263 N/A\nT3 Dermatology HAM1000019\nISIC201818\n10,208 N/A 1,512 N/A\nT4 Dermatology SIIM-ISIC202020 26,045 1,644 7,081 412\nT5 Ophthalmology EyePACS21 73,622 36,811 15,078 7,539\nAPTOS-201922 0 0 3,662 N/A\nT6 Ophthalmology AIROGS23\nODIR-201924\n101,442\n0\n54,274\n0\n0\n7,000\n0\n3,500\nT7 Radiology PadChest26 86,715 59,975 7,943 7,272\nT8 Radiology OAI27,28\nMOST27,28\n39,921\n16,264\n3,831\n2,594\n7,108\n2,796\n677\n418\nNEJMImage Challenge Benchmarking\nIn this study, we collected 931 clinical cases from the New England Journal of Medicine\n(NEJM) Image Challenge from October 2005 to August 2023. Each case presented a\nmedical image accompanied by a short text describing the clinical context, culminating in a\nspecific question such as \"What is the diagnosis?\" (seeFigure S1for an example). We\nprovided five possible answers for each case and tasked DeepMind's Flamingo model with\nselecting the correct answer.29 The datasetcoveredawiderangeofmedical fields,including\npathology, dermatology, ophthalmology, and radiology, providing a comprehensive mix of\nmedical imaging data. Statistics on the number of correct answers provided by NEJM\nreaders were used to stratify the difficulty ofthe questionsintofiveequal intervalsaccording\ntothepercentage ofcorrectanswersprovidedbyhumanreaders. 3\nWe used a few-shot, in-context learning approach to test Flamingo on the NEJM cases.30\nThis involved providing the first two cases from the dataset (dated October 13th and 20th,\n5\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n2005) to the model (Figure S2). The remaining 929 cases were then used as a test set to\nassessthemodel'sabilityto interpretmedical imagesacrossdifferentdisciplines.\nVision-Language Model\nWe used the open-source Flamingo architecture,31 which was trained by HuggingFaceM4\nand is available in two sizes:Flamingo-80Bwith80billionparametersandFlamingo-9Bwith\n9 billion parameters. Both models are vision-language models that accept text interleaved\nwith images and output free-form text. Flamingo combines a pre-trained large language\nmodel (LLM, LLaMA-65B for Flamingo-80B and Llama-7B for Flamingo-9B 32) and a\npre-trained Vision Transformer (ViT, 632M parameters33) via a transformer-based mapper\n(Perceiver Sampler34). To fuse vision and text signals, Flamingo uses cross-attention layers\ninterleaved with LLM residual blocks (seeFigure 2c). LLaMA-65B was pre-trained on 1.4\ntrillion tokensfrompubliclyavailabledatasources,includingWikipedia,arXiv,Github,Books,\nStackExchange, C4, and CommonCrawl32. The ViT was pre-trained on 2.3 billion images\nobtained from the web as part of the LAION-5B dataset35. The combined Flamingo model\nwas pre-trained for its perceiver samplers and cross-attention blocks on 141 million\ninterleavedimage-textdocumentsand353million images 31.\nTesting Medical Image Interpretation\nTo test the medical reasoning of the models and their ability to stratify medical images for\ndownstream tasks, we use a method similar to recently published approaches36–39, i.e., we\npresenttherespectiveimagesto themodel alongwithageneral prompt,e.g.,\"Whatcanyou\nsee on this radiological image?”. We then extract the representation of the images in the\nmodel's internal latent space and test whether these representations can be used for\nclassification by a simple linear logistic regression model, seeFigure 2c. This concept is\ncalled \"probing the model\" and tests whether the internal representation of the images is\nlinearly separable, i.e. whether the LLM has allocated healthy and pathological images to\nseparateregionsofitshigh-dimensional space.\nCLIPas a Comparison Model\nWe used OpenAI's CLIP (Contrastive Language-Image Pre-training) as a benchmark to\nevaluate Flamingo'sperformance.CLIP,specificallytheCLIP-ViT-B/32model,istrainedona\ncorpus of over 400 million Internet-sourced image-text pairs, providing robust \"zero-shot\"\nlearning capabilities40. We use this baseline model in all tasks T1-T8. As a second baseline\nmodel, focused only on the pathology tasks, we employ PLIP (Pathology Language-Image\nPre-training), which has been trained with contrastive learning specifically on pathology\nimages sourced from X(formerlyTwitter) and hasrecentlybeenpresentedasafoundational\nmodel withstate-of-the-artperformanceinhistopathology 11.\nImage Pre-processing\nImages larger than 1024×1024 pixels were downsampled to 1024×1024 pixels and\nunderwent normalization relative to their maximum pixel value to ensure uniformity across\nthe datasets. T2 and T8 required specific preprocessing: in T2, images of nuclei were\nprocessed according to the work of Huang et al11. The image was considered 'malignant' if\nthe total number of neoplastic cells was more than ten and covered more than 30% of the\n6\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \ntotal cells. Images wereconsidered'benign' ifnoneoplasticcellswerepresent.Thisresulted\nin 2,866 malignant images and 3,368 benign images. For T8, knee radiographs were\npreprocessed to include only a 140 mm×140 mm region using a pre-trained hourglass\nnetworkreportedbyTiulpinetal 41.\nComputational Resources\nWe use four NVIDIA A6000 (48GB) GPUs on a local server systemtoprobethemodels.To\ntrain the logistic regression model on theinternal probesofFlamingoactivations,anNVIDIA\nRTX3090(24GB) GPUwasused.\nEvaluation and Statistical Analysis\nFor T3 to T8, the performance of the classifiers was evaluated by the area under the\nreceiver-operator curve (AUC). For T1andT2,theclassificationperformancewasevaluated\nby the F1 score according to Huang et al11. Standard deviations (SDs) and P values were\ncalculatedusingbootstrappingwith1,000replicatesandpaired 2-tailedt-tests.\nData availability\nTheNEJMchallengequestionsareavailabletothepublicvia:\nhttps://www.nejm.org/image-challenge.The validationdatasetsarepubliclyavailableand\ncanbeaccessedfromthefollowing:Kather Colon(https://zenodo.org/record/1214456);\nPanNuke(https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke);ISIC-2018\n(https://challenge.isic-archive.com/data/#2018);ISIC-2020\n(https://challenge.isic-archive.com/data/#2020);EyePACSDiabeticRetinopathyDetection\n(https://www.kaggle.com/c/diabetic-retinopathy-detection/);\nAPTOS-2019(https://www.kaggle.com/c/aptos2019-blindness-detection);AIROGS\n(https://zenodo.org/records/5793241);ODIR-2019\n(https://odir2019.grand-challenge.org/Download/);PadChest\n(https://bimcv.cipf.es/bimcv-projects/padchest/);OAI\n(https://nda.nih.gov/oai/query-download);MOST\n(https://most.ucsf.edu/multicenter-osteoarthritis-study-most-public-data-sharing).\nCode availability\nThesourcecodescanbeaccessedat https://github.com/peterhan91/Multimodal-Probes.\nTheweightsofopen-sourcedFlamingomodelscanbedownloadedvia\nhttps://huggingface.co/HuggingFaceM4/idefics-80b-instructand\nhttps://huggingface.co/HuggingFaceM4/idefics-9b-instruct.\nOpenAI’sCLIPmodel canbe downloadedvia\nhttps://huggingface.co/openai/clip-vit-base-patch32.\nInferencingofmultimodal LLMswasperformedusingHuggingface transformerslibrary\n(v.4.34.0.dev0,https://huggingface.co/docs/transformers/index) andPyTorch(v.2.0.1,\nhttps://pytorch.org/).AnalysisofLLM’srepresentationswasperformedusingPython\n(v.3.9.17,https://www.python.org/),scikit-learn(v.1.3.0, https://scikit-learn.org/stable/),and\nSciPy(v.1.11.1, https://scipy.org/).\n7\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nResults\nAccuracy in a Complex Diagnostic Challenge\nFirst, weanalyzedFlamingo-80B’sperformanceinansweringclinical vignettequestionsfrom\nthe NEJM challenge to mimic direct human-machine interaction through text. When testing\non 929 cases, Flamingo-80B's primary diagnosis matched the correct diagnosis in 40.4%\n(375of929) ofcases(Figure 1d ).Whenthemodel wasprompted three timesinsuccession,\nit included the correctdiagnosisin54.3%(504of929) ofcases,asdeterminedbystochastic\ntop-K sampling with T=1.0 and top k=50. Notably, Flamingo-80B's performance\noutperformed guesswork at various levels of difficulty, except for the most difficult category\n(Figure 1d). In Figure 1a-c, we illustrate selected Flamingo-80B responses and the\nexplanation as provided by the model. These results highlight Flamingo-80B's ability to\nprovide medical insight and to integrate medical knowledge, albeit with the need for careful\ninterpretationandvalidationinreal-worldsettings.\nSystematic Testing in Histopathology, Dermatology, Ophthalmology, and\nRadiology\nTo put the vision-language models’ performance in a wide range of medical disciplines in\ncontext to existing pre-training methods, we presented image data with textual prompts to\nFlamingo-80BandFlamingo-9B,aswell asusing OpenAI'sCLIPasabenchmark.\nVision-Language Models are Concept Encoders in Pathology\nWe investigated if histopathological images can be stratified byvision-languagemodelsand\ntested twotasks:classificationofcolorectal tissueandclassificationofnuclei.\nThe colorectal tissue classification task (T1) focused on classifying tissue into nine\ncategories based on hematoxylin &eosin-stainedhistologicimagesfromahuman colorectal\ncancer (CRC) cohort. In this task, a linear classifier was trained on internal activations\nobtained from multimodal LLMs and the CLIP model, analyzing a total of 7,158\nhistopathological image patches. The results showed that Flamingo-80B's internal\nrepresentations achieved a higher average F1 score of 0.892 as compared to the CLIP\nmethod, which scored 0.764. Notably, Flamingo-80B also outperformed the domain-specific\nfoundation model developed by Huang et al.11, which was pre-trained on Twitter, withanF1\nscore of 0.892 versus 0.877. Detailed results for the different categories can be found in\nFigure 3a-i.\nIn the nuclear classification task (T2), our goal was to discriminate between benign and\nmalignant cases among samples from 19 different organs using the PanNuke dataset\n(Figure S6). By applying a linear classifier to the internal activations derived from both\nmultimodal LLMs and the CLIP model, Flamingo-80B demonstrated superior performance.\nSpecifically, its internal representations yieldedaconsistentlyhigher F1scoreof0.870(95%\nCI: [0.847 to 0.891]) compared to the baseline CLIP method's 0.797 (95% CI: [0.774 to\n0.821]) (t-statistic=139.7, P<0.001), as detailed in Figure 3j. These results collectively\nconfirm the advanced capabilities of multimodal LLMs over traditional pre-training methods\nin histopathology, even matching the accuracies of specialized foundation models that rely\nondomain-specificdata.\n8\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nVision-Language Models can Interpret Dermatological Images\nWe investigated if photos of skin lesions can be stratified by vision-language models and\nutilized twotaskstocomparetheperformanceofFlamingo-80BtoCLIPpre-training:\nThe first skin lesion task (T3) involves the multiclass classification ofdermatological images\ninto seven classes: melanoma, basal cell carcinoma, actinic keratosis carcinoma,\nmelanocytic nevus, benignkeratinocyticlesions,dermatofibroma,and vascular lesions.After\ntraining the linear classifier on the internal activations extracted from multimodal LLMs and\nthe CLIP model Flamingo-80B's internal representations resulted in a consistently higher\nAUCascomparedwiththebaselineCLIPmethodinall sevenclasses,see Figure 3k-qfor a\nmoredetailedbreakdown(P<0.001for all).\nIn the second skin lesion task (T4) on a separate dataset the models classified 33,126\ndermatological images into malignant or benign lesions. Following the same architecture as\nabove, Flamingo-80B achieved a significantly higher AUC on this task than CLIP (0.885,\n95%CI:[0.859to0.909]vs.0.834,95%CI:[0.810 to0.857],P<0.001),see Figure 3r.\nTogether, these results show that vision-language models that have been pre-trained on\nnon-domain-specifictext- andimage-datacandifferentiatebetweenphotosofdermatological\nlesions.\nVision-Language Models are Generalist Image Interpreters in Ophthalmology\nWe performed additional experiments with two datasets of fundoscopic images to compare\nthevision-languagemodels’ performance topre-trainingwithCLIP.\nT5 focuses on the detectionofdiabeticretinopathyusingover 90,000fundusphotographsin\ntheUSandIndia. Flamingo-80Bshowssuperior performanceingradingdiabeticretinopathy\n(see Figure 4), especially in detecting proliferative and severe diabetic retinopathy (Figure\n4a, b), achieving state-of-the-art results (AUC=0. 949, 95% CI: 0.939 to 0.958; and\nAUC=0.903, 95% CI: 0.889 to 0.917) and significantly outperformed the baseline CLIP\nmodel (AUC=0.883, 95% CI: 0.870 to 0.896 and AUC=0.826, 95% CI: 0.808 to 0.846; P<\n0.001 for both classes). Performance in detecting mild diabetic retinopathy is lower for all\nthree models (Figure 4d), possibly due to class imbalance and labeling ambiguity, with\nFlamingo-80Bperformingbestwithan AUCof0.629 (95%CI:0.612to0.644).\nT6 addresses another significant visual impairment cause, glaucoma, assessed in a large\npatient cohort from Beijing, China, comprising 3,500 individuals42. Here again, the probe\ntrained on the Flamingo-80B activations showed superior performance in AUC (0.868)\ncompared to both its smaller variant, Flamingo-9B (AUC: 0.843; P<0.001),andthebaseline\nCLIPmodel (AUC:0.716;P<0.00, Figure 4f).\nTogether theseresultsshowthegeneral applicabilityofvision-languagemodelstopreviously\nunseen images inophthalmologyunderliningtheir potential for the developmentofgeneralist\nimageinterpreters.\nVision-Language Models can find Radiological Abnormalities\nWe performed two experiments with radiological images to testthe vision-language models’\nidentificationofradiological findingsandcomparetheir performanceCLIP.\nThe chest X-ray classification task (T7) aims at allocating 54 radiographic findings to chest\nX-rays from the PadChest dataset. We utilized 94,658 chest X-rays of which 27.9% were\nlabeled manually by board-certified radiologists. A subset of 7,943 manually labeled chest\nX-rays was set aside for testing. After training the linear classifier ontheinternal activations\nof the multimodal LLMs, Flamingo-80B led to an AUC of at least0.90in7findingsandofat\n9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nleast 0.70 in 40 findings. CLIP achieved these AUC thresholds in none and only 6 findings,\nrespectively,see Figure 5.\nT8 investigates the performance of diagnosing osteoarthritis (OA) in knee X-rays. OA was\ngraded based onmanual labelsbyboard-certifiedradiologists. 4 Againtraining alinear model\non the internal activations led to the superior performance of Flamingo-80B in severe OA\n(0.971, 95% CI: 0.965 to 0.976), moderate OA (0.870, 95% CI: 0.860 to 0.880), and noOA\n(0.815, 95% CI: 0.807 to 0.824). CLIP’s performance wasconsistentlylower with anAUCof\n(0.907, 95% CI: 0.894 to 0.920) in severe OA, (0.734, 95% CI: 0.720 to 0.748) in moderate\nOA,and(0.706,95%CI:0.696to0.715) in noOA,see Figure 4g-k.\nTogether these resultsdemonstratethatvision-languagemodelscanidentifyawiderangeof\nradiological findingswithouthavingbeenspecificallytrainedfor thistask.\nVision-Language Models are data efficient\nOur goal was to determine whether LLMs' inherent knowledge and inference capabilities\ncould facilitate the development of AI modelsusingareducednumber oflabels.Tothisend,\nwe conducted a series of label efficiency experiments.Theseexperimentsweredesignedto\ndetermine the minimum amount of training data and labels required for LLMs to achieve\nspecificperformancebenchmarksonvariousmedical tasks. 10\nOur results were particularlystrikingwithFlamingo-80B.Usingonly10%ofthetraining data,\nFlamingo-80B was able to retain good performance across four medical disciplines.\nSpecifically, it maintained 95.8% (comparing an F1 score of 0.855 with 10% data to an F1\nscore of 0.892 with 100% data), 94.3% (comparing an AUC of 0.892 with 10% data to an\nAUC of 0.945 with 100% data), 95. 2% (comparing an AUC of 0.764 with 10% data to an\nAUC of 0.803 with 100% data) and 94.7%(comparinganAUCof0.767with10%datatoan\nAUC of 0.810 with 100% data) of its peak performance in pathology, dermatology,\nophthalmology, and radiology, respectively. Detailed results of these findings are shown in\nFigure 6.\nThese results suggest that the knowledge and inference capabilities embedded in\nmultimodal LLMs are highly effective, enabling the development of AI models with limited\namounts of labeled data.8 This feature of LLMs holds great promise for applications where\nlargelabeleddatasetsarenotreadilyavailable.\n10\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nDiscussion\nDeep Learning methods have been applied to avastvarietyofmedical problemsinthepast\ndecade and their performance has increased to such anextentthattheynowequal or excel\nclinical expertsindedicatednarrowtasks 1,43,44.However,modelsthataregoodatsolvingone\ntask,butfail togeneralizeto other tasksareoflimiteduseindailyclinical routine.\nThis has ushered a push towards more general foundation models that are envisioned to\ncarry out a diverse set of tasks using very little or no task-specific labeled data8. Firststeps\ninthisdirection havebeentakenbygroupsthattraineddomain-specificfoundationmodelsin\nophthalmology10 and histopathology11. However, the training of such models still requires\naccess to very large domain-specific datasets comprising millions of images. This limits the\ndevelopment of such models to a few groups. Furthermore, these models can not be\nregarded as true general foundation models as their applicability is limited to their specific\ndomain. In this work, we present evidence that large vision-language models can serve as\nthe sought general foundation models. We show how these vision-language models can be\nused as medical image interpreter base models that can be fine-tunedtospecifictaskswith\na fraction of the data necessary to train conventional deep learning models. This opens up\nnew possibilities in the application of AI to medical problems: there is a large number of\nmedical tasks for which no AI models have been developed so far due to the scarcity of\ndata. With vision-language models as general image interpreters, these tasks may now be\ntackledwhichcanultimatelybenefitclinical routine.\nWe performed our experiments with publicly available models that have been trained on\npublicly available data in a transparent manner. We thus set ourselves apart from research\non the proprietary model GPT4-Vision45,46 which is touted as the state-of-the-art foundation\nmodel However, not much is known about the internal architecture, the model size, or the\ntraining data of GPT4-Vision. Not least due to its proprietary nature, the extent to which it\ncan be used to drive progress in medicine is thus severely limited. Rather we focus on the\nopen-source vision-language model Flamingo-80B. We show that Flamingo-80B inherently\npossesses medical knowledge and excelsatclassificationtaskswithoutspecializedtraining.\nFor this, we performed an extensive evaluation of eight datasets from four medical\nspecialties comprising more than 450,000 medical images and demonstrated the wide\napplicabilityofour findings.\nOur findings suggest a reevaluation of the current approach to AI in medicine, where\nspecialist models are trained for new applications. Instead, generalist vision-language\nmodels offer a versatile, cost- and data-efficient alternative to the development of multiple\nspecialized models. In addition, their inherent knowledge and ability to process information\nfrom other domains can facilitate the linking ofdifferentdomainswithinthe medical fieldand\ntheincorporationofexistingknowledge 47,48.\nOur work has limitations and leaves room for future research. Specifically, we performed a\nproof-of-concept and focused solely on imaging information. Therefore, we did not\ninvestigate the fusion of imaging information with morecomplextextual information,suchas\npatientreportsor patienthistory.Second,themodel exhibitedhallucinationswhen answering\nsome of the clinical vignette questions for the NEJM challenge. We provided examples in\nFigure S3and Figure S4. There is thus an urgent need for the developmentofsafeguards\nagainstthesehallucinations.\nThird, the NEJMchallengequestionsare nota factual representation oftheclinical workflow,\nbut rather a vignette of clinical cases used to evaluate the LLM's clinical reasoning skills.\n11\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFollow-up studies are necessary to establish the real clinical use of such models. Most\nimportantly, we used LLaMA as the LLM backbone. While there are more powerful models\nlike GPT4-Vision by OpenAI and Gemini Ultra by Google, we were unable totestthesedue\nto their closed nature, but we anticipate that they, along with future open-source LLMs, will\nresultinevenbetter performingvision-languagemodels.\nConclusions\nLarge vision-language modelsthathavebeentrainedonpubliclyavailabledatacanserveas\ngeneral medical image interpreters. They are data-efficient and publiclyavailable,rendering\nthem ideal for the development of new AI models in medical areas where the lack of large\nlabeled datasetshassofar beenprohibitive.\nAcknowledgments\nNone.\nAuthor contributions\nT.H., L.C.A., K.K.B., J.N.K., and D.T. devised the study concept, and D.T. performed the\nreader tests. T.H. wrote the code and performed the performance studies. T.H. andD.T.did\nthe statistical analysis. T.H., L.C.A., K.K.B., J.N.K., and D.T. wrote the first draft of the\nmanuscript.All authorscontributedto correctingthemanuscript.\nCompeting interests\nD.T. holds shares in StratifAI GmbH and reports speaker fees from Bayer, Germany.K.K.B.\nreports speaker fees from Canon Medical Systems Corporation and GE HealthCare. JNK\ndeclares consulting services for Owkin, France; DoMore Diagnostics, Norway; Panakeia,\nUK, and Scailyte, Basel, Switzerland; furthermore JNK holds shares in Kather Consulting,\nDresden, Germany; and StratifAI GmbH,Dresden,Germany,and hasreceivedhonorariafor\nlectures and advisory board participation by AstraZeneca, Bayer, Eisai, MSD, BMS, Roche,\nPfizer andFresenius.Noother disclosuresarereported.\nFunding\nDT is supported by the German Federal Ministry of Education and Research (SWAG,\n01KD2215A; TRANSFORM LIVER), the European Union’s Horizon Europe and innovation\nprogramme (ODELIA, 101057091). K.K.B. reports grants from the European Union\n(101079894) and Wilhelm-Sander Foundation and serves as an advisor for the EU Horizon\n2020 LifeChamps project (875329) and the EU IHI Project IMAGIO (101112053). JNK is\nsupported by the German Federal Ministry of Health (DEEP LIVER, ZMVI1-2520DAT111;\nSWAG, 01KD2215B), the Max-Eder-Programme of the German Cancer Aid (grant\n#70113864),theGerman Federal MinistryofEducationandResearch(PEARL,01KD2104C;\nCAMINO, 01EO2101; SWAG, 01KD2215A; TRANSFORM LIVER, 031L0312A;\nTANGERINE, 01KT2302 through ERA-NET Transcan), the German Academic Exchange\nService (SECAI, 57616814), the German Federal Joint Committee (Transplant.KI,\n01VSF21048) the European Union’s Horizon Europe and innovation programme (ODELIA,\n12\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n101057091; GENIAL, 101096312) and the National Institute for Health and Care Research\n(NIHR,NIHR213331) LeedsBiomedical Research Centre.\n13\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nReferences\n1. Lång,K. et al.Artificial intelligence-supportedscreenreadingversusstandarddouble\nreadingintheMammographyScreeningwithArtificial Intelligencetrial (MASAI):a\nclinical safetyanalysisofa randomised,controlled,non-inferiority,single-blinded,\nscreeningaccuracystudy. Lancet Oncol.24,936–944 (2023).\n2. Eriksen,A.V.,Möller,S.&Ryg,J.UseofGPT-4to DiagnoseComplexClinical Cases.\nNEJM AI(2023) doi:10.1056/AIp2300031.\n3. Han,T. et al.ComparativeAnalysisofGPT-4Vision,GPT-4andOpenSourceLLMsin\nClinical DiagnosticAccuracy:ABenchmarkAgainstHumanExpertise.\n2023.11.03.23297957Preprintathttps://doi.org/10.1101/2023.11.03.23297957(2023).\n4. Han,T. et al.Imagepredictionofdiseaseprogressionfor osteoarthritisbystyle-based\nmanifoldextrapolation. Nat. Mach. Intell.4,1029–1039(2022).\n5. Ardila,D. et al.End-to-endlungcancer screeningwiththree-dimensional deeplearning\nonlow-dosechestcomputedtomography. Nat. Med.25,954–961(2019).\n6. Barata,C. et al.Areinforcementlearningmodel for AI-based decisionsupportinskin\ncancer.Nat. Med.29,1941–1946(2023).\n7. Hannun,A.Y. et al.Cardiologist-level arrhythmiadetectionandclassificationin\nambulatoryelectrocardiogramsusingadeepneural network. Nat. Med.25,65–69\n(2019).\n8. Moor,M. et al.Foundationmodelsfor generalistmedical artificial intelligence.Nature\n616,259–265(2023).\n9. Bommasani,R.,Hudson,D.A.,Altman,E.A.R.&Arora,S.OntheOpportunitiesand\nRisksofFoundationModels.\n10. Zhou,Y.et al.Afoundationmodel for generalizablediseasedetectionfromretinal\nimages.Nature 622,156–163(2023).\n11. Huang,Z.,Bianchi,F.,Yuksekgonul,M.,Montine,T.J.&Zou,J.Avisual–language\nfoundationmodel for pathologyimageanalysisusingmedical Twitter. Nat. Med.29,\n2307–2316(2023).\n14\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n12. Lu,M.Y.,Chen,B.&Mahmood,F.Harnessingmedical twitter datafor pathologyAI. Nat.\nMed. 29,2181–2182(2023).\n13. Thirunavukarasu,A.J. et al.Largelanguage modelsinmedicine. Nat. Med.29,\n1930–1940(2023).\n14. Buckley,T.,Diao,J.A.,Rodman,A.&Manrai,A.K.AccuracyofaVision-Language\nModel onChallengingMedical Cases.Preprintat\nhttps://doi.org/10.48550/arXiv.2311.05591(2023).\n15. Moor,M. et al.Med-Flamingo:aMultimodal Medical Few-shotLearner.Preprintat\nhttp://arxiv.org/abs/2307.15189(2023).\n16. Kather,J.N. et al.Predictingsurvival fromcolorectal cancer histologyslidesusingdeep\nlearning:Aretrospectivemulticenter study. PLoS Med.16,e1002730(2019).\n17. Gamper,J.,Alemi Koohbanani,N.,Benet,K.,Khuram,A.&Rajpoot,N.PanNuke:An\nOpenPan-Cancer HistologyDatasetfor Nuclei InstanceSegmentation and\nClassification.in Digital Pathology(eds.Reyes-Aldasoro,C.C.,Janowczyk,A.,Veta,M.,\nBankhead,P.&Sirinukunwattana,K.) 11–19(Springer International Publishing,2019).\ndoi:10.1007/978-3-030-23937-4_2.\n18. Codella,N. et al.SkinLesionAnalysisTowardMelanomaDetection2018:AChallenge\nHostedbythe International SkinImagingCollaboration(ISIC).Preprintat\nhttps://doi.org/10.48550/arXiv.1902.03368(2019).\n19. Tschandl,P.,Rosendahl,C.&Kittler,H.TheHAM10000dataset,alarge collectionof\nmulti-source dermatoscopicimagesofcommon pigmentedskinlesions. Sci. Data5,\n180161(2018).\n20. Rotemberg,V.et al.Apatient-centricdatasetofimagesandmetadatafor identifying\nmelanomasusingclinical context. Sci. Data8,34(2021).\n21. DiabeticRetinopathyDetection.\nhttps://kaggle.com/competitions/diabetic-retinopathy-detection.\n22. APTOS2019BlindnessDetection.\nhttps://kaggle.com/competitions/aptos2019-blindness-detection.\n15\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n23. deVente,C. et al.AIROGS:Artificial Intelligencefor RObustGlaucomaScreening\nChallenge.Preprintathttps://doi.org/10.48550/arXiv.2302.01738(2023).\n24. Hemelings,R. et al.Ageneralizable deeplearningregressionmodel for automated\nglaucomascreeningfromfundusimages. Npj Digit. Med.6,1–15 (2023).\n25. Han,T.et al.ReconstructionofPatient-SpecificConfoundersinAI-basedRadiologic\nImageInterpretationusingGenerativePretraining.Preprintat\nhttps://doi.org/10.48550/arXiv.2309.17123(2023).\n26. Bustos,A.,Pertusa,A.,Salinas,J.-M.&delaIglesia-Vayá,M.PadChest:Alargechest\nx-rayimage datasetwithmulti-label annotatedreports. Med. Image Anal.66,101797\n(2020).\n27. Eckstein,F.,Wirth,W.&Nevitt,M.C.Recentadvancesinosteoarthritisimaging—the\nOsteoarthritisInitiative. Nat. Rev. Rheumatol.8,622–630(2012).\n28. Segal,N.A. et al.TheMulticenter OsteoarthritisStudy:Opportunitiesfor Rehabilitation\nResearch.PM&R 5,647–654(2013).\n29. Alayrac,J.-B. et al.Flamingo:aVisual LanguageModel for Few-ShotLearning.Preprint\nathttps://doi.org/10.48550/arXiv.2204.14198(2022).\n30. Brown,T.B. et al.LanguageModelsare Few-ShotLearners.Preprintat\nhttps://doi.org/10.48550/arXiv.2005.14165(2020).\n31. Laurençon,H. et al.OBELICS:AnOpenWeb-ScaleFilteredDatasetofInterleaved\nImage-TextDocuments.Preprintathttps://doi.org/10.48550/arXiv.2306.16527(2023).\n32. Touvron,H.et al.LLaMA:OpenandEfficientFoundationLanguageModels.Preprintat\nhttp://arxiv.org/abs/2302.13971(2023).\n33. Dosovitskiy,A.et al.AnImageisWorth16x16Words:Transformersfor Image\nRecognitionatScale.Preprintathttps://doi.org/10.48550/arXiv.2010.11929(2021).\n34. Jaegle,A. et al.Perceiver:General PerceptionwithIterativeAttention.Preprintat\nhttps://doi.org/10.48550/arXiv.2103.03206(2021).\n35. Schuhmann,C. et al.LAION-5B:An openlarge-scaledatasetfor trainingnext\ngenerationimage-textmodels.Preprintathttps://doi.org/10.48550/arXiv.2210.08402\n16\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n(2022).\n36. Gurnee,W.&Tegmark,M.LanguageModelsRepresentSpaceand Time.Preprintat\nhttps://doi.org/10.48550/arXiv.2310.02207(2023).\n37. Li,K. et al.EmergentWorldRepresentations:ExploringaSequenceModel Trainedona\nSyntheticTask.Preprintathttps://doi.org/10.48550/arXiv.2210.13382(2023).\n38. Belinkov,Y.ProbingClassifiers:Promises,Shortcomings,andAdvances. Comput.\nLinguist. 48,207–219(2022).\n39. Alain,G.&Bengio,Y.Understandingintermediatelayersusing linear classifier probes.\nPreprintathttps://doi.org/10.48550/arXiv.1610.01644(2018).\n40. Radford,A. et al.LearningTransferableVisual ModelsFromNatural Language\nSupervision.Preprintathttps://doi.org/10.48550/arXiv.2103.00020(2021).\n41. Tiulpin,A.,Melekhov,I.&Saarakkala,S.KNEEL:Knee Anatomical Landmark\nLocalizationUsingHourglassNetworks.Preprintat\nhttps://doi.org/10.48550/arXiv.1907.12237(2019).\n42. ODIR-2019- GrandChallenge. grand-challenge.org\nhttps://odir2019.grand-challenge.org/.\n43. Vanguri,R.S. et al.Multimodal integrationofradiology,pathologyandgenomicsfor\npredictionofresponsetoPD-(L)1blockadeinpatientswithnon-small cell lungcancer.\nNat. Cancer3,1151–1164(2022).\n44. Esteva,A. et al.Dermatologist-level classificationofskincancer with deepneural\nnetworks.Nature 542,115–118(2017).\n45. OpenAI.GPT-4Technical Report.Preprintathttps://doi.org/10.48550/arXiv.2303.08774\n(2023).\n46. Yang,Z.et al.TheDawnofLMMs:PreliminaryExplorationswithGPT-4V(ision).Preprint\nathttp://arxiv.org/abs/2309.17421(2023).\n47. Singhal,K. et al.Largelanguage modelsencodeclinical knowledge. Nature 1–9(2023)\ndoi:10.1038/s41586-023-06291-2.\n48. Truhn,D.,Reis-Filho,J.S.&Kather,J.N.Largelanguagemodelsshould beusedas\n17\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nscientificreasoningengines,notknowledge databases. Nat. Med.29,2983–2984\n(2023).\n18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigures\nFigure 1: Performance of the multimodal LLM with 80 billion parameters on the NEJM\nImage Challenge Cases. (a)-(c): Selected NEJM cases correctly answered by the\nmultimodal 80B LLM. The model provided the answer along with an explanation that was\nchecked by a board-certified radiologist with 12 years of experience. (d): performance of\nFlamingo-80B in the NEJM challenge as compared to non-selective human participants.\nBars indicate accuracy means; vertical lines indicate standard deviations.NEJM - The New\nEngland Journal ofMedicine.\n19\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure 2: Setup of Experiments.(a)-(b):Flamingo (80Band9B) modelswereevaluatedon\neight image classification tasks of four medical imaging domains. (c): Visualization of the\nprobing of internal statesusedfor theclassification.Bothvisionand LLM-trainedweightsare\nfrozenduringprobing (coloredinlightblue).\n20\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n21\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure 3: Performance in histopathological and dermatological image classification.\n(a-j) F1-score when classifying tissue type in task 1. Linear probes are fine-tuned on each\ndataset (Kather Colon and PanNuke) and evaluated on a hold-out test set. (a) to (i):\nclassification of nine tissue types from colorectal cancer patients using image datafromthe\nKather Colon dataset. (j): Malignancy classification in the PanNuke dataset in task 2. (k-r)\nAUC when classifying skin lesions. The probes are trained onthemultimodal LLM’sinternal\nrepresentations to predict the type of skin lesions (k-q) and malignancy (r). The center of\neach bar represents the mean of the metrics (F1 and AUC) and the error bars indicate the\nSDs.SDsandP-valuesarecalculatedusingbootstrappingandpaired,two-tailedt-tests.\n22\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \n23\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure 4: Performance in ophthalmological and radiological image classification.(a-e):\nGrading of diabetic retinopathy (DR). Linear probesareadaptedtotheEyePACSdatasetby\nfine-tuning and evaluated on a hold-out test set to differentiate different stages of DR, such\nas proliferative DR,mildDR,andnoDReyes.(f):Classificationofreferrable glaucoma.(g-k)\nPerformanceinOAdiagnosisbasedonkneeradiographs.Thecenter ofeachbar represents\nthe mean AUC, and the error bars indicate the SDs. SDsandP-valuesarecalculatedusing\nbootstrappingandpaired,two-tailedt-tests.\n24\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure 5: Detection of imaging and radiological findings on PadChest radiographs.\nMean AUC and SD are shown for each finding with more than 50 entries in the PadChest\ntesting cohort. The top 27 imaging findings are shown in the left panel and the remaining\nimaging findings are shown in the right panel. Flamingo-90B (green) consistently achieves\nhigher AUCthanCLIP(blue).\n25\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure 6: Robustness of our approach to data scarcity.Inall four tasks,theperformance\nof Flamingo-80B is robust to reducing the training data. Tuning on only 10% of the training\ndata, we maintained 95.8%, 94.3%, 95.2%, and 94.7% of the best performances in the\npathology, dermatology, ophthalmology, and radiology tasks, respectively. For tasks with\nmultipleclassificationsubtasks,we givethemeanAUC.SDsisgiven ascoloredbands.\n26\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nOnline Supplement\nFigure S1: An illustrative example of the clinical casedescriptionsandanswer choicesfrom\nthe“NEJMImageChallenge”.\nFigure S2: Two-shot example prompt used to query multimodal LLMs to answer NEJM\nImageChallengequestions.\n27\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure S3: Cases from the NEJM Image Challenge with hallucinations. Flamingo-80B\nanswered these questions correctly but reasoned incorrectly. We observed that multimodal\nLLMscanhallucinatestronglyin certainmedical casessuchas(a),(b),and(c).\n28\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure S4: Selection of NEJM Image Challenge cases that were answered incorrectly.\nFlamingo-80B struggled to give the correct answer in these cases. We observe that\nFlamingo-80Bmainlysufferedfromhallucinations(c) and(d) or misperceptions(a) and (b).\n29\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure S5: Testing the AUC for linear probes trained on each layer of the Flamingo-80B\nmodel. We select one layer (i.e., 32, highlighted in black, dashed lines) in a pre-experiment\nand then use it consistently for all subsequent experiments. In contrast to previously\nreported results,45 representations from the 80B multimodal LLM regularly fluctuate in\nquality across layers. We found that this phenomenon generalizes across evaluations in\npathology (a), dermatology (b), ophthalmology (c), and radiology (d). The SDs of the AUCs\nare plotted in color bands, and the midpoints of the bands indicate the mean value of the\nAUC.\n30\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint \nFigure S6:EvaluationofactivationprobesinthePanNukedatasetwithineachorgan type.\n31\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 29, 2023. ; https://doi.org/10.1101/2023.12.21.23300146doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7790849208831787
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.621527910232544
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6059513688087463
    },
    {
      "name": "Scalability",
      "score": 0.5873810648918152
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5787129998207092
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4744861125946045
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.47081977128982544
    },
    {
      "name": "Machine learning",
      "score": 0.4597049653530121
    },
    {
      "name": "Task (project management)",
      "score": 0.4574122130870819
    },
    {
      "name": "Test set",
      "score": 0.45345139503479004
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4456426203250885
    },
    {
      "name": "Data science",
      "score": 0.41543421149253845
    },
    {
      "name": "Natural language processing",
      "score": 0.3373086452484131
    },
    {
      "name": "Medicine",
      "score": 0.13433349132537842
    },
    {
      "name": "Pathology",
      "score": 0.09445613622665405
    },
    {
      "name": "Programming language",
      "score": 0.08810034394264221
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887968799",
      "name": "RWTH Aachen University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210111460",
      "name": "National Center for Tumor Diseases",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210162051",
      "name": "University Hospital Carl Gustav Carus",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802164966",
      "name": "University Hospital Heidelberg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210148371",
      "name": "Deutsches Herzzentrum München",
      "country": "DE"
    }
  ],
  "cited_by": 15
}