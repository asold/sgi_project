{
  "title": "A Security Risk Taxonomy for Prompt-Based Interaction With Large Language Models",
  "url": "https://openalex.org/W4388926496",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3192247924",
      "name": "Derner, Erik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377459498",
      "name": "Batistič, Kristina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2603534934",
      "name": "Zahálka, Jan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221817472",
      "name": "Babuška, Robert",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Batisti\\v{c}, Kristina",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Zah\\'alka, Jan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Babu\\v{s}ka, Robert",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319793023",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W4378474356",
    "https://openalex.org/W4377864714",
    "https://openalex.org/W4287270166",
    "https://openalex.org/W3172314079",
    "https://openalex.org/W3106051020",
    "https://openalex.org/W3046764764",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4367000527",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4318719246",
    "https://openalex.org/W4380319827",
    "https://openalex.org/W4320843360",
    "https://openalex.org/W4387595846",
    "https://openalex.org/W4366735819",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W4378465191",
    "https://openalex.org/W3123340107",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4378227790",
    "https://openalex.org/W4404518472",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287028455",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4283162604",
    "https://openalex.org/W4321782661",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4362514994",
    "https://openalex.org/W4377371478",
    "https://openalex.org/W4321472144",
    "https://openalex.org/W4389518968",
    "https://openalex.org/W3191132518",
    "https://openalex.org/W2995518182",
    "https://openalex.org/W2832719672",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W4376455086",
    "https://openalex.org/W139675679",
    "https://openalex.org/W4221138850",
    "https://openalex.org/W4313484477",
    "https://openalex.org/W3163378277",
    "https://openalex.org/W3189711920",
    "https://openalex.org/W3133979110",
    "https://openalex.org/W4317910584"
  ],
  "abstract": "As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by specifically focusing on security risks posed by LLMs within the prompt-based interaction scheme, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline and categorizes the attacks by target and attack type alongside the commonly used confidentiality, integrity, and availability (CIA) triad. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.",
  "full_text": "Received 29 June 2024, accepted 10 August 2024, date of publication 26 August 2024, date of current version 17 September 2024.\nDigital Object Identifier 10.1 109/ACCESS.2024.3450388\nA Security Risk Taxonomy for Prompt-Based\nInteraction With Large Language Models\nERIK DERNER\n 1,2, KRISTINA BATISTIČ 3, JAN ZAHÁLKA\n 2,\nAND ROBERT BABUŠKA\n 2,4, (Member, IEEE)\n1ELLIS Alicante, 03001 Alicante, Spain\n2Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, 160 00 Prague, Czech Republic\n3Independent Researcher, 1000 Ljubljana, Slovenia\n4Department of Cognitive Robotics, Delft University of Technology, 2628 CD Delft, The Netherlands\nCorresponding author: Erik Derner (erik.derner@cvut.cz)\nThe work of Erik Derner was supported in part by a nominal grant received at the ELLIS Unit Alicante Foundation from the Regional\nGovernment of Valencia in Spain (Convenio Singular signed with Generalitat Valenciana, Conselleria de Innovación, Industria, Comercio y\nTurismo, Dirección General de Innovación); in part by Intel Corporation; and in part by European Union’s Horizon 2020 Research and\nInnovation Programme under Grant 951847. The work of Jan Zahálka and Robert Babuška was supported by European Union’s Horizon\nEurope Research and Innovation Programme under Grant 101070254 CORESENSE, and in part by European Union through the project\nROBOPROX under Grant CZ.02.01.01/00/22_008/0004590.\nABSTRACT As large language models (LLMs) permeate more and more applications, an assessment of their\nassociated security risks becomes increasingly necessary. The potential for exploitation by malicious actors,\nranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a\ngap in current research by specifically focusing on security risks posed by LLMs within the prompt-based\ninteraction scheme, which extends beyond the widely covered ethical and societal implications. Our work\nproposes a taxonomy of security risks along the user-model communication pipeline and categorizes the\nattacks by target and attack type alongside the commonly used confidentiality, integrity, and availability\n(CIA) triad. The taxonomy is reinforced with specific attack examples to showcase the real-world impact\nof these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM\napplications, enhancing their safety and trustworthiness.\nINDEX TERMS Large language models, security, jailbreak, natural language processing.\nI. INTRODUCTION\nLarge language models (LLMs) have taken the world\nby storm, revolutionizing workflows across many applied\nknowledge domains. LLMs are natural language processing\nmodels trained on vast amounts of data, capable of generating\ncoherent and meaningful textual outputs. The most famous\nexample of an LLM architecture is the Generative Pre-trained\nTransformer (GPT) series by OpenAI.\nGPT uses transformers [1] pre-trained on massive amounts\nof text data using unsupervised learning. Once pre-trained,\nGPT can be fine-tuned for tasks such as question answering,\nsentiment analysis, or machine translation. The flagship\nexample of a pioneer tool using GPT is ChatGPT [2], whose\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Agostino Forestiero\n.\nprowess and versatility in generating human-like responses\nto natural language queries has penetrated applications such\nas content creation, text summarization, and software code\ngeneration. With the release of additional LLM-based tools,\nsuch as Anthropic’s Claude and Google’s Gemini, we can\nassume LLMs are here to stay. Alongside their surging\nimportance, there is a growing concern about LLMs’ security\nrisks.\nOne of the main LLM security concerns is posed by\nprompt-based attacks, in which attackers achieve their\nintended malicious outcome solely by manipulating the\nprompt(s) and/or response(s) flowing between the LLM and\nits users. The LLM itself is left intact, the attacker requires\nno knowledge of the model’s architecture, its parameters,\nor access to the machines the model resides on. Note that this\ngreatly lowers the barrier of entry for the attackers. Crafting\n126176\n\n 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\ntraditional adversarial attacks requires non-trivial security\nknowledge and a degree of technical prowess, but a prompt-\nbased attack merely requires the attacker to strategically\nmanipulate the interactions with the LLM.\nFIGURE 1. Examples of prompt-based attacks. Malicious behavior and\noutcomes are colored red.\nIn this paper, we address this topic by focusing on\nprompt security: the discipline of protecting LLMs and\ntheir users from prompt-based attacks by malicious actors.\nGood prompt security directly increases robustness, builds\ntrust, and significantly contributes to transforming LLMs\nfrom experimental prototypes to reliable tools. In recent\nliterature, there has been a growing body of works on safety,\nsocietal, and ethical topics related to LLMs: for instance,\nbiases and discrimination [3], [4], [5], societal and economic\nharm [6], or the impact on academia [7]. These resources are\nhighly relevant to the broader safety and robustness objective.\nStill, to the best of our knowledge, the specific topic of\nprompt security risks of LLMs has not yet been covered\nsystematically.\nLLMs pose a number of risks and prompt security implica-\ntions, illustrated in Figure 1. The ability of LLMs to generate\nconvincing responses can be exploited by malicious actors\nto spread disinformation, launch phishing attacks, or even\nimpersonate individuals [6]. It is crucial to continuously\nmonitor and assess the security vulnerabilities in LLMs and\ndevelop appropriate measures to mitigate them because their\nconsequences are far-reaching. The consequences include\nfinancial losses, data breaches, privacy violations, impacting\nsocial connections, causing emotional harm, and incurring\nreputational damage to individuals and organizations. This\npaper aims to classify different types of LLM prompt security\nrisks and discuss their possible consequences.\nIn particular, this paper brings the following three contri-\nbutions to the ongoing discussion about the impact of LLMs\non society:\n• The key contribution of this paper is a comprehensive\ntaxonomy of security risks associated with prompt-\nbased attacks on LLMs. The taxonomy combines\ncategorization based on the target—the user, the model,\nand a third party—with the confidentiality, integrity, and\navailability (CIA) triad [8] widely used in information\nsecurity.\n• The paper provides a broad list describing relevant\nattack instancesand their potential adversarial impact.\n• By outlining the potential risks and attack vectors\nassociated with LLMs, this paper lays a solid foun-\ndational framework for future research in this area\nthat integrates seamlessly with existing cybersecurity\nframeworks.\nII. RELATED WORK\nMultiple surveys and analyses discuss societal challenges\nand risks associated with LLMs [6], [9], [10]. These risks\ninclude discrimination, misinformation, malicious use, and\nuser interaction-based harm. There is a growing concern\nfor developing safe and responsible dialogue systems that\naddress abusive and toxic content, unfairness, ethics, and\nprivacy issues [11], [12]. Many studies address biases,\nstereotypes, discrimination, and exclusion in LLMs [4],\n[5], [13], [14], [15], and new benchmarks and metrics are\nproposed to mitigate these issues [3], [16].\nLLMs also have the potential to generate false outputs,\nwhich may be harmful, especially in sensitive domains such\nas health and law [6], [17]. Several approaches have been\nsuggested to address various drawbacks associated with\nLLMs, such as statistical frameworks for creating equitable\ntraining datasets [18] and conditional-likelihood filtration to\nmitigate biases and harmful views in LLM training data [19].\nA framework for assessing and documenting risks associated\nwith language model applications called RiskCards was\nintroduced by Derczynski et al. [20]. Regulation of large\ngenerative models is also proposed to ensure transparency,\nrisk management, and non-discrimination [21].\nMany works focus on ChatGPT as the representative\nexample of LLMs due to its widespread adoption and\nextensive utilization in various domains. Five priorities for\nChatGPT’s role in research are suggested by van Dis et al.\n[22]: focusing on human verification, developing rules for\naccountability, investing in truly open LLMs, embracing the\nbenefits of AI, and widening the debate on LLMs. The\nethical concerns related to the use of ChatGPT are addressed\nby Zhuo et al. [23]. The paper highlights the need for\naccountable LLMs due to the potential social prejudice and\ntoxicity exhibited by these models. The specific impact of\nChatGPT on academia and libraries is discussed by Lund and\nWang [7], and the implications on education are explored by\nRudolph et al. [24]\nWhile there is a large body of literature on the risks\nand drawbacks of LLMs in general, there are fewer\nresources on LLM security fundamentals. Among the most\nimportant recent publications is the Top 10 for LLMs [25],\nwhich addresses the urgent need for comprehensive security\nprotocols. It highlights the high-risk issues associated with\nLLMs and provides a valuable resource for developers\nand stakeholders to ensure the safer adoption of this\ntechnology. Iqbal et al. propose a systematic evaluation\nframework for LLM platforms, particularly focusing on the\nsecurity implications of third-party plugins in platforms\nlike ChatGPT [26]. This work underscores the complexities\nintroduced by integrating external services and the need for\nVOLUME 12, 2024 126177\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nrobust security measures. Rao et al. address LLM jailbreak\nattacks, classifying them into seven categories, including\ndirect instruction, syntactical transformation, and cognitive\nhacking [27]. Sun et al. provide a safety assessment of\nChinese LLMs by several criteria, such as insult, unfairness,\nand discrimination [28]. Huang et al. provide an extensive\nsurvey on the safety and trustworthiness of LLMs, analyzing\nvulnerabilities categorized into inherent issues, intentional\nattacks, and unintended bugs [29]. The analysis of security\nattacks on general machine learning models [30] can also\napply to LLMs.\nOne of the most important security issues is the potential\nexposure of private and sensitive data through membership\ninference attacks, where an adversary can extract the training\ndata [31], [32], [33], [34]. The most prominent examples\nof extracting the training data from LLMs can be found\nin the work of Carlini et al. [35], which demonstrates that\nmemorized content, including personal information, could\nbe extracted from GPT-2. Brown et al. [36] discuss privacy\nconcerns regarding LLMs’ tendency to memorize phrases.\nThe authors conclude that existing protection methods\ncannot ensure privacy and suggest addressing the risk by\nusing exclusively public text data to train language models.\nPan et al. [37] demonstrate practical threats to sensitive data\nand propose four different defenses to mitigate the risks.\nShao et al. [38] examine the increasing capability of LLMs\nto aggregate information as they scale up, noting that this\nproficiency is particularly strong when entities have shorter\nco-occurrence distances or higher co-occurrence frequencies.\nIn addition to stealing sensitive data, the attacker may aim to\nsteal the model itself. Modern model extraction attacks are\ncapable of stealing the model with increasingly lower query\nbudgets [39].\nHeidenreich and Williams [40] investigate the use of\nuniversal adversarial triggers to affect the topic and stance\nof natural language generation models, in particular GPT-2.\nPerez et al. [41] propose using red teamingto automatically\ngenerate test cases to identify harmful, undesirable behaviors\nin language models before deployment, avoiding the expense\nof human annotation. Code generation models such as\nGitHub Copilot are widely used in programming, but their\nunsanitized training data can lead to security vulnerabilities\nin generated code [42], [43]. A novel approach to finding\nvulnerabilities in black-box code generation models by\nHajipour et al. [43] shows its effectiveness in finding\nthousands of vulnerabilities in various models, including\nGitHub Copilot, based on the GPT model series. A security\nstudy by Sandoval et al. [44] reveals a positive trend,\nthough: LLM-assisted participants introduced only 10%\nmore security issues in C code than the control group.\nIn addition, LLMs can be used to generate disinformation\nfor malicious purposes [6], such as in phishing [45] or\ntargeting fact verification systems [46]. Moskal et al. [47]\ndemonstrate how LLMs can enhance cyber threat testing by\nautomating the reasoning and decision-making processes in\ncyber campaigns.\nWhile the body of literature on LLM prompt security fun-\ndamentals is limited, several security taxonomies can inspire\nthe development of a novel taxonomy for prompt security\nrisks. The work of Derbyshire et al. [48] categorizes and\nevaluates existing cyber-attack taxonomies. Nai-Fovino et al.\n[49] propose a comprehensive taxonomy for European\ncybersecurity competencies, emphasizing the need for coher-\nent and comprehensive categorization to understand and\nmitigate cyber threats. The A VOIDIT taxonomy proposed by\nSimmons et al. [50] offers a structured approach to classify\ncyber-attacks. The Common Attack Pattern Enumeration and\nClassification (CAPEC) schema described by Barnum [51]\nprovides a foundational framework for representing attack\npatterns. Charfeddine et al. [52], in addition to conventional\ncybersecurity attacks, address jailbreaks and prompt-based\nattacks targeting legitimate third parties, and discuss the\ndefensive use of ChatGPT. Gupta et al. [53] also focus\non jailbreak and third-party attacks but also cover prompt\ninjection attacks.\nA number of LLM security works address the associated\nrisks from the cybersecurity perspective. Addington [54]\ndiscusses selected cybersecurity threats posed by ChatGPT,\nsuch as the risk of information leakage, phishing attacks,\nand manipulation leading to biased and harmful responses.\nSeifried et al. [55] address ChatGPT applications in cyber-\nsecurity both on the offensive and the defensive side.\nRanade et al. [56] demonstrate how transformer-based LLMs\ncan generate fake Cyber Threat Intelligence text, misleading\ncyber-defense systems and performing a data poisoning\nattack. Charan et al. [57] analyze the possible misuse cases\nof ChatGPT and Google Bard by cybercriminals. The authors\ngenerated code for the top 10 techniques from the MITRE 1\ndatabase of cyber-attacks, showing that ChatGPT has the\npotential to perform more sophisticated and better-targeted\nattacks.\nKang et al. [58] address bypassing ChatGPT’s defense\nmechanisms against malicious use through mechanisms such\nas prompt obfuscation, code injection, and payload splitting\ninspired by classic cybersecurity. Li et al. [59] evaluate Chat-\nGPT’s safety defenses and show that they are effective against\ndirect prompts but insufficient when jailbreaking prompts\nare used. The study also explores the LLM capabilities\nintegrated into the Bing search engine, concluding that it is\nsubstantially more vulnerable to direct prompts. Adversarial\nattack research in NLP further spawned a number of works on\nprompt injection attacks that manipulate the model [60], [61],\n[62], jailbreak attacks [63], or a combination of both [64].\nSebastian [65] presents the results of an online survey\nwith ten questions, asking 259 respondents about their views\non ChatGPT’s security. A follow-up study [66] reports\non an online survey with 177 respondents on ChatGPT\nprivacy implications and offers insights into the possible\nstrategies to secure private information in LLMs. The work\nof Shi et al. [67] proposes BadGPT, claimed to be the\n1https://attack.mitre.org/\n126178 VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nfirst backdoor attack against the reinforcement learning from\nhuman feedback (RLHF) fine-tuning used in LLMs. The\nexperimental evaluation is performed with GPT-2.\nTo the best of our knowledge, the prompt security\nimplications of LLMs and, in particular, conversational AI\nsystems such as ChatGPT have not yet been systematically\ncovered in the literature. The taxonomy proposed in this paper\naims to cover this gap and provide a concise, comprehensive\ntool for prompt security risk assessment.\nIII. TAXONOMY OF LLM ATTACKS\nWe propose a taxonomy that delineates different types of\nsecurity threats and their implications in the prompt-based\ninteraction of users with LLMs. It has been designed with\na broad and diverse audience in mind, including model\ndevelopers and owners, LLM users, researchers, and policy-\nmakers. The taxonomy provides a broad, systematic overview\nof relevant security risks, enabling us to understand them\nbetter and develop robust measures to mitigate these threats.\nWe follow the best practices of developing a taxonomy\ninspired by the cybersecurity attack taxonomy analysis [48] to\ndesign a taxonomy that is mutually exclusive, comprehensive,\nand intuitive.\nThe taxonomy exclusively focuses on the user-model\ncommunication pipeline, covering a broad spectrum of\nattacks that exploit prompts or misuse a given trained model.\nSpecifically, we are interested in potential threats within\na black-box setting, where we have no or very limited\nknowledge of the model’s inner workings and lack direct\naccess. The scope of our work excludes classic cyber-attacks\non the infrastructure hosting the model, training backdoors,\nand direct model edits.\nA. CLASSIFICATION BY ATTACK TARGET\nThe main approach to classifying the potential attacks that\nwe consider the most useful is by their target. Three main\ncategories emerge:\n• The user: Disrupting the user’s workflow by compro-\nmising the exchange between the user and the LLM.\n• The model: Disrupting the model or coaxing it into\nunintended outputs.\n• A third party: Utilizing the model as a tool to launch\nattacks on third parties.\nEach category implies a different attack strategy and reflects\ndistinct security considerations.\nB. CLASSIFICATION BY CIA TRIAD\nThe CIA triad [8] is a model used in information security\nto identify the three properties of information that need to\nbe protected: confidentiality, integrity, and availability. Each\nof them specifies one way in which information could be\nthreatened.\n• Confidentiality: Limiting access to interaction with\nthe LLM to the authorized recipient only. In case\nconfidentiality is compromised, information is disclosed\nto undesired individuals.\n• Integrity: Maintaining the accuracy, validity, and com-\npleteness of the interaction with the LLM. In case the\nintegrity is compromised, data can be changed on the\nway between the sender and the recipient without their\nknowledge.\n• Availability: Ensuring that authorized users can access\nthe LLM when needed. If availability is degraded,\nthe service cannot be accessed or used effectively for\nlegitimate purposes.\nThese form the basis of the second criterion in our\ntaxonomy, helping define potential effects on the interaction\nwith an LLM.\nFIGURE 2. Overview of attacks on the user.\nC. ATTACKS TARGETING THE USER\nAttacks on users exploit the vulnerabilities in the user-model\ncommunication pipeline. Attackers may be in-line, intercept-\ning network traffic, or establishing fraudulent services to\nforward queries. Figure 2 provides an overview of the attacks.\nPrompt blocking focuses on interrupting availability [25].\nThe attacker intercepts and discards the user’s prompt,\nessentially creating a break in the communication pipeline.\nSuch attacks could lead to decreased efficiency and user\nfrustration, as users may have to repeatedly send prompts,\noften without understanding why their prompts are not being\nprocessed. This can be particularly impactful when LLMs\nare employed in time-sensitive environments, like emergency\nresponse or medical advice systems. A strategic deployment\nof such an attack could potentially result in significant\ndisruption to important services.\nIn prompt tampering, the attacker targets communication\nintegrity by modifying the user’s prompt before it reaches\nthe model [25], [27], [29], [40], [58], [59]. This could\ninvolve changing prompt semantics to get different results or\ninjecting misleading information. For instance, the attacker\ncould subtly modify a lawyer’s prompt, asking for relevant\ncase precedents to request fictional cases instead, inducing\nhallucination even if the model would otherwise provide a\nfactually correct response. The unwitting user might then\nbase their actions on false information.\nIn contrast to prompt tampering, prompt bloating is an\nattack on availability, where the attacker manipulates the\nuser’s prompt in such a way that it results in an excessively\nVOLUME 12, 2024 126179\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nlong response from the model [25]. This could be achieved\nby appending the original prompt with additional, potentially\nirrelevant or nonsensical requests. The result is a ‘flood’ of\ninformation that the user has to wade through, making it\ndifficult to find the information they actually need. In extreme\ncases, the bloating might be so severe that the response cannot\nbe efficiently processed, effectively blocking the user from\nperforming their tasks.\nResponse blocking hinders availability by preventing\nthe model’s response from reaching the user [25]. The\nattacker might intercept the response or cause a break in the\ncommunication after the model has processed the prompt.\nSimilarly to prompt blocking, this could potentially hinder\nimportant operations if the LLM is being used in a critical\nsystem. Additionally, since the prompt has arrived but the\nresponse has not, this attack can create an illusion of the\nmodel being inefficient and unreliable. This may severely\ndamage user trust.\nResponse tampering is an integrity attack where the\nattacker alters the model’s response before it reaches the\nuser [25]. Tampering could involve removing important\ninformation, adding misinformation, or changing the tone\nof the response. The implications of such attacks could\nrange from minor confusion to serious misinformation. For\ninstance, if a medical practitioner uses an LLM for diagnostic\nassistance, altered responses could lead to misdiagnoses,\nendangering patient health.\nFinally, attacks on confidentiality include eavesdropping,\nwhere the attacker illicitly ‘listens in’ on the prompts and\nresponses being sent between the user and the model [29].\nThis can lead to breaches of privacy, particularly if sensi-\ntive information like personal details, proprietary business\ninformation, or confidential legal advice is being discussed.\nBesides the immediate privacy concerns, such information\ncould be exploited in further attacks or used for blackmail\nor corporate espionage.\nFIGURE 3. Overview of attacks on the model.\nD. ATTACKS TARGETING THE MODEL\nTo safeguard LLMs, it is crucial to anticipate and comprehend\nthe potential security threats targeting the model itself. The\nattack types in this category are depicted in Figure 3 and\ndescribed below.\nPrompt injection involves altering the behavior of the\nmodel to carry out tasks that it was not intended for,\nthereby compromising the model’s integrity [59]. Malicious\nactors might manipulate the model to generate certain\nresponses when specific prompts are asked. In contrast\nto traditional security threats, this kind of attack can\nbe relatively easy to execute, especially with persistent\nor ‘rolling’ sessions, where prompts can incrementally\nsteer the model’s responses. An attacker could manipulate\nan LLM into generating defamatory content or disin-\nformation, undermining the integrity of the model and\nits output.\nData poisoning attacks, originally conceptualized in\ncomputer vision [68], alter data to make it unusable\nas training data. Data poisoning can be used for noble\nreasons, such as protecting one’s privacy [69], [70], [71],\nbut also maliciously, to subtly, yet permanently sabotage\na model’s performance [72]. In the LLM context, a data\npoisoning attack aims to disrupt the usability of user-LLM\nconversations as training data for training or fine-tuning\nfuture LLM models. The attacker conducts a large number\nof conversations where they provide misleading information\nor incorrect feedback, stating a response was correct when it\nwas, in fact, incorrect, or vice versa. Data poisoning attacks\nare only usable on LLMs that use user-LLM conversations as\ntraining data. Data poisoning is an especially attractive attack\nvector on smaller proprietary LLMs that receive less traffic\nbecause the attacker only needs a lower amount of poisoned\nconversations to succeed.\nIn intensive prompt attacks, the attacker sends prompts\nthat require an unusually high computational effort to\nprocess [25]. These intensive prompts can slow down the\nLLM considerably, thereby decreasing its availability by\ndisrupting the service for other users. If used strategically,\nthis could effectively function as a denial-of-service attack.\nThis type of attack potentially impacts a wide array of users,\nfrom individuals to large organizations relying on the LLM\nfor important tasks.\nModel extraction involves an attempt to extract the\nunderlying structure and weights of the LLM, impacting\nthe confidentiality of the model [25], [30], [35], [36], [37].\nIf successful, the attacker would gain access to the ‘blueprint’\nof the model and could create a copy of it. Besides intellectual\nproperty theft, such attacks could enable malicious actors to\nfine-tune the stolen model for nefarious purposes or exploit\nspecific weaknesses in the model that they discover through\nanalysis of the model structure.\nMembership inference attacks aim to compromise the\nconfidentiality of the model’s training data [25], [29], [30],\n[31], [32], [37], [38], [41], [54]. The attacker tries to\ninfer whether specific data instances were included in the\ntraining set of the LLM. The successful execution of such\nan attack could potentially reveal sensitive information,\nsuch as personal details or confidential documents, that\n126180 VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nwere included in the training data. This has serious privacy\nimplications and can also lead to legal issues, especially if\nthe model has been trained on data that should not have been\npublicly accessible.\nInformation gathering is an instance of membership\ninference. It is a wide-ranging attack that could be further\nbroken down into two subcategories:\n• Extracting explicitly protected information.The attacker\ntricks the model into revealing information it has been\nspecifically programmed to withhold. They do this\nby carefully crafting prompts to sidestep the model’s\nsecurity measures and expose the protected information.\n• Extracting sensitive but unprotected information.The\nattacker coaxes the model into disclosing information\nthat it should not share, but that has not been explicitly\nprotected. This may occur because the model does not\nrecognize the information as sensitive, underscoring\nthe challenges in defining what constitutes ‘sensitive’\ninformation in the context of LLMs.\nE. ATTACKS TARGETING A THIRD PARTY\nAs the versatility of LLMs increases, so does the risk of them\nbeing used as tools to target third parties not directly involved\nin the interaction between the user and the model. The attacks\ncan impact the target in any aspect of the CIA triad. In attacks\non third parties, the model serves as a resource for the attacker\nto achieve their malicious goals, often using techniques to\nmanipulate the model to overcome any built-in guardrails.\nThese attacks are outlined in Figure 4 and described below.\nFIGURE 4. Overview of attacks on legitimate third parties.\nWith their ability to generate convincing and contextually\nrelevant text, LLMs can be misused to generate malicious\ntext, which has a number of adversarial uses. Malicious\nactors may leverage LLMs to generate disinformation: craft\ndeceptive narratives or false claims that can then be spread on\nsocial platforms, contributing to the wider problem of ‘fake\nnews’ [11], [56]. The potential for disinformation generation\nby LLMs heightens the need for careful moderation and\nregulation of their use, especially in politically charged or\nsensitive contexts.\nLLMs can be further misused to generate offensive\ncontent, which can then be distributed via various online\nchannels [11], [41], [54]. The capability of these models\nto generate large volumes of text quickly makes them an\nefficient tool for creating hate speech, defamatory statements,\nor other harmful content. The offensive material, when\ndistributed, can cause serious harm and distress to individuals\nor communities targeted by such attacks.\nIn a phishing attack, an attacker poses as a trust-\nworthy entity to trick individuals into providing sensitive\ninformation [45], [54]. LLMs could be used to compose\nhighly convincing phishing e-mails, enabling attackers to\ncarry out these schemes more effectively and on a larger\nscale. For instance, the model can generate personalized\ne-mails that convincingly mimic the style and tone of a\nlegitimate organization, thereby increasing the likelihood of\nunsuspecting recipients falling for the scam.\nThe final flagship example of malicious text generation\nis using LLMs to conduct extensive spam campaigns [41],\n[58]. By automating the creation of vast amounts of unso-\nlicited messages, attackers can overwhelm communication\nchannels or manipulate social discourse. These campaigns\ncan be disruptive, harmful, and difficult to manage due to their\nvolume and speed of generation.\nAnother large group of misuse cases is malicious code\ngeneration [42], [43], [57]. LLMs, such as GPT-3 and\nits successors, have demonstrated an impressive ability to\ngenerate computer code based on prompts. If harnessed\nmaliciously, this could lead to the automatic generation of\nmalicious software or scripts. The generated code could be\nused to exploit software vulnerabilities, carry out cyber-\nattacks, or even automate the creation of malware, which\ncan then be used in more extensive attacks. This provides\nsignificant attack assistance to attackers and lowers the\nbarrier of entry for script kiddies, malicious actors with\nlimited cybersecurity knowledge that only use existing\ntechnology to attempt attacks [47].\nWith their natural language generation capabilities, LLMs\ncould be exploited to generate text that is designed to evade\ndetection by content filters or security systems [58]. For\nexample, an attacker might use an LLM to produce e-mails,\nbypassing content filters. Furthermore, LLMs could be used\nto craft messages that trigger or exploit vulnerabilities in\nsystems that process textual input, much like a traditional\ncode injection attack, but carried out via natural language\nprocessing systems.\nIV. EMPIRICAL INSTANCES OF LLM ATTACKS\nIn order to illustrate the theoretical concepts discussed previ-\nously, this section provides specific instances of LLM attacks.\nSome of these examples draw from existing resources, while\nothers are based on our experimentation with ChatGPT.\nIn this section, we provide a short summary for each custom\ninstance with reference to the Appendix to link the summary\nto the full attack. The experiments were performed using the\nmost recent state-of-the-art OpenAI’s model GPT-4o. 2\n2https://openai.com/index/hello-gpt-4o/\nVOLUME 12, 2024 126181\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nA. INSTANCES OF ATTACKS ON THE USER\nDeception through fraudulent services poses a significant\nsecurity risk tied to the popular and widely-known LLMs\nsuch as ChatGPT. In these scenarios, malicious actors\nexploit the technology to develop counterfeit applications\nor platforms that mimic the LLM or falsely promise\nunrestricted access to its capabilities, thereby threatening\nconfidentiality.\nMalicious actors fabricate applications and services\npromising consistent and free access to the LLM’s features,\nas observed in existing reports. 3 They might also clone\ngenuine websites or applications, e.g., creating a convincing\nfacade of ChatGPT. 4 Users who fall victim to these scams\noften expose their personal information or devices to\nserious risks. Typically, these fraudulent applications target\npopular platforms like Windows and Android. They have the\ncapability to harvest sensitive information from users through\neavesdropping attacks, compromising their confidentiality.\nThey target sensitive data such as credit card numbers and\naccount credentials. Once collected, this information can\nbe exploited for identity theft, financial fraud, or further\ncyber-attacks. These fraudulent applications can get beyond\ndata harvesting to classic cyber-attacks, e.g., by installing\nmalware.\nThe prompt tampering technique is another method that\nmalicious actors can use. This involves manipulating the\nLLM to generate false or misleading outputs in response\nto a user’s legitimate query, thereby compromising the\nintegrity of the information received. An example of this\ntechnique is detailed in Table 1 in the Appendix. Additionally,\nthe prompt bloating technique, illustrated in Table 2 in\nthe Appendix, involves inflating the response to a user’s\nquery to an excessive length, impacting the availability\nby making it difficult for the user to extract useful\ninformation from the response. Both of these techniques\ncan undermine user trust and degrade the overall user\nexperience.\nB. INSTANCES OF ATTACKS ON THE MODEL\nThere are privacy and security concerns associated with\nthe potential disclosure of personal information by LLMs,\ncompromising confidentiality. Attackers attempt to recover\nparts of the training data through membership inference\nattacks, potentially exposing sensitive information. Despite\nimplementing safety measures to prevent the extraction of\npersonal and sensitive information, the risk of accidentally\ndisclosing phone numbers, e-mail addresses, and other\nprivate details remains. For example, Li et al. [59] attempted\nto recover e-mail addresses from ChatGPT, succeeding for\nfrequent e-mails.\nMalicious actors exploit the LLM’s generative capabilities\nto gather information about potential targets, posing another\n3https://www.bleepingcomputer.com/news/security/hackers-use-fake-\nchatgpt-apps-to-push-windows-android-malware/\n4https://blog.cyble.com/2023/02/22/the-growing-threat-of-chatgpt-\nbased-phishing-attacks/\nthreat to confidentiality. The intelligence gathered can be\nused in the early stages of a cyber-attack when attackers seek\nto understand the target better to launch the most effective\nattack. As described on a Reddit thread, 5 ChatGPT can be\ndirected to collect intelligence about a chosen target, thus\nperforming a membership inference attack. For example,\nChatGPT can list information about IT systems employed by\na specific bank; see Table 3 in the Appendix.\nMoreover, ChatGPT’s potential misuse extends to gen-\nerating speculative or harmful content about individuals,\nwhich can lead to reputational damage or privacy violations,\nthus undermining the model’s integrity. In a custom prompt\ninjection attack instance, we were able to make ChatGPT\ndivulge personal information about a prominent politician\n(full instance in Table 4 in the Appendix). It was primed by\na set of role-playing instructions, which we intentionally do\nnot report in full due to ethical concerns.\nC. INSTANCES OF ATTACKS ON A THIRD PARTY\nAlthough LLM-based systems like ChatGPT undergo rigor-\nous fine-tuning processes and use methods such as RLHF\nto continuously improve them, attackers can still exploit\nthem to compromise a third party’s confidentiality, integrity,\nor availability. By ingeniously crafting prompts or engaging\nin role-playing scenarios, users can manipulate the model to\nproduce undesired outputs.\nAdvanced LLMs can generate code, which raises several\nsecurity concerns. For instance, malicious actors could use\nChatGPT to create obfuscated code, making it challenging\nfor security analysts to detect and understand their activities.\nAs an example, we have been able to exploit ChatGPT to pro-\nduce proof-of-concept code for testing Log4j vulnerabilities\n(full instance in Table 5 in the Appendix).\nAs for malicious text generationattacks, we demonstrate\nthat ChatGPT can craft convincing phishing e-mails. For\ninstance, an attacker might direct ChatGPT to write an\ne-mail notifying employees about a salary increase. The\nunsuspecting employee, pleased with the news, would follow\nthe e-mail’s instructions, thereby exposing their device to a\nthreat embedded in an Excel file attachment (full instance in\nTable 6 in the Appendix).\nV. DISCUSSION\nLLM security impacts all actors involved with LLM-\npowered systems: users, LLM stakeholders, policymak-\ners, and, broadly speaking, society in general. Therefore,\nit is quickly becoming an integral part of cybersecurity,\nand it is imperative that LLM security is safeguarded\nnow. In this section, we discuss the contributions of the\nproposed taxonomy toward LLM and cybersecurity best\npractices.\nFirstly, the broad view of LLM attacks presented in\nthe taxonomy reveals a high diversity of the attacks,\n5https://www.reddit.com/r/OSINT/comments/10tq6iz/how_to_use_\nchatgpt_for_osint/\n126182 VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nwhich motivates the need for new defense mechanisms.\nIn LLM-powered systems, a classic cybersecurity policy\nrestricting access to the system to authorized users only\nis certainly important. Notably, attacks on the user hinge\non the third-party attacker gaining unauthorized access to\nthe user-LLM pipeline. On its own, however, this is not a\nsufficient security measure. Similarly to other AI models,\nLLMs are susceptible to adversarial attacks that manipulate\nthe model inputs to achieve an undesirable model output.\nThese can be launched by registered, authenticated users\nwith authorized access. The attack vector is the prompt\nitself, and it is notoriously difficult to disambiguate between\nbenign and adversarial text—one essentially needs an AI to\njudge an AI’s input. With LLMs, this critical weakness is\nexacerbated further because LLM outputs can be used for\nmalicious deeds outside the model’s ecosystem, as evidenced\nby malicious content and code attacks targeting a third party.\nThe proposed taxonomy takes these emerging risks fully\ninto account.\nSecondly, while the attacks described in our taxonomy\ncan be standalone attacks, they can also be stacked together\nto form more complex attacks, referred to as kill chains\nin cybersecurity [73]. In such cases, each attack forms\na building block of a kill chain: a sequence of attacks\nthat completes an objective more complex than achievable\nby a single attack. In the modern context, one kill chain\nmay realistically feature both classic and LLM attacks.\nFor example, consider the following scenario. An attacker\nexecutes a model manipulationattack to obtain from an LLM\nsensitive information about key people in a certain company.\nThen, they perform a malicious text generationattack, asking\nthe LLM to write a phishing e-mail specifically targeting a\nkey person in the company. Finally, they gain unauthorized\naccess to the target company. The LLM attacks defined in the\nproposed taxonomy are scoped to complement the existing\ncyber-attacks.\nFinally, the proposed taxonomy combines a general and\nactor-specific view of LLM security. It is important to\nbalance the general safety of LLM-powered systems for a\nsociety increasingly using AI and the value for individual\nparticipants of the user-LLM exchange. The general view\nis covered by the CIA categorization: we need to protect\nthe confidentiality, integrity, and availability of LLMs for\neveryone in the pipeline. This perspective is especially\nimportant for policymakers, security experts, and scientists.\nThe actor-specific view is covered by the categorization by\nattack target. Users, LLM stakeholders, and third parties can\nuse the proposed taxonomy as a structured, broad framework\nto protect their interests. As a result, the proposed taxonomy\ntakes into account everyone in the pipeline and society in\ngeneral, which we hope will result in broad adoption and\nusefulness.\nVI. CONCLUSION\nThis paper contributes to a critical understanding of LLMs’\npotential misuse and exploitation. Through systematic\nclassification of attack types along the user-model com-\nmunication pipeline, we have provided a comprehensive\ntaxonomy that elucidates the key areas where such misuse\nmight occur: attacks targeting the user, the model, and\nthird parties. We also classified the attacks from the\nperspective of the widely adopted confidentiality, integrity,\nand availability (CIA) triad. These classifications are not\nmerely hypothetical; our detailed exploration of empirical\ninstances corroborates the potential for these attacks to\noccur and demonstrates how the threats can manifest in the\nreal world.\nOur research underscores the complexity of safeguarding\nLLMs. While considerable strides have been made in\nimproving LLM safety measures, our findings reveal that\nthey are not immune to determined or creative misuse. This\nhighlights the need for enhanced security solutions, model\ntraining refinements, and fine-tuning of safety mechanisms to\nsuppress emerging threats. We further illustrate the potential\nfor LLMs to be manipulated into malicious outputs, whether\nthrough fraudulent services, information theft, or harmful\ncontent creation. The reported empirical instances spotlight\nthe risks stemming from the misuse.\nThe proposed taxonomy is intended to serve as a\nframework for future research in the field of LLM security,\nproviding a reference for identifying and addressing potential\nthreats. We encourage researchers and practitioners to build\nupon our taxonomy when considering the potential security\nrisks and ethical implications posed by LLMs. In the pursuit\nof AI benefits, we must ensure that these powerful tools are\nnot used to inflict harm.\nLIMITATIONS\nWhile we made our best efforts to provide a comprehensive\ntaxonomy of potential security risks, there are several\nlimitations. Our exploration is based on the versions and\napplications of LLMs available up until the point of\nwriting, using ChatGPT as the most prominent example.\nAs AI technology rapidly evolves, newer versions of models\nmight present different vulnerabilities or improve upon the\nones we have identified. However, the timelessness of our\nwork is supported by the fact that the risks identified in\nour previous study [74] when using one of the earliest\nGPT-3.5 versions still manifest when using the latest\nGPT-4o model.\nFurthermore, the instances provided in our study do not\ncover all potential misuses or attack vectors. We empirically\ntested only the attacks that did not interfere with the\nfunctioning of the model, the Terms of Use, and any\napplicable legislation. LLMs can be exploited in unique\nand unforeseen ways, particularly by innovative or highly\nskilled threat actors. Our work is intended to encourage future\nresearch aiming at uncovering these evolving threats.\nETHICS STATEMENT\nThe authors affirm that all research was conducted following\nwidely adopted ethical guidelines. Special care was taken to\nVOLUME 12, 2024 126183\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nensure that all experimentation with the ChatGPT model was\nconducted responsibly, with no intent to cause harm or exploit\nvulnerabilities for malicious purposes. We did not use the\nmodel to actually disclose private information or deploy any\npotentially harmful generated content in an attack on a third\nparty; any such outputs shown in the paper were generated\nsolely for illustrative purposes within an ethical boundary\nTABLE 1. Prompt tampering using ChatGPT. The first example shows a\nbenign prompt and response. In the second example, the benign prompt\nwas prepended with malicious instructions.\nTABLE 2. Prompt bloating using ChatGPT. The first example shows a\nbenign prompt and response. In the second example, malicious\ninstructions that generate clutter were appended to the prompt.\nand were anonymized and carefully selected to prevent\nmisuse.\nTABLE 3. An example of using ChatGPT to gather information on the IT\nsystems used by the target. Sensitive identifying information has been\nmasked out to protect privacy.\nTABLE 4. An example of ChatGPT primed to override its ethical behavior\nwithin a role-playing scheme. The model discloses details from the\nprivate life of a public person. Sensitive identifying information has been\nmasked out to protect privacy.\n126184 VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\nThis research is intended to shed light on potential\nvulnerabilities and misuse of LLMs. We believe that by\ndrawing attention to these issues, we can contribute to\nthe ongoing efforts to improve the security and ethical\ndeployment of these models. It is our hope that our work\nwill help to inform the development of safer, more reliable\nAI systems and foster a responsible and ethical approach to\nAI research and development.\nAPPENDIX\nEMPIRICAL INSTANCES OF ATTACKS\nThis appendix lists sessions with ChatGPT demonstrating\nreal-world instances of LLM attacks described in Section IV.\nThe interactions reported were carried out using the version\nof ChatGPT from May 13, 2024, based on GPT-4o, accessed\nvia the web interface.\nTABLE 5. An example of malicious code generation using ChatGPT –\nLog4j vulnerability testing. Part of the code has been masked out in the\noutput.\nTABLE 6. An example of malicious text writing using ChatGPT –\na phishing e-mail.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[2] OpenAI. (Nov. 2022). Introducing ChatGPT. [Online]. Available:\nhttps://openai.com/blog/chatgpt\n[3] J. Dhamala, T. Sun, V . Kumar, S. Krishna, Y . Pruksachatkun, K.-W. Chang,\nand R. Gupta, ‘‘BOLD: Dataset and metrics for measuring biases in open-\nended language generation,’’ in Proc. ACM Conf. Fairness, Accountability,\nTransparency, Mar. 2021, pp. 862–872.\n[4] P. P. Liang, C. Wu, L.-P. Morency, and R. Salakhutdinov, ‘‘Towards\nunderstanding and mitigating social biases in language models,’’ in Proc.\n38th Int. Conf. Mach. Learn., vol. 139, 2021, pp. 6565–6576.\n[5] Y . Chen, C. Mahoney, I. Grasso, E. Wali, A. Matthews, T. Middleton,\nM. Njie, and J. Matthews, ‘‘Gender bias and under-representation in natural\nlanguage processing across human languages,’’ in Proc. AAAI/ACM Conf.\nAI, Ethics, Soc., Jul. 2021, pp. 24–34.\n[6] L. Weidinger et al., ‘‘Taxonomy of risks posed by language models,’’\nin Proc. ACM Conf. Fairness, Accountability, Transparency, Jun. 2022,\npp. 214–229.\n[7] B. D. Lund and T. Wang, ‘‘Chatting about ChatGPT: How may AI and\nGPT impact academia and libraries?’’ Library Hi Tech News, vol. 40, no. 3,\npp. 26–29, May 2023.\n[8] D. Parker, ‘‘Our excessively simplistic information security model and how\nto fix it,’’ ISSA J., vol. 8, no. 7, pp. 12–21, 2010.\n[9] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, ‘‘Understanding the\ncapabilities, limitations, and societal impact of large language models,’’\n2021, arXiv:2102.02503.\n[10] R. Bommasani et al., ‘‘On the opportunities and risks of foundation\nmodels,’’ 2021, arXiv:2108.07258.\n[11] J. Deng, J. Cheng, H. Sun, Z. Zhang, and M. Huang, ‘‘Towards safer\ngenerative language models: A survey on safety risks, evaluations, and\nimprovements,’’ 2023, arXiv:2302.09270.\n[12] M. Brundage, K. Mayer, T. Eloundou, S. Agarwal, S. Adler,\nG. Krueger, J. Leike, and P. Mishkin. (Mar. 2023). Lessons Learned\non Language Model Safety and Misuse. [Online]. Available:\nhttps://openai.com/research/language-model-safety-and-misuse\n[13] H. Devinney, J. Björklund, and H. Björklund, ‘‘Theories of ‘gender’\nin NLP bias research,’’ in Proc. ACM Conf. Fairness, Accountability,\nTransparency, 2022, pp. 2083–2102.\nVOLUME 12, 2024 126185\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\n[14] W. I. Cho, J. Kim, J. Yang, and N. S. Kim, ‘‘Towards cross-lingual\ngeneralization of translation gender bias,’’ in Proc. ACM Conf. Fairness,\nAccountability, Transparency, Mar. 2021, pp. 449–457.\n[15] H. R. Kirk, Y . Jun, F. V olpin, H. Iqbal, E. Benussi, F. Dreyer,\nA. Shtedritski, and Y . Asano, ‘‘Bias out-of-the-box: An empirical analysis\nof intersectional occupational biases in popular generative language\nmodels,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021, pp. 2611–\n2624.\n[16] A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein,\n‘‘Detoxifying language models risks marginalizing minority voices,’’ in\nProc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang.\nTechnol., 2021, pp. 2390–2397.\n[17] S. Lin, J. Hilton, and O. Evans, ‘‘TruthfulQA: Measuring how models\nmimic human falsehoods,’’ in Proc. 60th Annu. Meeting Assoc. Comput.\nLinguistics, 2022, pp. 3214–3252.\n[18] W. Cai, R. Encarnacion, B. Chern, S. Corbett-Davies, M. Bogen,\nS. Bergman, and S. Goel, ‘‘Adaptive sampling strategies to construct\nequitable training datasets,’’ in Proc. ACM Conf. Fairness, Accountability,\nTransparency, Jun. 2022, pp. 1467–1478.\n[19] H. Ngo, C. Raterink, J. G. Araújo, I. Zhang, C. Chen, A. Morisot,\nand N. Frosst, ‘‘Mitigating harm in language models with conditional-\nlikelihood filtration,’’ 2021, arXiv:2108.07790.\n[20] L. Derczynski, H. Rose Kirk, V . Balachandran, S. Kumar, Y . Tsvetkov,\nM. R. Leiser, and S. Mohammad, ‘‘Assessing language model deployment\nwith risk cards,’’ 2023, arXiv:2303.18190.\n[21] P. Hacker, A. Engel, and M. Mauer, ‘‘Regulating ChatGPT and other large\ngenerative AI models,’’ 2023, arXiv:2302.02337.\n[22] E. A. M. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L. Bockting,\n‘‘ChatGPT: Five priorities for research,’’ Nature, vol. 614, no. 7947,\npp. 224–226, Feb. 2023.\n[23] T. Yue Zhuo, Y . Huang, C. Chen, and Z. Xing, ‘‘Red teaming\nChatGPT via jailbreaking: Bias, robustness, reliability and toxicity,’’ 2023,\narXiv:2301.12867.\n[24] J. Rudolph, S. Tan, and S. Tan, ‘‘ChatGPT: Bullshit spewer or the end of\ntraditional assessments in higher education?’’ J. Appl. Learn. Teach., vol. 6,\nno. 1, pp. 342–363, 2023.\n[25] OWASP. (Aug. 2023). OWASP Top 10 for LLM. [Online]. Available:\nhttps://owasp.org/www-project-top-10-for-large-language-model-\napplications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0.pdf\n[26] U. Iqbal, T. Kohno, and F. Roesner, ‘‘LLM platform security: Applying a\nsystematic evaluation framework to OpenAI’s ChatGPT plugins,’’ 2023,\narXiv:2309.10254.\n[27] A. Rao, S. Vashistha, A. Naik, S. Aditya, and M. Choudhury, ‘‘Tricking\nLLMs into disobedience: Formalizing, analyzing, and detecting jail-\nbreaks,’’ 2023, arXiv:2305.14965.\n[28] H. Sun, Z. Zhang, J. Deng, J. Cheng, and M. Huang, ‘‘Safety assessment\nof Chinese large language models,’’ 2023, arXiv:2304.10436.\n[29] X. Huang, W. Ruan, W. Huang, G. Jin, Y . Dong, C. Wu, S. Bensalem,\nR. Mu, Y . Qi, X. Zhao, K. Cai, Y . Zhang, S. Wu, P. Xu, D. Wu,\nA. Freitas, and M. A. Mustafa, ‘‘A survey of safety and trustworthiness\nof large language models through the lens of verification and validation,’’\n2023, arXiv:2305.11391.\n[30] N. Brink, Y . Kamphuis, Y . Maas, G. Jansen-Ferdinandus, J. van Stijn,\nB. Poppink, P. de Haan, and I. Chiscop. (Feb. 2023). Adversarial AI in\nthe Cyber Domain. [Online]. Available: http://resolver.tudelft.nl/uuid\n[31] S. Hisamoto, M. Post, and K. Duh, ‘‘Membership inference attacks on\nsequence-to-sequence models: Is my data in your machine translation\nsystem?’’ Trans. Assoc. Comput. Linguistics, vol. 8, pp. 49–63, Dec. 2020.\n[32] H. Hu, Z. Salcic, L. Sun, G. Dobbie, P. S. Yu, and X. Zhang, ‘‘Membership\ninference attacks on machine learning: A survey,’’ ACM Comput. Surv.,\nvol. 54, no. 11s, pp. 1–37, Jan. 2022.\n[33] M. Li, J. Wang, J. Wang, and S. Neel, ‘‘MoPe: Model perturbation based\nprivacy attacks on language models,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., H. Bouamor, J. Pino, and K. Bali, Eds., Singapore:\nAssociation for Computational Linguistics, Dec. 2023, pp. 13647–13660.\n[Online]. Available: https://aclanthology.org/2023.emnlp-main.842\n[34] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and\nR. Shokri, ‘‘Quantifying privacy risks of masked language mod-\nels using membership inference attacks,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., Y . Goldberg, Z. Kozareva, and\nY . Zhang, Eds., Abu Dhabi, United Arab Emirates: Association for Com-\nputational Linguistics, Dec. 2022, pp. 8332–8347. [Online]. Available:\nhttps://aclanthology.org/2022.emnlp-main.570\n[35] N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-V oss,\nK. Lee, A. Roberts, T. Brown, D. Song, Ú. Erlingsson, A. Oprea, and\nC. Raffel, ‘‘Extracting training data from large language models,’’ in Proc.\n30th USENIX Secur. Symp.Berkeley, CA, USA: USENIX Association,\nAug. 2021, pp. 2633–2650.\n[36] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, and F. Tramèr, ‘‘What does\nit mean for a language model to preserve privacy?’’ in Proc. ACM Conf.\nFairness, Accountability, Transparency, Jun. 2022, pp. 2280–2292.\n[37] X. Pan, M. Zhang, S. Ji, and M. Yang, ‘‘Privacy risks of general-purpose\nlanguage models,’’ in Proc. IEEE Symp. Secur. Privacy (SP), May 2020,\npp. 1314–1331.\n[38] H. Shao, J. Huang, S. Zheng, and K. Chen-Chuan Chang, ‘‘Quantifying\nassociation capabilities of large language models and its implications on\nprivacy leakage,’’ 2023, arXiv:2305.12707.\n[39] C. Dai, M. Lv, K. Li, and W. Zhou, ‘‘MeaeQ: Mount model extraction\nattacks with efficient queries,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., H. Bouamor, J. Pino, and K. Bali, Eds., Singapore:\nAssociation for Computational Linguistics, Dec. 2023, pp. 12671–12684.\n[Online]. Available: https://aclanthology.org/2023.emnlp-main.781\n[40] H. S. Heidenreich and J. R. Williams, ‘‘The Earth is flat and the sun is not a\nstar: The susceptibility of GPT-2 to universal adversarial triggers,’’ in Proc.\nAAAI/ACM Conf. AI, Ethics, Soc., Jul. 2021, pp. 566–573.\n[41] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\nN. McAleese, and G. Irving, ‘‘Red teaming language models with language\nmodels,’’ 2022, arXiv:2202.03286.\n[42] M. Chen et al., ‘‘Evaluating large language models trained on code,’’ 2021,\narXiv:2107.03374.\n[43] H. Hajipour, K. Hassler, T. Holz, L. Schönherr, and M. Fritz, ‘‘CodeLMSec\nbenchmark: Systematically evaluating and finding security vulnerabilities\nin black-box code language models,’’ 2023, arXiv:2302.04012.\n[44] G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and\nB. Dolan-Gavitt, ‘‘Lost at C: A user study on the security\nimplications of large language model code assistants,’’ in Proc.\n32nd USENIX Secur. Symp. Anaheim, CA, USA: USENIX\nAssociation, Aug. 2023, pp. 2205–2222. [Online]. Available: https://\nwww.usenix.org/conference/usenixsecurity23/presentation/sandoval\n[45] R. Karanjai, ‘‘Targeted phishing campaigns using large scale language\nmodels,’’ 2022, arXiv:2301.00665.\n[46] Y . Du, A. Bosselut, and C. D. Manning, ‘‘Synthetic disinformation attacks\non automated fact verification systems,’’ in Proc. AAAI Conf. Artif. Intell.,\n2022, vol. 36, no. 10, pp. 10581–10589.\n[47] S. Moskal, S. Laney, E. Hemberg, and U.-M. O’Reilly, ‘‘LLMs killed the\nscript kiddie: How agents supported by large language models change the\nlandscape of network threat testing,’’ 2023, arXiv:2310.06936.\n[48] R. Derbyshire, B. Green, D. Prince, A. Mauthe, and D. Hutchison,\n‘‘An analysis of cyber security attack taxonomies,’’ in Proc. IEEE Eur.\nSymp. Secur. Privacy Workshops (EuroS PW), Apr. 2018, pp. 153–161.\n[49] I. Nai-Fovino, R. Neisse, J. Hernández-Ramos, N. Polemi, G. Ruzzante,\nM. Figwer, and A. Lazari, ‘‘A proposal for a European cybersecurity\ntaxonomy,’’ Office Eur. Union, Brussels, Belgium, Tech. Rep. JRC118089,\n2019.\n[50] C. Simmons, C. Ellis, S. Shiva, D. Dasgupta, and Q. Wu, ‘‘A VOIDIT: A\ncyber attack taxonomy,’’ Dept. Comput. Sci., Univ. Memphis, Memphis,\nTN, USA, Tech. Rep. CS-09-003, 2009.\n[51] S. Barnum, ‘‘Common attack pattern enumeration and classification\n(CAPEC) schema description,’’ Dept. Homeland Secur., Washington, DC,\nUSA, Tech. Rep., 2008.\n[52] M. Charfeddine, H. M. Kammoun, B. Hamdaoui, and M. Guizani,\n‘‘ChatGPT’s security risks and benefits: Offensive and defensive use-\ncases, mitigation measures, and future implications,’’ IEEE Access, vol. 12,\npp. 30263–30310, 2024.\n[53] M. Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj, ‘‘From ChatGPT\nto ThreatGPT: Impact of generative AI in cybersecurity and privacy,’’ IEEE\nAccess, vol. 11, pp. 80218–80245, 2023.\n[54] S. Addington, ‘‘ChatGPT: Cyber security threats and countermeasures,’’\nSSRN Electron. J., Apr. 2023.\n[55] K. Seifried, S. Heide, B. Filip, V . Manral, L. Ruddigkeit, W. Dula,\nE. E. Cohen, B. Toney, S. Ghose, and M. Bregkou. (Apr. 2023).\nSecurity Implications of ChatGPT . [Online]. Available: https://\ncloudsecurityalliance.org/artifacts/security-implications-of-chatgpt/\n[56] P. Ranade, A. Piplai, S. Mittal, A. Joshi, and T. Finin, ‘‘Generating fake\ncyber threat intelligence using transformer-based models,’’ in Proc. Int.\nJoint Conf. Neural Netw. (IJCNN), Jul. 2021, pp. 1–9.\n126186 VOLUME 12, 2024\nE. Derner et al.: Security Risk Taxonomy for Prompt-Based Interaction With LLMs\n[57] P. V . S. Charan, H. Chunduri, P. M. Anand, and S. K. Shukla, ‘‘From text to\nMITRE techniques: Exploring the malicious use of large language models\nfor generating cyber attack payloads,’’ 2023, arXiv:2305.15336.\n[58] D. Kang, X. Li, I. Stoica, C. Guestrin, M. Zaharia, and T. Hashimoto,\n‘‘Exploiting programmatic behavior of LLMs: Dual-use through standard\nsecurity attacks,’’ 2023, arXiv:2302.05733.\n[59] H. Li, D. Guo, W. Fan, M. Xu, J. Huang, F. Meng, and Y . Song, ‘‘Multi-step\njailbreaking privacy attacks on ChatGPT,’’ 2023, arXiv:2304.05197.\n[60] Z. Zhou, Q. Wang, M. Jin, J. Yao, J. Ye, W. Liu, W. Wang, X. Huang, and\nK. Huang, ‘‘Mathattack: Attacking large language models towards math\nsolving ability,’’ in Proc. AAAI Conf. Artif. Intell., 2024, vol. 38, no. 17,\npp. 19750–19758.\n[61] H. Zhu, Q. Zhao, W. Shang, Y . Wu, and K. Liu, ‘‘LimeAttack: Local\nexplainable method for textual hard-label adversarial attack,’’ in Proc.\nAAAI Conf. Artif. Intell., 2024, vol. 38, no. 17, pp. 19759–19767.\n[62] M. Ye, C. Miao, T. Wang, and F. Ma, ‘‘Texthoaxer: Budgeted hard-label\nadversarial attacks on text,’’ in Proc. AAAI Conf. Artif. Intell., 2022, vol. 36,\nno. 4, pp. 3877–3884.\n[63] X. Qi, K. Huang, A. Panda, P. Henderson, M. Wang, and P. Mittal, ‘‘Visual\nadversarial examples jailbreak aligned large language models,’’ in Proc.\nAAAI Conf. Artif. Intell., 2024, vol. 38, no. 19, pp. 21527–21536.\n[64] S. Schulhoff, J. Pinto, A. Khan, L.-F. Bouchard, C. Si, S. Anati,\nV . Tagliabue, A. Kost, C. Carnahan, and J. Boyd-Graber, ‘‘Ignore this title\nand HackAPrompt: Exposing systemic vulnerabilities of LLMs through\na global prompt hacking competition,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., H. Bouamor, J. Pino, and K. Bali, Eds., Singapore:\nAssociation for Computational Linguistics, Dec. 2023, pp. 4945–4977.\n[Online]. Available: https://aclanthology.org/2023.emnlp-main.302\n[65] G. Sebastian, ‘‘Do ChatGPT and other AI chatbots pose a cybersecurity\nrisk: An exploratory study,’’ Int. J. Secur. Privacy Pervasive Comput.,\nvol. 15, no. 1, pp. 1–11, 2023.\n[66] G. Sebastian, ‘‘Privacy and data protection in ChatGPT and other AI\nchatbots: Strategies for securing user information,’’ Int. J. Secur. Privacy\nPervasive Comput., vol. 15, no 1, pp. 1–14, 2023.\n[67] J. Shi, Y . Liu, P. Zhou, and L. Sun, ‘‘BadGPT: Exploring security\nvulnerabilities of ChatGPT via backdoor attacks to InstructGPT,’’ 2023,\narXiv:2304.12298.\n[68] P. W. Koh and P. Liang, ‘‘Understanding black-box predictions via\ninfluence functions,’’ in Proc. Int. Conf. Mach. Learn., 2017, pp. 1885–\n1894.\n[69] J. Feng, Q.-Z. Cai, and Z.-H. Zhou, ‘‘Learning to confuse: Generating\ntraining time adversarial data with auto-encoder,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 32, 2019, pp. 1–11.\n[70] L. Fowl, M. Goldblum, P.-Y . Chiang, J. Geiping, W. Czaja, and\nT. Goldstein, ‘‘Adversarial examples make strong poisons,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 34, 2021, pp. 30339–30351.\n[71] H. Huang, X. Ma, S. M. Erfani, J. Bailey, and Y . Wang, ‘‘Unlearnable\nexamples: Making personal data unexploitable,’’ in Proc. Int. Conf. Learn.\nRepresent., May 2021, pp. 1–17.\n[72] J. Zahálka, ‘‘Trainwreck: A damaging adversarial attack on image\nclassifiers,’’ 2023, arXiv:2311.14772.\n[73] E. M. Hutchins, M. J. Cloppert, and R. M. Amin, ‘‘Intelligence-driven\ncomputer network defense informed by analysis of adversary campaigns\nand intrusion kill chains,’’ in Leading Issues in Information Warfare and\nSecurity Research, vol. 1. U.K.: Academic 2011, p. 80.\n[74] E. Derner and K. Batistič, ‘‘Beyond the safeguards: Exploring the security\nrisks of ChatGPT,’’ 2023, arXiv:2305.08005.\nERIK DERNER received the M.Sc. degree (Hons.)\nin artificial intelligence and computer vision\nand the Ph.D. degree in control engineering\nand robotics from Czech Technical University\n(CTU) in Prague, Czech Republic, in 2022.\nHe is currently an ELLIS Postdoctoral Researcher\nat the ELLIS Alicante. His research interests\ninclude human-centric artificial intelligence, large\nlanguage models, robotics, sample-efficient model\nlearning, genetic algorithms, and computer vision.\nThe central topics in his research are currently the safety, security, and ethical\naspects of generative and conversational artificial intelligence. His Ph.D.\nthesis received two prestigious awards.\nKRISTINA BATISTIČ received the B.Sc. degree\nin computer and information science from the\nUniversity of Ljubljana, Slovenia, and the double\nmaster’s degree in cybersecurity from the Tech-\nnical University of Denmark and the KTH Royal\nInstitute of Technology, Sweden. She is currently\na Cybersecurity Expert with a strong academic\nand professional background. Since 2016, she\nhas been active in cybersecurity and earned a\nnumber of widely recognized certifications. She\nhas experience working internationally in the field of cybersecurity with\nclients from the private and public sectors, and from small companies to\nglobal enterprises. Her current research, in collaboration with CIIRC CTU\nin Prague and ELLIS Unit Alicante, focuses on identifying and mitigating\nsecurity risks related to large language models. Her key contributions address\nemerging security threats in the field of artificial intelligence.\nJAN ZAHÁLKA received the M.Sc. degree (Hons.)\nin artificial intelligence from Czech Technical\nUniversity in Prague (CTU), in 2013, and the Ph.D.\ndegree in computer science from the University of\nAmsterdam, in 2017. He is currently a Researcher\nwith Czech Institute of Informatics, Robotics and\nCybernetics (CIIRC), CTU. His research interests\ninclude artificial intelligence (AI) and machine\nlearning (ML). His current main research focus\nis AI/ML security, other topics of interest are\nlarge language models, image classification, interactive and/or multimodal\nlearning, and multimedia analytics.\nROBERT BABUŠKA (Member, IEEE) received\nthe M.Sc. degree (Hons.) in control engineer-\ning from Czech Technical University in Prague,\nin 1990, and the Ph.D. degree (cum laude)\nfrom Delft University of Technology (TU Delft),\nThe Netherlands, in 1997. He has had faculty\nappointments with Czech Technical University\nin Prague and with the Electrical Engineering\nFaculty, TU Delft. He is currently a Full Professor\nof intelligent control and robotics with the Faculty\nof Mechanical Engineering, Department of Cognitive Robotics, TU Delft.\nIn the past, he made seminal contributions to the field of nonlinear control\nand identification with the use of fuzzy modeling techniques. His current\nresearch interests include reinforcement learning, adaptive and learning robot\ncontrol, nonlinear system identification, and state estimation. He has been\ninvolved in the applications of these techniques in various fields, ranging\nfrom process control to robotics and aerospace.\nVOLUME 12, 2024 126187",
  "topic": "Computer security",
  "concepts": [
    {
      "name": "Computer security",
      "score": 0.6906994581222534
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.6422300338745117
    },
    {
      "name": "Confidentiality",
      "score": 0.6419422626495361
    },
    {
      "name": "Disinformation",
      "score": 0.5762243270874023
    },
    {
      "name": "Reputation",
      "score": 0.5184800028800964
    },
    {
      "name": "Computer science",
      "score": 0.49250397086143494
    },
    {
      "name": "Internet privacy",
      "score": 0.47437983751296997
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.331574022769928
    },
    {
      "name": "Business",
      "score": 0.31394535303115845
    },
    {
      "name": "Political science",
      "score": 0.26701170206069946
    },
    {
      "name": "Social media",
      "score": 0.20961609482765198
    },
    {
      "name": "World Wide Web",
      "score": 0.10513061285018921
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387155719",
      "name": "ELLIS Alicante",
      "country": null
    },
    {
      "id": "https://openalex.org/I44504214",
      "name": "Czech Technical University in Prague",
      "country": "CZ"
    },
    {
      "id": "https://openalex.org/I98358874",
      "name": "Delft University of Technology",
      "country": "NL"
    }
  ]
}