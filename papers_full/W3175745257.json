{
    "title": "Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental",
    "url": "https://openalex.org/W3175745257",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2767348819",
            "name": "Morteza Rohanian",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A2118269980",
            "name": "Julian Hough",
            "affiliations": [
                "Queen Mary University of London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2252180568",
        "https://openalex.org/W3100031205",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2790147715",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2895445019",
        "https://openalex.org/W2140721185",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2963729456",
        "https://openalex.org/W2338317587",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964078338",
        "https://openalex.org/W3015596056",
        "https://openalex.org/W2121282288",
        "https://openalex.org/W2604693796",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3094909210",
        "https://openalex.org/W2214962597",
        "https://openalex.org/W2146734135",
        "https://openalex.org/W2165487751",
        "https://openalex.org/W2166637769",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2169058332",
        "https://openalex.org/W3177040647",
        "https://openalex.org/W3034323214",
        "https://openalex.org/W3116963205",
        "https://openalex.org/W2023612782",
        "https://openalex.org/W2996888978",
        "https://openalex.org/W4289422206",
        "https://openalex.org/W1557701237"
    ],
    "abstract": "Morteza Rohanian, Julian Hough. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3693–3703\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3693\nBest of Both Worlds: Making High Accuracy Non-incremental\nTransformer-based Disﬂuency Detection Incremental\nMorteza Rohanian and Julian Hough\nCognitive Science Group\nSchool of Electronic Engineering and Computer Science\nQueen Mary University of London\n{m.rohanian, j.hough} @qmul.ac.uk\nAbstract\nWhile Transformer-based text classiﬁers pre-\ntrained on large volumes of text have yielded\nsigniﬁcant improvements on a wide range of\ncomputational linguistics tasks, their imple-\nmentations have been unsuitable for live in-\ncremental processing thus far, operating only\non the level of complete sentence inputs. We\naddress the challenge of introducing meth-\nods for word-by-word left-to-right incremen-\ntal processing to Transformers such as BERT,\nmodels without an intrinsic sense of linear\norder. We modify the training method and\nlive decoding of non-incremental models to de-\ntect speech disﬂuencies with minimum latency\nand without pre-segmentation of dialogue acts.\nWe experiment with several decoding meth-\nods to predict the rightward context of the\nword currently being processed using a GPT-2\nlanguage model and apply a BERT-based dis-\nﬂuency detector to sequences, including pre-\ndicted words. We show our method of incre-\nmentalising Transformers maintains most of\ntheir high non-incremental performance while\noperating strictly incrementally. We also evalu-\nate our models’ incremental performance to es-\ntablish the trade-off between incremental per-\nformance and ﬁnal performance, using differ-\nent prediction strategies. We apply our sys-\ntem to incremental speech recognition results\nas they arrive into a live system and achieve\nstate-of-the-art results in this setting.\n1 Introduction\nConversational systems provide a signiﬁcant ad-\ndition to the present approaches in mental health\ncare delivery. Interactions with these conversa-\ntional agents have been shown to contain observ-\nable indicators of cognitive states, such as the rate\nof ﬁlled pauses and different temporal and turn-\nrelated features (Gratch et al., 2014). Alzheimer’s\nDisease (AD) patients, for example, have trouble\nperforming tasks that leverage semantic informa-\ntion; they have difﬁculties with verbal ﬂuency and\nobject recognition. AD patients speak more slowly\nwith long pauses and spend extra time looking for\nthe correct word, which leads to speech disﬂuency\n(L´opez-de Ipi˜na et al., 2013; Nasreen et al., 2021).\nDisﬂuency markers can be key features for identi-\nfying certain cognitive disorders for application in\nconversational agents (Rohanian et al., 2020).\nSuch conversational systems are primarily used\nfor content processing, which is then analyzed of-\nﬂine. There is much work on detecting disﬂuen-\ncies for ofﬂine analysis of transcripts. However,\ngiven that these disﬂuency detection models do not\nwork for live systems and depend on rich transcrip-\ntion data, including pre-segmentation of dialogue\nacts, to facilitate more cost-effective analysis of\nother data, we need systems capable of performing\ndirectly and incrementally off the speech signal,\nor at least from the results of automatic speech\nrecognition (ASR) as they arrive in the system.\nAs it receives word-by-word data, an incremen-\ntal model must operate with minimum latency and\ndo so without changing its initial assumptions and\ndelivering its best decisions as early as possible\nfollowing the principles outlined in (Hough and\nPurver, 2014). Here we design and evaluating mod-\nels that work with online, incremental speech recog-\nnition output to detect disﬂuencies with varying\nlevels of granularity.\nThe best neural language encoders currently\nused in computational linguistics consider word se-\nquences as a whole, and their implementations have\nbeen unsuitable for live incremental processing.\nTransformers (Vaswani et al., 2017), for instance,\noperate on representations that do not naturally\nhave an organizing principle of linear word or-\nder. We analyze how these models work under\nincremental frameworks, where it is essential to\npresent partial output relying on partial input pro-\n3694\nvided up to a certain time step that may occur in\ninteractive healthcare systems. We explore whether\nwe can adjust such models to function incremen-\ntally and how useful they are in terms of overall\naccuracy and incremental metrics.\nTo further enhance the models’ incremental per-\nformance, we use two general strategies to adjust\nthe training regime and the real-time procedure:\nincremental training (‘chunk-based’ training and\nadd-M training) and incremental decoding (con-\nstant latency and prophecies). We employ three\nprominent decoding methods to predict the right-\nward context of the word currently being processed:\nbeam search, top-k sampling, and top-p sampling.\nWe also measure our models’ incremental perfor-\nmance to set the trade-off between incremental per-\nformance and ﬁnal performance.\n2 Related Work\nAlthough considerable work has been done on de-\ntecting disﬂuencies, much of this work uses tran-\nscripts as texts rather than live speech inputs, with\nthe goal of ‘cleaning’ the disﬂuent content for\npost-processing purposes. They are almost exclu-\nsively conducted on pre-segmented utterances of\nthe Switchboard corpus of telephone conversations\n(Godfrey et al., 1992). Several disﬂuency detec-\ntion efforts involve sentence-based parsing and\nlanguage models (Johnson and Charniak, 2004;\nZwarts et al., 2010). Sequence labeling models\nwith start-inside-outside (BIO) style tags have been\nused in recent neural sequence approaches to disﬂu-\nency detection based on bi-directional Long Short\nTerm Memory (BiLSTM) networks and Transform-\ners, in which the sequences are available in full\n(Zayats et al., 2016; Lou and Johnson, 2020; Wang\net al., 2020).\nSuch ofﬂine methods are insufﬁcient if we in-\ntend to infer meaning from repairs and edit words\nfor disﬂuency detection in real-time, which is ben-\neﬁcial in a healthcare domain dialogue system that\nseeks to get a consistent and clear understanding of\nuser statements and the user’s cognitive state.\nMethods based on strictly incremental opera-\ntion have been rare. Hough and Purver (2014) used\na line of classiﬁers and language model features\nin a strong incremental operating system without\nlooking ahead. Incremental dependency parsing\ncombined with the removal of disﬂuency was also\nstudied (Rasooli and Tetreault, 2015). Some studies\nhave used recurrent neural networks for live dis-\nﬂuency identiﬁcation. Using a basic Elman Recur-\nrent Neural Network (RNN), Hough and Schlangen\n(2015) investigated incremental processing, with\nan objective coupling detection accuracy with low\nlatency.\nLanguage models have been used as an addi-\ntional task for the identiﬁcation of disﬂuencies, re-\nlying on the intuition that disﬂuencies can be de-\ntected by divergences from clean language models,\nwith Johnson and Charniak (2004)’s noisy chan-\nnel model beginning this effort. Shalyminov et al.\n(2018) made language modelling an auxiliary task\nto disﬂuency detection in a deep multi-task learn-\ning (MTL) set-up, gaining accuracy over a vanilla\nRNN tagger. POS tags have also been used as\nan input for detecting disﬂuencies, showing slight\nincreases in disﬂuency detection over using word\nvalues alone (Purver et al., 2018).\nWhile the work above operates only on tran-\nscripts pre-segmented into utterances, recent re-\nsearch has been performed on combining disﬂu-\nency detection with utterance segmentation. This\nwas done in a joint tagset of disﬂuency, and utter-\nance segmentation tags by (Hough and Schlangen,\n2017), showing an improvement over the perfor-\nmance of the individual tasks, and (Rohanian and\nHough, 2020) show an improvement in both tasks\nwhen framed as a multi-task learning (MTL) set-up\nwith a Long Short-term Memory network (LSTM),\nalso simultaneously doing POS-tagging and lan-\nguage modelling.\nThe recent live incremental systems fall short of\nthe same accuracies achievable on pre-segmented\ntranscripts, so there is a natural interest in using\nthe best non-incremental sequence models and\nadapting them for incrementality. Madureira and\nSchlangen (2020) take up this effort in several other\nsequence tagging and classiﬁcation tasks, showing\nhow bidirectional encoders and Transformers can\nbe modiﬁed to work incrementally. To reduce the\nimpact of the partiality of the input, the models pre-\ndict future content and wait for more rightward con-\ntext. Dalvi et al. (2018) also use truncated inputs\nduring the training phase of live machine transla-\ntion to address the partial input sentence decoding\nproblem Bidirectional encoders face. Here, we\nseek to add to this growing effort to investigate the\ntrade-off of incremental performance against the\nﬁnal output quality of deep neural network-based\nlanguage processing, applied to incremental disﬂu-\nency detection.\n3695\n| A uh ﬂight [ to Boston + { uh I mean } to Denver ] on Friday | Thank you |\nDisﬂuency f e f f f e e e rpS −5 rpnSub f f f f\nUtterance segmentation . w- - w- - w- - w- -w- - w- - w- - w- - w- - w- - w- - w. . w- - w.\nPOS tags DT UH NN IN NNP UH PRP V B IN NNP IN NNP V B PRP\nFigure 1: An utterance with the disﬂuency tags (repair structures and edit terms) and the utterance segmentation\ntags and POS tags used for preprocessing.\n3 Disﬂuency Detection\nDisﬂuencies are generally assumed to have\na reparandum-interregnum-repair structure in\ntheir fullest form as speech repairs (Shriberg, 1994;\nMeteer et al., 1995). A reparandum is a stretch of\nspeech later corrected by the speaker; the corrected\nexpression is a repair, the beginning of which is\nreferred to as repair onset. An interregnum word\nis a ﬁller or a reference expression between the\nrepair and reparandum, usually an interruption and\nhesitation step when the speaker expresses a repair,\ngiving the structure as in (1).\nJohn [ likes  \nreparandum\n+ { uh }  \ninterregnum\nloves ]  \nrepair\nMary\n(1)\nIn the absence of reparandum and repair, the\ndisﬂuency is reduced to an isolated edit term. A\nmarked, lexicalised edit term such as a ﬁlled pause\n(“uh” or “um”) or more phrasal terms such as “I\nmean” and “you know” may occur. The identiﬁ-\ncation of these elements and their structure is then\nthe task of disﬂuency detection.\nThe task of detecting incremental disﬂuencies\nadds to the difﬁculty of doing this in real-time,\nword-by-word, from left to right. Disﬂuency recog-\nnition is then treated as the same problem that a\nhuman processor faces with a disﬂuent expression:\nonly when an interregnum is detected, or maybe\neven when a repair is initiated, does it become clear\nthat the earlier content is now to be regarded as ‘to\nbe repaired,’ i.e., to be classiﬁed as a reparandum.\nTherefore, the task cannot be deﬁned as a simple\nsequence labeling task in which the tags for the\nreparandum, interregnum, and repair phases are as-\nsigned left-to-right over words as seen in the above\nexample; in this case, it will require the assumption\nthat “likes” would be repaired, at a time when there\nis no data to make it available.\nWe use a tag set that encodes the start of the\nreparandum only at a time when it can be inferred,\nprimarily when the repair starts – the disﬂuency\ndetection task is to tag words as in the top line of\ntags in Fig. 1 as either ﬂuent ( f) an edit term (e),\na repair onset word (rpS−N for the reparandum\nstarting N words back) and a repair end word of\nthe type repeat ( rpnRep), substitution ( rpnSub)\nor delete (rpnDel).\n4 Model\nTo incrementalise a Transformer-based model for\nword-by-word disﬂuency detection, we devise a\nmodel built on top of a pre-trained BERT archi-\ntecture (Devlin et al., 2019) with a Conditional\nRandom Field (CRF) output architecture to tag\nsequences with tags such as those in the top line\nof Fig. 1. We use a BERT-based encoder and try\ndifferent strategies to incrementalise the system’s\noperation and output, using language models to\npredict future word sequences as described in Sec-\ntion 5 while maintaining BERT’s non-incremental\nquality.\nUtterance segmentation Our models are de-\nsigned to work not only with pre-segmented data\nbut also on raw transcripts and ASR results, where\nutterance segmentation is required to leverage\nthe use of sentence-based linguistic knowledge in\nBERT. Utterance segmentation has a clear interde-\npendence with and inﬂuence on the detection of\ndisﬂuency as disﬂuent restarts and repairs may be\nincorrectly predicted at ﬂuent utterance boundaries\nwithout segmentation. In this paper, rather than\nperforming utterance segmentation in tandem with\ndisﬂuency detection, we perform it on words as\nthey arrive in the system as a live segmentation\ntask before sending the current preﬁx of the utter-\nance to the disﬂuency detection system. We use\nthe word-by-word segmentation system from (Ro-\nhanian and Hough, 2020) where four output tags\ndeﬁne ranges of transcribed words or word hypothe-\nses using a BIES tag scheme (Beginning, Inside,\nEnd, and Single) to allow for the prediction of an\nutterance ending. The tagset allows information to\nbe captured from the context of the word to decide\nwhether this word continues a current utterance\n(the - preﬁx) or starts anew (the . preﬁx), and also\nallows live prediction of whether the next word\nwill continue the current utterance (the - sufﬁx) or\n3696\nwhether the current word ﬁnishes the utterance (the\n. sufﬁx). An example of the scheme is shown in\nthe second line of Fig. 1.\nCRF We use a CRF output architecture to pre-\ndict a tag for every token. Although this model\ngenerates predictions for the whole sequence, the\nlabels are outputted individually. There are impor-\ntant dependencies between adjacent labels in dis-\nﬂuency detection, and explicit modeling of these\nrelationships can help. The addition of the CRF\nenables the model to test for the most optimal path\nacross all available label sequences.\n4.1 Input Features\nIn addition to the word values, we also experiment\nwith two other inputs:\nPart-of-speech tags POS tags may enhance the\nidentiﬁcation of disﬂuencies on various settings.\nPOS tagging helps detect disﬂuency structure as the\nparallelism between the reparandum and repair in\nsubstitutions, as shown in the repeated IN NNP\nsequences in Fig. 1.\nWord timings We also experiment with the du-\nration from the ending of the previous word to\nthe ending of the current word as it enters the sys-\ntem, either from ground truth word transcriptions\nor from ASR results.\n5 Strategies for Incrementalising BERT\nHere we describe the different strategies we used\nto modify the training and live decoding methods\nof non-incremental models to detect speech disﬂu-\nencies word-by-word incrementally. The general\nprinciple is to leverage high accuracy full sequence\nclassiﬁcation using BERT but deploying it on se-\nquences, including future predictions for words up\nto the hypothesised end of the current utterance.\n5.1 Modifying the Training Procedure\nTraining is performed on full sentences/utterances,\nbut the decoder produces outputs based on par-\ntial input data at the test time. This disparity be-\ntween training and decoding can potentially affect\nour models’ performance. Based on (Dalvi et al.,\n2018), we present two methods to address this is-\nsue: chunk-based training and add-M training.\nChunk-based training In chunk-based training,\nwe change the training scheme by removing the\nends of each sentence in the training set and sim-\nply break each training sentence into chunks of N\ntokens. Here we use 2 and 3 for N.\nAdd-M training We begin with the ﬁrst N\nwords in training sentences in add- M training.\nThe next training instances are then generated by\nN + M, N+ 2M, N+ 3M... words before the end\nof the sentence is reached. In our experiments, we\nfound setting N=1 and M=1 worked best.\n5.2 Modifying the Decoding Procedure\nConstant latency The technique of constant la-\ntency requires allowing certain ‘future’ words to\nbe seen before a label to previous words is given.\nIt is a form of look-ahead based on Baumann et al.\n(2011), in which before making the ﬁrst decision\nwith respect to previous time steps, the processor\nis required to wait for some correct context. We\nexplore the one- or two-word contexts of our in-\nput. This suggests that the model generates the\nﬁrst label for word t after the word t + 1is seen or\nthe model observes words t + 1and t + 2before\ntagging word t. This has an inherent limit on the\nlatency achievable, and we use this as a baseline\nincremental decoding system.\nProphecy-based decoding For our other decod-\ning strategies, we use a ‘prophecy’-based approach\nto predicting future word sequences, following the\ntask of open-ended language generation, which,\ngiven an input text passage as context, is to pro-\nduce text that constitutes a cohesive continuation\n(Holtzman et al., 2019). Inspired by (Madureira\nand Schlangen, 2020), using the GPT-2 language\nmodel (Radford et al., 2019), we ﬁrst give each\nword as a left context and create a continuation un-\ntil the end of an utterance to create a hypothetical\ncomplete context that satisﬁes the requirements of\nthe models’ non-incremental structure.\nFormally, with m tokens x1...xm as our context,\nthe task is to create the next n continuation tokens\nto achieve the completed sequence x1...xm+n. It\nis assumed that the models compute P(x1:m+n)\nusing a standard left-to-right decomposition of the\ntext probability as in (2). This process is used\nto build the utterance continuation token-by-token\nusing a speciﬁc decoding technique.\nP(x1:m+n) =\nm+n∏\ni=1\nP(xi|x1...xi−1) (2)\nThree of the most common decoding methods\nare used in this paper: Beam search, Top- k sam-\npling, and Top- p sampling. Example word se-\nquence prophecies from these decoding methods\n3697\n(a)\n(b)\n(c)\nFigure 2: Using a ‘prophecy’-based approach to pre-\ndict future word sequences, following the task of open-\nended language generation with three different decod-\ning methods. (a) Beam search. (b) Top- k sampling.\n(c) Top-p sampling.\nare shown in Fig. 2. The right-most block shows\nthe prediction of the continuation of the word se-\nquences as each new word in the sequence “John\nlikes uh loves Mary” is fed into the language model.\nBeam search Assuming that the model gives a\ngreater likelihood to better quality text, we are look-\ning for a sequence with the highest probability. Dur-\ning the search, a group of stacks is used to hold\nhypotheses. Beam size N is used to manage the\nsearch space by expanding the top N hypotheses\nin the existing stack. We used beam size 10 for all\nthe models.\nTop-k sampling We deﬁne sampling as ran-\ndomly choosing the next word based on its con-\nditional probability distribution as in (3).\nxi ∼ P(x|x1:i−1) (3)\nIn the Top-k sampling, the most probable next k\nwords are extracted and the probability mass is\nredistributed between only the following k words\n(Fan et al., 2018). Given a distributionP(x|x1:i−1),\nwe extract its top-k vocabulary V (k) ⊂V as the set\nof size k which maximizes ∑\nx∈V (k) P(x|x1:i−1).\nAfter an initial investigation, we set k to 50 in all\nexperiments.\nTop-p sampling Rather than selecting only the\nmost probable K words, in Top-p sampling, we\nselect the smallest possible range of words with\ntheir total likelihood exceeds the probability p\n(Holtzman et al., 2019). The probability mass is\nthen redistributed between this set of words. With\nthis method, the size of the word set will dynami-\ncally adjust based on the probability distribution of\nthe next word. With the distribution P(x|x1:i−1),\nwe consider its top- p sequence, with vocabulary\nV (p) ⊂V as the smallest set withP(x|x1:i−1) ≥p.\nWe set p = 0.95.\n6 Experimental Set-up\nWe train on transcripts and test on both transcripts\nand ASR hypotheses. All models in testing have\nstrictly word-by-word left to right input. In addition\nto using the latest word hypothesis as input, we\ntrain and evaluate the presented models with two\nkinds of additional inputs: time elapsed from the\nend of the previous word (hypothesis) to the current\none and the POS tag of the current word. Results\non the development set were used to ﬁnd the best\nmodel to be evaluated on the test set.\nWe used the data from (Hough and Schlangen,\n2017) for ASR hypotheses – this was generated\nby a free trial version of IBM’s Watson Speech-\nTo-Text service for incremental ASR. The service\noffers good quality ASR on noisy data-on our se-\nlected held-out data on Switchboard, and the aver-\nage WER is 26.5%. The Watson service, crucially\nfor our task, does not ﬁlter out hesitation markers\nor disﬂuencies (Baumann et al., 2017). The service\ndelivers results incrementally, so silence-based end-\npointing is not used. It also outputs word timings,\nwhich are close enough to the source timings to use\nas features in the live version of our system.\nThe word embedding for LSTM was initialised\nwith 50-dimensional embedding trained on Google\nNews (Mikolov et al., 2013). The model has been\nimplemented using Tensorﬂow 2.1. We train all\nmodels for a maximum of 50 epochs; otherwise,\nstop training if there is no improvement on the best\nscore on the validation set after 7 epochs.\nA large version of the pre-trained BERT is used\nwith 340M parameters (24-layer blocks, 16 self-\n3698\nInput Model\nPre-segmented transcripts\n(per word)\nTranscripts\n(per word)\nASR\n(per 10 second window)\nFrm FrpS Fe Frm FrpS Fe Frm FrpS Fe\nWords\nSTIR (HS’15/ PHH’18)0.741 / 0.749 -/0.827 0.880/- - - - - - -\nRNN (HS’15) 0.689 - 0.873 - - - - - -\nLSTM 0.686 0.771 0.928 0.59 0.678 0.904 - 0.548 0.726\nLSTM-MTL (RH’20) 0.737 0.799 0.938 0.629 0.743 0.917 - 0.573 0.757\nBERT 0.758 0.851 0.960 0.659 0.782 0.947 0.524 0.603 0.812\nWord +\nTimings\nLSTM 0.681 0.777 0.921 0.623 0.718 0.908 - 0.555 0.721\nLSTM-MTL (RH’20) 0.741 0.812 0.929 0.629 0.741 0.922 - 0.559 0.751\nBERT 0.752 0.842 0.958 0.678 0.791 0.939 0.502 0.594 0.793\nWord +\nPOS\nSTIR (HP’14 / PHH’18)0.779 / 0.768-/0.833 0.937/- - - - - - -\nRNN (HS’15 / PHH’18)0.711 / 0.668 -/0.790 0.902/- - - - - - -\nLSTM joint tagset (HS’17) - - - 0.599 0.686 0.907 - 0.557 0.726\nLSTM-MTL (SEL’18) 0.753 0.816 0.919 - - - - 0.548 -\nWords +\nTimings +\nPOS\nLSTM joint tagset (HS’17) - - - 0.601 0.719 0.918 - 0.555 0.727\nLSTM 0.692 0.778 0.931 0.601 0.720 0.910 - 0.557 0.727\nLSTM-MTL (RH’20) 0.743 0.811 0.932 0.633 0.743 0.931 - 0.571 0.757\nBERT 0.757 0.853 0.958 0.676 0.802 0.944 0.522 0.605 0.809\nTable 1: Final disﬂuency detection accuracy results on Switchboard data\nattention heads, and 1024 hidden-size) for the\nmodel. In our analysis, when ﬁne-tuning BERT,\nwe followed the hyper-parameters of (Devlin et al.,\n2019). Since the datasets we use are tokenized,\nand each token has a matching tag, we adopt the\ndirections provided by (Devlin et al., 2019) to deal\nwith the sub-tokenization of BERT: to determine\nits label, the scores of the ﬁrst sub-token are used,\nand further sub-token scores are discarded.\nData We use standard Switchboard training data\n(all conversation numbers starting sw2*,sw3 * in\nthe Penn Treebank III release: 100k utterances,\n650k words) and use standard held-out data (PTB\nIII ﬁles sw4[5-9] *: 6.4k utterances, 49k words)\nas our validation set. We test on the standard test\ndata (PTB III ﬁles 4[0-1] *) with partial words\nand punctuation stripped away from all ﬁles. We\nonly choose a subset of the held-out and test data\nfor the ASR results in assessment, whereby both\nchannels achieve below 40 percent WER to ensure\ngood separation- this left us with 18 dialogues in\nvalidation data and 17 dialogues for test data.\n6.1 Evaluation Criteria\nWe calculate F1 accuracy for repair onset detec-\ntion FrpS and for edit term words Fe, which in-\ncludes interregna and Frm for reparandum detec-\ntion. Performing the task live, on hypotheses of\nspeech recognition that may not be quite equiva-\nlent to the annotated gold-standard transcription\ninvolves the use of time-based local accuracy met-\nrics in a time window (i.e., within this time frame,\nhas a disﬂuency been detected, even if not on the\nidentical words?)-we, therefore, measure the F1\nscore over 10-second windows of each speaker’s\nchannel.\nFor incremental performance, we measure la-\ntency and output stability over time. We use the\nﬁrst time to detection (FTD) metric of (Zwarts et al.,\n2010) for latency: the average latency (in number\nof words) before the ﬁrst detection of a gold stan-\ndard repair onset or edit term word. For stability,\nwe evaluate the edit overhead (EO) of output labels\n(Baumann et al., 2011), the proportion of the un-\nnecessary edits (insertions and deletions) required\nto achieve the ﬁnal labels produced by the model,\nwith perfect performance being 0%.\n6.2 Competitor Baselines\nWe compare our incrementalised BERT model\nagainst a number of existing baselines, largely from\nexisting incremental disﬂuency detection systems\ntrained and tested on the same data:\nSTIR (HP’14/HS’15/PHH’18): Hough and\nPurver (2014)’s STrongly Incremental Repair de-\ntection (STIR) non-deep model using n-gram lan-\nguage model features in a pipeline of Random\nForest classiﬁers. The reparandum is detected by\na backward search, showing robustness for longer\nlengths of repair compared to deep sequence tag-\nging models (Purver et al., 2018). A state-of-\nthe-art incremental model on pre-segmented tran-\nscripts.\nRNN (HS’15): (Hough and Schlangen, 2015)’s\nRNN-based model, the ﬁrst deep learning-based\n3699\nTraining\nScheme Model Final output F1 Incrementality\nFrm FrpS Fe EO FTD\nChunk\nLSTM .591 .674 .901 0.21 0.06\nMTL .631 .739 .911 0.41 0.07\nBERT .647 .780 .938 0.61 0.32\nAdd-M\nLSTM .598 .683 .909 0.20 0.03\nMTL .628 .751 .921 0.38 0.10\nBERT .664 .788 .949 0.60 0.31\nTable 2: Final accuracy vs. incremental performance\ntrade-off in the different models on un-segmented tran-\nscripts.\nincremental disﬂuency detection model using\nthe same tagset as in our model. Results from\nPurver et al. (2018) are used, which reproduced\nthe model with some degradation in the results.\nLSTM: An LSTM version of Hough and\nSchlangen (2015) on pre-segmented transcripts\nLSTM joint tagset (HS’17) Hough and\nSchlangen (2017)’s model, which simultaneously\npredicts utterance segmentation using a joint tag\nset of utterance segmentation tags and disﬂuency\ntags, the latter of which is the same as our own.\nThis is the only other work to use word timing\ninformation and to be testable on ASR results.\nLSTM-MTL (SEL’18) Shalyminov et al.\n(2018)’s multi-task learning model, which tags\naccording to our tag set but simultaneously does\nlanguage modelling by predicting the probability\nof the current word given the history. Also adds\nground-truth POS tags to input.\nLSTM-MTL (RH’20): Rohanian and Hough\n(2020)’s multi-task learning model, which simul-\ntaneously predicts utterance segmentation, POS\ntags and language model probabilities, exhibiting\nstate-of-the-art results for a strictly incremental\ndeep model. The model is used as described by\nthe authors and also here with the addition of\ntiming information and gold standard POS infor-\nmation (as opposed to simultaneously predicted\nPOS tags). It is also applied to ASR results as it is\na suitable model to do so. This same model pro-\nvides the automatic live utterance segmentation\nin our own model.\n7 Results\nThe results in terms of the ﬁnal output of our best\nperforming incremental BERT system in the three\ntesting regimes versus its competitors is shown in\nModel F1\nRepeats Substitution Deletes\nWith Standard Training\nLSTM 0.94 0.70 0.48\nMTL 0.96 0.72 0.46\nBERT 0.96 0.77 0.54\nWith Add-M Training\nLSTM 0.95 0.71 0.48\nMTL 0.96 0.73 0.47\nBERT 0.96 0.79 0.54\nTable 3: Performance on different types of repair.\nTable 1.1 We found our best model was the add-M\ntrained model, and the best decoding strategy was\nusing top-p sampling for predicting future words.\nDisﬂuency detection on transcripts For repair\ndetection, our system’s bestFrpS score for detect-\ning repair onsets on pre-segmented transcripts at\n0.853 beats state-of-the-art incremental systems.\nThis performance degrades using automatic seg-\nmentation to 0.802, a state-of-the-art result for this\nsetting. Its Frm accuracy of 0.757 on reparandum\nwords on pre-segmented transcripts is only beaten\nby HP’14/PHH’18 model using word and POS in-\nput, making it a state-of-the-art strictly incremental\ndeep model. This performance degrades to 0.678\non raw transcripts but is a state-of-the-art result for\nthis setting. In terms of edit term detection, state-\nof-the-art detection results of 0.960 and 0.944 are\nachieved on the pre-segmented and unsegmented\nsettings, improving over the existing benchmarks\nof HP’14 and RH’20. These results suggest we\nhave achieved the aim of a strictly incremental\nmodel achieving high ﬁnal accuracies.\nDisﬂuency detection on ASR results Using the\nASR results from HS’17 for comparison, a signiﬁ-\ncant improvement can be seen over the previously\nreported results on FrpS and Fe per 10-second\nwindow, improving from 0.557 to 0.605 and from\n0.727 to 0.809 respectively. Given the previously\nreported best system gave strong correlations in\nterms of real repair rates, this is encouraging that\nour system could be very useful in a live setting.\n7.1 Incremental Performance\nThe purpose of this paper was to adapt a high-\nperforming, non-incremental model for incremen-\ntal operation. As can be seen in Table 2 and in\nFig. 3, while our BERT model with top- p sam-\nple utterance prediction outperforms the multi-task\n1Experiments are reproducible from https://github.\ncom/mortezaro/tr-disfluency\n3700\n(a)\n(b)\nFigure 3: Incremental results of ﬁrst time to detection (FTD) metric for rpS and e and edit overhead (EO) for\ndisﬂuency detection labels.(a) On unsegmented transcripts. (b) On ASR results.\nmodel and vanilla LSTM model in terms of ﬁnal\noutput accuracy, its incremental output stability is\nslightly below its competitors, with the best edit\noverhead of 63% unnecessary edits versus 25%\n(LSTM joint tagset (HS’17)) and 42% (LSTM-\nMTL (RH’20)) on ASR results, meaning the output\nis slightly, though not severely, more jittery.\nOf the prophecy-based approaches, we found\nthe top-p sampling method gave the most stable re-\nsults (EO=61% with chunk training, EO=60% with\nadd-M training) and beam search gave the least\nstable. As shown in Fig. 3, while the constant la-\ntency approaches offer large advantages in EO over\nprophecy-based models on transcripts, that advan-\ntage disappears on ASR results, where the prophecy\nmodels generally outperform them. As can be seen\nin Table 2, there is a slight improvement in stability\nacross all systems using the add-M training regime\nfor ﬁnal output and incremental performance.\nIn terms of latency, results are even more encour-\naging, with the best FTD for rpS of 0.31 words\n(versus 0.03 and 0.07) on transcripts, which shows\na relatively short latency of detecting the repair for\nthe ﬁrst time– this suggests a responsive, sensitive\nsystem.\n7.2 Error Analysis\nWe conduct an error analysis in terms of perfor-\nmance on different repair types and in terms of\nrepairs with different lengths. Table 3 shows the\nperformance in terms of FrpS score on detecting re-\npairs of the three different types: verbatim repeats,\nsubstitutions, and deletes (restarts). Our BERT\nmodel performs best, either jointly or uniquely,\nacross all three types, with a gain of 0.06 over its\nnearest competitors for substitutions and deletes.\nThrough large-scale training, the enhanced linguis-\ntic knowledge equips it to recognize the syntactic\n3701\nModel Reparandum length Reparandum length of\nnested disﬂuencies\n1 2 3 4 5 6 1 2 3 4 5 6\nWith Standard Training\nLSTM .843 .675 .405 .311 .134 .131 .747 .586 .382 .320 .110 .104\nMTL .856 .683 .431 .335 .134 .131 .763 .586 .405 .291 .110 .104\nBERT .892 .716 .469 .379 .310 .187 .818 .623 .405 .320 .130 .140\nWith Add-M Training\nLSTM .843 .675 .434 .334 .134 .131 .741 .586 .382 .320 .110 .104\nMTL .851 .709 .468 .335 .134 .131 .779 .586 .405 .291 .130 .104\nBERT .892 .719 .472 .379 .310 .187 .833 .645 .405 .320 .130 .140\nTable 4: F1 of models on repairs with reparanda of different length\nand lexical parallelism in more complex repairs\nwhile retaining high accuracy on repeats. Table 4\nshows the degradation in performance in detecting\nrepairs of different lengths. With Add-M training,\nthe BERT model degrades less and performs (joint)\nbest on all lengths and nested disﬂuencies. While\nthe performance on length ﬁve repairs is consider-\nably better than the other deep models, the 0.187\naccuracy on length six repairs is what gives it a\nslight disadvantage compared to the HP’14 explicit\nbacktracking system (reported as high as 0.500 in\nPHH’18), which likely accounts for the lowerFrm\nscore despite the superior FrpS score of our system.\n8 Discussion and Conclusion\nOur incremental GPT-2 and BERT-driven sys-\ntem performs well at detecting repair disﬂuencies\non pre-segmented and unsegmented transcripts,\nachieving state-of-the-art results for a strictly incre-\nmental repair onset detection. Our system is com-\npetitive at reparadnum word detection and achieves\nstate-of-the-art results in edit term detection. The\nresults on ASR transcripts are also state-of-the-art.\nThe high sequence-ﬁnal performance comes at\nthe expense of marginally increased jitter in the\nword-by-word output, but with sensitive and fast\nrepair detection, on average ﬁrst detecting the re-\npair under a third of a second after the end of the\nrepair onset word. These results suggest it is begin-\nning to enjoy the best of both worlds in leveraging\nthe right-ward context which BERT uses for its\nhigh performance, while the continuation predic-\ntions from the GPT-2 model are good enough to\nallow good incremental performance before the\ntrue right-ward context is available.\nThe linguistic knowledge in the BERT model\nallows it to recognize parallelism in reparandum\nand repair phases and the absence thereof to in-\ncrease performance on detecting substitution and\ndelete repairs. This improvement to existing deep\ndisﬂuency detection models, and, with appropriate\nuse of open-ended language generation techniques\nwith a GPT-2 language model, its good incremen-\ntal performance, is consistent with a growing body\nof work (Heeman and Allen, 1999; Johnson and\nCharniak, 2004; Zwarts et al., 2010; Hough and\nPurver, 2014; Shalyminov et al., 2018; Rohanian\nand Hough, 2020), showing good language mod-\nelling can lead to good disﬂuency detection, as they\nare inherently part of the same process.\nOur system still fails to detect longer repairs\ncompared to an explicit backtracking mechanism\nlike (Hough and Purver, 2014). While the van-\nishing gradient problem is partly overcome here,\nthe strictly left-to-right constraint on decoding puts\nmemory limitations on any repair detection system.\nIn future, we will explore efﬁcient ways to navigate\nthis space whilst not ﬁltering out rarer repair forms.\nThe results on ASR results show our disﬂuency\ndetection system is ready for use in a live set-\nting with a good degree of accuracy, and work is\ncurrently underway to use it to help detect a va-\nriety of different cognitive conditions, including\nAlzheimer’s Disease, in a live diagnostic system.\nAcknowledgments\nWe thank the anonymous ACL-IJCNLP reviewers\nfor their helpful comments and Matthew Purver\nfor his continuous support and supervision on the\nwider project.\nReferences\nTimo Baumann, Okko Buß, and David Schlangen.\n2011. Evaluation and optimisation of incremental\nprocessors. Dialogue & Discourse, 2(1):113–141.\nTimo Baumann, Casey Kennington, Julian Hough, and\nDavid Schlangen. 2017. Recognising conversational\nspeech: What an incremental asr should do for a di-\nalogue system and how to get there. In Dialogues\nwith social robots, pages 421–432. Springer.\n3702\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, and\nStephan V ogel. 2018. Incremental decoding and\ntraining methods for simultaneous translation in neu-\nral machine translation. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 493–499.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. arXiv preprint\narXiv:1805.04833.\nJohn J Godfrey, Edward C Holliman, and Jane Mc-\nDaniel. 1992. Switchboard: Telephone speech cor-\npus for research and development. In Acoustics,\nSpeech, and Signal Processing, IEEE International\nConference on , volume 1, pages 517–520. IEEE\nComputer Society.\nJonathan Gratch, Ron Artstein, Gale M Lucas, Giota\nStratou, Stefan Scherer, Angela Nazarian, Rachel\nWood, Jill Boberg, David DeVault, Stacy Marsella,\net al. 2014. The distress analysis interview corpus\nof human and computer interviews. In LREC, pages\n3123–3128.\nPeter A Heeman and James Allen. 1999. Speech re-\npains, intonational phrases, and discourse markers:\nmodeling speakers’ utterances in spoken dialogue.\nComputational Linguistics, 25(4):527–572.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nJulian Hough and Matthew Purver. 2014. Strongly in-\ncremental repair detection. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 78–89.\nJulian Hough and David Schlangen. 2015. Recurrent\nneural networks for incremental disﬂuency detec-\ntion. In Sixteenth Annual Conference of the Inter-\nnational Speech Communication Association.\nJulian Hough and David Schlangen. 2017. Joint, incre-\nmental disﬂuency detection and utterance segmenta-\ntion from speech. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers,\npages 326–336.\nKarmele L ´opez-de Ipi ˜na, Jesus-Bernardino Alonso,\nCarlos Manuel Travieso, Jordi Sol´e-Casals, Harkaitz\nEgiraun, Marcos Faundez-Zanuy, Aitzol Ezeiza,\nNora Barroso, Miriam Ecay-Torres, Pablo Martinez-\nLage, et al. 2013. On the selection of non-\ninvasive methods based on speech analysis oriented\nto automatic alzheimer disease diagnosis. Sensors,\n13(5):6730–6745.\nMark Johnson and Eugene Charniak. 2004. A TAG-\nbased noisy-channel model of speech repairs. In\nACL, pages 33–39.\nParia Jamshid Lou and Mark Johnson. 2020. Im-\nproving disﬂuency detection by self-training a self-\nattentive model. arXiv preprint arXiv:2004.05323.\nBrielen Madureira and David Schlangen. 2020. In-\ncremental processing in the age of non-incremental\nencoders: An empirical assessment of bidirectional\nmodels for incremental NLU. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 357–374,\nOnline. Association for Computational Linguistics.\nM. Meteer, A. Taylor, R. MacIntyre, and R. Iyer. 1995.\nDisﬂuency annotation stylebook for the switchboard\ncorpus. ms. Technical report, Department of Com-\nputer and Information Science, University of Penn-\nsylvania.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nShamila Nasreen, Morteza Rohanian, Matthew Purver,\nand Julian Hough. 2021. Alzheimer’s dementia\nrecognition from spontaneous speech using disﬂu-\nency and interactional features. Frontiers in Com-\nputer Science, 3:49.\nMatthew Purver, Julian Hough, and Christine Howes.\n2018. Computational models of miscommunication\nphenomena. Topics in cognitive science, 10(2):425–\n451.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMohammad Sadegh Rasooli and Joel R. Tetreault.\n2015. Yara parser: A fast and accurate depen-\ndency parser. Computing Research Repository ,\narXiv:1503.06733. Version 2.\nMorteza Rohanian and Julian Hough. 2020. Re-\nframing incremental deep language models for di-\nalogue processing with multi-task learning. In\nProceedings of the 28th International Confer-\nence on Computational Linguistics, pages 497–507,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nMorteza Rohanian, Julian Hough, and Matthew Purver.\n2020. Multi-modal fusion with gating using audio,\n3703\nlexical and disﬂuency features for alzheimer’s de-\nmentia recognition from spontaneous speech. In\nProc. Interspeech, pages 2187–2191.\nIgor Shalyminov, Arash Eshghi, and Oliver Lemon.\n2018. Multi-task learning for domain-general spo-\nken disﬂuency detection in dialogue systems. InPro-\nceedings of the 22nd SemDial Workshop on the Se-\nmantics and Pragmatics of Dialogue (AixDial), Aix-\nen-Provence.\nElizabeth Shriberg. 1994. Preliminaries to a Theory\nof Speech Disﬂuencies . Ph.D. thesis, University of\nCalifornia, Berkeley.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nShaolei Wang, Wangxiang Che, Qi Liu, Pengda Qin,\nTing Liu, and William Yang Wang. 2020. Multi-task\nself-supervised learning for disﬂuency detection. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 34, pages 9193–9200.\nVicky Zayats, Mari Ostendorf, and Hannaneh Ha-\njishirzi. 2016. Disﬂuency detection using a bidirec-\ntional lstm. arXiv preprint arXiv:1604.03209.\nSimon Zwarts, Mark Johnson, and Robert Dale. 2010.\nDetecting speech repairs incrementally using a noisy\nchannel approach. In Proceedings of the 23rd Inter-\nnational Conference on Computational Linguistics\n(Coling 2010), pages 1371–1378."
}