{
    "title": "Emotion Analysis of Arabic Tweets: Language Models and Available Resources",
    "url": "https://openalex.org/W4220674815",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2794716062",
            "name": "Ghadah Alqahtani",
            "affiliations": [
                "King Saud University"
            ]
        },
        {
            "id": "https://openalex.org/A1858608336",
            "name": "Abdulrahman Alothaim",
            "affiliations": [
                "King Saud University"
            ]
        },
        {
            "id": "https://openalex.org/A2794716062",
            "name": "Ghadah Alqahtani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1858608336",
            "name": "Abdulrahman Alothaim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2539162359",
        "https://openalex.org/W2471147443",
        "https://openalex.org/W3133440961",
        "https://openalex.org/W2910830936",
        "https://openalex.org/W3176169354",
        "https://openalex.org/W2741447225",
        "https://openalex.org/W4301188379",
        "https://openalex.org/W1973264851",
        "https://openalex.org/W2805351602",
        "https://openalex.org/W6756020842",
        "https://openalex.org/W3131387325",
        "https://openalex.org/W6766036824",
        "https://openalex.org/W3106209420",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W4297823766",
        "https://openalex.org/W3153642904",
        "https://openalex.org/W2252067416",
        "https://openalex.org/W2806028205",
        "https://openalex.org/W6772834199",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W6721933647",
        "https://openalex.org/W2737990573",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3022569409",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3111704875",
        "https://openalex.org/W4211260480",
        "https://openalex.org/W2751193242",
        "https://openalex.org/W3129831852",
        "https://openalex.org/W2515404780",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1986614398",
        "https://openalex.org/W6799796359",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W6779254361",
        "https://openalex.org/W2805744755",
        "https://openalex.org/W6778931571",
        "https://openalex.org/W6691525129",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W6606144089",
        "https://openalex.org/W3116641301",
        "https://openalex.org/W3184219057",
        "https://openalex.org/W6691620099",
        "https://openalex.org/W2250224884",
        "https://openalex.org/W1487713954",
        "https://openalex.org/W2111620038",
        "https://openalex.org/W2204900098",
        "https://openalex.org/W6754002923",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2520145273",
        "https://openalex.org/W2322777686",
        "https://openalex.org/W4287755489",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W3000337150",
        "https://openalex.org/W3191860801",
        "https://openalex.org/W2950018712",
        "https://openalex.org/W2464743361",
        "https://openalex.org/W937194849",
        "https://openalex.org/W3032746405",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2542475055",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4231827381",
        "https://openalex.org/W3031973162"
    ],
    "abstract": "One of the most popular social media platforms is Twitter. Emotion analysis and classification of tweets have become a significant research topic recently. The Arabic language faces challenges for emotion classification on Twitter, requiring more preprocessing than other languages. This article provides a practical overview and detailed description of a material that can help in developing an Arabic language model for emotion classification of Arabic tweets. An emotion classification of Arabic tweets using NLP, overall current practical practices, and available resources are highlighted to provide a guideline and overview sight to facilitate future studies. Finally, the article presents some challenges and issues that can be future research directions.",
    "full_text": "REVIEW\npublished: 30 March 2022\ndoi: 10.3389/frai.2022.843038\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 1 March 2022 | Volume 5 | Article 843038\nEdited by:\nAlexandra Balahur,\nEuropean Commission, Joint\nResearch Centre (JRC), Italy\nReviewed by:\nHassan Sajjad,\nQatar Computing Research\nInstitute, Qatar\nMounir Zrigui,\nUniversity of Monastir, Tunisia\n*Correspondence:\nGhadah Alqahtani\nghadahalqahtani94@gmail.com\nSpecialty section:\nThis article was submitted to\nNatural Language Processing,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nReceived: 24 December 2021\nAccepted: 04 February 2022\nPublished: 30 March 2022\nCitation:\nAlqahtani G and Alothaim A (2022)\nEmotion Analysis of Arabic Tweets:\nLanguage Models and Available\nResources.\nFront. Artif. Intell. 5:843038.\ndoi: 10.3389/frai.2022.843038\nEmotion Analysis of Arabic Tweets:\nLanguage Models and Available\nResources\nGhadah Alqahtani* and Abdulrahman Alothaim\nInformation Systems Department, College of Computer and Info rmation Sciences, King Saud University, Riyadh, Saudi Arab ia\nOne of the most popular social media platforms is Twitter. Em otion analysis and\nclassiﬁcation of tweets have become a signiﬁcant research t opic recently. The\nArabic language faces challenges for emotion classiﬁcatio n on Twitter, requiring more\npreprocessing than other languages. This article provides a practical overview and\ndetailed description of a material that can help in developi ng an Arabic language model\nfor emotion classiﬁcation of Arabic tweets. An emotion clas siﬁcation of Arabic tweets\nusing NLP , overall current practical practices, and availa ble resources are highlighted\nto provide a guideline and overview sight to facilitate futu re studies. Finally, the article\npresents some challenges and issues that can be future resea rch directions.\nKeywords: emotion analysis, language models, natural language processing, Arabic tweets, social emotion\nINTRODUCTION\nThe amount of subjective information available on the inter net has increased tremendously in\nrecent years. Especially after the coronavirus disease-201 9 (COVID-19) pandemic, the number\nof users who generated and shared content in various online pl atforms increased rapidly.\nFurthermore, social media platforms are a useful source of in formation, since they give users the\nability to discuss and share their opinions and emotions. The re was no way to know what people\nthought about certain items, services, or even events until these platforms arose. This valuable\ninformation is accessible today for anyone because of social media platforms such as Twitter, where\nindividuals regularly share their ideas openly and even use ot her people’s opinions to inﬂuence\ntheir actions.\nOpinions of users can be analyzed to give feedback on speciﬁc ser vices, events, or products.\nAnalysis of users’ opinions can be performed with the use of nat ural language processing (NLP),\ntext mining, computational linguistics, and machine learni ng algorithms for detecting, analyzing,\nand classifying human’s feelings, opinions, sentiments, ev aluations, appraisals, attitudes, and\nemotions (\nShukla and Shukla, 2015 ). Sentiment analysis (SA) can be described as an NLP task\naiming to identify a subjective content that contains feeli ng and sentiment, which are classiﬁed\nas positive, negative, or neutral ( Singh et al., 2013 ). Furthermore, textual data can be divided into\ntwo categories: facts and opinions. Facts are objective stat ements about things, events, and their\ncharacteristics. Opinions are usually subjective expressio ns of people’s sentiments, assessments, or\nfeelings concerning entities, events, and their attribute s. When building a model for sentiment\nanalysis and classiﬁcation, some of the literature conside ring focusing on the polarity of opinion\n(positive or negative). On the other hand, a number of researc h studies consider objective\nexpression as a neutral class in SA to build a classiﬁer able to d istinguish between subjective and\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nobjective opinions. Correspondingly, the ﬁeld of emotion\nanalysis and classiﬁcation diﬀers from that of SA. It goes dee per\nthan SA in describing a person’s emotions. Emotion analysis\nallows for more detailed emotions to be analyzed and classiﬁe d\n(\nAl-A ’abed and Al-Ayyoub, 2016 ). Emotion is a part of the\nnervous system that is linked to an emotional state like joy,\nanger, or sadness. The process of emotion analysis is identify ing\nwhether the content of a text contains emotions, and detecti ng\nthat emotion by classifying it into a suitable emotion categ ory.\nPublished information on Twitter comes in the shape of tweets\nthat contain text, in various languages, and users post more t han\n400 million tweets every day (\nWikarsa and Thahir, 2015 ). Posted\ntweets contain users’ emotions and feelings in diﬀerent lang uages.\nAnalysis of emotions in tweets has many diﬃculties, because\nthese tweets have many grammatical errors, misspellings, sl ang,\nsocial shortcuts, and multimedia contents. Many researche rs\nstudy emotions in English language tweets, but none of them\nclassify the emotions in Arabic language tweets because of i ts\nchallenges. Most feeling analyses of Arabic tweets are focus ed on\nclassifying a sentiment into a positive or a negative feeling .\nThe Arabic language, until today, lacks in studies and\nresources in the ﬁeld of social emotion analysis, and models\nfor extracting and classifying emotions in Arabic tweets wi ll\nbe useful in a variety of ﬁelds, including improving E-learnin g\napplications, assisting psychologists in detection of terror ist\nbehavior, improving customer service, improving product\nquality, and many others.\nLanguage is complex, and processing it is computationally\nchallenging. Words are essential building blocks of langua ge. In\nnatural language processing (NLP), words must be converted\nto a numerical format to aid machines in understanding\nthe language by giving an appropriate representation. As a\nresult, language models evolved, and vector space models\nwere used, in which words are represented by numbers\nin the form of vectors. In recent years, the emergence\nof contextualized language representations has ushered in a\nrevolution in NLP. The evolution of NLP language models\nwas from simple frequency counts like bag of words, n-grams,\nand term frequency-inverse document frequency (TF–IDF) to\nmore advanced representations like Word2vec (\nMikolov et al.,\n2013), GloVe (Pennington et al., 2014 ), and FastText ( Bojanowski\net al., 2017 ), which use neural networks to learn features\nunsupervised in big data sets. Despite the fact that these early\nmodels have made signiﬁcant progress, there is still lack of\ncontextualized information.\nRecently, with the rise of the neural attention-based\ndeep architecture, referred to as transformer architectur e, the\ndiscipline of NLP has just seen major progress (\nVaswani\net al., 2017 ). Transformers excel when dealing with long-term\ntext dependencies. Several advanced architectures have bee n\nproposed since the release of initial transformer models: BERT\n(\nDevlin et al., 2018 ), XLNet ( Yang et al., 2019 ), GPT ( Radford\net al., 2018 ), RoBERTa (Liu et al., 2019 ), and ALBERT ( Lan et al.,\n2019). Typically, transformers and their variants are pretrained\non large unlabeled language data sets and corpora, such as the\nweb corpus and Wikipedia, to learn word representations that ar e\ncontextual, which means each word has diﬀerent representatio ns\ndepending on the context it appears in, allowing for the capture\nof words used in a variety of contexts. Furthermore, pre-trai ned\nlanguage models (PLMs) can be ﬁne-tuned and used on a\ndownstream task. In practically every NLP task, ﬁne-tuning a\nPLM in a variety of downstream tasks has resulted in signiﬁca nt\nimprovement in performance (\nQiu et al., 2020 ).\nThe evolution of language modeling for emotion classiﬁcatio n\ntasks is our focus in this study to give researchers in these ﬁ eld an\noverview, which can be a starting point or a guide, since we wil l\ndiscuss available resources and current practices specially f or the\nArabic language. The contributions of this article are as fo llows:\n• We compare the model’s used transformer in an emotion\nclassiﬁcation task for Arabic language.\n• We went through the current status of the Arabic NLP\ndata sets for emotion classiﬁcation, and discovered the\nmethodology and practices to build an emotion tweet data set.\n• We provide a detail of available tools, resources, and open\nsource python libraries that can be used for building a model\nto classify emotions in Arabic tweets.\nThe article is structured as follows: Section Related Studie s and\nBackground looks into related studies in the literature; Se ction\nEmotion Analysis of Arabic Tweets goes through some of existi ng\nemotion models for Arabic tweets, highlights the Arabic PLM\navailable for ﬁne-tuning, and summarizes available resour ces\nand tools; Section Issues and Challenges in Emotion Analysis of\nArabic Tweets discusses tissues and challenges in the emoti on\nanalysis of Arabic tweets.\nRELATED STUDIES AND BACKGROUND\nTo the best of our knowledge, previous studies that conducted\nreviews for available resources and current practices were e ither\nin the English language or the attention was on speciﬁc tasks s uch\nas question answering with a speciﬁc language model (\nMahdi,\n2021) or text classiﬁcation ( Cruz and Cheng, 2020 ) and machine\ntranslation (Harrat et al., 2019 ).\nThere has been a number of research studies in the ﬁeld of\nArabic language focused on the analysis of articles in the ﬁe ld\nof NLP. For instance, Mohammad (2020) reviewed the literature\nwith over 1.1 million article information data sets acquire d from\nGoogle Scholar. A comprehensive study by Farha and Magdy\n(2021) provided a thorough comparative analysis of available\napproaches on a sentiment analysis task. Furthermore, study\nsurvey the NLP literature on dialectical Arabic data sets th at were\navailable.\nShoufan and Alameri (2015) evaluated the dialectical\nArabic NLP literature, and the work can be used as a reference\nto ﬁnd relevant contributions for distinct Arabic dialects t hat\naddress speciﬁc NLP aspects. However, this research was focus ed\nonly on data sets for Arabic dialects that were accessible.\nCurrently, a practical overview and detailed description of\nmaterials that can help in developing and coding an Arabic\nlanguage model for emotion classiﬁcation of Arabic tweets in the\nliterature are not available. The emotion classiﬁcation of A rabic\ntweets using NLP overall current practical practices and avai lable\nresources needs to be considered to provide a guideline and\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 2 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\noverview sight to facilitate future studies. Throughout th e next\nsections, we will give an overview of related terms and conce pts\nstarting with the language nature of the data down to the main\nfocus of our study, Arabic language models.\nArabic Language\nThe Arabic language is both diﬃcult and fascinating. It is\nfascinating because of its history, strategic signiﬁcance of its\npeople and the region they inhabit, and its cultural and litera ry\nheritage. It is also diﬃcult because of its complicated langua ge\nstructure. Classical Arabic has remained unmodiﬁed, intel ligible,\nand functional for centuries at a historical level. In terms of\nculture, the Arabic language is linked to Islam and a prestigi ous\nbody of literature. It is the native language of more than 400\nmillion people (\nBoudad et al., 2018 ) who live in a strategic\nregion with vast oil supplies that are vital to the global econo my.\nClassical Arabic (CA) was the language spoken by Arabs over\nfourteen centuries ago, whereas Modern Standard Arabic (MSA)\nis an evolving variation of Arabic with constant borrowings\nand innovations, demonstrating how Arabic reinvents itsel f to\nﬁt the changing demands of its speakers (\nAlhawarat et al.,\n2015).According to that, the importance of the Arabic language\ncannot be overstated. Not only is it used by hundreds of\nmillions of people, its online presence in terms of users and\ncontent is rapidly growing. Furthermore, Arabic has a variet y\nof diﬀerent characteristics that make automated handling an d\ncomprehension of Arabic text complex and fascinating. The\nArabic language is characterized as being morphologically r ich\nand complex. Multiple preﬁxes, suﬃxes, and inﬁxes can be added\nto a single stem in Arabic words.\nIn addition, there are two diﬀerent types of morphology,\nderivational and inﬂectional. The derivational morphology is\nprevalent, allowing numerous words to be derived from a single\nstem. Inﬂectional morphology, on the other hand, can combine\nnumerous morphemes into a single stem to produce considerably\nlonger and more complicated words. As a result, there are abou t\n5,000 or more diﬀerent forms of verbs. This diverse morphology\ngave rise to a wide range of dialects throughout the Arabic\nworld. Furthermore, the language’s complexity makes languag e\nmodeling diﬃcult in general. As we have mentioned above,\nthere are three major types of Arabic: CA, a form of Arabic\nlanguage used in the Quran and is formal and textual (\nSharaf\nand Atwell, 2012 ); MSA, which is used for writing and formal\nconversations; last but not least, Arabic dialect (AD), whic h is\nused in our daily lives and informal textual conversations. More\ndetails about the Arabic language used in social media are gi ven\nin the following sections.\nArabic Language in Social Media\nMore than 400 million people speak Arabic, which is the oﬃcial\nlanguage of 22 nations. It is the fourth most widely used lang uage\non the Internet (\nBoudad et al., 2018 ). Languages used in social\nmedia, for example Twitter, diﬀer signiﬁcantly from that util ized\nin other platforms such as Wikipedia. Nonlinguistic contents\nare written out, such as laughter, sound representations, an d\nemoticons, and the vocabulary is informal with intentional\ndeviations from standard orthography such as repeated letter s for\nemphasis errors, and nonstandard abbreviations are widespre ad.\nThis issue is aggravated in the case of Arabic social media fo r a\nvariety of reasons, including the fact that Arabic dialects often\nused in social media are phonologically, morphologically, an d\nlexically distinct from MSA, and they lack a standard (\nHabash,\n2010).\nConsequently, social media Arabic text has special\ncharacteristics compared to those of CA, MSA, and AD.\nWritten text in these platforms could be a combination of all of\nthese types, as well as non-Arabic words, samples, notations, and\northographic elements like spelling mistakes, repeated letter s, or\nemoticons. As a result, in addition to dealing with the Arabi c\nlanguage’s complexity concerns, other issues such as preparing\nthe text and removing noise, and dealing with dialects and\nlanguages, need to be considered (\nHegazi et al., 2021 ).\nLanguage Representation Models\nMany approaches and algorithms have been developed over the\nyears to infer vectors from text, whether at the character, w ord,\nsentence, or document level. All of the methods are intended\nto gain better measuring of the information’s richness and m ake\nit more machine-learnable. Early language representations , such\nas TF–IDF, a bag of words, and n-grams, depended primarily\non capturing the frequencies of word occurrences in a text.\nThese models, however, failed to capture the words’ syntactic\nand semantic meaning, and, thus, suﬀered from the constraint o f\nhigh dimensionality. In addition, these language represent ation\nmodels were insuﬃcient for text categorization on their own a nd\nmust always be augmented with speciﬁc features created by han d,\nwhich can be a time-consuming and labor-intensive process.\nThe shortcomings of these models led researchers to learn th e\nrepresentation of distributed words in a low dimension space\n(\nBolukbasi et al., 2016 ).\nBecause of the limits of traditional feature extraction\nmethods, other models have been proposed in the past that\nautomatically discover representations for downstream task s\nsuch as classiﬁcation. Feature learning or representation l earning\nrefers to methods that can extract features without need for\nmanual extraction. It is essential because the representati ons\nof an input have a signiﬁcant impact on machine learning\n(ML) models’ performance (\nBengio et al., 2013 ). Because of\nprior knowledge of various ML models, word embeddings have\nbecome particularly eﬃcient representation methods in the ﬁe ld\nof NLP to increase the performance of many downstream\ntasks. Because of their high capacity of representation learn ing,\ntraditional feature learning approaches have been supplanted b y\nmethods based on neural networks. Word embedding is a featur e\nlearning method in which a vocabulary or a word is mapped\nto an N-dimensional vector. Word2Vec (\nMikolov et al., 2013 ),\nGlove ( Pennington et al., 2014 ), and FastText ( Bojanowski et al.,\n2017) are some of the word embedding methods that have been\npresented. Because they contain fewer parameters than sparse\nvectors of explicit counts, these neural language models are\neasier to include as features in ML systems, and they generaliz e\nbetter and help minimize overﬁtting. Using a shallow neural\nnetwork, they learn automatically from text. As a result, cu rrent\nNLP research has concentrated on representation-learning\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 3 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\napproaches to learn features automatically and unsupervised\nfrom inputs in order to eliminate the considerable human labo r\nof feature creation.\nRecently, NLP research has concentrated on representation\nlearning, which is the process of autonomously learning feat ures\nfrom inputs in an unsupervised approach. Despite the fact\nthat these representations have improved performance on text\nclassiﬁcation tasks, they still have a number of shortcomin gs.\nFor instance, they are static, shallow, and unaﬀected by cont ext;\neach word has only one vector representation regardless of\nits meaning. Moreover, the out-of-vocabulary problem, which\nmeans that some terms that are not in the data set, are not\nrepresented yet.\nThe ﬁeld of NLP has grown signiﬁcantly since the\nintroduction of transformers (\nVaswani et al., 2017). Transformers\nare composed of various layers of encoders and decoders\nthat contain, among other things, multiple attention head\ncomponents. Transformers address the constraint of previous\nrecurrent neural network (RNN) limitation of only looking at\nthe previous words of a sequence by looking at all the words\naround the target word and assigning more weight to keywords ,\nwhich is known as the self-attention mechanism. As a result, each\nword is represented in relation to its context, and words migh t\nhave many vector representations depending on their meaning.\nThis is in contrast to prior word embedding methods, which\nused a single vector representation for each word regardless o f\ncontext or meaning. Transformers also have the ability to an alyze\ntext in parallel rather than sequentially, which speeds up the\nprocess. The current emergence of pre-trained language models\ncan be attributed to the combination of transformers and the\nconcept of transfer learning. The aim is to use the knowledge\nof a pre-trained model while completing a diﬀerent task. Many\nmachine learning models, including deep learning models suc h\nas artiﬁcial neural networks and reinforcement learning mo dels,\ncan beneﬁt from transfer learning.\nThe transformer’s evolution and the development of various\nmodels to learn universal language representations lead to b etter\nperformance and speed up convergence on downstream tasks.\nFollowing the pre-training step, the ﬁne-tuning process refer s\nto the adjustment of PLMs for downstream NLP tasks. A\nclassiﬁcation layer is added to this model, which uses SoftM ax\nto compute label probabilities. These massive PLM, which have\nbillions of parameters and have been learned from nearly all te xt\npublished on the internet, have improved the state-of-the-ar t on\nnearly every downstream NLP task, such as question answerin g,\nconversational agents, and sentiment analysis (\nQiu et al., 2020 ).\nFor language modeling, a transformer has been shown to\nbe more eﬃcient and faster than long short-term memory\n(LSTM) or convolutional neural network (CNN), and its basic\narchitecture consists of encoder and decoder blocks (\nVaswani\net al., 2017 ). The size of a model is determined by whether\nthe encoder or decoder is used alone or both by stacking\nmultiple encoder and decoder blocks. The bidirectional enco der\nrepresentations from transformers (BERT) model, for example,\nrelies solely on the encoder and comes in two sizes: the base,\nwhich stacks 12 encoders, and the large, which stacks 24\nencoders. The BERT model has pre-trained with two objectives :\nmasked language model (MLM), when given a sentence, the\nmodel masks 15% of the words from the input randomly,\nthen runs the entire masked sentence through the model,\nwhich must predict the masked words, and next sentence\nprediction (NSP), which requires the model to predict the next\nsentence from a given sentence. More language understanding\nmodels have been developed after BERT’s introduction, including\nRoBERTa (\nLiu et al., 2019 ), XLNet ( Yang et al., 2019 ), ALBERT\n(Lan et al., 2019 ), and ELECTRA ( Clark et al., 2020 ), which\nhave improved performance by experimenting with new pre-\ntraining approaches, updated model architectures, and larger\ntraining corpora.\nEMOTION ANALYSIS OF ARABIC TWEETS\nFirst, we will go through some of the existing emotion models\nfor Arabic tweets. Following that, we will highlight the Ara bic\nPLM available for ﬁne-tuning. Then, we will present the corpus,\nlexicons, and data sets that can be used to pre-train and ﬁne-\ntune the PLM for Arabic language and emotion classiﬁcation\nas a downstream task. Finally, we will discuss other availab le\nresources and tools, which exist in the literature and may hel p\nin pre-training language models for the Arabic language and\nemotion classiﬁcation of tweets.\nEmotion Models\nEmotion detection and classiﬁcation of Arabic text have rec ently\ngotten a lot of attention; however, there is a lot of work that\nneeds to be done in this ﬁeld to develop more accurate emotion\ndetection models in social media.\nIn the domain of Arabic emotion analysis, previous studies\nhave relied on lexicon-based methods (\nAbd Al-Aziz et al.,\n2015; Al-A ’abed and Al-Ayyoub, 2016 ), ML methods ( Hussien\net al., 2016; Al-Khatib and El-Beltagy, 2017 ), and a hybrid\napproach combined with ML methods with textual features of\ncontents to classify textual data. For instance,\nAbdullah et al.\n(2018) combined the lexicon-based method with a multi-criteria\ndecision-making approach to classify text into emotion classes .\nMachine learning (ML) methods can be categorized into\nsupervised and unsupervised approaches, which are used to\ndetect emotional expressions in text, such as happiness, sadne ss,\nand anger, automatically. Studies in the body of literature\nhave shown that a supervised machine learning approach can\noutperform lexicon-based approaches (\nSyed, 2015 ). Moreover,\nRabie and Sturm (2014) provided a method for detecting and\nclassifying emotions in Arabic tweets written in both the st andard\nand slang Egyptian dialects. Techniques for preprocessing were\ndiscussed, and the average accuracy score for all emotions w as\n64.3%. The authors of this article went into various points abo ut\npreprocessing, although the data set utilized in this study wa s\nspeciﬁc to the Egyptian revolution of 2011. Furthermore, the d ata\nset is small, with 1,605 tweets. On the other hand, the study re sults\nshould be improved.\nTraditional ML algorithms are constrained by the\nrequirement that both the training and testing data sets sho uld\nbelong to the same feature space and justify the same distributi on\n(\nWeiss et al., 2016 ). Transfer learning, on the other hand, allows\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 4 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nfor a wide range of domains, distributions, and tasks utiliz ed to\ntrain and test models ( Lu et al., 2015 ). The primary objective of\ntransfer learning (TL) is to develop a high-performing model in\na target domain using data from a source domain. The model’s\nperformance is improved by incorporating knowledge from a\nrelated area (\nWeiss et al., 2016 ). When obtaining training data\nsets is expensive, diﬃcult, or impossible, the technique migh t\nbe used. Training and test data for TL must not be independent\nand identically, to allow for evaluation of TL in case there i s lack\nof training data (\nTan et al., 2018 ). TL has been investigated for\nclassiﬁcation, clustering, and regression problems in trad itional\nML. Because DL is such a powerful ML technique, it is crucial to\nlook into deep TL applications (\nTan et al., 2018 ).\nAfter analyzing the literature on Arabic emotion classiﬁca tion\nof tweets, we discovered that the majority of previous studie s\nutilized either traditional ML with features or emotion lexi cons,\nor deep learning algorithms such as Word2vec or FastText, wh ich\nuse non-contextual embeddings. Only two studies, however,\nattempted to use PLMs and TL for the task of Arabic emotion\nclassiﬁcation. The ﬁrst model was a BERT-based model named\nMARBERT (\nAbdul-Mageed et al., 2020 ), which was trained on\ntweets, and social meaning tasks were considered in the ﬁne-\ntuning. The social meaning tasks included emotion detectio n.\nThe second model was the QARiB model (\nAbdelali et al., 2021 )\nthat was trained on a collection of over 420 million tweets, 1 80\nmillion sentences of text that was a combination from Arabic\nGigaWord, Abulkhair Arabic Corpus, and OPUS. The emotion\ndetection task was performed on the QARiB and four BERT-\nbased models.\nUntil now, emotion analysis of Arabic tweets still requires\nmore eﬀort, especially considering the beneﬁts of pre-trainin g\nand ﬁne-tuning a model for these tasks can provide. A signiﬁca nt\nnumber of Arabic PLMs were found in the literature, and we will\ngo through them in detail in the next section.\nPre-Trained Language Models for Arabic\nConstruction of numerous bidirectional transformer types,\nparticularly for Arabic, has seen a major revolution in the la st\nfew years. They serve as eﬀective transfer learning techniqu es for\na variety of NLP tasks such as text classiﬁcation, named enti ty\nrecognition (NER), and question answering (QA). While ﬁne-\ntuning transformer-based models, researchers produced cut ting-\nedge outcomes on a variety of downstream NLP tasks.\nIn terms of contextual language models for non-English\nlanguages, a multilingual BERT model was developed that was\ntrained on Wikipedia dumps of over 100 languages such as\nArabic. The literature, on the other hand, demonstrates tha t pre-\ntraining monolingual BERT performs better than multilingual\nBERT. The AraBERT (\nAntoun et al., 2020a ) model, which is\na BERT-based model pre-trained for Arabic language, and a\npre-trained contextualized text representation model. The mo del\nwas trained on Arabic Wiki dumps, the 1.5 billion-word Arabic\nCorpus, the OSIAN Corpus, Assaﬁr news articles, and four\nadditional manually crawled news websites. Furthermore, i t is\navailable in several versions, including AraBERT v1, AraBERTv02,\nand AraBERTv2, and distinction is in the usage of pre-segmented\ntext in AraBERT v1, where preﬁxes and suﬃxes were split using\nFarasaSegmenter (\nAbdelali et al., 2016 ), an Arabic-speciﬁc text\nsegmenter. AraBERT’s applicability to tasks involving dialec ts\nis limited, because it is pre-trained using MSA data. Three\ntasks were used to evaluate AraBERT: sentiment analysis,\nNER, and QA. Recently, AraBERT released a new version,\nAraBERTv0.2Twitterbase1, and large are two new models for Arabic\ndialects and tweets, trained by continuing the pre-training o n\nover 60 M Arabic tweets.\nMeanwhile,\nSafaya et al. (2020) proposed ArabicBERT, which\nincreases the amount of corpus used in the earlier AraBERT.\nThe models were pre-trained using the OSCAR in Arabic, a\nrecent dump of Arabic Wikipedia, and other Arabic resources.\nArabicBERT is available in four sizes depending on the size\nof the architecture: mini, medium, base, and large. Table 1\nshows the architectures of these four versions of ArabicBER T\nand the earlier AraBERT. In another study on PLM for the\nArabic language, a transformer architecture, ELECTRA, whi ch\ncontains two modules, a generator and a discriminator, was pre -\ntrained. Usually, a discriminator is taken and ﬁne-tuned fo r\ndownstream tasks. The Arabic-speciﬁc ELECTRA provided by\nAntoun et al. (2020b) , was trained on the same textual corpus\nused for AraBERT. However, we noticed that AraELECTRA\nis one of the top performing models while being much more\neﬃcient in its computational cost and achieves similar result s to\nAraBERT that has doubled the number of parameters. As a result ,\nit may be the best option when working with limited resources.\nAntoun et al. (2020c) presented AraGPT2, a stacked\ntransformer decoder model trained using the causal language\nmodeling objective, which is based on the original GPT2\n(\nRadford et al., 2019 ) architecture. The model was trained\nusing the same text corpus as AraELECTRA and AraBERT. The\nauthors did not test GPT2 on any data sets, because it was\ntrained using a causal language modeling objective; thus, t hey\nrelied on the perplexity reported during training. As we have\nmentioned, AraGPT2 is trained using a causal language modeli ng\nobjective, which appears to be eﬀective for sentence completion\nand language generation tasks but not for classiﬁcation task s. An\nArabic version of ALBERT was provided by Arabic ALBERT.\nThis model was trained on data from the Arabic version of\nthe OSCAR corpus and the Arabic Wikipedia. There are three\nvariants of this model based on the number of parameters. The\ndetails of the models are shown in Table 1. The model uses\nparameter reduction techniques such as factorized embeddin g\nparameterization, cross-layer parameter sharing to reduce th e\nnumber of parameters by 18 × and faster training by 1.7 × .\nArabic ALBERT2 provides an Arabic version of ALBERT (\nLan\net al., 2019 ). The Arabic Wikipedia and the Arabic version of\nthe OSCAR corpus were used to train this model. This model\nis available in three main versions depending on the number\nof parameters. The model reduces the number of parameters\nand speeds up training using parameter reduction approaches\nsuch as factorized embedding parameterization and cross-la yer\nparameter sharing. While we were investigating emotion anal ysis\nof Arabic tweets, we noticed that MSA is mostly utilized to\n1https://huggingface.co/aubmindlab/bert-base-arabertv02-tw itter\n2https://github.com/KUIS-AI-Lab/Arabic-ALBERT/\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 5 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nTABLE 1 |Summary of Arabic pre-trained language model versions, arc hitecture details, and training data sets.\nArabic pre-trained language models\nName versions Details Pre-train data\nLayers Hidden Heads Parameters\nMultilingual BERT Multilingual Cased 12 768 12 110M W\nMultilingual Uncased 12 768 12 110M\nAraBert AraBERT base 12 768 12 136M OS, W, N\nAraBERTlarge 12 768 12 371M\nAraBERTTwitterbase 12 768 12 136M OS, W, N, T\nAraBERTTwitterlarge 12 768 12 371M\nArabicBERT ArabicBERT Mini 4 256 4 11M OS, W\nArabicBERTMedium 8 512 8 42M\nArabicBERTBase 12 768 12 110M\nArabicBERTLarge 24 1024 16 340M\nQARiB QARiB25 mix 12 768 12 110M S, N, T\nAraGPT2 AraGPT2 base 12 768 12 135M OS, W, N\nAraGPT2medium 24 1024 16 370M\nAraGPT2large 36 1280 20 792M\nAraGPT2mega 48 1536 25 1,46B\nAraELECTRA AraELECTRA base 12 256 4 60M OS, W, N.\nArabic ALBERT ALBERT base 12 768 12 12M W, OS\nALBERTlarge 24 layers 1024 16 18M\nALBERTxlarge 24 layers 2048 32 60M\nMARBERT MARBERT 12 layers 256 12 160M OS, W, N, B, T\nThe pre-train data are indicated as follow: Tweets (T), Wikipedia (W), n ews (N), OSCAR corpus (OS), subtitles (S), and books (B).\ntrain a language model for Arabic. Textual data on Twitter ar e\nfrequently dialectal, with dialects lacking spelling norms and\ninformal in nature, and may include emojis, hashtags, and us er\nmentions. Only QARiB ( Abdelali et al., 2021 ) and MARBERT\n(Abdul-Mageed et al., 2020 ), as mentioned above, used tweets to\ntrain transformer models and ﬁne-tune for emotion classiﬁc ation\ntasks, and both of them are BERT-based. Moreover, even when\ntesting on informal text, increasing the variety of trainin g data\nby including both formal (MSA) and informal (AD) text is\nbetter than using informal text alone (\nAbdelali et al., 2021 ).\nChowdhury et al. introduced a new Arabic BERT in ( Abdelali\net al., 2021 ) QARiB. The authors attempted to improve the\nmodel’s performance by varying the training data in their stu dy.\nThey show that a BERT model trained on a mixture of data\nhas considerably superior generalization capability than a B ERT\nmodel trained just on MSA text in their experiments.\nIn a similar vein,\nAbdul-Mageed et al. (2020) trained\nMARBERT to increase its capacity to handle dialectal Arabic.\nThey enhanced the training data with a set of 1 Billion Arabic\ntweets that were randomly collected. The total amount of tex t in\nthe ﬁnal training data set was roughly 128 GB, with almost hal f\nof it being tweets. This model has been evaluated on sentimen t\nanalysis, social meaning prediction, topic categorization, dialect\nidentiﬁcation, and named entity recognition, among other N LP\ntasks. MSA and AD were used to train models like MARBERT\nand QARiB. The performance of these models is noticeably\nsuperior to that of other similar or even larger versions, suc h as\nArabicBERTlarge and Arabic ALBERT large. However, it is worth\nnoting that the applied prepossessing has an eﬀect on the result\nof similar models. AraBERT base and ArabicBERT, for example,\nare built on the same architecture and trained on the same\ndata; however, AraBERT outperforms ArabicBERT. Non-Arabic\nwords are removed during AraBERT preprocessing; however,\nnon-Arabic words are in line with ArabicBERT data.\nFurthermore, it is obvious that the training process has a\nsigniﬁcant impact on a model’s performance. For classiﬁcation\ntasks, models trained with masked language modeling\n(MLM), BERT variants, or replacement token detection\n(RTD) ELECTRA perform better. AraGPT2 is trained using a\ncausal language modeling objective, which appears to be eﬀectiv e\nfor sentence completion and language production tasks but not\nfor classiﬁcation tasks. More details about the available la nguage\nmodel for Arabic are shown in Table 1.\nThe eﬀect of parameters with diﬀerent input layers, hidden\nlayers, and attention heads on the performance of downstream\ntasks depends on the number of parameters used in training\nalong with layers, hidden layers, and attention heads. Seve ral\nmodels utilized a very low number of parameters outperform\nmodels with higher parameters. With less parameters, diﬀerent\ncombinations are also used such as those with less parameters but\nwere pre-trained with deep hidden layers. More experiments in\nthe side of model architecture and the eﬀect of it in the result of\ndownstream task need to be take attention for develop the ﬁel d of\nArabic PLM.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 6 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nData Sets and Corpus\nAnnotated data sets are required for training a model\nin supervised learning approaches for text classiﬁcation.\nFurthermore, the ﬁne-tuning task requires label data once a\ntransformer model has been pre-trained. Only few of these can\nbe found in the literature in the ﬁeld of emotion detection\nfrom text. Furthermore, the size of the datasets described i s\nextremely limited. Due to the fact that emotion detection in\nArabic text is a relatively new research domain, there are on ly\nfew data sets available for this task; for example,\nEl Gohary\net al. (2013) annotated a data set derived from children’s stories.\nThe information was split into 2,514 non-overlapping sequenti al\nsentences from 100 articles. When annotating data, six emot ion\nclasses were used: surprise, disgust, rage, fear, sadness, a nd\nhappiness. Additionally, a neutral class was created for sent ences\nthat did not communicate any emotion. For testing, another\n35 documents were used. Each phrase was classiﬁed using\na vocabulary that mapped individual terms to the six target\ncategories. Emotional classes and phrases were represented a s\nvectors, with each vector consisting of emotional word co-\noccurrence frequencies. To determine the proper class for eac h\nsentence, the cosine similarity metric was employed to comput e\nthe similarity between sentences and classes. If a sentence ’s\ncosine similarity to all emotional class vectors did not sur pass a\nparticular threshold, it was deemed neutral.\nArSEL, a large-scale Arabic Sentiment and Emotion Lexicon,\nwas provided in\nBadaro et al. (2018b) as an extension of ArSenL\n(Badaro et al., 2014 ), with the addition of eight emotion scores\nto most ArSenL Arabic lemmas. EmoWordNet ( Badaro et al.,\n2018a), a WordNet-based English emotion lexicon, was used to\nobtain emotion ratings. The researchers produced a large-sc ale\nmood and emotion lexicon in Arabic, which contained over\n32,000 terms. Their language is based on ArSenL, an existing\nArabic sentiment lexicon. They used the SeEval 2018 shared t ask\nto test their lexicon and reported an encouraging accuracy.\nA study by\nAbdul-Mageed and Ungar (2017) used a deep\nlearning (DL) approach for emotion detection in text and creat ed\nEmoNet, a very large data set for ﬁne-grained emotions that i s\nclaimed to be the largest data set developed from tweets; deta ils\nof the dataset are presented in Table 2. In the study by Al-Khatib\nand El-Beltagy (2017) , they constructed and balanced a data\nset for Arabic emotions, which was compiled from a variety of\nsources, one of which being tweets scraped using Twitter’s se arch\nAPI. Egypt’s geolocation was used to ﬁlter the data. These twe ets\nwere gathered over the course of a month and are primarily\nin Egyptian. Furthermore, happiness, anger, pity, sadness, fea r,\nsurprise, love, and none were the emotion categories. To make\nthe process of emotion annotation easier, they used a web-bas ed\nfront end.\nA multi-dialect data set for Arabic emotion analysis named\nDINA was created in\nAbdul-Mageed et al. (2016) , which was\none of the earliest attempts on building datasets for Arabic\nemotions. A seed list of Arabic emotions based on the Ekman\nclassiﬁcation of six emotions is used to collect the data set . This\nseed list was used to query the Twitter API. Then, 500 tweets w ere\nchosen for each of the six emotions, giving a data set of 3,000\ntweets. Human annotators manually annotated the data set. T he\nannotators were asked to determine the emotion ﬁrst, then th e\nintensity of the emotion using a scale of zero, weak, fair, or strong\nfor each of the six emotions. In addition,\nAlhuzali et al. (2018)\ncreated a new data set of Arabic tweets for emotion recogniti on.\nThe Plutchik primary emotions are anger, anticipation, disgu st,\nfear, joy, sadness, surprise, and trust, and they use a list of\nArabic seeds to represent each of them. After that, two data\nsets were created. The LAMA dataset contains 8,000 tweets,\n1,000 for each of the eight emotions, which were manually\nannotated by four annotators. This data set had 7,268 tweets\nafter cleaning and deleting duplicates. Using the same seed lis t\nof eight emotions, the second dataset, LAMA-DIST, was create d\nby distant supervision. The ﬁnal data set contains 182,690 tw eets\nafter cleaning and deleting duplicates and tweets with less tha n\nﬁve words.\nA study by\nAl-Khatib and El-Beltagy (2017) made another\nattempt to give an annotated data set of Arabic tweets for\nemotions, with a data set of 10,000 tweets annotated for eigh t\nemotions: joy, happiness, rage, sympathy, sadness, fear, surpr ise,\nlove, and none. Human annotators manually annotated the\ndata set; more details about the data set are shown in Table 2.\nMoreover,\nHussien et al. (2016) presented an emoji-based\nautomatic technique for emotion annotation. The feasibili ty of\nthe proposed approach, which was evaluated using two classiﬁers,\nsupport vector machine (SVM) and multinomial naive Bayes\n(MNB), was demonstrated by a comparison with a manually\nannotated data set. The result of the experiments reveals tha t\nSVM and MNB-based automatic labeling systems outperform\nmanual labeling techniques. A study by\nAlmahdawi and Teahan\n(2019) introduced the Iraqi Arabic Emotion Data set (IAEDS), an\nArabic dataset from Facebook posts written in the Iraqi dialec t.\nMoreover, the data set was annotated according to Ekman’s\nbasic emotions. Recently, two studies had considered emotion i n\nTwitter during the COVID-19 pandemic, SenWave (\nYang et al.,\n2020) and AraEmoCorpus ( Al-Laith and Alenezi, 2021 ), which\nare described in detail in Table 2.\nAvailable Resources and Tools\nTo develop or pre-train language models, diﬀerent languages\nshould have various quantities of resources and training da ta.\nHigh-, medium-, and low-resource languages are the three ty pes.\nLanguages with a lot of resources, such as English, Chinese,\nand Russian, have a lot of open-source resources and contents\nthat can be used in training. As a result, NLP researchers hav e\nrecently focused their eﬀorts on developing resources, tools, a nd\ntraining data. There are some existing resources that can be\nutilized in the preprocessing and training of an Arabic langua ge\nmodel. There were eﬀorts aimed at making researchers’ work\neasy, such as by automating routine workﬂows, cleaning, and\npreprocessing. Huggingface3 and AllenNLP4 are two examples of\nthis. Majority of these tools are designed to work in English o r to\ngeneralize a pipeline so it could be used with other languages.\nArabic makes contributions as well, although not as much as\nother languages. Some promising examples are MADAMIRA\n3https://huggingface.co\n4https://allennlp.org\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 7 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nTABLE 2 |Emotion data set available for Arabic language.\nRef. no Emotion dataset details\nName Description Size\nBadaro et al. (2018b) ArSEL ArSEL is an Arabic sentiment and emotion lexicon\ndesigned to supplement the publicly available Arabic\nSentiment Lexicon, ArSenL, and provide a large-scale\nlexicon with emotion and sentiment labels for practically\nevery lemma in ArSenL.\n32,196 Arabic lemmas\nwere annotated with\nsentiment and emotion\nscores at the same\ntime.\nAl-Khatib and El-Beltagy (2017) AETD Egyptian dialect tweets constitute the dataset. Anger ,\nfear, happiness, love, sadness, surprise, sympathy, or\nnone were used to categorize the tweets.\nThe total number of\ntweets is 10,065\nAlmahdawi and Teahan (2019) IAEDS This corpus is consisting of Iraqi dialect Facebook\npostings. It is divided into six datasets, each of which\ncontains instances of Ekman’s basic emotions.\n1,365 posts from\nFacebook\nMohammad et al. (2018) SemEval-2018 This opinion corpus consists of tweets labele d as neutral\nor as one or more of 11 emotions include anger,\nanticipation, disgust, fear, happiness, love, optimism,\npessimism, sadness, surprise, and trust.\n4,381 tweets\nSaad (2015) Emotion-lexicon Arabic and English emotion lexicon, each e ntry in this\nlexicon annotated with one of Ekman’s basic emotions.\nEach emotion label has\n3,207 Arabic words\nassociated with it\nAbdul-Mageed et al. (2016) DINA Tweets dataset annotated by the following emotions,\nhappiness, sadness, anger, disgust, surprise, and fear.\nThe tweets were gathered by utilizing a set of seeds to\nquery Twitter for each class.\n3,000 tweets\nAlhuzali et al. (2018) LAMA-DINA Dataset for MSA and AD emotion, the tweets were\nlabeled with the Plutchik emotions.\n9,064 tweets\nYang et al. (2020) SenWave Collected tweets annotated for the task of ﬁne-grain ed\nsentiment analysis with 11 emotions including optimistic,\nthankful, empathetic, pessimistic, anxious, sad,\nannoyed, denial, ofﬁcial report, surprise, and joking.\n10 K Arabic and English\ntweets\nAl-Laith and Alenezi (2021) AraEmoCorpus The corpus contains Arabic tweets tagged with emotion\ncategories: anger, disgust, fear, joy, sadness, and\nsurprise.\n5.5 million Arabic\ntweets\nShakil et al. (2021) AEELex Tweets of users living in Mecca and Riyadh were\ncollected, Arabic English emotion lexicon was classiﬁed\non the basis of Plutchik’s eight basic emotion categories.\n35,383 tweets.(Pasha et al., 2014 ), Farasa ( Abdelali et al., 2016 ), CAMeL NLP\n(Obeid et al., 2020 ), AraNLP ( Althobaiti et al., 2014 ), ARBML\n(Alyafeai and Al-Shaibani, 2020 ), and AraNet ( Abdul-Mageed\net al., 2019 ).\nMADAMIRA was developed by Pasha et al. (2014) for\nmorphological analysis using algorithms. MADAMIRA ’s source\ncode is accessible through email for educational purposes onl y,\nand it is also available via API and a web interface. The Research\nby\nAbdelali et al. (2016) developed Farasa, an Arabic NLP tool\nkit that could be used for segmentation and stemming, Named\nEntity Recognition (NER), Part Of Speech tagging (POS tagging) ,\nand Diacritization (Tashkeel), among other things. The too l kit\nwas implemented in Java (\nObeid et al., 2020 ) and provided a\ncollection of open-source tools for an Arabic NLP in Python\nnamed CAMeL NLP. Currently, CAMeL provides utilities for\npre-processing, morphological modeling, dialect identiﬁcati on,\nnamed entity recognition, and sentiment analysis.\nAnother study by\nAlthobaiti et al. (2014) developed a Java-\nbased tool kit, the AraNLP library, for processing Arabic\ntext. Meanwhile, most preprocessing steps are supported, such\nas diacritic and punctuation removal, tokenization, senten ce\nsegmentation, POS labeling, root stemming, light stemming ,\nand word segmentation. Additionally, ARBML (\nAlyafeai and Al-\nShaibani, 2020 ) is a collection of tools that make Arabic NLP\naccessible through a variety of interfaces. These studies ha ve\naimed to address the NLP pipeline from dataset scraping to\npreprocessing, tokenization, training, and deployment. Ther e are\nalso three libraries, tnqeeb, tnkeeh, and tkseem, that devel opers\ncan use to create tools that support Arabic NLP. The tools\nutilize Arabic’s morphological nature to provide a variety of\nfeatures that are unique to Arabic. Furthermore, ARBML uses\nKeras and TensorFlow to provide various training models with\ndiﬀerent techniques for both command lines and web interface s.\nIt also provides a pipeline from cleaning the dataset to model\ntraining and deployment, which is documented in detail as Col ab\nNotebooks, and users can test diﬀerent models directly in the\nbrowser. The source code, as well as the training notebooks, is\navailable on GitHub.\nIn terms of tweets as a data source, we may point to AraNet\n(\nAbdul-Mageed et al., 2019 ), which is a collection of deep\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 8 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nlearning Arabic social media processing tools that analyze 15\ndatasets related to Arabic sentiment analysis, including MS A and\nAD, using the multilingual BERT model. Additionally, AraNe t\nanalyzed social media posts to predict age, dialect, gender,\nemotion, irony, and sentiment. By providing a single approach\nto test new models against AraNet predictions, AraNet has the\npotential to relieve challenges connected to comparing acros s\ndiverse Arabic social media NLP tasks.\nThese tools help in segmentation, part of speech tagging,\nnamed entity recognition, discretization, and grammatica l\nanalysis, among other NLP tasks for Arabic. However, most of\nthese tools may not take advantage of current developments\nin NLP. Unfortunately, open-source contributions are not\nfrequently welcomed in Arabic tweet analysis and classiﬁca tion.\nBecause they comply with Twitter’s Terms of Service, many dat a\nsets were collection of tweet IDs and labels; if you tend to us e,\nit you need to spend some time to retrieve and preprocess these\ntweets. Although some resources can be given on demand, this\napproach is still not developer-friendly and is time-consumin g.\nAdvances in the ﬁeld of Arabic classiﬁcation tasks, althoug h it\nhas great potential, are still limited, especially when we tal k about\ntasks involving AD. In the next section, we will discuss some\nissues and challenges.\nISSUES AND CHALLENGES IN EMOTION\nANALYSIS OF ARABIC TWEETS\nThere are some challenges that researchers have faced in the\nﬁeld of Arabic tweet emotion analysis. Because of diﬀerences\nbetween AD and MSA, the dialectal amount of data on the\nInternet has expanded as a result of the rise of social media,\nand NLP tools that support MSA are not well-suited to analyze\nthese data. In addition, a rich body of literature considers\nutilizing the MSA corpus to pre-train LM; the ﬁne-tuning of\nsuch a model in tasks involving social media data produced\nunsatisfying results. Consequentially, in MRABERT experime nts,\nthe AraNETEmo data set was used to ﬁne-tune MRABERT\nfor emotion classiﬁcation, and model F1-score was 75.83, wh ile\nmBERT, AraBERT, and ARABERT, using the same dataset, F1-\nscores were 65.79, 65.68, and 67.73, respectively. This experi ment\nhighlighted the importance of pre-training a model with data\nfrom the origin where the ﬁne-tuning task data will be. Anothe r\nexperiment used social media data, especially tweets, for pre-\ntraining a language model, QARiB, which was ﬁne-tuned in\nthe SemEval-2018 data set, Macro-averaged F1 for QARiB was\n46.8, which was higher than that of other Bert-based models\ninvolved in that experiment. However, the results in the ﬁeld\nof emotion analysis are not comparable with the results in\nsentiment analysis.\nThere is a limitation on the dialect’s resources, Twitter use rs\nwho generate Arabic tweets from the various region, over 25\ndialects of Arabic are spread globally, fetching Arabic tweets\ncan collect tweets from multiple dialects, the PLM need to be\npre-train in dialects corpus as well as the ﬁne-tuning, large\nand balance dataset for emotion classiﬁcation of tweets need to\ndevelop and available for the researchers’ community.\nIn general, the Arabic language suﬀers from orthographic\nambiguity, in which the form of characters and spelling of\nwords can vary depending on their context; on the other\nhand, emotions in short text like tweets can be ambiguous.\nAdditionally, emotional words in Arabic can be understood\ndiﬀerently according to the context that appeared. Similarly,\nﬁnding implicit emotions in emotional text that lacks explici t\nemotional phrases is far more diﬃcult than ﬁnding explicit\nemotions in direct, explicit text. Furthermore, Arabic is a\nmorphologically rich language, which means that the same ver b\ncan have thousands of diﬀerent forms.\nAs previously stated, tweets include characteristics that\ndiﬀerentiate them from other types of writing and add to\nthe task’s complexity: the text is brief, informal, and contain s\nmisspellings, unique symbols such as emoticons and emojis,\nshort forms of words, hashtags, and abbreviations. Emotico ns\nmay assist in the classiﬁcation of emotions in text, but beca use\nthey are not normal words and are not found in dictionaries,\nthey are still considered anomalies in text. As a result, gre at care\nshould be taken to ensure that they are not eliminated throug hout\nthe preprocessing step. In this section, we tried to discuss som e\nof the challenges in the ﬁeld and review some issues to help in\ndeveloping the ﬁeld of emotion analysis of Arabic tweets using\nlanguage models.\nCONCLUSION\nIn this study, we tried to cover the available models, resourc es,\ndatasets, tools for Arabic LM and emotion classiﬁcation of tw eets,\nand the available PLM that can be utilized for ﬁne-tuning in t he\nemotion detection task. According to the current state of th e\nﬁeld, we discovered some issues and challenges. Regardless of the\nfact that we recognize that the NLP ﬁeld is rapidly evolving an d\nthat both Arabic NLP researchers and practitioners recogniz e the\nimportance of incorporating Arabic into language technologi es,\nthe AD still needs more eﬀort to drive the ﬁeld of social media\nclassiﬁcation tasks more in that direction.\nAUTHOR CONTRIBUTIONS\nGA and AA designed, conducted, and wrote the study. All\nauthors contributed to the article and approved the ﬁnal versi on.\nREFERENCES\nAbd Al-Aziz, A. M., Gheith, M., and Eldin, A. S. (2015). “Lexico n based and\nmulti-criteria decision making (MCDM) approach for detecting emotio ns\nfrom Arabic microblog text, ” in 2015 First International Conference on\nArabic Computational Linguistics (ACLing) (Piscataway, NJ: IEEE), 100–105.\ndoi: 10.1109/ACLing.2015.21\nAbdelali, A., Darwish, K., Durrani, N., and Mubarak, H. (2016). Faras a:\na fast and furious segmenter for arabic. in Proceedings of the 201 6\nconference of the North American chapter of the association for\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 9 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\ncomputational linguistics: Demonstrations 3, 11–16. doi: 10.18653/v1/N\n16-3003\nAbdelali, A., Hassan, S., Mubarak, H., Darwish, K., and Samih, Y. (2 021).\nPre-training BERT on Arabic tweets: practical considerations. arXiv Prepr.\narXiv2102.10684. doi: 10.48550/arXiv.2102.10684\nAbdullah, M., Hadzikadicy, M., and Shaikhz, S. (2018). “SEDAT : sentiment and\nemotion detection in Arabic text using CNN-LSTM deep learning, ” in 2018 17th\nIEEE International Conference on Machine Learning and Appli cations (ICMLA)\n(Piscataway, NJ: IEEE), 835–840. doi: 10.1109/ICMLA.2018 .00134\nAbdul-Mageed, M., AlHuzli, H., and DuaaAbu Elhija, M. D. (2016). “D ina: a multi-\ndialect dataset for arabic emotion analysis, ” in The 2nd Workshop on Arabic\nCorpora and Processing Tools, 29.\nAbdul-Mageed, M., Elmadany, A., and Nagoudi, E. M. B. (2020). AR BERT\n& MARBERT: deep bidirectional transformers for Arabic. arXiv Prepr.\narXiv2101.01785. doi: 10.18653/v1/2021.acl-long.551\nAbdul-Mageed, M., and Ungar, L. (2017). “Emonet: ﬁne-grained emo tion detection\nwith gated recurrent neural networks, ” in Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (v olume 1: Long\nPapers), 718–728. doi: 10.18653/v1/P17-1067\nAbdul-Mageed, M., Zhang, C., Hashemi, A., and Nagoudi, E. M. B. (2019). Aranet:\na deep learning toolkit for arabic social media. arXiv Prepr. arXiv1912.13072 .\ndoi: 10.48550/arXiv.1912.13072\nAl-A ’abed, M., and Al-Ayyoub, M. (2016). “A lexicon-based approach for emotion\nanalysis of arabic social media content, ” in The International Computer Sciences\nand Informatics Conference (ICSIC) , 343–351.\nAlhawarat, M., Hegazi, M., and Hilal, A. (2015). Processing the text of the\nHoly Quran: a text mining study. Int. J. Adv. Comput. Sci. Appl. 6, 262–267.\ndoi: 10.14569/IJACSA.2015.060237\nAlhuzali, H., Abdul-Mageed, M., and Ungar, L. (2018). “Enabling de ep\nlearning of emotion with ﬁrst-person seed expressions, ” in Proceedings of\nthe Second Workshop on Computational Modeling of People’s O pinions,\nPersonality, and Emotions in Social Media, 25–35. doi: 10.18653/v1/W18-\n1104\nAl-Khatib, A., and El-Beltagy, S. R. (2017). “Emotional tone det ection\nin arabic tweets, ” in International Conference on Computational\nLinguistics and Intelligent Text Processing (Berlin: Springer), 105–114.\ndoi: 10.1007/978-3-319-77116-8_8\nAl-Laith, A., and Alenezi, M. (2021). Monitoring people’s emotions and symptoms\nfrom Arabic tweets during the COVID-19 pandemic. Information 12:86.\ndoi: 10.3390/info12020086\nAlmahdawi, A. J., and Teahan, W. J. (2019). “A new arabic dataset for emotion\nrecognition, ” inIntelligent Computing-Proceedings of the Computing Confer ence\n(Berlin: Springer), 200–216. doi: 10.1007/978-3-030-22868-2_ 16\nAlthobaiti, M., Kruschwitz, U., and Poesio, M. (2014). AraNLP: A Java-Based\nLibrary for the Processing of Arabic Text . University of Essex.\nAlyafeai, Z., and Al-Shaibani, M. (2020). “ARBML: democritizin g arabic natural\nlanguage processing tools, ” in Proceedings of Second Workshop for NLP Open\nSource Software (NLP-OSS) , 8–13. doi: 10.18653/v1/2020.nlposs-1.2\nAntoun, W., Baly, F., and Hajj, H. (2020a). AraBERT: transformer-b ased\nmodel for Arabic language understanding. arXiv Prepr. arXiv2003.00104 .\ndoi: 10.48550/arXiv.2003.00104\nAntoun, W., Baly, F., and Hajj, H. (2020b). Araelectra: pre-training\ntext discriminators for arabic language understanding. arXiv Prepr.\narXiv2012.15516. doi: 10.48550/arXiv.2012.15516\nAntoun, W., Baly, F., and Hajj, H. (2020c). AraGPT2: pre-trained\ntransformer for Arabic language generation. arXiv Prepr. arXiv2012.15520 .\ndoi: 10.48550/arXiv.2012.15520\nBadaro, G., Baly, R., Hajj, H., Habash, N., and El-Hajj, W. (2014) . “A large\nscale Arabic sentiment lexicon for Arabic opinion mining, ” in Proceedings of\nthe EMNLP 2014 Workshop on Arabic Natural Language Processing ( ANLP),\n165–173. doi: 10.3115/v1/W14-3623\nBadaro, G., Jundi, H., Hajj, H., and El-Hajj, W. (2018a). “EmoWordN et: automatic\nexpansion of emotion lexicon using English WordNet, ” in Proceedings of the\nSeventh Joint Conference on Lexical and Computational Seman tics, 86–93.\ndoi: 10.18653/v1/S18-2009\nBadaro, G., Jundi, H., Hajj, H., El-Hajj, W., and Habash, N. (2018 b). Arsel: a large\nscale arabic sentiment and emotion lexicon. OSACT 3:26.\nBengio, Y., Courville, A., and Vincent, P. (2013). Representati on learning: a review\nand new perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 35, 1798–1828.\ndoi: 10.1109/TPAMI.2013.50\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). E nriching word\nvectors with subword information. Trans. Assoc. Comput. Linguist. 5, 135–146.\ndoi: 10.1162/tacl_a_00051\nBolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. (2016).\nMan is to computer programmer as woman is to homemaker? debiasing word\nembeddings. Adv. Neural Inf. Process. Syst. 29, 4349–4357.\nBoudad, N., Faizi, R., Thami, R. O. H., and Chiheb, R. (2018). Se ntiment\nanalysis in Arabic: a review of the literature. Ain Shams Eng. J. 9, 2479–2490.\ndoi: 10.1016/j.asej.2017.04.007\nClark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). Ele ctra: pre-\ntraining text encoders as discriminators rather than generators . arXiv Prepr.\narXiv2003.10555. doi: 10.48550/arXiv.2003.10555\nCruz, J. C. B., and Cheng, C. (2020). Establishing baselines for text\nclassiﬁcation in low-resource languages. arXiv Prepr. arXiv2005.02068 .\ndoi: 10.48550/arXiv.2005.02068\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ert: pre-training\nof deep bidirectional transformers for language understanding. arXiv Prepr.\narXiv1810.04805. doi: 10.48550/arXiv.1810.04805\nEl Gohary, A. F., Sultan, T. I., Hana, M. A., and El Dosoky, M. M. ( 2013). A\ncomputational approach for analyzing and detecting emotions in Arab ic text.\nInt. J. Eng. Res. Appl. 3, 100–107.\nFarha, I. A., and Magdy, W. (2021). A comparative study of eﬀecti ve\napproaches for arabic sentiment analysis. Inf. Process. Manag. 58:102438.\ndoi: 10.1016/j.ipm.2020.102438\nHabash, N. Y. (2010). Introduction to Arabic natural language\nprocessing. Synth. Lect. Hum. Lang. Technol. 3, 1–187.\ndoi: 10.2200/S00277ED1V01Y201008HLT010\nHarrat, S., Meftouh, K., and Smaili, K. (2019). Machine translatio n\nfor Arabic dialects (survey). Inf. Process. Manag. 56, 262–273.\ndoi: 10.1016/j.ipm.2017.08.003\nHegazi, M. O., Al-Dossari, Y., Al-Yahy, A., Al-Sumari, A., and Hilal, A.\n(2021). Preprocessing Arabic text on social media. Heliyon 7:e06191.\ndoi: 10.1016/j.heliyon.2021.e06191\nHussien, W. A., Tashtoush, Y. M., Al-Ayyoub, M., and Al-Kabi, M. N. (2016). “Are\nemoticons good enough to train emotion classiﬁers of arabic tweet s?, ” in2016\n7th International Conference on Computer Science and Informat ion Technology\n(CSIT) (Piscataway, NJ: IEEE), 1–6. doi: 10.1109/CSIT.2016.7549 459\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R . (2019).\nAlbert: a lite bert for self-supervised learning of language representat ions. arXiv\nPrepr. arXiv1909.11942. doi: 10.48550/arXiv.1909.11942\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2 019). Roberta: a\nrobustly optimized bert pretraining approach. arXiv Prepr. arXiv1907.11692 .\ndoi: 10.48550/arXiv.1907.11692\nLu, J., Behbood, V., Hao, P., Zuo, H., Xue, S., and Zhang, G. (201 5). Transfer\nlearning using computational intelligence: a survey. Knowledge-Based Syst. 80,\n14–23. doi: 10.1016/j.knosys.2015.01.010\nMahdi, A. F. (2021). Survey: using BERT model for Arabic Questi on Answering\nSystem. Turkish J. Comput. Math. Educ. 12, 723–729.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J . (2013). Distributed\nrepresentations of words and phrases and their compositionality. Adv Neural\nInformat Processing Syst. 2013, 3111–3119.\nMohammad, S. (2020). “Nlp scholar: a dataset for examining the sta te of nlp\nresearch, ” in Proceedings of the 12th Language Resources and Evaluation\nConference, 868–877.\nMohammad, S., Bravo-Marquez, F., Salameh, M., and Kiritchenko, S. ( 2018).\n“Semeval-2018 task 1: aﬀect in tweets, ” in Proceedings of the 12th International\nWorkshop on Semantic Evaluation , 1–17. doi: 10.18653/v1/S18-1001\nObeid, O., Zalmout, N., Khalifa, S., Taji, D., Oudah, M., Alhafni , B., et al. (2020).\n“CAMeL tools: an open source python toolkit for Arabic natural langu age\nprocessing, ” in Proceedings of the 12th Language Resources and Evaluation\nConference, 7022–7032.\nPasha, A., Al-Badrashiny, M., Diab, M. T., El Kholy, A., Eskande r, R., Habash, N.,\net al. (2014). “Madamira: a fast, comprehensive tool for morphological analysis\nand disambiguation of arabic, ” in Lrec (Kuala Lumpur: Citeseer), 1094–1101.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 10 March 2022 | Volume 5 | Article 843038\nAlqahtani and Alothaim Emotion Analysis of Arabic Tweets\nPennington, J., Socher, R., and Manning, C. D. (2014). “Glove : global\nvectors for word representation, ” in Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , 1532–1543.\ndoi: 10.3115/v1/D14-1162\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. (2020). Pre -trained\nmodels for natural language processing: a survey. Sci. China Technol. Sci. 3,\n1–26. doi: 10.1007/s11431-020-1647-3\nRabie, O., and Sturm, C. (2014). “Feel the heat: emotion detecti on in Arabic\nsocial media content, ” inThe International Conference on Data Mining, Internet\nComputing, and Big Data (BigData2014) . (Kuala Lumpur: Citeseer), 37–49.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (201 8). Improving\nLanguage Understanding by Generative Pre-Training . The University of British\nColumbia: OpenAI research group.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskev er, I. (2019).\nLanguage models are unsupervised multitask learners. OpenAI Blog 1:9.\nSaad, M. K. (2015). Mining Documents and Sentiments in Cross-lingual Context .\nNancy: Univ. Lorraine.\nSafaya, A., Abdullatif, M., and Yuret, D. (2020). “Kuisail at se meval-2020\ntask 12: Bert-cnn for oﬀensive speech identiﬁcation in social media, ” in\nProceedings of the Fourteenth Workshop on Semantic Evaluat ion, 2054–2059.\ndoi: 10.18653/v1/2020.semeval-1.271\nShakil, K. A., Tabassum, K., Alqahtani, F. S., and Wani, M. A. (20 21). Analyzing\nuser digital emotions from a holy versus non-pilgrimage city in Saud i Arabia\non Twitter Platform. Appl. Sci. 11:6846. doi: 10.3390/app11156846\nSharaf, A.-B. M., and Atwell, E. (2012). “QurAna: Corpus of the Quran\nannotated with Pronominal Anaphora, ” in LREC (Kuala Lumpur: Citeseer),\n130–137.\nShoufan, A., and Alameri, S. (2015). “Natural language processing f or dialectical\nArabic: a survey, ” in Proceedings of the Second Workshop on Arabic Natural\nLanguage Processing, 36–48. doi: 10.18653/v1/W15-3205\nShukla, A., and Shukla, S. (2015). A survey on sentiment classiﬁc ation\nand analysis using data mining. Int. J. Adv. Res. Comput. Sci. 6:603–12.\ndoi: 10.1109/ABLAZE.2015.7154934\nSingh, V. K., Piryani, R., Uddin, A., and Waila, P. (2013). “Sent iment\nanalysis of movie reviews: a new feature-based heuristic for aspec t-\nlevel sentiment classiﬁcation, ” in 2013 International Mutli-Conference on\nAutomation, Computing, Communication, Control and Compre ssed Sensing\n(iMac4s) (Piscataway, NJ: IEEE), 712–717. doi: 10.1109/iMac4s.201 3.6526500\nSyed, A. Z. (2015). “Applying sentiment and emotion analysis on bran d\ntweets for digital marketing, ” in 2015 IEEE Jordan Conference on Applied\nElectrical Engineering and Computing Technologies (AEECT ) (Piscataway, NJ:\nIEEE), 1–6.\nTan, C., Sun, F., Kong, T., Zhang, W., Yang, C., and Liu, C. (20 18). “A survey\non deep transfer learning, ” in International Conference on Artiﬁcial Neural\nNetworks (Berlin: Springer), 270–279. doi: 10.1007/978-3-030-01424-7_ 27\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\net al. (2017). Attention is all you need. arXiv Prepr. arXiv1706.03762 .\ndoi: 10.48550/arXiv.1706.03762\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). “Transfe r learning\ntechniques, ” in Big Data Technologies and Applications (Berlin: Springer),\n53–99. doi: 10.1007/978-3-319-44550-2_3\nWikarsa, L., and Thahir, S. N. (2015). “A text mining application of emotion\nclassiﬁcations of Twitter’s users using Naive Bayes method, ” in 2015 1st\nInternational Conference on Wireless and Telematics (ICWT) (Piscataway, NJ:\nIEEE), 1–6. doi: 10.1109/ICWT.2015.7449218\nYang, Q., Alamro, H., Albaradei, S., Salhi, A., Lv, X., Ma, C., et al.\n(2020). Senwave: monitoring the global sentiments under the Cov id-\n19 pandemic. arXiv Prepr. arXiv2006.10842 . doi: 10.48550/arXiv.2006.\n10842\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R. , and Le, Q. V. (2019).\nXlnet: generalized autoregressive pretraining for language understa nding. Adv.\nNeural Inf. Process. Syst. 32:10842.\nConﬂict of Interest: The authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Alqahtani and Alothaim. This is an open-access a rticle distributed\nunder the terms of the Creative Commons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is permitted, p rovided the original\nauthor(s) and the copyright owner(s) are credited and that th e original publication\nin this journal is cited, in accordance with accepted academ ic practice. No use,\ndistribution or reproduction is permitted which does not co mply with these terms.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 11 March 2022 | Volume 5 | Article 843038"
}