{
  "title": "A Survey of Backpropagation-free Training For LLMS",
  "url": "https://openalex.org/W4393307007",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5094276273",
      "name": "Hanzi Mei",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2718158942",
      "name": "Dongqi Cai",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2138188336",
      "name": "Yaozong Wu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2300114384",
      "name": "Shangguang Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2101727902",
      "name": "Mengwei Xu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320859095",
    "https://openalex.org/W1853291804",
    "https://openalex.org/W4387356156",
    "https://openalex.org/W4318751549",
    "https://openalex.org/W4206740629",
    "https://openalex.org/W4380558639",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W1547224907",
    "https://openalex.org/W4320341763",
    "https://openalex.org/W2999185414",
    "https://openalex.org/W2966459288",
    "https://openalex.org/W4378770452",
    "https://openalex.org/W4366850528",
    "https://openalex.org/W3146366485",
    "https://openalex.org/W4385325027",
    "https://openalex.org/W4390974506",
    "https://openalex.org/W4309877123",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W3121562065",
    "https://openalex.org/W4304192684",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W2072690700",
    "https://openalex.org/W4306703248",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4366850636",
    "https://openalex.org/W3169356297",
    "https://openalex.org/W2102013737",
    "https://openalex.org/W4386270173",
    "https://openalex.org/W4387723845",
    "https://openalex.org/W4390215533",
    "https://openalex.org/W4320560108",
    "https://openalex.org/W3035548285",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W4295992767",
    "https://openalex.org/W4384920109",
    "https://openalex.org/W2064558818",
    "https://openalex.org/W4386272856",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4225893763",
    "https://openalex.org/W3174394143",
    "https://openalex.org/W4362514873",
    "https://openalex.org/W4376653500",
    "https://openalex.org/W4221142421",
    "https://openalex.org/W4386590109",
    "https://openalex.org/W4221147883",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4385573411",
    "https://openalex.org/W2959472552",
    "https://openalex.org/W4313304716",
    "https://openalex.org/W1548898283",
    "https://openalex.org/W4313598299",
    "https://openalex.org/W4387427534",
    "https://openalex.org/W4310001898",
    "https://openalex.org/W4377865039",
    "https://openalex.org/W4388937805",
    "https://openalex.org/W4385573069",
    "https://openalex.org/W4389708319",
    "https://openalex.org/W4392011615",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W2962727772",
    "https://openalex.org/W4390962670",
    "https://openalex.org/W2998249317",
    "https://openalex.org/W2596367596",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4388312258"
  ],
  "abstract": "Large language models (LLMs) have achieved remarkable performance in various downstream tasks.However, training LLMs is computationally expensive and requires a large amount of memory.To address this issue, backpropagation-free (BP-free) training has been proposed as a promising approach to reduce the computational and memory costs of training LLMs.In this survey, we provide a comprehensive overview of BP-free training for LLMs.We first outline three mainstream BP-free training methods.Subsequently, we introduce their optimizations for LLMs.The goal of this survey is to provide a comprehensive understanding of BP-free training for LLMs and to inspire future research in this area.",
  "full_text": null,
  "topic": "Backpropagation",
  "concepts": [
    {
      "name": "Backpropagation",
      "score": 0.6756308078765869
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6667176485061646
    },
    {
      "name": "Computer science",
      "score": 0.3000635504722595
    },
    {
      "name": "Artificial intelligence",
      "score": 0.18077558279037476
    },
    {
      "name": "Artificial neural network",
      "score": 0.17992040514945984
    },
    {
      "name": "Geography",
      "score": 0.16037848591804504
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    }
  ],
  "cited_by": 3
}