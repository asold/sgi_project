{
    "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
    "url": "https://openalex.org/W3005700362",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4226981990",
            "name": "Dodge, Jesse",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221397823",
            "name": "Ilharco, Gabriel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221786251",
            "name": "Schwartz, Roy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2273117395",
            "name": "Farhadi, Ali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4200857851",
            "name": "Hajishirzi, Hannaneh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287821391",
            "name": "Smith, Noah",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2525127255",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W3006647218",
        "https://openalex.org/W2970283086",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2963815651",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2962685744",
        "https://openalex.org/W2963504252",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2109553965",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2106411961",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963334428",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3036267641",
        "https://openalex.org/W1533179050",
        "https://openalex.org/W2085989833"
    ],
    "abstract": "Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.",
    "full_text": "Fine-Tuning Pretrained Language Models:\nWeight Initializations, Data Orders, and Early Stopping\nJesse Dodge1 2 Gabriel Ilharco3 Roy Schwartz2 3 Ali Farhadi2 3 4 Hannaneh Hajishirzi2 3 Noah Smith2 3\nAbstract\nFine-tuning pretrained contextual word embed-\nding models to supervised downstream tasks has\nbecome commonplace in natural language pro-\ncessing. This process, however, is often brittle:\neven with the same hyperparameter values, dis-\ntinct random seeds can lead to substantially differ-\nent results. To better understand this phenomenon,\nwe experiment with four datasets from the GLUE\nbenchmark, ﬁne-tuning BERT hundreds of times\non each while varying only the random seeds. We\nﬁnd substantial performance increases compared\nto previously reported results, and we quantify\nhow the performance of the best-found model\nvaries as a function of the number of ﬁne-tuning\ntrials. Further, we examine two factors inﬂuenced\nby the choice of random seed: weight initializa-\ntion and training data order. We ﬁnd that both\ncontribute comparably to the variance of out-of-\nsample performance, and that some weight ini-\ntializations perform well across all tasks explored.\nOn small datasets, we observe that many ﬁne-\ntuning trials diverge part of the way through train-\ning, and we offer best practices for practitioners\nto stop training less promising runs early. We\npublicly release all of our experimental data, in-\ncluding training and validation scores for 2,100\ntrials, to encourage further analysis of training\ndynamics during ﬁne-tuning.\n1. Introduction\nThe advent of large-scale self-supervised pretraining has\ncontributed greatly to progress in natural language process-\ning (Devlin et al., 2019; Liu et al., 2019; Radford et al.,\n2019). In particular, BERT (Devlin et al., 2019) advanced\n1Language Technologies Institute, School of Computer Sci-\nence, Carnegie Mellon University 2Allen Institute for Artiﬁcial\nIntelligence 3Paul G. Allen School of Computer Science and Engi-\nneering, University of Washington 4XNOR.AI. Correspondence\nto: Jesse Dodge <jessed@cs.cmu.edu>.\nMRPC RTE CoLA SST\nBERT (Phang et al., 2018) 90.7 70.0 62.1 92.5\nBERT (Liu et al., 2019) 88.0 70.4 60.6 93.2\nBERT (ours) 91.4 77.3 67.6 95.1\nSTILTs (Phang et al., 2018) 90.9 83.4 62.1 93.2\nXLNet (Yang et al., 2019) 89.2 83.8 63.6 95.6\nRoBERTa (Liu et al., 2019) 90.9 86.6 68.0 96.4\nALBERT (Lan et al., 2019) 90.9 89.2 71.4 96.9\nTable 1.Fine-tuning BERT multiple times while varying only ran-\ndom seeds leads to substantial improvements over previously pub-\nlished validation results with the same model and experimental\nsetup (top rows), on four tasks from the GLUE benchmark. On\nsome tasks, BERT even becomes competitive with more modern\nmodels (bottom rows). Best results with standard BERT ﬁne-\ntuning regime are indicated in bold, best overall results are under-\nscored.\naccuracy on natural language understanding tasks in pop-\nular NLP benchmarks such as GLUE (Wang et al., 2018)\nand SuperGLUE (Wang et al., 2019), and variants of this\nmodel have since seen adoption in ever-wider applications\n(Schwartz et al., 2019; Lu et al., 2019). Typically, these\nmodels are ﬁrst pretrained on large corpora, then ﬁne-tuned\non downstream tasks by reusing the model’s parameters as\na starting point, while adding one task-speciﬁc layer trained\nfrom scratch. Despite its simplicity and ubiquity in modern\nNLP, this process has been shown to be brittle (Devlin et al.,\n2019; Phang et al., 2018; Zhu et al., 2019; Raffe et al., 2019),\nwhere ﬁne-tuning performance can vary substantially across\ndifferent training episodes, even with ﬁxed hyperparameter\nvalues.\nIn this work, we investigate this variation by conducting a\nseries of ﬁne-tuning experiments on four tasks in the GLUE\nbenchmark (Wang et al., 2018). Changing only training\ndata order and the weight initialization of the ﬁne-tuning\nlayer—which contains only 0.0006% of the total number of\nparameters in the model—we ﬁnd substantial variance in\nperformance across trials.\nWe explore how validation performance of the best found\nmodel varies with the number of ﬁne-tuning experiments,\nﬁnding that, even after hundreds of trials, performance has\nnot fully converged. With the best found performance across\narXiv:2002.06305v1  [cs.CL]  15 Feb 2020\nWeight Initializations, Data Orders, and Early Stopping\nall the conducted experiments of ﬁne-tuning BERT, we ob-\nserve substantial improvements compared to previous pub-\nlished work with the same model (Table 1). On MRPC\n(Dolan & Brockett, 2005), BERT performs better than more\nrecent models such as XLNet (Yang et al., 2019), RoBERTa\n(Liu et al., 2019) and ALBERT (Lan et al., 2019). More-\nover, on RTE (Wang et al., 2018) and CoLA (Warstadt et al.,\n2019), we observe a 7% (absolute) improvement over previ-\nous results with the same model. It is worth highlighting that\nin our experiments only random seeds are changed—never\nthe ﬁne-tuning regime, hyperparameter values, or pretrained\nweights. These results demonstrate how model comparisons\nthat only take into account reported performance in a bench-\nmark can be misleading, and serve as a reminder of the value\nof more rigorous reporting practices (Dodge et al., 2019).\nTo better understand the high variance across ﬁne-tuning\nepisodes, we separate two factors that affect it: the weight\ninitialization for the task-speciﬁc layer; and the training\ndata order resulting from random shufﬂing. The contribu-\ntions of each of these have previously been conﬂated or\noverlooked, even by works that recognize the importance of\nmultiple trials or random initialization (Phang et al., 2018).\nBy conducting experiments with multiple combinations of\nrandom seeds that control each of these factors, we quantify\ntheir contribution to the variance across runs. Moreover,\nwe present evidence that some seeds are consistently better\nthan others in a given dataset for both weight initializations\nand data orders. Surprisingly, we ﬁnd that some weight\ninitializations perform well across all studied tasks.\nBy frequently evaluating the models through training, we\nempirically observe that worse performing models can often\nbe distinguished from better ones early in training, moti-\nvating investigations of early stopping strategies. We show\nthat a simple early stopping algorithm (described in Section\n5) is an effective strategy for reducing the computational\nresources needed to reach a given validation performance\nand include practical recommendations for a wide range of\ncomputational budgets.\nTo encourage further research in analyzing training dynam-\nics during ﬁne-tuning, we publicly release all of our experi-\nmental data. This includes, for each of the 2,100 ﬁne-tuning\nepisodes, the training loss at every weight update, and vali-\ndation performance on at least 30 points in training.\nOur main contributions are:\n• We show that running multiple trials with different\nrandom seeds can lead to substantial gains in perfor-\nmance on four datasets from the GLUE benchmark.\nFurther, we present how the performance of the best-\nfound model changes as a function of the number of\ntrials.\n• We investigate weight initialization and training data\norder as two sources of randomness in ﬁne-tuning by\nvarying random seeds that control them, ﬁnding that\n1) they are comparable as sources of variance in per-\nformance; 2) in a given dataset, some data orders and\nweight initializations are consistently better than oth-\ners; and 3) some weight initializations perform well\nacross multiple different tasks.\n• We demonstrate how a simple early stopping algorithm\ncan effectively be used to improve expected perfor-\nmance using a given computational budget.\n• We release all of our collected data of 2,100 ﬁne-\ntuning episodes on four popular datasets from the\nGLUE benchmark to incentivize further analyses of\nﬁne-tuning dynamics.\n2. Methodology\nOur experiments consist of ﬁne-tuning pretrained BERT to\nfour downstream tasks from the GLUE benchmark. For\na given task, we experiment multiple times with the same\nmodel using the same hyperparameter values, while modify-\ning only the random seeds that control weight initialization\n(WI) of the ﬁnal classiﬁcation layer and training data order\n(DO). In this section we describe in detail the datasets and\nsettings for our experiments.\n2.1. Data\nWe examine four datasets from the GLUE benchmark, de-\nscribed below and summarized in Table 2. The data is\npublicly available and can be download from the repository\njiant.1 Three of our datasets are relatively small (MRPC,\nRTE, and CoLA), and one relatively large (SST). Since all\ndatasets are framed as binary classiﬁcation, the model struc-\nture for each is the same, as only a single classiﬁcation layer\nwith two output units is appended to the pretrained BERT.\nMicrosoft Research Paraphrase Corpus (MRPC;\nDolan & Brockett, 2005) contains pairs of sentences,\nlabeled as either nearly semantically equivalent, or not. The\ndataset is evaluated using the average of F1 and accuracy.\nRecognizing Textual Entailment (RTE; Wang et al.,\n2018) combines data from a series of datasets (Dagan et al.,\n2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Ben-\ntivogli et al., 2009). Each example in RTE is a pair of\nsentences, and the task is to predict whether the ﬁrst (the\npremise) entails the second (the hypothesis).\nCorpus of Linguistic Acceptability (CoLA; Warstadt\net al., 2019) is comprised of English sentences labeled as\n1https://github.com/nyu-mll/jiant\nWeight Initializations, Data Orders, and Early Stopping\nMRPC RTE CoLA SST\nevaluation metric Acc./ F1 Acc. MCC Acc.\nmajority baseline 0.75 0.53 0.00 0.51\n# training samples 3.7k 2.5k 8.6k 67k\n# validation samples 409 277 1,043 873\nTable 2.The datasets used in this work, which comprise four out\nof nine of the tasks in the GLUE benchmark (Wang et al., 2018).\neither grammatical or ungrammatical. Models are evaluated\non Matthews correlation (MCC; Matthews, 1975), which\nranges between –1 and 1, with random guessing being 0.\nStanford Sentiment Treebank (SST; Socher et al., 2013)\nconsists of sentences annotated as expressingpositive or neg-\native sentiment (we use the binary version of the annotation),\ncollected from movie reviews.\n2.2. Fine-tuning\nFollowing standard practice, we ﬁne-tune BERT (BERT-\nlarge, uncased) for three epochs (Phang et al., 2018; Devlin\net al., 2019). We ﬁne-tune the entire model (340 million\nparameters), of which the vast majority start as pretrained\nweights and the ﬁnal layer (2048 parameters) is randomly\ninitialized. The weights in the ﬁnal classiﬁcation layer\nare initialized using the standard approach used when ﬁne-\ntuning pretrained transformers like BERT, RoBERTa, and\nALBERT (Devlin et al., 2019; Liu et al., 2019; Lan et al.,\n2019): sampling from a normal distribution with mean 0\nand standard deviation 0.02. All experiments were run on\nP100 GPUs with 16 GB of RAM. We train with a batch\nsize of 16, a learning rate of 0.00002, and dropout of 0.1;\nthe open source implementation, pretrained weights, and\nfull hyperparameter values and experimental details can be\nfound in the HuggingFace transformer library (Wolf et al.,\n2019).2\nEach experiment is repeated N2 times, with all possible\ncombinations of N distinct random seeds for WI and N for\nDO.3 For the datasets MRPC, RTE, and CoLA, we run a\ntotal of 625 experiments each (N=25). For the larger SST,\nwe run 225 experiments (N=15).\n3. The large impact of random seeds\nOur large set of ﬁne-tuning experiments evidences the siz-\nable variance in performance across trials varying only ran-\ndom seeds. This effect is especially pronounced on the\nsmaller datasets; the validation performance of the best-\n2https://github.com/huggingface/\ntransformers\n3Although any random numbers would have sufﬁced, for com-\npleteness: we use the numbers {1, . . . , N} as seeds.\nfound model from multiple experiments is substantially\nhigher than the expected performance of a single trial. In\nparticular, in Table 1 we report the performance of the best\nmodel from all conducted experiments, which represents\nsubstantial gains compared to previous work that uses the\nsame model and optimization procedure. On some datasets,\nwe observe numbers competitive with more recent mod-\nels which have improved pretraining regimes (Phang et al.,\n2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019);\ncompared to BERT, these approaches pretrain on more data,\nand some utilize more sophisticated modeling or optimiza-\ntion strategies. We leave it to future work to analyze the\nvariance from random seeds on these other models, and note\nthat running analogous experiments would likely also lead\nto performance improvements.\nIn light of these overall gains and the computational bur-\nden of running a large number of experiments, we explore\nhow the number of trials inﬂuences the expected validation\nperformance.\n3.1. Expected validation performance\nTo quantify the improvement found from running more\nexperiments, we turn to expected validation performance as\nintroduced by Dodge et al. (2019). The standard machine\nlearning experimental setup involves a practitioner training\nx models, evaluating each of them on validation data, then\ntaking the model which has the best validation performance\nand evaluating it on test data. Intuitively, as the number\nof trained models x increases, the best of those x models\nwill improve; expected validation performance calculates\nthe expected value of the best validation performance as a\nfunction of x.4\nWe plot expected validation curves for each dataset in Fig-\nure 1 with (plus or minus) the standard deviation shaded.5\nThe leftmost point on each of these curves (x = 1) shows\nthe expected performance for a budget of a single training\nrun. For all datasets, Figure 1 shows, unsurprisingly, that\nexpected validation performance increases as more compu-\ntational resources are used. This rising trend continues even\nup to our largest budget, suggesting even larger budgets\ncould lead to improvements. On the three smaller datasets\n(MRPC, RTE, and CoLA) there is signiﬁcant variance at\nsmaller budgets, which indicates that individual runs can\nhave widely varying performance.\nIn the most common setup for ﬁne-tuning on these datasets,\nmodels are evaluated on the validation data after each epoch,\nor once after training for multiple epochs (Phang et al., 2018;\nDevlin et al., 2019). In Figure 1 we show expected perfor-\nmance as we vary the number of evaluations on validation\n4A full derivation can be found in Dodge et al. (2019).\n5We shade between the observed minimum and maximum.\nWeight Initializations, Data Orders, and Early Stopping\n100 101 102\nRandom seed assignments\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92Expected val.  Acc./F1\nMRPC\n100 101 102\nRandom seed assignments\n0.60\n0.65\n0.70\n0.75Expected val.  Accuracy\nRTE\n100 101 102\nRandom seed assignments\n0.50\n0.55\n0.60\n0.65Expected val.  MCC\nCoLA\n100 101 102\nRandom seed assignments\n0.930\n0.935\n0.940\n0.945\n0.950Expected val.  Accuracy\nSST\neval 10x per epoch\neval 1x per epoch\neval 1x in training\nFigure 1.Expected validation performance (Dodge et al., 2019), plus and minus one standard deviation, as the number of experiments\nincreases. The x-axis represents the budget (e.g., x = 10indicates a budget large enough to train 10 models). The y-axis is the expected\nperformance of the best of the x models trained. Each plot shows three evaluation scenarios: in the ﬁrst, the model is frequently evaluated\non the validation set during training (blue); in the second, at the end of each epoch (orange); and in the third, only at the end training\n(green). As we increase the number of evaluations per run we see higher expected performance and smaller variances. Further, more\nfrequently evaluating the model on validation data leads to higher expected validation values.\ndata during training (all models trained for three epochs):\nonce after training (green), after each of the three epochs\n(orange), and frequently throughout training (ten times per\nepoch, blue).6 Considering the beneﬁts of more frequent\nevaluations as shown in Figure 1, we thus recommend this\npractice in similar scenarios.\n4. Weight initialization and data order\nTo better understand the high variance in performance across\ntrials, we analyze two source of randomness: the weight\ninitialization of the ﬁnal classiﬁcation layer and the order the\ntraining data is presented to the model. While previous work\non ﬁne-tuning pretrained contextual representation models\n(Devlin et al., 2019; Phang et al., 2018) has generally used a\nsingle random seed to control these two factors, we analyze\nthem separately.\nOur experiments are conducted with every combination of\na set of weight initialization seeds (WI) and a set of data\norder (DO) seeds that control these factors. One data order\ncan be viewed as one sample from the set of permutations\nof the training data. Similarly, one weight initialization\ncan be viewed as a speciﬁc set of samples from the normal\ndistribution from which we draw them.\nAn overview of the collected data is presented in Figure\n2, where each colored cell represents the validation per-\nformance for a single experiment. In the plots, each row\nrepresents a single weight initialization and each column\nrepresents a single data order. We sort the rows and columns\nby their averages; the top row contains experiments with the\n6Compared to training, evaluation is typically cheap, since the\nvalidation set is smaller than the training set and evaluation requires\nonly a forward pass. Moreover, evaluating on the validation data\ncan be done in parallel to training, and thus does not necessarily\nslow down training.\nMRPC RTE CoLA SST\nAgg. over WI .058 .066 .090 .0028\nAgg. over DO .059 .067 .095 .0024\nTotal .061 .069 .101 .0028\nTable 3.Expected (average) standard deviation in validation per-\nformance across runs. The expected standard deviation of given\nWI and DO random seeds are close in magnitude, and only slightly\nbelow the overall standard deviation.\nWI with the highest average performance, and the rightmost\ncolumn contains experiments with the DO with the highest\naverage performance.7\nFor MRPC, RTE, and CoLA, a fraction of the trained models\ndiverge, yielding performance close to that of predicting the\nmost frequent label (see Table 2). This partially explains the\nlarge variance found in the expected validation curves for\nthose three datasets in Figure 1.\n4.1. Decoupling\nFrom Figure 2, it is clear that different random seed com-\nbinations can lead to substantially different validation per-\nformance. In this section, we investigate the sources of this\nvariance, decoupling the distribution of performance based\non each of the factors that control randomness.\nFor each dataset, we compute for each WI and each DO\nseed the standard deviation in validation performance across\nall trials with that seed. We then compute the expected (av-\nerage) standard deviation, aggregated under all WI or all\nDO seeds, which are shown in Table 3; we show the dis-\ntribution of standard deviations in the appendix. Although\ntheir magnitudes vary signiﬁcantly between the datasets,\n7Each cell represents an independent sample, so the rows and\ncolumns can be reordered.\nWeight Initializations, Data Orders, and Early Stopping\nData order random seedsWeight initialization\nrandom seeds\nMRPC - Acc./F1\n0.75\n0.78\n0.80\n0.83\n0.85\n0.88\n0.90\nData order random seeds\nRTE - Accuracy\n0.55\n0.60\n0.65\n0.70\n0.75\nData order random seeds\nCoLA - MCC\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\nData order random seeds\nSST - Accuracy\n0.936\n0.938\n0.940\n0.942\n0.944\n0.946\n0.948\n0.950\nFigure 2.A visualization of validation performance for all experiments, where each colored cell represents the performance of a training\nrun with a speciﬁc WI and DO seed. Rows and columns are sorted by their average, such that the best WI seed corresponds to the top row\nof each plot, and the best DO seed correspond to the right-most column. Especially on smaller datasets a large variance in performance\nis observed across different seed combinations, and on MRPC and RTE models frequently diverge, performing close to the majority\nbaselines (listed in Table 2).\n0.75 0.80 0.85 0.90\nAcc./F1\n0\n5\n10\n15\n20\n25Kernel Density Estimation\nMRPC\n0.5 0.6 0.7 0.8\nAccuracy\n0\n2\n4\n6\nRTE\n0.0 0.2 0.4 0.6 0.8\nMCC\n0\n2\n4\n6\n8\n10\nCoLA\n0.93 0.94 0.95\nAccuracy\n0\n20\n40\n60\n80\n100\n120\nSST\nbest WI\nbest DO\nworst WI\nworst DO\nFigure 3.Some seeds are better then others. Plots show the kernel density estimation of the distribution of validation performance for\nbest and worst WI and DO seeds. Curves for DO seeds are shown in dashed lines and for WI in solid lines. MRPC and RTE exhibit\npronounced bimodal shapes, where one of the modes represents divergence; models trained with the worst WI and DO are more likely to\ndiverge than learn to predict better than random guessing. Compared to the best seeds, the worst seeds are conspicuously more densely\npopulated in the lower performing regions, for all datasets.\nthe expected standard deviation from the WI and DO seeds\nis comparable, and are slightly below the overall standard\ndeviation inside a given task.\n4.2. Some random seeds are better than others\nTo investigate whether some WI or DO seeds are better\nthan their counterparts, Figure 3 plots the random seeds\nwith the best and worst average performance. The best and\nworst seeds exhibit quite different behavior: compared to the\nbest, the worst seeds have an appreciably higher density on\nlower performance ranges, indicating that they are generally\ninferior. On MRPC, RTE, and CoLA the performance of\nthe best and worst WIs are more dissimilar than the best and\nworst DOs, while on SST the opposite is true. This could be\nrelated to the size of the data; MRPC, RTE, and CoLA are\nsmaller datasets, whereas SST is larger, so SST has more\ndata to order and more weight updates to move away from\nthe initialization.\nMRPC RTE CoLA SST\nWI 2.0×10−6 2.8×10−4 7.0×10−3 3.3×10−2\nDO 8.3×10−3 3.2×10−3 1.1×10−2 1.3×10−5\nTable 4. p-values from ANOV A indicate that there is evidence to\nreject the null hypothesis that the performance of the best and worst\nWIs and DOs have distributions with the same means (p <0.05).\nUsing ANOV A (Fisher, 1935) to test for statistical signif-\nicance, we examine whether the performance of the best\nand worst DOs and WIs have distributions with different\nmeans. The results are shown in Table 4. For all datasets,\nwe ﬁnd the best and worst DOs and WIs are signiﬁcantly\ndifferent in their expected performance ( p < 0.05). We\ninclude a discussion of the assumptions behind ANOV A in\nthe appendix.\nWeight Initializations, Data Orders, and Early Stopping\n4.3. Globally good initializations\nA natural question that follows is whether some random\nseeds are good across datasets. While the data order is\ndataset speciﬁc, the same weight initialization can be ap-\nplied to multiple classiﬁers trained with different datasets:\nsince all tasks studied are binary classiﬁcation, models for\nall datasets share the same architecture, including the classi-\nﬁcation layer that is being randomly initialized and learned.\nWe compare the different weight initializations across\ndatasets. We ﬁnd that some initializations perform con-\nsistently well. For instance, WI seed 12 has the best perfor-\nmance on CoLA and RTE, the second best on MRPC, and\nthird best on SST. This suggests that, perhaps surprisingly,\nsome weight initializations perform well across tasks.\nStudying the properties of good weight initializations and\ndata orders is an important question that could lead to sig-\nniﬁcant empirical gains and enhanced understanding of the\nﬁne-tuning process. We defer this question to future work,\nand release the results of our 2,100 ﬁne-tuning experiments\nto facilitate further study of this question by the community.\n5. Early stopping\nOur analysis so far indicates a high variance in the ﬁne-\ntuning performance of BERT when using different random\nseeds, where some models fail to converge. 8 In this sec-\ntion we show that better performance can be achieved with\nthe same computational resources by using early stopping\nalgorithms that stop the least promising trials early in train-\ning. We also include recommendations for practitioners for\nsetting up experiments meeting a variety of computational\nbudgets.\nEarly discovery of failed experimentsFigure 4 shows\nthat performance divergence can often be recognized early\nin training. These plots show the performance values of\n20 randomly chosen models at different times across train-\ning. In many of the curves, continuing training of lower\nperforming models all the way through can be a waste of\ncomputation. In turn, this suggests the potential of early\nstopping least promising trials as a viable means of saving\ncomputation without large decreases in expected perfor-\nmance. For instance, after training halfway through the\nﬁrst epoch on CoLA the models which diverged could be\nstopped.\nWe further examine the correlation of validation perfor-\nmances at different points throughout training, shown in\nFigure 5. One point in one of these plots represents the\n8This was also observed by Phang et al. (2018), who showed\nthat their proposed STILTs approach reduced the number of di-\nverging models.\nSpearman’s rank correlation between performance at iter-\nation i and iteration j across trials. High rank correlation\nmeans that the ranking of the models is similar between the\ntwo evaluation points, and suggests we can stop the worst\nperforming models early, as they would likely continue to\nunderperform.9 On MRPC, RTE and CoLA, there exists\na high correlation between the models’ performance early\non (part way through the ﬁrst epoch) and their ﬁnal perfor-\nmance. On the larger SST dataset, we see high correlation\nbetween the performance after training for two epochs and\nthe ﬁnal performance.\nEarly stopping Considering the evidence from the train-\ning curves and correlation plots, we analyze a simple al-\ngorithm for early stopping. Our algorithm is inspired by\nexisting approaches to making hyperparameter search more\nefﬁcient by stopping some of the least promising experi-\nments early (Jamieson & Talwalkar, 2016; Li et al., 2018).10\nHere we apply an early stopping algorithm to select the\nbest performing random seed. 11 The algorithm has three\nparameters: t, f, and p. We start by training t trials, and\npartially through training (f, a fraction of the total number\nof epochs) evaluate all of them and only continue to fully\ntrain the p most promising ones, while discarding the rest.\nThis algorithm takes a total of(tf +p(1 −f))s steps, where\ns is the number of steps to fully train a model.12\nStart many, stop early, continue someAs shown earlier,\nthe computational budget of running this algorithm can be\ncomputed directly from an assignment to the parameters t,\nf, and p. Note that there are different ways to assign these\nparameters that lead to the same computational budget, and\nthose can lead to signiﬁcantly distinct performance in expec-\ntation; to estimate the performance for each conﬁguration\nwe simulate this algorithm by sampling 50,000 times from\nfrom our full set of experiments. In Figure 6 we show the\nbest observed assignment of these parameters for budgets\nbetween 3 and 90 total epochs of training, or the equivalent\nof 1 to 30 complete training trials. There are some surpris-\ningly consistent trends across datasets and budgets – the\nnumber of trials started should be signiﬁcantly higher than\nthe number trained fully, and the number of trials to train\nfully should be around x/2. On three out of four datasets,\nstopping least promising trials after 20–30% of training (less\nthan one epoch) yielded the best results—and on the fourth\n9Similar plots with Pearson correlation can be found in the\nappendix.\n10“Early stopping” can also relate to stopping a single training\nrun if the loss hasn’t decreased for a given number of epochs. Here\nwe refer to the notion of stopping a subset of multiple trials.\n11Our approach does not distinguish between DO and WI. While\ninitial results suggest that this distinction could inspire more so-\nphisticated early-stopping criteria, we defer this to future work.\n12In our experiments, s = 3epochs.\nWeight Initializations, Data Orders, and Early Stopping\n0 1 2 3\nEpochs\n0.2\n0.4\n0.6\n0.8Validation performance\nMRPC\n0 1 2 3\nEpochs\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nRTE\n0 1 2 3\nEpochs\n0.0\n0.2\n0.4\n0.6\nCoLA\n0 1 2 3\nEpochs\n0.5\n0.6\n0.7\n0.8\n0.9\nSST\nFigure 4.Some promising seeds can be distinguished early in training. The plots show training curves for 20 random WI and DO\ncombinations for each dataset. Models are evaluated every 10th of an epoch (except SST, which was evaluated every 100 steps, equivalent\nto 42 times per epoch). For the smaller datasets, training is unstable, and a non-negligible portion of the models yields poor performance,\nwhich can be identiﬁed early on.\n0 1 2 3\n0\n1\n2\n3\nMRPC\n0 1 2 3\n0\n1\n2\n3\nRTE\n0 1 2 3\n0\n1\n2\n3\nCoLA\n0 1 2 3\n0\n1\n2\n3\nSST\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 5.Performance early in training is highly correlated with performance late in training. Each ﬁgure shows the Spearman’s rank\ncorrelation between the validation performance at different points in training; the axes represent epochs. A point at coordinates i and j in\nthe plots indicates the correlation between the best found performances after i and after j evaluations. Note that the plots are symmetric.\ndataset this is still a strong strategy.\nEarly stopping works We compare this algorithm with\nour baseline of running multiple experiments all the way\nthrough training, without any early stopping (f=1, t=p) and\nusing the same amount of computation. Speciﬁcally, for a\ngiven computational budget equivalent to fully training t\nmodels, we measure improvement as the relative error reduc-\ntion from using early stopping with the best found settings\nfor that computational budget. Figure 7 shows the relative\nerror reduction for each dataset as the computational budget\nvaries, where we observe small but reasonably consistent\nimprovements on all tasks.\n6. Related work\nMost work on hyperparameter optimization tunes a number\nof impactful hyperparameters, such as the learning rate, the\nwidth of the layers in the model, and the strength of the\nregularization (Li et al., 2018; Bergstra et al., 2011). For\nmodern machine learning models such tuning has proven\nto have a large impact on the performance; in this work we\nonly examine two oft-overlooked choices that can be cast as\nhyperparameters and still ﬁnd room for optimization.\nMelis et al. (2018) heavily tuned the hyperpamareters of an\nLSTM language model, for some experiments running 1,500\nrounds of Bayesian optimization (thus, training 1,500 mod-\nels). They showed that an LSTM, when given such a large\nbudget for hyperparameter tuning, can outperform more\ncomplicated neural models. While such work informs the\ncommunity about the best performance found after expend-\ning very large budgets, it is difﬁcult for future researchers\nto build on this without some measure of how the perfor-\nmance changes as a function of computational budget. Our\nwork similarly presents the best-found performance using a\nlarge budget (Table 1), but also includes estimates of how\nperformance changes as a function of budget (Figure 1).\nA line of research has addressed the distribution from which\ninitializations are drawn. The Xavier initialization (Glorot\n& Bengio, 2010) and Kaiming initialization (He et al., 2015)\ninitialize weights by sampling from a uniform distribution\nor normal distribution with variance scaled so as to preserve\ngradient magnitudes through backpropagation. Similarly,\nWeight Initializations, Data Orders, and Early Stopping\n0\n 5\n 10\n 15\n 20\n 25\n 30\n0%\n14%\n29%\n43%\n57%\n71%\n86%\n100%\n0\n 5\n 10\n 15\n 20\n 25\n 30\n0%\n20%\n40%\n60%\n80%\n100%\n0\n 5\n 10\n 15\n 20\n 25\n 30\n0%\n14%\n29%\n43%\n57%\n71%\n86%\n100%\n0\n 5\n 10\n 15\n 20\n 25\n 30\n0%\n17%\n33%\n50%\n67%\n83%\n100%\n0\n10\n20\n30\n40\n50\n60\n70\nMRPC\n0\n20\n40\n60\n80\n100\nRTE\n0\n20\n40\n60\n80\n100\n120\n140\nCoLA\n0\n10\n20\n30\n40\n50\n60\nSST\nExperiments started\nExperiments fully trained\nFraction of the\ntraining budget\nExperiments started\nExperiments fully trained\nFraction of the\ntraining budget\nExperiments started\nExperiments fully trained\nFraction of the\ntraining budget\nExperiments started\nExperiments fully trained\nFraction of the\ntraining budget\nExp. started\nExp. trained fully\nFraction of training budget\nComputational budget sufficient to fully train X models\nFigure 6.Best observed early stopping parameters on each dataset.\nFor a given budget large enough to fully train x models (each\ntrained for 3 epochs), this plot shows the optimal parameters for\nearly stopping. For instance, in MRPC with a budget large enough\nfor 20 trials, the best observed performance came by starting 41\ntrials (blue), then continuing only the 11 most promising trials\n(orange) after 30% of training (green).\n0 5 10 15 20 25 30\nComputational budget\n(number of trials)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12Reative error reduction\nMRPC\nSST\nCoLA\nRTE\nFigure 7.Relative error reduction from the early stopping approach\nin Figure 6, compared to the baseline of training x models on the\nfull training budget. Performance on RTE and SST is measured\nusing accuracy, on MRPC it is the average of accuracy and F1, and\non CoLA it is MCC. “Error” here refers to one-minus-performance\nfor each of these datasets. As the budget increases, the absolute\nperformance on all four datasets increases, and the absolute im-\nprovement from early stopping is fairly consistent.\northogonal initializations (Saxe et al., 2014) aim to prevent\nexploding or vanishing gradients. In our work, we instead\nexamine how different samples from an initialization distri-\nbution behave, and we hope future work which introduces\nnew initialization schemes will provide a similar analysis.\nActive learning techniques, which choose a data order using\na criterion such as the model’s uncertainty (Lewis & Gale,\n1994), have a rich history. Recently, it has even been shown\nthat that training on mini-batches which are diverse in terms\nof data or labels (Zhang et al., 2017) can be more sample\nefﬁcient. The tools we present here can be used to evaluate\ndifferent seeds for a stochastic active learning algorithm, or\nto compare different active learning algorithms.\n7. Conclusion\nIn this work we study the impact of random seeds on ﬁne-\ntuning contextual embedding models, the currently domi-\nnant paradigm in NLP. We conduct a large set of experiments\non four datasets from the GLUE benchmark and observe\nsigniﬁcant variance across these trials. Overall, these experi-\nments lead to substantial performance gains on all tasks. By\nobserving how the expected performance changes as we al-\nlocate more computational resources, we expect that further\ngains would come from an even larger set of trials. More-\nover, we examine the two sources of variance across trials,\nweight initialization and training data order, ﬁnding that in\nexpectation, they contribute comparably to the variance in\nperformance. Perhaps surprisingly, we ﬁnd that some data\norders and initializations are better than others, and the lat-\nter can even be observed even across tasks. A simple early\nWeight Initializations, Data Orders, and Early Stopping\nstopping strategy along with practical recommendations is\nincluded to alleviate the computational costs of running\nmultiple trials. All of our experimental data containing\nthousands of ﬁne-tuning episodes is publicly released.\nReferences\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo,\nD., Magnini, B., and Szpektor, I. The second pascal\nrecognising textual entailment challenge. In Proc. of the\nII PASCAL challenge, 2006.\nBentivogli, L., Clark, P., Dagan, I., and Giampiccolo, D.\nThe ﬁfth pascal recognizing textual entailment challenge.\nIn TAC, 2009.\nBergstra, J., Bardenet, R., Bengio, Y ., and Kegl, B. Al-\ngorithms for hyper-parameter optimization. In Proc. of\nNeurIPS, 2011.\nDagan, I., Glickman, O., and Magnini, B. The pascal recog-\nnising textual entailment challenge. In Machine Learning\nChallenges Workshop, 2005.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proc. of the ACL, 2019.\nDodge, J., Gururangan, S., Card, D., Schwartz, R., and\nSmith, N. A. Show your work: Improved reporting of\nexperimental results. In Proc. of EMNLP, 2019.\nDolan, B. and Brockett, C. Automatically constructing a\ncorpus of sentential paraphrases. In Proc. of IWP, 2005.\nFisher, R. A. Statistical methods for research workers.\nOliver & Boyd (Edinburgh), 1935.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. The\nthird pascal recognizing textual entailment challenge. In\nProc. of the ACL-PASCAL workshop on textual entailment\nand paraphrasing, 2007.\nGlorot, X. and Bengio, Y . Understanding the difﬁculty of\ntraining deep feedforward neural networks. In Proc. of\nAISTATS, 2010.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectiﬁers: Surpassing human-level performance on\nimagenet classiﬁcation. In Proc. of ICCV, 2015.\nJamieson, K. and Talwalkar, A. Non-stochastic best arm\nidentiﬁcation and hyperparameter optimization. In Proc.\nof AISTATS, 2016.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv:1909.11942,\n2019.\nLewis, D. D. and Gale, W. A. A sequential algorithm for\ntraining text classiﬁers. In Proc. of SIGIR, 1994.\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and\nTalwalkar, A. Hyperband: A novel bandit-based approach\nto hyperparameter optimization. The Journal of Machine\nLearning Research, 2018.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv:1907.11692, 2019.\nLu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-\nand-language tasks. In Proc. of NeurIPS, 2019.\nMatthews, B. W. Comparison of the predicted and observed\nsecondary structure of t4 phage lysozyme. Biochimica et\nBiophysica Acta (BBA)-Protein Structure, 1975.\nMelis, G., Dyer, C., and Blunsom, P. On the state of the\nart of evaluation in neural language models. In Proc. of\nICLR, 2018.\nPhang, J., F´evry, T., and Bowman, S. R. Sentence encoders\non stilts: Supplementary training on intermediate labeled-\ndata tasks. arXiv:1811.01088, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. OpenAI Blog, 2019.\nRaffe, C., Shazeer, N., Roberts, A., Lee, K. L., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv:1910.10683, 2019.\nSaxe, A. M., McClelland, J. L., and Ganguli, S. Exact\nsolutions to the nonlinear dynamics of learning in deep\nlinear neural networks. In Proc. of ICLR, 2014.\nSchwartz, D., Toneva, M., and Wehbe, L. Inducing brain-\nrelevant bias in natural language processing models. In\nProc. of NeurIPS, 2019.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of EMNLP, 2013.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. GLUE: A multi-task benchmark and analysis\nplatform for natural language understanding. In Proc. of\nthe EMNLP Workshop BlackboxNLP, 2018.\nWang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. R. Super-\nglue: A stickier benchmark for general-purpose language\nunderstanding systems. In Proc. of NeuRIPS, 2019.\nWeight Initializations, Data Orders, and Early Stopping\nWarstadt, A., Singh, A., and Bowman, S. R. Neural network\nacceptability judgments. TACL, 7:625–641, 2019.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nand Brew, J. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771,\n2019.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov, R.,\nand Le, Q. V . Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Proc. of NeuRIPS,\n2019.\nZhang, C., Kjellstrm, H., and Mandt, S. Determinantal point\nprocesses for mini-batch diversiﬁcation. In Proc. of UAI,\n2017.\nZhu, C., Cheng, Y ., Gan, Z., Sun, S., Goldstein, T., and Liu,\nJ. Freelb: Enhanced adversarial training for language\nunderstanding. arXiv:1909.11764, 2019.\nWeight Initializations, Data Orders, and Early Stopping\nA. Appendix\nWe plot the distribution of standard deviations in ﬁnal validation performance across multiple runs, aggregated under a ﬁxed\nrandom seed, either for weight initialization or data order. The results are shown in Figure 8, indicating that the inter-seed\naggregated variances are comparable in magnitude, considering aggregation over both WI and DO.\n0.03 0.04 0.05 0.06 0.07\nAcc./F1 (standard dev.)\n0\n20\n40\n60\n80\n100Kernel Density Estimation\nMRPC\n0.04 0.05 0.06 0.07 0.08 0.09\nAccuracy (standard dev.)\n0\n20\n40\n60\n80\nRTE\n0.00 0.05 0.10 0.15 0.20\nMCC (standard dev.)\n0\n2\n4\n6\n8\n10\n12\n14\nCoLA\n0.001 0.002 0.003 0.004\nAccuracy (standard dev.)\n0\n200\n400\n600\n800\n1000\n1200\nSST\nDO\nWI\nFigure 8.Kernel density estimation of the distribution of standard deviation in validation performance aggregated under ﬁxed random\nseeds, either for weight initialization (blue) or data order (orange). The red dashed line shows the overall standard deviation for each\ndataset. The DO and WI curves have expected standard deviation values of similar magnitude, which are also comparable with the overall\nstandard deviation.\nB. ANOV A assumptions\nANOV A makes three assumptions: 1) independence of the samples, 2) homoscedasticity (roughly equal variance across\ngroups), and 3) normally distributed data.\nANOV A is not robust to violations of independence, but each DO and WI is an I.I.D. sample, and thus independent.\nANOV A is generally robust to groups with somewhat differing variance if the groups are the same size, which is true in our\nexperiments. ANOV A is more robust to non-normally distributed data for larger sample sizes; our SST experiments are quite\nclose to normally distributed, and the distribution of performance on the smaller datasets is less like a normal distribution\nbut we have larger sample sizes.\nC. Pearson Correlation\nIn Figure 9 we include the Pearson correlation between different points in training, whereas Figure 5 showed the rank\ncorrelation of the same data. One point in one of these plots represents the Pearson’s correlation between performance at\niteration i and iteration j across trials. High correlation means that the performance of the models is similar between the two\nevaluation points.\n0 1 2 3\n0\n1\n2\n3\nMRPC\n0 1 2 3\n0\n1\n2\n3\nRTE\n0 1 2 3\n0\n1\n2\n3\nCoLA\n0 1 2 3\n0\n1\n2\n3\nSST\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 9.Performance early in training is highly correlated with performance late in training. Each ﬁgure shows the Spearman’s rank\ncorrelation between the validation performance at different points in training; the axes represent epochs. A point at coordinates i and j in\nthe plots indicates the correlation between the best found performances after i and after j evaluations. Note that the plots are symmetric."
}