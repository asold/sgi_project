{
  "title": "Self-Supervised Learning with Cross-Modal Transformers for Emotion Recognition",
  "url": "https://openalex.org/W3108792608",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4227458649",
      "name": "Khare, Aparna",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2745409654",
      "name": "Parthasarathy, Srinivas",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A4287536158",
      "name": "Sundaram, Shiva",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W6768107342",
    "https://openalex.org/W2962753610",
    "https://openalex.org/W2796315435",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3084283759",
    "https://openalex.org/W3035299099",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963980299",
    "https://openalex.org/W6755964169",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2546919788",
    "https://openalex.org/W4245744384",
    "https://openalex.org/W2950299014",
    "https://openalex.org/W6686369754",
    "https://openalex.org/W2056415169",
    "https://openalex.org/W2130162821",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4234508787",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6674387193",
    "https://openalex.org/W6680106237",
    "https://openalex.org/W2886300652",
    "https://openalex.org/W2808631503",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6712930963"
  ],
  "abstract": "Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets. Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language. Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering. In this work, we extend self-supervised training to multi-modal applications. We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features. This model is fine-tuned on the downstream task of emotion recognition. Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3% compared to the baseline.",
  "full_text": "SELF-SUPERVISED LEARNING WITH CROSS-MODAL TRANSFORMERS FOR EMOTION\nRECOGNITION\nAparna Khare, Srinivas Parthasarathy, Shiva Sundaram\nAmazon.com, Sunnyvale, CA\nABSTRACT\nEmotion recognition is a challenging task due to limited avail-\nability of in-the-wild labeled datasets. Self-supervised learn-\ning has shown improvements on tasks with limited labeled\ndatasets in domains like speech and natural language. Models\nsuch as BERT learn to incorporate context in word embed-\ndings, which translates to improved performance in down-\nstream tasks like question answering. In this work, we ex-\ntend self-supervised training to multi-modal applications. We\nlearn multi-modal representations using a transformer trained\non the masked language modeling task with audio, visual and\ntext features. This model is ﬁne-tuned on the downstream\ntask of emotion recognition. Our results on the CMU-MOSEI\ndataset show that this pre-training technique can improve the\nemotion recognition performance by up to 3% compared to\nthe baseline.\nIndex Terms— self-supervised, multi-modal, emotion\nrecognition\n1. INTRODUCTION\nHuman communication is inherently multi-modal in nature.\nOur expressions and tone of voice augment verbal com-\nmunication. This can include vocal features like speaking\nrate, intonation and visual features like facial expressions\n[1]. Non-verbal communication is important for tasks that\ninvolve higher level cognitive expressions like emotions [2],\npersuasiveness [3] and mental health analysis [4]. We focus\non a multi-modal approach to emotion recognition because\nhumans fundamentally express emotions verbally using spo-\nken words [5], as well as with acoustic signals [6] and visual\nexpressions [7].\nGetting large-scale labeled datasets for emotion recogni-\ntion can be challenging. Our primary motivation for this paper\nis to study effective utilization of large unlabeled datasets to\nimprove performance of multi-modal emotion recognition\nsystems. The signals we consider are speech, visual informa-\ntion and spoken text. Our motivation stems from the popular\nuse of pre-trained models in natural language, speech and\nvisual understanding tasks to circumvent data limitations.\nBERT is a popular model for natural language understanding\n[9] that was trained using self-supervision. Devlin et al. use\nthe masked language modeling (LM) task on the Wikipedia\ncorpus for pre-training. The model was successfully ﬁne-\ntuned to improve performance on several tasks like question\nanswering and the general language understanding evalua-\ntion benchmarks [9]. Self-supervised learning has also been\nsuccessfully applied to speech based applications. Schneider\net al. in [10] use unsupervised pre-training on speech data\nby distinguishing an audio sample in the future from noise\nsamples. Fine-tuning this model shows state of the art results\non automatic speech recognition (ASR). Liu et al. show in\n[11] that a BERT-like pre-training approach can be applied\nto speech. By predicting masked frames instead of masked\nwords, the performance on tasks like speaker recognition,\nsentiment recognition and phoneme classiﬁcation can be im-\nproved. For emotion recognition, Tseng et al. show in [12]\nthat text-based self-supervised training can outperform state\nof the art models. The authors use a language modeling task,\nthat involves predicting a word given its context, to pre-train\nthe model. Another area of work that has leveraged unlabeled\ndata is detection and localization of visual objects and spoken\nwords in multi-modal input. Harwath et al. in [13, 14] train\nan audio-visual model on an image-audio retrieval task. The\nmodels are trained to learn a joint audio-visual representation\nin a shared embedding space. This model can learn to recog-\nnize word categories by sounds without explicit labels. Moti-\nvated by the success of these approaches, we study if similar\nmethods can be applied to multi-modal emotion recognition.\nTo the best of our knowledge, a joint self-supervised training\napproach using text, audio and visual inputs has not been well\nexplored for emotion recognition.\nMulti-modal emotion recognition models have been well\nstudied in literature and typically outperform uni-modal sys-\ntems [8]. These models need to combine inputs with varying\nsequence lengths. In video, the sequence lengths for audio and\nvisual frames differ from the length of text tokens by orders of\nmagnitude. There has been considerable prior work in fusing\nmulti-modal features. Liang et al. in [8] studied multiple fu-\nsion techniques for multi-modal emotion recognition and sen-\ntiment analysis. Their methods included early and late fusion\narXiv:2011.10652v1  [cs.CL]  20 Nov 2020\nof modalities, and a dynamic fusion graph based network.\nThey showed that the graph fusion model outperforms other\nmethods. Early fusion and graph fusion techniques both re-\nquire alignment between various modalities. Late fusion can\nbe performed without alignment, but does not allow interac-\ntion of features from different modalities at the frame level. To\novercome this limitation, Tsai et al. introduce the cross-modal\ntransformer in [15]. It scales the features using cross-modal\nattention. In the process, the modalities are projected into se-\nquences of equal lengths, eliminating the need for any align-\nment. This architecture has been successfully applied to prob-\nlems like emotion recognition, sentiment analysis [15, 16]\nand speech recognition [17]. Recently, another transformer-\nbased method to combine multi-modal inputs was introduced\nby Rahman et al. in [18], which uses a multi-modal adapta-\ntion gate.\nIn this paper, we propose using the same pre-training\nscheme as BERT, but extend it to a model that uses au-\ndio, visual and text inputs. We discuss the relevance of this\napproach in Section 2.2. The multi-modal representations\nlearned in pre-training are ﬁne-tuned for emotion recogni-\ntion. We evaluate the efﬁcacy of the pre-training approach.\nWe also perform experiments to understand the importance\nof each modality on the CMU-MOSEI dataset and provide\ncase-studies to interpret the results.\nThis paper is organized as follows. In Section 2 we de-\nscribe our model architecture and the self-supervised ap-\nproach for pre-training, along with further motivation for the\nself-supervised learning we choose. In Section 3, we dis-\ncuss the training setup and data. We present our results and\nanalysis in Section 4 and conclude in Section 5.\n2. SELF-SUPERVISED TRAINING WITH\nCROSS-MODAL TRANSFORMERS\n2.1. Model architecture\nNot all information in a given sequence is equally important\nfor emotion recognition. If we consider visual inputs, emo-\ntionally relevant cues may appear only in certain frames. Sim-\nilarly, each spoken word in the sentence does not contribute\nequally to the expressed emotion. Given this nature of the\nsequence recognition problem, transformer-based models are\na good choice for extracting a ﬁxed length representation for\nemotion recognition.\nWe use the cross-modal transformer for emotion recogni-\ntion since it showed state of the art results on sentiment anal-\nysis [15]. We chose a modiﬁed version of the proposed model\nand will describe it in this section. The architecture allows\neach sample from each modality to interact with each sample\nfrom each other modality, providing the beneﬁts of low-level\nfusion. It also projects all the sequences into equal lengths\nwhich allows for frame level late fusion after the transforma-\ntion.\nOur overall architecture is shown in Figure 1. The trans-\nformer model trained for emotion recognition allows for\nattending to speciﬁc input features (visual frames, words,\nspeech segments) that are relevant to the task [15]. The ﬁrst\npart of our model architecture achieves this by using self-\nattention based transformer encoder for individual modali-\nties. We add positional embeddings to the input features as\ndiscussed in [19]. Intuitively, positional embeddings would\nbe useful in the task because for extracting the context from\nthe input, the order of the words matter. We did not study\nthe importance of positional embeddings since our work is\nfocused on self-supervised learning. These features are pro-\ncessed by the transformer encoder. The architecture of the\nencoder layers is identical to [19] and is shown in Figure 1.\nThe transformer encoder consists of N layers. The ﬁrst op-\neration in each layer transforms the input into keys, queries\nand values. If the input to a given layer for modality M is\nrepresented by FM, then the query QM, the key KM and the\nvalue VM for the corresponding modality is computed as\nQM = WqM(FM)\nKM = WkM(FM)\nVM = WvM(FM)\n(1)\nwhere Wαβ represents a linear projection. After obtaining the\nkeys, queries and values, the self-attention layer scales the\nvalue VM. The output of the attention layer, represented by\nAM, is computed as\nAM = softmax(QMKT\nM√\nd\n)VM (2)\nwhere d denotes the dimensionality of the keys. In practice,\nwe use the multi-head version of the scaled dot-product atten-\ntion that uses k scaled dot-product attention heads. The ﬁnal\noutput of the transformer encoder layer, SM, is computed as\nfollowing\nOM = LayerNorm (VM + AM)\nSM = LayerNorm (OM + FeedForward (OM)) (3)\nwhere OM is the normalized output after adding a residual\nconnection from VM to the output of the scaled dot-product\nattention layer. We use N encoder layers to obtain the self-\nattended outputs SA, SV and ST from the audio, visual and\ntext modalities respectively.\nNext, we combine the uni-modal transformer encoder out-\nputs, SA, SV and ST, to learn the ﬁnal multi-modal representa-\ntion for emotion recognition. This is done by the cross-modal\ntransformer, which computes the attention map between fea-\ntures from two different modalities M1 and M2. The cross-\nmodal attention allows for increasing attention weights on\nMulti-head \nattention\nQ K V\nInput Features F\nM\nPositional \nencoding\nAdd & Norm\nFeed forward \nlayer\nx N layers\nTransformer Encoder\nOutput SM\nAdd & Norm\nMulti-head \nattention\nQ K V\nAdd & Norm\nFeed forward \nlayer\nx N layers\nCross-modal Encoder\nOutput EM1->M2\nAdd & Norm\nInput  SM2 Input  SM1\nCross Modal \nTransformer\nLinear Decoder\nK V Q\nText \nEncoder\nText \ninput\n(Anchor\nmodality\n)\nCross Modal \nTransformer\nK V Q\nWeighted \nsum\nFeed forward\nClassification layer\nVideo input\nVideo \nEncoder\nAudio input\nAudio \nEncoder\nMasked LM Loss\nOverall architecture\nFig. 1. Cross-modal transformer based self-supervised learning architecture. The ﬁgure shows the self-attention based trans-\nformer encoder layers, the cross-modal attention encoder module and the overall architecture of our self-supervised model.\nfeatures that are deemed important for emotion recognition\nby more than one modality. This property was shown in [15].\nThe output EM1−>M2 from the cross-modal transformer\nis computed as:\nEM1−>M2 = CM (QEM2\n, KEM1\n, VEM1\n) (4)\nwhere CM denotes the transformations applied by the cross-\nmodal transformer, shown in Figure 1. The cross-modal trans-\nformer is identical to the transformer encoder, with one ex-\nception. The key and value matrices (KEM1\nand VEM1\n) are ob-\ntained from the encoded output SM1 . The query QEM2\n, which\nprovides contextualization, is obtained from the encoded out-\nput of modality SM2 . The keys, queries and values are ob-\ntained from the outputs of the corresponding uni-modal trans-\nformers by using fully connected layers, similar to the uni-\nmodal transformer encoders described above. The ﬁnal en-\ncoder output EAVT is the weighted sum of the encoded out-\nput of the text modality and the cross-modal transformer out-\nput attending to the audio and visual inputs using the context\nquery from the text domain.\nEAVT = w1 ·ET + w·EA−>T + w3 ·EV−>T (5)\nFor our experiments, we used a ﬁxed weight of 0.33 for w1,\nw2 and w3. The classiﬁcation consists of average pooling fol-\nlowed by a linear layer that maps the representation into emo-\ntion classes.\nOur model differs from the architecture in [15] in two\nways. Instead of using convolutional layers to increase tem-\nporal awareness, we use uni-modal encoders to encode the\nvarious modalities. We used a second modiﬁcation by limit-\ning the cross-modal transformer layers to only compute atten-\ntion between audio and text, and visual input and text as de-\nscribed above. We call text the anchor modality in this archi-\ntecture. For this study, we chose text as the anchor modality\nfor ease of the self-supervised task. This allows us to train the\nself-supervised model without a decoder. For comparison, we\ntried training our baseline models with both audio and video\nas anchors and the results were similar. Note that our focus\nin this work is to study the impact of self-supervised training,\nand not to tune the model architecture for emotion recogni-\ntion. Hence, we chose the simpliﬁed model architecture for\nall the experiments. Our work does not compare our model to\nthe one proposed in [15] and we will do so in our future work.\n2.2. Self-supervised training\nOur primary motivation for this study is to understand how to\nleverage large unlabeled multi-modal datasets to learn repre-\nsentations which can be ﬁne-tuned for emotion recognition.\nWe consider what self-supervised task would be relevant\ngiven our downstream task of interest. We note that spoken\nwords are one of the strongest expressions of human emotion.\nThis has been studied in psychological literature [5], and also\nin the emotion recognition literature. Embeddings like ELMo,\nthat encode language context, can be applied successfully to\nthe emotion recognition task [12]. Such representations from\ntext have been learned successfully with self-supervised tasks\nlike skip-gram, continuous bag-of-words model [20] and\nmore recently using the masked LM task [9]. We extend this\nwork by learning representations that encode context using\nthe text input, as well as the audio and visual inputs.\nWe choose the masked LM task to train the model, simi-\nlar to the BERT model [9]. We propose to predict words by\nlooking at audio and visual context in addition to the con-\ntext words around the masked input. Intuitively, the auxiliary\ninformation present in visual expressions and audio features\nlike intonation would provide relevant input for predicting the\nmasked words. For example, consider the phrase “This movie\nis [MASK]” as an input to the model. The [MASK] word\ncould be predicted as “amazing” if the audio and visual fea-\ntures show that speaker is happy. Alternatively, the prediction\ncould be “terrible” if the speaker seems discontent while talk-\ning. This information cannot be derived from text only in-\nput. We posit that the latent representations learned using the\nmasked LM task with multi-modal input will not just encode\ncontext, but also the relevant emotional information which\ncould be used to predict those words.\nFor training, we mask 15% of the input words and the\naudio and visual features corresponding to those words. The\nword boundaries are obtained using an existing ASR system.\nMore details on the ASR system used will be discussed in\nSection 3.1. The model predicts the words at each sequence,\nand the loss is computed only for the words that were masked\nin the input sequence. Instead of providing a mask token as\ninput for masked words, we choose to set the masked in-\nput for all modalities to zero. We are able to do so as we\nuse GLoVe embeddings to represent the text input instead of\nlearning an embedding layer. Similarly, to mask the audio and\nvisual inputs for the corresponding masked words, we replace\nthe input features with zeros. For this task, we replace the av-\nerage pooling and linear classiﬁer described in Section 2.1\nwith a linear layer of output size equal to the model vocabu-\nlary. Since the encoder layer uses bi-directional attention, we\ndo not need a decoder to attend to past predictions from the\nmodel. In addition, since the encoder output length is equal\nto the sequence length of the input text, we do not require a\ntransformer decoder layer. This allows training to be simpli-\nﬁed and was one of the reasons we chose the model architec-\nture.\nFor the loss function, we use a full softmax loss as well as\nnoise contrastive estimation (NCE) [21] to train our models.\nNCE has been used successfully to learn the inverse language\nmodeling task, that involves predicting the context words\ngiven a word. Minh et al. show in [22] that NCE can reduce\ncomputation by estimating the normalization factor for com-\nputing softmax using noise samples. For a task which has\na similar vocabulary size as our dataset, they demonstrated\na reduction of up to 50% in training time. We compare the\nmodels trained with NCE loss with the full softmax loss.\nThis would inform if the multi-modal transformer can be\ntrained with similar accuracy but more efﬁciently. Our im-\nplementation is exactly the same as [22], except we use a\nnormalization factor of vocabulary size which we found to be\ncritical for training our model.\n3. EXPERIMENTAL SETUP\n3.1. Dataset and training details\nOur setup involves ﬁrst training the cross-modal transformer\non the masked LM task on a large dataset, followed by ﬁne-\ntuning for emotion recognition. For pre-training, we utilize\nthe publicly available V oxCeleb2 dataset [23]. We chose this\ndataset since it provides all the modalities we are interested\nin and is sufﬁciently large (1.1 million videos in the train par-\ntition). More importantly, this data is emotion rich, as shown\nin [24]. This dataset does not provide text transcriptions. We\nused a TDNN ASR model trained with the standard Kaldi\nrecipe on the Librispeech dataset to get transcriptions [25].\nWe use 40-dimensional Log-Filter bank energy (LFBE) fea-\ntures using a 10ms frame rate to represent the audio input. Vi-\nsual frames are represented by 4096-dimensional features ex-\ntracted from the VGG-16 model. 300-dimensional GloVe em-\nbeddings represent the text input. We chose to use GLoVe em-\nbeddings for this task instead of an embedding layer because\nour dataset has a limited number of sentences and vocabulary.\nThe vocabulary size for this dataset as obtained from ASR\ntranscriptions is 88000. Using GloVe embeddings allows the\nmodel to take advantage of pre-trained embeddings trained\non billions of words [26]. The disadvantage is the inability to\nhandle out of vocabulary words, which we ignore for all our\nexperiments.\nThe model is pre-trained using pytorch with the learning\nschedule described in [27]. We stack 5-frames of the LFBE\nfeatures for a ﬁnal audio feature dimensionality of 200. This\nwas done to reduce the memory requirements for training the\nmodel. We select only the English language videos from\nV oxCeleb2 for training. The ﬁltering is done by selecting a\nheuristic threshold on the likelihood scores from the ASR de-\ncoder. For all our experiments, we use only the dev portion of\nthe V oxCeleb2 dataset. Our ﬁnal training dataset consists of\n978k utterances from 4820 speakers. We use the architecture\ndescribed in Section 2.1 to train the model. The model has\nkeys, values and queries of dimension 512, 4 encoder layers,\nfeed forward layer of dimension 200 and 4 attention heads for\nboth the uni-modal and cross-modal encoders. We trained the\nmodel for 20 epochs and chose the ﬁnal model with the lowest\nloss on a held-out set.\nFor evaluating performance on the emotion recognition\ntask, we ﬁne-tune the model on the CMU-MOSEI dataset\n[8]. It is the largest publicly available multi-modal dataset\nfor emotion recognition with natural conversations. It con-\ntains 23,453 single-speaker video segments from YouTube.\nThe clips have been manually transcribed and annotated for\nsentiment and emotion. The dataset consists of 6 emotions;\nhappy (12135 examples), sad (5856 examples), angry (4903\nexamples), disgust (4208 examples), surprise (2262 exam-\nples) and fear (1850 examples). The labels for each class are\non a Likert scale of [0, 3]. We convert the labels into binary\ntargets. A clip is assigned a 0 label if the score on the Likert\nscale is 0, and 1 otherwise. A greater than 0 score on the Lik-\nert scale represents the presence of the speciﬁc emotion, and\n0 the absence of the emotion. This was reﬂected in the binary\ninterpretation that we chose. For ﬁne-tuning the model, we\nremove the decoder layer for the masked LM task and add the\naverage pooling and decoder layer for emotion recognition.\nEach example in this dataset can be labeled with the presence\nof multiple emotions. Therefore, we use a sigmoid output for\neach of the 6 nodes in the output layer to get the probability\nfor each emotion. The positive and negative examples for var-\nious emotions in the dataset are imbalanced. During training,\nwe weigh the loss for the each training sample appropriately\nto ensure that the positive and negative examples across all\nemotions contribute equally to the loss.\n4. RESULTS\nWe use the weighted accuracy (W A) and F1-score for each\nemotion as the metrics for the task. We also report average of\nthese two metrics over the 6 emotions, keeping in line with\nprior work [8]. For evaluating the baseline model, we follow\nthe procedure in [12]. The model is randomly initialized and\ntrained 10 different times. The best model is chosen based on\nthe average of the weighted accuracy and F1-scores over all\nthe emotions on the dev set over the 10 runs.\n4.1. Results on emotion recognition\nTable 1 shows the results of our experiments and state of the\nart results on the same dataset from other publications. The\ntransformer baseline outperforms or is comparable to pub-\nlished results for most of the metrics. Our model shows a\n2.4% absolute improvement in the weighted accuracy of the\nanger emotion and a 2.6% absolute improvement in the F1-\nscore of the surprise emotion. We observe a degradation in\nthe weighted accuracy of the fear emotion. This comparison\nwith other state of the art models is pertinent for the rest of\nour work as we want to build upon a strong baseline model.\nThe next set of results in Table 1 are with the pre-trained\nmodel on the V oxCeleb2 dataset, ﬁne-tuned for emotion\nrecognition. Our results show up to 3% absolute improvement\nin the weighted accuracy of 4 out of the 6 emotions, with a\nslight degradation in the weighted accuracy of the anger\nemotion. The average weighted accuracy over all emotions\nimproves by 0.8%. The weighted accuracy of the surprise and\nfear emotions improve by 2.2% and 3% absolute respectively.\nThe 95% conﬁdence intervals of these emotions don’t overlap\nwith the baseline, demonstrating the statistical signiﬁcance\nof the results. The F1-score is comparable to the baseline for\nall emotions other than happy, where we see a 1% absolute\nimprovement. The model trained using the NCE optimiza-\ntion has similar improvements. It shows that we can achieve\nthe same improvements with a lower computational cost of\ntraining. These results validate our hypothesis that we can\neffectively leverage a large unlabeled multi-modal dataset to\nimprove results on emotion recognition using self-supervised\npre-training.\nIn order to understand the impact of ASR errors on the\nmodel, we generated transcriptions on the CMU-MOSEI\ndataset using a commercial ASR system. The word error rate\nof the machine-generated transcriptions was 29%. We then\nre-evaluated the performance of our baseline model with ASR\nbased transcriptions instead of the transcriptions provided as\npart of the dataset. We did not observe a degradation in emo-\ntion recognition performance. Note that for this experiment,\nthe baseline model was trained with the original transcrip-\ntions. ASR errors have been studied well in literature and it\nhas been shown that the top contributors to errors are shorter\nwords like ’on’, ’was’, ’in’ etc. [28]. These words do not\ncontribute to emotion expression, which would explain the\nobservations we made.\n4.2. Analysis and case studies\nWe analyze the results to understand the contribution of each\nmodality towards accuracy. We look at predictions from the\nbaseline model with missing inputs from select modalities.\nNote that we cannot ablate the text input. The output of the\ncross-modal transformers will be 0 if the text input is 0 since\nthe attention maps will be all zeros. For subjective analysis\nwith missing text input, we trained a baseline model with au-\ndio as the anchor modality. As noted in Section 2.1, the choice\nof anchor modality does not change the performance of the\nbaseline model. We describe our subjective analysis below.\nThe ﬁrst example we observe is ID “HeZS2-Prhc[8]” in\nthe dataset. From visual inspection, the video shows that the\nspeaker is laughing, which conveys a happy emotion. How-\never, the speaker is talking about the cost for drugs and its\nimpact on communities. This is why the visual modality is\nthe key to accurately predicting the emotion, and the model\nis not able to classify the emotion as happy with text input\nalone. On the contrary, the second example, ID “10219[11]”,\nTable 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4\nModel Happy Sad Anger Surprise Disgust Fear Average\nW A F1 W A F1 W A F1 W A F1 W A F1 W A F1 W A F1\nM-ELMo + NN [12](A+T) 67.0 65.2 63.1 72.0 65.8 74.7 63.8 83.3 74.2 81.7 63.2 85.1 66.2 77.0\nGraph-MFN [8] 66.3 66.3 60.4 66.9 62.6 72.8 53.7 85.5 69.1 76.6 62.0 89.9 62.3 76.3\nTransformer (baseline) 67.4 67.1 64.6 72.5 68.2 74.7 62.9 88.1 74.8 82.4 61.5 86.5 66.6 78.5\nTransformer with pre-training and full softmax loss68.1 68.1 65.1 72.1 67.0 74.4 65.1 88.0 74.5 82.3 64.5 86.4 67.4 78.6\nTransformer with pre-training and NCE loss68.1 68.2 64.3 72.4 67.3 74.8 65.1 87.7 73.6 82.4 63.0 86.6 66.9 78.7\nTable 2. Ablation studies with the baseline model. Note that\nthe text modality cannot be ablated with this architecture.\nWA F1\nText only 64.5 76.7\nAudio + Text 65.3 78.1\nVideo + Text 65.3 78.5\nAudio + Video + Text 66.6 78.5\nshows the speaker with a neutral face and a neutral tone of\nvoice. The speaker is talking about a positive movie review,\nwhich leads to the text classifying the emotion as happy. The\nmodel was not able to classify the emotion in this example\nas happy without the text input. In the third example, ID “-\n9y-fZ3swSY[1]”, the speaker is talking about a neutral topic\nwith a slightly positive face, but in a very positive tone of\nvoice. The model predicts that the speaker in this video is\nhappy only when the audio features are present. This subjec-\ntive analysis shows the importance of multi-modal features in\nhuman communication, and how each of them contribute to\nemotion recognition.\nNext, we show the overall results with each missing\nmodality in Table 2. Adding audio and visual input along\nwith text improves both metrics by 2% absolute. The results\nshow that with text alone, we can recover most of the baseline\nperformance. The subjective examples, however, suggest that\nfor several cases, other modalities are required for accurate\nprediction. Therefore, the importance of text should not be\ngeneralized for the problem of emotion recognition in-the-\nwild. However, for the CMU-MOSEI dataset, text is the most\nimportant modality for emotion recognition. To analyze this,\nwe look at the distribution of topics in the dataset. The 5 most\nfrequent topics are: reviews (16.2%), debate (2%), consulting\n(1.8%), ﬁnancial (1.8%) and speech (1.6%). For these top-\nics, the perceived emotion by a human annotator is strongly\nbased on what is being said. This would explain why text\nis the most important input. We posit that for more diverse\ntopics, speciﬁcally involving human to human communica-\ntion, the other modalities would start to gain importance for\nrecognizing the emotions accurately.\n5. CONCLUSION\nIn this paper, we present state of the art results on the emo-\ntion recognition task using the cross-modal transformer on the\nCMU-MOSEI dataset. We utilize a BERT-like pre-training\nscheme using audio, visual and text inputs. We use the V ox-\nCeleb2 dataset to pre-train the model and ﬁne-tune it for the\nemotion recognition task. We demonstrate up to a 3% im-\nprovement over the baseline with the ﬁne-tuned model. We\npresented our subjective analysis on the contribution of vari-\nous modalities to emotion recognition. We also show results\nwith missing input modalities to understand the importance\nof each modality for the emotion recognition task.\nFor our future work, we propose to initialize the text en-\ncoder with a text-only model like BERT, before multi-modal\nself-supervised training. V oxCeleb2 dataset, although large in\nterms of number of hours of video, is smaller when compared\nto the Wikipedia corpus which has billions of words. Taking\nadvantage of a larger text-only corpus could provide improve-\nments. We would also like to experiment with adapting the\nmodel on the CMU-MOSEI dataset. Both the V oxCeleb2 and\nCMU-MOSEI datasets are obtained from YouTube, but there\ncould be domain mismatch between the two datasets. Adapt-\ning could help bridge the mismatch. We would also like to ex-\nplore weak labels to adapt the pre-trained representations for\nthe downstream task. Tseng et al. showed in [29] that weakly\nsupervised labels can be used to effectively bias the embed-\ndings learned by a pre-trained model. Even though we study\nthe impact of ASR errors on emotion recognition, we do not\nknow how these errors impact the self-supervised training.\nWe would like to study that in the future. As noted before,\nour model architecture doesn’t allow ablation of text. For our\nfuture work, we will focus on overcoming that limitation.\n6. REFERENCES\n[1] J. K. Burgoon, L. K. Guerrero, and K. Floyd, Nonverbal\ncommunication. Routledge, 2016.\n[2] B. Schuller, M. Valstar, F. Eyben, G. McKeown,\nR. Cowie, and M. Pantic, “Avec 2011–the ﬁrst interna-\ntional audio/visual emotion challenge,” in International\nConference on Affective Computing and Intelligent In-\nteraction. Springer, 2011, pp. 415–424.\n[3] B. Nojavanasghari, D. Gopinath, J. Koushik, T. Bal-\ntruˇsaitis, and L.-P. Morency, “Deep multimodal fusion\nfor persuasiveness prediction,” in Proceedings of the\n18th ACM International Conference on Multimodal In-\nteraction, 2016, pp. 284–288.\n[4] A. Haque, M. Guo, A. S. Miner, and L. Fei-Fei,\n“Measuring depression symptom severity from spoken\nlanguage and 3d facial expressions,” arXiv preprint\narXiv:1811.08592, 2018.\n[5] C. Chung and J. W. Pennebaker, “The psychological\nfunctions of function words,” Social communication,\nvol. 1, pp. 343–359, 2007.\n[6] R. Banse and K. R. Scherer, “Acoustic proﬁles in vocal\nemotion expression.” Journal of personality and social\npsychology, vol. 70, no. 3, p. 614, 1996.\n[7] J. F. Cohn, “Foundations of human computing: facial\nexpression and emotion,” in Proceedings of the 8th in-\nternational conference on Multimodal interfaces, 2006,\npp. 233–238.\n[8] P. Liang, R. Salakhutdinov, and L.-P. Morency, “Com-\nputational modeling of human multimodal language:\nThe mosei dataset and interpretable dynamic fusion,”\n2018.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“Bert: Pre-training of deep bidirectional transformers\nfor language understanding,” inProceedings of the 2019\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers),\n2019, pp. 4171–4186.\n[10] S. Schneider, A. Baevski, R. Collobert, and M. Auli,\n“wav2vec: Unsupervised pre-training for speech recog-\nnition,” Proc. Interspeech 2019, pp. 3465–3469, 2019.\n[11] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y.\nLee, “Mockingjay: Unsupervised speech representation\nlearning with deep bidirectional transformer encoders,”\nin ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 6419–6423.\n[12] S.-Y . Tseng, P. Georgiou, and S. Narayanan, “Mul-\ntimodal embeddings from language models,” arXiv\npreprint arXiv:1909.04302, 2019.\n[13] D. Harwath and J. Glass, “Learning word-like units from\njoint audio-visual analysis,” in Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 2017, pp. 506–\n517.\n[14] D. Harwath, A. Recasens, D. Sur ´ıs, G. Chuang, A. Tor-\nralba, and J. Glass, “Jointly discovering visual objects\nand spoken words from raw sensory input,” Interna-\ntional Journal of Computer Vision, vol. 128, no. 3, pp.\n620–641, 2020.\n[15] Y .-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-\nP. Morency, and R. Salakhutdinov, “Multimodal trans-\nformer for unaligned multimodal language sequences,”\nin Proceedings of the conference. Association for Com-\nputational Linguistics. Meeting, vol. 2019. NIH Public\nAccess, 2019, p. 6558.\n[16] A. Khare, S. Parthasarathy, and S. Sundaram, “Multi-\nmodal embeddings using multi-task learning for emo-\ntion recognition,” Proc. Interspeech 2020, pp. 384–388,\n2020.\n[17] G. Paraskevopoulos, S. Parthasarathy, A. Khare, and\nS. Sundaram, “Multimodal and multiresolution speech\nrecognition with transformers,” in Proceedings of the\n58th Annual Meeting of the Association for Computa-\ntional Linguistics, 2020, pp. 2381–2387.\n[18] W. Rahman, M. K. Hasan, S. Lee, A. B. Zadeh, C. Mao,\nL.-P. Morency, and E. Hoque, “Integrating multimodal\ninformation in large pretrained transformers,” in Pro-\nceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, 2020, pp. 2359–2369.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n“Attention is all you need,” inAdvances in neural infor-\nmation processing systems, 2017, pp. 5998–6008.\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient\nestimation of word representations in vector space,”\narXiv preprint arXiv:1301.3781, 2013.\n[21] M. U. Gutmann and A. Hyv ¨arinen, “Noise-contrastive\nestimation of unnormalized statistical models, with ap-\nplications to natural image statistics,” Journal of Ma-\nchine Learning Research, vol. 13, no. Feb, pp. 307–361,\n2012.\n[22] A. Mnih and K. Kavukcuoglu, “Learning word em-\nbeddings efﬁciently with noise-contrastive estimation,”\nin Advances in neural information processing systems,\n2013, pp. 2265–2273.\n[23] J. S. Chung, A. Nagrani, and A. Zisserman, “V oxceleb2:\nDeep speaker recognition,” inINTERSPEECH, 2018.\n[24] S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisser-\nman, “Emotion recognition in speech using cross-modal\ntransfer in the wild,” in ACM Multimedia, 2018.\n[25] V . Peddinti, D. Povey, and S. Khudanpur, “A time de-\nlay neural network architecture for efﬁcient modeling\nof long temporal contexts,” in Sixteenth Annual Confer-\nence of the International Speech Communication Asso-\nciation, 2015.\n[26] J. Pennington, R. Socher, and C. D. Manning,\n“Glove: Global vectors for word representation,” in\nEmpirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 1532–1543. [Online]. Available:\nhttp://www.aclweb.org/anthology/D14-1162\n[27] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a\nno-recurrence sequence-to-sequence model for speech\nrecognition,” in 2018 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2018, pp. 5884–5888.\n[28] A. Stolcke and J. Droppo, “Comparing human and\nmachine errors in conversational speech transcription,”\nProc. Interspeech 2017, pp. 137–141, 2017.\n[29] S.-Y . Tseng, B. Baucom, and P. Georgiou, “Unsuper-\nvised online multitask learning of behavioral sentence\nembeddings,” PeerJ Computer Science, vol. 5, p. e200,\n2019.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I-1",
      "name": null,
      "country": null
    }
  ]
}