{
  "title": "Reformer: The Efficient Transformer",
  "url": "https://openalex.org/W2994673210",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288528019",
      "name": "Kitaev, Nikita",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286867883",
      "name": "Kaiser, Łukasz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379227",
      "name": "Levskaya, Anselm",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2118323718",
    "https://openalex.org/W2400680200",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964267515",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W2399033357",
    "https://openalex.org/W2941620283",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2919624000"
  ],
  "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
  "full_text": "Published as a conference paper at ICLR 2020\nREFORMER : T HE EFFICIENT TRANSFORMER\nNikita Kitaev∗\nU.C. Berkeley & Google Research\nkitaev@cs.berkeley.edu\nŁukasz Kaiser ∗\nGoogle Research\n{lukaszkaiser,levskaya}@google.com\nAnselm Levskaya\nGoogle Research\nABSTRACT\nLarge Transformer models routinely achieve state-of-the-art results on a number\nof tasks but training these models can be prohibitively costly, especially on long\nsequences. We introduce two techniques to improve the efﬁciency of Transform-\ners. For one, we replace dot-product attention by one that uses locality-sensitive\nhashing, changing its complexity from O(L2) to O(L log L), where L is the length\nof the sequence. Furthermore, we use reversible residual layers instead of the\nstandard residuals, which allows storing activations only once in the training pro-\ncess instead of N times, where N is the number of layers. The resulting model,\nthe Reformer, performs on par with Transformer models while being much more\nmemory-efﬁcient and much faster on long sequences.\n1 I NTRODUCTION\nThe Transformer architecture (Vaswani et al., 2017) is widely used in natural language processing\nand yields state-of-the-art results on a number of tasks. To obtain these results, researchers have\nresorted to training ever larger Transformer models. The number of parameters exceeds 0.5B per\nlayer in the largest conﬁguration reported in (Shazeer et al., 2018) while the number of layers goes\nup to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences.\nUp to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when\nprocessing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even\nlonger sequences are commonplace. These large-scale long-sequence models yield great results but\nstrain resources to the point where some argue that this trend is breaking NLP research 1. Many\nlarge Transformer models can only realistically be trained in large industrial research laboratories\nand such models trained with model parallelism cannot even be ﬁne-tuned on a single GPU as their\nmemory requirements demand a multi-accelerator hardware setup even for a single training step.\nDo large Transformer models fundamentally require such huge resources or are they simply inefﬁ-\ncient? Consider the following calculation: the 0.5B parameters used in the largest reported Trans-\nformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024\nand batch size 8 account for 64K ×1K ×8 = 0.5B ﬂoats, requiring another 2GB of memory. If\nour memory use was only per-layer, then we should fairly easily ﬁt a large Transformer even on\nsequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT\nonly requires 17GB to store. Why is it then that we cannot even ﬁne-tune these models on single\nmachines?\nThe above estimate includes only per-layer memory and input activations cost and does not take into\naccount the following major sources of memory use in the Transformer.\n• Memory in a model with N layers is N-times larger than in a single-layer model due to the\nfact that activations need to be stored for back-propagation.\n• Since the depth dff of intermediate feed-forward layers is often much larger than the depth\ndmodel of attention activations, it accounts for a large fraction of memory use.\n• Attention on sequences of length L is O(L2) in both computational and memory complex-\nity, so even for a single sequence of 64K tokens can exhaust accelerator memory.\n∗Equal Contribution\n1https://hackingsemantics.xyz/2019/leaderboards/\n1\narXiv:2001.04451v2  [cs.LG]  18 Feb 2020\nPublished as a conference paper at ICLR 2020\nWe introduce the Reformer model which solves these problems using the following techniques:\n• Reversible layers, ﬁrst introduced in Gomez et al. (2017), enable storing only a single copy\nof activations in the whole model, so the N factor disappears.\n• Splitting activations inside feed-forward layers and processing them in chunks removes the\ndff factor and saves memory inside feed-forward layers.\n• Approximate attention computation based on locality-sensitive hashing replaces the O(L2)\nfactor in attention layers with O(L log L) and so allows operating on long sequences.\nWe study these techniques and show that they have negligible impact on the training process com-\npared to the standard Transformer. Splitting activations in fact only affects the implementation; it is\nnumerically identical to the layers used in the Transformer. Applying reversible residuals instead of\nthe standard ones does change the model but has a negligible effect on training in all conﬁgurations\nwe experimented with. Finally, locality-sensitive hashing in attention is a more major change that\ncan inﬂuence the training dynamics, depending on the number of concurrent hashes used. We study\nthis parameter and ﬁnd a value which is both efﬁcient to use and yields results very close to full\nattention.\nWe experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image\ngeneration task (imagenet-64 generation) with sequences of length 12K. In both cases we show that\nReformer matches the results obtained with full Transformer but runs much faster, especially on the\ntext task, and with orders of magnitude better memory efﬁciency.\n2 L OCALITY -SENSITIVE HASHING ATTENTION\nDot-product attention. The standard attention used in the Transformer is the scaled dot-product\nattention (Vaswani et al., 2017). The input consists of queries and keys of dimension dk, and values\nof dimension dv. The dot products of the query with all keys are computed, scaled by √dk, and a\nsoftmax function is applied to obtain the weights on the values. In practice, the attention function\non a set of queries is computed simultaneously, packed together into a matrixQ. Assuming the keys\nand values are also packed together into matrices K and V , the matrix of outputs is deﬁned as:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (1)\nMulti-head attention. In the Transformer, instead of performing a single attention function with\ndmodel-dimensional keys, values and queries, one linearly projects the queries, keys and values h\ntimes with different, learned linear projections to dk, dk and dv dimensions, respectively. Attention\nis applied to each of these projected versions of queries, keys and values in parallel, yielding dv-\ndimensional output values. These are concatenated and once again projected, resulting in the ﬁnal\nvalues. This mechanism is known as multi-head attention.\nMemory-efﬁcient attention. To calculate the memory use of the attention mechanism, let us\nfocus on the attention computation from Equation 1. Let us assume that Q, K and V all have\nthe shape [batch size, length, dmodel]. The main issue is the term QKT , which has the shape\n[batch size, length, length]. In the experimental section we train a model on sequences of length\n64K – in this case, even at batch-size of 1, this is a64K ×64K matrix, which in 32-bit ﬂoats would\ntake 16GB of memory. This is impractical and has hindered the use of the Transformer for long\nsequences. But it is important to note that the QKT matrix does not need to be fully materialized\nin memory. The attention can indeed be computed for each query qi separately, only calculating\nsoftmax(qiKT\n√dk\n)V once in memory, and then re-computing it on the backward pass when needed for\ngradients. This way of computing attention may be less efﬁcient but it only uses memory propor-\ntional to length. We use this memory-efﬁcient implementation of attention to run the full-attention\nbaselines presented in the experimental section.\nWhere do Q, K, V come from? The multi-head attention described above operates on keys,\nqueries and values, but usually we are only given a single tensor of activations A of the shape\n[batch size, length, dmodel] – e.g., coming from embedding the tokens in a sentence into vectors.\n2\nPublished as a conference paper at ICLR 2020\nFigure 1: An angular locality sensitive hash uses random rotations of spherically projected points to\nestablish buckets by an argmax over signed axes projections. In this highly simpliﬁed 2D depiction,\ntwo points x and y are unlikely to share the same hash buckets (above) for the three different angular\nhashes unless their spherical projections are close to one another (below).\nTo build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K\nand V with different parameters. For models with LSH attention, we want queries and keys (Q and\nK) to be identical. This is easily achieved by using the same linear layer to go from A to Q and\nK, and a separate one for V . We call a model that behaves like this a shared-QK Transformer. It\nturns out that sharing QK does not affect the performance of Transformer, even if we additionally\nnormalize the length of the keys K, as we show in the experimental Section 5.\nHashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape\n[batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the atten-\ntion computation from Equation 1. As already mentioned, the main issue is the term QKT ,\nwhich has the shape [batch size, length, length]. But note that we are actually only interested\nin softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only\nneed to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi\nwe could only consider a small subset of, say, the32 or 64 closest keys. That is much more efﬁcient,\nbut how can we ﬁnd the nearest neighbors among the keys?\nLocality sensitive hashing. The problem of ﬁnding nearest neighbors quickly in high-dimensional\nspaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector\nx to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability\nand distant ones do not. In our case, we actually only require that nearby vectors get the same hash\nwith high probability and that hash-buckets are of similar size with high probability.\nWe achieve this by employing random projections as follows (see Figure 1). To get b hashes, we\nﬁrst ﬁx a random matrix R of size [dk, b/2]. We then deﬁne h(x) = arg max([xR; −xR]) where\n[u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al.,\n2015) and is easy to implement and apply to batches of vectors.\nLSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now\nformalize the LSH attention we use in this paper. We ﬁrst rewrite the equation for normal attention,\n(1), for a single query position i at a time:\noi =\n∑\nj∈Pi\nexp (qi ·kj −z(i, Pi)) vj where Pi = {j : i ≥j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to\ndenote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit\nscaling by √dk.\nFor batching purposes we typically perform attention over a larger set ˜Pi = {0, 1, . . . , l}⊇P i\nwhile masking out elements not in Pi:\noi =\n∑\nj∈˜Pi\nexp (qi ·kj −m(j, Pi) −z(i, Pi)) vj where m(j, Pi) =\n{∞ if j /∈Pi\n0 otherwise (3)\n3\nPublished as a conference paper at ICLR 2020\nFigure 2: Simpliﬁed depiction of LSH Attention showing the hash-bucketing, sorting, and chunking\nsteps and the resulting causal attentions. (a-d) Attention matrices for these varieties of attention.\nNow we turn to LSH attention, which we can think of in terms of restricting the set Pi of target\nitems a query position i can attend to, by only allowing attention within a single hash bucket.\nPi = {j : h(qi) =h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant. Part (a) depicts\nthat the attention matrix for full attention is typically sparse, but the computation does not take\nadvantage of this sparsity. In (b), the queries and keys have been sorted according to their hash\nbucket. Since similar items fall in the same bucket with high probability, the full attention pattern\ncan be approximated by only allowing attention within each bucket.\nHash buckets in this formulation tend to be uneven in size, which makes it difﬁcult to batch across\nbuckets. Moreover, the number of queries and the number of keys within a bucket may be unequal –\nin fact, it is possible for a bucket to contain many queries but no keys. To alleviate these issues, we\nﬁrst ensure that h(kj) = h(qj) by setting kj = qj\n∥qj∥. Next, we sort the queries by bucket number\nand, within each bucket, by sequence position; this deﬁnes a permutation wherei ↦→si after sorting.\nIn the sorted attention matrix, pairs from the same bucket will cluster near the diagonal (as depicted\nin Figure 2c). We can follow a batching approach where chunks of m consecutive queries (after\nsorting) attend to each other, and one chunk back (Figure 2d). Following our earlier notation, this\ncorresponds to setting:\n˜Pi =\n{\nj :\n⌊si\nm\n⌋\n−1 ≤\n⌊sj\nm\n⌋\n≤\n⌊si\nm\n⌋}\n(5)\nIf maxi |Pi|< m, then Pi ⊆˜Pi. In practice we set m = 2l\nnbuckets\n(where l is the sequence length).\nThe average bucket size is l\nnbuckets\n, and we assume that the probability of a bucket growing to twice\nthat size is sufﬁciently low. The overall process of LSH attention is summarized in Figure 2.\nMulti-round LSH attention. With hashing, there is always a small probability that similar items\nnevertheless fall in different buckets. This probability can be reduced by doing multiple rounds of\nhashing with nrounds distinct hash functions {h(1), h(2), . . .}, such that:\nPi =\nnrounds⋃\nr=1\nP(r)\ni where P(r)\ni =\n{\nj : h(r)(qi) =h(r)(qj)\n}\n(6)\nThe multi-round case essentially involves performing LSH attention nrounds times in parallel; the\ndetails of the procedure are described in in Appendix A.\nCausal masking for shared-QK attention. In a Transformer decoder, masking (denoted by\nm(j, Pi) in Equation 3) is used to prevent positions from attending into the future. To implement\nmasking in LSH attention, we associate every query/key vector with a position index, re-order the\nposition indices using the same permutations used to sort the query/key vectors, and then use a\ncomparison operation to compute the mask.\n4\nPublished as a conference paper at ICLR 2020\nTable 1: Memory and time complexity of attention variants. We write l for length, b for batch size,\nnh for the number of heads,nc for the number of LSH chunks,nr for the number of hash repetitions.\nAttention Type Memory Complexity Time Complexity\nScaled Dot-Product max(bnhldk, bnhl2) max( bnhldk, bnhl2)\nMemory-Efﬁcient max(bnhldk, bnhl2) max( bnhldk, bnhl2)\nLSH Attention max(bnhldk, bnhlnr(4l/nc)2) max( bnhldk, bnhnrl(4l/nc)2)\nTable 2: Accuracies on the duplication task of a 1-layer Transformer model with full attention and\nwith locality-sensitive hashing attention using different number of parallel hashes.\nTrain\nEval Full Attention LSH-8 LSH-4 LSH-2 LSH-1\nFull Attention 100% 94.8% 92.5% 76.9% 52.5%\nLSH-4 0.8% 100% 99.9% 99.4% 91.9%\nLSH-2 0.8% 100% 99.9% 98.1% 86.8%\nLSH-1 0.8% 99.9% 99.6% 94.8% 77.9%\nWhile attention to the future is not allowed, typical implementations of the Transformer do allow\na position to attend to itself. Such behavior is undesirable in a shared-QK formulation because the\ndot-product of a query vector with itself will almost always be greater than the dot product of a\nquery vector with a vector at another position. We therefore modify the masking to forbid a token\nfrom attending to itself, except in situations where a token has no other valid attention targets (e.g.\nthe ﬁrst token in a sequence).\n2.1 A NALYSIS ON A SYNTHETIC TASK\nTo verify the performance of LSH attention and study its behavior, we start with the following\nsynthetic task: duplicate a sequence of symbols. In this task, each training and testing example has\nthe form 0w0w where w ∈{1, . . . , N}∗is a sequence of symbols ranging from 1 to N (we use\nN = 127in our experiments). An example with the word w of length 3 is given below.\nExample: 0 19 113 72 0 19 113 72\nTo study LSH attention, we train a language model on examples of the above form where each w\nis of length 511 (so the whole input 0w0w is of length 1024). As this is a language modeling task,\nwe always predict the next symbol given all the previous ones, but we mask the loss and accuracy to\nonly consider positions in the second half of the input, i.e., those that can actually be predicted.\nThe above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer\nmodel. Note though, that it requires non-local attention lookups, so it cannot be solved by any\nmodel relying on sparse attention with a limited span. To make it easy and fast to train but similar\nto models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256and 4 heads. We\ntrain it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1,\nnrounds = 2and nrounds = 4.\nFrom the results summarized in Table 2 we see that a model trained with full attention can be imme-\ndiately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH\nattention, the model trained with 4 hashes achieves almost perfect accuracy as well. Interestingly,\nthe accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or\n1 hashes. Models trained with less hashes show worse results but even the model trained with just 1\nhash performs almost perfectly when evaluated with 8 hashes.\n5\nPublished as a conference paper at ICLR 2020\n3 R EVERSIBLE TRANSFORMER\nAs the above section shows, the complexity of attention can be reduced from square in length to\nlinear, provided an approximation is acceptable. But it is clear from Table 1 that each ﬁeld starts\nwith a b ·nh ·l term: the b ·nh ·l ·dk, or alternatively b ·l ·dmodel cost cannot be avoided. Indeed,\nthe activations before each layer are already of the sizeb ·l ·dmodel, so the memory use of the whole\nmodel with nl layers is at least b ·l ·dmodel ·nl. Even worse: inside the feed-forward layers of\nTransformer this goes up to b ·l ·dff ·nl. In a big Transformer it is usual to set dff = 4K and\nnl = 16so with l = 64K this again would use an impractical 16GB of memory\nIn this section, we show how to reduce this cost by ﬁrst dealing with the nl part of the term using\nreversible layers and then showing how chunking can allow us to handle the dff problem. The\neffects of each of these approaches on memory and time complexity are summarized in Table 3.\nRevNets. Reversible residual networks were introduced by Gomez et al. (2017) where it was shown\nthat they can replace ResNets for image classiﬁcation. The main idea is to allow the activations at\nany given layer to be recovered from the activations at the following layer, using only the model\nparameters. Rather than having to checkpoint intermediate values for use in the backward pass,\nlayers can be reversed one-by-one as back-propagation proceeds from the output of the network to\nits input. Whereas a normal residual layer performs a function x ↦→y that operates on a single input\nand produces a single output and has the form y = x + F(x), a reversible layer works on pairs of\ninputs/outputs: (x1, x2) ↦→(y1, y2), and follows the equations:\ny1 = x1 + F(x2) y2 = x2 + G(y1) (7)\nA layer can be reversed by subtracting (rather than adding) the residuals:\nx2 = y2 −G(y1) x1 = y1 −F(x2) (8)\nReversible Transformer.We apply the RevNet idea to the Transformer by combining the attention\nand feed-forward layers inside the revnet block. In the notation above, F becomes an attention layer\nwhile G becomes the feed-forward layer. Note that Layer Normalization (Ba et al., 2016) is moved\ninside the residual blocks.\nY1 = X1 + Attention(X2) Y2 = X2 + FeedForward(Y1) (9)\nThe reversible Transformer does not need to store activations in each layer and so gets rid of the nl\nterm. In Section 5 we show that it performs the same as the normal Transformer when using the\nsame number of parameters; we achieve this by having both x1 and x2 have size dmodel.\nChunking. While reversibility covers the nl term, the thicker layers can still use a lot of memory.\nThe feed-forward layer in particular can use intermediate vectors of dimensionality dff = 4K or\nhigher. However, computations in feed-forward layers are completely independent across positions\nin a sequence, so the computation can be split into c chunks:\nY2 =\n[\nY (1)\n2 ; . . .; Y (c)\n2\n]\n=\n[\nX(1)\n2 + FeedForward(Y (1)\n1 ); . . .; X(c)\n2 + FeedForward(Y (c)\n1 )\n]\n(10)\nThis layer is typically batched by performing operations for all positions in parallel, but operating\non one chunk at a time can reduce memory. The reverse computation in (8) and the backward pass\nare also chunked. In addition to the feed-forward layers, for models with large vocabulary (more\nthan dmodel word types) we also chunk the log-probabilities at the output and calculate the loss for\nsections of the sequence at a time.\nChunking, large batches and parameter reuse. With chunking and reversible layers the memory\nwe use for activations in the whole network is independent of the number of layers. The same is\nnot true for parameters though as their number grows with the number of layers. This problem is\nremedied though because we can swap layer parameters to and from CPU memory when this layer\nis not computing. In a standard Transformer this would be inefﬁcient because memory transfer to\nCPU is slow. The batch size multiplied by length in Reformer is much larger though and therefore\nthe amount of compute done with the parameters amortizes the cost of their transfer.\n6\nPublished as a conference paper at ICLR 2020\nTable 3: Memory and time complexity of Transformer variants. We writedmodel and dff for model\ndepth and assume dff ≥dmodel; b stands for batch size, l for length, nl for the number of layers.\nWe assume nc = l/32 so 4l/nc = 128and we write c = 1282.\nModel Type Memory Complexity Time Complexity\nTransformer max(bldff , bnhl2)nl (bldff + bnhl2)nl\nReversible Transformer max(bldff , bnhl2) ( bnhldff + bnhl2)nl\nChunked Reversible Transformer max(bldmodel, bnhl2) ( bnhldff + bnhl2)nl\nLSH Transformer max(bldff , bnhlnrc)nl (bldff + bnhnrlc)nl\nReformer max(bldmodel, bnhlnrc) ( bldff + bnhnrlc)nl\n4 R ELATED WORK\nThe Transformer model introduced in (Vaswani et al., 2017) has been used widely in natural lan-\nguage tasks and further extended to model diverse data such as music scores (Huang et al., 2018),\nand images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has\nbeen applied successfully in the self-supervised training of extremely large language models (Devlin\net al., 2018; Radford et al., 2019).\nGiven the enormous computational requirements of state of the art sequence models, there has been\nincreasing interest in ﬁnding methods to reduce the memory footprint and computational require-\nments of Transformer models. In addition to standard methods such as precision reduction and\ngradient checkpointing (Sohoni et al., 2019), more efﬁcient versions of the Transformer model’s\nself-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored.\nIn particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the\nsparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of atten-\ntion. Using product-key attention to increase the key space has also been used to reduce memory\nrequirements in the feed-forward layers with no loss in performance (Lample et al., 2019).\nLocality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer\nattention layers before. But previous work using external memory with neural networks has dealt\nwith memories of large sizes. The original implementation of memory networks (Weston et al.,\n2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size\nin the millions. The cost of doing so is that the memory must be ﬁxed prior to training. Moreover,\nsince during the beginning of training the model is unlikely to query the memory correctly, strong\nsupervision is used to encourage the model to query memory locations that are useful. These hints\nare either given as additional supervising information by the task or determined heuristically as in\nHill et al. (2015). The requirement that the memory be ﬁxed before has been removed in Santoro\net al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper\nconsidered memory lookups with approximate nearest neighbors including both LSH and random\nkd-trees, but only for lookups in external memory.\n5 E XPERIMENTS\nIn this section we present experimental results demonstrating the techniques described above. We\nanalyze the techniques one-by-one to make clear which combinations have impact on performance.\nWe start by showing that reversible layers and shared query-key spaces do not impact performance,\nthen proceed to analyze hashing attention and ﬁnally the full Reformer model.\nWe ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant\nof enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for\nour ablations so as to make it tractable to compare with the regular Transformer, which has high\nmemory usage and performs fullO(l2) attention. All experiments havedmodel = 1024, dff = 4096,\nnheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer\n& Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German\ntranslation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\n7\nPublished as a conference paper at ICLR 2020\nFigure 3: Effect of shared query-key space (left) and reversibility (right) on performance on enwik8\nand imagenet64 training. The curves show bits per dim on held-out data.\nTable 4: BLEU scores on newstest2014 for WMT English-German (EnDe). We additionally report\ndetokenized BLEU scores as computed by sacreBLEU (Post, 2018).\nsacreBLEU\nModel BLEU Uncased3 Cased4\nVaswani et al. (2017), base model 27.3\nVaswani et al. (2017), big 28.4\nOtt et al. (2018), big 29.3\nReversible Transformer (base, 100K steps) 27.6 27.4 26.9\nReversible Transformer (base, 500K steps, no weight sharing) 28.0 27.9 27.4\nReversible Transformer (big, 300K steps, no weight sharing) 29.1 28.9 28.4\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made\npublicly available.2\nEffect of sharing QK. We ﬁrst consider the effect of shared-QK attention on a regular Transformer\nmodel. Shared-QK attention sets kj = qj\n∥qj∥ and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves\nfor both regular and shared-QK attention. A shared query-key space does not perform worse than\nregular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not\nsacriﬁcing accuracy by switching to shared-QK attention.\nEffect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Trans-\nformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have\nidentical parameter counts, and the learning curves likewise appear to be nearly the same. These\nresults show that the memory savings in the reversible Transformer do not come at the expense of\naccuracy.\nReversible layers in machine translation. We also evaluate reversible layers in the context of an\nencoder-decoder Transformer model for machine translation from English to German. We start by\nmaking both the encoder and the decoder fully reversible in the Transformer-base architecture, and\n2https://github.com/google/trax/tree/master/trax/models/reformer\n3BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\n4BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\n8\nPublished as a conference paper at ICLR 2020\nFigure 4: LSH attention performance as a function of hashing rounds on imagenet64.\nFigure 5: Left: LSH attention performance as a function of number of layers on enwik8. Right:\nSpeed of attention evaluation as a function of input length for full- and LSH- attention.\nsee that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K\nsteps. We also evaluate training for a greater number of steps and with a larger model. Reformer\nmodels are very memory-efﬁcient, so for the latter two experiments we do not need to save mem-\nory by sharing embedding and output projection weight matrices throughout the model. Results\nare shown in Table 4. We do not apply LSH attention in this setting because examples are single\nsentences, and sentences tend to be relatively short. Our typical LSH attention conﬁguration uses\nchunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all\nshorter than 128 tokens.\nLSH attention in Transformer. LSH attention is an approximation for full attention that, as evi-\ndenced in Figure 4, becomes more accurate as the number of hashes increases. At nrounds = 8, it\nalready almost matches full attention. The computational cost of a model grows with the number\nof hashes, so this hyperparameter can be adjusted depending on the available compute budget. Ad-\nditionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more\naccurate results. On the right half of Figure 5, we plot the speed of different attention types vs. the\nsequence length, while holding the total number of tokens ﬁxed. We see that while regular attention\nbecomes slower at longer sequence length, LSH attention speed remains ﬂat.\nLarge Reformer models. To verify that the Reformer can indeed ﬁt large models on a single core\nand train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64.\nAs can be seen in Figure 5, these models ﬁt into memory and train. We were not able to train Trans-\nformer baselines in this case as they are too slow and memory-hungry, but we see clear improvement\nwith the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate\nof 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer\nwith further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set.\n9\nPublished as a conference paper at ICLR 2020\n6 C ONCLUSION\nReformer combines the modeling capacity of a Transformer with an architecture that can be executed\nefﬁciently on long sequences and with small memory use even for models with a large number of\nlayers. We believe that this will help large, richly-parameterized Transformer models become more\nwidespread and accessible. Also, the ability to handle long sequences opens the way for the use\nof the Reformer on many generative tasks. In addition to generating very long coherent text, the\nReformer can bring the power of Transformer models to other domains like time-series forecasting,\nmusic, image and video generation.\nREFERENCES\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. CoRR, abs/1808.04444, 2018. URL http:\n//arxiv.org/abs/1808.04444.\nAlexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya P. Razenshteyn, and Ludwig Schmidt. Practical\nand optimal LSH for angular distance. CoRR, abs/1509.02897, 2015. URL http://arxiv.\norg/abs/1509.02897.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question\nanswering with memory networks. CoRR, abs/1506.02075, 2015. URL http://arxiv.org/\nabs/1506.02075.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, and Yoshua Ben-\ngio. Hierarchical memory networks. arXiv preprint arXiv:1605.07427, 2016.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. URL https://openai.com/blog/sparse-transformers, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL\nhttp://arxiv.org/abs/1810.04805.\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-\nwork: Backpropagation without storing activations. InAdvances in neural information processing\nsystems, pp. 2214–2224, 2017.\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading\nchildren’s books with explicit memory representations. CoRR, abs/1511.02301, 2015. URL\nhttp://arxiv.org/abs/1511.02301.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M Dai, Matthew D Hoffman, and Douglas Eck. Music transformer: Generating music with\nlong-term structure. arXiv preprint arXiv:1809.04281, 2018.\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ´e\nJ´egou. Large memory layers with product keys. CoRR, abs/1907.05242, 2019. URL http:\n//arxiv.org/abs/1907.05242.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long sequences. CoRR, abs/1801.10198, 2018.\nURL http://arxiv.org/abs/1801.10198.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on Machine Translation: Research Papers,\npp. 1–9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi:\n10.18653/v1/W18-6301. URL https://www.aclweb.org/anthology/W18-6301.\n10\nPublished as a conference paper at ICLR 2020\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku.\nImage transformer. CoRR, abs/1802.05751, 2018. URL http://arxiv.org/abs/1802.\n05751.\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference\non Machine Translation: Research Papers, pp. 186–191, Belgium, Brussels, October 2018. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\nW18-6319.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nJack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex\nGraves, and Timothy P Lillicrap. Scaling memory-augmented neural networks with sparse reads\nand writes. In Advances in Neural Information Processing Systems, (NIPS), 2016.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. CoRR, abs/1906.05909, 2019. URL http:\n//arxiv.org/abs/1906.05909.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. One-\nshot learning with memory-augmented neural networks. CoRR, abs/1605.06065, 2016. URL\nhttp://arxiv.org/abs/1605.06065.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nCoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hecht-\nman. Mesh-tensorﬂow: Deep learning for supercomputers. CoRR, abs/1811.02084, 2018. URL\nhttp://arxiv.org/abs/1811.02084.\nNimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christo-\npher R´e. Low-memory neural network training: A technical report.CoRR, abs/1904.10631, 2019.\nURL http://arxiv.org/abs/1904.10631.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive atten-\ntion span in transformers. CoRR, abs/1905.07799, 2019a. URL http://arxiv.org/abs/\n1905.07799.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herv´e J´egou, and Armand Joulin. Aug-\nmenting self-attention with persistent memory. CoRR, abs/1907.01470, 2019b. URL http:\n//arxiv.org/abs/1907.01470.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, 2017. URL http:\n//arxiv.org/abs/1706.03762.\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks.CoRR, abs/1410.3916, 2014.\nURL http://arxiv.org/abs/1410.3916.\n11\nPublished as a conference paper at ICLR 2020\nA M ULTI -ROUND LSH ATTENTION\nIn this section we describe in more detail the multi-hash version of our LSH attention mechanism.\nWe ﬁrst repeat Equation (3) from the main text, which describes a general formulation of attention\nwith sparsity:\noi =\n∑\nj∈˜Pi\nexp (qi ·kj −m(j, Pi) −z(i, Pi)) vj where m(j, Pi) =\n{∞ if j /∈Pi\n0 otherwise (3)\nIn the multi-round case, a query positioni can attend to key positions Pi as deﬁned in (6), which we\nalso repeat here:\nPi =\nnrounds⋃\nr=1\nP(r)\ni where P(r)\ni =\n{\nj : h(r)(qi) =h(r)(qj)\n}\n(6)\nFor batching purposes, attention is performed on chunks of sorted queries/keys:\n˜P(r)\ni =\n{\nj :\n⌊\ns(r)\ni\nm\n⌋\n−1 ≤\n⌊\ns(r)\nj\nm\n⌋\n≤\n⌊\ns(r)\ni\nm\n⌋}\n(11)\nCombining (3) and (6) gives:\noi =\n∑\nj∈˜Pi\nexp (qi ·kj −m(j, Pi) −z(i, Pi)) vj (12)\n=\nnrounds∑\nr=1\nexp\n(\nz(i, P(r)\ni ) −z(i, Pi)\n) ∑\nj∈˜P(r)\ni\n1\nNi,j\nexp\n(\nqi ·kj −m(j, P(r)\ni ) −z(i, P(r)\ni )\n)\nvj\n(13)\n=\nnrounds∑\nr=1\nexp\n(\nz(i, P(r)\ni ) −z(i, Pi)\n)\no(r)\ni (14)\no(r)\ni =\n∑\nj∈˜P(r)\ni\nexp\n(\nqi ·kj −m(r)\ni,j −z(i, P(r)\ni )\n)\nvj (15)\nwhere Ni,j =\n⏐⏐⏐\n{\nr′: j ∈P(r′)\ni\n}⏐⏐⏐ and m(r)\ni,j =\n\n\n\n∞ if j /∈P(r)\ni\n105 if i = j\nlog Ni,j otherwise\n(16)\nEach round of LSH attention produces a vector o(r)\ni that can be computed independently from other\nrounds, except for the inclusion of a termNi,j to avoid double-counting elements when constructing\nthe union of P(r)\ni sets. In our implementation we fold the Ni,j factor into the masking term m(r)\ni,j .\nWe also modify m(r)\ni,j to introduce a special case for i = j. This case is added because causal\nmasking in a standard Transformer allows position i to attend to itself, which is not desirable in a\nshared-QK formulation. We set the mask to a large but ﬁnite value to disallow attention-in-place,\nexcept in the situation where a token has no other valid attention targets. For example, the ﬁrst token\nin a sequence attends only to itself, because no prior context is available.\n12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7032643556594849
    },
    {
      "name": "Computer science",
      "score": 0.618776798248291
    },
    {
      "name": "Locality",
      "score": 0.5169874429702759
    },
    {
      "name": "Residual",
      "score": 0.5073558688163757
    },
    {
      "name": "Hash function",
      "score": 0.47950416803359985
    },
    {
      "name": "Locality-sensitive hashing",
      "score": 0.4730854630470276
    },
    {
      "name": "Algorithm",
      "score": 0.3566127419471741
    },
    {
      "name": "Engineering",
      "score": 0.17084908485412598
    },
    {
      "name": "Electrical engineering",
      "score": 0.14872807264328003
    },
    {
      "name": "Voltage",
      "score": 0.12810590863227844
    },
    {
      "name": "Hash table",
      "score": 0.11724814772605896
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}