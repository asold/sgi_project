{
  "title": "ER-Test: Evaluating Explanation Regularization Methods for Language Models",
  "url": "https://openalex.org/W4385573428",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2797377455",
      "name": "Brihi Joshi",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2183297351",
      "name": "Aaron Chan",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2112844620",
      "name": "Zi-Yi Liu",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2561852407",
      "name": "Shaoliang Nie",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2164522516",
      "name": "Maziar Sanjabi",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2971547511",
      "name": "Hamed Firooz",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3206843525",
    "https://openalex.org/W2951576127",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W3165941178",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3166727371",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1902674502",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3216147161",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2951071799",
    "https://openalex.org/W4221140922",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4205216562",
    "https://openalex.org/W3173650485",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W3103035585",
    "https://openalex.org/W2956090150",
    "https://openalex.org/W4249736682",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4285215850",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W2979445021",
    "https://openalex.org/W4225881819",
    "https://openalex.org/W3103136066",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W3094173182",
    "https://openalex.org/W2996507500",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W3139145960",
    "https://openalex.org/W3101118235",
    "https://openalex.org/W3034340683",
    "https://openalex.org/W3173848082",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W3105928338",
    "https://openalex.org/W4226429370",
    "https://openalex.org/W3213444437",
    "https://openalex.org/W4221149036",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W3049254243",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W4289367650",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3040711047",
    "https://openalex.org/W3035215724",
    "https://openalex.org/W4308900254",
    "https://openalex.org/W3035441470",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3212603771",
    "https://openalex.org/W3110300144",
    "https://openalex.org/W3034282334",
    "https://openalex.org/W4287691524",
    "https://openalex.org/W4223431595",
    "https://openalex.org/W2962833140"
  ],
  "abstract": "By explaining how humans would solve a given task, human rationales can provide strong learning signal for neural language models (NLMs). Explanation regularization (ER) aims to improve NLM generalization by pushing the NLM‚Äôs machine rationales (Which input tokens did the NLM focus on?) to align with human rationales (Which input tokens would humans focus on). Though prior works primarily study ER via in-distribution (ID) evaluation, out-of-distribution (OOD) generalization is often more critical in real-world scenarios, yet ER‚Äôs effect on OOD generalization has been underexplored.In this paper, we introduce ER-Test, a framework for evaluating ER models‚Äô OOD generalization along three dimensions: unseen datasets, contrast set tests, and functional tests. Using ER-Test, we comprehensively analyze how ER models‚Äô OOD generalization varies with the rationale alignment criterion (loss function), human rationale type (instance-level v/s task-level), number and choice of rationale-annotated instances, and time budget for rationale annotation. Across two tasks and six datasets, we show that ER has little impact on ID performance but yields large OOD performance gains, with the best ER criterion being task-dependent. Also, ER can improve OOD performance even with task-level or few human rationales. Finally, we find that rationale annotation is more time-efficient than label annotation for improving OOD performance. Our results with ER-Test help demonstrate ER‚Äôs utility and establish best practices for using ER effectively.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3315‚Äì3336\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nER-T EST : Evaluating Explanation\nRegularization Methods for Language Models\nBrihi Joshi‚ô£‚àó Aaron Chan‚ô£‚àó Ziyi Liu‚ô£‚àó\nShaoliang Nie‚ô¢ Maziar Sanjabi‚ô¢ Hamed Firooz‚ô¢ Xiang Ren‚ô£\n‚ô£University of Southern California ‚ô¢Meta AI\n{brihijos, chanaaro, zliu2803, xiangren}@usc.edu\n{snie, maziars, mhfirooz}@fb.com\nAbstract\nBy explaining how humans would solve a\ngiven task, human rationales can provide strong\nlearning signal for neural language models\n(NLMs). Explanation regularization (ER) aims\nto improve NLM generalization by pushing the\nNLM‚Äôs machine rationales (Which input tokens\ndid the NLM focus on?) to align with human\nrationales (Which input tokens would humans\nfocus on?). Though prior works primarily study\nER via in-distribution (ID) evaluation, out-of-\ndistribution (OOD) generalization is often more\ncritical in real-world scenarios, yet ER‚Äôs ef-\nfect on OOD generalization has been underex-\nplored. In this paper, we introduce ER-T EST ,\na framework for evaluating ER models‚Äô OOD\ngeneralization along three dimensions: unseen\ndatasets, contrast set tests, and functional tests.\nUsing ER-T EST , we comprehensively analyze\nhow ER models‚Äô OOD generalization varies\nwith the rationale alignment criterion (loss func-\ntion), human rationale type (instance-level vs.\ntask-level), number and choice of rationale-\nannotated instances, and time budget for ra-\ntionale annotation. Across two tasks and six\ndatasets, we show that ER has little impact on\nID performance but yields large OOD perfor-\nmance gains, with the best ER criterion being\ntask-dependent. Also, ER can improve OOD\nperformance even with task-level or few hu-\nman rationales. Finally, we find that rationale\nannotation is more time-efficient than label an-\nnotation for improving OOD performance. Our\nresults with ER-T EST help demonstrate ER‚Äôs\nutility and establish best practices for using ER\neffectively.\n1 Introduction\nNeural language models (NLMs) have achieved\nstate-of-the-art performance on a broad array of\nnatural language processing (NLP) tasks (Devlin\net al., 2018; Liu et al., 2019). Even so, NLMs‚Äô rea-\nsoning processes are notoriously opaque (Rudin,\n‚àóEqual contribution.\nFigure 1: Explanation Regularization (ER). ER improves\nmodel generalization on NLP tasks by pushing the model‚Äôs\nmachine rationales (Which input tokens did the model focus\non?) to align with human rationales (Which input tokens would\nhumans focus on?) (Sec. 2).\n2019; Doshi-Velez and Kim, 2017; Lipton, 2018),\nwhich has spurred significant interest in designing\nalgorithms to automatically explain NLM behav-\nior (Denil et al., 2014; Sundararajan et al., 2017;\nCamburu et al., 2018; Rajani et al., 2019; Luo et al.,\n2021). Most of this work has focused on rationale\nextraction, which explains a NLM‚Äôs output on a\ngiven task instance by highlighting the input tokens\nthat most influenced the output (Denil et al., 2014;\nSundararajan et al., 2017; Li et al., 2016; Jin et al.,\n2019; Lundberg and Lee, 2017; Chan et al., 2022).\nRecent studies have investigated how machine\nrationales outputted by rationale extraction algo-\nrithms can be utilized to improve NLM decision-\nmaking (Hase and Bansal, 2021; Hartmann and\nSonntag, 2022). Among these prior works, the\nmost common paradigm is explanation regulariza-\ntion (ER), which aims to improve NLM by regu-\nlarizing the NLM to yield machine rationales that\nalign with human rationales(Fig. 1) (Ross et al.,\n2017; Huang et al., 2021; Ghaeini et al., 2019;\nZaidan and Eisner, 2008; Kennedy et al., 2020;\nRieger et al., 2020; Liu and Avci, 2019). Human ra-\ntionales can be created by annotating each training\ninstance individually (Lin et al., 2020; Camburu\net al., 2018; Rajani et al., 2019) or by applying\ntask-level human priors across all training instances\n(Rieger et al., 2020; Ross et al., 2017; Liu and Avci,\n2019).\n3315\nThough prior works primarily evaluate ER mod-\nels‚Äô in-distribution (ID) generalization, the results\nare mixed, and it is unclear when ER is actually\nhelpful. Furthermore, out-of-distribution (OOD)\ngeneralization is often more crucial in real-world\nsettings (Chrysostomou and Aletras, 2022; Ruder,\n2021), yet ER‚Äôs impact on OOD generalization has\nbeen underexplored (Ross et al., 2017; Kennedy\net al., 2020). In particular, due to the lack of unified\ncomparison of different works using ER, little is un-\nderstood about how OOD performance is affected\nby major design choices in building an ER pipeline,\nlike the rationale alignment criterion (i.e., loss func-\ntion), human rationale type (instance-level vs. task-\nlevel), number and choice of rationale-annotated\ninstances, and time budget for rationale annotation.\nIn light of this, we propose ER-T EST 1(Fig. 2), a\nframework for evaluating ER methods‚Äô OOD gen-\neralization via: (1) unseen datasets, (2) contrast\nset tests, and (3) functional tests. For (1), ER-\nTEST tests ER models‚Äô performance on datasets\nbeyond their training distribution. For (2), ER-\nTEST tests ER models‚Äô performance on real-world\ndata instances that are semantically perturbed. For\n(3), ER-T EST tests ER models‚Äô performance on\nsynthetic data instances created to capture specific\nlinguistic capabilities.\nUsing ER-T EST , we study four questions: (A)\nWhich rationale alignment criteriaare most effec-\ntive? (B) Is ER effective with task-level human\nrationales? (C) How is ER affected by the num-\nber/choice of rationale-annotated instances? (D)\nHow does ER performance vary with the ratio-\nnale annotation timebudget? For two text clas-\nsification tasks, we show that ER has little im-\npact on ID performance but yields large gains on\nOOD performance, with the best ER criteria being\ntask-dependent (Sec. 5.2). Furthermore, ER can\nimprove OOD performance even with distantly-\nsupervised (Sec. 5.3) or few (Sec. 5.4) human ra-\ntionales. Finally, we find that rationale annotation\nyields more improvements than label annotation,\nspecifically with limited time-budget for annotat-\ning (Sec. 5.5). ER-T EST results further show ER‚Äôs\nutility and establish best practices for using ER\neffectively.\n2 Explanation Regularization (ER)\nGiven an NLM for an NLP task, the goal of ER\nis to improve NLM generalization on the task by\n1Code available at https://github.com/INK-USC/er-test.\npushing the NLM‚Äôs (extractive) machine rationales\n(Which input tokens did the NLM focus on?) to\nalign with human rationales ( Which input tokens\nwould humans focus on?). The hope is that this\ninductive bias encourages the NLM to solve the\ntask in a manner that follows humans‚Äô reasoning\nprocess.\nLet Fbe an NLM for M-class text classification.\nFusually has a BERT-style architecture (Devlin\net al., 2018), consisting of a Transformer encoder\n(Vaswani et al., 2017) followed by a linear layer\nwith softmax classifier. Let xi = [ xt\ni]n\nt=1 be the\nn-token input sequence (e.g., a sentence) for task\ninstance i. Let yi denote F‚Äôs predicted class for\nxi. Given F, xi, and yi, the goal of rationale ex-\ntraction is to output machine rationale ri = [rt\ni]n\nt=1,\nsuch that each 0 ‚â§rt\ni ‚â§1 is an importance score\nindicating how strongly token xt\ni influenced Fto\npredict class yi (Luo et al., 2021). Let Gdenote a\nrationale extractor, such that ri = G(F,xi,yi).\nGcan also be used to compute machine ratio-\nnales w.r.t. other classes besides yi (e.g., target\nclass Àôyi). Let ÀÜ ri denote the machine rationale for\nxi w.r.t. Àôyi. Given ÀÜ ri obtained via Gand F, many\nworks have explored ER, in which Fis regular-\nized such that ÀÜ ri aligns with human rationale Àô ri\n(Zaidan and Eisner, 2008; Lin et al., 2020; Rieger\net al., 2020; Ross et al., 2017). Àô ri can either be\nhuman-annotated for individual instances, or gen-\nerated via human-annotated lexicons for a given\ntask. Typically, Àô ri is a binary vector, where ones\nand zeros indicate positive (important) and negative\n(unimportant) tokens, respectively.\nWe formalize the ER loss as: LER = Œ¶(ÀÜ ri,Àô ri),\nwhere Œ¶ is an ER criterion measuring alignment\nbetween ÀÜ ri and Àô ri. Thus, the full learning objective\nis: L= Ltask + ŒªERLER, where Ltask is the task\nloss (e.g., cross-entropy loss) ŒªER ‚ààR is the ER\nstrength (i.e., loss weight) for LER. While there are\nmany choices for Œ¶, it is unclear how Œ¶ impacts\ntraining and when certain Œ¶ should be preferred.\nAlso, as a baseline, let FNo-ER denote an NLM that\nis trained without ER, such that L= Ltask.\n3 ER-T EST\nExisting works primarily evaluate ER models via\nID generalization (Zaidan and Eisner, 2008; Lin\net al., 2020; Rieger et al., 2020; Liu and Avci, 2019;\nRoss et al., 2017; Huang et al., 2021; Ghaeini et al.,\n2019; Kennedy et al., 2020), though a small num-\nber of works have done auxiliary evaluations of\nOOD generalization (Ross et al., 2017; Kennedy\n3316\nüêµ\nNolan is a great director.T ext : \nLabel : pos\n@deltaairways I love\n<3 Delta !!!!!\nT ext : \nLabel : pos\nOOD\nID\n Nolan is a great director.T ext : \nLabel : pos\nNolan is an awful\ndirector .\nSometimes, Nolan is a\nterrible director .\nneg\nneg\nPERTURBED TEXT\nNolan is a very good\ndirector .\nT ext : \nLabel : pos\nVocabulary Tests\nNolan is not a great\ndirector .\nT ext : \nLabel : \nLogic Tests\nneg\nNloan is a great\ndirector . @hjhbkghkv\nT ext : \nLabel : \nRobustness Tests\npos\nRajeev is a great\ndirector .\nT ext : \nLabel : \nEntity Tests\npos\n1. Unseen Dataset Tests 2. Contrast Set Tests 3. Functional Tests üéØ\nüôà üòàüôÇ‚û° üêµ‚û° \nFigure 2: ER-T EST Framework. While existing ER works focus on ID generalization, ER-T EST evaluates ER‚Äôs\nOOD generalization w.r.t. (A) unseen datasets, (B) contrast set tests, and (C) functional tests (Sec. 3, A.2.3).\net al., 2020; Rieger et al., 2020). However, these\nOOD evaluations have been relatively small-scale,\nonly covering a narrow range of OOD generaliza-\ntion aspects, ER criteria, tasks, and datasets. As\na result, little is understood about ER‚Äôs impact on\nOOD generalization. To address this gap, we pro-\npose ER-T EST (Fig. 2), a framework for designing\nand evaluating ER models‚Äô OOD generalization\nalong three dimensions: (1) unseen dataset tests;\n(2) contrast set tests; and (3) functional tests.\nLet Dbe an M-class text classification dataset,\nwhich we call the ID dataset. Assume Dcan be\npartitioned into training set Dtrain, development set\nDdev, and test set Dtest, where Dtest is the ID test set\nfor D. After training Fon Dtrain with ER, we mea-\nsure F‚Äôs ID generalization via task performance on\nDtest and F‚Äôs OOD generalization via (1)-(3).\n3.1 Unseen Dataset Tests\nFirst, we evaluate OOD generalization w.r.t. un-\nseen datasets (Fig. 2A). Besides D, suppose we\nhave datasets {ÀúD(1), ÀúD(2),...}for the same task as\nD. Each ÀúD(i) has its own train/dev/test sets and\ndistribution shift from D. After training Fwith\nER on Dtrain and hyperparameter tuning on Ddev,\nwe measure F‚Äôs performance on each OOD test\nset ÀúD(i)\ntest. This tests ER‚Äôs ability to help Flearn\ngeneral (i.e., task-level) knowledge representations\nthat can (zero-shot) transfer across datasets.\n3.2 Contrast Set Tests\nSecond, we evaluate OOD generalization w.r.t. con-\ntrast set tests (Fig. 2B). Dataset annotation artifacts\n(Gururangan et al., 2018) can cause NLMs to learn\nspurious decision rules that work on the test set but\ndo not capture linguistic abilities that the dataset\nwas designed to assess. Thus, we testFon contrast\nsets (Gardner et al., 2020), which are constructed\nby manually perturbing the test instances of real-\nworld datasets to express counterfactual meanings.\nContrast set tests reveal the dataset‚Äôs intended deci-\nsion boundaries and if Fhas learned undesirable\ndataset-specific shortcuts. Given ÀúD(i)\ntest, we can con-\nvert ÀúD(i)\ntest to contrast set ÀúC(i)\ntest using various types of\nsemantic perturbation, such as inversion (e.g., ‚Äúbig\ndog‚Äù ‚Üí‚Äúsmall dog‚Äù), numerical modification (e.g.,\n‚Äúone dog‚Äù ‚Üí‚Äúthree dogs‚Äù), and entity replacement\n(e.g., ‚Äúgood dog‚Äù ‚Üí‚Äúgood cat‚Äù). However, since\ncontrast sets are built from real-world datasets, they\nprovide less flexibility in testing linguistic abilities,\nas a given perturbation type may not apply to all in-\nstances in the dataset. Note that, unlike adversarial\nexamples (Gao and Oates, 2019), contrast sets are\nnot conditioned on Fspecifically to attack F.\n3.3 Functional Tests\nThird, we evaluate OOD generalization w.r.t. func-\ntional tests (Fig. 2C). Whereas contrast sets are cre-\nated by perturbing real-world datasets, functional\ntests evaluate Fon synthetic datasets, which are\nmanually created via templates to assess specific\nlinguistic abilities (Ribeiro et al., 2020; Li et al.,\n2020). While contrast set tests focus on semantic\nabilities, functional tests consider both semantic\n(e.g., perception of word/phrase sentiment, sensi-\ntivity to negation) and syntactic ( e.g., robustness\nto typos or punctuation addition/removal) abilities.\nTherefore, functional tests trade off data realism\nfor evaluation flexibility. If ER improves F‚Äôs func-\ntional test performance for a given ability, then ER\nmay be a useful inductive bias for OOD generaliza-\ntion w.r.t. that ability. Across all tasks, ER-T EST\ncontains four general categories of functional tests:\nV ocabulary, Robustness, Logic, and Entity (Ribeiro\net al., 2020). See Sec. A.2.3 for more details.\n4 ER-T EST Design Choices\nAn ER model consists of three key components:\nrationale alignment criterion, type of human ratio-\nnales, and instance selection strategy. ER-T EST\nevaluates the design choices for each component.\n4.1 Rationale Alignment Criteria\nCompared to existing works, ER-T EST uses a\nwider range of rationale alignment criteria to eval-\n3317\nConfidence\nRandomTop k% least\nconfident\nTop k% most\nconfident\nMAE\nBCE\nMSE\nHuber\nOrder\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nSentiment Analysis\nNLI\n‚úî \n‚úî \nTasksER Criteria\nRQ1: Which rationale alignment\ncriteria are most effective?\nRQ3: How is ER affected by the\nnumber/choice of train instances\nwith human rationales?\nLexicon List\nGreat\nAwful\nTerrible\nAwesome\nNolan is a great\ndirector .Text: \nLabel: pos\nRQ2: Is ER effective with distantly\nsupervised human rationales?\nRQ4: How is ER affected by the time\ntaken to annotate human rationales?\nKLDiv‚úÖ\nNolan is a great director .pos\nNolan is a great director .\nNolan is a great director .\nOnly Label Annotation\nOnly Expl Annotation\nLabel + Expl Annotation\npos\npos\nFigure 3: Experiment Setup: We use ER-T EST to investigate four research questions. (Sec. 5).\nuate ER model generalization. This provides a\nmore comprehensive picture of ER‚Äôs impact on\nboth ID and OOD generalization, helping us un-\nderstand why and when certain criteria work well.\nTo demonstrate ER-T EST ‚Äôs utility, we consider\nsix representative rationale alignment criteria (i.e.,\nchoices of Œ¶), described below.\nMean Squared Error (MSE) is used in Liu\nand Avci (2019), Kennedy et al. (2020), and Ross\net al. (2017): Œ¶MSE(ÀÜ ri,Àô ri) = 1\nn‚à•ÀÜ ri ‚àíÀô ri‚à•2\n2.\nMean Absolute Error (MAE) is used in Rieger\net al. (2020): Œ¶MAE(ÀÜ ri,Àô ri) = 1\nn|ÀÜ ri ‚àíÀô ri|.\nBinary Cross Entropy (BCE) loss is used\nin Chan et al. (2022) and Chan et al. (2021):\nŒ¶BCE(ÀÜ ri,Àô ri) = ‚àí1\nn\n‚àën\nt=1 Àôrt\ni log(ÀÜrt\ni).\nHuber Loss (Huber, 1992) is a hybrid of MSE\nand MAE, but is still unexplored for ER. Our exper-\niments use the default Œ¥= 1 (Paszke et al., 2019).\nŒ¶Huber(ÀÜ ri,Àô ri)\n=\n{\n1\n2 Œ¶MSE(ÀÜ ri,Àô ri), Œ¶MAE(ÀÜ ri,Àô ri) <Œ¥\nŒ¥(Œ¶MAE(ÀÜ ri,Àô ri) ‚àí1\n2 Œ¥), otherwise\n(1)\nOrder Loss Recall that the human rationale\nÀô ri labels each token as positive (one) or negative\n(zero). Whereas other criteria generally push pos-\nitive/negative tokens‚Äô importance scores to be as\nhigh/low as possible, order loss (Huang et al., 2021)\nrelaxes MSE to merely enforce that all positive to-\nkens‚Äô importance scores are higher than all negative\ntokens‚Äô importance scores. This is especially useful\nif Àô ri is somewhat noisy (e.g., some positive-labeled\ntokens should not really be positive).\nŒ¶Order(ÀÜ ri,Àô ri) =\n‚àë\nÀôrt\ni=1\n(\nmin\n( ÀÜrt\ni\nmax\nÀôrt\nj=0\nÀÜrt\nj\n‚àí1,0\n))2\n(2)\nKL Divergence (KLDiv) is used by Pruthi et al.\n(2020), Chan et al. (2022), and Chan et al. (2021):\nŒ¶KLDiv(ÀÜ ri,Àô ri) = 1\nn\n‚àën\nt=1 Àôrt\ni log( Àôrt\ni /ÀÜrt\ni).\nMachine Rationale Extractors We consider\nthree types of machine rationale extractors:\ngradient-based, which computes rationales via\nF(xi)‚Äôs gradients (Sundararajan et al., 2017;\nSanyal and Ren, 2021; Shrikumar et al., 2017);\nattention-based, which computes rationales via\nF(xi)‚Äôs attention weights (Ding and Koehn, 2021;\nWiegreffe and Pinter, 2019); and learned, which\ntrains a model to compute rationales w.r.t. faith-\nfulness and/or plausibility objectives (Chan et al.,\n2022). While other rationale extractor types, such\nas perturbation-based (Li et al., 2016), can also be\nused, we focus on the first three types since they\nare relatively less compute-intensive. In our experi-\nments, we use Input*Gradient (IxG) (Denil et al.,\n2014), Attention (Attn) (Ding and Koehn, 2021),\nand UNIREX (Chan et al., 2022) as representatives\nfor these three types, respectively.\n4.2 Types of Human Rationales\nUnlike prior works, ER-T EST considers both\ninstance-level and task-level human rationales.\nInstance-Level Rationales Human rationales\nare often created by annotating each training in-\nstance individually (Lin et al., 2020; Camburu et al.,\n2018; Rajani et al., 2019). For each instance, hu-\nmans are asked to mark tokens that support the\ngold label as positive, with the remaining tokens\ncounted as negative. Here, each human rationale is\nspecifically conditioned on the input and gold la-\nbel for the given instance. However, instance-level\nrationales are expensive to obtain, given the high\nmanual effort required per instance.\nTask-Level Rationales Some works construct\ndistantly-supervised human rationales by apply-\ning task-level human priors across all training in-\nstances (Kennedy et al., 2020; Rieger et al., 2020;\nRoss et al., 2017; Liu and Avci, 2019). Given a\ntask-level token lexicon, each instance‚Äôs rationale\nis created by marking input tokens present in the\nlexicon as positive and the rest as negative, or vice\nversa. Here, rationales are not as fine-grained or\ntailored for the given dataset, but may provide a\nmore general learning signal for solving the task.\n4.3 Instance Selection Strategies\nIn real-world applications, it is often infeasible to\nannotate instance-level human rationales Àô ri for all\ntraining instances (Chiang and Lee, 2022; Kaushik\n3318\net al., 2019). Besides task-level rationales, another\napproach for addressing this issue could be to an-\nnotate only a subset Strain ‚äÇ Dtrain of training\ninstances. Given a constant budget of |Strain|=\nk\n100 |Dtrain|instances, where 0 <k< 100, our goal\nis to select Strain such that ER withStrain maximizes\nF‚Äôs task performance. While instance selection via\nactive learning is well-studied for general classi-\nfication (Schr√∂der and Niekler, 2020), this prob-\nlem has not been explored in ER. Given non-ER\nNLM FNo-ER, we use ER-T EST to compare five\nactive-learning-inspired instance selection strate-\ngies. Note that these are just basic strategies, used\nto show ER-T EST ‚Äôs utility. In practice, one could\nconsider more sophisticated strategies that account\nfor other factors like data diversity.\nRandom Sampling (Rand) constructs Strain by\nuniformly sampling |Strain|instances from D.\nLowest Confidence (LC) selects the |Strain|in-\nstances for which FNo-ER yields the lowest target\nclass confidence probability FNo-ER( Àôyi|xi) (Zheng\nand Padmanabhan, 2002).\nHighest Confidence (HC) selects the |Strain|\ninstances for which FNo-ER yields the highest target\nclass confidence probability FNo-ER( Àôyi|xi). This is\nthe opposite of LC.\nLowest Importance Scores (LIS) Given ma-\nchine rationale ÀÜ ri for FNo-ER and 0 <k‚Ä≤<100, let\nÀÜ r(k‚Ä≤)\ni denote a vector of the top-k% highest impor-\ntance scores in ÀÜ ri. With rS = (1/|ÀÜ r(k‚Ä≤)\ni |) ‚àëÀÜ r(k‚Ä≤)\ni\nas the mean score in ÀÜ r(k‚Ä≤)\ni , LIS selects the |Strain|\ninstances for which rSis lowest. This is similar to\nselecting instances with the highest ÀÜ ri entropy.\nHighest Importance Scores (HIS) Given rS,\nHIS selects the |Strain|instances for which rS is\nhighest. This is the opposite of LIS.\n5 Experiments\nNow that ER-T EST lays a foundation for evalu-\nation and design choices, we conduct a system-\natic study of the ER pipeline through four research\nquestions (Fig. 3).\nFirst, we compare different rationale alignment\ncriteria and analyse which performs better for\nwhich task. ( RQ1: Sec. 5.2). Second, we com-\npare ER pipelines with different types of available\nhuman rationales: either dense, instance-level vs.\ndistantly-supervised task-level rationales. ( RQ2:\nSec. 5.3) Third, we look into strategies on how\nto select instances on which ER should be ap-\nplied, given resource constraints on the number\nof rationale-annotated samples. ( RQ3: Sec. 5.4).\nLastly, we investigate whether ER is worth doing,\ngiven a time-budget to obtain rationale-annotated\ninstances, while comparing it methods without ER\nand the same time-budget to obtain more labelled\ndata. (RQ4: Sec. 5.5).\n5.1 Tasks and Datasets\nER-T EST uses a diverse set of text classification\ntasks. We mainly focus on sentiment analysis and\nnatural language inference (NLI), but also consider\nnamed entity recognition (NER) and hate speech\ndetection in Appendix A.5.2 First, for sentiment\nanalysis, we use SST (short movie reviews) (Socher\net al., 2013; Carton et al., 2020) as the ID dataset.\nAs OOD datasets, we use Yelp (restaurant reviews)\n(Zhang et al., 2015), Amazon (product reviews)\n(McAuley and Leskovec, 2013), and Movies (long\nmovie reviews) (Zaidan and Eisner, 2008; DeY-\noung et al., 2019). Second, for NLI, we use e-\nSNLI (Camburu et al., 2018; DeYoung et al., 2019)\nand MNLI (Williams et al., 2017) as the ID and\nOOD datasets, respectively. See Sec. A.3.1 for\nmore details about datasets, dataset-specific con-\ntrast/functional tests, and other tasks.\n5.2 RQ1: Which rationale alignment criteria\nare most effective?\nHere, we study the effectiveness of different ratio-\nnale alignment criteria specified in ER-T EST for\ntwo tasks - sentiment analysis and NLI.\nSetup. Rationale alignment criteria described in\nSection 4.1 are used to align instance-level ratio-\nnales for the train set (ID datasets SST and e-SNLI\nfor sentiment analysis and NLI tasks respectively).\nFor the NLM architecture, we use BigBird-Base\n(Zaheer et al., 2020), in order to handle input se-\nquences of up to 4096 tokens. For all results, we\nreport the mean over three seeds, as well as the stan-\ndard deviation. We use a learning rate of 2e‚àí5 and\neffective batch size of 32. Further implementation\ndetails are in Appendix A.3.3.\nResults. Tables 1 and 2, and Figure 4 summarize\nthe results for this research question.\nID Generalization. We observe (Table 1) that using\nID task performance, it is difficult to distinguish\nbetween different rationale alignment criteria, as\nall of them yield about the same task performance\nas the None baseline (for both SST and eSNLI).\nUnseen Datasets. For sentiment analysis, MAE\nyields significant gains over all other rationale\n3319\nMethods\nSentiment Analysis NLI\nIn-Distribution Out-of-Distribution In-Distribution Out-of-Distribution\nSST Amazon Yelp Movies e-SNLI MNLI\nNone 94.22 (¬±0.77) 90.72 (¬±1.36) 92.07 (¬±2.66) 89.83 (¬±6.79) 76.18 (¬±1.28) 46.15 (¬±4.38)\nIxG+MSE 94.29 (¬±0.05) 90.58 (¬±0.77) 92.17 (¬±0.64) 90.00 (¬±5.63) 78.98 (¬±1.00) 54.23 (¬±2.67)\nIxG+MAE 94.11 (¬±0.38) 92.02 (¬±0.25) 94.55 (¬±0.30) 95.50 (¬±1.32) 78.77 (¬±1.01) 52.41 (¬±4.50)\nIxG+BCE 94.15 (¬±0.53) 90.70 (¬±1.19) 91.82 (¬±2.30) 92.00 (¬±6.98) 79.07 (¬±0.83) 53.68 (¬±4.15)\nIxG+Huber 94.19 (¬±0.19) 90.43 (¬±1.45) 92.38 (¬±2.11) 91.83 (¬±3.75) 78.99 (¬±0.81) 53.97 (¬±3.11)\nIxG+Order 94.37 (¬±0.11) 89.47 (¬±2.71) 87.95 (¬±6.36) 84.50 (¬±10.15) 79.11 (¬±0.87) 55.26 (¬±3.56)\nIxG+KLDiv 94.62 (¬±0.61) 91.63 (¬±0.51) 93.55 (¬±1.69) 93.00 (¬±2.18) 73.68 (¬±4.77) 46.57 (¬±1.35)\nAttn+MSE 94.71 (¬±0.75) 91.88 (¬±0.53) 94.70 (¬±0.18) 95.83 (¬±1.15) 76.04 (¬±0.43) 48.60 (¬±2.55)\nAttn+KLDiv 94.29 (¬±0.65) 91.43 (¬±0.71) 94.58 (¬±0.51) 96.67 (¬±0.76) 77.35 (¬±0.59) 49.66 (¬±2.47)\nUNIREX 93.30 (¬±1.06) 83.27 (¬±6.43) 92.30 (¬±1.36) 93.00 (¬±0.50) 72.23 (¬±0.97) 42.92 (¬±3.35)\nTable 1: RQ1: Which rationale alignment criteria are most effective?: Metrics displayed are Accuracy (‚Üë) for sentiment\nanalysis and Macro F1 (‚Üë) for NLI. Cells highlighted in blue show significant improvement over the None baseline. (p< 0.05)\nalignment criteria. However, despite performing\nbest on SST, Order performs much worse than all\nother rationale alignment criteria. For NLI, Or-\nder loss leads to the highest performance with the\nMNLI dataset. Overall, OOD task performance\nis much better than ID at distinguishing between\nER criteria, especially showing ER‚Äôs improvement\nover None.\nER Criteria\nContrast Set\nSentiment Analysis NLI\nOriginal Contrast‚àÜ Original Contrast‚àÜ\nNone 88.39 (¬±2.05) 85.11 (¬±2.72) -3.28 46.15 (¬±4.38) 43.73 (¬±2.81) -2.42\nIxG+MSE 88.11 (¬±2.33) 86.07 (¬±2.48) -2.04 54.23 (¬±2.67) 51.95 (¬±1.21 ) -2.28IxG+MAE 91.12 (¬±0.59) 89.82 (¬±1.20)-1.30 52.41 (¬±4.50) 52.02 (¬±1.49)-0.39IxG+BCE 89.55 (¬±1.42) 87.30 (¬±4.03) -2.25 53.68 (¬±4.15) 52.37 (¬±1.42) -1.31IxG+Huber 89.20 (¬±1.67) 86.13 (¬±1.74) -3.07 53.97 (¬±3.11) 52.32 (¬±1.04) -1.65IxG+Order 86.00 (¬±5.27) 83.40 (¬±6.16) -2.60 55.26 (¬±3.56) 52.78 (¬±0.74) -2.48\nTable 2: RQ1 - Contrast Set Tests: ‚àÜ (‚Üì) is the difference\nin performance of Fbetween the contrast and original set. A\nvalue farther from 0 suggests that Fhas learnt shortcuts spe-\ncific a dataset, which are not generalizable. Cells highlighted\nin pink show least drop in performance with a contrast set.\nContrast Set Tests. We observe (Table 2) the drop\nin performance (‚àÜ) for sentiment analysis and NLI\nwhen using a contrast set designed for the given\ndataset. MAE leads to the least drop in performance\nand all methods apart fromOrder yield lower drops\nthan None. All of them also have a higher per-\nformance on the original and contrast sets. For\nsentiment analysis, we observe that Order has the\nhighest variance, and for NLI, it has the highest\ndrop in performance. We believe that some of it\ncan be attributed to the soft-ranking that is imposed\nby Order, which may be indifferent towards mi-\nnor label-changing edits, that is observed by the\ncontrast sets.\nFunctional Tests. Figure 4 demonstrates failure\nrates on functional tests. We observe that apart\nfrom the entity-based tests, rationale alignment cri-\nteria generally have lower failure rates than None.\nGenerally, all methods perform well on robustness-\nFigure 4: RQ1 - Funtional Tests: Shown here are Failure\nRates (out of 100) ( ‚Üì) for four functional tests (See. Table\n11). Each rationale alignment criterion corresponds to the IxG\nrationale extractor.\nbased tests, as they have lower failure rates, with\norder loss having the least. What is important to\nnote is the significant improvement by Order loss in\nvocabulary-based tests than None, even though all\nof the methods are exposed to the same training set\ninstances. We hypothesize that the biases induced\nby ER alleviates the shortcuts learnt by None, also\ndemonstrated by an overall lower failure rate of\nrationale alignment criteria.\n5.3 RQ2: Is ER effective with distantly\nsupervised human rationales?\nAs described in Section 4.2, obtaining instance-\nlevel rationales are expensive to obtain. In this\nresearch question, we compare and contrast differ-\nences between instance-level and task-level human\nrationales with ER-T EST , on the Sentiment Analy-\nsis task.\nSetup. In this RQ, we use the same setup for\ninstance-level rationales as described in RQ 1 (Sec.\n5.2). For generating task-level rationales, we merge\nthe AFINN (Nielsen, 2011) and SenticNet (Cam-\nbria et al., 2020) lexicons.\nResults. We show ID and OOD performance of\ninstance and task-based rationales in Table 3. Al-\n3320\nRationale Type Criteria\nSentiment Analysis\nIn-Distribution Out-of-Distribution\nSST Amazon Yelp Movies\nNone - 94.22 ( ¬±0.77) 90.72 (¬±1.36) 92.07 (¬±2.66) 89.83 (¬±6.79)\nInstance-levelIxG+MAE 94.69 (¬±0.93) 91.28 (¬±0.74) 93.28 (¬±2.16) 94.83 (¬±2.08)IxG+Huber 94.27 (¬±0.84) 91.17(¬±1.50) 93.40 (¬±1.45) 91.17 (¬±3.33)\nTask-levelIxG+MAE 94.53 (¬±0.60)92.02 (¬±0.45) 94.10 (¬±0.91) 95.83 (¬±1.26)IxG+Huber 93.81 (¬±0.47) 91.05 (¬±1.45) 93.88 (¬±0.41) 94.00 (¬±0.50)\nTable 3: RQ2: Instance-level vs. Task-level rationales. (Sec.\n5.3). All values are Accuracy (‚Üë).\nthough ID performance (SST) is comparable for\nboth rationale types, task-level rationales lead to\nminor improvements in OOD cases (Amazon, Yelp,\nMovies). We believe one of the reasons this boost\nin performance is observed is because the lexicon\nlist used to generate rationales are task-specific\n(for sentiment analysis) and dataset-agnostic, and\ncontain more general sentiment-related terms that\nare also present in OOD datasets, unlike instance-\nbased rationales that contain nuances. However,\nhaving general lexicons that are not nuanced pro-\nhibits task-based models to generalise to harder\ninstances which may not be resolved by lexicons\nthemselves. This is observed by poor performance\nof task-based rationales in both contrast set and\nfunctional tests, when compared to instance-based\nrationales. (More in Sec. A.5.1)\n5.4 RQ3: How is ER affected by the\nnumber/choice of training instances with\nhuman rationales?\nSo far, we have looked into ER when rationales\n(whether instance- or task-based) are available for\nall of the instances. However, it is important to\ndetermine which instances should be prioritized\nfor human rationales annotation when annotation\nresources are limited. In this RQ, we investigate\nthe performance of the different instance-selection\nmethods outlined in Section 4.3, under varying\nresource constraints.\nSetup. For experiments in this RQ, we compare\nthe best performing rationale alignment criterion\n(IxG+MAE) and vary the instance budget (k) be-\ntween 5/15/50/100%. (More in Sec. A.6.1)\nResults. Table 4 demonstrates that importance-\nscore based instance selection strategies (LIS,\nHIS) generally yield better improvements in OOD\ndatasets when compared to label-confidence based\nstrategies (LC, HC). Furthermore, certain instance\nselection criterion (like LC) perform significantly\nworse as the instance budget is increased. Zooming-\ninto LIS, we compare it to random sampling and\nNon-ER settings in Figure 5. We can observe that\nk% Method\nSentiment Analysis\nIn-Distribution Out-of-Distribution\nSST Amazon Yelp Movies\nNone - 94.22 (¬±0.77) 90.72 (¬±1.36) 92.07 (¬±2.66) 89.83 (¬±6.79)100 - 94.11 ( ¬±0.38) 92.02 (¬±0.25) 94.55 (¬±0.30) 95.50 (¬±1.32)\n5 Random 94.36 (¬±0.05) 91.57 (¬±0.10) 93.36 (¬±0.15) 92.39 (¬±2.50)LC 93.14 (¬±1.97) 90.72 (¬±0.43) 93.50 (¬±0.53) 93.17 (¬±1.26)HC 94.32 (¬±0.42) 91.57 (¬±0.19) 93.03 (¬±0.81) 91.33 (¬±3.09)LIS 93.92 (¬±1.07)92.42 (¬±0.48)94.28 (¬±0.31)96.50 (¬±1.5)HIS 93.94 (¬±0.83) 90.58 (¬±0.95) 91.47 (¬±2.37) 92.00 (¬±4.58)\n50 Random 93.47 (¬±0.02) 90.28 (¬±1.42) 91.85 (¬±2.11) 89.78 (¬±5.68)LC 87.07 (¬±5.15) 78.82 (¬±20.68) 77.73 (¬±26.53) 76.67 (¬±19.08)HC 92.93 (¬±0.17) 92.15 (¬±0.36) 94.48 (¬±0.94) 91.00 (¬±6.50)LIS 93.17 (¬±0.55) 90.60 (¬±0.25) 92.72 (¬±0.53)93.50 (¬±0.87)HIS 94.23 (¬±0.65) 88.85 (¬±2.67) 91.47 (¬±1.47)93.67 (¬±1.89)\nTable 4: Sample Selection Methods: Settings mentioned\nin blue are significantly better than None settings. (See\nAppendix 17 for more details)\nfor low resource cases (5/50%), LIS leads to similar\nOOD performance to k= 100% (using all samples\nfor ER), and is always greater than Random/Non-\nER. This also shows that carefully selecting a small\nsubset of samples for rationale annotation can yield\nsame benefits like that of annotating all the sam-\nples, with a lower annotation cost, and significant\nimprovements over Random/No-ER.\n5.5 RQ4: How is ER affected by the time taken\nto annotate human rationales?\nSo far, we explored questions surrounding ER that\nassumed the ease of obtaining rationale-annotated\ninstances. However, obtaining rationales for ER\nare not only tedious, but also time-consuming. In\nthis RQ, we benchmark the time efficiency of ER\nthrough the lens of time taken to collect such data,\nwhen compared to collecting labelled data without\nrationales.\nSetup. Our setup comprises of two steps - firstly,\nestimating the time taken to annotate one instance\nusing Amazon Mechanical Turk (MTurk), followed\nby using these estimates to create training sets with\nvarying time budgets. On MTurk2 (details in A.7),\nwe devise three tasks ‚Äì one where the annotators\nhave to first select a sentiment for an instance, and\nthen highlight rationale tokens that support their\nselected sentiment (Label + Expl), one where they\nhave to highlight the rationales given a ground truth\nsentiment (Only Expl) and one baseline task where\nthey only have to label an instance with a sentiment\n(Only Label). For each of the above tasks, through\nour MTurk experiments, our 178 Turkers yielded\nmean/std times of 140.56s¬±8.45 (Only Label),\n110.31s¬±3.21 (Only Expl), and 263.10s¬±7.31\n(Label+Expl). By filtering out ‚Äòcheaters‚Äô who sub-\nmitted empty/low-effort responses, we achieved\n2https://www.mturk.com/\n3321\nFigure 5: RQ3: How is ER affected by the number/choice of training instances with human rationales?: Task Performance\n(Accuracy) vs. % of rationale-annotated data for different sample selection criteria on four sentiment analysis datasets.\nFigure 6: RQ4: How is ER affected by the time taken to annotate human rationales?: Task Performance (Accuracy) vs.\ntime budget for rationale annotation, for each kind of annotation strategy, for each of the four sentiment analysis datasets. There\nare 1000 instances in the baseline training set, and 1 hr of annotation corresponds to 42, 98 and 77 instances each for Label +\nExpl, Only Expland Only Labelannotation tasks respectively. Annotation is done on ID dataset (SST) only.\nhigh inter-annotator agreement. For Only Label\nand Label+Expl, the Fleiss‚Äô kappa scores were\n0.74 and 0.70. For Only Expland Label+Expl, the\nrationale overlap rates (Zaidan and Eisner, 2008)\nwere 0.78 and 0.66. We replicated this experiment\nin a small-scale study with nine CS students and\nobserved similar trends.\nUsing these time estimates, we devise three ex-\nperiment settings. Given a baseline labelled train-\ning set Dbase of 1000 instances and a time budget\nT, we can - 1) Add human rationales for a sub-\nset ST\nexpl of Dbase, 2) Add new instances DT\nlabel\nwith only labels to Dbase, or 3) Add new instances\nDT\nlabel+expl with labels and rationales to Dbase.\nNote that, the number of new instances added in\neach of the above experiment settings depend on\nthe time taken to annotate the Only Expl, Only\nLabel and Label + Expltasks respectively.\nResults. As we can observe in Figure 6, when\nprovided with a lower time budget for annotation\n(‚â§5 hours), annotating rationales for existing in-\nstances in the training set ( Only Expl) yield im-\nprovements over adding new instances with labels\n(and rationales), in all of the OOD datasets. How-\never, their performance saturates over time. When\nprovided with a higher time-budget, adding new in-\nstances with both labels and their rationales (Label\n+ Expl) is better than only adding labelled data with-\nout rationales (Only Label). This is even though\nLabel + Expltakes the most amount of time to\nannotate, and thus fewer instances with these anno-\ntations would be added with a given time budget. In\ngeneral, we observe that about24 hours of Only La-\nbel annotation yields the same OOD performance\nwith just 30 mins of Only Explannotation. This\nvalidates that ER not only provides improvements\nin generalization, but also does it in a time- (and\ncost-) efficient manner.\n6 Related Work\nExplanation-Based Learning Many methods\nhave been proposed for explanation-based learning\n(Hase and Bansal, 2021; Hartmann and Sonntag,\n2022), especially using human explanations (Tan,\n2022). ER, which is based on machine-human ratio-\nnale alignment, is a common paradigm for learning\nfrom human explanations. In ER, the human ratio-\nnale can be obtained by annotating each instance\nindividually (Zaidan and Eisner, 2008; Lin et al.,\n2020; Camburu et al., 2018; Rajani et al., 2019;\nDeYoung et al., 2019) or by applying domain-level\nlexicons across all instances (Rieger et al., 2020;\nRoss et al., 2017; Ghaeini et al., 2019; Kennedy\net al., 2020; Liu and Avci, 2019). Existing choices\nof rationale alignment criteria include MSE (Liu\nand Avci, 2019; Kennedy et al., 2020; Ross et al.,\n2017), MAE (Rieger et al., 2020), BCE (Chan et al.,\n2021), order loss (Huang et al., 2021), and KL di-\nvergence (Chan et al., 2021). Beyond ER, there\nare other ways to learn from explanations. Lu et al.\n(2022) used human-in-the-loop feedback on ma-\nchine rationales for data augmentation. Meanwhile,\nYe and Durrett (2022) used machine rationales to\ncalibrate black box models and improve their per-\nformance on low-resource domains.\nEvaluating ER Existing works have primarily\nevaluated ER models via ID generalization (Zaidan\nand Eisner, 2008; Lin et al., 2020; Huang et al.,\n2021), which only captures one aspect of ER‚Äôs\n3322\nimpact. Meanwhile, a few works have consid-\nered auxiliary evaluations ‚Äî e.g., machine-human\nrationale alignment (Huang et al., 2021; Ghaeini\net al., 2019), task performance on unseen datasets\n(Ross et al., 2017; Kennedy et al., 2020), social\ngroup fairness (Rieger et al., 2020; Liu and Avci,\n2019). Carton et al. (2022) showed that maximiz-\ning machine-human rationale alignment does not\nalways improve task performance, while human\nrationales vary in their ability to provide useful\ninformation for task prediction. further showed\nthat\n7 Conclusion and Future Work\nIn this work, we study explanation regularization\n(ER) ‚Äì aligning machine rationales with human ra-\ntionales, in detail. We propose ER-T EST , that eval-\nuates ER‚Äôs OOD generalization along three pillars\n- unseen datasets, contrast set tests and functional\ntests, and uses it to investigate four research ques-\ntions surrounding the choice of the rationale align-\nment criterion, type of human rationale, choice of\nand time taken to obtain rationale-annotation in-\nstances. Although ER shows minor impact on ID\ntask performance, improvements on OOD datasets\nis significant. Furthermore, ER not only works well\nwith dense, instance-level human rationales, but\nalso with distantly supervised task-level rationales.\nLastly, ER is shown to provide benefits even with\nlimited number of rationale-annotated instances,\nor within time constraints for rationale annotation.\nIn future, we aim to study ER as a tool for im-\nproving human-in-the-loop (HITL) debugging of\nNLMs. Furthermore, currently ER-T EST is only\ndefined for extractive rationales. Human feedback\nfor free-text machine rationales is also a promising\nextension for ER-T EST .\n8 Acknowledgments\nThis research is based upon work supported in part\nby the Office of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA), via Contract No. 2019-\n19051600007, DARPA MCS program under Con-\ntract No. N660011924033, NSF IIS 2048211, and\ngift awards from Google and Amazon. The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of ODNI, IARPA, or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein. We would also like to thank all of our col-\nlaborators at the USC INK Research Lab for their\nconstructive feedback on this work.\n9 Limitations\nSome tests defined by ER-T EST might not be\napplicable for all NLP tasks. It is difficult to\ndefine tests like contrast set tests for certain NLP\ntasks like NER. Furthermore, currently these tests\nhave been picked up from their respective releases,\nhowever, they are extremely tedious to design and\ngenerate for new tasks and datasets.\nCurrent work simulates human rationales in\nthe ER pipeline. ER is meant to align machine-\ngenerated rationales with human-rationales. How-\never in our current work, we use human rationales\nthat are pre-annotated as part of the datasets we\nuse. This simulation of live human feedback is\nused in rationale alignment criterion. We believe\nthis limitation can be easily addressed, by collect-\ning human-in-the-loop rationale annotations.\nCurrent work assumes ER pipelines to be offline\nin nature. Fine-tuning strategies have shown to\ndistort the underlying data distribution (Kumar\net al., 2022), therefore, once Fundergoes ER, its\nmachine rationales can differ from before. Cur-\nrently, ER is being studied in an offline manner ‚Äì\nonce human rationales are collected, they are used\nto update model weights. However, what is more\neffective is to study the effect of ER when applied\nincrementally in an online manner, thus improving\nrationale alignment.\n10 Ethics Statement\nData. All the datasets that we use in our work\nare released publicly for usage and have been duly\nattributed to their original authors.\nUser Study. As part of our user study conducted\nin Section 5.5, we collected information about the\ntime taken to annotate rationales and labels for\ninstances. We provide the instructions given to\nMTurkers in Appendix A.7, along with screenshots\nof the UI displayed to them. Further details about\nthe task setup and results are provided in Section\n5.5. Each task is setup in a manner that ensure that\nthe annotators receive compensation that is above\nminimum wage.\n3323\nReferences\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644‚Äì1650, Online. Association for Computational\nLinguistics.\nErik Cambria, Yang Li, Frank Z. Xing, Soujanya Poria,\nand Kenneth Kwok. 2020. Senticnet 6: Ensemble\napplication of symbolic and subsymbolic ai for senti-\nment analysis. In Proceedings of the 29th ACM Inter-\nnational Conference on Information amp; Knowledge\nManagement, CIKM ‚Äô20, page 105‚Äì114, New York,\nNY , USA. Association for Computing Machinery.\nOana-Maria Camburu, Tim Rockt√§schel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. arXiv preprint arXiv:1812.01193.\nSamuel Carton, Surya Kanoria, and Chenhao Tan. 2022.\nWhat to learn, and how: Toward effective learning\nfrom rationales. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 1075‚Äì\n1088, Dublin, Ireland. Association for Computational\nLinguistics.\nSamuel Carton, Anirudh Rathore, and Chenhao Tan.\n2020. Evaluating and characterizing human ratio-\nnales. arXiv preprint arXiv:2010.04736.\nAaron Chan, Maziar Sanjabi, Lambert Mathias, Liang\nTan, Shaoliang Nie, Xiaochang Peng, Xiang Ren,\nand Hamed Firooz. 2022. Unirex: A unified learning\nframework for language model rationale extraction.\nInternational Conference on Machine Learning.\nAaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal,\nTanishq Gupta, and Xiang Ren. 2021. Salkg: Learn-\ning from knowledge graph explanations for common-\nsense reasoning. Advances in Neural Information\nProcessing Systems, 34.\nCheng-Han Chiang and Hung-yi Lee. 2022. Re-\nexamining human annotations for interpretable nlp.\nGeorge Chrysostomou and Nikolaos Aletras. 2022. An\nempirical study on explanations in out-of-domain\nsettings.\nOna de Gibert, Naiara Perez, Aitor Garc√≠a-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11‚Äì20, Brussels, Belgium. Association for\nComputational Linguistics.\nMisha Denil, Alban Demiraj, and Nando De Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. arXiv preprint arXiv:1412.6815.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. 2019. Eraser: A benchmark to\nevaluate rationalized nlp models. arXiv preprint\narXiv:1911.03429.\nShuoyang Ding and Philipp Koehn. 2021. Evaluating\nsaliency methods for neural language models. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5034‚Äì5052, Online. Association for Computational\nLinguistics.\nFinale Doshi-Velez and Been Kim. 2017. Towards a\nrigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608.\nHang Gao and Tim Oates. 2019. Universal adversarial\nperturbation for text classification.\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\net al. 2020. Evaluating models‚Äô local decision\nboundaries via contrast sets. arXiv preprint\narXiv:2004.02709.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. Allennlp: A deep semantic natural language\nprocessing platform.\nReza Ghaeini, Xiaoli Z Fern, Hamed Shahbazi, and\nPrasad Tadepalli. 2019. Saliency learning: Teaching\nthe model where to pay attention. arXiv preprint\narXiv:1902.08649.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107‚Äì112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nMareike Hartmann and Daniel Sonntag. 2022. A survey\non improving NLP models with human explanations.\nIn Proceedings of the First Workshop on Learning\nwith Natural Language Supervision, pages 40‚Äì47,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nPeter Hase and Mohit Bansal. 2021. When can models\nlearn from explanations? a formal framework for\nunderstanding the roles of explanation data. arXiv\npreprint arXiv:2102.02201.\n3324\nQuzhe Huang, Shengqi Zhu, Yansong Feng, and\nDongyan Zhao. 2021. Exploring distantly-labeled\nrationales in neural network models. arXiv preprint\narXiv:2106.01809.\nPeter J Huber. 1992. Robust estimation of a location\nparameter. In Breakthroughs in statistics, pages 492‚Äì\n518. Springer.\nXisen Jin, Francesco Barbieri, Brendan Kennedy, Aida\nMostafazadeh Davani, Leonardo Neves, and Xiang\nRen. 2021. On transferability of bias mitigation ef-\nfects in language model fine-tuning. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3770‚Äì3783,\nOnline. Association for Computational Linguistics.\nXisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue,\nand Xiang Ren. 2019. Towards hierarchical im-\nportance attribution: Explaining compositional se-\nmantics for neural sequence models. arXiv preprint\narXiv:1911.06194.\nErik Jones, Robin Jia, Aditi Raghunathan, and Percy\nLiang. 2020. Robust encodings: A framework for\ncombating adversarial typos. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2752‚Äì2765, Online. Asso-\nciation for Computational Linguistics.\nDivyansh Kaushik, Eduard Hovy, and Zachary C Lipton.\n2019. Learning the difference that makes a differ-\nence with counterfactually-augmented data. arXiv\npreprint arXiv:1909.12434.\nBrendan Kennedy, Mohammad Atari, Aida M Da-\nvani, Leigh Yeh, Ali Omrani, Yehsong Kim,\nKris Coombs, Shreya Havaldar, Gwenyth Portillo-\nWightman, Elaine Gonzalez, and et al. 2018. Intro-\nducing the gab hate corpus: Defining and applying\nhate-based rhetoric to social media posts at scale.\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-\nvani, Morteza Dehghani, and Xiang Ren. 2020. Con-\ntextualizing hate speech classifiers with post-hoc ex-\nplanation. arXiv preprint arXiv:2005.02439.\nAnanya Kumar, Aditi Raghunathan, Robbie Jones,\nTengyu Ma, and Percy Liang. 2022. Fine-tuning\ncan distort pretrained features and underperform out-\nof-distribution.\nChuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu,\nXuhui Zhou, and Shane Steinert-Threlkeld. 2020.\nLinguistically-informed transformations (LIT): A\nmethod for automatically generating contrast sets.\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 126‚Äì135, Online. Association for Com-\nputational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nBill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan\nMoreno, Xiao Huang, Prashant Shiralkar, and Xi-\nang Ren. 2020. Triggerner: Learning with entity\ntriggers as explanations for named entity recognition.\narXiv preprint arXiv:2004.07493.\nZachary C Lipton. 2018. The mythos of model inter-\npretability: In machine learning, the concept of in-\nterpretability is both important and slippery. Queue,\n16(3):31‚Äì57.\nFrederick Liu and Besim Avci. 2019. Incorporating\npriors with feature attribution on text classification.\narXiv preprint arXiv:1906.08286.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJinghui Lu, Linyi Yang, Brian Namee, and Yue Zhang.\n2022. A rationale-centric framework for human-in-\nthe-loop machine learning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6986‚Äì6996, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. In Proceed-\nings of the 31st international conference on neural\ninformation processing systems, pages 4768‚Äì4777.\nSiwen Luo, Hamish Ivison, Caren Han, and Josiah Poon.\n2021. Local interpretations for explainable natu-\nral language processing: A survey. arXiv preprint\narXiv:2103.11072.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: understanding rating dimen-\nsions with review text. In Proceedings of the 7th\nACM conference on Recommender systems, pages\n165‚Äì172.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428‚Äì3448, Florence,\nItaly. Association for Computational Linguistics.\nNinareh Mehrabi, Thamme Gowda, Fred Morstatter,\nNanyun Peng, and Aram Galstyan. 2020. Man is to\nPerson as Woman is to Location: Measuring Gender\nBias in Named Entity Recognition, page 231‚Äì232.\nAssociation for Computing Machinery, New York,\nNY , USA.\nShubhanshu Mishra, Sijun He, and Luca Belli. [link].\nF. √Ö. Nielsen. 2011. Afinn.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\n3325\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Bj√∂rkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards robust\nlinguistic analysis using ontonotes. In Proceedings\nof the Seventeenth Conference on Computational Nat-\nural Language Learning, pages 143‚Äì152.\nDanish Pruthi, Bhuwan Dhingra, Livio Baldini Soares,\nMichael Collins, Zachary C Lipton, Graham Neubig,\nand William W Cohen. 2020. Evaluating explana-\ntions: How much do explanations from the teacher\naid students? arXiv preprint arXiv:2012.00893.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. arXiv preprint arXiv:1906.02361.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist. arXiv\npreprint arXiv:2005.04118.\nLaura Rieger, Chandan Singh, William Murdoch, and\nBin Yu. 2020. Interpretations are useful: penaliz-\ning explanations to align neural networks with prior\nknowledge. In International conference on machine\nlearning, pages 8116‚Äì8126. PMLR.\nAndrew Slavin Ross, Michael C Hughes, and Finale\nDoshi-Velez. 2017. Right for the right reasons: Train-\ning differentiable models by constraining their expla-\nnations. arXiv preprint arXiv:1703.03717.\nSebastian Ruder. 2021. Challenges and Opportuni-\nties in NLP Benchmarking. http://ruder.io/\nnlp-benchmarking.\nCynthia Rudin. 2019. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine\nIntelligence, 1(5):206‚Äì215.\nErik F Sang and Fien De Meulder. 2003. Introduction\nto the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nSoumya Sanyal and Xiang Ren. 2021. Discretized in-\ntegrated gradients for explaining language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10285‚Äì10299, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nChristopher Schr√∂der and Andreas Niekler. 2020. A\nsurvey of active learning for text classification using\ndeep neural networks.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In Proceedings\nof the 34th International Conference on Machine\nLearning - Volume 70, ICML‚Äô17, page 3145‚Äì3153.\nJMLR.org.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631‚Äì1642.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Inter-\nnational Conference on Machine Learning, pages\n3319‚Äì3328. PMLR.\nAarne Talman and Stergios Chatzikyriakidis. 2018.\nTesting the generalization power of neural network\nmodels across nli benchmarks.\nChenhao Tan. 2022. On the diversity and limits of\nhuman explanations. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2173‚Äì2188, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nTianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang\nLi, Jilin Chen, Alex Beutel, and Ed Chi. 2020. CAT-\ngen: Improving robustness in NLP models via con-\ntrolled adversarial text generation. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 5141‚Äì\n5146, Online. Association for Computational Lin-\nguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. arXiv preprint arXiv:1908.04626.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019. Hug-\ngingface‚Äôs transformers: State-of-the-art natural lan-\nguage processing.\nXi Ye and Greg Durrett. 2022. Can explanations be use-\nful for calibrating black box models? In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6199‚Äì6212, Dublin, Ireland. Association for\nComputational Linguistics.\n3326\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. In NeurIPS.\nOmar Zaidan and Jason Eisner. 2008. Modeling an-\nnotators: A generative approach to learning from\nannotator rationales. In Proceedings of the 2008 con-\nference on Empirical methods in natural language\nprocessing, pages 31‚Äì40.\nMatthew D Zeiler and Rob Fergus. 2013. Visualizing\nand understanding convolutional networks.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassification. arXiv:1509.01626 [cs].\nZhiqiang Zheng and B. Padmanabhan. 2002. On active\nlearning for data acquisition. In 2002 IEEE Interna-\ntional Conference on Data Mining, 2002. Proceed-\nings., pages 562‚Äì569.\nA Appendix\nA.1 Section 2 Appendix\nGfirst computes raw importance scores si ‚ààRn,\nthen normalizes si as probabilities ri using the sig-\nmoid function.\nBroadly, heuristic G‚Äôs can either be a gradient-\nbased, that assign importance scores based on gradi-\nent changes in F(Sundararajan et al., 2017; Sanyal\nand Ren, 2021; Shrikumar et al., 2017), sampling-\nbased, that assign important scores based on the\nneighbours/context of a given token (Zeiler and Fer-\ngus, 2013; Jin et al., 2019), or attention-based, that\nuse the attention-scores or a function of them to\nassign importance scores. (Ding and Koehn, 2021).\nA.2 Section 3 Appendix\nA.2.1 ID Generalization\nWhile ER-T EST ‚Äôs main focus is on evaluating\nOOD generalization, ER-T EST also considers\nID generalization as a baseline evaluation. Let\nD= {X,Y}N\ni=1 be a M-class text classification\ndataset, where X = {xi}N\ni=1 are the text inputs,\nY= {Àôyi}N\ni=1 are the target classes, and N is the\nnumber of instances (xi, Àôyi) in D. We call Dthe\nID dataset. Assume Dcan be partitioned into train\nset Dtrain, dev set Ddev, and test set Dtest, where\nDtest is an ID test set for D. After using ER to train\nFon Dtrain, we measure F‚Äôs task performance on\nthe ID test set Dtest. Note that this is a standard pro-\ntocol used by existing works to evaluate ER models\n(Zaidan and Eisner, 2008; Rieger et al., 2020; Liu\nand Avci, 2019; Ross et al., 2017; Huang et al.,\n2021; Ghaeini et al., 2019; Kennedy et al., 2020)\nA.2.2 Contrast Sets\nGiven ÀúD(i)\ntest(j) (a jth instance belonging to an OOD\ntest set ÀúD(i)\ntest), a perturbation functionŒ≤(i)\np is applied\nto that instance, where pdenotes the kind of per-\nturbation taking effect, and it often changes the\ntarget label for that instance. For example, pcan\nsignify semantic (e.g., changing tall to short), nu-\nmeral (e.g., changing one dogto three dogs), or\nentities (e.g., changing dogs to cats). Each per-\nturbation type is specific to the dataset it is being\ncreated for, so that instance labels are changed in a\nmeaningful manner. The resulting set of instances\nC(‚ü©) = Œ≤(i)\np ( ÀúD(i)\ntest(j))‚àÄj,p are termed as a contrast\nset for that dataset. Based on the way they are cre-\nated, contrast sets are a property of the dataset, and\nare not created to explicitly challenge F(unlike\nadversarial examples (Gao and Oates, 2019)).\nA.2.3 Functional Tests\nVocabulary Tests V ocabulary tests are used to\nevaluate F‚Äôs capability to address changes in the\nvocabulary of the text, and is particularly diverse\nw.r.t the parts-of-speech it caters to. For exam-\nple, certain vocabulary tests evaluate the relation-\nship (taxonomy) between different nouns in a sen-\ntence, whereas some swap the modifiers or the\nverbs present in a sentence in a meaningful manner\nbased on the task at hand, to capture F‚Äôs targeted\nperformance towards such changes (Ribeiro et al.,\n2020).\nRobustness Tests Robustness tests evaluate F‚Äôs\nbehavior under character-level edits to words in\na sentence, keeping the rest of the context same\nso as to not change the overall prediction. They\ninclude testing against typos as well as contractions\nin words, as well as addition of tokens that are\nirrelevant for the downstream task (like URLs or\ngibberish like Twitter handles). (Jones et al., 2020;\nWang et al., 2020)\nLogic Tests Testing F‚Äôs reasoning capabilities\ntowards logical changes in a sentence is also im-\nportant to evaluate its reliance on shortcut-patterns.\nThese tests perturb sentences in a logical manner\n(by adding or removing negations, or purposefully\ninducing contradictions) that also change the target\nlabel in the same manner. (Talman and Chatzikyri-\nakidis, 2018; McCoy et al., 2019)\nEntity Tests For certain tasks, named entities\nlike numbers, locations and proper nouns are not\nrelevant for predicted a target label, and are often\n3327\na source of gender or demographic biases (Mishra\net al.; Mehrabi et al., 2020). Entity tests measure\nF‚Äôs sensitivity towards changes in named entities\nsuch that the overall context as well as the task\nlabel remains the same (Ribeiro et al., 2020).\nA.3 Section 4 Appendix\nA.3.1 Tasks and Datasets\nTo evaluate ER models, ER-T EST considers a di-\nverse set of sequence and token classification tasks.\nFor each, task ER-T EST provides one ID dataset\n(annotated with human rationales) and multiple\nOOD datasets. Compared to prior works, ER-\nTEST ‚Äôs task/dataset diversity enables more exten-\nsive analysis of ER model generalization.\nFirst, we have sentiment analysis, using SST\n(movie reviews) (Socher et al., 2013; Carton et al.,\n2020) as the ID dataset. For OOD datasets, we use\nYelp (restaurant reviews) (Zhang et al., 2015), Ama-\nzon (product reviews) (McAuley and Leskovec,\n2013), and Movies (movie reviews) (Zaidan and\nEisner, 2008; DeYoung et al., 2019). Movies‚Äô in-\nputs are much longer than the other three datasets‚Äô.\nFor contrast set tests, we use an OOD contrast set\nfor sentiment analysis released by the authors of\nthe original paper (Gardner et al., 2020), which are\ncreated for the Movies dataset. Furthermore, for\nfunctional tests, we use an OOD test suite (flight re-\nviews) from the CheckList (Ribeiro et al., 2020)\nwhich contains both template-based instances to\ntest linguistic capabilities, as well as real-world\ndata (tweets).\nSecond, we have natural language inference\n(NLI), using e-SNLI (Camburu et al., 2018; DeY-\noung et al., 2019) as the ID dataset. For the OOD\ndataset, we use MNLI (Williams et al., 2017). e-\nSNLI contains only image captions, while MNLI\ncontains both written and spoken text, covering var-\nious topics, styles, and formality levels. For NLI,\nwe also use an OOD contrast set created for the\nMNLI dataset (Li et al., 2020). Functional tests for\nNLI are generated from the AllenNLP test suite\n(Gardner et al., 2017) for textual entailment.\nThird, we have named entity recognition (NER),\nusing CoNLL-2003 (Sang and De Meulder, 2003;\nLin et al., 2020) as the ID dataset. For the OOD\ndataset, we use OntoNotes v5.0 (Pradhan et al.,\n2013). CoNLL-2003 contains only Reuters news\nstories, while OntoNotes v5.0 contains text from\nnewswires, magazines, telephone conversations,\nwebsites, and other sources.\nA.3.2 Intrinsic Evaluation of ER\nER in general is sensitive to certain hyperparam-\neters for yielding meaningful training curves and\nactually attaining alignment between machine and\nhuman rationales. Due to a large set of tunable\nhyperparameters, running all configurations of ER\nare not feasible. Therefore, we intrinsically eval-\nuate hyperparameter configurations by assessing\nthe loss curves (which model alignment between\nmachine and human rationales) w.r.t different hy-\nperparameters values. We observe that the accept-\nable band of learning rates for ER is very narrow,\nand we use 2e‚àí5 in all of our experiments. Fur-\nthermore, we also observe that settingŒªER = 1 and\nŒ≥ER = 100 yields the most drop in the loss curves\nwhile training, so we use these hyperparameters\nfor the rest of our experiments. We detail these\nexperiments in Appendix A.3.3.\nA.3.3 Intrinsic Evaluation: evaluating ER‚Äôs\nsensitivity to hyperparameters\nWhen using ER to train F, it is important to assess\nwhether ER exhibits expected training behavior,\northogonally to task performance. If ER improves\ntask performance, this kind of analysis can help us\nbetter understand ER‚Äôs effectiveness. Conversely,\nif ER does not improve task performance, such\nanalysis can help us identify the problem.\nLet Œ≥ER >0 be the rationale scaling factor, used\nto scale ÀÜ si prior to sigmoid normalization. If the\nmagnitudes of the ÀÜ si scores are lower, then the ÀÜ ri\nscores will be closer to 0.5 (i.e., lower confidence).\nHowever, scalingÀÜ si by Œ≥ER >1 will increase the\nmagnitude of ÀÜ si, yielding ÀÜ ri scores closer to 0 or 1\n(i.e., higher confidence).\nMotivated by this, ER-T EST ‚Äôs intrinsic evalua-\ntion is based on machine-human rationale align-\nment, captured by the ER loss LER = Œ¶(ÀÜ ri,Àô ri).\nWhen using ER, we should generally expect the\nER loss to decrease asFis trained. In practice, this\nmay not always be the case, even when ER leads\nto slightly higher task performance (which is likely\na mirage caused by lucky random seeds)! That\nis, by definition, non-decreasing ER loss signals\nineffective ER usage, since the machine rationales\nare not becoming more similar to the human ra-\ntionales. This can stem from a number of issues:\ne.g., poor choice of ER criteria Œ¶, improper ER\nstrength ŒªER, improper rationale scaling factor Œ≥ER,\nnoisy human rationale Àô ri, insufficient Fcapacity.\nThus, we measure machine-human rationale align-\nment as the first step in diagnosing such issues. Let\n3328\nFigure 7: ER Loss Curves (Rationale Scaling Factor). For rationale extractor, we use IxG\nFigure 8: ER Strength vs. Task Performance. For\nvarious combinations of sentiment analysis dataset and\nER strength, we plot task performance using IxG+MAE.\nER loss curvedenote a chart which plots LER vs.\nthe number of train epochs. For each combination\nof ER criteria Œ¶ and some training configuration,\nwe plot ER loss curves for the training set. Each\ncomponent of our intrinsic evaluation varies a dif-\nferent hyperparameter in the training configuration:\n(A) ER strength ŒªER; (B) rationale scaling factor\nŒ≥ER; and (C) learning rate Œ±. In contrast, prior\nworks do not explore the relationship between LER\nand these training variables (Huang et al., 2021;\nGhaeini et al., 2019).\nFor intrinsic evaluation, we use ER strength\nŒªER = 1 , rationale scaling factor Œ≥ER = 1 , and\nlearning rate Œ± = 2e‚àí5, unless otherwise speci-\nfied. As a proof of concept, we focus on SST here,\nbut plan to add other datasets in future work.\nA.3.4 Misc. Details\nAll models are trained on GeForce GTX 1080 Ti\nand Quadro RTX 6000 GPUs. All implementations\nare done using the HuggingFace API (Wolf et al.,\n2019).\nA.3.5 ER Strength\nFig. 10 displays the ER loss curves for different\nER strengths ŒªER = [0.5,1,10,100,300], on SST\nusing MAE. Among the ŒªER values, we see that\nŒªER = 1 yields ER loss curves with the greatest de-\ncrease (Table 5), signaling good ER optimization.\nA.3.6 Rationale Scaling Factor\nFig. 7 displays the ER loss curves for different\nrationale scale factors Œ≥ER = [1 ,10,100,1000],\nER Strength\nER criteria0.5 1 10 100 300\nIxG+MSE 0.911.52 1.41 1.29 1.35\nIxG+MAE 1.892.01 1.72 1.80 1.74\nIxG+BCE 1.992.17 1.65 1.65 1.75\nIxG+Huber 1.85 2.09 2.24 2.272.40\nIxG+Order 2.15 2.40 1.602.53 1.89\nTable 5: Relative Decrease in ER Loss. For various\nER strengths, we report the percentage decrease in ER\ntrain loss (on SST), from max point to min point.\nRationale Scaling Factor\nER criteria1 10 100 1000\nIxG+MSE 0.69 4.6018.3511.41\nIxG+MAE 0.04 0.401.29 1.17\nIxG+BCE 0.10 0.34 0.901.03\nIxG+Huber 0.10 7.7516.679.30\nIxG+Order 7.21 9.3847.97 1.89\nTable 6: Relative Decrease in ER Loss. For various\nER rationale scaling factors, we report the percentage\ndecrease in ER train loss (on SST), from max point to\nmin point.\non SST. Among the four Œ≥ER values, we see that\nŒ≥ER = 100 yields ER loss curves with the greatest\ndecrease (Table 6), signaling good ER optimization.\nMeanwhile, although ER works use Œ≥ER = 1 by\ndefault, we see that Œ≥ER = 1 yields nearly flat ER\nloss curves for all five Œ¶ choices. This suggests\npoor ER optimization. Based on these results, we\nfix Œ≥ER = 100 for all experiments (Sec. 5), thus\ngreatly reducing the hyperparameter search space.\nA.3.7 Learning Rate\nHere, we obtain similar conclusions, with Œ± =\n2e‚àí5 yielding the best ER loss curves. Fig. 11\ndisplays the ER loss curves for different learning\nrates Œ± = [2e‚àí6,2e‚àí5,2e‚àí4]. Among the three\nlearning rates, we see that Œ± = 2e ‚àí5 yields the\nmost steadily decreasing ER loss curves.\nA.3.8 ER performance with different\nhyperparameters\nER Strength vs. Task Performance To measure\nER‚Äôs impact on task performance, we plot F‚Äôs task\nperformance as a function of ER strengthŒªER. This\nis conducted for ID test sets.\n3329\nFigure 9: Task Performance vs. ER Loss. Here we use IxG as rationale extractor\nFigure 10: ER Loss Curves (ER Strength). Here, we use the MAE criterion and IxG as rationale extractor\nDev Test\nER criteriaSlope (‚Üì) R2 (‚Üë) Slope (‚Üì) R2 (‚Üë)\nIxG+MSE -7.48 0.050 -6.75 0.059\nIxG+MAE-128.60 0.083 -133.03 0.110\nIxG+BCE -17.48 0.003 -56.30 0.040\nIxG+Huber -23.59 0.091 -8.40 0.022\nIxG+Order -0.49 0.101 -0.085 0.004\nTable 7: ER Loss vs. Task Performance. We summarize\nthe line plots in Fig. 9 (ER Loss vs. Task Performance), using\nslope and R2 score (Sec. A.3.8). Ideally, Fig. 9‚Äôs lines would\nhave low slopeand high R2, indicating that ER helps improve\ntask performance. We see that MAE yields the best ER results.\nFor each sentiment analysis dataset, Fig. 8\nshows task performance for ER strengths ŒªER =\n[0,0.5,1,10,100,300], using MAE. Note that\nŒªER = 0 is equivalent to training the NLM with-\nout ER (i.e., None in Table 1). For the ID dataset\n(SST), we see that all ER strengths yield very sim-\nilar task performance, suggesting that ER has lit-\ntle effect on ID task performance. However, for\nthe OOD datasets (Amazon, Yelp, Movies), task\nperformance generally increases as ŒªER increases,\nshowing ER‚Äôs positive impact on NLM generaliza-\ntion. Overall, based on OOD task performance, we\nfind that ŒªER = [1,100] are the best ER strengths.\nThis aligns with the results of Sec. A.3.5.\nER Loss vs. Task Performance To measure\nER‚Äôs impact on task performance, we plot F‚Äôs task\nperformance as a function of ER loss LER. This is\nconducted for both ID and OOD test sets.\nFig. 9 displays the SST results for ID task perfor-\nmance (accuracy) vs. ER loss. For a given ER cri-\nterion, each point in the corresponding scatter plot\nrepresents the checkpoint at some train epoch of the\nER-trained model, evaluated on either the dev set or\ntest set (yielding two point sets). Fitting each point\nset with linear regression, we find that there is an\nER criteria\nSentiment Analysis (Out-of-Domain)\nAmazon Yelp Movies Mean\nNone 90.72 (¬±1.36) 92.07 (¬±2.66) 89.83 (¬±6.79) 90.87 (¬±3.60)\nIxG+MAE (ŒªER= 0.5) 90.12 (¬±2.98) 92.27 (¬±3.29) 92.00 (¬±5.68) 91.46 (¬±0.91)IxG+MAE (ŒªER= 1) 92.02 (¬±0.25)94.55 (¬±0.30)95.50 (¬±1.32)94.02 (¬±2.15)IxG+MAE (ŒªER= 10) 91.27 (¬±0.28) 93.10 (¬±1.08) 90.67 (¬±3.79) 91.68 (¬±1.06)IxG+MAE (ŒªER= 100) 92.33 (¬±0.28) 94.92 (¬±0.56) 95.50 (¬±0.50) 94.25 (¬±1.89)IxG+MAE (ŒªER= 300) 91.83 (¬±0.42) 93.97 (¬±1.28) 95.00 (¬±0.50) 93.60 (¬±1.74)\nIxG+MAE (Œ≥ER= 1) 90.63 (¬±1.88) 92.32 (¬±2.23) 88.67 (¬±4.25) 90.54 (¬±2.22)IxG+MAE (Œ≥ER= 10) 92.30 (¬±1.21)93.01 (¬±2.14)96.83 (¬±1.04) 94.07 (¬±3.89)IxG+MAE (Œ≥ER= 100) 92.02 (¬±0.25)94.55 (¬±0.30)95.50 (¬±1.32)94.02 (¬±2.15)IxG+MAE (Œ≥ER= 1000) 90.47 (¬±2.06) 92.80 (¬±2.90) 92.67 (¬±6.25) 91.98 (¬±1.14)\nIxG+MAE (Œ±= 2e‚àí4) 89.35 (¬±2.85) 91.23 (¬±2.84) 93.00 (¬±2.65) 91.19 (¬±2.22)IxG+MAE (Œ±= 2e‚àí5) 92.02 (¬±0.25)94.55 (¬±0.30)95.50 (¬±1.32)94.02 (¬±2.15)IxG+MAE (Œ±= 2e‚àí6) 88.60 (¬±1.60) 83.27 (¬±6.49) 81.17 (¬±6.93) 84.34 (¬±9.70)\nTable 8: Task Performancevs. {ER Strength (ŒªER), Ratio-\nnale Scaling Factor (Œ≥ER)}. Higher values are better.\ninverse relationship between task performance and\nER loss. In other words, higher machine-human ra-\ntionale alignment (i.e., low ER loss) corresponds to\nhigher task performance, which validates the usage\nof ER to improve generalization. Table 7 displays\nthe slopes and R2 scores of the lines in Fig. 9.\nThe slope indicates the strength of the relationship\nbetween machine-human rationale alignment and\ntask performance (lower is better), while the R2\nscore indicates how accurately each line fits its cor-\nresponding data points. Among the five ER criteria,\nacross dev and test, we find that MAE has the low-\nest slopes and highestR2 scores overall, suggesting\nthat using ER with MAE is most effective.\nER Opportunity Cost An ER-trained NLM\nFtask, ER and a non-ER-trained NLM Ftask, No-ER\nare likely to yield different outputs given the same\ninputs. Let D+\nER ‚äÜD and D+\nNo-ER ‚äÜD denote\nthe sets of instances predicted correctly byFtask, ER\nand Ftask, No-ER, respectively. Ideally, we would\nhave D+\nNo-ER ‚äÇD+\nER. This means there is no oppor-\ntunity costin using ER, as ER increases the number\nof correct instances without turning any previously-\ncorrect incorrect. However, this may not necessar-\n3330\nFigure 11: ER Loss Curves (Learning Rate). Here we use IxG as rationale extractor\nER criteria\nSentiment Analysis\nIn-Domain Out-of-Domain\nSST Amazon Yelp Movies\nNone 0.00 (¬±0.00) 0.00 (¬±0.00) 0.00 (¬±0.00) 0.00 (¬±0.00)\nIxG+MSE 0.32 (¬±1.05)-1.25 (¬±1.20) -2.33 (¬±4.64)-6.50 (¬±40.66)IxG+MAE -0.09 (¬±0.24) -0.58 (¬±3.45) -0.94 (¬±11.21)-7.00 (¬±40.66)IxG+BCE-0.16 (¬±0.33)0.46 (¬±4.11) 0.96 (¬±26.99) 0.16 (¬±47.72)IxG+Huber 0.12 (¬±0.42) 0.19 (¬±2.25) -1.05 (¬±4.11) -4.33 (¬±37.72)IxG+Order 1.90 (¬±1.38) 6.98 (¬±3.87) 19.86 (¬±45.54) 21.66 (¬±35.72)\nTable 9: ID/OOD Opportunity Cost. Lower values are\nbetter.\nPercentage of Dev Instances inincor‚ÜícorGroup, Binned byFNo-ERTarget Class Confidence\n0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0\n22.85 26.00 40.38 49.20 28.78 0.00 0.00 0.00 0.00 0.00\nTable 10: Change in Target Class Confidence. For bins\nwhere FNo-ER‚Äôs target class confidence is low, there is a higher\npercentage of instances that are predicted incorrectly/correctly\nwithout/with ER. This suggests that instances with low target\nclass confidence are more likely to benefit from ER.\nily be the case, so we measure ER‚Äôs opportunity\ncost as follows. Let n+\nER = |D+\nER\\(D+\nER ‚à©D+\nNo-ER)|\nbe the number of instances predicted correctly by\nFtask, ER, but not by Ftask, No-ER. Let n+\nNo-ER =\n|D+\nNo-ER\\(D+\nNo-ER ‚à©D+\nER)|be the number of in-\nstances predicted correctly by Ftask, No-ER, but not\nby Ftask, ER. Then, the opportunity cost of using\nER is defined as:\noER = n+\nNo-ER ‚àín+\nER\n|D| (3)\nIn practice, instead of defining oER for all of D, we\nonly consider test sets Dtest and ÀúDtest.\nFigure 12: Change in Target Class Confidence\nChange in Target Class Confidence Let FNo-ER\nand FER denote non-ER-trained (vanilla) and\nER-trained NLMs, respectively. For each test\ninstance, we plot FNo-ER‚Äôs predicted target class\nconfidence probability vs. FER‚Äôs. Each point in\nthe plot is color-coded by whether ER changes\nthe prediction from correct to incorrect, changes\nthe prediction from incorrect to correct, keeps\nthe prediction as correct, or keeps the prediction\nas incorrect. The purpose of this plot is to\nvisualize how individual instances‚Äô predictions\nare affected by ER. We conduct this for ID dev sets.\nWe consider ER with the MAE criterion,\ntrained/evaluated on SST (via dev ID task per-\nformance). Fig. 12 visualizes how ER changes\neach dev instance‚Äôs target class confidence as a re-\nsult of ER, color-coding each point w.r.t. how ER\nchanges the model‚Äôs predicted class for this point.\nAmong instances for which FNo-ER‚Äôs target class\nconfidence is low, there is a higher percentage of\ninstances that are predicted incorrectly/correctly\nwithout/with ER (i.e., incor‚Üícor). This suggests\nthat, for FNo-ER, instances with low target class con-\nfidence are more likely to benefit from ER (Table\n10). Also, based on the T-test, target class confi-\ndence scores are significantly higher (p <0.005)\nwith ER than without.\nTable 9 displays the opportunity cost results for\nsentiment analysis. Generally, the opportunity cost\nresults mirror the task performance results in Table\n1, such that the methods with highest task perfor-\nmance tend to have the lowest opportunity cost.\nHowever, using opportunity cost, the variance is\nvery high for OOD datasets, making it difficult\nto compare methods. In future work, we plan to\nmodify the opportunity cost metrics to better ac-\ncommodate OOD settings.\nA.3.9 Efficient hyperparameter tuning with\nER-T EST\nIn intrinsic evaluation (Sec. A.3.2), we used ER\nloss curves as priors for selecting three key ER\nhyperparameters (i.e., ER strength ŒªER, rationale\n3331\nscaling factor Œ≥ER, learning rate Œ±). In Sec. 5, we\nassumed a tuning budget that allows only one value\nfor each of ŒªER, Œ≥ER, and Œ±. By not tuning these\nhyperparameters, we greatly reduced our hyperpa-\nrameter search space. Since ER has little effect\non ID task performance, tuning based on ID task\nperformance is unlikely to have helped anyway.\nER works better on OOD data, but it also does\nnot make sense to tune based on OOD task perfor-\nmance (otherwise, it would not be OOD). Though\nthe ER hyperparameters chosen via intrinsic evalu-\nation generally improved OOD task performance,\nwe seek to verify their effectiveness compared to\nother possible hyperparameter values.\nIn Table 8, we report sentiment analysis OOD\n(Amazon, Yelp, Movies) task performance, while\nvarying each of the three hyperparameters. We\ninclude a Mean column, which averages the Ama-\nzon/Yelp/Movies columns. Our hyperparameters\nchosen via ER loss curves are highlighted in blue .\nFor ŒªER, 1 (ours) and 100 yield very similar Mean\nresults, while considerably beating the other three\nvalues. For Œ≥ER, we see the same trend for 100\n(ours) and 10. For Œ±, 2e‚àí5 (ours) vastly outper-\nforms other values in all columns. These results\nvalidate the utility of ER-T EST ‚Äôs intrinsic evalua-\ntion for low-resource ER hyperparameter tuning.\nA.4 RQ1\nNER Results We also have named entity recog-\nnition (NER) task, using CoNLL-2003 (Sang and\nDe Meulder, 2003; Lin et al., 2020) as the ID\ndataset. For the OOD dataset, we use OntoNotes\nv5.0 (Pradhan et al., 2013). CoNLL-2003 con-\ntains only Reuters news stories, while OntoNotes\nv5.0 contains text from newswires, magazines, tele-\nphone conversations, websites, and other sources.\nIn Table 15, we display the ID and OOD results of\nNER. In ID, we see more variance in task perfor-\nmance among ER criteria, although the variance\nis still quite small among the best methods (MSE,\nMAE, Huber). Here, MAE yields the highest task\nperformance, while BCE yields the lowest by far.\nIn OOD, MAE still performs best, while MSE and\nHuber are competitive.\nFunctional Tests We provide details for differ-\nent functional tests listen in Section 3.3. We break-\ndown each subcategory of functional tests and show\nperformances of different ER criteria on those indi-\nvidual tests. For functional tests on the sentiment\nanalysis task, refer to Table 11. NLI functional\ntests are listed in Table 14.\nA.5 RQ2\nA.5.1 Lexicon-matching\nLet LD be a list of lexicons curated by human\nannotators, specific to a given dataset D. Let l(¬∑)\nbe an indicator function that searches for a given\nlexicon list in all the tokens of an instance, and\nreturns a binary representation of the same size as\nthe instance with 1s in places with lexicon matches\n(0 otherwise). Therefore, we can obtain distantly-\nsupervised human rationales Àô ri = l(LD,xi) and\napply rationale alignment criteria as described in\nSection 4.1.\nEach lexicon is matched to n-grams(uni-/bi-/tri-\ngrams), which leads to 93% of the train set in-\nstances to be matched. Additionally, since we com-\nbined two lexicons as resources, there are words\nappeared as positive and negative. We maintain lex-\nicon overlapping with different sentiment polarities\nwhen matching with tokens. For equal compari-\nson, we use instance-based rationales on the same\ntrain subset. We also run contrast set tests and\nfunctional tests on both lexicon-based and instance-\nbased methods. The results are shown in Table 12\nand Table 13.\nA.5.2 Hate Speech Detection Tests\nTask-Level Rationales For example, Kennedy\net al. (2020) used a ‚Äúblacklist‚Äù lexicon to distantly\nsupervise human rationales for the hate speech\ndetection task. In the past, hate speech detection\nmodels were largely oversensitive to certain\ngroup identifier words ( e.g., ‚Äúblack‚Äù, ‚ÄúMuslim‚Äù,\n‚Äúgay‚Äù), almost always predicting hate speech for\ntext containing these words. To address this,\nthey first manually annotated a lexicon of group\nidentifiers that should be ignored for hate speech\ndetection. Then, for all training instances, they\nautomatically marked only tokens belonging to\nthe lexicon as negative (and the rest as positive).\nBy using these human rationales for ER, they\ntrained the NLM to be less biased w.r.t. these\ngroup identifiers. For the purpose of our study, we\nuse the lexicons as used by (Jin et al., 2021) to\ngenerate distantly-supervised rationales for the\nStormfromt (Stf) dataset (de Gibert et al., 2018).\nEach instance in the Stf dataset is matched to\none or more lexicons by simple character-level\nmatching, and the rationales are generated as\ndescribed above. We train Fwith the Stf dataset.\nWe report all accuracies in Table 16. As it was\n3332\nCapability Test Type ER criteria\nNone IxG+MSE IxG+MAE IxG+BCE IxG+Huber IxG+Order\nVocabulary\nSentiment-laden words in context 1.20 (¬±0.74) 0.60 (¬±0.16) 1.27 (¬±0.84) 1.00 (¬±0.86) 1.13 (¬±0.50) 0.80 (¬±0.28)Change Neutral words with BERT 5.59 (¬±0.16) 5.13 (¬±0.90) 5.40 (¬±0.28) 5.67 (¬±0.68) 5.67 (¬±0.74) 5.60 (¬±1.63)Intensifiers 2.13 ( ¬±1.63) 1.80 (¬±0.16) 1.40 (¬±0.16) 2.67 (¬±0.77) 2.67 (¬±0.96) 1.60 (¬±0.65)Reducers 23.85 ( ¬±7.18) 35.00 (¬±46.01) 27.38 (¬±5.95) 25.00 (¬±25.00) 17.46 (¬±13.65) 0.77 (¬±0.43)Add +ve phrases 1.40 ( ¬±0.28) 2.33 (¬±1.84) 0.67 (¬±0.50) 1.27 (¬±1.00) 2.33 (¬±1.76) 2.07 (¬±1.52)Add -ve phrases 22.86 ( ¬±7.43) 14.80 (¬±1.40) 20.67 (¬±4.07) 17.40 (¬±3.64) 20.67 (¬±3.35) 16.93 (¬±1.91)\nRobustness\nAdding Random URLs and Handles 9.80 (¬±0.48) 7.27 (¬±2.23) 9.07 (¬±1.80) 7.87 (¬±2.76) 10.27 (¬±0.9) 9.6 (¬±2.47)Punctuations 3.93 ( ¬±0.89) 1.93 (¬±0.41) 3.00 (¬±1.02) 2.87 (¬±0.19) 3.80 (¬±0.28) 2.67 (¬±0.34)Typos 2.60 ( ¬±0.90) 2.53 (¬±0.82) 2.60 (¬±0.57) 3.13 (¬±0.90) 2.60 (¬±0.75) 2.00 (¬±0.86)2 Typos 3.93 ( ¬±0.65) 3.87 (¬±1.24) 4.27 (¬±0.5) 4.13 (¬±1.2) 4.6 (¬±0.43) 3.33 (¬±0.25)Contractions 1.00 ( ¬±0.00) 0.80 (¬±0.33) 0.87 (¬±0.25) 0.80 (¬±0.43) 0.47 (¬±0.09) 0.53 (¬±0.50)\nLogic Negatives 5.20 ( ¬±2.75) 4.27 (¬±1.65) 4.47 (¬±3.07) 4.47 (¬±1.75) 3.93 (¬±1.57) 5.67 (¬±1.68)Non-negatives 59.73 ( ¬±9.48) 59.00 (¬±15.81) 37.47 (¬±10.41) 63.27 (¬±17.61) 59.07 (¬±14.97) 45.87 (¬±24.13)Negation of positive with neutral stuff in the middle 32.2 (¬±14.65) 35.13 (¬±1.91) 35.00 (¬±16.52) 19.00 (¬±8.66) 40.93 (¬±4.31) 29.13 (¬±10.60)\nEntity Change Names 0.70 ( ¬±0.14 1.91 (¬±0.71) 1.11 (¬±0.51) 0.81 (¬±0.14) 1.61 (¬±0.62) 1.91 (¬±1.51)Change Locations 3.33 ( ¬±0.74) 2.73 (¬±1.15) 3.40 (¬±0.86) 3.07 (¬±1.79) 3.00 (¬±0.33) 3.20 (¬±1.57)Change Numbers 0.80 ( ¬±0.00) 0.53 (¬±0.34) 0.47 (¬±0.41) 0.60 (¬±0.33) 0.60 (¬±0.43) 0.67 (¬±0.81)\nTable 11: Functional Tests: Sentiment Analysis\nFigure 13: Functional Tests‚Äô Failure Rates (lower the better): We plot the failure rates of the four functional tests (vocab.,\nrobust., logic, entity) as described in Section 3.3, as well as the overall failure rate on all of the tests combined (mean). Each of\nthe values are out of 100, but plotted accordingly for visible comparison. Here we use IxG as rationale extractor.\nMethod ER Criteria\nSentiment Analysis\nOriginal Contrast Delta\nLexiconIxG+MAE 91.46 (¬±0.72) 89.82 (¬±2.46) -1.64\nIxG+Huber 90.64 (¬±1.25) 88.52 (¬±2.25) -2.12\nInstanceIxG+MAE 91.12 (¬±0.59) 89.82 (¬±1.20) -1.3\nIxG+Huber 89.20 (¬±1.67) 86.13 (¬±1.74) -3.15\nTable 12: Contrast Set Tests: Lexicon-based VS\nInstance-based. We use\nobserved in Section 5.2, ER does not lead to a\nsignificant improvement in performance for the\nStf test set. However, it is important to note\nthat ‚Äúblacklisting‚Äù group identifier lexicons does\nnot lead to a drop in ID performance either.\nBenefits of ‚Äúblacklisting‚Äù are then observed in\nCapabilityV ocabulary Robustness Logic Entity Overall\nLexicon 10.11 4.21 32.27 2.28 12.22\nInstance 11.90 3.574 30.2 1.86 11.89\nTable 13: Functional Tests: Lexicon-based VS\nInstance-based. Here we use MAE criterion and IxG as\nrationale extractor.\nOOD generalization. We evaluate ER methods\non OOD hate speech detection datasets like\nHatEval (Barbieri et al., 2020) and Gab Hate\nCorpus (GHC) (Kennedy et al., 2018). All of\nthe datasets contain binary labels for hateful and\nnon-hateful content. The Stf dataset is collected\nfrom a white-supremacist forum, whereas HatEval\n3333\nCapability Test Type ER criteria\nNone IxG+MSE IxG+MAE IxG+BCE IxG+Huber IxG+Order\nVocabularyAntonym in Hypothesis 71.66 (¬±20.98) 64.77 (¬±21.97) 84.55 (¬±11.53) 65.88 (¬±21.40) 74.77 (¬±20.41) 62.55 (¬±13.16)Synonym in Hypothesis 32.61 (¬±7.41) 24.11 (¬±7.62) 30.11 (¬±6.42) 25.88 (¬±6.86) 30.77 (¬±7.07) 29.27 (¬±6.95)Supertype in Hypothesis 24.44 (¬±15.95) 11.00 (¬±3.62) 13.77 (¬±6.71) 9.31 (¬±5.90) 8.77 (¬±8.06) 13.55 (¬±7.10)\nRobustness\nPunctuation 14.55 ( ¬±4.13) 9.44 (¬±2.79) 11.33 (¬±1.63) 8.11 (¬±1.19) 10.00 (¬±2.58) 9.88 (¬±2.51)Typo 15.88 ( ¬±3.44) 10.22 (¬±3.04) 12.33 (¬±1.63) 9.66 (¬±2.10) 10.88 (¬±2.68) 10.77 (¬±2.52)2 Typos 15.33 ( ¬±3.68) 9.77 (¬±1.81) 12.00 (¬±1.76) 9.44 (¬±2.31) 11.11 (¬±2.99) 10.00 (¬±2.66)Contractions 24.69 ( ¬±6.98) 24.69 (¬±8.72) 25.92 (¬±9.07) 22.22 (¬±9.07) 25.92 (¬±7.40) 14.81 (¬±5.23)\nLogic Negation in the Hypothesis 50.88 (¬±32.25) 27.77 (¬±37.24) 9.77 (¬±15.66) 41.33 (¬±41.54) 15.22 (¬±28.77) 18.44 (¬±23.21)Induce Contradiction 99.88 (¬±0.31) 98.54 (¬±3.78) 91.69 (¬±20.37) 98.65 (¬±2.56) 98.42 (¬±4.44) 99.88 (¬±0.31)Same Premise and Hypothesis 14.22 (¬±8.63) 14.33 (¬±10.14) 19.44 (¬±12.12) 18.16 (¬±12.69) 14.38 (¬±9.23) 17.38 (¬±10.16)\nEntity Switch one Entity in the Hypothesis 77.21 (¬±39.57) 88.88 (¬±24.11) 79.91 (¬±22.20) 85.18 (¬±30.04) 83.83 (¬±24.25) 96.40 (¬±4.85)\nTable 14: Functional Tests: NLI\nMethods\nNER\nIn-Distribution Out-of-Distribution\nCoNLL-2003 OntoNotes v5.0\nNone 77.24 (¬±0.20) 20.78 (¬±0.41)\nIxG+MSE 78.02 (¬±0.69) 21.60 (¬±0.46)IxG+MAE78.34 (¬±0.81) 21.73 (¬±0.31)IxG+BCE 64.53 (¬±13.22) 17.32 (¬±3.59)IxG+Huber 77.83 (¬±1.09) 21.38 (¬±0.16)IxG+Order 72.62 (¬±5.01) 19.14 (¬±1.75)\nTable 15: ID/OOD Task Performance on NER (Instance-\nBased Human Rationales).\ninstances are tweets and GHC instances are taken\nfrom the Gab forum. Table 16 shows that while\nthe improvements in HatEval are not significant,\nthere are significant accuracy improvements for\nthe GHC test set, which are due to the Order ER\ncriterion.\nFairness Tests In addition to generic perfor-\nmance metrics like accuracy, we also measure\ngroup identifier bias (against the groups detailed\nby group identifier lexicons) by evaluating the\nFalse Positive Rate Difference (FPRD) as shown\nby (Jin et al., 2021). FPRD is computed as‚àë\nz |FPRz ‚àíFPRoverall|, where FPRz is the false\npositive rate of all of the test instances mentioning\ngroup identifier z, and FPRoverall is the false posi-\ntive rate of all the test instance. Essentially, FPRD\nevaluates if Fis more biased against a given group\nidentifier z, than all of the groups. A lower FPRD\nvalue indicates less biased against the listed group\nidentifiers by F.\nTable 16 lists the FPRD values of all the ER cri-\nteria in ID and OOD datasets. While all other crite-\nria suffer with higher bias than None, we observe\nthat Order criterion consistently leads to the least\nbias, both in-distribution and out-of-distribution.\nFurthermore, the reduction in bias is significant\nwhen compared to None. Interestingly, Order\nER Criteria\nHate Speech DetectionIn-Distribution Out-of-DistributionStf HatEval GHCAccuracy‚Üë FPRD‚Üì Accuracy‚Üë FPRD‚Üì Accuracy‚Üë FPRD‚ÜìNone 89.50 (¬±0.20) 1.11 (¬±0.58) 63.68 (¬±0.78) 1.64 (¬±0.66) 89.43 (¬±0.98) 1.09 (¬±0.12)IxG+MSE 89.46 (¬±0.21) 2.18 (¬±0.47) 64.30 (¬±1.52) 1.99 (¬±0.26) 88.19 (¬±0.62) 1.50 (¬±0.10)IxG+MAE89.59 (¬±0.06)1.39 (¬±0.62) 63.30 (¬±0.49) 1.80 (¬±0.59) 88.07 (¬±1.66) 1.43 (¬±0.24)IxG+BCE 89.42 (¬±0.71) 1.87 (¬±0.45) 63.54 (¬±0.57) 1.87 (¬±0.45) 88.99 (¬±0.83) 1.36 (¬±0.58)IxG+Huber 89.50 (¬±0.51) 1.90 (¬±0.35)64.85 (¬±1.50)2.11 (¬±0.27) 87.77 (¬±1.21) 1.84 (¬±0.34)IxG+Order 89.21 (¬±1.18)0.56 (¬±0.09)64.46(¬±1.18)0.92 (¬±0.92) 92.84 (¬±0.46) 0.59 (¬±0.25)\nTable 16: ID/OOD Task Performance (Distantly-\nsupervised Human Rationales): Higher values for accuracy\nand lower values for FPRD are considered better. All models\ndisplayed are trained on the ID dataset (Stf) with distantly\nsupervised rationales (for ER criteria) and no rationales (for\nNone) and evaluated on ID and OOD test splits.\nER criterion was initially conceived for distantly-\nsupervised rationales (Huang et al., 2021), and the\nauthors of the original paper also demonstrated ex-\nperiments with rationales generated from lexicons\nwhere Order criterion leads to improvements. Our\nobservations are in-line with theirs, and we also\nshow its benefit in reducing bias in F.\nA.6 RQ3\nA.6.1 Details for Instance Prioritisation\nExperiments\nIn this section, we provide further implementation\ndetails for confidence-based instance prioritisation\nexperiments as described in Section 4.3.\nGiven that we have 3-seed runs for the None\nmodel in Table 1, we extract the confidence scores\nbased on the given metric (LC/HC/LIS/HIS), and\nthen average these confidence/importance scores\nacross the 3 seed runs to obtain a single score for\nevery instance. This process is done for training set\ninstances only. This is followed by ranking each\ninstance by the aggregated confidence metric and\nselecting the top k% of samples from this ranking.\nFor experiments with random sampling based pri-\noritisation, we generate 3 random subsets selected\nin a uniform manner.\nWhile training in this setting, we ensure that\n3334\nwithin each batch, certain (one third to be specific)\nset of instances have available rationales. For these\ninstances, we calculate the ER loss LER, whereas,\nfor the rest of the instances in the batch, we com-\npute the task loss Ltask. All prioritisation settings\nare trained with 3 different model seeds and the\naggregated results for ID and OOD datasets are\nshown in Table 4.\nk(in%) Selection Method\nSentiment AnalysisIn-Distribution Out-of-DistributionSST Amazon Yelp MoviesNone - 94.22 ( ¬±0.77) 90.72 (¬±1.36) 92.07 (¬±2.66) 89.83 (¬±6.79)100 - 94.11 ( ¬±0.38) 92.02 (¬±0.25) 94.55 (¬±0.30) 95.50 (¬±1.32)\n5 Random 94.36 (¬±0.05) 91.57 (¬±0.10) 93.36 (¬±0.15) 92.39 (¬±2.50)LC 93.14 (¬±1.97) 90.72 (¬±0.43) 93.50 (¬±0.53)93.17 (¬±1.26)HC 94.32 (¬±0.42) 91.57 (¬±0.19) 93.03 (¬±0.81) 91.33 (¬±3.09)LIS 93.92 (¬±1.07) 92.42 (¬±0.48) 94.28 (¬±0.31) 96.50 (¬±1.5)HIS 93.94 (¬±0.83) 90.58 (¬±0.95) 91.47 (¬±2.37) 92.00 (¬±4.58)\n15 Random 94.46 (¬±0.21) 90.06 (¬±1.17) 90.81 (¬±2.63) 86.22 (¬±2.94)LC 93.48 (¬±0.80) 90.12 (¬±2.66) 90.90 (¬±5.30) 83.67 (¬±14.02)HC 94.39 (¬±0.27) 90.38 (¬±1.12) 93.48 (¬±0.64) 91.33 (¬±5.11)LIS 94.25 (¬±0.37) 91.15 (¬±0.22) 94.00 (¬±0.56) 95.33 (¬±1.26)HIS 94.47 (¬±0.22) 91.13 (¬±0.60) 92.67 (¬±0.98) 93.50 (¬±3.12)\n50 Random 93.47 (¬±0.02) 90.28 (¬±1.42) 91.85 (¬±2.11) 89.78 (¬±5.68)LC 87.07 (¬±5.15) 78.82 (¬±20.68) 77.73 (¬±26.53) 76.67 (¬±19.08)HC 92.93 (¬±0.17) 92.15 (¬±0.36) 94.48 (¬±0.94) 91.00 (¬±6.50)LIS 93.17 (¬±0.55) 90.60 (¬±0.25) 92.72 (¬±0.53) 93.50 (¬±0.87)HIS 94.23 (¬±0.65) 88.85 (¬±2.67) 91.47 (¬±1.47) 93.67 (¬±1.89)\nTable 17: Instance Prioritisation Methods (with ID/OOD\nPerformance): All values are accuracy (higher the better)\non sentiment analysis. None corresponds to models trained\nwithout ER, where k= 100% corresponds to no annotation\nbudget. Each of the k = [5 ,15,50]% have 3 instance pri-\noritisation methods. ‚ñ°corresponds to cases where HC and\nRandom are significantly similar and greater than LC. ‚àócor-\nresponds to cases where HC is significantly greater than Ran-\ndom and greater than LC. ‚Ä¢corresponds to cases where all the\nthree methods are significantly similar.‚ãÑand ‚ãÜcorrespond to\ncases where the 100% ER setup is significantly similar and\ngreater than None respectively. All tests are conducted with\n(p< 0.05).\nA.7 RQ4\nA.7.1 MTurk Annotation\nIn this section, we demonstrate the MTurk exper-\niment setup. Each MTurk annotator is paid mini-\nmum wage. Figures 14, 15 and 16 demonstrate UIs\nused by MTurk annotators for time-budget experi-\nments.\nA.7.2 Task Setup\nEach task is timed and have the same set of 200 in-\nstances to be annotated. Each instance is annotated\nby three annotators.\nUsing the annotations we receive, we aggregate\nthe time taken across all annotators and instances\nto obtain a rough time estimate taken to annotate\none instance for a given task.\n3335\nFigure 14: Label + Expl: Instructions and setup for Label + Expl annotation\nFigure 15: Only Expl: Instructions and setup for Only Expl annotation\nFigure 16: Only Label: Instructions and setup for Only Label annotation\n3336",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7345699071884155
    },
    {
      "name": "Generalization",
      "score": 0.7081136107444763
    },
    {
      "name": "Annotation",
      "score": 0.6556446552276611
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6121572852134705
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5938758850097656
    },
    {
      "name": "Machine learning",
      "score": 0.5786723494529724
    },
    {
      "name": "Task (project management)",
      "score": 0.5061604380607605
    },
    {
      "name": "Test set",
      "score": 0.4687137007713318
    },
    {
      "name": "Language model",
      "score": 0.4150769114494324
    },
    {
      "name": "Focus (optics)",
      "score": 0.41068798303604126
    },
    {
      "name": "Natural language processing",
      "score": 0.39997413754463196
    },
    {
      "name": "Mathematics",
      "score": 0.11065146327018738
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}