{
  "title": "AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models",
  "url": "https://openalex.org/W4386566605",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2004970051",
      "name": "Alexandra Chronopoulou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108007937",
      "name": "Matthew Peters",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2099684778",
      "name": "Alexander Fraser",
      "affiliations": [
        "Munich Center for Machine Learning"
      ]
    },
    {
      "id": "https://openalex.org/A2105646908",
      "name": "Jesse Dodge",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2096733369",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3200245893",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4385572749",
    "https://openalex.org/W4283157527",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4221155125",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W4286856923",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W4287888691",
    "https://openalex.org/W4286901653",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4385567093"
  ],
  "abstract": "Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the performance of a PLM on new domains while obtaining strong in-domain results. We explore various approaches for choosing which adapters to combine, such as text clustering and semantic similarity. We find that using clustering leads to the most competitive results on novel domains.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2054–2063\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nAdapterSoup: Weight Averaging to Improve Generalization of Pretrained\nLanguage Models\nAlexandra Chronopoulou⋆†▽ Matthew E. Peters‡ Alexander Fraser▽ Jesse Dodge⋆‡\n▽Center for Information and Language Processing, LMU Munich, Germany\n▽Munich Center for Machine Learning, Germany\n‡Allen Institute for Artificial Intelligence, Seattle, W A\nAbstract\nPretrained language models (PLMs) are trained\non massive corpora, but often need to special-\nize to specific domains. A parameter-efficient\nadaptation method suggests training an adapter\nfor each domain on the task of language mod-\neling. This leads to good in-domain scores\nbut can be impractical for domain- or resource-\nrestricted settings. A solution is to use a related-\ndomain adapter for the novel domain at test\ntime. In this paper, we introduce Adapter-\nSoup, an approach that performs weight-space\naveraging of adapters trained on different do-\nmains. Our approach is embarrassingly par-\nallel: first, we train a set of domain-specific\nadapters; then, for each novel domain, we deter-\nmine which adapters should be averaged at test\ntime. We present extensive experiments show-\ning that AdapterSoup consistently improves\nperformance to new domains without extra\ntraining. We also explore weight averaging\nof adapters trained on the same domain with\ndifferent hyper-parameters, and show that it\npreserves the performance of a PLM on new do-\nmains while obtaining strong in-domain results.\nWe explore various approaches for choosing\nwhich adapters to combine, such as text cluster-\ning and semantic similarity. We find that using\nclustering leads to the most competitive results\non novel domains.\n1 Introduction\nLarge LMs are pre-trained using massive amounts\nof data in a self-supervised way (Peters et al., 2018;\nDevlin et al., 2019; Liu et al., 2019; Radford et al.,\n2019) and obtain general-domain knowledge. In or-\nder to adapt them to a new domain, continuing train-\ning using in-domain data has been shown to be help-\nful (Han and Eisenstein, 2019; Lee et al., 2020; Gu-\nrurangan et al., 2020). To avoid fine-tuning all pa-\nrameters, efficient methods such as domain-specific\nmixtures-of-experts (Gururangan et al., 2022) and\n⋆Correspondence to achron@cis.lmu.de or jessed@allenai.org\n†Work done during an internship at Allen AI\nSelect adapters \nfor new domain\nTrain an adapter per \ndomain D1, D2, . . . , Dk\nDomain \nAdapter \nDomain \nAdapter \nPLM \nDomain \nAdapter \nθm\n.\n.\n.\n1\nl(θD1+θD2+θD3+...+θDk−1)\nDomain \nAdapter \nAdapter \nSoup \nWeight-average \nselected adapters\nθD1\nDomain \nAdapter \nDomain \nAdapter \nDomain \nAdapter \nθD3.\n.\n.\n1\nn(θD1 +θD4 +... +θDk)\nDomain \nAdapter \nAdapter \nSoup \nθD1\nθD2\nθDk\nX\nPLM \nθm\nX\nX\nθD2\nθD3\nθDk\nDomain \nAdapter \nθD1\nDomain \nAdapter \nθD2\nDomain \nAdapter \nθD3\nDomain \nAdapter \nθDk\nFigure 1: Illustration of AdapterSoup. Starting from\nthe same random seed, an adapter is trained for each\ndomain (domain adapter) on top of a PLM. Adapter-\nSoup averages the weights of the adapters that are most\nrelated to the new domain to improve out-of-domain\nperformance of a PLM at test time. The inference cost\nis independent of the number of adapters (lor n) used.\nhierarchical domain adapters (Chronopoulou et al.,\n2022) have been proposed. Additional in-domain\ngains can be obtained using weight-space averaging\n(Wortsman et al., 2022a; Matena and Raffel, 2021).\nMotivated by this, we propose using weight-space\naveraging at test time to improve performance on\nnovel domains without extra training.\nOur approach, AdapterSoup, ensembles adapters\nin the weight space to improve performance on\nnovel domains at test time without parameter up-\ndates. To this end, we train adapters on top of a\nPLM, each in a different domain. We compare\nseveral methods for selecting which adapters to\n2054\nuse for each novel domain at test time and propose\nweight-space averaging models selected using text\nclustering. We find that AdapterSoup improves\nperformance on novel domains. We also explore\nweight averaging adapters trained in the same do-\nmain, each with a different hyper-parameter con-\nfiguration, and find that combining models trained\nwith a low learning rate provides competitive in-\ndomain scores, while averaging models trained\nwith high learning rates performs similarly to a\ngeneral-purpose PLM on novel domains.\nOur contributions are the following: 1) We pro-\npose combining domain-adapted PLMs at inference\ntime using adapters. Our approach leads to consis-\ntent gains in novel domains. We compare several\nmethods for choosing the models of the Adapter-\nSoup, concluding that text clustering provides the\nbest performance across all domains. 2) We per-\nform weight-space averaging of PLMs adapted to\nthe same domain with varied hyper-parameters us-\ning adapters. We find that we can obtain competi-\ntive in-domain scores but also preserve the general-\nization ability of a PLM.\n2 Proposed Approach\nProblem Statement.Assuming we have a PLM\nadapted to kdomains D1,...,D k, we want a model\nthat performs well in a novel domainDk+1 without\ntraining more parameters. We use the provenance\nof a piece of text (that is, the website from which\nthe text was scraped) as a proxy for textual domain.\nThis follows Chronopoulou et al. (2022); Gururan-\ngan et al. (2022).\nIf we assume that we have a PLM fine-tuned on a\nsingle domain Di with different hyper-parameters,\nwe want to combine the fine-tuned models in order\nto both obtain good in-domain performance and\npreserve the generalization ability of the PLM to\nnovel domains.\n2.1 Cross-Domain AdapterSoup\nAn illustration of the cross-domain AdapterSoup\nis provided in Figure 1. Let f(x,θm) be a PLM\nwith input data xand parameters θm ∈Rd. We add\nadapters with a parameter initialization θα. While\nin this work we parameterize θα with adapters,\nour method is general and could be extended to\nother efficient fine-tuning methods. We only fine-\ntune the adapters, without updating the parame-\nters θm of the PLM, for language modeling us-\ning cross-entropy loss. Let us assume that θ =\nFineTune(θm,θα,ϕ,D ) denote the parameters ob-\ntained by fine-tuning a PLM with adapters in a\ndomain D, using hyper-parameters ϕ.\nLet ϕbe a fixed hyper-parameter configuration.\nWe vary only the textual domain. We first train\nk different adapters, one for each of the training\ndomains. Then, we combine their weights:\nAdapterSoup(x) =f(x,1\nl\nl∑\ni=1\nθi), (1)\ni.e., we use the average of the parameters of lfine-\ntuned models, selected by one of the methods de-\nscribed in §2.3 (l<= k). If l= k, this model is a\nuniform soup (Wortsman et al., 2022a).\n2.2 Single-Domain AdapterSoup\nIn this setup, we want to learn an LM that performs\nwell in a single training domain D, while main-\ntaining the performance of the initial PLM θm in\nnovel domains. To this end, we train adapters on\nthe same domain, varying thehyper-parameter con-\nfiguration. Each of the nmodels is optimized with\ndifferent hyper-parameters ϕi, with i ∈ 1,...,n .\nWe then compute the weight-space average follow-\ning Equation 1, with l= 3. This is similar to logit\nensembling, but only adds to the PLM the inference\ncost of a single adapter, while the added inference\ncost of logit ensembling scales linearly with the\nnumber of adapters.\n2.3 Model Selection for AdapterSoup\nIn this section we describe two methods for se-\nlecting the combination of models to create our\nAdapterSoup (by weight-space averaging) which\nwill be evaluated on a novel domain Dk+1. Follow-\ning standard practice (Gururangan et al., 2022; Li\net al., 2022) we use a small amount of validation\ndata from the novel domain Dk+1 for each of the\nbelow approaches. We note that we keep the test\ndata unseen and only use it to perform our test-set\nevaluations.\nSentence similarity.We use pretrained sentence-\nBERT (Reimers and Gurevych, 2019), an approach\nthat modifies BERT (Devlin et al., 2019) using\nsiamese and triplet networks (Schroff et al., 2015)\nto obtain sentence embeddings. We compute the\nembeddings for 100 sentences from each of the\ntraining domains D1,...,D k, plus the novel domain\nDk+1. Then we compute the average cosine sim-\nilarity between each of D1,...,D k and Dk+1. We\nadd up to 5 adapters to the AdapterSoup in order of\nhighest cosine similarity (only considering models\n2055\n10 Evaluation Domains\nMethod reuters techcrunch fastco nme fool inquisitr mashable tripadv ncbi yelp Avg.\nGPT-2 (zero-shot) 21.5 27.7 27.9 28.2 23.8 22.4 27.1 40.4 20.7 36.2 27.6\nSingle Adapter Chosen Using:\n- Sentence similarity 18.9 22.0 22.0 23.1 22.9 18.4 25.3 37.0 18.2 49.4 24.4\n- Clustering 17.6 22.4 24.0 21.1 23.3 18.7 23.6 37.7 18.2 44.3 24.0\nAdapterSoup (Weight-space average):\n- Uniform 18.2 23.1 22.9 22.2 22.4 18.4 23.1 37.0 19.1 36.2 24.3\n- Sentence similarity 17.6 22.0 21.3 20.7 22.2 18.4 22.4 36.2 17.6 35.2 23.4\n-Clustering 17.3 21.8 21.3 21.1 22.2 17.8 22.2 34.8 17.6 34.8 23.1\nOracle\n- Best adapter per domain 17.6 22.0 21.5 21.1 22.9 17.8 22.2 37.0 18.2 35.9 23.6\n- Clustering + 2 best 17.3 21.8 21.3 20.7 22.0 17.6 22.0 33.4 17.6 33.4 22.7\nHierarchy adapter 16.4 20.1 20.1 20.1 22.2 16.4 22.2 33.1 18.2 34.5 22.3\nTable 1: Perplexity (↓) scores on 10 evaluation domains. All single adapter and AdapterSoup experiments have\nthe same inference cost; bold indicates the best perplexity for each novel domain and best average. We find that\nAdapterSoup using clustering as a selection method on average leads to the best out-of-domain performance.\ntrained on domains with cosine similarity greater\nthan 0.15 to Dk+1). We experimented with several\nvalues to define the threshold (3,5,10,15). We did\nnot observe significant improvement when scaling\nup from 5 to 10 adapters and for that reason, we\nused up to 5 adapters in each AdapterSoup.\nDomain clustering. Our domain clustering ap-\nproach follows Aharoni and Goldberg (2020). We\nencode 100 sequences from each of the training\ndomains using a PLM and fit a Gaussian Mixture\nModel (GMM) with 21 components (equal to the\nnumber of training domains), which gives us a do-\nmain clustering. We then use 100 sequences from\nour held-out set (not used for test-set evaluation)\nand find which clusters they are closest to. We\nadd up to 5 adapters to the AdapterSoup in order\nof which clusters the most held-out domain text is\nmapped to. If at least 10% of the sequences of the\nDk+1 is mapped to the cluster of Di, we add the\nmodel trained on Di to the AdapterSoup.\nIn-domain. To select the models that perform best\nin-domain, we exhaustively combine all models\ntrained on a single textual domain (in this case, text\nfound in the website booking.com), using combina-\ntions of size 3. Each model has been trained with\na different hyper-parameter configuration. Specifi-\ncally, we vary the learning rate and data order. We\ncompare them to the best-performing single model\nper domain and to a uniform soup.\n3 Experimental Setup\nDatasets. We assume that text found in a specific\nwebsite (e.g., tripadvisor) can be used as a proxy\nof a textual domain. We use 21 training domains\nand 10 evaluation domains (text from 21 and 10\nwebsites accordingly) from the released version\n(Dodge et al., 2021) of C4 (Raffel et al., 2020)\n(details in the Appendix). We hypothesize that the\nvariety of training domains plays an important role\nin this setting. We randomly sampled domains that\nbelong to the 100 high-resource domains of C4,\nbut further work could consider using M2D2 (Reid\net al., 2022), a multi-domain language modeling\ndataset released concurrently to this work.\nModel Architecture. We use GPT-2 (Radford\net al., 2019); specifically, we use a publicly avail-\nable pretrained checkpoint of the small version, i.e.,\ngpt2 from the HuggingFace library (Wolf et al.,\n2020). We add an adapter to each Transformer\n(Vaswani et al., 2017) layer after the feed-forward\nlayer. We train only the adapters for language mod-\neling in each training domain. The adapters follow\nthe Bapna and Firat (2019) architecture and have\nbottleneck size 64. For the cross-domain Adapter-\nSoup, we train all models with an initial learning\nrate 1e-4. For the single-domain AdapterSoup, we\nuse different learning rates and data seeds shown\nin the Appendix.\n4 Results\nResults are presented in Table 1. For each experi-\nment, we evaluate both perplexity and efficiency.\n4.1 Cross-domain\nAs a first baseline, we use GPT-2 (zero-shot), with-\nout further training or additional parameters. This\nhas worse perplexity than all other approaches but\nis most efficient at inference.\nSingle Adapters.We then evaluateSentence simi-\nlarity and Clustering in the scenario where only a\nsingle adapter is chosen using each approach (this\ncan be thought of as a soup of size 1). This is an\n2056\nevaluation of how well these two approaches mea-\nsure similarity between the novel domainDk+1 and\nthe training domains; this baseline shows the per-\nformance of a single model which can be directly\ncompared to AdapterSoups. Both approaches are\nsignificantly better than GPT-2 (zero-shot), and\nClustering outperforms Sentence similarity, sug-\ngesting it is better at identifying related domains.\nAdapterSoup. We evaluate three types of Adapter-\nSoup which differ only in how the models added\nto the soup are selected. All three are equally\nas efficient at inference as using a single adapter.\nUniform is a uniform soup (weight-averaging all\ntrained models). This performs worse than all ap-\nproaches except GPT-2 (zero-shot); we hypoth-\nesize that it performs worse due to negative in-\nterference between adapters trained on unrelated\ndomains. Using Sentence similarity as described\nin §2.3 leads to marginally better scores than the\nsingle-best adapter per domain, indicating even rel-\natively naively-created soups can outperform the\nbest (oracle) single model. On 8/10 novel domains,\nthe sentence similarity AdapterSoup outperforms\nthe single adapter chosen by Sentence similarity,\nindicating that the soup leads to better performance.\nNext, using Clustering as described in §2.3 leads\nto perplexity improvements in 8/10 novel domains\ncompared to sentence similarity, indicating that the\nmethod for selecting models for the soup has a\nlarge impact. On 9/10 novel domains, the Cluster-\ning AdapterSoup outperforms the single adapter\nchosen by clustering, indicating that our approach\nleads to better performance.\nOracle Experiments and Larger Models.Best\nadapter per domain shows the performance of the\nsingle-best adapter on each novel domain. This is\nthe upper bound for a single adapter, and we see\nthat our Single Adapter Chosen Using Clustering\nmatches these scores on 3/10 novel domains, and\nis close on the rest, suggesting the clustering ap-\nproach is reasonably good. Clustering + 2 best\nshows the performance of adding the two (oracle)\nbest models to our AdapterSoup made by cluster-\ning; our clustering approach is close to these scores,\nbut there is room for future work on better choosing\nmodels for the AdapterSoup. Hierarchy adapter is\ntaken from Chronopoulou et al. (2022), and is less\nefficient in terms of both data and parameters.\nSelecting Models for the Soup. We qualita-\ntively compare the selection methods for choosing\nadapters to include in the AdapterSoup for 3 novel\nNovel Domaini Sentence Sim. Clustering\ntripadvisor booking booking\ninsiderpages insiderpages\nlonelyplanet\nncbi journals journals\nfrontiersin frontiersin\nspringer springer\nreuters csmonitor dailymail\nwired express\nentrepreneur\nTable 2: Domains of models selected for the Adapter-\nSoup using either sentence similarity or clustering. The\nclustering method seems to more accurately match each\nnovel domain to training domains that are similar to it.\nbooking frontiers journals yelp\nID OOD OOD OOD\nGPT-2 (zero-shot) 29.7 22.2 24.5 36.2\nBest single adapter 10.2 27.7 30.3 49.4\nAdapterSoup:\n- lr7e-3 27.7 23.3 24.8 37.7\n- lr4e-3 24.5 23.8 25.5 39.6\n- lr1e-3 11.5 24.0 26.3 42.5\n- lr5e-4 10.0 26.3 29.1 47.5\n- lr1e-4 10.4 27.4 30.0 48.9\nBest AdapterSoup:\n- in-domain 10.0 26.3 29.1 47.5\n- out-of-domain 26.8 22.9 24.5 37.3\nLogit ensemble 9.2 25.0 27.7 47.7\nTable 3: Perplexity scores in- and out-of-domain (re-\nspectively ID and OOD) of models trained on book-\ning.com. Low learning rates lead to good in-domain\nscores, while high learning rates improve the out-of-\ndomain performance.\ndomains in Table 2. In the case of tripadvisor, 2/3\ndomains Sentence similarity and Clustering select\nare identical, while for ncbi (science domain) both\nmethods select the same domains. When select-\ning domains similar to reuters (news), clustering\nseems to find a good match, choosing news do-\nmains. However, Sentence similarity selects do-\nmains that are not quite as related to the novel\ndomain. Reuters contains heterogeneous data, so\nthe average cosine similarity on the sentence level\nis not a suitable metric to find related domains.\n4.2 Single-domain\nIn this section we evaluate how models trained on\nthe same domain can be combined into an Adapter-\nSoup. We train a set of models using adapters\non booking.com by varying the data order and the\nlearning rate (see Appendix A.3, note our experi-\nments kept the initialization of each adapter fixed),\nthen evaluate all combinations of adapters of size3,\nand evaluate the performance of the AdapterSoup\nboth in-domain (booking.com) and on 3 held-out\ndomains. We explore this controlled setting to bet-\n2057\nter understand the setup described in Wortsman\net al. (2022a), who also noted that the learning\nrate is important; their experiments indicated that\nsmaller learning rates led to better model soups.\nOur experiments in Table 3 show a more nuanced\nresult: AdapterSoups made from adapters trained\nwith small learning rates (5e-4) performed best in-\ndomain (confirming the result from Wortsman et al.,\n2022b), but AdapterSoups made from adapters\ntrained with larger learning rates (7e-3, 4e-3, and\n7e-4) generalize better to novel domains. The num-\nber of updates for each adapter is the same, and they\nall have the same initialization, so we hypothesize\nthat AdapterSoups made from small learning rates\nact similarly to averaging across steps in gradient\ndescent, leading to a model that is closer to a local\noptimum. As for why larger learning rates leads to\nbetter generalization to novel domains, we hypoth-\nesize that each model in the AdapterSoup travels\na farther distance from the initialization, leading\nto learning somewhat more diverse representations.\nWe leave further exploration to future work.\n5 Related Work\nAs training large models from scratch has a severe\ncomputational and environmental cost (Strubell\net al., 2019; Dodge et al., 2022), efficient meth-\nods such as mixtures-of-experts (MoE) (Shazeer\net al., 2017; Fedus et al., 2021; Artetxe et al., 2022),\nadapters (Rebuffi et al., 2017; Houlsby et al., 2019;\nPfeiffer et al., 2020), and LoRA layers (Hu et al.,\n2022) have recently been proposed. Both adapters\nand MoEs have shown to work well for domain\nadaptation (Cooper Stickland et al., 2021; Gururan-\ngan et al., 2022; Chronopoulou et al., 2022). The\nhierarchy adapter (Chronopoulou et al., 2022) out-\nperforms our approach but is significantly more\nexpensive. It adds a training cost of 4LdmodeldT\n(following Kaplan et al., 2020) over the cost of\nrunning GPT-2 for a model with Llayers, dimen-\nsion dmodel, adapter bottleneck size d, average tree\ndepth T (T = 8 in the hierarchy adapter paper),\nwhile AdapterSoup needs 4Ldmodeldflops. As a\nresult, training the hierarchy adapter is a factor of\nT slower than our approach. At inference time, the\nhierarchy adapter activates 2 paths in the tree and\ninvokes a cost 4LdmodeldT ×2, i.e., inference is a\nfactor of 2T slower than our approach.\nAveraging weights of models independently fine-\ntuned on the same task (Wortsman et al., 2022a) has\nshown to improve in-domain performance. Matena\nand Raffel (2021) weight-average fine-tuned PLM\nmodels using Fisher merging to avoid intermediate\ntask training and then perform downstream fine-\ntuning. Wang et al. (2022) fine-tune MoEs using\nadapters on a downstream task and average their\nweights at test time. Our paper, however, focuses\non improving test-time scores of a model on novel\ndomains.\nWang et al. (2021) improve performance in an\nunseen (target) language by ensembling the source\nlanguage adapter and language adapters similar to\nthe target language. This approach uses weighted\nensembling of the outputs of adapters, whereas we\nensemble the weights of the adapters. AdapterSoup\nhas the inference cost of a single adapter, while\nWang et al. (2021) require inference time that scales\nlinearly to the number of adapters.\nContemporaneous work (Li et al., 2022) also ex-\nplores performance in novel domains using weight\naveraging, but uses MoEs instead of adapters.\n6 Conclusion\nA PLM can be adapted to new domains using\nadapters. However, this requires training a new set\nof adapters for each domain. We propose a method\nbased on weight-space averaging of adapters se-\nlected using text clustering. Our approach improves\nperformance on novel domains without updating\nparameters or increasing the inference cost. Future\nwork could explore more sophisticated selection\nmethods to try to match the performance of the\noracle experiments.\nLimitations\nThe conclusions we draw in this work about how\nour approach compares to other approaches (e.g.,\nour baselines) are only supported by evidence on\nthe task of language modeling, with textual do-\nmains taken from the C4 dataset. We expect such\nresults to hold more generally, but do not have ex-\nperimental evidence to support any other scenarios.\nAs with all work on language modeling, the models\nwe have trained could be used to generate language,\nbut we do not have evaluations of generated text\n(e.g., on fluency, factuality, or other common met-\nrics used to evaluate generated language). Our\npaper focuses on using adapters; while we expect\nsimilar approaches to work for other types of mod-\nels, we only have evidence to support AdapterSoup\nworking for adapters.\n2058\nAcknowledgements\nWe thank Ayyoob Imani for feedback on the final\nversion of the paper and Jonas Pfeiffer for helpful\ndiscussions. We also thank Mitchell Wortsman and\nLudwig Schmidt for preliminary comments on the\nfirst version of this idea.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pa-\nsunuru, Giridharan Anantharaman, Xian Li, Shuohui\nChen, Halil Akin, Mandeep Baines, Louis Martin,\nXing Zhou, Punit Singh Koura, Brian O’Horo, Jef-\nfrey Wang, Luke Zettlemoyer, Mona Diab, Zornitsa\nKozareva, and Veselin Stoyanov. 2022. Efficient\nlarge scale language modeling with mixtures of ex-\nperts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 11699–11732, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlexandra Chronopoulou, Matthew Peters, and Jesse\nDodge. 2022. Efficient hierarchical domain adapta-\ntion for pretrained language models. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1336–1351,\nSeattle, United States. Association for Computational\nLinguistics.\nAsa Cooper Stickland, Alexandre Berard, and Vassilina\nNikoulina. 2021. Multilingual domain adaptation\nfor NMT: Decoupling language and domain infor-\nmation with adapters. In Proceedings of the Sixth\nConference on Machine Translation, pages 578–598,\nOnline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Taylor Prewitt, Remi Tachet des Combes,\nErika Odmark, Roy Schwartz, Emma Strubell,\nAlexandra Sasha Luccioni, Noah A. Smith, Nicole\nDeCario, and Will Buchanan. 2022. Measuring the\ncarbon intensity of ai in cloud instances. In 2022\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’22, page 1877–1894, New\nYork, NY , USA. Association for Computing Machin-\nery.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1286–1305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2022. DEMix\nlayers: Disentangling domains for modular language\nmodeling. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5557–5576, Seattle, United States.\nAssociation for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248, Hong Kong,\nChina. Association for Computational Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the International Conference on Ma-\nchine Learning, Proceedings of Machine Learning\nResearch, pages 2790–2799.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\n2059\nlanguage models. In International Conference on\nLearning Representations.\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeff Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. ArXiv,\nabs/2001.08361.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A. Smith, and Luke Zettle-\nmoyer. 2022. Branch-train-merge: Embarrassingly\nparallel training of expert language models.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nMichael Matena and Colin Raffel. 2021. Merging mod-\nels with fisher-weighted averaging.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems.\nMachel Reid, Victor Zhong, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. M2D2: A massively multi-\ndomain language modeling dataset. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 964–975, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. Facenet: A unified embedding for\nface recognition and clustering. In 2015 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 815–823.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nXinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Gra-\nham Neubig. 2021. Efficient test time adapter en-\nsembling for low-resource language varieties. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 730–737, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYaqing Wang, Subhabrata Mukherjee, Xiaodong Liu,\nJing Gao, Ahmed Hassan Awadallah, and Jian-\nfeng Gao. 2022. Adamix: Mixture-of-adapter for\nparameter-efficient tuning of large language models.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\n2060\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,\nRebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-\ncos, Hongseok Namkoong, Ali Farhadi, Yair Car-\nmon, Simon Kornblith, and Ludwig Schmidt. 2022a.\nModel soups: averaging weights of multiple fine-\ntuned models improves accuracy without increasing\ninference time. In Proceedings of the 39th Interna-\ntional Conference on Machine Learning.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook\nKim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali\nFarhadi, Hongseok Namkoong, and Ludwig Schmidt.\n2022b. Robust fine-tuning of zero-shot models. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n7959–7971.\n2061\nA Appendix\nA.1 Training details\nWe build our code using PyTorch (Paszke et al.,\n2019) and the HuggingFace library (Wolf et al.,\n2020). Each model is trained on a single NVIDIA\nA100 GPU with 40GB of RAM, batch size 64 and\ngradient accumulation over 5 steps. We train each\nmodel for 20 epochs, without using early stopping.\nWe compute semantic similarity using sentence-\ntransformers1 and a publicly available pretrained\nmodel.2\nWe noticed from preliminary experiments that\nthe choice of random seed is important when aver-\naging weights of domain adapters. We empirically\nfound that averaging domain adapters initialized\nfrom different random seeds led to poor perfor-\nmance of AdapterSoup. We suggest initializing\nthe adapters from the same random seed in order\nto effectively combine adapters trained on various\ndomains.\nA.2 Dataset sizes\nWe use textual corpora from 31 of the 100 most\nhigh-resource internet domains of C4. The sizes of\nthe training domains are shown in Table 4, while\nthe sizes of the evaluation domains are shown in\nTable 5.\nA.3 Single-domain AdapterSoup\nWe present the hyper-parameters we tried in Ta-\nble 6. In this setup, we computed in- and out-\nof-domain scores for 455 different combinations\n(there are 15 models and computed all Adapter-\nSoups of size 3). The trend we observed is\nthat higher learning rates improved results out-of-\ndomain, while lower learning rates provided the\nbest in-domain scores.\nA.4 Cross-domain AdapterSoup\nWe present in Table 7 the evaluation scores of each\nof the single adapter models. Each adapter has\nbeen trained in a different training domain (column\n1), and evaluated in 10 novel domains.\n1https://github.com/UKPLab/\nsentence-transformers\n2huggingface.co/sentence-transformers/\nall-mpnet-base-v2\nInd Training Domain Train (Eval.) Tokens\n1 dailymail.co.uk 25M (3M)\n2 wired.com 18M (2M)\n3 express.co.uk 16M (2M)\n4 npr.org 25M (3M)\n5 librarything.com 3M (500K)\n6 instructables.com 25M (3M)\n7 entrepreneur.com 16M (2M)\n8 link.springer.com 28M (4M)\n9 insiderpages.com 8M (1M)\n10 ign.com 10M (1M)\n11 eventbrite.com 11M (1M)\n12 forums.macrumors.com 22M (3M)\n13 androidheadlines.com 14M (2M)\n14 glassdoor.com 4M (500K)\n15 pcworld.com 14M (2M)\n16 csmonitor.com 23M (3M)\n17 lonelyplanet.com 6M (1M)\n18 booking.com 30M (4M)\n19 journals.plos.org 53M (6M)\n20 frontiersin.org 38M (6M)\n21 medium 22M (3M)\nTable 4: Sizes of training corpora. We fine-tune GPT-2\nusing adapters on each of these domains. We perform\nweight-averaging of these 21 domain-adapted LMs.\nInd Novel Domain Train (Eval.) Tokens\n1 reuters.com 17M (2M)\n2 techcrunch.com 13M (2M)\n3 fastcompany.com 14M (2M)\n4 nme.com 5M (1M)\n5 fool.com 34M (4M)\n6 inquisitr.com 13M (2M)\n7 mashable.com 14M (2M)\n8 tripadvisor.com 7M (1M)\n9 ncbi.nlm.nih.gov 23M (3M)\n10 yelp.com 68M (6M)\nTable 5: Sizes of held-out corpora.\nHyper-parameter Value\nlearning rates 7e-3, 4e-3\n1e-3, 5e-4, 1e-4\nrandom seed 1, 2, 3\nTable 6: Hyper-parameters for single-domain Adapter-\nSoups. We exhaustively compute the AdapterSoup for\nevery combination of 3 models in this set.\n2062\nEvaluation Domains\nTraining Domain reuters techcrunch fastco nme fool inquisitr mashable tripadv. ncbi yelp Avg\ndailymail 17.6 23.6 24.0 21.1 23.3 18.4 23.6 39.6 20.5 44.3 25.6\nwired 18.0 22.0 21.5 22.0 22.9 18.2 22.2 40.0 19.9 41.3 24.8\nexpress 19.5 25.8 26.0 22.6 25.8 20.1 26.3 42.9 23.3 48.9 28.1\nnpr 20.1 25.5 25.0 27.7 23.3 20.5 23.6 42.1 21.1 42.9 27.2\nlibrarything 19.5 24.5 24.0 24.8 23.6 19.7 24.8 38.9 21.1 39.3 26.0\ninstructables 20.5 25.5 25.5 25.5 24.5 20.5 25.5 40.0 21.1 41.7 27.0\nentrepreneur 18.2 22.4 22.0 22.6 22.9 18.4 23.1 40.9 21.1 43.4 25.5\nspringer 19.7 25.0 24.5 24.5 25.3 19.9 26.8 42.9 18.4 43.8 27.1\ninsiderpages 23.1 28.8 29.1 32.1 25.5 23.1 27.9 37.7 23.3 35.9 28.7\nign 18.9 23.8 23.6 22.6 23.3 18.7 23.6 40.9 21.1 39.6 25.6\neventbrite 19.1 24.3 23.8 23.1 24.3 19.3 25.0 39.6 20.9 41.7 26.1\nmacrumors 20.3 26.0 26.3 26.3 24.5 20.9 25.5 41.3 22.4 43.4 27.7\nandroidheadlines 20.7 24.8 25.8 26.0 24.5 20.1 25.3 44.7 22.6 42.9 27.8\nglassdoor 20.7 26.0 25.8 27.7 24.8 21.1 26.8 42.5 22.0 42.5 28.0\npcworld 18.7 22.6 22.9 23.6 23.1 18.7 23.1 42.1 21.5 42.9 25.0\ncsmonitor 18.9 24.0 23.8 24.0 23.6 18.9 23.8 41.3 21.5 43.4 26.3\nlonelyplanet 20.7 26.0 25.8 25.0 25.3 20.7 26.6 40.4 22.6 42.9 27.6\nbooking 27.4 33.4 33.1 35.9 31.5 27.4 35.5 37.0 30.6 49.4 34.1\njournals 21.3 26.8 26.0 27.4 26.0 21.5 28.2 46.1 18.2 46.5 28.8\nfrontiersin 21.1 26.8 25.5 27.7 26.0 27.7 26.0 45.6 19.3 46.5 29.2\nmedium 17.8 22.2 21.8 21.3 25.0 17.8 25.3 39.3 19.9 43.4 25.4\nTable 7: We show the performance of each trained adapter (for the cross-domain setting) on the 10 evaluation\ndomains. Each model has been trained for language modeling with an initial learning rate 1e−4 for 20 epochs.\n2063",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7926760911941528
    },
    {
      "name": "Adapter (computing)",
      "score": 0.7556672692298889
    },
    {
      "name": "Cluster analysis",
      "score": 0.6548742651939392
    },
    {
      "name": "Domain adaptation",
      "score": 0.6294383406639099
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5632860064506531
    },
    {
      "name": "Test set",
      "score": 0.5589825510978699
    },
    {
      "name": "Generalization",
      "score": 0.5526031255722046
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5509301424026489
    },
    {
      "name": "Language model",
      "score": 0.5463611483573914
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5013947486877441
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4906083643436432
    },
    {
      "name": "Machine learning",
      "score": 0.45709294080734253
    },
    {
      "name": "Task (project management)",
      "score": 0.44313064217567444
    },
    {
      "name": "Domain model",
      "score": 0.41716042160987854
    },
    {
      "name": "Natural language processing",
      "score": 0.3328438699245453
    },
    {
      "name": "Domain knowledge",
      "score": 0.11699140071868896
    },
    {
      "name": "Mathematics",
      "score": 0.0949617326259613
    },
    {
      "name": "Classifier (UML)",
      "score": 0.09075602889060974
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4403386549",
      "name": "Munich Center for Machine Learning",
      "country": null
    }
  ],
  "cited_by": 18
}