{
  "title": "Episodic Transformer for Vision-and-Language Navigation",
  "url": "https://openalex.org/W3163303190",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2903163601",
      "name": "Alexander Pashevich",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A2111851554",
      "name": "Cordelia Schmid",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2102541890",
      "name": "Chen Sun",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035232877",
    "https://openalex.org/W6629631241",
    "https://openalex.org/W6676848125",
    "https://openalex.org/W6684031499",
    "https://openalex.org/W3034253961",
    "https://openalex.org/W6757724268",
    "https://openalex.org/W6777076129",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W6774959054",
    "https://openalex.org/W6747106673",
    "https://openalex.org/W2974759213",
    "https://openalex.org/W6760439459",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3034549794",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W2120221335",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W2158782408",
    "https://openalex.org/W6786772999",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6639082767",
    "https://openalex.org/W2981799368",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W6753516098",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W6751885507",
    "https://openalex.org/W6780924892",
    "https://openalex.org/W2807172739",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W3039732322",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6787082296",
    "https://openalex.org/W6784429693",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W2810720462",
    "https://openalex.org/W6602038092",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2962744691",
    "https://openalex.org/W2118781169",
    "https://openalex.org/W2979727876",
    "https://openalex.org/W2810346659",
    "https://openalex.org/W2947630374",
    "https://openalex.org/W3034201026",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6787304049",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W1962416794",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W6691476020",
    "https://openalex.org/W2189089430",
    "https://openalex.org/W2122223050",
    "https://openalex.org/W158164316",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2963367210",
    "https://openalex.org/W1933065844",
    "https://openalex.org/W6768810269",
    "https://openalex.org/W2229480318",
    "https://openalex.org/W6776907606",
    "https://openalex.org/W6636408305",
    "https://openalex.org/W6772530755",
    "https://openalex.org/W1506588809",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W2989897153",
    "https://openalex.org/W1602500555",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W2111742432",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W2963726321",
    "https://openalex.org/W3111739346",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W2964217371",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W2922052901",
    "https://openalex.org/W3126325318",
    "https://openalex.org/W3109085430",
    "https://openalex.org/W3108144224",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2998926068",
    "https://openalex.org/W3112356180",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W51104714",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W3025552214",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3031840745"
  ],
  "abstract": "Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.",
  "full_text": "Episodic Transformer for Vision-and-Language Navigation\nAlexander Pashevich1* Cordelia Schmid2 Chen Sun2,3\n1 Inria 2 Google Research 3 Brown University\nAbstract\nInteraction and navigation defined by natural language\ninstructions in dynamic environments pose significant chal-\nlenges for neural agents. This paper focuses on addressing\ntwo challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose\nEpisodic Transformer (E.T.), a multimodal transformer that\nencodes language inputs and the full episode history of vi-\nsual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate represen-\ntation that decouples understanding the visual appearance\nof an environment from the variations of natural language\ninstructions. We demonstrate that encoding the history with\na transformer is critical to solve compositional tasks, and\nthat pretraining and joint training with synthetic instruc-\ntions further improve the performance. Our approach sets\na new state of the art on the challenging ALFRED bench-\nmark, achieving 38.4% and 8.5% task success rates on seen\nand unseen test splits.\n1. Introduction\nHaving an autonomous agent performing various house-\nhold tasks is a long-standing goal of the research com-\nmunity. To benchmark research progress, several simu-\nlated environments [3, 53, 56] have recently emerged where\nthe agents navigate and interact with the environment fol-\nlowing natural language instructions. Solving the vision-\nand-language navigation (VLN) task requires the agent to\nground human instructions in its embodied perception and\naction space. In practice, the agent is often required to\nperform long compositional tasks while observing only a\nsmall fraction of the environment from an egocentric point\nof view. Demonstrations manually annotated with human\ninstructions are commonly used to teach an agent to accom-\nplish specified tasks.\nThis paper attempts to address two main challenges of\nVLN: (1) handling highly compositional tasks consisting of\nmany subtasks and actions; (2) understanding the complex\nhuman instructions that are used to specify a task. Figure 1\n*Work done as an intern at Google Research.\nFigure 1: An example of a compositional task in the ALFRED\ndataset [56] where the agent is asked to bring two vases to a cab-\ninet. We show several frames from an expert demonstration with\ncorresponding step-by-step instructions. The instructions expect\nthe agent to be able to navigate to a fireplace which is not visible\nin its current egocentric view and to remember its previous loca-\ntion by referring to it as ”where you were standing previously”.\nshows an example task that illustrates both challenges. We\nshow six key steps from a demonstration of 53 actions. To\nfulfill the task, the agent is expected to remember the loca-\ntion of a fireplace at t = 0 and use this knowledge much\nlater (at t = 31). It also needs to solve object- ( e.g. “an-\nother vase”) and location-grounded ( e.g. “where you were\nstanding previously”) coreference resolution in order to un-\nderstand the human instructions.\nAddressing the first challenge requires the agent to re-\nmember its past actions and observations. Most recent VLN\napproaches rely on recurrent architectures [39, 60, 68, 73]\nwhere the internal state is expected to keep information\nabout previous actions and observations. However, the re-\ncurrent networks are known to be inefficient in capturing\nlong-term dependencies [66] and may fail to execute long\naction sequences [25, 56]. Motivated by the success of the\nattention-based transformer architecture [65] at language\nunderstanding [9, 17] and multimodal learning [18, 59], we\npropose to use a transformer encoder to combine multi-\nmodal inputs including camera observations, language in-\nstructions, and previous actions. The transformer encoder\nhas access to the history of the entire episode to allow long-\nterm memory and outputs the action to take next. We name\nour proposed architecture Episodic Transformer (E.T.).\nAddressing the second challenge requires revisiting dif-\nferent ways to specify a task for the autonomous agent. We\nobserve that domain-specific language [22] and temporal\nlogic [24, 43] can unambiguously specify the target states\nand (optionally) their temporal dependencies, while being\ndecoupled from the visual appearance of a certain environ-\nment and the variations of human instructions. We hypoth-\nesize that using these synthetic instructions as an intermedi-\nate interface between the human and the agent would help\nthe model to learn more easily and generalize better. To this\nend, we propose to pretrain the transformer-based language\nencoder in E.T. by predicting the synthetic instructions from\nhuman instructions. We also explore joint training, where\nhuman instructions and synthetic instructions are mapped\ninto a shared latent space.\nTo evaluate the performance of E.T., we use the AL-\nFRED dataset [56] which consists of longer episodes than\nthe other vision-and-language navigation datasets [3,13,53]\nand also requires object interaction. We experimentally\nshow that E.T. benefits from full episode memory and is bet-\nter at solving tasks with long horizons than recurrent mod-\nels. We also observe significant gains by pretraining the\nlanguage encoder with the synthetic instructions. Further-\nmore, we show that when used for training jointly with nat-\nural language such intermediate representations outperform\nconventional data augmentation techniques for vision-and-\nlanguage navigation [20] and work better than image-based\nannotations [37].\nIn summary, our two main contributions are as follows.\nFirst, we propose Episodic Transformer (E.T.), an attention-\nbased architecture for vision-and-language navigation, and\ndemonstrate its advantages over recurrent models. Second,\nwe propose to use synthetic instructions as the intermediate\ninterface between the human and the agent. Both contribu-\ntions combined allow us to achieve a new state-of-the-art on\nthe challenging ALFRED dataset.\nCode and models are available on the project page1.\n2. Related work\nInstruction following agents.Building systems to under-\nstand and execute human instructions has been the subject\nof many previous works [7, 8, 10, 12, 37, 41, 46, 47, 52, 62].\nInstruction types include structured commands or logic pro-\ngrams [22,43,53], natural language [12,61], target state im-\nages [37], or a mix [38]. While earlier work focuses on\nmapping instructions and structured world states into ac-\n1https://github.com/alexpashevich/E.T.\ntions [4, 45, 51], it is desirable for the agents to be able\nto handle raw sensory inputs, such as images or videos.\nTo address this, the visual-and-language navigation (VLN)\ntask is proposed to introduce rich and unstructured vi-\nsual context for the agent to explore, perceive and execute\nupon [3, 13, 32, 33, 44]. The agent is requested to navigate\nto the target location based on human instructions and real,\nor photo-realistic image inputs, implemented as navigation\ngraphs [3, 13] or a continuous environment [32] in simula-\ntors [16, 31, 55, 63]. More recently, the ALFRED environ-\nment [56] introduces the object interaction component to\ncomplement visual-language navigation. It is a more chal-\nlenging setup as sequences are longer than in other vision-\nlanguage navigation datasets and all steps of a sequence\nhave to be executed properly to succeed. We focus on the\nALFRED environment and its defined tasks.\nTraining a neural agent for VLN.State-of-the-art models\nin language grounded navigation are neural agents trained\nusing either Imitation Learning [20], Reinforcement Learn-\ning [34], or a combination of both [60, 68]. In addition,\nauxiliary tasks, such as progress estimation [39, 40], back-\ntracking [30], speaker-driven route selection [20], cross-\nmodal matching [29, 68], back translation [60], pretrain-\ning on subtasks [74], and text-based pretraining [14, 57] are\nproposed to improve the performance and generalization of\nneural agents in seen and unseen environments. Most of\nthese approaches use recurrent neural networks and encode\nprevious observations and actions as hidden states. Our\nwork proposes to leverage transformers [65] which enables\nencoding the full episode of history for long-term naviga-\ntion and interaction. Most relevant to our approach are\nVLN-BERT [42] and Recurrent VLBERT [28], which also\nemploy transformers for VLN. Unlike our approach, VLN-\nBERT [42] trains a transformer to measure the compatibil-\nity of an instruction and a set of already generated trajec-\ntories. Concurrently, Recurrent VLBERT [28] uses an ex-\nplicit recurrent state and a pretrained VLBERT to process\none observation for each timestep, which might have diffi-\nculty solving long-horizon tasks [66] such as ALFRED. In\ncontrast, we do not introduce any recurrency and process all\nthe history of observations at once.\nMultimodal Transformers. Transformers [65] have\nbrought success to a wide range of classification and gen-\neration tasks, from language [9, 17, 65] to images [11, 19]\nand videos [23, 67]. In [48], the authors show that train-\ning transformers for long time horizon planning with RL\nis challenging and propose a solution. The convergence of\nthe transformer architecture for different problem domains\nalso leads to multimodal transformers, where a unified\ntransformer model is tasked to solve problems that require\nmultimodal information, such as visual question answer-\ning [36], video captioning and temporal prediction [59], or\nretrieval [21]. Our Episodic Transformer can be considered\nFigure 2: Episodic Transformer (E.T.) architecture. To predict the next action, the E.T. model is given a natural language instruction x1:L,\nvisual observations since the beginning of an episodev1:t, and previously taken actionsa1:t−1. Here we show an example that corresponds\nto the 6th timestep of an episode: t = 6. After processing x1:L with a transformer-based language encoder, embedding v1:t with a ResNet-\n50 backbone and passing a1:t−1 through a look-up table, the agent outputs t actions. During training we use all predicted actions for a\ngradient descent step. At test time, we apply the last action at to the environment.\na multimodal transformer, where the inputs are language\n(instructions), vision (images), and actions.\nSemantic parsing of human instructions.Semantic pars-\ning focuses on converting natural language into logic forms\nthat can be interpreted by machines. It has applications in\nquestion answering [6,70,71] and can be learned either with\npaired supervision [6, 69, 72] or weak supervision [5, 50].\nFor instruction following, semantic parsing has been ap-\nplied to map natural language into lambda calculus expres-\nsions [5] or linear temporal logic [50]. We show that rather\nthan directly using the semantic parsing outputs, it is more\nbeneficial to transfer its pretrained language encoder to the\ndownstream VLN task.\n3. Method\nWe first define the vision-and-language navigation task\nin Section 3.1 and describe the Episodic Transformer (E.T.)\nmodel in Section 3.2. We then introduce the synthetic lan-\nguage and explain how we leverage it for pretraining and\njoint training in Section 3.3.\n3.1. VLN background\nThe vision-and-language navigation task requires an\nagent to navigate in an environment and to reach a goal\nspecified by a natural language instruction. Each demon-\nstration is a tuple (x1:L, v1:T , a1:T ) of a natural language\ninstruction, expert visual observations, and expert actions.\nThe instruction x1:L is a sequence ofL word tokens xi ∈ R.\nThe visual observations v1:T is a sequence of T camera im-\nages vt ∈ RW×H×3 where T is the demonstration length\nand W × H is the image size. The expert actions a1:T is a\nsequence of T action type labels at ∈ {1, . . . , A} used by\nthe expert and A is the number of action types.\nThe goal is to learn an agent function f that approxi-\nmates the expert policy. In the case of a recurrent archi-\ntecture, the agent predicts the next action ˆat given a lan-\nguage instruction x1:L, a visual observation vt, the previ-\nously taken action ˆat−1, and uses its hidden state ht−1 to\nkeep track of the history:\nˆat, ht = f(x1:L, vt, ˆat−1, ht−1). (1)\nFor an agent with full episode observability, all previ-\nous visual observations v1:t and all previous actions ˆa1:t−1\nare provided to the agent directly and no hidden state is re-\nquired:\nˆat = f(x1:L, v1:t, ˆa1:t−1). (2)\n3.2. Episodic Transformer model\nOur Episodic Transformer (E.T.) model shown in Fig-\nure 2 relies on attention-based multi-layer transformer en-\ncoders [65]. It has no hidden state and observes the full his-\ntory of visual observations and previous actions. To inject\ninformation about the order of words, frames, and action\nsequences, we apply the sinusoidal encoding to transformer\ninputs. We refer to this encoding as positional encoding for\nlanguage tokens and temporal encoding for expert observa-\ntions and actions.\nFigure 3: Training with natural and synthetic language. Left: We pretrain the language encoder of the model to translate natural language\ninstructions to synthetic language. Due to a more task-oriented synthetic representation, the language encoder learns better representations.\nWe use the language encoder weights to initialize the language encoder of the agent (shown in yellow). Right: We jointly use demonstra-\ntions annotated with natural language and demonstrations annotated with synthetic language to train the agent. Due to the larger size of the\nsynthetic language dataset, the resulting agent has better performance even when evaluated on natural language annotations.\nOur E.T. architecture consists of four encoders: language\nencoder, visual encoder, action encoder, and multimodal en-\ncoder. The language encoder shown in the bottom-left part\nof Figure 2 gets instruction tokens x1:L as input. It consists\nof a look-up table and a multi-layer transformer encoder and\noutputs a sequence of contextualized language embeddings\nhx\n1:L. The visual encoder shown in the bottom-center part\nof Figure 2 is a ResNet-50 backbone [27] followed by 2\nconvolutional and 1 fully-connected layers. The visual en-\ncoder projects a visual observation vt into its embedding\nhv\nt . All the episode visual observations v1:T are projected\nindependently using the same encoder. The action encoder\nis a look-up table shown in the bottom-right part of Figure 2\nwhich maps action types a1:T into action embeddings ha\n1:T .\nThe multimodal encoder is a multi-layer transformer\nencoder shown in the middle of Figure 2. Given the\nconcatenated embeddings from modality-specific encoders\n(hx\n1:L, hv\n1:T , ha\n1:T ), the multimodal encoder returns output\nembeddings (zx\n1:L, zv\n1:T , za\n1:T ). The multimodal encoder\nemploys causal attention [65] to prevent visual and action\nembeddings from attending to subsequent timesteps. We\ntake the output embeddings zv\n1:T and add a single fully-\nconnected layer to predict agent actions ˆa1:T .\nDuring E.T. training, we take advantage of the sequential\nnature of the transformer architecture. We input a language\ninstruction x1:L as well as all visual observations v1:T and\nall actions a1:T of an expert demonstration to the model.\nThe E.T. model predicts all actions ˆa1:T at once as shown\nat the top of Figure 2. We compute and minimize the cross-\nentropy loss between predicted actions ˆa1:T and expert ac-\ntions a1:T . During testing at timestep t, we input visual ob-\nservations v1:t up to a current timestep and previous actions\nˆa1:t−1 taken by the agent. We select the action predicted for\nthe last timestepˆat, ˆct and apply it to the environment which\ngenerates the next visual observation vt+1. In Figure 2 we\nshow an example that corresponds to the 6th timestep of an\nepisode where the action Left will be taken next.\n3.3. Synthetic language\nTo improve understanding of human instructions that\npresent a wide range of variability, we propose to pretrain\nthe agent language encoder with a translation into a syn-\nthetic language, see Figure 3 (left). We also generate ad-\nditional demonstrations, annotate them with synthetic lan-\nguage and jointly train the agent using both synthetic and\nnatural language demonstrations, see Figure 3 (right).\nAn example of the synthetic language and a corre-\nsponding natural language instruction is shown in Figure 3\n(left). The synthetic annotation is generated for each expert\ndemonstration using the expert path planner arguments. In\nALFRED, each expert path is defined with Planning Do-\nmain Definition Language (PDDL) [22] which consists of\nseveral subgoal actions. Each subgoal action has a type and\na target class, e.g. Put Apple Table or Goto Bed\nwhich we use as a synthetic annotation for this subgoal ac-\ntion. Note that such annotation only defines a class but not\nan instance of the target. We annotate each expert demon-\nstration with subgoal action annotations concatenated in\nchronological order to produce a synthetic annotationy1:M .\nWe use synthetic language to pretrain the language en-\ncoder of the agent on a sequence-to-sequence (seq2seq)\ntranslation task. The translation dataset consists of corre-\nsponding pairs (x1:L, y1:M ) of natural and synthetic instruc-\ntions. The translation model consists of a language encoder\nand a language decoder as shown in Figure 3 (left). The\nlanguage encoder is identical to the agent language encoder\ndescribed in Section 3.2. The language decoder is a multi-\nlayer transformer decoder with positional encoding and the\nsame hyperparameters as the encoder. Given a natural lan-\nguage annotation x1:L, we use the language encoder to pro-\nduce embeddings h1:L. The embeddings are passed to the\nlanguage decoder which predicts N translation tokens ˆyi.\nWe train the model by minimizing the cross-entropy loss\nbetween predictions ˆy1:N and synthetic annotations y1:M .\nOnce the training converges, we use the weights of the\ntranslator language encoder to initialize the language en-\ncoder of the agent.\nWe also explore joint training by generating an addi-\ntional dataset of expert demonstrations annotated with syn-\nthetic language. We use the AI2-THOR simulator [31] and\nscripts provided by Shridhar et al . [56]. Apart from the\nannotations, the synthetic dataset differs from the original\none in terms of objects configurations and agent initial po-\nsitions. We train the agent to predict actions using both nat-\nural and synthetic language datasets as shown on the right\nin Figure 3. We use the same language, vision, and action\nencoders for both datasets but two different look-up tables\nfor natural and synthetic language tokens which we found\nto work the best experimentally. For both datasets, we sam-\nple batches of the same size, compute the two losses and do\na single gradient descent step. After a fixed number of train-\ning epochs, we evaluate the agent on natural and synthetic\nlanguage separately using the same set of validation tasks.\n4. Results\nIn this section, we ablate different components of E.T.\nand compare E.T. with state-of-the-art methods. First, we\ndescribe the experimental setup and the dataset in Sec-\ntion 4.1. Next, we compare our method to a recurrent base-\nline and highlight the importance of full episode observ-\nability in Section 4.2. We then study the impact of joint\ntraining and pretraining with synthetic instructions in Sec-\ntion 4.3 and compare with previous state-of-the-art methods\non the ALFRED dataset in Section 4.4.\n4.1. Experimental setup\nDataset. The ALFRED dataset [56] consists of demon-\nstrations of an agent performing household tasks following\ngoals defined with natural language. The tasks are compo-\nsitional with nonreversible state changes. The dataset in-\ncludes 8, 055 expert trajectories (v1:T , a1:T ) annotated with\n25, 743 natural language instructions x1:L. It is split into\n21, 023 train, 1, 641 validation, and 3, 062 test annotations.\nThe validation and test folds are divided into seen splits\nwhich contain environments from the train fold and unseen\nsplits which contain new environments. To leverage syn-\nthetic instructions to pretrain a language encoder, we pair\nevery annotated instructionx1:L with its corresponding syn-\nthetic instruction y1:M in the train fold. For joint training,\nwe generate 44, 996 demonstrations (y1:M , v1:T , a1:T ) from\nthe train environments annotated automatically with syn-\nthetic instructions. For ablation studies in Section 4.2 and\nSection 4.3, we use the validation folds only. For compari-\nson with state-of-the-art in Section 4.4, we report results on\nboth validation and test folds.\nBaselines. In Section 4.2, we compare our model to a model\nbased on a bi-directional LSTM [56]. We use the same hy-\nperparameters as Shridhar et al. [56] and set the language\nencoder hidden size to 100, the action decoder hidden size\nto 512, the visual embeddings size to 2500, and use 0.3\ndropout for the decoder hidden state. We experimentally\nfind the Adam optimizer with no weight decay and a weight\ncoefficient 0.1 for the target class cross-entropy loss to work\nbest. The LSTM model uses the same visual encoder as the\nE.T. model. In Section 4.4, we also compare our model to\nMOCA [58] and the model of Nguyen et al. [64].\nEvaluation metrics. For our ablation studies in Sections\n4.2 and 4.3, we report agent success rates. To understand\nthe performance difference with recurrent-based architec-\ntures in Section 4.2, we also report success rates on individ-\nual subgoals. This metric corresponds to the proportion of\nsubgoal tasks completed after following an expert demon-\nstration until the beginning of the subgoal and conditioned\non the entire language instruction. We note that the aver-\nage task length is 50 timesteps while the average length of\na subgoal is 7 timesteps.\nImplementation details. Among the 13 possible action\ntypes, 7 actions involve interacting with a target object in\nthe environment. The target object of an action at is chosen\nwith a binary mask mt ∈ {0, 1}W×H that specifies the pix-\nels of visual observation vt that belong to the target object.\nThere are 119 object classes in total. The pixel masks mt\nare provided along with expert demonstrations during train-\ning. We follow Singhet al. [58] and ask our agent to predict\nthe target object class ct, which is then used to retrieve the\ncorresponding pixel mask ˆmt generated by a pretrained in-\nstance segmentation model. The segmentation model takes\nvt as input and outputs (ˆct, ˆmt).\nThe agent observations are resized to 224 × 224. The\nmask generator receives images of size300×300 following\nSingh et al. [58]. Both the visual encoder and the mask gen-\nerator are pretrained on a dataset of 325K frames of expert\ndemonstrations from the train fold and corresponding class\nsegmentation masks. We use ResNet-50 Faster R-CNN [54]\nfor the visual encoder pretraining and ResNet-50 Mask R-\nCNN [26] for the mask generator. We do not update the\nmask generator and the visual encoder ResNet backbone\nduring the agent training. In the visual encoder, ResNet fea-\ntures are average-pooled4 times to reduce their size and0.3\ndropout is applied. Resulting feature maps of512×7×7 are\nfed into 2 convolutional layers with256 and 64 filters of size\n1 by 1 and mapped into an embedding of the size 768 with\na fully connected layer. Both transformer encoders of E.T.\nhave 2 blocks, 12 self-attention heads, and the hidden size\nof 768. We use 0.1 dropout inside transformer encoders.\nWe use the AdamW optimizer [35] with 0.33 weight de-\ncay and train the model for 20 epochs. Every epoch in-\ncludes 3, 750 batches of 8 demonstrations each. For joint\ntraining, each batch consists of 4 demonstrations with hu-\nman instructions and 4 demonstrations with synthetic in-\nModel Task Sub-goal\nSeen Unseen Seen Unseen\nLSTM 23.2 2 .4 75.5 58 .7\nLSTM + E.T. enc. 27.8 3.3 76.6 59 .5\nE.T. 33.8 3.2 77.3 59.6\nTable 1: Comparison of E.T. and LSTM architectures: (1) an\nLSTM-based model [56], (2) an LSTM-based model trained with\nthe transformer language encoder of the E.T. model, (3) E.T., our\ntransformer-based model. All models are trained using the natural\nlanguage dataset only and evaluated on validation folds. The two\nparts of the table show the success rate for tasks (average length\n50) and sub-goals (average length 7). While the sub-goal success\nrates of all models are relatively close, E.T. outperforms both re-\ncurrent agents on full tasks which highlights the importance of the\nfull episode observability.\nVisible Frames Actions\nSeen Unseen Seen Unseen\nNone 0.5 0 .2 23.7 1 .7\n1 last 28.9 2 .2 33.8 3.2\n4 last 31.5 2 .0 32.0 2 .4\n16 last 33.5 2 .9 31.1 2 .8\nAll 33.8 3.2 27.1 2 .2\nTable 2: Ablation on accessible history length of E.T., in terms of\nvisual frames (left two columns) and actions (right two columns).\nstructions. For all experiments, we use a learning rate of\n10−4 during the first 10 epochs and 10−5 during the last\n10 epochs. Following Shridhar et al. [56], we use auxiliary\nlosses for overall and subgoal progress [39] which we sum\nto the model cross-entropy loss with weights 0.1. All the\nhyperparameter choices were made using a moderate size\ngrid search. Once the training is finished, we evaluate ev-\nery 2-nd epoch on the validation folds. Following Singh et\nal. [58], we use Instance Association in Time and Obstruc-\ntion Detection modules during evaluation.\n4.2. Model analysis\nComparison with recurrent models.To validate the gain\ndue to the episodic memory, we compare the E.T. archi-\ntecture with a model based on a recurrent LSTM architec-\nture. We train both models using the dataset with natural\nlanguage annotations only. As shown in Table 1, the recur-\nrent model succeeds in23.2% of tasks in seen environments\nand in 2.4% of tasks in unseen environments. E.T. succeeds\nin 33.8% and 3.2% of tasks respectively which is a relative\nimprovement of 45.6% and 33.3% compared to the LSTM-\nbased agent. However, the success rate computed for in-\ndividual subgoals shows only 2.3% and 1.5% of relative\nimprovement of E.T. over the recurrent agent in seen and\nunseen environments respectively. We note that a task con-\nsists on average of 6.5 subgoals which makes the long-term\nmemory much more important for solving full tasks.\nTo understand the performance difference, we train an\nLSTM-based model with the E.T. language encoder. Given\nthat both LSTM and E.T. agents receive the same visual\nfeatures processed by the frozen ResNet-50 backbone and\nhave the same language encoder architecture, the princi-\npal difference between the two models is the processing of\nprevious observations. While the E.T. agent observes all\nprevious frames using the attention mechanism, the LSTM-\nbased model relies on its recurrent state and explicitly ob-\nserves only the last visual frame. The recurrent model per-\nformance shown in the 2-nd row of Table 1 is similar to the\nE.T. performance in unseen environments but is 17.7% less\nsuccessful than E.T. in seen environments. This comparison\nhighlights the importance of the attention mechanism and\nfull episode observability. We note that E.T. needs only one\nforward pass for a gradient descent update on a full episode.\nIn contrast, the LSTM models need to do a separate forward\npass for each episode timestep which significantly increases\ntheir training time with respect to E.T. models. We further\ncompare how E.T. and LSTM models scale with additional\ndemonstrations in Section 4.3.\nAccessible history length. We train E.T. using different\nlengths of the episode history observed by the agent in terms\nof visual frames and previous actions and show the results in\nTable 2. The first two columns of Table 2 compare different\nlengths of visual observations history from no past frames\nto the entire episode. The results indicate that having access\nto all visual observations is important for the model perfor-\nmance. We note that the performance of the model with 16\ninput frames is close to the performance of the full episode\nmemory agent, which can be explained by the average task\nlength of 50 timesteps.\nThe last two columns of Table 2 show that the agent does\nnot benefit from accessing more than one past action. This\nbehavior can be explained by the “causal misidentification”\nphenomenon: access to more information can yield worse\nperformance [15]. It can also be explained by poor general-\nizability due to the overfitting of the model to expert demon-\nstrations. We also note that the model observing no previ-\nous actions is 29.8% and 46.8% relatively less successful\nin seen and unseen environments than the agent observing\nthe last action. We, therefore, fix the memory size to be un-\nlimited for visual observations and to be 1 timestep for the\nprevious actions.\nModel capacity. Transformer-based models are known to\nbe expressive but prone to overfitting. We study how the\nmodel capacity impacts the performance while training on\nthe original ALFRED dataset. We change the number of\ntransformer blocks in the language encoder and the multi-\nmodal encoder and report results in Table 3. The results\nindicate that the model with a single transformer block is\n# Blocks Seen Unseen\n1 25.0 1 .6\n2 33.8 3.2\n3 28.6 2 .2\n4 19.8 1 .1\nTable 3: Ablation of E.T. model capacity. We compare E.T. mod-\nels with different number of transformer blocks in language and\nmultimodal encoders.\nSynthetic instr. Test on synthetic Test on human\nSeen Unseen Seen Unseen\nExpert frames 54.0 6.1 28.5 3 .4\nSpeaker text 36.3 3 .1 37.4 3 .9\nSubgoal actions 47.2 5 .9 38.5 5.4\nNo synthetic - - 33.8 3 .2\nTable 4: Comparison of different synthetic instructions used for\njoint training. We jointly train E.T. using demonstrations with\nhuman annotations and demonstrations with different types of syn-\nthetic instructions. In the first two columns, we evaluate the re-\nsulting models using the same type of synthetic annotations that\nis used during training. In the last two columns, the models are\nevaluated on human annotated instructions.\nTrain data LSTM E.T.\nSeen Unseen Seen Unseen\nHuman annotations 23.2 2 .4 33.8 3 .2\nHuman + synthetic 25.2 2 .9 38.5 5.4\nTable 5: Comparison of an LSTM-based model and E.T. trained\njointly with demonstrations annotated by subgoal actions. The\nresults indicate that E.T. scales better with additional data than the\nLSTM-based agent.\nnot expressive enough and the models with 3 and 4 blocks\noverfit to the train data. The model with 2 blocks represents\na trade-off between under- and overfitting and we, therefore,\nkeep this value for all the experiments.\nAttention visualization.We visualize text and visual atten-\ntion heatmaps in Appendices A.4 and A.5 of [49].\n4.3. Training with synthetic annotations\nJoint training. We train the E.T. model using the origi-\nnal dataset of 21, 023 expert demonstrations annotated with\nnatural language and the additionally generated dataset of\n44, 996 expert demonstrations with synthetic annotations.\nWe compare three types of synthetic annotations: (1) di-\nrect use of visual embeddings from the expert demonstra-\ntion frames, no language instruction is generated. A sim-\nilar approach can be found in Lynch and Sermanet [38];\n(2) train a model to generate instructions,e.g. with a speaker\nmodel [20], where the inputs are visual embeddings from\nthe expert demonstration frames, and the targets are human-\nannotated instructions; and (3) subgoal actions and objects\nannotations described in Section 3.3. For (1), we experi-\nmentally find using all expert frames from a demonstration\nworks significantly better than a subset of frames. The vi-\nsual embeddings used in (1) and (2) are extracted from a\npretrained frozen ResNet-50 described in Section 4.1. To\ngenerate speaker annotations, we use a transformer-based\nseq2seq model (Section 3.3) with the difference that the in-\nputs are visual embeddings instead of text.\nWe report success rates of models trained jointly and\nevaluated independently on synthetic and human-annotated\ninstructions in Table 4. The results are reported on the val-\nidation folds. The model trained on expert frames achieves\nthe highest performance when evaluated on synthetic in-\nstructions. However, when evaluated on human instruc-\ntions, this model has 15.6% relatively lower success rate in\nseen environments than the baseline without joint training.\nThis indicates that the agent trained to take expert frames as\ninstructions does not generalize well to human instructions.\nUsing speaker translation annotations improves over the no\njoint training baseline by 10.6% and 21.8% in seen and un-\nseen environments respectively. Furthermore, our proposed\nsubgoal annotations bring an even larger relative improve-\nment of 13.9% and 68.7% in seen and unseen environments\nwhich highlights the benefits of joint training with synthetic\ninstructions in the form of subgoal actions.\nFinally, we study if the recurrent baseline also benefits\nfrom joint training with synthetic data. Table 5 shows that\nthe relative gains of joint training are 2.3 and 4.4 times\nhigher for E.T. than for the LSTM-based agent in seen and\nunseen environments respectively. These numbers clearly\nshow that E.T. benefits more from additional data and con-\nfirms the advantage of our model over LSTM-based agents.\nLanguage encoder pretraining. Another application of\nsynthetic instructions is to use them as an intermediate rep-\nresentation that decouples the visual appearance of an en-\nvironment from the variations of human-annotated instruc-\ntions. For this purpose, we pretrain the E.T. language en-\ncoder with the synthetic instructions. In particular, we pre-\ntrain a seq2seq model to map human instructions into syn-\nthetic instructions as described in Section 3.3, and study\nwhether it is more beneficial to transfer explicitly the “trans-\nlated” text or implicitly as representations encoded by the\nmodel weights. Our pretraining is done on the original\ntrain fold with no additionally generated trajectories. The\nseq2seq translation performance is very competitive, reach-\ning 97.1% in terms of F1 score. To transfer explicitly\nthe translated (synthetic) instructions, we first train an E.T.\nagent to follow synthetic instructions on the training fold\nand then evaluate the agent on following human instructions\nby translating these instructions into synthetic ones with our\npretrained seq2seq model.\nObjective Transfer Seen Unseen\nNone - 33.8 3 .2\nBERT Text embedding 32.3 3 .4\nSeq2seq Translated text 35.2 3 .6\nSeq2seq Text encoder 37.6 3.8\nTable 6: Comparison of models with different language encoder\npretraining strategies. We pretrain a seq2seq model to map hu-\nman instructions into synthetic instructions and transfer either its\noutput text (third row) or its learned weights (fourth row). For\ncompleteness, we also compare with no pretraining (first row) and\nBERT pretraining (second row).\nTable 6 compares these two pretraining strategies. We\ncan see that both strategies outperform the no pretraining\nbaseline (first row) significantly and that transferring the\nencoder works better than explicit translation. For com-\npleteness, we also report results with BERT pretraining [17]\n(second row). The BERT model is pretrained on generic\ntext data ( e.g. Wikipedia). We use the BERT base model\nwhose weights are released by the authors. We extract its\noutput contextualized word embeddings and use them as the\ninput word embeddings to the language encoder. To our sur-\nprise, when compared with the no pretraining baseline, the\nBERT pretraining decreases the performance in seen envi-\nronments by 4.4% and brings a marginal improvement of\n6.2% relative in unseen environments. We conjecture that\ndomain-specific language pretraining is important for the\nALFRED benchmark. Overall, these experiments show an-\nother advantage of the proposed synthetic annotations and\nhighlight the importance of intermediate language represen-\ntations to better train instruction-following agents.\nWe finally combine the language encoder pretraining and\nthe joint training objectives and present the results in Ta-\nble 7. We observe that these two strategies are complemen-\ntary to each other: the overall relative improvements of in-\ncorporating synthetic data over the baseline E.T. model are\n37.8% and 228.1% in seen and unseen environments, re-\nspectively. We conclude that synthetic data is especially\nimportant for generalization to unseen environments. A\ncomplete breakdown of performance improvements can be\nfound in Appendix A.2 of [49].\n4.4. Comparison with state-of-the-art\nWe compare the E.T. agent with models with associated\ntech reports on the public leaderboard2. The results on val-\nidation and test folds are shown in Table 8. The complete\ntable with solved goal conditions and path-length-weighted\nscores [2] is given in Appendix A.1 of [49] . The E.T. model\ntrained without synthetic data pretraining and joint train-\ning sets a new state-of-the-art on seen environments (row\n2https://leaderboard.allenai.org/alfred, the results\nwere submitted on February 22, 2021.\nPretraining Joint training Seen Unseen\n33.8 3 .2\n✓ 37.6 3 .8\n✓ 38.5 5 .4\n✓ ✓ 46.6 7.3\nTable 7: Ablation study of joint training and language encoder\npretraining with synthetic data. We present baseline results with-\nout leveraging synthetic data (first row), the independent perfor-\nmance of pretraining (second row) and joint training (third row),\nand their combined performance (fourth row).\nModel Validation Test\nSeen Unseen Seen Unseen\nShridhar et al. [56] 3.70 0 .00 3.98 0 .39\nNguyen et al. [64] N/A N/A 12.39 4 .45\nSingh et al. [58] 19.15 3 .78 22.05 5 .30\nE.T. 33.78 3 .17 28.77 5 .04\nE.T. (pretr.) 37.63 3 .76 33.46 5 .56\nE.T. (pretr. & joint tr.) 46.59 7.32 38.42 8.57\nHuman performance - - - 91.00\nTable 8: Comparison with the models submitted to the public\nleaderboard on validation and test folds. The highest value per fold\nis shown in blue. ‘N/A’ denotes that the scores are not reported on\nthe leaderboard or in an associated publication. Our method sets a\nnew state-of-the-art on all metrics.\n4). By leveraging synthetic instructions for pretraining, our\nmethod outperforms the previous methods [56, 58, 64] and\nsets a new state-of-the-art on all metrics (row 5). Given ad-\nditional 45K trajectories for joint training, the E.T. model\nfurther improves the results (row 6).\n5. Conclusion\nWe propose E.T., a transformer-based architecture for\nvision-and-language navigation tasks. E.T. observes the full\nepisode history of vision, language, and action inputs and\nencodes it with a multimodal transformer. On the ALFRED\nbenchmark, E.T. outperforms competitive recurrent base-\nlines and achieves state-of-the-art performance on seen en-\nvironments. We also propose to use synthetic instructions\nfor pretraining and joint training with human-annotated in-\nstructions. Given the synthetic instructions, the perfor-\nmance is further improved in seen and especially, in un-\nseen environments. In the future, we want to explore other\nforms of synthetic annotations and techniques to automati-\ncally construct them, for example with object detectors.\nAcknowledgement: We thank Peter Anderson, Ellie\nPavlick, and Dylan Ebert for helpful feedback on the draft.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying attention\nflow in transformers. In ACL, 2020. 12\n[2] Peter Anderson, Angel X. Chang, Devendra Singh Chaplot,\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\nand Amir Roshan Zamir. On evaluation of embodied navi-\ngation agents. arXiv preprint arXiv:1807.06757 , 2018. 8,\n12\n[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S ¨underhauf, Ian Reid, Stephen Gould, and\nAnton van den Hengel. Vision-and-Language Navigation:\nInterpreting visually-grounded navigation instructions in real\nenvironments. In CVPR, 2018. 1, 2\n[4] Jacob Andreas and Dan Klein. Alignment-based composi-\ntional semantics for instruction following. In EMNLP, 2015.\n2\n[5] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learn-\ning of semantic parsers for mapping instructions to actions.\nTACL, 2013. 3\n[6] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. Semantic parsing on freebase from question-answer\npairs. In EMNLP, 2013. 3\n[7] Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas\nRoy, and Daniela Rus. Interpreting and executing recipes\nwith a cooking robot. Experimental Robotics, 2013. 2\n[8] Satchuthananthavale RK Branavan, Harr Chen, Luke S\nZettlemoyer, and Regina Barzilay. Reinforcement learning\nfor mapping instructions to actions. In ACL, 2009. 2\n[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In NeurIPS, 2020. 1, 2\n[10] Guido Bugmann, Stanislao Lauria, Theocharis Kyriacou,\nEwan Klein, Johan Bos, and Kenny Coventry. Using verbal\ninstructions for route learning: Instruction analysis. Proc.\nTIMR, 2001. 2\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 2\n[12] David Chen and Raymond Mooney. Learning to interpret\nnatural language navigation instructions from observations.\nIn AAAI, 2011. 2\n[13] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely,\nand Yoav Artzi. Touchdown: Natural language navigation\nand spatial reasoning in visual street environments. InCVPR,\n2019. 2\n[14] Marc-Alexandre C ˆot´e, ´Akos K ´ad´ar, Xingdi Yuan, Ben Ky-\nbartas, Tavian Barnes, Emery Fine, James Moore, Matthew\nHausknecht, Layla El Asri, Mahmoud Adada, et al.\nTextworld: A learning environment for text-based games. In\nWorkshop on Computer Games, 2018. 2\n[15] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal\nconfusion in imitation learning. In NeurIPS, 2019. 6\n[16] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha\nKembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador,\nDustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca\nWeihs, Mark Yatskar, and Ali Farhadi. Robothor: An open\nsimulation-to-real embodied ai platform. In CVPR, 2020. 2\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 1, 2,\n8\n[18] David Ding, Felix Hill, Adam Santoro, and Matt Botvinick.\nObject-based attention for spatio-temporal reasoning: Out-\nperforming neuro-symbolic models with flexible distributed\narchitectures. arXiv preprint arXiv:2012.08508, 2020. 1\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 2\n[20] Daniel Fried, Ronghang Hu, V olkan Cirik, Anna Rohrbach,\nJacob Andreas, Louis-Philippe Morency, Taylor Berg-\nKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.\nSpeaker-follower models for vision-and-language naviga-\ntion. In NeurIPS, 2018. 2, 7\n[21] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid. Multi-modal transformer for video retrieval. In\nECCV, 2020. 2\n[22] M. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram,\nM. Veloso, D. Weld, and D. Wilkins. PDDL: The Planning\nDomain Definition Language, 1998. 2, 4\n[23] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n2\n[24] Nakul Gopalan, Dilip Arumugam, Lawson Wong, and Ste-\nfanie Tellex. Sequence-to-sequence language grounding of\nnon-markovian task specifications. In RSS, 2018. 2\n[25] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim\nHarley, Ivo Danihelka, Agnieszka Grabska-Barwi ´nska,\nSergio G ´omez Colmenarejo, Edward Grefenstette, Tiago\nRamalho, John Agapiou, Adri `a Puigdom `enech Badia,\nKarl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam\nCain, Helen King, Christopher Summerfield, Phil Blunsom,\nKoray Kavukcuoglu, and Demis Hassabis. Hybrid comput-\ning using a neural network with dynamic external memory.\nNature, 2016. 1\n[26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 5\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 4\n[28] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-\nOpazo, and Stephen Gould. A Recurrent Vision-and-\nLanguage BERT for Navigation. CVPR, 2021. 2\n[29] Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku,\nGabriel Magalhaes, Jason Baldridge, and Eugene Ie. Trans-\nferable representation learning in vision-and-language navi-\ngation. In ICCV, 2019. 2\n[30] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe\nGan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha\nSrinivasa. Tactical rewind: Self-correction via backtracking\nin vision-and-language navigation. In CVPR, 2019. 2\n[31] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vander-\nBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke\nZhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An\nInteractive 3D Environment for Visual AI. arXiv preprint\narXiv:1712.05474, 2017. 2, 5\n[32] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,\nand Stefan Lee. Beyond the nav-graph: Vision-and-language\nnavigation in continuous environments. In ECCV, 2020. 2\n[33] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie,\nand Jason Baldridge. Room-Across-Room: Multilingual\nVision-and-Language Navigation with Dense Spatiotempo-\nral Grounding. EMNLP, 2020. 2\n[34] Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu,\nYueting Zhuang, and William Yang Wang. Unsupervised re-\ninforcement learning of transferable meta-skills for embod-\nied navigation. In CVPR, 2020. 2\n[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 5\n[36] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In NeurIPS, 2019. 2\n[37] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar,\nJonathan Tompson, Sergey Levine, and Pierre Sermanet.\nLearning latent plans from play. In CoRL, 2019. 2\n[38] Corey Lynch and Pierre Sermanet. Grounding language in\nplay. RSS, 2021. 2, 7\n[39] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib,\nZsolt Kira, Richard Socher, and Caiming Xiong. Self-\nmonitoring navigation agent via auxiliary progress estima-\ntion. In ICLR, 2019. 1, 2, 6\n[40] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming\nXiong, and Zsolt Kira. The regretful agent: Heuristic-aided\nnavigation through progress estimation. In CVPR, 2019. 2\n[41] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers.\nWalk the talk: Connecting language, knowledge, and action\nin route instructions. In AAAI, 2006. 2\n[42] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter An-\nderson, Devi Parikh, and Dhruv Batra. Improving vision-\nand-language navigation with image-text pairs from the web.\nIn ECCV, 2020. 2\n[43] Zohar Manna and Amir Pnueli. The Temporal Logic of Reac-\ntive and Concurrent Systems. Springer-Verlag, Berlin, Hei-\ndelberg, 1992. 2\n[44] Harsh Mehta, Yoav Artzi, Jason Baldridge, Eugene Ie,\nand Piotr Mirowski. Retouchdown: Adding touchdown to\nstreetlearn as a shareable resource for language grounding\ntasks in street view. arXiv preprint arXiv:2001.03671, 2020.\n2\n[45] Hongyuan Mei, Mohit Bansal, and Matthew Walter. Listen,\nattend, and walk: Neural mapping of navigational instruc-\ntions to action sequences. In AAAI, 2016. 2\n[46] Dipendra Misra, John Langford, and Yoav Artzi. Mapping\ninstructions and visual observations to actions with rein-\nforcement learning. In EMNLP, 2017. 2\n[47] Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh\nSaxena. Tell me dave: Context-sensitive grounding of natu-\nral language to manipulation instructions. The International\nJournal of Robotics Research, 2016. 2\n[48] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg,\nRapha¨el Lopez Kaufman, Aidan Clark, Seb Noury, Matthew\nBotvinick, Nicolas Heess, and Raia Hadsell. Stabilizing\ntransformers for reinforcement learning. In ICML, 2020. 2\n[49] Alexander Pashevich, Cordelia Schmid, and Chen Sun.\nEpisodic Transformer for Vision-and-Language Navigation\n– supplementary material. ICCV, 2021. 7, 8\n[50] Roma Patel, Ellie Pavlick, and Stefanie Tellex. Grounding\nlanguage to non-markovian tasks with no supervision of task\nspecifications. In RSS, 2020. 3\n[51] Roma Patel, Roma Pavlick, and Stefanie Tellex. Learning to\nground language to temporal logical form. In NAACL, 2019.\n2\n[52] Rohan Paul, Jacob Arkin, Derya Aksaray, Nicholas Roy,\nand Thomas M Howard. Efficient grounding of abstract\nspatial concepts for natural language interaction with robot\nplatforms. The International Journal of Robotics Research ,\n2018. 2\n[53] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu\nWang, Sanja Fidler, and Antonio Torralba. Virtualhome:\nSimulating household activities via programs. In CVPR,\n2018. 1, 2\n[54] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015. 5\n[55] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat-\nform for embodied ai research. In ICCV, 2019. 2\n[56] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan\nBisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer,\nand Dieter Fox. ALFRED: A Benchmark for Interpreting\nGrounded Instructions for Everyday Tasks. In CVPR, 2020.\n1, 2, 5, 6, 8, 12\n[57] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C ˆot´e,\nYonatan Bisk, Adam Trischler, and Matthew Hausknecht.\nAlfworld: Aligning text and embodied environments for in-\nteractive learning. ICLR, 2021. 2\n[58] Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim,\nRoozbeh Mottaghi, and Jonghyun Choi. MOCA: A Modular\nObject-Centric Approach for Interactive Instruction Follow-\ning. arXiv preprint arXiv:2012.03208, 2020. 5, 6, 8, 12\n[59] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In ICCV, 2019. 1, 2\n[60] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to nav-\nigate unseen environments: Back translation with environ-\nmental dropout. In NAACL, 2019. 1, 2\n[61] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew\nWalter, Ashis Banerjee, Seth Teller, and Nicholas Roy. Un-\nderstanding natural language commands for robotic naviga-\ntion and mobile manipulation. In AAAI, 2011. 2\n[62] Moritz Tenorth, Daniel Nyga, and Michael Beetz. Under-\nstanding and executing instructions for everyday manipula-\ntion tasks from the world wide web. In ICRA, 2010. 2\n[63] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A\nphysics engine for model-based control. In IROS, 2012. 2\n[64] Takayuki Okatani Van-Quang Nguyen. A hierarchical atten-\ntion model for action learning from realistic environments\nand directives. In ECCV EVAL Workshop, 2020. 5, 8, 12\n[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 3, 4\n[66] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya\nSutskever, and Geoffrey Hinton. Grammar as a foreign lan-\nguage. In NeurIPS, 2015. 1, 2\n[67] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 2\n[68] Xin Wang, Qiuyuan Huang, Celikyilmaz Asli, Jianfeng\nGao, Dinghan Shen, Yuan-Fang Wang, William Wang, and\nLei Zhang. Reinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nIn CVPR, 2019. 1, 2\n[69] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu\nWang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle\nRoman, et al. Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and text-to-\nsql task. arXiv preprint arXiv:1809.08887, 2018. 3\n[70] John M Zelle and Raymond J Mooney. Learning to parse\ndatabase queries using inductive logic programming. In\nAAAI, 1996. 3\n[71] Luke Zettlemoyer and Michael Collins. Online learning\nof relaxed ccg grammars for parsing to logical form. In\nEMNLP, 2007. 3\n[72] Luke S Zettlemoyer and Michael Collins. Learning to\nmap sentences to logical form: Structured classification\nwith probabilistic categorial grammars. arXiv preprint\narXiv:1207.1420, 2012. 3\n[73] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang.\nVision-language navigation with self-supervised auxiliary\nreasoning tasks. In CVPR, 2020. 1\n[74] Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vi-\nhan Jain, Eugene Ie, and Fei Sha. Babywalk: Going farther\nin vision-and-language navigation by taking baby steps. In\nACL, 2020. 2\nA. Appendix\nIn this appendix we provide additional results and analy-\nsis, including the state-of-the-art comparison with all evalu-\nation metrics, the complete breakdown of performance im-\nprovements, the impact of synthetic data size, and visual-\nizations of transformer attention maps.\nA.1. Comparison with state of the art\nWe present a complete table comparing E.T. to the state-\nof-the-art methods from the public leaderboard. In addi-\ntion to the success rates on validation and test folds re-\nported in the main paper (denoted as Full task), we mea-\nsure the amount of subgoal conditions completed for each\ntask on average [56] (denoted asGoal Cond.). We also com-\npute path-length-weighted scores [2] for both metrics which\nweight the original metric value by the ratio of the agent\npath length and the expert path length [56]. Table A1 shows\nthat the results on the additional metrics strongly correlate\nwith the full task success rates reported in the main paper.\nModel ValidationSeen UnseenFull task Goal Cond. Full task Goal Cond.\nShridharet al. [56] 3.70(2.10) 10.00(7.00) 0.00(0.00) 6.90(5.10)\nNguyenet al. [64] N/A N/A N/A N/A\nSinghet al. [58] 19.15(13.60) 28.50(22.30) 3.78(2.00) 13.40(8.30)\nE.T. 33.78(24.90) 42.48(33.10) 3.17(1.34) 13.12(7.41)\nE.T. (pretr.) 37.63(28.03) 47.59(37.27) 3.76(2.20) 14.65(8.44)\nE.T. (pretr. + & joint tr.)46.59(32.26) 52.82(42.24) 7.32(3.34) 20.87(11.31)\nModel TestSeen UnseenFull task Goal Cond. Full task Goal Cond.\nShridharet al. [56] 3.98(2.02) 9.42(6.27) 0.39(0.80) 7.03(4.26)\nNguyenet al. [64] 12.39(8.20) 20.68(18.79) 4.45(2.24) 12.34(9.44)\nSinghet al. [58] 22.05(15.10) 28.29(22.05) 5.30(2.72) 14.28(9.99)\nE.T. 28.77(19.77) 36.47(28.00) 5.04(1.94) 15.01(8.73)\nE.T. (pretr.) 33.46(23.82) 41.08(31.52) 5.56(2.82) 15.44(9.62)\nE.T. (pretr. & joint tr.)38.42(27.78) 45.44(34.93) 8.57(4.10) 18.56(11.46)\nHuman - - 91.00(85.80) 94.50(87.60)\nTable A1: Comparison with the models submitted to the public\nleaderboard on validation and test folds. We report success rates\nof the models on full tasks and subgoal conditions. We weight\nagent path lengths with expert path lengths and report path-length-\nweighted scores in parenthesis. The highest value per fold is\nshown in blue. ‘N/A’ denotes that the scores are not reported on\nthe leaderboard or in an associated publication.\nA.2. Complete performance analysis\nWe present a complete breakdown of performance im-\nprovements with respect to the components added to the\nLSTM-based baseline model proposed by Shridhar et\nal. [56]. First, we replace ImageNet visual features with\nfeatures pretrained to detect objects in ALFRED as ex-\nplained in Section 4.1. Next, we replace explicit pixel mask\npredictions with a pretrained MaskRCNN model proposed\nby Singh et al. [58]. These two components combined bring\nComponents Seen Unseen\nLSTM baseline (Shridhar et al. [56]) 4.8 0 .2\n+ ALFRED detection pretraining 8.5 0 .4\n+ Pretrained MaskRCNN [58] 23.2 2 .4\n- LSTM; + Transformer (E.T.) 33.8 3 .2\n+ Synthetic language pretraining 37.6 3 .8\n+ Joint training with 45K demonstrations 46.6 7.3\nTable A2: Complete breakdown of performance improvements.\nWe report the performance of the model proposed by Shridhar et\nal. [56] and sequentially add components that improve its success\nrate one by one. The components include (1) visual features pre-\ntrained to detect objects in ALFRED, (2) a pretrained MaskRCNN\nto predict pixel masks, (3) the E.T. model, (4) language encoder\npretraining on human to synthetic translation, (5) joint training\nwith additional data.\na significant improvement over the original baseline perfor-\nmance [56]. We then replace the LSTM model with the E.T.\narchitecture, pretrain the language encoder of the agent to\ntranslate human language to synthetic representations, and\njointly train the agent using additional 45K demonstrations\nto achieve the state-of-the-art performance reported in Ta-\nble A1.\nA.3. Impact of synthetic demonstration size\nWe extend the results of Table 5 and train the E.T. agent\nusing different number of demonstrations annotated with\nsynthetic instructions. The results are shown in Table A3.\nWe can see that increasing the number of synthetic demon-\nstrations in the joint training up to 22K brings a significant\nimprovement over the model trained on human annotations\nonly. However, doubling the synthetic demonstrations up to\n44K has a very minor impact on the agent performance. We\nuse 44K synthetic data in the main paper.\nTrain data Seen Unseen\n21K human only 33.8 3 .2\n21K human & 11K synth. 35.5 4 .1\n21K human & 22K synth. 38.3 5.5\n21K human & 44K synth. 38.5 5.4\nTable A3: Joint trainingof the E.T. model using different number\nof demonstrations annotated with synthetic instructions. We report\nsuccess rates on the validation folds.\nA.4. Visualizing visual attention\nTo better understand the impact of using a transformer\nencoder for action predictions, we show several qualitative\nexamples of attention weights produced by the multimodal\nencoder of an E.T. agent. We use attention rollout [1] to\ncompute attention weights from an output action to previ-\nous visual observations. Attention rollout averages attention\nFigure A1: A visualization of normalized attention heatmap to previous visual observations, from white (no attention) to red (high atten-\ntion). In this example, a microwave is first observed at the 8th timestep, and is highlighted by the visual attention at the 19th timestep when\nthe agent is asked to put the apple in the microwave.\nFigure A2: A visualization of normalized attention heatmap to previous visual observations. In this example, the agent is asked to cut a\npotato (timesteps 17 − 18) and to put a slice of it in a pot. At timestep 39 when the agent is asked to retrieve the sliced potato, it attends to\nframes at timesteps 17 − 18 to decide where to go.\nFigure A3: A visualization of normalized attention heatmap to previous visual observations. In this example, the agent is asked to move\ntwo identical pans. It moves the first pan at timesteps 20 − 22 and attends the frame at timestep 29 when moving the second pan (see the\ntwo corresponding pink squares).\nFigure A4: A visualization of normalized attention heatmap to previous visual observations. In this example, the agent is asked to wash a\ncloth and to put it in a cupboard. The agent washes the cloth at timestep 20 but the washed cloth does not look very different from a dirty\none. At timestep 31, the agent attends to the previous frames where the washing action is visible to keep track of the cloth state change.\nof all heads and recursively multiplies attention weights of\nall transformer layers taking into account skip connections.\nFigures A1-A4 show examples where an E.T. model attends\nto previous visual frames to successfully solve a task. The\nframes attention weights are showed with a horizontal bar\nwhere frames corresponding to white squares have close\nto zero attention scores and frames corresponding to red\nsquares have high attention scores. We do not include the\nattention score of the current frame as it is always signifi-\ncantly higher than scores for previous frames.\nIn Figure A1 the agent is asked to pick up an apple and\nto heat it using a microwave. The agent walks past a mi-\ncrowave at timestep 8, picks up an apple at timestep 18 and\nattends to the microwave frame in order to recall where to\nbring the apple. In Figure A2 the agent slices a potato at\ntimesteps 17 − 18 (hard to see on the visual observations).\nLater, the agent gets rid of the knife and follows the next\ninstruction asking to pick up a potato slice. At timestep 39,\nthe agent attends to the frames 17 − 18 where the potato\nwas sliced in order to come back to the slices and complete\nthe task. In Figure A3 the agent needs to sequentially move\ntwo pans. While picking up the second pan at timestep 29,\nthe agent attends to the frames 20 − 22 where the first pan\nwas replaced. In Figure A4 the agent is asked to wash a\ncloth and to put it to a drawer. The agent washes the cloth\nat timestep 20 but the cloth state change is hard to notice at\nthe given frames. At timestep 31, the agent attends to the\nframe with an open tap in order to keep track of the cloth\nstate change. To sum up, the qualitative analysis of the at-\ntention mechanism over previous visual frames shows that\nthey are used by the agent to solve challenging tasks and\naligns with the quantitative results presented in Section 4.3.\nA.5. Visualizing language attention\nFigure A5 illustrates transformer attention scores from\nan output action to input language tokens by comparing two\nmodels: (1) E.T. model trained from scratch, (2) E.T. model\nwhose language encoder is pretrained as in Section 3.3.\nSimilarly to the visual attention, we use attention rollout\nand highlight the words with high attention scores with red\nbackground color.\nIn the first example of Figure A5, the agent needs to pick\nup a bat. While the non-pretrained E.T. model has approx-\nimately equal attention scores for multiple tokens (those\nwords are highlighted with pale pink color) and does not\nsolve the task, the pretrained E.T. attends to “bat” tokens\n(highlighted with red) and successfully finds the bat. In\nthe second example, the agent needs to first cool an egg\nin a fridge and to heat it in a microwave later. The non-\npretrained E.T. has the similar attention scores for “mi-\ncrowave” and “refridgerator” tokens (they are highlighted\nwith pink) and makes a mistake by choosing to heat the\negg first. The pretrained E.T. agent has higher attention\nscores for the “refridgerator” tokens and correctly decides\nto cool the egg first. In the third example, the agent needs\nto pick up a knife to cut a potato later. The non-pretrained\nagent distributes its attention over many language tokens\nand picks up a fork which is incorrect. The pretrained E.T.\nagent strongly attends to the “knife” token and picks the\nknife up. The demonstrated examples show that the lan-\nguage pretraining of E.T. results in language attention that\nis better aligned with human interpretation.\nA.6. Qualitative analysis\nWe show 3 successful and 2 failed examples of the E.T.\nagent solving tasks from the ALFRED validation fold. In\nFigure A6 the agent successfully heats an apple and puts\nit on a table. The agent understands the instruction “bring\nthe heated apple back to the table on the side” and navi-\ngates back to its previous position. In Figure A7 the agent\nbrings a washed plate to a fridge. The agent does not know\nwhere the plate is and walks along a counter checking sev-\neral places. Finally, it finds the plate, washes it and brings\nit to the fridge. In Figure A8 the agent performs a sequence\nof 148 actions and successfully solves a task. This example\nshows that the agent is able to pick up small objects such as\na knife and a tomato slice. The agent puts both of them to a\nplate and brings the plate to a fridge.\nAmong the most common failure cases are picking up\nwrong objects and mistakes during navigation. In Figure A9\nthe agent misunderstands the instruction “pick up the bowl\nto the right of the statue on the table” and decides to pick\nup a statue on the frame marked with red. It then brings the\nstatue to a correct location but the full task is considered to\nbe failed. Figure A10 shows a failure mode in an unseen\nenvironment. The agent is asked to pick up a basketball and\nto bring it to a lamp. The agent first wanders around a room\nbut eventually picks up the basketball. It then fails to locate\nthe lamp and finds itself staring into a mirror. The agent\ngives up on solving the task and decides to terminate the\nepisode.\nFigure A5: Visualizations of normalized language attention heatmaps, without and with the language encoder pretraining. Red indicates a\nhigher attention score. We observe that the agent trained without language pretraining misses word tokens that are important for the task\naccording to human interpretation (marked with blue rectangles). In contrast, the pretrained E.T. agent often is able to pay attention to\nthose tokens and solve the tasks successfully.\nt = 0\n t = 4\n t = 5\n t = 26\n t = 36\nt = 38\n t = 40\n t = 43\n t = 56\n t = 58\nGoal: Put a heated apple on the table. Instructions: Turn left and go to the table. Pick up the apple on the table. Go right\nand bring the apple to the microwave. Heat the apple in the microwave. Bring the heated apple back to the table on the side.\nPut the heated apple on the table in front of the salt.\nFigure A6: Example of a successfully solved task. The agent picks up an apple, puts it into a microwave, closes it, turns it on, opens it,\npicks up the apple again, then navigates back to the table on the side and puts the apple on the same table.\nt = 0\n t = 6\n t = 12\n t = 13\n t = 20\nt = 21\n t = 26\n t = 31\n t = 32\n t = 33\nt = 34\n t = 39\n t = 48\n t = 49\n t = 50\nGoal: Place a rinsed plate in the fridge. Instructions: Walk ahead to the door, then turn left and take a step, then turn left\nand face the counter. Pick up the dirty plate on the counter. Walk left around the counter, and straight to the sink. Clean the\nplate in the sink. Turn left and walk to the fridge. Place the plate on the top shelf of the fridge. Place a pan containing slicing\ntomato in the refrigerator.\nFigure A7: Example of a successfully solved task. The agent does not know where the dirty plate is and looks at several places on the\ncounter (the first row). It then sees the plate in the corner of the top right image, picks it up, goes to a sink, opens a tap, picks the plate\nagain, navigates to a fridge, opens it and puts the plate to the top shelf of the fridge.\nt = 0\n t = 14\n t = 28\n t = 29\n t = 30\nt = 31\n t = 51\n t = 64\n t = 65\n t = 77\nt = 97\n t = 98\n t = 110\n t = 129\n t = 130\nt = 131\n t = 140\n t = 146\n t = 147\n t = 148\nGoal: Place a pan containing slicing tomato in the refrigerator.Instructions: Turn right, move to the table opposite the chair.\nPick up the knife that is near the tomato. Turn left, move to the table opposite the chair. Slice the tomato that is on the table.\nTurn left, move to the counter that is left of the bread, right of the potato. Put the knife in the pan. Turn left, move to the table\nopposite the chair. Pick up a slice of tomato from the counter. Turn left, move to the counter that is left of the bread, right of\nthe potato. Put the tomato slice in the pan. Pick up the pan from the counter. Turn left, move to in front of the refrigerator.\nPut the pan in the refrigerator.\nFigure A8: Example of a successfully solved task. The agent uses 148 actions to complete the task. The agent picks up a knife from a\ntable, slices a tomato in the first image of the second row, brings the knife to a stove, puts the knife on a plate, walks back to the table,\ngrabs a tomato slice, returns to the stove, puts the tomato slice on the same plate, picks up the plate, navigates to a fridge, opens it, puts the\nplate with the knife and the tomato slice on a shelf and closes the fridge.\nt = 0\n t = 4\n t = 17\n t = 28\n t = 34\nt = 35\n t = 45\n t = 54\n t = 57\n t = 58\nGoal: Move a bowl from the table to the coffee table. Instructions: Move across the room to the dining room table where\nthe statue is. Pick up the bowl to the right of the statue on the table. Carry the bowl to the glass coffee table. Place the bowl\non top of the coffee table between the statue and the square black tray.\nFigure A9: Failure example in a seen environment. The agent correctly finds both dining and coffee tables but gets confused with ”the\nbowl to the right of the statue” reference. The agent decides to pick up a statue instead of a bowl and fails to solve the task.\nt = 0\n t = 9\n t = 14\n t = 21\n t = 29\nt = 35\n t = 45\n t = 46\n t = 62\n t = 63\nt = 68\n t = 77\n t = 85\n t = 88\n t = 96\nGoal: Look at a basketball in the lamp light. Instructions: Turn around and go to the foot of the bed. Pick up the basketball\nfrom the floor. Turn around and go to the desk in the corner.Turn on the lamp.\nFigure A10: Failure example in an unseen environment. The agent is exposed to an unknown environment and fails to follow the navigation\ninstructions. It wanders around the room, eventually finds a basketball but fails to locate a lamp and decides to terminate the episode in\nfront of a mirror.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.720767617225647
    },
    {
      "name": "Transformer",
      "score": 0.6828920841217041
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6103121638298035
    },
    {
      "name": "Natural language",
      "score": 0.5285702347755432
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5273566842079163
    },
    {
      "name": "Natural language understanding",
      "score": 0.5248532295227051
    },
    {
      "name": "Language understanding",
      "score": 0.44652560353279114
    },
    {
      "name": "Natural language processing",
      "score": 0.3837060332298279
    },
    {
      "name": "Machine learning",
      "score": 0.33531302213668823
    },
    {
      "name": "Engineering",
      "score": 0.11765867471694946
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210115519",
      "name": "Centre de Recherche en Informatique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    }
  ]
}