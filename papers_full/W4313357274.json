{
  "title": "A novel application of transformer neural network (TNN) for estimating pan evaporation rate",
  "url": "https://openalex.org/W4313357274",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3206755115",
      "name": "Mustafa Abed",
      "affiliations": [
        "Swinburne University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1875076788",
      "name": "Monzur Alam Imteaz",
      "affiliations": [
        "Swinburne University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120726788",
      "name": "Ali Najah Ahmed",
      "affiliations": [
        "Universiti Tenaga Nasional"
      ]
    },
    {
      "id": "https://openalex.org/A2110939240",
      "name": "Yuk Feng Huang",
      "affiliations": [
        "Universiti Tunku Abdul Rahman"
      ]
    },
    {
      "id": "https://openalex.org/A3206755115",
      "name": "Mustafa Abed",
      "affiliations": [
        "Swinburne University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1875076788",
      "name": "Monzur Alam Imteaz",
      "affiliations": [
        "Swinburne University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120726788",
      "name": "Ali Najah Ahmed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110939240",
      "name": "Yuk Feng Huang",
      "affiliations": [
        "Universiti Tunku Abdul Rahman"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2117692137",
    "https://openalex.org/W3207848089",
    "https://openalex.org/W4288758399",
    "https://openalex.org/W2978591474",
    "https://openalex.org/W2883112011",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2965191834",
    "https://openalex.org/W2029719981",
    "https://openalex.org/W2490869500",
    "https://openalex.org/W2769950239",
    "https://openalex.org/W3131795733",
    "https://openalex.org/W2041534329",
    "https://openalex.org/W2061819166",
    "https://openalex.org/W1740585449",
    "https://openalex.org/W4294724153",
    "https://openalex.org/W2018669430",
    "https://openalex.org/W2527066115",
    "https://openalex.org/W3081241543",
    "https://openalex.org/W2013355544",
    "https://openalex.org/W2146096861",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2898791292",
    "https://openalex.org/W1665103756",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2115322028",
    "https://openalex.org/W2010876855",
    "https://openalex.org/W2291534902",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2775229114",
    "https://openalex.org/W6638444622",
    "https://openalex.org/W3120882540",
    "https://openalex.org/W2922056388",
    "https://openalex.org/W4214905925",
    "https://openalex.org/W1975336843",
    "https://openalex.org/W4304784501",
    "https://openalex.org/W2558918493",
    "https://openalex.org/W2397130888",
    "https://openalex.org/W2551393996",
    "https://openalex.org/W2991192488",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W2802436364",
    "https://openalex.org/W71235788"
  ],
  "abstract": "Abstract For decision-making in farming, the operation of dams and irrigation systems, as well as other fields of water resource management and hydrology, evaporation, as a key activity throughout the universal hydrological processes, entails efficient techniques for measuring its variation. The main challenge in creating accurate and dependable predictive models is the evaporation procedure's non-stationarity, nonlinearity, and stochastic characteristics. This work examines, for the first time, a transformer-based deep learning architecture for evaporation prediction in four different Malaysian regions. The effectiveness of the proposed deep learning (DL) model, signified as TNN, is evaluated against two competitive reference DL models, namely Convolutional Neural Network and Long Short-Term Memory, and with regards to various statistical indices using the monthly-scale dataset collected from four Malaysian meteorological stations in the 2000–2019 period. Using a variety of input variable combinations, the impact of every meteorological data on the E p forecast is also examined. The performance assessment metrics demonstrate that compared to the other benchmark frameworks examined in this work, the developed TNN technique was more precise in modelling monthly water loss owing to evaporation. In terms of predictive effectiveness, the proposed TNN model, enhanced with the self-attention mechanism, outperforms the benchmark models, demonstrating its potential use in the forecasting of evaporation. Relating to application, the predictive model created for E p projection offers a precise estimate of water loss due to evaporation and can thus be used in irrigation management, agriculture planning based on irrigation, and the decrease in fiscal and economic losses in farming and related industries where consistent supervision and estimation of water are considered necessary for viable living and economy.",
  "full_text": "Vol.:(0123456789)1 3\nApplied Water Science (2023) 13:31 \nhttps://doi.org/10.1007/s13201-022-01834-w\nORIGINAL ARTICLE\nA novel application of transformer neural network (TNN) \nfor estimating pan evaporation rate\nMustafa Abed1 · Monzur Alam Imteaz1 · Ali Najah Ahmed2 · Yuk Feng Huang3\nReceived: 12 October 2022 / Accepted: 16 November 2022 / Published online: 30 December 2022 \n© The Author(s) 2022\nAbstract\nFor decision-making in farming, the operation of dams and irrigation systems, as well as other fields of water resource \nmanagement and hydrology, evaporation, as a key activity throughout the universal hydrological processes, entails efficient \ntechniques for measuring its variation. The main challenge in creating accurate and dependable predictive models is the \nevaporation procedure's non-stationarity, nonlinearity, and stochastic characteristics. This work examines, for the first time, \na transformer-based deep learning architecture for evaporation prediction in four different Malaysian regions. The effective-\nness of the proposed deep learning (DL) model, signified as TNN, is evaluated against two competitive reference DL models, \nnamely Convolutional Neural Network and Long Short-Term Memory, and with regards to various statistical indices using \nthe monthly-scale dataset collected from four Malaysian meteorological stations in the 2000–2019 period. Using a variety of \ninput variable combinations, the impact of every meteorological data on the Ep forecast is also examined. The performance \nassessment metrics demonstrate that compared to the other benchmark frameworks examined in this work, the developed \nTNN technique was more precise in modelling monthly water loss owing to evaporation. In terms of predictive effectiveness, \nthe proposed TNN model, enhanced with the self-attention mechanism, outperforms the benchmark models, demonstrating \nits potential use in the forecasting of evaporation. Relating to application, the predictive model created for E p projection \noffers a precise estimate of water loss due to evaporation and can thus be used in irrigation management, agriculture plan-\nning based on irrigation, and the decrease in fiscal and economic losses in farming and related industries where consistent \nsupervision and estimation of water are considered necessary for viable living and economy.\nKeywords Evaporation · Transformer neural network · Self-attention · Long short-term memory · Convolutional neural \nnetwork\nIntroduction\nBackground\nA crucial step in the hydrological cycle is evaporation, which \nconverts liquid water from the surface of earth into steam. \nGreater evaporation rates are a key marker of global warm-\ning (Yizhong Chen et al. 2018). Evaporation also results in \nconsiderable water loss, which has an impact on lake and \nreservoir water levels as well as the water budget. Conse-\nquently, accurate measurement and estimation of water loss \nbecause of evaporation are crucial for effective management \nof water resources (Abtew & Melesse 2012). Both indi-\nrect and direct methods are used to estimate evaporation, \nincluding evaporation pan, water balance, Penman method, \nenergy balance, and mass transfer (L. Wu et al. 2020a). The \nevaporation pan technique is the most widely used since it \n * Yuk Feng Huang \n huangyf@utar.edu.my\n Mustafa Abed \n mustafaabed@swin.edu.au\n Monzur Alam Imteaz \n mimteaz@swin.edu.au\n Ali Najah Ahmed \n mahfoodh@uniten.edu.my\n1 Department of Civil and Construction Engineering, \nSwinburne University of Technology, Hawthorn, Melbourne, \nVIC 3122, Australia\n2 Institute of Energy Infrastructure and Department of Civil \nEngineering, College of Engineering, Universiti Tenaga \nNasional (UNITEN), 43000 Selangor, Malaysia\n3 Department of Civil Engineering, Lee Kong Chian Faculty \nof Engineering and Science, Universiti Tunku Abdul \nRahman, Selangor, Malaysia\n Applied Water Science (2023) 13:31\n1 331 Page 2 of 19\nis comparatively easier and less costly (Kisi et al. 2016). \nThe current work attempts to estimate evaporation pan (Ep) \nwith an accuracy comparable to real evaporation consider -\ning (Kahler & Brutsaert 2006) demonstration that the pan \nevaporation technique provides a precise rate of the real \nalterations in evaporation. For E p estimation, techniques \nbased on meteorological datasets associated to the experi-\nmental evaporation equation, Energy Budget, and Water \nBudget have been used (L. Wang et al. 2016). The intri-\ncate stochastic characteristics of the evaporative procedure, \nwhich is not sufficiently represented by the linear modelling \nmethod, might cause the predicted errors in these techniques \nto be rather substantial (M. Abed et al. 2021). Furthermore, \nempirical models must have their model coefficients cali -\nbrated before being applied to various agroclimatic zones \nbecause they behave differently under various conditions.\nLiterature review\nScientists have concentrated their initiatives on machine \nlearning approaches to estimate losses caused by evaporation \ndue to the low performance levels and difficulties with con-\nceptual and practical gauging techniques. These Artificial \nIntelligence (AI) systems are easier to use, more reliable, \nand capable of accurately simulating intricate nonlinear pro-\ncedures (M. M. Abed et al. 2010; Kişi, 2009; Sudheer et al. \n2002). Numerous studies have been conducted on utilising \nAI to estimate various hydrological factors (Ashrafzadeh \net al. 2019). Researchers suggest that ANN frameworks \noffer more accurate projections compared to traditional \napproaches (Ditthakit et al. 2022; Pham et al. 2022). As \na result, AI-based modelling techniques have been effec-\ntively applied in a variety of engineering research fields. \nWhen comparing the Box & Jenkins approach with ANN for \ninstance, Castellano-Méndez et al. 2004 found that the latter \noffers higher runoff simulation performance with regards to \naccuracy. For estimating pan evaporation, numerous research \nstudies have also been carried out by utilising ML techniques \nwith multiple optimisation works (Ashrafzadeh et al. 2020; \nMalik et al. 2020). Goyal et al. 2014) tested LSSVR, Fuzzy \nLogic (FL), ANN, and ANFIS strategies for projecting daily \nEp, and the results were compared to those of the Stephens-\nStewart (SS) and Hargreaves-Samani (HGS) empirical \nmethods. Results of this research have demonstrated that \nLSSVR and FL approaches are more effective than conven-\ntional methods for estimating daily evaporation. In order \nto calculate pan evaporation over monthly timeframes, Kişi \n2013 developed evolutionary neural networks. The findings \nshowed that the models were more accurate compared to \nempirical techniques. In their study on monthly water loss \nfrom evaporation, Deo et al. 2016 employed Multivariate \nAdaptive Regression Spline (MARS), Extreme Learning \nMachine (ELM), and Relevance Vector Machine (RVM). \nMeteorological factors were used as predictor variables, \nand RVM was discovered being the most successful strat-\negy. According to Sudheer et al. 2002, ANN approaches \ncould be used to predict evaporation using weather data. \nThey developed an ANN technique for modelling daily \nevaporation. Falamarzi et al. 2014 examined the applica-\ntion of wavelet ANN and ANN for day-to-day evaporation \nforecasting. They used measurements of the wind speed and \ntemperature as model predictors. The outcomes showed that \nthe two techniques provided accurate evaporation estimates. \nThese shallow learning techniques have proven successful \nat forecasting E p for a variety of climatic situations. How -\never, Deep Learning (DL) algorithm-based modelling has \nbecome increasingly popular in many engineering research \ndisciplines to produce predictions that are more precise and \ntrustworthy (Yunzhi Chen et al. 2022).\nSince deep learning (DL) approaches, which use \nimproved multi-layered neural networks, are attractive for \ntime series applications, they may open new possibilities \nfor Ep estimations. This is because they are currently gain-\ning popularity among artificial intelligence techniques that \nare utilised in both commercial and scientific fields owing \nto their increased precision (Hu et al. 2018). Recurrent \nneural networks (RNN), which form the foundation of DL \napproaches and are better candidates for estimating and pro-\njecting time series data because to their capacity to main-\ntain and use memory from past network states, are known \nfor their ability to do so (Chang et al. 2016; Daliakopoulos \net al. 2005). Although the typical RNN model structures are \ncapable of capturing the patterns of the time series data, they \nstruggle to maintain the variables' longer-term dependence \nand have problems with vanishing and exploding gradients \n(Bengio et al. 1994 ). Because of these two fundamental \nflaws in the typical RNN, network training might result in \nunrealistic network weights that are either zero or too large. \nPractically speaking, remembering vital information, and \navoiding redundant or unneeded information among dif-\nferent network states are the two key factors that guaran-\ntee improved network training. Long Short-Term Memory \n(LSTM), an enhanced class of conventional RNN architec-\ntures, has been developed as a potent algorithm capable of \noutclassing the training shortcomings of RNNs (vanishing \nand exploding gradient issues) by retaining important infor-\nmation for model establishment while preventing needless \ninformation from being conveyed to the following states in \nthe model development procedure.\nLSTM has been effectively used in research on natural \nlanguage processing (NLP), financial time series forecast-\ning, travelling period predictions, traffic congestion, and \nmany other areas. Despite its wide applicability in a variety \nof research domains, LSTM approaches have lately been \nemployed in hydrologic time series forecasting (Hu et al. \n2018). Zhang et al. 2018 employed an LSTM method for \nApplied Water Science (2023) 13:31 \n1 3 Page 3 of 19 31\npredicting water tables in rural areas. Moreover, the authors \ncompared the outcome scheme using LSTM techniques with \nthat of a standard ANN and noted that the former approach \noutperforms the ANN. Research was conducted by Majhi \net al. 2020 to forecast evaporation with use of LSTM-based \nmodels. In this research, the LSTM-based approach was con-\ntrasted against Multilayer-ANN as well as empirical tech-\nniques such as Blaney–Criddle and Hargreaves, to demon-\nstrate its superiority in predicting daily evaporative losses \nover selected benchmark schemes. Convolutional Neural \nNetwork (CNN), an alternative and powerful deep learn-\ning technique, has recently drawn widespread attention as a \nresult of its varied application in a range of fields, including \nobject recognition processing (Krizhevsky et al. 2017), time \nseries categorisation (Z. Wang et al. 2017b), robotic haptic \nand visual data classification (Gao et al. 2016), weather fore-\ncasting (Liu et al. 2014), and audio signal classification (Lee \net al. 2009). For instance, Ferreira and da Cunha (Ferreira & \nda Cunha 2020) examined one-dimensional Convolutional \nNeural Network (1D-CNN) plus a combination of LSTM-\nCNN, LSTM, as well as ML strategies (ANN and RF), for \napplication in predicting multi step-ahead daily Ep with use \nof data from weather stations located in Brazil. They estab-\nlished that the developed DL approaches achieved relatively \nbetter results than ML strategies. It is notable that numerous \nresearchers have used CNN in several time series forecast-\ning fields such as electrical load estimations, solar energy \nforecasting, and other modelling scenarios. CNN has largely \nshown performance superior to that of conventional machine \nlearning models across many studies and achieves state-of-\nthe-art performance in most cases.\nRecently, attention-based models have been employed in \ntime series forecasting with effectiveness. Transformer archi-\ntecture is derived purely from self-attention (intra-attention) \nmechanisms and the approach has recently become more \npopular. Transformers were first used in machine transla-\ntion applications by (Vaswani et al. 2017) and demonstrated \nan exceptional capability for generalising other key tasks, \nincluding sequence modelling and computer vision. In con-\ntrast to recurrent networks, a transformer has no vanishing \ngradient problem and can access all points in the past irre-\nspective of distance. This feature enables the transformer \nto find long-running dependencies. Unlike with recurrent \nnetworks, a transformer forgoes sequential computation \nand so can run completely in parallel and at higher speeds. \nIn sum, transformer mechanisms do not analyse inputs in a \nsequential manner for the architecture relies on a self-atten-\ntion mechanism that overcomes certain issues in recurrent \nand convolutional sequence-to-sequence modelling. The \ntransformer has been successfully employed in various tasks \nrelated to time series forecasting and it outperforms numer-\nous forecasting methods. In this context, certain work has \nbeen carried out with the goal of improving recurrent DL \nmodels through use of self-attention mechanisms. As an \nexample, a deep transformer model used in influenza-like ill-\nness forecasting was introduced in (N. Wu et al. 2020b) that \noutperforms sequence-to-sequence and LSTM architectures. \nThe self-attention mechanism of transformer-based mod-\nels performs better at forecasting than the linear-attention \nmechanism used in sequence-to-sequence models. Trans-\nformer methods therefore have great potential to simulate \nthe complex dynamics found in time series data that are \ndifficult for sequence schemes to handle. The approach may \nlargely resolve the vanishing gradient problem that impedes \nRecurrent Neural Networks (RNNs) modelling of long-term \npredictions.\nThe review of literature confirmed that use of ANN \nvia appropriate learning techniques can properly simulate \nevaporation in different locations, with results superior to \nthat of relatively complex traditional approaches (Biazar \net al. 2019). Nevertheless, identifying and developing effi-\ncient, reliable, and properly generalised estimation methods \nremains challenging due to the nonlinear complex nature \nof evaporation processes. Of the various ANN techniques \nemployed recently, the innovative DL model has great \npotential for resolving prediction problems and is known \nto outperform more complex techniques. In particular, the \nliterature shows that of the various DL methods, CNN and \nLSTM offer the strongest performance potential and there-\nfore will be considered as modelling benchmarks. In recent \nresearch, attention-based models have similarly been used \nin time series forecasting with much success in overcom-\ning the problems found in standard RNN and convolutional \nsequence-to-sequence modelling. This research presents a \nnovel approach in the field of evaporative losses, for it is \nthe first to attempt use of a transformer model that relies \non a self-attention mechanism for E p predictions. Success-\nful development would be of high significance particularly \nfor water resources management towards sustainability of \nfarming.\nObjectives\nThe current study aims to evaluate the applicability, predict-\nability, and accuracy of Transformer Neural Network (TNN) \nschemes in predicting monthly Ep levels in four regions \nacross Malaysia, using climatological data sets for the dura-\ntion from 2000 to 2019. The TNN model performance is \ncontrasted with two well-known deep learning approaches, \nLong Short-Term Memory (LSTM) and Convolutional Neu-\nral Network (CNN). Both methods are effective and compete \nwell in DL modelling. The predictive accuracy of the models \nis investigated across a range of input combination scenarios \nto attain the highest possible levels of precision. Efficiency \nvalues for the model are analysed and evaluated using con-\nventional statistical performance metrics that establish their \n Applied Water Science (2023) 13:31\n1 331 Page 4 of 19\nsuitability for forecasting evaporation rates. Moreover, \nadequate analysis was performed in this research to prove \nTNN modelling reliability with the objective of developing \na reliable approach to forecasting evaporation, a task which \nis particularly essential to agricultural planning and water \nresource management. This study presents a novel approach \nin the field of evaporative water loss studies, for it is the first \nattempt in which transformer‐based architecture is used for \nEp predictions.\nStudy area and data\nStudy area\nMalaysia is in the tropics and thus receives a substantial \namount of rain. Nonetheless, development has led to grow -\ning water demand in recent years. In addition, climate \nchange appears to have extended the dry season all while \nincreasing evaporation rates in reservoirs. Many research-\ners believe that drought is a very complex and inadequately \nunderstood natural disaster, one which affects populations \nfar more than other threats (Shaaban & Low 2003). Precise \nevaporation forecasting is therefore key to developmental \nefforts. This study's intent is to devise accurate schemes for \nforecasting Ep that would be particularly beneficial in agri-\ncultural planning and water resource management. Monthly \nclimate data is recorded across four meteorological sites \nlocated as follows: Alor Setar station (longitude 100° 24′ E, \nlatitude 6° 12′ N, elevation 3.4 m), Kota Bharu station (lon-\ngitude 102° 18′ E, latitude 6° 10′ N, elevation 4.4 m), KLIA \nSepang station (longitude 101° 42′  E, latitude 2° 44′  N, \nelevation 16.1 m), and Kuantan station (longitude 103° 13′ \nE, latitude 3° 46′ N, elevation 15.2 m). These locations have \nbeen selected as a case study due to the existence of good-\nquality daily evaporation data as well as the importance of \nthese cities in the region. All sites are run by the Malaysian \nMeteorological Department (MMD) and their outputs are \nrelied on to both calibrate and validate recommended pre -\ndiction models. Climatic variables were gathered across a \nvariety of regions across Malaysia, as shown in Fig.  1. As \nwell, Google Maps was utilised to map and describe the \nareas under study.\nData description\nAll proposed prediction models were constructed using \nseven meteorological variables, which include minimum, \nmaximum, and mean air temperatures (Tmin, Tmax, Ta), rela-\ntive humidity (RH), wind speed (S w), solar radiation (R s), \nand open pan evaporation (E p). Data sets comprised some \n19 years of daily statistics from 2000 until 2019. Various \nmeteorological parameters logged every month that pertain \nto the quantified weather data, as collected by the four pre-\nviously mentioned stations, are displayed in Table  1. Addi-\ntionally, Fig.  2 displays average monthly variations for all \nmeteorological parameters during the period from 2000 until \n2019.\nIn the table provided, X min, Xmax, Xmean, Cx, Cv, and S x \ncorrespond to the minimum, maximum, mean, skewness, \ncoefficient of variation, and standard deviation values of \nthe modelled weather indicators. From the table data, the \nminimum value of Ep was recorded at Kuantan station, while \nthe greatest value was logged at KLIA Sepang station. This \ntrend might relate to site variation in the value of relative \nhumidity, which is conversely proportional to evaporation. \nKuantan station recorded the highest rate of relative humid-\nity, whereas KLIA Sepang station recorded the lowest rate. \nOn the other hand, the maximum skewness of Ep was logged \nin KLIA Sepang station, whereas the minimum skewness \nwas logged at Kuantan. Positive values of skewness imply \nthat the attached information is not proportional and does \nnot follow the standard dispersion.\nModel development procedure\nInput combination scenarios\nSelection of suitable predictors is a key step in the devel-\nopment of robust predictive models (Tofiq et al. 2022); \nvarious input sets of weather parameters were considered \nto devise successful input–output schemes and enhance \nthe predictive capability of the ML model. This approach \nallows for more pragmatic understanding of how all input \nparameters influence evaporation estimates for a region \n(M. Abed et al. 2021). Input variables (predictors) were \nselected based on Pearson correlation coefficient con-\nsiderations (Freedman et al. 2007). Pearson correlation \nproduces bound test that quantify the statistically sig-\nnificant correlation, or correlation, between two continu-\nous factors. It is recognised as a magnificent means of \nmeasuring correlations among parameters under study, \nfor it derives from the autocorrelation method (Hauke \n& Kossowski 2011). The technique provides data on the \nassociation (the magnitude of correlation) and the path of \nthe trend relationship. Both variables can be positively or \nnegatively associated, with no relationship between the \ntwo variables determinable if the correlation coefficient \nequals 0. Regarding the relevant features of the meteoro-\nlogical parameters used in estimating monthly Ep, ranges \nand interpretations of the Pearson correlation coefficient \nresults were discussed in the previous research of Mustafa \net al. (M. Abed et al. 2022). Pearson correlation coefficient \nwas used to determine the meteorological parameters that \nmost affect evaporation estimates, as listed in Table  2. The \nApplied Water Science (2023) 13:31 \n1 3 Page 5 of 19 31\nresults displayed in Table  2 indicate that T max, Tmin, RH, \nRs, and Sw were each related to some degree with Ep, and \ntherefore may have a key role in forecasting evaporation \nparameters for data from all stations. In particular, the \nRH and Tmax parameters for every site show the strong-\nest association with Ep. RH and Tmax therefore will be \napplied in every input combination to strengthen E p esti-\nmation accuracy. Previous studies have also implied that \nTmax, Tmin, RH, R s, and S w are among the key predictors \nof evaporation (Dalkiliç et al. 2014; L. Wang et al. 2016).\nThe current study also investigated the effect of the input \nparameter Ep on enhancing the model's performance in \nevaporation prediction. In this context, data were selected \nbased on correlations with previous records and their rela-\ntion with the forecasted outcome. As depicted in Fig.  3, for \nevery station, the autocorrelation assessment concerning the \nFig. 1  Meteorological stations location [Imagery ©2021 TerraMetrics, Map data ©2021 Google]\n Applied Water Science (2023) 13:31\n1 331 Page 6 of 19\nevaluated monthly time series for Ep levels indicated that the \nrelationship reduced notably when it crossed the second lag-\nperiod record. It indicates that the second prior evaporation \ncharacteristics impacted evaporation at a given time. Hence, \nprevious pan evaporation data can be used along with corre-\nlation evaluation. Consequently, using historical evaporation \npan rate data with the benefit of correlation examination, the \nmodel was built using the most significant time lag concern-\ning the past two records.\nTherefore, the present study evaluated six distinct inputs \nfor the proposed TNN framework (Table 3). Every climatic \ndataset was split as 80% and 20%, representing the train-\ning and testing (calibration and validation) sets. Initial years \nfrom the split dataset were used for training the model, while \nthe remaining were used for testing.\nData pre‑processing\nConsidering the time series aspect of this problem, data \nconcerning specific predictors were normalised in the \n(0,1) range to eliminate variance before the framework was \ndevised and trained. Since this process comprises regression \nand forecasting, the max–min scaling technique is employed \nas per the following equation.\nwhere /uni0302.s1X i and xi denote the normalised and observed num-\nbers, xmin andxmax  are the minimum and maximum observed \nvalues. The normalised predictor and predicted variable \nvalues were divided into training and validation sets. As \nspecified previously, the training set comprised 80% of the \nobservations, while the remaining 20% were used for testing.\nMachine learning algorithms\nThe proposed TNN model and the other contrasting bench-\nmark approaches (i.e. LSTM and CNN) were developed \nusing an Intel Core i7-1195G7 CPU @ 2.90 GHz and 16 GB \nof RAM computer, built using TensorFlow in Python 3.9.0.\nTransformer neural network (TNN) model for Ep prediction\nThe transformer framework comprises a self-attention \n(intra-attention) mechanism that attempts to address sev -\neral concerns encountered in recurrent and convolutional \nsequence-to-sequence approaches. The transformer approach \nemploys the self-attention technique to retain only the criti-\ncal data from the preceding token by selecting vital details \n(1)̂Xi = xi − xmin\nxmax − xmin\nTable 1  Weather variables and \ndescriptive statistics Station Dataset Unit Xmean Sx Cv Cx Xmin Xmax\nAlor Setar Tmax °C 32.82 1.24 3.79 1.09 30.61 37.34\nTmin °C 24.18 0.69 2.87 −0.40 21.58 25.98\nRH % 80.87 5.89 7.28 −1.13 63.88 89.98\nSw m/s 1.66 0.41 24.66 0.10 0.51 2.95\nRs MJ  m−2 18.45 2.17 11.76 0.32 13.95 24.19\nEp mm 4.44 0.98 22.11 0.93 2.61 6.10\nKota Bharu Tmax °C 31.33 1.29 4.14 −0.25 28.04 34.54\nTmin °C 24.22 0.60 2.49 −0.20 21.81 26.48\nRH % 80.58 2.90 3.60 0.29 72.97 87.51\nSw m/s 2.32 0.52 22.42 1.07 1.48 3.93\nRs MJ  m−2 19.01 2.70 14.22 −0.48 10.75 24.64\nEp mm 4.22 0.69 16.36 0.14 2.66 6.08\nKLIA Sepang Tmax °C 32.20 0.81 2.54 0.63 30.29 34.77\nTmin °C 24.42 0.49 2.02 0.10 23.24 25.69\nRH % 79.62 4.13 5.19 −0.82 63.51 87.40\nSw m/s 1.87 0.27 14.62 0.45 1.15 2.81\nRs MJ  m−2 17.55 2.38 13.59 0.59 11.12 24.76\nEp mm 4.17 0.48 11.66 1.13 3.20 6.12\nKuantan Tmax °C 32.17 1.24 3.88 −0.63 28.52 34.89\nTmin °C 23.71 0.64 2.70 −0.64 21.14 25.53\nRH % 84.29 3.01 3.58 0.33 77.33 92.39\nSw m/s 1.64 0.30 18.49 0.64 0.91 2.65\nRs MJ  m−2 17.26 2.16 12.56 −0.47 11.73 22.39\nEp mm 3.79 0.53 13.64 −0.26 2.28 5.13\nApplied Water Science (2023) 13:31 \n1 3 Page 7 of 19 31\nconcerning the present token’s encoding. Put differently, \nthe attention approach is modified to calculate the latent \nspace equivalent for the encoder and decoder. Neverthe-\nless, the loss of recurrence requires positional encoding to \nbe integrated with the inputs and outputs. Likewise, con -\nsidering the recurrent time-step, positional data offers the \ntransformer system sequences of the inputs and outputs. \nThe encoding layer comprises two components: multi-head \nself-attention (MSA) and the feed-forward layer. The atten-\ntion system creates a one-to-one correlation concerning the \ntime-specific moments. Aspects of human attention have \nstimulated the attention layers; however, essentially, it com-\nprises a weighted mean reduction. Three inputs are fed to \nthe attention layer: values, query, and keys. Every sub-layer \ncomprises residual associations; subsequently, the layers are \nnormalised. The objective behind several heads is typically \nFig. 2  Monthly variations of Ep and related meteorological indicators\n Applied Water Science (2023) 13:31\n1 331 Page 8 of 19\ncontrasted against using several CNN filters, where every fil-\nter extracts latent features from the input. Likewise, several \nlatent features concerning the input are extracted by several \nheads in the multiheaded attention approach. The outputs \nfrom every head are combined using the concatenation \nfunction. In contrast to recurrent networks, the transformer \napproach is free from the vanishing gradient issue and can \nreference any previous point, notwithstanding its distance. \nThis aspect permits the transformer system to identify \nlong-term dependencies. Moreover, in contrast to recurrent \nsystems, the transformer does not require sequential com-\nputing, allowing faster speed using parallel processing. Put \ndifferently, transformer inputs are not assessed sequentially. \nHence, the vanishing gradient issue is inherently eliminated. \nOn the other hand, Recurrent Neural Networks (RNNs) suf-\nfer from this issue for long-term forecasts. Figure 4 presents \nthe fundamental difference concerning how information \nis handled in an RNN vs the self-attention system. Com-\nparatively, transformers preserve direct associations to \nevery previous timestamp so that data can be moved over \nextended sequences. Nevertheless, there is a new concern: \nthe framework directly correlates with massive data inputs. \nThe self-attention mechanism is used in the transformer \nframework to segregate non-essential information.\nTNN model customization Several studies have attempted \nto establish that DL frameworks are superior to other \nmachine learning techniques when measured using fore-\ncasting accuracy. Nevertheless, there is no mention of \nself-attention frameworks in the literature for evaporation \nforecast modelling. Hence, this review aims to assess the \nefficacy of the transformer approach for evaporation estima-\ntion when measured on the efficiency and accuracy metrics. \nNevertheless, the transformer model suitable for machine \ntransliteration cannot be directly employed for estimating \ntime series. The following section specifies the modifica-\ntions applied to the transformer model to allow its use for \npredicting time series. Hence, the embedding layers con-\ncerning the framework input associated with NLP are dis-\nregarded, and the time series magnitude at a specific time \nis provided as the input to the system. The soft-max clas-\nsification layer at the output is also disregarded. Moreover, \nTable 2  Pearson's correlation \ncoefficient matrix at each \nselected station\nTmax Tmin RH Sw Rs Ep\nPearson's correlation coefficient array based on data from the Alor Setar station\nTmax 1 −0.0797 −0.7183 0.4671 0.7904 0.8257\nTmin 1 0.4625 −0.1716 0.0322 −0.3519\nRH 1 −0.7365 −0.5843 −0.8773\nSw 1 0.5610 0.6562\nRs 1 0.7339\nEp 1\nPearson's correlation coefficient array based on data from the Kota Bharu station\nTmax 1 0.6820 −0.4975 −0.4272 0.7011 0.6029\nTmin 1 −0.2089 −0.1485 0.2871 0.3957\nRH 1 −0.2386 −0.6435 −0.7009\nSw 1 0.0413 0.3071\nRs 1 0.5818\nEp 1\nPearson's correlation coefficient array based on data from the KLIA Sepang station\nTmax 1 0.6557 −0.6245 0.2781 0.5006 0.6887\nTmin 1 −0.3692 0.1567 0.4022 0.4104\nRH 1 −0.4481 −0.4582 −0.7096\nSw 1 0.3911 0.5025\nRs 1 0.6653\nEp 1\nPearson's correlation coefficient array based on data from the Kuantan station\nTmax 1 0.7072 −0.5687 −0.4319 0.7011 0.7021\nTmin 1 −0.1759 −0.4913 0.3202 0.4156\nRH 1 −0.1455 −0.6274 −0.7961\nSw 1 0.0415 −0.6826\nRs 1 0.6720\nEp 1\nApplied Water Science (2023) 13:31 \n1 3 Page 9 of 19 31\nthe output layer is modified to provide a linear activation \nfunction. The regression-based mean square error (MSE) \nexpression is employed as the loss function. The original \ntransformer framework’s encoder is employed for the train-\ning scheme. Every encoding later comprises two sub-layer: \nthe self-attention and a fully connected feed-forward. This \nstudy comprises a one-dimensional convolutional system as \na substitute for the fully connected layer to identify high-\nFig. 3  Partial Autocorrelation analysis for the studied stations (Monthly)\nTable 3  Meteorological variable input combinations for TNN model\nTNN predictive \nmodel\nInput combinations No. of inputs\nTNN-1 RH, Tmax 2\nTNN-2 RH, Tmax, Tmin 3\nTNN-3 RH, Tmax, Tmin, Sw 4\nTNN-4 RH, Tmax, Tmin, Rs 4\nTNN-5 RH, Tmax, Tmin, Rs, Sw 5\nTNN-6 RH, Tmax, Tmin, Rs, Sw, Ep 6\nFig. 4  Self-attention mechanism versus RNN\n Applied Water Science (2023) 13:31\n1 331 Page 10 of 19\nlevel characteristics. Moreover, the convolutions lack dense \nconnections; all output nodes are not affected by all input \nnodes. Such an arrangement provides convolutional layers \nadditional versatility in learning data attributes. Briefly, the \nencoding system comprises the transformer input compris-\ning a specific time series, as depicted in Fig.  5. The data \nis then used as input for the self-attention layer compris-\ning the encoding system; subsequently, layer normalisation \nis implemented. Further, a feed-forward layer is used with \nanother layer normalisation process. This study uses a TNN \nframework where four similar encoding blocks are associ-\nated in a feed-forward manner. Figure  5 presents the TNN \nframework setup.\nThis work used an exhaustive search process for the sys-\ntem design and training hyperparameters to build optimal \nstructures for the studied TNN model. Consequently, many \ndifferently configured models have been assessed to deter -\nmine the optimal architecture. The optimal hyperparam-\neters of this work are listed in Table  4. Based on the out-\ncomes presented in the table, the optimal TNN framework \nis devised using four similar transformer encoders whose \noutput is processed using a one-dimensional Global Average \nPooling layer. Global average pooling is beneficial because \nit is fundamentally close to the convolutional architecture \nby implementation of communication between feature maps \nand classifications. Hence, it is simple to understand fea-\nture maps as category confidence indicators. Further, global \naverage pooling lacks parameter optimisation need, thereby \navoiding overfitting (Lin et al. 2013). Global average pooling \nis followed by adding a 128-neuron dense layer comprising \nthe ELU activation function. Also, a dropout layer is intro-\nduced to control overfitting (Ferreira & da Cunha 2020). \nLastly, one more single-output fully connected linear layer \n(i.e. forecast Ep numbers) is used. Model training was based \non several iterations comprising 16 batches and 200 epochs \nthat followed the system configurations discussed above. \nNetwork weights were regulated using the Adam algorithm \n(Kingma & Ba 2014) for loss function reduction. Moreo-\nver, network performance was evaluated at a 1e-2 learning \nspeed. After devising framework architectures, the system \nwas trained by validating it based on the data in the train-\ning set. The subsequent step evaluated the model’s forecast-\ning ability based on fresh data. The forecast numbers were \ndenormalised to facilitate visual representation, followed by \na comparison with actual values. Figure 6 depicts the devel-\nopment steps concerning the TNN prediction framework.\nBaseline models used for performances assessment \nof the proposed TNN model\nThis study benchmarked model performance based on Long \nShort-Term Memory Neural Networks (LSTM) and Convo-\nlutional Neural Networks (CNN) to contrast the proposed \nmodel’s performance. These two benchmark frameworks \nhave different architectures and belong to distinct families \nin the DL framework. The optimal hyperparameters con -\ncerning the CNN and LSTM models are listed in Table  5. \nThe hyperparameters are values that control the learning \nprocess and determine the values of model parameters that \na learning algorithm learns.\nThe LSTM network is an adapted and enhanced form of \nRNN that can learn extended correlations between several \ntime-steps comprising a data sequence. LSTMs are appropri-\nate for forecasting sequence information since they control \nthe exploding and vanishing gradient challenges faced by \nconventional RNNs. These problems are addressed by imple-\nmenting gating expressions and state information. Using a \nspecifically devised structure, the LSTM approach exhib-\nited enhanced modelling ability for distinct time series prob-\nlems. The LSTM system comprises several memory blocks \nconnected using layers that comprise multiple recurrently \nassociated cells. The primary blocks of a simple LSTM \nsystem comprise an input layer for feeding sequences (i.e. \ntime series data); the model layer is employed for training \nthe system for long-term use of the sequence (time series) \ndata. To address a fundamental regression issue, four lay -\ners are used in the LSTM system: the network originates \nusing the ‘sequence input layer’ and the ‘LSTM layer’. The \nterminal side comprises the ‘fully connected layer’ and the \n‘regression output layer’. Theoretical notions of the LSTM \nare detailed in (Hochreiter & Schmidhuber 1997).\nThe CNN approach belongs to the deep learning para-\ndigm. This neural network is primarily unlike a traditional \nANN (i.e. MLP) since it comprises convolutional layers (of \nfilters). Automated feature identification is implemented in \nsuch layers, where critical input data features are mapped to \nthe required input–output association. Hence, CNN can pro-\ncess raw data, eliminating pre-processing or manual feature \nidentification. CNN is typically employed for image process-\ning. Hence, two-dimensional (2D CNN) convolutional fil-\nters have been employed (representation matches an image). \nNevertheless, sequential or temporal data evaluation uses \nsingle-dimension (1D CNN) convolutional filters (Li et al. \n2017). Such filters crawl the inputs to record probable tem-\nporal patterns for time series sequences. Hence, this paper \nuses 1D CNN. The conceptual framework of the CNN is \nexplained by LeCun et al. (LeCun et al. 1998).\nThe primary objective for choosing these benchmark frame-\nworks is to enhance precision and validity for performance \nassessment. Hence, the models’ distinct architectures and \ncommendable performance were the primary selection rea-\nsons; several recent papers have employed these for forecast-\ning evaporation (M. Abed et al. 2021, 2022). The target TNN \nmodel’s performance can be assessed from a wider perspective \nsince these benchmark frameworks have distinct architectures \nand belong to a broad spectrum of deep learning approaches. \nApplied Water Science (2023) 13:31 \n1 3 Page 11 of 19 31\nFig. 5  The proposed TNN model architecture\n Applied Water Science (2023) 13:31\n1 331 Page 12 of 19\nThis study does not employ empirical techniques for bench-\nmarks. It is better to choose sophisticated machine learning \nbenchmark frameworks that perform better than empirical \ntechniques specified in the literature. Moreover, employ -\ning models with relatively poor performance than superior \nnon-conventional machine learning approaches might high-\nlight significant performance gaps, causing TNN framework \noverestimation.\nPerformance evaluation\nIt is crucial to choose suitable performance indicators since \neach indicator has its own set of attributes. Furthermore, the \nway a model performs can be better understood by knowing the \nproperties of every statistical indicator. Thus, in this research, \nthe predictive performance of the model was assessed by using \nseveral statistical indicators, which are described below:\n(1) Coefficient of determination (R 2): The coefficient of \ndetermination represents the relationship between the esti-\nmated and real outputs; its value has a span of 0–1 (including \nboth limits). A value of zero signifies a stochastic framework, \nwhile a value of one signifies perfect fit. R2 is very widespread \nand makes model comparison more consistent and easier. It \naims to assess how well a prediction model fits a dataset, \nproviding researchers with instant feedback on the model's \nperformance.\n(2)R 2 =\n∑n\ni=1\n�y − y��\n̂y − ̂y\n�\n�\n∑n\ni=1\n�y − y�2 ∑n\ni=1\n�\n̂y − ̂y\n�2\n(2) Root mean square error (RMSE): RMSE represents \nthe square root of the error squares average with respect to \nthe estimated and real values. In regression model perfor -\nmance assessment, RMSE is more widely used compared \nto MSE. Moreover, RMSE is simple and easy to determine. \nAdditionally, RMSE penalises huge errors, and thus become \nmore acceptable.\n(3) Mean absolute error (MAE): The MAE is the absolute \ndifference between the estimated and actual output. MAE \ndoes not penalise high errors caused due to outliers.\n(4) Nash–Sutcliffe efficiency (NSE): NSE is a normal-\nised metric that determines the intensity of residual variance \n(noise) compared to that of the computed variability (infor-\nmation). It is still extensively used in hydrologic modelling, \npartly because it normalises the precision to a more under -\nstandable level.\nwhere n represents sample count, y represents the real out-\nput, /uni0302.s1y represents the predicted values, and y represents the \nreal output average.\nResults and discussion\nResults\nTo indicate that the TNN model for evaporation prediction \nis robust, this portion provides complete analysis of the \nempirical outcomes derived from experimenting using this \nmodel and comparative performance assessment of various \nmodels. In this study, in all, three models were used includ-\ning the model presented by the author, i.e. TNN model and \nstandard models; CNN and LSTM were used in monthly Ep \nforecasting task at four sites, namely Kuantan- 103° 13′  E, \n3° 46′ N; KLIA Sepang- 101° 42′ E, 2° 44′ N; Kota Bharu- \n102° 18′ E, 6° 10′ N; and Alor Setar- 100° 24′ E, 6° 12′ N in \nMalaysia. The performance of the TNN forecasting model \nwas first studied under different input combinations to obtain \nthe highest accuracy of the forecast. Then, comparison of the \nbest model was carried out against the other two standard \nmodels. All models, including the proposed model, were \nassessed using the outcomes of the following performance \n(3)RMSE =\n�∑n\ni=1 (y − ̂y)2\nn\n(4)MAE = 1\nn\nn/uni2211.s1\ni=1\n/uni007C.vary − ̂y/uni007C.var\n(5)NSE = 1 −\n∑n\ni=1 (y − ̂y)2\n∑n\ni=1\n�y − y�2\nTable 4  Hyperparameter tuning of Transformer model\nElement Hyperparameter Selected\nTNN Architecture Transformer Encoders 4\nNumber of neurons in MLP 128\nActivation function ELU\nDropout ratio 0.2\nOutput activation function Linear\nEncoder Number of heads 4\nHead size 64\nDropout ratio 0.25\nNumber of output filters in Con-\nvolutional 1D\n16\nKernal size (size of filter) 1\nTraining Loss function MSE\nOptimizer Adam\nLearning rate 1e-2\nNumber of epochs 200\nBatch size 16\nApplied Water Science (2023) 13:31 \n1 3 Page 13 of 19 31\nindicators: RMSE, MAE, NSE, and R2 in the testing duration \nfor all research sites.\nThe value of R2 is utilised to evaluate the effectiveness of \nall models investigated in this research in respect of degree \nof correlation between observed (E p) and forecasted (E p) \nvalues. For each model, the best statistical metrics have been \ndisplayed in bold. As can be noticed in Table  6, there is \nindeed a significant difference in the monthly Ep prediction \naccuracy determined by the input combinations. It had been \npossible to recognise the best prediction accuracy through \nthe model by using the entire meteorological dataset (RH, \nEP, Tmax, Tmin, Rs, and Sw) with respect to all sites, when \ncompared with combination of inputs concerning other inad-\nequate data input. In general, it further indicated that the \naccuracy of prediction models enhanced with extended input \nvariables, which was consistent with the findings of previous \nresearch (Fan et al. 2016; L. Wang et al. 2017a). Four input \ncombinations that did not include R s or Sw were sufficient \nto achieve reasonable accuracy with respect to monthly E p \nprediction. When only Tmax and RH data were available, the \nTNN model’s prediction accuracy was found to be inad-\nequate for all stations. This indicated that utilising advanced \ncapacities, such as that of AI, may not enhance the predictive \nperformance of the ML model, especially when there are \nlimited number of meteorological inputs. In addition, there \nwas a slight improvement in the prediction accuracy when \nEp was used as an input.\nIn comparison with the TNN and the benchmark mod -\nels, the values of R 2 for TNN tested at the four studied sta-\ntions are recorded to be 0.977 for Alor Setar, 0.989 for Kota \nBharu, 0.972 for KLIA Sepang, and 0.974 for Kuantan. For \nall the stations, the R2 values produced by the standard mod-\nels considered for this study are lower than that of the TNN \nmodel (see Table 7). Thus, it can be said that the TNN model \nshows the greatest degree of collinearity between estimated \n(Ep) and observed (Ep) values. Moreover, R2 values produced \nFig. 6  The TNN prediction \nmodel development procedure\n\n Applied Water Science (2023) 13:31\n1 331 Page 14 of 19\nby the TNN model for all stations are rather near to 1. It \ndeserves to be remarked that the value of the coefficient of \ndetermination (R2) achieves 1 in case of ideal performance \nof the model. Therefore, the TNN model apparently shows a \nbetter performance in comparison with the other benchmark \nmodels with respect to the value of R 2 determined for the \nestimated and observed values.\nThe current results suggest that the values of the Mean \nAbsolute Error (MAE) and Root Mean Square Error \n(RMSE) become lower with higher model performance. \nFor instance, for all study sites, the TNN model gives \nRMSE values of 0.138, 0.023, 0.091, and 0.082, and MAE \nvalues of 0.097, 0.016, 0.075, and 0.067, respectively. \nImportantly, the benchmark models yield higher RMSE \nand MAE values versus the TNN model for all the study \nstations, as seen in Table  7. For example, with regards to \nLSTM, RMSE, and MAE values were 0.166 and 0.135, \nand for CNN, they were 0.175 and 0.143, respectively. \nAt the Alor Setar site, all values are greater when com-\npared with the RMSE (0.138) and MAE (0.097) values \nwith regards to the TNN model. Such evaluation criteria \nevaluate the models based on forecast errors, and thus it \nis presumed that these forecasting models will yield lower \nvalues. As per the RMSE and MAE values, the TNN \nTable 5  Hyperparameter tuning of LSTM and CNN models\nModel Hyperparameter Selected\nLSTM Number of hidden layers 2\nNumber of neurons in the first hidden layer 512\nNumber of neurons in the second hidden layer 64\nNumber of epochs 500\nBatch size 8\nTraining algorithm Adam\nDropout ratio 0.4\nActivation function ReLu\nLearning rate 0.001\nLoss function MAE\nCNN Number of Conv1D layer 1\nNumber of filters in the Conv1D layer 32\nKernal size 2\nDropout ratio in the Conv1D layer 0.2\nNumber of fully connected layers (MLP) 2\nNumber of neurons in the first hidden layer 128\nNumber of neurons in the second hidden layer 256\nDropout ratio in the fully connected layers 0.1\nNumber of epochs 500\nBatch size 16\nTraining algorithm Adam\nActivation function ReLu\nLearning rate 0.001\nLoss function MSE\nTable 6  Statistical performance of the TNN model (testing period) \nused for estimating monthly Ep under six input scenarios for the stud-\nied stations\nStation Predictive Model R2 MAE RMSE NSE\nAlor Setar TNN-1 0.875 0.219 0.319 0.876\nTNN-2 0.914 0.187 0.265 0.915\nTNN-3 0.935 0.168 0.230 0.936\nTNN-4 0.954 0.142 0.193 0.955\nTNN-5 0.968 0.128 0.163 0.969\nTNN-6 0.977 0.097 0.138 0.978\nKota Bharu TNN-1 0.865 0.177 0.221 0.866\nTNN-2 0.878 0.160 0.209 0.879\nTNN-3 0.880 0.159 0.205 0.881\nTNN-4 0.929 0.120 0.155 0.930\nTNN-5 0.959 0.019 0.117 0.960\nTNN-6 0.989 0.016 0.023 0.987\nKLIA Sepang TNN-1 0.857 0.135 0.196 0.878\nTNN-2 0.912 0.111 0.145 0.913\nTNN-3 0.929 0.096 0.132 0.930\nTNN-4 0.940 0.091 0.107 0.941\nTNN-5 0.952 0.087 0.103 0.953\nTNN-6 0.972 0.075 0.091 0.968\nKuantan TNN-1 0.875 0.131 0.174 0.876\nTNN-2 0.889 0.124 0.164 0.890\nTNN-3 0.920 0.100 0.130 0.929\nTNN-4 0.919 0.107 0.148 0.920\nTNN-5 0.966 0.071 0.089 0.967\nTNN-6 0.974 0.067 0.082 0.975\nTable 7  Statistical performance of the proposed TNN and benchmark \nmodels (testing period) used for predicting monthly Ep for the studied \nstations\n* results obtained from (M. Abed et al. 2021)\n** results obtained from (M. Abed et al. 2022)\nStation Predictive Model R2 MAE RMSE NSE\nAlor Setar LSTM* 0.970 0.135 0.166 0.971\nCNN 0.960 0.143 0.175 0.961\nTNN-6 0.977 0.097 0.138 0.978\nKota Bharu LSTM* 0.986 0.058 0.074 0.986\nCNN 0.973 0.072 0.095 0.974\nTNN-6 0.989 0.016 0.023 0.987\nKLIA Sepang LSTM 0.947 0.047 0.121 0.948\nCNN** 0.965 0.079 0.091 0.966\nTNN-6 0.972 0.075 0.091 0.968\nKuantan LSTM 0.957 0.080 0.102 0.958\nCNN** 0.962 0.084 0.103 0.962\nTNN-6 0.974 0.067 0.082 0.975\nApplied Water Science (2023) 13:31 \n1 3 Page 15 of 19 31\nmodel showed significantly improved performance when \ncompared with all other comparable models as it dem-\nonstrated lesser forecasting errors. As another measure \npertaining to model accuracy, Fig.  7 displayed the radar \nplots in terms of the RMSE for the TNN model and the \nbenchmark models pertaining to all study sites. Moreover, \nthe Nash–Sutcliffe efficiency (NSE) value, which can be \nregarded as another metric employed for evaluating the \nput forward deep learning model’s efficacy, seems to be \nnear to a value of oneness for all study regions versus the \nbenchmark models. To emphasise this metric with regards \nto the TNN model, for instance, NSE ≥ 0.968 has been \nnoted for all the study regions. More importantly, these \nvalues were found to be higher when compared with the \ncomparative models with regards to all the study stations, \nas presented in Table  7. Overall, the current investigation \noffers convincing proof supporting that a significant poten-\ntial is associated with the TNN model to predict monthly \nEp, and this performance was found to be higher versus \ncomparative models pertaining to all the study sites in \nMalaysia.\nIt is worthy to note that the TNN model showed lower \npredictive error for all the study stations versus the bench-\nmark models employed in the testing phase. Figures  8 and \n9 display the scatter plots as well as time series that have \nbeen observed compared with the predicted monthly Ep with \nregards to the TNN model as well as the benchmark models \nwith regards to the testing phase. These scatter plots give \nthe coefficient of determination (R 2), which can inform a \nreader how effectively the variability in observed E p com-\npares to that of modelled E p, where this value is between \n0 and 1. If the model fits well and there is good resonation \nin the comparative correlations between the observed and \nforecasted Ep, the R 2 can be deemed to be nearer to one-\nness. As per Figs.  8 and 9, all the models showed high R 2 \nvalues closer to unity. However, the TNN model gave the \nhighest R2 value versus the other models with regards to \nall study sites. This deduction demonstrated that the TNN \nmodel could yield better accuracy with regards to Ep versus \nFig. 7  Spider chart of the Root Mean Square Error (RMSE) for the developed TNN and the comparative benchmark models\n Applied Water Science (2023) 13:31\n1 331 Page 16 of 19\nFig. 8  Scatter plot of measured Ep versus predicted Ep of TNN and benchmark models at all stations\nFig. 9  Time series of measured Ep versus predicted Ep of TNN and benchmark models at all stations\nApplied Water Science (2023) 13:31 \n1 3 Page 17 of 19 31\nthe other models examined in this study and proved that \nthe putative DL model could be regarded as an important \ntool for predicting E p. As per the results, the put forward \nTNN model was found to be better when compared with all \nbenchmark models in all sites, demonstrating the positive \nimpact cast by self-attention mechanism on improving the \naccuracy of prediction.\nDiscussion\nFor deep learning, transformers are regarded to be a state-\nof-the-art approach. The adoption of attention has been rev-\nolutionised by the transformer model via dispensing with \nconvolution and recurrence and, alternatively, depending \nexclusively on a self-attention mechanism. When it comes \nto time series forecasting, transformers are not able to ana-\nlyse their input in a sequential manner. Thus, this has helped \ndeal with the vanishing gradient problem that hampers the \nrecurrent neural networks (RNNs) with regards to long-term \nprediction. The present study applies a novel application \npertaining to self-attention algorithms with regards to the \nevaporative prediction domain. The put forward TNN model \nhas demonstrated good performances with regards to E p \nprediction for all the four selected stations, whereas other \nmodels investigated in this study have been placed in differ-\nent positions by considering the Ep prediction performances \npertaining to various sites. The consistency demonstrated by \nthe put forward TNN model with regards to the Ep prediction \nperformances by being the best E p prediction model for all \nstudy sites validates the impact of the self-attention mecha-\nnism. Thus, the architecture of the transformer, which is \nsolely based on self-attention mechanisms (intra-attention), \nhas demonstrated potential to enhance the DL predictive \nmodels’ performance.\nWith regards to the study limitations, data were gathered \nand modelled based on just four study regions in Malaysia \n(as a case study) to accomplish the goals. While this pio-\nneering research has yielded a new modelling structure with \nregards to the E p prediction, despite its restricted context, \nfurther research could choose a broader range of regions \nelsewhere, signifying different weather conditions. Never -\ntheless, the deep learning seems to have considerable impli-\ncations with regards to managing irrigations as well as other \nwater resource systems by monitoring the changes pertaining \nto monthly Ep.\nOne of the research’s practical implications is that the Ep \nmodelling approach, which can give a quite close estimate \nof the real water loss because of evaporation as well as its \nrelation to managing water resources, could be employed \nas a science-based strategic approach that can be applied \nto irrigation and other agricultural tasks. When E p values \nis multiplied by the surface area pertaining to the irrigation \nwater resources, the amount of water shortages resulting \nfrom evaporation (a primary component of water loss com-\nmon for the existing water asset volume) can be evaluated. \nThus, it becomes easy to estimate the total amount of exist-\ning water that can be used for irrigation, and planning and \nimplementation of a toolset of intelligent irrigation sched -\nules. These schedules can also help dodge unnecessary water \nlosses since irrigation practices become more relaxed. Thus, \nthe current TNN model employed for predicting E p is also \nbelieved to provide considerable economic advantages to \nthe agriculturists, especially in areas where farming is influ-\nenced by water resource issues, droughts, and other types \nof hydrological disparities. In addition, this study offers \noptimal advice for hydrologists on how to effectively ana -\nlyse non-stationary and nonlinear behaviours pertaining to \nhydrological cycles employing soft computing.\nConclusion\nThis study concentrated on developing a transformer‐based \narchitecture (TNN) model that can be employed practically \nfor prediction of monthly E p losses and provide a detailed \ncomparison with other benchmark DL models, including \nCNN and LSTM approaches. To evaluate the capabilities \npertaining to the designed DL models, monthly data from \nfour meteorological stations in Malaysia were considered \nfor predicting the E p rates. Time series data with regards \nto the monthly E p, like T min, Tmax, RH, R s, Sw, and E p, in \nthe years 2000–2019 were employed for testing (validation) \nand training (calibration) for the designed models. The input \nparameters (predictors) were chosen based on the Pearson's \ncorrelation coefficient values to detect the most effective \ninput combinations pertaining to the TNN model. Based on \nstandard statistical measures, the performance of each model \nand its efficacy with regard to evaporation forecasting were \nevaluated.\nThe investigation provided the following results:\n• A high level of prediction accuracy for the monthly E p \nwas observed with the three developed DL models.\n• The TNN model delivered enhanced performance when \nit comes to predicting monthly Ep versus the benchmark \nmodels with regards to all study sites.\n• Models that considered complete meteorological datasets \n(Tmin, Tmax, Sw, Rs, Ep, and RH) were found to achieve \nthe best prediction accuracy at all stations, versus other \ncombinations employing limited data input.\n• As evident in the results, the performance of the devel-\noped TNN model was significantly better versus other \nbenchmark models at all study sites. This supported the \nfact that the TNN model can be employed in an efficient \nmanner for predicting monthly Ep data series.\n Applied Water Science (2023) 13:31\n1 331 Page 18 of 19\n• In terms of application, the TNN model provides a pre-\ncise estimate of water loss due to evaporation and can \nthus be used in irrigation management, agriculture plan-\nning based on irrigation, and the reduction in fiscal and \neconomic losses in farming and related industries where \nconsistent supervision and estimation of water are con-\nsidered necessary for viable living and economy.\n• In the future, the applicability of the proposed technique \ncan be tested for different areas in Malaysia or elsewhere \nusing different data sets to develop a reliable, generaliz-\nable model that can predict evaporation.\nAcknowledgements The author would like to thank the Australian \nGovernment Research Training Programme Scholarship (RTP) for its \nsupport. Also, the Malaysian Meteorological Department (MMD) for \nproviding this study with the data.\nAuthor Contributions MA contributed to methodology, formal analy-\nsis, visualisation, and writing—review and editing; MAI contributed \nto writing—review and editing and supervision; ANA contributed to \nwriting—review and editing and supervision; YFH contributed to data \ncuration and writing—review and editing.\nFunding The APC was covered by Universiti Tunku Abdul Rahman \n(UTAR), Malaysia, via Research Publication Scheme (Project Number: \nUTARRPS 6251/H03) and UTARFSJPP.\nData availability The datasets used during the current study are avail-\nable from the first author on reasonable request.\nDeclarations \nConflict of interests The authors declare no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAbed MM, El-Shafie A, Osman SAB (2010) Creep predicting model \nin masonry structure utilizing dynamic neural network. J Comput \nSci 6(5):597\nAbed M, Imteaz MA, Ahmed AN, Huang YF (2021) Application of \nlong short-term memory neural network technique for predicting \nmonthly pan evaporation. Sci Rep 11(1):1–19\nAbed M, Imteaz MA, Ahmed AN, Huang YF (2022) Modelling \nmonthly pan evaporation utilising random forest and deep learn-\ning algorithms. Sci Rep 12(1):1–29\nAbtew W, Melesse A (2012) Evaporation and evapotranspiration: \nmeasurements and estimations. Springer Science & Business \nMedia, Berlin\nAshrafzadeh A, Ghorbani MA, Biazar SM, Yaseen ZM (2019) Evapo-\nration process modelling over northern Iran: application of an \nintegrative data-intelligence model with the krill herd optimiza-\ntion algorithm. Hydrol Sci J 64(15):1843–1856\nAshrafzadeh A, Malik A, Jothiprakash V, Ghorbani MA, Biazar \nSM (2020) Estimation of daily pan evaporation using neural \nnetworks and meta-heuristic approaches. ISH J Hydraulic Eng \n26(4):421–429\nBengio Y, Simard P, Frasconi P (1994) Learning long-term dependen-\ncies with gradient descent is difficult. IEEE Trans Neural Net-\nworks 5(2):157–166. https:// doi. org/ 10. 1109/ 72. 279181\nBiazar SM, Ghorbani MALI, Shahedi K (2019) Uncertainty of artifi-\ncial neural networks for daily evaporation prediction (Case study: \nRasht and Manjil Stations). J Watershed Manag Res 10(19):1–12\nCastellano-Méndez M, González-Manteiga W, Febrero-Bande M, \nPrada-Sánchez JM, Lozano-Calderón R (2004) Modelling of \nthe monthly and daily behaviour of the runoff of the Xallas \nriver using box-jenkins and neural networks methods. J Hydrol \n296(1–4):38–58\nChang F-J, Chang L-C, Huang C-W, Kao I-F (2016) Prediction of \nmonthly regional groundwater levels through hybrid soft-comput-\ning techniques. J Hydrol 541:965–976. https:// doi. org/ 10. 1016/j. \njhydr ol. 2016. 08. 006\nChen Y, He L, Li J, Zhang S (2018) Multi-criteria design of shale-gas-\nwater supply chains and production systems towards optimal life \ncycle economics and greenhouse gas emissions under uncertainty. \nComput Chem Eng 109:216–235\nChen Y, Chen W, Janizadeh S, Bhunia GS, Bera A, Pham QB, Linh \nNTT, Balogun A-L, Wang X (2022) Deep learning and boost-\ning framework for piping erosion susceptibility modeling: spatial \nevaluation of agricultural areas in the semi-arid region. Geocarto \nInt 37(16):4628–4654\nDaliakopoulos IN, Coulibaly P, Tsanis IK (2005) Groundwater level \nforecasting using artificial neural networks. J Hydrol 309(1):229–\n240. https:// doi. org/ 10. 1016/j. jhydr ol. 2004. 12. 001\nDalkiliç Y, Okkan U, Baykan N (2014) Comparison of different ANN \napproaches in daily pan evaporation prediction. J Water Resour \nProtect 6:319–326\nDeo RC, Samui P, Kim D (2016) Estimation of monthly evaporative \nloss using relevance vector machine, extreme learning machine \nand multivariate adaptive regression spline models. Stoch Env Res \nRisk Assess 30(6):1769–1784\nDitthakit P, Pinthong S, Salaeh N, Weekaew J, Tran TT, & Pham QB \n(2022) Comparative study of machine learning methods and \nGR2M model for monthly runoff prediction. Ain Shams Eng J \n101941\nFalamarzi Y, Palizdan N, Huang YF, Lee TS (2014) Estimating evapo-\ntranspiration from temperature and wind speed data using artifi-\ncial and wavelet neural networks (WNNs). Agric Water Manag \n140:26–36\nFan J, Wu L, Zhang F, Xiang Y, Zheng J (2016) Climate change effects \non reference crop evapotranspiration across different climatic \nzones of China during 1956–2015. J Hydrol 542:923–937\nFerreira LB, da Cunha FF (2020) Multi-step ahead forecasting of daily \nreference evapotranspiration using deep learning. Comput Elec-\ntron Agric 178:105728. https:// doi. org/ 10. 1016/j. compag. 2020. \n105728\nFreedman D, Pisani R, Purves R, Adhikari A (2007) Statistics. WW \nNorton & Company, New York\nGao Y, Hendricks LA, Kuchenbecker KJ, Darrell T (2016) Deep learn-\ning for tactile understanding from visual and haptic data. IEEE Int \nConf Robot. Autom (ICRA) 2016:536–543\nApplied Water Science (2023) 13:31 \n1 3 Page 19 of 19 31\nGoyal MK, Bharti B, Quilty J, Adamowski J, Pandey A (2014) Mod-\neling of daily pan evaporation in sub tropical climates using \nANN, LS-SVR, Fuzzy Logic, and ANFIS. Expert Syst Appl \n41(11):5267–5276\nHauke J, Kossowski T (2011) Comparison of values of Pearson’s \nand Spearman’s correlation coefficient on the same sets of data. \nQuaest Geogr 30:87\nHochreiter S, Schmidhuber J (1997) Long short-term memory. Neural \nComput 9(8):1735–1780\nHu C, Wu Q, Li H, Jian S, Li N, Lou Z (2018) Deep learning with a \nlong short-term memory networks approach for rainfall-runoff  \nsimulation. Water. https:// doi. org/ 10. 3390/ w1011 1543\nKahler DM, Brutsaert W (2006) Complementary relationship between \ndaily evaporation in the environment and pan evaporation. Water \nResour Res 42(5)\nKingma DP, & Ba J (2014). Adam: a method for stochastic optimiza-\ntion. ArXiv Preprint http:// arxiv. org/ abs/ 1412. 6980\nKişi Ö (2009) Daily pan evaporation modelling using multi-layer per-\nceptrons and radial basis neural networks. Hydrol Process Int J \n23(2):213–223\nKişi Ö (2013) Evolutionary neural networks for monthly pan evapora-\ntion modeling. J Hydrol 498:36–45\nKisi O, Genc O, Dinc S, Zounemat-Kermani M (2016) Daily pan evap-\noration modeling using chi-squared automatic interaction detec-\ntor, neural networks, classification and regression tree. Comput \nElectron Agric 122:112–117\nKrizhevsky A, Sutskever I, Hinton GE (2017) ImageNet classifica-\ntion with deep convolutional neural networks. Commun ACM \n60(6):84–90\nLeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-\nbased learning applied to document recognition. Proc IEEE \n86(11):2278–2324\nLee H, Pham P, Largman Y, Ng A (2009) Unsupervised feature learn-\ning for audio classification using convolutional deep belief net-\nworks. Adv Neural Inf Process Syst 22:1096–1104\nLi D, Zhang J, Zhang Q, Wei X (2017) Classification of ECG signals \nbased on 1D convolution neural network. In: 2017 IEEE 19th \nInternational Conference on E-Health Networking, Applications \nand Services (Healthcom), 1–6. Doi: https:// doi. org/ 10. 1109/ Healt \nhCom. 2017. 82107 84\nLin, M., Chen, Q., & Yan, S. (2013). Network in network. Preprint \nhttp:// arxiv. org/ abs/ 1312. 4400\nLiu JNK, Hu Y, You JJ, Chan PW (2014). Deep neural network based \nfeature representation for weather forecasting.In: Proceedings on \nthe International Conference on Artificial Intelligence (ICAI), 1\nMajhi B, Naidu D, Mishra AP, Satapathy SC (2020) Improved predic-\ntion of daily pan evaporation using Deep-LSTM model. Neural \nComput Appl 32(12):7823–7838\nMalik A, Kumar A, Kim S, Kashani MH, Karimi V, Sharafati A, Ghor-\nbani MA, Al-Ansari N, Salih SQ, Yaseen ZM (2020) Modeling \nmonthly pan evaporation process over the Indian central Himala-\nyas: application of multiple learning artificial intelligence model. \nEng Appl Comput Fluid Mech 14(1):323–338\nPham QB, Kumar M, Di Nunno F, Elbeltagi A, Granata F, Islam \nARM, Talukdar S, Nguyen XC, Ahmed AN, & Anh DT (2022). \nGroundwater level prediction using machine learning algorithms \nin a drought-prone area. Neural Comput Appl 1–23\nShaaban AJ, Low KS (2003) Droughts in Malaysia: a look at its char-\nacteristics, impacts, related policies and management strategies. \nIn: Water and Drainage 2003 Conference, 28–29\nSudheer KP, Gosain AK, Mohana Rangan D, Saheb SM (2002) Mod-\nelling evaporation using an artificial neural network algorithm. \nHydrol Process 16(16):3189–3202\nTofiq YM, Latif SD, Ahmed AN, Kumar P, El-Shafie A (2022) Opti-\nmized model inputs selections for enhancing river streamflow \nforecasting accuracy using different artificial intelligence tech-\nniques. Water Resour Manag 1–18\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, \nKaiser Ł, & Polosukhin I (2017) Attention is all you need. Adv \nNeural Inform Process Syst 30\nWang L, Kisi O, Zounemat-Kermani M, Li H (2017a) Pan evaporation \nmodeling using six different heuristic computing methods in dif-\nferent climates of China. J Hydrol 544:407–427\nWang L, Kisi O, Zounemat-Kermani M, Gan Y (2016) Comparison of \nsix different soft computing methods in modeling evaporation in \ndifferent climates. Hydrol Earth Syst Sci Discuss 1–51\nWang Z, Yan W, Oates T (2017b) Time series classification from \nscratch with deep neural networks: a strong baseline. In: 2017b \nInternational Joint Conference on Neural Networks (IJCNN), \n1578–1585\nWu L, Huang G, Fan J, Ma X, Zhou H, Zeng W (2020a) Hybrid \nextreme learning machine with meta-heuristic algorithms for \nmonthly pan evaporation prediction. Comput Electron Agric \n168:105115\nWu N, Green B, Ben X, O’Banion S (2020b). Deep transformer models \nfor time series forecasting: The influenza prevalence case. Preprint \nhttp:// arxiv. org/ abs/ 2001. 08317\nZhang J, Zhu Y, Zhang X, Ye M, Yang J (2018) Developing a long \nshort-term memory (LSTM) based model for predicting water \ntable depth in agricultural areas. J Hydrol 561:918–929\nPublisher's note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.6310741901397705
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6309887170791626
    },
    {
      "name": "Computer science",
      "score": 0.5794485211372375
    },
    {
      "name": "Artificial neural network",
      "score": 0.5435599684715271
    },
    {
      "name": "Water resources",
      "score": 0.5047913789749146
    },
    {
      "name": "Pan evaporation",
      "score": 0.460050106048584
    },
    {
      "name": "Evaporation",
      "score": 0.45353105664253235
    },
    {
      "name": "Benchmarking",
      "score": 0.4456932842731476
    },
    {
      "name": "Transformer",
      "score": 0.44199004769325256
    },
    {
      "name": "Environmental science",
      "score": 0.42745110392570496
    },
    {
      "name": "Machine learning",
      "score": 0.4210440218448639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4168699085712433
    },
    {
      "name": "Data mining",
      "score": 0.4028898775577545
    },
    {
      "name": "Meteorology",
      "score": 0.38807663321495056
    },
    {
      "name": "Hydrology (agriculture)",
      "score": 0.3473498821258545
    },
    {
      "name": "Operations research",
      "score": 0.33740562200546265
    },
    {
      "name": "Mathematics",
      "score": 0.21189436316490173
    },
    {
      "name": "Engineering",
      "score": 0.15278753638267517
    },
    {
      "name": "Geography",
      "score": 0.09148627519607544
    },
    {
      "name": "Cartography",
      "score": 0.09130579233169556
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geotechnical engineering",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57093077",
      "name": "Swinburne University of Technology",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I79156528",
      "name": "Universiti Tenaga Nasional",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I931681460",
      "name": "Universiti Tunku Abdul Rahman",
      "country": "MY"
    }
  ],
  "cited_by": 33
}