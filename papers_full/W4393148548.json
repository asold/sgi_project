{
    "title": "ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining",
    "url": "https://openalex.org/W4393148548",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5012042767",
            "name": "Dezhi Peng",
            "affiliations": [
                "South China University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5037101589",
            "name": "Chongyu Liu",
            "affiliations": [
                "South China University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100389396",
            "name": "Yuliang Liu",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5080674767",
            "name": "Lianwen Jin",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6760808182",
        "https://openalex.org/W3100936573",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W3193700193",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W6849617936",
        "https://openalex.org/W6841110117",
        "https://openalex.org/W6803870738",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3139587317",
        "https://openalex.org/W4322746719",
        "https://openalex.org/W2146751553",
        "https://openalex.org/W4225106329",
        "https://openalex.org/W6652761251",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3192658445",
        "https://openalex.org/W4306704603",
        "https://openalex.org/W4286750974",
        "https://openalex.org/W6782830577",
        "https://openalex.org/W6849290737",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4327717010",
        "https://openalex.org/W4293518729",
        "https://openalex.org/W6737717365",
        "https://openalex.org/W3204182250",
        "https://openalex.org/W2884959301",
        "https://openalex.org/W3162234211",
        "https://openalex.org/W3194722168",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3104631613",
        "https://openalex.org/W6772004348",
        "https://openalex.org/W6773500736",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W2963647456",
        "https://openalex.org/W4385696635",
        "https://openalex.org/W6848334058",
        "https://openalex.org/W3016138282",
        "https://openalex.org/W6766668266",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W6803924480",
        "https://openalex.org/W3104953317",
        "https://openalex.org/W4283821822",
        "https://openalex.org/W6774577581",
        "https://openalex.org/W6604801084",
        "https://openalex.org/W6756441975",
        "https://openalex.org/W6736305252",
        "https://openalex.org/W4312804044",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W2144554289",
        "https://openalex.org/W4289751996",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3003218881",
        "https://openalex.org/W3082245115",
        "https://openalex.org/W2995942257",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3003433862",
        "https://openalex.org/W2967615747",
        "https://openalex.org/W2972866811",
        "https://openalex.org/W2785383245",
        "https://openalex.org/W3179897446",
        "https://openalex.org/W4304091583",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4312528217",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4319300528",
        "https://openalex.org/W2996685953",
        "https://openalex.org/W3010428518",
        "https://openalex.org/W2125389028",
        "https://openalex.org/W4313038702",
        "https://openalex.org/W3035679705",
        "https://openalex.org/W4214633470",
        "https://openalex.org/W4386453743",
        "https://openalex.org/W4312349930",
        "https://openalex.org/W2008806374",
        "https://openalex.org/W2963005009",
        "https://openalex.org/W4306987814",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2605982830",
        "https://openalex.org/W4367315935",
        "https://openalex.org/W3213555225",
        "https://openalex.org/W4312646899"
    ],
    "abstract": "Scene text removal (STR) aims at replacing text strokes in natural scenes with visually coherent backgrounds. Recent STR approaches rely on iterative refinements or explicit text masks, resulting in high complexity and sensitivity to the accuracy of text localization. Moreover, most existing STR methods adopt convolutional architectures while the potential of vision Transformers (ViTs) remains largely unexplored. In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed ViTEraser. Following a concise encoder-decoder framework, ViTEraser can easily incorporate various ViTs to enhance long-range modeling. Specifically, the encoder hierarchically maps the input image into the hidden space through ViT blocks and patch embedding layers, while the decoder gradually upsamples the hidden features to the text-erased image with ViT blocks and patch splitting layers. As ViTEraser implicitly integrates text localization and inpainting, we propose a novel end-to-end pretraining method, termed SegMIM, which focuses the encoder and decoder on the text box segmentation and masked image modeling tasks, respectively. Experimental results demonstrate that ViTEraser with SegMIM achieves state-of-the-art performance on STR by a substantial margin and exhibits strong generalization ability when extended to other tasks, e.g., tampered scene text detection. Furthermore, we comprehensively explore the architecture, pretraining, and scalability of the ViT-based encoder-decoder for STR, which provides deep insights into the application of ViT to the STR field. Code is available at https://github.com/shannanyinxiang/ViTEraser.",
    "full_text": "ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal\nwith SegMIM Pretraining\nDezhi Peng1,3, Chongyu Liu1, Yuliang Liu4, Lianwen Jin1,2,3,*\n1South China University of Technology\n2SCUT-Zhuhai Institute of Modern Industrial Innovation\n3INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition\n4Huazhong University of Science and Technology\npengdzscut@foxmail.com, eelwjin@scut.edu.cn\nAbstract\nScene text removal (STR) aims at replacing text strokes in\nnatural scenes with visually coherent backgrounds. Recent\nSTR approaches rely on iterative refinements or explicit text\nmasks, resulting in high complexity and sensitivity to the\naccuracy of text localization. Moreover, most existing STR\nmethods adopt convolutional architectures while the potential\nof vision Transformers (ViTs) remains largely unexplored.\nIn this paper, we propose a simple-yet-effective ViT-based\ntext eraser, dubbed ViTEraser. Following a concise encoder-\ndecoder framework, ViTEraser can easily incorporate vari-\nous ViTs to enhance long-range modeling. Specifically, the\nencoder hierarchically maps the input image into the hid-\nden space through ViT blocks and patch embedding layers,\nwhile the decoder gradually upsamples the hidden features\nto the text-erased image with ViT blocks and patch splitting\nlayers. As ViTEraser implicitly integrates text localization\nand inpainting, we propose a novel end-to-end pretraining\nmethod, termed SegMIM, which focuses the encoder and de-\ncoder on the text box segmentation and masked image mod-\neling tasks, respectively. Experimental results demonstrate\nthat ViTEraser with SegMIM achieves state-of-the-art perfor-\nmance on STR by a substantial margin and exhibits strong\ngeneralization ability when extended to other tasks,e.g., tam-\npered scene text detection. Furthermore, we comprehensively\nexplore the architecture, pretraining, and scalability of the\nViT-based encoder-decoder for STR, which provides deep in-\nsights into the application of ViT to the STR field. Code is\navailable at https://github.com/shannanyinxiang/ViTEraser.\nIntroduction\nScene text removal (STR) aims to realistically erase the text\nstrokes in the wild by replacing them with visually plausible\nbackground, which has been widely applied to privacy pro-\ntection (Inai et al. 2014), image editing (Wu et al. 2019), and\nimage retrieval (Tursun et al. 2019a). Existing approaches to\nSTR have evolved from the one-stage paradigm which im-\nplicitly integrates the text localization and background in-\npainting into a single network without the guidance of ex-\nplicit text masks (Nakamura et al. 2017; Zhang et al. 2019b;\nLiu et al. 2020), to the two-stage framework which contains\nexplicit text localizing processes and uses the resulting text\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nEncoder\n DecoderImage OutputViT blocks\n(e) Ours\n(ViTEraser)\nSegMIM Pretraining\n... Image Output\nSTR Net\nSTR Net(d) Multi-step \nText Loc.Image Output\nInpaintText\nMask\nGT\n(c) Two stage\n(Separate)\nText Loc.Image Output\nInpaintText\nMask\n(b) Two stage \n(End to end)\nEncoder\n DecoderImage Output(a) One stage\nViT blocks\nFigure 1: Comparison of ViTEraser with existing STR\nparadigms. Our method revisits the conventional single-step\none-stage framework and improves it with ViTs for feature\nmodeling and the proposed SegMIM pretraining. Dashed ar-\nrows indicate cutting off gradient flow. (Loc.: Localization)\nmasks to facilitate background inpainting (Tang et al. 2021;\nLee and Choi 2022; Wang et al. 2023; Du et al. 2023b).\nDespite the great success achieved by previous meth-\nods, there still remain two critical issues. (1) The dominant\ntwo-stage methods suffer from the complex system design\nwith two sub-tasks. The sequential text localizing and back-\nground inpainting pipeline introduces additional parameters,\ndecreases the inference speed, and, more importantly, breaks\nthe integrity of the entire model. The error of text localiza-\ntion can be easily propagated to the background inpainting,\nespecially for the methods that require pre-supplied text de-\ntectors (Tang et al. 2021; Liu et al. 2022a; Lee and Choi\n2022). (2) Recent advances (Liu et al. 2020; Lyu and Zhu\n2022; Du et al. 2023b; Wang et al. 2023) tend to employ a\nmulti-step paradigm in a coarse-to-fine or progressive fash-\nion, which significantly undermines efficiency and makes it\ndifficult to balance the parameters involved in multiple steps.\nTo this end, we revisit the one-stage paradigm and pro-\npose a novel simple-yet-effective ViT-based method for\nSTR, termed as ViTEraser. Fig. 1 compares our method with\nexisting STR approaches. The ViTEraser follows the con-\nventional one-stage framework which comprises a single-\nstep encoder-decoder network and is free of text mask in-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4468\nput or text localizing processes. This concise pipeline per-\nfectly gets rid of the drawbacks of the two-stage and multi-\nstep approaches mentioned above, but has been discarded\nin recent advances because of the unexpected artifacts and\ninexhaustive erasure issues caused by the implicit text lo-\ncalization mechanism. However, we argue that these limita-\ntions are actually due to the insufficient capacity of previ-\nous CNN-based architectures. Recently, vision Transform-\ners (ViTs) (Dosovitskiy et al. 2021) have achieved incredible\nsuccess on diverse computer vision tasks (Han et al. 2022)\nbut are rarely investigated for STR. Nevertheless, ViT is per-\nfect for STR since global information is indispensable for\ndetermining text locations and background textures, espe-\ncially for large texts that existing STR systems still struggle\nwith. Therefore, for the first time in the STR field, the pro-\nposed ViTEraser thoroughly utilizes ViTs for feature repre-\nsentation in both the encoder and decoder. Specifically, the\nencoder hierarchically maps the input image into the hidden\nspace through ViT blocks and patch embedding layers, while\nthe decoder gradually upsamples the hidden features to the\ntext-erased image with ViT blocks and patch splitting layers.\nThanks to its high generality, ViTEraser can be effortlessly\nintegrated with various ViTs, e.g., Swin Transformer (v2)\n(Liu et al. 2021, 2022b), PVT (Wang et al. 2021, 2022a).\nDespite the powerful ViT-based structure, the implicit in-\ntegration of text localizing and background inpainting still\nsignificantly challenges the model capacity of ViTEraser,\nrequiring both high-level text perception and fine-grained\npixel infilling abilities. However, the insufficient scale of ex-\nisting STR datasets (Liu et al. 2020) limits the full learn-\ning of these abilities and makes the large-capacity ViT-based\nmodel prone to overfitting. To solve similar issues, pretrain-\ning plays a crucial role in a variety of fields (Kenton and\nToutanova 2019; Xu et al. 2020; Yang et al. 2022) but is quite\nunder-explored in the STR realm. Moreover, with the rapid\ndevelopment of large-scale scene text detection datasets and\ncommercial optical character recognition (OCR) APIs, nu-\nmerous real-world images with text bounding boxes are eas-\nily available. Therefore, we propose SegMIM which fully\npretrains STR models using large-scale scene text detec-\ntion data. Concretely, by assigning two pretraining tasks of\ntext box segmentation and mask image modeling (MIM)\n(He et al. 2022; Xie et al. 2022) to the output features of\nthe encoder and decoder, respectively, the STR performance\ncan be effectively boosted with enhanced text localizing, in-\npainting, and global reasoning abilities.\nExtensive experiments are conducted on two STR bench-\nmarks including SCUT-EnsText (Liu et al. 2020) and SCUT-\nSyn (Zhang et al. 2019b). Furthermore, we comprehensively\nexplore the architecture, pretraining, and scalability of the\nViT-based encoder-decoder for STR. The experimental re-\nsults demonstrate the clear superiority of ViTEraser with\nand without the SegMIM pretraining. Additionally, ViT-\nEraser also achieves state-of-the-art performance on tam-\npered scene text detection using the Tampered-IC13 (Wang\net al. 2022b) dataset, exhibiting strong generalization ability.\nIn summary, the contributions of this paper are as follows.\n• We propose a novel ViT-based one-stage method for\nSTR, termed as ViTEraser. The ViTEraser adopts a con-\ncise single-step encoder-decoder paradigm, thoroughly\nintegrating ViTs for feature representation in both the en-\ncoder and decoder.\n• We propose SegMIM, a new pretraining scheme for STR.\nWith SegMIM, ViTEraser acquires enhanced global rea-\nsoning capabilities, enabling it to effectively distinguish\nand generate text and background regions.\n• We conduct a comprehensive investigation into the ar-\nchitecture, pretraining, and scalability of the ViT-based\nencoder-decoder for STR, which provides deep insights\ninto the application of ViT to the STR field.\n• The experiments demonstrate that ViTEraser achieves\nstate-of-the-art performance on STR, and its potential for\nextension to other domains is also highlighted.\nRelated Work\nScene Text Removal\nScene text removal aims at realistically erasing the texts in\nnatural scenes. Existing methods can be divided into one-\nstage and two-stage categories based on whether there are\nexplicit text localizing processes.\nOne-stage methods follow a concise image-to-image\ntranslation pipeline, implicitly integrating text localizing and\nbackground inpainting procedures into a single network.\nNakamura et al. (2017) pioneered in erasing texts at patch\nlevel using a convolution-to-deconvolution encoder-decoder\nstructure. Inspired by Pix2Pix (Isola et al. 2017), Zhang et al.\n(2019b) proposed an end-to-end cGAN-based (Mirza and\nOsindero 2014) EnsNet which directly erases texts at image\nlevel. EraseNet (Liu et al. 2020) further improved EnsNet\nfollowing a coarse-to-refine pipeline. From a data perspec-\ntive, Jiang et al. (2022) proposed a controllable synthesis\nmodule based on EraseNet.\nTwo-stage methodsdecompose STR into the text local-\nizing and background inpainting processes. The text local-\nizing component produces explicit text masks which are fed\ninto subsequent modules to facilitate background inpainting.\nThe two-stage methods can be further divided into separate\nand end-to-end categories. The separate two-stage meth-\nods depend on separately trained text detectors (Zdenek and\nNakayama 2020; Conrad and Chen 2021; Liu et al. 2022a)\nor ground truth (GT) (Qin, Wei, and Manduchi 2018; Tur-\nsun et al. 2019b; Tang et al. 2021; Lee and Choi 2022) to\nobtain text masks. In contrast, end-to-end two-stage meth-\nods end-to-end optimize the text localizing modules with\nother components (Keserwani and Roy 2021). Under this\nparadigm, recent advances tended to devise coarse-to-refine\n(Tursun et al. 2020; Du et al. 2023a) or progressive frame-\nworks (Lyu and Zhu 2022; Bian et al. 2022; Du et al. 2023b;\nWang et al. 2023) with text segmentation modules. On the\ncontrary, Hou, Chen, and Wang (2022) expanded the width\nof the network in a multi-branch fashion. Additionally, Lyu\net al. (2023) incorporated text segmentation maps at feature\nlevel using the proposed FET mechanism. Although the two-\nstage methods have dominated the STR field, they suffer\nfrom the high complexity caused by multiple modules and\nprogressive erasing and are prone to text localizing accuracy.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4469\nPatch Embed\nViT Blocks\nViT Blocks\nPatch Embed\nViT Blocks\nPatch Embed\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\n(b) Decoder\nStage  3 Stage  1 Stage  2 Stage  3 Stage  4 Stage  5\nPatch Embed\nViT Blocks\n(a) Encoder\nStage  4Stage  1 Stage  2\nFigure 2: Overall architecture of ViTEraser. The ViTEraser follows the one-stage paradigm but is thoroughly equipped with\nViTs, yielding a simple-yet-effective STR approach that is free of progressive refinements and text localizing processes.\nVision Transformer\nThe Transformer (Vaswani et al. 2017) was first proposed for\nnatural language processing (Kenton and Toutanova 2019)\nbut rapidly swept the computer vision field (Han et al. 2022).\nEarly ViTs (Dosovitskiy et al. 2021; Touvron et al. 2021)\nfirst tokenized images with large window sizes and then kept\nthe feature size throughout all Transformer layers. Recently,\nthe research on ViTs has focused on producing pyramid fea-\nture maps, e.g., PVT (Wang et al. 2021, 2022a), HVT (Pan\net al. 2021), Swin Transformer (Liu et al. 2021, 2022b), and\nPiT (Heo et al. 2021). Nowadays, ViTs have played an im-\nportant role in many tasks, such as object detection (Carion\net al. 2020), semantic segmentation (Xie et al. 2021; Cao\net al. 2022), text spotting (Peng et al. 2022; Liu et al. 2023),\nand document understanding (Xu et al. 2020).\nViTEraser\nAs shown in Fig. 2, we revisit the conventional single-step\none-stage paradigm, getting rid of the complicated itera-\ntive refinement and the susceptibility to text localizing ac-\ncuracy. The proposed ViTEraser pioneers in thoroughly em-\nploying ViTs instead of CNN in both the encoder and de-\ncoder, yielding a simple-yet-effective pipeline. Concretely,\nthe encoder hierarchically maps the input image into the hid-\nden space through successive ViT blocks and patch embed-\nding layers, while the decoder gradually upsamples the hid-\nden features to the text-erased image with successive ViT\nblocks and patch splitting layers. ViT blocks throughout the\nencoder-decoder provide sufficient global context informa-\ntion, enabling the implicit integration of text localization and\nbackground inpainting into a single network within a single\nforward pass. Moreover, lateral connections are devised be-\ntween the encoder and decoder to preserve the input details.\nEncoder\nAs shown in Fig. 2(a), the encoder of ViTEraser consists of\nfour stages. Given an input image Iin ∈ RH×W×3, the en-\ncoder hierarchically produces four feature maps {fenc\ni }4\ni=1\nwith strides of {2i+1}4\ni=1 w.r.tthe input image and channels\nof {Cenc\ni }4\ni=1, respectively. Specifically, the i-th stage first\ndownsamples the spatial size using a patch embedding layer\nand then captures global correlation through a stack ofNenc\ni\nViT blocks.\nPatch Embedding Layer Given an input feature map\nfin ∈ Rh×w×cin, a patch embedding layer with a downsam-\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\nViT Blocks\nPatch Split\nStage  3 Stage  4 Stage  5\nDecoder (a) TS (b) Multi-scale TE\nFigure 3: Auxiliary outputs of ViTEraser during training, in-\ncluding (a) text box segmentation map and (b) multi-scale\ntext erasing results. (TS: Text Segmentation, TE: Text Eras-\ning)\npling ratio of d and an output channel of cout first flattens\neach d×d patch, yielding a h\nd × w\nd ×(d2 ×cin) feature map.\nThen a 1×1 convolution layer is applied to transform this in-\ntermediate feature map into the output fout ∈ R\nh\nd ×w\nd ×cout.\nDecoder\nThe decoder contains five stages as illustrated in Fig. 2(b).\nBased on the final feature fenc\n4 of the encoder, the de-\ncoder hierarchically generates five feature maps {fdec\ni ∈\nR\nH\n25−i × W\n25−i ×Cdec\ni }5\ni=1. Concretely, in each stage, the fea-\nture is first processed with Ndec\ni ViT blocks and then up-\nsampled by 2 via a patch splitting layer. Moreover, lateral\nconnections (Liu et al. 2020) are built between the features\n{fenc\ni }3\ni=1 of the encoder and the features {fdec\n4−i}3\ni=1 of the\ndecoder. Finally, the text-erased image is predicted via a\n3×3 convolution based on the featurefdec\n5 ∈ RH×W×Cdec\n5 .\nPatch Splitting Layer Patch splitting is designed as the\ninverse operation of the patched embedding to upsample the\nspatial size of features. Fed with an input feature map fin ∈\nRh×w×cin, the patch splitting layer first decomposes each\ncin-dimensional token into a2×2 patch with cin\n4 dimension,\nexpanding the input feature map to a shape of2h×2w× cin\n4 .\nAfter that, a 1 × 1 convolution layer is adopted to produce\nthe output feature map fout ∈ R2h×2w×cout.\nTraining\nAs depicted in Fig. 3, auxiliary outputs are produced during\nonly training, including a text box segmentation map Mout\nand multi-scale text erasing results{I\n1\n2\nout, I\n1\n4\nout}. Specifically,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4470\nEncoder \n Decoder \nRandom\nMasking\n Reconstruct\nText Box Segmentation\nL1 Loss\n Dice Loss\nFigure 4: Pipeline of the proposed SegMIM pretraining. Given a randomly masked image, the text box segmentation and masked\nimage modeling tasks are accomplished on top of the encoder and decoder, respectively.\nMout is predicted based on fdec\n4 via a 3 × 3 deconvolution\nfor 2× upsampling and a 3 × 3 convolution with Sigmoid\nactivation. Besides,I\n1\n2\nout ∈ R\nH\n2 ×W\n2 ×3 and I\n1\n4\nout ∈ R\nH\n4 ×W\n4 ×3\nare generated from featuresfdec\n4 and fdec\n3 , respectively, each\nthrough a 3 × 3 convolution.\nThe model training adopts a GAN-based paradigm with a\nlocal-global discriminator (Liu et al. 2020). Given an input\nimage with corresponding text-erased image and groud-truth\n(GT) text box mask, the losses comprise multi-scale recon-\nstruction loss, perceptual loss, style loss, segmentation loss,\nand adversarial loss, following EraseNet (Liu et al. 2020).\nSegMIM Pretraining\nUnlike two-stage methods that utilize task-specific mod-\nules and training objectives, ViTEraser implicitly integrates\ntext localizing and background inpainting tasks into a sin-\ngle encoder-decoder, thus facing challenges in fully learn-\ning to handle both tasks and susceptible to overfitting when\nscaled up. This limitation arises from the scarcity of training\nsamples in STR and the high costs associated with anno-\ntating them. Moreover, enormous natural scene images with\ntext bounding boxes are easily available as the advancing of\nscene text detection datasets and OCR APIs. To this end,\nleveraging the availability of extensive scene text detection\ndatasets, we exploit large-scale pretraining techniques which\nhave recently shown significant advancements (Yang et al.\n2022; Xu et al. 2020) but rarely been investigated for STR.\nBecause STR can be decomposed into text localizing and\nbackground inpainting sub-tasks, we intuitively propose a\nnew pretraining paradigm for STR, termed SegMIM, which\nfocuses the encoder on text box segmentation and the de-\ncoder on masked image modeling (MIM) as shown in Fig. 4.\nDespite its simplicity, the clear advantages and interpretabil-\nity of SegMIM are manifold. (1) The model learns the dis-\ncriminative representation of texts and backgrounds through\nthe text box segmentation task, which is crucial to STR. (2)\nThe model learns the generative features of texts and back-\ngrounds via MIM, enhancing the text perception and back-\nground recovery.(3) The global reasoning capacity is signif-\nicantly improved due to the high mask ratio (0.6).\nArchitecture\nThe network architecture during pretraining inherits the\nencoder-decoder structure as shown in Fig. 2 but adds two\nextra heads for text box segmentation and image reconstruc-\ntion, respectively. Given an input image Iin ∈ RH×W×3, a\nbinary mask Mmim ∈ RH×W×1 is randomly generated fol-\nlowing SimMIM (Xie et al. 2022). Then, the masked image\nImask combining Iin and Mmim is fed into the network.\nText Box Segmentation Head Based on the final feature\nfenc\n4 ∈ R\nH\n32 ×W\n32 ×Cenc\n4 of the encoder, a 1 × 1 convolu-\ntion layer changes its dimension from Cenc\n4 to 1024. Sub-\nsequently, after transforming each1024-dimension vector to\na 32 × 32 patch and activating using a sigmoid function, a\ntext box segmentation map Iseg ∈ RH×W×1 is obtained.\nImage Reconstruction Head Through a 3 × 3 convolu-\ntion, a reconstructed image Irec ∈ RH×W×3 is predicted\nusing the final feature fdec\n5 ∈ RH×W×Cdec\n5 of the decoder.\nOptimization\nThe loss Lpre for pretraining is the sum of a text box seg-\nmentation loss Ldice and a MIM loss Lmim as follows.\nLpre =Ldice + Lmim, (1)\nLdice =1 −\n2 P\ni,j Iseg(i,j) × Sgt(i,j)\nP\ni,j(Iseg(i,j))2 + P\ni,j(Sgt(i,j))2 , (2)\nLmim =||Ψ(Irec, Mmim) − Ψ(Iin, Mmim)||1, (3)\nwhere Sgt ∈ RH×W×1 is the GT text box mask and the\nfunction Ψ fetches the image pixels at masked positions.\nExperiments\nDatasets\nScene Text Removal Datasetsinclude SCUT-EnsText(Liu\net al. 2020) and SCUT-Syn (Zhang et al. 2019b). Specif-\nically, SCUT-EnsText is a real-world dataset containing\n2,749 samples for training and 813 samples for testing.\nSCUT-Syn is a synthetic dataset with 8,000 and 800 sam-\nples for training and testing, respectively.\nPretraining Datasets include the training sets of IC-\nDAR2013 (Karatzas et al. 2013), ICDAR2015 (Karatzas\net al. 2015), MLT2017(Nayef et al. 2017),ArT (Chng et al.\n2019), LSVT (Sun et al. 2019), and ReCTS (Zhang et al.\n2019a), as well as the training and validating sets of Tex-\ntOCR (Singh et al. 2021). After removing the overlapping\nsamples with the test set of SCUT-EnsText (Liu et al. 2020),\nthere are totally 88,340 valid samples for pretraining.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4471\nEncoder Decoder SCUT-EnsText Params↓\n(M)PSNR↑ MSSIM↑ MSE↓\nConv Deconv 35.05 97.20 0.0893 131.45\nConv+TE Deconv 34.85 97.13 0.1043 133.04\nConv+TE TD+Deconv 34.89 97.14 0.1007 139.43\nSwinv2-Tiny Deconv 36.06 97.40 0.0573 65.83\nSwinv2-Tiny TD+Deconv 35.92 97.39 0.0591 71.37\nSwinv2-Tiny MLP 26.18 81.07 0.3532 28.21\nViTEraser-Swinv2-Tiny 36.32 97.48 0.0569 65.39\nTable 1: Comparison of different Transformer-based STR ar-\nchitectures.\nDataset Encoder Decoder SCUT-EnsText\nPSNR↑ MSSIM↑ MSE↓\n× × × 33.34 96.70 0.1854\nImageNet-1k\nCLS × 36.55 97.56 0.0497\nSimMIM × 36.38 97.51 0.0622\nCLS CLS 36.54 97.55 0.0517\nCLS SimMIM 36.45 97.55 0.0508\nScene Text\nDetection\nDataset\nText Seg. × 36.89 97.59 0.0490\nSimMIM × 36.43 97.49 0.0554\nText Seg. 36.78 97.57 0.0487\nSimMIM 36.68 97.58 0.0480\nSegMIM 37.08 97.62 0.0447\nTable 2: Comparison of different pretraining strategies of\nViTEraser-Swinv2-Small. (CLS: Classification)\nImplementation Details\nNetwork Architecture We explore three types of ViT\nblocks, i.e., Pyramid Vision Transformer block (PVT), Swin\nTransformer block (Swin), and Swin Transformer v2 block\n(Swinv2), to implement the proposed ViTEraser. Based on\nthe original scale settings of these ViTs (Wang et al. 2021;\nLiu et al. 2021, 2022b), we obtain four scales of PVT-based\nViTEraser (ViTEraser-PVT-Tiny/Small/Medium/Large),\nthree scales of Swin-based ViTEraser (ViTEraser-Swin-\nTiny/Small/Base), and three scales of Swinv2-based ViT-\nEraser (ViTEraser-Swinv2-Tiny/Small/Base). For con-\nciseness, ViTEraser refers to the Swinv2-based ViT-\nEraser by default.\nPretraining The input image is resized to512×512. Ran-\ndom masking is performed on the input image with a ra-\ntio of 0.6 and a patch size of 32. Besides, a mask token is\nadded to the encoder to represent the masked patches. Using\n4 NVIDIA A6000 GPUs with 48GB memory, the network\nis pretrained for 100 epochs with an AdamW optimizer, a\nbatch size of 64, and learning rates of 0.0001 before the 80th\nepoch and 0.00001 afterward. Because the mask token can\nnegatively affect the encoder, the encoder will be finetuned\nsolely with the text box segmentation task after end-to-end\npretraining, following the training strategy of SimMIM. The\nfinetuning lasts for 20 epochs with an initial learning rate of\n(a) Input\n (b) Masked\n (c) Reconstruct\n (d) Pred. Mask\n (e) GT Mask\nFigure 5: Visualizations of SegMIM. (Pred.: Predicted)\nArchitecture Scale SCUT-EnsText Params↓\n(M)PSNR↑ MSSIM↑ MSE↓\nViTEraser\n(PVT)\nTiny 34.48 96.98 0.1251 37.85\nSmall 35.03 97.15 0.1019 60.36\nMedium 35.09 97.16 0.1089 99.80\nLarge 34.91 97.08 0.1183 134.13\nViTEraser\n(Swin)\nTiny 35.95 97.41 0.0647 65.26\nSmall 35.81 97.44 0.0589 107.90\nBase 36.17 97.47 0.0637 191.66\nViTEraser\n(Swinv2)\nTiny 36.32 97.48 0.0569 65.39\nSmall 36.55 97.56 0.0497 108.15\nBase 36.32 97.51 0.0565 191.97\nTable 3: Comparison of different scales of ViTEraser.\n0.00125 and a cosine decay learning rate schedule.\nTraining The training procedure on SCUT-EnsText or\nSCUT-Syn uses only its corresponding training set. The in-\nput size of the images is set to 512 × 512. The network is\ntrained with an AdamW optimizer for 300 epochs using 2\nNVIDIA A6000 GPUs with 48GB memory. The learning\nrate is initialized as 0.0005 and linearly decayed to 0.00001\nat the last epoch. The training batch size is set to 16.\nEvaluation Metrics\nFollowing previous studies (Liu et al. 2020, 2022a), the\nimage-eval metrics include PSNR, MSSIM, MSE, AGE,\npEPs, pCEPs, and FID, while the detection-eval metrics in-\nvolve the precision (P), recall (R), and f-measure (F) using\nthe pretrained CRAFT (Baek et al. 2019) for text detection.\nExperiments on Architecture\nWhich architecture is the best for the integration of Trans-\nformer into STR models?To answer this question, we first\nintroduce the encoders and decoders compared in Tab. 1.\nEncoder (1)Conv represents a ResNet50 (He et al. 2016).\n(2) Conv+TE indicates the concatenation of a ResNet50\nand a 6-layer Transformer encoder with 256 channels. (3)\nSwinv2-Tiny is the tiny version of Swin Transformer v2 (Liu\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4472\nMethod Image-Eval Detection-Eval↓ Params↓\n(M)\nSpeed↑\n(fps)PSNR↑ MSSIM↑ MSE↓ AGE↓ pEPs↓ pCEPs↓ FID↓ R P F\nOriginal - - - - - - - 69.5 79.4 74.1 - -\nPix2pix (Isola et al. 2017) 26.70 88.56 0.37 6.09 0.0480 0.0227 46.88 35.4 69.7 47.0 54.42 133\nSTE (Nakamura et al. 2017) 25.47 90.14 0.47 6.01 0.0533 0.0296 43.39 5.9 40.9 10.2 - -\nEnsNet (Zhang et al. 2019b) 29.54 92.74 0.24 4.16 0.0307 0.0136 32.71 32.8 68.7 44.4 12.40 199\nMTRNet++ (Tursun et al. 2020) 29.63 93.71 0.28 3.51 0.0305 0.0168 35.68 15.1 63.8 24.4 18.67 53\nEraseNet (Liu et al. 2020) 32.30 95.42 0.15 3.02 0.0160 0.0090 19.27 4.6 53.2 8.5 17.82 71\nSSTE (Tang et al. 2021) 35.34 96.24 0.09 - - - - 3.6 - - 30.75 7.8\nPSSTRNet (Lyu and Zhu 2022) 34.65 96.75 0.14 1.72 0.0135 0.0074 - 5.1 47.7 9.3 4.88 56\nCTRNet (Liu et al. 2022a) 35.20 97.36 0.09 2.20 0.0106 0.0068 13.99 1.4 38.4 2.7 159.81 5.1\nGaRNet§ (Lee and Choi 2022) 35.45 97.14 0.08 1.90 0.0105 0.0062 15.50 1.6 42.0 3.0 33.18 22\nMBE (Hou, Chen, and Wang 2022) 35.03 97.31 - 2.06 0.0128 0.0088 - - - - - -\nPEN (Du et al. 2023b) 35.21 96.32 0.08 2.14 0.0097 0.0037 - 2.6 33.5 4.8 - -\nPEN* (Du et al. 2023b) 35.72 96.68 0.05 1.95 0.0071 0.0020 - 2.1 26.2 3.9 - -\nPERT (Wang et al. 2023) 33.62 97.00 0.13 2.19 0.0135 0.0088 - 4.1 50.5 7.6 14.00 24\nSAEN (Du et al. 2023a) 34.75 96.53 0.07 1.98 0.0125 0.0073 - - - - 19.79 62\nFETNet (Lyu et al. 2023) 34.53 97.01 0.13 1.75 0.0137 0.0080 - 5.8 51.3 10.5 8.53\n77\nViTEraser-Tiny 36.32 97.48 0.0569 1.81 0.0073 0.0040 11.77 0.717 32.7 1.403 65.39 24\nViTEraser-Tiny + SegMIM 36.80 97.55 0.0491 1.79 0.0067 0.0036 10.79 0.430 27.3 0.847 65.39 24\nViTEraser-Small 36.55 97.56 0.0497 1.73 0.0072 0.0039 11.46 0.778 42.2 1.528 108.15 17\nViTEraser-Small + SegMIM 37.08 97.62 0.0447 1.69 0.0064 0.0034 10.16 0.430 30.9 0.848 108.15 17\nViTEraser-Base 36.32 97.51 0.0565 1.86 0.0074 0.0041 11.68 0.635 37.8 1.248 191.97 15\nViTEraser-Base + SegMIM 37.11 97.61 0.0474 1.70 0.0066 0.0035 10.15 0.389 29.7 0.768 191.97 15\nTable 4: Comparison with state of the arts on SCUT-EnsText. (Bold: state of the art, underline: the second best)\net al. 2022b). The ResNet50 and Swinv2-Tiny are pretrained\nusing ImageNet-1k (Deng et al. 2009).\nDecoder (1) Deconv decoder hierarchically upsam-\nples a 16 × 16 feature map with 2048 channels\nto sizes of {32, 64, 128, 256, 512} and channels of\n{1024, 512, 256, 64, 64} through five deconvolution lay-\ners. (2) TD+Deconv decoder adds a 6-layer Transformer\ndecoder with 256 channels before a Deconv decoder. With\n256 learnable queries, the Transformer decoder produces\na hidden feature of 256 tokens and 256 channels which is\nsubsequently resized to a 16 × 16 × 256 feature map. This\nfeature map then undergoes a 1 × 1 convolution with 2048\nchannels before being processed by the Deconv decoder.\n(3) MLP follows the decoder of SegFormer (Xie et al.\n2021). The multi-scale features produced by the encoder are\ntransformed to 256 channels, then interpolated to a size of\n512 × 512, and finally fused via a 1 × 1 convolution.\nBased on the results in Tab. 1, the discussions are as fol-\nlows. (1) Inserting Transformer into CNNs can not effec-\ntively improve the STR results (2nd& 3rd rows v.s. 1st row).\nThe Transformer encoder only performs global attention on\nhigh-level features produced by CNN, omitting fine-grained\ncorrelations such as detailed textures. Moreover, the learn-\nable queries adopted in the Transformer decoder may cause\nspatial misalignment. (2) Pure ViT-based encoder (4th to\n6th rows) makes a significant improvement. The window-\nbased Swinv2-Tiny can effectively capture local and global\ndependencies at both low- and high-level feature spaces. (3)\nThe ViTEraser, which thoroughly utilizes ViTs in both the\nencoder and decoder, provides the best architecture for ap-\nplying Transformer to STR with a substantial margin.The\nSwinv2 blocks enable the decoder to fill the background\nconsidering both surrounding and long-distance context.\nExperiments on Pretraining\nIn this section, we comprehensively explore pretraining\nschemes for STR based on ViTEraser. The pretraining strate-\ngies for comparison include: (1) When using ImageNet-1k,\nthe encoder can be pretrained with the classification or Sim-\nMIM tasks. Moreover, because the first four stages of the\ndecoder are empirically set to be symmetric to the encoder,\ntheir parameters can also be initialized by the encoder’s pre-\ntrained weights symmetrically. (2) When using scene text\ndetection datasets, the encoder or encoder-decoder can be\npretrained with text box segmentation or SimMIM tasks.\nThe experiment results in Tab. 2 demonstrate that Seg-\nMIM achieves the best performance. Moreover, Fig. 5 illus-\ntrates the visualizations of SegMIM. It can be seen that the\npretrained model can accurately determine text locations and\nrealistically reconstruct masked patches.\nExperiments on Scalability\nWe investigate the scalability of ViTEraser in Tab. 3. It can\nbe seen that as the scale goes up, the performance tends\nto increase in general. However, for ViTEraser-Swinv2 and\nViTEraser-PVT, the performance of the largest scale is in-\nferior to a smaller one. This may be due to the overfitting\ncaused by the dramatically increased parameters and limited\ntraining samples. However, the potential of large models can\nbe stimulated when pretrained with SegMIM (Tab. 4).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4473\n(a) Input\n (b) GT\n (c) MTRNet++\n (d) EraseNet\n (e) SSTE\n (f) GaRNet\n (g) CTRNet\n (h) PERT\n (i) ViTEraser\nFigure 6: Qualitative comparison of existing methods and ViTEraser-Small (w/ SegMIM) on SCUT-EnsText.\nComparison with State of the Arts\nSCUT-EnsText In Tab. 4, we compare ViTEraser with ex-\nisting approaches on SCUT-EnsText. For a fair comparison,\ninstead of using GT text box masks, MTRNet++ (Tursun\net al. 2020) uses empty coarse masks and GaRNet (Lee and\nChoi 2022) uses the text box masks produced by pretrained\nCRAFT (Baek et al. 2019). All inference speeds are tested\nusing an RTX3090 GPU with a batch size of 1, consider-\ning the time consumption of the model forward and post-\nprocessing. As for the model size, we calculate the number\nof minimum required parameters during inference. Besides,\nthe parameters and time cost of external text detectors are\nconsidered for SSTE (Tang et al. 2021), GaRNet, and CTR-\nNet (Liu et al. 2022a).\nThe quantitative results in Tab. 4 demonstrate the state-\nof-the-art performance of ViTEraser on real-world STR. For\nimage-eval metrics, a substantial improvement can be ob-\nserved over previous methods, e.g., boosting PSNR from\n35.72 dB to 37.11 dB. For detection-eval metrics, the recall\nand f-measure reach a milestone of lower than 1%, indicat-\ning nearly all the texts have been effectively erased. Espe-\ncially for ViTEraser-Base with SegMIM, remarkably low re-\ncall (0.389%) and f-measure (0.768%) have been achieved.\nMoreover, SegMIM significantly boosts all three scales of\nViTEraser, improving PSNRs of ViTEraser-Tiny, Small, and\nBase by 0.48, 0.53, and 0.79 dB, respectively.\nThe visualizations on SCUT-EnsText are shown in Fig. 6,\nqualitatively demonstrating the effectiveness of ViTEraser.\nSCUT-Syn The quantitative and qualitative comparisons\non synthetic SCUT-Syn are presented in Tab. 5 and Fig. 7,\nrespectively. It can be observed that ViTEraser outperforms\nexisting methods except for the MBE (Hou, Chen, and Wang\n2022) that ensembles multiple STR networks.\n(a) Input\n (b) GT\n (c) EraseNet\n (d) SSTE\n (e) V\niTEraser\nFigure 7: Qualitative comparison of previous methods and\nViTEraser-Base (w/ SegMIM) on SCUT-Syn.\n(a) Input\n (b) GT\n (c) Real\n (d) Tampered\n (e) Text\nFigure 8: Visualization results on Tampered-IC13.\nExtension to Tampered Scene Text Detection\nTo verify the generalization ability of ViTEraser, we ex-\ntend it to the tampered scene text detection (TSTD) task\n(Wang et al. 2022b) that aims to localize both tampered and\nreal texts from natural scenes. Using Tampered-IC13 dataset\n(Wang et al. 2022b), we train a ViTEraser-Tiny (w/o Seg-\nMIM) whose three-channel outputs correspond to the box-\nlevel segmentation of real texts, tampered texts, and both of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4474\nMethod PSNR↑ MSSIM↑ MSE↓ AGE↓ pEPs↓ pCEPs↓\nPix2pix (Isola et al. 2017) 26.76 91.08 0.27 5.47 0.0473 0.0244\nSTE (Nakamura et al. 2017) 25.40 90.12 0.65 9.49 0.0553 0.0347\nEnsNet (Zhang et al. 2019b) 37.36 96.44 0.21 1.73 0.0069 0.0020\nMTRNet++ (Tursun et al. 2020) 34.55 98.45 0.04 - - -\nEraseNet (Liu et al. 2020) 38.32 97.67 0.02 1.60 0.0048 0.0004\nZdenek and Nakayama (2020) 37.46 93.64 - - - -\nConrad and Chen (2021) 32.97 94.90 - - - -\nSSTE (Tang et al. 2021) 38.60 97.55 0.02 - - -\nPSSTRNet (Lyu and Zhu 2022) 39.25 98.15 0.02 1.20 0.0043 0.0008\nCTRNet (Liu et al. 2022a) 41.28 98.52 0.02 1.33 0.0030 0.0007\nMBE (Hou, Chen, and Wang 2022) 43.85 98.64 - 0.94 0.0013 0.00004\nPEN (Du et al. 2023b) 39.26 98.03 0.02 1.29 0.0038 0.0004\nPEN* (Du et al. 2023b) 38.87 97.83 0.03 1.38 0.0041 0.0004\nPERT (Wang et al. 2023) 39.40 97.87 0.02 1.41 0.0046 0.0007\nSEAN (Du et al. 2023a) 38.63 98.27 0.03 1.39 0.0043 0.0004\nFETNet (Lyu et al. 2023) 39.14 97.97 0.02 1.26 0.0046 0.0008\nViTEraser-Tiny 42.24 98.42 0.0112 1.23 0.0021 0.000020\nViTEraser-Tiny + SegMIM 42.40 98.44 0.0106 1.17 0.0018 0.000015\nViTEraser-Small 42.45 98.43 0.0109 1.19 0.0020 0.000019\nViTEraser-Small + SegMIM 42.66 98.49 0.0099\n1.13 0.0016 0.000012\nViTEraser-Base 42.53 98.45 0.0102 1.19 0.0018 0.000016\nViTEraser-Base + SegMIM 42.97 98.55 0.0092 1.11 0.0015 0.000011\nTable 5: Comparison with state of the arts on SCUT-Syn.\nMethod Tampered Text Real Text mF↑\nR↑ P↑ F↑ R↑ P↑ F↑\nS3R (Wang et al. 2022b) + EAST 69.97 70.23 69.94 27.32 50.46 35.45 52.70\nViTEraser-Tiny + EAST 77.87 79.66 78.76 32.45 65.23 43.34 61.05\nS3R (Wang et al. 2022b) + PSENet 79.43 79.92 79.67 41.89 61.56 49.85 64.76\nViTEraser-Tiny + PSENet 82.38 83.23 82.80 39.70 64.96 49.28 66.04\nS3R (Wang et al. 2022b) + ContourNet 91.45 86.68 88.99 54.80 77.88 64.33 76.66\nViTEraser-Tiny + ContourNet 92.62 85.77 89.06 56.84 75.82 64.97 77.02\nTable 6: Comparison with existing methods on Tampered-IC13. (mF: Average f-measure of real and tampered texts)\nthem, respectively. The dice losses on these three segmenta-\ntion maps are utilized to optimize the network. Furthermore,\nto calculate the evaluation metrics including recall (R), pre-\ncision (P), and f-measure (F) of tampered and real texts,\nwe incorporate an EAST (Zhou et al. 2017), PSENet (Wang\net al. 2019), or ContourNet (Wang et al. 2020) trained with\nTampered-IC13 to produce text bounding boxes. Specifi-\ncally, a bounding box will be regarded as tampered if more\nthan 50% pixels within it are classified as tampered by ViT-\nEraser. Similarly, the bounding boxes of real texts can also\nbe determined. The quantitative performance is presented in\nTab. 6 and the visualizations are shown in Fig. 8. It can be\nseen that ViTEraser can achieve state-of-the-art performance\non Tampered-IC13, showing strong generalization potential.\nConclusion\nIn this paper, we propose a novel simple-yet-effective one-\nstage ViT-based approach for STR, termed ViTEraser. ViT-\nEraser employs a concise encoder-decoder paradigm, elimi-\nnating the need for text localizing modules, external text de-\ntectors, and progressive refinements. Moreover, ViTEraser\npioneers in thoroughly utilizing ViTs in place of CNNs\nin both the encoder and decoder, significantly enhancing\nthe long-range modeling ability. Furthermore, we propose\na novel pretraining scheme, called SegMIM, which focuses\nthe encoder and decoder on the text box segmentation and\nMIM tasks, respectively. Without bells and whistles, the pro-\nposed method substantially outperforms previous STR ap-\nproaches. ViTEraser also exhibits outstanding performance\nin tampered scene text detection, exhibiting strong general-\nization potential. Additionally, we comprehensively explore\nthe architecture, pretraining, and scalability of ViT-based\nencoder-decoder for STR. We believe this study can inspire\nmore research on ViT-based STR and contribute to the de-\nvelopment of the unified model for pixel-level OCR tasks.\nAcknowledgements\nThis research is supported in part by NSFC (Grant No.:\n61936003), National Key Research and Development Pro-\ngram of China (2022YFC3301703), and INTSIG-SCUT\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4475\nJoint Lab Fundation (CG-0274-200703).\nReferences\nBaek, Y .; Lee, B.; Han, D.; Yun, S.; and Lee, H. 2019. Char-\nacter region awareness for text detection. In Proc. CVPR,\n9365–9374.\nBian, X.; Wang, C.; Quan, W.; Ye, J.; Zhang, X.; and Yan,\nD.-M. 2022. Scene text removal via cascaded text stroke\ndetection and erasing. Comput. Vis. Media, 8: 273–287.\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2022. Swin-Unet: Unet-like pure Transformer\nfor medical image segmentation. In Proc. ECCV Workshop,\n205–218.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith Transformers. In Proc. ECCV, 213–229.\nChng, C. K.; Liu, Y .; Sun, Y .; Ng, C. C.; Luo, C.; Ni, Z.;\nFang, C.; Zhang, S.; Han, J.; Ding, E.; et al. 2019. IC-\nDAR2019 robust reading challenge on arbitrary-shaped text-\nRRC-ArT. In Proc. ICDAR, 1571–1576.\nConrad, B.; and Chen, P.-I. 2021. Two-stage seamless text\nerasing on real-world scene images. In Proc. ICIP, 1309–\n1313.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In Proc. CVPR, 248–255.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. InProc.\nICLR, 1–21.\nDu, X.; Zhou, Z.; Zheng, Y .; Ma, T.; Wu, X.; and Jin, C.\n2023a. Modeling stroke mask for end-to-end text erasing.\nIn Proc. WACV, 6151–6159.\nDu, X.; Zhou, Z.; Zheng, Y .; Wu, X.; Ma, T.; and Jin, C.\n2023b. Progressive scene text erasing with self-supervision.\nComput. Vis. Image Underst., 233: 103712.\nHan, K.; Wang, Y .; Chen, H.; Chen, X.; Guo, J.; Liu, Z.;\nTang, Y .; Xiao, A.; Xu, C.; Xu, Y .; et al. 2022. A survey\non vision Transformer. IEEE Trans. Pattern Anal. Mach.\nIntell., 45(1): 87–110.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll´ar, P.; and Girshick, R.\n2022. Masked autoencoders are scalable vision learners. In\nProc. CVPR, 16000–16009.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proc. CVPR, 770–778.\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision Transformers.\nIn Proc. ICCV, 11936–11945.\nHou, Y .; Chen, J. J.; and Wang, Z. 2022. Multi-branch net-\nwork with ensemble learning for text removal in the wild. In\nProc. ACCV, 1333–1349.\nInai, K.; P ˚alsson, M.; Frinken, V .; Feng, Y .; and Uchida, S.\n2014. Selective concealment of characters for privacy pro-\ntection. In Proc. ICPR, 333–338.\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\nto-image translation with conditional adversarial networks.\nIn Proc. CVPR, 1125–1134.\nJiang, G.; Wang, S.; Ge, T.; Jiang, Y .; Wei, Y .; and Lian, D.\n2022. Self-supervised text erasing with controllable image\nsynthesis. In Proc. ACM MM, 1973–1983.\nKaratzas, D.; Gomez-Bigorda, L.; Nicolaou, A.; Ghosh, S.;\nBagdanov, A.; Iwamura, M.; Matas, J.; Neumann, L.; Chan-\ndrasekhar, V . R.; Lu, S.; et al. 2015. ICDAR 2015 competi-\ntion on robust reading. In Proc. ICDAR, 1156–1160.\nKaratzas, D.; Shafait, F.; Uchida, S.; Iwamura, M.; i Big-\norda, L. G.; Mestre, S. R.; Mas, J.; Mota, D. F.; Almazan,\nJ. A.; and De Las Heras, L. P. 2013. ICDAR 2013 robust\nreading competition. In Proc. ICDAR, 1484–1493.\nKenton, J. D. M.-W. C.; and Toutanova, L. K. 2019. BERT:\nPre-training of deep bidirectional Transformers for language\nunderstanding. In Proc. NAACL-HLT, 4171–4186.\nKeserwani, P.; and Roy, P. P. 2021. Text region condi-\ntional generative adversarial network for text concealment in\nthe wild. IEEE Trans. Circuits Syst. Video Technol., 32(5):\n3152–3163.\nLee, H.; and Choi, C. 2022. The surprisingly straightforward\nscene text removal method with gated attention and region\nof interest generation: A comprehensive prominent model\nanalysis. In Proc. ECCV, 457–472.\nLiu, C.; Jin, L.; Liu, Y .; Luo, C.; Chen, B.; Guo, F.; and Ding,\nK. 2022a. Don’t forget me: Accurate background recovery\nfor text removal via modeling local-global context. In Proc.\nECCV, 409–426.\nLiu, C.; Liu, Y .; Jin, L.; Zhang, S.; Luo, C.; and Wang, Y .\n2020. EraseNet: End-to-end text removal in the wild. IEEE\nTrans. Image Process., 29: 8760–8775.\nLiu, Y .; Zhang, J.; Peng, D.; Huang, M.; Wang, X.; Tang,\nJ.; Huang, C.; Lin, D.; Shen, C.; Bai, X.; and Jin, L. 2023.\nSPTS v2: Single-Point Scene Text Spotting. IEEE Trans.\nPattern Anal. Mach. Intell., 45(12): 15665–15679.\nLiu, Z.; Hu, H.; Lin, Y .; Yao, Z.; Xie, Z.; Wei, Y .; Ning, J.;\nCao, Y .; Zhang, Z.; Dong, L.; et al. 2022b. Swin Trans-\nformer v2: Scaling up capacity and resolution. In Proc.\nCVPR, 12009–12019.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical vision\nTransformer using shifted windows. In Proc. ICCV, 10012–\n10022.\nLyu, G.; Liu, K.; Zhu, A.; Uchida, S.; and Iwana, B. K. 2023.\nFETNet: Feature erasing and transferring network for scene\ntext removal. Pattern Recognit., 109531.\nLyu, G.; and Zhu, A. 2022. PSSTRNet: Progressive\nsegmentation-guided scene text removal network. In Proc.\nICME, 1–6.\nMirza, M.; and Osindero, S. 2014. Conditional generative\nadversarial nets. arXiv preprint arXiv:1411.1784.\nNakamura, T.; Zhu, A.; Yanai, K.; and Uchida, S. 2017.\nScene text eraser. In Proc. ICDAR, 832–837.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4476\nNayef, N.; Yin, F.; Bizid, I.; Choi, H.; Feng, Y .; Karatzas, D.;\nLuo, Z.; Pal, U.; Rigaud, C.; Chazalon, J.; et al. 2017. IC-\nDAR2017 robust reading challenge on multi-lingual scene\ntext detection and script identification-RRC-MLT. In Proc.\nICDAR, volume 1, 1454–1459.\nPan, Z.; Zhuang, B.; Liu, J.; He, H.; and Cai, J. 2021. Scal-\nable vision Transformers with hierarchical pooling. In Proc.\nICCV, 377–386.\nPeng, D.; Wang, X.; Liu, Y .; Zhang, J.; Huang, M.; Lai, S.;\nLi, J.; Zhu, S.; Lin, D.; Shen, C.; et al. 2022. SPTS: Single-\npoint text spotting. In Proc. ACM MM, 4272–4281.\nQin, S.; Wei, J.; and Manduchi, R. 2018. Automatic seman-\ntic content removal by learning to neglect. In Proc. BMVC,\n1–12.\nSingh, A.; Pang, G.; Toh, M.; Huang, J.; Galuba, W.; and\nHassner, T. 2021. TextOCR: Towards large-scale end-to-end\nreasoning for arbitrary-shaped scene text. In Proc. CVPR,\n8802–8812.\nSun, Y .; Ni, Z.; Chng, C.-K.; Liu, Y .; Luo, C.; Ng, C. C.;\nHan, J.; Ding, E.; Liu, J.; Karatzas, D.; et al. 2019. ICDAR\n2019 competition on large-scale street view text with partial\nlabeling-RRC-LSVT. In Proc. ICDAR, 1557–1562.\nTang, Z.; Miyazaki, T.; Sugaya, Y .; and Omachi, S. 2021.\nStroke-based scene text erasing using synthetic data for\ntraining. IEEE Trans. Image Process., 30: 9306–9320.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In Proc. ICML,\n10347–10357.\nTursun, O.; Denman, S.; Sivapalan, S.; Sridharan, S.;\nFookes, C.; and Mau, S. 2019a. Component-based attention\nfor large-scale trademark retrieval.IEEE Trans. Inf. Forensic\nSecur., 17: 2350–2363.\nTursun, O.; Denman, S.; Zeng, R.; Sivapalan, S.; Sridha-\nran, S.; and Fookes, C. 2020. MTRNet++: One-stage mask-\nbased scene text eraser. Comput. Vis. Image Underst., 201:\n103066.\nTursun, O.; Zeng, R.; Denman, S.; Sivapalan, S.; Sridharan,\nS.; and Fookes, C. 2019b. MTRNet: A generic scene text\neraser. In Proc. ICDAR, 39–44.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Proc. NeurIPS, 30.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision Trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In Proc. ICCV, 568–578.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2022a. PVT v2: Improved\nbaselines with pyramid vision Transformer. Comput. Vis.\nMedia, 8(3): 415–424.\nWang, W.; Xie, E.; Li, X.; Hou, W.; Lu, T.; Yu, G.; and Shao,\nS. 2019. Shape robust text detection with progressive scale\nexpansion network. In Proc. CVPR, 9336–9345.\nWang, Y .; Xie, H.; Wang, Z.; Qu, Y .; and Zhang, Y . 2023.\nWhat is the Real Need for Scene Text Removal? Exploring\nthe Background Integrity and Erasure Exhaustivity Proper-\nties. IEEE Trans. Image Process., 32: 4567–4580.\nWang, Y .; Xie, H.; Xing, M.; Wang, J.; Zhu, S.; and Zhang,\nY . 2022b. Detecting tampered scene text in the wild. In\nProc. ECCV, 215–232.\nWang, Y .; Xie, H.; Zha, Z.-J.; Xing, M.; Fu, Z.; and Zhang,\nY . 2020. ContourNet: Taking a further step toward accu-\nrate arbitrary-shaped scene text detection. In Proc. CVPR,\n11753–11762.\nWu, L.; Zhang, C.; Liu, J.; Han, J.; Liu, J.; Ding, E.; and\nBai, X. 2019. Editing text in the wild. In Proc. ACM MM,\n1500–1508.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient de-\nsign for semantic segmentation with Transformers. In Proc.\nNeurIPS, 12077–12090.\nXie, Z.; Zhang, Z.; Cao, Y .; Lin, Y .; Bao, J.; Yao, Z.; Dai,\nQ.; and Hu, H. 2022. SimMIM: A simple framework for\nmasked image modeling. In Proc. CVPR, 9653–9663.\nXu, Y .; Li, M.; Cui, L.; Huang, S.; Wei, F.; and Zhou, M.\n2020. LayoutLM: Pre-training of text and layout for docu-\nment image understanding. In Proc. KDD, 1192–1200.\nYang, M.; Liao, M.; Lu, P.; Wang, J.; Zhu, S.; Luo, H.; Tian,\nQ.; and Bai, X. 2022. Reading and writing: Discrimina-\ntive and generative modeling for self-supervised text recog-\nnition. In Proc. ACM MM, 4214–4223.\nZdenek, J.; and Nakayama, H. 2020. Erasing scene text with\nweak supervision. In Proc. WACV, 2238–2246.\nZhang, R.; Zhou, Y .; Jiang, Q.; Song, Q.; Li, N.; Zhou, K.;\nWang, L.; Wang, D.; Liao, M.; Yang, M.; et al. 2019a. IC-\nDAR 2019 robust reading challenge on reading Chinese text\non signboard. In Proc. ICDAR, 1577–1581.\nZhang, S.; Liu, Y .; Jin, L.; Huang, Y .; and Lai, S. 2019b.\nEnsNet: Ensconce text in the wild. In Proc. AAAI, 801–808.\nZhou, X.; Yao, C.; Wen, H.; Wang, Y .; Zhou, S.; He, W.; and\nLiang, J. 2017. EAST: An efficient and accurate scene text\ndetector. In Proc. CVPR, 5551–5560.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4477"
}