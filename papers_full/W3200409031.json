{
  "title": "Do Long-Range Language Models Actually Use Long-Range Context?",
  "url": "https://openalex.org/W3200409031",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2169183507",
      "name": "Simeng Sun",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Intuit (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2948613238",
      "name": "Kalpesh Krishna",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Intuit (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4209249003",
      "name": "Andrew Mattarella-Micke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068391019",
      "name": "Mohit Iyyer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3035214886",
    "https://openalex.org/W1729310130",
    "https://openalex.org/W4238334289",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W3103878009",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1991145433",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3105478763",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3118895645",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W3166398787",
    "https://openalex.org/W3044284384",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2572743863",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W3174401451",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2335559472",
    "https://openalex.org/W2142269169",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2124741472"
  ],
  "abstract": "Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807–822\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n807\nDo Long-Range Language Models Actually Use Long-Range Context?\nSimeng Sun1 Kalpesh Krishna1 Andrew Mattarella-Micke2 Mohit Iyyer1\nUniversity of Massachusetts Amherst1 Intuit AI2\n{simengsun,kalpesh,miyyer}@cs.umass.edu\nandrew_mattarella-micke@intuit.com\nAbstract\nLanguage models are generally trained on\nshort, truncated input sequences, which limits\ntheir ability to use discourse-level information\npresent in long-range context to improve their\npredictions. Recent efforts to improve the efﬁ-\nciency of self-attention have led to a prolifera-\ntion of long-range Transformer language mod-\nels, which can process much longer sequences\nthan models of the past. However, the ways in\nwhich such models take advantage of the long-\nrange context remain unclear. In this paper, we\nperform a ﬁne-grained analysis of two long-\nrange Transformer language models (includ-\ning the Routing Transformer, which achieves\nstate-of-the-art perplexity on the PG-19 long-\nsequence LM benchmark dataset) that accept\ninput sequences of up to 8K tokens. Our re-\nsults reveal that providing long-range context\n(i.e., beyond the previous 2K tokens) to these\nmodels only improves their predictions on a\nsmall set of tokens (e.g., those that can be\ncopied from the distant context) and does not\nhelp at all for sentence-level prediction tasks.\nFinally, we discover that PG-19 contains a va-\nriety of different document types and domains,\nand that long-range context helps most for lit-\nerary novels (as opposed to textbooks or mag-\nazines).\n1 Introduction\nUnderstanding long documents requires model-\ning various discourse-level phenomena, including\nanaphora (Hobbs, 1979; Grosz et al., 1995), argu-\nment structure (Grimshaw, 1990), narrative scripts\nand trajectories (Schank and Abelson, 1977; Labov\nand Waletzky, 1997), and causal links between\nconcepts (Mooney and DeJong, 1985). Unfortu-\nnately, most language models (LMs) are trained\nto predict the next word given only a small win-\ndow of local context, which prevents them from\nusing long-range discourse structure to improve\ntheir predictions. Many research efforts over the\nyears have attempted to address this issue: for ex-\nample, Rosenfeld (1996) incorporated statistics\nfrom distant tokens to improve n-gram models,\nwhile Ji et al. (2015) added document-level context\nto neural LMs.\nMore recently, the Transformer LM (Vaswani\net al., 2017), which forms the backbone of state-of-\nthe-art NLP systems (Devlin et al., 2019; Brown\net al., 2020), has become the focus of numerous\nefforts to process longer input sequences. Trans-\nformer LMs are constrained by the inefﬁciency of\nthe self-attention mechanism, whose complexity\nscales quadratically with the input sequence length.\nAs such, more efﬁcient methods based on sparse\nattention (Correia et al., 2019) and cached mem-\nory (Rae et al., 2020) have been proposed to in-\ncrease the maximum sequence length, which has\nprogressed from GPT-2’s 1024 tokens (Radford\net al., 2019) to 4096 tokens (Zaheer et al., 2020)\nand ﬁnally 8192 tokens (Roy et al., 2021, Rout-\ning Transformer). When evaluated on the PG-19\nbenchmark dataset (Rae et al., 2020), which con-\ntains long documents in the public domain, these\n“long-range” Transformer LMs also reach lower\nperplexities than baseline models on held-out data.\nHow do “long-range” Transformer LMs make\nuse of the long-range context? Do they actually en-\ncode important discourse information to improve\ntheir predictions? In this paper, we conduct a se-\nries of ﬁne-grained analysis experiments to answer\nthese questions, several of which are inspired by the\ncontext analysis of LSTM LMs conducted by Khan-\ndelwal et al. (2018). We focus speciﬁcally on ana-\nlyzing the behavior of the state-of-the-art Routing\nTransformer and a simpler baseline model in the\npresence of various perturbations (e.g., word shuf-\nﬂing, random document replacement), and look\nclosely at how different types of tokens are affected.\nOur results show that:\n• Providing long-range context (i.e., further\nthan 2K tokens away) to these models hasneg-\n808\nligible impact on the perplexity of tokens near\nthe end of a sequence in aggregate. However,\na ﬁne-grained analysis reveals that it does help\na small set of tokens (tokens within subword\nclusters and those that can only be copied\nfrom the distant context), as well as partic-\nular types of books (ﬁctional and continuous).\n• Despite the aforementioned improvements on\na fraction of tokens, signiﬁcantly perturbing\nthe long-term context with word shufﬂing and\nrandom replacement has no notable impact on\nperplexity overall, suggesting that the evalu-\nated models encode long-range context super-\nﬁcially at best.\n• Long-range context is not used for sequence-\nlevel prediction tasks that move outside the\nteacher-forced setting of the previous experi-\nments.\nWhile modern LMs can process much longer in-\nput sequences than those of the past, we conclude\nthat they do not exploit the information available in\nthe long-range context. We recommend that future\nresearch on long-range LMs includes analysis ex-\nperiments such as those in our work to shed light on\nhow and when they are using the distant context.\n2 Background & Setup\nIn this section, we ﬁrst provide an overview of\nthe long-range language models analyzed in this\nwork (Local Transformer, Routing Transformer).\nNext, we describe the experimental setup used in\nthe remainder of the paper to measure the impact\nof the long-term context.\n2.1 Long-range Language Modeling\nGiven preceding context tokens w<i (the preﬁx),\na language model (LM) computes the probability\ndistribution of the next token p(wi |w<i). LMs\nare commonly evaluated using perplexity, which\nis the exponentiated negative log likelihood of a\nheld-out corpus:\nppl= exp\n(\n− 1\nN\nN∑\ni=1\nlog p(wi |w<i)\n)\nModern LMs are most often implemented with\nTransformers (Vaswani et al., 2017), which com-\npute vector representations for every token in the\npreﬁx at multiple layers and combine them together\nusing self-attention. Since self-attention requires\nscoring every token in the sequence against every\nother token, it scales quadratically in both compute\ntime and memory usage, which limits its applica-\ntion to very long sequences.\nLocal Transformer A simple way to improve\nthe efﬁciency of self-attention blocks is to con-\nstrain the attention at each layer to a local window\nof the previousktokens. Such Transformers, which\nwe refer to as Local Transformers, can be feasibly\nscaled up to large input sequence lengths. The re-\nceptive ﬁeld of a Local Transformer scales linearly\nwith the number of layers, as the lth layer of this\nmodel can access the previous k×ltokens (Luong\net al., 2015; Child et al., 2019; Sukhbaatar et al.,\n2019).\nRouting Transformer The Routing Trans-\nformer (Roy et al., 2021, RT) takes a more\nintelligent approach to scaling self-attention.\nSpeciﬁcally, the RT assigns keys and queries in\nself-attention to kclusters, the centroids of which\nare learned during training. A routing attention\nstrategy computes attention A only over the\nqueries Qi and keys Kj that belong to the same\ncluster µ(Qi) (i.e., those whose centroid is closest\nto query Qi).\nXi =\n∑\nj:Kj ∈µ(Qi),\nj<i\nAijVj\nIn contrast to the position-based local attention,\nthis clustering-based attention takes the content of\nthe token representations into account. This sparse\nself-attention mechanism reduces the complexity\nfrom O(N2) to O(N1.5) and has led to state-of-\nthe-art results on tasks such as long-form question\nanswering (Krishna et al., 2021).\n2.2 Experimental setup\nWhile the previously-described models can be\ntrained with longer inputs than standard Transform-\ners, it remains unclear how they make use of the\nadditional context tokens to improve their predic-\ntions of the next word. To shed light on the behav-\nior of long-range Transformer LMs, we perform\na series of experiments in which we manipulate\nthe input sequence (both length and content). For\ntoken-level experiments (§ 3, § 4), we only evaluate\nthe perplexity of k tokens near the end1 of an N\n1An artifact exhibited by RT causes tokens at the very end\nof a sequence to have much higher losses than the others; this\n809\ntoken-long input sequence ( k ≪N) to focus on\nthe effect of long-range context.2\nDataset: We conduct all of our analyses on the\nvalidation set of the PG-19 dataset (Rae et al.,\n2020). This dataset contains ∼29K books from\nProject Gutenberg repository published before\n1919 and was constructed speciﬁcally to evaluate\nlong-range LMs (average document length ∼69K\ntokens). The validation set contains 50 books3 and\n∼3 million tokens in total. Evaluating every token\nin the validation set with large preﬁx sizes (e.g. 8K\ntokens) is computationally infeasible.4 Thus, we\nset the number of target tokens per context k= 10\nand sample a subset of 220K validation tokens for\nour experiments, which is the same data size used\nin the LM analysis experiments of Khandelwal et al.\n(2018). Evaluating RT in this way yields slightly\nbetter perplexity on the validation set of PG-19\nthan using the evaluation setting in the original RT\npaper (35.2 vs. 36.3). We ensure that the number\nof tokens sampled from each validation book is\nproportional to the length of that book.\nModels: As training long-range Transformer\nLMs is also infeasible without immense compu-\ntational resources, we use publicly-available pre-\ntrained checkpoints for all of our experiments. The\nRouting Transformer (RT) checkpoint contains\n490M parameters and processes sequences up to\n8192 tokens long, achieving 33.2 perplexity on the\nPG-19 test set. The released checkpoint, which was\ntrained on 128 TPUv3 cores for several weeks, has\na subword vocabulary of ∼98K types,5 along with\n8 attention heads in each of its 22 layers. The top\nphenomena does not exist with LT. After correspondence with\nthe RT authors, we decide to exclude the very last 40 tokens\n(whose losses are affected) from all of our evaluations. More\ndetails about this artifact can be found in the Appendix.\n2Language models are normally evaluated on non-\noverlapping sequences, with the begin and end sequence to-\nkens receiving different amount of context. In our setting, all\ntarget tokens have roughly the same amount of context.\n3We observe a signiﬁcant gap of ∼10 perplexity between\nthe PG-19 test and validation sets and discover that this gap\nis largely due to the presence of an annotated edition of The\nCanterbury Tales and Other Poems. This book intertwines\nline-by-line annotations with the main text, which causes the\npreprocessed version in the dataset to be unreadable. We\nremove this book in all of our experiments, which decreases\nthe test/validation perplexity gap to ∼3.\n4On one RTX8000 GPU, it takes around 104h to evaluate\nthe entire PG-19 validation set with sequence length 8K and\ntarget sequence 10.\n5We follow the RT paper (Roy et al., 2021) by scaling\nthe loss by 1.248 before computing the perplexity in order to\nmatch the word-level perplexity reported by Rae et al. (2020).\n2000 4000 6000 8000\nprefix length\n36\n38\n40perplexity\nAll target tokens\nRT\nLT\nFigure 1: The perplexity of all target tokens plateaus\nafter 2K preﬁx tokens for both Routing Transformer\n(RT) and Local Transformer ( LT), showing the negli-\ngible overall impact of the long-range context.\ntwo RT layers include content-based clustering at-\ntention while the remaining are composed entirely\nof local attention.\nOur Local Transformer (LT) is derived from\nthe same checkpoint as RT (and thus has identi-\ncal model size), except that all clustering heads\nare replaced with local attention heads. It achieves\nslightly better perplexity on the PG-19 test set (38.3\nvs. 39.3) compared to the LT model trained from\nscratch by Roy et al. (2021), possibly because the\nlocal attention heads learn a better representation of\nthe weight matrices by using the information from\nthe clustering heads.67 The attention heads in this\nmodel attend to the previous 256 tokens, which re-\nsults in an effective receptive ﬁeld of∼5K tokens.8\nWhile we would have also liked to analyze other\nlong-range LMs such as the Compressive Trans-\nformer (Rae et al., 2020) and Longformer (Beltagy\net al., 2020), these models do not have publicly-\navailable PG-19 checkpoints; additionally, they dif-\nfer from RT and LT in model size, which makes it\nhard to perform controlled experiments.\n3 The effect of longer context\nHow does the size of the preﬁx affect the perplexity\nof long-range Transformer LMs? In this section,\nwe evaluate our RT and LT checkpoints on the\n6The original fully trained LT checkpoint was not made\npublicly available before the EMNLP submission deadline.\n7The RT authors released a new LT checkpoint during\nthe review period of this paper. We also evaluate this\nnewly-released LT checkpoint and include the results in the\nAppendix F. Both the RT and the LT checkpoints can be found\nat https://github.com/google-research/\ngoogle-research/tree/master/routing_\ntransformer\n8A preliminary experiment veriﬁed that the clustering\nheads in the RT do attend to the long-range context, beyond\n5K tokens, demonstrating that it is at least theoretically incor-\nporating more context than the LT.\n810\n2000\n2200\n2400\nInfrequent tokens\n2500 5000 7500\nprefix length\n1200\n1400\n2500 5000 7500\nprefix length\n25\n26\n27\n28perplexity\nFrequent tokens\nRT\nLT\nFigure 2: RT perplexity of infrequent tokens continues\nto decrease until preﬁx length is 5K.\nPG-19 validation set with varied preﬁx length. We\ndiscover that although these models are theoreti-\ncally able to encode long sequences, increasing the\npreﬁx length beyond 2K tokens does not bring dis-\ncernible improvements in aggregate. However, we\ndo identify small subsets of tokens that beneﬁt from\nlong-range context. Additionally, we ﬁnd that these\nmodels take advantage of long-range context to dif-\nferent degrees on different types of books (e.g.,\ncontinuous ﬁctional narratives vs. discontinuous\nmagazine articles).\nValidation perplexity does not improve when\nthe preﬁx length grows beyond 2K tokens: As\nshown in Figure 1, RT perplexity plateaus when\nevaluated with preﬁxes longer than 2K. 9 In con-\ntrast, relative to RT, the perplexity curve for the\nmore primitive LT starts ﬂattening earlier at around\n1K tokens (note that its effective context size is\nonly 5K). We conclude that while RT’s clustering\nheads take better advantage of global context than\nLT, the long-range context beyond 2K tokens is not\nhelping overall. Surprisingly, the perplexity gap be-\ntween RT and LT is relatively consistent regardless\nof the preﬁx length, which indicates that much of\nRT’s gains do not come from its increased ability to\nleverage long-range context but rather from better\nmodeling of local context.\nInfrequent tokens can beneﬁt from increased\npreﬁx length: While the overall perplexity does\nnot improve with increasing preﬁx length, we do\nobserve different behavior when ﬁltering the target\ntokens by frequency, as shown in Figure 2. We\ndeﬁne frequent tokens to be the top 10% more\nfrequently-occurring tokens in the subword vocab-\nulary of PG-19 while the rest of tokens are clas-\n9As a point of comparison, the perplexity of the much\nsmaller LSTM language models evaluated by Khandelwal\net al. (2018) plateaus after 200 words. Additionally, Press\net al. (2020) discover that the perplexity ﬂattens after 1K for a\nsmaller standard Transformer LM.\n18000\n20000\n22000\nFirst subword token\n2500 5000 7500\nprefix length\n7000\n8000\n9000\n2500 5000 7500\nprefix length\n3.5\n4.0\n4.5\n5.0\n5.5\nRest of subword tokens\nRT\nLT\nPerplexity\nFigure 3: RT perplexity of tokens inside subword clus-\nters continues to decrease until a preﬁx length of 5K.\n2500 5000 7500\nprefix length\n170\n180\n190\n200perplexity\nOccurs >2K tokens away\n2500 5000 7500\nprefix length\n800\n900\n1000\nNot in prefix\nRT\nLT\nFigure 4: RT perplexity of target tokens whose last ap-\npearance is more than 2K tokens away in the preﬁx\nkeeps decreasing.\nsiﬁed as infrequent.10 While adding long-range\ncontext does not improve either model’s predic-\ntions of frequent tokens, the RT’s perplexity of\ninfrequent tokens decreases from ∼1200 with a\npreﬁx length of 2K to 1180 with preﬁx length of\n5K. However, we do observe that infrequent to-\nken perplexity increases back to 1200 as the input\nis further extended, suggesting that the additional\ncontext perhaps confounds the model. Meanwhile,\nthe LT is signiﬁcantly worse than RT on infrequent\ntoken prediction, and its perplexity increases as the\npreﬁx length is increased.11\nTokens inside a subword cluster beneﬁt from\nlonger contexts: One issue with the previous ex-\nperiment is that the frequency categorization was\ncomputed at a subword level and so may not ex-\nactly correspond to word frequency, especially for\ninfrequent words (e.g., entities) that are split into\nmultiple subwords. We therefore do a follow-up\nexperiment by isolating all words that are split into\nmultiple subwords, and then examining perplex-\nity of these tokens as a function of their position\nin the subword cluster. For example, the word\n10Around 20K tokens in our target token set are classiﬁed\nas infrequent, which amounts to 9% of all target tokens.\n11This is likely an artifact due to the elimination of routing\nattention heads from the RT checkpoint, since the LT trained\nfrom scratch does not exhibit such behavior. More details are\nincluded in Appendix F.\n811\n2500 5000 7500\nprefix length\n34\n35\n36\n37\n38perplexity\nfic. & non-fic.\nfic\nnon-fic\n2500 5000 7500\nprefix length\n35\n36\n37\ncont. & discont.\ncont.\ndiscont.\nFigure 5: RT takes better advantage of context beyond\n2K tokens for ﬁctional and continuous books than non-\nﬁctional and discontinuous books, respectively.\n“Trocadero” is separated into three subword tokens\n“Tro”, “cade”, and “ro”. We speciﬁcally distinguish\nbetween the ﬁrst subword in the cluster (“Tro”)\nfrom the rest of the subwords (“cade” and “ro”) in\nthe plots shown in Figure 3. The perplexities are\ncomputed over 4.1K ﬁrst and 5.1K rest subword\ntokens. The ﬁrst subword category exhibits the\nsame curve shape as those for infrequent tokens\nfor both models, although the magnitude of the\nperplexities is far higher. The rest of the subwords\nare far easier for both models to predict, but the\nRT perplexity curve shows some positive impact\nfrom the long-range context until a preﬁx size of\n5K tokens.\nRouting Transformers are able to copy tokens\nthat occur in the long-range context: Target\nsubword tokens that can be copied from somewhere\nin the preﬁx form another interesting group to an-\nalyze.12 While this is commonplace for frequent\nwords (e.g., determiners, pronouns), it also occurs\nfor entities and rare words (e.g., character names\nin a novel); sometimes, a word can occur several\nthousand tokens after its last occurrence. We focus\non the latter category of tokens, speciﬁcally using a\npreﬁx length of 2K tokens as a cutoff to distinguish\nlocal and long-range context. Perplexities are com-\nputed over 22k tokens which occur last time more\nthan 2K tokens away, and 36K tokens that never\nappear in the preﬁx. In particular, the left plot in\nFigure 4 shows the perplexity of tokens that can-\nnot be found in the previous 2K tokens, but occur\nsomewhere in the long-range context (2K to 8K\ntokens away). While the LT curve for such tokens\nplateaus after 2K tokens, indicating that LT can-\nnot take advantage of repeated words in the distant\n12Note that there is some overlap between the token cate-\ngories we have analyzed so far. We verify in the Appendix C\nTable 2 that the overlap between these categories is not signiﬁ-\ncant enough to confound the results.\n2500 5000 7500\nprefix length\n1.0\n1.1\n1.2perplexity\nRest of subword tokens\nfic\nnon-fic\n2500 5000 7500\nprefix length\n1.0\n1.1\n1.2\nRest of subword tokens\ncont.\ndiscont.\nFigure 6: The majority of improvements on tokens in-\nside subword clusters (i.e., excluding the ﬁrst token in\nsubword clusters) from preﬁxes longer than 2K tokens\ncomes from ﬁctional and continuous books.\n0 2000 4000 6000 8000\nperturbation length\n36\n38\n40\n42perplexity\nRouting Transformer\nRT-shuffle\nRT-random\nRT-8K\n0 2500 5000\n35.25\n35.50\nFigure 7: Perturbing up to 6K preﬁx tokens does not no-\ntably affect RT’s overall perplexity. The corresponding\nplot for LT is included in Appendix D.\ncontext, the RT curve steadily decreases until 8K\ntokens. The right plot, which shows the subset of\ntarget tokens which do not occur anywhere in the\nshort or long-term context, decreases until about\n5K tokens and then plateaus. Overall, these results\nshow that long-range context is helpful for tokens\nthat appear even several thousands tokens away.\nFollowing patterns in the long-range context:\nBesides the token categories examined above, we\nalso qualitatively look at some examples that are\ntoo infrequent to analyze at scale. Interestingly,\nwe observe some simple patterns (slightly more\ncomplex than copying) that the RT model picks\nup on. Speciﬁcally, it learns to increment chapter\nnumbers even if the previous chapter title appears\nmore than 2K tokens away: for example, when pre-\ndicting “Chapter V” in the validation book Keith\nof the Border, modifying the previous chapter title\n“Chapter IV”, which occurs 2300 tokens away, to\n“Chapter V” causes the loss of the predicted token\n“V” to increase by over 10.\nThe impact of book type on the beneﬁts of long-\nrange context: PG-19 contains a diverse array of\ntopics, genres, and formats, not all of which equally\n812\nperturbed prefix unperturbed prefixTarget\nMax sequence length (8K)\nx-axis\nFigure 8: Generic illustration of the x-axis in all the\nperturbation analysis.\nbeneﬁt from long-range context modeling. For ex-\nample, while continuous narratives (e.g., novels)\ncertainly build up many high-level discourse struc-\ntures over a long sequence of tokens, discontinuous\ntext like magazines, textbooks, or short story col-\nlections may require primarily local modeling. To\nbetter understand the effect of the type of book on\nlong-range LM perplexity, we annotate every book\nin PG-19’s validation set as either ﬁction or non-\nﬁction and continuous13 or discontinuous.14 Out of\n49 books we annotated, 30 are non-ﬁction,15 31 are\ndiscontinuous, and 25 books are both non-ﬁction\nand discontinuous.\nWe observe in Figure 5 that the RT model\ntakes better advantage of long-range context for\nﬁctional and continuous books, as the perplexity\nfor these books plateaus at around 5K tokens. Fig-\nure 6 shows ﬁctional and continuous books exploit\nbetter the long-range context while predicting to-\nkens within subword clusters. Overall, we ﬁnd\nthe improvement stems largely from continuous\nand ﬁctional books; more details are included in\nAppendix B.\n4 The perturbation of long-range context\nThe experiments in the previous section show that\nincorporating long-range context (further than 2K\ntokens away from the target) yields only marginal\nimprovements to the overall perplexities of RT and\nLT. However, the long-range context does have\na notable positive impact on a subset of tokens\nand book types. Do these improvements persist in\nthe presence of severe perturbations to the distant\ncontext? If so, this would indicate that they are not\nencoding any complex discourse structure in the\ncontext but rather relying on surface information\n(e.g., token presence) to make better predictions. In\n13We consider books with related but distinct sections (such\nas textbooks) to be discontinuous in our annotation.\n14We also annotate whether the work has been written by\nthe same author or various authors, which is presented in the\nAppendix.\n15Some magazines contain short stories or poems inter-\nspersed with news articles and essays; we count these as non-\nﬁction in our analysis.\n0 2000 4000 6000 8000\nperturbation length\n25\n26\n27\n28\n29perplexity\nFrequent tokens\nRT-shuffle\nRT-random\nRT-8K\n0 2000 4000 6000 8000\nperturbation length\n1250\n1500\n1750\n2000\n2250\nInfrequent tokens\n0 5000\n1200\n1225\nFigure 9: While perturbing up to 6K preﬁx tokens has\nlittle impact on frequent tokens, it increases the perplex-\nity of infrequent tokens.\n0 2000 4000 6000 8000\nperturbation length\n8000\n10000\n12000\n14000\n16000perplexity\nFirst subword token\nRT-shuffle\nRT-random\nRT-8K\n0 5000\n7000\n7500\n0 2000 4000 6000 8000\nperturbation length\n3.6\n3.8\n4.0\n4.2\nRest of subword tokens\nFigure 10: Both shufﬂing and random replacement in-\ncrease the perplexity of tokens inside subword clusters,\nwith the former having more negative impact.\nthis section, we perform a perturbation analysis to\nquantitatively measure the robustness of the state-\nof-the-art RT model.16\nFormally, assume we are given a preﬁx sequence\nP = (w0,w1,...,w n) with which we want to pre-\ndict target sequence (wn+1,wn+2,...,w n+k). We\napply a perturbation ρto the ﬁrst mtokens of the\npreﬁx (w0:m) to obtain the perturbed preﬁx\n˜P = (ρ(w0,...,w m),wm+1,...,w n).\nWe deﬁne the following three perturbation oper-\nations for ρand report results averaged over ﬁve\nruns for each of them.\n• Sequence shufﬂing: Tokens within the per-\nturbed window w0:m are shufﬂed across the\nentire window (i.e., sentence boundaries are\nnot respected).\n• Random sequence replacement: w0:m is re-\nplaced with a random sequence from another\nvalidation book that is mtokens long.\n• Speciﬁc token drop: Speciﬁc tokens within\nw0:m (e.g., those that occur in the target) are\ndropped and replaced with the padding token.\n16Figure 20 in the Appendix shows that the Local Trans-\nformer never uses context beyond 3K. Due to this limitation,\nwe only present results on RT for in this section.\n813\n0 2000 4000 6000 8000\nperturbation length\n170\n180\n190\n200\n210\n220perplexity\nOccurs >2K tokens away\nRT-shuffle\nRT-random\nRT-8K\n0 5000\n170\n175\n0 2000 4000 6000 8000\nperturbed prefix length\n800\n850\n900\n950\n1000\n1050\nNot in prefix\n0 5000\n780\n790\nFigure 11: Both perturbation operations increase the\nperplexity of target tokens whose last appearance in the\npreﬁx is more than 2K tokens away.\nSequence-level perturbations further than 2K\ntokens from the target have minimal impact on\nperplexity: We ﬁrst apply sequence-level shuf-\nﬂing and random replacement to the distant con-\ntext. Both operations have minimal impact on the\nperplexity of all tokens (Figure 7) as well as fre-\nquent/infrequent tokens (Figure 9) provided at least\n2K tokens are left unperturbed. However, these\nperturbations do have increasing impact as the per-\nturbations come closer to the target, especially for\ninfrequent tokens. Zooming in on the long-range\ncontext, we ﬁnd that random replacement consis-\ntently results in higher perplexity than shufﬂing, but\nalso that shufﬂing distant context actually achieves\nslightly lower perplexity than when the model is\ngiven completely unperturbed preﬁxes. Overall,\nthese results demonstrate that RT is insensitive to\nthe word order of the long-range context.\nTokens inside subword clusters and tokens re-\npeated in the distant context depend on word\norder: Similar to the analysis in Section 3, the\nexperiments above may hide impacts on small sub-\nsets of tokens, which motivates us to do a more\nﬁne-grained analysis. We ﬁnd that tokens inside\nsubword clusters (Figure 10) and those that can\nonly be copied from long-range context (Figure 11)\nare sensitive to both the order and the content of\nthe remote context. While random replacement is\nmore harmful than shufﬂing for tokens that can be\ncopied in the distant context (172 shufﬂed vs 174\nrandom replacement perplexity when perturbing\n6K tokens), shufﬂing is more detrimental for to-\nkens inside subword clusters (3.8 vs 3.7 perplexity\nwhen perturbing 6K tokens).\nRouting Transformer encodes token identity in\nthe long-range context: While the previous per-\nturbations affected entire contiguous blocks of the\npreﬁx, we move now to more targeted perturbations.\nAn interesting question to ask given the observation\n2000 4000 6000 8000\nperturbation length\n14.1\n14.2\n14.3\n14.4perplexity\nOccurs <2K tokens\ndrop-target\ndrop-random\n2000 4000 6000 8000\nperturbation length\n167\n168\n169\n170\n171\n172\nOccurs >2K tokens away\nFigure 12: Perplexity of target tokens whose last occur-\nrence in the preﬁx is within previous 2K tokens ( left)\nor more than 2K tokens away ( right), when dropping\neither these target tokens or random tokens in the per-\nturbed range. The curve on the right indicates RT mem-\norizes token identity in the distant preﬁx to some ex-\ntent.\nthat RT perplexity decreases on copied tokens as\nsequence length increases (§ 3) is how much that\nperplexity decrease depends on word order and\nsurrounding content. In response, we drop tokens\nin the distant context whose next appearance is in\nthe target sequence. As a control experiment, we\ndrop the same number of random tokens for each\nperturbation length.\nAs shown in the right plot of Figure 12, dropping\nthe previous long-range occurrences of target to-\nkens increases the perplexity of those target tokens,\nwhich shows that RT indeed memorizes token iden-\ntity in the long-range context to some extent. The\nleft plot shows that dropping long-range duplicate\ntokens does not affect tokens that also occur within\nthe local context (i.e., the prior 2K tokens). The\nﬂat curve before 6K indicates the model relies only\non the most recent occurrences for prediction.\n5 Sequence-level analysis\nAll of the previous experiments have focused on\ntoken-level perplexity, which is the standard way in\nwhich LMs are evaluated. However, the preﬁxes in\nthese evaluations consist solely of ground-truth text,\nmirroring the “teacher-forcing” setup that LMs are\ntrained with. When these models are deployed\npractically to generate text, they have to rely on\ntheir previous predictions instead of ground-truth\ntext, and several prior works have noted different\nbehavior in this setting (Wang and Sennrich, 2020;\nHoltzman et al., 2020; Welleck et al., 2020). In\nthis section, we shift from token-level tasks to ana-\nlyzing RT and LT performance onsequence-level\ntasks. In particular, we ﬁrst look at how well the\nmodels can memorize an exact sequence in the dis-\ntant context, as opposed to a single token as we did\n814\n0 2000 4000 6000 8000\ndistance of duplicate from target\n15\n20\n25\n30\n35\n40perplexity\nSequence copying\nRT\nLT\n0 2000 4000 6000 8000\nprefix length\n0.30\n0.35\n0.40\n0.45accuracy\nSuffix identification\nRT\nLT\nFigure 13: Left: Both models assign low perplexity if\na duplicate sequence appears within previous 512 to-\nkens. Right: Both models have almost identical perfor-\nmance on our sufﬁx identiﬁcation task. Adding context\nbeyond 2K tokens does not improve performance of ei-\nther sequence-level prediction task.\nPreﬁx ... If the doctor’s prophecy is\ncorrect... (∼700 tokens)... \"How\nfar is it to his place?\"\"Oh, a mile at\nleast.We can have a cab.\"\"A mile?\nGold sufﬁx: Then we shall see if there is any truth\nin what that swab of a doctor said...\nNegative 1: If I can see Mr.Elberry to-day we may\nlet you have a cheque...\nNegative 2: It was not until he had signed and sent\nit off that the full signiﬁcance of all...\nNegative 3: He hurried in, fearing that she might\nhave taken some turn for the worse...\nNegative 4: look!!!\"Her voice had fallen suddenly\nto a quivering whisper and she was...\nNegative 5: Again the Admiral burst out cheer-\ning.\"There remains, therefore...\nTable 1: An example of sufﬁx identiﬁcation task, the\nfull version of this example is included in Appendix E.\npreviously. Next, we examine the models’ ability\nto identify which of six 128-token sufﬁxes follows\na given preﬁx, which examines their behavior out-\nside the standard teacher-forced setting.\nSequence-level copying: As a sequence-level\nanalogue to the token-copying analysis in the pre-\nvious section, we examine both RT and LT’s ability\nto memorize a sequence that occurs in the distant\ncontext. To test this ability, we copy the target se-\nquence and paste it into different positions of the\npreﬁx. The left plot in Figure 13 shows that both\nmodels give a very low perplexity to the target se-\nquence if its duplicate appears within the previous\n512 tokens. However, both models lose their abil-\nity to take advantage of the copied sequence if\nit occurs more than 2K tokens away . This con-\nﬁrms our previous discovery that sequence order is\nin general not encoded in the long-range context.\nSufﬁx identiﬁcation: To move beyond token-\nlevel experiments, we adopt a similar setting as the\nmultiple choice task in SW AG (Zellers et al., 2018).\nSpeciﬁcally, a preﬁx is paired with the ground-truth\nnext 128 tokens (or sufﬁx) as well as ﬁve randomly\nsampled sequences of length 128 that come from\nthe same book and do not occur in the preﬁx or\ngold sufﬁx. We constrain the preﬁx to end at a full\nstop and each candidate sufﬁx to start from a new\nsentence so that the difference in perplexity is not\ndue to ungrammaticality. An example is shown in\nTable 1. We construct 7K examples and compute\nthe accuracy of both models at correctly choosing\nthe correct sufﬁx. The model makes a correct pre-\ndiction when the gold sufﬁx has lower perplexity\nthan all other sufﬁxes. As shown in the right plot of\nFigure 13, increasing preﬁx length beyond 2K does\nnot improve sufﬁx identiﬁcation accuracy. Surpris-\ningly, the LT and RT model have almost identical\n(and poor) performance on this task.17 While RT\nis a signiﬁcantly better LM in terms of token-level\nperplexity, it does not appear to be superior in terms\nof using long-range context to improve sequence\nprediction. Overall, both models often predict ob-\nviously wrong negative sufﬁxes: the full version\nof Table 1 together with RT’s perplexity score for\neach sufﬁx is included in Appendix E.\nCombined with our previous token-level analy-\nsis, we conclude that the distant context helps a sub-\nset of tokens in superﬁcial ways; however, distant\ncontext is currently not helpful for sequence-level\nprediction tasks.\n6 Related work\nOur work examines recent advances in efﬁcient\nTransformer variants (Sukhbaatar et al., 2019; Ki-\ntaev et al., 2020; Choromanski et al., 2021; Tay\net al., 2021; Katharopoulos et al., 2020; Wang et al.,\n2020; Wu et al., 2020) that accept longer sequences\nthan prior approaches. Longer effective context\nsize is often achieved by sparse attention (Child\net al., 2019), recurrence (Dai et al., 2019), and\ncached memory (Weston et al., 2015; Rae et al.,\n2020). Our work is also related to methods that\nincorporate long context (Wang and Cho, 2016) as\nwell as document-level tasks that inherently require\nmodeling long-range context (Zhang et al., 2018;\n17While evaluating on the newly released LT checkpoint,\nthe performance of LT is slightly worse, but the trend is similar.\nAdding context beyond 2K tokens does not keep improving the\nsufﬁx identiﬁcation accuracy. We direct reader to Appendix F\nfor more details.\n815\nHofstätter et al., 2020; Zhang et al., 2020).\nThis work is also similar to other analysis of\nlanguage models, especially for long-range con-\ntext. Khandelwal et al. (2018) analyze the usage\nof long-term context of smaller LSTM LMs. Sha-\nran et al. (2018) prove long-term context is not\nneeded for HMM LM due to teacher forcing. Rae\nand Razavi (2020) conduct an analysis exclusively\nfor the Transformer-XL (Dai et al., 2019) model.\nRae et al. (2020) show that Compressive Trans-\nformer improves the performance of infrequent to-\nkens. Our work also relates to that of Lai et al.\n(2020), who investigate the impact of context for\npretrained masked LMs. More recently, Press et al.\n(2020) also observe negligible beneﬁts of long-term\ncontext; we step further in this direction by explor-\ning larger models with more ﬁne-grained analysis.\n7 Conclusion\nWe perform a ﬁne-grained analysis of the impact\nof long-range context to both token- and sequence-\nlevel improvements on two long-range Transformer\nlanguage models, using the PG-19 dataset as a\ntestbed. Our results suggest these models rarely\ntake advantage of the long-term context, and when\nthey do it is mostly in superﬁcial ways (e.g, by\ncopying rare tokens from far away). With the pro-\nliferation of research in increasing the input size of\nTransformer LMs, we hope that our research will\nlead to more meaningful progress on integrating\ndiscourse information into these models.\nEthical concerns\nMisuse of language models The two large lan-\nguage models we evaluated in this work share com-\nmon ethical concerns with works on language mod-\nels and language generation. These pre-trained\nLMs can be used maliciously to generate unfaith-\nful, hallucinated, and biased output. Our reported\nresults do not include any kind of generation.\nEnergy costs We conduct all our analysis experi-\nments on RTX8000 GPUs. Although our work does\nnot include training large language models, the\nenergy costs of evaluating large pre-trained LMs,\nsuch as the Routing Transformer, should not be\nignored. Each example of 8K tokens long takes\naround 1.3s ∼1.4s to run one forward pass with\nthe RT model. We hope our analysis can shed light\non more efﬁcient and effective method to encode\nlong-term context.\nAcknowledgements\nWe are grateful to Aurko Roy for releasing the\ncode and checkpoints and for discussing the Rout-\ning Transformer results with us. We thank Nader\nAkoury, Brendan O’Connor and the rest of UMass\nNLP group for the great advice on the draft of this\npaper. We thank the anonymous reviewers for their\nthoughtful comments on the paper. This project\nwas partially funded by a grant from Intuit AI and\nalso by award IIS-1955567 from the National Sci-\nence Foundation (NSF).\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Interna-\ntional Conference on Learning Representations.\nGonçalo M. Correia, Vlad Niculae, and André F. T.\nMartins. 2019. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2174–\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nJane Grimshaw. 1990. Argument structure. the MIT\nPress.\n816\nBarbara J. Grosz, Aravind K. Joshi, and Scott Wein-\nstein. 1995. Centering: A framework for model-\ning the local coherence of discourse. Computational\nLinguistics, 21(2):203–225.\nJerry R Hobbs. 1979. Coherence and coreference.\nCognitive science, 3(1):67–90.\nSebastian Hofstätter, Hamed Zamani, Bhaskar Mitra,\nNick Craswell, and Allan Hanbury. 2020. Local\nself-attention over long text for efﬁcient document\nretrieval.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2015. Document context lan-\nguage models. arXiv preprint arXiv:1511.03962.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Francois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294, Melbourne, Australia. Association\nfor Computational Linguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answer-\ning. In North American Association for Computa-\ntional Linguistics.\nWilliam Labov and Joshua Waletzky. 1997. Narrative\nanalysis: Oral versions of personal experience.\nYi-An Lai, Garima Lalwani, and Yi Zhang. 2020. Con-\ntext analysis for pre-trained masked language mod-\nels. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3789–3804,\nOnline. Association for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nRaymond J Mooney and Gerald DeJong. 1985. Learn-\ning schemata for natural language processing. In IJ-\nCAI, pages 681–687.\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2020.\nShortformer: Better language modeling using\nshorter inputs.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7524–7529, Online. As-\nsociation for Computational Linguistics.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learn-\ning Representations.\nRoni Rosenfeld. 1996. A maximum entropy approach\nto adaptive statistical language modeling.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:53–68.\nRoger C Schank and Robert P Abelson. 1977. Scripts,\nplans, goals, and understanding: an inquiry into hu-\nman knowledge structures.\nVatsal Sharan, Sham Kakade, Percy Liang, and Gre-\ngory Valiant. 2018. Prediction with a short memory.\nIn Proceedings of the 50th Annual ACM SIGACT\nSymposium on Theory of Computing , STOC 2018,\npage 1074–1087, New York, NY , USA. Association\nfor Computing Machinery.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2021. Synthesizer: Re-\nthinking self-attention for transformer models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nChaojun Wang and Rico Sennrich. 2020. On exposure\nbias, hallucination and domain shift in neural ma-\nchine translation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3544–3552, Online. Association for\nComputational Linguistics.\n817\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nTian Wang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1319–1329, Berlin, Germany.\nAssociation for Computational Linguistics.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2015. Memory networks.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range\nattention.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 93–\n104, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020.\nLong-short term masking transformer: A simple\nbut effective baseline for document-level neural ma-\nchine translation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1081–1087, Online. As-\nsociation for Computational Linguistics.\nA Routing Transformer and\nend-sequence degradation\nIn our analysis, instead of picking the last 10 to-\nkens in a sequence, we chose the last 50 to last 40\ntokens due to an artifact introduced by the cluster-\ning heads in the RT model. We ﬁnd that in general\nthe last 20 tokens in a sequence tend to have in-\ncreasing perplexity as we evaluate on longer and\nlonger sequence lengths. As shown in Figure 14,\nthis phenomenon is only native to the RT model and\ndisappears when the clustering heads are replaced\nwith local attentions. Therefore, to make the RT\nand the LT comparable, we select the tokens from\nthe range that is not affected by the end-sequence\nissue. Although it’s not the last 10 tokens, this\nshort target chunk is still located near the end of a\nsequence, preceded by enough long context.\nB Effect of longer context\nIn section 3 we discussed that books that are ﬁc-\ntional and continuous beneﬁt more from the long-\nrange context. We also annotated the validation set\nby the authorship (i.e., whether a book is written by\nsingle author or various authors). Out of 49 books,\n11 are written by various authors, 10 of which are\nnon-ﬁctions. Due to this overlap, we only show\nresults of ﬁc/non-ﬁc in the main text.\nIn this section , we also further break down all\ntargets to the three types of tokens we examined\nin section 3, and display the results by book types.\nPerplexity of infrequent tokens (Figure 15), tokens\ninside subword clusters (Figure 16), and tokens\nwhose last occurrence is more than 2K tokens away\n(Figure 17). In general, for the small set of to-\nkens whose perplexity keep decreasing as adding\nin more context, the major source of improvements\nare from the continuous and ﬁctional books.\ninfreq in-subword distant\ninfreq 1. 0.09 0.08\nin-subword 0.36 1. 0.1\ndistant 0.07 0.02 1.\nTable 2: Ratio of overlapped target tokens among dif-\nferent categorizations. infreq are infrequent tokens, in-\nsubword are tokens within a subword cluster (i.e., ex-\ncluding the ﬁrst word in a subword cluster),distant are\ntokens only appear in the distant context (more than\n2K away). Row 1 column 3 the number 0.08 indicates\naround 8% of the infrequent tokens are those that can\nonly be found in the distant context.\n818\n0 2000 4000 6000 8000\nmax sequence length\n36\n38\n40\n42\n44\n46\n48perplexity\nRouting Transformer\n0 2000 4000 6000 8000\nmax sequence length\n40\n42\n44\n46\n48\n50\n52\n54\n56\nLocal Transformer\n[-11:-1]\n[-15:-5]\n[-20:-10]\n[-25:-15]\n[-30:-20]\n[-40:-30]\n[-50:-40]\nFigure 14: Perplexity of target chunk of 10 tokens long near the end of sequence. The legend indicates how far\naway the evaluated chunk is from the end of sequence (e.g., [-15:-5] means evaluating the last 15 tokens to the last\n5 tokens). The left plot shows the clustering mechanism in the Routing Transformer assigns higher perplexity to\naround the last 15 tokens in a sequence. Due to this artifacts, in our analysis, we avoid tokens within this range to\nmake comparable comparison with the Local Transformer. In the main text, our analysis are conducted over the\nlast 50 to last 40 tokens, which are not affected by this artifact.\n2000 4000 6000 8000\nprefix length\n1000\n1500\n2000\n2500\n3000perplexity\ngenre\nnonfics\nfics\n2000 4000 6000 8000\nprefix length\n1000\n1500\n2000\n2500\n3000perplexity\ncontinuity\ndiscontinuous\ncontinuous\n2000 4000 6000 8000\nprefix length\n1200\n1300\n1400\n1500\n1600perplexity\nauthor\nvarious_author\nsame_author\nFigure 15: Perplexity of infrequent target tokens, broken down by genre (left), continuity (middle), and authorship\n(right). Perplexity of infrequent tokens in ﬁctional, continuous and single-authored books decreases as the context\nlength increases to around 5K. On the other hand, the rest types of books rely on more local context while predicting\nthe infrequent tokens.\n2000 4000 6000 8000\nprefix length\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75perplexity\ngenre\nnonfics\nfics\n2000 4000 6000 8000\nprefix length\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75perplexity\ncontinuity\ndiscontinuous\ncontinuous\n2000 4000 6000 8000\nprefix length\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75perplexity\nauthor\nvarious_author\nsame_author\nFigure 16: Perplexity of target tokens inside subword clusters (i.e., excluding the ﬁrst subword in each cluster),\nbroken down by genre ( left), continuity ( middle), and authorship ( right). Perplexity of these tokens in ﬁctional\nand continuous books improves as the length increases up to around 4K, whereas nonﬁctional and discontinuous\nbooks are not taking any advantage of long-range context at all.\n819\n2000 4000 6000 8000\nprefix length\n165\n170\n175\n180perplexity\ngenre\nnonfics\nfics\n2000 4000 6000 8000\nprefix length\n170\n175\n180\n185\ncontinuity\ndiscontinuous\ncontinuous\n2000 4000 6000 8000\nprefix length\n165\n170\n175\n180\n185\n190\nauthorship\nvarious_author\nsame_author\nFigure 17: Perplexity of target tokens that can only be found in the distant context, when evaluated with the Routing\nTransformer on subset of PG-19 validation set, broken down by genre ( left), continuity (middle), and authorship\n(right). Fictional, continuous, and single-authored books continue to have improved perplexity for this type of\ntokens as the preﬁx length increases up to 8K. The single-authored books also contain discontinuous books which\nrequire less modeling of long-range context. The decreasing curve indicates the model might have acquired author\nspeciﬁc token statistics from incorporating longer context.\n2000 4000 6000 8000\nprefix length\n36\n37\n38perplexity\nAll target tokens\nRT\nLT\nFigure 18: The perplexity of all target tokens plateaus\nafter 2K preﬁx tokens for both Routing Transformer\nand Local Transformer. The LT model is the one re-\nleased in 2021 summer.\nC Token overlaps\nWe have shown in section 3 that infrequent tokens,\ntokens inside subword clusters, and tokens that can\nonly be copied from distant context beneﬁt from\ncontext longer than 2K tokens. It is possible that\nthese improvements come from the same set of to-\nkens shared across these three types of tokens. To\nverify if there are signiﬁcant overlaps among those\nthree types of tokens, we compute the overlapped\nratio in Table 2. Except for in-subword and infre-\nquent tokens, the overlapped ratios are all below\n0.1.\nD Perturbation\nPerplexity with perturbed distant preﬁx when evalu-\nated with Local Transformer is shown in Figure 20.\nPerplexity hardly changes when perturbing up to\naround 6K tokens. Because LT doesn’t properly\nuse long-range context, we only present the results\n0 2000 4000 6000 8000\nperturbation length\n38\n40\n42perplexity\nLocal Transformer\nLT-shuffle\nLT-random\nLT-8K\n2500 5000\n36\n37\nFigure 19: Perplexity of all target tokens when evalu-\nated with Local Transformer released in 2021 summer.\nof Routing Transformer in the main text.\nE Sufﬁx Identiﬁcation\nIn the main text, we present the sufﬁx identiﬁcation\nresults with 128-token long sufﬁx. Here, we pro-\nvide results when evaluate the accuracy with sufﬁx\nof different length. Interestingly, the accuracy of\ndistinguishing a gold sufﬁx ﬁrst increases with the\nsufﬁx length, reaching the best when the sufﬁx\nlength is around 10 to 20, then decreases as the\nsufﬁx becomes longer. This is likely because the\nsequence becomes more probable as incorporating\nmore local context (part of the sufﬁx).\nIn Table 3 and Table 4, we present a complete\nexample of the sufﬁx identiﬁcation task. The preﬁx\ncontains 1024 tokens, and each of the sufﬁxes con-\ntains 128 tokens. Lower perplexity of obviously\nwrong sufﬁx (e.g. negative 1,3,4) indicates cur-\nrent RT model is not properly taking advantage of\ndistant context to make sequence-level predictions.\n820\n0 2000 4000 6000 8000\nperturbation length\n40\n42\n44\n46\n48perplexity\nLocal Transformer\nLT-shuffle\nLT-random\nLT-8K\n2500 5000\n39.5\n40.0\n40.5\nFigure 20: Perplexity of all target tokens when eval-\nuated with Local Transformer derived from the RT\ncheckpoint.\n0 2000 4000 6000 8000\nprefix length\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60accuracy\nsuffix-len-5\nsuffix-len-10\nsuffix-len-20\nsuffix-len-40\nsuffix-len-80\nsuffix-len-128\nFigure 21: Sufﬁx identiﬁcation accuracy when evalu-\nated with different preﬁx length.\nF Local Transformer checkpoint results\nIn the main text, we analyzed both the RT check-\npoint and an LT model derived from the same RT\ncheckpoint by replacing the clustering heads with\nlocal attention heads. After the submission dead-\nline and before the camera ready deadline, the au-\nthor of the Routing Transformer released a new LT\ncheckpoint, which has 24 layers in total18. To make\nsure the behavior of our former LT is the same with\nan LT trained from scratch, we also conducted all\nour analysis again on this new checkpoint. Overall,\nwe ﬁnd the new LT checkpoint performs slightly\nbetter on token-level tasks but is inferior to the\nprevious LT checkpoint on sufﬁx identiﬁcation, a\nsequence-level task. Since the trends are the same,\nno conclusion needs to be changed.\nThe Effect of Longer Context In Figure 18 are\nthe perplexities in aggregate over all target tokens\nof both the RT and the new LT checkpoints. The\ntarget tokens are the same ones we used in the main\ntext. The new LT has better perplexity than the one\n18Both the RT model and the former LT model have 22\nlayers.\n0 2000 4000 6000 8000\ndistance of duplicate from target\n15\n20\n25\n30\n35perplexity\nSequence copying\nRT\nLT\n0 2000 4000 6000 8000\nprefix length\n0.30\n0.35\n0.40\n0.45accuracy\nSuffix identification\nRT\nLT\nFigure 22: Left: Both models assign low perplexity if a\nduplicate sequence appears within previous 512 tokens.\nRight: Adding context beyond 2K tokens does not im-\nprove performance of sufﬁx identiﬁcation. Moreover,\nthe recently released LT performs worse than the one\nderived from the RT checkpoint.\npresented in the main text (36.5 vs. 40 as the preﬁx\nlength extends to 8K).\nPerturbation of long-range context Similar to\nthe former LT (Figure 20), the newly released LT\ncheckpoint(Figure 19) is not sensitive at all to the\nperturbation further than local 2K tokens. Both\nmodels are impacted by local random replacement\nmore than shufﬂing, however, the new LT check-\npoint has overall better perplexity than the RT-\nderived LT.\nSequence-level tasks Figure 22 shows the per-\nformance of both RT and the released LT check-\npoint on sequence-level tasks. Compared to the\nresults in the main text, the new LT checkpoint\nperforms better at sequence-copying task, however,\nthe trend remains the same – the order of tokens\nbeyond 2K tokens is not properly encoded. On the\nother hand, the new LT checkpoint is slightly worse\nin sufﬁx identiﬁcation while the former RT-derived\nLT has almost identical performance as the RT.\nThis implies even though the clustering heads are\nremoved from the previous LT, useful information\nis preserved by the local attention heads.\nOverall, we ﬁnd the released LT checkpoint has\nbetter token-level performance but performs worse\non sufﬁx identiﬁcation. Each plot shares the same\ntrend with its corresponding one in the main text,\nthus no conclusion needs to be modiﬁed.\n821\nPreﬁx Not a bit, sir.Out with it!I have faced death too often to ﬂinch from it now, though I\nsaw it as near me as you are.\"\"Well, well, we must go by averages of course.Shall we\nsay two years?I should think that you have a full two years before you.\"\"In two years\nyour pension would bring you in L1,600.Now I will do my very best for you, Admiral!I\nwill advance you L2,000, and you can make over to me your pension for your life.It\nis pure speculation on my part.If you die to-morrow I lose my money.If the doctor’s\nprophecy is correct I shall still be out of pocket.If you live a little longer, then I may\nsee my money again.It is the very best I can do for you.\"\"Then you wish to buy my\npension?\"\"Yes, for two thousand down.\"\"And if I live for twenty years?\"\"Oh, in that\ncase of course my speculation would be more successful.But you have heard the doctor’s\nopinion.\"\"Would you advance the money instantly?\"\"You should have a thousand at\nonce.The other thousand I should expect you to take in furniture.\"\"In furniture?\"\"Yes,\nAdmiral.We shall do you a beautiful houseful at that sum.It is the custom of my clients to\ntake half in furniture.\"The Admiral sat in dire perplexity.He had come out to get money,\nand to go back without any, to be powerless to help when his boy needed every shilling\nto save him from disaster, that would be very bitter to him.On the other hand, it was so\nmuch that he surrendered, and so little that he received.Little, and yet something.Would\nit not be better than going back empty-handed?He saw the yellow backed chequebook\nupon the table.The moneylender opened it and dipped his pen into the ink.\"Shall I ﬁll\nit up?\"said he.\"I think, Admiral,\" remarked Westmacott, \"that we had better have a\nlittle walk and some luncheon before we settle this matter.\"\"Oh, we may as well do\nit at once.It would be absurd to postpone it now,\" Metaxa spoke with some heat, and\nhis eyes glinted angrily from between his narrow lids at the imperturbable Charles.The\nAdmiral was simple in money matters, but he had seen much of men and had learned\nto read them.He saw that venomous glance, and saw too that intense eagerness was\npeeping out from beneath the careless air which the agent had assumed.\"You’re quite\nright, Westmacott,\" said he.\"We’ll have a little walk before we settle it.\"\"But I may\nnot be here this afternoon.\"\"Then we must choose another day.\"\"But why not settle it\nnow?\"\"Because I prefer not,\" said the Admiral shortly.\"Very well.But remember that my\noffer is only for to-day.It is off unless you take it at once.\"\"Let it be off, then.\"\"There’s\nmy fee,\" cried the doctor.\"How much?\"\"A guinea.\"The Admiral threw a pound and a\nshilling upon the table.\"Come, Westmacott,\" said he, and they walked together from the\nroom.\"I don’t like it,\" said Charles, when they found themselves in the street once more;\n\"I don’t profess to be a very sharp chap, but this is a triﬂe too thin.What did he want to\ngo out and speak to the doctor for?And how very convenient this tale of a weak heart\nwas!I believe they are a couple of rogues, and in league with each other.\"\"A shark and a\npilot ﬁsh,\" said the Admiral.\"I’ll tell you what I propose, sir.There’s a lawyer named\nMcAdam who does my aunt’s business.He is a very honest fellow, and lives at the other\nside of Poultry.We’ll go over to him together and have his opinion about the whole\nmatter.\"\"How far is it to his place?\"\"Oh, a mile at least.We can have a cab.\"\"A mile?\nGold\nsufﬁx\n(ppl=74.4)\nThen we shall see if there is any truth in what that swab of a doctor said.Come, my\nboy, and clap on all sail, and see who can stay the longest.\"Then the sober denizens\nof the heart of business London saw a singular sight as they returned from their lun-\ncheons.Down the roadway, dodging among cabs and carts, ran a weather-stained elderly\nman, with wide ﬂapping black hat, and homely suit of tweeds.With elbows braced back,\nhands clenched near his armpits, and chest protruded, he scudded along, while close at\nhis heels lumbered a large-limbed, heavy, yellow mustached young man, who seemed\nto feel the exercise a good deal more than his senior.On they dashed, helter-skelter, until\nthey pulled up panting at the ofﬁce where the lawyer of the\nTable 3: Sufﬁx identiﬁcation example, extracted from the book Beyond the City by Arthur Conan Doyle. For\neach sufﬁx, we show the perplexity of the sufﬁx evaluated with the Routing Transformer. RT assigns a lot lower\nperplexity to examples (e.g. negative 1,3,4) that are obviously wrong given the preﬁx.\n822\nNegative 1\n(ppl=34.6):\nIf I can see Mr.Elberry to-day we may let you have a cheque to-morrow.Try another\npinch.No?Well, good-bye.I am very happy to have been of service.\"Mr.McAdam bowed\nthem out, for he was a very busy man, and they found themselves in the street once\nmore with lighter hearts than when they had left it.\"Well, Westmacott, I am sure I am\nvery much obliged to you,\" said the Admiral.\"You have stood by me when I was the\nbetter for a little help, for I’m clean out of my soundings among these city sharks.But\nI’ve something to do now which is more in my own line, and I need not trouble you any\nmore.\"\"Oh, it is no trouble.I have nothing\nNegative\n2\n(ppl=71.69):\nIt was not until he had signed and sent it off that the full signiﬁcance of all that he\nhad done broke upon him.He had sacriﬁced everything.His pension was gone.He had\nnothing save only what he could earn.But the stout old heart never quailed.He waited\neagerly for a letter from the Saint Lawrence Shipping Company, and in the meanwhile\nhe gave his landlord a quarter’s notice.Hundred pound a year houses would in future\nbe a luxury which he could not aspire to.A small lodging in some inexpensive part of\nLondon must be the substitute for his breezy Norwood villa.So be it, then!Better that\na thousand fold than that his name should be associated with failure and disgrace.On\nthat morning Harold Denver was to meet the creditors of the ﬁrm, and to explain the\nsituation to them.It was a hateful task\nNegative\n3\n(ppl=33.54):\nHe hurried in, fearing that she might have taken some turn for the worse, but he was\nreassured to ﬁnd her sitting up in her bed, with Clara and Ida Walker in attendance upon\nher.She had removed the handkerchief, and had put on a little cap with pink ribbons,\nand a maroon dressing-jacket, daintily fulled at the neck and sleeves.\"My dear friend,\"\nsaid she as he entered, \"I wish to make a last few remarks to you.No, no,\" she continued,\nlaughing, as she saw a look of dismay upon his face.\"I shall not dream of dying for at\nleast another thirty years.A woman should be ashamed to die before she is seventy.I\nwish, Clara, that you would ask your father to step up.And you, Ida, just pass me my\ncigarettes, and\nNegative\n4\n(ppl=34.92):\nlook!!!\"Her voice had fallen suddenly to a quivering whisper and she was pointing to\nthe Westmacotts’ house.Her sister gave a gasp of horror, and stood with a clutch at\nMonica’s arm, staring in the same direction.There was a light in the front room, a slight,\nwavering light such as would be given by a small candle or taper.The blind was down,\nbut the light shone dimly through.Outside in the garden, with his ﬁgure outlined against\nthe luminous square, there stood a man, his back to the road, his two hands upon the\nwindow ledge, and his body rather bent as though he were trying to peep in past the\nblind.So absolutely still and motionless was he that in spite of the moon they might well\nhave overlooked him were it not for that tell-tale light behind.\nNegative\n5\n(ppl=47.66):\nAgain the Admiral burst out cheering.\"There remains, therefore, about L3,200 which\nhas to be found within ten days.No man shall lose by me.I gave them my word in the\nroom that if I worked my soul out of my body every one of them should be paid.I\nshall not spend a penny upon myself until it is done.But some of them can’t wait.They\nare poor men themselves, and must have their money.They have issued a warrant for\nPearson’s arrest.But they think that he has got away to the States.\"\"These men shall\nhave their money,\" said the Admiral.\"Dad!\"\"Yes, my boy, you don’t know the resources\nof the family.One never does know until one tries.What have you yourself now?\"\"I have\nabout a\nTable 4: Sufﬁx identiﬁcation example, extracted from the book Beyond the City by Arthur Conan Doyle. For\neach sufﬁx, we show the perplexity of the sufﬁx evaluated with the Routing Transformer. RT assigns a lot lower\nperplexity to examples (e.g. negative 1,3,4) that are obviously wrong given the preﬁx.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8508851528167725
    },
    {
      "name": "Computer science",
      "score": 0.8022494912147522
    },
    {
      "name": "Language model",
      "score": 0.7763243317604065
    },
    {
      "name": "Transformer",
      "score": 0.6455388069152832
    },
    {
      "name": "Natural language processing",
      "score": 0.5283718109130859
    },
    {
      "name": "Context model",
      "score": 0.5118443369865417
    },
    {
      "name": "Sentence",
      "score": 0.5076490640640259
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.504111647605896
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46406516432762146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4352450370788574
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.41386714577674866
    },
    {
      "name": "History",
      "score": 0.0878499448299408
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    }
  ]
}