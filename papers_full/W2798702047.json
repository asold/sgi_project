{
  "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
  "url": "https://openalex.org/W2798702047",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288544679",
      "name": "Khandelwal, Urvashi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105465932",
      "name": "He He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096350588",
      "name": "Qi Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223899260",
      "name": "Jurafsky, Dan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2605133364",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W2275625487",
    "https://openalex.org/W2964267515"
  ],
  "abstract": "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.",
  "full_text": "Sharp Nearby, Fuzzy Far Away: How Neural\nLanguage Models Use Context\nUrvashi Khandelwal, He He, Peng Qi, Dan Jurafsky\nComputer Science Department\nStanford University\n{urvashik,hehe,pengqi,jurafsky}@stanford.edu\nAbstract\nWe know very little about how neural lan-\nguage models (LM) use prior linguistic\ncontext. In this paper, we investigate the\nrole of context in an LSTM LM, through\nablation studies. Speciﬁcally, we ana-\nlyze the increase in perplexity when prior\ncontext words are shufﬂed, replaced, or\ndropped. On two standard datasets, Penn\nTreebank and WikiText-2, we ﬁnd that the\nmodel is capable of using about 200 to-\nkens of context on average, but sharply\ndistinguishes nearby context (recent 50 to-\nkens) from the distant history. The model\nis highly sensitive to the order of words\nwithin the most recent sentence, but ig-\nnores word order in the long-range context\n(beyond 50 tokens), suggesting the distant\npast is modeled only as a rough seman-\ntic ﬁeld or topic. We further ﬁnd that the\nneural caching model (Grave et al., 2017b)\nespecially helps the LSTM to copy words\nfrom within this distant context. Overall,\nour analysis not only provides a better un-\nderstanding of how neural LMs use their\ncontext, but also sheds light on recent suc-\ncess from cache-based models.\n1 Introduction\nLanguage models are an important component\nof natural language generation tasks, such as\nmachine translation and summarization. They\nuse context (a sequence of words) to estimate\na probability distribution of the upcoming word.\nFor several years now, neural language models\n(NLMs) (Graves, 2013; Jozefowicz et al., 2016;\nGrave et al., 2017a; Dauphin et al., 2017; Melis\net al., 2018; Yang et al., 2018) have consistently\noutperformed classical n-gram models, an im-\nprovement often attributed to their ability to model\nlong-range dependencies in faraway context. Yet,\nhow these NLMs use the context is largely unex-\nplained.\nRecent studies have begun to shed light on the\ninformation encoded by Long Short-Term Mem-\nory (LSTM) networks. They can remember sen-\ntence lengths, word identity, and word order (Adi\net al., 2017), can capture some syntactic structures\nsuch as subject-verb agreement (Linzen et al.,\n2016), and can model certain kinds of semantic\ncompositionality such as negation and intensiﬁca-\ntion (Li et al., 2016).\nHowever, all of the previous work studies\nLSTMs at the sentence level, even though they can\npotentially encode longer context. Our goal is to\ncomplement the prior work to provide a richer un-\nderstanding of the role of context, in particular,\nlong-range context beyond a sentence. We aim\nto answer the following questions: (i) How much\ncontext is used by NLMs, in terms of the number\nof tokens? (ii) Within this range, are nearby and\nlong-range contexts represented differently? (iii)\nHow do copy mechanisms help the model use dif-\nferent regions of context?\nWe investigate these questions via ablation stud-\nies on a standard LSTM language model (Merity\net al., 2018) on two benchmark language modeling\ndatasets: Penn Treebank and WikiText-2. Given a\npretrained language model, we perturb the prior\ncontext in various ways at test time, to study how\nmuch the perturbed information affects model per-\nformance. Speciﬁcally, we alter the context length\nto study how many tokens are used, permute to-\nkens to see if LSTMs care about word order in\nboth local and global contexts, and drop and re-\nplace target words to test the copying abilities of\nLSTMs with and without an external copy mech-\nanism, such as the neural cache (Grave et al.,\n2017b). The cache operates by ﬁrst recording tar-\narXiv:1805.04623v1  [cs.CL]  12 May 2018\nget words and their context representations seen\nin the history, and then encouraging the model to\ncopy a word from the past when the current con-\ntext representation matches that word’s recorded\ncontext vector.\nWe ﬁnd that the LSTM is capable of using about\n200 tokens of context on average, with no observ-\nable differences from changing the hyperparame-\nter settings. Within this context range, word or-\nder is only relevant within the 20 most recent to-\nkens or about a sentence. In the long-range con-\ntext, order has almost no effect on performance,\nsuggesting that the model maintains a high-level,\nrough semantic representation of faraway words.\nFinally, we ﬁnd that LSTMs can regenerate some\nwords seen in the nearby context, but heavily rely\non the cache to help them copy words from the\nlong-range context.\n2 Language Modeling\nLanguage models assign probabilities to se-\nquences of words. In practice, the probability can\nbe factorized using the chain rule\nP(w1,...,w t) =\nt∏\ni=1\nP(wi|wi−1,...,w 1),\nand language models compute the conditional\nprobability of a target word wt given its preced-\ning context, w1,...,w t−1.\nLanguage models are trained to minimize the\nnegative log likelihood of the training corpus:\nNLL = −1\nT\nT∑\nt=1\nlog P(wt|wt−1,...,w 1),\nand the model’s performance is usually evaluated\nby perplexity (PP) on a held-out set:\nPP = exp(NLL).\nWhen testing the effect of ablations, we focus\non comparing differences in the language model’s\nlosses (NLL) on the dev set, which is equivalent to\nrelative improvements in perplexity.\n3 Approach\nOur goal is to investigate the effect of contextual\nfeatures such as the length of context, word or-\nder and more, on LSTM performance. Thus, we\nuse ablation analysis, during evaluation, to mea-\nsure changes in model performance in the absence\nof certain contextual information.\nPTB Wiki\nDev Test Dev Test\n# Tokens 73,760 82,430 217,646 245,569\nPerplexity (no cache) 59.07 56.89 67.29 64.51\nAvg. Sent. Len. 20.9 20.9 23.7 22.6\nTable 1: Dataset statistics and performance rele-\nvant to our experiments.\nTypically, when testing the language model on a\nheld-out sequence of words, all tokens prior to the\ntarget word are fed to the model; we call this the\ninﬁnite-context setting. In this study, we observe\nthe change in perplexity or NLL when the model\nis fed a perturbed context δ(wt−1,...,w 1), at test\ntime. δrefers to the perturbation function, and we\nexperiment with perturbations such as dropping\ntokens, shufﬂing/reversing tokens, and replacing\ntokens with other words from the vocabulary. 1 It\nis important to note that we do not train the model\nwith these perturbations. This is because the aim is\nto start with an LSTM that has been trained in the\nstandard fashion, and discover how much context\nit uses and which features in nearby vs. long-range\ncontext are important. Hence, the mismatch in\ntraining and test is a necessary part of experiment\ndesign, and all measured losses are upper bounds\nwhich would likely be lower, were the model also\ntrained to handle such perturbations.\nWe use a standard LSTM language model,\ntrained and ﬁnetuned using the Averaging SGD\noptimizer (Merity et al., 2018).2 We also augment\nthe model with a cache only for Section 6.2, in\norder to investigate why an external copy mech-\nanism is helpful. A short description of the ar-\nchitecture and a detailed list of hyperparameters is\nlisted in Appendix A, and we refer the reader to\nthe original paper for additional details.\nWe analyze two datasets commonly used for\nlanguage modeling, Penn Treebank (PTB) (Mar-\ncus et al., 1993; Mikolov et al., 2010) and\nWikitext-2 (Wiki) (Merity et al., 2017). PTB\nconsists of Wall Street Journal news articles with\n0.9M tokens for training and a 10K vocabulary.\nWiki is a larger and more diverse dataset, con-\ntaining Wikipedia articles across many topics with\n2.1M tokens for training and a 33K vocabulary.\nAdditional dataset statistics are provided in Ta-\n1Code for our experiments available at https://\ngithub.com/urvashik/lm-context-analysis\n2Public release of their code at https://github.\ncom/salesforce/awd-lstm-lm\nble 1.\nIn this paper, we present results only on the dev\nsets, in order to avoid revealing details about the\ntest sets. However, we have conﬁrmed that all re-\nsults are consistent with those on the test sets. In\naddition, for all experiments we report averaged\nresults from three models trained with different\nrandom seeds. Some of the ﬁgures provided con-\ntain trends from only one of the two datasets and\nthe corresponding ﬁgures for the other dataset are\nprovided in Appendix B.\n4 How much context is used?\nLSTMs are designed to capture long-range depen-\ndencies in sequences (Hochreiter and Schmidhu-\nber, 1997). In practice, LSTM language models\nare provided an inﬁnite amount of prior context,\nwhich is as long as the test sequence goes. How-\never, it is unclear how much of this history has a\ndirect impact on model performance. In this sec-\ntion, we investigate how many tokens of context\nachieve a similar loss (or 1-2% difference in model\nperplexity) to providing the model inﬁnite context.\nWe consider this the effective context size.\nLSTM language models have an effective con-\ntext size of about 200 tokens on average.We\ndetermine the effective context size by varying the\nnumber of tokens fed to the model. In particular,\nat test time, we feed the model the most recent n\ntokens:\nδtruncate(wt−1,...,w 1) = (wt−1,...,w t−n), (1)\nwhere n > 0 and all tokens farther away from\nthe target wt are dropped.3 We compare the dev\nloss (NLL) from truncated context, to that of the\ninﬁnite-context setting where all previous words\nare fed to the model. The resulting increase in loss\nindicates how important the dropped tokens are for\nthe model.\nFigure 1a shows that the difference in dev loss,\nbetween truncated- and inﬁnite-context variants of\nthe test setting, gradually diminishes as we in-\ncrease nfrom 5 tokens to 1000 tokens. In particu-\nlar, we only see a 1% increase in perplexity as we\nmove beyond a context of 150 tokens on PTB and\n250 tokens on Wiki. Hence, we provide empirical\nevidence to show that LSTM language models do,\nin fact, model long-range dependencies, without\nhelp from extra context vectors or caches.\n3Words at the beginning of the test sequence with fewer\nthan n tokens in the context are ignored for loss computation.\nChanging hyperparameters does not change\nthe effective context size. NLM performance\nhas been shown to be sensitive to hyperparame-\nters such as the dropout rate and model size (Melis\net al., 2018). To investigate if these hyperpa-\nrameters affect the effective context size as well,\nwe train separate models by varying the follow-\ning hyperparameters one at a time: (1) number\nof timesteps for truncated back-propogation (2)\ndropout rate, (3) model size (hidden state size,\nnumber of layers, and word embedding size). In\nFigure 1b, we show that while different hyperpa-\nrameter settings result in different perplexities in\nthe inﬁnite-context setting, the trend of how per-\nplexity changes as we reduce the context size re-\nmains the same.\n4.1 Do different types of words need different\namounts of context?\nThe effective context size determined in the pre-\nvious section is aggregated over the entire cor-\npus, which ignores the type of the upcoming word.\nBoyd-Graber and Blei (2009) have previously in-\nvestigated the differences in context used by dif-\nferent types of words and found that function\nwords rely on less context than content words.\nWe investigate whether the effective context size\nvaries across different types of words, by catego-\nrizing them based on either frequency or parts-of-\nspeech. Speciﬁcally, we vary the number of con-\ntext tokens in the same way as the previous sec-\ntion, and aggregate loss over words within each\nclass separately.\nInfrequent words need more context than fre-\nquent words. We categorize words that appear\nat least 800 times in the training set as frequent,\nand the rest as infrequent. Figure 1c shows that\nthe loss of frequent words is insensitive to missing\ncontext beyond the 50 most recent tokens, which\nholds across the two datasets. Infrequent words,\non the other hand, require more than 200 tokens.\nContent words need more context than function\nwords. Given the parts-of-speech of each word,\nwe deﬁne content wordsas nouns, verbs and adjec-\ntives, and function words as prepositions and de-\nterminers.4 Figure 1d shows that the loss of nouns\nand verbs is affected by distant context, whereas\nwhen the target word is a determiner, the model\nonly relies on words within the last 10 tokens.\n4We obtain part-of-speech tags using Stanford\nCoreNLP (Manning et al., 2014).\n5 10 20 50 100 200 500 1000\nContext Size (number of tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Increase in Loss\nPTB\nWiki\n(a) Varying context size.\n10 50 100 200 500 1000\nContext Size (number of tokens)\n60\n70\n80\n90\n100\n110Perplexity\nDefault Model, PTB\nLSTM Hidden 575 (vs. 1150)\n2 layers (vs. 3)\nWord Emb 200 (vs. 400)\nNo LSTM layer dropout (vs. 0.25)\nNo recurrent dropout (vs. 0.5)\nBPTT 100 (vs. 70)\nBPTT 10 (vs. 70) (b) Changing model hyperparameters.\n5 10 20 50 100 200 500 1000\nContext Size (number of tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Increase in Loss\ninfrequent words,PTB\nfrequent words,PTB\ninfrequent words,Wiki\nfrequent words,Wiki\n(c) Frequent vs. infrequent words.\n5 10 20 50 100 200 500 1000\nContext Size (number of tokens)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Increase in Loss\nNN,Wiki\nJJ,Wiki\nVB,Wiki\nIN,Wiki\nDT,Wiki (d) Different parts-of-speech.\nFigure 1: Effects of varying the number of tokens provided in the context, as compared to the same\nmodel provided with inﬁnite context. Increase in loss represents an absolute increase in NLL over the\nentire corpus, due to restricted context. All curves are averaged over three random seeds, and error bars\nrepresent the standard deviation. (a) The model has an effective context size of 150 on PTB and 250 on\nWiki. (b) Changing model hyperparameters does not change the context usage trend, but does change\nmodel performance. We report perplexities to highlight the consistent trend. (c) Infrequent words need\nmore context than frequent words. (d) Content words need more context than function words.\nDiscussion. Overall, we ﬁnd that the model’s ef-\nfective context size is dynamic. It depends on\nthe target word, which is consistent with what we\nknow about language, e.g., determiners require\nless context than nouns (Boyd-Graber and Blei,\n2009). In addition, these ﬁndings are consistent\nwith those previously reported for different lan-\nguage models and datasets (Hill et al., 2016; Wang\nand Cho, 2016).\n5 Nearby vs. long-range context\nAn effective context size of 200 tokens allows for\nrepresenting linguistic information at many lev-\nels of abstraction, such as words, sentences, top-\nics, etc. In this section, we investigate the impor-\ntance of contextual information such as word order\nand word identity. Unlike prior work that studies\nLSTM embeddings at the sentence level, we look\nat both nearby and faraway context, and analyze\nhow the language model treats contextual informa-\ntion presented in different regions of the context.\n5.1 Does word order matter?\nAdi et al. (2017) have shown that LSTMs are\naware of word order within a sentence. We investi-\ngate whether LSTM language models are sensitive\nto word order within a larger context window. To\ndetermine the range in which word order affects\nmodel performance, we permute substrings in the\ncontext to observe their effect on dev loss com-\npared to the unperturbed baseline. In particular,\nwe perturb the context as follows,\nδpermute(wt−1,...,w t−n) =\n(wt−1,..,ρ (wt−s1−1,..,w t−s2 ),..,w t−n) (2)\nwhere ρ∈{shuﬄe,reverse}and (s1,s2] denotes\nthe range of the substring to be permuted. We re-\nfer to this substring as the permutable span. For\n1 5 10 15 20 30 50 100 200\nDistance of perturbation from target (number of tokens)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Increase in Loss\nShuffle 20 token windows PTB\nReverse 20 token windows PTB\nShuffle 20 token windows Wiki\nReverse 20 token windows Wiki\n(a) Perturb order locally, within 20 tokens of each point.\n1 5 10 15 20 30 50 100 200\nDistance of perturbation from target (number of tokens)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Increase in Loss\nShuffle entire context\nReverse entire context\nReplace context with random sequence (b) Perturb global order, i.e. all tokens in the context before a\ngiven point, in Wiki.\nFigure 2: Effects of shufﬂing and reversing the order of words in 300 tokens of context, relative to an\nunperturbed baseline. All curves are averages from three random seeds, where error bars represent the\nstandard deviation. (a) Changing the order of words within a 20-token window has negligible effect on\nthe loss after the ﬁrst 20 tokens. (b) Changing the global order of words within the context does not\naffect loss beyond 50 tokens.\nthe following analysis, we distinguish local word\norder, within 20-token permutable spans which\nare the length of an average sentence, from global\nword order, which extends beyond local spans to\ninclude all the farthest tokens in the history. We\nconsider selecting permutable spans within a con-\ntext of n = 300 tokens, which is greater than the\neffective context size.\nLocal word order only matters for the most re-\ncent 20 tokens. We can locate the region of con-\ntext beyond which the local word order has no rel-\nevance, by permuting word order locally at various\npoints within the context. We accomplish this by\nvarying s1 and setting s2 = s1 + 20. Figure 2a\nshows that local word order matters very much\nwithin the most recent 20 tokens, and far less be-\nyond that.\nGlobal order of words only matters for the most\nrecent 50 tokens. Similar to the local word or-\nder experiment, we locate the point beyond which\nthe general location of words within the context\nis irrelevant, by permuting global word order. We\nachieve this by varying s1 and ﬁxing s2 = n. Fig-\nure 2b demonstrates that after 50 tokens, shufﬂing\nor reversing the remaining words in the context has\nno effect on the model performance.\nIn order to determine whether this is due to in-\nsensitivity to word order or whether the language\nmodel is simply not sensitive to any changes in\nthe long-range context, we further replace words\nin the permutable span with a randomly sampled\nsequence of the same length from the training set.\nThe gap between the permutation and replacement\ncurves in Figure 2b illustrates that the identity of\nwords in the far away context is still relevant, and\nonly the order of the words is not.\nDiscussion. These results suggest that word or-\nder matters only within the most recent sentence,\nbeyond which the order of sentences matters for\n2-3 sentences (determined by our experiments on\nglobal word order). After 50 tokens, word or-\nder has almost no effect, but the identity of those\nwords is still relevant, suggesting a high-level,\nrough semantic representation for these faraway\nwords. In light of these observations, we deﬁne 50\ntokens as the boundary between nearby and long-\nrange context, for the rest of this study. Next, we\ninvestigate the importance of different word types\nin the different regions of context.\n5.2 Types of words and the region of context\nOpen-class or content words such as nouns, verbs,\nadjectives and adverbs, contribute more to the\nsemantic context of natural language than func-\ntion words such as determiners and prepositions.\nGiven our observation that the language model\nrepresents long-range context as a rough seman-\ntic representation, a natural question to ask is how\nimportant are function words in the long-range\n5 20 100\nDistance of perturbation from target (number of tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Increase in Loss\nDrop all content words (52.6%)\nDrop all function words (47.4%)\nRandom drop 52.6% tokens of 300\nRandom drop 47.4% tokens of 300\nFigure 3: Effect of dropping content and function\nwords from 300 tokens of context relative to an un-\nperturbed baseline, on PTB. Error bars represent\n95% conﬁdence intervals. Dropping both content\nand function words 5 tokens away from the target\nresults in a nontrivial increase in loss, whereas be-\nyond 20 tokens, only content words are relevant.\ncontext? Below, we study the effect of these\ntwo classes of words on the model’s performance.\nFunction words are deﬁned as all words that are\nnot nouns, verbs, adjectives or adverbs.\nContent words matter more than function\nwords. To study the effect of content and func-\ntion words on model perplexity, we drop them\nfrom different regions of the context and compare\nthe resulting change in loss. Speciﬁcally, we per-\nturb the context as follows,\nδdrop(wt−1,...,w t−n) =\n(wt−1,..,w t−s1 ,fpos(y,(wt−s1−1,..,w t−n)))\n(3)\nwhere fpos(y,span) is a function that drops all\nwords with POS tag yin a given span. s1 denotes\nthe starting offset of the perturbed subsequence.\nFor these experiments, we set s1 ∈{5,20,100}.\nOn average, there are slightly more content words\nthan function words in any given text. As shown in\nSection 4, dropping more words results in higher\nloss. To eliminate the effect of dropping differ-\nent fractions of words, for each experiment where\nwe drop a speciﬁc word type, we add a control\nexperiment where the same number of tokens are\nsampled randomly from the context, and dropped.\nFigure 3 shows that dropping content words as\nclose as 5 tokens from the target word increases\nmodel perplexity by about 65%, whereas dropping\nthe same proportion of tokens at random, results in\na much smaller 17% increase. Dropping all func-\ntion words, on the other hand, is not very differ-\nent from dropping the same proportion of words\nat random, but still increases loss by about 15%.\nThis suggests that within the most recent sentence,\ncontent words are extremely important but func-\ntion words are also relevant since they help main-\ntain grammaticality and syntactic structure. On the\nother hand, beyond a sentence, only content words\nhave a sizeable inﬂuence on model performance.\n6 To cache or not to cache?\nAs shown in Section 5.1, LSTM language models\nuse a high-level, rough semantic representation for\nlong-range context, suggesting that they might not\nbe using information from any speciﬁc words lo-\ncated far away. Adi et al. (2017) have also shown\nthat while LSTMs are aware of which words ap-\npear in their context, this awareness degrades with\nincreasing length of the sequence. However, the\nsuccess of copy mechanisms such as attention and\ncaching (Bahdanau et al., 2015; Hill et al., 2016;\nMerity et al., 2017; Grave et al., 2017a,b) suggests\nthat information in the distant context is very use-\nful. Given this fact, can LSTMs copy any words\nfrom context without relying on external copy\nmechanisms? Do they copy words from nearby\nand long-range context equally? How does the\ncaching model help? In this section, we investi-\ngate these questions by studying how LSTMs copy\nwords from different regions of context. More\nspeciﬁcally, we look at two regions of context,\nnearby (within 50 most recent tokens) and long-\nrange (beyond 50 tokens), and study three cate-\ngories of target words: those that can be copied\nfrom nearby context (Cnear), those that can only be\ncopied from long-range context ( Cfar), and those\nthat cannot be copied at all given a limited context\n(Cnone).\n6.1 Can LSTMs copy words without caches?\nEven without a cache, LSTMs often regenerate\nwords that have already appeared in prior context.\nWe investigate how much the model relies on the\nprevious occurrences of the upcoming target word,\nby analyzing the change in loss after dropping and\nreplacing this target word in the context.\nLSTMs can regenerate words seen in nearby\ncontext. In order to demonstrate the usefulness\nnearby long-range none (control set)\nFirst occurrence of target in context\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12Increase in Loss\nDrop 250 most distant tokens\nDrop only target\n(a) Dropping tokens\nnearby long-range\nFirst occurrence of target in context\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14Increase in Loss\nDrop only target\nReplace target with <unk>\nReplace target with similar token (b) Perturbing occurrences of target word in context.\nFigure 4: Effects of perturbing the target word in the context compared to dropping long-range context\naltogether, on PTB. Error bars represent 95% conﬁdence intervals. (a) Words that can only be copied\nfrom long-range context are more sensitive to dropping all the distant words than to dropping the target.\nFor words that can be copied from nearby context, dropping only the target has a much larger effect\non loss compared to dropping the long-range context. (b) Replacing the target word with other tokens\nfrom vocabulary hurts more than dropping it from the context, for words that can be copied from nearby\ncontext, but has no effect on words that can only be copied from far away.\nof target word occurrences in context, we experi-\nment with dropping all the distant context versus\ndropping only occurrences of the target word from\nthe context. In particular, we compare removing\nall tokens after the 50 most recent tokens, (Equa-\ntion 1 with n = 50 ), versus removing only the\ntarget word, in context of size n= 300:\nδdrop(wt−1,...,w t−n) =\nfword(wt,(wt−1,...,w t−n)), (4)\nwhere fword(w,span) drops words equal to win a\ngiven span. We compare applying both perturba-\ntions to a baseline model with unperturbed context\nrestricted to n = 300. We also include the target\nwords that never appear in the context (Cnone) as a\ncontrol set for this experiment.\nThe results show that LSTMs rely on the rough\nsemantic representation of the faraway context to\ngenerate Cfar, but direclty copy Cnear from the\nnearby context. In Figure 4a, the long-range con-\ntext bars show that for words that can only be\ncopied from long-range context ( Cfar), removing\nall distant context is far more disruptive than re-\nmoving only occurrences of the target word (12%\nand 2% increase in perplexity, respectively). This\nsuggests that the model relies more on the rough\nsemantic representation of faraway context to pre-\ndict these Cfar tokens, rather than directly copy-\ning them from the distant context. On the other\nhand, for words that can be copied from nearby\ncontext ( Cnear), removing all long-range context\nhas a smaller effect (about 3.5% increase in per-\nplexity) as seen in Figure 4a, compared to remov-\ning the target word which increases perplexity by\nalmost 9%. This suggests that these Cnear tokens\nare more often copied from nearby context, than\ninferred from information found in the rough se-\nmantic representation of long-range context.\nHowever, is it possible that dropping the tar-\nget tokens altogether, hurts the model too much\nby adversely affecting grammaticality of the con-\ntext? We test this theory by replacing target words\nin the context with other words from the vocab-\nulary. This perturbation is similar to Equation 4,\nexcept instead of dropping the token, we replace\nit with a different one. In particular, we exper-\niment with replacing the target with <unk>, to\nsee if having the generic word is better than not\nhaving any word. We also replace it with a word\nthat has the same part-of-speech tag and a simi-\nlar frequency in the dataset, to observe how much\nthis change confuses the model. Figure 4b shows\nthat replacing the target with other words results\nin up to a 14% increase in perplexity for Cnear,\nwhich suggests that the replacement token seems\nto confuse the model far more than when the to-\nken is simply dropped. However, the words that\nrely on the long-range context, Cfar, are largely\nunaffected by these changes, which conﬁrms our\nconclusion from dropping the target tokens: Cfar\nwitnesses in the morris ﬁlm </s> served up as a solo however the music lacks the UNK provided by a context within another\nmedium </s> UNK of mr. glass may agree with the critic richard UNK 's sense that the NUM music in twelve parts is as UNK and\nUNK as the UNK UNK </s> but while making the obvious point that both UNK develop variations from themes this comparison\nUNK the intensely UNK nature of mr. glass\n</s> snack-food UNK increased a strong NUM NUM in the third quarter while domestic proﬁt increased in double UNK mr.\ncalloway said </s> excluding the british snack-food business acquired in july snack-food international UNK jumped NUM NUM\nwith sales strong in spain mexico and brazil </s> total snack-food proﬁt rose NUM NUM </s> led by pizza hut and UNK bell\nrestaurant earnings increased about NUM NUM in the third quarter on a NUM NUM sales increase </s> UNK sales for pizza hut\nrose about NUM NUM while UNK bell 's increased NUM NUM as the chain continues to beneﬁt from its UNK strategy </s> UNK\nbell has turned around declining customer counts by permanently lowering the price of its UNK </s> same UNK for kentucky fried\nchicken which has struggled with increased competition in the fast-food chicken market and a lack of new products rose only NUM\nNUM </s> the operation which has been slow to respond to consumers ' shifting UNK away from fried foods has been developing a\nUNK product that may be introduced nationally at the end of next year </s> the new product has performed well in a market test in\nlas vegas nev. mr. calloway\nsend a delegation of congressional staffers to poland to assist its legislature the UNK in democratic procedures </s> senator pete\nUNK calls this effort the ﬁrst gift of democracy </s> the poles might do better to view it as a UNK horse </s> it is the vast shadow\ngovernment of NUM congressional staffers that helps create such legislative UNK as the NUM page UNK reconciliation bill that\nclaimed to be the budget of the united states </s> maybe after the staffers explain their work to the poles they 'd be willing to come\nback and do the same for the american people </s> UNK UNK plc a ﬁnancially troubled irish maker of ﬁne crystal and UNK china\nreported that its pretax loss for the ﬁrst six months widened to NUM million irish punts $ NUM million from NUM million irish\npunts a year earlier </s> the results for the half were worse than market expectations which suggested an interim loss of around\nNUM million irish punts </s> in a sharply weaker london market yesterday UNK shares were down NUM pence at NUM pence\nNUM cents </s> the company reported a loss after taxation and minority interests of NUM million irish\nsim has set a fresh target of $ NUM a share by the end of </s> reaching that goal says robert t. UNK applied 's chief ﬁnancial ofﬁcer\nwill require efﬁcient reinvestment of cash by applied and UNK of its healthy NUM NUM rate of return on operating capital </s> in\nbarry wright mr. sim sees a situation very similar to the one he faced when he joined applied as president and chief operating ofﬁcer\nin NUM </s> applied then a closely held company was UNK under the management of its controlling family </s> while proﬁtable it\nwas n't growing and was n't providing a satisfactory return on invested capital he says </s> mr. sim is conﬁdent that the drive to\ndominate certain niche markets will work at barry wright as it has at applied </s> he also UNK an UNK UNK to develop a corporate\nculture that rewards managers who produce and where UNK is shared </s> mr. sim considers the new unit 's operations\nfundamentally sound and adds that barry wright has been fairly successful in moving into markets that have n't interested larger\ncompetitors </s> with a little patience these businesses will perform very UNK mr. sim\nwas openly sympathetic to swapo </s> shortly after that mr. UNK had scott stanley arrested and his UNK conﬁscated </s> mr.\nstanley is on trial over charges that he violated a UNK issued by the south african administrator general earlier this year which made\nit a crime punishable by two years in prison for any person to UNK UNK or UNK the election commission </s> the stanley affair\ndoes n't UNK well for the future of democracy or freedom of anything in namibia when swapo starts running the government </s> to\nthe extent mr. stanley has done anything wrong it may be that he is out of step with the consensus of world intellectuals that the\nUNK guerrillas were above all else the victims of UNK by neighboring south africa </s> swapo has enjoyed favorable western\nmedia treatment ever since the u.n. general assembly declared it the sole UNK representative of namibia 's people in </s> last year\nthe u.s. UNK a peace settlement to remove cuba 's UNK UNK from UNK and hold free and fair elections that would end south africa\n's control of namibia </s> the elections are set for nov. NUM </s> in july mr. stanley\njuly snack-food international UNK jumped NUM NUM with sales strong in spain mexico and brazil </s> total snack-food proﬁt rose\nNUM NUM </s> led by pizza hut and UNK bell restaurant earnings increased about NUM NUM in the third quarter on a NUM\nNUM sales increase </s> UNK sales for pizza hut rose about NUM NUM while UNK bell 's increased NUM NUM as the chain\ncontinues to beneﬁt from its UNK strategy </s> UNK bell has turned around declining customer counts by permanently lowering the\nprice of its UNK </s> same UNK for kentucky fried chicken which has struggled with increased competition in the fast-food\nchicken market and a lack of new products rose only NUM NUM </s> the operation which has been slow to respond to consumers '\nshifting UNK away from fried foods has been developing a UNK product that may be introduced nationally at the end of next year\n</s> the new product has performed well in a market test in las vegas nev. mr. calloway said </s> after a four-year $ NUM billion\nacquisition binge that brought a major soft-drink company soda UNK a fast-food chain and an overseas snack-food giant to pepsi mr.\ncalloway\nof london 's securities traders it was a day that started nervously in the small hours </s> by UNK the selling was at UNK fever </s>\nbut as the day ended in a UNK wall UNK rally the city UNK a sigh of relief </s> so it went yesterday in the trading rooms of london\n's ﬁnancial district </s> in the wake of wall street 's plunge last friday the london market was considered especially vulnerable </s>\nand before the opening of trading here yesterday all eyes were on early trading in tokyo for a clue as to how widespread the fallout\nFigure 5: Success of neural cache on PTB. Brightly shaded region shows peaky distribution.\nmanagement equity participation </s> further many institutions today holding troubled retailers ' debt securities will be UNK to\nconsider additional retailing investments </s> it 's called bad money driving out good money said one retailing UNK </s>\ninstitutions that usually buy retail paper have to be more concerned </s> however the lower prices these retail chains are now\nexpected to bring should make it easier for managers to raise the necessary capital and pay back the resulting debt </s> in addition\nthe fall selling season has generally been a good one especially for those retailers dependent on apparel sales for the majority of their\nrevenues </s> what 's encouraging about this is that retail chains will be sold on the basis of their sales and earnings not liquidation\nvalues said joseph e. brooks chairman and chief\nofferings outside the u.s. </s> goldman sachs & co. will manage the offering </s> macmillan said berlitz intends to pay quarterly\ndividends on the stock </s> the company said it expects to pay the ﬁrst dividend of NUM cents a share in the NUM ﬁrst quarter </s>\nberlitz will borrow an amount equal to its expected net proceeds from the offerings plus $ NUM million in connection with a credit\nagreement with lenders </s> the total borrowing will be about $ NUM million the company said </s> proceeds from the borrowings\nunder the credit agreement will be used to pay an $ NUM million cash dividend to macmillan and to lend the remainder of about $\nNUM million to maxwell communications in connection with a UNK note </s> proceeds from the offering will be used to repay\nborrowings under the short-term parts of a credit agreement </s> berlitz which is based in princeton n.j. provides language\ninstruction and translation services through more than NUM language centers in NUM countries </s> in the past ﬁve years more\nthan NUM NUM of its sales have been outside the u.s. </s> macmillan has owned berlitz since NUM </s> in the ﬁrst six months\nsaid that despite losses on ual stock his ﬁrm 's health is excellent </s> the stock 's decline also has left the ual board in a UNK </s>\nalthough it may not be legally obligated to sell the company if the buy-out group ca n't revive its bid it may have to explore\nalternatives if the buyers come back with a bid much lower than the group 's original $ 300-a-share proposal </s> at a meeting sept.\nNUM to consider the labor-management bid the board also was informed by its investment adviser ﬁrst boston corp. of interest\nexpressed by buy-out funds including kohlberg kravis roberts & co. and UNK little & co. as well as by robert bass morgan stanley 's\nbuy-out fund and pan am corp </s> the takeover-stock traders were hoping that mr. davis or one of the other interested parties might\nUNK with the situation in disarray or that the board might consider a recapitalization </s> meanwhile japanese bankers said they\nwere still UNK about accepting citicorp 's latest proposal </s> macmillan inc. said it plans a public offering of NUM million shares\nof its berlitz international inc. unit at $ NUM to $ NUM a share\ncapital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we are doing it so we\ncan concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executive vice president\n</s> we are only going to sell at the right price </s> hertz equipment had operating proﬁt before depreciation of $ NUM million on\nrevenue of $ NUM million in NUM </s> the closely held hertz corp. had annual revenue of close to $ NUM billion in NUM of\nwhich $ NUM billion was contributed by its hertz rent a car operations world-wide </s> hertz equipment is a major supplier of rental\nequipment in the u.s. france spain and the UNK </s> it supplies commercial and industrial equipment including UNK UNK UNK\nand electrical equipment UNK UNK UNK and trucks </s> UNK inc. reported a net loss of $ NUM million for the ﬁscal third quarter\nended aug. NUM </s> it said the loss resulted from UNK and introduction costs related to a new medical UNK equipment system\n</s> in the year-earlier quarter the company reported net income of $ NUM or\nacquisition of nine businesses that make up the group the biggest portion of which was related to the NUM purchase of a UNK co.\nunit </s> among other things the restructured facilities will substantially reduce the group 's required amortization of the term loan\nportion of the credit facilities through september NUM mlx said </s> certain details of the restructured facilities remain to be\nnegotiated </s> the agreement is subject to completion of a deﬁnitive amendment and appropriate approvals </s> william p. UNK\nmlx chairman and chief executive said the pact will provide mlx with the additional time and ﬂexibility necessary to complete the\nrestructuring of the company 's capital structure </s> mlx has ﬁled a registration statement with the securities and exchange\ncommission covering a proposed offering of $ NUM million in long-term senior subordinated notes and warrants </s> dow jones &\nco. said it acquired a NUM NUM interest in UNK corp. a subsidiary of oklahoma publishing co. oklahoma city that provides\nelectronic research services </s> terms were n't disclosed </s> customers of either UNK or dow jones UNK are able to access the\ninformation on both services </s> dow jones is the publisher of the wall street\nvideo games electronic information systems and playing cards posted a NUM NUM unconsolidated surge in pretax proﬁt to NUM\nbillion yen $ NUM million from NUM billion yen $ NUM million for the ﬁscal year ended aug. NUM </s> sales surged NUM NUM\nto NUM billion yen from NUM billion </s> net income rose NUM NUM to NUM billion yen from NUM billion </s> UNK net fell\nto NUM yen from NUM yen because of expenses and capital adjustments </s> without detailing speciﬁc product UNK UNK\ncredited its bullish UNK in sales including advanced computer games and television entertainment systems to surging UNK sales in\nforeign markets </s> export sales for leisure items alone for instance totaled NUM billion yen in the NUM months up from NUM\nbillion in the previous ﬁscal year </s> domestic leisure sales however were lower </s> hertz corp. of park UNK n.j. said it retained\nmerrill lynch capital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we are\ndoing it so we can concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executive\nvice president\nso-called road show to market the package around the world </s> an increasing number of banks appear to be considering the option\nFigure 6: Failure of neural cache on PTB. Lightly shaded regions show ﬂat distribution.\nwords are predicted from the rough representation\nof faraway context instead of speciﬁc occurrences\nof certain words.\n6.2 How does the cache help?\nIf LSTMs can already regenerate words from\nnearby context, how are copy mechanisms help-\ning the model? We answer this question by ana-\nlyzing how the neural cache model (Grave et al.,\n2017b) helps with improving model performance.\nThe cache records the hidden state ht at each\ntimestep t, and computes a cache distribution over\nthe words in the history as follows:\nPcache(wt|wt−1,...,w 1; ht,...,h 1)\n∝\nt−1∑\ni=1\n1 [wi = wt] exp(θhT\ni ht),\n(5)\nwhere θ controls the ﬂatness of the distribution.\nThis cache distribution is then interpolated with\nthe model’s output distribution over the vocabu-\nlary. Consequently, certain words from the history\nare upweighted, encouraging the model to copy\nthem.\nCaches help words that can be copied from\nlong-range context the most. In order to study\nthe effectiveness of the cache for the three\nclasses of words ( Cnear,Cfar,Cnone), we evaluate\nan LSTM language model with and without a\ncache, and measure the difference in perplexity for\nthese words. In both settings, the model is pro-\nvided all prior context (not just 300 tokens) in or-\nnearby long-range none\nFirst occurrence of target in context\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nIncrease in Loss\nDataset = PTB, Cache size = 500 words\nnearby long-range none\nFirst occurrence of target in context\nDataset = Wiki, Cache size = 3,785 words\nFigure 7: Model performance relative to using a\ncache. Error bars represent 95% conﬁdence inter-\nvals. Words that can only be copied from the dis-\ntant context beneﬁt the most from using a cache.\nder to replicate the Grave et al. (2017b) setup. The\namount of history recorded, known as the cache\nsize, is a hyperparameter set to 500 past timesteps\nfor PTB and 3,875 for Wiki, both values very sim-\nilar to the average document lengths in the respec-\ntive datasets.\nWe ﬁnd that the cache helps words that can\nonly be copied from long-range context ( Cfar)\nmore than words that can be copied from nearby\n(Cnear). This is illustrated by Figure 7 where with-\nout caching, Cnear words see a 22% increase in\nperplexity for PTB, and a 32% increase for Wiki,\nwhereas Cfar see a 28% increase in perplexity\nfor PTB, and a whopping 53% increase for Wiki.\nThus, the cache is, in a sense, complementary to\nthe standard model, since it especially helps regen-\nerate words from the long-range context where the\nlatter falls short.\nHowever, the cache also hurts about 36% of\nthe words in PTB and 20% in Wiki, which are\nwords that cannot be copied from context (Cnone),\nas illustrated by bars for “none” in Figure 7. We\nalso provide some case studies showing success\n(Fig. 5) and failure (Fig. 6) modes for the cache.\nWe ﬁnd that for the successful case, the cache\ndistribution is concentrated on a single word that\nit wants to copy. However, when the target is\nnot present in the history, the cache distribution\nis more ﬂat, illustrating the model’s confusion,\nshown in Figure 6. This suggests that the neural\ncache model might beneﬁt from having the option\nto ignore the cache when it cannot make a conﬁ-\ndent choice.\n7 Discussion\nThe ﬁndings presented in this paper provide a\ngreat deal of insight into how LSTMs model con-\ntext. This information can prove extremely use-\nful for improving language models. For instance,\nthe discovery that some word types are more im-\nportant than others can help reﬁne word dropout\nstrategies by making them adaptive to the different\nword types. Results on the cache also show that\nwe can further improve performance by allowing\nthe model to ignore the cache distribution when it\nis extremely uncertain, such as in Figure 6. Dif-\nferences in nearby vs. long-range context suggest\nthat memory models, which feed explicit context\nrepresentations to the LSTM (Ghosh et al., 2016;\nLau et al., 2017), could beneﬁt from representa-\ntions that speciﬁcally capture information orthog-\nonal to that modeled by the LSTM.\nIn addition, the empirical methods used in this\nstudy are model-agnostic and can generalize to\nmodels other than the standard LSTM. This opens\nthe path to generating a stronger understanding of\nmodel classes beyond test set perplexities, by com-\nparing them across additional axes of information\nsuch as how much context they use on average, or\nhow robust they are to shufﬂed contexts.\nGiven the empirical nature of this study and the\nfact that the model and data are tightly coupled,\nseparating model behavior from language charac-\nteristics, has proved challenging. More speciﬁ-\ncally, a number of confounding factors such as vo-\ncabulary size, dataset size etc. make this separa-\ntion difﬁcult. In an attempt to address this, we\nhave chosen PTB and Wiki - two standard lan-\nguage modeling datasets which are diverse in con-\ntent (news vs. factual articles) and writing style,\nand are structured differently (eg: Wiki articles are\n4-6x longer on average and contain extra informa-\ntion such as titles and paragraph/section markers).\nMaking the data sources diverse in nature, has pro-\nvided the opportunity to somewhat isolate effects\nof the model, while ensuring consistency in re-\nsults. An interesting extension to further study this\nseparation would lie in experimenting with differ-\nent model classes and even different languages.\nRecently, Chelba et al. (2017), in proposing a\nnew model, showed that on PTB, an LSTM lan-\nguage model with 13 tokens of context is similar\nto the inﬁnite-context LSTM performance, with\nclose to an 8% 5 increase in perplexity. This is\ncompared to a 25% increase at 13 tokens of con-\ntext in our setup. We believe this difference is\nattributed to the fact that their model was trained\nwith restricted context and a different error propa-\ngation scheme, while ours is not. Further investi-\ngation would be an interesting direction for future\nwork.\n8 Conclusion\nIn this analytic study, we have empirically shown\nthat a standard LSTM language model can effec-\ntively use about 200 tokens of context on two\nbenchmark datasets, regardless of hyperparame-\nter settings such as model size. It is sensitive to\nword order in the nearby context, but less so in\nthe long-range context. In addition, the model is\nable to regenerate words from nearby context, but\nheavily relies on caches to copy words from far\naway. These ﬁndings not only help us better un-\nderstand these models but also suggest ways for\nimproving them, as discussed in Section 7. While\nobservations in this paper are reported at the to-\nken level, deeper understanding of sentence-level\ninteractions warrants further investigation, which\nwe leave to future work.\nAcknowledgments\nWe thank Arun Chaganty, Kevin Clark, Reid\nPryzant, Yuhao Zhang and our anonymous review-\ners for their thoughtful comments and suggestions.\nWe gratefully acknowledge support of the DARPA\nCommunicating with Computers (CwC) program\nunder ARO prime contract no . W911NF15-1-\n0462 and the NSF via grant IIS-1514268.\n5Table 3, 91 perplexity for the 13-gram vs. 84 for the\ninﬁnite context model.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov,\nOfer Lavi, and Yoav Goldberg. 2017. Fine-\ngrained analysis of sentence embeddings using\nauxiliary prediction tasks. International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=BJh6Ztuxl.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. International\nConference on Learning Representations (ICLR)\nhttps://arxiv.org/pdf/1409.0473.pdf.\nJordan Boyd-Graber and David Blei. 2009. Syn-\ntactic topic models. In Advances in neu-\nral information processing systems . pages 185–\n192. https://papers.nips.cc/paper/3398-syntactic-\ntopic-models.pdf.\nCiprian Chelba, Mohammad Norouzi, and Samy\nBengio. 2017. N-gram language model-\ning using recurrent neural network esti-\nmation. arXiv preprint arXiv:1703.10724\nhttps://arxiv.org/pdf/1703.10724.pdf.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling\nwith gated convolutional networks. Interna-\ntional Conference on Machine Learning (ICML)\nhttps://arxiv.org/pdf/1612.08083.pdf.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural informa-\ntion processing systems (NIPS) . pages 1019–1027.\nhttps://arxiv.org/pdf/1512.05287.pdf.\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016. Contextual lstm\n(clstm) models for large scale nlp tasks. Work-\nshop on Large-scale Deep Learning for Data Min-\ning, KDD https://arxiv.org/pdf/1602.06291.pdf.\nEdouard Grave, Moustapha M Cisse, and Ar-\nmand Joulin. 2017a. Unbounded cache model\nfor online language modeling with open vo-\ncabulary. In Advances in Neural Information\nProcessing Systems (NIPS) . pages 6044–6054.\nhttps://papers.nips.cc/paper/7185-unbounded-\ncache-model-for-online-language-modeling-with-\nopen-vocabulary.pdf.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving Neural Language Mod-\nels with a Continuous Cache. International\nConference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=B184E5qee.\nAlex Graves. 2013. Generating se-\nquences with recurrent neural net-\nworks. arXiv preprint arXiv:1308.0850\nhttps://arxiv.org/pdf/1308.0850.pdf.\nFelix Hill, Antoine Bordes, Sumit Chopra, and\nJason Weston. 2016. The goldilocks princi-\nple: Reading children’s books with explicit\nmemory representations. International Con-\nference on Learning Representations (ICLR)\nhttps://arxiv.org/pdf/1511.02301.pdf.\nSepp Hochreiter and J ¨urgen Schmidhu-\nber. 1997. Long short-term memory.\nNeural computation 9(8):1735–1780.\nhttps://doi.org/10.1162/neco.1997.9.8.1735.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. Inter-\nnational Conference on Learning Representations\n(ICLR) https://openreview.net/pdf?id=r1aPbsFle.\nRafal Jozefowicz, Oriol Vinyals, Mike Schus-\nter, Noam Shazeer, and Yonghui Wu. 2016.\nExploring the limits of language mod-\neling. arXiv preprint arXiv:1602.02410\nhttps://arxiv.org/pdf/1602.02410.pdf.\nJey Han Lau, Timothy Baldwin, and Trevor Cohn.\n2017. Topically Driven Neural Language Model.\nAssociation for Computational Linguistics (ACL)\nhttps://doi.org/10.18653/v1/P17-1033.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan\nJurafsky. 2016. Visualizing and understanding\nneural models in nlp. North American As-\nsociation of Computational Linguistics (NAACL)\nhttp://www.aclweb.org/anthology/N16-1082.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics (TACL)\nhttp://aclweb.org/anthology/Q16-1037.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural lan-\nguage processing toolkit. In Proceedings of 52nd\nannual meeting of the association for computational\nlinguistics: system demonstrations . pages 55–60.\nhttps://doi.org/10.3115/v1/P14-5010.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large\nannotated corpus of english: The penn tree-\nbank. Computational linguistics 19(2):313–330.\nhttp://aclweb.org/anthology/J93-2004.\nGabor Melis, Chris Dyer, and Phil Blunsom.\n2018. On the State of the Art of Evalua-\ntion in Neural Language Models. International\nConference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=ByJHuTgA-.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and Optimizing\nLSTM Language Models. International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=SyyGPP0TZ.\nStephen Merity, Caiming Xiong, James Brad-\nbury, and Richard Socher. 2017. Pointer\nSentinel Mixture Models. International Con-\nference on Learning Representations (ICLR)\nhttps://openreview.net/pdf?id=Byj72udxe.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. European\nChapter of the Association for Computational Lin-\nguistics http://aclweb.org/anthology/E17-2025.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,\nand Rob Fergus. 2013. Regularization of neural\nnetworks using dropconnect. In International Con-\nference on Machine Learning (ICML). pages 1058–\n1066.\nTian Wang and Kyunghyun Cho. 2016. Larger-Context\nLanguage Modelling with Recurrent Neural Net-\nwork. Association for Computational Linguistics\n(ACL) https://doi.org/10.18653/v1/P16-1125.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax\nbottleneck: a high-rank rnn language model. Inter-\nnational Conference on Learning Representations\n(ICLR) https://openreview.net/pdf?id=HkwZSG-\nCZ.\nHyperparameter PTB Wiki\nWord Emb. Size 400 400\nHidden State Dim 1150 1150\nLayers 3 3\nOptimizer ASGD ASGD\nLearning Rate 30 30\nGradient clip 0.25 0.25\nEpochs (train) 500 750\nEpochs (ﬁnetune) 500 (max) 750 (max)\nBatch Size 20 80\nSequence Length 70 70\nLSTM Layer Dropout 0.25 0.2\nRecurrent Dropout 0.5 0.5\nWord Emb. Dropout 0.4 0.65\nWord Dropout 0.1 0.1\nFF Layers Dropout 0.4 0.4\nWeight Decay 1.2 × 10−6 1.2 × 10−6\nTable 2: Hyperparameter Settings.\nA Hyperparameter settings\nWe train a vanilla LSTM language model, aug-\nmented with dropout on recurrent connections,\nembedding weights, and all input and output con-\nnections (Wan et al., 2013; Gal and Ghahramani,\n2016), weight tying between the word embedding\nand softmax layers (Inan et al., 2017; Press and\nWolf, 2017), variable length backpropagation se-\nquences and the averaging SGD optimizer (Merity\net al., 2018). We provide the key hyperparameter\nsettings for the model in Table 2. These are the\ndefault settings suggested by (Merity et al., 2018).\nB Additional Figures\nThis section contains all ﬁgures complementary to\nthose presented in the main text. Some ﬁgures,\nsuch as Figures 1b, 1d etc. present results for only\none of the two datasets, and we present the re-\nsults for the other dataset here. It is important to\nnote that the analysis and conclusions remain un-\nchanged. Just as before, all results are averaged\nfrom three models trained with different random\nseeds. Error bars on curves represent the standard\ndeviation and those on bar charts represent 95%\nconﬁdence intervals.\n1 5 10 15 20 30 50 100 200\nDistance of perturbation from target (number of tokens)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Increase in Loss\nShuffle entire context\nReverse entire context\nReplace context with rand sequence\nFigure 8: Complementary to Figure 2b. Perturb\nglobal order, i.e. all tokens in the context before\na given point, in PTB. Effects of shufﬂing and re-\nversing the order of words in 300 tokens of con-\ntext, relative to an unperturbed baseline. Changing\nthe global order of words within the context does\nnot affect loss beyond 50 tokens.\n5 20 100\nDistance of perturbation from target (number of tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Increase in Loss\nDrop all content words (54.4%)\nDrop all function words (45.6%)\nRandom drop 54.4% tokens of 300\nRandom drop 45.6% tokens of 300\nFigure 9: Complementary to Figure 3. Effect of\ndropping content and function words from 300 to-\nkens of context relative to an unperturbed base-\nline, on Wiki. Dropping both content and func-\ntion words 5 tokens away from the target results\nin a nontrivial increase in loss, whereas beyond 20\ntokens, content words are far more relevant.\n10 50 100 200 500 1000\nContext Size (number of tokens)\n70\n80\n90\n100\n110\n120Perplexity\nDefault Model, Wiki\nLSTM Hidden 575 (vs. 1150)\n2 layers (vs. 3)\nWord Emb 200 (vs. 400)\nNo LSTM layer dropout (vs. 0.2)\nNo recurrent dropout (vs. 0.5)\nBPTT 100 (vs. 70)\nBPTT 10 (vs. 70)\n(a) Changing model hyperparameters for Wiki.\n5 10 20 50 100 200 500 1000\nContext Size (number of tokens)\n0.0\n0.2\n0.4\n0.6\n0.8Increase in Loss\nNN,PTB\nJJ,PTB\nVB,PTB\nIN,PTB\nDT,PTB (b) Different parts-of-speech for PTB.\nFigure 10: Complementary to Figures 1b and 1d, respectively. Effects of varying the number of tokens\nprovided in the context, as compared to the same model provided with inﬁnite context. Increase in loss\nrepresents an absolute increase in NLL over the entire corpus, due to restricted context. (a) Changing\nmodel hyperparameters does not change the context usage trend, but does change model performance.\nWe report perplexities to highlight the consistent trend. (b) Content words need more context than\nfunction words.\nnearby long-range none (control set)\nFirst occurrence of target in context\n0.00\n0.05\n0.10\n0.15\n0.20Increase in Loss\nDrop 250 most distant tokens\nDrop only target\n(a) Dropping tokens\nnearby long-range\nFirst occurrence of target in context\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Increase in Loss\nDrop only target\nReplace target with <unk>\nReplace target with similar (b) Perturbing occurrences of target word in context.\nFigure 11: Complementary to Figure 4. Effects of perturbing the target word in the context compared\nto dropping long-range context altogether, on Wiki. (a) Words that can only be copied from long-range\ncontext are more sensitive to dropping all the distant words than to dropping the target. For words that\ncan be copied from nearby context, dropping only the target has a much larger effect on loss compared to\ndropping the long-range context. (b) Replacing the target word with other tokens from vocabulary hurts\nmore than dropping it from the context, for words that can be copied from nearby context, but has no\neffect on words that can only be copied from far away.\nCurrently displaying: attn_vis_data.json\nText span\nRichmond 's father ﬁred him as the driver . The next season , Al Richmond bought a UNK better suited to his son 's driving style . In\n1977 Tim Richmond became both UNK UNK 's Rookie of the Year and the UNK class track champion . </s> Richmond returned to\nracing sprint cars in the United States Automobile Club 's ( UNK ) national sprint car tour in 1978 . UNK in 12 races , he ﬁnished\n30th in points as the series ' Rookie of the Year . That year he attended Jim Russell 's road racing school at UNK Springs\nInternational UNK Park , setting a student course record . Richmond raced in a 1978 Mini UNK car event at Phoenix International\nUNK , winning the Formula Super UNK support event in a UNK UNK . The win attracted sponsors and attention from major\nowners like Roger UNK . He also competed in UNK 's Silver Crown series . </s> Richmond 's father bought an Eagle UNK Car\nchassis and an UNK engine for the 1979 race at Michigan International UNK . Richmond qualiﬁed 21st fastest with a 175 @.@\nUNK mph ( UNK @.@ UNK km\n) . Standing roughly 15 metres ( 49 ft ) away , the cadres now raised their weapons . \" You have taken our land , \" one of them said .\n\" Please don 't shoot us ! \" one of the passengers cried , just before they were killed by a sustained burst of automatic gunﬁre . </s>\nHaving collected water from the nearby village , UNK and his companions were almost back at the crash site when they heard the\nshots . UNK it was personal ammunition in the luggage exploding in the heat , they continued on their way , and called out to the\nother passengers , who they thought were still alive . This alerted the insurgents to the presence of more survivors ; one of the\nguerrillas told UNK 's group to \" come here \" . The insurgents then opened ﬁre on their general location , prompting UNK and the\nothers to ﬂee . Hill and the UNK also ran ; they revealed their positions to the ﬁghters in their UNK , but successfully hid themselves\nbehind a ridge . After Hill and the others had hidden there for about two hours\nfor shooting the video . In the background of the video is a sign for The Million Dollar Hotel , which was rebuilt to create some\ninterest , in case no one showed up at the ﬁlm shoot . Although the video is of a live performance , the audio used is from the studio\n@-@ recorded version of the song . The video won the Grammy Award for Best Performance Music Video at the 1989 Grammy\nAwards . </s> </s> = = = B @-@ sides = = = </s> </s> \" Race Against Time \" was released on the 12 @-@ inch , cassette , and CD\nversions of the single . The song developed from the band 's interest in urban funk , and was described by The Edge as \" a kind of\nAfro @-@ rhythmic piece \" and \" a study in rhythm . \" The bass riff in the song , inspired by the UNK , was played by The Edge ,\nbut stemmed from some of Clayton 's unused bass parts . Mullen 's drum part was recorded in a single take . The song is primarily an\ninstrumental piece but does contain\n</s> </s> The ﬁlm received several award nominations , including a UNK Award for Best Supporting Actor for Colin Farrell , and\nthree nominations from the 2012 Comedy Awards , including Comedy Actor for Bateman , Comedy Actress for Aniston , and best\nComedy Film . Farrell and Aniston were both nominated for Best On @-@ Screen Dirt UNK at the 2012 MTV Movie Awards , with\nAniston claiming the award . Farrell also received a nomination for Best On @-@ Screen UNK . </s> </s> = = = Home media = = =\n</s> </s> On July 26 , 2011 , UNK obtained the rights to the network premiere of the ﬁlm . </s> UNK UNK was released on DVD\nand Blu @-@ ray Disc in the United States on October 11 , 2011 . The DVD version sold an estimated 400 @,@ 682 units in the\nUnited States during its ﬁrst week , earning approximately $ 6 @.@ 1 million . It was the number 2 best selling DVD of the week ,\nﬁnishing behind Green Lantern , and the number 3 Blu @-@ ray disc ﬁlm behind Green Lantern and The Lion King . As of\nNovember\n, too deep for troops to cross even without the prospect of enemy opposition . He considered using boats to ferry the troops across ,\nbut the Americans , with timely advice from General Lee , adopted a strong defensive position that was virtually impossible to\nbombard from ships or the Long Island position . As a result , the British and American forces faced each other across the channel ,\nengaging in occasional and largely UNK cannon ﬁre at long range . Clinton reported that this meant that Admiral Parker would have\n\" the glory of being defeated alone . \" The attack was originally planned for June 24 , but bad weather and contrary wind conditions\nprompted Parker to call it off for several days . </s> </s> = = Battle = = </s> </s> On the morning of June 28 , Fort Sullivan was\ndefended by Colonel UNK , commanding the 2nd South Carolina Regiment and a company of the 4th South Carolina Artillery ,\nnumbering 435 men . At around 9 : 00 am that morning , a British ship ﬁred a signal gun indicating all was ready for the attack .\nLess than an hour\n\" UNK \" into the mix . Alexis UNK from The Guardian called the song as triumphant . Kitty Empire from the same newspaper said\nthat \" ' UNK ' sees Madonna taking a lover to task over an UNK dance @-@ pop rush . \" Alan Light from Rolling Stone called the\nsong \" UNK \" . Thomas UNK from UNK magazine commented that \" UNK \" and ﬁrst single \" UNK Up \" may not be as UNK like\nMadonna 's initial singles \" Burning Up \" ( 1984 ) or \" Physical UNK \" ( 1984 ) , but they have the same UNK UNK of being\ndesigned for all @-@ night dancing . </s> </s> = = Chart performance = = </s> </s> In the United States , \" UNK \" debuted at\nnumber 70 on the Billboard Hot 100 chart for the issue dated March 11 , 2006 and reached a peak of 58 the following week . The\nsame week it reached a peak of 46 on the Pop 100 chart . Its low chart performance in America was attributed to limited radio\nFigure 12: Failure of neural cache on Wiki. Lightly shaded regions show ﬂat distribution.\nCurrently displaying: attn_vis_data.json\nText span\nLa Fortuna , Mexico . UNK just off the coast of Mexico , the system interacted with land and began weakening . UNK later ,\nconvection rapidly diminished as dry air became entrained in the circulation . In response to quick degradation of the system 's\nstructure , the NHC downgraded UNK to a tropical storm . Rapid weakening continued throughout the day and by the evening hours\n, the storm no longer had a deﬁned circulation . Lacking an organized center and deep convection , the ﬁnal advisory was issued on\nUNK . The storm 's remnants persisted for several more hours before dissipating roughly 175 mi ( 280 km ) southwest of Cabo\nCorrientes , Mexico . </s> </s> = = Preparations and impact = = </s> </s> Following the classiﬁcation of Tropical Depression Two\n@-@ E on June 19 , the Government of Mexico issued a tropical storm warning for coastal areas between UNK and Manzanillo . A\nhurricane watch was also put in place from UNK de UNK to Punta San UNK . Later that day , the tropical storm warning was\nupgraded to a hurricane warning and the watch was extended westward to La Fortuna\nthe Lamar Hotel was adapted for use as a county annex building . In 1988 it was listed as a Mississippi Landmark . </s> The UNK\nYoung Hotel was built in 1931 . A staple in the African @-@ American business district that developed west of the city 's core , the\nhotel was one of the only places in the city during the years of segregation where a traveling African American could ﬁnd a room .\n</s> As the city suburbs developed in the 1960s and ' 70s , most hotels moved outside of downtown . UNK of the Riley Center in\n2006 has increased demand and a push for a new downtown hotel . The UNK Building has been proposed for redevelopment for this\npurpose , but restoration efforts stalled with a change in city administrations . The UNK Preservation Society was formed in 2013 to\nraise public awareness and support for the building 's renovation , featuring tours of the ﬁrst ﬂoor and anniversary events . </s> </s>\n= = = Historic districts = = = </s> </s> Meridian has nine historic districts that are listed on the National Register of Historic Places .\nThe Meridian Downtown\nCorner . It was later replaced by M @-@ 48 in 1926 . </s> </s> = = = Current designation = = = </s> </s> The current M @-@ 82\ndates back to 1926 . It ran from US 31 in Hart to the northern junction of US 131 and M @-@ 46 in Howard City . The highway was\nrouted through Ferry , UNK and UNK , replacing M @-@ 41 . In late 1936 , M @-@ 46 was extended along the section between\nUNK and Howard City , forming a M @-@ 46 / M @-@ 82 concurrency to ﬁll a gap in the M @-@ 46 routing . This concurrent\nsection became just M @-@ 46 in 1938 , UNK M @-@ 82 back to the northern M @-@ 37 junction in UNK . The highway was\nmoved to a new alignment west of Ferry in late 1947 or early 1948 . Instead of heading northwesterly to Hart , it was continued west\nto end in Shelby . </s> Two UNK in 1963 and 1964 rerouted the western end of the highway again . This time it was realigned to run\nfrom UNK to New Era\n50 continues north on Cape May Avenue , passing through developed areas . It leaves Mays Landing and heads into back into forests\n. The route turns to the northeast , passing near the UNK Leaf Lakes residential development , before coming to an interchange with\nUS 322 ( Black Horse Pike ) . </s> Past this interchange , Route 50 UNK to a four @-@ lane divided highway and reaches a full\ninterchange with the Atlantic City Expressway . The route becomes a two @-@ lane undivided road again and continues through\ninhabited areas , crossing into Galloway Township , where there is an intersection with CR UNK . A short distance later , the road\ncrosses New Jersey Transit ’ s Atlantic City Line near the UNK Harbor City Station and enters UNK Harbor City , turning into\nPhiladelphia Avenue . A block after the railroad crossing , Route 50 ends at an intersection with US 30 and CR UNK ( White Horse\nPike ) , with CR UNK continuing north on Philadelphia Avenue at this point . </s> Route 50 is an important route linking the\nAtlantic City Expressway with the Jersey Shore resorts in Cape May\nbeat him at the event due to interference from Kevin Nash . This led to Nash stating that Joe could not beat Booker T in a one @-@\non @-@ one match later in the broadcast . After this segment , Joe announced that Booker T and he would face at Victory Road on\nJuly 13 for the title . On the July 10 episode of Impact ! , Sting proclaimed that he did not know which of the two would win at\nVictory Road , but that he would be there to watch . At the event , Joe beat Booker T till he was bloody , causing several UNK and\nsecurity personnel to try to stop him to no UNK . He was stopped when Sting UNK in the contest by bashing Joe with a baseball bat\n. Booker T then covered Joe for an unofﬁcial pinfall victory that was counted by Booker T 's legitimate wife UNK . The match result\nwas ruled a no @-@ contest , with Joe retaining the title . </s> UNK UNK was joined by Christian Cage and UNK in his feud with\nKurt Angle , who was joined by Team 3D\nroom \" , including Dwight , who learned from UNK that Michael did not put in a recommendation and thus UNK Michael . Jim\nquietly tells Michael that he chose to resign his job and needs to come to terms with both that choice and the fact that life is going to\ngo on at the ofﬁce . UNK then steps out of his meeting and asks Michael for advice for how to run the meeting , saying that he 's a\ngood manager and Dunder Mifﬂin won 't be the same without him . The two make up with a reverse UNK , UNK telling him that he\nshould start enjoying his retirement . Michael returns to his ofﬁce while UNK conducts his meeting in the conference room . </s> In\nanother effort to impress UNK , Jim and Pam bring in UNK , to which UNK reacts positively . While they celebrate getting back on\nhis good side , UNK reveals to the camera that he is in fact indifferent to UNK and was just being polite . </s> </s> = = Production =\nFigure 13: Success of neural cache on Wiki. Brightly shaded region shows peaky distribution.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8078423738479614
    },
    {
      "name": "Computer science",
      "score": 0.7779743671417236
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6946907639503479
    },
    {
      "name": "Treebank",
      "score": 0.5714448094367981
    },
    {
      "name": "Language model",
      "score": 0.5585856437683105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5542939901351929
    },
    {
      "name": "Context model",
      "score": 0.5300583243370056
    },
    {
      "name": "Natural language processing",
      "score": 0.521079421043396
    },
    {
      "name": "Sentence",
      "score": 0.49844908714294434
    },
    {
      "name": "Artificial neural network",
      "score": 0.4359622597694397
    },
    {
      "name": "Word (group theory)",
      "score": 0.4323950409889221
    },
    {
      "name": "Linguistics",
      "score": 0.23252332210540771
    },
    {
      "name": "History",
      "score": 0.14147648215293884
    },
    {
      "name": "Parsing",
      "score": 0.08975169062614441
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    }
  ]
}