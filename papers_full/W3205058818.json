{
  "title": "Meta-learning via Language Model In-context Tuning",
  "url": "https://openalex.org/W3205058818",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101655279",
      "name": "Yanda Chen",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2767776798",
      "name": "Ruiqi Zhong",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2795637056",
      "name": "Sheng Zha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A219814910",
      "name": "George Karypis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105465932",
      "name": "He He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2921156067",
    "https://openalex.org/W3099910226",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2950774882",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2991497298",
    "https://openalex.org/W2971048662",
    "https://openalex.org/W2964316912",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3035172316",
    "https://openalex.org/W2795900505",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W3170978252",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3194309076",
    "https://openalex.org/W2888541716",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3212870931",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W3198002980",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2970678056",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3100609281",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2963303956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4294646197",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3113529090",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3035749055",
    "https://openalex.org/W2951775809",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W2972954809",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W3161024824",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W2969454448",
    "https://openalex.org/W2950323881",
    "https://openalex.org/W4288404646",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W2970697704"
  ],
  "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose \\textit{in-context tuning} (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 719 - 730\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMeta-learning via Language Model In-context Tuning\nYanda Chen1∗ Ruiqi Zhong2 Sheng Zha3 George Karypis3 He He34\n1Columbia University, 2University of California, Berkeley,3AWS AI\n4New York University\nyc3384@columbia.edu, ruiqi-zhong@berkeley.edu,\n{zhasheng, gkarypis, hehea}@amazon.com\nAbstract\nThe goal of meta-learning is to learn to adapt\nto a new task with only a few labeled examples.\nInspired by the recent progress in large lan-\nguage models, we propose in-context tuning\n(ICT), which recasts task adaptation and pre-\ndiction as a simple sequence prediction prob-\nlem: to form the input sequence, we concate-\nnate the task instruction, labeled in-context ex-\namples, and the target input to predict; to meta-\ntrain the model to learn from in-context ex-\namples, we ﬁne-tune a pre-trained language\nmodel (LM) to predict the target label given\nthe input sequence on a collection of tasks.\nWe benchmark our method on two collections\nof text classiﬁcation tasks: LAMA and Bina-\nryClfs. Compared to MAML which adapts the\nmodel through gradient descent, our method\nleverages the inductive bias of pre-trained\nLMs to perform pattern matching, and out-\nperforms MAML by an absolute 6% average\nAUC-ROC score on BinaryClfs, gaining more\nadvantage with increasing model size. Com-\npared to non-ﬁne-tuned in-context learning\n(i.e. prompting a raw LM), in-context tuning\nmeta-trains the model to learn from in-context\nexamples. On BinaryClfs, ICT improves the\naverage AUC-ROC score by an absolute 10%,\nand reduces the variance due to example order-\ning by 6x and example choices by 2x.\n1 Introduction\nFew-shot learning (FSL) refers to a system’s ability\nto quickly adapt to new tasks when very few labeled\nexamples are available for training. FSL is a key\nfeature of human learning (Lake et al., 2016), but\ncurrent machine learning systems often rely on\nlarge amounts of labeled training data (Silver et al.,\n2016; He et al., 2016; Adiwardana et al., 2020).\nRecently, prompting large pre-trained language\nmodels (LMs) for FSL has achieved remarkable\nprogress (Brown et al., 2020; Schick and Schütze,\n∗ Work done during summer internship at AWS AI.\n2021a). LM prompting with in-context learning\nreduces the “task learning and predict” process to\na simple sequence prediction problem. To perform\na new task, Brown et al. (2020) prompt a raw LM\n(i.e., a pre-trained LM not ﬁne-tuned on any labeled\ndata) with the concatenation of the task instruction,\nsome input-output examples, and the target input\nto be predicted on; then they extract the answer\nfrom the LM’s continuation of the concatenated\nsequence (Figure 1 left). For example, to coax the\nmodel into performing sentiment classiﬁcation on\nthe target input “This movie is a waste of time”, we\nprompt the LM with the sequence “I like the movie!\nPositive review? Yes. Horrible Movie! Positive\nreview? No. This movie is a waste of time. Positive\nreview? ___ ”, and predict “positive” if the next\nword is more likely to be “Yes” rather than “No”.\nHowever, raw LMs are not optimized for in-\ncontext FSL during pre-training, and exhibit unde-\nsirable behavior when used for FSL. For example,\nZhao et al. (2021) observed that LMs suffer from\nthe “recency bias”, which assigns higher probabil-\nity to labels that appear closer to the target input.\nAs a result, the accuracy becomes extremely sen-\nsitive to the ordering of the in-context examples.\nPrevious work has also shown that prompting raw\nLMs is often oversensitive to example choices and\ninstruction wording (Schick and Schütze, 2021a;\nJiang et al., 2020; Gao et al., 2021; Liu et al., 2021).\nWe address this weakness through a meta-\nlearning lens and directly ﬁne-tune the LM for\nFSL. Under the meta-learning framework, we meta-\ntrain a model to learn to adapt to new tasks from a\nfew examples on a wide range of tasks, so that it\nlearns to leverage the few-shot examples to adapt\nto new tasks at test time. Since LM prompting\nalready reduces the “task learning and predict” pro-\ncess to a simple sequence prediction problem, we\nmeta-train a LM by directly ﬁne-tuning it to op-\ntimize for this sequence prediction problem on a\nwide range of tasks (Figure 1 left). Since we ﬁne-\n719\nInstructionx1y1 x’ Y’x2y2\nMeta-Update via Gradient Descent\nIn-Context Tuning \nθ:=θ−ΔFew-shot Adaptation via In-context Learning\nMAML  \ny1x1Instruction\ny2x2Instruction\nθ′ :=θ−Δ\ny'x’Instruction\nCalculate losswith θ′ \nMeta-Update: Optimize     to minimize the loss.θFew-shot Adaptation via Gradient Descent\nInstruction: “Is the comment positive?”x1: “Good movie!” y1: “yes”x2: “Bad  movie!”  y2: “no”\nFigure 1: MAML (right): MAML aims to learn a task-agnostic model initialization θ that can adapt fast to new\ntasks. To adapt the model initialization to a new task ˜T, a task-speciﬁc model θ′ initialized with θ is updated\nwith gradient descent using task examples from ˜T. Meta-training of MAML involves bi-level optimization, where\nthe inner optimization learns a task-speciﬁc model θ′ using task examples from ˜T, and the outer optimization\nlearns a meta-initialization θ to minimize few-shot prediction loss of θ′ on task ˜T. In-context Tuning (ours)\n(left): our approach adapts to new tasks via in-context learning, and learns a single model θshared across all tasks\nthat is directly optimized with the FSL objective (Section 2.2). Because model parameters are frozen during task\nadaptation, our approach does not involve bi-level optimization during meta-training.\ntune our model to learn in-context learning, we\ncall our approach in-context tuning(ICT). Unlike\noptimization-based meta learning approaches such\nas MAML (Finn et al., 2017), in-context tuning\nadapts to new tasks through in-context learning\nwhere model parameters are frozen, thus it avoids\nthe challenging nested optimization problem in\nMAML (Figure 1).\nWe benchmark our algorithm on LAMA (Petroni\net al., 2019), a dataset for testing models’ factual\nknowledge, and BinaryClfs (Zhong et al., 2021),\na wide range of binary classiﬁcation tasks each\nannotated with a few language descriptions of the\ntask. Compared to prompting raw LMs, in-context\ntuning improves performance by 7.6 Precision@1\npoints on LAMA and 10.6% AUC-ROC score on\nBinaryClfs. In addition, in-context tuning mitigates\nthe over-sensitivity of raw LM prompting, signiﬁ-\ncantly reducing the variance of the performance\nwith respect to example ordering (by 68% on\nLAMA and 83% on BinaryClfs), example choices\n(by 56% on LAMA and 40% on BinaryClfs), and\ninstruction wording (by 19% on LAMA).\nOur approach also out-performs MAML, which\nadapts the model by gradient descent on a few ex-\namples and learns an initialization that can adapt\nto a new task through a few gradient steps (Finn\net al., 2017; Nichol et al., 2018). Since our ap-\nproach better takes advantage of the inductive bias\nof LMs to extrapolate from in-context examples,\nour approach out-performs ﬁrst-order MAML by\n2.8 points on LAMA and 5.1 points on BinaryClfs,\nwith increasing advantage as models become larger.\nGiven the empirical effectiveness of in-context\ntuning (Section 4.1), we conjecture that the few-\nshot learning potential of large LMs (e.g., GPT-3)\nmay be broadly underestimated if prompted with-\nout any direct optimization for FSL. We also con-\njecture that in-context tuning can mitigate vari-\nous undesirable properties of LM prompting, such\nas over-sensitivity to example ordering, example\nchoices, and instruction wording (Section 4.2).\n2 Approach\nWe introduce the problem setup (Section 2.1), de-\nscribe our in-context tuning algorithm (Section 2.2),\ncompare our algorithm to gradient-based adapta-\ntion methods (Section 2.3) and other baselines (Sec-\ntion 2.4).\n2.1 Problem Setup\nWe focus on the few-shot classiﬁcation problem,\nwhere the model ﬁrst learns from a set of training\ntasks T ∈Ttrain, each associated with its natural\nlanguage instructions IT and a large amount of\ntask input-output examples DT = {(xi\nT,yi\nT)}(see\nFigure 1 left for examples). At test time, we ask the\nmodel to learn a new task ˜T given its instruction\nand only a few ( K) labeled examples, i.e. S˜T ⊆\nD˜T,|S˜T|= K. We denote the task input to be\npredicted at test time as xtarget\n˜T .\nNote that “task input” is different from “model\ninput”. For example, on the left panel of Figure 1,\nthe task input is “Good movie!” while the model\n720\ninput can be a concatenation of the instruction, task\ninputs and task outputs.\n2.2 In-context Tuning Algorithm\nIn-context tuning directly optimizes pre-trained\nLMs with the few-shot in-context learning objec-\ntive (Brown et al., 2020): task-agnostic LMs are\nmeta-trained to perform few-shot in-context learn-\ning on a wide variety of training tasks. Similar to\nin-context learning, LMs trained with in-context\ntuning adapt to a new task by using few-shot train-\ning examples as the input preﬁx.\nFormally, during meta-training, we build the\nmodel input by concatenating the task instruction\nIT, task input-output pairs ST ⊆DT, and the task\ninput xtarget\nT\n1 to be classiﬁed. We then ﬁne-tune a\npre-trained LM to predict ytarget\nT and hope that the\nmodel learns to use the in-context examples ST.\nHere is the few-shot in-context tuning objective L:\nLT(θ) :=\n∑\n(xtgt\nT ,ytgt\nT )∈DT\n[−log pθ(ytgt\nT |xtgt\nT ,ST,IT)]\n(1)\nL(θ) :=\n∑\nT∈Ttrain\nLT(θ) (2)\nTo adapt to a new task ˜T at test time, we di-\nrectly concatenate the few-shot examples S˜T with\nthe instruction I˜T and the target task input xtarget\n˜T\nto be classiﬁed to form the model input, and ask\nthe model to predict its corresponding output. No\ngradient update is performed during adaptation.\n2.3 Gradient-based Task Adaptation\nWe compare in-context tuning with two classical\nfew-shot learning methods: multi-task ﬁne-tuning\n(instruction tuning + ﬁne-tuning) and MAML. Both\nmethods adapt the model parameters to new tasks\nby gradient descent on few-shot examples.\nInstruction Tuning + Fine-tuning (InsT + FT)\nWe extend the recent work on zero-shot instruc-\ntion tuning (Wei et al., 2021) to the FSL setting\nas a multi-task ﬁne-tuning baseline. During meta-\ntraining, the model is optimized to predict the task\noutput given the task instruction and the task in-\nput on a wide range of tasks (Zhong et al., 2021).\nFormally, we train the model parameter θto pre-\ndict yi\nT given IT ◦xi\nT, where θis shared across all\ntasks and ◦represents the concatenation operation.\n1We sometimes abbreviate “target” as “tgt” to save space.\nDuring the few-shot adaptation phase, the model is\npresented with a new task ˜T, its natural language\ninstruction I˜T and a small set of ( K) task input-\noutput examples S˜T = {(xi\n˜T,yi\n˜T)|i ∈[K]}. We\nthen ﬁne-tune the model to predict the task output\nyi\n˜T from the new task given I˜T ◦xi\n˜T and update θ\nwith a few gradient steps to get θ˜T. Finally, we use\nthe updated model θ˜T to predict the output from\nthe task input xtarget\n˜T and the instruction I˜T under\nthe test task ˜T.\nMAML The few-shot adaptation stage of\nMAML is the same as instruction tuning + ﬁne-\ntuning, where we update the model parameters (ini-\ntialized with θ) by gradient descent on Kexamples\nS˜T ⊆D˜T. However, during meta-training, MAML\naims to learn a task-agnostic model initialization\nθsuch that, θT, which is to be found by initializ-\ning with θand performing gradient descent on ST,\nwould lead to good performance (Finn et al., 2017).\nTherefore, MAML involves two levels of opti-\nmization, an inner optimization to learn θT given θ\nand ST ⊆DT, and an outer optimization to learn\nθgiven θT. Due to the bi-level structure in this op-\ntimization problem, MAML has been found to be\nempirically unstable, sensitive to hyperparameters,\nand computationally expensive (Finn et al., 2017;\nNikolaev et al., 2020). Even worse, few-shot task\nadaptation is known to be highly sensitive to opti-\nmization hyperparameters (Antoniou et al., 2019),\nwhile a large labeled validation set for hyperpa-\nrameter tuning may not be available under a FSL\nsetting (Perez et al., 2021).\nIn comparison, in-context tuning simpliﬁes the\ntwo-stage process of (1) few-shot task adaptation\nand (2) task-speciﬁc prediction as one sequence\nprediction problem, where task-speciﬁc examples\nare concatenated to the model input to provide in-\nformation about the task. Hence, in-context tun-\ning removes the bi-level optimization during meta-\ntraining, which can be empirically unstable and\nexpensive. Additionally, since model weights are\nfrozen during task adaptation, it is not sensitive to\nadaptation hyperparameters.\n2.4 Other Baselines\nRaw In-context Learning (Raw IC-L) We di-\nrectly evaluate a raw LM on a new task using the\nsame evaluation set-up for in-context tuning, with-\nout ﬁne-tuning the LM on any labeled data.\n721\nMethod Adaptation Meta-train\nIn-context Tuning In-context Few-shot\nMAML Gradient Few-shot\nInsT None Zero-shot\nInsT + FT Gradient Zero-shot\nRaw IC-L In-context LM\nTable 1: We categorize our approach and the baselines\naccording to 1) how the few-shot examples (if any) are\nused for adaptation, and 2) the meta-training objective.\nIns-T refers to instruction tuning.\nInstruction Tuning (InsT) The model learns to\npredict the target output only based on the instruc-\ntion and the target input. Only the instruction\nis available during the adaptation phase, and this\nsetup is also known as zero-shot learning.\nWe categorize all approaches in our paper based\non their meta-training objective and how they use\ntask-speciﬁc examples in Table 1. In-context tuning\nis the only method that directly optimizes the FSL\nobjective without gradient-based adaptation.\n3 Experimental Setup\n3.1 Datasets and Metrics\nWe experiment with two meta-datasets that contain\na wide range of tasks, LAMA and BinaryClfs. Each\ntask is associated with several different natural lan-\nguage descriptions, and we call them instructions\nfor convenience, even though some of them are\nrealized as questions.\nLAMA LA nguage Model Analysis (Petroni\net al., 2019) is a dataset that tests the factual and\ncommonsense knowledge learned by LMs. In our\nexperiments, we use the TREx-UHN portion of\nLAMA (Poerner et al., 2020), which consists of\n(subject, relation, object) triples from Wikidata.\nLAMA is an entity prediction task, where a model\nis asked to predict the object entity given the sub-\nject entity and the relation. In our experiments, we\ntreat one relation as a task as in Perez et al. (2021).\nInitial experiments on LAMA showed that LMs\ntake signiﬁcant advantage of “majority label bias”\n(Zhao et al., 2021), where they assign higher prob-\nability to object entities that have appeared in the\nin-context examples, thus inﬂating the accuracy. To\nreﬂect the improvement due to few-shot learning\nrather than this simple heuristic to copy answers,\nfor all tasks we prune the LAMA dataset so that all\nobject entities appear less than 2.5% of times. Our\nﬁnal ﬁltered LAMA dataset consists of 29 relations\n(tasks) and 12k (subject, relation, object) examples.\nWe use task instructions from two datasets:\nLAMA and LPAQA (Jiang et al., 2020). LAMA\ncontains one task instruction for each task, and the\nauxiliary LPAQA dataset contains on average 10\nadditional instructions for each LAMA task.\nWe use the same evaluation protocol as in\nPetroni et al. (2019): 1) the object entity is pre-\ndicted from a pre-deﬁned vocabulary set of 21k\nwords (each LAMA task is 21k-way classiﬁca-\ntion); 2) we compute mean precision at one (P@1)\nfor each task, and report the average across tasks.\nBecause LAMA does not have an ofﬁcial train-\nvalidation-test split, we use 8-fold cross-validation\nin our experiments. We randomly partition the\n29 tasks into 8 groups of similar sizes. For each\ncross-validation split, we use six groups for train-\ning, one group for validation, and one group for\ntesting. The test sets of the eight folds are disjoint\nand their union is the set of all tasks.\nBinaryClfs This dataset contains a wide range\nof binary classification tasks, and each task can be\ndescribed by 1-4 “yes/no\" questions, which we con-\ncatenate to the input context as instructions. There\nare in total 204 different tasks, and 73 of them are\nused for testing, which include sentiment classi-\nﬁcation, topic classiﬁcation, deﬁnition detection,\nstance classiﬁcation, etc. We use the same eval-\nuation protocol as in Zhong et al. (2021): 1) we\ngroup the tasks by similarity and do not allow train-\ning tasks to be similar to testing tasks; 2) we treat\n“Yes” answer as the positive class and calculate the\nAUC-ROC score for each instruction of each task.\nTo ﬁt model inputs (concatenation of in-context\nexamples and task input to classify) within the max-\nimum context length (1024) of our LMs, we leave\nout ﬁve evaluation tasks where the maximum task\ninput length exceeds 230 BPE tokens. We also\nleave out the spam classiﬁcation task due to its\nsmall test set. BinaryClfs does not come with an\nofﬁcial validation set. To perform hyperparameter\ntuning, for each testing group, we randomly sample\nanother testing group as its validation group.\n3.2 Implementation Details\nArchitecture We use BERT models for LAMA\n(BERT-Base [110M parameters], BERT-Large\n[340M] and DeBERTa-XLarge-V2 [900M]) and\nGPT2 models for BinaryClfs (GPT2-Medium\n[345M] and GPT2-Large [774M]). We use the Hug-\n722\nLAMA BinaryClfs\nBERT-Base BERT-Large DeBERTa-xlarge GPT2-M GPT2-L\n0-S 1-S 2-S 5-S 0-S 1-S 2-S 5-S 0-S 1-S 2-S 5-S 0-S 5-S 0-S 5-S\nRaw IC-L 10.3 8.5 10.8 14.1 12.7 12.1 15.4 18.6 11.2 12.6 20.6 23.7 50.5 57.8 51.0 58.3\nInsT + FT / 17.5 18.6 20.0 / 21.6 22.6 23.9 / 24.7 25.6 27.0 / 67.0 / 69.4\nICT 14.6 16.3 17.6 19.6 18.0 21.6 23.4 24.3 21.9 26.0 27.5 28.8 62.9 67.4 66.3 69.8\nRaw IC-L w/o Ins 1.5 4.9 8.7 12.3 1.4 3.5 7.0 12.5 2.7 13.0 19.5 22.6 / / / /\nICT w/o Ins 7.1 14.6 17.0 18.2 9.3 19.4 19.9 22.9 10.6 23.5 26.0 27.6 / / / /\nTable 2: Few-shot learning accuracy of our in-context tuning approach (ICT) compared to in-context learning\nwith raw LMs (Raw IC-L) and instruction tuning + ﬁne-tuning (InsT + FT). K-S: K-shot learning. GPT2-M:\nGPT2-Medium. GPT2-L: GPT2-Large. Task instructions are used except the last two rows labeled with “w/o Ins”.\nBy deﬁnition, InsT + FT is the same as ICT for 0-S. We only experiment with the no-instruction setting on the\nLAMA dataset. Since we modify the LAMA dataset and BinaryClfs dataset (Section 3.1), the numbers reported\nin our work are not directly comparable to other work.\nLAMA BinaryClfs\nBB BL GPT2-M GPT2-L\nMAML 16.9 21.4 63.3 63.9\nICT 19.6 24.3 67.4 69.8\nTable 3: In-context tuning consistently out-performs\nMAML on both datasets and all model sizes under\nthe 5-shot setting. BB: BERT-Base. BL: BERT-Large.\nGPT2-M: GPT2-Medium. GPT2-L: GPT2-Large.\ngingface implementation (Wolf et al., 2020).\nHyperparameters We select hyperparameters\nbased on few-shot classiﬁcation accuracy on vali-\ndation tasks. Our validation tasks and testing tasks\nare disjoint, so hyperparameter tuning on validation\ntasks does not use extra labeled examples on the\ntesting tasks (Perez et al., 2021). See Appendix A\nfor the hyperparameters we tuned.\nSampling Different instructions and few-shot ex-\nample choices can lead to different predictions\n(Section 2.2). At training time, we expose the\nmodel to diverse task instructions and few-shot\nchoices by randomly sampling task instructions\nand few-shot examples for each target example.\nAt test time, we report the average accuracy\nacross task instructions and few-shot choices.\nSince computing the average across all few-shot\nchoices is intractable (there are combinatorically\nmany distinct few-shot choices), we thus calculate\nthe average accuracy of multiple random samplings\nof few-shot choices as approximation.\n4 Results\nIn-context tuning out-performs MAML and vari-\nous baselines on the two text classiﬁcation meta-\ndatasets (Section 4.1). It also signiﬁcantly reduces\nmodel sensitivity to instruction wording, example\nchoices, and example ordering compared to prompt-\ning raw LMs (Section 4.2).\n4.1 Few-shot Learning Performance\nIn-context tuning improves in-context learning\naccuracy over raw LMs. We compare ICT with\nRaw IC-L in Table 2. In-context tuning consistently\nout-performs raw LM prompting by 7.6 points on\nLAMA and 10.6 points on BinaryClfs (averaged\nacross model size and number of few-shots). As ex-\npected, directly optimizing the few-shot in-context\nlearning objective (Section 2.2) improves the few-\nshot in-context learning accuracy.\nFew-shot examples lead to more effective task\nadaptation. We compare few-shot in-context\ntuning with instruction tuning (equivalent to 0-\nshot ICT) in Table 2. Few-shot in-context tun-\ning consistently out-performs instruction tuning\non both LAMA and BinaryClfs, with increasing\nperformance gains as number of shots increases.\nSpeciﬁcally, we observe that 5-shot in-context tun-\ning out-performs instruction tuning by 6.1 points\non LAMA and 4.0 points on BinaryClfs. Results\nshow that demonstration examples besides task in-\nstructions facilitate more effective task adaptation.\nIn-context tuning better leverages the induc-\ntive bias for pattern matching. By comparing\nMAML (the ﬁrst row of Table 3) to instruction\n723\ntuning (equivalent to 0-shot ICT) of Table 2, we\nsee that MAML out-performs instruction tuning\nin most evaluation settings, which indicates that\nMAML is indeed able to take advantage of the\nfew-shot task examples for task adaptation. How-\never, Table 3 shows that our approach of 5-shot\nin-context tuning out-performs 5-shot MAML con-\nsistently on both datasets with an accuracy gain\nof 2.8 points on LAMA and 5.1 points on Bina-\nryClfs (averaged across model size). We argue that\nin-context tuning out-performs MAML because\nin-context tuning better leverages the existing in-\nductive bias of pre-trained LMs to perform pattern\nmatching with in-context examples.\nWe also compare in-context tuning to the\npipeline of instruction tuning + task-speciﬁc ﬁne-\ntuning (Table 2). Surprisingly, ﬁne-tuning an\ninstruction-tuned model on as few as one task-\nspeciﬁc example signiﬁcantly improves task accu-\nracy, without over-ﬁtting to the few labeled exam-\nples. We observe that instruction tuning + 1-shot\nﬁne-tuning out-performs instruction tuning (equiv-\nalent to 0-shot ICT) by 3.1 points on LAMA (Ta-\nble 2). Our in-context tuning approach performs\ncomparable or better than instruction tuning + ﬁne-\ntuning, with increasing accuracy gains as models\nget bigger (Table 2). For DeBERTa-XLarge-v2\n(the largest models we use in this work), in-context\ntuning out-performs InsT + FT across all numbers\nof shots, achieving an accuracy gain of 1.7 points\non LAMA (averaged across all numbers of shots).\nWe conjecture that in-context tuning will be in-\ncreasingly effective for bigger models that have a\nstronger inductive bias of pattern matching.\nIn-context tuning reduces the need of task in-\nstructions. As coming up with good task instruc-\ntions can be hard (Schick and Schütze, 2021a;\nJiang et al., 2020), we further investigate the ef-\nfectiveness of in-context tuning without task in-\nstructions (Table 2). In-context tuning is effective\nin the no-instruction setting as well, consistently\nout-performing raw in-context learning with no in-\nstructions by an average margin of 9.5 points on\nLAMA. Comparing raw in-context learning with\n(Raw IC-L) and without instructions (Raw IC-L\nw/o Ins) (Table 2), we observe that task instruc-\ntions yield the most signiﬁcant performance gains\nwhen model size is relatively small (+2.5 points on\nBERT-Base, +7.7 points on BERT-Large, only +0.6\npoints on DeBERTa-xlarge). We conjecture that\nsmaller models may be weaker at inferring patterns\nLAMA BinaryClfs\nBB BL GPT2-M GPT2-L\nRaw IC-L 1.82 2.14 9.26 8.84\nICT 0.66 0.61 1.41 1.58\nTable 4: In-context tuning is signiﬁcantly less sensitive\nto example ordering compared to in-context learning\nwith raw LMs.\nfrom in-context examples alone compared to larger\nmodels, which is why instructions yield larger per-\nformance gains on smaller models. On BERT-Base\nand BERT-Large models where task instructions\nare most helpful, in-context tuning reduces the im-\nprovement gain from task instructions from 5.1\npoints (raw in-context learning) to 1.8 points (aver-\naged across BERT-Base and BERT-Large), which\nindicates that in-context tuning reduces the need\nof task instructions compared to raw in-context\nlearning. However, we note that instructions still\nyield performance improvement even if in-context\ntuning is applied.\n4.2 Sensitivity Analysis\nWe analyze the sensitivity of in-context tuning ac-\ncuracy with respect to example ordering, example\nchoices, and instruction wording, and compare it\nwith prompting raw LMs. LetIdenote a random se-\nlection of task instruction, ST a random unordered\nset of few-shot training examples with size K, σa\nrandom permutation of Kexamples. The accuracy\nµis a function of these three random variables, i.e.\nµ: (ST,σ,I ) ↦→[0,1]. We can decompose the to-\ntal variance of µinto its variance w.r.t. each of the\nthree random variables, since they are independent\n(order variance is independent to choice variance\nbecause ST is unordered):\nVarST ,σ,I[µ] = VarI[EST ,σ[µ|I]]  \ninstruction wording variance\n+ EI[VarST [Eσ[µ|I,ST]]]  \nexample choice variance\n+ EI,ST [Varσ[µ|I,ST]]  \nexample order variance\nWe analyze each type of variance below.\nIn-context tuning is signiﬁcantly less sensitive\nto example ordering. We compare the variance\nwith respect to example ordering for in-context\n724\nLAMA BinaryClfs\nBB BL GPT2-M GPT2-L\nRaw IC-L 3.74 6.30 18.52 20.33\nICT 1.78 2.57 11.46 11.62\nTable 5: In-context tuning is signiﬁcantly less sensi-\ntive to example choices compared to in-context learn-\ning with raw LMs.\nBERT-Base BERT-Large\nRaw IC-L ICT Raw IC-L ICT\n1-shot 35.38 26.31 34.03 28.78\n2-shot 33.79 25.40 17.71 19.35\n5-shot 24.90 15.64 6.36 5.16\nTable 6: In-context tuning is much less sensitive to\ntask instruction wording compared to in-context learn-\ning with raw LMs.\ntuning and in-context prompting with raw LMs in\nTable 4. Results show that in-context tuning is sig-\nniﬁcantly less sensitive to ordering of in-context ex-\namples compared to in-context prompting with raw\nLMs, reducing the sensitivity by 68% on LAMA\nand 83% on BinaryClfs.\nIn-context tuning is signiﬁcantly less sensitive\nto example choices. We compare the variance\nwith respect to example choices for in-context tun-\ning and in-context prompting with raw LMs in\nTable 5. Results show that in-context tuning is sig-\nniﬁcantly less sensitive to selection of in-context\nexamples compared to in-context prompting with\nraw LMs across both datasets and all model sizes,\nreducing the sensitivity by 56% on LAMA and 40%\non BinaryClfs (averaged across model sizes). We\nconjecture that in-context tuning is signiﬁcantly\nless sensitive to example ordering and selection\nbecause the model is exposed to various example\norderings and selections during in-context tuning.\nIn-context tuning is less sensitive to instruction\nwording. We report the variance with respect to\ninstruction wording for in-context tuning and in-\ncontext prompting with raw LMs in Table 6. Re-\nsults show that in-context tuning is less sensitive to\ninstruction wording compared to in-context prompt-\ning with raw LMs in ﬁve out of six evaluation set-\ntings, reducing the variance by 19% on LAMA\n(averaged across model size and number of shots).\nWe also observe that in-context tuning is espe-\ncially effective on task instructions with low accu-\nracy under raw in-context learning. For each task,\nwe compute the Pearson correlation between the\nraw in-context learning accuracy and the accuracy\ngain from in-context tuning (over raw in-context\nlearning) on all instructions. On the LAMA dataset,\nwe see a strong negative correlation of -0.563 (aver-\naged across all tasks), with p-value <0.05 on 63%\nof the tasks. We conjecture that in-context tuning is\nmuch less sensitive to instruction wording because\nthe model is exposed to a wide variety of different\ntask instructions during in-context tuning.\nIn-context examples are complementary to in-\nstructions. We observe that in-context tuning is\nespecially effective on task instructions with low\naccuracy under instruction tuning. For each task,\nwe compute the Pearson correlation between the\ninstruction tuning accuracy and the accuracy gain\nfrom in-context tuning (over instruction tuning) on\nall instructions. On the LAMA dataset, we see\na strong negative correlation of -0.910 (averaged\nacross all tasks), with p-value <0.01 on 91% of\nthe tasks. We conjecture that in-context tuning is\nmuch less sensitive to instruction wording because\nfew-shot in-context examples provide additional\ntask information besides the task instructions.\n5 Related Work\nLM Prompting for FSL Pre-trained LMs can be\nused to perform various FSL tasks when prompted\nwith a natural language task instruction and several\ntask examples (Radford et al., 2019; Brown et al.,\n2020; Schick and Schütze, 2021b; Li and Liang,\n2021; Lester et al., 2021; Qin and Eisner, 2021).\nHowever, prompting pre-trained LMs directly for\nFSL is known to be sensitive to various artifacts,\nsuch as the wording of the task instruction and the\nselection and ordering of few-shot training exam-\nples (Schick and Schütze, 2021a; Jiang et al., 2020;\nZhao et al., 2021; Gao et al., 2021; Liu et al., 2021).\nOur work is the ﬁrst to show that meta-learning\nwith an explicit FSL objective signiﬁcantly reduces\nthe sensitivity of LM prompting with respect to the\nin-context examples and instruction wording.\nMeta-learning for FSL Meta-learning is a\nwidely used technique in NLP to improve cross-\ndomain transfer (Yu et al., 2018; Geng et al., 2019;\nHolla et al., 2020; Deng et al., 2020) and cross-\ntask transfer (Gu et al., 2018; Bansal et al., 2020;\nDou et al., 2019). Existing optimization-based\nmeta-learning methods mostly perform task adap-\n725\ntation by ﬁne-tuning a task-agnostic model on task-\nspeciﬁc examples using gradient descent (Finn\net al., 2017; Jiang et al., 2019; Nichol et al., 2018).\nHowever, ﬁne-tuning on few-shot task examples is\nsensitive to hyperparameters (Antoniou et al., 2019)\nand nested optimization during meta-training is of-\nten unstable (Nichol et al., 2018; Antoniou et al.,\n2019; Rajeswaran et al., 2019). In contrast, our ap-\nproach performs few-shot task adaptation by using\ntask-speciﬁc examples as part of the model input\nwhile keeping the model parameters frozen and\ntask-agnostic during the adaptation stage.\nMulti-task Learning In multi-task learning, a\nsingle model is trained on the union of training sets\nof multiple tasks to learn a shared representation\n(Liu et al., 2019). The multi-task model is then\nﬁne-tuned on task-speciﬁc examples to adapt to\nnew tasks. Multi-task learning is shown to improve\nperformance on various downstream tasks, espe-\ncially tasks with small training sets (Khashabi et al.,\n2020; Ye et al., 2021; Aghajanyan et al., 2021).\nCompared to meta-learning, multi-task learning\ndoes not optimize task adaptation directly.\nFine-tuned LMs for Instruction Learning Re-\ncent work shows that ﬁne-tuning LMs to learn task\ninstructions on a wide variety of tasks can further\nleverage the inductive bias of LMs to perform in-\nstruction learning (Zhong et al., 2021; Mishra et al.,\n2021; Wei et al., 2021). Our work is partially in-\nspired by this line of work, but we work under the\nmore generic few-shot meta-learning setting, and\nshow that our approach out-performs both instruc-\ntion tuning and existing few-shot meta-learning\nmethods (e.g., MAML). While previous work fo-\ncuses on the accuracy improvement gained from\ninstruction ﬁne-tuning, our work also looks into\nthe well-known over-sensitivity issue of FSL and\nshows that in-context tuning effectively reduces the\nsensitivity of FSL with respect to various factors.\nConcurrent to our work, Min et al. (2021) also\nexplores in-context tuning under more general\nSeq2Seq tasks. In comparison, our work com-\npares in-context tuning to a meta-learning baseline\nMAML, and shows that in-context tuning mitigates\nthe well-known oversensitivity issue of LM prompt-\ning. Contrary to our paper, Min et al. (2021) ﬁnds\nthat in-context tuning under-performs InsT + FT.\nThis might be because they use many more shots\n(16-shot), which could give gradient-based meth-\nods more advantage.\n6 Future Directions\nScaling Up and Broader Applications Our\nwork only considers simple binary classiﬁcation\nand knowledge retrieval tasks, at most 5 in-context\nexamples, and models with fewer than 1 billion\nparameters. Nevertheless, it is straightforward to\nscale up our framework to a wider and more di-\nverse range of general sequence-to-sequence tasks\n(Ye et al., 2021), more few-shot examples (which\nrequires a longer context size (Dai et al., 2019;\nWang et al., 2020)), and larger models (Brown et al.,\n2020; Kaplan et al., 2020). It is also straightfor-\nward to apply in-context tuning to a broader range\nof scenarios that require adapting to a new setup,\ne.g., adapting to a new label in classiﬁcation tasks\n(Xia et al., 2021), an unseen database in semantic\nparsing tasks (Suhr et al., 2020; Lee et al., 2021),\nor a new language pair in machine translation (Gu\net al., 2018; Aharoni et al., 2019), etc.\nMeta-learning for Robustness Our work as-\nsumed that the few-shot training examples come\nfrom the same distribution as the test examples, but\nthis assumption does not necessarily hold in prac-\ntice. For example, the test distribution might con-\nstitute new input compositions (Lake and Baroni,\n2018), rare subgroups (Sagawa et al., 2019), other\ntypes of distribution shifts (Hendrycks and Diet-\nterich, 2019), or even adversarial examples (Kang\net al., 2019). More effective meta-learning meth-\nods might learn a more robust learning mechanism\nand combat these generalization challenges.\nUnderstanding In-context Learning Many\nproperties of in-context learning are still unknown.\nIs in-context learning more robust to distribution\nshift (Lester et al., 2021)? Can we combine\nin-context learning and gradient learning to get the\nbeneﬁt of both worlds (Wortsman et al., 2021)?\n7 Conclusion\nIn this work, we propose meta-learning via in-\ncontext tuning, which recasts the few-shot learn-\ning process of task adaptation and task-speciﬁc\nprediction as a simple sequence prediction prob-\nlem, where few-shot labeled examples are concate-\nnated with the target example to form the model\ninput. In-context tuning out-performs a wide va-\nriety of baselines in terms of accuracy, including\nraw LM prompting, MAML and instruction tun-\ning. Meanwhile, sensitivity study shows that our\nFSL approach of in-context tuning is signiﬁcantly\n726\nless sensitive to few-shot examples and instruction\nwording compared to raw LM prompting.\nGiven the empirical effectiveness of in-context\ntuning, we conjecture that the few-shot learning po-\ntential of large LMs (e.g., GPT-3) might be broadly\nunderestimated, and that in-context tuning can elim-\ninate well-known artifacts of few-shot LM prompt-\ning such as over-sensitivity to example ordering,\nexample selection and instruction wording.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Le. 2020. Towards a human-like open-\ndomain chatbot.\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava,\nXilun Chen, Luke Zettlemoyer, and Sonal Gupta.\n2021. Muppet: Massive multi-task representations\nwith pre-ﬁnetuning.\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nAntreas Antoniou, Harrison Edwards, and Amos\nStorkey. 2019. How to train your MAML. In Inter-\nnational Conference on Learning Representations.\nTrapit Bansal, Rishikesh Jha, and Andrew McCallum.\n2020. Learning to few-shot learn across diverse\nnatural language classiﬁcation tasks. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5108–5123, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nShumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi\nZhang, Wei Zhang, and Huajun Chen. 2020. Meta-\nlearning with dynamic-memory-based prototypical\nnetwork for few-shot event detection. Proceedings\nof the 13th International Conference on Web Search\nand Data Mining.\nZi-Yi Dou, Keyi Yu, and Antonios Anastasopoulos.\n2019. Investigating meta-learning algorithms for\nlow-resource natural language understanding tasks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1192–\n1197, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners.\nRuiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu,\nPing Jian, and Jian Sun. 2019. Induction networks\nfor few-shot text classiﬁcation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3904–3913, Hong Kong,\nChina. Association for Computational Linguistics.\nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,\nand Kyunghyun Cho. 2018. Meta-learning for low-\nresource neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3622–3631,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , pages 770–\n778.\nDan Hendrycks and Thomas Dietterich. 2019. Bench-\nmarking neural network robustness to common\ncorruptions and perturbations. arXiv preprint\narXiv:1903.12261.\nNithin Holla, Pushkar Mishra, Helen Yannakoudakis,\nand Ekaterina Shutova. 2020. Learning to learn\nto disambiguate: Meta-learning for few-shot word\nsense disambiguation. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 4517–4533, Online. Association for Compu-\ntational Linguistics.\n727\nXiang Jiang, Mohammad Havaei, Gabriel Chartrand,\nHassan Chouaib, Thomas Vincent, Andrew Jesson,\nNicolas Chapados, and Stan Matwin. 2019. Atten-\ntive task-agnostic meta-learning for few-shot text\nclassiﬁcation.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How Can We Know What Language\nModels Know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDaniel Kang, Yi Sun, Dan Hendrycks, Tom Brown,\nand Jacob Steinhardt. 2019. Testing robustness\nagainst unforeseen adversaries. arXiv preprint\narXiv:1908.08016.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational conference on machine learning , pages\n2873–2882. PMLR.\nBrenden M. Lake, Tomer D. Ullman, Joshua B. Tenen-\nbaum, and Samuel J. Gershman. 2016. Building ma-\nchines that learn and think like people.\nChia-Hsuan Lee, Oleksandr Polozov, and Matthew\nRichardson. 2021. KaggleDBQA: Realistic evalu-\nation of text-to-SQL parsers. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2261–2273, Online. As-\nsociation for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt-3?\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks for\nnatural language understanding. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4487–4496, Florence,\nItaly. Association for Computational Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn\nin context. arXiv preprint arXiv:2110.15943.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task general-\nization via natural language crowdsourcing instruc-\ntions.\nAlex Nichol, Joshua Achiam, and John Schulman.\n2018. On ﬁrst-order meta-learning algorithms.\nDmitry Nikolaev, Oﬁr Arviv, Taelin Karidi, Neta Ken-\nneth, Veronika Mitnik, Lilja Maria Saeboe, and\nOmri Abend. 2020. Fine-grained analysis of cross-\nlinguistic syntactic divergences. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 1159–1176, On-\nline. Association for Computational Linguistics.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-yet-effective entity em-\nbeddings for BERT. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 803–818, Online. Association for Computa-\ntional Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203–5212, Online. Association for Compu-\ntational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAravind Rajeswaran, Chelsea Finn, Sham M Kakade,\nand Sergey Levine. 2019. Meta-learning with im-\nplicit gradients. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\n728\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto,\nand Percy Liang. 2019. Distributionally robust neu-\nral networks for group shifts: On the importance of\nregularization for worst-case generalization. arXiv\npreprint arXiv:1911.08731.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352, Online. As-\nsociation for Computational Linguistics.\nDavid Silver, Aja Huang, Christopher Maddison,\nArthur Guez, Laurent Sifre, George Driessche, Ju-\nlian Schrittwieser, Ioannis Antonoglou, Veda Pan-\nneershelvam, Marc Lanctot, Sander Dieleman, Do-\nminik Grewe, John Nham, Nal Kalchbrenner, Ilya\nSutskever, Timothy Lillicrap, Madeleine Leach, Ko-\nray Kavukcuoglu, Thore Graepel, and Demis Has-\nsabis. 2016. Mastering the game of go with deep\nneural networks and tree search. Nature, 529:484–\n489.\nAlane Suhr, Ming-Wei Chang, Peter Shaw, and Ken-\nton Lee. 2020. Exploring unexplored generalization\nchallenges for cross-database semantic parsing. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8372–\n8388, Online. Association for Computational Lin-\nguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned lan-\nguage models are zero-shot learners.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\nMitchell Wortsman, Gabriel Ilharco, Mike Li,\nJong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. 2021.\nRobust ﬁne-tuning of zero-shot models. arXiv\npreprint arXiv:2109.01903.\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip\nYu. 2021. Incremental few-shot text classiﬁcation\nwith multi-round new classes: Formulation, dataset\nand system. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1351–1360, Online. As-\nsociation for Computational Linguistics.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren.\n2021. Crossﬁt: A few-shot learning challenge for\ncross-task generalization in nlp. arXiv preprint\narXiv:2104.08835.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsiﬁcation with multiple metrics. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1206–1215, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021. Adapting language models for zero-shot\nlearning by meta-tuning on dataset and prompt col-\nlections.\n729\nA Hyperparameters\nIn this section, we report the hyperparameters we\ntuned for our approach and each baseline.\nIn-Context Tuning (ours) We tune number of\ntraining epochs ([10, 15, 30] for LAMA and [1e-7,\n3e-7, 1e-6, 3e-6] for BinaryClfs) and learning rate\n([1e-7, 3e-7, 1e-6, 3e-6] for LAMA and [3e-6, 1e-5,\n3e-5, 1e-4] for BinaryClfs).\nMAML We assume that inner optimization and\nouter optimization use the same learning rate. We\ntuned number of adapt steps ([1, 2, 4] for both\ndatasets) and learning rate ([3e-7, 1e-6, 3e-6, 1e-5,\n3e-5, 1e-4, 3e-4, 1e-3] for LAMA and [3e-6, 1e-5,\n3e-5, 1e-4, 3e-4, 1e-3] for BinaryClfs).\nInstruction-Tuning + Fine-tuning For instruc-\ntion tuning we tuned the same set of hyperparame-\nters as in in-context tuning. The instruction tuning\nmodel with the highest validation performance are\nused for downstream task ﬁne-tuning. For task ﬁne-\ntuning, we tuned number of training epochs ([5,\n10, 15, 30, 40] for LAMA and [5, 10, 15, 30, 40]\nfor BinaryClfs) and learning rate ([1e-7, 3e-7, 1e-6,\n3e-6, 1e-5, 3e-5] for LAMA and [3e-6, 1e-5, 3e-5,\n1e-4, 3e-4, 1e-3] for BinaryClfs).\n730",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8110584616661072
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6793968677520752
    },
    {
      "name": "Language model",
      "score": 0.6592725515365601
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.650139570236206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6496843099594116
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5937873125076294
    },
    {
      "name": "Machine learning",
      "score": 0.5866877436637878
    },
    {
      "name": "Task (project management)",
      "score": 0.5809386372566223
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4937017858028412
    },
    {
      "name": "Context model",
      "score": 0.4922982454299927
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4817464351654053
    },
    {
      "name": "Natural language processing",
      "score": 0.38526445627212524
    },
    {
      "name": "Speech recognition",
      "score": 0.379291296005249
    },
    {
      "name": "Statistics",
      "score": 0.13782772421836853
    },
    {
      "name": "Mathematics",
      "score": 0.0946165919303894
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 47
}