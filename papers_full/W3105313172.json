{
    "title": "Adapting Open Domain Fact Extraction and Verification to COVID-FACT through In-Domain Language Modeling",
    "url": "https://openalex.org/W3105313172",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2110335847",
            "name": "Zhenghao Liu",
            "affiliations": [
                "Tsinghua University",
                "Center for Information Technology",
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2226924701",
            "name": "Chenyan Xiong",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2398480995",
            "name": "Zhuyun Dai",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2115549363",
            "name": "Si Sun",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2157167650",
            "name": "Maosong Sun",
            "affiliations": [
                "Tsinghua University",
                "Beijing Academy of Artificial Intelligence",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2051269448",
            "name": "Zhiyuan Liu",
            "affiliations": [
                "Center for Information Technology",
                "Beijing Academy of Artificial Intelligence",
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288364131",
        "https://openalex.org/W3020786614",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3107199975",
        "https://openalex.org/W3095432437",
        "https://openalex.org/W3091894171",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034828027",
        "https://openalex.org/W2964068236",
        "https://openalex.org/W4288090189",
        "https://openalex.org/W3101739755",
        "https://openalex.org/W3017344694",
        "https://openalex.org/W2922551710",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W3099977667",
        "https://openalex.org/W2950336186",
        "https://openalex.org/W3022143756",
        "https://openalex.org/W3102663935",
        "https://openalex.org/W2963961878"
    ],
    "abstract": "With the epidemic of COVID-19, verifying the scientifically false online information, such as fake news and maliciously fabricated statements, has become crucial. However, the lack of training data in the scientific domain limits the performance of fact verification models. This paper proposes an in-domain language modeling method for fact extraction and verification systems. We come up with SciKGAT to combine the advantages of open-domain literature search, state-of-the-art fact verification systems and in-domain medical knowledge through language modeling. Our experiments on SCIFACT, a dataset of expert-written scientific fact verification, show that SciKGAT achieves 30% absolute improvement on precision. Our analyses show that such improvement thrives from our in-domain language model by picking up more related evidence pieces and accurate fact verification. Our codes and data are released via Github.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2395–2400\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n2395\nAdapting Open Domain Fact Extraction and Veriﬁcation to COVID-FACT\nthrough In-Domain Language Modeling\nZhenghao Liu1,2, Chenyan Xiong5, Zhuyun Dai6, Si Sun4, Maosong Sun1,3, Zhiyuan Liu1,3\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China\nInstitute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology\n2State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China\n3Beijing Academy of Artiﬁcial Intelligence\n4Department of Electronic Engineering, Tsinghua University, Beijing, China\n5Microsoft Research, Redmond, USA\n6Carnegie Mellon University, USA\nAbstract\nWith the epidemic of COVID-19, verifying\nthe scientiﬁcally false online information, such\nas fake news and maliciously fabricated state-\nments, has become crucial. However, the lack\nof training data in the scientiﬁc domain lim-\nits the performance of fact veriﬁcation models.\nThis paper proposes an in-domain language\nmodeling method for fact extraction and veriﬁ-\ncation systems. We come up with SciKGAT\nto combine the advantages of open-domain\nliterature search, state-of-the-art fact veriﬁca-\ntion systems and in-domain medical knowl-\nedge through language modeling. Our experi-\nments on SCIFACT, a dataset of expert-written\nscientiﬁc fact veriﬁcation, show that SciKGAT\nachieves 30% absolute improvement on preci-\nsion. Our analyses show that such improve-\nment thrives from our in-domain language\nmodel by picking up more related evidence\npieces and accurate fact veriﬁcation. Our\ncodes and data are released via Github1.\n1 Introduction\nOnline contents with false information, such as lies,\nrumors and conspiracy theories, have been grow-\ning signiﬁcantly and spreading widely during the\nCOVID-19 epidemic. An automatic fact-checking\nsystem is urgently needed to check these scientiﬁc\nclaims, which can avoid undesired consequences.\nAutomatic fact-checking has drawn lots of attention\nfrom NLP community. Researchers mainly focus\non stopping misinformation transmission through\nvideos and texts (Cinelli et al., 2020; Hossain et al.,\n2020; Li et al., 2020; Serrano et al., 2020).\nThe scientiﬁc fact veriﬁcation task (Wadden\net al., 2020) is come up to deal with COVID-FACT\nwith high-quality articles of spanning domains\nfrom basic science to clinical medicine. Neverthe-\nless, the small-scale training data of SCIFACT may\n1https://github.com/thunlp/KernelGAT\nlimit the performance of COVID-FACT checking.\nThe state-of-the-art model (Wadden et al., 2020)\nachieves only 46.6% precision of fact veriﬁcation,\nwhich is hard to be trusted for users.\nThis paper presents the Scientiﬁc KGAT (SciK-\nGAT) to deal with low-resource COVID-FACT ver-\niﬁcation. SciKGAT employs the in-domain lan-\nguage model in the fact extraction and veriﬁcation\npipeline (Thorne et al., 2018; Wadden et al., 2020)\nto adapt fact-checking into COVID domain. The\nin-domain language model transfers COVID do-\nmain knowledge into pre-trained language models\nwith continuous training and learns medical token\nsemantics towards COVID with mask language\nmodel based training. The state-of-the-art fact ver-\niﬁcation model KGAT (Liu et al., 2020; Ye et al.,\n2020) is also used in SciKGAT for multi-evidence\nreasoning in the fact veriﬁcation module.\nOur experiments show that the in-domain lan-\nguage modelings achieve better performance for\nvarious components in the whole fact extraction\nand veriﬁcation pipeline by achieving more accu-\nrate evidence selection and fact veriﬁcation. Our in-\ndomain language modelings improve the fact veri-\nﬁcation performance with more than 10% absolute\nF1 score and 30% absolute precision (from 46.6%\nto 76%) than previous state-of-the-art on SCIFACT.\nSuch improvement shows that our model provides\na set of solutions for low-resource fact veriﬁcation\ntasks, such as COVID-19.\n2 Related Work\nExisting fact extraction and veriﬁcation models\nusually employ a three-step pipeline system (Chen\net al., 2017): document retrieval (abstract retrieval),\nsentence selection (rationale selection) and fact ver-\niﬁcation (Thorne et al., 2018; Wadden et al., 2020).\nThe preliminary fact veriﬁcation methods con-\ncatenate all evidence pieces (Nie et al., 2019; Wad-\n2396\nden et al., 2020) for fact veriﬁcation. KGAT (Liu\net al., 2020) conducts ﬁne-grained multiple evi-\ndence reasoning with a graph and achieves the state-\nof-the-art for fact veriﬁcation (Ye et al., 2020).\nThe reasoning ability of the pre-trained language\nmodel is crucial and helps improve fact veriﬁcation\nperformance (Devlin et al., 2019; Li et al., 2019;\nZhou et al., 2019; Soleimani et al., 2019). Some\nwork (Beltagy et al., 2019; Lee et al., 2020) trans-\nfers medical domain knowledge into pre-trained\nlanguage models for better medical semantic un-\nderstanding, which provides a potential way to deal\nwith COVID-FACT checking problem.\n3 Methodology\nThis section describes our SciKGAT for fact ex-\ntraction and veriﬁcation. We ﬁrst introduce the\npipeline of fact extraction and veriﬁcation (Sec. 3.1)\nand then continuously train the BERT based model\n(Sec. 3.2) for the whole.\n3.1 Preliminary\nGiven a claimc, we aim to predict the claim labely.\nWe usually implement the fact extraction and veri-\nﬁcation pipeline with three steps: abstract retrieval,\nrationale selection and fact veriﬁcation.\nAbstract Retrieval. For the claimc and abstract\nD = {a1, . . . , al}, we aim to retrieve three ab-\nstracts for the following steps.\nWe ﬁrst retrieve top-100 abstracts with TF-IDF\nfrom the abstract collection D, which is the same\nas the previous work (Wadden et al., 2020). For\nthe claim c and abstract abstract a = {e1, . . . , ek}\nwith k evidence pieces and title t, we concatenate\nclaim, title and abstract to get the representation\nHe of the pair⟨c, a⟩with BERT (Devlin et al., 2019):\nH= BERT([CLS] ◦c ◦[SEP] ◦t ◦a ◦[SEP]), (1)\nwhere ◦is the concatenate operation. The repre-\nsentation Hof⟨c, a⟩consists of representations of\ntokens from both claim and evidence. The 0-th\nrepresentation H0 denotes the [CLS] representa-\ntion. The relevance label ya between claim c and\nabstract a is calculated:\np(ya|c, a) =softmaxya (MLP(H0)). (2)\nWe rerank abstracts according to the probability\np(ya = 1|c, a) and top-3 abstracts are reserved.\nRationale Selection. Given the retrieved ab-\nstract a, rationale selection focuses on selecting\nrelevant sentences for fact veriﬁcation.\nSimilarly, for the evidence e of the retrieved\nabstract a, we can get the representationH of claim\nand evidence pair⟨c, e⟩:\nH = BERT([CLS] ◦c ◦[SEP] ◦e ◦[SEP]). (3)\nThen we predict the relevance label yr of claim\nc and evidence e:\np(yr|c, e) =softmaxyr (MLP(H0)). (4)\nThe related evidence pieces ( p(yr = 0|c, e) <\np(yr = 1|c, e)) are reserved to form the retrieved\nevidence set E = {e1, ..., eq}of each abstract a.\nFact Veriﬁcation. For the claim c and retrieved\nevidence set E, fact veriﬁcation model aims to pre-\ndict claim label y. We employ the state-of-the-art\nmodel KGAT (Liu et al., 2020) as our fact veri-\nﬁcation module. For the i-th evidence ei in the\nevidence set E, we can get the sentence pair rep-\nresentation Hi of the i-th pair⟨c, ei⟩through BERT.\nThen the probability of claim label y is calculated:\np(y|c, E) =KGAT(H1, . . . , Hq). (5)\n3.2 Continuous In-Domain Training\nTo deal with the low-resource COVID-FACT check-\ning, we propose continuous training methods to\ntransfer domain knowledge into pretrained lan-\nguage models.\nFor COVID-FACT checking, the medical do-\nmain knowledge is useful to understand medical\nwords (Beltagy et al., 2019). However, these med-\nical domain pre-trained language models will be\nout-of-date with the medical development or emer-\ngence of a new virus, such as COVID-19.\nContinuous in-domain training provides a poten-\ntial way to deal with this problem with the latest\nmedical corpus. Hence we come up with two in-\ndomain language models for the fact extraction and\nveriﬁcation pipeline with continuous training.\nRationale prediction based training.We ﬁrst\ncome up with the rationale prediction style training\nto continuously train BERT for better reasoning\nability towards the COVID-FACT. For the claim\nand evidence⟨c, e⟩, we optimize BERT model with\nsupervisions from SCIFACT:\nLr(c, e) =CrossEntropy(p(yr|c, e), y∗\nr ), (6)\nwhere y∗\nr denotes the ground truth rationale predic-\ntion label of the pair⟨c, e⟩. Then we get a supervised\nin-domain language model, BERT-RP, for the fact\nveriﬁcation module.\n2397\nModel Development set Testing Set\nSentence Level Abstract Level Sentence Level Abstract Level\nPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1\nBaselines\nSciBERT 45.78 38.52 41.84 51.93 44.98 48.21 - - - - - -\nRoBERTa 46.51 38.25 41.98 53.30 46.41 49.62 38.6 40.5 39.5 46.6 46.4 46.5\nSciKGAT\nKGAT 57.07 31.97 40.98 72.73 38.28 50.16 - - - - - -\nSciKGAT (w. A) 42.07 47.81 44.76 47.66 58.37 52.47 40.50 48.38 44.09 47.06 57.66 51.82\nSciKGAT (w. AR) 50.00 47.81 48.88 53.15 56.46 54.76 41.67 45.95 43.70 47.47 54.96 50.94\nSciKGAT (Full) 74.36 39.62 51.69 84.26 43.54 57.41 61.15 42.97 50.48 76.09 47.30 58.33\nTable 1: Overall Performance of Fact Extraction and Veriﬁcation. RoBERTa is the large version. SciKGAT (w. A)\nand SciKGAT (w. AR) are ablation models with the abstract retrieval and evidence selection of SciKGAT.\nMask language model based training.To help\nthe model better comprehend the semantics of\nCOVID related words, we substitute tokens with\n[MASK] and ask the model to generate appropri-\nate tokens for ﬁlling it. With continuous training,\nthe language model now sees the language from\nthe new corpus, thus being able to pick up the new\nterminologies, such as COVID-19. The continu-\nous training with COVID related corpus is able to\nbetter capture the context/semantics of such new\nterminologies (Gururangan et al., 2020).\nWe use data from COVID-19 Open Research\nDataset Challenge2 for continuous training, which\ntowards the medical topic. In this corpus, there are\nabout 86K papers before 2020, which are about\ncoronaviruses but not about COVID-19, and 54K\npapers after 2020. Based on the ﬁlters used by AI2\nto create this dataset, those papers that after 2020\nare almost about COVID-19. Thus roughly there\nare about 40% papers in this corpus that are about\nCOVID-19 (Wang et al., 2020).\n4 Experimental Methodology\nThis section describes the dataset, evaluation met-\nrics, baselines, and implementation details.\nDataset. The recently released dataset SCI-\nFACT (Wadden et al., 2020) is leveraged in our\nexperiments. It consists of 1,409 annotated claims\nwith 5,183 scientiﬁc articles. All claims are\nclassiﬁed as SUPPORT, CONTRADICT or NOT\nENOUGH INFO. The training, development and\ntesting sets contain 809, 300 and 300 claims, re-\nspectively. FEVER (Thorne et al., 2018) is also\nused by ofﬁcial baselines to train the fact veriﬁ-\ncation modules of baselines and our models. The\nFEVER consists of 185,455 annotated claims with\n5,416,537 Wikipedia documents.\n2https://www.kaggle.com/allen-institute-for-ai/CORD-\n19-research-challenge\nEvaluation Metrics. Precision, Recall and F 1\nscore are used to evaluate model performance, fol-\nlowing SCIFACT (Wadden et al., 2020). These\nevaluations are inspired by FEVER score (Thorne\net al., 2018) and consider if the evidence is selected\ncorrectly from the abstract level and sentence level.\nBaselines. Since the scientiﬁc fact veriﬁca-\ntion task is recently released, our baselines are\nmainly from Wadden et al. (2020). They ﬁrst\nuse TF-IDF for abstract retrieval and then use\nRoBERTa (Large) and SiBERT for rationale se-\nlection. KGAT and RoBERTa (Large) are lever-\naged for fact veriﬁcation. The rationale selection\nmodule is trained with SCIFACT and the fact veri-\nﬁcation module is trained with data from FEVER\nand SCIFACT (Wadden et al., 2020).\nImplementation Details. In all experiments,\nwe use SciBERT, RoBERTa (Base) and RoBERTa\n(Large) (Liu et al., 2019; Beltagy et al., 2019), and\ninherit huggingface’s PyTorch implementation 3.\nAdam is utilized for parameter optimization. For\nrationale selection, we keep the same setting as\nWadden et al. (2020). For abstract retrieval and fact\nveriﬁcation, we set the max length to 256, learning\nrate to 2e-5, batch size to 8 and accumulate step to\n4 during training. The other parameters are kept\nthe same with KGAT (Liu et al., 2020).\nFor the abstract retrieval module, we follow the\nprevious work (MacAvaney et al., 2020) and ﬁne-\ntune our in-domain language model with the medi-\ncal corpus from MS-MARCO (Bajaj et al., 2016) to\nﬁt our abstract retrieval module to the open-domain\nCOVID related literature search.\n5 Evaluation Result\nThis section ﬁrst tests the overall performance of\nSciKGAT. Then it studies the impacts of our in-\ndomain language modeling techniques in knowl-\n3https://github.com/huggingface/pytorch-transformers\n2398\nAblation Model\nEvidence Retrieval Fact Checking\nRanking Accuracy Sentence Level Abstract Level\nPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1\nAbstract TF-IDF 16.11 69.38 26.15 46.51 38.25 41.98 53.30 46.41 49.62\nRetrieval w. SciBERT 19.78 85.17 32.10 42.09 47.27 44.53 48.18 56.94 52.19\nw. SciBERT-MLM 20.33 87.56 33.00 42.07 47.81 44.76 47.66 58.37 52.47\nRationale SciBERT 36.90 65.03 47.08 43.22 46.99 45.03 48.94 55.02 51.80\nSelection SciBERT-MLM 43.73 60.93 50.91 50.00 47.81 48.88 53.15 56.46 54.76\nFact SciBERT 43.73 60.93 50.91 36.55 38.25 37.38 36.92 45.93 40.94\nVeriﬁcation w. KGAT - - - 51.61 34.97 41.69 58.99 39.23 47.13\nw. KGAT (RP Init) - - - 60.10 33.33 42.88 66.38 36.84 47.38\nw. KGAT (MLM Init) - - - 56.00 34.43 42.64 65.32 38.76 48.65\nRoBERTa-Base 43.73 60.93 50.91 42.72 36.89 39.59 44.50 46.41 45.43\nw. KGAT - - - 61.05 31.69 41.73 68.87 34.93 46.35\nw. KGAT (RP Init) - - - 61.19 36.61 45.81 67.48 39.71 50.00\nw. KGAT (MLM Init) - - - 60.35 37.43 46.21 67.19 41.15 51.04\nRoBERTa-Large 43.73 60.93 50.91 50.00 47.81 48.88 53.15 56.46 54.76\nw. KGAT - - - 62.87 40.71 49.42 72.39 46.41 56.56\nw. KGAT (RP Init) - - - 73.47 39.34 51.25 83.33 43.06 56.78\nw. KGAT (MLM Init) - - - 74.36 39.62 51.69 84.26 43.54 57.41\nTable 2: In-Domain Language Model Performance of Fact Extraction and Veriﬁcation on Development Set. Model\nperformance with SciBERT on both abstract retrieval and rationale selection scenarios is presented. For fact veriﬁ-\ncation, the in-domain language modeling methods, MLM (Mask Language Model) and RP (Rationale Prediction),\nare evaluated with the state-of-the-art fact veriﬁcation model KGAT (Liu et al., 2020; Ye et al., 2020).\nClaim:Basophils counteract disease development in patients\nwith systemic lupus erythematosus (SLE).\nEvidence 1:. . .basophilsand IgE autoantibodies amplify\nautoantibody production thatleads to lupus nephritis. . .\nEvidence 2:Individuals with SLE also have elevatedserum\nIgE, self-reactive IgEs andactivated basophilsthat. . .\nSciKGAT:ContradictRoBERTa:Not Enough Info\nClaim:In adult tissue, most T cells are memory T cells.\nEvidence 1:Whereas adult tissues contain a predominance\nof memory T cells, in pediatric blood and tissues the main\nsubset consists of naive recent thymic emigrants. . .\nSciKGAT:SupportKGAT:Contradict\nTable 3: Examples of Fact Veriﬁcation. All models are\nimplemented with RoBERTa (Large). The contents are\nemphasized that can verify the given claim.\nedge transfer. Finally, it provides case studies.\n5.1 Overall Performance\nThe overall performance of SciKGAT is shown\nin Table 1. The ofﬁcial baseline model uses TF-\nIDF for abstract retrieval and RoBERTa (Large) for\nrationale selection and fact veriﬁcation, which is\nstate-of-the-art. We add modules of SciKGAT step\nby step to evaluate the model’s effectiveness.\nSciKGAT (w. A) and SciKGAT (w. AR) show\nsigniﬁcant improvement than baselines, which\ndemonstrates our literature search with an in-\ndomain language model is effective in selecting\nrelated evidence from abstract and sentence lev-\nels. For fact veriﬁcation, our SciKGAT improves\npipeline performance by achieving 30% improve-\nment on label prediction precision. The high pre-\ncision of fact veriﬁcation demonstrates that our\nmodel has the ability to provide high quality and\nconvinced COVID-FACT veriﬁcation results.\n5.2 In-Domain Effectiveness\nIn this experiment, we evaluate the impacts of the\nin-domain language model on individual fact ex-\ntraction and veriﬁcation components of SciKGAT.\nAs shown in Table 2, we ﬁrst compare SciB-\nERT and SciBERT-MLM on the abstract retrieval\nand rationale selection tasks. Then we ﬁx the se-\nlected evidence and evaluate the reasoning ability\nof the fact veriﬁcation module, using two kinds of\nin-domain language models, MLM model ( mask\nlanguage model training) and RP model (rationale\nprediction training) with three BERT variants.\nFor abstract retrieval and rationale selection,\nSciBERT-MLM shows better ranking accuracy\nthan SciBERT, and consequently results in better\nfact veriﬁcation results. It demonstrates that the\nmask language model learns speciﬁc medical do-\nmain knowledge through the latest COVID related\npapers and thrives on our evidence selection parts\nwith continuous training.\nThen we evaluate the effectiveness of in-domain\nlanguage models on fact veriﬁcation with various\nBERT based models. Our in-domain language mod-\nels signiﬁcantly improve fact veriﬁcation perfor-\nmance and illustrate their stronger reasoning ability\ncompared to vanilla pre-trained language models.\nCompare to the RP model, MLM model usually\nachieves better performance. Importantly, MLM\n2399\nmodel does not rely on annotation data, provid-\ning a common resolution for COVID related tasks.\nThe consistent improvement on all BERT variants\nfurther manifests the robustness of our model.\n5.3 Case Study\nAs shown in Table 3, two examples from the devel-\nopment set are used to illustrate SciKGAT’s effec-\ntiveness for fact veriﬁcation.\nIn the ﬁrst example, both evidence 1 and evi-\ndence 2 indicate that basophils can lead to systemic\nlupus erythematosus, which contradicts the claim.\nThe concatenation based model, RoBERTa, fails to\nverify the claim, while SciKGAT makes the right\nprediction. It demonstrates the effectiveness of\nKGAT’s ﬁne-grained reasoning with multiple evi-\ndence pieces. In the second example, the evidence\npiece indicates that memory T cellsare the most\nin T cellsfor adults. SciKGAT predicts claim la-\nbel correctly and shows its effectiveness by recog-\nnizing and comprehending these medical phrases,\nwhich thanks to the in-domain language modeling.\n6 Conclusion\nThis paper presents in-domain language modeling\nmethods for open domain fact extraction and veri-\nﬁcation, which transfer domain knowledge for the\nCOVID-FACT checking task. Our experiments\nshow that our pipeline signiﬁcantly improves the\nfact-checking performance of the state-of-the-art\nmodel with more than 30% absolute prediction pre-\ncision. Our analyses illustrate that our model has\nstronger reasoning ability with continuous training\nand beneﬁts from COVID related knowledge.\nAcknowledgments\nThis work is supported by the National Key RD\nProgram of China (2020AAA0105200), Beijing\nAcademy of Artiﬁcial Intelligence (BAAI) and the\nNExT++ project from the National Research Foun-\ndation, Prime Minister’s Ofﬁce, Singapore under\nits IRC@Singapore Funding Initiative.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. Ms marco: A human generated machine\nreading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of ACL, pages\n1870–1879.\nMatteo Cinelli, Walter Quattrociocchi, Alessandro\nGaleazzi, Carlo Michele Valensise, Emanuele Brug-\nnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo,\nand Antonio Scala. 2020. The covid-19 social media\ninfodemic. arXiv preprint arXiv:2003.05004.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL, pages 8342–8360.\nTamanna Hossain, Robert L Logan IV , Arjuna Ugarte,\nYoshitomo Matsubara, Sameer Singh, and Sean\nYoung. 2020. Detecting covid-19 misinformation on\nsocial media.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nTianda Li, Xiaodan Zhu, Quan Liu, Qian Chen, Zhi-\ngang Chen, and Si Wei. 2019. Several experi-\nments on investigating pretraining and knowledge-\nenhanced models for natural language inference.\narXiv preprint arXiv:1904.12104.\nYunyao Li, Tyrone Grandison, Patricia Silveyra,\nAli Douraghy, Xinyu Guan, Thomas Kieselbach,\nChengkai Li, and Haiqi Zhang. 2020. Jennifer for\ncovid-19: An nlp-powered chatbot built for the peo-\nple and by the people to combat misinformation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n2400\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact veriﬁcation\nwith kernel graph attention network. In Proceedings\nof ACL, pages 7342–7351.\nSean MacAvaney, Arman Cohan, and Nazli Gohar-\nian. 2020. Sledge: A simple yet effective baseline\nfor coronavirus scientiﬁc knowledge search. arXiv\npreprint arXiv:2005.02365.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and veriﬁcation with neu-\nral semantic matching networks. In Proceedings of\nAAAI, pages 6859–6866.\nJuan Carlos Medina Serrano, Orestis Papakyriakopou-\nlos, and Simon Hegelich. 2020. Nlp-based feature\nextraction for the detection of covid-19 misinforma-\ntion videos on youtube.\nAmir Soleimani, Christof Monz, and Marcel Worring.\n2019. BERT for evidence retrieval and claim veriﬁ-\ncation. arXiv preprint arXiv:1910.02655.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERiﬁcation. In Proceedings of NAACL, pages\n809–819.\nDavid Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan\nLin, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. 2020. Fact or ﬁction: Verifying\nscientiﬁc claims. arXiv preprint arXiv:2004.14974.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Darrin Eide, Kathryn\nFunk, Rodney Kinney, Ziyang Liu, William Merrill,\net al. 2020. Cord-19: The covid-19 open research\ndataset. ArXiv.\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu,\nMaosong Sun, and Zhiyuan Liu. 2020. Coreferen-\ntial reasoning learning for language representation.\narXiv preprint arXiv:2004.06870.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: Graph-based evidence aggregating and rea-\nsoning for fact veriﬁcation. In Proceedings of ACL,\npages 892–901."
}