{
  "title": "Applying a Pre-trained Language Model to Spanish Twitter Humor Prediction",
  "url": "https://openalex.org/W2953728110",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5034601301",
      "name": "Bobak Farzin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077565121",
      "name": "Piotr Czapla",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5007075789",
      "name": "Jeremy Howard",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2970380064",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W2897686202",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2952005787",
    "https://openalex.org/W2743945814"
  ],
  "abstract": "Our entry into the HAHA 2019 Challenge placed $3^{rd}$ in the classification task and $2^{nd}$ in the regression task. We describe our system and innovations, as well as comparing our results to a Naive Bayes baseline. A large Twitter based corpus allowed us to train a language model from scratch focused on Spanish and transfer that knowledge to our competition model. To overcome the inherent errors in some labels we reduce our class confidence with label smoothing in the loss function. All the code for our project is included in a GitHub repository for easy reference and to enable replication by others.",
  "full_text": "Applying a Pre-trained Language Model to\nSpanish Twitter Humor Prediction\nBobak Farzin1 Piotr Czapla2 and Jeremy Howard3\nJune 2019\n1 USF Data Institue, WAMRI Visting Scholar, USA\nbfarzin@gmail.com\n2 n-waves, Poland\nPiotr.Czapla@n-waves.com\n3 University of San Francisco & Fast.ai, USA\nj@fast.ai\nAbstract. Our entry into the HAHA 2019 Challengeplaced 3rd in the\nclassiﬁcation task and 2 nd in the regression task. We describe our sys-\ntem and innovations, as well as comparing our results to a Naive Bayes\nbaseline. A large Twitter based corpus allowed us to train a language\nmodel from scratch focused on Spanish and transfer that knowledge to\nour competition model. To overcome the inherent errors in some labels\nwe reduce our class conﬁdence with label smoothing in the loss function.\nAll the code for our project is included in a GitHub 4 repository for easy\nreference and to enable replication by others.\nKeywords: Natural Language Processing · Transfer Learning · Senti-\nment Analysis · Humor Classiﬁcation\n1 Introduction\n- ¡Socorro, me ha picado una vbora!\n- ¿Cobra?\n- No, gratis.5\nGoogle Translation:\n- Help, I was bitten by a snake!\n- Does it charge?\n- Not free.\nCopyright c⃝ 2019 for this paper by its authors. Use permitted under Creative Com-\nmons License Attribution 4.0 International (CC BY 4.0). IberLEF 2019, 24 Septem-\nber 2019, Bilbao, Spain.\narXiv:1907.03187v1  [cs.CL]  6 Jul 2019\nHumor does not translate well because it often relies on double-meaning or\na subtle play on word choice, pronunciation, or context. These issues are further\nexacerbated in areas where space is a premium (as frequent on social media\nplatforms), often leading to usage and development of shorthand, in-jokes, and\nself-reference. Thus, building a system to classify the humor of tweets is a diﬃcult\ntask. However, with transfer-learning and the Fast.ai library6, we can build a high\nquality classiﬁer in a foreign language. Our system outperforms a Naive Bayes\nSupport Vector Machine (NBSVM) baseline, which is frequently considered a\n”strong baseline” for many Natural Language Processing (NLP) related tasks\n(see Wang et al [12]).\nRather than hand-crafted language features, we have taken an ”end to end”\napproach building from the raw text to a ﬁnal model that achieves the tasks\nas presented. Our paper lays out the details of the system and our code can be\nfound in a GitHub repository for use by other researchers to extend the state of\nthe art in sentiment analysis.\nContribution Our contributions are three fold. First, we apply transfer-learning\nof a language model based on a larger corpus of tweets. Second, we use a label\nsmoothed loss, which provides regularization and allows full training of the ﬁnal\nmodel without gradual unfreezing. Third, we select the best model for each task\nbased on cross-validation and 20 random-seed initialization in the ﬁnal network\ntraining step.\n2 Task and Dataset Description\nThe Humor Analysis based on Human Annotation (HAHA) 2019[4] competition\nasked for analysis of two tasks in the Spanish language based on a corpus of\npublicly collected data described in Castro et al. [2]:\n– Task1: Humor Detection:Determine if a tweet is humorous. System rank-\ning is based on F1 score which balances precision and accuracy.\n– Task2: Funniness Score :If humorous, what is the average humor rating\nof the tweet? System ranking is based on root mean-squared error (RMSE).\nThe HAHA dataset includes labeled data for 24,000 tweets and a test set of\n6,000 tweets (80%/20% train/test split.) Each record includes the raw tweet\ntext (including accents and emoticons), a binary humor label, the number of\nvotes for each of ﬁve star ratings and a “Funniness Score” that is the average\nof the 1 to 5 star votes cast. Examples and data can be found on the CodaLab\ncompetition webpage7.\n4 https://github.com/bfarzin/haha 2019 ﬁnal, Accessed on 19 June 2019\n5 https://www.ﬂuentin3months.com/spanish-jokes/, Accessed on 19 June 2019\n6 https://docs.fast.ai/, Accessed on 19 June 2019\n7 http://competitions.codalab.org/competitions/22194/ Accessed on 19 June 2019\n3 System Description\nWe modify the method of Universal Langage Model Fine-tuning for Text Clas-\nsiﬁcation (ULMFiT) presented in Howard and Ruder [6]. The primary steps\nare:\n1. Train a language model (LM) on a large corpus of data\n2. Fine-tune the LM based on the target task language data\n3. Replace the ﬁnal layer of the LM with a softmax or linear output layer and\nthen ﬁne-tune on the particular task at hand (classiﬁcation or regression)\nBelow we will give more detail on each step and the parameters used to generate\nour system.\n3.1 Data, Cleanup & Tokenization\n3.2 Additional Data\nWe collected a corpus for our LM based on Spanish Twitter using tweepy 8\nrun for three 4-hour sessions and collecting any tweet with any of the terms\n’el’,’su’,’lo’,’y’ or ’en’. We excluded retweets to minimize repeated examples in\nour language model training. In total, we collected 475,143 tweets - a data set\nis nearly 16 times larger than the text provided by the competition alone. The\nfrequency of terms, punctuation and vocabulary used on Twitter can be quite\ndiﬀerent from the standard Wikipedia corpus that is often used to train an LM\nfrom scratch.\nIn the ﬁne-tuning step, we combined the train and test text data without\nlabels from the contest data.\n3.3 Cleaning\nWe applied a list of default cleanup functions in sequence (see list below). They\nare close to the standard clean-up included in the Fast.ai library with the addi-\ntion of one function for the Twitter dataset. Cleanup of data is key to expressing\ninformation in a compact way so that the LM can use the relevant data when\ntrying to predict the next word in a sequence.\n1. Replace more than 3 repetitions of the same character (ie. grrrreat becomes\ng xxrep r 4 eat)\n2. Replace repetition at the word level (similar to above)\n3. Deal with ALL CAPS words replacing with a token and converting to lower\ncase.\n4. Add spaces between special chars (ie. !!! to ! ! !)\n5. Remove useless spaces (remove more than 2 spaces in sequence)\n6. Addition: Move all text onto a single line by replacing new-lines inside a\ntweet with a reserved word (ie. \\n to xxnl)\n8 http://github.com/tweepy/tweepy, Accessed on 19 June 2019\nThe following example shows the application of this data cleaning to a single\ntweet:\nSaber, entender y estar convencides que la frase \\\n#LaESILaDefendemosEntreTodes es nuestra linea es nuestro eje.\\\n#AlertaESI!!!!\nVamos por mas!!! e invitamos a todas aquellas personas que quieran \\\nse parte.\nxxbos saber , entender y estar convencides que la frase \\\n# laesiladefendemosentretodes es nuestra linea es nuestro eje.\\\nxxnl # alertaesi xxrep 4 ! xxnl vamos por mas ! ! ! e invitamos a \\\ntodas aquellas personas que quieran se parte.\n3.4 Tokenization\nWe used sentencepiece [7] to parse into sub-word units and reduce the possible\nout-of-vocabulary terms in the data set. We selected a vocab size of 30,000 and\nused the byte-pair encoding (BPE) model. To our knowledge this is the ﬁrst time\nthat the BPE toenization has been used with ULMFiT in a competition model.\n4 Training and Results\n4.1 LM Training and Fine-tuning\nWe train the LM using a 90/10 training/validation split, reporting the validation\nloss and accuracy of next-word prediction on the validation set. For the LM, we\nselected an ASGD Weight-Dropped Long Short Term Memory (AWD LSTM,\ndescribed in Merity et al.[8]) model included in Fast.ai. We replaced the typical\nLong Short Term Memory (LSTM) units with Quasi Recurrent Neural Network\n(QRNN, described in Bradbury et al.[1]) units. Our network has 2304 hidden-\nstates, 3 layers and a softmax layer to predict the next-word. We tied the embed-\nding weights[10] on the encoder and decoder for training. We performed some\nsimple tests with LSTM units and a Transformer Language model, ﬁnding all\nmodels were similar in performance during LM training. We thus chose to use\nQRNN units due to improved training speed compared to the alternatives. This\nmodel has about 60 million trainable parameters.\nParameters used for training and ﬁne-tuning are shown in Table 1. For all\nnetworks we applied a dropout multiplier which scales the dropout used through-\nout the network. We used the Adam optimizer with weight decay as indicated\nin the table.\nFollowing the work of Smith[11] we found the largest learning-rate that we\ncould apply and then ran a one-cycle policy for a single epoch. This largest\nweight is shown in Table 1 under ”Learning Rate.” Subsequent training epochs\nwere run with one-cycle and lower learning rates indicated in Table 1 under\n”Continued Training.”\nTable 1.LM Training Parameters\nParam LM Fine-Tune LM\nWeight Decay 0.1 0.1\nDropout Mult 1.0 1.0\nLearning Rate 1 epoch at 5 ∗ 10−3 5 epochs at 3 ∗ 10−3\nContinued Training 15 epochs at 1 ∗ 10−3 10 epochs at 1 ∗ 10−4\n4.2 Classiﬁcation and Regression Fitting\nAgain, following the play-book from Howard and Ruder[6], we change the pre-\ntrained network head to a softmax or linear output layer (as appropriate for the\ntransfer task) and then load the LM weights for the layers below. We train just\nthe new head from random initialization, then unfreeze the entire network and\ntrain with diﬀerential learning rates. We layout our training parameters in Table\n2.\nWith the same learning rate and weight decay we apply a 5-fold cross-\nvalidation on the outputs and take the mean across the folds as our ensemble. We\nsample 20 random seeds (see more in section 4.3) to ﬁnd the best initialization\nfor our gradient descent search. From these samples, we select the best validation\nF1 metric or Mean Squared Error (MSE) for use in our test submission.\nClassiﬁer setup For the classiﬁer, we have a hidden layer and softmax head.\nWe over-sample the minority class to balance the outcomes for better training us-\ning Synthetic Minority Oversampling Technique (SMOTE, described in Chawla\net al.[3]). Our loss is label smoothing as described in Pereyra et al.[9] of the\nﬂattened cross-entropy loss. In ULMFiT, gradual unfreezing allows us to avoid\ncatastropic forgetting, focus each stage of training and preventing over-ﬁtting of\nthe parameters to the training cases. We take an alternative approach to regu-\nlarization and in our experiments found that we got similar results with label\nsmoothing but without the separate steps and learning rate reﬁnement required\nof gradual unfreezing.\nRegression setup For the regression task, we ﬁll all #N/A labels with scores of\n0. We add a hidden layer and linear output head and MSE loss function.\n4.3 Random Seed as a Hyperparamter\nFor classiﬁcation and regression, the random seed sets the initial random weights\nof the head layer. This initialization aﬀects the ﬁnal F1 metric achievable.\nAcross each of the 20 random seeds, we average the 5-folds and obtain a\nsingle F1 metric on the validation set. The histogram of 20-seed outcomes is\nshown in Figure 1 and covers a range from 0.820 to 0.825 over the validation\nset. We selected our single best random seed for the test submission. With more\nTable 2.Classiﬁcation and Regression Training Parameters\nParam Value\nWeight Decay 0.1\nDropout Mult 0.7\nLearning Rate (Head) 2 epochs at 1 ∗ 10−2\nCont. Training 15 epochs with diﬀ lr:(1 ∗ 10−3/(2.64), 5 ∗ 10−3)\nexploration, a better seed could likely be found. Though we only use a single\nseed for the LM training, one could do a similar search with random seeds for\nLM pre-training, and further select the best down-stream seed similar to Czapla\net al [5].\nFig. 1.Histogram of F1 metric averaged across 5-fold metric\n4.4 Results\nTable 3 gives three results from our submissions in the competition. The ﬁrst is\nthe baseline NBSVM solution, with an F1 of 0.7548. Second is our ﬁrst random\nseed selected for the classiﬁer which produces a 0.8083 result. While better than\nthe NBSVM solution, we pick the best validation F1 from the 20 seeds we tried.\nThis produced our ﬁnal submission of 0.8099. Our best model achieved an ﬁve-\nfold average F1 of 0.8254 on the validation set shown in Figure 1 but a test set\nF1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. Also note\nthat our third place entry was 1.1% worse in F1 score than ﬁrst place but 1.2%\nbetter in F1 than the 4 th place entry.\nTable 3.Comparative Results\nAccuracy Precision Recall F1\nNBSVM 0.8223 0.8180 0.7007 0.7548\nFirst Seed 0.8461 0.7869 0.8309 0.8083\nBest Seed 0.8458 0.7806 0.8416 0.8099\n5 Conclusion\nThis paper describes our implementation of a neural net model for classiﬁcation\nand regression in the HAHA 2019 challenge. Our solution placed 3 rd in Task\n1 and 2 nd in Task 2 in the ﬁnal competition standings. We describe the data\ncollection, pre-training, and ﬁnal model building steps for this contest. Twitter\nhas slang and abbreviations that are unique to the short-format as well as gen-\nerous use of emoticons. To capture these features, we collected our own dataset\nbased on Spanish Tweets that is 16 times larger than the competition data set\nand allowed us to pre-train a language model. Humor is subtle and using a label\nsmoothed loss prevented us from becoming overconﬁdent in our predictions and\ntrain more quickly without the gradual unfreezing required by ULMFiT. We\nhave open-sourced all code used in this contest to further enable research on this\ntask in the future.\n6 Author Contributions\nBF was the primary researcher. PC contributed with suggestions for the random\nseeds as a hyper-parameters and label smoothing to speed up training. JH con-\ntributed with suggestion for higher dropout throughout the network for more\ngeneralization.\n7 Acknowledgements\nThe author would like to thank all the participants on the fast.ai forums 9 for\ntheir ideas and suggestions. Also, Kyle Kastner for his edits, suggestions and\nrecommendations in writing up these results.\nReferences\n1. Bradbury, J., Merity, S., Xiong, C., Socher, R.: Quasi-recurrent neural networks.\nCoRR abs/1611.01576 (2016), http://arxiv.org/abs/1611.01576\n2. Castro, S., Chiruzzo, L., Ros´ a, A., Garat, D., Moncecchi, G.: A crowd-annotated\nspanish corpus for humor analysis. In: Proceedings of the Sixth International Work-\nshop on Natural Language Processing for Social Media. pp. 7–11 (2018)\n9 http://forums.fasta.ai\n3. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: Synthetic\nminority over-sampling technique. J. Artif. Int. Res. 16(1), 321–357 (Jun 2002),\nhttp://dl.acm.org/citation.cfm?id=1622407.1622416\n4. Chiruzzo, L., Castro, S., Etcheverry, M., Garat, D., Prada, J.J., Ros´ a, A.: Overview\nof HAHA at IberLEF 2019: Humor Analysis based on Human Annotation. In:\nProceedings of the Iberian Languages Evaluation Forum (IberLEF 2019). CEUR\nWorkshop Proceedings, CEUR-WS, Bilbao, Spain (9 2019)\n5. Czapla, P., Howard, J., Kardas, M.: Universal language model ﬁne-tuning with\nsubword tokenization for polish. CoRR abs/1810.10222 (2018), http://arxiv.\norg/abs/1810.10222\n6. Howard, J., Ruder, S.: Universal language model ﬁne-tuning for text classiﬁcation.\nCoRR abs/1801.06146 (2018), http://arxiv.org/abs/1801.06146\n7. Kudo, T., Richardson, J.: Sentencepiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. CoRRabs/1808.06226\n(2018), http://arxiv.org/abs/1808.06226\n8. Merity, S., Keskar, N.S., Socher, R.: Regularizing and optimizing LSTM language\nmodels. CoRR abs/1708.02182 (2017), http://arxiv.org/abs/1708.02182\n9. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.E.: Regularizing neu-\nral networks by penalizing conﬁdent output distributions. CoRRabs/1701.06548\n(2017), http://arxiv.org/abs/1701.06548\n10. Press, O., Wolf, L.: Using the output embedding to improve language models.\nCoRR abs/1608.05859 (2016), http://arxiv.org/abs/1608.05859\n11. Smith, L.N.: A disciplined approach to neural network hyper-parameters: Part 1 -\nlearning rate, batch size, momentum, and weight decay. CoRR abs/1803.09820\n(2018), http://arxiv.org/abs/1803.09820\n12. Wang, S., Manning, C.D.: Baselines and bigrams: Simple, good sentiment and\ntopic classiﬁcation. In: Proceedings of the 50th Annual Meeting of the Association\nfor Computational Linguistics: Short Papers - Volume 2. pp. 90–94. ACL ’12,\nAssociation for Computational Linguistics, Stroudsburg, PA, USA (2012), http:\n//dl.acm.org/citation.cfm?id=2390665.2390688",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.4842493236064911
    },
    {
      "name": "Psychology",
      "score": 0.4414185583591461
    },
    {
      "name": "Computer science",
      "score": 0.4163975119590759
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4115789532661438
    },
    {
      "name": "Linguistics",
      "score": 0.37550005316734314
    },
    {
      "name": "Philosophy",
      "score": 0.10559487342834473
    }
  ],
  "institutions": []
}