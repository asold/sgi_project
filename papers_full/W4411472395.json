{
  "title": "Large language models show amplified cognitive biases in moral decision-making",
  "url": "https://openalex.org/W4411472395",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2152274041",
      "name": "Vanessa Cheung",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2400799195",
      "name": "Maximilian Maier",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2142775165",
      "name": "Falk Lieder",
      "affiliations": [
        "University of California System"
      ]
    },
    {
      "id": "https://openalex.org/A2152274041",
      "name": "Vanessa Cheung",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2400799195",
      "name": "Maximilian Maier",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2142775165",
      "name": "Falk Lieder",
      "affiliations": [
        "University of California System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367694135",
    "https://openalex.org/W4392504765",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4404780679",
    "https://openalex.org/W4323338447",
    "https://openalex.org/W4391872826",
    "https://openalex.org/W4378383736",
    "https://openalex.org/W4289527538",
    "https://openalex.org/W4362673335",
    "https://openalex.org/W4407084049",
    "https://openalex.org/W4292121845",
    "https://openalex.org/W3136913552",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W4378976798",
    "https://openalex.org/W4388067858",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4390723916",
    "https://openalex.org/W4392340595",
    "https://openalex.org/W2530269857",
    "https://openalex.org/W1593687854",
    "https://openalex.org/W2089012547",
    "https://openalex.org/W4386421934",
    "https://openalex.org/W4396608701",
    "https://openalex.org/W3126077297",
    "https://openalex.org/W3123960648",
    "https://openalex.org/W2039476912",
    "https://openalex.org/W4410929704",
    "https://openalex.org/W4388798177",
    "https://openalex.org/W1666036134",
    "https://openalex.org/W4253725888",
    "https://openalex.org/W2520323219",
    "https://openalex.org/W3007807084",
    "https://openalex.org/W2746351266",
    "https://openalex.org/W2167052337",
    "https://openalex.org/W2047174296",
    "https://openalex.org/W2009542038",
    "https://openalex.org/W3198909097",
    "https://openalex.org/W4283525019",
    "https://openalex.org/W4252488326",
    "https://openalex.org/W2122253967",
    "https://openalex.org/W2791773691",
    "https://openalex.org/W2226396244",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4392976263",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4396833703",
    "https://openalex.org/W4386981810",
    "https://openalex.org/W4391591806",
    "https://openalex.org/W4387929411",
    "https://openalex.org/W4367369313",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4401306886",
    "https://openalex.org/W4410954143",
    "https://openalex.org/W3046000984",
    "https://openalex.org/W4396519540",
    "https://openalex.org/W1986586077",
    "https://openalex.org/W2499446585",
    "https://openalex.org/W165840446",
    "https://openalex.org/W1497544907",
    "https://openalex.org/W2804021438",
    "https://openalex.org/W2166832985",
    "https://openalex.org/W3022851539",
    "https://openalex.org/W2623540162",
    "https://openalex.org/W1989658754",
    "https://openalex.org/W3210943603",
    "https://openalex.org/W1986646649",
    "https://openalex.org/W2800839347",
    "https://openalex.org/W2092923997",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W4224023100",
    "https://openalex.org/W4324129606",
    "https://openalex.org/W3202749896",
    "https://openalex.org/W4392056515",
    "https://openalex.org/W2100973578",
    "https://openalex.org/W1972605989",
    "https://openalex.org/W6722003616",
    "https://openalex.org/W4214936106",
    "https://openalex.org/W2168955200",
    "https://openalex.org/W2145073037",
    "https://openalex.org/W4404644684",
    "https://openalex.org/W4390824769",
    "https://openalex.org/W23274570",
    "https://openalex.org/W4402227480",
    "https://openalex.org/W2033768512",
    "https://openalex.org/W2051415128",
    "https://openalex.org/W2057453892",
    "https://openalex.org/W3125613397",
    "https://openalex.org/W2148237550",
    "https://openalex.org/W2025486241",
    "https://openalex.org/W2095690993",
    "https://openalex.org/W2077621850",
    "https://openalex.org/W2080835323",
    "https://openalex.org/W2075585362",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3088399774",
    "https://openalex.org/W1157560965",
    "https://openalex.org/W3091993284",
    "https://openalex.org/W4410535460",
    "https://openalex.org/W2313360560",
    "https://openalex.org/W2483932077",
    "https://openalex.org/W1560460460"
  ],
  "abstract": "As large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is, therefore, important to understand how well LLMs make moral decisions and how they compare to humans. We investigated these questions by asking a range of LLMs to emulate or advise on people’s decisions in realistic moral dilemmas. In Study 1, we compared LLM responses to those of a representative U.S. sample ( N = 285) for 22 dilemmas, including both collective action problems that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost–benefit reasoning against deontological rules. In collective action problems, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: They usually endorsed inaction over action. In Study 2 ( N = 474, preregistered), we replicated this omission bias and documented an additional bias: Unlike humans, most LLMs were biased toward answering “no” in moral dilemmas, thus flipping their decision/advice depending on how the question is worded. In Study 3 ( N = 491, preregistered), we replicated these biases in LLMs using everyday moral dilemmas adapted from forum posts on Reddit. In Study 4, we investigated the sources of these biases by comparing models with and without fine-tuning, showing that they likely arise from fine-tuning models for chatbot applications. Our findings suggest that uncritical reliance on LLMs’ moral decisions and advice could amplify human biases and introduce potentially problematic biases.",
  "full_text": null,
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.6236094832420349
    },
    {
      "name": "Action (physics)",
      "score": 0.5890620350837708
    },
    {
      "name": "Social psychology",
      "score": 0.5755234956741333
    },
    {
      "name": "Moral reasoning",
      "score": 0.4627091884613037
    },
    {
      "name": "Moral dilemma",
      "score": 0.4520924985408783
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}