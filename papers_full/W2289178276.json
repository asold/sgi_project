{
  "title": "GPURFSCREEN: a GPU based virtual screening tool using random forest classifier",
  "url": "https://openalex.org/W2289178276",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2911563893",
      "name": "Jayaraj P B",
      "affiliations": [
        "National Institute of Technology Calicut"
      ]
    },
    {
      "id": null,
      "name": "Ajay, Mathias K.",
      "affiliations": [
        "National Institute of Technology Calicut"
      ]
    },
    {
      "id": null,
      "name": "Nufail, M.",
      "affiliations": [
        "National Institute of Technology Calicut"
      ]
    },
    {
      "id": "https://openalex.org/A3145731015",
      "name": "Gopakumar G",
      "affiliations": [
        "National Institute of Technology Calicut"
      ]
    },
    {
      "id": "https://openalex.org/A4323132945",
      "name": "Jaleel U C A",
      "affiliations": [
        "Open Source Drug Discovery"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2024311391",
    "https://openalex.org/W2103581045",
    "https://openalex.org/W1974166884",
    "https://openalex.org/W1988195734",
    "https://openalex.org/W1985372952",
    "https://openalex.org/W1968506227",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1575085314",
    "https://openalex.org/W2014184229",
    "https://openalex.org/W1986303814",
    "https://openalex.org/W2162741763",
    "https://openalex.org/W2286462955",
    "https://openalex.org/W1493054534",
    "https://openalex.org/W4235474209",
    "https://openalex.org/W2199634952",
    "https://openalex.org/W2114704115",
    "https://openalex.org/W1985462363",
    "https://openalex.org/W1959511748",
    "https://openalex.org/W2140542009",
    "https://openalex.org/W2751318774",
    "https://openalex.org/W4243203374",
    "https://openalex.org/W2167865917",
    "https://openalex.org/W2256855865",
    "https://openalex.org/W1762731526",
    "https://openalex.org/W1554944419",
    "https://openalex.org/W2398736067",
    "https://openalex.org/W2125335678",
    "https://openalex.org/W1598873931",
    "https://openalex.org/W2040459007",
    "https://openalex.org/W164384110"
  ],
  "abstract": null,
  "full_text": "Jayaraj et al. J Cheminform  (2016) 8:12 \nDOI 10.1186/s13321-016-0124-8\nSOFTWARE\nGPURFSCREEN: a GPU based virtual \nscreening tool using random forest classifier\nP . B. Jayaraj1*, Mathias K. Ajay1, M. Nufail1, G. Gopakumar1 and U. C. A. Jaleel2\nAbstract \nBackground: In-silico methods are an integral part of modern drug discovery paradigm. Virtual screening, an in-silico \nmethod, is used to refine data models and reduce the chemical space on which wet lab experiments need to be \nperformed. Virtual screening of a ligand data model requires large scale computations, making it a highly time con-\nsuming task. This process can be speeded up by implementing parallelized algorithms on a Graphical Processing Unit \n(GPU).\nResults: Random Forest is a robust classification algorithm that can be employed in the virtual screening. A ligand \nbased virtual screening tool (GPURFSCREEN) that uses random forests on GPU systems has been proposed and evalu-\nated in this paper. This tool produces optimized results at a lower execution time for large bioassay data sets. The qual-\nity of results produced by our tool on GPU is same as that on a regular serial environment.\nConclusion: Considering the magnitude of data to be screened, the parallelized virtual screening has a significantly \nlower running time at high throughput. The proposed parallel tool outperforms its serial counterpart by successfully \nscreening billions of molecules in training and prediction phases.\nKeywords: In-silico drug discovery, Virtual screening, Ligand based drug discovery, Random forest classifier,  \nGPU computing, CUDA\n© 2016 Jayaraj et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, \nand indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/\npublicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\nBackground\nConventional drug design relied on in-vitro methods. \nIn these methods, wet lab experiments are employed \nto discover the activity of ligands with a target protein \nmolecule. Depending on this approach to determine the \nactivities of a large set of molecules would consume a lot \nof time and money. In-silico drug design breaks this bot -\ntleneck by using modern computational techniques. In-\nsilico approach increases the speed and reduces the cost \nof drug design. Depending upon the structural knowl -\nedge of the ligand and proteins, there are four different \napproaches used in in-silico drug discovery. They are \nstructure based drug design, ligand based drug design, \nde-novo design and library design. The most widely used \napproaches are structure based drug design and ligand \nbased drug design [1].\nLigand based drug design works by building a concep -\ntual model of the target protein. Ligand based virtual \nscreening uses this model to evaluate and separate active \nmolecules for a target protein. This process involves the \napplication of classification algorithms on the conceptual \nmodel. The model acts as the training data [ 2, 3] for the \nclassification algorithms used in virtual screening. These \nalgorithms are expected to learn the model parameters \nof the input training data. After the training phase, these \nalgorithms are applied on molecules whose activities with \nthe target protein are unknown. Based on the properties \nof the training data, the algorithms will be able to classify \nthese new molecules as active or inactive. This eliminates \nwet lab experiments involving inactive molecules from \nbeing carried out [ 4]. Hence, this approach drastically \nreduces the number of molecules with which the activity \nof a target is to be studied.\nImplementing virtual screening using sequential com -\nputing techniques often fail to produce results within \na reasonable time frame. The complexity of operations \nOpen Access\n*Correspondence:  jayarajpb@nitc.ac.in \n1 Department of Computer Science and Engineering, National Institute \nof Technology Calicut, NITC Campus, Calicut, Kerala 673601, India\nFull list of author information is available at the end of the article\nPage 2 of 10Jayaraj et al. J Cheminform  (2016) 8:12 \nexhibited by virtual screening encourages the usage of \nparallel computing techniques to tackle them. But, the \ninstallation and maintenance of an infrastructure for paral\n-\nlel computing with a large number of processors, such as \na Multiple Instruction Multiple Data (MIMD) system, is \nneither cost effective nor energy efficient. Computing on \nGraphical Processing Unit (GPU) offers a large number of \ncomputational cores that are capable of performing float\n-\ning point computations in parallel. The computation per -\nformance obtained by running this algorithm on a GPU is \ncomparable to that of a CPU cluster [5]. A GPU infrastruc\n-\nture can be installed and maintained at a comparatively \nlow cost. Hence, the advantage of using GPU computing \nis twofold; it is both cost effective and energy efficient. \nTherefore, implementing a parallelized version of virtual \nscreening using GPU computing is highly desirable.\nThe aim of virtual screening is to filter inactive mol\n-\necules from the active ones and not to misclassify any \nactive molecules. The success of ligand based drug discov\n-\nery depends on the effectiveness of the classifier used in \nvirtual screening. Ideal virtual screening requires a clas\n-\nsifier that could produce no false negatives and a lower \nnumber of false positives. This results a reduction in the \nspectrum of wet lab experiments being conducted. This \nreduces the cost. The popular classification algorithms \nthat are widely used for virtual screening are Support Vec\n-\ntor Machines (SVM) [6], Random Forest Classifiers (RFC) \n[7, 8] and Navïe Bayes Classifiers [9]. This is because they \nhave been found useful in many domains of bioinformat -\nics and medicinal chemistry [10–12].\nRandom forest classifier\nRandom forest classifiers work by growing a predeter -\nmined number of decision trees simultaneously [13]. \nThe internal nodes of a decision tree may contain one \nor more values. Each such internal node represents pos\n-\nsible outcomes of the problem given to it. Splitting of \ninternal nodes is based on maximum information gain \nusing GINI index. The growth of the tree is restricted \neither by pruning or by setting a threshold value on the \nsplitting criteria. The classification process is as follows. \nA test instance is run on all the decision trees grown in \nthe forest. Each tree’s classification of the test instance \nis recorded as a vote. The votes from all trees are aggre\n-\ngated and the test instance is assigned to the class that \nreceives the maximum vote.\nParallel decision trees\nThere are many reported works in this area. Some of \nthem are outlined in this section.\nNuno Amado et al. [14] presents an overview on the \nvarious ideas regarding implementing parallel decision \ntrees using data parallelism, task parallelism and hybrid \nparallelism in their work. They also describe a new par\n-\nallel implementation of the C4.5 decision tree construc -\ntion algorithm using breadth first strategy. A method \n[15] for implementing the evaluation and training of \ndecision trees and forests implemented completely on \na GPU (non-CUDA version) was reported in 2009. A \nubiquitous parallel computing approach for construct\n-\ning a decision tree on GPU is also available [16]. This \nwork exploits the divide and conquer parallelism in \nID3 at two levels. One at the outer level of building \nthe tree node by node in a top-down fashion. And the \nother at the inner level of sorting data records within \na single node level. Grahn et al. presents a parallel ver\n-\nsion of the Random Forest machine learning algorithm \nnamely CudaRF [17] which was implemented using the \nCompute Unified Device Architecture (CUDA). In this \nimplementation, one CUDA thread is used to build one \ntree in the forest. The forest is constructed on the GPU \nusing one thread for each decision tree in the random \nforest.\nThe study in [18] compares the effectiveness of Field \nProgrammable Gate Arrays (FGPAs), multi-core CPUs \nand GPUs for accelerating classification using models \ngenerated by Compact Random Forest (CRF) machine \nlearning classifiers. It was noted that FPGAs provided \nthe best performance and performance per watt. Multi-\ncore CPUs with OpenMP based implementation ensured \nscalability. GPUs offered the best performance per dollar \nand more than twice the performance of CPUs. The issue \nwith GPUs is that the performance deteriorated with \nlarger classifiers.\nAll previous works [15– 17] to use GPUs for Random \nForest classification have relied on coarse-grained \ntask parallelism and have yielded unsatisfactory \nresults. Their attempts seem to underutilize the avail\n-\nable parallelism of graphics hardware and have under -\ngone only cursory evaluations. Liao et  al. introduced \nCudaTree [19], a GPU Random Forest implementa\n-\ntion which adaptively switches between data and task \nparallelism.\nPage 3 of 10\nJayaraj et al. J Cheminform  (2016) 8:12 \nImplementation\nThe motivation for the parallelization of the process of \nbuilding a random forest stems from the fact that deci -\nsion trees in a random forest are built independent of one \nanother. This results in the faster construction of deci\n-\nsion trees. Thus, deployment of parallelized random for -\nest speeds up the ligand classification during the virtual \nscreening phase.\nFigure 1 illustrates the execution flow and communica\n-\ntion between the host and the device for the execution of \nthe proposed tool.\nIn GPU based computing, the process of growing each \ntree is assigned to a set of cores. The samples and features \nof a given tree are determined from the host system. The \ndecision tree is built level by level on a grid, by assigning \na block to build a level in a tree. The device memory of \nthe GPU is used to manage the tree data in a grid. Each \ngrid performs its evaluation independently.\nBuilding a random forest requires growing a certain \nnumber of decision trees. Many decision tree algorithms \nare based on recursion. But, the kernels executed on \nthe GPU do not support recursion [20]. So, the use of \nrecursion is not feasible in the GPU based Random For\n-\nest algorithm. This necessitates the design of an itera -\ntive tree building algorithm. The data and output of each \nsuch decision tree are not interdependent until execu -\ntion is completed. Hence, each such decision tree can \nbe handled as an independent thread in the GPU. There \nare other parallel decision tree learning algorithms [19, \n21, 22] that work by clubbing data and task parallelism. \nThis is similar to the approach taken in the proposed \nalgorithm.\nThere are two approaches to parallelize the building \nphase of a decision tree, namely data parallel depth first \ntree construction and fine grained task parallel breadth \nfirst construction [19]. The depth first tree construc\n-\ntion uses the GPU to compute the optimal split point for \na single node of the decision tree. Each CUDA thread \nblock is responsible for a subset of samples from a single \nfeature, followed by a parallel evaluation of the optimal \nfeature split thresholds. In breadth first tree construction, \ninstead of constructing a single tree node a whole level of \ntree nodes is created simultaneously.\nIn the serial implementation of Random Forest, the \ndepth first method is adopted to split the nodes based on \nselected features. From the GPU perspective, the depth \nfirst tree construction method will become more expen\n-\nsive as the tree grows deeper. This is because of a large \nnumber of kernels created to perform feature split on a \nrelatively small number of samples. Similarly, the breadth \nFig. 1 CPU–GPU execution control flow in the proposed parallel algorithm\nPage 4 of 10Jayaraj et al. J Cheminform  (2016) 8:12 \nfirst construction is less efficient at the top of a tree, as \nthe kernels would need to perform task parallel threads \non less number of nodes.\nIn order to completely utilize the parallelism pro\n-\nvided by the GPU, a hybrid approach is adopted in the \nproposed algorithm. The decision tree on GPU is con\n-\nstructed, starting from the root node in a depth first \nmanner. After a certain point, the tree construction \nis switched to breadth first approach. This crossover \npoint determines the efficiency of the GPU implemen\n-\ntation and the effectiveness of the results produced by \nthe Random Forest. This crossover point can be deter\n-\nmined by setting a threshold on the number of nodes \nthat is present in the decision tree. Once, the num\n-\nber of nodes in a decision tree crosses this threshold, \nthe tree construction strategy is switched from depth \nfirst to breadth first. A very low value of this thresh\n-\nold will result in poor performance of the algorithm \nwhereas a very large value will make the tree building \nslower. The only possible way to create a decision tree \nin a depth first manner in the GPU environment is by \nusing a stack. The breadth first method of tree construc\n-\ntion uses a queue. It is computationally faster to use \nthe breadth first method for large data on a GPU. So it \nis more productive to use this hybrid method for con\n-\nstructing decision trees on a GPU.\nDetermining the breadth first crossover threshold\nThe instance at which point the size of the sub-tree \nis large enough to merit a switch from depth first to \nbreadth first tree construction is critical. The scikit-learn \n[23] serial algorithm uses the depth first method for the \ntree construction. In the proposed parallel implementa\n-\ntion, calculating and fixing the crossover threshold deter-\nmines the speed of the algorithm. Since the number of \nfeatures used in the present implementation is fixed for \nthe parallelization of a given tree, the techniques derived \nby Liao et al. [19] are used here also. The optimal value of \nthe crossover threshold is derived by varying the number \nof samples, features, and classes used in the classification. \nThe optimal crossover value obtained through regression \ntechnique is:\nwhere n is the number of samples and f is the number \nof features considered at each node split. Therefore, the \noptimal crossover point is dependent on the number of \nfeatures and the number of training samples used in the \ntraining process.\nProposed parallel algorithm\nRandom forest building on a GPU begins with a parallel -\nized bootstrapping of the input data items on CUDA as \nshown in the Algorithm 1. It is then followed by the par\n-\nallel creation of decision trees in depth first manner. GINI \nindex is used for splitting the data. The childArray is used \nas a stack to create nodes as described in the Algorithm \n1. After the number of nodes in the decision tree crosses \nthe threshold, the tree creation switches to breadth first \nmode. Here, the childArray works as a queue and creates \na set of nodes level by level as shown in Algorithm 1. The \nalgorithm terminates by eliminating those nodes that fail \nto attain a threshold GINI index value.\n3705 + 0.0577 ∗n + 21.84 ∗f\nPage 5 of 10\nJayaraj et al. J Cheminform  (2016) 8:12 \nPage 6 of 10Jayaraj et al. J Cheminform  (2016) 8:12 \nRandom forest prediction run on a GPU is shown in \nAlgorithm 2. To classify a new instance, it is classified by \nall the trees in the forest in parallel. Each tree evaluates \nthe features for the new instance. The label after pro\n-\ncessing the input is recorded as a vote. The votes from \nall trees are combined and the class for which maximum \nvotes are counted (majority voting) is declared as the \nclass of the new instance.\nResults and discussion\nThe computational performance and quality of results \nobtained from the GPURFSCREEN for ligand based drug \ndiscovery are presented in this section.\nDatasets used\nDatasets with different numbers of molecules were used \nto gain insight into the quality of the result produced by \nTable 1 Datasets used\nPubChem bioassay  \ndatasets\nNumber  \nof molecules\nDate of access\nAID 1332 1193 22/07/2014\nAID 492952 2294 22/07/2014\nAID 651616 5569 22/07/2014\nAID 2330 36,869 22/07/2014\nAID 893 68,532 24/09/2014\nAID 778 95,859 22/07/2014\nAID 434955 323,578 28/07/2014\nAID 2314 2,964,564 10/09/2015\nTable 2 Hardware configuration used\nParticulars GPU1 GPU2\nGPU NVIDIA GeForce GTX 780 TESLA K20\nCUDA cores 2304 2496\nGPU clock speed 941 MHz 706 MHz\nGraphic memory 3072 MB 4800 MB\nMemory bandwidth 288.4 GB/S 208 GB/S\nPeak performance 4 TFlops 3.52 TFlops\nCompute capability 3.5 3.5\nCPU Intel Core i7 Xeon 2650\nTable 3 Performance of  random forest virtual screening \non serial environment\nDataset Recall Precision F-score ROC area Accuracy\nAID 1332 0.68 0.5 0.58 0.73 0.89\nAID 492952 0.68 0.87 0.76 0.65 0.68\nAID 651616 0.77 0.95 0.85 0.49 0.74\nAID 2330 0.58 0.37 0.45 0.67 0.93\nAID 893 0.73 0.49 0.59 0.74 0.94\nAID 778 0.49 0.34 0.40 0.62 0.78\nTable 4 Performance of  random forest virtual screening \non GPU\nDataset Recall Precision F-score ROC area Accuracy\nAID 1332 0.74 0.52 0.61 0.75 0.92\nAID 492952 0.73 0.87 0.8 0.66 0.72\nAID 651616 0.78 0.93 0.85 0.65 0.74\nAID 2330 0.54 0.38 0.45 0.68 0.93\nAID 893 0.72 0.49 0.58 0.73 0.94\nAID 778 0.48 0.34 0.4 0.62 0.78\nPage 7 of 10\nJayaraj et al. J Cheminform  (2016) 8:12 \nthis GPU version. Training data sets were obtained from \nNCBI PubChem [24] bioassay database which was pre -\npared from frozen stocks of Mtb H37Rv obtained from \nAmerican Type culture collections. For the screen, ami\n-\nkacin was included in the positive control wells in every \nassay plate. The bio-assay SDF file downloaded from \nPubChem was supplied as input to the PowerMV/CDK \n[25–28] feature extraction tool to generate 2D molecular \ndescriptors. A total of 179 descriptors were generated for \neach dataset of the experiment. The selection of descrip\n-\ntors was based on the criteria that they are sufficient to \ncharacterize the drug-likeness of a compound [4]. These \ndescriptors fall into three categories. The first eight \ndescriptors are used mainly to characterize the drug-\nlikeness of a compound. Another set of twenty-four con\n-\ntinuous descriptors considered are based on a variation \nof BCUT descriptors to define a low dimensional chem\n-\nistry space. The last 147 bit-string structural descriptors, \nknown as Pharmacophore Fingerprints, are based on \nbioisosteric principles. The dataset used for training are \nshown in Table  1. The dataset for testing is taken from \nGDB17 [29], a chemical universal database for unknown \ncompounds that has been enumerated by Lars Ruddig\n-\nkeit et al.\nThe technical specification of GPU hardware used in \nthe experimentation can be found in Table 2.\nInterpretation\nA Random Forest consists of a certain number of deci -\nsion trees. Their nodes split data according to randomly \nselected features. This offers a better method to split fea\n-\ntures apart from the split in data, thus enabling the clas -\nsifier to pick a combination of subtle changes in certain \nfeatures of the data.\nTables 3 and 4 show the comparison of serial and GPU \nimplementation of random forest on the basis of preci\n-\nsion, recall, accuracy, ROC area and F-score on different \nTable 5 Depth-bredth threshold crossover analysis for AID2314 training set\nCrossover  \nvalue\nRunning  \ntime (s)\nRecall Precision F-score Roc area Accuracy\n1000 10.53 0.63 0.40 0.49 0.681 0.91\n5000 10.75 0.62 0.39 0.48 0.679 0.91\n10,000 11.04 0.62 0.39 0.48 0.677 0.90\n15,000 10.58 0.62 0.40 0.48 0.681 0.90\n20,000 10.42 0.63 0.40 0.49 0.687 0.91\n25,000 10.07 0.62 0.39 0.48 0.681 0.90\n30,000 10.37 0.62 0.39 0.48 0.681 0.91\n40,000 10.73 0.62 0.40 0.48 0.685 0.91\n50,000 12.43 0.62 0.39 0.48 0.682 0.91\nTable 6 Running time of  serial and  GPU versions of  ran-\ndom forest virtual screening for training\nDataset No of  \nmolecules\nTime for  \n(s) serial\nTime for (s) \nfor GPU\nAID 1332 1193 0.0689 1.114\nAID 492952 2294 0.2095 1.2054\nAID 651616 5569 0.6641 1.7984\nAID 2330 36,869 4.2428 2.5487\nAID 893 68,532 11.5306 3.2746\nAID 778 95,859 35.8603 9.9027\nAID 434955 323,578 81.323 10.44\nAID 2314 330,664 100.57 10.77Table 7 Running time of serial and GPU versions of random forest virtual screening for classification\n* Serial exception error\nDataset No of molecules Date of access Time (s) for serial Time (s) for GPU\nGdb 17–0.5 million 0.5 million 12/11/2014 1.0296 6.5776\nGdb 17–1 million 1 million 12/11/2014 215.379 13.5085\nGdb 17–2 million 2 million 12/11/2014 1516.4383 25.9176\nGdb 17–2.5 million 2.5 million 12/11/2014 * 32.0973\nGdb 17–5 million 5 million 12/11/2014 * 69.4101\nGdb 17–7.5 million 7.5 million 12/11/2014 * 104.1336\nGdb 17–10 million 10 million 12/11/2014 * 129.7067\nPage 8 of 10Jayaraj et al. J Cheminform  (2016) 8:12 \nbioassay datasets. It can be concluded that the machine \nlearning metrics of Random Forest are not degraded \nwhen ported to the GPU. Random Forest Classifier from \nscikit-learn [23] was used to develop the serial version of \nvirtual screening. The GPU version of the virtual screen\n-\ning was developed in Python using PyCUDA libraries \n[30, 31].\nTable 5 shows the depth-breadth crossover analysis for \nthe AID2314 training set, for different values of crossover \npoint. With the number of descriptors being constant, \nthe optimal depth-breadth crossover largely depends on \nthe size of the training data set. Loading small training \ndata sets would build depth first driven trees while load\n-\ning large data sets would build breadth first driven trees. \nTherefore, depth-breadth crossover was studied over the \nrange from 1000 to 50,000. AID2314 is a balanced train\n-\ning set with 37,055 active out of the 296,456 compounds \npresent in it. It may be noted that performance matrices \ndo not change through the depth-breadth slide in the tree \nconstruction. The optimal crossover point of AID2314 is \nclose to 25,000. The running time of AID2314 is the best \nin the table. The end user can change the main param\n-\neters, such as bfs_threshold and no_of_trees_in_the_for -\nest in random-forest.py file in the code base for optimal \nperformance.\nTable 6 shows the performance comparison between \nthe training phase of the serial and that of GPU versions \nof RF based virtual screening on different ligand data \nmodels generated from the corresponding bioassay data \nin NCBI PubChem. Though there is a visible performance \ngain while using GPU, a major boost in performance is \nseen in the classification phase. The comparison of run\n-\nning times of the classification phase is shown in Table  7 \n(also see Fig.  2). For small input size, the performance \ngain was offset by the cost of copying the data to the GPU \nmemory. The GPU version of random forest guarantees \nan increase in execution speed by 2–20 times. The speed \nup of the implementation increases with the number of \nmolecules. The growth rate of execution time correspond\n-\ning to increasing input size is lower for the proposed par -\nallel implementation than the serial version. The proposed \ntool can easily take up billions of molecules for classifi\n-\ncation. Due to the difficulty in feature extraction of large \ninput data, the table size is limited to ten million.\nThe number of molecules that can be simultaneously \nclassified in the serial environment is constrained by the \namount of memory available in the machine. It is evident \nthat this new parallel tool GPURFSCREEN outperforms \nthe serial versions in terms of the number of molecules \nconsidered for training and classification. This parallel \nimplementation has successfully trained more than three \nhundred thousand molecules on a single batch. This can \nbe extended to up to one million in a single batch. The \nlarger the video RAM available on the GPU, greater the \nnumber of molecules that can be trained. It should be \nnoted that this implementation can take up billions of \nmolecules for screening, by using the technique of parti\n-\ntioning the data into batches.\nAs evident from the results, a huge performance gain \nis achieved in both training and prediction phases of \nthe learning algorithm. This contributes to a significant \nimprovement in virtual screening of ligand based data \nmodels. The performance of the random forest clas\n-\nsifier can be further improved by increasing the num -\nber of decision trees in the ensemble. The performance \nof the classifier algorithm improves with an increase in \nthe number of decision trees steadily up to a point, after \nwhich the performance starts to decline. This optimal \nFig. 2 Classification time comparison: serial versus GPU\nPage 9 of 10\nJayaraj et al. J Cheminform  (2016) 8:12 \npoint of the number of decision trees can be obtained by \nobserving the error rate. The global minima of error rate \nwith the change in the number of decision trees may be \ntaken as the optimal number of decision trees. However, \nthis number is specific to each ligand data set and cannot \nbe specified beforehand.\nConclusion\nA tool named GPURFSCREEN was developed for virtual \nscreening process using Random Forest technique that \nworks on a CUDA based GPU environment. Consid\n-\nering the large volume of data involved in ligand based \ndrug design, this parallelized version of virtual screening \nis favorable for two significant reasons: reduced running \ntime and high throughput. The computational perfor\n-\nmance offered by the GPU outperforms a multi-core sys -\ntem. Also, the cost of installation, power consumption \nand maintenance of a GPU based system are lower com\n-\npared to other multi-core systems. Thus, the GPU based \nvirtual screening for ligand based data sets is a viable \nalternative for quickly screening large quantities of ligand \ndata at a comparatively lower cost.\nA computational boost of 2–20 folds for Random For\n-\nest training and prediction is achieved on mediocre GPUs \nwith a moderate number of GPU cores and video RAM. \nGPUs with a large number of computational cores and \nlarger video RAM can run large bioassay data sets with \nsignificantly lower execution time. As a future extension, \nthe virtual screening of ligand data sets can be further \nimplemented and tested with other variants of random \nforest classifiers that implement balanced decision trees. \nThe GPU implementation can also be extended to work \nwith balanced decision trees for classification.\nAvailability and requirements\nName of tool: GPURFSCREEN\nTool home page: Source code available at http://ccc.\nnitc.ac.in/project/GPURFSCREEN\nOperating system: Linux Ubuntu 13.10\nProgramming language: Python\nFrame work: CUDA 6.0, PyCUDA.\nAuthors’ contributions\nPBJ, MKA, UCAJ and GG have contributed to the algorithm design and experi-\nmental evaluation. MKA along with PBJ have implemented the algorithm. \nNufail M has performed the testing. The manuscript was prepared by PBJ and \nGG. All authors read and approved the final manuscript.\nAuthor details\n1 Department of Computer Science and Engineering, National Institute \nof Technology Calicut, NITC Campus, Calicut, Kerala 673601, India. 2 Center \nfor Cheminformatics, Open Source Pharma, No. 22, World Trade Centre, \nMalleswaram, Bengaluru, Karnataka 560055, India. \nAcknowledgements\nThe authors express their heartfelt gratitude and appreciation to The Open \nSource Drug Discovery Consortium and TATA-CSIR for funding the project. \nThe authors are grateful to the Central Computer Centre, National Institute of \nTechnology Calicut for providing the CUDA based GPU infrastructure. Authors \nwould also like to express their sincere thanks to the Department of Computer \nScience and Engineering, National Institute of Technology Calicut for its \nconstant support.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 10 June 2015   Accepted: 16 February 2016\nReferences\n 1. Ekinsy S, Mestres J, Testa B (2007) In silico pharmacology for drug discov-\nery: methods for virtual ligand screening and profiling. Br J Pharmacol \n152:9–20\n 2. Gertrudes J, Maltarollo V, Silva R, Oliveira P , Honório K, da Silva A (2012) \nMachine learning techniques and drug design. Curr Med Chem \n19:4289–4297\n 3. Senanayake U, Prabuddha R, Ragel R (2013) Machine learning based \nsearch space optimisation for drug discovery. Proc IEEE Symp Comput \nIntell Bioinform Comput Biol 13:68–75\n 4. Schierz AC (2009) Virtual screening of bioassay data. J Cheminform \n21:1–12\n 5. Kirk DB, Hwu WW (2009) Programming massively parallel processors: a \nhands-on approach, 2nd edn. Morgan Kaufmann Publishers Inc., San \nFrancisco\n 6. Burbidge R, Trotter M, Buxton B, Holden S (2001) Drug design by machine \nlearning: support vector machines for pharmaceutical data analysis. \nComput Chem 26:5–14\n 7. Svetnik V, Liaw A, Tong C, Culberson JC, Sheridan RP , Feuston BP (2003) \nRandom forest: a classification and regression tool for compound clas-\nsification and QSAR modeling. J Chem Inf Comput Sci 43(6):1947–1958\n 8. Boulesteix AL, Janitza S, Kruppa J, König IR (2012) Overview of random \nforest methodology and practical guidance with emphasis on com-\nputational biology and bioinformatics. WIREs Data Min Knowl Discov \n2:493–507\n 9. Alpaydin E (2003) Introduction to machine learning, 2nd edn. MIT Press, \nCambridge\n 10. Mitchell T (1997) Machine learning, 1st edn. McGraw Hill, New York\n 11. Hastie T, Tibshirani R, Friedman J (2008) The elements of statistical learn-\ning data mining, inference and prediction statistics, 2nd edn. Springer, \nStanford\n 12. Chen B, Harrison RF, Papadatos G, Willett P , Wood DJ, Lewell XQ, Gree-\nnidge P , Stiefl N (2007) Evaluation of machine-learning methods for \nligand-based virtual screening. J Comput Aided Mol Des 21:53–62\n 13. Breiman L (2001) Random forests. Mach Learn 45:5–32\n 14. Amado N, Gama J, Silva F (2001) Parallel implementation of decision tree \nlearning algorithms. Prog Artif Intell 2258:6–13\n 15. Sharp T (2008) Implementing decision trees and forests on a GPU. Com-\nput Vis ECCV 2008(5305):595–608\n 16. Nasridinov A, Lee Y, Park YH (2014) Decision tree construction on GPU: \nubiquitous parallel computing approach. Computing 96:403–413\n 17. Grahn H, Lavesson N, Lapajne M, Slat D (2011) CudaRF: a CUDA-based \nimplementation of random forests. In: Proceedings of 9th IEEE/ACS inter-\nnational conference on computer systems and applications (AICCSA), pp \n95–101\n 18. Essen BV, Macaraeg C, Gokhale M, Prenger R (2012) Accelerating a ran-\ndom forest classifier: multi-core, GP-GPU, or FPGA? In: IEEE international \nsymposium on field-programmable custom computing machines vol 12, \npp 232–239\n 19. Liao Y, Rubinsteyn A, Power R, Li J (2013) Learning random forests on the \nGPU. New York University, Department of Computer Science\n 20. Jenkins J, Arkatkar I, Owens JD, Choudhary A, Samatova NF (2011) Les-\nsons learned from exploring the backtracking paradigm on the GPU. In: \nProceedings of 17th parallel processing international conference, Euro-\nPar 2011, Bordeaux, France, vol 6853, pp 425–434\nPage 10 of 10Jayaraj et al. J Cheminform  (2016) 8:12 \n 21. Kufrin R (1997) Decision trees on parallel processors. Mach Intell Pattern \nRecognit 20:279–306\n 22. Srivastava A, Han EH, Kumar V, Singh V (1998) Parallel formulations of \ndecision-tree classification algorithms. In: Proceedings of 27nd interna-\ntional conference on parallel processing, pp 237–244\n 23. Scikit-learn machine learning library. http://scikit-learn.org/\n 24. NCBI PubChem. https://pubchem.ncbi.nlm.nih.gov/\n 25. Chemistry Development Kit. http://cdk.sourceforge.net\n 26. PowerMv Molecular Viewer. http://nisla05.niss.org/PowerMV/\n 27. Liu K, Feng J, Brooks A, Young SS (2005) PowerMV: a software environ-\nment for molecular viewing, descriptor generation, data analysis and hit \nevaluation. J Chem Inf Model 45(2):515–522\n 28. Karelson M, Lobanov VS, Katrizky AR (1996) Quantum-chemical descrip-\ntors in QSAR/QSPR studies. Br J Pharmacol 9:1027–1041\n 29. Lars R, van Deursen R, Blum LC, Reymond JL (2012) Enumeration of 166 \nbillion organic small molecules in the chemical universe database GDB-\n17. J Chem Inf Model 52:2864–2875\n 30. Klockner A, Pinto N, Lee Y, Catanzaro B, Ivanov P , Fasih A (2012) PyCUDA \nand PyOpenCL: a scripting-based approach to GPU run-time code gen-\neration. Parallel Comput 38:157–174\n 31. Sanders J, Kandrot E (2011) CUDA by example: an introduction to general \npurpose GPU programming, 1st edn. Addison Wesley, Boston",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8382391929626465
    },
    {
      "name": "Virtual screening",
      "score": 0.8006591796875
    },
    {
      "name": "Random forest",
      "score": 0.7152171730995178
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5001912117004395
    },
    {
      "name": "Data mining",
      "score": 0.45936885476112366
    },
    {
      "name": "Task (project management)",
      "score": 0.42257770895957947
    },
    {
      "name": "Computation",
      "score": 0.4221244752407074
    },
    {
      "name": "Drug discovery",
      "score": 0.41925710439682007
    },
    {
      "name": "Chemical space",
      "score": 0.41718754172325134
    },
    {
      "name": "Process (computing)",
      "score": 0.4155414402484894
    },
    {
      "name": "Machine learning",
      "score": 0.3423546552658081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28388744592666626
    },
    {
      "name": "Algorithm",
      "score": 0.2432977855205536
    },
    {
      "name": "Bioinformatics",
      "score": 0.18977436423301697
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114845381",
      "name": "National Institute of Technology Calicut",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I2799982174",
      "name": "Open Source Drug Discovery",
      "country": "IN"
    }
  ]
}