{
  "title": "Foundation Models for Quantitative Biomarker Discovery in Cancer Imaging",
  "url": "https://openalex.org/W4386447283",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2222788173",
      "name": "Suraj Pai",
      "affiliations": [
        "Maastricht University",
        "Artificial Intelligence in Medicine (Canada)",
        "Brigham and Women's Hospital",
        "Dana-Farber Cancer Institute",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2970169823",
      "name": "Dennis Bontempi",
      "affiliations": [
        "Harvard University",
        "Maastricht University",
        "Brigham and Women's Hospital",
        "Dana-Farber Cancer Institute",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2490369133",
      "name": "Ibrahim Hadzic",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University",
        "Dana-Farber Cancer Institute",
        "Artificial Intelligence in Medicine (Canada)",
        "Maastricht University"
      ]
    },
    {
      "id": "https://openalex.org/A5092529477",
      "name": "Vasco Prudente",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Harvard University",
        "Brigham and Women's Hospital",
        "Maastricht University",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2548942906",
      "name": "Mateo Sokač",
      "affiliations": [
        "Aarhus University",
        "Aarhus University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2641497968",
      "name": "Tafadzwa L Chaunzwa",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Brigham and Women's Hospital",
        "Artificial Intelligence in Medicine (Canada)",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2277424640",
      "name": "Simon Bernatz",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Brigham and Women's Hospital",
        "Dana-Farber Cancer Institute",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2102130635",
      "name": "Ahmed Hosny",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Artificial Intelligence in Medicine (Canada)",
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2486088239",
      "name": "Raymond H. Mak",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Maastricht University"
      ]
    },
    {
      "id": "https://openalex.org/A4261283953",
      "name": "Nicolai J. Birkbak",
      "affiliations": [
        "Aarhus University",
        "Aarhus University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4303876386",
      "name": "Hugo JWL Aerts",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Maastricht University",
        "Dana-Farber Cancer Institute",
        "Dana-Farber Brigham Cancer Center",
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2222788173",
      "name": "Suraj Pai",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2970169823",
      "name": "Dennis Bontempi",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Dana-Farber Brigham Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2490369133",
      "name": "Ibrahim Hadzic",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Dana-Farber Brigham Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5092529477",
      "name": "Vasco Prudente",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2548942906",
      "name": "Mateo Sokač",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2641497968",
      "name": "Tafadzwa L Chaunzwa",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Dana-Farber Brigham Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2277424640",
      "name": "Simon Bernatz",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2102130635",
      "name": "Ahmed Hosny",
      "affiliations": [
        "Dana-Farber Brigham Cancer Center",
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2486088239",
      "name": "Raymond H. Mak",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A4261283953",
      "name": "Nicolai J. Birkbak",
      "affiliations": [
        "Aarhus University Hospital",
        "Aarhus University"
      ]
    },
    {
      "id": "https://openalex.org/A4303876386",
      "name": "Hugo JWL Aerts",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "Dana-Farber Brigham Cancer Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4366598511",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3032723274",
    "https://openalex.org/W2903150666",
    "https://openalex.org/W4292318865",
    "https://openalex.org/W4367055910",
    "https://openalex.org/W2883683269",
    "https://openalex.org/W2124932867",
    "https://openalex.org/W4233875907",
    "https://openalex.org/W3120430728",
    "https://openalex.org/W4291023040",
    "https://openalex.org/W3046015102",
    "https://openalex.org/W4286487934",
    "https://openalex.org/W2751538714",
    "https://openalex.org/W2888844929",
    "https://openalex.org/W3213687503",
    "https://openalex.org/W3033333779",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W2967302726",
    "https://openalex.org/W3203825712",
    "https://openalex.org/W4380854412",
    "https://openalex.org/W3202940709",
    "https://openalex.org/W2231056033",
    "https://openalex.org/W6754669440",
    "https://openalex.org/W3202023533",
    "https://openalex.org/W2584017349",
    "https://openalex.org/W3011008306",
    "https://openalex.org/W2555897561",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W4392697024",
    "https://openalex.org/W1408981388",
    "https://openalex.org/W2966710588",
    "https://openalex.org/W2979708377"
  ],
  "abstract": "Abstract Foundation models represent a recent paradigm shift in deep learning, where a single large-scale model trained on vast amounts of data can serve as the foundation for various downstream tasks. Foundation models are generally trained using self-supervised learning and excel in reducing the demand for training samples in downstream applications. This is especially important in medicine, where large labeled datasets are often scarce. Here, we developed a foundation model for imaging biomarker discovery by training a convolutional encoder through self-supervised learning using a comprehensive dataset of 11,467 radiographic lesions. The foundation model was evaluated in distinct and clinically relevant applications of imaging-based biomarkers. We found that they facilitated better and more efficient learning of imaging biomarkers and yielded task-specific models that significantly outperformed their conventional supervised counterparts on downstream tasks. The performance gain was most prominent when training dataset sizes were very limited. Furthermore, foundation models were more stable to input and inter-reader variations and showed stronger associations with underlying biology. Our results demonstrate the tremendous potential of foundation models in discovering novel imaging biomarkers that may extend to other clinical use cases and can accelerate the widespread translation of imaging biomarkers into clinical settings.",
  "full_text": "Foundation Models for Quantitative Biomarker Discovery in Cancer 1 \nImaging 2 \n 3 \nAuthors | Suraj Pai1,2,3, Dennis Bontempi1,2,3, Vasco Prudente1,2,3, Ibrahim Hadzic1,2.3, Mateo Sokač4,5, Tafadzwa 4 \nL. Chaunzwa1,3, Simon Bernatz1,3, Ahmed Hosny1,3, Raymond H Mak1,2, Nicolai J Birkbak4,5, Hugo JWL Aerts1,2,3,6 5 \n 6 \nAffiliations | 1 Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical 7 \nSchool, Harvard Institutes of Medicine, 77 Avenue Louis Pasteur, Boston, MA 02115, United States of America; 8 \n2Radiology and Nuclear Medicine, CARIM & GROW, Maastricht University, Universiteitssingel 40, 6229 ER 9 \nMaastricht, The Netherlands; 3Department of  Radiation Oncology, Brigham and Women’s Hospital, Dana -10 \nFarber Cancer Institute, Harvard Medical Scho ol, 75 Francis Street and 450 Brookline Avenue, Boston, MA 11 \n02115, USA; 4Department of Molecular Medicine, Aarhus University Hospital, 8200 Aarhus, Denmark; 12 \n5Department of Clinical Medicine, Aarhus University, 8200 Aarhus, Denmark; 6Department of Radiology,  13 \nBrigham and Women’s Hospital, Dana-Farber Cancer Institute, Harvard Medical School, 75 Francis Street and 14 \n450 Brookline Avenue, Boston, MA 02115, USA;  15 \n 16 \nRunning title | Foundation Model for Cancer Imaging Biomarkers 17 \n 18 \nCorresponding author | Hugo JWL Aerts, Ph.D., Artificial Intelligence in Medicine (AIM) Program, Mass 19 \nGeneral Brigham, Harvard Medical School, Harvard Institutes of Medicine – HIM 343, 77 Avenue Louis Pasteur, 20 \nBoston, MA 02115, P - 617.525.7156, F - 617.582.6037, Email: Hugo_Aerts@DFCI.harvard.edu 21 \n 22 \n  23 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract | Foundation models represent a recent paradigm shift in deep learning, where a single large-scale 24 \nmodel trained on vast amounts of data can serve as the foundation for various downstream tasks. 25 \nFoundation models are generally trained using self-supervised learning and excel in reducing the demand 26 \nfor training samples in downstream applications. This is especially important in medicine, where large 27 \nlabeled datasets are often scarce. Here, we developed a foundation model for imaging biomarker discovery 28 \nby training a convolutional encoder through self -supervised learning using a comprehensive dataset of 29 \n11,467 radiographic lesions. The foundation model was evaluated in distinct and clinically relevant 30 \napplications of imaging-based biomarkers. We found that they facilitated better and more efficient learning 31 \nof imaging biomarkers and yielded task-specific models that significantly outperformed their conventional 32 \nsupervised counterparts on downstream tasks. The performance gain was most prominent when training 33 \ndataset sizes were very limited. Furthermore, foundation models were more stable to input and inter-reader 34 \nvariations and showed stronger associations with underlying biology. Our results demonstrate the 35 \ntremendous potential of foundation models in discovering novel imaging biomarkers that may extend to 36 \nother clinical use cases and can accelerate the widespread translation of imaging biomarkers into clinical 37 \nsettings.  38 \n 39 \n 40 \nINTRODUCTION 41 \nFoundation models present a paradigm  shift in deep learning wherein a model trained on vast amounts of 42 \nunannotated data can serve as the foundation of a wide range of downstream tasks. Recently foundation 43 \nmodels have provided unprecedented performance gains in language, vision, and several o ther domains1. In 44 \nthe field of natural language processing (NLP), for example, foundation models drive the successes of 45 \napplications such as ChatGPT 2, BERT3, and CLIP 4. Similarly, foundation models, such as SimCLR 5 and DINO6, 46 \nhave reported considerable success in computer vision applications.  47 \nMedicine represents a vast potential for foundation models as labeled data are scarce, while 48 \nmultimodal data, such as medical images, biologic, and cl inical notes, are frequently collected in routine 49 \nclinical care7. Indeed, different applications of foundation models, such as augmented surgical procedures, 50 \nbedside decision support, interactive radiology reports, and note-taking, have been reported8.  51 \nWhile many studies investigating imaging -based biomarkers incorporate supervised  deep learning 52 \nalgorithms into their models9–11, they are typically applied in scenarios where large datasets are available for 53 \ntraining and testing. The quantity and quality of annotated data are strongly linked to the robustness of deep 54 \nlearning models. Access to large amounts of annotated data for specialized applications is often challenging 55 \nand demands expertise, time, and labor. In such scenarios, many in vestigators fall back on traditional 56 \nhandcrafted or engineered approaches based on defined mathematical and statistical algorithms that analyze 57 \nattributes like the shape and texture of objects in images, which limit the scope of discovery. This caveat is 58 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \ncommonplace in many scenarios where insights from imaging -based biomarkers have great potential in 59 \ninforming clinical care.  60 \nFoundation models are generally pre -trained using self -supervised learning (SSL), a set of methods 61 \nthat leverage innate information available within data by learning generalized, task-agnostic representations 62 \n(features) from large amounts of unannotated samples. Existing literature 12 has suggested several strategies 63 \nto pre-train networks to learn these representations. Approaches such as defining pre -text tasks that distort 64 \nan image and attempt to reconstruct the original or contrastively learning similar representations for 65 \naugmented views of the s ame image have primarily been investigated. Following pre -training, foundation 66 \nmodels can be applied to task -specific problems, improving generalization, especially in tasks with small 67 \ndatasets. The expanding literature on SSL in medical imaging 13 focuses primarily on two-dimensional images 68 \n(X-ray, whole slide images, dermatology images, fundus images, etc.) and diagnostic applications. There is still 69 \nlimited evidence investigating whether SSL can help train foundation models that learn general, robust, and 70 \ntransferrable representations that can act as imaging biomarkers, especially prognostic, for tasks of clinical 71 \nrelevance. 72 \nIn this study, we investigated whether founda tion models pre-trained using self-supervised learning 73 \ncan improve the development of deep learning -based imaging biomarkers. The foundation model was pre -74 \ntrained on 11,467 diverse and annotated lesions identified on computed tomography (CT) imaging from 2,312 75 \nunique patients14. The model was first technically validated on the classification of anatomical site lesions (use-76 \ncase 1). Subsequently, it was applied to two di stinct clinically relevant applications: the development of a 77 \ndiagnostic biomarker that predicts the malignancy of lung nodules (use-case 2) and a prognostic biomarker for 78 \nnon-small cell lung cancer tumors in confirmed cancer cases (use -case 3). We evaluat ed two distinct 79 \napproaches of how a pre -trained foundation model can be incorporated into training pipelines for 80 \ndownstream tasks, a direct approach of using the foundation model as a feature extractor combined with a 81 \nlinear classifier and another approach where the foundation model is fine-tuned through deep learning. The 82 \nperformance of the foundation model approaches was then evaluated and compared to conventional 83 \nsupervised approaches in the three clinical use cases. Our analysis delves into limited data  scenarios, 84 \nevaluating test-retest and inter-reader stability, determining explainability and interpretability through deep-85 \nlearning attribution methods, and exploring biological associations with gene expression data. Our results 86 \ndemonstrate the potential of foundation models in discovering novel imaging biomarkers and their particular 87 \nstrength in applications with limited datasets. This evidence may extend to other clinical use cases and imaging 88 \nmodalities and can accelerate the widespread development and translation of imaging biomarkers into clinical 89 \nsettings. To allow effortless incorporation, external evaluation, and validation, we are providing open access 90 \nto the foundation model along with reproducible workflows. 91 \n 92 \n 93 \n 94 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nRESULTS 95 \nWe developed a foundation deep learning model using SSL and tested the model's performance in three 96 \ndistinct use cases. The study design and the pre -training process are outlined in Fig. 1. We developed the 97 \nfoundation model using a dataset with 11,467 anno tated CT lesions identified from 2,312 unique patients. 98 \nLesion findings were diverse and included multiple lesions, such as lung nodules, cysts, and breast lesions, 99 \namong numerous others. A task-agnostic contrastive learning strategy was used to pre-train the model on 100 \nthese lesion findings (see Fig. 1a), which subsequently was evaluated in three diverse clinical applications and 101 \nfive distinct datasets (see Fig. 1b).  102 \n 103 \nLesion anatomical site classification (Use -case 1).  As a technical validation of the performance of the 104 \nfoundation model, we selected an in -distribution task (i.e., sourced from the same cohort as that of the 105 \nfoundation model pre-training) on 5,051 annotated lesions (see Use-case 1 in Fig. 1b). These specific lesions, 106 \nhowever, were not included in the pre -training data. Classification models were developed to predict the 107 \ncorrect anatomical site using a training and tuning dataset totaling 3,830 lesions. On an independent test set 108 \nof 1,221 lesions, we evalua ted the performance of two different implementations of the foundation model 109 \n(see Fig. 1c).  110 \nWe found that the foundation model approaches significantly outperformed the current standard 111 \nsupervised approach using a randomly initialized model (i.e., random initialization of weights; see Fig. 1d) in 112 \nterms of balanced accuracy (BA) and mean average precision (mAP) (see Fig. 2a, b ). When comparing 113 \nclassification performances, the foundation features-based classifier (0.779 [95% CI 0.749-0.809], p<0.01) and 114 \nthe fine-tuned foundation model (0.804 [95% CI 0.773-0.834], p<0.01), significantly improved BA (p<0.01) over 115 \nthe supervised model (0.72, [95% CI 0.689 -0.750], p<0.01) (see Fig. 2a ). In terms of mAP, the fine -tuned 116 \nfoundation model (0.856, [95% CI 0.828 -0.886], p<0.01) provided a significant (p<0.01) performance benefit 117 \nover the supervised model (mAP=0.818 [95% CI 0.779-0.847], p<0.01) (see Fig. 2b) 118 \nThe performance advantage of the foundation model was even stronger in limited data scenarios (see 119 \nFig. 2a, b). When we reduced training data to 50% (n=2526), 20% (n=1010), and 10% (n=505), the foundation 120 \nmodel as a feature extractor significantly improved BA and mAP over the supervised model. The fine -tuned 121 \nfoundation model also significantly improved over the supervised model but failed to improve when training 122 \ndata was reduced to 10%. Individual comparisons between each model at different data percentages can be 123 \nfound in the supplementary material (see Extended Data Table 1). 124 \nTo investigate feature separability, wh ich indicates how well features can discriminate between 125 \nanatomical sites, we used dimensionality reduction methods to visualize features generated on the test set by 126 \nthe foundation and the trained supervised models. The features from the foundation model produced 127 \nsemantically separable clusters for each anatomical site, while features from the supervised model showed 128 \npoor separability (see Fig. 2c-d). Of note, unlike the supervised model, the foundation model was not exposed 129 \nto anatomical site information during training. 130 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 131 \nNodule malignancy prediction (Use case 2). To assess the robustness of the foundation model, we chose an 132 \nout-of-distribution task (i.e., belonging to a different cohort than that of the foundation model training data) 133 \ninvolving predicting the malignancy of lung nodules from the LUNA16 dataset (see Use-case II in Fig. 1b). We 134 \nconducted our training on a labeled subset of 507 lung nodules with indications of malignancy suspicion. On 135 \nan independent test set of 170 nodules, we evaluated the pe rformance of the two foundation model 136 \nimplementations and two supervised learning approaches - random initialization and fine-tuning from another 137 \nsupervised model. The model trained in use case 1 was chosen for the supervised fine-tuning.  138 \nThe approach of fine-tuning the foundation model resulted in significant (p<0.01) superiority over 139 \nboth the supervised learning approaches (see Fig. 3a, b). The fine-tuned foundation model achieved an area-140 \nunder receiver operating curve (AUC) of 0.944 (95% CI 0.914-0.982, p<0.01) and mAP of 0.952 (95% CI 0.926-141 \n0.986, p<0.01) compared to the fine-tuned supervised model's AUC of 0.857 (95% CI 0.806-0.918, p<0.01) and 142 \nmAP of 0.874 (95% CI 0.822-0.936, p<0.01).  143 \nWhen analyzing reduced data sizes, the fine -tuned foundation mode l significantly (p<0.01) 144 \noutperformed the fine -tuned supervised model when data was reduced to 50% (n=254) and 20% (n=101). 145 \nHowever, it did not significantly improve when data was reduced to 10% (n=51). In contrast, the foundation 146 \nmodel as a feature -extractor improved significantly (p < 0.005) over all other models at 10%. Moreover, 147 \nperformance from the foundation model as a feature extractor remained relatively stable even when trained 148 \non 10% of the data, while all other models showed a significant drop in performance. Across the limited data 149 \nevaluation, although fine-tuned supervised models showed a trend of improvement over randomly initialized 150 \nsupervised models, they were not found to be significant (p>0.05). Detailed comparisons can be found in the 151 \nsupplementary material (see Extended Data Table 2) 152 \nWe observed that representations from the foundation model demonstrated superior linear 153 \ndiscrimination compared to the supervised model, where samples remained interspersed between the classes 154 \n(see Fig. 3c, 3d). 155 \n 156 \nPrognostication performance for non-small cell lung cancer (NSCLC) tumors (Use case 3). 157 \nIn the last use case, we evaluated the ability of the foundation model to capture quantitative radiographic 158 \nphenotypes of NSCLC tumors and consequently determine the prognosis of patients using three independent 159 \ncohorts of patients treated with surgery or radiation, HarvardRT (n=291), LUNG1 (n=421) and RADIO (n=144) 160 \n(see use-case 3 in Fig. 1b). We aimed to investigate the performance of foundation model implementatio ns 161 \nwhen trained and applied to cohorts with strong distribution shifts (cohorts from separate institutions with 162 \ndifferent standards of care). Therefore, we trained and tuned our prognostication models using data from the 163 \nHarvardRT cohort to predict 2 -year overall survival after treatment and then compared the performance of 164 \nthe foundation model and supervised approaches on the LUNG1 and RADIO cohorts. 165 \n 166 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nIn the LUNG1 cohort, foundation models outperformed both supervised methods, with statistical significance 167 \n(p<0.05). Features extracted from the foundation model obtained an AUC of 0.637 (95% CI 0.583-0.691), and 168 \nfine-tuning the foundation model resulted in an AUC of 0.619 (95% CI 0.564 -0.674), as shown in Fig. 4a. In 169 \ncomparison, training supervised models with randomly initialized weights resulted in an AUC of 0.531 (95% CI 170 \n0.475-0.587). Fine-tuning a supervised model trained on a different task (use-case 1) showed an AUC of 0.566 171 \n(95% CI 0.510-0.622). The best-supervised model (supervised fine-tuned) and the foundation model (features 172 \n+ linear classifier) were evaluated using Kaplan-Meier survival analysis, shown in Fig. 4c and 4e, respectively. 173 \nThe foundation model demonstrated higher prognostic power by better stratifying mortality, as shown by a 174 \nlower p-value (p<0.0001) when split by the median on the tuning set, compared to the supervised model 175 \n(p=0.03). Kaplan -Meier curves and univariate Cox regression for all of the models can be found in the 176 \nsupplementary (see Extended Data Fig. 1, Table 3) 177 \nIn the RADIO cohort, the foundation model as a feature extractor performed the best, with an AUC of 178 \n0.61 (95% CI 0.501-0.720).  Supervised models trained with random initialization had an AUC of 0.532 (95% CI 179 \n0.426-0.639) while fine-tuning a supervised mod el led to an AUC of 0.567 (95% CI 0.468 -0.665). Fine-tuning 180 \nthe foundation model did not improve performance, yielding an AUC of 0.532 (95% CI 0.428-0.636), as shown 181 \nin Fig. 4b. Using foundation model features was significantly better than the randomly ini tialized supervised 182 \nmodel (p<0.05), but none of the other networks showed significant differences from the rest (p>0.05). Kaplan-183 \nMeier survival analysis demonstrated significant stratification for the feature -extractor foundation model 184 \npredictions (p=0.008) compared to the fine -tuned supervised model (p=0.138), as shown in Fig. 4d and 4f. 185 \nKaplan-Meier curves and univariate Cox regression for all of the models can be found in the supplementary 186 \nmaterial (see Extended Data Fig. 1, Table 3). 187 \n 188 \nStability of the foundation model. We evaluated the stability of our foundation model and compared it against 189 \nsupervised approaches in two ways: through a test-retest scenario and an inter-reader variability analysis. To 190 \nassess test-retest robustness, we used scans from 26 patients from the RIDER dataset 15 taken within a 15 -191 \nminute interval using the same imaging protocol. We found that predictions from the best-performing models, 192 \nfeature-extractor foundation, and fine -tuned supervised had high stability with intraclass correlation 193 \ncoefficient (ICC) values of 0.98 and 0.97, respectively. Furthermore, the test-retest features for both networks 194 \nwere strongly correlated (as shown in Extended Data Fig. 2a and 2b). 195 \nTo evaluate stability against inter -reader variability, we used the LUNG1 dataset and perturbed the 196 \ninput seed point to extract the 3D volume, simulating variations among human readers. We found that the 197 \nfeature-extractor foundation mo dels had higher stability against simulated inter -reader variations in 198 \nprediction performance than the fine-tuned supervised models (see Extended Data Fig. 2c and 2d). 199 \n 200 \nSaliency maps for fine-tuned foundation models. To gain insight into the regions of the input volumes that 201 \ncontribute to a given prediction, we employed gradient-based saliency maps for foundation models fine-tuned 202 \non three selected use cases (as depicted in Fig. 5). We used smooth guided back-propagation16,17 to compute 203 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nthe gradient of the output with respect to the input while keeping the model weights constant. This provided 204 \ninsight into the regions of the input that had the most significant influence on the output prediction. 205 \nOur analysis revealed that fine -tuned foundation models for each use case focused on different 206 \nregions but largely converged on tissues within or in proximity to the tumor. This is consistent with research 207 \ndemonstrating the tumor microenvironment's influence on cancer development18 and prognosis.  Specifically, 208 \nlesion anatomical site classification models (as depicted in Fig. 5a) focused mainly on areas surrounding the 209 \nlesions, such as the parenchyma and bone regions in the lung and the trachea in mediastinal lesions. On the 210 \nother hand, nodule malignancy models (as depicted in Fig. 5b) primarily concentrated on the tissues of the 211 \nnodule while avoiding high -density bone regions. In the case of prognosis networks (as depicted in Fig. 5c), 212 \nthe model predictions were primarily attributed to areas surrounding t he center of mass of the tumor, with 213 \nsome contribution from high -density bone regions. Overall, these findings indicated that the areas that 214 \ncontribute to the networks' predictions varied in accordance with the specific use case, with the tumor and 215 \nsurrounding tissues playing a pivotal role.  216 \n 217 \nUnderlying biological basis of the foundation model. Finally, we investigated the biological basis of our 218 \nfoundation model by analyzing gene expression data associated with model predictions for 130 subjects from 219 \nthe RADIO dataset. To identify relevant genes, we selected the top 500 genes and performed a correlation 220 \nanalysis, comparing the feature-extractor foundation and fine-tuned supervised model predictions with gene 221 \nexpression profiles. We found that absolute corr elation coefficients between gene expression profiles and 222 \nmodel predictions were significantly higher (p=0.008) for the foundation model, indicating a stronger 223 \nassociation with underlying tumor biology (see Fig. 6a).  224 \nAdditionally, we examined the genes associated with these models through a gene set enrichment 225 \nanalysis (genes with a correlation coefficient> 0.1). Our analysis revealed that foundation models showed a 226 \npattern of enrichment of immune -associated pathways, including interferon signaling, interferon gamma 227 \nsignaling, MHC class II antigen presentation, and PD -1 signaling. Conversely, while the supervised model did 228 \nshow enrichment of individual pathways, no identifiable pattern was observed (see Fig. 6b). 229 \n 230 \n 231 \n 232 \n 233 \n 234 \n 235 \n 236 \n 237 \n 238 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nDISCUSSION 239 \nIn this study, we demonstrated that our foundation model trained using self -supervised learning, provided 240 \nrobust quantitative biomarkers for predicting anatomical site, malignancy, and prognosis across three 241 \ndifferent use cases in four cohorts. Several studies 19–21 have demonstrated the efficacy of self -supervised 242 \nlearning in medicine where only limited data might be available for training d eep learning networks. Our 243 \nfindings complement and extend this for identifying reliable imaging biomarkers for cancer -associated use 244 \ncases. We showed that our foundation model provided superior performance for anatomical lesion site and 245 \nmalignancy prediction. Modeling using features extracted from the foundation model was the most robust 246 \nacross tasks offering stable performance even when data sizes were considerably reduced to 51 samples (10% 247 \nof use-case 2). These features could also categorize data from these tasks into semantically separable clusters 248 \ncorresponding strongly with target classes, although these features were learned independent of class 249 \ninformation. Using these features provided the best performance on small cohorts in predicting prognosis and 250 \nalso demonstrated significant stratification of patients by their associated risk for each of the LUNG1 and 251 \nRADIO cohorts (p<0.01). Additionally, predictions using the foundation model features were found to be highly 252 \nstable against inter -reader (standard deviation=0.004) and test -retest variations (ICC=0.98). Regarding the 253 \ninterpretability of features, we observed that models focused on varying regions of the tumor and surrounding 254 \ntissue relevant to the associated use case. To gain insight into the under lying biological associations of these 255 \nfeatures, RNA sequencing analysis combined with imaging data showed that these features correlated with 256 \nimmune-associated pathways.  257 \n  Studies for predicting endpoints, such as overall survival on small cohorts largel y rely on statistical 258 \nfeature extraction (engineered radiomics) and classical machine learning -based modeling. Precise three -259 \ndimensional segmentations are required for extracting these statistical features from tumor volumes 260 \nincreasing the annotation burde n associated with these studies. Moreover, these statistical features are 261 \naffected by several confounders, such as inter -reader variability in segmentations 22 and acquisition settings 262 \nof the scanners 23. Deep learning methods, in comparison, are robust to differences in acquisition and 263 \nsegmentation variability and provide improved performance over statistical features 10. However, they remain 264 \nrestricted in their applicability in such low -data scenarios due to their dependency on large amounts of d ata 265 \nto provide robust performance. Training deep -learning models on small cohorts often lead to overfitting, 266 \nwhich diminishes performance when external data is introduced 11. Our foundation model approach has 267 \nseveral innovations: first, we developed a deep -learning system on a large corpus of 3D lesion images with 268 \nconsiderable diversity in their presentation. To our knowledge, our study is the first to pre -train a deep -269 \nlearning model using 11,467 3 -dimensional lesion volumes. Second, we demonstrated that our pre -trained 270 \nmodel learned generalizable features and improved performance across three tasks and associated endpoints. 271 \nOur model also provided prognostic value when t rained on small cohorts and applied to external validation 272 \ncohorts. Third, our models showed high robustness to test-retest and inter-reader variations. Finally, we share 273 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nour validated foundation model with the public, allowing external testing and future studies to facilitate their 274 \nadoption into external workflows.  275 \nSeveral studies have investigated deep learning algorithms for identifying cancer imaging biomarkers 276 \nin both small and large cohorts. Hosny et al. 10 trained a deep learning model for lung cancer prognostication 277 \nusing several multi-institutional cohorts and demonstrated strong performance using deep learning methods 278 \nover traditional radiomics features. Kumar e t al.24 identified radiomic sequences using deep convolutional 279 \nencoders for determining the malignancy of lung nodules from the LIDC-IDRI dataset considering 4306 lesions. 280 \nLao et al.25 proposed a deep-learning model-based radiomics signature for predicting survival in glioblastoma 281 \nmultiforme, trained and validated on relatively small cohorts. Haarburger et al.26 present a deep convolutional 282 \nnetwork-based approach to predict survival endpoints on the LUNG1 dataset. Cho et al. 27 developed a 283 \nradiomics-guided deep-learning model for stratifying the prognosis of lung adenocarcinoma and validated it 284 \nin a local cohort and an external validation cohort. A general trend observed across these studies is that the 285 \nperformance of deep learning models is more robust when larger and multi-institutional cohorts are available 286 \nfor training. Validation is subsequently performed on cohorts smaller than the training cohort. A demonstrated 287 \nstrength of our approach is that training on smaller cohorts performs well in larger validation cohorts. For the 288 \nprognostication use case, we performed well on two external validation cohorts with a combined size 289 \nconsiderably larger than the training cohort. Our pre -trained foundation model shows strong generalization 290 \nability across our diverse use cases and may apply to several other cancer imaging use cases out of the box. 291 \nFurthermore, extracting features from our model (inference only) followed by simple modeling methods is 292 \nresource-efficient, alleviating the need fo r expensive hardware for training standard deep -learning models 293 \nwhile providing on-par performance. 294 \nIn recent years, self -supervised pre -training has been applied to medical imaging with promising 295 \nresults19,21,28,29. Zhou et al.30 present an approach that constructs several pre-text tasks to train SSL networks 296 \nand show that they outperform solely supervised networks trained across five clinically relevant tasks. A novel 297 \ncontrastive SSL strategy incorporating both global and local information captured within medical images and 298 \nreporting their superior performance, especially in low-data settings, is proposed by Chaitanya et al.31. Azizi et 299 \nal.19 demonstrate that grouping multiple images attributed to the same medical condition along with 300 \ncombining natural and medical images for contrastive SSL training improves performance. Specifically for deep 301 \nradiomics applications, Li et al. 32 propose targeting data imbalance in existing data and present a combined 302 \napproach of traditional radiomic features and self -supervised learning representations, improving 303 \nperformance for  discriminating tumor grade and tumor staging tasks. Li et al. 33 proposed a novel self -304 \nsupervised collaborative approach for creating latent representations from radiomic features. Zhao and Yang34 305 \nused self-supervised learning to pre-train models via a radiomic-deep feature correspondence task. Although 306 \nthese studies have investigated self-supervised learning for radiomics tasks, they lacked external validation or 307 \nproposed limited evaluation of the generalizability of their approaches. Our study presents a foundation model 308 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nfor radiomic discovery by pre-training on a large cohort of lesions. The examined tasks are independent of the 309 \npre-training cohort and demonstrate the increased generalizability of our proposed approach.  310 \nDespite the strengths outlined in our study, we recognize several limitations that need to be addressed 311 \nprior to the clinical applicability of our foundation model. Features from the foundation m odel followed by 312 \nlinear classifiers provided the most robust performance across all investigated tasks. However, linear classifiers 313 \nmight be sub -optimal in identifying complex relationships between feature representations to predict 314 \nchallenging endpoints. As we aimed to demonstrate the benefits of our foundation model compared to 315 \nexisting approaches, we have limited our exploration with fine-grained feature and model selection strategies. 316 \nComprehensive selection approaches similar to Parmar et al. 35 might improve performance even further, 317 \nstrengthening our hypothesis for foundation models. 318 \nSimilarly, deep learning -based finetuning approaches employed in this study are re presentative of 319 \nbaseline performance. We observed that finetuning approaches for the foundation model in low data settings 320 \n(especially 10%) and smaller cohorts (HarvardRT) resulted in suboptimal performance compared to using 321 \nextracted features. We hypothes ize that in lower data settings, models overfit the training data and 322 \ndemonstrate worse generalization as the number of parameters to tune increases. However, with the steady 323 \nemergence of deep learning literature proposing improvements to handle aspects su ch as data imbalance, 324 \nhyperparameter selection, and optimization objectives, the performance of these models can be pushed far 325 \nabove the current baseline. Our prognostication model is also limited in its performance due to our focus on 326 \nsolely imaging data; incorporating clinical features has a large potential to improve its effectiveness.  327 \nOur foundation model's clinical applicability encounters challenges typically associated with deep 328 \nlearning, including generalizability, interpretability, and explainabil ity. Given the retrospective nature of this 329 \nstudy, our capacity to evaluate the real -world practicality of foundation model -based biomarkers is 330 \nconstrained. Deep learning models are notorious for being black boxes that offer little clarity on interpretable 331 \nand explainable reasoning behind their predictions. Although we used well -established saliency attribution 332 \nmethods to interpret our foundation model's predictions, the broader applicability of these insights is 333 \nhindered by the technical limitations of suc h methods 36,37. In addition to the limitations of deep learning 334 \nmethodology, the biological association analysis conducted to explain our model's predictions is preliminary 335 \nand requires further investigation to generate a concrete understanding. We anticipate that future external 336 \nvalidation of our open-access model will help confront these prevalent challenges. 337 \nIn conclusion, our foundation model offers a powerful and reliable framework for discovering cancer 338 \nimaging biomarkers, even in small datasets. Furthermore, it surpasses current deep learning techniques in 339 \nvarious tasks while fitting conveniently into existing radiomic research methods. This approach can potentially 340 \nuncover new biomarkers that significantly contribute to research and medical practice. We share our 341 \nfoundation model and reproducible workflows so that more studies can investigate our methods, determine 342 \ntheir generalizability, and incorporate them into their research studies.  343 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nMETHODS 344 \nStudy Population. We utilize a total of five distinct datasets, four of which are publicly accessible, and one is 345 \nan internal dataset. These were acquired from various institutions as components of separate investiga tions 346 \n(see Extended Data Table 4).   347 \nDeepLesion14 is a dataset comprising 32,735 lesions from 10,594 studies of 4,427 unique patients 348 \ncollected over two decades from the National Institute of Health Clinical Center PACS server. Various lesions, 349 \nincluding kidney, bone, and liver lesions - as well as enlarged lymph nodes and lung nodules, are annotated. 350 \nThe lesions are identified through radiologist -bookmarked RECIST diame ters across 32,120 CT slices. In our 351 \nstudy, we excluded CT scans with a slice thickness exceeding 3mm, resulting in 16,518 remaining lesions. 352 \nSubsequently, we divided this into 11,467 unlabelled lesions for contrastive training and 5,051 labeled lesions 353 \nfor anatomical site classification. The labeled lesion data were further separated randomly into training, 354 \ntuning, and testing sets, containing 2,610, 1,220, and 1,221 lesions, respectively. 355 \nLUNA1638 is a curated version of the LIDC -IDRI dataset of 888 diagnostic and lung cancer screening 356 \nthoracic CT scans obtained from seven academic centers and eight medical ima ging companies comprising 357 \n1,186 nodules. The nodules are accompanied by annotations agreed upon by at least 3 out of 4 radiologists. 358 \nAlongside nodule location annotations, radiologists also noted various observed attributes like internal 359 \ncomposition, calcification, malignancy, suspiciousness, and more. For our evaluation, we chose nodules with 360 \nat least one indication of malignancy suspicion, totaling 677. We randomly picked 338 nodules for training and 361 \n169 for tuning the malignancy prediction networks. The final 170 nodules were utilized to assess the networks' 362 \nperformance. 363 \nHarvardRT10 is a cohort of 317 patients with stage I -IIIB NSCLC treated with radiation therapy at the 364 \nDana-Farber Cancer Institute and Brigham and Women's Hospital, Boston, MA, US , between 2001 and 2015. 365 \nAll CT scans for this cohort  were acquired with and without intravenous contrast on the GE Lightspeed CT 366 \nscanner. The primary tumor site was contoured by radiation oncologists using soft tissue and lung windows. 367 \nA subset of 291 patients with a follow-up of 2 years was selected for this study. We used 203 tumor volumes 368 \nfor training the prognostication networks and the remaining 88 tumor volumes for tuning.  369 \nLUNG139 is a cohort of 422 patients with stage I-IIIB NSCLC treated with radiation therapy at MAASTRO 370 \nClinic, Maastricht, The Netherlands. FDG PET-CT scans were acquired with or without contrast on the Siemens 371 \nBiograph Scanner. Radiation oncologists used PET and CT images to delineate the gross tumor volume. For our 372 \nstudy, we selected CT scans of 421 patients with annotated primary gross tumor volumes and used these as 373 \nan independent test set for prognostication networks.  374 \nRADIO (NSCLC-Radiogenomics)40 dataset is a collection of 211 NSCLC stage I -IV patients recruited 375 \nbetween 2008 and 2012 who were referred for surgical treatment and underwent preoperative CT and PET/CT 376 \nscans. These patients were recruited from t he Stanford University School of Medicine and the Palo Alto 377 \nVeterans Affairs Healthcare System. Scan scans were obtained using various scanners and protocols depending 378 \non the institution and physician. A subset of 144 patients in the cohort has available t umor segmentations 379 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nindependently reviewed by two thoracic radiologists. In addition to imaging data, the dataset includes 380 \nmolecular data from EGFR, KRAS, ALK mutational testing, gene expression microarrays, and RNA sequencing. 381 \nFor the current study, we uti lized the subset of 144 patients with annotated gross tumor volumes as an 382 \nindependent test set for prognostication and also investigated the biological basis of our networks using this 383 \ndataset. 384 \n 385 \nData Preprocessing. CT scans were resampled using linear inte rpolation to achieve isotropic voxels with a 386 \n1mm³ resolution to address variations in slice-thickness and in-plane resolutions across study populations. We 387 \nextracted patches of 50 x 50 x 50 voxels from the scans centered around a seed point (refer to Extended Data 388 \nFig. 3). For the DeepLesion dataset, which provided annotations in the form of RECIST diameters, the seed 389 \npoint was determined by calculating the midpoint of the RECIST diameter. For the other datasets (i.e., LUNA16, 390 \nHarvardRT, LUNG1, and RADIO), which supplied annotations as 3D contours, the seed point was obtained by 391 \ncomputing the center of mass (CoM). This approach allows for significantly higher throughput than manual 392 \nsegmentation, which can be more tedious. We then normalized the voxel values in the patches by subtracting 393 \n-1024 (lower -bound Hounsfield unit) and dividing by 3072 (upper -bound Hounsfield unit), ensuring the 394 \nintensity values in the input data ranged between 0 and 1. 395 \n 396 \nTask-agnostic contrastive pre -training of the foundation model. We implemented contrastive pre -training 397 \nusing a modified version of the SimCLR framework 5. The SimCLR framework's general principle involves 398 \ntransforming a sing le data piece (e.g., a patch taken from a CT scan) into two correlated and augmented 399 \nsamples (e.g., the same patch rotated 15 degrees clockwise and flipped horizontally). A convolutional encoder 400 \nis then used to extract latent representations from these sam ples. Through a contrastive loss function41, the 401 \nmodel learns to identify similar representations from the same data sample and dissimilar representations 402 \nfrom differen t data samples. The framework emphasizes effective transformation choices, convolutional 403 \nencoder architectures, and contrastive loss functions for optimal self -supervised learning performance. To 404 \neffectively represent the nature of medical images, we made modifications to each of these components.  405 \nTransformations proposed in the original SimCLR framework for natural world images, such as cutout 406 \naugmentation, Sobel filtering, and color distortion, are unsuited for 3D medical images due to dynamic range 407 \nand color depth differences. Therefore, our study applies different augmentations to replace these 408 \ntransformations. For instance, we substituted the random color jitter transform with a random histogram 409 \nintensity shift transform, as they both induce variation in intensity distribution. 410 \nTo extract representations from the transformed 3D volumes, we selected the 3D ResNet50 411 \narchitecture as our deep convolutional encoder. While the SimCLR authors employed a 2D ResNet50 412 \narchitecture, we opted for its 3D counterpart , which has proven effective in handling 3D medical imaging 413 \ndata42. 414 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nRegarding loss functions, we extended normalized temperature-scaled cross-entropy loss (NT-Xent)43 415 \nto support contrastive training for lesion volumes. The modifications include: 1) selecting positive pairs as 3D 416 \npatches surrounding the lesion's seed point, 2) choosing negative pairs by randomly sampling 3D patches from 417 \nthe rest of the scan, and 3) computing the contrastive loss on these positive and negative pairs, with each 418 \niteration comprising N positive pairs and N*2(N -1) negative pairs. We also explored different temperature 419 \nparameters for the NT -Xent loss. However, the original value of 0.1 proposed by the original paper was the 420 \nmost effective.  421 \nOur model was pre-trained for 100 epochs using an effective batch size of 64 (32 x 2 training nodes) 422 \non two NVIDIA Quadro RTX 8000 GPUs taking approximately five days. We used Stochastic Gradient Descent 423 \n(SGD) as the optimizer, with layer-wise adaptive rate control (LARC), momentum, and weight-decay enabled. 424 \nTo improve the optimization process, we employed learning rate schedulers that combined linear and cosine 425 \ndecay strategies and a warmup phase to modify the learning rate at the beginning of training gradually. While 426 \nmost specifications were consistent with the original SimCLR experiments, we experimented with different 427 \nbatch sizes, patch sizes (50mm³ and 64mm³), learning rates, transforms, and model architectures.  428 \n 429 \nTask-specific training of the foundation model. Our foundation model was adapted for a specific task through 430 \ntwo approaches: 1) extracting features and fitting a linear classifier on top of them or 2) fine -tuning the pre-431 \ntrained ResNet50 for the given classification task. 432 \nWe extracted 4096 features from the foundation model for each data point and used them to train a 433 \nlogistic regression model using the scikit -learn framework 44. A comprehensive parameter search for the 434 \nlogistic regression model was performed using the optuna hyper -parameter optimization framework 45. No 435 \nperformance improvements were observed through feature selection strategies; therefore, all 4096 features 436 \nwere used in accordance with linear evaluation strategies prevalent in self-supervised learning (SSL) literature. 437 \nFine-tuning was carried out with all layers updated during training, utilizing cross-entropy loss. A series 438 \nof randomly chosen augmentations—random flips, random 90-degree rotations, and random translations of 439 \n±10 voxels across all axes —were applied throughout the training. SGD w as employed for network training, 440 \nwith momentum enabled and step -wise learning rate decay. Following the original SimCLR experiments, 441 \nconfigurations and similar parameters (including learning rate, transforms, and model architectures) were 442 \nexplored during hyperparameter tuning. Each network was trained for 100 epochs using a single NVIDIA 443 \nQuadro RTX 8000 GPU, and the best-performing model checkpoints was chosen based on the tuning set. 444 \nFor supervised baseline models, their weights were initialized randomly, and they were trained using 445 \nthe same configuration that was adopted for fine-tuning the foundation model. The supervised models for use 446 \ncases 2 and 3 were also fine -tuned, utilizing the same configuration as in the pre -trained fine-tuning process 447 \nbut by initializing them with the weights of the trained supervised baseline from use case 1. 448 \nTask-specific training was conducted on reduced dataset sizes in addition to utilizing the entire 449 \ndataset. We randomly sampled 50%, 20%, and 10% of the training and tuning datasets and constructed task-450 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nspecific models using these samples with the same configuration as the entire dataset. As the training dataset 451 \nsizes decreased, we considered training the models for a higher number of epochs; however, models 452 \nfrequently overfitted during extended training. The entire test dataset was employed to allow benchmarking 453 \nacross these splits. 454 \n 455 \nPerformance Analysis. Validation of the foundation model was performed using several use -case-relevant 456 \nmetrics. Lesion anatomical site classif ication performance was assessed using balanced accuracy (BA) as a 457 \nmulti-label counting metric and mean average precision (mAP) as a multi -threshold metric. The multi -label 458 \nmetric, BA, adjusts class-wise accuracy based on the class distribution at a chosen threshold (0.5). The multi-459 \nthreshold metric, mAP, enables the examination of a given class's performance across a range of prediction 460 \nthresholds. All classes other than the class of interest are considered negatives, and performance is averaged 461 \nacross all possible classes. We avoided using the area under the receiver operating curve (AUC -ROC) for this 462 \nuse case due to the high proportion of negatives relative to positives, which results in consistently low false -463 \npositive rates and might overestimate the AUC . However, due to a more balanced class distribution, nodule 464 \nmalignancy prediction was evaluated using AUC -ROC. NSCLC prognostication networks also employed AUC -465 \nROC for evaluation, as it estimates the ranking of subjects based on their survival times. 466 \nModels underwent pairwise comparison using permutation tests. N permutations (N=1000) were 467 \nconducted for each pair, and new models were computed after permuting class labels. Metrics were 468 \nrecalculated after resampling, and a  two-sided p-value was calculated to  test the null hypothesis of 469 \nobservations from each pair originating from the same underlying distribution. Additionally, 95% confidence 470 \nintervals were established for each model using a bootstrap test with N=9999 resamples. 471 \nKaplan-Meier (KM) curves were a lso used to determine the stratification of subjects based on their 472 \nprediction scores for the prognostication models. Groups were selected based on prediction scores on the 473 \ntuning set, and curves were plotted on the test set for these groups. Multivariate log-rank tests were used to 474 \nexamine the significance of the stratification. Univariate Cox regression models were built using the model 475 \npredictions as the categorical variables of interest, grouped similarly to the KM curve. 476 \n 477 \nFeature visualization and saliency maps. We used the foundation and top-performing supervised models as 478 \nfeature extractors to obtain 4096 distinct features per data point. To enable visual interpretation of these 479 \nhigh-dimensional features, we utilized t -SNE46 (t-Stochastic Neighbourhood Embeddings) to reduce their 480 \ndimensionality to 2D. To arrive at the most interpretable visualization, we explored various parameter 481 \nconfigurations, including perplexity, initialization, and learning rates. Points in the 2D visualization were color-482 \ncoded according to their respective target classes, despite dimensionality reduction being agnostic to these 483 \ndistinctions. Density contours were superimposed over the visualiz ations to enhance the understanding of 484 \ngroup patterns, offering a more comprehensive representation of trends across data points. 485 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nIn order to generate saliency maps for each task, the fine -tuned foundation model was used to 486 \ngenerate predictions on randomly  selected volumes from respective datasets. The fine -tuned foundation 487 \nmodel with a single output prediction (corresponding to the predicted target class) was chosen in contrast to 488 \nthe feature extractor as computing saliency maps over 4096 -dimensional outpu ts remains challenging in 489 \npractice. We used a combination of 1) smooth gradient backpropagation, which averages gradients of the 490 \noutput with respect to several noisy inputs, and 2) guided back -propagation which combines deconvolution 491 \nwith backpropagation, mainly stopping the flow of negative gradients or neurons that decrease the activation 492 \nsignal. The method is termed smooth guided-backpropagation and is implemented in the MONAI framework 493 \n47.  494 \n 495 \nStability Testing. To test the stability of our models, we performed a test -retest stability and inter -reader 496 \nvariation evaluation. For the test -retest evaluation, we compared model predictions (of outcome) from the 497 \nbest foundation and supervised models generated on chest CT scans taken in a 15 -minute interval for 32 498 \npatients. Intraclass correlation coefficient (ICC) was computed using the interrater reliability and agreement 499 \npackage ( irr) in R 48. We also tested the stability of the flattened features computed by the models by 500 \ncalculating Spearman correlation and R2.  501 \n For the inter -reader variation evaluation, we used the LUNG1 dataset and generated 50 random 502 \nperturbations sampled from a three -dimensional multivariate normal distribution with zero mean and 503 \ndiagonal covariance matrix for each seed point. Across each dime nsion, a variance of 16 voxels was used for 504 \ngenerating samples. We generated predictions on perturbed seed points using the best foundation and 505 \nsupervised model,  resulting in 50 different prediction models for each. The mean and variance of the 50 506 \nmodels were computed for each and compared.  507 \n 508 \nBiological Associations. The GSE103584 dataset contains 130 NSCLC (Non -Small Cell Lung Cancer) samples 509 \nthat consist of paired CT scans and gene expression profiles generated by RNA sequencing. To analyze gene 510 \nexpression profiles, we filtered them based on cohort mean expression and standard deviation. First, we took 511 \nonly the genes with a higher expression than the overall dataset mean and then picked the top 500 genes 512 \nbased on standard deviation. Next, we performed a c orrelation analysis comparing the best -supervised and 513 \nfoundation models. To further evaluate foundation model features’ association with tumor biology, we 514 \ncomputed the absolute value of the correlation coefficients and performed a gene set enrichment analy sis 515 \nwith all genes with a correlation coefficient above 0.1.  516 \n 517 \n 518 \n 519 \n 520 \n 521 \n 522 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nACKNOWLEDGEMENTS 523 \nThe authors acknowledge financial support from NIH ( H.J.W.L.A: NIH -USA U24CA194354, NIH -USA 524 \nU01CA190234, NIH -USA U01CA209414, and NIH -USA R35CA22052), and the European Uni on - European 525 \nResearch Council (H.J.W.L.A: 866504). 526 \n 527 \nAUTHOR CONTRIBUTIONS 528 \nStudy conceptualization: S.P, H.J.W.L.A.; Data acquisition, analysis, and interpretation: S.P, D.B, A.H, T.L.C, 529 \nH.J.W.L.A.; Methodological design and implementation: S.P, D.B.; Conceptualization of assessment strategies: 530 \nS.P, D.B, N.J.B, H.J.W.L.A; Statistical Analyses: S.P, M.S, N.J.B, H.J.W.L.A; Code and reproducibility: S.P, I.H, V.P; 531 \nWriting of the manuscript: S.P, D.B, M.S, S.B, H.J.W.L.A; Critical revision of the manuscript: All authors;  Study 532 \nsupervision: H.J.W.L.A 533 \n 534 \nDATA AVAILABILITY STATEMENT 535 \nThe majority of the datasets utilized in this study are openly accessible for both training and validation 536 \npurposes and can be obtained from the following sources:  i) DeepLesion [nihcc.app.box.com/v/DeepLesion], 537 \nused both for our pre-training and use-case 1  ii) LUNA16 [luna16.grand-challenge.org] used for developing 538 \nour diagnostic image biomarker iii) LUNG1 [wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics] 539 \nand iv) RADIO [wiki.cancerimagingarchive.net/display/Public/NSCLC+Radiogenomics] used for the validation 540 \nof our prognostic image biomarker model. The training dataset for our prognostic biomarker model, 541 \nHarvardRT, is internal and unavailable to the public. HarvardRT was collected under an IRB-approved 542 \nretrospective protocol with a waiver of consent (Dana-Farber/Harvard Cancer Center protocol 11-286). As 543 \nthe trained foundational model is public, all the results can be reproduced using the accessible test datasets. 544 \n 545 \nCODE AVAILABILITY STATEMENT 546 \nThe complete pipeline used in this study can be accessed either from the AIM webpage or directly on GitHub. 547 \nThis includes the code for  1) Data download and pre -processing: Starting from downloading the data to 548 \ngenerating splits used in our stu dy; 2) Replicating the training and inference of foundation and supervised 549 \nmodels across all tasks; and 3) Code for reproducing our comprehensive performance validation. In addition 550 \nto the code, we also provide trained model weights, extracted features, an d outcome predictions for all the 551 \nmodels used in our study. Most importantly, we provide our foundation model accessible through a simple 552 \npip package install and 2 lines of code to extract features for your data. We also provide a detailed 553 \ndocumentation website that can be accessed here. The final model weights will also be made available through 554 \nthe Zenodo.org platform as well as through Mhub.ai in a reproducible, containerized, off-the-shelf executable 555 \nformat. 556 \n 557 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 558 \nCOMPETING INTERESTS 559 \nThe authors declare no competing interests. 560 \n 561 \nREFERENCES 562 \n1. Bommasani, R. et al. On the Opportunities and Risks of Foundation Models. arXiv [cs.LG] (2021). 563 \n2. Ouyang, L. et al. Training language models to follow instructions with human feedback. arXiv [cs.CL] 564 \n27730–27744 (2022). 565 \n3. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers 566 \nfor Language Understanding. arXiv [cs.CL] (2018). 567 \n4. Radford, A. et al. Learning transferable visual models from natural language supervision. arXiv [cs.CV] 568 \n8748–8763 (18--24 Jul 2021). 569 \n5. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A Simple Framework for Contrastive Learning of Visual 570 \nRepresentations. arXiv [cs.LG] (2020). 571 \n6. Oquab, M. et al. DINOv2: Learning robust visual features without supervision. arXiv [cs.CV] (2023). 572 \n7. Anja Thieme Microsoft Health Futures, United Kingdom et al. Foundation Models in Healthcare: 573 \nOpportunities, Risks & Strategies Forward. https://doi.org/10.1145/3544549.3583177 574 \ndoi:10.1145/3544549.3583177. 575 \n8. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259–265 576 \n(2023). 577 \n9. Mahajan, A. et al. Deep learning-based predictive imaging biomarker model for EGFR mutation status in 578 \nnon-small cell lung cancer from CT imaging. J. Clin. Orthod. 38, 3106–3106 (2020). 579 \n10. Hosny, A. et al. Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics 580 \nstudy. PLoS Med. 15, e1002711 (2018). 581 \n11. Braghetto, A., Marturano, F., Paiusco, M., Baiesi, M. & Bettinelli, A. Radiomics and deep learning 582 \nmethods for the prediction of 2-year overall survival in LUNG1 dataset. Sci. Rep. 12, 14132 (2022). 583 \n12. Balestriero, R. et al. A Cookbook of Self-Supervised Learning. arXiv [cs.LG] (2023). 584 \n13. Huang, S.-C. et al. Self-supervised learning for medical image classification: a systematic review and 585 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nimplementation guidelines. NPJ Digit Med 6, 74 (2023). 586 \n14. Yan, K., Wang, X., Lu, L. & Summers, R. M. DeepLesion: automated mining of large-scale lesion 587 \nannotations and universal lesion detection with deep learning. J Med Imaging (Bellingham) 5, 036501 588 \n(2018). 589 \n15. Zhao, B. et al. Evaluating variability in tumor measurements from same-day repeat CT scans of patients 590 \nwith non-small cell lung cancer. Radiology 252, 263–272 (2009). 591 \n16. Springenberg, J. T., Dosovitskiy, A., Brox, T. & Riedmiller, M. Striving for Simplicity: The All Convolutional 592 \nNet. arXiv [cs.LG] (2014). 593 \n17. Smilkov, D., Thorat, N., Kim, B., Viégas, F. & Wattenberg, M. SmoothGrad: removing noise by adding 594 \nnoise. arXiv [cs.LG] (2017). 595 \n18. Hinshaw, D. C. & Shevde, L. A. The Tumor Microenvironment Innately Modulates Cancer Progression. 596 \nCancer Res. 79, 4557–4566 (2019). 597 \n19. Azizi, S. et al. Big Self-Supervised Models Advance Medical Image Classification. arXiv [eess.IV] (2021). 598 \n20. Krishnan, R., Rajpurkar, P. & Topol, E. J. Self-supervised learning in medicine and healthcare. Nat 599 \nBiomed Eng 6, 1346–1352 (2022). 600 \n21. Ghesu, F. C. et al. Self-supervised Learning from 100 Million Medical Images. arXiv [cs.CV] (2022). 601 \n22. Haarburger, C. et al. Radiomics feature reproducibility under inter-rater variability in segmentations of 602 \nCT images. Scientific Reports vol. 10 Preprint at https://doi.org/10.1038/s41598-020-69534-6 (2020). 603 \n23. Campello, V. M. et al. Minimising multi-centre radiomics variability through image normalisation: a pilot 604 \nstudy. Sci. Rep. 12, 12532 (2022). 605 \n24. Kumar, D. et al. Discovery Radiomics for Pathologically-Proven Computed Tomography Lung Cancer 606 \nPrediction. arXiv [cs.CV] (2015). 607 \n25. Lao, J. et al. A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma 608 \nMultiforme. Sci. Rep. 7, 10353 (2017). 609 \n26. Haarburger, C., Weitz, P., Rippel, O. & Merhof, D. Image-based Survival Analysis for Lung Cancer 610 \nPatients using CNNs. arXiv [cs.CV] (2018). 611 \n27. Cho, H.-H. et al. Radiomics-guided deep neural networks stratify lung adenocarcinoma prognosis from 612 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nCT scans. Commun Biol 4, 1286 (2021). 613 \n28. Taleb, A. et al. 3d self-supervised methods for medical imaging. Adv. Neural Inf. Process. Syst. 33, 614 \n18158–18172 (2020). 615 \n29. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest X-ray images via self-616 \nsupervised learning. Nat Biomed Eng 6, 1399–1406 (2022). 617 \n30. Zhou, Z. et al. Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis. Med. Image 618 \nComput. Comput. Assist. Interv. 11767, 384–393 (2019). 619 \n31. Chaitanya, K., Erdil, E., Karani, N. & Konukoglu, E. Contrastive learning of global and local features for 620 \nmedical image segmentation with limited annotations. arXiv [cs.CV] (2020). 621 \n32. Li, H. et al. Imbalance-Aware Self-supervised Learning for 3D Radiomic Representations. in Medical 622 \nImage Computing and Computer Assisted Intervention – MICCAI 2021 36–46 (Springer International 623 \nPublishing, 2021). 624 \n33. Li, Z. et al. A Novel Collaborative Self-Supervised Learning Method for Radiomic Data. arXiv [eess.IV] 625 \n(2023). 626 \n34. Zhao, Z. & Yang, G. Unsupervised Contrastive Learning of Radiomics and Deep Features for Label-627 \nEfficient Tumor Classification. in Medical Image Computing and Computer Assisted Intervention – 628 \nMICCAI 2021 252–261 (Springer International Publishing, 2021). 629 \n35. Parmar, C., Grossmann, P., Bussink, J., Lambin, P. & Aerts, H. J. W. L. Machine Learning methods for 630 \nQuantitative Radiomic Biomarkers. Sci. Rep. 5, 13087 (2015). 631 \n36. Adebayo, J., Gilmer, J. & Muelly, M. Sanity checks for saliency maps. Adv. Neural Inf. Process. Syst. 632 \n(2018). 633 \n37. Arun, N. et al. Assessing the Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical 634 \nImaging. Radiol Artif Intell 3, e200267 (2021). 635 \n38. Setio, A. A. A. et al. Validation, comparison, and combination of algorithms for automatic detection of 636 \npulmonary nodules in computed tomography images: The LUNA16 challenge. Med. Image Anal. 42, 1–637 \n13 (2017). 638 \n39. Kirby, J. NSCLC-Radiomics. https://wiki.cancerimagingarchive.net/display/Public/NSCLC-Radiomics. 639 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n40. Napel, S. NSCLC radiogenomics: Initial Stanford study of 26 cases. The Cancer Imaging Archive. 640 \n41. Wang, F. & Liu, H. Understanding the behaviour of contrastive loss. arXiv [cs.LG] 2495–2504 (2020). 641 \n42. Uemura, T., Näppi, J. J., Hironaka, T., Kim, H. & Yoshida, H. Comparative performance of 3D-DenseNet, 642 \n3D-ResNet, and 3D-VGG models in polyp detection for CT colonography. in Medical Imaging 2020: 643 \nComputer-Aided Diagnosis vol. 11314 736–741 (SPIE, 2020). 644 \n43. Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. Adv. Neural Inf. Process. 645 \nSyst. 29, (2016). 646 \n44. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python. arXiv [cs.LG] 2825–2830 (2012). 647 \n45. Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: A Next-generation Hyperparameter 648 \nOptimization Framework. in Proceedings of the 25th ACM SIGKDD International Conference on 649 \nKnowledge Discovery & Data Mining 2623–2631 (Association for Computing Machinery, 2019). 650 \n46. Gmail, L. & Hinton, G. Visualizing Data using t-SNE. 651 \nhttps://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl (2008). 652 \n47. Jorge Cardoso, M. et al. MONAI: An open-source framework for deep learning in healthcare. arXiv 653 \n[cs.LG] (2022). 654 \n48. Gamer, M. irr : Various Coefficients of Interrater Reliability and Agreement. http://cran.r-655 \nproject.org/web/packages/irr/irr.pdf (2010). 656 \n 657 \n 658 \n 659 \n 660 \n 661 \n 662 \n 663 \n 664 \n 665 \n 666 \n 667 \n 668 \n 669 \n 670 \n 671 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 672 \n 673 \nFIGURES 674 \n675 \n   676 \nFigure 1 | General overview of the study.  a. Foundation model pre-training. A foundation model, specifically a deep convolutional encoder model, 677 \nwas pre-trained by contrasting volumes with and without lesions. b. Clinical application of the foundation model.  The foundation model was used to 678 \nextract biomarkers and then evaluated fo r three classification tasks on diverse datasets. c. Foundation model implementation approaches  The 679 \nfoundation model was adapted to specific use cases by extracting features or through fine -tuning (left). d. Evaluation against supervised models with 680 \nselected performance metrics. We compared the performance of the foundation models against conventional supervised implementations, trained 681 \nfrom random intialization (left) and fine -tuned from a different task (right). The comparison was made through several cr iteria for the different use 682 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \ncases, including quantitative performance, stability, and biological analysis. Biological, clinical, and stability analyses a re limited to use case 2 due to 683 \nthe availability of associated data.  684 \n 685 \n 686 \nFigure 2 | Performance of foundation model for lesion anatomical site classification . We compared foundation model adaptation approaches 687 \nagainst a supervised model using balanced accuracy ( a) and mean average precision ( b).  We show performance on these metrics compu ted across the 688 \neight anatomical sites for the full training set and when the training data percentage is decreased to 50%, 20%, and 10%.  Er ror bars in (a) and (b) 689 \nshow 95% confidence intervals of the estimates. Visual representation of the features genera ted from the independent test -set for identifying lesion 690 \nanatomical sites, using c the foundation model as a feature extractor, and d the supervised model. For (c) and (d), the x-axis corresponds to dimension 691 \n1, and the y-axis to dimension 2 of the t -SNE dimensionality reduction. The density contours belonging to each class are underlaid for ( c) and (d) to 692 \nhighlight separability between classes in the feature space.  693 \n 694 \n 695 \n 696 \n 697 \n 698 \n 699 \n 700 \n 701 \n 702 \n 703 \n 704 \n 705 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 706 \n 707 \nFigure 3 | Performance comparison of the foundation model against supervised  for nodule malignancy prediction. We compared the foundation 708 \nmodel adaptation approaches against baseline supervised models using the full training dataset and on decreasing the training  data percentages to 709 \n50%, 20% and 10%. a Area under receiver operatin g curves (AUC-ROC) b Average precision (AP). Error bars in ( a) and (b) show 95% confidence 710 \nintervals of the estimates. Visual representation of the features generated from the independent test -set for the task of nodule malignancy prediction 711 \nusing, c the fine-tuned supervised model and d using the foundation model as a feature extractor.  For ( c) and (d), the x-axis corresponds to dimension 712 \n1, and the y-axis to dimension 2 of the t -SNE dimensionality reduction. The density contours belonging to each class a re underlaid for (c) and (d) to 713 \nhighlight separability between classes in the feature space.  714 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 715 \n 716 \nFigure 4 | Performance of the foundation model against supervised for prognostication of NSCLC tumors. We compared the foundation model 717 \nagainst the baseline supervised models using the area under the receiver operating curve (AUC) for 2 -year overall survival for a LUNG1 b RADIO.  718 \nKaplan-Meier (KM) curves for predictions generated from the foundation model as a feature -extractor for LUNG1 (c) and RADIO (d) as well as the fine-719 \ntuned supervised method for LUNG1 ( e) and RADIO (f).  To ensure a fair comparison, we calculated the threshold for the split between the KM groups 720 \non the tuning set for each network. Kaplan -Meier curves for the other approaches, fine -tuning the foundation model and training a supervised model 721 \nfrom random initialization can be found in Fi g. S1 in the supplementary.  722 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 723 \n 724 \nFigure 5 | Saliency maps for fine -tuned foundation models. We generated gradient-based saliency maps for each of the fine -tuned foundation 725 \nmodels from use cases I (a), II (b), and III (c) using smooth guided backpropagation and visualized salient regions on two samples from corresponding 726 \ntest datasets. The first and fourth columns show the central axial slice (50mm x 50mm) of the volume provided as input to the  self-supervised network. 727 \nThe second and fifth columns show isolin es for saliency contours. Finally, the third and sixth columns show saliency maps highlighting areas of the 728 \ninput volume that contribute the most to a change in the output prediction.   729 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 730 \n 731 \nFigure 6 | Underlying biological basis of the foundation model. We compared the foundation and supervised model predictions with gene expression 732 \nprofiles. a Box plot of absolute correlation coefficients (y -axis) of selected genes against model predictions (x -axis). b Gene-set enrichment analysis of 733 \ngenes with correlation coefficient > 0.1 revealed for the foundation (left) and supervised model predictions (rig ht). Genetic pathways are shown on the 734 \ny-axis, and the gene ratio is shown on the x -axis. Gene count and adjusted p -values are also shown in the legend.  735 \n 736 \n 737 \n 738 \n 739 \n 740 \n 741 \n 742 \n 743 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \nEXTENDED DATA 744 \n 745 \nFoundation Model \nImplementation \nData \npercentage \nIncrease in BA over \nsupervised (95% CI, p-value) \nIncrease in mAP \nover supervised \n(95% CI, p-value) \nFeature-extractor 50% \n(n=2526) \n0.153  (0.123-0.186,  \np<0.005) \n0.135  \n(0.104-0.168,   \np<0.005) \nFine-tuned  0.181 (0.147-0.214,  \np<0.005) \n0.127  \n(0.097-0.162,  \np<0.005) \n    \nFeature-extractor 20% \n(n=1010) \n0.194 (0.159-0.228,  \np<0.005) \n0.177  \n(0.142-0.216,  \n p<0.005) \nFine-tuned  0.130 (0.102-0.159,  \np<0.005) \n0.121  \n(0.089-0.159,  \np<0.005) \n    \nFeature-extractor 10% \n(n=505) \n0.189 (0.148-0.228,  \np<0.005) \n0.149  \n(0.112-0.189,   \np<0.005) \nFine-tuned  0.063  \n (0.028-0.098,  p<0.005) \n0.02  \n(-0.011- 0.061,  \np=0.28) \n \n 746 \nExtended Data Table 1 | Detailed comparison of the foundation model implementations against supervised methods in limited dat a settings for 747 \nlesion anatomical site classification Comparison of the foundation model as a feature -extractor and fine -tuned again st the randomly initialised 748 \nsupervised model at 50%, 20% and 10% training data. For each data percentage, the largest increase in performance between the two is shown italicised. 749 \nNot significant results are shown in red    750 \n 751 \n 752 \n 753 \n 754 \n 755 \n 756 \n 757 \n 758 \n 759 \n 760 \n 761 \n 762 \n 763 \n 764 \n 765 \n 766 \n 767 \n 768 \n 769 \n 770 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 771 \n 772 \n 773 \n 774 \nFoundation Model \nImplementation \nData \npercentage \nIncrease in AUC over \nsupervised random \ninitialization \n(95% CI, p-value) \nIncrease in \nmAP over \nsupervised \nrandom \ninitialization \n(95% CI, p-\nvalue) \nIncrease in AUC \nover supervised \nfine-tuned \n(95% CI, p-\nvalue) \nIncrease in \nmAP over \nsupervised \nfine-tuned \n(95% CI, p-\nvalue) \nFeature-extractor 50% \n(n=254) \n0.133 \n(0.064 - \n0.207, \np<0.005) \n0.15  \n(0.068 - \n0.222 \np<0.005) \n0.07  \n(0.021 - 0.167, \np<0.05) \n0.089  \n(0.024 - 0.153, \np=0.063) \nFine-tuned  \n0.136  \n(0.070-0.199,  \np<0.005) \n0.155 (0.083-\n0.223, \np<0.005) \n0.097 (0.035-\n0.155366, \np<0.05) \n0.095 (0.035-\n0.148, \np<0.005) \n      \nFeature-extractor 20% \n(n=101) \n0.285  \n(0.193-0.370, \n p<0.05) \n0.314 (0.227-\n0.420, \np<0.005) \n0.254 (0.173-\n0.330, p<0.05) \n0.251 (0.164-\n0.334, \np<0.005) \nFine-tuned  \n0.20  \n(0.092-0.308,  \np<0.005) \n0.24 (0.138-\n0.35 \np<0.005) \n0.169 (0.093-\n0.245, p<0.005) \n0.177 (0.089-\n0.260, \np<0.005) \n      \nFeature-extractor 10% \n(n=51) \n0.312  \n(0.211-0.408,  \np<0.005) \n0.323 (0.238-\n0.423, \np<0.005) \n0.212 (0.128-\n0.285, p<0.005) \n0.268 (0.179-\n0.376, \np<0.005) \nFine-tuned  \n0.008  \n(-0.089 -0.101,  \np=0.919) \n-0.005 (-\n0.095-0.08, \np=0.869) \n-0.091  \n(-0.015 - -\n0.171481, \np<0.05) \n-0.061 (-0.144 - \n0.023, \np=0.322) \n  775 \n 776 \nExtended Data Table 2 | Detailed comparison of the foundation model implementations against supervised methods in limited dat a settings for 777 \nnodule malignancy classification Comparison of the foundation model as a feature -extractor and fine-tuned against randomly initialised and fine-778 \ntuned supervised models at 50%, 20% and 10% of the training data. For each data percentage, the largest increase in performan ce between the two is 779 \nshown italicised. Not significant results are shown in red   780 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 781 \n 782 \nExtended Data Figure 1 | Kaplan Meier curves for all models investigated Kaplan Meier curves for the LUNG1 and RADIO datasets for the foundation 783 \nmodel as a feature -extractor (first row), fine -tuned foundation model (second row), fine -tuned supervised model (third row) and randomly initialised 784 \nsupervised model (last row)  785 \n 786 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 787 \n LUNG1 RADIO \n beta HR (95% CI \nfor HR) p.value beta HR (95% CI \nfor HR) p.value \nFoundation \nmodel as \nfeature \nextractor \n-0.44 0.65 (0.52-\n0.81) <0.005 -0.84 0.43 (0.23-\n0.82) 0.01 \nFoundation \nmodel fine-\ntuned \n-0.39 0.68 \n(0.5-0.92) 0.01 -0.32 0.72 \n(0.26-2.01) 0.53 \nSupervised \n(fine-tuned) -0.24 0.79 \n(0.64-0.98) 0.03 -0.43 0.65 \n(0.37-1.15) 0.14 \nSupervised  \n(random \ninitialization) \n-0.22 0.80 (0.65-\n1.00) 0.05 0.20 1.22 \n(0.59-2.53) 0.59 \n 788 \n 789 \nExtended Data Table 3 | Univariate cox regression  Results of univariate cox models showing the relationship between implementations of the 790 \nfoundation model and the supervised methods and survival on LUNG1 and RADIO datasets. The median split on the training dataset (HarvardRT) is used, 791 \nalso shown in Fig S4 in the Kaplan -Meier curves. 792 \n 793 \n 794 \n 795 \n 796 \n 797 \n 798 \n 799 \n 800 \n 801 \n 802 \n 803 \n 804 \n 805 \n 806 \n 807 \n 808 \n 809 \n 810 \n 811 \n 812 \n 813 \n 814 \n 815 \n 816 \n 817 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 818 \n 819 \nExtended Data Figure 2 | Stability of self -supervised learning networks. We analyzed the test-retest robustness on the RIDER dataset by comparing 820 \nthe correlation between features generated by a. the foundation model as a feature extractor and b. the fine-tuned supervised model. In c., the inter-821 \nreader variability is simulated b y adding perturbations from a sampling distribution. We perturb across x, y and z -axes although the distribution is 822 \nshown only for x and y perturbations for simplicity.  d Prognostic stability of the feature extractor foundation model against the fine -tuned supervised 823 \nmodel when the input seed point is perturbed, estimated through AUC for 2 -year survival. 824 \n 825 \n 826 \n 827 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 828 \n 829 \nExtended Data Figure 3 | Diameter distribution of DeepLesion Distribution of diameters in the x and y axes for the DeepLesion training dataset based 830 \non RECIST bookmarks identified on key slices. Input dimensions of 50x50x50 mm 3 were chosen as they covered 93% and 97% of the distribution in the x 831 \nand y axes respectively. 832 \n 833 \n 834 \n 835 \n 836 \n 837 \n 838 \n 839 \n 840 \n 841 \n 842 \n 843 \n 844 \n 845 \n 846 \n 847 \n 848 \n 849 \n 850 \n 851 \n 852 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint \n 853 \n Pre-training \nUse-case 1: Lesion \nAnatomical Site \nClassification \nUse-case 2: \nNodule \nMalignancy  \nClassification \nUse-case 3: \nClassification of survival for NSCLC \ntumors \nStability \nCohorts  DeepLesion DeepLesion LUNA16 HarvardRT LUNG1 RADIO RIDER \nInstitution NIH Clinical \nCenter NIH Clinical Center Multi-center Dana-Farber \nCancer Center \nMAASTRO \nClinic \nStanford &  \nPalo Alto VA MSKCC \nUsage Pre-train Train Tune Test Train Tune Test Train Tune Test Test Test \nScans 11,467 2610 1220 1221 338 169 170 203 88 421 144 52 \nPatients 2,312 553 379 390 266 149 150 203 88 421 144 26 \n \n 854 \n \nUse-case 1: \nLesion Anatomical \nSite Classification \nUse-case 2: \nNodule Malignancy \nClassification \nUse-case 3: \nClassification of survival for NSCLC tumors \nHarvardRT LUNG1 RADIO \nOutcome \nDistribution \nbone 4.1% \nbenign 51.7% alive (2-\nyear) 54.2% alive (2-\nyear) 59.8% alive (2-\nyear) 64.5% \nabdomen 16.3% \nmediastin\num 14.3% \nliver 9.7% \nlung 41.1% \nmalignant 48.3% dead (2-\nyear) 45.7% dead (2-\nyear) 40.1% dead (2-\nyear) 35.4% \nkidney 3.6% \nsoft  \ntissue 4.6% \npelvis 6.0% \nSex \nM 58.5% \nna \n52.2% 68.8% 75% \nF 41.5% 47.7% 31.1% 25% \nAge \n(median) 58.0 na 69.6 68.69 69.0 \n 855 \n 856 \nExtended Data Table 4 | Dataset breakdown Table showing the 6 different cohorts used in this study along with eligible scans and patients used. A 857 \nsecondary table shows the outcome, sex, and age distribution of each of the cohorts.  858 \n 859 \n 860 \n 861 \n 862 \n 863 \n . CC-BY-NC 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 5, 2023. ; https://doi.org/10.1101/2023.09.04.23294952doi: medRxiv preprint ",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7198837399482727
    },
    {
      "name": "Computer science",
      "score": 0.6649791598320007
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6648737788200378
    },
    {
      "name": "Machine learning",
      "score": 0.6125813126564026
    },
    {
      "name": "Deep learning",
      "score": 0.6021239757537842
    },
    {
      "name": "Biomarker",
      "score": 0.5341329574584961
    },
    {
      "name": "Biomarker discovery",
      "score": 0.5146661400794983
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.4238082468509674
    },
    {
      "name": "Data science",
      "score": 0.3399198651313782
    },
    {
      "name": "Biology",
      "score": 0.12054756283760071
    },
    {
      "name": "Proteomics",
      "score": 0.09309333562850952
    },
    {
      "name": "Engineering",
      "score": 0.08603215217590332
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I34352273",
      "name": "Maastricht University",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210087915",
      "name": "Massachusetts General Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204337017",
      "name": "Aarhus University",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I2802335433",
      "name": "Aarhus University Hospital",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I1283280774",
      "name": "Brigham and Women's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210117453",
      "name": "Dana-Farber Cancer Institute",
      "country": "US"
    }
  ],
  "cited_by": 6
}