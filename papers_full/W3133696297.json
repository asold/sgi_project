{
  "title": "Transformer in Transformer",
  "url": "https://openalex.org/W3133696297",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103390482",
      "name": "Han, Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153155721",
      "name": "Xiao An",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183178440",
      "name": "Wu, Enhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2369507686",
      "name": "Guo, Jianyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3087778642",
      "name": "XU Chunjing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A723829169",
      "name": "Wang, Yunhe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3176772026",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2952337489",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3109241881",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3185811833",
    "https://openalex.org/W3212161087",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3189898414",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2408279554",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2797977484",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2124386111"
  ],
  "abstract": "Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\\times$16) as \"visual sentences\" and present to further divide them into smaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.",
  "full_text": "Transformer in Transformer\nKai Han1,2 An Xiao2 Enhua Wu1,3‚àóJianyuan Guo2 Chunjing Xu2 Yunhe Wang2‚àó\n1State Key Lab of Computer Science, ISCAS & UCAS\n2Noah‚Äôs Ark Lab, Huawei Technologies\n3University of Macau\n{hankai,weh}@ios.ac.cn, yunhe.wang@huawei.com\nAbstract\nTransformer is a new kind of neural architecture which encodes the input data as\npowerful features via the attention mechanism. Basically, the visual transformers\nÔ¨Årst divide the input images into several local patches and then calculate both\nrepresentations and their relationship. Since natural images are of high complexity\nwith abundant detail and color information, the granularity of the patch dividing is\nnot Ô¨Åne enough for excavating features of objects in different scales and locations.\nIn this paper, we point out that the attention inside these local patches are also\nessential for building visual transformers with high performance and we explore\na new architecture, namely, Transformer iN Transformer (TNT). SpeciÔ¨Åcally, we\nregard the local patches (e.g., 16√ó16) as ‚Äúvisual sentences‚Äù and present to further\ndivide them into smaller patches (e.g., 4√ó4) as ‚Äúvisual words‚Äù. The attention of\neach word will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be ag-\ngregated to enhance the representation ability. Experiments on several benchmarks\ndemonstrate the effectiveness of the proposed TNT architecture,e.g., we achieve an\n81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the\nstate-of-the-art visual transformer with similar computational cost. The PyTorch\ncode is available at https://github.com/huawei-noah/CV-Backbones, and\nthe MindSpore code is available at https://gitee.com/mindspore/models/\ntree/master/research/cv/TNT.\n1 Introduction\nIn the past decade, the mainstream deep neural architectures used in the computer vision (CV) are\nmainly established on convolutional neural networks (CNNs) [19, 13, 12]. Differently, transformer\nis a type of neural network mainly based on self-attention mechanism [39], which can provide the\nrelationships between different features. Transformer is widely used in the Ô¨Åeld of natural language\nprocessing (NLP), e.g., the famous BERT [9] and GPT-3 [2] models. The power of these transformer\nmodels inspires the whole community to investigate the use of transformer for visual tasks.\nTo utilize the transformer architectures for conducting visual tasks, a number of researchers have\nexplored for representing the sequence information from different data. For example, Wang et al.ex-\nplore self-attention mechanism in non-local networks [41] for capturing long-range dependencies\nin video and image recognition. Carion et al. present DETR [ 3], which treats object detection\nas a direct set prediction problem and solve it using a transformer encoder-decoder architecture.\nChen et al.propose the iGPT [6], which is the pioneering work applying pure transformer model (i.e.,\nwithout convolution) on image recognition by self-supervised pre-training.\n‚àóCorresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2103.00112v3  [cs.CV]  26 Oct 2021\n‚Ä¶\n2\n1 2 3\n4 5 6\n7 8 9\n1\n1 2 3\n4 5 6\n7 8 9\n9\n1 2 3\n4 5 6\n7 8 9\nLinear Projection of Visual Sentences and Words\n0 *\nOutput\nInner \nTransformer\nBlock\nInner \nTransformer\nBlock\nInner \nTransformer\nBlock\n+ + +\nOuter Transformer Block\nùêø√ó\n1\n1\n*\nSentence position \nencoding\nSentence \nembedding\nWord embedding\nWord position \nencoding\nClass token‚Ä¶\n‚Ä¶\nFigure 1: Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The inner\ntransformer block is shared in the same layer. The word position encodings are shared across visual\nsentences.\nDifferent from the data in NLP tasks, there exists a semantic gap between input images and the\nground-truth labels in CV tasks. To this end, Dosovitskiy et al.develop the ViT [10], which paves the\nway for transferring the success of transformer based NLP models. Concretely, ViT divides the given\nimage into several local patches as a visual sequence. Then, the attention can be naturally calculated\nbetween any two image patches for generating effective feature representations for the recognition\ntask. Subsequently, Touvron et al.explore the data-efÔ¨Åcient training and distillation to enhance the\nperformance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy,\nwhich is comparable to that of the state-of-the-art convolutional networks. Chenet al.further treat the\nimage processing tasks (e.g., denosing and super-resolution) as a series of translations and develop\nthe IPT model for handling multiple low-level computer vision problems [4]. Nowadays, transformer\narchitectures have been used in a growing number of computer vision tasks [ 11] such as image\nrecognition [7, 44, 33], object detection [50], and segmentation [47, 42].\nAlthough the aforementioned visual transformers have made great efforts to boost the models‚Äô\nperformances, most of existing works follow the conventional representation scheme used in ViT,i.e.,\ndividing the input images into patches. Such a exquisite paradigm can effectively capture the visual\nsequential information and estimate the attention between different image patches. However, the\ndiversity of natural images in modern benchmarks is very high,e.g., there are over 120 M images with\n1000 different categories in the ImageNet dataset [30]. As shown in Figure 1, representing the given\nimage into local patches can help us to Ô¨Ånd the relationship and similarity between them. However,\nthere are also some sub-patches inside them with high similarity. Therefore, we are motivated to\nexplore a more exquisite visual image dividing method for generating visual sequences and improve\nthe performance.\nIn this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recog-\nnition as shown in Figure 1. To enhance the feature representation ability of visual transformers,\nwe Ô¨Årst divide the input images into several patches as ‚Äúvisual sentences‚Äù and then further divide\nthem into sub-patches as ‚Äúvisual words‚Äù. Besides the conventional transformer blocks for extracting\nfeatures and attentions of visual sentences, we further embed a sub-transformer into the architecture\nfor excavating the features and details of smaller visual words. SpeciÔ¨Åcally, features and attentions\nbetween visual words in each visual sentence are calculated independently using a shared network so\nthat the increased amount of parameters and FLOPs (Ô¨Çoating-point operations) is negligible. Then,\nfeatures of words will be aggregated into the corresponding visual sentence. The class token is also\nused for the subsequent visual recognition task via a fully-connected head. Through the proposed\nTNT model, we can extract visual information with Ô¨Åne granularity and provide features with more\ndetails. We then conduct a series of experiments on the ImageNet benchmark and downstream\ntasks to demonstrate its superiority and thoroughly analyze the impact of the size for dividing visual\nwords. The results show that our TNT can achieve better accuracy and FLOPs trade-off over the\nstate-of-the-art transformer networks.\n2\n2 Approach\nIn this section, we describe the proposed transformer-in-transformer architecture and analyze the\ncomputation and parameter complexity in details.\n2.1 Preliminaries\nWe Ô¨Årst brieÔ¨Çy describe the basic components in transformer [ 39], including MSA (Multi-head\nSelf-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization).\nMSA. In the self-attention module, the inputs X ‚ààRn√ód are linearly transformed to three parts,\ni.e., queries Q‚ààRn√ódk , keys K ‚ààRn√ódk and values V ‚ààRn√ódv where nis the sequence length, d,\ndk, dv are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product\nattention is applied on Q,K,V :\nAttention(Q,K,V ) =softmax(QKT\n‚àödk\n)V. (1)\nFinally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys\nand values to hparts and perform the attention function in parallel, and then the output values of each\nhead are concatenated and linearly projected to form the Ô¨Ånal output.\nMLP. The MLP is applied between self-attention layers for feature transformation and non-linearity:\nMLP(X) =FC(œÉ(FC(X))), FC(X) =XW + b, (2)\nwhere W and bare the weight and bias term of fully-connected layer respectively, and œÉ(¬∑) is the\nactivation function such as GELU [14].\nLN. Layer normalization [1] is a key part in transformer for stable training and faster convergence.\nLN is applied over each sample x‚ààRd as follows:\nLN(x) =x‚àí¬µ\nŒ¥ ‚ó¶Œ≥+ Œ≤ (3)\nwhere ¬µ ‚ààR, Œ¥ ‚ààR are the mean and standard deviation of the feature respectively, ‚ó¶is the\nelement-wise dot, and Œ≥ ‚ààRd, Œ≤ ‚ààRd are learnable afÔ¨Åne transform parameters.\n2.2 Transformer in Transformer\nGiven a 2D image, we uniformly split it intonpatches X= [X1,X2,¬∑¬∑¬∑ ,Xn] ‚ààRn√óp√óp√ó3, where\n(p,p) is the resolution of each image patch. ViT [10] just utilizes a standard transformer to process the\nsequence of patches which corrupts the local structure of a patch, as shown in Fig. 1(a). Instead, we\npropose Transformer-iN-Transformer (TNT) architecture to learn both global and local information\nin an image. In TNT, we view the patches as visual sentences that represent the image. Each patch is\nfurther divided into msub-patches, i.e., a visual sentence is composed of a sequence of visual words:\nXi ‚Üí[xi,1,xi,2,¬∑¬∑¬∑ ,xi,m], (4)\nwhere xi,j ‚ààRs√ós√ó3 is the j-th visual word of the i-th visual sentence, (s,s) is the spatial size of\nsub-patches, and j = 1,2,¬∑¬∑¬∑ ,m. With a linear projection, we transform the visual words into a\nsequence of word embeddings:\nYi = [yi,1,yi,2,¬∑¬∑¬∑ ,yi,m], y i,j = FC(Vec(xi,j)), (5)\nwhere yi,j ‚ààRc is the j-th word embedding, cis the dimension of word embedding, and Vec(¬∑) is\nthe vectorization operation.\nIn TNT, we have two data Ô¨Çows in which one Ô¨Çow operates across the visual sentences and the other\nprocesses the visual words inside each sentence. For the word embeddings, we utilize a transformer\nblock to explore the relation between visual words:\nY‚Ä≤i\nl = Yi\nl‚àí1 + MSA(LN(Yi\nl‚àí1)), (6)\nYi\nl = Y‚Ä≤i\nl + MLP(LN(Y‚Ä≤i\nl)). (7)\n3\nwhere l= 1,2,¬∑¬∑¬∑ ,L is the index of the l-th block, and Lis the total number of stacked blocks. The\ninput of the Ô¨Årst block Yi\n0 is just Yi in Eq. 5. All word embeddings in the image after transformation\nare Yl = [Y1\nl ,Y 2\nl ,¬∑¬∑¬∑ ,Y n\nl ]. This can be viewed as an inner transformer block, denoted as Tin. This\nprocess builds the relationships among visual words by computing interactions between any two\nvisual words. For example, in a patch of human face, a word corresponding to the eye is more related\nto other words of eyes while interacts less with forehead part.\nFor the sentence level, we create the sentence embedding memories to store the sequence of sentence-\nlevel representations: Z0 = [Zclass,Z1\n0 ,Z2\n0 ,¬∑¬∑¬∑ ,Zn\n0 ] ‚ààR(n+1)√ód where Zclass is the class token\nsimilar to ViT [ 10], and all of them are initialized as zero. In each layer, the sequence of word\nembeddings are transformed into the domain of sentence embedding by linear projection and added\ninto the sentence embedding:\nZi\nl‚àí1 = Zi\nl‚àí1 + FC(Vec(Yi\nl )), (8)\nwhere Zi\nl‚àí1 ‚ààRd and the fully-connected layerFC makes the dimension match for addition. With the\nabove addition operation, the representation of sentence embedding is augmented by the word-level\nfeatures. We use the standard transformer block for transforming the sentence embeddings:\nZ‚Ä≤\nl = Zl‚àí1 + MSA(LN(Zl‚àí1)), (9)\nZl = Z‚Ä≤\nl + MLP(LN(Z‚Ä≤\nl)). (10)\nThis outer transformer block Tout is used for modeling relationships among sentence embeddings.\nIn summary, the inputs and outputs of the TNT block include the visual word embeddings and\nsentence embeddings as shown in Fig. 1(b), so the TNT can be formulated as\nYl,Zl = TNT(Yl‚àí1,Zl‚àí1). (11)\nIn our TNT block, the inner transformer block is used to model the relationship between visual\nwords for local feature extraction, and the outer transformer block captures the intrinsic information\nfrom the sequence of sentences. By stacking the TNT blocks for Ltimes, we build the transformer-\nin-transformer network. Finally, the classiÔ¨Åcation token serves as the image representation and a\nfully-connected layer is applied for classiÔ¨Åcation.\nPosition encoding. Spatial information is an important factor in image recognition. For sentence\nembeddings and word embeddings, we both add the corresponding position encodings to retain\nspatial information as shown in Fig. 1. The standard learnable 1D position encodings are utilized\nhere. SpeciÔ¨Åcally, each sentence is assigned with a position encodings:\nZ0 ‚ÜêZ0 + Esentence, (12)\nwhere Esentence ‚ààR(n+1)√ód are the sentence position encodings. As for the visual words in a\nsentence, a word position encoding is added to each word embedding:\nYi\n0 ‚ÜêYi\n0 + Eword, i= 1,2,¬∑¬∑¬∑ ,n (13)\nwhere Eword ‚ààRm√óc are the word position encodings which are shared across sentences. In this\nway, sentence position encoding can maintain the global spatial information, while word position\nencoding is used for preserving the local relative position.\n2.3 Complexity Analysis\nA standard transformer block includes two parts, i.e., the multi-head self-attention and multi-layer\nperceptron. The FLOPs of MSA are 2nd(dk + dv) +n2(dk + dv), and the FLOPs of MLP are\n2ndvrdv where ris the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a\nstandard transformer block are\nFLOPsT = 2nd(dk + dv) +n2(dk + dv) + 2nddr. (14)\nSince ris usually set as 4, and the dimensions of input, key (query) and value are usually set as the\nsame, the FLOPs calculation can be simpliÔ¨Åed as\nFLOPsT = 2nd(6d+ n). (15)\n4\nThe number of parameters can be obtained as\nParamsT = 12dd. (16)\nOur TNT block consists of three parts: an inner transformer block Tin, an outer transformer block\nTout and a linear layer. The computation complexity of Tin and Tout are 2nmc(6c+ m) and\n2nd(6d+ n) respectively. The linear layer has FLOPs of nmcd. In total, the FLOPs of TNT block\nare\nFLOPsTNT = 2nmc(6c+ m) +nmcd+ 2nd(6d+ n). (17)\nSimilarly, the parameter complexity of TNT block is calculated as\nParamsTNT = 12cc+ mcd+ 12dd. (18)\nAlthough we add two more components in our TNT block, the increase of FLOPs is small since\nc‚â™dand O(m) ‚âàO(n) in practice. For example, in the DeiT-S conÔ¨Åguration, we have d= 384\nand n= 196. We set c= 24and m= 16in our structure of TNT-S correspondingly. From Eq. 15\nand Eq. 17, we can obtain that FLOPsT = 376M and FLOPsTNT = 429M. The FLOPs ratio of\nTNT block over standard transformer block is about 1.14√ó. Similarly, the parameters ratio is about\n1.08√ó. With a small increase of computation and memory cost, our TNT block can efÔ¨Åciently model\nthe local structure information and achieve a much better trade-off between accuracy and complexity\nas demonstrated in the experiments.\n2.4 Network Architecture\nWe build our TNT architectures by following the basic conÔ¨Åguration of ViT [10] and DeiT [35]. The\npatch size is set as 16√ó16. The number of sub-patches is set as m= 4¬∑4 = 16by default. Other\nsize values are evaluated in the ablation studies. As shown in Table 1, there are three variants of\nTNT networks with different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M,\n23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224 √ó224\nimage are 1.4B, 5.2B and 14.1B respectively.\nTable 1: Variants of our TNT architecture. ‚ÄòTi‚Äô means tiny, ‚ÄòS‚Äô means small, and ‚ÄòB‚Äô means base.\nThe FLOPs are calculated for images at resolution 224√ó224.\nModel Depth Inner transformer Outer transformer Params FLOPs\ndim c #heads MLP r dim d #heads MLP r (M) (B)\nTNT-Ti 12 12 2 4 192 3 4 6.1 1.4\nTNT-S 12 24 4 4 384 6 4 23.8 5.2\nTNT-B 12 40 4 4 640 10 4 65.6 14.1\n3 Experiments\nIn this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness\nof the proposed TNT architecture.\nTable 2: Details of used visual datasets.\nDataset Type Train size Val size #Classes\nImageNet [30] Pretrain 1,281,167 50,000 1,000\nOxford 102 Flowers [25]\nClassiÔ¨Åcation\n2,040 6,149 102\nOxford-IIIT Pets [26] 3,680 3,669 37\niNaturalist 2019 [38] 265,240 3,003 1,010\nCIFAR-10 [18] 50,000 10,000 10\nCIFAR-100 [18] 50,000 10,000 100\nCOCO2017 [22] Detection 118,287 5,000 80\nADE20K [49] Segmentation 20,210 2,000 150\n5\n3.1 Datasets and Experimental Settings\nDatasets. ImageNet ILSVRC 2012 [30] is an image classiÔ¨Åcation benchmark consisting of 1.2M\ntraining images belonging to 1000 classes, and 50K validation images with 50 images per class. We\nadopt the same data augmentation strategy as that in DeiT [35] including random crop, random clip,\nRand-Augment [8], Random Erasing [48], Mixup [46] and CutMix [45]. For the license of ImageNet\ndataset, please refer to http://www.image-net.org/download.\nIn addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate\nthe generalization ability of TNT. The details of used visual datasets are listed in Table 2. The data\naugmentation strategy of image classiÔ¨Åcation datasets are the same as that of ImageNet. For COCO\nand ADE20K, the data augmentation strategy follows that in PVT [ 40]. For the licenses of these\ndatasets, please refer to the original papers.\nImplementation Details. We utilize the training strategy provided in DeiT [ 35]. The main ad-\nvanced technologies apart from common settings [13] include AdamW [23], label smoothing [31],\nDropPath [20], and repeated augmentation [15]. We list the hyper-parameters in Table 3 for better\nunderstanding. All the models are implemented with PyTorch [27] and MindSpore [17] and trained on\nNVIDIA Tesla V100 GPUs. The potential negative societal impacts may include energy consumption\nand carbon dioxide emissions of GPU computation.\nTable 3: Default training hyper-parameters used in our method, unless stated otherwise.\nEpochs Optimizer Batch Learning LR Weight Warmup Label Drop Repeated\nsize rate decay decay epochs smooth path Aug\n300 AdamW 1024 1e-3 cosine 0.05 5 0.1 0.1 ‚àö\n3.2 TNT on ImageNet\nWe train our TNT models with the same training settings as that of DeiT [35]. The recent transformer-\nbased models like ViT [10] and DeiT [35] are compared. To have a better understanding of current\nprogress of visual transformers, we also include the representative CNN-based models such as\nResNet [13], RegNet [ 28] and EfÔ¨ÅcientNet [ 32]. The results are shown in Table 4. We can see\nthat our transformer-based model, i.e., TNT outperforms all other visual transformer models. In\nparticular, TNT-S achieves 81.5% top-1 accuracy which is 1.7% higher than the baseline model DeiT-\nS, indicating the beneÔ¨Åt of the introduced TNT framework to preserve local structure information\ninside the patch. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet.\nNote that all the transformer-based models are still inferior to EfÔ¨ÅcientNet which utilizes special\ndepth-wise convolutions, so it is yet a challenge of how to beat EfÔ¨ÅcientNet using pure transformer.\nTable 4: Results of TNT and other networks on ImageNet.\nModel Resolution Params (M) FLOPs (B) Top-1 Top-5\nCNN-based\nResNet-50 [13] 224√ó224 25.6 4.1 76.2 92.9\nResNet-152 [13] 224√ó224 60.2 11.5 78.3 94.1\nRegNetY-8GF [28] 224√ó224 39.2 8.0 79.9 -\nRegNetY-16GF [28] 224√ó224 83.6 15.9 80.4 -\nEfÔ¨ÅcientNet-B3 [32] 300√ó300 12.0 1.8 81.6 94.9\nEfÔ¨ÅcientNet-B4 [32] 380√ó380 19.0 4.2 82.9 96.4\nTransformer-based\nDeiT-Ti [35] 224√ó224 5.7 1.3 72.2 -\nTNT-Ti 224√ó224 6.1 1.4 73.9 91.9\nDeiT-S [35] 224√ó224 22.1 4.6 79.8 -\nPVT-Small [40] 224√ó224 24.5 3.8 79.8 -\nT2T-ViT_t-14 [44] 224√ó224 21.5 5.2 80.7 -\nTNT-S 224√ó224 23.8 5.2 81.5 95.7\nViT-B/16 [10] 384√ó384 86.4 55.5 77.9 -\nDeiT-B [35] 224√ó224 86.4 17.6 81.8 -\nT2T-ViT_t-24 [44] 224√ó224 63.9 13.2 82.2 -\nTNT-B 224√ó224 65.6 14.1 82.9 96.3\n6\n20 30 40 50 60 70 80 90\nParams (M)\n75\n76\n77\n78\n79\n80\n81\n82\n83Accuracy (%)\nTNT\nViT\nDeiT\nResNet\nResNeXt\nRegNet\n4 6 8 10 12 14 16 18 20\nFLOPs (B)\n75\n76\n77\n78\n79\n80\n81\n82\n83Accuracy (%)\nTNT\nViT\nDeiT\nResNet\nResNeXt\nRegNet\n(a) Acc v.s. Params (b) Acc v.s. FLOPs\nFigure 2: Performance comparison of the representative visual backbone networks on ImageNet.\nWe also plot the accuracy-parameters and accuracy-FLOPs line charts in Fig. 2 to have an intuitive\ncomparison of these models. Our TNT models consistently outperform other transformer-based\nmodels by a signiÔ¨Åcant margin.\nInference speed. Deployment of transformer models on devices is important for practical applica-\ntions, so we test the inference speed of our TNT model. Following [35], the throughput is measured\non an NVIDIA V100 GPU and PyTorch, with 224√ó224 input size. Since the resolution and content\ninside the patch is smaller than that of the whole image, we may need fewer blocks to learn its\nrepresentation. Thus, we can reduce the used TNT blocks and replace some with vanilla transformer\nblocks. From the results in Table 5, we can see that our TNT is more efÔ¨Åcient than DeiT and PVT by\nachieving higher accuracy with similar inference speed.\nTable 5: GPU throughput comparison of vision transformer models.\nModel Indices of TNT blocks FLOPs (B) Throughput (images/s) Top-1\nDeiT-S [35] - 4.6 907 79.8\nDeiT-B [35] - 17.6 292 81.8\nPVT-Small [40] - 3.8 820 79.8\nPVT-Medium [40] - 6.7 526 81.2\nTNT-S [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] 5.2 428 81.5\nTNT-S-1 [1, 4, 8, 12] 4.8 668 81.4\nTNT-S-2 [1, 6, 12] 4.7 704 81.3\nTNT-S-3 [1, 6] 4.7 757 81.1\nTNT-S-4 [1] 4.6 822 80.8\n3.3 Ablation Studies\nTable 6: Effect of position encoding.\nPosition encoding\nModel Sentence-level Word-level Top-1\nTNT-S\n% % 80.5\n\" % 80.8\n% \" 80.7\n\" \" 81.5\nEffect of position encodings. Position infor-\nmation is important for image recognition. In\nTNT structure, sentence position encoding is\nfor maintaining global spatial information, and\nword position encoding is used to preserve lo-\ncally relative position. We verify their effect by\nremoving them separately. As shown in Table 6,\nwe can see that TNT-S with both patch position\nencoding and word position encoding performs\nthe best by achieving 81.5% top-1 accuracy. Removing sentence/word position encoding results in\na 0.8%/0.7% accuracy drop respectively, and removing all position encodings heavily decrease the\naccuracy by 1.0%.\nNumber of heads. The effect of #heads in standard transformer has been investigated in multiple\nworks [24, 39] and a head width of 64 is recommended for visual tasks [10, 35]. We adopt the head\nwidth of 64 in outer transformer block in our model. The number of heads in inner transformer block\n7\nis another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer\nblock (Table 7). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance.\nTable 7: Effect of #heads in inner transformer block in TNT-S.\n#heads 1 2 4 6 8\nTop-1 81.0 81.4 81.5 81.3 81.1\nTable 8: Effect of #words m.\nm c Params FLOPs Top-1\n64 6 23.8M 5.1B 81.0\n16 24 23.8M 5.2B 81.5\n4 96 25.1M 6.0B 81.1\nNumber of visual words. In TNT, the input image is\nsplit into a number of 16 √ó16 patches and each patch is\nfurther split into m sub-patches (visual words) of size\n(s,s) for computational efÔ¨Åciency. Here we test the ef-\nfect of hyper-parameter mon TNT-S architecture. When\nwe change m, the embedding dimension calso changes\ncorrespondingly to control the FLOPs. As shown in Table 8, we can see that the value of mhas\nslight inÔ¨Çuence on the performance, and we use m= 16by default for its efÔ¨Åciency, unless stated\notherwise.\nTNT\nDeiT\n(a) Feature maps in Block-1/6/12. (b) T-SNE of Block-12.\nFigure 3: Visualization of the features of DeiT-S and TNT-S.\n3.4 Visualization\nVisualization of Feature Maps. We visualize the learned features of DeiT and TNT to further\nunderstand the effect of the proposed method. For better visualization, the input image is resized\nto 1024√ó1024. The feature maps are formed by reshaping the patch embeddings according to their\nspatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in Fig. 3(a) where 12\nfeature maps are randomly sampled for these blocks each. In TNT, the local information are better\npreserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using\nt-SNE [37] (Fig. 3(b)). We can see that the features of TNT are more diverse and contain richer\ninformation than those of DeiT. These beneÔ¨Åts owe to the introduction of inner transformer block for\nmodeling local features.\nIn addition to the patch-level features, we also visualize the pixel-level embeddings of TNT in Fig. 4.\nFor each patch, we reshape the word embeddings according to their spatial positions to form the\nfeature maps and then average these feature maps by the channel dimension. The averaged feature\nmaps corresponding to the 14√ó14 patches are shown in Fig. 4. We can see that the local information\nis well preserved in the shallow layers, and the representations become more abstract gradually as the\nnetwork goes deeper.\nVisualization of Attention Maps. There are two self-attention layers in our TNT block, i.e., an\ninner self-attention and an outer self-attention for modeling relationship among visual words and\nsentences respectively. We show the attention maps of different queries in the inner transformer\nin Figure 5. For a given query visual word, the attention values of visual words with similar\nappearance are higher, indicating their features will be interacted more relevantly with the query.\nThese interactions are missed in ViT and DeiT,etc. The attention maps in the outer transformer can\nbe found in the supplemental material.\n8\nFigure 4: Visualization of the averaged word embeddings of TNT-S.\nFigure 5: Attention maps of different queries in the inner transformer. Red cross symbol denotes the\nquery location.\n3.5 Transfer Learning\nTo demonstrate the strong generalization ability of TNT, we transfer TNT-S, TNT-B models trained\non ImageNet to the downstream tasks.\nPure Transformer Image ClassiÔ¨Åcation. Following DeiT [ 35], we evaluate our models on 4\nimage classiÔ¨Åcation datasets with training set size ranging from 2,040 to 50,000 images. These\ndatasets include superordinate-level object classiÔ¨Åcation (CIFAR-10 [ 18], CIFAR-100 [18]) and\nÔ¨Åne-grained object classiÔ¨Åcation (Oxford-IIIT Pets [26], Oxford 102 Flowers [25] and iNaturalist\n2019 [38]), shown in Table 2. All models are Ô¨Åne-tuned with an image resolution of 384√ó384. We\nadopt the same training settings as those at the pre-training stage by preserving all data augmentation\nstrategies. In order to Ô¨Åne-tune in a different resolution, we also interpolate the position embeddings\nof new patches. For CIFAR-10 and CIFAR-100, we Ô¨Åne-tune the models for 64 epochs, and for\nÔ¨Åne-grained datasets, we Ô¨Åne-tune the models for 300 epochs. Table 9 compares the transfer learning\nresults of TNT to those of ViT, DeiT and other convolutional networks. We Ô¨Ånd that TNT outperforms\nDeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level\nrelations to get better feature representation.\nTable 9: Results on downstream image classiÔ¨Åcation tasks with ImageNet pre-training. ‚Üë384 denotes\nÔ¨Åne-tuning with 384√ó384 resolution.\nModel Params (M) ImageNet CIFAR10 CIFAR100 Flowers Pets iNat-19\nCNN-based\nGraÔ¨Åt ResNet-50 [36] 25.6 79.6 - - 98.2 - 75.9\nGraÔ¨Åt RegNetY-8GF [36] 39.2 - - - 99.1 - 80.0\nEfÔ¨ÅcientNet-B5 [32] 30 83.6 98.7 91.1 98.5 - -\nTransformer-based\nViT-B/16‚Üë384 [10] 86.4 77.9 98.1 87.1 89.5 93.8 -\nDeiT-B‚Üë384 [35] 86.4 83.1 99.1 90.8 98.4 - -\nTNT-S‚Üë384 23.8 83.1 98.7 90.1 98.8 94.7 81.4\nTNT-B‚Üë384 65.6 83.9 99.1 91.1 99.0 95.0 83.2\nPure Transformer Object Detection. We construct a pure transformer object detection pipeline\nby combining our TNT and DETR [3]. For fair comparison, we adopt the training and testing settings\nin PVT [40] and add a 2√ó2 average pooling to make the output size of TNT backbone the same as\nthat of PVT and ResNet. All the compared models are trained using AdamW [23] with batch size\nof 16 for 50 epochs. The training images are randomly resized to have a shorter side in the range\nof [640,800] and a longer side within 1333 pixels. For testing, the shorter side is set as 800 pixels.\nThe results on COCO val2017 are shown in Table 10. Under the same setting, DETR with TNT-S\nbackbone outperforms the representative pure transformer detector DETR+PVT-Small by 3.5 AP\nwith similar parameters.\n9\nTable 10: Results of object detection on COCO2017 val set with ImageNet pre-training. ‚Ä†Results\nfrom our implementation.\nBackbone Params Epochs AP AP50 AP75 APS APM APL\nResNet-50 [40] 41M 50 32.3 53.9 32.3 10.7 33.8 53.0\nDeiT-S‚Ä†[35] 38M 50 33.9 54.7 34.3 11.0 35.4 56.6\nPVT-Small [40] 40M 50 34.7 55.7 35.4 12.0 36.4 56.7\nPVT-Medium [40] 57M 50 36.4 57.9 37.2 13.0 38.7 59.1\nTNT-S 39M 50 38.2 58.9 39.4 15.5 41.1 58.8\nTable 11: Results of semantic segmentation on\nADE20K val set with ImageNet pre-training.\n‚Ä†Results from our implementation.\nBackbone Params FLOPs Steps mIoU\nResNet-50 [42] 56.1M 79.3G 40k 39.7\nDeiT-S‚Ä†[35] 30.3M 27.2G 40k 40.5\nPVT-Small [47] 32.1M 31.6G 40k 42.6\nTNT-S 32.1M 30.4G 40k 43.6\nPure Transformer Semantic Segmentation.\nWe adopt the segmentation framework of\nTrans2Seg [42] to build the pure transformer\nsemantic segmentation based on TNT backbone.\nWe follow the training and testing conÔ¨Åguration\nin PVT [40] for fair comparison. All the com-\npared models are trained by AdamW optimizer\nwith initial learning rate of 1e-4 and polynomial\ndecay schedule. We apply random resize and\ncrop of 512√ó512 during training. The ADE20K\nresults with single scale testing are shown in Table 11. With similar parameters, Trans2Seg with\nTNT-S backbone achieves 43.6% mIoU, which is 1.0% higher than that of PVT-small backbone and\n2.8% higher than that of DeiT-S backbone.\n4 Conclusion\nIn this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual\nrecognition. In particular, we uniformly split the image into a sequence of patches (visual sentences)\nand view each patch as a sequence of sub-patches (visual words). We introduce a TNT block in\nwhich an outer transformer block is utilized for processing the sentence embeddings and an inner\ntransformer block is used to model the relation among word embeddings. The information of visual\nword embeddings is added to the visual sentence embedding after the projection of a linear layer.\nWe build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision\ntransformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve\nand model the local information for visual recognition. Extensive experiments on ImageNet and\ndownstream tasks have demonstrate the effectiveness of the proposed TNT architecture.\nAcknowledgement\nThis work was supported by NSFC (62072449, 61632003), Guangdong-Hongkong-Macao Joint\nResearch Grant (2020B1515130004) and Macao FDCT (0018/2019/AKP, 0015/2019/AKP).\nA Appendix\nA.1 Visualization of Attention Maps\nAttention between Patches. In Figure 6, we plot the attention maps from each patch to all the\npatches. We can see that for both DeiT-S and TNT-S, more patches are related as layer goes deeper.\nThis is because the information between patches has been fully communicated with each other in\ndeeper layers. As for the difference between DeiT and TNT, the attention of TNT can focus on the\nmeaningful patches in Block-12, while DeiT still pays attention to the tree which is not related to the\npandas.\nAttention between Class Token and Patches. In Figure 7, we plot the attention maps between\nclass token to all the patches for some randomly sampled images. We can see that the output feature\nmainly focus on the patches related to the object to be recognized.\n10\nFigure 6: Visualization of the attention maps between all patches in outer transformer block.\nFigure 7: Example attention maps from the output token to the input space.\nA.2 Exploring SE module in TNT\nInspired by squeeze-and-excitation (SE) network for CNNs [16], we propose to explore channel-wise\nattention for transformers. We Ô¨Årst average all the sentence (word) embeddings and use a two-layer\nMLP to calculate the attention values. The attention is multiplied to all the embeddings. The SE\nmodule only brings in a few extra parameters but is able to perform dimension-wise attention for\nfeature enhancement. From the results in Table 12, adding SE module into TNT can further improve\nthe accuracy slightly.\nTable 12: Exploring SE module in TNT.\nModel Resolution Params (M) FLOPs (B) Top-1 (%) Top-5 (%)\nTNT-S 224√ó224 23.8 5.2 81.5 95.7\nTNT-S + SE 224√ó224 24.7 5.2 81.7 95.7\n11\nA.3 Object Detection with Faster RCNN\nAs a general backbone network, TNT can also be applied with multi-scale vision models like Faster\nRCNN [29]. We extract the features from different layers of TNT to construct multi-scale features.\nIn particular, FPN takes 4 levels of features (1\n4 , 1\n8 , 1\n16 , 1\n32 ) as input, while the resolution of feature\nof every TNT block is 1\n16 . We select the 4 layers from shallow to deep (3rd, 6th, 9th, 12th) to form\nmulti-level representation. To match the feature shape, we insert deconvolution/convolution layers\nwith proper stride. We evaluate TNT-S and DeiT-S on Faster RCNN with FPN [ 21]. The DeiT\nmodel is used in the same way. The COCO2017 val results are shown in Table 13. TNT achieves\nmuch better performance than ResNet and DeiT backbones, indicating its generalization for FPN-like\nframework.\nTable 13: Results of Faster RCNN object detection on COCO minival set with ImageNet pre-training.\n‚Ä†Results from our implementation.\nBackbone Params (M) Epochs AP AP50 AP75 APS APM APL\nResNet-50 [21, 5] 41.5 12 37.4 58.1 40.4 21.2 41.0 48.1\nDeiT-S‚Ä†[35] 46.4 12 39.9 62.8 42.6 23.4 42.5 54.0\nTNT-S 48.1 12 41.5 64.1 44.5 25.7 44.6 55.4\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021.\n[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020.\n[7] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position\nencodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In CVPR Workshops, 2020.\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL-HLT (1), 2019.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, et al. A survey on vision transformer. arXiv preprint arXiv:2012.12556, 2020.\n[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features\nfrom cheap operations. In CVPR, 2020.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[14] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[15] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten HoeÔ¨Çer, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. In CVPR, 2020.\n[16] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n[17] Huawei. Mindspore. https://www.mindspore.cn/, 2020.\n12\n[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convolutional\nneural networks. In NeurIPS, pages 1097‚Äì1105, 2012.\n[20] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks\nwithout residuals. arXiv preprint arXiv:1605.07648, 2016.\n[21] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740‚Äì755, 2014.\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[24] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.\n[25] Maria-Elena Nilsback and Andrew Zisserman. Automated Ô¨Çower classiÔ¨Åcation over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages\n722‚Äì729. IEEE, 2008.\n[26] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, pages\n3498‚Äì3505. IEEE, 2012.\n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. NeurIPS, 2019.\n[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll√°r. Designing network\ndesign spaces. In CVPR, 2020.\n[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Advances in neural information processing systems, pages 91‚Äì99, 2015.\n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211‚Äì252, 2015.\n[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\n[32] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model scaling for convolutional neural networks. In\nICML, 2019.\n[33] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented shortcuts\nfor vision transformers. arXiv preprint arXiv:2106.15941, 2021.\n[34] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold\nregularized dynamic network pruning. In CVPR, pages 5018‚Äì5028, 2021.\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. In ICML, 2021.\n[36] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Herv√© J√©gou. GraÔ¨Åt: Learning\nÔ¨Åne-grained image representations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.\n[37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008.\n[38] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classiÔ¨Åcation and detection dataset. In CVPR, pages\n8769‚Äì8778, 2018.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn ICCV, 2021.\n[41] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\npages 7794‚Äì7803, 2018.\n[42] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting\ntransparent object in the wild with transformer. In IJCAI, 2021.\n[43] Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, and Chang Xu. Renas:\nRelativistic evaluation of neural architecture search. In CVPR, pages 4411‚Äì4420, 2021.\n13\n[44] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986, 2021.\n[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiÔ¨Åers with localizable features. In ICCV, 2019.\n[46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In CVPR, 2021.\n[48] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn AAAI, volume 34, pages 13001‚Äì13008, 2020.\n[49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017.\n[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In ICLR, 2021.\n14",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.787193775177002
    },
    {
      "name": "Computer science",
      "score": 0.67671799659729
    },
    {
      "name": "Architecture",
      "score": 0.6409820318222046
    },
    {
      "name": "Sentence",
      "score": 0.6114034652709961
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5227042436599731
    },
    {
      "name": "Granularity",
      "score": 0.510237455368042
    },
    {
      "name": "Programming language",
      "score": 0.12165170907974243
    },
    {
      "name": "Engineering",
      "score": 0.11417236924171448
    },
    {
      "name": "Electrical engineering",
      "score": 0.11186441779136658
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1007
}