{
  "title": "Joint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation",
  "url": "https://openalex.org/W4283514179",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2232586647",
      "name": "Qin Chuanbo",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2006508219",
      "name": "Yujie Wu",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2113879144",
      "name": "Junying Zeng",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2165688793",
      "name": "Lianfang Tian",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109520086",
      "name": "Yikui Zhai",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2098424515",
      "name": "Fang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113152592",
      "name": "Xiaozhi Zhang",
      "affiliations": [
        "University of South China"
      ]
    },
    {
      "id": "https://openalex.org/A2232586647",
      "name": "Qin Chuanbo",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2006508219",
      "name": "Yujie Wu",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2113879144",
      "name": "Junying Zeng",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2165688793",
      "name": "Lianfang Tian",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109520086",
      "name": "Yikui Zhai",
      "affiliations": [
        "Wuyi University"
      ]
    },
    {
      "id": "https://openalex.org/A2098424515",
      "name": "Fang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113152592",
      "name": "Xiaozhi Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3012740297",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3096678291",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W3119586106",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2118386984",
    "https://openalex.org/W2997638939",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W4254751698",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4213103335",
    "https://openalex.org/W2897432176",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3156643506",
    "https://openalex.org/W2914821433",
    "https://openalex.org/W3215502949",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3128090102"
  ],
  "abstract": "Abstract Automatic segmentation of breast cancer lesions in dynamic contrast-enhanced magnetic resonance imaging is challenged by low accuracy of delineation of the infiltration area, variable structure and shapes, large intensity heterogeneity changes, and low boundary contrast. This study constructed a two-stage breast cancer image segmentation framework and proposes a novel breast cancer lesion segmentation model (TR-IMUnet). The benchmark U-Net network model enables a rough delineation of the breast area in the acquired images and eliminates the influence of unrelated tissues (chest muscle, fat, and heart) on breast tumor segmentation. Based on the extracted results of the region of interest, the rectified linear unit (ReLU) function of the encoding–decoding structure in the model was replaced by an improved ReLU function to reserve and adjust the data dynamically according to input information. The segmentation accuracy of breast cancer lesions was improved by embedding a multi-scale fusion block and a transformer module in the coding path of the model, thereby obtaining multi-scale and global attention information. The experimental results showed that the breast tumor segmentation indexes Dice coefficient (Dice), Intersection over Union (IoU), Sensitivity (SEN), and Positive Predictive Value (PPV) increased by 4.27, 5.21, 3.37, and 3.68%, respectively, relative to the U-Net reference model. The proposed model improves the segmentation results of breast cancer lesions and reduces small area mis-segmentation and calcification segmentation.",
  "full_text": "FOUNDATION, ALGEBRAIC, AND ANALYTICAL METHODS IN SOFT COMPUTING\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer\nSegmentation\nChuanbo Qin1 • Yujie Wu1 • Junying Zeng1 • Lianfang Tian2 • Yikui Zhai1 • Fang Li3 • Xiaozhi Zhang4\nAccepted: 17 May 2022 / Published online: 25 June 2022\n/C211The Author(s) 2022\nAbstract\nAutomatic segmentation of breast cancer lesions in dynamic contrast-enhanced magnetic resonance imaging is challenged\nby low accuracy of delineation of the inﬁltration area, variable structure and shapes, large intensity heterogeneity changes,\nand low boundary contrast. This study constructed a two-stage breast cancer image segmentation framework and proposes\na novel breast cancer lesion segmentation model (TR-IMUnet). The benchmark U-Net network model enables a rough\ndelineation of the breast area in the acquired images and eliminates the inﬂuence of unrelated tissues (chest muscle, fat, and\nheart) on breast tumor segmentation. Based on the extracted results of the region of interest, the rectiﬁed linear unit (ReLU)\nfunction of the encoding–decoding structure in the model was replaced by an improved ReLU function to reserve and\nadjust the data dynamically according to input information. The segmentation accuracy of breast cancer lesions was\nimproved by embedding a multi-scale fusion block and a transformer module in the coding path of the model, thereby\nobtaining multi-scale and global attention information. The experimental results showed that the breast tumor segmentation\nindexes Dice coefﬁcient (Dice), Intersection over Union (IoU), Sensitivity (SEN), and Positive Predictive Value (PPV)\nincreased by 4.27, 5.21, 3.37, and 3.68%, respectively, relative to the U-Net reference model. The proposed model\nimproves the segmentation results of breast cancer lesions and reduces small area mis-segmentation and calciﬁcation\nsegmentation.\nKeywords DCE-MRI /C1 Segmentation /C1 Breast tumor /C1 U-net /C1 Transformer /C1 Dynamic ReLU\n1 Introduction\nBreast cancer is a life-threatening disease that can seriously\naffects both the physical and mental health of patients.\nCompared with imaging modalities such as molybdenum\ntarget, ultrasound, X-ray, magnetic resonance imaging\n(MRI), and thermography, and other types of imaging\nmodalities (Litjens et al. 2017), DCE-MRI (dynamic\ncontrast-enhanced MRI) is non-invasive and enables visu-\nalization of the microvascular network. DCE-MRI is\nextremely sensitive in detecting early breast cancer, and\nthus is routinely utilized in clinical practice to assess the\nlocal extent of disease and determine the molecular type\n(Xiao et al. 2021). Automated segmentation of the lesion\narea in DCE-MRI images has great clinical research value\nfor the early diagnosis, prevention, postoperative guidance,\nand prognosis of breast cancer; it is the key basis for the\ndevelopment and implementation of artiﬁcial intelligence\ncomputer-aided diagnosis systems.\nIn general, DCE-MRI breast cancer types are mainly\nclassiﬁed as either mass or non-mass types. Breast cancer\nlesions are mainly distributed along the gland. The mass\ntype of breast cancer is relatively easy to identify and\nsegment, whereas the non-mass varies in morphology.\nCavities, weak boundaries, and low contrast lesion inten-\nsity can easily lead to delayed or misdiagnosis, and inad-\nequate treatment. Moreover, manual delineation of the\n& Junying Zeng\nzengjunying@126.com\n1 Faculty of Intelligent Manufacturing, Wuyi University,\nJiangmen 529020, China\n2 Automation Science and Engineering, South China\nUniversity of Technology, Guangzhou 510641, China\n3 Jiangmen Maternal and Child Healthcare Hospital,\nJiangmen 529020, China\n4 School of Electrical Engineering, University of South China,\nHengyang 421001, China\n123\nSoft Computing (2022) 26:8317–8334\nhttps://doi.org/10.1007/s00500-022-07235-0(0123456789().,-volV)(0123456789().,- volV)\nbreast cancer lesion area is a very demanding and skillful\nprocess, and it is thus necessary for radiologists to have\nrich clinical experience. In fact, the delineation process can\nbe easily affected by subjectivity, psychological ﬂuctua-\ntions, and physical fatigue, leading to random breast lesion\ndelineation results. Consequently, DCE-MRI breast cancer\nsegmentation that is automated and accurate has great\nclinical application value and signiﬁcance, while remaining\na challenging task.\nThe present focus of mainstream research in automated\nsegmentation of breast cancer is performed using deep\nlearning strategies. In recent years, algorithms research\nbased on deep learning has made great progress in medical\nimage segmentation (Zhang et al. 2021; Pham et al. 2000).\nA mask-guided hierarchical fully convolutional network\n(FCN) segmentation framework was recently proposed at\nDuke University (Zhang et al. 2018). More speciﬁcally, a\nrough outline of the breast region and the accurate seg-\nmentation of breast cancer lesions was successfully per-\nformed using cascade FCN rough segmentation and FCN\nﬁne segmentation models. And their proposed models have\nbeen trained and tested on their own database, reaching a\nDice coefﬁcient of 0.72. However, troublesome issues are\nstill encountered, such as missing, incorrect, or inaccurate\nsegmentations of breast cancer lesions with small voxels.\nThis is because only the FCN cascade architecture and a\nsingle-model structure were used. No effective solutions\nthat might improve segmentation accuracy have been\nimplemented, such as multi-scale and dynamic image data\nmechanisms. Such novel mechanisms are quite recent in\ndevelopment for accurate segmentation of breast cancer\nlesions.\nRonneberger et al. ( 2015) proposed a U-type network\nstructure with encoder-decoder network architecture and a\nskip connection that has shown excellent performance in\nbiomedical image segmentation. In the U-type structure,\nthe image is continuously extracted and downsampled in\nthe encoder. Brieﬂy, different levels of feature information\nare extracted when reducing the image size to save memory\nresources. Such information is transmitted to the decoder\nthrough a skip connection. The decoder mines semantic\ninformation in the original image completely by combining\nhigh- and low-level feature information transmitted via the\nskip connection.\nSemantic information in breast cancer segmentation\ntasks include the location, size, contour, boundary, and\nother characteristics of the lesion needed to perform auto-\nmated segmentation. However, segmentation is vulnerable\nto noise, as breast cancer lesions are small while the larger\narea of the breast is complex with lobules, ducts, fatty and\nﬁbrous connective tissue, and so forth. Therefore, accuracy\ncan be improved if normal breast tissue regions of interest\ncan be delineated prior to segmentation (Zhang et al. 2018).\nPrevious studies (Piantadosi et al. 2020; Ronneberger et al.\n2015; Wei et al. 2018) have applied U-Net to separate the\nbreast region from the breast wall line, demonstrating the\nability and effectiveness of the U-Net network for breast\nROI extraction.\nThe transformer structure proposed by Vaswani et al.\n(2017) has achieved good results in natural language pro-\ncessing (NLP) and other tasks (Han et al. 2021; Uddin et al.\n2022), and researchers have been investigating its appli-\ncability for computerized visualization tasks (Li et al .\n2020; Liu et al. 2021). For example, Dosovitskiy et al.\n(2020) proposed the Vision Transformer (ViT) structure for\nimage classiﬁcation tasks, which abandons the traditional\nConvolution Neural Network (CNN) structure completely\nand directly uses the Transformer encoder part to classify\nimages. Furthermore, Carion et al. ( 2020) proposed the\nDetection with Transformer (DETR) structure for object\ndetection tasks. This structure ﬁrst uses CNN to extract\nfeatures from the image and then inputs the feature\nsequence into the Transformer encoder to output the object\ncategory and coordinates. Prangemeier et al. ( 2020) added\na multi-head attention branch and a decoder structure to\nDETR for cell instance segmentation. This structure\nemploys DETR to detect cell instances while using the\nmulti-head attention branch and the decoder structure to\nsegment cell instances simultaneously. In addition, Zheng\net al. ( 2020) proposed the Segmentation Transformer\n(SETR) structure for image semantic segmentation. This\nstructure passes the image block directly into the Trans-\nformer encoder and upsamples the feature output from the\nTransformer through a decoder to perform the segmenta-\ntion of different objects in the image.\nFinally, Liu et al. ( 2021) have recently proposed a novel\ntransformer (Swin Transformer) that could serve as the\nbackbone of computer vision: a hierarchical transformer\nthat can solve the differences caused when the visual entity\nscale is larger than the words used in the text. Medical\nimage processing is an important ﬁeld in computer visu-\nalization tasks, and we also believe that the Transformer\nstructure has excellent performance characteristics.\nTherefore, the present study aims to achieve the precise\nsegmentation of breast cancer lesions using Transformer\nstructure.\nBreast cancer lesions have small diameters that are\nvariable in histological appearance, or blurry boundaries,\nwhose surrounding background is complex. These aspects\ninduce a typical class-imbalance problem. Tissues, such as\nmuscle, heart, chest wall, skin, and fat, can seriously\ninterfere with automated segmentation of breast cancer\ntumors. Considering that breast cancer lesions usually\nappear in the breast area, we propose a two-stage breast\ncancer segmentation framework to eliminate the inﬂuence\n8318 C. Qin et al.\n123\nof irrelevant factors as much as possible and obtain com-\nplete image information.\nIn the ﬁrst stage, the U-Net model is trained to extract a\nrough breast ROI, eliminating the inﬂuence of breast\nmuscle, fat, heart, and other irrelevant tissues.\nBased on the breast ROI segmented in stage one, in the\nsecond stage, a TR-IMUnet model is proposed to perform\nbreast tumor segmentation. The proposed model references\nthe classic network U-Net. Then, the Transformer module,\nthe improved dynamic rectiﬁed linear unit (IDy-ReLU)\nmodule, and the Multi-Scale Parallel Convolution Fusion\n(MSPCF) module were designed and integrated to con-\nstruct the TR-IMUnet model. The model maximizes the\nadvantages of the global and local receptive ﬁelds, obtains\nrich global information, extracts ﬁne local information, and\ncan dynamically adjust and retain data based on the dif-\nferences in image data, thereby reducing image informa-\ntion loss and redundancy. The main work and contributions\nof this article are as follows:\n(i) Construction of the Transformer module (Vaswani\net al. 2017) and integration into the U-Net network\nto maximize the advantages of the convolution\noperation and the Transformer module, conferring\nthe model with a global receptive ﬁeld and the\nability to extract ﬁne local information.\n(ii) Considering the shortcomings of the dynamic\nReLU (Chen et al. 2020), an improved dynamic\nReLU module is constructed, allowing the model\nto maximize the spatial and channel information of\nthe input data to dynamically adjust and retain the\ndata, reduce data loss and redundancy, and\nimprove its robustness.\n(iii) In the coding path, an MSPCF module is pro-\nposed. A multi-scale parallel convolution structure\nis adopted, feature information of different scales\nis extracted through convolution operations of\ndifferent kernel sizes, and the model’s ability to\nextract edge and ﬁne information is enhanced.\nFig. 1 Proposed TR-IMUnet architecture\nFig. 2 DCE-MRI breast tumor. a, b cross-sectional images; c, d 3D\nrendering using the ITK-Snap tool\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8319\n123\nExperimental tests have shown that the proposed model\nperforms well in breast cancer segmentation tasks, can\naccurately locate and segment breast cancer lesions in\nsmall areas, and has high segmentation accuracy (Fig. 1).\n2 The proposed method\n2.1 Breast ROI extraction\nBreast cancer lesions do not generally appear at the chest\nwall or at the edge of the breast. Therefore, breast ROI\nextraction is conducive to the accurate segmentation of the\ntumor (Zhang et al. 2018). The benchmark U-Net model\n(Ronneberger et al. 2015) is commonly used in medical\nimage segmentation tasks. In the ﬁrst stage of the present\nstudy, the U-Net model was employed to achieve a rough\nbreast ROI delineation based on the breast region labels\noutlined by clinicians (Fig. 2). Figure 2a, b shows a DCE-\nMRI breast cancer cross-sectional image. The ROI is\nmarked in green, and the cancerous area is in yellow. A\ncomparison of the two images demonstrates the variability\nof breast cancer tumors in shape, boundaries, cavities, and\nother features. The ITK-Snap tool was then used to con-\nstruct a 3D rendering (Fig. 2c, d) of the spatial relationship\nbetween the breast tissue and the breast cancer.\n2.2 Proposed TRIMU-Net construction method\nA. Transformer model\nConvolution is limited by the size of the convolution\nkernel, and the receptive ﬁeld size is the same as the size of\nthe convolution kernel. The traditional method increases\nthe receptive ﬁeld by stacking the convolutional and\npooling layers together. However, this process makes the\nmodel deeper, and thus other issues are bound to emerge,\nsuch as gradient disappearance or gradient explosion, with\nan increased number of model parameters and calculations.\nConsequently, Vaswani et al. ( 2017) proposed the Trans-\nformer module, which was originally used for NLP tasks to\nobtain the dependency between each word. In the present\nstudy, the Transformer module was used for image seg-\nmentation to obtain the dependency between each pixel,\ni.e., the correlations information between long and short\ndistances. Then, every pixel has a global receptive ﬁeld. In\ncontrast to Zheng et al. ( 2020), who only used the Trans-\nformer’s Encoder structure to extract global information,\nthe present study used the U-Net encoder structure to\nextract features from the original image and pass them into\nthe Transformer. This method compensates for the lack of\nlocal information caused by the loss of the Transformer’s\nability to capture local features and maximizes the\nadvantages of the convolution operation and the Trans-\nformer module. The speciﬁc structure of the applied\nTransformer module is shown in Fig. 3.\nThe Transformer module accepts the feature embedding\nof a 1D sequence Z 2 R\nL/C2C as the input, where L is the\nsequence length and C is the hidden channel size. There-\nfore, the feature map needs to be converted from x 2\nRH/C2W/C2Z to Z before passing into the Transformer. To save\nmemory and reduce the number of calculations, only the\nTransformer was embedded into the last layer of the\nencoder structure. Conv-GN-ReLU was used to change the\nnumber of feature map channels, and thus meet the\nTransformer’s needs. The speciﬁc conversion was:\nX 2 R\nH/C2W/C2Z ! Z 2 RL/C2C\nA learnable position vector P 2 RL/C2C as deﬁned, with\nthe same size as Z to encode the position information of\neach pixel. The Transformer input is E ¼ Z þ P The\nencoder structure of the Transformer module is composed\nof a multi-head self-attention and feedforward neural net-\nwork. The multi-head self-attention has m layers, and each\nlayer is a self-attention operation, which is used to obtain\nattention in different representation spaces. The self-at-\ntention formula is:\nFig. 3 The Transformer module\n Fig. 4 Different improvement methods based on ReLU\n8320 C. Qin et al.\n123\nself /C0 attention Z l/C0/C1\n¼ Zl þ softmax Q /C1 KT\nﬃﬃﬃﬃ ﬃdk\np\n/C18/C19\n/C1 V ð1Þ\nwhere Q ¼ ZlWQ; K ¼ ZlWK ; V ¼ ZlWV dk ¼ dmodel ¼\nC=m and WQ=WK =WV 2 RC/C2d re the learnable parameters\nin the three linear transformation operations, and d is the\ndimension of Q=K=V. Therefore, the formula for multi-\nhead self-attention is as follows:\nMultiHead Z l/C0 1/C0/C1\n¼ Concat head 1; /C1/C1/C1 ; headmðÞ Wo\nwhereheadi ¼ self /C0 attention Z l/C0 1/C0/C1 ð2Þ\nwhere Wo 2 Rmd/C2C The feedforward network consists of\ntwo linear transformations and a ReLU activation function,\nand its formula is as follows:\nFFNN X 0ðÞ ¼ W2r W1X0ðÞ\nwhereX0 ¼ LayerNorm Z l/C0 1 þ MultiHeadðZl/C0 1Þ\n/C0/C1 ð3Þ\nwhere W1 nd W2 re the parameters of the two linear con-\nversion layers and r epresents the ReLU activation\nfunction.\nTherefore, the output formula of the Transformer\nencoder part is\nZl ¼ LNðLN Z l/C0 1 þ MultiHead Z l/C0 1/C0/C1/C0/C1\nþFFNNðLNðZl /C0 1 þ MultiHeadðZl/C0 1ÞÞÞÞ\nð4Þ\nB. Improved dynamic ReLU activation function\nThe ReLU (Vinod et al. 2010) is widely used in deep\nneural networks because it can improve the performance of\nfeedforward networks simply and effectively. The ReLU\ncan be found in many typical structures, such as U-Net\n(Ronneberger et al. 2015), ResNet (He et al. 2015), and\nUnet3 ? (Huang et al. 2020). ReLU-based improvements\ninclude leaky ReLU (Maas et al. 2013) and PReLU (He\net al. 2015). However, according to Chen et al. ( 2020) most\nof these improvements (Maas et al. 2013; He et al. 2015)\nare static, i.e., they are performed in exactly the same way\nregardless of the input (such as images). Therefore,\ndynamic ReLU (Dy-ReLU), which uses data-adaptive\napproaches that automatically correct data according to\ndifferent inputs, was proposed (Fig. 4). In the original\narticle, the author described three different forms of Dy-\nReLU, but only Dy-ReLU-B was deemed effective in our\ndataset, and therefore, the present study only discusses Dy-\nReLU-B. For convenience, all subsequent Dy-ReLUs refer\nto Dy-ReLU-B. The main idea was to use a hyperfunction\nhðxÞ to encode the entire content of the input data and form\nan activation function f\nhðxÞðxÞ that would be adaptive to the\ninput data.\nThe deﬁnition of dynamic ReLU by Chen et al. ( 2020)i s\nshown in formula ( 5):\nyc ¼ fh xðÞ xcðÞ ¼ max\n1 /C20k /C20K\nak\nc xðÞ xc þ bk\nc xðÞ\n/C8/C9\n; ð5Þ\nwhere xc epresents the data of the c-th channel of the input\nx and the coefﬁcient ðak\nc; bk\ncÞ is the output of the hyper-\nfunction hðxÞ:\na1\n1; /C1/C1/C1 ; a1\nC; /C1/C1/C1 ; aK\n1 ; /C1/C1/C1 ; aK\nC ; b1\n1; /C1/C1/C1 ; b1\nC; /C1/C1/C1 ; bK\n1 ; /C1/C1/C1 ; bK\nC\n/C2/C3 T\n¼ h xðÞ\nð6Þ\nwhere K s the number of functions and C s the number of\nchannels. The parameters ak\nc; bk\nc\n/C0/C1\ne related to all inputs x.\nIn Dy-ReLU (or Dy-ReLU-B), the hyperfunction hðxÞ is\nimplemented by modeling the input data through a light-\nweight network similar to Squeeze-and-Excitation (SE)\n(Hu et al. 2018), as shown in the left part of Fig. 5.F o ra n\ninput vector x with C /C2 H /C2 W dimensions, the SE module\nﬁrst compresses the spatial information through global\naverage pooling and then explicitly models the interde-\npendence between the channels through two fully con-\nnected layers and a normalization layer, thereby\nrecalibrating adaptively the channel characteristic\nresponse. Although the SE module is very effective, it still\nhas shortcomings. For example, the squeeze operation of\nthe SE module forces the output to lose its ability to cap-\nture interdependence among different spatial information.\nDy-ReLU-C suggests that a branch should be built to\nsupplement the spatial information. However, Dy-ReLU-C\nwas experimentally shown to induce a negative effect.\nThus, we referred to the Convolution Block Attention\nFig. 5 Comparison between dynamic ReLU and improved dynamic\nReLU modules\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8321\n123\nModule proposed by Woo et al. ( 2018) to improve the\nhyperfunction hðxÞ, as shown in the right part of Fig. 5.\nThe input vector x of C /C2 H /C2 W dimensions is com-\npressed along the channel dimension through global aver-\nage and global maximum pooling and then the\nconvolutional and normalization layers are used to model\nthe interdependence of spatial information explicitly. The\nspatial characteristic coefﬁcients are multiplied by the\noriginal data to adaptively recalibrate the spatial charac-\nteristic response. At that point, the interdependence among\ndifferent spatial information is captured, and the SE mod-\nule captures the interdependence between the channels.\nThe modiﬁed hyperfunction h xðÞ rmula is:\nh xðÞ ¼ M\nc Ms xðÞ /C3 xðÞ ð 7Þ\nwhere Ms s the acquired spatial attention map and Ms s the\nacquired channel attention map. The deﬁnitions of Ms nd\nMc re as follows:\nMs ¼ r f 3/C23 Maxpool xðÞ ; Avgpool xðÞ½/C138ðÞ\n/C0/C1\nð8Þ\nMc ¼ rðFFNN Maxpool xðÞðÞ þ FFNN Avgpool xðÞðÞ\n¼ r W2r0 W1XmaxðÞ þ W2r0 W1Xavg\n/C0/C1/C0/C1\n; ð9Þ\nwhere r represents the Sigmoid activation function, r0\nrepresents the ReLU activation function, f 3/C23 is the con-\nvolution operation with a 3 /C2 3 convolution kernel, FFNN\nis the feedforward network, which is composed of two\nlinear transformations and a ReLU function, W1 and W2\nare the parameters of the two linear conversion layers, and\nMaxpool and Avgpool represent the global maximum\npooling and global average pooling, respectively.\nSimilar to Dy-ReLU, 2 KC elements are the outputs that\ncorrespond to the coefﬁcients a1:K\n1:C and b1:K\n1:C and are denoted\nas Dak\nc xðÞ and Dbk\nc. The ﬁnal output is calculated by adding\nthe initial value and the output element in the following\nformula:\nak\nc xðÞ ¼ ak þ kaDak\nc xðÞ ; bk\nc xðÞ ¼ bk þ kbDbk\nc xðÞ ð 10Þ\nwhere ak and bk are the initial values of ak\nc and bk\nc\nrespectively, and ka and kb re scalars that control the\ncoefﬁcient range. In accordance with Chen et al. ( 2020),\nwe let K ¼ 2 and set a1 ¼ 1; a2 ¼ b1 ¼ b2 ¼ 0 ka and kb\ndefault to 1.0 and 0.5, respectively.\nC. MSPCF module\nThe MSPCF module was introduced because cancerous\nareas vary in size, shape, gray scale, and deﬁnitive\nboundary (Fig. 6). With the MSPCF module parallel con-\nvolution structures can obtain the characteristics of differ-\nent receptive ﬁelds in the same layer, improve the accuracy\nof segmentation in cases of blurred edges and small mas-\nses, and then transfer the information of different scales to\nthe next layer. Consequently, this can provide rich\nsemantic information for the subsequent down- and up-\nsampling of the decoder layer, thereby overall improving\nthe segmentation accuracy.\nThe MSPCF module contains three parallel convolu-\ntional layers and one global average pooling layer. These\nparallel convolution operations have 1 /C2 1, 3 /C2 3; and 5 /C2\n5 kernel sizes. The multi-scale context feature information\nis obtained by using the convolution of different kernel\nsizes, and the 1 /C2 1 convolution layer is used to retain the\nfeature information of the current scale. The MSPCF\nmodule also uses the global average pooling layer to obtain\npixel-level global information, and then use bilinear\ninterpolation to obtain the required dimensions. Using the\n1 /C2 1 convolution process, the global average pooling layer\ninformation is fused and compressed with the three parallel\nconvolutional layers. Finally, the output feature map is\npassed to the next layer.\nD. Improvement process\nThe Transformer improved dynamic ReLU, and multi-\nscale parallel convolution modules were added to the\nbenchmark U-Net model to discuss the effectiveness of\neach module. Finally, all modules were added to the\nbenchmark U-Net model to form the proposed TR-IMUnet\nFig. 6 The MSPCF Block\n8322 C. Qin et al.\n123\nmodel that achieved the best performance. Improvement\nwas in accordance with the following protocol:\nFirst, based on the classic U-Net network, the Trans-\nformer module was introduced to capture the global\ninformation of the image and form a global attention\nmechanism so that each pixel had a global receptive ﬁeld.\nTo allow feature extraction, and thus compensate for the\nfeature loss caused by the Transformer’s inability to cap-\nture local information, the encoder of the U-Net model was\nused. Thus, the new model should maximize the convolu-\ntion operation and the Transformer module. This model\nwas named TR-U-Net.\nSecond, based on the dynamic ReLU proposed by Chen\net al. ( 2020), we veriﬁed the effectiveness of the TR-U-Net\nmodel based on the benchmark U-Net model and identiﬁed\nits potential shortcomings experientially. Thereby, an\nimproved dynamic ReLU module was designed to replace\nthe ReLU operation in the U-Net model. Each activation\nfunction in the model was adaptive and speciﬁcally\ndesigned according to the input spatial data and channel\ninformation, thus maximizing the input information and\nimproving the generalization ability of the model. This\nmodel was called Improved U-Net (IUnet). The Trans-\nformer module and the improved dynamic ReLU module\nwere added into the benchmark U-Net model to combine\nthe advantages of each module and form the TR-IUnet\nmodel.\nFinally, based on the proposed TR-IUnet network, the\nMSPCF module was embedded behind the feature extrac-\ntion unit of each layer of the coding part, which was used to\nextract the feature information of different levels and\nscales, and then transmit it to the next layer after fusion and\ncompression to enrich the feature information extracted by\nthe network. At the same time, this is transmitted to the\ncorresponding decoder layer through a long connection to\nsupplement the semantic information lost during the\nupsampling process. Experimental tests showed that the\nTR-IMUnet model can enable optimal breast cancer\nsegmentation.\nE. Loss function\nData imbalance is a common issue in medical image\nsegmentation. In most cases, the number of lesion voxels in\nthe dataset is signiﬁcantly less than the number of non-\nlesion voxels. The same is true for breast cancer, and the\narea of lesions is much smaller than that of the breast area.\nFor this purpose, the Tversky (Salehi et al. 2017) loss\nfunction was adopted. Based on the Tversky index, this\nloss function can effectively solve the problem of data\nimbalance, allowing for a good balance between precision\nand recall rate. The formula is as follows:\nT a; bðÞ ¼\nPN\ni¼1 p0ig0i\nPN\ni¼1 p0ig0i þ a PN\ni¼1 p0ig1i þ b PN\ni¼1 p1ig0i\nð11Þ\nwhere p; g are the output result of the last sigmoid layer of\nthe network. Among them, p0i is the output probability of\nthe pathological voxel, and p1i is the output probability of\nthe non-pathological voxel. Similarly, gg0i represent the\noutput probability of a pathological voxel, and g1i repre-\nsents the output probability of a non-pathological voxel.\nThen, a; b control the false negatives and false positives\nseparately. We can control the trade-off between false\npositives and false negatives by adjusting a; b. In our\nexperimental process, we set a ¼ 0:3andb ¼ 0:7\n3 Experimental process and analysis\n3.1 Experimental environment\nThis study used an Intel Core I7-8700 K CPU with\n3.20 GHz base frequency as server hardware equipped\nwith 32 GB of memory. We also used a GeForce RTX\ngraphics card, Win10 operating system, and Python 3.6 as\nthe programming language. A deep learning framework\nbased on Pytorch1.3 was used.\n3.2 Dataset and preprocessing\nThe DCE-MRI imaging data used in this experiment\noriginated from a self-built clinical database consisting of\n160 cases. A Siemens MAGNETO 1.5 T MRA was used\nfor the acquisition of breast cancer images, equipped with a\nspecial four-channel phased array surface coil. All subjects\nwere in the prone position during the acquisition process.\nAfter screening, clinical data from 160 cases of DCE-MRI\nstage T2 were ﬁnally included in the database. Images were\nobtained in the transverse direction, and the image size was\n512 9 512. All breast ROIs were manually marked by the\nclinician with the LabelMe software, whereas 3D slicer\nwas used to label the breast cancer cells. The 160 cases\nwere randomly divided into 128 cases in the training set\nand 32 cases in the test set. We then performed data\nenhancement operations, such as mirroring, scaling, and\nelastic deformation, during the training process. In the\ntraining process, the data were effectively preprocessed.\nInitially, the ﬁrst 0.1% and the last 0.1% of pixel values\nwere deleted for denoising, and then gray scale normal-\nization was performed. For breast ROI region extraction\nand breast cancer lesion segmentation, the same batch of\ntraining and test sets were used. During the training pro-\ncess, both models were trained successively. First, we used\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8323\n123\nthe U-Net model for breast ROI extraction. Second, we\nused the He normalization algorithm to initialize the breast\ncancer segmentation model during the training process.\nAdam Optimizer (Kingma and Ba 2014) performed gra-\ndient descent, with the parameters set to\nb1 ¼ 0:9; b2 ¼ 0:999; e ¼ le/C0 8, and the learning rate was\nle/C0 8. The batch size was set to 3, and Early Stopping\n(Prechelt 1998) was used to monitor the training process.\nFinally, the output result was obtained through the sigmoid\nfunction. Because the range of the output value was (0,1),\nthe ﬁnal output result was assessed by setting the threshold\nto 0.5.\n3.3 Segmentation index\nFive commonly used medical image segmentation indexes\nwere used to evaluate the segmentation results: Dice\ncoefﬁcient (Dice), Intersection over Union (IoU), Sensi-\ntivity (SEN), Speciﬁcity (SPE), and Positive predictive\nvalue (PPV), and the formula is shown in (5). TP is the\nnumber of pixels with the correct foreground segmentation\nin pixel-level segmentation, TN is the number of pixels\nwith the correct background segmentation in pixel-level\nsegmentation, FP is the number of pixels with background\nsegmentation error in pixel-level segmentation, and FN is\nthe foreground segmentation error in pixel-level\nsegmentation.\nDSC ¼\n2TP\n2TP þ FN þ FP\nIOU ¼ TP\nTP þ FP þ FN\nSEN ¼ TP\nTP þ FN\nSPE ¼ TN\nTN þ FP\nPPV ¼ TP\nTP þ FP\nACC ¼ TP þ TN\nTP þ FN þ TN þ FP\nð12Þ\nFig. 7 Breast area coarse segmentation ( a). Original breast image ( b).\nBreast area label ( c). Breast area predicted by U-Net model\nTable 1 Breast area segmentation results\nType Dice IOU SEN SPE PPV ACC\nU-net 91.98 85.34 92.89 98.78 91.24 98.07\nImage\nLabel\nU-Net\n2D-VNet\nResUnet\nDense-Unet\nAttention-Unet\nFig. 8 Partial segmentation of breast tumor lesions using the U-Net,\n2D-VNet, ResUnet, 2D-DenseUnet and Attention-U-Net\n8324 C. Qin et al.\n123\n3.4 Breast ROI segmentation\nThe benchmark U-Net model was used to roughly delineate\nthe breast ROI, and the respective segmentation diagram is\nshown in Fig. 7. The ﬁrst row depicts images from three\npatients with breast cancer. The second row shows how the\nbreast region labels were manually segmented by the\nclinician. Finally, the red area in the third row demonstrates\nthe rough segmentation result of the breast ROI obtained\nby the U-Net model. The breast area can be extended to the\nsides of the chest to ensure that no cancerous area is mis-\nsed. Therefore, even small breast ROIs could be segmented\ncorrectly.\nThe experimental statistics of the segmentation results\nshowed that the Dice and ACC values could reach 0.92 and\n0.98, respectively, as shown in Table 1. These values meet\nthe requirement for the precise segmentation of the breast\ncancer area described in the next step. It should be men-\ntioned that the segmentation of breast cancer lesions\nremained unaffected.\n3.5 Experimental comparative analysis\nUsing the same limited dataset, the proposed model was\nthen compared with current mainstream segmentation\nmodels with excellent performance. Our improved models\nwere TR-U-Net, IUnet, TR-IUnet, and TR-IMUnet. Five\nexcellent mainstream medical image segmentation models\nwere selected for a comparative analysis: U-Net (Ron-\nneberger et al. 2015), 2D-VNet (Milletari et al. 2016),\nResUnet (He et al. 2016), 2D-DenseUnet (Li et al. 2018),\nand Attention-U-Net (Oktay et al . 2018) (Figs. 8, 9, and\n10).\nFigure 8 illustrates the segmentation results of three\npatients with breast cancer lesions using the selected ﬁve\nmainstream segmentation models. The ﬁrst row depicts the\noriginal images obtained from the three patients, the sec-\nond row represents their corresponding labels, and rows\n3–7 correspond to the segmentation results of the U-Net,\n2D-VNet, ResUnet, 2D Dense-Unet, and Attention-U-Net\nmodels, respectively. It is shown that the U-Net segmen-\ntation result in the third row had rough edges, and thus it\nwas impossible to distinguish the normal gland from the\nbreast cancer lesion area. Similarly, the ResUnet segmen-\ntation result shown in the ﬁfth row underlines that the\nlesion could not be distinguished from the surrounding\nnormal glands. Hence, the segmentation result can often be\nImage\nLabel \nDy-ReLUBUnet\nIUnet\nTR-Unet\nTR-IUnet\nTR-IMUnet\nFig. 9 Partial segmentation of breast tumor lesions using the Dy-\nReLUBUnet, IUnet, TR-Unet, TR-IUnet and TR-IMUnet\n(a) Image (b) Label (c) U-Net\n(d) Attention-Unet (e) Dy-ReLUBUnet (f) IUnet\n(g) TR-Unet (h) TR-IUnet (i) TR-IMUnet\nFig. 10 Global illustration of breast tumor segmentation\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8325\n123\nsmaller than the label area. The fourth row demonstrates\nthe 2D-VNet segmentation result. In this case, the lesions\ncould not be segmented due to the obstructive effect of the\nbreast edge and surrounding glands. Furthermore, the sixth\nrow shows the 2D Dense-UNet segmentation result.\nProblems of over-segmentation or under-segmentation\nemerged when segmenting this model because the lesion\nboundary had blurry characteristics. Finally, the seventh\nrow depicts the Attention-Unet segmentation result. From a\nholistic point of view, the model uses the attention mech-\nanism to locate the lesion accurately; however, edges and\ndetails are still lacking.\nIn contrast, the segmentation results of the four\nimproved models proposed in this paper are shown in rows\n4–7 of Fig. 9. The third row depicts the segmentation effect\nof Dy-ReLUBUnet, which incorporated the Dy-ReLU-B\nmodule proposed by Chen et al. ( 2020) into the U-Net. The\nsegmentation effect was slightly improved after fusing the\nDy-ReLU-B module, which can basically segment the\ncontour of the lesion, but still failed to provide ﬁne details.\nThe fourth row shows the segmentation effect of the\nimproved Dy-ReLU-B model, i.e., IUnet. This model could\ndistinguish the lesion edge more ﬁnely, and the segmen-\ntation area was signiﬁcantly closer to the label area. The\nﬁfth row is the segmentation result of the TR-Unet, which\nincorporated the Transformer module into U-Net. This\nmodel could capture the global information of the image\nmarkedly better than the previous models, and could thus\nlocalize the lesions accurately. The sixth row, which rep-\nresents the TR-IUnet model, highlights that this model\noffers the same advantages with the improved Dy-ReLU\nbut it can also capture the global information of the image,\nthereby providing useful information in breast cancer\nsegmentation and reﬁning the segmentation results.\nFinally, the last row reﬂects the TR-IMUnet model, which\nexhibits the highest segmentation accuracy among all\nexperimental networks. This model adds a multi-scale\nfusion mechanism on the basis of TR-IUnet and provides\nsemantic information for segmentation tasks by fusing\ninformation from different scales, thereby improving the\nsegmentation effect.\nFigure 10 uses a single patient case to analyze the\nsegmentation results from a global perspective. Compar-\nison between Fig. 10 (d) and (g) reveals that the Trans-\nformer characteristics are similar to the attention structure,\nand they both contribute to the accurate localization of the\nlesion. The difference is that the attention structure is local\nattention with a small receptive ﬁeld, whereas Transformer\nhas a global receptive ﬁeld. The improved dynamic ReLU\nmodule focuses on the shortcomings of the existing mod-\nule, and it successfully maximizes not only the spatial\ncontext information, but also the dependencies between\nchannels. Therefore, lesions are segmented more precisely.\nMeanwhile, the proposed TR-IMUnet model, which has the\nadvantage of integrating the Transformer module while\nimproving dynamic ReLU, can efﬁciently add multi-scale\nmodules and provide semantic information under different\nscales and representations for model learning, thereby\nimproving model performance and segmentation accuracy.\nCompared with all the experimental networks in this study,\nour proposed model shows the highest segmentation\naccuracy, suggesting that it has better robustness and\ncompatibility.\nFig. 11 Encoder self-attention map of a set of reference points\n8326 C. Qin et al.\n123\n3.6 Module feature analysis\nA. Global receptive ﬁeld of the Transformer module\nIn the second section, it was mentioned that the greatest\nadvantage of the Transformer module is that all pixels have\na global receptive ﬁeld, which allows the model to capture\nglobal information and model the relationship between\npixels in an explicit manner. Figure 11 highlights this\nfeature. We initially selected a random point and displayed\nit as an image. Each pixel could roughly capture the\noriginal image information, which is represented as the\noutline of the breast region in the breast cancer data set. In\naddition, the color distribution demonstrates that different\npixels provide different image information. For example,\nlesion pixels have a strong connection with the lesion and a\nweak connection with the background and other glands,\nwhereas background pixels have a strong connection with\nthe background and a weak connection with other pixels.\nMoreover, other gland pixels have a strong connection with\nthe lesion and other glands, which is consistent with our\ncognition. (Note that the yellow color reﬂects a strong\ncorrelation as opposed to the blue color which reﬂects a\nweak correlation.) In other words, the Transformer module\ncan make all pixels have global receptive ﬁelds, and can\nalso explicitly model the interdependence between each\npixel. This is very practical and helpful for lesion identi-\nﬁcation and ﬁne segmentation.\nB. Multi-scale function of the MSPCF module\nThe primary goal of medical image segmentation is to\nobtain a binary image that contains the lesion location only\n(lesion location is 1, and the rest is 0). Therefore, our\nneural network model should have the ability to identify\nand highlight lesion location and weak non-lesion location.\nThe output features of each layer of the network in Fig. 12\nshow that the continuous feature extraction of the CNN on\nthe input image through continuous convolution operation\nis indeed a process of constantly highlighting the lesion and\nweakening the non-lesion parts. This meets our needs.\nHowever, a single convolution operation cannot clearly\ndistinguish between lesion and non-lesion areas, as shown\nin Fig. 12(c). Some noise on the edge may be misdiag-\nnosed as lesion areas. The MSPCF module can correct or\nconﬁrm the identiﬁcation of the lesion and non-lesion areas\nin the image by fusing feature information of different\nscales to reduce the probability of misjudgment, as shown\nin Fig. 12g. Comparison of Fig. 12d and i, as well as\nFig. 12e and j, reveals that the MSPCF module can further\nhighlight the lesion location.\nC. Dynamic performance of the improved dynamic\nReLU module\nIn the second section, it was stated that the greatest\nadvantage of dynamic ReLU is its ability to construct a\nspeciﬁc activation function in accordance with different\ninput data, and to dynamically retain and adjust these input\ndata to reduce data loss or redundancy. As shown in\nFig. 13, the ReLU activation functions present different\ndistributions for different stages, and the activation func-\ntion of each channel in each stage is also different. In\naddition, the slope of each ReLU activation function is also\ndifferent. For example, the slope of the positive axis of\nReLU in blocks 5 and 6 is approximately one, whereas the\nslope of the positive axis of ReLU in blocks 2 to 4 is\napproximately two, indicating that dynamic ReLU can\n(a) Image (b) Encode Conv Layer1 (c) Encode Conv Layer2 (d) Encode Conv Layer3 (e) Encode Conv Layer4\n(f) Label (g) Encode MS Layer1 (h) Encode MS Layer2 (i) Encode MS Layer3 (j) Encode MS Layer4\nFig. 12 Visualization of the Encoder part\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8327\n123\nﬁlter the input data, and also enlarge or reduce these data\naccording to their importance.\nConsequently, this renders the model more ﬂexible and\nwith enhanced anti-interference ability.\nD. Model visualization\nFinally, the entire process of breast cancer segmentation\nwas reviewed by visualizing each stage of the proposed\nmodel. When a breast cancer image is input into the TR-\nIMUnet model, the encoder of the TR-IMUnet identiﬁes\nand highlights the lesion, and subsequently weakens the\nnon-lesion area via continuous convolution layers\n(Fig. 14). At the same time, the multi-scale layer can fur-\nther correct or conﬁrm the location of the lesion by fusing\nfeature information of different scales. The Transformer\nlayer can compensate for the narrow receptive ﬁeld of the\nconvolution layer by exploiting its global receptive ﬁeld,\nmaximizing the global and local feature information, and\ntransferring the acquired features to the decoder layer.\nConsequently, the decoder layer receives high-level and\nlow-level feature information from the encoder layer. On\nthe basis of the original image space information provided\nby the low-level feature information, the abstract feature\ninformation is restored to the corresponding position of the\noriginal image and is separated from the surrounding\nglands. Thus, we can obtain high-brightness and grayscale\nimages that represent lesions and non-lesions, respectively.\nFinally, after the thresholding operation and the activation\nof the sigmoid function, a segmented image containing\nonly the lesion is obtained.\n3.7 Stereoscopic segmentation results\nFor the effective diagnosis of breast cancer, the position or\narea on the 2D image, but also the shape, volume, and size\nof the entire cancer must be considered. This information is\nvital for the preoperative preparation and postoperative\nrehabilitation of breast cancer resection. Therefore, the 2D\nsegmentation image of the model was transformed into a\n3D structure to illustrate the segmentation results. Fig-\nure 15 shows that the present proposed model can efﬁ-\nciently segment the 3D outline of the entire breast lesion,\nand its outline size is basically consistent with the outline\nsize marked by professional radiologists. The proposed\nmodel can also correctly segment breast lesions with dif-\nferent shapes. For instance, the ﬁrst three rows (Fig. 15)\ndepict hollow, solid, and scattered blocks, respectively.\nFurthermore, the proposed TR-IMUnet model can restore\nthe 3D structure of the breast lesions and achieve the\nhighest segmentation accuracy by maximizing the advan-\ntages of each component module.\n3.8 Statistical analysis\nTo verify the segmentation performance of the proposed\nmodel for breast lesions, the evaluation indexes Dice, IoU,\nSensitivity, Speciﬁcity, and PPV were, respectively,\napplied to the dataset, and the results were analyzed sta-\ntistically. It should be mentioned that all models in the\ntable are based on the two-stage segmentation framework.\nFirst, the U-Net model was used to extract the breast ROI.\nThen different models were employed to segment breast\ncancer.\nThe segmentation accuracies of the ResUnet, DenseU-\nnet, Att-U-Net, DyReluB-U-net, TR-U-Net, IUnet, TR-\nIUnet, and TR-IMUnet models were each better than the\nbenchmark U-Net model (Table 2). The segmentation\naccuracy of DyReluBUnet was slightly better than that of\nthe benchmark U-Net, as evidenced by the increase in Dice\nby 2.11%. The IUnet model with an improved dynamic\nReLU module improved the segmentation accuracy by 1.08\nBlock1 Block2 Block3 Block4 Block5\nBlock6 Block7 Block8 Block9 Block10\nFig. 13 ReLU function distribution for different stages\n8328 C. Qin et al.\n123\nwhile the performance of the TR-U-net with the Trans-\nformer module was similar. Finally, TR-IMUnet, which\ncombines the advantages of both Transformer and\nImproved Dynamic ReLU and adds the MSPCF module,\nachieved the highest segmentation accuracy.\nCompared with the benchmark U-Net, the Dice coefﬁ-\ncient of the TR-IMUnet model is increased by 4.27%, IoU\nincreases by 5.21%, sensitivity increases by 3.37%, and\nPPV increases by 3.68%. A segmentation comparison\ndiagram of U-Net and TR-IMUnetis shown in Fig. 16.A s\nshown on the left side of Fig. 16, when the U-Net model is\nused to segment 2D tumor images, there are signiﬁcant\nproblems, such as improper detection of the shape, size,\nand contour of breast tumors, resulting in missing, incor-\nrect or even large-area loss of segmentation results.\nMoreover, the second column of the third row in Fig. 16a\ndemonstrates that U-Net performs poorly when accurate\nsegmentation is used in hard samples because it cannot\ndistinguish between lesion and the non-lesion positions. In\ncontrast, the proposed TR-IMUnet model can accurately\nlocate the position of breast tumors, detect the size and\nshape of tumor blocks correctly, and distinguish them from\n(a) Encode Conv Layer1 (b) Encode MS Layer1 (c) Encode Conv Layer2 (d) Encode MS Layer2\n(e) Encode Conv Layer3 (f) Encode MS Layer3 (g) Encode Conv Layer4 (h) Encode MS Layer4\n(i) Encode Conv Layer5 (j) Transformer Layer (k) Dncode Layer1 (l) Dncode Layer2\n(m) Dncode Layer3 (n) Dncode Layer4 (o) Output layer without \nactivation\n(p) Sigmoid activated \noutput\nFig. 14 Visualization of the whole network process\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8329\n123\nnon-lesion areas far more efﬁciently. At the same time,\nwhen detecting difﬁcult images, the proposed model can\nalso maintain high segmentation accuracy and segment the\ntumor meticulously. Moreover, by observing the segmen-\ntation results of U-Net and TR-IMUnet models from the\n3D perspective of Fig. 16b, it can be found that the U-Net\nsegmentation model performance is ordinary, and the\nperformance of the proposed TR-IMUnet model is better\nthan that of U-Net in ﬁne segmentation of the size, shape\nand details of breast tumor.\nFigure 17 provides a statistical comparison of various\nalgorithm BOX charts. The horizontal axis represents dif-\nferent models used in previous experimental comparison,\nwhereas the vertical axis represents the Dice coefﬁcients.\nAfter the improvement process, the index values and sta-\nbility of the proposed IUnet, TR-U-net, TR-IUnet, and TR-\nIMUnet models gradually increased, verifying the effec-\ntiveness of the proposed modules and the validity of the\nproposed model improvement.\n3.9 Analysis of model parameters\nThe parameters used in the proposed model were approx-\nimately 13.9 MB (Table 3). The proposed model replaces\nReLU in the benchmark U-Net encoder-decoder structure\nbased on the proposed improved dynamic ReLU module,\ni.e., Conv ? GroupNorm ? Improved Dy-ReLU ?\nConv ? GroupNorm ? Improved Dy-ReLU.\nThe MSPCF module is added to the coding path of TR-\nIMUnet, which contains four encode layers and four\nmaximum pooling operations, and the area of the feature\ngraph is halved after each operation. Except for MSPCF,\nall convolution layers have 3 9 3 convolution kernels, and\nthe number of channels is shown in the above table. The\nﬁrst encoding layer outputs 32 channels. With the deep-\nening of the network, the number of channels is doubled\nafter each coding layer and the maximum pooling\nCase1\nCase2\nCase3\nLabel TR-UNet IUNet TR-IUNet TR-IMUNet\nFig. 15 Stereo structure of our segmentation results\nTable 2 Comparison of segmentation results\nModel Type Dice IoU SEN SPE PPV\nU-Net 73.14 60.18 80.20 99.88 73.10\n2D-Vnet 74.16 61.59 83.90 99.85 72.33\nResUnet 75.55 63.22 82.19 99.87 75.26\nDenseUnet 76.67 65.29 80.31 99.90 77.81\nAtt-U-net 75.94 63.85 80.12 99.90 77.86\nDyReluUNet 75.25 63.01 80.04 99.89 76.47\nTR-U-Net 76.33 63.86 82.27 99.90 76.73\nIUNet 76.33 64.61 81.52 99.89 76.47\nTR-IUnet 76.65 64.51 80.22 99.91 78.74\nTR-IMUNet 77.41 65.39 83.57 99.89 76.78\n8330 C. Qin et al.\n123\noperation. The number of channels remains at 256 up until\nthe last coding layer, and then the channels are compressed\nto 128, and the feature information will be transmitted to\nthe Transformer.\nIn the decoder part, after upsampling the feature map,\nthe decoder still adopts the same operation as the encoder\ninfrastructure, i.e., Conv ? GroupNorm ? Improved Dy-\nReLU ? Conv ? GroupNorm ? Improved Dy-ReLU.\nAfter each operation, the area of the feature map is twice as\nlarge as the original, and the channel parameters decrease\nby half. The output of the upsampling operation is con-\nnected to the output of the corresponding part of the\nencoder as the input of the lower layer. The resulting\nfeature map is processed by convolution to maintain the\nLabel U-net TR-IMUnet Label U-net  TR-IMUnet\n(a) 2D segmentation comparison (b) 3D reconstruction comparison\nFig. 16 2D and 3D\nsegmentation comparison\nbetween the TR-IMUnet and the\nU-Net models\nFig. 17 BOX diagrams of all comparative experiments\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8331\n123\nsame number of channels as that of the symmetric encoder\npart. Finally, the ﬁnal result is output by one-time 1 9 1\nconvolution.\n3.9.1 Training and validation\nThe training loss of the present proposed model converged\nrapidly between none and 35 times (Fig. 18). In the sub-\nsequent iterations, the loss function curve gradually con-\nverged and tended to ﬂatten, and the convergence speed of\nthe veriﬁcation loss curve was faster between none and 25\niterations. The curve convergence tended to be ﬂat in\nsubsequent iterations. The early stop method was triggered\nto stop the training process when the loss remained con-\nstant for a certain number of rounds, to save time and\nresources. In summary, in training the proposed model was\nrobust.\n4 Conclusions\nThis study introduced a two-stage breast cancer image\nsegmentation model. The U-Net model was used ﬁrst to\nobtain a rough outline of the breast region. Then, a TR-\nIMUnet model was designed, in which the ReLU function\nTable 3 Main module parameters of UTB net\nLayer Type Parameters Size/Stride Output\nEncode1 Conv 1 ? 32 (Filters) 3X3/1 512X512\nGN 32 (Channels) / 32 (Groups)\nImproved Dy-ReLU 32 / /\nConv 32(Filters) 3X3/1 512X512\nGN 32 (Channels) / 32 (Groups)\nImproved Dy-ReLU 32 / /\nMulti-Scale Parallel Conv. Fusion 1 Parallel structure\nConv(1,1) Conv(3,3) Conv(5,5) AvgPool (256)\nGN (32,32) GN (32,32) Upsample (2.0, bilinear)\nPReLU(32) PReLU (32) GN (32,32)\nPReLU (32)\nConv 128 ? 32 (Filters) 1X1/1 512X512\nGN 32 (Channels) / 32 (Groups)\nImproved Dy-ReLU 32 / /\nDown-Sampling1 MaxPool / 2X2/2 256X256\nTransformer 128 ? 128 (Filters) 32X32\nUpSampling3 Up-Sampling / 2X2/2 256X256\nDecode Conv 64 ? 32 (Filters) 3X3/1 512X512\nGN 32 (Channels) / 32 (Groups)\nImproved Dy-ReLU 32 / /\nConv 32 ? 32 (Filters) 3X3/1 512X512\nGN 32 (Channels) / 32 (Groups)\nImproved Dy-ReLU 32 / /\nOut Conv 32 ? 1 (Filters) 1X1/1 512X512\nNumber of trainable parameters 13.9 MB in Model\nFig. 18 TR-IMUnet training and valid loss\n8332 C. Qin et al.\n123\nof the encoder-decoder structural unit was replaced by an\nimproved dynamic ReLU function, and the MSPCF and\nTransformer modules were added at the encoder path and\nend, respectively. Compared with the U-Net benchmark\nmodel, the Dice, IoU, SEN, and PPV breast cancer seg-\nmentation indexes were improved by 4.27, 5.21, 3.37, and\n3.68%, respectively. The segmentation of small mesh\nlesions warrants further improvement. To improve the\ndiagnosis, treatment, and prognosis of breast cancer, in the\nfuture we intend to study the 3D segmentation and location\nof breast cancer images and establish a prediction model in\ncombination with Radiomics.\nFunding This study is supported by the National Nature Science\nFoundation of China (No.62071213), Special Project in key Areas of\nArtiﬁcial Intelligence in Guangdong Universities (No.2019KZDZ\nX1017), Guangdong Basic and Applied Basic Research Foundation\n(No. 2019A1515010716), and open fund of Guangdong Key\nLaboratory of digital signal and image processing technology\n(2019GDDSIPL-03, 2020GDDSIPL-03).\nData availability The datasets and the code generated and/or analysed\nduring the current study are available from the corresponding author\non reasonable request.\nDeclarations\nConflict of interest The authors declare that they have no conflict of\ninterest.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\nCarion N, Massa F, Synnaeve G, et al (2020) End-to-end object\ndetection with transformers. In: European conference on com-\nputer vision. Springer, Cham, pp 213–229\nChen Y, Dai X, Liu M, et al (2020) Dynamic relu. In: European\nconference on computer vision. Springer, Cham, pp 351–367\nDosovitskiy A, Beyer L, Kolesnikov A, et al (2020) An image is\nworth 16x16 words: transformers for image recognition at scale.\narXiv preprint arXiv:2010.11929\nHan K, Xiao A, Wu E, et al (2021) Transformer in transformer. Adv\nNeural Inf Process Syst 34\nHe K, Zhang X, Ren S, et al (2015) Delving deep into rectiﬁers:\nSurpassing human-level performance on imageNet classiﬁcation.\nIn: Proceedings of the IEEE international conference on\ncomputer vision, pp 1026–1034\nHe K, Zhang X, Ren S, et al (2016) Deep residual learning for image\nrecognition. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp 770–778\nHu J, Shen L, Sun G (2014) Squeeze-and-excitation networks. In:\nProceedings of the IEEE conference on computer vision and\npattern recognition, pp 7132–7141\nHuang H, Lin L, Tong R, et al (2020) Unet 3 ?: A full-scale\nconnected unet for medical image segmentation. In: ICASSP\n2020–2020 IEEE international conference on acoustics, speech\nand signal processing (ICASSP). IEEE, pp 1055–1059\nKingma DP, Ba J (2014) Adam: a method for stochastic optimization.\narXiv preprint arXiv:1412.6980\nLi X, Chen H, Qi X et al (2018) H-DenseUNet: hybrid densely\nconnected UNet for liver and tumor segmentation from CT\nvolumes. IEEE Trans Med Imaging 37(12):2663–2674\nLi Z, Liu X, Creighton FX, et al (2020) Revisiting stereo depth\nestimation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint arXiv:2011.02910\nLitjens G, Kooi T, Bejnordi BE et al (2017) A survey on deep\nlearning in medical image analysis. Med Image Anal 42:60–88\nLiu R, Yuan Z, Liu T, et al (2021) End-to-end Lane shape prediction\nwith transformers. In: Proceedings of the IEEE/CVF winter\nconference on applications of computer vision, pp 3694–3702.\nLiu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B (2021)\nSwin transformer: Hierarchical vision transformer using shifted\nwindows. arXiv preprint arXiv:2103.14030\nMaas AL, Hannun AY, Ng Y (2013) Rectiﬁer nonlinearities improve\nneural network acoustic models. Proc. Icml. 30(1): 3.\nMilletari F, Navab N, Ahmadi SA (2016) V-net: Fully convolutional\nneural networks for volumetric medical image segmentation[C]//\n2016 fourth international conference on 3D vision (3DV). IEEE,\n2016: 565–571.\nNair V, Hinton GE (2010a) Rectiﬁed linear units improve restricted\nboltzmann machines\nNair V, Hinton GE (2010b) Rectiﬁed Linear Units Improve Restricted\nBoltzmann Machines. International Conference on Machine\nLearning\nOktay O, Schlemper J, Folgoc L L, et al (2018) Attention u-net:\nLearning where to look for the pancreas. arXiv preprint arXiv:\n1804.03999\nPham DL, Xu C, Prince JL (2000) Current methods in medical image\nsegmentation[J]. Annu Rev Biomed Eng 2(1):315–337\nPiantadosi G, Sansone M, Fusco R et al (2020) Multi-planar 3D breast\nsegmentation in MRI via deep convolutional neural networks.\nArtif Intell Med 103:101781\nPrangemeier T, Reich C, Koeppl H (2020) attention-based trans-\nformers for instance segmentation of cells in microstructures. In:\n2020 IEEE international conference on bioinformatics and\nbiomedicine (BIBM). IEEE, pp 700–707\nPrechelt L (1998) Early stopping-but when? [M]//Neural Networks:\nTricks of the trade. Springer, Berlin, Heidelberg, pp 55–69\nRonneberger O, Fischer P, Brox T (2015) U-net: Convolutional\nnetworks for biomedical image segmentation. In: International\nconference on medical image computing and computer-assisted\nintervention. Springer, Cham, pp 234–241\nSalehi SSM, Erdogmus D, Gholipour A (2017) Tversky loss function\nfor image segmentation using 3D fully convolutional deep\nnetworks[C]//International workshop on machine learning in\nmedical imaging. Springer, Cham, pp 379–387\nUddin MN, Li B, Ali Z et al (2022) Software defect prediction\nemploying BiLSTM and BERT-based semantic feature. Soft\nComput. https://doi.org/10.1007/s00500-022-06830-5\nVaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you\nneed. arXiv preprint arXiv:1706.03762\nJoint Transformer and Multi-scale CNN for DCE-MRI Breast Cancer Segmentation 8333\n123\nWei D, Weinstein S, Hsieh MK et al (2018) Three-dimensional whole\nbreast segmentation in sagittal and axial breast MRI with dense\ndepth ﬁeld modeling and localized self-adaptation for chest-wall\nline detection. IEEE Trans Biomed Eng 66(6):1567–1579\nWoo S, Park J, Lee J Y, et al (2018) Cbam: convolutional block\nattention module[C]//Proceedings of the European conference on\ncomputer vision (ECCV), pp 3–19\nXiao J, Rahbar H, Hippe DS et al (2021) Dynamic contrast-enhanced\nbreast MRI features correlate with invasive breast cancer\nangiogenesis. NPJ Breast Cancer 7:42\nZhang J, Saha A, Zhu Z et al (2018) Hierarchical convolutional neural\nnetworks for segmentation of breast tumors in MRI with\napplication to radiogenomics. IEEE Trans Med Imaging\n38(2):435–447\nZhang K, Shi Y, Hu C et al (2021) Nucleus image segmentation\nmethod based on GAN and FCN model. Soft Comput. https://\ndoi.org/10.1007/s00500-021-06449-y\nZheng S, Lu J, Zhao H, et al (2020) Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with transform-\ners. arXiv preprint arXiv:2012.15840\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n8334 C. Qin et al.\n123",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.8241523504257202
    },
    {
      "name": "Breast cancer",
      "score": 0.6431077718734741
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6060818433761597
    },
    {
      "name": "Computer science",
      "score": 0.6027883887290955
    },
    {
      "name": "Sørensen–Dice coefficient",
      "score": 0.5121297836303711
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5088391900062561
    },
    {
      "name": "Image segmentation",
      "score": 0.49478429555892944
    },
    {
      "name": "Magnetic resonance imaging",
      "score": 0.4489086866378784
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.41175520420074463
    },
    {
      "name": "Computer vision",
      "score": 0.3537357747554779
    },
    {
      "name": "Medicine",
      "score": 0.21686193346977234
    },
    {
      "name": "Radiology",
      "score": 0.17282050848007202
    },
    {
      "name": "Cancer",
      "score": 0.16366147994995117
    },
    {
      "name": "Internal medicine",
      "score": 0.09848904609680176
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210151615",
      "name": "Wuyi University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I91935597",
      "name": "University of South China",
      "country": "CN"
    }
  ]
}