{
  "title": "UAV Trajectory Planning for AoI-Minimal Data Collection in UAV-Aided IoT Networks by Transformer",
  "url": "https://openalex.org/W4295700726",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2116599736",
      "name": "Botao Zhu",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": "https://openalex.org/A2439267244",
      "name": "Ebrahim Bedeer",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": "https://openalex.org/A2114943966",
      "name": "Ha H. Nguyen",
      "affiliations": [
        "University of Saskatchewan"
      ]
    },
    {
      "id": "https://openalex.org/A1954965488",
      "name": "Robert Barton",
      "affiliations": [
        "Cisco Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2135624771",
      "name": "Zhen Gao",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3170139470",
    "https://openalex.org/W2962691117",
    "https://openalex.org/W3116767353",
    "https://openalex.org/W3088314401",
    "https://openalex.org/W3198132193",
    "https://openalex.org/W3038418740",
    "https://openalex.org/W1993918491",
    "https://openalex.org/W3046016465",
    "https://openalex.org/W2963085502",
    "https://openalex.org/W3111412729",
    "https://openalex.org/W2961899294",
    "https://openalex.org/W2796882881",
    "https://openalex.org/W3111740206",
    "https://openalex.org/W3048386157",
    "https://openalex.org/W2898733141",
    "https://openalex.org/W3161326013",
    "https://openalex.org/W3046106787",
    "https://openalex.org/W2097171912",
    "https://openalex.org/W2742734338",
    "https://openalex.org/W3007091509",
    "https://openalex.org/W2962141687",
    "https://openalex.org/W3134737640",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2805798351",
    "https://openalex.org/W6758687306",
    "https://openalex.org/W3146106549",
    "https://openalex.org/W6791743930",
    "https://openalex.org/W2744248483",
    "https://openalex.org/W2031834036",
    "https://openalex.org/W2944478701",
    "https://openalex.org/W2964023906",
    "https://openalex.org/W2962262066",
    "https://openalex.org/W3121675905",
    "https://openalex.org/W2117257995",
    "https://openalex.org/W6796707148",
    "https://openalex.org/W2016820997",
    "https://openalex.org/W2049354208",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2079012818",
    "https://openalex.org/W2302174052",
    "https://openalex.org/W3135302029",
    "https://openalex.org/W3101996971"
  ],
  "abstract": "Maintaining freshness of data collection in Internet-of-Things (IoT) networks\\nhas attracted increasing attention. By taking into account age-of-information\\n(AoI), we investigate the trajectory planning problem of an unmanned aerial\\nvehicle (UAV) that is used to aid a cluster-based IoT network. An optimization\\nproblem is formulated to minimize the total AoI of the collected data by the\\nUAV from the ground IoT network. Since the total AoI of the IoT network depends\\non the flight time of the UAV and the data collection time at hovering points,\\nwe jointly optimize the selection of hovering points and the visiting order to\\nthese points. We exploit the state-of-the-art transformer and the weighted A*,\\nwhich is a path search algorithm, to design a machine learning algorithm to\\nsolve the formulated problem. The whole UAV-IoT system is fed into the encoder\\nnetwork of the proposed algorithm, and the algorithm's decoder network outputs\\nthe visiting order to ground clusters. Then, the weighted A* is used to find\\nthe hovering point for each cluster in the ground IoT network. Simulation\\nresults show that the trained model by the proposed algorithm has a good\\ngeneralization ability to generate solutions for IoT networks with different\\nnumbers of ground clusters, without the need to retrain the model. Furthermore,\\nresults show that our proposed algorithm can find better UAV trajectories with\\nthe minimum total AoI when compared to other algorithms.\\n",
  "full_text": "arXiv:2401.02425v1  [cs.NI]  8 Nov 2023\n1\nUA V Trajectory Planning for AoI-Minimal Data\nCollection in UA V -Aided IoT Networks by\nTransformer\nBotao Zhu, Ebrahim Bedeer, Member , IEEE, Ha H. Nguyen, Senior Member , IEEE ,\nRobert Barton, Member , IEEE , and Zhen Gao, Member , IEEE\nAbstract—Maintaining freshness of data collection in Internet-\nof-Things (IoT) networks has attracted increasing attenti on. By\ntaking into account age-of-information (AoI), we investig ate the\ntrajectory planning problem of an unmanned aerial vehicle ( UA V)\nthat is used to aid a cluster-based IoT network. An optimizat ion\nproblem is formulated to minimize the total AoI of the collec ted\ndata by the UA V from the ground IoT network. Since the total\nAoI of the IoT network depends on the ﬂight time of the UA V and\nthe data collection time at hovering points, we jointly opti mize the\nselection of hovering points and the visiting order to these points.\nW e exploit the state-of-the-art transformer and the weight ed A*,\nwhich is a path search algorithm, to design a machine learnin g\nalgorithm to solve the formulated problem. The whole UA V -Io T\nsystem is fed into the encoder network of the proposed algori thm,\nand the algorithm’s decoder network outputs the visiting or der\nto ground clusters. Then, the weighted A* is used to ﬁnd the\nhovering point for each cluster in the ground IoT network.\nSimulation results show that the trained model by the propos ed\nalgorithm has a good generalization ability to generate sol utions\nfor IoT networks with different numbers of ground clusters,\nwithout the need to retrain the model. Furthermore, results show\nthat our proposed algorithm can ﬁnd better UA V trajectories with\nthe minimum total AoI when compared to other algorithms.\nIndex T erms—AoI, IoT , transformer , trajectory optimization,\nUA V\nI. I N T RO D U CT IO N\nT\nHe use of unmanned aerial vehicles (UA Vs) has attracted\na lot of attention from academia and industry. Given their\nhigh maneuvering capability and mobility, UA Vs can be used\nas wireless relays or mobile base stations to provide reliab le\ncommunications and better coverage for ground devices [1].\nThanks to these advantages, UA Vs can be ﬂexibly deployed\nto provide fast and reliable network access in different app li-\ncations, such as disasters [2], surveillance [3], monitori ng [4],\nto name a few .\nSince UA Vs can ﬂy close to the ground devices and provide\nlow-altitude air-to-ground communication links with them ,\nUA Vs can be deployed to hover the area of interest to collect\ndata from ground Internet-of-Things (IoT) networks. By doi ng\nso, UA V -aided data collection can save the energy of devices\nB. Zhu, E. Bedeer, and H. H. Nguyen are with the Department of E lectrical\nand Computer Engineering, University of Saskatchewan, Sas katoon, Canada\nS7N5A9. Emails: {botao.zhu, e.bedeer, ha.nguyen }@usask.ca.\nR. Barton is with Cisco Systems Inc. Email: robbarto@cisco. com.\nZ. Gao is with Beijing Institute of T echnology , Beijing, Chi na. Email:\ngaozhen16@bit.edu.cn.\nThis work was supported by NSERC/Cisco Industrial Research Chair in\nLow-Power Wireless Access for Sensor Networks.\nin traditional IoT networks, thus extending their lifetime [5].\nHowever, maintaining the freshness of the collected inform a-\ntion is an important issue in time-sensitive IoT applicatio ns,\nsuch as environmental monitoring and safety protection. In\nthese applications, the generated data needs to be sent to th e\ndestination as soon as possible. Outdated information can l ead\nto incorrect control and even cause major disasters [6]. The re-\nfore, it is essential to ensure the freshness of the data rece ived\nat the destination. T o measure the freshness of information ,\nthe age of information (AoI) as a new performance metric was\nproposed in [7]. In a nutshell, AoI describes the amount of ti me\nelapsed since the generation of the most recent data update.\nAoI-based data collection can guarantee information fresh ness\nin IoT networks, which is quite different from traditional\ndelay-based and throughput-based metrics [8]. As such, it h as\nattracted increasing attention.\nDue to the importance of AoI, a number of studies have been\ncarried out on AoI-oriented data collection in UA V -assiste d\nwireless networks. In [9], the authors aimed to minimize the\naverage AoI of the system by optimizing the trajectory of\nthe UA V in a UA V -aided data collection system. In [10], the\nauthors optimized the trajectory of the UA V to minimize the\nmaximal AoI and the average AoI of sensors. In [11], the\nauthors assumed the UA V supports three modes to collect data\nand jointly optimize the trajectory and data collection mod es of\nthe UA V to minimize the average AoI of all ground nodes. In\n[12], the UA V trajectory, energy, and service time allocati on\nwere jointly optimized by an iterative algorithm in order to\nminimize the overall peak AoI of the system. The authors\nin [13] developed an energy-efﬁcient navigation policy for the\nUA V to improve data freshness of the IoT network. In order to\nminimize the weighted sum of AoI, the authors in [14] jointly\noptimized the ﬂight trajectory of the UA V and the transmissi on\nscheduling of sensors. From the above discussion, it can be\nseen that AoI-oriented data collection problems in the UA V -\nassisted IoT network are typically related to UA V’s traject ory\ndesign.\nWhen collecting data in the UA V -assisted IoT network, if\nthe UA V is dispatched to visit every ground IoT device, the\nenergy consumption of the UA V will increase because of\nthe increased UA V trajectory. Hence, to reduce the energy\nconsumption of the UA V , clusters-based model have been\nextensively investigated in UA V -assisted wireless networ ks.\nFor instance, in [15], to gather compressive data measure-\nments, the authors divide the sensor network into multiple\n2\nclusters. In each cluster, all nodes build a forwarding tree\nbased on compressive data gathering to send data to the\ncluster head (CH). The UA V then traverses all CHs to collect\nthe aggregated data. The authors jointly optimized the UA V\ntrajectory, CH selection, and forward tree construction to\nminimize the total transmit power in the network. In [16],\nthe authors consider a pre-clustered network where a UA V\nequipped with multiple antennas communicates with multipl e\nground users simultaneously, in a given time slot, using spa ce\ndivision multiple access. The authors jointly optimized th e\ntime slot allocation and the UA V hovering time to minimize\nthe overall energy consumption. In [17], the authors consid ered\na UA V -enabled data collection system for massive machine-\ntype communications (mMTC) where machine-type commu-\nnication devices (MTCDs) are divided into several clusters .\nA UA V visits each hovering position which corresponds to a\nMTCD cluster and sequentially collects data from each MTCD\nin the corresponding cluster. They formulated a problem of\nminimizing the total energy consumption of the system. In\nour previous work [5], we considered using a UA V to collect\ndata from a clustered IoT network, where the hovering points\nof the UA V are determined by the unknown CHs location.\nIn other words, in [5], we jointly select the CHs and their\nvisiting order to minimize the total energy consumption. In\nthis paper, we examine the scenario where the UA V collects\ndata from a group of clusters and the UA V only interact\nwith the CHs. The problem of interest in this paper is to\njointly optimize the UA V’s hovering points and trajectory t o\nachieve the minimal AoI data collection in a cluster-based I oT\nnetwork. The optimization problem is formulated as a travel ing\nsalesman problem (TSP) with neighborhoods (TSPN), which\nis extremely challenging because it includes a continuous\nproblem (optimization of hovering points) and a combinator ial\nproblem (optimization of visiting order).\nThe hovering points of the UA V and the visiting order to\nthese hovering points have a great impact on the ﬂying time\nof the UA V and data collection time, which directly inﬂuence\nthe total AoI of collected data. There have been some works\non solving the TSPN efﬁciently. In [18], the Dubins TSPN\nwas converted to a generalized TSP (GTSP) by using the\nsampling-based roadmap method, and then to an asymmetric\nTSP that can be addressed by the Lin-Kernighan heuristic\nalgorithm. T o handle the continuous optimization problem\nof waypoints within each circular neighborhood, the author s\nin [19] proposed a discretization scheme that equidistantl y\nsamples possible locations along the circular border of the\ninterest neighborhood to determine the locations of the way -\npoints. In this paper, in order to reduce the computational\ncomplexity for solving the joint optimization of the UA V’s\nhovering points and trajectory to achieve the minimal AoI\ndata collection in a cluster-based IoT network, we transfor m\nthe formulated continuous optimization TSPN into a GTSP by\nborrowing the sampling-based idea. The transformed GTSP is\na combinatorial optimization problem that can be solved usi ng\ntraditional methods, such as exact algorithms, approximat e\nalgorithms, or heuristic algorithms. However, these tradi tional\nalgorithms may not achieve a good balance between optimalit y\nand computational complexity. Thus, by considering optima l-\nity, computational complexity, and generality, we shall de velop\na machine learning-based algorithm to solve the transforme d\nGTSP , i.e., the UA V’s trajectory design problem.\nMachine learning has been explored as a promising tech-\nnique for solving UA V’s trajectory planning problems in UA V -\nassisted IoT networks. T o minimize the weighted sum-AoI\nin a UA V -assisted network, the authors in [20] applied deep\nreinforcement learning (DRL) to optimize the UA V’s traject ory\nusing a deep Q network (DQN) and an artiﬁcial neural network\n(ANN). In [21], the authors utilized Q-learning to optimize\nAoI-optimal UA V path by considering the deadline constrain ts\nof data in the UA V -aided sensing network. In [22], the author s\njointly optimized the UA V’s trajectory and scheduling of th e\nstatus update packets to minimize the normalized weighted\nsum of AoI in a UA V -assisted wireless network. Speciﬁcally,\nthey used ANN, DQN, and long short-term memory (LSTM)\nto develop a DRL algorithm for learning the UA V trajectory in\nlarge-scale networks. Different from these works, we emplo y\nthe state-of-the-art transformer and the weighted A* searc h\nmethod to design a UA V trajectory planning algorithm for\nAoI-oriented data collection.\nTransformer was originally proposed by Google as a\nsequence-to-sequence model to deal with machine translati on\nproblem [23]. It has achieved great success in many areas\nof artiﬁcial intelligence in the past four years, such as com -\nputer vision, audio processing, document summarization, a nd\ndocument generation. Some researchers also attempt to use\ntransformer and its variants to tackle combinatorial probl ems,\nsuch as the TSP . In [24], the cities in the TSP were encoded\nby a transformer and decoded sequentially through a query\nconsisting of the last three cities in the partial tour. The\nused transformer was trained by reinforcement learning. In\n[25], the authors also used the transformer architecture as the\nencoder network and the decoder network outputs the result\nsequentially based on the embeddings from the encoder and\nthe outputs generated at previous steps. The encoder and de-\ncoder networks were trained using a reinforce algorithm wit h a\ndeterministic greedy baseline. The authors in [26] propose d a\ntransformer-based framework to automatically learn impro ved\nheuristics on two representative routing problems: the TSP\nand capacitated vehicle routing problem (CVRP). In [27], th e\nauthors used the standard transformer architecture to tack le\nTSP and achieve an improved performance over recent learned\nheuristics. Inspired by the success of employing transform er\nin solving various problems of route planning, we propose\nthe transformer-weighted-A* (TW A*) algorithm in this pape r\nfor solving our formulated GTSP combinatorial optimizatio n\nproblem. Although the Ptr-A* algorithm proposed in our\nprevious work [5] achieves good performance in solving the\nGTSP , the TW A* algorithm has the following two important\nadvantages over the Ptr-A* algorithm. First, TW A* does not\nrelay on past hidden states like Ptr-A*, and thus, avoids los ing\npast information. Second, TW A* has the ability of parallel\ncomputation which makes it faster than Ptr-A*.\nThe main contributions of this paper are summarized as\nfollows:\n1) W e propose an AoI-oriented data collection model in\na cluster-based IoT network and formulate a total AoI-\n3\nminimal trajectory planning problem where the hovering\npoints of the UA V and the visiting order to these points\nare jointly optimized.\n2) W e view the formulated problem as a “machine transla-\ntion” problem where the “source language” is the whole\nUA V -IoT network and the “target language” is the UA V\ntrajectory with the minimal total AoI. The state-of-the-\nart TW A* is employed to solve the formulated problem.\nThe parameters of the proposed algorithm are trained\nby reinforcement learning that only needs the reward\ncalculation.\n3) The learned policy by the proposed algorithm general-\nizes well on different sizes of problem instances. In other\nwords, the trained model by the proposed algorithm can\nautomatically ﬁnd a trajectory with the minimal total\nAoI for new problem instances, without retraining the\nmodel.\n4) Extensive simulations are conducted to evaluate the\nperformance of the proposed algorithm. Results show\nthat the proposed algorithm achieves signiﬁcant perfor-\nmance gain in maintaining data freshness while reducing\ncomputation time when compared with other baseline\nalgorithms.\nThe rest of this paper is organized as follows. Section\nII introduces the system model and presents the formulated\nproblem. Section III develops the proposed algorithm. Sect ion\nIV provides simulation results. Finally, Section V conclud es\nthe paper.\nII. S Y S T E M MO D E L A N D PRO BL E M FO RM U L AT IO N\nW e consider a UA V -assisted IoT network that consists of\none rotary-wing UA V , one ground base station (BS) located at\nb0, and M clusters of ground sensor nodes. Speciﬁcally, each\ncluster m, m = 1, . . . , M , has one CH, located as bm, and Nm\nordinary sensor nodes, located at as Bm = {b(1)\nm , . . . , b (Nm)\nm }.\nThe ground IoT network performs some sensing tasks in\nthe surrounding area where the ordinary sensor nodes are\nresponsible for sampling data and forwarding the collected\ndata to their corresponding CHs. The UA V is dispatched\nfrom the start hovering point c0 which is directly above\nb0 to visit M mission hovering points {c1, . . . , c m, . . . , c M }\nby a pre-designed trajectory for data collection, and then\nﬂies back to c0 after completing the data collection task.\nEach hovering point corresponds to one ground cluster and\nits position will be determined by the proposed algorithm.\nThe three-dimensional (3D) Cartesian coordinates system i s\nconsidered to deﬁne positions of hovering points and all CHs .\nThe coordinate of the m-th hovering point is denoted by\ncm = (xcm , y cm , H ) ∈R3, where H is the ﬂight height of\nthe UA V , whereas the location of the corresponding ground\nCH is given by bm = (xbm , y bm , 0) ∈R3.\nW e assume that the rotary-wing UA V supports a ﬂying-\nhovering mode without considering acceleration-decelera tion,\ni.e., it ﬂies to the hovering points with a ﬁxed speed vUA V and\nhovers at these points with static status to collect data fro m\nground CHs. W e illustrate the UA V -assisted data collecting\nprocess in Fig. 1. The UA V takes off from c0, determines\nthe position of the hovering point c2 that will be visited ﬁrst\nand arrives at it. The UA V repeats this procedure until data\ncollection of all clusters is completed, and ﬂies back to c0.\nHence, the ﬁnal trajectory of the UA V in this example is\n{c0, c 2, c 3, c 4, c 1, c 0}.\nA. Data Collection Model\nWhen the UA V arrives at cm, it sends a beacon message\nto wake up the corresponding CH bm from its sleep mode.\nThe beacon message includes the type of sensor nodes to be\nactivated in response to the beacon, the data collection hei ght\nof the UA V , a threshold to limit the number of sensor nodes\nin the CH (if necessary), and a trailer that has error detecti on\ncapabilities. Then, bm switches to its active mode and informs\nits member nodes in the same cluster to sample and send their\nsampled data sequentially according to the pre-allocated e qual-\nlength time slots using time-division multiplexing (TDM)\nprotocol to avoid collision. W e consider the generate-at-will\nmodel [28] as the data sampling model for all ordinary sensor\nnodes, by which nodes can generate information updates at\nany time. Speciﬁcally, we assume that each node can generate\nan update message of size Ldata only in its allocated time slot\nto eliminate the waiting time. Also, each message has a time\nstamp, which is the start of each time slot. The length of a\ntime slot is denoted as τ seconds. After the CH located at\nbm ﬁnishes collecting data from its member nodes, it will\nforward the collected data to the UA V . For ease of analysis,\nthe wake-up time of nodes, including CHs and all ordinary\nnodes, and the information sampling time of each node are\nassumed negligible as compared to the data collection time.\nThus, the data collection time of the UA V at each hovering\npoint mainly consists of two parts: the data transmission ti me\nfrom ordinary nodes to their CHs and the time consumed for\nforwarding the collected data from CHs to the UA V .\nW e consider both the line-of-sight (LoS) and non-line-of-\nsight (NLoS) links to design the ground-to-air communicati on\nwhen the UA V hovers at mission hovering points. The LoS\nlink probability is related to environment, elevation angl e, and\ntransmission distance, which can be expressed as [29]\nP (LoS)\ncm = 1\n1 +β exp\n(\n−˜β (θcm −β)\n) , (1)\nwhere β and ˜β are constants determined by the environ-\nment, θcm = arctan (H/R (cm,b m)) is the elevation angle\nbetween bm and the UA V when it hovers at cm, R(cm,b m) =√\n(xcm −xbm )2 + (ycm −ybm )2 is the horizontal distance\nbetween the CH bm and the hovering point. Correspondingly,\nthe probability of NLoS is given by P (NLoS)\ncm = 1 −P (LoS)\ncm .\nIn addition, the path loss models of LoS and NLoS between\nthe CH bm and the UA V follow [30]\nL(LoS)\ncm = 20 log10\n(4πfcd(cm,b m)\nvlight\n)\n+ ξLoS , (2)\nL(NLoS)\ncm = 20 log10\n(4πfcd(cm,b m)\nvlight\n)\n+ ξNLoS , (3)\n4\nCH Hovering point \n UAV \nFig. 1. System model of a UA V -assisted IoT network.\nwhere fc is the carrier frequency, vlight is the speed of light,\nd(cm,b m) =\n√\nH2 + R2\n(cm,b m) is the distance between the UA V\nand the CH bm, ξLoS and ξNLoS (ξLoS < ξNLoS ) are the excessive\npath losses in LoS and NLoS links, respectively. W e consider\nthe average path loss to describe the link from the ground CH\nto the UA V , which can be expressed as\nLcm = P (LoS)\ncm L(LoS)\ncm + P (NLoS)\ncm L(NLoS)\ncm . (4)\nT o avoid the interference among CHs, we assume that only\none CH can transmit data to the UA V at any given time.\nHence, the average available transmission rate in bits per\nsecond (bps) from CH bm to the UA V can be expressed\nas rcm = Bwidth log2 (1 +γcm ), where Bwidth is the channel\nbandwidth in hertz (Hz), γcm = PCH /\n(\nσ210\nLcm / 10\n)\nis the\nsignal-to-noise ratio (SNR) of the transmission link, σ2 is the\nnoise power at the UA V , and PCH is the transmission power\nof the CH. Regarding the transmission quality, we set a SNR\nthreshold γth and the transmission is considered successful if\nthe SNR is greater than the threshold. Thus, the SNR constrai nt\nat the UA V receiver is given as\nγcm ≥γth . (5)\nLemma 1: Given the ﬁxed ﬂight height H, cm should be\nlocated in a horizontal disk region centered at the position\nthat directly above bm and having the radius R∗ which can\nguarantee that the UA V successfully receives data. When\nR(cm,b m) = R∗, the received SNR of the UA V at cm is equal\nto γth .\nProof : See Appendix A.\nBased on Lemma 1 , we formally deﬁne a hovering disk\nregion for each mission hovering point (excluding the start\npoint c0) as\nOm = {cm : ||cm −b′\nm||= R(cm,b m) ≤R∗} (6)\nwhere b′m = (xbm , y bm , H ) ∈R3 is the center of the disk\nOm, and R∗ is the radius to maintain a pre-deﬁned quality-\nof-service, which can be found numerically. As long as the\nUA V enters a hovering disk region, it can collect data from\nthe corresponding ground CH. The total data collection time\nof the UA V at cm ∈Om (or its hovering time) can be simply\nwritten as\nT (hov)\ncm = Nmτ + NmLdata\nrcm\n(7)\nwhere the ﬁrst term in the right hand side is the time consumed\nfor transmitting data from ordinary nodes to their correspo nd-\ning CH bm, and the second term is the data transmission time\nfrom bm to the UA V . Therefore, the energy consumption of\npropulsion-related and communication-related activitie s of the\nUA V while hovering at cm is expressed as\nEcm = Phov T (hov)\ncm + Pcom\nNmLdata\nrcm\n(8)\nwhere Phov and Pcom are the UA V’s powers for hovering\nand communication, respectively. After ﬁnishing the data\ncollection task, bm switches to the sleep model for saving\nenergy. The UA V continues to select the next hovering point\nand executes the same processes to collect the sensed data\nfrom the corresponding ground cluster.\nB. UA V’s Mobility Model\nWithout loss of generality, the ﬂight trajectory of the\nUA V can be seen as a permutation of the visiting or-\nder to M mission hovering points, with the start point\nbeing c0, i.e., c = {c0, c 1, . . . , c M }. The set of all\npossible permutations is denoted as Φ with the size of\nM!. W e represent one of the permutations as π =\n{π(0), . . . , π (M +1)}and express the ordered hovering points\nas cπ = {cπ (0), c π (1), . . . , c π (M), c π (M+1)}, where cπ (t), t =\n0, . . . , M + 1, is the hovering point that is visited at step t\nin the trajectory, and cπ (0) = cπ (M+1) = c0. For ease of\nunderstanding, if the hovering point cm is visited at step t, its\ncorresponding cluster of ground ordinary nodes ( Bm) and the\nnumber of ordinary nodes ( Nm) are redeﬁned as Bπ (t) and\nNπ (t), respectively.\nAfter ﬁnishing data collection at cπ (t) with the hovering\nmodel, the UA V horizontally ﬂies to the next hovering point\ncπ (t+1) along the line segment connecting cπ (t) and cπ (t+1).\nThe ﬂying time of the UA V during this period is given by\nT (ﬂy)\n(cπ (t),c π (t+1)) = ||cπ (t) −cπ (t+1)||\nvUA V\n(9)\nwhere ||cπ (t) −cπ (t+1)||is the Euclidean distance between\ncπ (t) and cπ (t+1).\nFollowing [31], the propulsion power consumption of the\nUA V for horizontal movement is the function of speed vUA V\nand given by\nPmov (vUA V) =P0\n(\n1 +3v2\nUA V\nU2\ntip\n)\n+ P1\n((\n1 +v4\nUA V\n4v4\n0\n) 1/ 2\n−v2\nUA V\n2v2\n0\n) 1/ 2\n+ 1\n2 d0ρs0δv3\nUA V (10)\n5\nFig. 2. The time sequence of data collection in the considere d UA V -IoT\nsystem.\nwhere P0 and P1 represent, respectively, the blade proﬁle\npower and induced power in the hovering state, Utip is the\ntip speed of the rotor blade of the UA V , v0 is the mean rotor\ninduced velocity in the hovering state, d0 denotes the fuselage\ndrag ratio, s0 represents the rotor solidity, ρ is the density\nof air, and δ denotes the area of the rotor disk. According to\nthe analysis in [31], the power consumption Pmov (vUA V) ﬁrstly\ndecreases and then increases with the increasing value of th e\nspeed vUA V. The energy consumption in the UA V’s ﬂight from\ncπ (t) to cπ (t+1) is computed as\nE(cπ (t),c π (t+1)) = Pmov (vUA V)T (ﬂy)\n(cπ (t),c π (t+1)). (11)\nIn the hovering state, the power consumption of the UA V can\nbe obtained by substituting vUA V = 0into (10), Phov = P0+P1,\nwhich is a constant value.\nC. Age of Information Model in a UA V-IoT System\nW e use the AoI metric to measure the freshness of informa-\ntion. According to the deﬁnition of AoI in [32], the AoI of a\npacket collected from node b(n)\nπ (t) in the π(t)-th visited cluster\nat time ζ is deﬁned as\nA(n)\nπ (t)(ζ) =\n(\nζ −u(n)\nπ (t)(ζ)\n) +\n(12)\nwhere u(n)\nπ (t)(ζ) is the instant at which the packet is generated,\nand (x)+ = max {0, x }. When ζ < u (n)\nπ (t)(ζ), we deﬁne\nA(n)\nπ (t)(ζ) = 0. This is because the packet of node b(n)\nπ (t) has\nnot been sampled. It is evident that the AoI of a packet will\nincrease with time. In the considered UA V -IoT system, the BS\nis seen as the observer, thus, the AoI of a data packet can be\nseen as the amount of time elapsed from the instant at which\nthe packet is generated to the instant at which the UA V ﬂies\nback with the collected data to the BS.\nFor ease of analysis, for any ordinary node b(n)\nπ (t), n =\n1, . . . , N π (t) in the π(t)-th visited cluster, the AoI of its\npacket can be simply divided into two components. The ﬁrst\ncomponent is the time needed for the CH of its associated\ncluster to collect data from b(n)\nπ (t) and other nodes whose data\nhave not been gathered (i.e., nodes b(n+1)\nπ (t) , . . . , b\n(Nπ (t))\nπ (t) ) and\nforward the collected data to the UA V . The second component\nis the time consumed by the UA V to carry the packet of b(n)\nπ (t)\nto the end point cπ (M+1). Speciﬁcally, this period includes\nthe ﬂight time of the UA V to unvisited ground clusters and\nthe data collection time in these clusters. For example, aft er\ncompleting the data collection at cπ (t), the UA V will ﬂy to the\nnext hovering point cπ (t+1) and gather information from the\ncorresponding cluster. During this period, the AoI of the pa cket\nof b(n)\nπ (t) increases with time, which is the sum of the ﬂight time\nT(cπ (t),c π (t+1)) from cπ (t) to cπ (t+1) and data collection time\nTcπ (t+1) at hovering point cπ (t+1). Then, the UA V performs\nthe same process to unvisited clusters until it returns to th e\nend point. The time sequence of data collection in the UA V -\nIoT system is illustrated in Fig. 2. Mathematically, the tot al\nAoI of the packet generated by b(n)\nπ (t) in the UA V -IoT system\nis given as\nA(n)\nπ (t) =\n(\nNπ (t) −(n −1)\n)\nτ + Nπ (t)Ldata\nrcπ (t)\n  \nﬁrst component\n+\nM−1∑\ng=t\n(\nT (ﬂy)\n(cπ (g),c π (g+1)) + T (hov)\ncπ (g+1)\n)\n+ T (ﬂy)\n(cπ (M),c π (M+1))\n  \nsecond component\n(13)\nwhich can be further simpliﬁed as\nA(n)\nπ (t) =\nM∑\ng=t\n(\nT (hov)\ncπ (g) + T (ﬂy)\n(cπ (g),c π (g+1))\n)\n−(n −1)τ. (14)\nFor packets of nodes in the same cluster, we have\nA(1)\nπ (t) > A (2)\nπ (t) ··· > A\n(Nπ (t))\nπ (t) . (15)\nOn the other hand, the AoIs of packets in different clusters\nshould satisfy\nA(n)\nπ (1) > A (n)\nπ (2) > ··· > A (n)\nπ (M). (16)\nD. Problem F ormulation\nThe total AoI of all ordinary nodes in the network can be\ncomputed as\nA =\nM∑\nt=1\nNπ (t)∑\nn=1\nA(n)\nπ (t) =\nM∑\nt=1\nNπ (t)∑\nn=1\nM∑\ng=t\n(\nT (hov)\ncπ (g) + T (ﬂy)\n(cπ (g),c π (g+1))\n)\n−\nM∑\nt=1\nNπ (t)∑\nn=1\n(n −1)τ. (17)\nAccording to (17), the total AoI is expressed as a weighted\nsum of the ﬂight time of the UA V and the data collection time\nat each hovering point, which is determined by the locations of\nhovering points c, the visiting order to these hovering points\nπ . It is evident that the hovering points of the UA V and its\ntrajectory have a strong impact on the total AoI of data. If\nthe position of any hovering point cm is close to the center\nof the disk region Om, a high data transmission rate can be\nachieved. As a result, the data transmission time from CHs to\nthe UA V can be reduced, even though the UA V may have a\nlonger ﬂight trajectory, and hence the ﬂight time. Converse ly,\nif the UA V is located near the boundary of the disk region, the\nlength of the UA V’s trajectory might be reduced, but it will\nresult in a lower data transmission rate, and hence increase d\ndata transmission time.\nOur objective is to jointly ﬁnd the hovering point from each\ndisk and plan the visiting order to these hovering points for\nthe UA V to minimize the total AoI of data in the considered\n6\nUA V -IoT system. The optimization problem is expressed as\nfollows:\nP1 : minc, π\nA (c, π ) , (18a)\ns.t. π ∈Φ , (18b)\n(5), (7), (9), (15), and (16).\nConstraint (18b) is the trajectory constraint. The SNR con-\nstraint is given in (5), and (7) is the data collection constr aint.\nThe ﬂight time constraint is expressed as (9). AoI constrain ts\nare (15) and (16). It is evident that the formulated problem P1\nis a TSPN [33], which combines the determination of hovering\npoints at each disk with the problem of trajectory planning o f\nthe UA V . The traditional TSPN problem involves ﬁnding a\nminimum-cost tour (i.e., the total length of the tour is mini -\nmum) that travels each region exactly once for a collection o f\ncompact regions before returning to the initial departure p oint\n[34]. However, our formulated problem not only considers\nthe traveling cost but also the cost spent at each hovering\npoint. The problem P1 is extremely challenging because it is\ncomposed of a continuous problem (optimization of hovering\npoints c) and a combinatorial problem (optimization of visiting\norder π ). Given a set of hovering points c, the optimization\nof π can be viewed as the TSP , which can be normally be\nsolved quite effectively by some dedicated TSP solvers, suc h\nas Concorde [35], etc. However, the optimization of hoverin g\npoints c consists of an inﬁnite number of variables, which\nis infeasible to be solved optimally. T o reduce computation al\ntime, we leverage the sampling approach that samples ﬁnite\ndiscrete sets of hovering points from a continuous state spa ce\nto transform the continuous TSPN in P1 into the GTSP . Specif-\nically, each disk Om is equally partitioned into Lsub ×Lsub\nsub-regions and the center of each sub-region is selected as\nthe possible hovering point. For some marginal sub-regions\nwith non-square shape, we choose the centers of their actual\nareas. Hence, we can obtain a cluster Gm of sampling points\nwith the size of L2\nsub from Om. As a result, our objective is\nchanged to jointly select hovering points from M clusters of\nsampled hovering points and plan the UA V’s trajectory to vis it\nselected hovering points exactly once to minimize the total\nAoI. Using the sampling approach, the formulated problem\nP1 is converted to\nP2 : minc, π\nA (c, π ) , (19a)\ns.t. cm ∈Gm, G m ∈Om, m ∈{1, . . . , M }, (19b)\n(5), (7), (9), (15), (16), and (18b).\nObviously, the formulated problem P2 is a combinatorial\noptimization problem, and hence, NP-hard. There are two\ntraditional methods to handle combinatorial problems: exa ct\nalgorithms and heuristic algorithms. Exact algorithms can ﬁnd\noptimal solutions, but they will become intractable when th e\nsize of problems grows. Heuristic algorithms’ complexity i s\npolynomial and they commonly ﬁnd sub-optimal solutions.\nIn contrast, we cast the proposed GTSP as a sequence-to-\nsequence problem where the source sequence is a set of\nclusters of hovering points and CHs and the target sequence\nis a set of selected hovering points and the visiting order to\nthese points. W e adopt the transformer, the weighted A*, and\nreinforcement learning to efﬁciently solve this problem.\nIII. T RA N S F O RM E R -W E IG H T E D A* A L G O RIT H M\nBecause the UA V needs to sequentially collect data from\neach ground cluster in the IoT network, we view the problem\nof the total AoI-minimal trajectory planning as a “machine\ntranslation” problem that is common in natural language pro -\ncessing. The whole UA V -IoT network as the “source language”\nis translated into the “target language”, i.e., the UA V traj ectory,\nby using our proposed TW A* algorithm. The TW A* algorithm\nis composed of an encoder network, a decoder network, and\nthe weighted A* search algorithm which can effectively ﬁnd\nthe trajectory policy from hidden patterns behind a large\nnumber of training datasets.\nA. Encoder\nThe role of the encoder network is to take the UA V -IoT\nnetwork represented as an input sequence and map it into\nan abstract representation that is the learned information . The\ninput sequence includes the start point of the UA V , each and\nevery CH, number of nodes in each ground cluster, and all\nsampling points from each hovering disk. Speciﬁcally, we\ndeﬁne h(in)\n0 = c0 ∈ R3, and h(in)\nm = ( Gm, b m, N m) ∈\nR3(L2\nsub +1)+1, m ∈ {1, . . . , M }, where the cluster Gm of\nsampling points is represented as a 3L2\nsub-dimensional vector\nas it includes L2\nsub points with 3D Cartesian coordinates,\nthe CH bm is a 3-dimensional vector, and the number of\nnodes Nm is a constant. Hence, the input can be expressed\nas H(in) =\n(\nh(in)\n0 ; h(in)\n1 ; . . . ; h(in)\nM\n)\n. The encoder network\nused in this paper is the standard transformer encoder with\none embedding layer and six identical encoder layers as in\n[27]. Each encoder layer is composed of one multi-head self\nattention sub-layer and one point-wise feed-forward netwo rk\nsub-layer. Each sub-layer adds a residual connection and la yer\nnormalization. The embedding layer is to map each element of\ninput to the dem -dimensional vector space by a learnable linear\nprojection. Speciﬁcally, to enable the model to distinguis h the\nstart point of the UA V from clusters, we separately utilize\ndifferent parameters to compute the embeddings of the start\npoint and the other clusters as follows:\nh(0)\nm =\n{\nW0h(in)\nm + Wb0 , m = 0\nW1h(in)\nm + Wb, m = 1, . . . , M (20)\nwhere W0 ∈Rdem ×3, W1 ∈Rdem ×(3(L2\nsub +1)+1), Wb0 ∈Rdem ,\nand Wb ∈Rdem are learnable parameters. Then, the embed-\ndings H(0) =\n(\nh(0)\n0 ; h(0)\n1 ; . . . ; h(0)\nM\n)\n∈R(M+1)×dem are fed\ninto the encoder layers. Note that we do not consider the\npositional decoding used in the original transformer in [23 ]\nbecause the order of the input sequence is irrelevant to the\nGTSP .\nThe attention layer in each encoder layer uses the multi-\nhead self-attention mechanism with 8 heads to jointly atten d\nto information from different representation subspaces at dif-\nferent positions. The 8 heads perform the attention calcula tion\nin parallel and their results are merged to produce an input f or\n7\nEmbedding \nMulti-Head Attention \nBatchNorm \nFeed Forward \nBatchNorm \n+\n+\nLayer l\nLayer 6 \nMulti-Head Attention \nLayerNorm \n+\nMulti-Head Attention \nLayerNorm \n+\n... \n... \nLayer 1 \nLayer 1 \nLayer 2 \nSingle-head attention \n... \n... \n... \n... \n... \n \nFig. 3. The proposed algorithm framework.\nthe next step. In the encoder layer l, l = 1, . . . , 6, the output\nof self-attention on the h-th head, h = 1, . . . , 8, is computed\nas\nZ(l)\nh = Attention(Q(l)\nh , K(l)\nh , V (l)\nh )\n= softmax\n\nQ(l)\nh K(l)\nh\nT\n√dv\n\nV (l)\nh (21)\nwhere dv is used for scaling the dot products, Q(l)\nh ∈\nR(M+1)×dv , K(l)\nh ∈R(M+1)×dv , and V (l)\nh ∈R(M+1)×dv are\nmatrices query, key, and value for the h-th head, respectively.\nThey can be created by projecting the input query Q(l),\nkey K(l), and value V (l) of multi-head self attention with\nthree learnable weight matrices W Q(l)\nh ∈Rdem ×dv , W K(l)\nh ∈\nRdem ×dv , and W V (l)\nh ∈Rdem ×dv , respectively, as follows\nQ(l)\nh = Q(l)W Q(l)\nh , K(l)\nh = K(l)W K(l)\nh , V (l)\nh = V (l)W V (l)\nh\n(22)\nwhere Q(l) = K(l) = V (l) = H(l−1). In this paper H(l−1)\nis the output of the encoder layer (l −1) or the output of the\nembedding layer before the encoder layer 1. Matrices Q(l)\nh ,\nK(l)\nh , and V (l)\nh can be further expressed as\nQ(l)\nh =\n\n\n\nq0\n.\n.\n.\nqM\n\n\n, K(l)\nh =\n\n\n\nk0\n.\n.\n.\nkM\n\n\n, V (l)\nh =\n\n\n\nv0\n.\n.\n.\nvM\n\n\n\n(23)\nwhere ∀q, k, v ∈Rdv . Then, we can obtain the scaled attention\nscores\nQ(l)\nh\n(\nK(l)\nh\n) T\n√dv\n= 1√dv\n\n\n\n\n(q0, k0) . . . (q0, kM )\n(q1, k0) . . . (q1, kM )\n. . . (qi, kj ) . . .\n(qM , k0) . . . (qM , kM )\n\n\n\n\n=\n\n\n\n\nu00 . . . u 0M\nu10 . . . u 1M\n. . . u ij . . .\nuM0 . . . u MM\n\n\n\n (24)\nwhere (qi, kj ), i, j ∈ {0, . . . , M } is the inner product of\nvectors, which measures the similarity of vector qi and vector\nkj . The row-wise softmax function is used on each element\nof the above scaled attention scores matrix, which is given b y\nuij = euij / ∑ M\nj′=0 euij′ . Then, the output Z(l)\nh ∈R(M+1)×dv\nof the h-th head is expressed as\nZ(l)\nh =\n\n\n\n\n\n\n∑ M\nj=0\nu0j vj∑ M\nj=0\nu1j vj\n.\n.\n.∑ M\nj=0\nuMj vj\n\n\n\n\n\n\n=\n\n\n\n\n\nu0\nu1\n.\n.\n.\nuM\n\n\n\n\n. (25)\nHence, we end up with 8 different outputs from 8 heads\nwhere each head could learn something different. These out-\nputs are concatenated and multiplied by an additional learn able\nweight matrix W (l)\no ∈R8dv ×dem to generate the ﬁnal output\nof the multi-head attention layer, as follows:\nZ(l) =\n(\nZ(l)\n1 , . . . , Z(l)\n8\n)\nW (l)\no , Z(l) ∈R(M+1)×dem . (26)\nT o facilitate the understanding of multi-head attention la yer,\nall operations from (21) to (26) are deﬁned as a function\n8\n... \n... \n.\n... \nSoftmax \n.\n ... \nHead 1 \nFig. 4. Multi-head self attention.\nMHA(·). Thus, Z(l) = MHA\n(\nQ(l), K(l), V (l))\n. Then, Z(l) is\nadded to the input of the multi-head attention in this encode r,\nwhich is a residual connection operation. Subsequently, th e\noutput of the residual connection is fed into a batch nor-\nmalization, deﬁned as a function BN( ·), and it is written\nas Z\n′(l) = BN\n(\nH(l−1) + Z(l))\n. The use of the residual\nconnection is to avoid the degradation problem of the networ k\nin training, while the layer normalization can improve the\ntraining speed and the stability of the networks. The normal -\nized residual output goes through a pointwise feed-forward\nnetwork (deﬁned as a function FFN( ·)), which is a couple\nof linear layers with a ReLU activation in between. Then,\nthe output of the pointwise feed-forward network is added\nto its input by a residual connection and further normalized\nto obtain the ﬁnal output H(l) ∈R(M+1)×dem of the encoder\nlayer l, which is given by H(l) = BN\n(\nZ\n′(l) + FFN\n(\nZ\n′(l)\n))\n.\nIn each encoder layer, we perform the same computational\nprocess and ﬁnally output the ﬁnal result of the encoder part at\nlayer 6, H(6) =\n(\nh(6)\n0 ; h(6)\n1 ; . . . ; h(6)\nM\n)\n∈R(M+1)×dem , which\nis the continuous representation with attention informati on of\nthe input H(in). All of these operations will help the decoder\nnetwork focus on the appropriate elements in the input durin g\nthe decoding process.\nB. Decoder\nThe decoding is autoregressive and generates the result one\nby one. The output of the decoder network can be represented\nas an ordered sequence of the input of the encoder. The\ndecoder begins with the start point at decoding step 0 since\nthe trajectory of the UA V should start at the start point as we ll\nas end at this point. The output of each decoding step is based\non the information from the encoder and the already-generat ed\nprevious output in the decoder. Hence, the decoding process\ncan be modelled using the probability chain rule:\nP (π |H(in))\n=\nM+1∏\nt=0\nP (π(t)|π(0), . . . , π (t −1), H(in)). (27)\nThe decoding process aims at ﬁnding the optimal π to\nmaximize P (π |H(in)).\nThe decoder network is composed of two identical decoder\nlayers, and a single-head attention layer. Each decoder lay er\ncontains two multi-head attention sub-layers which employ\na residual connection around them followed by layer nor-\nmalization. These sub-layers have the same structure as the\nsub-layers in the encoder network but each of them has a\ndifferent job. Since the output of the decoder network is\nrelated to the order, we need to inject some information\nabout the positions into the input sequence of the decoder\nnetwork. The locations are implicitly represented by the or der\nof the data input to the decoder network. Hence, the input\nof the decoder network is the output of the encoder network\ncombined with the positional encoding. Suppose the outputs\nof the decoder network at previous t decoding steps are\nπ(0), π (1), . . . , π (t), the decoder wants to predict the output\nat t + 1 step. Then, the input to the decoder network is ex-\npressed as ˆH(0)\nt+1 =\n(\nˆh(0)\nπ (0); ˆh(0)\nπ (1); . . . ; ˆh(0)\nπ (t)\n)\n. Each element\nin ˆH(0)\nt+1 can be calculated by ˆh(0)\nπ (t) = h(6)\nπ (t) + PEt, where\nh(6)\nπ (t) ∈R1×dem is one element in H(6) which is decoded at\nthe t-th step, PE t ∈R1×dem is the positional encoding based\non the sinusoidal function, which is given by [36]\nPEt(di) =\n{ sin (ω di t), if di is even\ncos (ω di t), if di is odd (28)\nwhere di is the dimension, 1 ⩽ di ⩽ dem , ω di is the hand-\ncrafted frequency for each dimension. The position encodin g\nof each position successfully provides the position inform ation\nto the decoder network. The input ˆH(0)\nt+1 gets fed into the ﬁrst\nmulti-head attention sub-layer of the ﬁrst decoder layer an d\npass through the residual connection and layer normalizati on\n(denoted as a function LN( ·)) to prepare the query for the next\nmulti-head attention sub-layer, as follows\nˆZ(1)\nt+1 = MHA\n(\nˆh(0)\nπ (t), ˆH(0)\nt+1, ˆH(0)\nt+1\n)\n, ˆZ(1)\nt+1 ∈R1×dem (29)\nˆZ\n′(1)\nt+1 = LN\n(\nˆh(0)\nπ (t) + ˆZ(1)\nt+1\n)\n, ˆZ\n′(1)\nt+1 ∈R1×dem (30)\nwhere ˆh(0)\nπ (t) is the query, ˆH(0)\nt+1 works as the key and the\nvalue matrices in the current multi-head attention sub-lay er.\nThe second multi-head attention sub-layer is used to match\nthe encoder’s input to the decoder’s input to allow the decod er\nnetwork to decide the next possible output among the non-\nvisited elements. For this sub-layer, the encoder network’ s\noutput H(6) is the key and the value matrices, and ˆZ\n′(1) is\nthe query matrix. The calculations are given by\nˆZ\n′′(1)\nt+1 = MHA\n(\nˆZ\n′(1)\nt+1 , H(6), H(6)\n)\n, ˆZ\n′′(1)\nt+1 ∈R1×dem (31)\nˆH(1)\nt+1 = LN\n(\nˆZ\n′(1)\nt+1 + ˆZ\n′′(1)\nt+1\n)\n, ˆH(1)\nt+1 ∈R1×dem . (32)\n9\nNote that we add the mask of visited elements to the scaled\nattention scores in this sub-layer. Then, ˆH(1)\nt+1 goes through\nthe second decoder layer to get the output ˆH(2)\nt+1 ∈R1×dem . In\norder for the decoder network to compute output probabiliti es\nP (π(t + 1)|π(0), . . . , π (t), H(6)), ˆH(2)\nt+1 and the output H(6)\nof the encoder network get fed into a single-head attention t o\nget a distribution over the non-visited elements, which is g iven\nby [27]\nPt+1 = softmax\n(\ntanh\n(ˆQt+1 ˆKT\nt+1\n√dem\n⊙Mt+1\n))\n(33)\nwhere ˆQt+1 = ˆH(2)\nt+1 ˆW1, ˆKt+1 = H(6) ˆW2, ˆW1 ∈Rdem ×dem\nand ˆW2 ∈ Rdem ×dem are learnable weight matrices, Mt+1\nis the mask of the visited elements considered in this layer,\n⊙is the Hadamard product, and Pt+1 ∈ R1×(M+1) is the\ndistribution over the non-visited elements, which is compo sed\nof probability scores. Then, the output that will be selecte d is\nsampled from the distribution with three decoding methods:\n1) Greedy: At each decoding step, this method greedily\nselects the element with the largest probability P (π(t +\n1)|π(0), . . . , π (t), H(6)).\n2) Random Sampling: This method randomly samples\nWsampling solutions, where each solution includes fully visiting\norder, and selects the solution with the highest probabilit y as\nthe ﬁnal result.\n3) Beam Search: This method chooses the top Wbeam\npossible solutions that have the highest probability at eac h\nstep, where Wbeam is the beam width. Those Wbeam solutions\nwill move to the next time step, and the process repeats. Then ,\nwe can obtain a tree of solutions of each step and the π that\nhas the highest overall probability is picked as the ﬁnal res ult.\nW e assume that the index of the highest probability score\nin Pt+1 is selected with the greedy decoding as the output\nπ(t + 1) at step t + 1. Thus π(t + 1) points to the element\nat the same position of the input sequence H(in) of the\nencoder network, which is represented as h(it)\nπ (t+1). Then, the\ndecoder network takes the encoding information of h(it)\nπ (t+1)\nfrom H(6), i.e., h(6)\nπ (t+1), and adds it with its position encoding\nto the list of the decoder input to continue decoding for\nthe next step. Finally, we can obtain a set of the visiting\norder, π . As shown in the example in Fig. 3, H(in) =\n(c0; (G1, b 1, N 1) ; (G2, b 2, N 2) ; (G3, b 3, N 3) ; (G4, b 4, N 4)) is\nthe input to the encoder network and the decoder network out-\nputs the ﬁnal visiting order π = {π(0), π (1), π (2), π (3), π (4)}\nto elements in H(in).\nC. Selection of Hovering P oints\nGiven the visiting order π , we know the visiting order to\nall hovering points clusters and construct a graph containi ng\nall of them, as illustrated in Fig. 3. Each layer of the graph\nis composed of one hovering points cluster. Then, we will\ncalculate the path with the minimal total AoI starting from\nthe start point (marked as π(0) in the visiting order), going\nthrough each cluster Gm, and ending at the clone of the start\npoint (marked as π(M +1) in the visiting order). T o guarantee\nthat at most one hovering point is selected from each cluster ,\nAlgorithm 1: Pseudocode for weighted A* search\nalgorithm to ﬁnd hovering points\nInput: created graph\n1: FRONTIER = PriorityQueue()\n2: FRONTIER.put(c0, 0)\n3: CAME FROM = [ ]\n4: COST = [ ]\n5: CAME FROM[c0] = None\n6: COST[c0] = 0\n7: while FRONTIER is not empty do\n8: current point s′ = FRONTIER.get()\n9: if s′ = c0′ then\n10: break\n11: end if\n12: for each neighbor s of s′ do\n13: g(s) =COST[s′]+ the total AoI from s′ to s\n14: if s not in COST or g(s) < COST[s] then\n15: COST[s] = g(s)\n16: f(s) =g(s) +ωh (s)\n17: FRONTIER.put(s, f(s))\n18: CAME FROM[s] = s′\n19: end if\n20: end for\n21: end while\n22: calculate A according to CAME FROM\nwe assume that all edges between possible hovering points of\nconsecutive clusters to be directed by π . W e use the weighted\nA* search algorithm [37] to quickly ﬁnd the hovering point\nfrom each cluster to build the path with the minimal cost (tot al\nAoI). W e assume that the UA V currently reaches the point s′\nand will decide the next point to be expanded by the following\ncost function\nf(s) =g(s) +ωh (s) (34)\nwhere s is any neighbor point of s′, g(s) is the total movement\ncost on the path from the start point c0 to s, h(s) is the\nheuristic function to estimate cost from s to the end point\nc0′, and ω > 1 is a constant factor. The neighbor point\nwith a minimal f(s) value is expanded. The pseudocode is\ndescribed in Algorithm 1. W e use COST and FRONTIER to\nkeep track of g(s) and the expanding process, respectively.\nEach point that has been reached keeps a pointer to its parent in\nCAME\nFROM so that we can know where it came from. With\nCAME FROM, we can construct a path having the minimal\nAoI from the start point to the end point, as illustrated by th e\nsolid red line with arrow in the example in Fig. 3.\nD. Computational Complexity Analysis\nIn the encoder network, each encoder layer is the standard\ntransformer encoder with quadratic computational complex ity\nO((M + 1)2dem ) [23]. Since the number of layers is constant,\nthe computational complexity of the encoder network is stil l\nO((M + 1)2dem ). In the decoder network, although each\ndecoder layer contains two multi-head attention sub-layer s,\nits computational complexity is still estimated to be quadr atic\n10\nAlgorithm 2: Training TW A* by REINFORCE with\nrollout baseline\nInput: Epochs Eepochs , training steps S, batch size Bsize\n1: Initialize parameters ϑ, ϑ(BL) ←ϑ\n2: for epoch = 1 to Eepochs do\n3: for step = 1 to S do\n4: H(in)\ni ←generate instances() ∀i ∈{1, . . . , B size}\n5: π i ←Sampling solution Pϑ\n(\n·|H(in)\ni\n)\n6: π (BL)\ni ←Greedy solution Pϑ (BL)\n(\n·|H(in)\ni\n)\n7: Ai ←weighted A* (π i)\n8: A\n(BL)\ni ←weighted A*\n(\nπ (BL)\ni\n)\n9: ∇ϑ J ←∑ Bsize\ni=1\n(\nAi −A\n(BL)\ni\n)\n∇ϑ log Pϑ\n(\nπ i|H(in)\ni\n)\n10: ϑ ←Adam (ϑ, ∇ϑ J)\n11: end for\n12: if t-test(Pϑ (·), P ϑ (BL) (·)) < 5% then\n13: ϑ(BL) ←ϑ\n14: end if\n15: end for\nO((M + 1)2dem ) [23]. Likewise, the number of decoder\nlayers does not affect the computational complexity of the\ndecoder network. In addition, the computational complexit y\nof the single-head attention used in the ﬁnal step of the\ndecoder network is also quadratic O((M + 1)2dem ) [23].\nHence, the employed transformer model has the computa-\ntional complexity O((M + 1)2dem ), which is quadratic in\nthe length of the input sequence. Different data structures\nused to implement the weighted A* algorithm, and hence,\naffect its computational complexity. W e use the min heap\nto implement the weighted A* algorithm. W e assume that at\nmost ML2\nsub points (the total number of points in the search\ngraph) are visited, and the min heap uses O(log(ML2\nsub))\ncomputational complexity to extract a point each time [38].\nThe weighted A* algorithm’s computational complexity is\nestimated to be O(ML2\nsub log(ML2\nsub). Hence, the compu-\ntational complexity of the proposed algorithm is O((M +\n1)2dem ) +O(ML2\nsub log(ML2\nsub).\nE. T raining\nT o enable the transformer model to produce the optimal π ,\nwe use the well-known policy gradient approaches to train it .\nThe transformer model is parameterized by ϑ, which includes\nall trainable variables in the encoder and the decoder netwo rks.\nW e regard the UA V as an agent to learn a good policy π to\nmaximize long-term rewards by iteratively interacting wit h the\nenvironment to optimize parameter ϑ. At each step, the agent\nin a given state chooses an action by its decision policy, whi ch\nactually is the mapping from states to actions.\n1) State: The state consists of the environment encoded\nby the encoder network and the visited clusters before the\ncurrent step in the decoder, which are H(6) and ˆH(0)\nt+1 in the\ntransformer, respectively.\n2) Action: At each step, the agent makes an action π(t)\nbased on its state, which can be seen as the processes of the\nright-hand side of (27). Thus, we view all operations in the\ndecoder network as the action.\n3) Reward: The negative of the total AoI\nA in (17) is used\nas the reward.\nOur objective for training is given by\nJ\n(\nϑ|H(in)\n)\n= Eπ ∼Pϑ (·|H(in))\n(\nA\n)\n. (35)\nThe gradient of (35) is calculated using the REINFORCE\nalgorithm [39] with the greedy rollout baseline\nA\n(BL)\n[25]\n∇ϑ J\n(\nϑ|H(in)\n)\n= Eπ ∼Pϑ (·|H(in))\n[(\nA −A\n(BL))\n∇ϑ log Pϑ\n(\nπ |H(in)\n)]\n(36)\nwhere A is the cost of a solution that is obtained from the\ncurrent training transformer model by sampling decoding. W e\nset the greedy policy as the baseline policy in our model, and\nhence,\nA\n(BL)\nis the cost of a solution of the deterministic\ngreedy decoding, which is used to eliminate variance during\ntraining. By doing so, the transformer model is trained to\nimprove over its (greedy) self. The training process is sum-\nmarized in Algorithm 2. In each training step, new instances\nare generated ﬁrst (line 4). Then, the transformer model use s\nsampling decoding and greedy decoding to produce π i and\nπ (BL)\ni (lines 5 and 6), respectively. The total AoIs are further\nobtained from the weighted A* (lines 7 and 8). The gradient\nin (36) is approximated with Monte Carlo sampling in a batch\nsize Bsize (line 9). The model parameter ϑ is updated using the\nAdam optimizer (line 10). W e compare the current policy with\nthe greedy baseline policy and update the parameter ϑ(BL)\nonly if the improvement is signiﬁcant according to a paired\nt-test (5%) [25].\nAs pointed out earlier, Ptr-A* proposed in our previous work\n[5] can also be used to solve the formulated GTSP in this\nwork. Here, we give comparisons between TW A* and Ptr-\nA* in detail. First, they have different structures. In Ptr- A*,\nthe encoder network consists of the LSTM networks, and the\ndecoder network is composed of the LSTM networks and the\nattention mechanism. The LSTM networks have the form of\na chain of repeating modules of a neural network. The key\nto the LSTM networks is the cell state, which is the hidden\nstate. In theory, the hidden state can carry relevant inform ation\nthroughout the processing of the sequence. Since the LSTM\nnetworks process the elements of the input sequence one by\none, the hidden state of each element of the input sequence\nis calculated by the current element and the previous hidden\nstate. The ﬁnal hidden state of the encoder network is fed int o\nthe LSTM networks of the decoder network. Then, the atten-\ntion mechanism uses the hidden state of the decoder network t o\ngenerate the output sequence. In TW A*, the encoder network\nincludes six identical encoder layers in which each encoder\nlayer is mainly composed of one multi-head self-attention s ub-\nlayer and one point-wise feed-forward network. The decoder\nnetwork of TW A* consists of two identical decoder layers and\none single-head attention layer. The encoder network is use d\n11\nto map the input H(0) =\n(\nh(0)\n0 ; h(0)\n1 ; . . . ; h(0)\nM\n)\nto a sequence\nof continuous representations. The decoder network receiv es\nthe output of the encoder together with the decoder output\nat the previous time step to generate the output sequence.\nMulti-head self-attention is an attention mechanism relat ing\ndifferent positions of a single sequence in order to compute a\nrepresentation of the sequence. Multi-head self-attentio n helps\nTW A* to look at all elements in the input sequence for clues\nthat can help lead to a better encoding. Unlike hidden states\nused in Ptr-A*, TW A* does not rely on past hidden states to\ncapture dependencies with previous elements in the sequenc e.\nAs a result, TW A* does not suffer from long dependency\nissues, which are very common in recurrent-based networks,\nsuch as RNNs and LSTMs, and hence does not lost past\ninformation.\nSecond, TW A* can process sequences in parallel, which\nis faster than Ptr-A*. In Ptr-A*, the elements of a sequence\nmust be processed one by one and each element’s hidden state\nis assumed to be dependent only on the previously hidden\nstate. Hence, Ptr-A*’s recurrent structure makes it hard to\nuse parallel computing to process sentences and this means\nthat it is very slow in training and inference. All elements\nin a sequence are processed in TW A* as a whole rather than\none by one. Because of the use of multi-head self-attention\nthat is designed in parallel, TW A* has the ability of paralle l\ncomputation.\nThird, the training method used in this work is different\nfrom the training method in [5]. In [5], in order to obtain\nthe optimal parameter of Ptr-A*, we use the actor-critic\narchitecture to train Ptr-A* where a second critic network m ust\nbe trained. In this work, we use the greedy rollout baseline\nwhere the TW A* model is trained to improve over its (greedy)\nself. This training method can avoid all the inherent traini ng\ndifﬁculties associated with the actor-critic architectur e.\nIV . N U M E RICA L RE S U LT S\nW e conduct extensive experiments to investigate the per-\nformance of the proposed TW A* algorithm in solving the\nproblem of trajectory planning to minimize the total AoI for\nthe UA V -IoT network. The proposed model is implemented by\nPytorch 1.7 and Python 3.8 and trained on a machine with 1\nNVIDIA R TX 2080Ti GPU.\nA. T est Settings\n1) Decoding Strategies: As we mentioned in Section III-D,\nthe random sampling decoding and the greedy decoding are\nemployed for training the model. At inference, we evaluate\nperformance of all three decoding methods on test instances\nand they are marked as TW A*–greedy, TW A*–sampling\n(Wsampling = 5120), and TW A*–beam search ( Wbeam = 100).\n2) Comparison Algorithms: T o evaluate the effectiveness\nof the proposed model with different decoding methods, we\ncompare it with the genetic algorithm [40], the simulated\nannealing (SA) algorithm [41], and Ptr-A* with the sampling\nstrategy [5]. Common parameters are selected for the geneti c\nalgorithm: The population size is the number of all possible\nhovering points in one instance, the maximal iteration is\nT ABLE I\nSI M U L AT I O N PA R A M E T E R S\nParameter V alue Parameter V alue\nH 100 m β 12.08\n˜β 0.11 PCH 0.1 W\nγth 20 dB (default) ξLoS 1 dB\nξNLoS 20 dB σ 2 − 110 dBm\nvUA V 15 m/s fc 2 GHz\nLdata 5 Mb Bwidth 1 MHz\nPcom 0.1 W Lsub 5\nP0 99.66 W P1 120.16 W\nUtip 120 m/s v0 0.002 m/s\nd0 0.48 ρ 1.225 kg/m 3\nτ 0.1 s δ 0.5\ns0 0.0001\n10000, crossover is 0.1, and mutation probability is 0.8.\nThe parameters of SA for the initial temperature, cooling\ncoefﬁcient, and maximal iteration are taken as 100, 0.99, an d\n1000, respectively.\n3) Data Generation: W e assume there is a probability\ndistribution over a family of problems. During training,\nproblem instances are generated according to this distribu -\ntion, and any test examples are also produced from the\nsame distribution at inference. For any problem instances,\nall CHs {b1, . . . , b M }are randomly sampled from the dis-\ntribution U = torch. FloatTensor(1, 2). uniform(0, 3000).\nWith the SNR threshold γth and environment parameters, we\ncan calculate the hovering disk Om for each CH bm, and\neach cluster of candidate hovering points Gm is sampled\nfrom Om. The number of nodes Nm in each ground clus-\nter is randomly chosen from {5, 10, 15, 20, 25, 30}. Hence,\nany of the problem instances is obtained as H(in) =\n(c0; (G1, b 1, N 1) ;. . . ; (Gm, b m, N m) ;. . . ; (GM , b M , N M )).\n4) Environment P arameters and Hyperparameters: W e\nconsider a ground network with a size of 3 km ×3 km, and\nthe start position of the UA V is located at (0 m, 0 m, H m).\nEnvironment parameters are listed in T able I. The embedding\ndimension dem is equal to 512 and dv is equal to 64. W e train\nthe proposed model using the Adam optimizer with a learning\nrate of 0.0001 on Eepochs = 200 epochs, where each epoch\nincludes S = 1000 training steps. At each training step, the\nbatch size Bsize is equal to 512, which means there are 512\ninstances in each batch. In each instance, we set M = 10.\nB. Analysis of the Results\nW e ﬁrst compare the total AoI between our proposed\nalgorithm against the genetic and SA algorithms on the train ed\nmodel when the value of M varies. Although the model\nis trained on 10-clusters IoT networks ( M = 10), it still\nshows good performance on IoT networks with different\nsizes, like 20-clusters, 30-clusters, etc., as can be seen i n\nFig. 5 (a). Speciﬁcally, the TW A*-sampling algorithm alway s\nobtains the minimal total AoI when compared with other\ntwo decoding methods, as well as the three other algorithms\nunder comparison. The TW A*-beam search and TW A*-greedy\nalgorithms exhibit an obviously superior performance than the\ngenetic and SA algorithms in reducing the total AoI. The\nabove observations indicate that the proposed algorithm wi th\n12\n10 15 20 25 30 35 40 45\nM\n0\n10000\n20000\n30000\n40000\n50000\n60000AoI (second)\nTWA*-sampling\nTWA*-beam search\nTWA*-greedy\nPtr-A*\nGenetic\nSA\n(a) Comparison of the total AoI when M varies.\n10 15 20 25 30 35 40 45\nM\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500AoI of the oldest packet (second)\nTWA*-sampling\nTWA*-beam search\nTWA*-greedy\nPtr-A*\nGenetic\nSA\n(b) Comparison of the oldest packet’ s AoI when M varies.\nFig. 5. Comparison when M varies.\n10 15 20 25 30 35 40\nM\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nEnergy consumption ratio normalized \n to TWA*-sampling\nTWA*-sampling\nTWA*-beam search\nTWA*-greedy\nGenetic\nSA\nFig. 6. Comparison of energy consumption when M varies.\nthe three different decoding methods achieves an excellent\ngeneralization ability with respect to the size of the IoT\nnetwork used for training. When M = 10, TW A*-sampling,\nTW A*-beam search, TW A*-greedy, Ptr-A*, and the genetic\nalgorithms obtain almost the same total AoI; however, the\nSA algorithm has a higher total AoI when compared to our\nproposed algorithm with all three decoding strategies. As t he\nvalue of M increases, the performance gap increases gradually\nbetween our proposed algorithm and comparison algorithms.\nFor instance, when M = 25, the total AoI values of the\nTW A*-sampling, TW A*-beam search, TW A*-greedy, Ptr-A*,\ngenetic, and SA algorithms are 13134, 13134, 13546, 13431,\n15205, and 15452 seconds, respectively. Compared to Ptr-\nA*, TW A*-sampling has a performance gain of 2. 2%. As M\nincreases to 45, the total AoI values obtained by the TW A*-\nsampling, TW A*-beam search, and TW A*-greedy algorithms\nare 42803, 43971, and 46118 seconds, respectively. Compare d\nto Ptr-A* with a AoI value of 45663 seconds, TW A*-sampling\nhas a performance gain of 6. 3%. However, the total AoI\nvalues of the genetic and SA algorithms are 54061 and\n59537 seconds, respectively, which are obviously inferior than\nwhat obtained by our proposed algorithm. In summary, our\nproposed algorithm using any of the three decoding methods\ncan obtain better total AoI results than both the genetic and\nSA algorithms. In addition, TW A*-sampling always obtains\nbetter AoI values than Ptr-A* with the sampling strategy. Th is\ncomparison result is consistent with the conclusions in [25 ]\nand [27] that the transformer-based technique outperforms the\npointer network-based technique.\nNext, we compare the AoI of the oldest packet which is from\nthe node b(1)\nπ (1) that samples data ﬁrst in the whole IoT network,\namong different algorithms. As can be seen in Fig. 5 (b),\nour proposed algorithm also exhibits a good performance in\nreducing the AoI of the oldest packet when compared with the\ngenetic and SA algorithms. Furthermore, the TW A*-sampling\nalgorithm obtains the best results among the three decoding\nmethods and Ptr-A*.\nGiven that the proposed algorithm can ﬁnd the best UA V tra-\njectory with the minimal AoI in the UA V -IoT network among\nall the algorithms under comparison, it is of interest to fur ther\ninvestigate the effective energy consumption of the UA V . It\ncan be seen from (8) and (11) that the energy consumption\nof the UA V is related to its ﬂying time and hovering time.\nThe effective energy consumption is deﬁned as the energy\nconsumption of the UA V from the ﬁrst visited hovering point\nto the end point, i.e., in completing its data collection tas k.\nFig. 6 compares the effective energy consumptions for all th e\nalgorithms for different values of M. In particular, plotted\nin the ﬁgure are the average ratios of the effective energy\nconsumptions by different algorithms with over that of the\nTW A*-sampling algorithm. As can be seen, our proposed\nalgorithm with any of the three decoding methods has a better\nperformance when compared to the genetic and SA algorithms,\nwhereas the TW A*-sampling algorithm obtains the best resul t.\nThe results in Fig. 6 are in line with expectations because wi th\nour proposed algorithm, the UA V spends less time to gather\ndata than with the other two algorithms, which helps to reduc e\nthe effective energy consumption of the UA V .\nT able II compares the running time at inference. As M\nincreases, the running time of all the algorithms increases ,\nwhich is well expected. W e can see that among all the\nalgorithms and for all the values of M, the running time of the\nTW A*-greedy algorithm is always shortest. Although TW A*-\nsampling obtains the best performance in reducing the total\nAoI as well as the AoI of the oldest packet (as can be seen\nfrom Fig. 5 (a) and Fig. 5 (b)), it has a longer running time tha n\nTW A*-greedy, which is reasonable. Similarly, TW A*-beam\nsearch takes slightly more time than TW A*-greedy because it\n13\nT ABLE II\nCO M PA R I S O N O F RU N N I N G T I M E (S E C O N D ).\nAlgorithm\nM TW A*-sampling TW A*-beam search TW A*-greedy Ptr-A* Genetic SA\n10 1.9693 2.0653 1.9556 11.1556 47.57 5.5345\n15 2.1412 2.2861 2.1055 19.3212 95.46 6.4262\n20 2.3392 2.4900 2.3037 27.9023 163.41 6.8623\n25 2.6006 2.8087 2.5778 36.9560 261.69 7.4619\n30 2.8700 3.0876 2.8300 43.5498 392.97 8.3378\n35 3.2018 3.4531 3.1536 52.4981 562.25 9.2576\n40 3.8059 3.7506 3.5583 61.6301 749.16 9.6988\n45 4.5112 4.5190 3.8995 75.8817 991.85 10.2019\n10 20 30 40\nM\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000AoI (second)\n30 dB\n20 dB\n10 dB\n(a) Comparison of the total AoI for different values of γth .\n10 20 30 40\nM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentages of total flying time and total hovering time \n contributing to the AoI value of the oldest packet\nFlying time (30dB)\nHovering time (30dB)\nFlying time (20dB)\nHovering time (20dB)\nFlying time (10dB)\nHovering time (10dB)\n(b) Percentages of the total ﬂying time and the total hoverin g time\nfor different values of γth .\nFig. 7. Comparison for different values of γth .\nneeds more computational time to search for a better solutio n\nthan TW A*-greedy. W e can observe that the genetic algorithm\ntakes the longest time among all the algorithms and its runni ng\ntime signiﬁcantly increases as M increases. The running time\nof SA is acceptable in comparison with the genetic algorithm .\nOverall, the computational performance of our proposed mod el\nwith all three decoding methods is signiﬁcantly better than\nthe SA and genetic algorithms. The running time of Ptr-A*\nis much greater than that of TW A*-sampling, TW A*-beam\nsearch, and TW A*-greedy. For example, when M = 20, the\nrunning time of the Ptr-A* is 11.9 times that of TW A*-\nsampling. When M increases to 45, the running time of\nthe Ptr-A* is 16.8 times that of TW A*-sampling. This is\nbecause transformer-based techniques can process a sequen ce\nin parallel. However, with Ptr-A*, the elements of a sequenc e\nmust be processed one by one. Hence, our proposed TW A*\nwith three decoding methods is faster than Ptr-A*.\nIn order to provide insights about the effect of γth on\nthe total AoI, we set the same number of devices, namely\nNm = 20, in each ground cluster and evaluate in Fig. 7 (a)\nthe performance of TW A*-sampling for different values of γth .\nAccording to Lemma 1 and (6), the smaller the value of γth\nis, the larger the area of each hovering disk Om will be. This\nwill affect the positions of hovering points and thus the tot al\nAoI. As we can see in Fig. 7 (a), for any given number of\nground clusters, the total AoI gradually increases as the va lue\nof γth decreases. For example, when M = 30, the values of\ntotal AoI in 10 dB, 20 dB, and 30 dB are 22707, 21655, and\n20457 seconds, respectively. W e can also observe that the to tal\nAoI gap among three values of γth increases as γth becomes\nhigher.\nNext, we compare the total ﬂying time and the total hovering\ntime of the UA V that make up of the AoI value A(1)\nπ (1)\nof the oldest packet in networks with different number of\nclusters. Speciﬁcally these total ﬂying time and total hove ring\ntime of the UA V are calculated as ∑ M\nt=1 T (ﬂy)\n(cπ (t),c π (t+1)) and\n∑ M\nt=1 T (hov)\ncπ (t) , respectively. In each network, we also compare\nthese portions of time when γth varies. Note that, the AoI\nvalues of the oldest packet are different for different γth values\nin a network. The percentages of the total hovering and the\ntotal ﬂying time that contribute to the AoI value of the oldes t\npacket are plotted in Fig. 7 (b). When M = 10, the total\nﬂying time is always higher than the total hovering time for\nany thresholds γth . In addition, as γth increases, the ﬂying time\nportion increases. This is because the selected hovering po int\nmay be closer to the center of each hovering disk if the value\nof γth is large, which will cause the ﬂight distance to be longer\nand thus increases the total ﬂying time. When M increases,\nthe UA V needs more time to collect data, and we can see that\nthe hovering time portion slowly increases as expected.\nFig. 8 (a) compares the total AoI for different algorithms an d\nfor different numbers of devices in each cluster. W e test the\ntrained model on a 20-clusters instance with the same number\nof devices in each cluster. It can be seen that the proposed\nalgorithm with the sampling decoding method always obtains\nthe minimal values when compared to the genetic and SA\n14\n5 10 15 20 25 30\nN\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000AoI (second)\nTWA*-sampling\nGenetic\nSA\n(a) Comparison of the total AoI when N varies\n5 10 15 20 25 30\nN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPercentages of total flying time and total hovering time \n contributing to the AoI value of the oldest packet\nFlying time (TWA*-sampling)\nHovering time (TWA*-sampling)\nFlying time (Genetic)\nHovering time (Genetic)\nFlying time (SA)\nHovering time (SA)\n(b) Percentages of ﬂying time and hovering time when N varies.\nFig. 8. Comparison when N varies.\nalgorithms. This clearly shows that our proposed algorithm\ncan ﬁnd a better trajectory in reducing the total AoI. As N\nincreases, there is a large performance gap between TW A*-\nsampling and the two algorithms under comparison.\nFig. 8 (b) plots the percentages of the total ﬂying time and\nthe total hovering time that make up of the AoI value of the\noldest packet in the 20-clusters network when N varies. For\nall the algorithms considered in Fig. 8 (b), as N increases, the\npercentage of the total hovering time gradually increases. This\ntrend is justiﬁed because the UA V needs more time to collect\ndata from a larger number of ground nodes.\nV . C O N CL U S IO N S\nIn this paper, we have investigated and solved the problem\nof AoI-oriented data collection in UA V -enabled cluster-ba sed\nIoT networks. With the aim of minimizing the total AoI of\nthe collected data, we formulated the trajectory optimizat ion\nproblem as the GTSP by jointly optimizing the selection of\nhovering points of the UA V and the visiting order to these\nhovering points. T o solve the formulated problem, we design ed\na novel algorithm framework based on the state-of-the-art\ntransformer. In particular, the formulated trajectory pla nning\nproblem is viewed as a “translation problem”. The whole UA V -\nIoT network serves as the “source language” to the proposed\nmodel and the “target language” of the model is the UA V’s\ntrajectory with the minimal total AoI, where the transforme r\nis utilized to generate the visiting order and the weighted\nA* is used to quickly ﬁnd the hovering points. The proposed\nmodel is trained by reinforcement learning to learn a trajec tory\nplanning policy. Comprehensive experiments were conducte d\nto evaluate the performance of the proposed algorithm. The\nobtained simulation results showed that the learned policy\nby the proposed algorithm has a strong generalization abili ty.\nWhen compared with other algorithms, our proposed algorith m\nwith three different decoding methods not only reduces the\ntotal AoI, but also reduces the AoI of the oldest packet and\nthe effective energy consumption of the UA V . Moreover, our\nmethod also has lower computation time. In future, we plan\nto extend the system model and the proposed algorithm to the\nmultiple UA Vs-assisted IoT network.\nAP P E N D IX A\nPRO O F O F LE M M A 1\nBy substituting (1)–(4) into (5), we can get the formu-\nlation (A.1) shown on top of the next page. For a ﬁxed\nH, 20 log10\n(\n4πfc\n√\nH2 + R2\n(cm,b m)/c\n)\nis monotonically in-\ncreasing with respect to R(cm,b m). As the analysis in [29]\nshows, P (LoS)\ncm is monotonically increasing with respect to\nθcm . Since θcm = arctan(H/R (cm,b m)), P (LoS)\ncm is mono-\ntonically decreasing with respect to R(cm,b m) for a ﬁxed\nH. Then, (ξLoS −ξNLoS )\n/(\n1 +β exp\n(\n−˜β (θcm −β)\n))\nis\nmonotonically increasing with respect to R(cm,b m) because\nξLoS < ξ NLoS . Finally, we arrive at the conclusion that the\nleft side of (A.1) is monotonically decreasing with respect to\nR(cm,b m) for a ﬁxed H. When the SNR γcm decreases to the\nthreshold γth , we can obtain the maximum R∗. Hence, for\nany cm, the UA V can successfully receive data from bm if\nR(cm,b m) ≤R∗.\nRE F E RE N CE S\n[1] S. Hu, X. Chen, W . Ni, E. Hossain, and X. W ang, “Distribute d machine\nlearning for wireless communication networks: T echniques , architec-\ntures, and applications, ” IEEE Communications Surveys & Tutorials ,\nvol. 23, pp. 1458–1493, Jun. 2021.\n[2] M. Mozaffari, W . Saad, M. Bennis, Y . Nam, and M. Debbah, “ A tutorial\non UA Vs for wireless networks: Applications, challenges, a nd open\nproblems, ” IEEE Communications Surveys & Tutorials , vol. 21, no. 3,\npp. 2334–2360, Mar . 2019.\n[3] S. Hu, W . Ni, X. W ang, A. Jamalipour, and D. T a, “Joint opti mization of\ntrajectory , propulsion, and thrust powers for covert UA V -o n-UA V video\ntracking and surveillance, ” IEEE Transactions on Information F orensics\nand Security , vol. 16, pp. 1959–1972, 2021.\n[4] S. Hu, Q. Wu, and X. W ang, “Energy management and trajecto ry\noptimization for UA V-enabled legitimate monitoring syste ms, ” IEEE\nTransactions on W ireless Communications , vol. 20, no. 1, pp. 142–155,\nJan. 2021.\n[5] B. Zhu, E. Bedeer, H. H. Nguyen, R. Barton, and J. Henry , “U A V\ntrajectory planning in wireless sensor networks for energy consumption\nminimization by deep reinforcement learning, ” IEEE Transactions on\nV ehicular T echnology, vol. 70, no. 9, pp. 9540–9554, Sep. 2021.\n[6] S. Zhang, H. Zhang, Z. Han, H. V . Poor, and L. Song, “ Age of i nforma-\ntion in a cellular internet of UA Vs: Sensing and communicati on trade-off\ndesign, ” IEEE Transactions on W ireless Communications , vol. 19, pp.\n6578–6592, Oct. 2020.\n[7] S. Kaul, R. Y ates, and M. Gruteser, “Real-time status: Ho w often should\none update, ” in Proc. IEEE Conference on Computer Communications\n(INFOCOM), Mar . 2012, pp. 2731–2735.\n15\nPCH\n1\nσ 2\n(\nP (LoS)\ncm L(LoS)\ncm +\n(\n1 − P (LoS)\ncm\n)\nL(NLoS)\ncm\n) ≥ γth ,\nPCH\n1\nσ 2\n(\n20 log10\n( 4πf cd(cm,b m)\nvlight\n)\n+ P (LoS)\ncm (ξLoS − ξNLoS ) +ξNLoS\n) ≥ γth ,\nPCH\n1\nσ 2\n(\n20 log10\n(\n4πf c\n√\nH2+R2\n(cm,b m)\nvlight\n)\n+ ξLoS − ξNLoS\n1+β exp (− ˜β (θcm − β )) + ξNLoS\n) ≥ γth . (A.1)\n[8] H. Hu, K. Xiong, G. Qu, Q. Ni, P . Fan, and K. B. Letaief, “AoI -minimal\ntrajectory planning and data collection in UA V-assisted wi reless powered\nIoT networks, ” IEEE Internet of Things Journal , vol. 8, no. 2, pp. 1211–\n1223, Jan. 2021.\n[9] J. Liu, X. W ang, B. Bai, and H. Dai, “ Age-optimal trajecto ry planning for\nUA V-assisted data collection, ” in Proc. IEEE Conference on Computer\nCommunications W orkshops (INFOCOM) , Apr . 2018, pp. 553–558.\n[10] J. Liu, P . T ong, X. W ang, B. Bai, and H. Dai, “UA V-aided da ta\ncollection for information freshness in wireless sensor ne tworks, ” IEEE\nTransactions on W ireless Communications , vol. 20, no. 4, pp. 2368–\n2382, Apr . 2021.\n[11] Z. Jia, X. Qin, Z. W ang, and B. Liu, “ Age-based path plann ing and data\nacquisition in UA V-assisted IoT networks, ” in Proc. IEEE International\nConference on Communications W orkshops (ICC) , May 2019, pp. 1–6.\n[12] M. A. Abd-Elmagid and H. S. Dhillon, “ A verage peak age-o f-\ninformation minimization in UA V-assisted IoT networks, ” IEEE Trans-\nactions on V ehicular T echnology , vol. 68, no. 2, pp. 2003–2008, Feb.\n2019.\n[13] S. F . Abedin, M. S. Munir, N. H. Tran, Z. Han, and C. S. Hong ,\n“Data freshness and energy-efﬁcient UA V navigation optimi zation: A\ndeep reinforcement learning approach, ” IEEE Transactions on Intelligent\nTransportation Systems , vol. 22, no. 9, pp. 5994–6006, Sep. 2021.\n[14] M. Y i, X. W ang, J. Liu, Y . Zhang, and B. Bai, “Deep reinfor cement\nlearning for fresh data collection in UA V-assisted IoT netw orks, ” in\nProc. IEEE Conference on Computer Communications W orkshop s (IN-\nFOCOM), Jul. 2020, pp. 716–721.\n[15] D. Ebrahimi, S. Sharafeddine, P . Ho, and C. Assi, “UA V-a ided\nprojection-based compressive data gathering in wireless s ensor net-\nworks, ” IEEE Internet of Things Journal , vol. 6, no. 2, pp. 1893–1905,\nApr . 2019.\n[16] Y . Y uan, L. Lei, T . X. V u, S. Chatzinotas, S. Sun, and B. Ot tersten,\n“Energy minimization in UA V-aided networks: Actor-critic learning for\nconstrained scheduling optimization, ” IEEE Transactions on V ehicular\nT echnology, vol. 70, no. 5, pp. 5028–5042, May 2021.\n[17] L. Shen, N. W ang, Z. Zhu, Y . Fan, X. Ji, and X. Mu, “UA V-ena bled data\ncollection for mMTC networks: AEM modeling and energy-efﬁc ient tra-\njectory design, ” in IEEE International Conference on Communications\n(ICC), 2020, pp. 1–6.\n[18] J. T . Isaacs and J. P . Hespanha, “Dubins traveling sales man problem with\nneighborhoods: A graph-based approach, ” Algorithms, vol. 6, no. 1, pp.\n84–99, Feb. 2013.\n[19] R. P ˇ eniˇ cka, J. Faigl, P . V ´ a ˇ na, and M. Saska, “Dubins orienteering\nproblem with neighborhoods, ” in Proc. IEEE International Conference\non Unmanned Aircraft Systems (ICUAS) , Jun. 2017, pp. 1555–1562.\n[20] M. A. Abd-Elmagid, A. Ferdowsi, H. S. Dhillon, and W . Saa d, “Deep\nreinforcement learning for minimizing age-of-informatio n in UA V-\nassisted networks, ” in Proc. IEEE Global Communications Conference ,\nDec. 2019, pp. 1–6.\n[21] W . Li, L. W ang, and A. Fei, “Minimizing packet expiratio n loss with\npath planning in UA V-assisted data sensing, ” IEEE W ireless Communi-\ncations Letters , vol. 8, no. 6, pp. 1520–1523, Dec. 2019.\n[22] A. Ferdowsi, M. A. Abd-Elmagid, W . Saad, and H. S. Dhillo n, “Neural\ncombinatorial deep reinforcement learning for age-optima l joint trajec-\ntory and scheduling design in UA V-assisted networks, ” IEEE Journal\non Selected Areas in Communications , vol. 39, no. 5, pp. 1250–1265,\nMay 2021.\n[23] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jone s, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “ Attention is all you need, ” in Proc.\nAdvances in Neural Information Processing Systems (NIPS) , Dec. 2017,\npp. 5998–6008.\n[24] M. Deudon, P . Cournut, A. Lacoste, Y . Adulyasak, and L.- M. Rousseau,\n“Learning heuristics for the TSP by policy gradient, ” in Proc. Inter-\nnational Conference on the Integration of Constraint Progr amming,\nArtiﬁcial Intelligence, and Operations Research (CP AIOR) , Jun. 2018,\npp. 170–181.\n[25] W . Kool, H. van Hoof, and M. W elling, “ Attention, learn t o solve\nrouting problems!” in Proc. International Conference on Learning\nRepresentations (ICLR) , May 2019, pp. 1–25.\n[26] Y . Wu, W . Song, Z. Cao, J. Zhang, and A. Lim, “Learning imp rovement\nheuristics for solving routing problems, ” IEEE Transactions on Neural\nNetworks and Learning Systems , pp. 1–13, Apr . 2021.\n[27] X. Bresson and T . Laurent, “The transformer network for the traveling\nsalesman problem, ” arXiv preprint arXiv:2103.03012 , 2021.\n[28] Y . Sun, E. Uysal-Biyikoglu, R. D. Y ates, C. E. Koksal, an d N. B. Shroff,\n“Update or wait: How to keep your data fresh, ” IEEE Transactions on\nInformation Theory , vol. 63, no. 11, pp. 7492–7508, Nov . 2017.\n[29] A. Al Hourani, S. Kandeepan, and S. Lardner, “Optimal LA P altitude\nfor maximum coverage, ” IEEE W ireless Communications Letters , vol. 3,\nno. 6, pp. 569–572, Dec. 2014.\n[30] J. Y ao and N. Ansari, “Qos-aware power control in intern et of drones\nfor data collection service, ” IEEE Transactions on V ehicular T echnology ,\nvol. 68, no. 7, pp. 6649–6656, Jul. 2019.\n[31] Y . Zeng, J. Xu, and R. Zhang, “Energy minimization for wi reless\ncommunication with rotary-wing UA V, ” IEEE Transactions on W ireless\nCommunications, vol. 18, no. 4, pp. 2329–2345, Apr . 2019.\n[32] P . T ong, J. Liu, X. W ang, B. Bai, and H. Dai, “UA V-enabled age-optimal\ndata collection in wireless sensor networks, ” in Proc. IEEE International\nConference on Communications W orkshops (ICC) , May 2019, pp. 1–6.\n[33] A. Dumitrescu and J. S. Mitchell, “ Approximation algor ithms for TSP\nwith neighborhoods in the plane, ” Journal of Algorithms , vol. 48, no. 1,\npp. 135–159, Aug. 2003.\n[34] B. Y uan, M. Orlowska, and S. Sadiq, “On the optimal robot routing\nproblem in wireless sensor networks, ” IEEE Transactions on Knowledge\nand Data Engineering , vol. 19, no. 9, pp. 1252–1261, Sep. 2007.\n[35] Concorde, [Online]. A vailable: http://www .math.uwa terloo.ca/tsp/concor\n-de.html.\n[36] T . Lin, Y . W ang, X. Liu, and X. Qiu, “ A survey of transform ers, ” arXiv\npreprint arXiv:2106.04554 , 2021.\n[37] R. Ebendt and R. Drechsler, “W eighted A search unifying view and\napplication, ” Artiﬁcial Intelligence , vol. 173, no. 14, pp. 1310–1342,\nSep. 2009.\n[38] G. Ramalingam and T . Reps, “On the computational comple xity of\ndynamic graph problems, ” Theoretical Computer Science , vol. 158,\nno. 1, pp. 233–277, May 1996.\n[39] R. J. Williams, “Simple statistical gradient-followi ng algorithms for\nconnectionist reinforcement learning, ” Machine Learning , vol. 8, no. 3,\npp. 229–256, 1992.\n[40] J. Y ang, C. Wu, H. P . Lee, and Y . Liang, “Solving travelin g salesman\nproblems using generalized chromosome genetic algorithm, ” Progress\nin Natural Science , vol. 18, no. 7, pp. 887–892, 2008.\n[41] S. Zhan, J. Lin, Z. Zhang, and Y . Zhong, “List-based simu lated annealing\nalgorithm for traveling salesman problem, ” Computational Intelligence\nand Neuroscience , vol. 2016, 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7685080766677856
    },
    {
      "name": "Trajectory",
      "score": 0.5583996772766113
    },
    {
      "name": "Real-time computing",
      "score": 0.5575352311134338
    },
    {
      "name": "Data collection",
      "score": 0.4997832775115967
    },
    {
      "name": "Transformer",
      "score": 0.4933992922306061
    },
    {
      "name": "Internet of Things",
      "score": 0.4810807406902313
    },
    {
      "name": "Computer network",
      "score": 0.3865766227245331
    },
    {
      "name": "Embedded system",
      "score": 0.2750542163848877
    },
    {
      "name": "Electrical engineering",
      "score": 0.12374338507652283
    },
    {
      "name": "Voltage",
      "score": 0.10316920280456543
    },
    {
      "name": "Engineering",
      "score": 0.08773547410964966
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32625721",
      "name": "University of Saskatchewan",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I135428043",
      "name": "Cisco Systems (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    }
  ]
}