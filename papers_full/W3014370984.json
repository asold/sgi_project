{
    "title": "Scalable diagnostic screening of mild cognitive impairment using AI dialogue agent",
    "url": "https://openalex.org/W3014370984",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2563482067",
            "name": "Fengyi Tang",
            "affiliations": [
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A2788112091",
            "name": "Ikechukwu Uchendu",
            "affiliations": [
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A1983569093",
            "name": "Fei Wang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2144697054",
            "name": "Hiroko H. Dodge",
            "affiliations": [
                "Oregon Health & Science University"
            ]
        },
        {
            "id": "https://openalex.org/A2097879502",
            "name": "Jiayu Zhou",
            "affiliations": [
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A2563482067",
            "name": "Fengyi Tang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2788112091",
            "name": "Ikechukwu Uchendu",
            "affiliations": [
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A1983569093",
            "name": "Fei Wang",
            "affiliations": [
                "Cornell University",
                "Michigan State University"
            ]
        },
        {
            "id": "https://openalex.org/A2144697054",
            "name": "Hiroko H. Dodge",
            "affiliations": [
                "Oregon Health & Science University"
            ]
        },
        {
            "id": "https://openalex.org/A2097879502",
            "name": "Jiayu Zhou",
            "affiliations": [
                "Michigan State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2015514982",
        "https://openalex.org/W2142367912",
        "https://openalex.org/W2130126582",
        "https://openalex.org/W2167840686",
        "https://openalex.org/W1513935208",
        "https://openalex.org/W2440640648",
        "https://openalex.org/W2345669831",
        "https://openalex.org/W2796681735",
        "https://openalex.org/W2798054687",
        "https://openalex.org/W2594844191",
        "https://openalex.org/W1853705225",
        "https://openalex.org/W2963044149",
        "https://openalex.org/W2136914353",
        "https://openalex.org/W1983349802",
        "https://openalex.org/W2089109585",
        "https://openalex.org/W2233997862",
        "https://openalex.org/W4290742115",
        "https://openalex.org/W2168490009",
        "https://openalex.org/W4236362309",
        "https://openalex.org/W1937723447",
        "https://openalex.org/W2125593110",
        "https://openalex.org/W2115015491",
        "https://openalex.org/W2538467637",
        "https://openalex.org/W2055537935",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W2395579298",
        "https://openalex.org/W2145339207",
        "https://openalex.org/W2344449162",
        "https://openalex.org/W4231416775",
        "https://openalex.org/W1755363965",
        "https://openalex.org/W2806876144",
        "https://openalex.org/W2273088453",
        "https://openalex.org/W2017541761",
        "https://openalex.org/W1786709202",
        "https://openalex.org/W2485112130",
        "https://openalex.org/W2605584187",
        "https://openalex.org/W3121541553",
        "https://openalex.org/W2289552935",
        "https://openalex.org/W2119567691",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2046629104",
        "https://openalex.org/W607505555",
        "https://openalex.org/W2911283634"
    ],
    "abstract": null,
    "full_text": "1Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreports\nScalable diagnostic screening of \nmild cognitive impairment using Ai \ndialogue agent\nfengyi tang1,2, Ikechukwu Uchendu1, Fei Wang3, Hiroko H. Dodge4 & Jiayu Zhou1*\nThe search for early biomarkers of mild cognitive impairment (MCI) has been central to the Alzheimer’s \nDisease (AD) and dementia research community in recent years. To identify MCI status at the earliest \npossible point, recent studies have shown that linguistic markers such as word choice, utterance and \nsentence structures can potentially serve as preclinical behavioral markers. Here we present an adaptive \ndialogue algorithm (an AI-enabled dialogue agent) to identify sequences of questions (a dialogue \npolicy) that distinguish MCI from normal (NL) cognitive status. Our AI agent adapts its questioning \nstrategy based on the user’s previous responses to reach an individualized conversational strategy per \nuser. Because the AI agent is adaptive and scales favorably with additional data, our method provides \na potential avenue for large-scale preclinical screening of neurocognitive decline as a new digital \nbiomarker, as well as longitudinal tracking of aging patterns in the outpatient setting.\nThe search for early biomarkers of mild cognitive impairment (MCI) has been central to Alzheimer’s Disease \n(AD) and dementia research community in recent years. While there exists in-vivo biomarkers (e.g., beta amyloid \nand tau) that can serve as indicators of pathological progression toward AD, biomarker screenings are prohibi-\ntively expensive to scale if widely used among pre-symptomatic individuals in the outpatient setting\n1. Classically, \nthe structural magnetic resonance imaging (MRI) modality has been shown to capture a set of physiologic mark-\ners in the AD pathological process\n2,3. However, the identification of MCI from normal aging (NL) is challenging \nwith MRI due to the fact that structural changes in the brain at this phase are minor and hard to detect4. Although \nrecent studies5–7 have shown that inferring structural connections among brain regions may provide promising \nresults of MCI detection, they generally involve identifying structural changes that proceed from the point where \nclinical changes (i.e., physiologic changes from cognitive decline) have already occurred.\nTo identify MCI status at the earliest possible point, the feasibility of obtaining preclinical markers (i.e., before \nthe onset of detectable physiologic changes) needs to be investigated. We note that in this study, we use the term \n“preclinical” to identify those who are likely to receive clinical diagnosis of Alzheimer’s Disease in the future, not \nnecessarily based on the amyloid deposition, pathological tau, and neurodegeneration (ATN) biomarker-based \nframework presented in Jack et al.\n8 Specifically, we are interested in identifying those who are currently clinically \nnormal but are on the trajectory to develop MCI in the near future. However, the identification of such develop-\nmental trajectories requires that we first identify a set of invariant markers that can distinguish MCI patients from \nnormal aging crosssectionally.\nIdeally, such markers should be inexpensive to obtain and scalable to applications outside of the clinical set-\nting. Fortunately, recent studies have shown that simple linguistic markers  such as word choice, phrasing (i.e., \n“utterance”) and short speech patterns possess predictive power in assessing MCI status in the elderly popula-\ntion\n9. Note that this is quite different from “speech markers” that involve auditory changes in pronunciations10–12 \nwhich reflect early symptomatic changes in speech generation. Behavior and social markers such as language, \nspeech and conversational behaviors reflect cognitive changes that may precede physiological changes and offer \na much more cost-effective option for preclinical MCI detection\n13,14, especially if they can be extracted from \na non-clinical setting. However, extensive semi-structured conversation on the scale of several hours may be \nrequired to obtain reliable linguistic markers of MCI, as shown in the previous study\n9. In the current study, we are \n1Department of Computer Science and Engineering, Michigan State University College of Engineering, East Lansing, \nUSA. 2College of Osteopathic Medicine, Michigan State University, East Lansing, USA. 3Department of Healthcare \nPolicy and Research, Weill Cornell Medical School, Cornell University, New York, USA. 4Layton Aging and Alzheimer’s \nDisease Center, Department of Neurology, Oregon Health & Science University, Portland, USA. *email: jiayuz@\nmsu.edu\nopen\n2Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\ninterested in not only identifying linguistic markers which distinguish MCI from those with normal cognition, \nbut also in proposing methods that can compose new sets of questions capable of identifying MCI with only a few \nconversational turns with the participants.\nWe develop a prototype AI dialogue agent that conducts screening conversations with participants. \nSpecifically, this AI agent must learn to ask the specific sequence of questions that are more likely to elicit \nresponses containing linguistic markers that distinguish MCI form normal (NL) aging. We propose a reinforce-\nment learning (RL)\n15 pipeline, along with a dialogue simulation environment15–17, to provide a training ground \nfor the AI agent to explore over a range of semi-structured questions. The dialogue agent arrives at an optimal \ndialogue strategy – we call it a “policy”– for conducting adaptive conversations with users.\nThe proposed framework thus provides a potentially cost-effective and scalable way of screening the aging \npopulation for MCI-risk in an individualized manner. Our statistical learning approach leverages a principled \nway of minimizing generalization error\n18 which allows the agent to handle a diverse set of user conversational \nstyles. Additionally, unlike classical supervised learning 18, we incorporate a feedback loop  between the RL and \ndialogue simulation module to allow rapid adaptation to unseen users without prior knowledge of their dialogue \ntendencies. In experiments, we demonstrate proof-of-concept results using cross-sectional data from a completed \nbehavioral intervention trial. Results demonstrate that such an approach provides a potential avenue for longitu-\ndinal tracking of aging patterns through strategic and data-driven dialogue.\nMethods\nStudy design and participants.  We train and validate our AI dialogue agent based on transcribed data \nfrom a randomized controlled behavioral intervention NIH funded study (R01AG033581, ClinicalTrials.gov : \nNCT01571427) which was completed in 201419. Briefly social isolation or lack of social interactions were found \nto be risk factors of dementia in epidemiological studies 20–22. Therefore, this behavioral randomized controlled \nintervention trial aimed to examine whether increasing social interactions through video-chat conversations \nimprove cognitive functions. User-friendly video-chat devices were created specifically for this project in order \nto reduce the effect derived from the stimulation of learning how to operate the device. The experimental group \nengaged in video-chat conversations with trained conversational staff for 30 minutes, 5 times per week for 6 \nweeks. Control group received only weekly 10 minutes phone check-in to monitor their social engagement activ-\nities and improve their retention. Participants were assessed by a full battery of neuropsychological tests used \nin all National Institute of Health (NIH)-funded Alzheimer’s Disease Centers in the United States (National \nAlzheimer’s Coordinating Center Uniform Data Set Version 2) at baseline and received clinical diagnosis by \nclinicians. To be eligible, the participants have to be at least 70 years old and free from frank dementia (mean \nage 80.43, 71% women, average years of education 15.7). Out of 83 subjects who completed the trial, we use the \ntranscribed data from 41 subjects (14 MCI, 27 NL) in the current study who consented to have conversations \ntranscribed and shared among researchers. There were no significant differences in age, gender, education and \nmarital status between the group who consented to use their recorded conversation for this research study com-\npared to those who did not. A subset of the total conversations (2.81 conversational episodes per participant) were \ntranscribed and used for the current study. Basic characteristics of participants, inclusion and exclusion criteria are \nsummarized in Supplementary Table 1.\nConversation structure and preprocessing. The conversational transcripts were first processed into \nutterances, which are unstructured responses to questions provided by interviewers. The interview questions were \ngenerated from a pool of over 150 possible questions which are organized into the following categories: trans -\nportation, childhood, cities, entertainment, family, personal preferences, gifts and celebrations, health problems, past \njobs, spouse, social opinions, politics, comment about photographic images, and significant relationships. During the \ninterviews, the interviewer was allowed to adapt the specific wordings of questions to the participant. However, \nthese behaviors are not accounted for in the current version of the AI conversation agent.\nFor the purposes of this study, we re-compiled the question list into 107 general questions which were ubiq-\nuitous across all conversations. For some of the questions, we delexicalised certain topic words such as “<activ-\nity > ”,  “<social topic > ” , whereby specific nouns are replaced by contextual descriptors\n23. This is done to reduce \nthe size of the question pool without sacrificing their contextual meaning. For participant responses, we did not \nremove “stopwords” (i.e. “uhh” , “hmm”), as their usage frequencies reflect some lexical properties of the user. A \nsample of the categorical interview questions is shown in Supplementary Table 2.\nPretraining and fine-tuning skip-thought embeddings.  The benefit of using language models such \nas word- and sentence-level embeddings is that these representations can be learned using datasets outside of \nour task. The rationale is that sentence representations have some invariant properties – so called latent features \n– that can be observed and learned from compositions of multiple types (texts, dialogues, speech, pose etc)\n24. We \nutilize a pretraining + fine-tuning framework25 whereby we initialize the language models used in our dialogue \nsimulators with Skip-thought models pretrained on other corpuses. Specifically, we used pretrained encoder and \ndecoder from Kiros et al.26, which is conditioned on over 74 million sentences from the Bookcorpus dataset. We \nthen fine-tune the decoder based on dialogue responses from our data.\nMCI screening as a markov decision process. Reinforcement learning is a field within artificial intelli-\ngence (AI) research that aims to model strategic planning problems as Markov Decision Processes (MDPs)15,27. In \nthis work, we model the trajectory of conversations in our dialogue problem as a MDP . Under such a framework, \nfinding an optimal conversational policy involves decomposing the problem into a sequence of decisions, \nwhereby the goal is to obtain the best action (i.e. question to ask) at each decision step based on the current infor-\nmation state (i.e. conversational state)\n15,27. We define the MDP for our problem γR{, ,, ,}SA T  as follows:\n3Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n•\t State space : a finite space of high-dimensional vectors representing the status of the conversation. This \nhigh-dimensional vector spaces captures sentence-level properties of responses from participants. In the orig-\ninal Asgari et al . work, “conversational states” are captured by summing LIWC word vectors for user \nresponses9. Here, we encode conversational responses by training a sentence-level neural network that pro-\njects word embeddings to 4800-dimensional Skip-Thought vector s26. More details can about Skip-Thought  \n(SKP) vectors can be found in Kiros et al.26. The benefit of SKP vectors is that they capture sentence-level fea-\ntures such as semantics, grammatical structure and various word choices. In our case, we use a Skip-Thought \nencoder to project user responses, which consist of sequences of words, into a fixed length vector of 4800 \ndimensions. As the conversation progresses, we use the transitional operator  , described below, to encode \nthe SKP vectors …o o,, t1  of the user up to time t. The latent state of the transitional model represents the \ncurrent conversational state, st.\n•\t Action space : a set of discrete actions available at each state st at time t. In our case, the action space is com-\nprised of the pool of 107 categorical questions. At different stages of conversation, we designed censors over \nthe available actions (i.e. no personal questions before introductions are made) to inject common knowledge \nabout human conversations into agent’s decisions. We denote at as the question output of the AI agent at time \nt, in response to the current conversational state st.\n•\t Transition operator  : an approximate description of the dynamics of conversations27. For our problem,   is \na function which generates a response, i.e. a probability distribution over the next conversational state +st 1, \nbased on the current state st and action at and is learned directly from available data. We note here that the \n“conversational state” st at time t is different from the SKP observation of the user response at time t, which we \ndenote ot. Specifically, st contains information from previous conversational turns while ot is only the \nencoded user response at time t. st satisfies the property p ss sp ss( ,, )( )tt tt11 …= −  (Markovian property) \nwhile the SKP observations do not.\n•\t Formally, we design a recurrent neural network fo oa( ,,,) tt1 …  to model the transition between st to +st 1, \ngiven the question at:\nso Wmax{ 0, }( Initialization)T\nos00=\n=+ −zo Ws Wmax{ 0, }( Update Gate)tt\nT\noz t\nT\nsz1\n=+ −ro Ws Wmax{ 0, }( ResetG ate)tt\nT\nor t\nT\nsr1\n=+ −ho Wr sWtanh{( )} (State Update)tt\nT\noh tt\nT\nsr1\n=− ++−sz hz h(1 )) (Transition)tt tt t11 \nos Wb (Output)t\nT\nyy1 =++\nHere, the neural network Θfo a(; ;)tt1:  takes in SKP observations …o o,, t1  and outputs the next SKP \nvector response +ot 1. We denote {}W, W, ,W ,boz os yyΘ =…  as the set of weights parameterizing the \nrecurrent neural network model. Initially, o0 is the “greetings” state of the conversation, which is set to the \nSKP vector corresponding to a default greeting response (i.e., “hi”). As the conversation progresses, we \nupdate the Update, Reset, State, and Transition gates of f based on the observed SKP responses \no o,, t1 …  and \nagent questions. Thus, we denote the repeated application of f  as dialogue simulation since the recurrent \nprocess of applying ff f,, , t2 …  generates a “trajectory” of conversational responses based on the agent \nquestions …aa,, t1 . We train Θfo a(, ;)tt1:  based on the original dialogue data using the loss function:\n fo ao() 1\n2\n(, )\n(1)t\nT\ntt t\n0\n1: 1\n2\n2ˆ∑ λΘ= −+ Θ.\n=\n+\not 1+ˆ  denotes the true SKP vector observed at time t 1+  in the dialogue. This loss function trains the parameters \nΘ of the transition model f  by minimizing the distance between the predicted SKP vectors ( =+of oa(( , )tt t11 :  \nand the actual SKP vector from the conversational data +ˆo() t 1  across time, i.e., T1,,… . The λΘ2 term is a regular-\nization mechanism used to prevent overfitting on the training data when building the transition module during \ndialogue simulation. It is important to note that the transition rules differ among participants, each capturing \ndifferent personal and topic preferences that cannot be captured by the sentence-level encoding of responses \nalone. These issues are addressed in the Dialogue Simulation section below.\n•\t Reward function R : a set of rules which assigns a scalar value to each question based on response of the \nparticipant. In our MDP , we designed a per-turn penalty to limit the agent from conducting extensively long \nconversations. At the end of the conversation, we enforce a largely positive or largely negative reward, based \non the prediction accuracy to ensure that the agent is asking questions which extract the relevant features \npertaining to MCI status. More formally, we design the reward function as follows:\n4Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nτ\n=\n\n\n\n\n\n−− −\n−\n+\nRs a\nnont erminals tate\nterminal statew misclassification\nterminal statew correct status\n(, )\n1,\n50,/\n100,/\ntt\nHere, τ denotes a confidence threshold, which is defined as the probability of either the NL or MCI class \nhaving 0.65 or higher probability under the MCI classifier. The idea is to penalize the AI agent for each \nnon-terminal conversational turn (i.e., questions that keep the conversation going), especially for questions \nthat are asked while the AI agent is already “confident” in its prediction of the MCI status. This mechanism \nenforces the AI agent to be “efficient” , which is to end the conversation when it is confident in the data it \nhas collected up to time t. This efficiency mechanism is counterbalanced by a cumulative reward based on \naccuracy: a large positive reward for producing sequences of questions leading to the correct MCI \nprediction, and a large negative reward for questions leading to incorrect predictions.\nWe note here that for the terminal state reward signals, the reward function uses the MCI classifier, which \nutilizes \n∑ = ot\nT\nt1  as input (rather than st) to predict the MCI label at the end of the episode. This subtle \ndifference distinguishes the fact that st is used by the RL system for state-tracking but the MCI classifier \nuses a separate set of features o() t\nT\nt1∑ =  to generate the reward signal.\n•\t Discount factor γ: Since conversations can potentially “last forever” , a discount factor which serves to limit the \ncontribution of future expected rewards; the damping of future signals serves to balance the tradeoff between \nasking enough questions to arrive at a confident MCI prediction and limiting the questions required to get \nthere.\nDialogue simulation. In the real-world setting, human users will have differences in conversational pref-\nerence which may or may not be relevant to the underlying MCI status. The AI agent should thus be able to \nproduce individualized sets of questions which can distinguish users with speech characteristics consistent with \nMCI compared to the NL control. For example, MCI participants use relational, filler and sentiment words with \ndifferent frequencies compared to NL\n9, a pattern that can only be distilled from sequential dialogue turns with a \ngiven person.\nWe model individual differences in word choice as variance in transitional operators (i.e. ∈i  ) from our \nMDP . This was done by incrementally learning individualized transition operators …∈T T,, N1  , transferring a \nsmall subset of shared parameters from previous models … −T T,, N11  to initialize the learning of new ones TN.\nIt should be noted that the AI agent does not have knowledge of the actual user transition dynamics as the \ndialogue simulators are part of the RL environment. Thus, in order to obtain states, the agent estimates the transi-\ntion operators   by fitting the recurrent model on the observed SKP vectors using the loss function from Eq. 1. \nAt each conversational turn, the agent only receives reward signals R sa(, )tt  and observation of the response (ot) \nfrom the environment and adapts its internal model and policy network accordingly. Thus, for unseen participant \nin our dataset, the AI agent does not have prior knowledge of the linguistic tendencies of these participants when \ninitiating the conversation. At test time, we only use the dialogue simulators to produce responses to the agent \nquestions to compare to actual responses from the original dataset.\nLearning individualized conversational strategy with reinforcement learning. In reinforcement \nlearning, there are several ways to train the agent to learn an optimal policy /uni204Eπ  which solves the proposed MDP . \nA policy π can be thought of as a strategy function which assigns a decision a t() ∈  to each state ∈st  at each \ntime-step t15. While following a policy π, a value function Qπ can be formulated to reflect the expected cumulative \nrewards for the agent while strictly adhering to the given strategy15. An optimal policy /uni204Eπ  can thus be described as \na strategy which traverses the state-action trajectories in a way that maximizes the expected cumulative rewards15. \nTo solve for the optimal policy, we use Deep Q-learning (DQN), a method which leverages properties of the \nQ-function to approximate a policy with provides the highest expected cumulative reward\n28. The advantage of this \nmethod is that a neural network is used to automatically learn the set of salient linguistic features from conversa-\ntional responses which contribute to the accumulation of rewards and penalties, which in our case relate to the \nagent’s confidence over the MCI prediction accuracy, given the current conversational states, as well as the effi-\nciency of conversational turns. An overview description of our entire dialogue system can be found in Fig. 1.\nExperimental setup and statistical analysis. To evaluate the conversational strategies discovered by the \nAI agent against the original dialogue conversations, we directly assess the area under receiver operating curve \n(AUC), sensitivity and specificity of MCI predictions based on the conversations generated by the AI agent and \nthose generated by the interviewers from the original manuscript. We split the dataset into 65% training and \n35% testing, and we perform 10 randomized shuffle splits and document the confidence interval (CI) across all \ngenerated test sets.\nDue to the limited nature of the existing dialogue data, which cannot reflect responses of participants to \nnovel sequence of questions, we used the dialogue simulators to sample approximate individualized responses \nto questions posed by the AI agent. Individualized conversation simulators are widely used in the field of natural \nlanguage processing to evaluate the effectiveness of goal-oriented dialogue systems\n17. We account for the discrep-\nancy between responses generated from dialogue simulation and real responses by off-policy evaluation  in the \nproceeding sections.\nIn total, we deployed 41 dialogue simulators, corresponding to each participant in the dataset. During both the \ntraining and testing phases of reinforcement learning, the AI agent does not have access to the internal model of \n5Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\ndialogue simulators. Instead, the simulator takes in questions from the AI agent and produces a simulated \nresponse (i.e. sentences) which the AI agent uses to decide its actions. At testing phase, we use these simulators to \nestimate responses to the AI strategies π()  for each participant. The responses to the human expert conversational \nquestions π() expert  are simply the observed responses from the transcribed dialogue.\nResults\nSkip-thought representations of user responses are more predictive of MCI status. We first con-\nduct MCI predictions based on the original set of transcribed conversational data from Oregon & Health Sciences \nUniversity (OHSU)\n19. Details of the data can be found in Materials and Methods section below. Figure 2 illustrates \nthe two main statistical learning approaches compared in this study.\nHere, the classical ML approach presented in Asgari et al.9 for identifying MCI linguistic markers is denoted \nby the supervised learning pipeline. Under the supervised learning setting, all the conversational responses by a \nsubject is compiled into a corpus of words. Responses are then converted into word vectors, where each dimen-\nsion of the word vector is a {0,1} indicator of a distinguishable feature based on its grammatical usage, semantic \nmeaning and various contextual identifiers. The specifics of the word vector dimensions are manually deter -\nmined by linguists who produced the Linguistic Inquiry and Word Count (LIWC) mappings for more than 15,000 \nEnglish words commonly used in NLP studies\n29–31. ML classifiers such as support vector machines  (SVM) and \nfeed-forward neural networks (MLP)18 are then applied on the word vectors to predict the {0,1} label of the MCI \nstatus. For the SVM classifier, we used the same settings as in Asgari et al.9 which showed that the SVM classifier \nwith L1-regularization obtained the highest AUC, sensitivity and specificity for MCI prediction using the LIWC \nfeatures. When using LIWC features (69 in total), we confirm that this is the case. Table  1 shows that our SVM \nimplementation was able to achieve 0.712 AUC with confidence interval (CI) of (0.615–0.811) on 5 different ran-\ndomized shuffle splits. Our SVM results are close to the reported 0.725 AUC in the original Asgari et al. paper\n9. \nWe also see from Table 1 that a shallow neural network (MLP model with 2 layers, 512 units each) performed \npoorly using the LIWC features, with AUC of 0.689 (0.560–0.818). This is consistent with the fact that deep mod-\nels are prone to overfitting, especially with small sample sizes and simple feature representations.\nBy contrast, the inverse trend is observed when deep representations (Skip-thought vectors, denoted SKP) are \nused to represent the linguistic data. SKP representations differ from LIWC in that they are pretrained from other \nFigure 1. Feedback loop of the reinforcement learning environment for training the MCI diagnosis agent. The \nuser simulator trained from the original dialogue corpus is used to generate simulated user response to new \nquestions from the MCI diagnosis agent (i.e., the Reinforcement Learning Agent). At each conversational turn, \nthe “user state” of the simulated patient is updated based on the questions asked by the MCI diagnosis agent. We \ndesigned a Dialogue Manager which produces a reward signal to the MCI diagnosis agent based on the quality \nof questions asked.\nFigure 2. Overview of proposed algorithm for conversational generation and linguistic marker identification \nusing a RL pipeline. Supervised learning pipeline denotes the classical approach by Asgari et al.9. Our approach \nis summarized in the RL pipeline and involves a feedback loop with the MCI diagnosis agent generating \nquestions to new users for the purposes of predicting their MCI status using a trained ML classifier.\n6Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\ndatasets prior to the construction of our user simulators (see Methodology for details). Under the SKP setting, \nconversational responses are projected into 4800-dimensional Skip-Thought vectors26 that capture sentence-level \nfeatures such as semantics, grammatical structure and various word choices. A MCI classifiers are then trained \nto map Skip-Thought vector responses to the MCI labels by minimizing the binary cross-entropy loss between \nsamples\n18,26. We see from Table 1 that the MLP model achieves an AUC value of 0.811 with CI of (0.715–0.907), \nwhich is comparable to the SVM classifier 0.797 (0.719–0.879). Thus, we show here that a neural network MCI \npredictor can produce slight improvements over SVM without drastically overfitting to the training data, despite \nhigher model complexity\n32. This can be explained by the fact that SKP representations capture more complex, \nsentence-level features33, leading to a more expressive feature representation compared to the LIWC9, which only \ncaptures word-level features.\nAI-generated dialogues produce more predictive linguistic markers.  The bottom part of Fig.  2, \nlabeled Reinforcement Learning (RL) Pipeline, summarizes our approach, which consists of training an AI agent \n(i.e., MCI Diagnosis Agent) to generate questions with the user in a feedback loop in order to obtain the linguistic \nmarkers. Details of this approach will be outlined in the Materials and Methods  section below. Table 1 further \nshows various AUC, sensitivity and specificity scores of conversations produced by the AI agent when given con-\nstraints on the length of conversations. For example, RL(T = 35) means that the AI agent was given a maximum \nof 35 turns to complete the conversation with the simulated participant. This is because we noted that the aver-\nage conversation in the original corpus lasted 35 turns. However, the dialogue corpus contained on average 2.8 \n30-minute conversations per participant, conducted within 6 weeks. The average number of total dialogue turns \nper person for supervised learning (i.e. SVM and supervised DL) is 107.5, which is more than 3 times the number \nof dialogue data needed by the AI-agent to produce comparable results.\nThe last line of Table  1 shows the difference  in AUC, sensitivity and specificity scores per train-test split \nbetween RL(T = 20) and the Supervised DL model. The RL(T = 20) agent generated 20 sequential questions and \nreceived 20 responses from the dialogue simulator for each test-set participant, which the AI agent has no prior \nconversational training data. The 20 responses are used by the 2-layer neural network to predict the MCI-status \n(0 = NL, 1 = MCI) for a given participant. In contrast, the Supervised DL model uses all the available dialogue \nresponses for a given user to predict his or her MCI-status. We can see that the CI for their difference in AUC \n(−0.049–0.172), F1-score (−0.078–0.259), sensitivity (−0.083–0.410), and specificity (−0.130–0.050) all include \n0.0. Beyond 20 turns, we see that the RL policy is able to achieve increasing performance in AUC scores. We \nquantify the impact of additional conversational turns in the proceeding sections.\nThe AI policy adaptively finds the high-yield questions for unseen users. To further investigate the \nefficiency of our dialogue agent, we compared the rate of increase in the predictive power of linguistic markers as \na function of AUC-gain per added conversational turn. In Fig. 3, we observe from the slope of the AUC-gain that \nthe rate is fastest at the beginning, suggesting that the MCI diagnosis agent identifies and prioritizes the most rel-\nevant questions to assess MCI status early in the conversation. Perhaps most noteworthy is that at evaluation time, \nthe MCI diagnosis agent has never seen the new subject and has to adapt its conversational strategy on-the-fly. \nAs a result, we see that the AUC-gain curve is non-smooth: some prediction errors result from the fact that the \nnew user behaves differently than the user simulation environment from which the diagnosis agent was trained \nin. However, the AUC-gain curves also demonstrate the capacity of the MCI diagnosis agent to self-correct its \nstrategy in the face of measurement errors, in real-time, during new conversation.\nModel AUC F1-Score Sensitivity Specificity\nSVM w/ LIWC 0.712 (0.612–0.811) 0.631 (0.500–0.761) 0.680 (0.476–0.886) 0.744 (0.563–0.922)\nSupervised DL w/ LIWC 0.689 (0.560–0.818) 0.182 (0.055–0.370) 0.300 (0.010–0.758) 0.767 (0.364–0.970)\nSVM w/ SKP 0.797 (0.719–0.879) 0.719 (0.591–0.846) 0.654 (0.473–0.835) 0.939 (0.855–1.0)\nSupervised DL w/ SKP 0.811 (0.715–0.907) 0.642 (0.469–0.813) 0.600 (0.366–0.833) 0.911 (0.838–0.984)\nRL (T = 5) 0.633 (0.535–0.703) 0.486 (0.288–0.680) 0.459 (0.280–0.630) 0.811 (0.661–0.936)\nRL (T = 10) 0.741 (0.631–0.852) 0.590 (0.352–0.829) 0.560 (0.309–0.811) 0.922 (0.823–0.969)\nRL (T = 15) 0.721 (0.618–0.827) 0.595 (0.399–0.790) 0.50 (0.327–0.713) 0.922 (0.856–0.987)\nRL (T = 20) 0.809 (0.706–0.914) 0.726 (0.551–0.901) 0.620 (0.413–0.827) 0.988 (0.953–1.0)\nRL (T = 30) 0.853 (0.796–0.914) 0.801 (0.733–0.880) 0.818 (0.678–0.958) 0.898 (0.828–0.969)\nRL(T = 35) 0.859 (0.787–0.952) 0.808 (0.735–0.883) 0.818 (0.677–0.958) 0.911 (0.839–1.0)\nDifference 0.0616 (−0.049–0.172) 0.089 (−0.078–0.259) 0.163 (−0.083–0.410) −0.040 (−0.130–\n0.050)\nTable 1. Classification of MCI based on complete transcript vs. simulated conversations. Abbreviations: \nParentheses denotes confidence interval (CI) for the metric. SVM denotes support vector machines classifier, \nand Supervised DL denotes 2-layer feed-forward neural network classifier. RL denotes reinforcement learning \nagent. For feature representation of corpus, LIWC is the original word-level embedding used in Asgari et al., 8. \nSKP denotes a 4800-dimensional Skip-Thought vector embedding was used to represent each conversational \nturn. A dialogue summary is obtained by averaging across all turn-based responses for each user. We then \nevaluate the performance of our RL-agent across 10 stratified shuffle splits. Each split uses 65% of data for \ntraining and 35% for testing.\n7Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nHere, we define conversational efficiency as the number of dialogue turns required to achieve indistinguish-\nable MCI-status prediction accuracy, as measured by overlapping confidence intervals of AUC, F1, sensitivity \nand specificity scores with expert interviewers πexpert. Figure 3 illustrates that the conversational efficiency of the \nAI agent is 20, which means that the AI agent requires only 20 conversational turns to produce users’ responses \nwhose C.I. measures are consistent with π\nexpert using the original dataset.\nFurthermore, Table 2 shows the magnitude of incremental prediction improvement at 5, 10, 20, 30 and 35 \nturns between the prediction models. We bolded the rows in which the lower-bound on AUC confidence inter-\nval of the AI agent exceeds the upper-bound of the SVM confidence interval, indicating statistically significant \nimprovement for the corresponding turn-adjusted predictions. Specifically, the AI agent produced statistically \nsignificant improvements at 10, 20, and 30 turns when compared with supervised learning methods. Interestingly, \nthe neural network is also able to offer some improvement in the mean AUC scores across various turn restric-\ntions, compared to the SVM, but it does not do so in a statistically significant way compared to the AI agents.\nQuality of simulated responses depend on sentence lengths. In addition to quantitative metrics, \nwe provide some concrete comparisons of dialogue simulations and the original dialogue corpus. Table  3 pro-\nvides snapshots of 2 conversations, one with a verbose participant and one with a concise participant. In the left  \ncolumn, we see the original dialogue response to the questions posed by the interviewer. On the right column, we \nobserve predicted response by the dialogue simulation based on maximum-likelihood estimation. It is notable \nthat in short response (5–15 words), the dialogue simulation produces relatively stable responses both in terms \nof word choice as well as semantic meaning. On longer responses, as in the first user, we see that the dialogue \nsimulator generates similar sentiment words as the original response, but the topical nouns and subject references \nmay differ greatly.\nEvaluating the accuracy of dialogue simulation has long been a difficult problem in natural language \nresearch\n17. A major problem resides in the fact that word-by-word comparisons such as perplexity or BLEU score \ndoes not adequately capture changes in grammatical structure, sentiment and semantics between sentences 17. \nFor this reason, our analysis has focused mainly on predictive ability of the AI policy as a result of reinforcement \nlearning under imperfect dialogue simulation.\nAI policy leads to questions with greater cumulative rewards during off-policy evaluation. To \naccount for potential bias in our dialogue simulators, we also compared the AI conversational strategies π/uni204E against \nexpertπ  directly over cumulative rewards on the original corpus conversations. Specifically, we deploy \nhigh-confidence off-policy evaluation (HCOPE)34 to weigh the reward difference between actions taken by the AI \nFigure 3. Conversational efficiency of AI agents. The x-axis represents the number of dialogue turns elapsed. \nThe y-axis represents various performance metrics. Baseline refers to the performance of MCI classifier using \nall the responses generated from the original dataset. By contrast, RL refers to the performance of MCI classifier \nusing responses generated by the user simulator, in response to the agent-generated questions at test time.\n8Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nagent compared to the original actions in the corpus. In HCOPE, weighted importance sampling (WIS) is used to \nde-bias the rewards accumulated by the AI agent by bootstrapping the true policy values of the test set conversa-\ntional trajectories34. In this setting, we evaluate both /uni204Eπ  and expertπ  with the sample conversations from the original \ncorpus at test time, scaling their episodic rewards by the WIS factor. Table 3 summarizes the per-turn reward and \nthe per-conversation cumulative reward differences between the /uni204Eπ  and πexpert. Here, we observe that /uni204Eπ , as \ndenoted by RL(T = 35) accumulates on average 408.29 (72.41–744.17) while expertπ  accumulates on average 91.71 \n(−255.12–438.68) per conversation. Although the confidence intervals overlap, we see that both the cumulative \nadvantage (250.78–354.58), and advantage per-turn (7.16–10.13) are non-zero in the C.I. interval, suggesting that \nπ/uni204E produces significantly higher reward per action than the πexpert.\nWe note here that the interviewers in the original clinical trial were not instructed to optimize for the effi-\nciency of conversation but rather to fill the entire 30 minutes of semi-structured conversation. However, we sim-\nply demonstrate here that a goal-oriented policy is possible – i.e., that reward signals and RL training regimes can \nbe designed in a way that induce conversational strategies to survey the salient linguistic features using less time \n(as reflected in dialogue turns). By turning the original supervised learning problem into a reinforcement learning \nproblem, we allow more efficient feature sensing strategies to be discovered.\nDiscussion\nIn this work, we introduce a data-driven method for developing an automated and scalable diagnostic screening \ntool for efficient detection of early MCI status based on linguistic data. Traditionally, diagnostic predictions in the \nmedical domain are modeled as supervised learning problems, whereby diagnostic labels provided by physician \nexperts are used to guide the modeling process. By formulating the MCI screening problem as a Markov Decision \nProcess (MDP)\n15, we transform the learning task into an active sampling process by which the AI agent partici-\npates both in the data mining process (interacting with a virtual user) as well as the prediction process (assessing \nthe MCI status). The resulting AI agent obtains not only the ability to make diagnostic predictions, but also learns \nan efficient data-generating strategy for detecting the disease of interest in a natural setting.\nModel (Turns) AUC F1-Score Sensitivity Specificity\nSVM (T = 5) 0.493 (0.439–0.547) 0.169 (0.061–0.275) 0.12 (0.046–0.193) 0.860 (0.776–0.950)\nSVM (T = 10) 0.550 (0.479–0.620) 0.275 (0.113–0.428) 0.200 (0.083–0.319) 0.900 (0.820–0.970)\nSVM (T = 20) 0.624 (0.563–0.685) 0.405 (0.232–0.578) 0.360 (0.171–0.548) 0.888 (0.789–0.989)\nSVM (T = 30) 0.633 (0.557–0.707) 0.424 (0.247–0.601) 0.320 (0.187–0.458) 0.944 (0.882–1.0)\nSVM (T = 35) 0.714 (0.627–0.801) 0.576 (0.420–0.732) 0.440 (0.277–0.602) 0.968 (0.944–1.0)\nSupervised DL (T = 5) 0.497 (0.392–0.603) 0.104 (0.015–0.182) 0.111 (0.091–0.129) 0.880 (0.812–0.980)\nSupervised DL (T = 10) 0.527 (0.459–0.594) 0.278 (0.123–0.433) 0.200 (0.088–0.316) 0.933 (0.856–1.0)\nSupervised DL (T = 20) 0.673 (0.588–0.758) 0.399 (0.212–0.583) 0.320 (0.139–0.500) 0.945 (0.888 - (1.0)\nSupervised DL (T = 30) 0.720 (0.643–0.796) 0.477 (0.317–0.638) 0.360 (0.228–0.491) 0.955 (0.914–0.996)\nSupervised DL (T = 35) 0.780 (0.695–0.864) 0.490 (0.327–0.654) 0.366 (0.229–0.500) 0.966 (0.928–1.0)\nRL (T = 5) 0.633 (0.535–0.703) 0.486 (0.288–0.680) 0.459 (0.280–0.630) 0.811 (0.661–0.936)\nRL (T = 10) 0.741 (0.631–0.852) 0.590 (0.352–0.829) 0.560 (0.309–0.811) 0.922 (0.823–0.969)\nRL (T = 20) 0.809 (0.706–0.914) 0.726 (0.551–0.901) 0.620 (0.413–0.827) 0.988 (0.953–1.0)\nRL (T = 30) 0.853 (0.796–0.914) 0.801 (0.733–0.880) 0.818 (0.678–0.958) 0.898 (0.828–0.969)\nRL(T = 35) 0.859 (0.787–0.952) 0.808 (0.735–0.883) 0.818 (0.677–0.958) 0.911 (0.839–1.0)\nTable 2. MCI prediction of transcript and simulated conversations with turn restrictions. Abbreviations: SVM \ndenotes support vector machines classifier, Supervised DL denotes 2-layer feedforward neural network, and \nRL denotes reinforcement learning agent. In all three cases, conversations were cut off at various turn lengths \n(T), and performance with the classifier was performed to obtain the AUC, F1, sensitivity and specificity scores. \nConfidence intervals were obtained on 10 randomized shuffle splits for all experiments.\nPolicy Avg. Reward/Turn WIS Score DR estim./turn DR Score\nRL (T = 35) 11.68 (2.06–21.35) 408.29 (72.41–744.17) 13.10 (12.91–13.35) 458.64 (452.40–464.87)\nExpert Policy 2.62 (−7.28–12.51) 91.71 (−255.12 − 438.68) 10.82 (10.51–11.14) 379.07 (367.89–390.25)\nAdvantage 8.68 (7.16–10.13) 302.67 (250.78–354.58) — —\nTable 3. Comparison of AI and interviewer strategies using off-policy evaluation. Weighted Importance \nSampling (WIS) indicates off-policy evaluation of a given policy while sampling trajectories from the original \ndataset corpus25. For the expert policy, no importance weights are needed, and the cumulative rewards are used \nover entire conversational episodes. For the AI agent, a cut-off of 35 turns is again used to bound the length of \noff-policy trajectories. Average reward per turn is used to assess the average expected reward for the agent based \non the reward function used to train the RL agent.\n9Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nIn our experiments, we introduce a method for comparing the efficiency of AI dialogue strategy against that \nof the human interviewers. Specifically, we defined conversational efficiency, which quantifies the efficiency of \ndifferent intervention strategies (AI-simulated vs. observed) based on the AUC-gains resulting from the differ -\nent data provision strategies. Additionally, we also introduce an off-policy evaluation strategy which provides a \nlower-bound confidence on the expected performance of the AI policy compared to that of the human interview-\ners. Despite the imperfectness of the dialogue simulators, we illustrate a way to empirically assess the margin of \nerror between the AI and human policies in a per-turn and per-conversation basis.\nThere are several limitations to our study. We note that MCI does not necessarily progress to AD or demen-\ntia and there are discrepancies between pathological burden and clinical diagnoses. However, this limitation \nis precisely the motivation behind our approach: cognitive prodrome states do not have clean signals that can \ndistinguish “normal” from “pathologic” trajectories. Instead, we focus on detecting linguistic markers (text-based \nrather than acoustic, since changes in phoneme frequencies often suggest existing physiologic processes\n11), as \nthe rise of heuristic search – through statistical machine learning or otherwise – can efficiently search through a \npotentially infinite space of dialogue sequences. Unlike classical statistical learning methods, our learning frame-\nwork utilize a feedback control  loop between a RL module and simulation  to continuously improve language \nmarker identification and enable repeated predictions through interaction with the patient.\nEmpirically, the most notable limitation is the size of the cohort and the setup of our study did not permit \nus to confirm the cognitive status of participants beyond cross-sectional analyses. We were also unable to con-\nfirm biomarkers (e.g., amyloid, tau) for our participants. It would be ideal if we could apply our methods to \ndifferentiate MCIs who were destined to develop dementia from those who remained normal using longitudinal \nfollow-up. Groups with access to datasets with high-resolution and longitudinal biomarkers may be motivated to \napply our proposed framework to (a) augment feature space of their models with our proposed linguistic marker \nacquisition protocol, (c) associate discovered linguistic markers with downstream biomarkers, and (b) test the \nreproducibility of our methods at scale.\nIn terms of statistical properties, all participants are in the 70+ age group and are from geographically similar \nareas. Although the p-values in Supplemental Table 1 show that such meta-data did not contribute in a statisti-\ncally significant manner to MCI prediction, these factors inevitably introduce bias into our simulation models. We \nnote that this is also the motivation behind using simulation during evaluation: ideally, we would like to conduct \nlarge-scale active learning during test time, involving large cohorts of patients. However, this would require the \ndeployment of our framework in a real-world setting, a situation that prerequisites extensive efficacy in clinical \ntrials. As a result, we present this study as a proof-of-concept to justify clinical trial and patient recruitment which \nwould enable more powerful evaluation methods.\nAdditionally, there are several limitations with the construction and use of dialogue simulators in our exper-\niments. Firstly, we note that punctuations were removed during the process, which lead to biased results during \nthe SKP vector encodings, especially for longer sentences. However, since the original Skip-thought encoder was \ntrained on the Bookcorpus dataset that featured 13.5 words-per-sentence on average, our performance on simu-\nlating short responses were relatively stable. Secondly, we restricted the set of questions on the interviewer side to \na pool of 107 general questions (see Supplemental Table 2) that were observed across patients. In reality, the set of \npossible actions (i.e., questions) should be much richer than the one used in this preliminary study. One possible \nextension of this work is to investigate the feasibility of our hypothesis under a completely open-dialogue setting, \nwhereby the dialogue agent and the user use both use unstructured questions and answers in conversation. Going \nfrom a pool of 107 questions to open dialogue requires a different question selection mechanism to be used in \nthe RL pipeline, and the representation of questions will likely need to be adapted to achieve this. However, we \nexpect that one can apply Guided Policy Search (GPS)\n35 to limit the deviation from the original set of questions.\nAnother limitation of our dialogue simulation is that it does not fully capture the difference between human-AI \nand human-human conversations in real-world conversation (i.e., the human-in-the-loop problem)36. One way \nwe can alleviate this problem is to continuously improve the accuracy of our dialogue simulation. To fully capture \nthe difference between human-AI and human-human conversations, a future direction of our study is to apply \ntransfer learning to update our simulator models based on dialogue agents trained from other NLP datasets that \nare validated by Wizard-of-Oz evaluation\n36,37. Wizard-of-Oz evaluation is a technique in NLP research whereby \nthe human interacts with a computer and does not know ahead of time whether the AI or another human will \ngenerate the questions for the proceeding conversation. The success of the dialogue agent in this case is measured \nby the degree to which the human user cannot distinguish between the agent’s questions from those of human \nquestioners. Future studies may look to deploy Wizard of Oz evaluation on top of the RL pipeline to quantify the \ndifference between human and AI delivery of questions\n38. Additionally, another promising direction include the \nimmersion of our text-based approach with current state-of-the-art audio-based approaches11,12 as well as existing \nbiomarkers. This step is beyond the scope of this prototype study, but it can improve the generalizability of our \ndialogue models and provide interpretability of the discovered features.\nFinally, interpretability of the linguistic feature vectors (i.e., skip-thought embeddings of sentences) is an open \nproblem in NLP . The original LIWC features in Asgari et al .\n9 provided a means of interpreting the linguistic \nfeature vectors due to expert labeling of the latent dimensions (see Pennebaker et al.30 for details). However, the \nlatent dimensions of skip-thought vectors are learned in an unsupervised manner (i.e., without expert labels). We \nare currently working to systematically study reliable interpretations of these latent features, but the interpretabil-\nity of deep representations is a subfield of machine learning on its own right and outside the scope of the current \nstudy, which is to formulate an algorithm to discover questions which can elicit them.\nThe maturation of AI techniques in neuroscience is a process built in development cycles and clinical phases. \nWe here introduce a novel framework for assessing the MCI status of aging populations that extends the appli-\ncation of ML methods beyond predictive modeling of disease processes. The proposed AI framework could pro-\nvide a potentially cost-effective alternative to in-person interviews and may present a scalable way of screening \n10Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nfor aging populations to distinguish normal aging from MCI-risk in an individualized manner. While still in a \nproof-of-concept phase, our results show a scalable and more robust version of our proposed framework pro-\nvides an avenue for large-scale preclinical screening of neurocognitive decline through automated digital bio-\nmarker detection. If such a system is implemented at scale, longitudinal surveillance of dementia status can be \ngreatly improved, potentially saving millions in outpatient costs and resource planning for the management of \nAlzheimer’s disease progression. More importantly, dialogue-based algorithms may present a step toward extend-\ning clinical care beyond the classical hospital and clinical settings.\nEthics. Oregon Health & Science University Institutional Review Board approved the study protocol (proto-\ncol no. 5590), and all participants provided written informed consent. The project is listed in ClinicalTrials.gov \n(NCT01571427).\nCompliance. All methods were carried out in accordance with relevant guidelines and regulations.\nData availability\nThe datasets generated during and/or analysed during the current study are not publicly available due to \nidentifying information but are available from the corresponding author on reasonable request.\nReceived: 15 November 2019; Accepted: 3 March 2020;\nPublished: xx xx xxxx\nReferences\n 1. Cummings, J. L., Doody, R. & Clark, C. Disease-modifying therapies for Alzheimer disease: challenges to early intervention. \nNeurology 69, 1622–1634 (2007).\n 2. Johnson, K. A., Fox, N. C., Sperling, R. A. & Klunk, W . E. Brain imaging in Alzheimer disease. Cold Spring Harb. Perspect. Med. 2, \na006213 (2012).\n 3. Heister, D. et al. Predicting MCI outcome with clinically available MRI and CSF biomarkers. Neurology 77, 1619–1628 (2011).\n 4. Jack, C. R. et al . Hypothetical model of dynamic biomarkers of the Alzheimer’s pathological cascade. Lancet Neurol.  9, 119–128 \n(2010).\n 5. Zhan, L. et al . Boosting brain connectome classification accuracy in Alzheimer’s disease using higher-order singular value \ndecomposition. Front. Neurosci. 9, 257 (2015).\n 6. Wang, Q., Zhan, L., Thompson, P . M., Dodge, H. H. & Zhou, J. Discriminative fusion of multiple brain networks for early mild \ncognitive impairment detection. 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI) https://doi.org/10.1109/\nisbi.2016.7493332 (2016).\n 7. Montero-Odasso, M. Gait performance as a biomarker of mild cognitive impairment: Clinical and imaging correlates. Alzheimer’s \n& Dementia 11, P511–P512 (2015).\n 8. Jack, C. R. Jr. et al. NIA-AA Research Framework: Toward a biological definition of Alzheimer’s disease. Alzheimers Dement. 14(4), \n535–562 (2018).\n 9. Asgari, M., Kaye, J. & Dodge, H. Predicting mild cognitive impairment from spontaneous spoken utterances. Alzheimers. Dement. \n3, 219–228 (2017).\n 10. Fraser, K. C., Meltzer, J. A. & Rudzicz, F . Linguistic Features Identify Alzheimer’s Disease in Narrative Speech. J. Alzheimers. Dis. 49, \n407–422 (2016).\n 11. Alhanai, T., Au, R. & Glass, J. Spoken language biomarkers for detecting cognitive impairment. 2017 IEEE Automatic Speech \nRecognition and Understanding Workshop (ASRU) https://doi.org/10.1109/asru.2017.8268965 (2017).\n 12. Roark, B., Mitchell, M., Hosom, J.-P ., Hollingshead, K. & Kaye, J. Spoken Language Derived Measures for Detecting Mild Cognitive \nImpairment. IEEE Trans. Audio Speech Lang. Processing 19, 2081–2090 (2011).\n 13. Aisen, P . S. et al. Report of the task force on designing clinical trials in early (predementia) AD. Neurology 76, 280–286 (2011).\n 14. König, A. et al. Automatic speech analysis for the assessment of patients with predementia and Alzheimer’s disease. Alzheimers. \nDement. 1, 112–124 (2015).\n 15. Sutton, R. S. Reinforcement Learning. (Springer Science & Business Media, 1992).\n 16. Chen, H., Liu, X., Yin, D. & Tang, J. A Survey on Dialogue Systems. ACM SIGKDD Explorations Newsletter 19, 25–35 (2017).\n 17. Schatzmann, J., Weilhammer, K., Stuttle, M. & Y oung, S. A survey of statistical user simulation techniques for reinforcement-\nlearning of dialogue management strategies. Knowl. Eng. Rev. 21, 97 (2006).\n 18. Shalev-Shwartz, S. & Ben-David, S. Understanding Machine Learning: From Theory to Algorithms. (Cambridge University Press, \n2014).\n 19. Dodge, H. H. et al . Web-enabled conversational interactions as a method to improve cognitive functions: Results of a 6-week \nrandomized controlled trial. Alzheimer’s & Dementia: Translational Research & Clinical Interventions 1, 1–12 (2015).\n 20. Dodge, H. H., Ybarra, O. & Kaye, J. A. Tools for advancing research into social networks and cognitive function in older adults. Int \nPsychogeriatr. 26(4), 533–9 (2014).\n 21. Kim, C. H. & Y ou, J. Social isolation and quality of life in Alzheimer’s dementia patients with Parkinson’s disease. Alzheimers. \nDement. 8, P379 (2012).\n 22. McHugh, J., Lawlor, B., Steptoe, A. & Kee, F . Interactive impacts of loneliness and social isolation on incident dementia in the english \nlongitudinal study of ageing. Alzheimers. Dement. 12, P807 (2016).\n 23. Henderson, M., Thomson, B. & Y oung, S. Robust dialog state tracking using delexicalised recurrent neural networks and \nunsupervised adaptation. in 2014 IEEE Spoken Language Technology Workshop (SLT) https://doi.org/10.1109/slt.2014.7078601  \n(2014).\n 24. Mikolov, T., Deoras, A., Povey, D., Burget, L. & Cernocky, J. Strategies for training large scale neural network language models. 2011 \nIEEE Workshop on Automatic Speech Recognition & Understanding https://doi.org/10.1109/asru.2011.6163930 (2011).\n 25. Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of transfer learning. Journal of Big Data 3, (2016).\n 26. Kiros, R. et al Skip-thought vectors. in In Advances in neural information processing systems 3294–3302 (2015).\n 27. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. (John Wiley & Sons, 2014).\n 28. Mnih, V . et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).\n 29. Litvinova, T. & Litvinova, O. A study of linguistic features of deceptive texts with the use of the program linguistic inquiry and word \ncount. Bulletin of the Moskow State Regional University (Linguistics) 71–77 https://doi.org/10.18384/2310-712x-2015-4-71-77 (2015).\n 30. Pennebaker, J. W . & Francis, M. E. Linguistic Inquiry and Word Count. Lawrence Erlbaum Assoc Incorporated, (1999).\n 31. Tobin, R. M. Measuring Emotions With the Linguistic Inquiry and Word Count (LIWC). PsycEXTRA Dataset https://doi.\norg/10.1037/e525752006-001 (2005).\n\n11Scientific  RepoRtS  |         (2020) 10:5732  | https://doi.org/10.1038/s41598-020-61994-0\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n 32. Kim, K. I. & Simon, R. Overfitting, generalization, and MSE in class probability estimation with high-dimensional data. Biom. J. 56, \n256–269 (2013).\n 33. Brassard, A., Kuculo, T., Boltuzic, F . & Šnajder, J. TakeLab at SemEval-2018 Task12: Argument Reasoning Comprehension with \nSkip-Thought Vectors. in Proceedings of The 12th International Workshop on Semantic Evaluation https://doi.org/10.18653/v1/s18-\n1192 (2018).\n 34. Thomas P ., Georgios T., Mohammad G. High confidence off-policy evaluation. in AAAI’15 Proceedings of the Twenty-Ninth AAAI \nConference on Artificial Intelligence 3000–3006 (2015).\n 35. Jakab, H. S. & Csato, L. Reinforcement learning with guided policy search using Gaussian processes. The 2012 International Joint \nConference on Neural Networks (IJCNN) https://doi.org/10.1109/ijcnn.2012.6252509 (2012).\n 36. Waibel, A., Steusloff, H., Stiefelhagen, R. & Watson, K. Computers in the Human Interaction Loop. Computers in the Human \nInteraction Loop 3–6 https://doi.org/10.1007/978-1-84882-054-8_1 (2009).\n 37. Read, J. C. Using Wizard of Oz to Evaluate Mobile Applications. Handbook of Research on User Interface Design and Evaluation for \nMobile Technology 802–813 https://doi.org/10.4018/978-1-59904-871-0.ch047 (2008).\n 38. Katz, A., Tepper, R. & Shtub, A. Simulation Training: Evaluating the Instructor’s Contribution to a Wizard of Oz Simulator in \nObstetrics and Gynecology Ultrasound Training. JMIR Med Educ 3, e8 (2017).\nAcknowledgements\nWe thank Lifang Zeng at Michigan State University for proofreading the manuscript for errors. This study \nwas funded by National Institute on Aging: R01AG056102, R01AG051628, R01AG033581, P30AG053760, \nP30AG008017; National Science Foundation: IIS-1615597, IIS-1750326, IIS-1749940; Office of Naval Research: \nN00014-17-1-2265.\nAuthor contributions\nJ.Z. and F .T. designed the study and model conception, F .T. implemented the algorithm, processed and analyzed \nthe data. I.U. performed data pre-processing. F .T., F .W . and J.Z. performed interpretation. H.D. collected data and \nassisted revisions of the manuscript. All authors edited and reviewed the manuscript.\ncompeting interests\nThe funding source of the study was not involved in the study design, data generation, modeling building, data \nanalysis, or writing of the manuscript. All authors of this manuscript had full access to all the data used in this \nstudy. Author responsibilities can be found in the author forms section.\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41598-020-61994-0.\nCorrespondence and requests for materials should be addressed to J.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons license, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2020"
}