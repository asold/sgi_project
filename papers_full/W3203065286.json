{
  "title": "Language Modeling using LMUs: 10x Better Data Efficiency or Improved Scaling Compared to Transformers",
  "url": "https://openalex.org/W3203065286",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283333797",
      "name": "Chilkuri, Narsimha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2409546397",
      "name": "Hunsberger, Eric",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Voelker, Aaron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301754528",
      "name": "Malik, Gurshaant",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2256981078",
      "name": "Eliasmith, Chris",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3167805910",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2970783931",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2952180055",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2773393927",
    "https://openalex.org/W3015468748"
  ],
  "abstract": "Recent studies have demonstrated that the performance of transformers on the task of language modeling obeys a power-law relationship with model size over six orders of magnitude. While transformers exhibit impressive scaling, their performance hinges on processing large amounts of data, and their computational and memory requirements grow quadratically with sequence length. Motivated by these considerations, we construct a Legendre Memory Unit based model that introduces a general prior for sequence processing and exhibits an $O(n)$ and $O(n \\ln n)$ (or better) dependency for memory and computation respectively. Over three orders of magnitude, we show that our new architecture attains the same accuracy as transformers with 10x fewer tokens. We also show that for the same amount of training our model improves the loss over transformers about as much as transformers improve over LSTMs. Additionally, we demonstrate that adding global self-attention complements our architecture and the augmented model improves performance even further.",
  "full_text": "Preprint. Work in progress.\nLANGUAGE MODELING USING LMU S:\n10X BETTER DATA EFFICIENCY OR IMPROVED\nSCALING COMPARED TO TRANSFORMERS\nNarsimha Chilkuri, Eric Hunsberger, Aaron Voelker, Gurshaant Malik & Chris Eliasmith\nApplied Brain Research\nWaterloo, Canada\nfirst.last@appliedbrainresearch.com\nABSTRACT\nRecent studies have demonstrated that the performance of transformers on the\ntask of language modeling obeys a power-law relationship with model size over\nsix orders of magnitude. While transformers exhibit impressive scaling, their per-\nformance hinges on processing large amounts of data, and their computational\nand memory requirements grow quadratically with sequence length. Motivated\nby these considerations, we construct a Legendre Memory Unit based model\nthat introduces a general prior for sequence processing and exhibits an O(n) and\nO(nln n) (or better) dependency for memory and computation respectively. Over\nthree orders of magnitude, we show that our new architecture attains the same\naccuracy as transformers with 10x fewer tokens. We also show that for the same\namount of training our model improves the loss over transformers about as much\nas transformers improve over LSTMs. Additionally, we demonstrate that adding\nglobal self-attention complements our architecture and the augmented model im-\nproves performance even further.\n1 I NTRODUCTION\nSelf-attention architectures such as the transformer (Vaswani et al., 2017) have been extremely suc-\ncessful in dealing with sequential data and have come to replace LSTMs and other RNN-based\nmethods, especially in the domain of Natural Language Processing (Radford et al., 2018; 2019; De-\nvlin et al., 2018). Transformers facilitate parallelization within training examples, and this allows\nthem to fully leverage hardware accelerators such as GPUs, making training on datasets as large\nas 750GB feasible (Raffel et al., 2019; Gao et al., 2020). In addition to parallelizing training, self-\nattention architectures are much better at handling long-range dependencies relative to traditional\nRNNs, and this allows them to take advantage of context much longer than the ∼100-1000 tokens\ntypical of RNNs, like the LSTM (V oelker & Eliasmith, 2018; Kaplan et al., 2020).\nTransformers are general-purpose architectures that can be applied to a wide variety of problems and\nmodalities. One of the drawbacks of such generality is the lack of a prioristructure, which makes\nthem heavily reliant on large quantities of data to achieve good results. Another limiting factor is\nthat self-attention involves the computation of the attention matrix,QKT , which is of shape n×n,\nwith n being the sequence length. Thus, transformer’s compute and memory requirements grow\nquadratically with respect to the sequence length.\nIn this work, we explore a way of addressing these limitations. We base our approach on the non-\nparametric Linear Time-Invariant (LTI) component of the Legendre Memory Unit (V oelker et al.,\n2019). This LTI system, which we refer to it here as the LMU, 1 projects a sliding window of the\ninput sequence onto Legendre polynomials to provide a temporal representation and compression of\nthe input signal. Although the LTI system is an RNN, it has been shown to support both sequential\nand parallel processing of sequences (Chilkuri & Eliasmith, 2021). Another crucial component of\n1We refer to the LTI system as the LMU as it is the distinguishing layer of our architectures. The original\nLMU was deﬁned to include the LTI system as well as a subsequent nonlinear layer. We have essentially\nexpanded this nonlinear layer to include a variety of familiar layers.\n1\narXiv:2110.02402v1  [cs.LG]  5 Oct 2021\nPreprint. Work in progress.\nLMULMULMU LMULMULMULMU L TI LMULMULMU LMULMULMULMU L TI LMULMULMU LMULMULMULMU L TILMULMULMU LMULMULMULMU L TI\nMultiply\nSum\nFigure 1: (left) Illustration of the standard sequential implementation for computing the hidden state\nm4. The input x1 is fed into the linear recurrent unit to compute the hidden state m1, which, along\nwith x2, is then used to compute the next hidden state, and so on. (right) Illustration of the time-\ndomain parallel implementation for computing the hidden state m4. The inputs x1-x4 are used to\ncompute the intermediate multiplies, which are then added together to compute the hidden statem4,\nall without the need for any sequential operations.\nour model is a modiﬁed attention mechanism that operates only on the output of the LMU at each\ntime step, and not across time steps. The LMU state at each step captures information about the past\ntokens, and hence we call this attention mechanism implicit self-attention.\nRecent work (Kaplan et al., 2020; Gao et al., 2020) has explored the scaling properties of trans-\nformers in the context of autoregressive language modelling. They show that the performance of\ntransformers scales as a power-law with model size (excluding embedding parameters), dataset size\nand the amount of compute used for training, when not bottlenecked by the other two. Inspired by\nthese results, we validate our method by studying the scaling properties of our models. To that end,\nwe start by reviewing some necessary background in Section 2, and then present the architectural\ndetails of our model in Section 4. In Section 5, we present our experiments with models containing\nup to 10 million parameters (or 1 million non-embedding parameters), which demonstrate a smooth\npower-law relationship with respect to the model size, and a loss that scales better than transformers.\nWhen the loss is matched to that of transformers, 10x fewer tokens are required.\n2 B ACKGROUND : T HE LEGENDRE MEMORY UNIT\nThe non-parametric LTI component of the LMU (V oelker & Eliasmith, 2018) is the focus of our\nstudy. This LTI system is mathematically derived to project a sliding window of length θ of the\ninput sequence onto qLegendre polynomials. Thus, the two main hyper-parameters to choose when\nusing it are θand q. Naturally, if we desire to capture the ﬁne-grained details of the input sequence, a\nlarge θ, which sets the length of the sliding window, should be accompanied by a largeq, the number\nof Legendre polynomials used in approximating the input. We present the state-update equations of\nthe LTI component of the LMU below,2\nmt = ¯Amt−1 + ¯Bxt, (1)\nwhere the ¯A = eA ∈Rq×q and ¯B = A−1(eA −I)B ∈Rq×1 matrices are frozen during training,\nwith A and B deﬁned as follows:\nAi,j = (2i+ 1)\nθ\n{−1 i<j\n(−1)i−j+1 i≥j , (2)\nBi = (2i+ 1)(−1)i\nθ . (3)\nCrucially, when needed, the LTI equation (1) above can be evaluated as a convolution, in parallel, as\nshown below (Chilkuri & Eliasmith, 2021):\nmt =\nt∑\nj=1\n¯At−j ¯Bxj, (4)\n2Focusing on one-dimensional inputs for now.\n2\nPreprint. Work in progress.\nor equivalently, deﬁning\nH =\n[¯A0 ¯B ¯A ¯B ...\n]\n∈Rq×n, (5)\nx = [xn xn−1 xn−2 ... x 1]T ∈Rn×1, (6)\nthe above convolution equation can be written as an element-wise multiplication in the Fourier space\nas follows:\nm1:n = F−1{F{H}·F{ x}}. (7)\n3 R ELATED WORK\nOur work falls under the broad category of combining convolution with self-attention. Recent work\nhas demonstrated that combining these two modules can be beneﬁcial for language modeling, speech\nand other NLP applications (Yang et al., 2019; Wu et al., 2020; Gulati et al., 2020). For instance, Wu\net al. (2020) introduce the Long-short Range Attentionmodule that features a two-branch design,\nwith the self-attention branch learning global interactions and the convolutional branch learning\nlocal interactions. Gulati et al. (2020) on the other hand propose a model that uses a single branch\narchitecture, with the convolutional block connected directly to the attention block, and demonstrate\nimproved performance on the task of speech recognition.\nWhile our work is mathematically related to the models mentioned above, it differs from the previ-\nous approaches in three crucial ways: ﬁrst, we do not learn the convolutional weights, but instead\nwork with the analytically deﬁned weights of the LMU; second, we introduce the novel implicit\nself-attention module that works on the hidden states of the LMU at each time-step; ﬁnally, we\nsystematically study the scaling properties of our model in the context of language modeling.\nOur model is also related to the many studies on reducing the quadratic computational and memory\ncomplexity of self-attention (Tay et al., 2020). Out of the several ways of improving efﬁciency,\nour work shares some similarity with the sliding window attention approach (Beltagy et al., 2020;\nZaheer et al., 2020). While these methods use a form of masking of the full self-attention matrix\nto constraint attention to the k-nearest tokens, our technique relies on the LMU to compute optimal\ncompressed representations of sliding windows of input vectors, each of which is then used as input\nto the implicit attention module.\n4 A RCHITECTURE\nIn this paper, we modify the architecture presented in Chilkuri & Eliasmith (2021) to better deal\nwith the task of language modelling, especially when the sequences are long and high-dimensional.\nStarting with the base LMU model, we describe the major components of our model below. An\nillustration of our architecture is presented in Figure 2.\nMemory Matrix Consider an input sequence {x1,x2,..., xn}of length nwhere the individual\nelements are of dimension xi ∈Rd. The most natural way of using an LMU-based model on such\nsequences is to set θ = n and use an appropriately large q. The downside of using the LMU in\nsuch a manner, however, is that the hidden state of the LMU scales with input dimension and order:\nm ∈Rdq. For example, in Section 5, we deal with n = 1024 and dthat is as large as 204. Even\nwhen using a small qof 100, we may end up with hidden states that are as large as R20k, which is\nhighly undesirable.\nOne way around this issue is to take inspiration from standard convolutional network architectures\n(CNNs) and work with a smaller sliding window, θ ≈10, which in turn allows us to use a small\nLMU order, q ≈5, thus taming the hidden state dimension (see Chilkuri & Eliasmith (2021) for\nmore details). However, enforcing a small sliding window prompts the use of many stacked LMU\nlayers in order to increase the ‘receptive ﬁeld’ (or the effectiveθ) of the model, very similar to how\nCNNs often use small kernels with many convolutional layers. Unsurprisingly, such an approach\nresults in very deep models, which can be problematic to train.\nHere, we choose to follow the middle path, i.e,0 ≪q≪n, but instead of working directly with the\npotentially high-dimensional hidden statem, we disentangle the input dimensions from the order. In\n3\nPreprint. Work in progress.\nHidden State\nLMU\nOutput\nVKQ\nInput, t\nM\nHidden State\nLMU\nOutput\nVKQ\nInput, t\nM\nHidden State\nLMU\nOutput\nVKQ\nInput, t\nM\nHidden State\nLMU\nOutput\nReshape and Project\nImplicit Self-Attention\nVKQ\nProject and FFN\n FFN / Global Attention\nInput\nM\n(n, dq)\n(n, q’, d)\n(n, 3, q’, d)\n(n, d)\n(n, d)\nFigure 2: The LMU and implicit self-attention architecture along with output dimensions. In the\nillustration, nrefers to the sequence length, q is the order and q′is the reduced order, and dis the\nembedding dimension. Normalization layers and skip connections are not shown. One variant uses\nthe FFN component right after the input, and the other variant uses global attention.\nother words, we perform our operations on the matrix M ∈Rd×q and not on the vector m ∈Rdq.\nThis is beneﬁcial because while a fully-connected layer needs d2 ·q2 parameters to process the m\nvector, processing the matrix M with the help of two fully connected layers, one for the rows and\none for the columns, requires only d2 + q2 parameters.\nImplicit Self-Attention The main feature distinguishing our architecture from past work is the\nLMU. As mentioned above, the output of the LMU layer compresses past history at each time-\nstep, which is captured by the M ∈Rq×d matrix. Our modiﬁed self-attention acts on this matrix to\ncombine temporal information. As a result, self-attention does not act directly on the input sequence,\nbut rather on a compressed version of the input, which is available at each moment in time and covers\na window, determined by θ. Ignoring the bias vectors, normalization layers, and skip-connections,\nwe ﬁrst execute the following sets of operations simultaneously:\nQ = σ(L1M) K = σ(L2M) V = σ(L3M), (8)\nwhere Li ∈Rq′×q, σis a non-linearity such asgelu, and the matricesQ, K, V are all in Rq′×q. In\nour experiments, we have found the setting q′= q/10 to work well, and thus our attention matrices\ncontain far fewer elements than n2. For example, in our largest model we set q = 250, resulting in\nq′= 25.\nFollowing the computation of Q, K, and V two additional computations result in a d-dimensional\nvector m:\nM′= softmax(QKT )V , (9)\nm = pM′, (10)\nwhere p ∈R1×q′\n.\nIn practice, equation (8) can be made far more efﬁcient computationally, especially during inference,\nby following the recipe outlined in Section A.1.\nFeedforward Network We have also found it beneﬁcial to include a feedforward network (FFN)\n(Vaswani et al., 2017) before the LMU and after the implicit self-attention block. The FFN compo-\n4\nPreprint. Work in progress.\nTable 1: Memory and computation scaling with sequence length during training and inference.\nLayer Memory Compute\nFull Attention O(n2) O(n2)\nLMU (Parallel) O(n) O(nln n)\nLMU (Recurrent) O(1) O(n)\nTable 2: Parameter counts and compute (forward pass) for one layer of the network, per token.\nThe ﬁrst row indicates the number of FLOPs when following the implementation in Section A.1.\nAdditional background information regarding various implementations of the LMU is provided in\nAppendix A.2.\nOperation Parameters FLOPs per Token\nLMU + Q + K + V 3qq′ 3d[5(q′+ 1)(log2 n+ 1) + 6q′] + 6qq′\nQKT – 2dq′2\nM′ – 2dq′2 + dq′\nm q′ 2dq′\nFFN 2dd′ 4dd′\nnent is deﬁned below:\ny(x) = σ(xW1 + b1)W2 + b2, (11)\nwhere W1 ∈Rd×d′\n, W2 ∈Rd′×d, b1 ∈Rd′\nand b2 ∈Rd.\nGlobal Self-Attention We also explore the use of a global self-attention block in place of the FFN\ncomponent before the LMU. We ﬁnd that introducing this layer, while computationally expensive,\nfurther improves the cross-entropy score. We believe that the improvement comes from the fact the\nLMU and self-attention are complementary: the LMU’s implicit self-attention is good at prediction\nwith limited context, and the traditional self-attention captures long-range dependencies. We wish\nto explore the use of efﬁcient self-attention blocks – which scale better than O(n2) – in the future.\nComplexity As shown in Table 1, our architecture employing the parallel LMU along with implicit\nself-attention has memory requirements that are linear with respect to the sequence length, and it\nhas computational requirements that also grow as nln n. When we use the recurrent version of\nthe LMU, the memory and compute requirements scale as O(1) and O(n) respectively. Recurrent\nimplementations, while not as efﬁcient on GPU architectures for large batch sizes, are ideally suited\nto edge applications, especially with efﬁcient hardware support. Notably, if we add global attention\nto our model, then both compute and memory become quadratic, just like the original transformer.\nWe also list the number of ﬂoating point operations per-token for the (parallel) LMU model in\nTable 2.\n5 E XPERIMENTS\nDataset We train our models on the publicly available internet text dataset called OpenWebText2\n(OWT2).3 Similar to the WebText2 dataset (Radford et al., 2019), OWT2 was created using URLs\nextracted from Reddit submissions with a minimum score of 3 as a proxy for quality, and it consists\nof Reddit submissions from 2005 up until April 2020. After additional ﬁltering, applying the pre-\ntrained GPT2 tokenizer containing 50257 tokens (Radford et al., 2019) results in approximately 8\nbillion tokens in total. We use a train/validation/test split of 96/3/1%.\n3https://www.eleuther.ai/projects/open-web-text2/\n5\nPreprint. Work in progress.\n105 106\nParameters (non-embedding)\n4 × 100\n5 × 100\nLoss\nPerformance vs Parameters\nLSTM\nTransformer L(N,S)\nLMU\nLMU + Attention\nFigure 3: Cross-entropy scores in nats, averaged across all the tokens in the sequence. Transformers\nand LSTMs ﬁts are from Kaplan et al. (2020). Our models perform better than Transformers and\nLSTM models up to 1 million non-embedding parameters.\nTraining Details We train our models in an autoregressive manner using the Adam optimizer with\nall the default settings. We use sequences containing 1024 tokens, and in cases where the documents\nhave fewer than 1024 tokens, we pack multiple documents into the same sequence, separated by the\n<|endofsequence|> token. We use a learning rate schedule with a linear warmup and cosine\ndecay to zero, while also reducing the learning rate on plateau. We chose to train our models to\nprocess a maximum of 13 billion tokens; at a batch size of 512, this amounts to training for 25000\nsteps. Additionally, one of the important considerations when doing NLP experiments is the size of\nthe embedding vectors, d. In this work, in order to facilitate a fair comparison to the transformer\nmodels (Kaplan et al., 2020), we use the following rule to determine d:\nd=\n√\nN\n24,\nwhere N represents the number of non-embedding (and trainable) parameters.\nResults Here we present the results of our experiments that use the LMU architecture described\nabove with the non-embedding (and trainable) parameters ranging from 55k to 1M (i.e., from 2.5M\nto 10M, if we include all parameters). The cross-entropy results are presented in Figure 3. For the\ntransformer models, we list the scores obtained by using the following power-law ﬁt,\nTransformer(N,S) =\n( N\n6.5 ·1013\n)−0.077\n+\n( S\nSmin(S)\n)−0.76\n, (12)\nwhere N refers to the number of non-embedding parameters and S is the total number of training\nsteps (set to 25000 at batch size of 512, or equivalent). The power-law is obtained from Kaplan et al.\n(2020); the ﬁts were generated by training several transformer models, ranging in size from 768 to\n1.5 billion non-embedding parameters, on OpenAI’s WebText2 dataset. In addition, we compare\nagainst the power-law for LSTM models, also from Kaplan et al. (2020), that use 10x more training\nsteps than the transformer and LMU models:\nLSTM(N) =\n( N\n7.45 ·1014\n)−0.071\n.\nSimilar to the transformer and LSTM models, we notice that the performance of our models depends\nstrongly on scale, with the loss of the LMU model exhibiting the following power-law relationship\nwith respect to N:\nLMU(N) =\n( N\n1.95 ·1014\n)−0.072\n.\n6\nPreprint. Work in progress.\n100 101 102 103\nSequence Index\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0Per-token Loss 55k\n1M\nPer-token Loss\nTransformer\nLMU + Attention\n100 101 102 103\nSequence Index\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5Per-token Loss\n1M\nPer-token Loss\nTransformer\nLMU\nFigure 4: (left) Comparison of per-token loss of an LMU model (with global attention) and a trans-\nformer model. (right) Per-token loss of an LMU model (without global attention) alongside the\ntransformer’s loss.\nThe LMU model with global self-attention scales as follows:\nLMUG(N) =\n( N\n3.80 ·1014\n)−0.069\n.\nIt remains to be seen whether our models retain this performance advantage when N ≫106.\nThe utility of adding global attention becomes clear when we observe the per-token loss plots of\nthe three models. In Figure 4 (right), we notice that although the LMU’s per-token loss is better\noverall than the transformer’s, it ﬂattens relatively early, around 100 tokens, suggesting that implicit\nattention alone does not capture long context. The LMU and Attention model, on the other hand,\ncontinues improving with increasing context, similar to the transformer.\nIt is also interesting to note that we can compare LMUs and transformers by determining approxi-\nmately how much training is required for a transformer to match LMU loss. Figure 5 demonstrates\nthat our models, trained on13 billion tokens, have similar scaling to transformers trained on130 bil-\nlion tokens. Consequently, the LMU architecture is 10x more data efﬁcient. In addition, our LMU\nmodels with global attention continue to outperform transformer models trained on 10x more tokens\n(or with 10x more training steps) by a signiﬁcant margin.\n6 D ISCUSSION\nSemi-supervised learning has proven to be a very effective technique in Natural Language Process-\ning. General purpose language models pre-trained on a large corpus of text in an unsupervised\nmanner and ﬁne-tuned on tasks such as sentiment analysis and question answering often outperform\nhighly task-speciﬁc architectures that receive no pre-training. The performance of models on the\ntask of language modelling is thus a crucial metric that is indicative of the downstream performance\nof such models on a slew of tasks involving natural language.\nWhile the performance of our models on the task of language modelling suggests an interesting\ntrend, due to the scale of our experiments however, we do not consider this to be deﬁnitive evidence\nfor the superiority of our LMU architecture. As a result, a core objective for future research is to\nshow that the observed trends hold over 6 orders of magnitude, as demonstrated by Kaplan et al.\n(2020) for transformers.\n7 C ONCLUSION\nIn this work, we employ the Legendre Memory to construct a model that is well-suited to han-\ndling long sequences with high-dimensional elements. We apply our architectures to model natural\nlanguage in the inﬁnite data limit, demonstrating that: (1) like the established architectures such\n7\nPreprint. Work in progress.\n105 106\nParameters (non-embedding)\n4 × 100\n5 × 100\nLoss\nPerformance vs Parameters\nLSTM\nTransformer L(N,S)\nLMU\nLMU + Attention\nFigure 5: Approximately matching the loss between transformers and LMUs requires 10x more\ntraining for the transformer. The LMU and Attention model continues to signiﬁcantly outperform\ntransformers with 10x less training.\nas transformers and LSTMS, our models also exhibit a power-law relationship between the cross-\nentropy loss and model size; and (2) at the small-medium scale, our models have better scaling\nproperties than other approaches.\n8\nPreprint. Work in progress.\nREFERENCES\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nNarsimha Reddy Chilkuri and Chris Eliasmith. Parallelizing legendre memory unit train-\ning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Confer-\nence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.\n1898–1907. PMLR, 18–24 Jul 2021. URL http://proceedings.mlr.press/v139/\nchilkuri21a.html.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer\nfor speech recognition. arXiv preprint arXiv:2005.08100, 2020.\nSteven G. Johnson and Matteo Frigo. Implementing FFTs in Practice. In C. Sidney Burrus\n(ed.), Fast Fourier Transforms. 2012. URL https://cnx.org/contents/ulXtQbN7@\n15/Implementing-FFTs-in-Practice .\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.arXiv\npreprint arXiv:2009.06732, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nAaron V oelker, Ivana Kaji´c, and Chris Eliasmith. Legendre memory units: Continuous-time repre-\nsentation in recurrent neural networks. In Advances in Neural Information Processing Systems,\npp. 15544–15553, 2019.\nAaron R V oelker and Chris Eliasmith. Improving spiking dynamical networks: Accurate delays,\nhigher-order synapses, and time cells. Neural computation, 30(3):569–609, 2018.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short\nrange attention. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=ByeMPlHKPH.\nBaosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu. Convolutional self-\nattention networks. arXiv preprint arXiv:1904.03107, 2019.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. In NeurIPS, 2020.\n9\nPreprint. Work in progress.\nA A PPENDIX\nA.1 R EDUCED -ORDER LMU\nWhen implementing the LMU in these models, we use a parallelizable approach that computes the\nimpulse responses of the LMU (which are essentially the Legendre polynomials), and convolve those\nwith the input sequences (either using raw convolution or FFT-based convolution). Speciﬁcally,\ngiven the q-dimensional impulse response H, we compute the LMU memory state M ∈Rd×q at\nthe current time as\nM = X ∗H (13)\nwhere X ∈Rn×d is the time-series of previous inputs to the LMU,H ∈Rq×n is the LMU impulse\nresponse, and ∗is the convolution operator.\nWe have found that our models are most expressive when using a value ofqthat is signiﬁcantly larger\nthan q′, as this allows the LMU to ”remember” the time history with high ﬁdelity, but only use the\nparts of the history that are most relevant. Rather than explicitly computing the full LMU outputM\nand then reducing this with the Li transformations as per equation (8), we propose applying the Li\ntransformations directly to the impulse responses\n˜Hi = LiH (14)\nand then applying these individually to directly compute Q, K, and V :\nQ = σ(X ∗ ˜H1) K = σ(X ∗ ˜H2) V = σ(X ∗ ˜H3) (15)\nThis is mathematically equivalent to the formulation expressed previously, but uses signiﬁcantly\nfewer operations per token, particularly when qis large or the ratio of q′to qis small:\nLLMU+Q+K+V = 3[dLFFT(q′) + 2qq′] (16)\nwhere LFFT(q′) is given by Equation 19 with q→q′.\nA.2 LMU I MPLEMENTATION TRADE -OFFS\nThe LMU itself is a LTI dynamical system, with a number of options for implementation. One\nimplementation is to perform the update each timestep in state-space, using state-space matrices\ndiscretized using the zero-order hold (ZOH) method for high accuracy. The operations required\n(per LMU layer and per token) are the multiplications by the ¯A and ¯B matrices (with number of\nelements q2 and q, respectively):\nLSS = 2d(q2 + q). (17)\nAnother option is to use an explicit Runge-Kutta method to update the LMU states. By taking\nadvantage of the unique structure of theA and B matrices (Equations 2 and 3), this implementation\nis able to reduce the complexity from O(q2) to O(q), requiring the following approximate number\nof operations:\nLRK = 6rdq. (18)\nwhere ris the order of the Runge-Kutta method. The disadvantage to this option is that it does not\nimplement the exact same dynamics as the ideal system discretized with ZOH, and is less numeri-\ncally stable particularly for higher values of q.\nA disadvantage to both these options is that they must update LMU states sequentially, which is\nparticularly ill-suited when using highly parallel hardware (e.g. GPU) with a long sequence of\ninputs available. In this case, we can take the impulse response of the LMU system (discretized with\nZOH), and convolve it with an input in the FFT domain. This implements the exact same dynamics\nas the ZOH state-space system, but with a complexity that is O(q) rather than O(q2):\nLFFT = d\nn[C(2n) + cmqn+ qC(2n)]\n= d[5(log2 n+ 1)(q+ 1) + 6q] . (19)\n10\nPreprint. Work in progress.\nHere, C(n) is the number of FLOPs for a radix-2 Cooley-Tukey FFT implementation (Johnson &\nFrigo, 2012):\nC(n) = 2C\n(n\n2\n)\n+ n\n2 (cm + 2ca) (20)\n= 5nlog2 n (21)\nwhere cm = 6 is the number of FLOPs per complex multiply, and ca is the number of FLOPs per\ncomplex addition. For our standard sequence length of n= 1024, this results in:\nLFFT−1024 = d(61q+ 55). (22)\n11",
  "topic": "Scaling",
  "concepts": [
    {
      "name": "Scaling",
      "score": 0.6529510021209717
    },
    {
      "name": "Transformer",
      "score": 0.6015224456787109
    },
    {
      "name": "Computer science",
      "score": 0.48528334498405457
    },
    {
      "name": "Mathematics",
      "score": 0.22031888365745544
    },
    {
      "name": "Electrical engineering",
      "score": 0.17767944931983948
    },
    {
      "name": "Engineering",
      "score": 0.12685710191726685
    },
    {
      "name": "Voltage",
      "score": 0.08359447121620178
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}