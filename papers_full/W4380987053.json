{
    "title": "Multitask Fine Tuning on Pretrained Language Model for Retrieval-Based Question Answering in Automotive Domain",
    "url": "https://openalex.org/W4380987053",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2124420670",
            "name": "Zhiyi Luo",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A2810381042",
            "name": "Sirui Yan",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A2100320873",
            "name": "Shuyun Luo",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A2124420670",
            "name": "Zhiyi Luo",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A2810381042",
            "name": "Sirui Yan",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        },
        {
            "id": "https://openalex.org/A2100320873",
            "name": "Shuyun Luo",
            "affiliations": [
                "Zhejiang Sci-Tech University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2098880560",
        "https://openalex.org/W6685275506",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W4287888456",
        "https://openalex.org/W2912817604",
        "https://openalex.org/W2144211451",
        "https://openalex.org/W2250729567",
        "https://openalex.org/W2143933463",
        "https://openalex.org/W6753056052",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2740711318",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4206209937",
        "https://openalex.org/W4312973876",
        "https://openalex.org/W2165558283",
        "https://openalex.org/W6676179522",
        "https://openalex.org/W2024668293",
        "https://openalex.org/W2124509324",
        "https://openalex.org/W2147717514",
        "https://openalex.org/W2012592962",
        "https://openalex.org/W2982479999",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2108406823",
        "https://openalex.org/W2171278097",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W3121694563"
    ],
    "abstract": "Retrieval-based question answering in the automotive domain requires a model to comprehend and articulate relevant domain knowledge, accurately understand user intent, and effectively match the required information. Typically, these systems employ an encoder–retriever architecture. However, existing encoders, which rely on pretrained language models, suffer from limited specialization, insufficient awareness of domain knowledge, and biases in user intent understanding. To overcome these limitations, this paper constructs a Chinese corpus specifically tailored for the automotive domain, comprising question–answer pairs, document collections, and multitask annotated data. Subsequently, a pretraining–multitask fine-tuning framework based on masked language models is introduced to integrate domain knowledge as well as enhance semantic representations, thereby yielding benefits for downstream applications. To evaluate system performance, an evaluation dataset is created using ChatGPT, and a novel retrieval task evaluation metric called mean linear window rank (MLWR) is proposed. Experimental results demonstrate that the proposed system (based on BERTbase), achieves accuracies of 77.5% and 84.75% for Hit@1 and Hit@3, respectively, in the automotive domain retrieval-based question-answering task. Additionally, the MLWR reaches 87.71%. Compared to a system utilizing a general encoder, the proposed multitask fine-tuning strategy shows improvements of 12.5%, 12.5%, and 28.16% for Hit@1, Hit@3, and MLWR, respectively. Furthermore, when compared to the best single-task fine-tuning strategy, the enhancements amount to 0.5%, 1.25%, and 0.95% for Hit@1, Hit@3, and MLWR, respectively.",
    "full_text": null
}