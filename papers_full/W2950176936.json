{
  "title": "Fooling Vision and Language Models Despite Localization and Attention Mechanism",
  "url": "https://openalex.org/W2950176936",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1948936033",
      "name": "Xu Xiaojun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2323455119",
      "name": "Chen, Xinyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098561542",
      "name": "Liu Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3187794171",
      "name": "Rohrbach, Anna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744191946",
      "name": "Darrell, Trevor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751273816",
      "name": "Song, Dawn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W2442626797",
    "https://openalex.org/W2950774971",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2949103145",
    "https://openalex.org/W2951807304",
    "https://openalex.org/W2952246170",
    "https://openalex.org/W2584723080",
    "https://openalex.org/W1932198206",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2962706528",
    "https://openalex.org/W2412400526",
    "https://openalex.org/W2764216487",
    "https://openalex.org/W2963744840",
    "https://openalex.org/W2230472587",
    "https://openalex.org/W2612637113",
    "https://openalex.org/W2625220439",
    "https://openalex.org/W2293453011",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2949311987",
    "https://openalex.org/W2732016772",
    "https://openalex.org/W1488163396",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963066927",
    "https://openalex.org/W2951590222",
    "https://openalex.org/W2963398599",
    "https://openalex.org/W2517229335",
    "https://openalex.org/W2590523583",
    "https://openalex.org/W2963656855",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2552767274",
    "https://openalex.org/W2964138017",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W2593871270",
    "https://openalex.org/W2736899637",
    "https://openalex.org/W2963672682",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W2180612164",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W2408141691",
    "https://openalex.org/W2543927648",
    "https://openalex.org/W2616841723",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2604394466",
    "https://openalex.org/W2963954913",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W2396147015",
    "https://openalex.org/W2964082701",
    "https://openalex.org/W2605631833",
    "https://openalex.org/W2950864148",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2418349398",
    "https://openalex.org/W2963191264",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2613526370"
  ],
  "abstract": "Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., &gt; 90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.",
  "full_text": "Fooling Vision and Language Models\nDespite Localization and Attention Mechanism\nXiaojun Xu 1,2, Xinyun Chen 2, Chang Liu 2, Anna Rohrbach 2,3, Trevor Darrell 2, Dawn Song 2\n1Shanghai Jiao Tong University, 2EECS, UC Berkeley, 3MPI for Informatics\nAbstract\nAdversarial attacks are known to succeed on classiﬁers,\nbut it has been an open question whether more complex vi-\nsion systems are vulnerable. In this paper, we study ad-\nversarial examples for vision and language models, which\nincorporate natural language understanding and complex\nstructures such as attention, localization, and modular ar-\nchitectures. In particular, we investigate attacks on a dense\ncaptioning model and on two visual question answering\n(VQA) models. Our evaluation shows that we can generate\nadversarial examples with a high success rate (i.e., > 90%)\nfor these models. Our work sheds new light on understand-\ning adversarial attacks on vision systems which have a lan-\nguage component and shows that attention, bounding box\nlocalization, and compositional internal structures are vul-\nnerable to adversarial attacks. These observations will in-\nform future work towards building effective defenses.\n1. Introduction\nMachine learning, especially deep learning, has achieved\ngreat success in various application scenarios, such as im-\nage classiﬁcation, speech recognition, and machine trans-\nlation. However, recent studies prove the existence of ad-\nversarial examples for many vision-based learning models,\nwhich may hinder the adoption of deep learning techniques\nto security-sensitive applications [\n19, 43, 56, 65]. Most ex-\nisting works consider image classiﬁcation and demonstrate\nthat it is almost always possible to fool these models to clas-\nsify an adversarially generated image as a class speciﬁed by\nthe adversary [\n66]. Albeit numerous defenses have been\nproposed [ 19, 60, 52, 67, 51, 48], almost all of them are\nlater shown to be broken [ 8, 23, 9].\nRecently, there has been an increasing interest in whether\nadversarial examples are practical enough to attack more\ncomplex vision systems [\n44, 45, 6]. In the latest results of\nthis debate, Lu et al. show that previous adversarial exam-\nples constructed to fool CNN-based classiﬁers cannot fool\nstate-of-the-art detectors [\n45]. We are interested in whether\nother forms of localization and/or language context offer ef-\nfective defense.\nIn this work, we extend the investigation towards more\ncomplex models that not only include a vision component\nbut also a language component to deepen our understand-\ning of the practicality of adversarial examples. In particular,\nwe investigate two classes of systems. First, we are inter-\nested in dense captioning systems, such as DenseCap [\n30],\nwhich identify regions of interest ﬁrst and then generate\ncaptions for each region. Second, we are interested in vi-\nsual question answering (VQA) systems, which answer a\nnatural language question based on a given image input.\nThe state-of-the-art VQA systems typically compute atten-\ntion maps based on the input and then answer the question\nbased on the attended image regions. Therefore, both types\nof models have a localization component , and thus they are\ngood targets for studying whether localization can help pre-\nvent adversarial attacks. Further, we explore state-of-the-art\nVQA models based on Neural Modular Networks [\n25], and\nevaluate whether such compositional architectures are also\nvulnerable to adversarial attacks; in these models, a new\nnetwork architecture is instantiated for each question type,\npotentially providing a buffer against attacks.\nWe evaluate adversarial examples against these vision\nand language models. We ﬁnd that in most cases, the attacks\ncan successfully fool the victim models despite their inter-\nnal localization component via attention heatmaps or region\nproposals, and/or modular structures. Our study shows that,\nin an online (non-physical) setting when the attackers have\nfull access to the victim model including its localization\ncomponent (white-box attack), the generated adversarial ex-\namples can fool the entire model regardless of the localiza-\ntion component. Therefore, our evaluation results provide\nfurther evidence that employing a localization in combina-\ntion with a classiﬁer may not be sufﬁcient to defend against\nadversarial examples, at least in non-physical settings.\nWe also make the following additional contributions.\nFirst, we develop a novel attack approach for VQA models,\nwhich signiﬁcantly outperforms the previous state-of-the-\nart attacks. Second, we observe and analyze the effect of a\n1\narXiv:1709.08693v2  [cs.AI]  6 Apr 2018\nlanguage prior in attacking VQA models, and deﬁne a prin-\nciple which explains which adversarial examples are likely\nto fail. In particular, when the target answer is not com-\npatible with the question, it is difﬁcult to ﬁnd a successful\nadversarial attack using existing approaches. To sum up,\nour work sheds new light on understanding adversarial at-\ntacks on vision and language systems and shows that atten-\ntion, bounding box localization and compositional internal\nstructures are vulnerable to adversarial attacks. These ob-\nservations will inform future work towards building effec-\ntive defenses.\n2. Related Work\nIn the following, we ﬁrst review recent work on image\ncaptioning and visual question answering. We focus on the\nmodels that incorporate some form of localization, e.g. soft\nattention or bounding box detection. We then review the\nstate-of-the-art methods to generate adversarial examples as\nwell as defense strategies against these methods.\nImage Captioning Most recent image captioning ap-\nproaches have an encoder-decoder architecture [\n11, 12, 32,\n33, 50, 69]. A spatial attention mechanism for image cap-\ntioning was ﬁrst introduced by [ 73]. They explored soft at-\ntention [ 7] as well as hard attention. Others have adopted\nthis idea [ 15, 42, 46, 76] or extended it to perform attention\nover semantic concepts, or attributes [ 77, 79]. Recently [ 61]\nproposed an end-to-end model which regresses a set of im-\nage regions and learns to associate caption words to these\nregions. Notably, [\n2, 12, 32] exploited object detection re-\nsponses as input to the captioning system. As opposed to\nimage captioning of the entire image, [\n30] have proposed\ndense captioning , which requires localization and descrip-\ntion of image regions (typically bounding boxes). Some\nother dense captioning approaches include [\n40, 74].\nVisual Question Answering. Early neural models for vi-\nsual question answering (VQA) were largely inspired by\nimage captioning approaches, e.g. relying on a CNN for im-\nage encoding and a RNN for question encoding [\n17, 49, 62].\nInspired by [ 73], a large number of works have adopted an\nattention mechanism for VQA [ 16, 47, 64, 72, 75, 80]. Se-\nmantic attention has been explored by [ 78]. Other direc-\ntions explored by recent work include Dynamic Memory\nNetworks (DMN) [\n36, 71], and dynamic parameter layers\n(DPP) [ 55]. Recently a new line of work focused on de-\nveloping more compositional approaches to VQA, namely\nneural module networks [\n3, 4, 25, 29]. These approaches\nhave shown an advantage over prior work for visual ques-\ntion answering which involve complex reasoning.\nAdversarial Examples. Existing works on adversarial\nexample generation mainly focus on image classiﬁcation\nmodels. Several different approaches have been pro-\nposed for generating adversarial examples, including fast\ngradient-based methods [\n19, 43], optimization-based meth-\nods [ 66, 10], and others [ 58, 54]. In particular, Carlini\net al. [ 10] proposed the state-of-the-art attacks under con-\nstraints on L0, L2, and L∞ norms. Our work improves [ 10]\non both attack success rate and adversarial probability.\nAnother line of research studies adversarial examples\nagainst deep neural networks for other tasks, such as recur-\nrent neural networks for text processing [\n59, 28], deep re-\ninforcement learning models for game playing [ 41, 26, 34],\nsemantic segmentation [ 14, 70], and object detection [ 24,\n70]. To our best knowledge, our work is the ﬁrst to study\nadversarial examples against vision-language models.\nWhile our work assumes that models are known to the\nattacker, prior works demonstrate that adversarial examples\ncan transfer between different deep neural networks for im-\nage classiﬁcation [\n66, 19, 43, 56, 58, 53], which can be used\nfor black-box attacks. We brieﬂy analyze the transferability\nof VQA models in Appendix\nD.2.\nDefense against Adversarial Examples. On the defense\nside, numerous strategies have been proposed against ad-\nversarial examples [\n19, 60, 52]. Early attempts to build a\ndefense using distillation [ 60] were soon identiﬁed as vul-\nnerable [ 8]. Some recent proposals attempt to build a de-\ntector to distinguish adversarial examples from natural im-\nages [ 52, 21, 18, 13]. Others study ensembles of different\nmodels and defense strategies to see whether that helps to\nincrease the robustness of deep neural networks [\n67, 68, 51].\nHowever, He et al. show that with the knowledge of the\ndetector network and the defense strategies being used, an\nattacker can generate adversarial examples that can mislead\nthe model, while still bypassing the detector [\n23].\nThe most promising line of defense strategies is called\nadversarial training [19, 37, 67, 48]. The idea is to generate\nadaptive adversarial examples and train the model on them\niteratively. The latest results along the line [\n48] show that\nsuch an approach can build a robust MNIST model. But the\nsame approach currently fails on extending to CIFAR-10.\n3. Generating Targeted Adversarial Examples\nIn this section, we ﬁrst present a generic adversarial ex-\nample generation algorithm, and then our implementations\nfor dense captioning models and VQA models.\n3.1. Background: targeted adversarial examples\nfor a classiﬁcation model\nConsider a classiﬁcation model fθ(x), where θ is the pa-\nrameters and x is the input. Given a source image x, a tar-\ngeted adversarial example is deﬁned as x⋆ such that\nfθ(x⋆) =yt ∧ d(x⋆, x ) ≤ B (1)\nwhere yt is the target label, and d(x⋆, x ) ≤ B says that the\ndistance between x and x⋆ is bounded by a constant B.\nWithout loss of generality, fθ(x) predicts the dimension\nof the largest softmax output. We denote Jθ(x) as the soft-\nmax output, then a standard training algorithm typically op-\ntimizes the empirical loss ∑\ni L(Jθ(xi), y i) with respect to\nθ using a gradient decent-based approach. Existing adver-\nsarial example generation algorithms leverage the fact that\nJθ(x) is differentiable, and thus solve (\n1) by optimizing the\nfollowing objective:\nargminx⋆ L(Jθ(x⋆), y t) +λd(x⋆, x ) (2)\nwhere λ > 0 is a hyper-parameter. In fact, the state-of-the-\nart attack [ 10] approximates the solution to ( 2) using Adam.\n3.2. Targeted adversarial examples for DenseCap\nThe DenseCap model [ 30] predicts M = 1000 regions,\nranks them based on conﬁdence, and then generates a cap-\ntion for each region. It uses a localization network, similar\nto Fast R-CNN [\n63], for predicting regions. For each re-\ngion, the model uses a CNN to compute the embedding and\nthen uses an RNN to generate a sequence of tokens from the\nembedding to form the caption.\nTo train the DenseCap model, Johnson et al. include ﬁve\nterms in the loss: four for training the region proposal net-\nwork, and the last one to train the RNN caption generator.\nTo fool the model to predict the wrong target caption, we\ncan leverage a similar process as discussed above. Note that\nexisting works [\n24, 70] have demonstrated that an object\ndetection/segmentation model can be fooled by adversarial\nexamples. In this work, we focus on generating adversarial\nexamples to fool the captioning module of the model, while\nretaining the proposed regions unchanged.\nTo achieve this goal, assuming the target caption is\nCt and the ground truth regions for a source image are\n{Ri}, we construct a new set of target region-caption pairs\n{(Ri, C t)}. Using these target region-caption pairs as the\nnew “ground truth”, we can use the DenseCap loss, with\naddition of the λd(x⋆, x ) term as in (\n2), as the new objec-\ntive, and minimize it with respect to x⋆.\n3.3. Targeted adversarial examples for VQA models\nWe now brieﬂy present our novel targeted adversarial at-\ntack against VQA models. More details can be found in\nAppendix\nA. Our design is inspired by two goals: (1) maxi-\nmizing the probability of the target answer, which is equiv-\nalent to the conﬁdence score of the model’s prediction; and\n(2) removing the preference of adversarial examples with\nsmaller distance to the source image, as long as this dis-\ntance is small enough (i.e., below an upper bound). Our\nevaluation shows that our algorithm performs better than\nthe previous state-of-the-art [\n10].\nAlgorithm 1 Targeted Adversarial Generation Algorithm\nagainst a VQA model\nInput: θ, x, Q, y t, B, ǫ, λ 1, λ 2, η, maxitr\nOutput: x⋆\n1 x1 ← x + δ for δ sampled from a uniform\ndistribution between [−B, B ];\n2 for i = 1→ maxitr do\n3 yp ← fθ(xi, Q );\n4 if yp = yt and i > 50 then\n5 return xi as x⋆;\n6 xi+1 ← update(xi, η, ∇xξ(yp));\n7 return xmaxitr+1 as x⋆;\nA VQA model takes an additional natural language in-\nput Q, and predicts an answer from a candidate set of K\nanswers. Similar to ( 1), a targeted adversarial example x⋆\ngiven a question Q is deﬁned to be a solution to:\nfθ(x⋆, Q ) =yt ∧ d(x⋆, x ) ≤ B (3)\nWe employ Algorithm 1 to generate the adversarial\nexample x⋆. The algorithm takes as input: model pa-\nrameters θ, source image x, question Q, target answer\nyt, the distance bound B, and several hyper-parameters:\nǫ, λ 1, λ 2, η, maxitr. This algorithm iteratively approxi-\nmates the optimal solution to the following objective:\nξ(yp) = L(Jθ(x⋆, Q ), y t)\n+λ1 · 1(yt ̸= yp) · (τ − L(Jθ(x⋆, Q ), y p))\n+λ2 · ReLU(d(x⋆, x ) − B + ǫ) (4)\nand returns the ﬁnal result as output. There are two termi-\nnating conditions: (1) after at least 50 iterations, if the pre-\ndiction matches the target, then the algorithm stops and re-\nturns the current xi as output; or (2) after a maximal number\nof iterations ( maxitr), if the prediction still does not match\nthe target, the algorithm returns xmaxitr+1 as output.\nWe now take a closer look at (\n4). yp denotes the pre-\ndiction in each iteration. The objective ( 4) contains three\ncomponents. The ﬁrst is the same as in ( 2). The second\ncomponent maximizes the difference between Jθ(x, Q ) and\nthe prediction yp when yp is not the target yt. τ is a con-\nstant, e.g., log(K), set to ensure that the second compo-\nnent is always non-negative. The third component mod-\nels the constraint d(x⋆, x ) ≤ B in (\n3). ǫ is a small con-\nstant set to (L(fθ(x, Q ), y t) +λ1τ)/λ2 ensures that the ad-\nversarial example x⋆ which optimizes ( 4) always satisﬁes\nd(x⋆, x ) ≤ B. By using a ReLU function, our attack no\nlonger minimizes the distance d(x⋆, x ) if it is smaller than\nB − ǫ. In practice we choose d(x, x ⋆) = ||x − x⋆||2/\n√\nN\nand set B = 20. Other hyper-parameters η, maxitr are\nthe learning rate and the maximal number of iterations. We\ndefer a formal analysis to Appendix\nA.\n(a) Caption 1\n (b) Caption 2\n (c) Caption 3\n (d) Caption 4\n (e) Caption 5\nFigure 1: Top- K accuracy on the Caption Adataset averaged across 1000 images generated with each target caption\n4. Experiments With Dense Captioning\nIn this section, we evaluate our attacks on Dense-\nCap [ 30], the state-of-the-art dense captioning model.\nDenseCap employs a region proposal network to ﬁrst iden-\ntify the bounding boxes of objects, and then generates cap-\ntions for each bounding box. We obtain the pre-trained\nmodel from their website\n1.\nTo evaluate the attack, we use Visual Genome\ndataset [ 35], which was originally used to evaluate Dense-\nCap in [ 30]. For an extensive evaluation, we create the fol-\nlowing three attack sets from Visual Genome:\n1) Caption A.We randomly select 5 captions as the tar-\nget captions and 1000 images as the source images;\n2) Caption B.We randomly select 1000 captions as tar-\nget captions and 5 images as source images;\n3) Gold. We select 100 images where DenseCap model\ngenerates correct captions and manually select target cap-\ntions irrelevant to the images.\nFor each caption-image pair, we set the caption as the tar-\nget, and the image as the source to generate an adversarial\nexample. To evaluate the attack effectiveness, we measure\nthe percentage of top- K predictions from generated adver-\nsarial examples that match the target captions. We consider\ntwo metrics to determine caption matching:\n1) Exact-match. The two captions are identical.\n2) METEOR > ω . The METEOR score [\n38] between\nthe two captions is above a threshold ω. We consider the\nthreshold ω to be 0.15, 0.2, or 0.25, similar to [ 30].\nFormally, we measure Accµ,K(x⋆, C t) =∑K\ni=1 µ(Ct, C i)/K where Ct is the target caption,\nx⋆ is the adversarial example, Ci for i = 1, ..., K are the\ntop-K predictions for x⋆, and µ is the matching metric (i.e.,\nExact-match or METEOR > ω ).\n4.1. Results and Observations\nThe evaluation results on Caption A are presented in\nFigure\n1. Each subﬁgure shows the results for one target\ncaption. For each caption and each K ∈ {1, 2, 3, 4, 5}, we\n1https://github.com/jcjohnson/densecap\ncompute Accµ,K for each of the 1000 randomly selected\nimages, and report the average value of Accµ,K across 1000\nimages. Each plot contains such 5 top- K accuracy values\nfor each metric described above (see the legend).\nWe observe that using the metric derived from METEOR\nscore, the accuracy is higher than using the Exact-match\nmetric. This is intuitive, since Exact-match is an over-\nconservative metric, which may treat a semantically correct\ncaption as a wrong answer. In contrast, using METEOR\nscore as the metric can mitigate this issue. Even with Exact-\nmatch, we observe that all captions have an average top- K\naccuracy above 30%. Further, for target captions Caption 1-\n3, the top-1 accuracy is always above 50%. That means, at\nleast 500 generated adversarial examples can successfully\nfool the DenseCap system to produce the exact target cap-\ntions with the highest conﬁdence score.\nWe further investigate the number of attack “failures”\namong caption-image pairs in Caption A. The attack fails\nif none of the top- 5 predictions matches the target based\non METEOR > 0.15. We ﬁnd only 17 such caption-image\npairs, i.e., 0.35% of the entire set, which lead to adversarial\nattack failure. This means that for the rest 99.65% caption-\nimage pairs, the attacks are successful in the sense that there\nexists at least one prediction for each adversarial example\nthat matches the target caption. The 17 cases can be found\nin Appendix\nB.\nThe results on Caption B set are similar, and we ob-\nserve that 97.24% of the caption-image pairs can be suc-\ncessfully fooled in the sense described above. For the Gold\nset we ﬁnd that our attack fails only on one image. Due to\nspace limitations, we defer detailed results on Caption B\nand Gold sets to Appendix\nB.\nNote that the attack does not achieve a 100% success\nrate. We attribute it to two reasons: (1) it is challenging\nto train an RNN-based caption generation model to gener-\nate the exactly matching captions; and (2) the DenseCap\nnetwork involves randomness, and thus may not produce\nthe same results for all runs. Still, we observe that the at-\ntack success rate is over 97%, and thus we conclude that the\nDenseCap model can be fooled by adversarial examples.\n(a)\n (b)\n (c)\n (d)\n (e)\nFigure 2: Adversarial examples generated from different images with the target caption to be “a window on a building”.\n(a) head of a person\n (b) the plate is white\n (c) the water is calm\n (d) a key on a keyboard\n (e) this is an outside scene\nFigure 3: Adversarial examples generated from Image 4 with different target captions (shown as sub-ﬁgure captions).\n4.2. Qualitative Study\nWe conduct qualitative study to investigate the generated\nadversarial examples and their predictions. In Figure 2, we\npresent ﬁve adversarial examples generated for the same tar-\nget caption. We see that most of the predicted captions ex-\nactly match the target (e.g., all top-5 predictions for Fig-\nure\n2a and Figure 2b), or be semantically equivalent to the\ntarget (e.g., the top-2 prediction for Figure 2e). We further\nexamine the bounding boxes of the regions proposed by the\nmodel. We ﬁnd that the model localizes objects in the adver-\nsarial examples, although the caption generation module of\nthe model is completely fooled. For example, in Figure\n2c,\nthe model can successfully identify the plates, but label all\nof them as “a window on a building”.\nTo further understand this effect, in Figure\n3, we show\nthe adversarial examples generated from the same source\nimage but with different target captions. We observe that all\nadversarial images look identical to each other, and the re-\ngions proposed for different images are also similar. For ex-\nample, we observe that the top proposed regions for the ﬁrst\nfour images all circumscribe the tree on the left. However,\nthe top captions generated for this region are all different,\nand match the target captions very well.\n5. Experiments with VQA\nIn this section, we evaluate the previous state-of-the-art\nattack [\n10] and our novel algorithm on two VQA models.\nWe also investigate the effect of adversarial attacks on atten-\ntion maps of the VQA models to gain more insights about\nthe way the attacks work. Finally, we analyze the successes\nand failures of our attacks with respect to language prior .\nMore results on qualitative study, transferability, and fur-\nther investigations to the failure cases can be found in Ap-\npendix\nD and E.\n5.1. Models\nWe experiment with two state-of-the-art models for\nopen-ended visual question answering, namely the MCB\nmodel [\n16], which is the winner of the VQA challenge in\n2016, and the compositional model N2NMN [ 25]. Both\nmodels achieve similar performance on the VQA bench-\nmark [\n5], while being very different in terms of internal\nstructures. MCB relies on a single monolithic network\narchitecture for all questions, while N2NMN dynamically\npredicts a network layout for every given question. In our\nexperiments we investigate whether such compositional dy-\nnamic architecture is more resilient than the monolithic one.\nWe retrieve the pre-trained model of MCB from their\nwebsite\n2, and the pre-trained model of N2NMN by con-\ntacting the authors through email directly. The code imple-\nmenting N2NMN is acquired from the website.\n3 Notice that\nthe MCB model is trained not only on the VQA dataset but\n2https://github.com/akirafukui/vqa-mcb\n3https://github.com/ronghanghu/n2nmn\nMCB, ours\nN2NMN, ours\nN2NMN, carlini\nMCB, carlini\nFigure 4: CDF of adversarial probability on the Gold set.\nalso on the Visual Genome dataset [ 35], while the N2NMN\nmodel only considers the VQA dataset.\n5.2. Datasets\nTo evaluate different adversarial example generation al-\ngorithms we derive three datasets from the VQA dataset [\n5].\nIn particular, we choose source images and question-answer\ntargets from the VQA validation set as follows:\n1) VQA-A: We randomly select 6,000 question-answer\npairs and 5 source images to constitute 30,000 triples;\n2) VQA-B: We randomly select 5,000 source images and\n10 question-answer pairs to construct 50,000 triples;\n3) Gold: We manually select 100 triples, such that MCB\nand N2NMN models can correctly answer the questions\nbased on the images, and the target answers are plausible\nfor the questions but incorrect for the images.\nFor each triple of question-answer-image, we generate\nan adversarial example close to the source image using the\nanswer as the target. More details can be found in Ap-\npendix\nC.\n5.3. Evaluation metrics\nGiven a set of question-answer pairs (Q, y t) and the gen-\nerated adversarial examples {x⋆}, we evaluate two metrics:\nthe attack success rate and the adversarial probability .\nAttack success rate. The attack is considered successful if\nfθ(x⋆, Q ) =yt. The attack success rate is computed as the\npercentage of successful attacks over all triples in a dataset.\nAdversarial probability. The adversarial probability is\ncomputed as Jθ(x⋆, Q )yt , where J(·, ·)i indicates the i-th\ndimension of the softmax output. Adversarial probability\nindicates the conﬁdence score of the model to predict the\ntarget answer yt, and thus provides a ﬁne-grained metric.\n5.4. Results\nHere we report the overall success of adversarial attacks\non VQA models, and also compare our new algorithm de-\nscribed above with the performance of the previous attack\nalgorithm (CW [\n10]) applied to this novel VQA setting. We\npresent the quantitative results below, and defer more qual-\nitative results to Appendix\nD.1.\nImage # 1 2 3 4 5\nMCB ours 94.67 94.78 94.97 95.02 95.15\nCW [ 10] 94.10 94.28 94.27 94.52 94.78\nN2NMN ours 94.25 94.53 95.57 95.80 96.15\nCW [ 10] 93.82 93.78 95.02 95.08 95.37\nTable 1: Attack success rate (%) on VQA-A.\nMCB, ours\nN2NMN, ours\nN2NMN, carlini\nMCB, carlini\nFigure 5: CDF of adversarial probability on VQA-A.\nGold. For both MCB and N2NMN models, we achieve\n100% attack success rate using either approach. Note that\nboth models can correctly answer all the questions on the\noriginal source images. The 100% attack success rate for\nboth VQA models shows that both of them are vulnerable\nto targeted adversarial examples.\nWe inspect the adversarial probabilities of the generated\nadversarial examples, and plot the Cumulative Distribution\nFunction (CDF) in Figure\n4. Note that a lower CDF curve\nindicates a higher probability in general. From the ﬁgure we\nobserve that the CDF curve of N2NMN is above MCB’s, in-\ndicating that N2NMN is slightly more resilient than MCB.\nHowever, we also observe that for both models, almost in\nall cases the adversarial probability is above 0.7. Thus, we\nconclude that our attack is very successful at misleading the\nVQA models to predict the target answers. We also observe\nthat the CDF curve of CW attack is much higher than ours,\nshowing that our approach is more effective at achieving a\nhigh adversarial probability. Overall, we show that such at-\ntacks can be performed very successfully for target answers\nthat are meaningful to questions.\nVQA-A. We further investigate VQA adversarial examples\nacross a wide range of target question-answer pairs. We\nseparately compute the attack success rate using each im-\nage as the source. The results are presented in Table\n1, and\nthe corresponding CDF curves are plotted in Figure 5. We\ncan draw similar conclusions as for the Gold set: (1) the\nattack success rate is high, i.e., > 90%; (2) the adversarial\nprobability of our attack is high; and (3) our attack is more\neffective than CW attack.\nWe observe that the attack’s performance against\nN2NMN model is worse than against MCB. In particular,\nfrom Figure\n5, we see that the adversarial probability of at-\nOriginal image Benign attention maps Adversarial attention maps\nMCB Attention N2NMN Attention MCB Attention N2NMN Attention\nWhat is the man holding? Original answer: racket Target: phone\nWhat does this sign say? Original answer: stop Target: one way\nWhat type of vehicle is this? Original answer: train Target: bus\nTable 2: Attention maps of benign and adversarial images on MCB and N2NMN models.\ntacks generated on the N2NMN model is signiﬁcantly lower\nthan the MCB model. This further shows that N2NMN\nmodel is somewhat more robust against adversarial attacks.\nWe also observe that the attack success rate with respect to\ndifferent images does not vary too much. We hypothesize\nthat the attack success is not sensitive to a source image,\nbut more dependent on a target question-answer pair. Our\nfurther investigations on VQA-B and language priors below\nprovide more evidence to conﬁrm this hypothesis.\nThe attack success rate is not 100%, which shows that\nthere exist a few question-answer pairs where neither ours\nnor the CW attack can succeed. In fact, for these question-\nanswer pairs, we have also tried other attack methods and\nnone of them can succeed in fooling the victim VQA model.\nWe ﬁnd that these question-answer pairs tend to appear in-\nfrequently in the training set, and this observation leads to\nour hypothesis regarding language prior . We present more\nanalysis of the language prior in the following section.\nVQA-B. We test the hypothesis that the attack success rate\nis not strongly dependent on the choice of source images us-\ning the VQA-B dataset. In our evaluation, we observe that\nfor 9 out of 10 question-answer pairs, the adversarial exam-\nples generated from any of the 5,000 source images fool the\nvictim model with 100% attack success rate. For the one\nremaining question-answer pair, however, we cannot gener-\nate successful adversarial examples from any of the source\nimages. This result further conﬁrms our hypothesis. Inter-\nestingly, we observe that the “hard” question-answer pairs\nfor the two VQA models are different. For the MCB model,\nthe question is “Why is the girl standing in the middle of\nthe room with an object in each hand?” with the target\nanswer “playing wii”; for the N2NMN model, the ques-\ntion and answer are “Who manufactured this plane?” and\n“japan”, respectively. This suggests that the hard question-\nanswer pairs are model-speciﬁc, which further motivates us\nto investigate language prior in VQA models.\n5.5. Adversarial examples fool attention mechanism\nWe conduct a qualitative study to gain more insights as to\nhow the attack succeeds. In the following we use the Gold\ndataset. In particular, both models in our experiments have\nattention mechanism. That is, to answer a question, a model\nﬁrst computes an attention map , which is a weight distri-\nbution over local features extracted from a CNN based on\nthe image and the question. Intuitively, a well-performing\nmodel should put more weight, i.e. attend to, the image\nregion that is most informative to answer the question.\nWe demonstrate the attention heatmaps for three source\nimages and their adversarial counterparts in Table\n2. We ob-\nserve that the adversarial examples mislead the VQA mod-\nels to ignore the regions that support the correct answer to\nthe question. For example, in the second source image both\nMCB and N2NMN focus on the stop sign when answer-\ning the question. The adversarial examples fool MCB and\nN2NMN to pay attention to the street sign instead, which\nleads to predicting a one-way trafﬁc sign, likely because\nboth signs are long rectangular metal plates. In the last ex-\nample, the attention is mislead to ignore the rail tracks but\nfocusing on the windows which look similar to those on a\nbus. Therefore, we observe that adversarial examples can\nfool both the attention and the classiﬁcation component of\nthe VQA models to achieve the malicious goal.\n5.6. Language Prior\nWe illustrate the language prior phenomenon in Fig-\nure\n6. It provides an example which cannot be successfully\nattacked by any algorithm in our evaluation. We show an\nadversarial example generated by our attack algorithm, and\nthe top-5 predictions from the MCB model. Clearly, the\nmodel is confused about the image and the question. The\nanswer with the highest probability only has a probability\nof less than 5%. Although the model is confused, it cannot\nbe fooled to predict the target answer “partly” to the ques-\ntion “what animal is next to the man?”. This observation\nis different than those reported in the literature [\n66], i.e.,\nthat targeted adversarial examples can always be success-\nfully generated against an image classiﬁer regardless of the\nimage and the target label. We believe that the observed\nphenomenon is due to the internal mechanism of a VQA\nmodel which learns to process natural language questions\nand predict semantically relevant answers.\nIn all previous experiments we choose question-answer\npairs from the VQA validation set, and thus the answers are\nlikely meaningful to the questions. To evaluate the effect of\nlanguage prior we construct the Non-Sense dataset. Specif-\nically, we choose question-answer pairs, such that answers\ndo not match the questions semantically, as they belong to\nquestions of a different type (e.g. “what color” vs. “how\nmany”). We ﬁnd that the attack success rates using our ap-\nproach against MCB and N2NMN are only 7.8% and 4.6%\nrespectively; the corresponding numbers for CW attack are\neven lower, 6.8% and 3.8%. This experiment further con-\nﬁrms the signiﬁcance of the language prior.\nPrior work has noted the effect of language prior, i.e. that\nthe VQA models capture the training data biases and tend\nto predict the most frequent answers [\n1, 20, 27, 31]. We\nﬁnd that N2NMN is more inﬂuenced by language prior than\nMCB. Speciﬁcally, N2NMN produces a smaller number of\ndistinct answers, predicting question-relevant answers inde-\npendent of image content. This may explain why it is more\ndifﬁcult to achieve a high probability on some targets with\nN2NMN than with MCB. We include more results and anal-\nysis in Appendix\nE.\nSource image Adversarial example\nRank Answer Probability\n1 yes 0.042\n2 middle 0.041\n3 on wall 0.040\n4 left 0.031\n5 background 0.025\nFigure 6: The effect of language prior. The target question /\nanswer are “ What animal is next to the man? ”/“partly”. We\nshow the top-5 predictions from MCB after the attack.\n6. Conclusion\nIn this work, we study adversarial attacks against vi-\nsion and language models, speciﬁcally, dense captioning\nand visual question answering models. The models in our\nstudy are more complex than previously studied image clas-\nsiﬁcation models, in the sense that they contain language\ngeneration component, localization, attention mechanism,\nand/or compositional internal structures. Our investigation\nshows that (1) we can generate targeted adversarial exam-\nples against all victim models in our study with a high\nsuccess rate (i.e., > 90%); and (2) the attacks can ei-\nther retain the localization output or also fool the attention\nheatmaps to fool the victim model. While studying attacks\non VQA models, as additional contributions, we propose a\nbetter attack method than the previous state-of-the-art ap-\nproach. Also, we observe and evaluate the effect of lan-\nguage prior that may explain which question-answer pairs\nrepresent harder targets. Our work sheds new light on un-\nderstanding adversarial attacks on complex vision and lan-\nguage systems, and these observations will inform future\ndirections towards building effective defenses.\nAcknowledgement\nThis work was supported in part by the National Sci-\nence Foundation under Grant No. TWC-1409915, Berke-\nley DeepDrive, DARPA STAC under Grant No. FA8750-\n15-2-0104, and Center for Long-Term Cybersecurity. Any\nopinions, ﬁndings, and conclusions or recommendations ex-\npressed in this material are those of the author(s) and do not\nnecessarily reﬂect the views of the National Science Foun-\ndation.\nReferences\n[1] A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi. Don’t\njust assume; look and answer: Overcoming priors for visual\nquestion answering. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , 2018.\n8\n[2] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,\nS. Gould, and L. Zhang. Bottom-up and top-down at-\ntention for image captioning and vqa. arXiv preprint\narXiv:1707.07998, 2017.\n2\n[3] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning\nto compose neural networks for question answering. In Proc.\nof NAACL , 2016. 2\n[4] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural\nmodule networks. In Proc. of CVPR , 2016. 2\n[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.\nZitnick, and D. Parikh. Vqa: Visual question answering. In\nProc. of ICCV , 2015.\n5, 6\n[6] A. Athalye and I. Sutskever. Synthesizing robust adversarial\nexamples. arXiv preprint arXiv:1707.07397 , 2017. 1\n[7] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine trans-\nlation by jointly learning to align and translate. In Proc. of\nICLR, 2015. 2\n[8] N. Carlini and D. Wagner. Defensive distillation is not robust\nto adversarial examples. arXiv preprint arXiv:1607.04311 ,\n2016. 1, 2\n[9] N. Carlini and D. Wagner. Adversarial examples are not eas-\nily detected: Bypassing ten detection methods. In AISec,\n2017. 1\n[10] N. Carlini and D. Wagner. Towards evaluating the robustness\nof neural networks. In 2017 IEEE Symposium on Security\nand Privacy (SP) , pages 39–57. IEEE, 2017. 2, 3, 5, 6, 12,\n13\n[11] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,\nS. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-\nrent convolutional networks for visual recognition and de-\nscription. In Proc. of CVPR , 2015.\n2\n[12] H. Fang, S. Gupta, F. N. Iandola, R. Srivastava, L. Deng,\nP. Doll ´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zit-\nnick, and G. Zweig. From captions to visual concepts and\nback. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2015.\n2\n[13] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner.\nDetecting adversarial samples from artifacts. arXiv preprint\narXiv:1703.00410, 2017. 2\n[14] V . Fischer, M. C. Kumar, J. H. Metzen, and T. Brox. Ad-\nversarial examples for semantic image segmentation. arXiv\npreprint arXiv:1703.01101 , 2017. 2\n[15] K. Fu, J. Jin, R. Cui, F. Sha, and C. Zhang. Aligning where\nto see and what to tell: Image captioning with region-based\nattention and scene-speciﬁc contexts. 2016.\n2\n[16] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,\nand M. Rohrbach. Multimodal compact bilinear pooling for\nvisual question answering and visual grounding. In Pro-\nceedings of the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 2016.\n2, 5\n[17] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.\nAre you talking to a machine? dataset and methods for mul-\ntilingual image question answering. In Proc. of NIPS , 2015.\n2\n[18] Z. Gong, W. Wang, and W.-S. Ku. Adversarial and clean data\nare not twins. arXiv preprint arXiv:1704.04960 , 2017. 2\n[19] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and\nharnessing adversarial examples. In Proc. of ICLR , 2015. 1,\n2\n[20] Y . Goyal, T. Khot, D. Summers-Stay, D. Batra, and\nD. Parikh. Making the v in vqa matter: Elevating the role\nof image understanding in visual question answering. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR) , 2017.\n8\n[21] K. Grosse, P. Manoharan, N. Papernot, M. Backes, and\nP. McDaniel. On the (statistical) detection of adversarial ex-\namples. arXiv preprint arXiv:1702.06280 , 2017.\n2\n[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2016. 19\n[23] W. He, J. Wei, X. Chen, N. Carlini, and D. Song. Adversar-\nial example defenses: Ensembles of weak defenses are not\nstrong. arXiv preprint arXiv:1706.04701 , 2017.\n1, 2\n[24] J. Hendrik Metzen, M. Chaithanya Kumar, T. Brox, and\nV . Fischer. Universal adversarial perturbations against se-\nmantic image segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2017.\n2, 3\n[25] R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.\nLearning to reason: End-to-end module networks for visual\nquestion answering. In Proc. of ICCV , 2017.\n1, 2, 5\n[26] S. Huang, N. Papernot, I. Goodfellow, Y . Duan, and\nP. Abbeel. Adversarial attacks on neural network policies.\narXiv preprint arXiv:1702.02284 , 2017.\n2\n[27] A. Jabri, A. Joulin, and L. van der Maaten. Revisiting visual\nquestion answering baselines. In European conference on\ncomputer vision (ECCV) , 2016. 8\n[28] R. Jia and P. Liang. Adversarial examples for evaluating\nreading comprehension systems. In Proceedings of the Con-\nference on Empirical Methods in Natural Language Process-\ning (EMNLP) , 2017.\n2\n[29] J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman,\nL. Fei-Fei, C. L. Zitnick, and R. Girshick. Inferring and ex-\necuting programs for visual reasoning. In Proc. of ICCV ,\n2017.\n2\n[30] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully\nconvolutional localization networks for dense captioning. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 4565–4574, 2016.\n1, 2, 3, 4\n[31] K. Kaﬂe and C. Kanan. An analysis of visual question an-\nswering algorithms. In 2017 IEEE International Conference\non Computer Vision (ICCV) , 2017. 8\n[32] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In Proc. of CVPR ,\n2015. 2\n[33] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying\nvisual-semantic embeddings with multimodal neural lan-\nguage models. Transactions of the Association for Compu-\ntational Linguistics (TACL) , 9:595–603, 2015.\n2\n[34] J. Kos and D. Song. Delving into adversarial attacks on deep\npolicies. arXiv preprint arXiv:1705.06452 , 2017. 2\n[35] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,\nS. Chen, Y . Kalantidis, L.-J. Li, D. A. Shamma, et al. Vi-\nsual genome: Connecting language and vision using crowd-\nsourced dense image annotations. International Journal of\nComputer Vision , 123(1):32–73, 2017.\n4, 6\n[36] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce,\nP. Ondruska, I. Gulrajani, and R. Socher. Ask me anything:\nDynamic memory networks for natural language processing.\nIn Proceedings of the International Conference on Machine\nLearning (ICML) , 2016.\n2\n[37] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial ma-\nchine learning at scale. In Proc. of ICLR , 2017. 2\n[38] M. D. A. Lavie. Meteor universal: Language speciﬁc trans-\nlation evaluation for any target language. In Proceedings\nof the Annual Meeting of the Association for Computational\nLinguistics (ACL) , page 376, 2014.\n4\n[39] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE , 86(11):2278–2324, 1998. 12\n[40] Y . Li, W. Ouyang, B. Zhou, K. Wang, and X. Wang. Scene\ngraph generation from objects, phrases and caption regions.\nIn Proc. of ICCV , 2017.\n2\n[41] Y .-C. Lin, Z.-W. Hong, Y .-H. Liao, M.-L. Shih, M.-Y . Liu,\nand M. Sun. Tactics of adversarial attack on deep reinforce-\nment learning agents. In Proc. of IJCAI , 2017.\n2\n[42] C. Liu, J. Mao, F. Sha, and A. Yuille. Attention correctness\nin neural image captioning. In Proc. of AAAI , 2017. 2\n[43] Y . Liu, X. Chen, C. Liu, and D. Song. Delving into transfer-\nable adversarial examples and black-box attacks. In Proc. of\nICLR, 2017. 1, 2, 13, 17, 19\n[44] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to\nworry about adversarial examples in object detection in au-\ntonomous vehicles. arXiv preprint arXiv:1707.03501 , 2017.\n1\n[45] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. Standard detectors\naren’t (currently) fooled by physical adversarial stop signs.\narXiv preprint arXiv:1710.03337 , 2017.\n1\n[46] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when\nto look: Adaptive attention via a visual sentinel for image\ncaptioning. In Proc. of CVPR , 2017.\n2\n[47] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical co-\nattention for visual question answering. Proc. of NIPS , 2,\n2016. 2\n[48] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and\nA. Vladu. Towards deep learning models resistant to ad-\nversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.\n1,\n2\n[49] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu-\nrons: A neural-based approach to answering questions about\nimages. In Proc. of CVPR , pages 1–9, 2015.\n2\n[50] J. Mao, W. Xu, Y . Yang, J. Wang, Z. Huang, and A. Yuille.\nDeep captioning with multimodal recurrent neural networks\n(m-rnn). In Proc. of ICLR , 2015.\n2\n[51] D. Meng and H. Chen. Magnet: a two-pronged defense\nagainst adversarial examples. In CCS, 2017. 1, 2\n[52] J. H. Metzen, T. Genewein, V . Fischer, and B. Bischoff. On\ndetecting adversarial perturbations. In 2017, 2017. 1, 2\n[53] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and\nP. Frossard. Universal adversarial perturbations. In Proc.\nof CVPR , 2017. 2, 17\n[54] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecog-\nnizable images. In Proc. of CVPR , pages 427–436. IEEE,\n2015.\n2\n[55] H. Noh, P. H. Seo, and B. Han. Image question answering\nusing convolutional neural network with dynamic parameter\nprediction. In Proc. of CVPR , 2016.\n2\n[56] N. Papernot, P. McDaniel, and I. Goodfellow. Transferability\nin machine learning: from phenomena to black-box attacks\nusing adversarial samples. arXiv preprint arXiv:1605.07277 ,\n2016.\n1, 2, 17, 19\n[57] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha,\nZ. Berkay Celik, and A. Swami. Practical black-box attacks\nagainst deep learning systems using adversarial examples.\narXiv preprint arXiv:1602.02697 , 2016.\n19\n[58] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,\nand A. Swami. The limitations of deep learning in adversar-\nial settings. In Proc. of Euro S&P , pages 372–387. IEEE,\n2016.\n2\n[59] N. Papernot, P. McDaniel, A. Swami, and R. Harang. Craft-\ning adversarial input sequences for recurrent neural net-\nworks. In MILCOM. IEEE, 2016.\n2\n[60] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.\nDistillation as a defense to adversarial perturbations against\ndeep neural networks. In Security and Privacy (SP), 2016\nIEEE Symposium on , pages 582–597. IEEE, 2016.\n1, 2\n[61] M. Pedersoli, T. Lucas, C. Schmid, and J. Verbeek. Areas\nof attention for image captioning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 2017.\n2\n[62] M. Ren, R. Kiros, and R. Zemel. Image question answering:\nA visual semantic embedding model and a new dataset. In\nProc. of NIPS , 2015.\n2\n[63] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nProc. of NIPS , 2015.\n3\n[64] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus\nregions for visual question answering. In Proc. of CVPR ,\n2016. 2\n[65] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. arXiv:1409.4842, 2014.\n1\n[66] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,\nI. Goodfellow, and R. Fergus. Intriguing properties of neural\nnetworks. In Proc. of ICLR , 2014.\n1, 2, 8, 17\n[67] F. Tram `er, A. Kurakin, N. Papernot, D. Boneh, and P. Mc-\nDaniel. Ensemble adversarial training: Attacks and defenses.\narXiv preprint arXiv:1705.07204 , 2017.\n1, 2\n[68] F. Tram `er, N. Papernot, I. Goodfellow, D. Boneh, and P. Mc-\nDaniel. The space of transferable adversarial examples.\narXiv preprint arXiv:1704.03453 , 2017.\n2\n[69] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and\ntell: A neural image caption generator. arXiv:1411.4555,\n2014. 2\n[70] C. Xie, J. Wang, Z. Zhang, Y . Zhou, L. Xie, and A. Yuille.\nAdversarial examples for semantic segmentation and object\ndetection. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2017.\n2, 3\n[71] C. Xiong, S. Merity, and R. Socher. Dynamic memory net-\nworks for visual and textual question answering. In Proceed-\nings of the International Conference on Machine Learning\n(ICML), 2016.\n2\n[72] H. Xu and K. Saenko. Ask, attend and answer: Exploring\nquestion-guided spatial attention for visual question answer-\ning. In Proceedings of the European Conference on Com-\nputer Vision (ECCV) , 2016.\n2\n[73] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-\nnov, R. Zemel, and Y . Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In Interna-\ntional Conference on Machine Learning , pages 2048–2057,\n2015.\n2\n[74] L. Yang, K. Tang, J. Yang, and L.-J. Li. Dense captioning\nwith joint inference and visual context. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) , 2017.\n2\n[75] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked\nattention networks for image question answering. In Proc. of\nCVPR, 2016. 2\n[76] Z. Yang, Y . Yuan, Y . Wu, R. Salakhutdinov, and W. W. Co-\nhen. Encode, review, and decode: Reviewer module for cap-\ntion generation. In Proc. of NIPS , 2016.\n2\n[77] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-\ntioning with semantic attention. In Proc. of CVPR , pages\n4651–4659, 2016. 2\n[78] D. Yu, J. Fu, T. Mei, and Y . Rui. Multi-level attention net-\nworks for visual question answering. In Proc. of CVPR ,\n2017. 2\n[79] Y . Yu, H. Ko, J. Choi, and G. Kim. End-to-end concept\nword detection for video captioning, retrieval, and question\nanswering. In Proc. of CVPR , 2017.\n2\n[80] Y . Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W:\nGrounded Question Answering in Images. In Proc. of CVPR ,\n2016. 2\nA. Implementation details of improved attacks\nto VQA models\nIn this appendix, we detail our attack strategy against\nVQA models, which is brieﬂy sketched in Section 3.3. We\nﬁrst provide in A.1 the full background setup and some ter-\nminology, as well as a formal deﬁnition of targeted adver-\nsarial examples for VQA models. Then, we present the at-\ntack details in\nA.2 and A.3 using the terminology deﬁned\nin A.1. In A.4, we will explain other implementation de-\ntails, such as hyperparameter values, used in our evaluation.\nIn the end, we provide some proofs to the theoretical analy-\nsis behind our technique in\nA.5.\nA.1. Targeted adversarial examples for VQA\nWe denote a VQA model as fθ(I, Q ), where θ is the pa-\nrameters of the model, I is the input image, and Q is the\ninput question. The output fθ(I, Q ) is the predicted answer\nto the question Q given the image I.\nMost existing VQA models consider this task as a clas-\nsiﬁcation problem. That is, they choose the most probable\nanswer among the top-K most frequent answers in the train-\ning set (or both training and testing set). Typically, state-of-\nthe-art VQA models use K = 3000.\nWe consider that the target to a VQA model fθ is a\nquestion-answer pair (Qtarget, A target), and a targeted ad-\nversarial example is an image Iadv such that\nfθ(Iadv, Q target) =Atarget s.t. d (Iadv, I ori) ≤ B\nwhere we refer to Iori as the benign image .\nA neural network-based VQA model fθ can also be rep-\nresented as fθ(I, Q ) =argmaxiJθ(I, Q ), where Jθ(I, Q )\noutputs a K-dimensional vector in which each dimension\nindicates the probability of corresponding choice to be the\npredicted answer. Therefore, we can generate adversarial\nexamples by solving the following optimization problem\nargminIadv L(Jθ(Iadv, Q target), A target) (5)\ns.t. d (Iadv, I ori) ≤ B (6)\nwhere Iori is a benign image, and the goal is to ﬁnd an ad-\nversarial example Iadv that is close to Iori. Typically, L is\nchosen as the same loss function for training the model, but\nother alternatives which are monotonic to the training loss\ncan also be used. In particular, Carlini et al . show that the\nchoice of different loss functions has a signiﬁcant impact on\nthe attack success rate [\n10], when the attacks are evaluated\non MNIST dataset [ 39]. In this work, we consider L to be\nthe cross-entropy loss, which is equivalent to the best loss\nfunction used in [\n10].\nA.2. Solving the optimization problem\nIn our attack method, we approximate the optimization\nproblem using an alternative objective function ( 4). We re-\nstate it below using the notation deﬁned in A.1:\nξ(Apredict) = L(Jθ(x, Q target), A target)\n+λ1 · 1(Atarget ̸= Apredict)\n·(τ − L(Jθ(x, Q target), A predict))\n+λ2 · ReLU(d(x, I ori) − B + ǫ) (7)\nIn this formula, we use x to represent the image. Thus\nthe adversarial example is the value of x that minimizes the\nobjective ( 7). This objective has three components. The ﬁrst\ncomponent, L(Jθ(x, Q target), A target), is the same as ob-\njective ( 5). The latter two are the innovations in this work,\nand we elaborate their design in the following.\nThe second component. The second component is\nλ1·1(Atarget ̸= Apredict)·(τ−L(Jθ(x, Q target), A predict))\nHere the hyperparameter λ1 is used to balance this com-\nponent and others, and Apredict is the prediction of the\noriginal image. The value of Apredict is set dynamically\nduring the iterative optimization process, so that each iter-\nation may choose a different value of Apredict. We will\nexplain this process in more details in the next subsection.\nWe set τ to be a constant, e.g., log(K) when L is cho-\nsen as the cross-entropy loss. This constant guarantees\nthe second term is always non-negative, especially when\n1(Atarget ̸= Apredict). In fact, we have the following\ntheorem:\nTheorem 1. Assuming τ = log K, where K is the num-\nber of output classes, L is the cross-entropy loss, i.e.,\nL(u, i ) =−log ui, the last layer of J is a softmax operator,\nand Apredict is the prediction of the model over input im-\nage x and question Qtarget, i.e., argmaxiJθ(x, Q target),\nthen we have\n1(Atarget ̸= Apredict)\n·(τ − L(Jθ(x, Q target), A predict)) ≥ 0 (8)\nTo understand how this component works, we consider\ntwo possible cases. First, in the case Apredict = Atarget,\nthe image generated in the last iteration is already an ad-\nversarial example, and thus this component is 0 since\n1(Atarget ̸= Apredict) = 0. In this case, optimizing ob-\njective (\n7) is equivalent to maximizing the probability of\npredicting the target answer Atarget.\nSecond, when Apredict ̸= Atarget, minimiz-\ning the second component is essentially maximizing\nL(Jθ(x, Q target), A predict), which is equivalent to min-\nimizing the probability of the model to predict Apredict,\nwhich is different from the target answer Atarget. As for\nAlgorithm 2 Targeted Adversarial Generation Algorithm\nInput: θ, I ori, Q target, A target, B, ǫ, λ 1, λ 2, η, maxitr\nOutput: Iadv\n1 I1 ← Iori + δ for δ sampled from a uniform\ndistribution between [−B, B ];\n2 for i = 1→ maxitr do\n3 Apredict = fθ(Ii, Q target);\n4 if Apredict = Atarget and i > 50 then\n5 return Ii\n6 Ii+1 ← update(Ii, η, ∇xξ(Apredict));\n7 return Imaxitr+1\nthe value of the hyperparameter λ1, which is used to bal-\nance between this component and others, we ﬁnd that set-\nting λ1 = 1 works well in most cases. Notice that in this\ncase, jointly optimizing the ﬁrst and the second component\nis equivalent to optimizing the best loss function used in\nCarlini’s attack.\nThe third component. The third component is set to\nenforce the constraint (\n6). In particular, ReLU(x) =\nmax(0, x ) is the rectiﬁer function, and ǫ is a small pos-\nitive hyper-parameter that we will explain later. When\nd(Iadv, I ori) ≤ B − ǫ < B , i.e., constraint (\n6) is satis-\nﬁed, the third component is 0, and thus has no effective on\nthe objective. On the other hand, if an adversarial exam-\nple Iadv does not satisfy constraint (\n6), we show that it is\nnever optimal for ( 7) when λ2ǫ is large enough. We have\nthe following theorem:\nTheorem 2. When λ2ǫ > L(fθ(Iori, Q target), A target) +\nλ1τ, the solution Iadv minimizing the objective (\n7) satisﬁes\nconstraint ( 6) as well.In practice, we can set ǫ to be a small value (e.g., 2), and\nset λ2 to be a large value (e.g., 10), then the generated ad-\nversarial examples end up not activating the ReLU function\n(i.e., the output of the function is 0). Even when the ReLU\nfunction is activated, its value is not larger than ǫ, and thus\nthe constraint (\n6) is still satisﬁed.\nNotice that in most previous iterative optimization-based\napproaches [ 10, 43], optimizing ( 5) while satisfying con-\nstraint ( 6) is converted into a joint optimization problem of\nL(...) +λd(...), which minimizes both the lost function ( 5)\nand the distance function d(Iadv, I ori). The most promi-\nnent difference is that our approach does not minimize this\ndistance as long as it is within the bound B.\nA.3. Putting everything together\nThe overall adversarial generation method is presented\nin Algorithm\n1 (see the main paper). We restate the algo-\nrithm in Algorithm 2 using the notation deﬁned in A.1, and\nexplain the details below.\nThis algorithm takes the hyper-parameters deﬁned\nabove, along with η, representing the learning rate, and\nmaxitr, representing the maximal number iterations that\nthe algorithm runs. In the algorithm, I1 is initialized with a\nrandom starting point satisfying constraint ( 6) (line 1). Then\nthe algorithm iteratively updates Ii (lines 2-6). In each iter-\nation, the prediction Apredict is ﬁrst computed (line 3). If\nthis prediction already matches the target, and the algorithm\nhas run for at least 50 iterations, the algorithm stops and\nreturns Ii as a successful adversarial example (lines 4-5).\nHere, 50 is a hyperparameter that can be further tuned. In\nthis work, we ﬁx it to be 50 in all experiments. On the other\nhand, if the algorithm does not stop at line 5, then Ii+1 will\nbe updated based on the gradient ∇xξ(Apredict) and the\nlearning rate η (line 6). Here, update can be any optimiza-\ntion algorithm. We evaluated the algorithm’s performance\nby using SGD, Adam, or RMSProp, and found that Adam\nalways yields the best attack success rate. Therefore, we\nuse Adam as the update function through out this work. In\nthe end, if it does not return at line 5 during some iteration,\nthen the algorithm fails at ﬁnding an adversarial example,\nand it returns Imaxitr+1 as a result. In our evaluation, we\nset η = 1.0 and maxitr = 1000for evaluation.\nNote that Carlini et al . [\n10] also suggest running the op-\ntimization algorithm multiple times with different random\nstarting points (i.e., line 1) to avoid local optima. We em-\nploy the same trick and pick the best adversarial example\ngenerated among different executions of Algorithm\n1 as the\nﬁnal result.\nA.4. Adversarial example generation algorithms\ndetails\nIn our evaluation, we examine both attack methods, i.e.\nCarlini et al . [\n10] and our proposed algorithm. For Carlini’s\nattack, we choose to minimize the loss function:\nReLU(L(Jθ(x, Q target), A predict)\n− L(Jθ(x, Q target), A target)) +λd(x, I ori)\nwhere x = 255× (tanh(δ) + 1)/2 to simulate the boxed\nconstraint that each pixel value can only take value from\n[0, 255]. This approach is demonstrated to be the most ef-\nfective one in [\n10]. Here λ is chosen to be 0.1 by a grid\nsearch.\nFor our approach, as we discussed in Section 3, we\nchoose the values of hyper-parameters as follows: ǫ =\n2, λ 1 = 1, λ 2 = 10, η = 1.0, maxitr = 1000. Note that\nthese hyper-parameters are set based on each image being\nrepresented as a vector of pixel values from [0, 255].\nWhen we generate adversarial examples, we employ the\nRMSE distance function as used in [\n43]. In particular, as-\nsuming there are two N-dimensional vectors x1, x 2, then\nthe RMSE between the two vectors is computed as\nRMSE (x1, x 2) =\n√\n||x1 − x2||2\n2/N = ||x1 − x2||2/\n√\nN\nwhere || · ||2 denotes the L2-norm of a vector. Further, in\nall experiments, the bound on the distance B = 20. In our\nexperiments, the average distance for generated adversarial\nexamples is below 10. We demonstrate several adversarial\nexamples in Section ?? to illustrate that the generated adver-\nsarial examples are visually similar to their benign counter-\nparts.\nA.5. Proofs to the theorems\nWe now present the formal proofs to Theorem\n1 and The-\norem A.2 presented in A.2.\nProof of Theorem 1. We consider two cases between the\nrelationship between Atarget and Apredict. First, when\nAtarget = Apredict, the left-hand side of ( 8) is 0, and thus\n(8) is trivially true.\nSecond, when Atarget ̸= Apredict, then the left-hand\nside of (8) becomes\nτ − L(Jθ(x, Q target), A predict)\nThus proving ( 8) is equivalent to prove\nL(Jθ(x, Q target), A predict) ≤ τ = logK\nWe prove this by contradiction. Assuming\nL(Jθ(x, Q target), A predict) > log K, then we have\n−log Jθ(x, Q target)Apredict > log K\nand thus\nJθ(x, Q target)Apredict < 1/K\nBy deﬁnition, we have\nApredict = argmaxiJθ(x, Q target)i\nthus we know\n∀i ∈ {1, ..., K }.Jθ(x, Q target)i < 1/K\nTherefore, we know that\nK∑\ni=1\nJθ(x, Q target)i < K × (1/K) = 1\nHowever, we assume the last layer of J is a softmax layer,\nand thus we have\nK∑\ni=1\nJθ(x, Q target)i = 1\nwhich is a contradiction. Therefore, we conclude that The-\norem\n1 is true.\nProof of Theorem A.2. We prove this by contradiction. We\nassume an adversarial example Iadv = I⋆ does not satisfy\n(6), but optimizes ( 7). In this case, d(Iadv, I ori) > B >\nB−ǫ, and thus the ReLU function is activated and its output\nmust be greater than ǫ. Thus, the third component is at least\nλ2ǫ. Since the other two components are also non-negative,\ntherefore, the objective of ( 7) is at least λ2ǫ as well. On the\nother hand, we can set Iadv = Iori, so that the value of ob-\njective ( 7) is at most L(fθ(Iori, Q target), A target) +λ1τ.\nSince λ2ǫ > L(fθ(Iori, Q target), A target) +λ1τ, we have\nthat setting Iadv = Iori results in a lower value of objective\n(7) than Iadv = I⋆, which contradicts the assumption!\nB. Further evaluation on DenseCap\nWe present the top- K accuracy results for Caption Bin\nFigure 7. The 17 failed adversarial examples of Caption A\nare presented in Figure 9. We omit the 148 failed adversarial\nexamples of Caption Bdue to size limitation.\nWe also report the top-K accuracy results for our Gold\nset in Figure 8.\nC. VQA attack dataset construction details\nWe construct multiple attack datasets in our experiments.\nEach dataset contains a set of (I, Q, A ) triples, where I\nis a benign image, and (Q, A ) is a target question-answer\npair. We explain how these triples are selected in different\ndatasets below.\nGold. For this dataset, we manually create triples where\nthe target question is meaningful to the image, and the tar-\nget answer is incorrect to the question and image pairs. To\nachieve this goal, we randomly select 100 images. For each\nof them, we manually choose questions that are meaning-\nful to the image, while both MCB and N2NMN models can\nanswer correctly the questions based on the image. If none\nof such questions exist for an image, we replace it with an-\nother randomly selected image. We repeat this process until\nwe get 100 question-image pairs where both models predict\ncorrect answers. Then, for each question-image pair, we\nmanually choose an answer that makes sense for the ques-\ntion but is incorrect in the context of the image. In the end,\nwe have 100 (I, Q, A ) triples that constitute the Gold set.\nVQA-A. This dataset is designed to be a combination of\ntwo sub-datasets, i.e., Popular-QA and Rare-QA. These\ntwo aim at evaluating the resilience of the two VQA models\nagainst adversarial examples with different target question-\nanswer pairs.\nFor the Popular-QA dataset, we select 3,000 popular\nquestion-answer pairs. In particular, we ﬁrst remove all an-\nswers appearing less than 3 times along with their questions\nin the VQA training set. This is because we observe that\namong these least frequent answers, many are simply typos,\n(a) Image 1\n (b) Image 2\n (c) Image 3\n (d) Image 4\n (e) Image 5\nFigure 7: Top- K accuracy on the Caption Bdataset averaged across 5 images generated with each target caption\nFigure 8: The result for adversarial attack against DenseCap\nmodel on the Gold set.\n(e.g., spelling “kitchen” as “kitten”). Therefore, we remove\nthem from consideration.\nWe consider the top-1000 most frequent questions in the\nremaining set as popular. Further, for each popular ques-\ntion, we choose its top-3 most frequent answers and con-\nsider each corresponding question-answer pair as popular.\nTo ensure each question has at least 3 answers, we also re-\nmove all questions with less than 3 answers before selecting\nthe top-1000 most frequent questions.\nWe are interested in the popular question-answer pairs,\nbecause they appear more frequently in the training set, and\nthus the models may more likely remember these question-\nanswer pairs. Therefore, we hypothesize that it is more\nlikely to successfully generate an adversarial example with\nsuch a target for an irrelevant image. We create this dataset\nto test this hypothesis.\nWe also randomly select 5 images, which are provided\nin the top raw of Figure\n11. For each question-answer pair\n(Q, A ) and each image I, we add the triple (I, Q, A ) to the\nPopular-QA set. In the end, there are 15,000 triples in this\ndataset.\nThe second dataset, i.e., Rare-QA, is similar to Popular-\nQA, but the question-answer pairs are rare. In particular,\nwe ﬁlter out the answers appearing less than 3 times, and\nall questions with less than 3 remaining answers in the same\nway as during construction of Popular-QA.\nAmong the remaining questions, we select the top-1000\nleast frequent ones, and for each of them, we select the three\nPopular question-answer pairs\nQA1 What room is this? bathroom\nQA2 What sport is this? baseball\nQA3 What is the man doing? skateboarding\nQA4 What is the man holding? frisbee\nQA5 Is it raining? no\nRare question-answer pairs\nQA1 What vegetable can be seen? carrot\nQA2 What is the fence covered with? net\nQA3 What does the blue signs represent? handicap\nQA4\nWhy is the girl standing in the\nmiddle of the room with an object playing wii\nin each hand?\nQA5 Who manufactured this plane? japan\nTable 3: The question-answer pairs used in Scale-Image,\npopular (top) and rare (bottom).\nleast frequent answers. We consider the question-answer\npairs selected by such criteria as rare, and in the end, we\nhave 3,000 rare question-answer pairs. We use the same 5\nimages as in Popular-QA, and generate a triple using each\nquestion-answer pair and each image to construct 15,000\ntriples which constitute Rare-QA.\nIn doing so, we can evaluate the resilience of the two\nVQA models against adversarial examples on both popular\nquestion-answer pairs and rare question-answer pairs.\nVQA-B. This dataset is similar to VQA-A, but is designed\nto evaluate the adversarial generation algorithm’s perfor-\nmance across different benign images. To this end, we ran-\ndomly select ﬁve popular question-answer pairs and ﬁve\nrare question-answer pairs, listed in Table\n3, as well as\n5,000 images to construct 50,000 triples in total. These\ntriples constitute Scale-Image.\nD. More Results on Experiments with VQA\nWe present the CDF curves of adversarial examples\ngenerated using both our approach and Carlini’s approach\nFigure 9: 17 failing adversarial examples generated from Caption A. For all these examples, the target caption is “white\nclouds in blue sky”.\nagainst MCB and N2NMN from the ﬁve images used in\nVQA-A, but we separately plot the analysis for Popular-\nQA and Rare-QA. For each combination of attack-model-\ndataset, we plot ﬁve curves on the same ﬁgure. The re-\nsults are presented in Figure\n10. The results show that for\nthe same speciﬁcation, the CDF curves for different images\nare close to each other. This shows that the attack perfor-\nmance is less dependent on images and more on the QA\ntargets. We also see that the Rare-QA targets are more dif-\nﬁcult than Popular-QA targets, and that our attack achieves\nhigher probabilities that the Carlini’s attack.\nD.1. Qualitative study\nFigure\n11 presents some qualitative examples from our\nexperiments on the Rare-QA pairs. We provide both benign\nimages and adversarial examples generated against MCB\nand N2NMN. We observe that it is hard to distinguish the\nbenign images from adversarial ones visually.\nWe show the highest predictions of both VQA models\non the benign images (top) and on the adversarial examples\ngenerated for the target QA pairs (bottom). We show targets\nin “[]” and highlight the failed attacks in italics. First, we\nnote that even for the questions irrelevant to the images, ini-\ntially, both VQA models can make reasonable predictions.\nWe then review the models’ behavior on the adversarial\nexamples. We observe that the MCB model is more fre-\nquently fooled by the adversaries than the N2NMN model,\nfor instance in the case of the ﬁrst question. For the second\nquestion both models predict “left” instead of the target “to\nleft”, so essentially the attack succeeds, but it is counted as\na failure case. Therefore, our quantitative results provide\nan over-conservative estimation on the attack success rate.\nFinally, for the third question all the attacks fail, and top\npredictions such as “yes” indicate the models’ confusion.\nInterestingly, N2NMN model predicts “military” instead of\n“navy” for Image 2, which can also be counted as a success.\nD.2. Transferability Discussion\nIn this work, we focus on white-box adversarial exam-\nples, which means that the generation of these adversar-\n(a) CDF on adversarial probability of adversarial examples gener-\nated by our approach against MCB on Popular-QA.\n(b) CDF on adversarial probability of adversarial examples gener-\nated by Carlini’s approach against MCB on Popular-QA.\n(c) CDF on adversarial probability of adversarial examples gener-\nated by our approach against N2NMN on Popular-QA.\n(d) CDF on adversarial probability of adversarial examples gener-\nated by Carlini’s approach against N2NMN on Popular-QA.\n(e) CDF on adversarial probability of adversarial examples gener-\nated by our approach against MCB on Rare-QA.\n(f) CDF on adversarial probability of adversarial examples gener-\nated by Carlini’s approach against MCB on Rare-QA.\n(g) CDF on adversarial probability of adversarial examples gener-\nated by our approach against N2NMN on Rare-QA.\n(h) CDF on adversarial probability of adversarial examples gener-\nated by Carlini’s approach against N2NMN on Rare-QA.\nFigure 10: More CDF ﬁgures\nial examples requires full knowledge of the model archi-\ntectures. However, we also demonstrate that an adversary\ncould likely generate black-box adversarial examples with-\nout such knowledge. This is possible due to the transfer-\nability of adversarial examples, i.e., their ability to transfer\nbetween different network architectures [\n43, 53, 56, 66].\nPrevious work demonstrates transferability between: (1)\ntwo models with the same architecture trained on different\nPredictions on the benign images\nImage 1 Image 2 Image 3 Image 4 Image 5\nWhat are the people there for?\nMCB: baseball ﬂying kites airplane tennis elephants\nN2NMN: baseball kites ﬂying tennis parade\nWhere is the boy’s shadow?\nMCB: ground ground plane court ground\nN2NMN: ground kite sky tennis court ground\nWhy is the man wearing a head covering?\nMCB: protection safety safety tennis protection\nN2NMN: protection ﬂying kite safety sweat shade\nPredictions on the adversarial examples\nWhat are the people there for? [festival]\nMCB: festival festival festival festival festival\nN2NMN: parade parade parade parade festival\nWhere is the boy’s shadow? [to left]\nMCB: left left left left left\nN2NMN: left left left left left\nWhy is the man wearing a head covering? [navy]\nMCB: yes yes safety yes costume\nN2NMN: yes military yes yes yes\nFigure 11: MCB and N2NMN’s predictions on benign and adversarial images and QA pairs from Rare-QA. The target\nanswer is provided in “[]” along with the question. The text in italics indicates that the targeted adversarial examples do not\nmislead the model to produce the exact target answer.\ntraining data; (2) two models with different architectures\ntrained on the same training data; and (3) even a neural net-\nwork model and a non-neural network model (e.g., kNN,\nSVM). Most previous work demonstrates transferability of\nnon-targeted adversarial examples. In [\n43], Liu et al . fur-\nther demonstrate that almost none of targeted adversarial\nexamples generated for one model transfer to another one,\nand developed a novel approach to generate targeted adver-\nsarial examples for an ensemble of multiple state-of-the-art\nclassiﬁcation models to achieve better transferability.\nThe transferability of adversarial examples enables the\nadversary to generate black-box adversarial examples from\na white-box adversary. To do so, the adversary can simply\ngenerate adversarial examples against a white-box model\nthat performs the same task as the black-box model, and\nthese adversarial examples would transfer to the black-box\nmodel with a high probability. Papernot et al . show that they\ncan effectively generate non-targeted black-box adversarial\nexamples against black-box online machine learning sys-\ntems hosted by Amazon, Google and MetaMind [\n57, 56].\nFurther, Liu et al . demonstrate successful non-targeted\nand targeted black-box adversarial examples against Clar-\nifai.com, which is a commercial company providing state-\nof-the-art image classiﬁcation services [\n43].\nAgain, all these previous work only study image classi-\nﬁcation models. In this work, we are interested in the trans-\nferability of targeted adversarial examples between vision-\nlanguage models, which we show below.\nExperiment Results. We test the transferability of the gen-\nerated adversarial examples between MCB and N2NMN.\nWe use the Gold set to generate adversarial examples for\nthis evaluation. We ﬁnd that 79 out of 100 adversarial exam-\nples generated for the MCB model can transfer to N2NMN,\nwhile the number is 60 in the other direction. This shows\nthat adversarial examples on VQA models can transfer well,\nand thus opens the door for black-box attacks.\nNotice that in existing work [\n43], Liu et al . demonstrate\nthat it is non-trivial to generate transferable targeted adver-\nsarial examples from a single image classiﬁcation model.\nWe note that both MCB and N2NMN employ the same pre-\ntrained ResNet-152 features [\n22] as their image representa-\ntion. Thus, we attribute the good transferability results to\nthe use of ResNet-152 in both models.\nE. Analysis on Hard Targets for Generating\nAdversarial Examples\nWhile we observe that adversarial examples can be gen-\nerated for most target question-answer pairs, in some cases\nthe adversarial generation algorithm fails. We notice that\nwhether the attack will succeed or not depends on the tar-\nget question-answer pair rather than on the benign image.\nIn this section, we investigate the failure cases and provide\nsome insights into why some targeted attacks may be hard.\nE.1. The effectiveness of language priors\nAs we have observed in the experimental results de-\nscribed in Section\n5 (in the main paper), whether a question-\nanswer pair is a hard target depends more on the question-\nanswer pair itself and less on the image. Therefore, we hy-\npothesize that the language component in the VQA models\nmay prevent adversarial examples to fool the models with\ncertain targets. This phenomenon can be considered the lan-\nguage prior of VQA models. That is, given a question, if\nthe model is less likely to predict a certain answer, we are\nalso less likely to successfully generate targeted adversarial\nexamples using it as the target answer.\nIn this section, we evaluate this phenomenon to verify\nour hypothesis. In particular, we choose a question, “What\nsport is this?”. We ﬁrst evaluate the answer frequency as\nfollows. We run the VQA model on each of the 5,000 im-\nages in the VQA validation set and the selected question\nto get 5,000 answers. We compute the frequency of each\nanswer in this set.\nIntuitively, the answer frequency is a Monte-Carlo sim-\nulation of the answer distribution of the VQA model, and\nour goal is to examine the relationship between the answer\ndistribution and the success of using an answer as the tar-\nget to generate adversarial examples. In particular, we want\nto show that the answer frequency is positively correlated\nwith the adversarial probability for each answer. To this\nend, we sequentially set each answer as the target answer,\nwhile setting the question chosen above (i.e., “what sport is\nthis”) as the target question, and Image 1 in Figure\n11 as\nthe benign image. Then we compute the adversarial prob-\nability of each answer. We sort all the answers in the de-\nscending order of their adversarial probabilities, and jointly\nplot the adversarial probabilities and the answer frequen-\ncies. Figures\n12a and 12b show the corresponding plots\nfor MCB and N2NMN. In these plots, each point in the x-\naxis indicates a label of an answer, so that the answer with\nthe highest adversarial probability is labeled as 0, and so\non. The blue line plots the adversarial probability of all an-\nswers, while the red dots plot the answer frequency. We\nonly plot the answers whose frequency is at least 1, namely\nthe answers must appear in the model’s prediction set.\nFrom both ﬁgures, we can observe a clear relationship\nbetween the answer frequency and the adversarial probabil-\nity. That is, all answers with a frequency of 1 and higher can\nbe predicted with a large probability (e.g., > 0.1), and all\nthese answers can be used as targets to generate adversarial\nexamples. Further, we observe that the answer frequency\nloosely aligns with the adversarial probability. This obser-\nvation supports our hypothesis that the answer frequency is\npositively correlated with the adversarial probability.\nFurther, we observe that N2NMN has fewer answers\n(a) MCB\n (b) N2NMN\nMCB\nN2NMN\n(c) Answer frequency of MCB and\nN2NMN\nFigure 12: Answer frequency versus adversarial probability. Figure\n12a and Figure 12b show that the answer frequency\nis positively correlated with the adversarial probability on MCB and N2NMN respectively. Figure 12c shows the answer\nfrequency of the MCB model and the N2NMN model.\nwith a positive frequency. We illustrate this phenomenon\nin Figure\n12c. In this ﬁgure, we sort all answers in the de-\nscending order of their frequencies based on the N2NMN\nmodel, and the x-axis corresponds to their rank. The blue\nplot shows the distribution of the answer frequency com-\nputed based on the N2NMN model, while the orange dots\nare each answer’s frequency computed based on the MCB\nmodel. We can observe that many answers have a large\nfrequency based on the MCB model, but their frequency\nbased on the N2NMN model is 0. Therefore, combined with\nthe observation above, this demonstrates that the N2NMN\nmodel has a smaller range of answers that can be used as\nthe target to generate adversarial examples than the MCB\nmodel.\nNotice that all these answers are generated based on the\nsame questions. We investigate the results, and ﬁnd that\nmany of the answers predicted by the MCB model are irrele-\nvant to the question used in this evaluation. This shows that,\nsince N2NMN composes the network modules according to\nthe input question, it is more effective at constructing corre-\nsponding ﬁlter modules, which can eliminate the answers\nirrelevant to the question. On the other hand, the MCB\nmodel does not have this functionality, since its architecture\nis identical throughout all questions. Therefore, when an\nimage is less relevant to the question, the MCB model may\npredict answers considering the image more than the ques-\ntion. In this sense, the answer set of N2NMN is smaller than\nthe one of MCB, since the former only includes answers rel-\nevant to the question. This also indicates that N2NMN has a\nstronger language prior than MCB, which partially explains\nwhy N2NMN behaves slightly more resilient than MCB in\nour previous experiments.\nE.2. Meaningless question-answer targets\nWe further evaluate the effect of language prior by con-\nstructing a dataset of meaningless question-answer targets.\nWe select 100 questions from 5 categories starting with\n(1) “What color”; (2) “What animal”; (3) “Is”; (4) “How\nmany”; and (5) “Where”. Then we construct the set of\nmeaningful answers to each type of questions: for example,\n“silver” is a meaningful answer to a “what color” question.\nIn doing so, the answer assigned to one type of question\nis guaranteed to be meaningless to the questions in another\ntype. Thus we choose a meaningless answer for each of the\n100 questions. We use them as targets and the 5 images used\nin Popular-QA and Rare-QA as the benign images to gen-\nerate the adversarial examples. In the end, we observe that\nthe attack success rates using our approach against MCB\nand N2NMN are only 7.8% and 4.6% respectively; the cor-\nresponding numbers for Carlini’s attack are 6.8% and 3.8%\nrespectively. This experiment further conﬁrms the signif-\nicance of the language prior and again demonstrates that\nN2NMN is more resilient against adversarial examples than\nMCB.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.8944590091705322
    },
    {
      "name": "Closed captioning",
      "score": 0.8230409622192383
    },
    {
      "name": "Computer science",
      "score": 0.7156673669815063
    },
    {
      "name": "Modular design",
      "score": 0.6535595655441284
    },
    {
      "name": "Bounding overwatch",
      "score": 0.6193415522575378
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.5289958715438843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5168402791023254
    },
    {
      "name": "Natural language",
      "score": 0.5038556456565857
    },
    {
      "name": "Language model",
      "score": 0.47802790999412537
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.44759729504585266
    },
    {
      "name": "Work (physics)",
      "score": 0.4304533898830414
    },
    {
      "name": "Natural language processing",
      "score": 0.3532898426055908
    },
    {
      "name": "Cognitive science",
      "score": 0.35095757246017456
    },
    {
      "name": "Psychology",
      "score": 0.1285194754600525
    },
    {
      "name": "Image (mathematics)",
      "score": 0.12490099668502808
    },
    {
      "name": "Epistemology",
      "score": 0.10294678807258606
    },
    {
      "name": "Engineering",
      "score": 0.09329524636268616
    },
    {
      "name": "Programming language",
      "score": 0.08881875872612
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210109712",
      "name": "Max Planck Institute for Informatics",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I149899117",
      "name": "Max Planck Society",
      "country": "DE"
    }
  ]
}