{
  "title": "Speaking Multiple Languages Affects the Moral Bias of Language Models",
  "url": "https://openalex.org/W4385571491",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092596270",
      "name": "Katharina Haemmerl",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092596271",
      "name": "Bjoern Deiseroth",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2789883686",
      "name": "Patrick Schramowski",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Hessian Center for Artificial Intelligence",
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2310900135",
      "name": "Jindřich Libovický",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3181166206",
      "name": "Constantin Rothkopf",
      "affiliations": [
        "Technical University of Darmstadt",
        "Hessian Center for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2099684778",
      "name": "Alexander Fraser",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Munich Center for Machine Learning"
      ]
    },
    {
      "id": "https://openalex.org/A2252032993",
      "name": "Kristian Kersting",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Technical University of Darmstadt",
        "Hessian Center for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287116904",
    "https://openalex.org/W2948861349",
    "https://openalex.org/W3207835719",
    "https://openalex.org/W4285155368",
    "https://openalex.org/W3199411432",
    "https://openalex.org/W2892488560",
    "https://openalex.org/W3186903869",
    "https://openalex.org/W3112090146",
    "https://openalex.org/W4287887283",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4214903622",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W2799146523",
    "https://openalex.org/W2966551734",
    "https://openalex.org/W3123610095",
    "https://openalex.org/W3173650973",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2148165142",
    "https://openalex.org/W4385572734",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W2419539795",
    "https://openalex.org/W3103490574",
    "https://openalex.org/W3200538116",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W4287115969",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2037549259",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W2963281280",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3082760180",
    "https://openalex.org/W3203765809",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W4226287576",
    "https://openalex.org/W2117401803",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4285217655",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W4386566829",
    "https://openalex.org/W3177035927",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Katharina Haemmerl, Bjoern Deiseroth, Patrick Schramowski, Jindřich Libovický, Constantin Rothkopf, Alexander Fraser, Kristian Kersting. Findings of the Association for Computational Linguistics: ACL 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2137–2156\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSpeaking Multiple Languages Affects the Moral Bias of Language Models\nKatharina Hämmerl1,2 and Björn Deiseroth3,4 and Patrick Schramowski4,5,9\nJindˇrich Libovický6 and Constantin A. Rothkopf5,7,8\nAlexander Fraser1,2 and Kristian Kersting4,5,8,9\n1Center for Information and Language Processing, LMU Munich, Germany\n{lastname}@cis.lmu.de\n2Munich Centre for Machine Learning (MCML), Germany\n3Aleph Alpha GmbH, Heidelberg, Germany\n4Artificial Intelligence and Machine Learning Lab, TU Darmstadt, Germany\n5Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany\n6Faculty of Mathematics and Physics, Charles University, Czech Republic\n7Institute of Psychology, TU Darmstadt, Germany\n8Centre for Cognitive Science, TU Darmstadt, Germany\n9German Center for Artificial Intelligence (DFKI)\nAbstract\nPre-trained multilingual language models\n(PMLMs) are commonly used when dealing\nwith data from multiple languages and cross-\nlingual transfer. However, PMLMs are trained\non varying amounts of data for each language.\nIn practice this means their performance is of-\nten much better on English than many other\nlanguages. We explore to what extent this also\napplies to moral norms. Do the models capture\nmoral norms from English and impose them\non other languages? Do the models exhibit\nrandom and thus potentially harmful beliefs in\ncertain languages? Both these issues could neg-\natively impact cross-lingual transfer and poten-\ntially lead to harmful outcomes. In this paper,\nwe (1) apply the MORAL DIRECTION frame-\nwork to multilingual models, comparing results\nin German, Czech, Arabic, Chinese, and En-\nglish, (2) analyse model behaviour on filtered\nparallel subtitles corpora, and (3) apply the\nmodels to a Moral Foundations Questionnaire,\ncomparing with human responses from differ-\nent countries. Our experiments demonstrate\nthat PMLMs do encode differing moral biases,\nbut these do not necessarily correspond to cul-\ntural differences or commonalities in human\nopinions. We release our code and models.1\n1 Introduction\nRecent work demonstrated large pre-trained lan-\nguage models capture some symbolic, relational\n1https://github.com/kathyhaem/\nmultiling-moral-bias\nkill pollute steal divorce love thank\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nMoral score\nmono\nar\ncs\nde\nen\nzh\nmulti\nar\ncs\nde\nen\nzh\nFigure 1: MORAL DIRECTION score (y-axis) for several\nverbs (x-axis), as in Schramowski et al. (2022). We\nshow scores for each language both from the respective\nmonolingual model (triangles, left) and a multilingual\nmodel (rhombuses, right).\n(Petroni et al., 2019), but also commonsense (Davi-\nson et al., 2019) knowledge. The undesirable side\nof this property is seen in models reproducing bi-\nases and stereotypes (e.g., Caliskan et al., 2017;\nChoenni et al., 2021). However, in neutral terms,\nlanguage models trained on large data from particu-\nlar contexts will reflect cultural “knowledge” from\nthose contexts. We wonder whether multilingual\nmodels will also reflect cultural knowledge from\nmultiple contexts, so we study moral intuitions and\nnorms that the models might capture.\nRecent studies investigated the extent to\nwhich language models reflect human values\n(Schramowski et al., 2022; Fraser et al., 2022).\nThese works addressed monolingual English mod-\n2137\nels. Like them, we probe what the models encode,\nbut we study multilingual models in comparison\nto monolingual models. Given constantly evolving\nsocial norms and differences between cultures and\nlanguages, we ask: Can a PMLM capture cultural\ndifferences, or does it impose a Western-centric\nview regardless of context? This is a broad ques-\ntion which we cannot answer definitively. However,\nwe propose to analyse different aspects of mono-\nand multilingual model behaviour in order to come\ncloser to an answer. In this paper, we pose three\nresearch questions, and present a series of experi-\nments that address these questions qualitatively:\n1. If we apply the MORAL DIRECTION frame-\nwork (Schramowski et al., 2022) to pretrained\nmultilingual language models (PMLMs), how\ndoes this behave compared to monolingual\nmodels and to humans? (§ 3)\n2. How does the framework behave when ap-\nplied to parallel statements from a different\ndata source? To this end, we analyse model\nbehaviour on Czech-English and German-\nEnglish OpenSubtitles data (§ 4).\n3. Can the mono- and multi-lingual models make\nsimilar inferences to humans on a Moral Foun-\ndations Questionnaire (Graham et al., 2011)?\nDo they behave in ways that appropriately re-\nflect cultural differences? (§ 5)\nThe three experiments reinforce each other in\nfinding that our models grasp the moral dimension\nto some extent in all tested languages. There are\ndifferences between the models in different lan-\nguages, which sometimes line up between multi-\nand mono-lingual models. This does not necessar-\nily correspond with differences in human judge-\nments. As an illustration, Figure 1 shows examples\nof the MORAL DIRECTION score for several verbs\nin our monolingual and multilingual models.\nWe also find that the models are very reliant\non lexical cues, leading to problems like misun-\nderstanding negation, and disambiguation failures.\nThis unfortunately makes it difficult to capture nu-\nances. In this work we compare the behaviour of\nthe PMLM both to human data and to the behaviour\nof monolingual models in our target languages Ara-\nbic, Czech, German, Chinese, and English.\n2 Background\n2.1 Pre-Trained Multilingual LMs\nPMLMs, such as XLM-R (Conneau et al., 2020),\nare trained on large corpora of uncurated data,\nwith an imbalanced proportion of language data\nincluded in the training. Although sentences with\nthe same semantics in different languages should\ntheoretically have the same or similar embeddings,\nthis language neutrality is hard to achieve in prac-\ntice (Libovický et al., 2020). Techniques for im-\nproving the model’s internal semantic alignment\n(e.g., Zhao et al., 2021; Cao et al., 2020; Alqahtani\net al., 2021; Chi et al., 2021; Hämmerl et al., 2022)\nhave been developed, but these only partially mit-\nigate the issue. Here, we are interested in a more\ncomplex type of semantics and how well they are\ncross-lingually aligned.\n2.2 Cultural Differences in NLP\nSeveral recent studies deal with the question of\nhow cultural differences affect NLP. A recent com-\nprehensive survey (Hershcovich et al., 2022) high-\nlights challenges along the cultural axes of about-\nness, values, linguistic form, and common ground.\nSome years earlier, Lin et al. (2018) mined cross-\ncultural differences from Twitter data, focusing\non named entities and slang terms from English\nand Chinese. Yin et al. (2022) probed PMLMs\nfor “geo-diverse commonsense”, concluding that\nfor this task, the models are not particularly biased\ntowards knowledge about Western countries. How-\never, in their work the knowledge in question is\noften quite simple. We are interested in whether\nthis holds for more complex cultural values. In the\npresent study, we assume that using a country’s\nprimary language is the simplest way to probe for\nvalues from the target cultural context. Our work\nanalyses the extent to which one kind of cultural\ndifference, moral norms, is captured in PMLMs.\n2.3 Moral Norms in Pre-Trained LMs\nMultiple recent studies have investigated the extent\nto which language models reflect human values\n(Schramowski et al., 2022; Fraser et al., 2022).\nFurther, benchmark datasets (Hendrycks et al.,\n2021; Emelin et al., 2021; Ziems et al., 2022) aim-\ning to align machine values with human labelled\ndata have been introduced. Several such datasets\n(Forbes et al., 2020; Hendrycks et al., 2021; Alhas-\nsan et al., 2022) include scenarios from the “Am\nI the Asshole?” subreddit, an online community\n2138\nwhere users ask for an outside perspective on per-\nsonal disagreements. Some datasets use the com-\nmunity judgements as labels directly, others involve\ncrowdworkers in the dataset creation process.\nOther works have trained models specifically to\ninterpret moral scenarios, using such datasets. A\nwell-known example is Jiang et al. (2021), who\npropose a fine-tuned UNICORN model they call\nDELPHI . This work has drawn significant criticism,\nsuch as from Talat et al. (2021), who argue “that\na model that generates moral judgments cannot\navoid creating and reinforcing norms, i.e., being\nnormative”. They further point out that the training\nsets sometimes conflate moral questions with other\nissues such as medical advice or sentiments.\nHulpus, et al. (2020) explore a different direc-\ntion. They project the Moral Foundations Dictio-\nnary, a set of lexical items related to foundations in\nMoral Foundations Theory (§ 2.4), onto a knowl-\nedge graph. By scoring all entities in the graph for\ntheir relevance to moral foundations, they hope to\ndetect moral values expressed in a text. Solaiman\nand Dennison (2021) aim to adjust a pre-trained\nmodel to specific cultural values as defined in a tar-\ngeted dataset. For instance, they assert “the model\nshould oppose unhealthy beauty [...] standards”.\nA very interesting and largely unexplored area\nof research is to consider whether multilingual lan-\nguage models capture differing moral norms. For\ninstance, moral norms in the Chinese space in a\nPMLM might systematically differ from those in\nthe Czech space. Arora et al. (2022) attempt to\nprobe pre-trained models for cultural value differ-\nences using Hofstede’s cultural dimensions the-\nory (Hofstede, 1984) and the World Values Survey\n(Haerpfer et al., 2022). They convert the survey\nquestions to cloze-style question probes, obtaining\nscore values by subtracting the output distribution\nlogits for two possible completions from each other.\nHowever, they find mostly very low correlations\nof model answers with human references. Only\na few of their results show statistically significant\ncorrelations. They conclude that the models differ\nbetween languages, but that these differences do\nnot map well onto human cultural differences.\nDue to the observation that the output distribu-\ntions themselves do not reflect moral values well,\nwe choose the MORAL DIRECTION framework for\nour studies. In previous work, this approach identi-\nfied a subspace of the model weights relating to a\nsense of “right” and “wrong” in English.\n2.4 Moral Foundations Theory\nMoral Foundations Theory (Haidt and Joseph,\n2004) is a comparative theory describing what it\ncalls foundational moral principles, whose rela-\ntive importance can be measured to describe a\ngiven person’s or culture’s moral attitudes. Graham\net al. (2009) name the five factors “Care/Harm”,\n“Fairness/Reciprocity”, “Authority/Respect”, “In-\ngroup/Loyalty”, and “Purity/Sanctity”. Their im-\nportance varies both across international cultures\n(Graham et al., 2011) and the (US-American) po-\nlitical spectrum (Graham et al., 2009). The theory\nhas been criticised by some for its claim of in-\nnateness and its choice of factors, which has been\ndescribed as “contrived” (Suhler and Churchland,\n2011). Nevertheless, the associated Moral Founda-\ntions Questionnaire (Graham et al., 2011) has been\ntranslated into many languages and the theory used\nin many different studies (such as Joeckel et al.,\n2012; Piazza et al., 2019; Do ˘gruyol et al., 2019).\nAn updated version of the MFQ is being developed\nby Atari et al. (2022). In § 5, we score these ques-\ntions using our models and compare with human\nresponses from previous studies on the MFQ.\n2.5 Sentence Transformers\nBy default, BERT-like models output embeddings\nat a subword-token level. However, for many appli-\ncations, including ours, sentence-level representa-\ntions are necessary. In our case, inducing the moral\ndirection does not work well for mean-pooled to-\nken representations, leading to near-random scores\nin many cases (see § 3.1). Reimers and Gurevych\n(2019) proposed Sentence-Transformers as a way\nto obtain meaningful, constant sized, sentence rep-\nresentations from BERT-like models. The first\nSentence-BERT (S-BERT) models were trained\nby tuning a pre-trained model on a sentence pair\nclassification task. By encoding each sentence sep-\narately and using a classification loss, the model\nlearns more meaningful representations.\nTo obtain multilingual sentence representations,\nthey proposed a student-teacher training approach\nusing parallel corpora (Reimers and Gurevych,\n2020), where a monolingual S-BERT model acts as\na teacher and a pre-trained multilingual model as a\nstudent model. Such an approach forces the parallel\nsentences much closer together than in the original\nPMLM, which is not always desirable. In our case,\nwe might be unable to distinguish the effects of the\nS-BERT training from the original model, which\n2139\nwould interfere with probing the original model.\nUnlike their work, we train a multilingual sen-\ntence transformer by translating the initial training\ndata into our target languages (§ 3.2), and show that\nthis is effective. With this contribution, we show\nthat multilingual S-BERT models can be trained in\nthe same way as monolingual ones. Our approach\ndoes not require a teacher-student training setup.\nNote that we do require comparable datasets in size\nand ideally topics for each language. While we do\nnot explicitly align the data, we solve this by using\nmachine translated versions of existing datasets,\nwhich means we have implicitly parallel data.\n3 Inducing Moral Dimensions in PMLMs\nWe choose five languages to evaluate the behaviour\nof the multilingual models: Modern Standard Ara-\nbic (ar), Czech (cs), German (de), English (en),\nand Mandarin Chinese (zh). These are all relatively\nhigh-resource languages, so we hope the model will\nbe able to reliably detect cultural knowledge in each\nlanguage. Since we rely on machine translation,\nusing high-resource languages also ensures good\ntranslation quality. We note here that languages\nand cultures or countries are at best approximately\nequivalent (cf. Lin et al., 2018; Sahlgren et al.,\n2021). For instance, Arabic, English, and Chinese\nare standardised varieties that are written in a wide\nrange of different contexts or cultures, which are\nlikely merged together in the model to some degree.\nHowever, separation by language is the best way\nwe have to distinguish cultural contexts within a\nmultilingual model. As a point of comparison, we\nchoose a monolingual language model for each of\nour target languages (see App. A for details).\n3.1 M ORAL DIRECTION Framework\nWe use the MORAL DIRECTION framework by\nSchramowski et al. (2022). In the first step, this\nmethod encodes a set of positively and negatively\nconnotated action verbs with a sentence embedding\nmodel. In Schramowski et al. (2022), this is an S-\nBERT model. Each action verb is inserted into a\nset of ten template questions, such as “Should I\n[verb]?”, “Is it examplary to [verb]?”, and the out-\nput embedding for a verb is the mean over the em-\nbeddings of these questions. Next, PCA is applied\nto the outputs, to obtain the “moral direction” sub-\nspace of the model. Since the inputs are templates\nwith only individual verbs changing, they are lin-\nguistically homogeneous, and the most salient dif-\nferences for the PCA are the value judgements. Ide-\nally, a high amount of variance should be explained\nby the first principal component. The scores of\nthese initial verbs are then normalised to lie within\n[−1, 1]. Subsequent scores can sometimes lie out-\nside this range despite applying the normalisation.\nThe scores are then read as a value estimation along\none axis, with scores around 0 being “neutral”,\nscores close to -1 being very “bad”, and scores\nclose to 1 very “good”. However, note that the re-\nsults we list in Tables 1-3 and 8 are correlations of\nmodel scores with user study data or correlations\nof model scores with other model scores.\nWe choose to useMORAL DIRECTION because it\nis able to work directly with sentence embeddings\nand extract a reasonably human-correlated moral\ndirection from them, producing a value score along\na single axis. This makes it computationally inex-\npensive to transfer to other languages and datasets.\nA drawback is that it is induced on short, unam-\nbiguous phrases, and can be expected to work better\non such phrases. Deriving a score along a single\naxis can also be limiting or inappropriate in certain\ncontexts. See also the discussion in Limitations.\nFor a list of the verbs and questions used to\nderive the transformation, see the source paper.\nSchramowski et al. (2022) also conduct a user study\non Amazon MTurk to obtain reference scores for\nthe statements in question.\nTo test this method on multilingual and non-\nEnglish monolingual models, we machine translate\nboth the verbs and the filled question templates\nused in the above study. See Appendix B for the\nMT systems used, and a discussion of translation\nquality. We edited some of the questions to ensure\ngood translation.2 Our primary measure is the cor-\nrelation of resulting model scores with responses\nfrom the study in Schramowski et al. (2022).\nWe initially tested the method on mBERT (De-\nvlin et al., 2019) and XLM -R (Conneau et al.,\n2020), as well as a selection of similarly sized\nmonolingual models (Devlin et al., 2019; Antoun\net al., 2020; Straka et al., 2021; Chan et al., 2020),\nby mean-pooling their token representations. See\nAppendix Table 5 for a list of the models used. Ta-\nble 1 shows these initial results with mean-pooling.\nHowever, this generally did not achieve a correla-\ntion with the user study. There were exceptions to\nthis rule—i.e., the Chinese monolingual BERT, and\nthe English and Chinese portions of mBERT. This\n2e.g. “smile to sb.” →“smile at sb.”\n2140\nModel en ar cs de zh\nmBERT (mean-pooled) 0.65 -0.10 0.12 -0.18 0.62\nXLM-R (mean-pooled) -0.30 -0.07 -0.03 -0.14 0.10\nmonolingual (mean-pooled) -0.13 0.46 0.07 0.10 0.70\nmonolingual S-BERT-large 0.79 — — — —\nXLM-R (S-BERT) 0.85 0.82 0.85 0.83 0.81\nTable 1: Correlation of MORAL DIRECTION scores\nwith user study data for different pre-trained mono-\nand multi-lingual models. First three rows used mean-\npooled sentence embeddings; last two rows used embed-\ndings resulting from sentence-transformers (Reimers\nand Gurevych, 2019).\nmay be due to details in how the different models\nare trained, or how much training data is available\nfor each language in the multilingual models. Ta-\nble 1 also includes results from the monolingual,\nlarge English S-BERT, and an existing S-BERT\nversion of XLM-R3 (Reimers and Gurevych, 2020).\nThese two models did show good correlation with\nthe global user study, highlighting that this goal\nrequires semantic sentence representations.\n3.2 Sentence Representations\nThe existing S-BERT XLM -R model uses the\nstudent-teacher training with explicitly aligned data\nmentioned in § 2.5. As we discuss there, we\naim to change semantic alignment in the PMLM\nas little as possible before probing it. We also\nneed S-BERT versions of the monolingual models.\nTherefore, we train our own S-BERT models. We\nuse the sentence-transformers library (Reimers and\nGurevych, 2019), following their training proce-\ndure for training with NLI data.4 Although we do\nnot need explicitly aligned data, we do require com-\nparable corpora in all five languages, so we decide\nto use MNLI in all five languages. In addition to the\noriginal English MultiNLI dataset (Williams et al.,\n2018), we take the German, Chinese and Arabic\ntranslations from XNLI (Conneau et al., 2018), and\nprovide our own Czech machine translations (cf.\nAppendix B). Each monolingual model was tuned\nwith the matching translation, while XLM-RBase\nwas tuned with all five dataset translations. Thus,\nour multilingual S-BERT model was not trained di-\nrectly to align parallel sentences, but rather trained\nwith similar data in each involved language (with-\nout explicit alignment). For more training details,\n3We used sentence-transformers/xlm-r-\n100langs-bert-base-nli-mean-tokens.\n4https://github.com/UKPLab/\nsentence-transformers/blob/master/examples/\ntraining/nli/training_nli_v2.py\nModel en ar cs de zh\nXLM-R + MNLI\n(S-BERT, all 5 langs) 0.86 0.77 0.74 0.81 0.86\nmonolingual + MNLI\n(S-BERT, respective lang) 0.86 0.76 0.81 0.84 0.80\nTable 2: Correlation of MORAL DIRECTION scores from\nour mono- and multi-lingual S-BERT models with user\nstudy data.\nsee Appendix D. We release the resulting S-BERT\nmodels to the Huggingface hub.\n3.3 Results\nTable 2 shows the user study correlations of our\nS-BERT models. Clearly, sentence-level represen-\ntations work much better for inducing the moral di-\nrection, and the method works similarly well across\nall target languages. Figures 1 and 5 show exam-\nples of verb scores across models and languages,\nfurther illustrating that this method is a reasonable\nstarting point for our experiments.\nFor Arabic and the Czech portion of XLM-R, the\ncorrelations are slightly lower than the other mod-\nels. Notably, Arabic and Czech are the smallest of\nour languages in XLM-R, at 5.4 GB and 4.4 GB of\ndata (Wenzek et al., 2020), while their monolingual\nmodels contain 24 GB and 80 GB of data.\nSince in the case of Czech, the correlation is\nhigher in the monolingual model, and XLM-R and\nthe monolingual model disagree somewhat (Ta-\nble 3), the lower correlation seems to point to a\nflaw of its representation in XLM -R. For Arabic,\nthe correlation of the monolingual model with En-\nglish is similar to that seen in XLM -R, but the\nmonolingual model also disagrees somewhat with\nthe XLM-R representation (Table 3). This may\nmean there is actually some difference in attitude\n(based on the monolingual models), but XLM -R\nalso does not capture it well (based on the XLM-R\ncorrelations). Unfortunately, Schramowski et al.\n(2022) collected no data specifically from Arabic\nor Czech speakers to illuminate this.\nIn Table 3 we compare how much the scores\ncorrelate with each other when querying XLM-R\nand the monolingual models in different languages.\nThe diagonal shows correlations between the mono-\nlingual model of each language and XLM-R in that\nlanguage. Above the diagonal, we show how much\nthe monolingual models agree with each other,\nwhile below the diagonal is the agreement of differ-\nent languages within XLM-R. On the diagonal, we\ncompare each monolingual model with the match-\n2141\nlanguage en ar cs de zh\nen 0.93 0.86 0.92 0.89 0.91\nar 0.86 0.84 0.89 0.89 0.86\ncs 0.90 0.78 0.86 0.92 0.92\nde 0.95 0.87 0.88 0.95 0.91\nzh 0.94 0.89 0.84 0.94 0.94\nTable 3: Correlation of languages between our S-BERT\nmodels on the user study questions. Below diago-\nnal: XLM-R model, tuned with MNLI data in five lan-\nguages. Above diagonal: Monolingual models, tuned\nwith MNLI data in the respective languages. On the\ndiagonal: Correlation of the monolingual models with\nXLM-R in the respective language.\ning language in XLM-R. For English, German and\nChinese, these show high correlations. The lowest\ncorrelation overall is between the Czech and Arabic\nportions of XLM-R, while the respective monolin-\ngual models actually agree more. The monolingual\nS-BERT models are generally at a similar level\nof correlation with each other as the multilingual\nmodel. German and Chinese, however, show a\nhigher correlation with English in the multilingual\nmodel than in their respective monolingual models,\nwhich may show some interference from English.\nWe also show the correlations of languages\nwithin the pre-existing S-BERT model,5 which was\ntrained with parallel data, in Table 8. Here, the cor-\nrelations between languages are much higher, show-\ning that parallel data training indeed changes the\nmodel behaviour on the moral dimension. These\ncorrelations are higher than that of any one model\nwith the user study data, so this likely corresponds\nto an artificial similarity with English, essentially\nremoving cultural differences from this model.\nSummarised, the experiments in this section ex-\ntend Schramowski et al. (2022) to a multilingual\nsetting and indicate that multilingual LMs indeed\ncapture moral norms. The high mutual correlations\nof scores show that the differences between models\nand languages are relatively small in this respect.\nNote, however, that the tested statements provided\nby Schramowski et al. (2022) are not explicitly de-\nsigned to grasp cultural differences. We thus add\nfurther experiments to address this question.\n4 Qualitative Analysis on Parallel Data\nTo better understand how these models generalise\nfor various types of texts, we conduct a qualita-\n5sentence-transformers/xlm-r-100langs-\nbert-base-nli-mean-tokens\ntive study using parallel data. For a parallel sen-\ntence pair, the MORAL DIRECTION scores should\nbe similar in most cases. Sentence pairs where the\nscores differ considerably may indicate cultural dif-\nferences, or issues in the models. In practice, very\nlarge score differences appear to be more related to\nthe latter. This type of understanding is important\nfor further experiments with these models.\nWe conduct our analysis on OpenSubtitles paral-\nlel datasets (Lison and Tiedemann, 2016),6 which\nconsist of relatively short sentences. Given that the\nMORAL DIRECTION is induced on short phrases,\nwe believe that short sentences will be easier for\nthe models. The subtitles often concern people’s\nbehaviour towards each other, and thus may carry\nsome moral sentiment. We use English-German\nand English-Czech data for our analysis. To obtain\nthe moral scores, we encode each sentence with the\nrespective S-BERT model, apply the PCA transfor-\nmation, and divide the first principal component by\nthe normalising parameter.\nOur analysis focuses on sentence pairs with very\ndifferent scores. We take steps to filter out mis-\ntranslated sentence pairs—see Appendix H. Below,\nwe discuss examples of where scores differ notice-\nably even when the translations are adequate. Us-\ning Czech-English and German-English data, we\ncompare the monolingual models with XLM -R,\nXLM-R with the monolingual models, and the\nmonolingual models with each other. This anal-\nysis is based on manual inspection of 500 sentence\npairs with the highest score differences for each\ncombination. Note that many of the sentence pairs\nwere minor variations of each other, which signif-\nicantly sped up the analysis. Relevant examples\nare listed with their MORAL DIRECTION scores in\nTable 4 and Table 10 in the Appendix.\n4.1 Reliance on Lexical Items\nA common theme for many examples is an over-\nreliance on individual lexical items. For example,\n“Traitors ... like you!” receives a positive score in\nEnglish, while the German equivalent is correctly\nscored as negative. Most likely, the English models\ntook a shortcut: “like you” is seen as a good thing.\nSimilarly, XLM-R in English scores “They’re\ndying to meet you.” somewhat negatively. The\nEnglish BERT gives a positive score. However,\narguably this is a case where the most correct an-\nswer would be neutral, since this is more a positive\n6http://www.opensubtitles.org/\n2142\nmonoling XLM-R\nde en de en de en\nPures Gift. Pure poison. -0.61 -0.71 0.65 -0.69\nIch erwürg dich! I’ll strangle you! -0.41 -0.58 0.90 -0.62\nHab jemandem einen Gefallen getan. I did someone a favour. 0.39 0.28 -0.41 0.73\nVerräter ... wie Sie! Traitors ... like you! -0.56 0.19 -0.39 0.72\nSie brennen darauf, dich kennenzuler-\nnen.\nThey’re dying to meet you. 0.44 0.73 0.52 -0.31\nIch vermisse ihn sehr. I really miss him. 0.69 0.23 -0.41 -0.26\nEr schätzt mich. He values me. 1.12 0.31 0.04 0.88\nTable 4: Examples from the German-English OpenSubtitles data for which there is a large, spurious contrast between\nMORAL DIRECTION scores. Scores that stand out as unreasonable are italicised.\nsentiment than any moral concern.\n4.2 Multilinguality and Polysemy\nContinuing the theme of literalness, another dimen-\nsion is added to this in the multilingual setting.\nFor instance, XLM -R scores the German “Pures\nGift.” ( pure poison) as positive, likely because\nthe key word “Gift” looks like English “gift”, as\nin present. However, the model also makes less\nexplainable mistakes: many sentences with “erwür-\ngen” (to strangle) receive a highly positive score.\nIn the Czech-English data, there are even more\nobvious mistakes without a straightforward expla-\nnation. Some Czech words are clearly not under-\nstood by XLM-R: For instance, sentences with “štˇe-\ndrý” (generous) are negative, while any sentence\nwith “páˇcidlo” (crowbar) in it is very positive in\nXLM-R. Phrases with “vrah” (murderer) get a pos-\nitive score in XLM-R, possibly because of translit-\nerations of the Russian word for medical doctor.\nMost of these obvious mistakes of XLM-R are not\npresent in RobeCzech. However, “Otrávils nás”\n(You poisoned us) receives a positive score from\nRobeCzech for unknown reasons.\nConfusing one word for another can also be a\nproblem within a single language: For example,\n“Gefallen” (a favour) receives a negative score from\nXLM-R in many sentences. It is possible this model\nis confusing this with “gefallen” (past participle of\n“fallen”, to fall), or some other similar word from\na different language. “Er schätzt mich” and sim-\nilar are highly positive in gBERT, as well as En-\nglish XLM-R, but have a neutral score in German\nXLM-R. Likely the latter is failing to disambiguate\nhere, and preferring “schätzen” as in estimate.\n5 Moral Foundations Questionnaire\nThe MFQ has been applied in many different stud-\nies on culture and politics, meaning there is human\nresponse data from several countries available. We\npose the MFQ questions from Graham et al. (2011)\nto our models, in order to compare the model scores\nwith data from previous studies. We use the transla-\ntions provided on the Moral Foundations website.7\nSince the first part of the MFQ consists of very\ncomplex questions, we rephrase these into simple\nstatements (see Appendix J). Many of the state-\nments in the first half of the questionnaire become\nreverse-coded by simplifying them, that is, some-\none who values the aspect in question would be\nexpected to answer in the negative. For these state-\nments, we multiply the model score by -1. Further,\nwe know that language models struggle with nega-\ntion (Kassner and Schütze, 2020), so we remove\n“not” or “never” from two statements and flip the\nsign accordingly. In the same way, we remove “a\nlack of” from two statements.\nThese adjustments already improved the coher-\nence of the resulting aspect scores, but we found\nfurther questions being scored by the models as\nif reverse-coded, i.e., with a negative score when\nsome degree of agreement was expected. These\nwere not simply negated statements, but they did\ntend to contain lexical items that were strongly\nnegatively associated, and in multiple cases con-\ntained a negative moral judgement of the action\nor circumstance in question. Because the models\nappear to be so lexically focused (see § 4.1), this\ncombination led to a strong negative score for some\nof these questions. We decided to rephrase such\nstatements as well, usually flipping their sign while\nchanging the wording as little as possible. Still, we\n7https://moralfoundations.org/questionnaires/\n2143\nCare Fairness Loyalty Authority Purity\n0\n1\n2\n3\n4\n5\nCzech (Bene  2021)\nGerman (Joeckel 2012)\nUS (Graham 2011)\nChina (Wang 2019)\nCare Fairness Loyalty Authority Purity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\narabert\nrobeczech\ngbert\nen bert\nzh bert\nCare Fairness Loyalty Authority Purity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nar\ncs\nde\nen\nzh\nFigure 2: MFQ aspect scores from humans and models. Left: Examples of human data from studies in different\ncountries. Middle: Scores obtained from monolingual MORAL DIRECTION models. Right: Scores from XLM-R\nMORAL DIRECTION in five languages.\nCare Fairness Loyalty Authority Purity\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\nar\ncs\nde\nen\nzh\nFigure 3: Sanity check—MFQ aspect scores from the\nXLM-R MORAL DIRECTION models without Sentence-\nBERT tuning. This model had not obtained good corre-\nlations with human scores in § 3.\nnote here that this should be considered a type of\nprompt engineering, and that implicatures of the\nstatements may have changed through this process.\nWe provide the list of rephrased English statements\nand multipliers in Appendix Table 11.\nWe manually apply the same changes to the\ntranslations. The full list of English and trans-\nlated statements, as well as model scores for each\nquestion, is available as a CSV file. Finally, we\nmean-pool the question scores within each aspect\nto obtain the aspect scores. Most of the model\nscores for each question will be within [-1, 1]. The\nresults are shown in Figure 2.\n5.1 Human Response Data\nAlso in Figure 2, we show German data from\nJoeckel et al. (2012), Czech data from Beneš\n(2021), US data from Graham et al. (2011), and\nChinese data from Wang et al. (2019) for compari-\nson. Note that these are not necessarily representa-\ntive surveys. The majority of the data in question\nwere collected primarily in a university context and\nthe samples skew highly educated and politically\nleft. For Germany, the US and the Czech Repub-\nlic, the individual variation, or variation between\npolitical ideologies seems to be larger than the vari-\nation between the countries. The Chinese sample\nscores more similarly to conservative respondents\nin the Western countries. Although many individ-\nuals score in similar patterns as the average, the\ndifference between individuals in one country can\nbe considerable. As an example, see Figure 7.\nNone of our models’ scores map directly onto\naverage human responses. The model scores do not\nuse the full range of possible values, but even the\npatterns of relative importance do not match the\naverage human patterns. Scores sometimes vary\nconsiderably in different models and different lan-\nguages within XLM -R, and not necessarily in a\nway that would follow from cultural differences.\nThe average scores within XLM-R are somewhat\nmore similar to each other than the scores from\nthe monolingual models are, giving some weak ev-\nidence that the languages in the multilingual model\nassimilate to one another. However, some differ-\nences between the monolingual models are also\nreflected in the multilingual model.\n5.2 Sanity Check\nWe compare against scores from the unmodified,\nmean-pooled XLM-R models, shown in Figure 3.\nThese models did not have the Sentence-BERT tun-\ning applied to them, but otherwise we used the\nsame procedure to obtain the scores. The incon-\nsistent and very unlike human scores reinforce the\nfinding from § 3 that mean-pooled representations\nare not useful for our experiments. They also con-\nfirm that the results in our main MFQ experiments\nare not arbitrary.\n2144\n6 Conclusions\nWe investigated themoral dimensionof pre-trained\nlanguage models in a multilingual context. In this\nsection, we discuss our research questions:\n(1) Multilingual MORAL DIRECTION . We ap-\nplied the MORAL DIRECTION framework to\nXLM-R, as well as monolingual language models\nin five languages. We were able to induce models\nthat correlate with human data similarly well as\ntheir English counterparts in Schramowski et al.\n(2022). We analysed differences and similarities\nacross languages.\nIn the process, we showed that sentence-level\nrepresentations, rather than mean-pooled token-\nlevel representations, are necessary in order to in-\nduce a reasonable moral dimension for most of\nthese models. We trained monolingual S-BERT\nmodels for our five target languages Arabic, Czech,\nGerman, English, and Mandarin Chinese. As well,\nwe created a multilingual S-BERT model from\nXLM-R which was trained with MNLI data in all\nfive target languages.\n(2) Behaviour on Parallel Subtitles. A limita-\ntion of the MORAL DIRECTION is that it is induced\non individual words, and thus longer sentences are\na significant challenge for the models. Still, we\nwere able to test them on parallel subtitles data,\nwhich contains slightly longer, but predominantly\nstill short, sentences. Problems that showed up re-\npeatedly in this experiment were an over-reliance\non key lexical items and a failure to understand\ncompositional phrases, particularly negation. Addi-\ntionally, typical problems of PMLMs, such as dis-\nambiguation problems across multiple languages,\nwere noticeable within XLM-R. Non-English lan-\nguages appeared more affected by such issues, de-\nspite the fact that all our target languages are rela-\ntively high resource.\n(3) Moral Foundations Questionnaire. Our ex-\nperiments with the MFQ reinforce the conclusion\nthat the MORAL DIRECTION models capture a gen-\neral sense of right and wrong, but do not display\nentirely coherent behaviour. Again, compositional\nphrases and negation were an issue in multiple\ncases. We had set out to investigate whether cul-\ntural differences are adequately reflected in the\nmodels’ cross-lingual behaviour. However, our\nfindings indicate that rather, there are other issues\nwith the cross-lingual transfer that mean we cannot\nmake such nuanced statements about the model be-\nhaviour. To the extent that model behaviour differs\nfor translated data, this does not seem to match cul-\ntural differences between average human responses\nfrom different countries.\nWe had initially wondered whether models\nwould impose values from an English-speaking\ncontext on other languages. Based on this evidence,\nit seems that the models do differentiate between\ncultures to some extent, but there are caveats: The\ndifferences are not necessarily consistent with hu-\nman value differences, which means the models\nare not always adequate. The problem appears to\nbe worse when models are trained on smaller data\nfor a given language. Meanwhile, German and\nChinese have noticeably high agreement with En-\nglish in our multilingual model, and all languages\nare extremely highly correlated in the pre-existing\nparallel-data S-BERT model (Table 8). This clearly\nshows that training with parallel data leads to more\nsimilar behaviour in this dimension, more or less\nremoving cultural differences, but indeed there may\nbe some transference even without parallel data.\nFuture Work. This leads to several future re-\nsearch questions: (i) Can we reliably investigate\nencoded (moral) knowledge reflected by PMLMs\non latent representations or neuron activations? Or\ndo we need novel approaches? For instance, Jiang\net al. (2021) suggest evaluating the output of gener-\native models and, subsequently, Arora et al. (2022)\napply masked generation using PMLMs to probe\ncultural differences in values. However, the gener-\nation process of LMs is highly dependent, among\nother things, on the sampling process. Therefore,\nit is questionable if such approaches provide the\nrequired transparency. Nevertheless, Arora et al.\n(2022) come to a similar conclusion as indicated by\nour results: PMLMs encode differences between\ncultures. However, these are weakly correlated\nwith human surveys, which leads us to the second\nfuture research question: (ii) How can we reliably\nteach large-scale LMs to reflect cultural differences\nbut also commonalities? Investigating PMLMs’\nmoral direction and probing the generation process\nleads to inconclusive results, i.e., these models en-\ncode differences, which, however, do not correlate\nwith human opinions. But correlating with human\nopinions is a requirement for models to work faith-\nfully in a cross-cultural context. Therefore, we\nadvocate for further research on teaching cultural\ncharacteristics to LMs.\n2145\nLimitations\nThe MORAL DIRECTION framework works primar-\nily for short, unambiguous phrases. While we show\nthat it is somewhat robust to longer phrases, it does\nnot deal well with negation or certain types of\ncompositional phrases. We showed that in such\ncases, prompt engineering seems to be necessary\nin order to get coherent answers. Inducing the\nMORAL DIRECTION was done on a small set of\nverbs, and the test scenarios in this paper—apart\nfrom § 4—are also relatively small.\nThe scope of our work is specific to our stated\ntarget languages, which are all relatively high-\nresource, meaning the method may not hold up for\nlanguages with smaller corpora, especially in the\ncontext of PMLMs. This work presents primarily\nan exploratory analysis and qualitative insights.\nAnother point is that the monolingual models we\nused may not be precisely comparable. Table 5 lists\ndetails of parameter size, training, tokenizers, data\nsize and data domain. The models are all similarly\nsized, but data size varies considerably. XLM -R\nand RobeCzech do not use next sentence prediction\nas part of their training objective. However, the\nauthors of RoBERTa (Liu et al., 2019) argue this\ndifference does not affect representation quality.\nFurther, exactly comparable models do not exist\nfor every language we use. We rather choose well-\nperforming, commonly-used models. Thus, we\nbelieve the model differences play a negligible role\nin the context of our scope.\nMore broadly speaking, the present work makes\nthe strong assumption that cultural context and lan-\nguage are more or less equivalent, which does not\nhold up in practice. Furthermore, MORAL DIREC -\nTION , like related methods, only consider a single\naxis, representing a simplistic model of morality.\nIn the same vein, these models will output a score\nfor any input sentence, including morally neutral\nones, sometimes leading to random answers.\nBroader Impacts\nLanguage models should not decide moral ques-\ntions in the real world, but research in that direction\nmight suggest that this is in fact possible. Besides\nundue anthropomorphising of language models, us-\ning them to score moral questions could lead to\nmultiple types of issues: The models may repro-\nduce and reinforce questionable moral beliefs. The\nmodels may hallucinate beliefs. And particularly\nin the context of cross-lingual and cross-cultural\nwork, humans might base false, overgeneralising,\nor stereotyping assumptions about other cultures\non the output of the models.\nAcknowledgements\nWe thank Sven Jöckel for providing us with their\nraw results from their MFQ studies, and Hashem\nSellat and Wen Lai for their help with formulating\nthe MFQ questions in Arabic and Chinese. Thank\nyou to Morteza Dehghani.\nThis publication was supported by LMUexcel-\nlent, funded by the Federal Ministry of Educa-\ntion and Research (BMBF) and the Free State\nof Bavaria under the Excellence Strategy of the\nFederal Government and the Länder; and by the\nGerman Research Foundation (DFG; grant FR\n2829/4-1). The work at CUNI was supported by\nCharles University project PRIMUS/23/SCI/023\nand by the European Commission via its Horizon\nresearch and innovation programme (No. 870930\nand 101070350). Further, we gratefully acknowl-\nedge support by the Federal Ministry of Education\nand Research (BMBF) under Grant No. 01IS22091.\nThis work also benefited from the ICT-48 Network\nof AI Research Excellence Center “TAILOR\" (EU\nHorizon 2020, GA No 952215), the Hessian re-\nsearch priority program LOEWE within the project\nWhiteBox, the Hessian Ministry of Higher Edu-\ncation, and the Research and the Arts (HMWK)\ncluster projects “The Adaptive Mind” and “The\nThird Wave of AI”.\nReferences\nAreej Alhassan, Jinkai Zhang, and Viktor Schlegel.\n2022. ‘Am I the bad one’? Predicting the moral\njudgement of the crowd using pre–trained language\nmodels. In Proceedings of the Language Resources\nand Evaluation Conference, page 267–276, Mar-\nseille, France. European Language Resources As-\nsociation.\nSawsan Alqahtani, Garima Lalwani, Yi Zhang, Salva-\ntore Romeo, and Saab Mansour. 2021. Using opti-\nmal transport as alignment objective for fine-tuning\nmultilingual contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3904–3919, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\n2146\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nArnav Arora, Lucie-Aimée Kaffee, and Isabelle Au-\ngenstein. 2022. Probing pre-trained language mod-\nels for cross-cultural differences in values. CoRR,\nabs/2203.13722.\nMohammad Atari, Jonathan Haidt, Jesse Graham, Sena\nKoleva, Sean T Stevens, and Morteza Dehghani.\n2022. Morality beyond the WEIRD: How the nomo-\nlogical network of morality varies across cultures.\nPsyArXiv.\nMichal Beneš. 2021. Psychometrické hodnocení\ndotazníku moral foundations questionnaire [online].\nMaster thesis, Masarykova univerzita, Filozofická\nfakulta, Brno, Czech Republic. Supervisor: Helena\nKlimusová.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Multi-\nlingual alignment of contextual word representations.\nCoRR, abs/2002.03518.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nBranden Chan, Stefan Schweter, and Timo Möller. 2020.\nGerman’s next language model. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 6788–6796, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nPinzhen Chen, Jind ˇrich Helcl, Ulrich Germann, Lau-\nrie Burchell, Nikolay Bogoychev, Antonio Valerio\nMiceli Barone, Jonas Waldendorf, Alexandra Birch,\nand Kenneth Heafield. 2021. The University of Ed-\ninburgh’s English-German and English-Hausa sub-\nmissions to the WMT21 news translation task. In\nProceedings of the Sixth Conference on Machine\nTranslation, pages 104–109, Online. Association for\nComputational Linguistics.\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-\nLing Mao, Heyan Huang, and Furu Wei. 2021. Im-\nproving pretrained cross-lingual language models via\nself-labeled word alignment. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3418–3430, Online. As-\nsociation for Computational Linguistics.\nRochelle Choenni, Ekaterina Shutova, and Robert van\nRooij. 2021. Stepmothers are mean and academics\nare pretentious: What do pretrained language models\nlearn about you? In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1477–1491, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178, Hong Kong, China. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBurak Do ˘gruyol, Sinan Alper, and Onurcan Yilmaz.\n2019. The five-factor model of the moral foundations\ntheory is stable across weird and non-weird cultures.\nPersonality and Individual Differences, 151:109547.\nDenis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell\nForbes, and Yejin Choi. 2021. Moral stories: Situ-\nated reasoning about norms, intents, actions, and\ntheir consequences. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 698–718, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. In Proceedings of the 2020 Conference on\n2147\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nKathleen C. Fraser, Svetlana Kiritchenko, and Esma\nBalkir. 2022. Does moral code have a moral code?\nprobing delphi’s moral philosophy. In Proceedings\nof the 2nd Workshop on Trustworthy Natural Lan-\nguage Processing (TrustNLP 2022), pages 26–42,\nSeattle, U.S.A. Association for Computational Lin-\nguistics.\nJesse Graham, Jonathan Haidt, and Brian A. Nosek.\n2009. Liberals and conservatives rely on different\nsets of moral foundations. Journal of Personality and\nSocial Psychology, 96(5):1029–46.\nJesse Graham, Brian Nosek, Jonathan Haidt, Ravi Iyer,\nSena P Koleva, and Peter H Ditto. 2011. Mapping\nthe moral domain. Journal of Personality and Social\nPsychology, 101 (2):366–385.\nC Haerpfer, R Inglehart, A Moreno, C Welzel,\nK Kizilova, J Diez-Medrano, M Lagos, P Norris,\nE Ponarin, and B Puranen. 2022. World values sur-\nvey: Round seven—country-pooled datafile version\n3.0. JD Systems Institute: Madrid, Spain.\nJonathan Haidt and Craig Joseph. 2004. Intuitive ethics:\nHow innately prepared intuitions generate culturally\nvariable virtues. Daedalus, 133(4):55–66.\nKatharina Hämmerl, Jindˇrich Libovický, and Alexander\nFraser. 2022. Combining static and contextualised\nmultilingual embeddings. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2022,\npages 2316–2329, Dublin, Ireland. Association for\nComputational Linguistics.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nhsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Effi-\ncient natural language response suggestion for smart\nreply. CoRR, abs/1705.00652.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning AI with shared human values. In Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR). OpenReview.net.\nDaniel Hershcovich, Stella Frank, Heather Lent,\nMiryam de Lhoneux, Mostafa Abdou, Stephanie\nBrandl, Emanuele Bugliarello, Laura Cabello Pi-\nqueras, Ilias Chalkidis, Ruixiang Cui, Constanza\nFierro, Katerina Margatina, Phillip Rust, and Anders\nSøgaard. 2022. Challenges and strategies in cross-\ncultural NLP. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6997–7013,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nGeert Hofstede. 1984. Culture’s Consequences: Inter-\nnational Differences in Work-Related Values. Cross\nCultural Research and Methodology. SAGE Publica-\ntions.\nIoana Hulpus, , Jonathan Kobbe, Heiner Stuckenschmidt,\nand Graeme Hirst. 2020. Knowledge graphs meet\nmoral values. In Proceedings of the Ninth Joint Con-\nference on Lexical and Computational Semantics,\npages 71–80, Barcelona, Spain (Online). Association\nfor Computational Linguistics.\nLiwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ro-\nnan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny\nLiang, Oren Etzioni, Maarten Sap, and Yejin Choi.\n2021. Delphi: Towards machine ethics and norms.\nCoRR, abs/2110.07574.\nSven Joeckel, Nicholas David Bowman, and Leyla Do-\ngruel. 2012. Gut or game? the influence of moral\nintuitions on decisions in video games. Media Psy-\nchology, 15(4):460–485.\nMarcin Junczys-Dowmunt. 2018. Dual conditional\ncross-entropy filtering of noisy parallel corpora. In\nProceedings of the Third Conference on Machine\nTranslation: Shared Task Papers, pages 888–895,\nBelgium, Brussels. Association for Computational\nLinguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nJindˇrich Libovický, Rudolf Rosa, and Alexander Fraser.\n2020. On the language neutrality of pre-trained mul-\ntilingual representations. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 1663–1674, Online. Association for Computa-\ntional Linguistics.\nBill Yuchen Lin, Frank F. Xu, Kenny Zhu, and Seung-\nwon Hwang. 2018. Mining cross-cultural differences\nand similarities in social media. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 709–719, Melbourne, Australia. Association\nfor Computational Linguistics.\nPierre Lison and Jörg Tiedemann. 2016. OpenSub-\ntitles2016: Extracting large parallel corpora from\nmovie and TV subtitles. In Proceedings of the Tenth\nInternational Conference on Language Resources\nand Evaluation (LREC’16), pages 923–929, Portorož,\nSlovenia. European Language Resources Association\n(ELRA).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. CoRR.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\n2148\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nJared Piazza, Paulo Sousa, Joshua Rottman, and\nStylianos Syropoulos. 2019. Which appraisals are\nfoundational to moral judgment? Harm, injustice,\nand beyond. Social Psychological and Personality\nScience, 10(7):903–913.\nMartin Popel, Marketa Tomkova, Jakub Tomek, Łukasz\nKaiser, Jakob Uszkoreit, Ondˇrej Bojar, and Zdenˇek\nŽabokrtsk`y. 2020. Transforming machine transla-\ntion: a deep learning system reaches news translation\nquality comparable to human professionals. Nature\ncommunications, 11(1):1–15.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512–4525,\nOnline. Association for Computational Linguistics.\nMagnus Sahlgren, Fredrik Carlsson, Fredrik Olsson,\nand Love Börjeson. 2021. It’s basically the same lan-\nguage anyway: the case for a Nordic language model.\nIn Proceedings of the 23rd Nordic Conference on\nComputational Linguistics (NoDaLiDa), pages 367–\n372, Reykjavik, Iceland (Online). Linköping Univer-\nsity Electronic Press, Sweden.\nPatrick Schramowski, Cigdem Turan, Nico Andersen,\nConstantin A. Rothkopf, and Kristian Kersting. 2022.\nLarge pre-trained language models contain human-\nlike biases of what is right and wrong to do. Nature\nMachine Intelligence.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (palms) with\nvalues-targeted datasets. In Advances in Neural Infor-\nmation Processing Systems, volume 34, pages 5861–\n5873. Curran Associates, Inc.\nMilan Straka, Jakub Náplava, Jana Straková, and David\nSamuel. 2021. RobeCzech: Czech RoBERTa, a\nmonolingual contextualized language representation\nmodel. In Text, Speech, and Dialogue, pages 197–\n209, Cham. Springer International Publishing.\nChristopher Suhler and Pat Churchland. 2011. Can in-\nnate, modular “foundations” explain morality? Chal-\nlenges for Haidt’s Moral Foundations Theory. Jour-\nnal of Cognitive Neuroscience, 23:2103–16; discus-\nsion 2117.\nZeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira\nGanesh, Ryan Cotterell, and Adina Williams. 2021.\nA word on machine ethics: A response to Jiang et al.\n(2021). CoRR, abs/2111.04158.\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT – building open translation services for the world.\nIn Proceedings of the 22nd Annual Conference of\nthe European Association for Machine Translation,\npages 479–480, Lisboa, Portugal. European Associa-\ntion for Machine Translation.\nRuile Wang, Qi Yang, Peng Huang, Liyang Sai, and Yue\nGong. 2019. The association between disgust sensi-\ntivity and negative attitudes toward homosexuality:\nThe mediating role of moral foundations. Frontiers\nin Psychology, 10.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nDa Yin, Hritik Bansal, Masoud Monajatipoor, Liu-\nnian Harold Li, and Kai-Wei Chang. 2022. GeoM-\nLAMA: Geo-diverse commonsense probing on multi-\nlingual pre-trained language models. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2039–2055, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Isabelle\nAugenstein. 2021. Inducing language-agnostic mul-\ntilingual representations. In Proceedings of *SEM\n2021: The Tenth Joint Conference on Lexical and\nComputational Semantics, pages 229–240, Online.\nAssociation for Computational Linguistics.\n2149\nCaleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy,\nand Diyi Yang. 2022. The moral integrity corpus: A\nbenchmark for ethical dialogue systems. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 3755–3773, Dublin, Ireland. Association\nfor Computational Linguistics.\nA Details of Models Used\nTable 5 lists the models we tuned and evaluated\nwith their exact names, sizes, objectives and data.\nThe models are all of a similar size, although\ndata size varies by up to one order of magnitude\nbetween monolingual models. XLM-R has much\nlarger data in total, but data size for individual lan-\nguages is more comparable to the other models.\nThe data domains vary but overlap (Web, Wiki,\nNews). XLM -R and RobeCzech do not use next\nsentence prediction as part of their training objec-\ntive. However, we believe these differences play a\nnegligible role in the context of our work.\nB Machine Translation Quality\nMachine translation is used to translate the tem-\nplated sentences from English into Arabic, Czech,\nGerman and Chinese. For Arabic and Chinese, we\nuse Google Translate. The sentences are short and\ngrammatically very simple.\nFor translation into Czech, we use CUBBITT\n(Popel et al., 2020), a machine translation system\nthat scored in the first cluster in WMT evaluation\ncampaigns 2019–2021. For translation into Ger-\nman, we use the WMT21 submission of the Uni-\nversity of Edinburgh (Chen et al., 2021). To vali-\ndate our choice of machine translation systems, we\nestimate the translation quality using the reference-\nfree version of the COMET score (Rei et al., 2020)\n(model wmt21-comet-qe-mqm) on the 2.7k gener-\nated questions.\nTo train the S-BERT models, we use the\nTRANSLATE-TRAIN part of the XNLI dataset\nthat is distributed with the dataset (without specify-\ning what translation system was used). For trans-\nlation into Czech, we use CUBBITT again. To\nensure the translation quality is comparable, we\nuse the same evaluation metric as in the previous\ncase on 5k randomly sampled sentences.\nC Correlations in Existing (Parallel Data)\nS-BERT\nTable 8 shows the correlations of languages within\nthe pre-existing multilingual S-BERT, trained with\nparallel data. The correlations within this model\nare extremely high, considerably higher than that\nof any one model with the user study.\nD Sentence-BERT Tuning Procedure\nWe follow the training script provided by Reimers\nand Gurevych (2019) in the sentence-tranformers\nrepository. As training data, we use the complete\nMNLI (Williams et al., 2018; 433k examples) in\nthe five respective languages. The dev split from\nthe STS benchmark (Cer et al., 2017; 1500 exam-\nples) serves as development data. We also machine\ntranslate this into the target languages. The loss\nfunction is Multiple Negatives Ranking Loss (Hen-\nderson et al., 2017), which benefits from larger\nbatch sizes. We use sentence-transformers version\n2.2.0. Table 9 lists further training parameters.\nE Computational Resources\nIn addition to the six models used for further ex-\nperiments, we trained five XLM -R with single-\nlanguage portions of data. Each of the monolingual\nmodels, as well as the XLM-R versions tuned with\none part of the data, took around 0.6 hours to train.\nTuning XLM-R with data in all five languages ac-\ncordingly took around three hours. S-BERT tuning\nwas done on one Tesla V100-SXM3 GPU, with 32\nGB RAM, at a time. We also trained one version of\nXLM-R on English data with a smaller batch size\non an NVIDIA GeForce GTX 1080 GPU with 12\nGB RAM. In all other experiments, the language\nmodels were used in inference mode only, and they\nwere mostly run on the CPU.\nF Variance in MORAL DIRECTION Scores\nIn this section we discuss another aspect of\nMORAL DIRECTION scores in multilingual versus\nmonolingual models: How much they vary between\ndifferent languages for each statement. For in-\nstance, if the variance is smaller in the multilingual\nmodel, this would mean that the multilingual model\napplies more similar judgements across languages.\nTo quantify this, we calculate the score variance for\neach of the basic verbs from Schramowski et al.\n(2022) over the five monolingual models, as well\nas over the five portions of the multilingual model.\nWe furthermore grouped the verbs into “posi-\ntive” and “negative”, depending on whether their\nmean score from the multilingual model is greater\nor lower than zero. This results in 35 positive and\n29 negative verbs. Figure 4 shows box-plots of\n2150\nLng Name Params Objective Tokenizer Data size Domain\nar aubmindlab/bert-base-arabertv02\n(Antoun et al., 2020)\n110M MLM+NSP SP, 60k 24 GB Wiki, News\ncs ufal/robeczech-base (Straka et al., 2021) 125M MLM BPE, 52k 80 GB News, Wiki,\nWeb\nde deepset/gbert-base (Chan et al., 2020) 110M MLM+NSP WP, 31k 136 GB Web, Wiki,\nLegal\nen bert-base-cased (Devlin et al., 2019) 110M MLM+NSP WP, 30k 16 GB Books, Wiki\nzh bert-base-chinese (Devlin et al., 2019) 110M MLM+NSP WP, 21k ? Wiki\n— xlm-roberta-base (Conneau et al., 2020) 125M MLM SP, 250k 2.5TB Web\nTable 5: The monolingual pre-trained language models used. We tuned each model with the S-BERT framework\nbefore using it for our experiments. Objectives: MLM = masked language modelling, NSP = next sentence\nprediction, Tokenization: WP = WordPiece, SP = SentecePiece, unigram model.\nLng Model COMET\nar Google Translate .1163\nOPUS MT 2022 .1183\nOPUS MT 2020 .1163\ncs CUBBITT .1212\nOPUS MT 2022 .1212\nOPUS MT 2020 .1197\nGoogle Translate .1193\nde UEdin WMT21 .1191\nFacebook, WMT19 .1191\nGoogle Translate .1190\nOPUS MT 2022 .1180\nOPUS MT 2020 .1123\nzh Google Translate .1111\nOPUS MT 2020 .1101\nTable 6: Machine translation quality of the templated\nsentences use in the MoralDimension estimation mea-\nsured by the reference-free COMET score. Unused\nalternatives are in gray.\nLng Model COMET\nar\nas in XNLI\n.1013\nde .1051\nzh .1017\ncs CUBBITT .1153\nGoogle Translate .1150\nOPUS MT 2022 .1144\nOPUS MT 2020 .1126\nTable 7: Machine translation quality of the MNLI data\nused for training S-BERT models measured by the\nreference-free COMET score. Unused alternatives are\nin gray.\nlanguage en ar cs de zh\nen\nar 0.97\ncs 0.98 0.97\nde 0.98 0.97 0.98\nzh 0.97 0.96 0.96 0.96\nTable 8: In-model correlation of scores on the user study\nquestions, within sentence-transformers/xlm-r-\n100langs-bert-base-nli-mean-tokens.\nParameter Value\nBatch size 128\nMax seq length 75\nEpochs 1\nWarmup 10% of train data\nSave steps 500\nOptimizer AdamW\nWeight decay 0.01\nTable 9: Sentence-BERT tuning parameters.\nthe variance for those groups. Overall, variances\nare similar for monolingual and multilingual mod-\nels. The positive verbs have a lower variance in\nthe multilingual than in the monolingual models.\nHowever, the opposite is true for the group of neg-\native verbs, averaging out to very similar variances\noverall. Therefore, analysing variances does not\nlead us to conclusions about differing behaviour of\nmonolingual versus multilingual models.\nG More Examples M ORAL DIMENSION\nfor Verbs\nAdditional examples to Figure 1 are shown in Fig-\nure 5.\nH OpenSubtitles Filtering Details\nFigure 6 shows the statistical correlation of the\nMORAL DIRECTION scores on the OpenSubtitles\ndataset, evaluated for the German-English text\n2151\nmono multi mono multi\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\npositive negative\nFigure 4: MORAL DIRECTION score variances through-\nout all languages of the verbs taken from Schramowski\net al. (2022), on monolingual and multilingual models.\nThe verbs are further grouped into positive and negative\nbased on mean scores.\npairs. The high Pearson correlation values give\nfurther evidence for a strong correlation of the com-\npared scores and the plausibility of this experiment.\nAs observed before with Section 3, evaluating on\nthe multilingual XLM-R model strengthens the cor-\nrelation of the MORAL DIRECTION .\nInitially, the most “controversial” sentence\npairs—i.e., ones with extremely different\nMORAL DIRECTION scores—in the OpenSubtitles\ndata seemed to be due to mistranslated or mis-\naligned subtitles. In order to exclude these cases,\nwe applied filtering using dual cross-entropy score\n(Junczys-Dowmunt, 2018) based on OpusMT\nmodels (Tiedemann and Thottingal, 2020). For\nGerman-English, the correlation of the moral\nscore delta and the translation score pre-filtering\nwas -0.28 for XLM-R, -0.19 for the monolingual\nmodels, underlining that translation quality was\na frequent issue. After filtering this dropped to\n-0.10 and -0.05. The score deltas are normally\ndistributed.\nI OpenSubtitles: Further Czech-English\nExamples\nTable 10 shows further examples from the Czech-\nEnglish OpenSubtitles corpus. Several of them\ncontain verbs with prefixo- which seems to confuse\nthe multilingual model.\nJ Rephrased MFQ Questions\nThe first 15 questions of MFQ have the format,\n“When you decide whether something is right or\nwrong, to what extent are the following consider-\nations relevant to your thinking? Please rate each\nstatement using this scale: – not at all relevant (This\nconsideration has nothing to do with my judgments\nof right and wrong) – not very relevant – slightly\nrelevant – somewhat relevant – very relevant – ex-\ntremely relevant (This is one of the most important\nfactors when I judge right and wrong): Whether or\nnot someone suffered emotionally ”. We rephrase\nthis to “Someone suffered emotionally.”\nTable 11 lists the English MFQ questions that\nwe rephrased along with the sign of the multiplier\napplied (either -1 or +1).\nK Role of the “Catch” Questions in MFQ\nThe MFQ contains two catch questions, which are\ndesigned to have an obvious, uncontroversial an-\nswer. For human respondents, their purpose is to\nfilter out people who are not paying attention. For\nthe language models, they may indeed be infor-\nmative as well. In English, these questions are:\n“Someone was good at math.” and “It is better to\ndo good than to do bad.” For the first, we would ex-\npect the answer to be 0—this should be a perfectly\nneutral statement in a moral sense. For the other,\nwe expect an answer at least close to the maximum\nscore, since “doing good” is trivially better than\n“doing bad”.\nThe English, Chinese, and Czech models do give\nscores close to 0 for the maths question. In Arabic,\nour monolingual model assigns a slight negative\nscore, while XLM-R gives a moderately positive\nscore. In German, both models give a moderately\npositive score, likely because the chosen translation\n“Jemand zeigt in Mathematik gute Leistungen” con-\ntains the somewhat positively connotated “Leistun-\ngen” (performance, accomplishments, etc.). The\nsecond catch question gets anything from fairly\nnegative (-0.55), to neutral, to slightly positive\nscores, which again seems to fit with an over-\nreliance on lexical cues. This behaviour shows\nagain that while the models do capture the “moral\ndimension” to some degree, they have significant\nweaknesses, particularly with respect to the com-\npositional meanings of longer phrases. In a real\nsurvey, they may not even have been considered\n“serious” respondents.\n2152\ndestruct bully misdirect lie blame apologize care go be dream inspire hug compliment cheer\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nMoral score\nmono\nar\ncs\nde\nen\nzh\nmulti\nar\ncs\nde\nen\nzh\nFigure 5: M ORAL DIRECTION score (y-axis) for more verbs (x-axis) than in Figure 1.\nmonoling XLM-R\ncs en cs en cs en\nTo je opravdu štˇedré. It’s very generous. 0.50 0.88 -0.46 1.00\nTvá neteˇr mˇe velmi ohromila. I was very impressed with your niece. 0.34 0.42 -0.58 0.84\nOhrožuje vaši budoucnost. He threatens your future. -0.88 -0.70 0.42 -0.56\nPolíbila jsi ho. You kissed him. 0.52 0.21 1.01 -0.36\nJste vrah! You’re a murderer! -0.97 -0.99 0.38 -0.77\nOtrávils nás. You poisoned us. 0.37 -0.82 -0.62 -0.63\nTable 10: Examples from the (Czech-English) OpenSubtitles data for which there is a large, spurious contrast\nbetween MORAL DIRECTION scores. Scores that stand out as unreasonable are italicised.\n1.0\n 0.5\n 0.0 0.5 1.0\nde_xlmr_score\n1.0\n0.5\n0.0\n0.5\n1.0\nen_xlmr_score\np=0.89\n1.0\n 0.5\n 0.0 0.5 1.0\nde_gbert_score\n1.0\n0.5\n0.0\n0.5\n1.0\nen_bert_score\np=0.78\n1.0\n 0.5\n 0.0 0.5 1.0\nen_bert_score\n1.0\n0.5\n0.0\n0.5\n1.0\nen_xlmr_score\np=0.82\n1.0\n 0.5\n 0.0 0.5 1.0\nde_gbert_score\n1.0\n0.5\n0.0\n0.5\n1.0\nde_xlmr_score\np=0.84\nFigure 6: Correlation of the MORAL DIRECTION scores\nfor all German-English model combinations on the\nOpenSubtitles dataset.\nCare Fairness Loyalty Authority Purity\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\nFigure 7: Example of human variation. Five different\nrespondents from the German data collected by Joeckel\net al. (2012).\n2153\nOriginal (after simplifying) sign Rephrased sign\nSomeone showed a lack of respect for authority. - Someone showed respect for authority. +\nSomeone showed a lack of loyalty. - Someone showed loyalty. +\nPeople should not do things that are disgusting,\neven if no one is harmed.\n+ People should do things that are disgusting, if no\none is harmed.\n-\nOne of the worst things a person could do is hurt a\ndefenseless animal.\n+ One of the best things a person could do is hurt a\ndefenseless animal.\n-\nI would call some acts wrong on the grounds that\nthey are unnatural.\n+ I would call some acts right on the grounds that\nthey are natural.\n+\nIt can never be right to kill a human being. + It can be right to kill a human being. -\nI think it’s morally wrong that rich children inherit\na lot of money while poor children inherit nothing.\n+ I think it’s morally right that rich children inherit a\nlot of money while poor children inherit nothing.\n-\nIf I were a soldier and disagreed with my com-\nmanding officer’s orders, I would obey anyway\nbecause that is my duty.\n+ If I were a soldier and disagreed with my com-\nmanding officer’s orders, I would disobey.\n-\nTable 11: Rephrased MFQ statements in English. Unchanged statements are omitted from this table.\n2154\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2155\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n2156",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.5411078929901123
    },
    {
      "name": "Computer science",
      "score": 0.46295955777168274
    },
    {
      "name": "Association (psychology)",
      "score": 0.4504460096359253
    },
    {
      "name": "Philosophy",
      "score": 0.35002031922340393
    },
    {
      "name": "Epistemology",
      "score": 0.18133017420768738
    }
  ]
}