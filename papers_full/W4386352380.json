{
  "title": "Empirical evaluation of language modeling to ascertain cancer outcomes from clinical text reports",
  "url": "https://openalex.org/W4386352380",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4313550507",
      "name": "Haitham A. Elmarakeby",
      "affiliations": [
        "Harvard University",
        "Dana-Farber Cancer Institute",
        "Broad Institute",
        "Al-Azhar University"
      ]
    },
    {
      "id": null,
      "name": "Pavel S. Trukhanov",
      "affiliations": [
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2887055285",
      "name": "Vidal M. Arroyo",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A1984599700",
      "name": "Irbaz Bin Riaz",
      "affiliations": [
        "Harvard University",
        "Mayo Clinic in Arizona",
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2151190238",
      "name": "Deborah Schrag",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2128166213",
      "name": "Eliezer M. Van Allen",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Harvard University",
        "Broad Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2265725135",
      "name": "Kenneth L. Kehl",
      "affiliations": [
        "Harvard University",
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4313550507",
      "name": "Haitham A. Elmarakeby",
      "affiliations": [
        "Dana-Farber Cancer Institute",
        "Al-Azhar University",
        "Harvard University",
        "Broad Institute"
      ]
    },
    {
      "id": null,
      "name": "Pavel S. Trukhanov",
      "affiliations": [
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1984599700",
      "name": "Irbaz Bin Riaz",
      "affiliations": [
        "Mayo Clinic in Arizona",
        "Harvard University",
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2151190238",
      "name": "Deborah Schrag",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2128166213",
      "name": "Eliezer M. Van Allen",
      "affiliations": [
        "Broad Institute",
        "Dana-Farber Cancer Institute",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2265725135",
      "name": "Kenneth L. Kehl",
      "affiliations": [
        "Harvard University",
        "Dana-Farber Cancer Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2171756499",
    "https://openalex.org/W3217485783",
    "https://openalex.org/W2963676638",
    "https://openalex.org/W4200481888",
    "https://openalex.org/W3047190783",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3186552329",
    "https://openalex.org/W4281479095",
    "https://openalex.org/W3200849552",
    "https://openalex.org/W3198380000",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2551013093",
    "https://openalex.org/W1978394996",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4385573637",
    "https://openalex.org/W2999309192",
    "https://openalex.org/W2396881363"
  ],
  "abstract": null,
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328  \nhttps://doi.org/10.1186/s12859-023-05439-1\nBMC Bioinformatics\nEmpirical evaluation of language modeling \nto ascertain cancer outcomes from clinical text \nreports\nHaitham A. Elmarakeby1,2,3,4*, Pavel S. Trukhanov1, Vidal M. Arroyo5, Irbaz Bin Riaz1,3,6, Deborah Schrag7, \nEliezer M. Van Allen1,3,4 and Kenneth L. Kehl1,3 \nAbstract \nBackground: Longitudinal data on key cancer outcomes for clinical research, such \nas response to treatment and disease progression, are not captured in standard cancer \nregistry reporting. Manual extraction of such outcomes from unstructured electronic \nhealth records is a slow, resource-intensive process. Natural language processing (NLP) \nmethods can accelerate outcome annotation, but they require substantial labeled \ndata. Transfer learning based on language modeling, particularly using the Trans-\nformer architecture, has achieved improvements in NLP performance. However, there \nhas been no systematic evaluation of NLP model training strategies on the extraction \nof cancer outcomes from unstructured text.\nResults: We evaluated the performance of nine NLP models at the two tasks of iden-\ntifying cancer response and cancer progression within imaging reports at a single \nacademic center among patients with non-small cell lung cancer. We trained the clas-\nsification models under different conditions, including training sample size, classifica-\ntion architecture, and language model pre-training. The training involved a labeled \ndataset of 14,218 imaging reports for 1112 patients with lung cancer. A subset of mod-\nels was based on a pre-trained language model, DFCI-ImagingBERT, created by fur-\nther pre-training a BERT-based model using an unlabeled dataset of 662,579 reports \nfrom 27,483 patients with cancer from our center. A classifier based on our DFCI-\nImagingBERT, trained on more than 200 patients, achieved the best results in most \nexperiments; however, these results were marginally better than simpler “bag of words” \nor convolutional neural network models.\nConclusion: When developing AI models to extract outcomes from imaging reports \nfor clinical cancer research, if computational resources are plentiful but labeled training \ndata are limited, large language models can be used for zero- or few-shot learning \nto achieve reasonable performance. When computational resources are more limited \nbut labeled training data are readily available, even simple machine learning architec-\ntures can achieve good performance for such tasks.\nKeywords: Cancer, Natural language processing, Clinical outcomes, Information \nextraction, Transformer-based language models\n*Correspondence:   \nhaithama_elmarakeby@dfci.\nharvard.edu\n1 Dana-Farber Cancer Institute, \nBoston, MA, USA\n2 Al-Azhar University, Cairo, Egypt\n3 Harvard Medical School, \nBoston, MA, USA\n4 The Broad Institute of MIT \nand Harvard, Cambridge, MA, \nUSA\n5 Stanford University, Stanford, \nCA, USA\n6 Mayo Clinic, Rochester, MN, \nUSA\n7 Memorial-Sloan Kettering \nCancer Center, New York, NY, USA\nPage 2 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nBackground\nPrecision oncology, defined as tailoring cancer treatment to the individual clinical and \nmolecular characteristics of patients and their tumors [1], is an increasingly impor -\ntant goal in cancer medicine. This strategy requires linking tumor molecular data [2] \nto data on patient outcomes to ask research questions about the association between \ntumor characteristics and treatment effectiveness. Despite the increasing sophistication \nof molecular and bioinformatic techniques for genomic data collection, the ascertain -\nment of corresponding clinical outcomes from patients who undergo molecular testing \nhas remained a critical barrier to precision cancer research. Outside of therapeutic clini -\ncal trials, key clinical outcomes necessary to address major open questions in precision \noncology, such as which biomarkers predict cancer response (improvement) and pro -\ngression (worsening), are generally recorded only in the free text documents generated \nby radiologists and oncologists as they provide routine clinical care.\nClinical cancer outcomes other than overall survival are not generally captured in \nstandard cancer registry workflows; historically, abstraction of such outcomes from \nthe electronic health record (EHR) has therefore required resource-intensive man -\nual annotation. If this abstraction has occurred at all, it has generally been performed \nwithin individual research groups in the absence of data standards, yielding datasets \nof questionable generalizability. To address this gap, our research group developed \nthe ‘PRISSMM’ framework for EHR review. PRISSMM is a structured rubric for man -\nual annotation of each pathology, radiology/imaging, and medical oncologist report \nto ascertain cancer features and outcomes; each imaging report is reviewed in its own \nright to determine whether it describes cancer response, progression, or neither [3]. \nThis annotation process also effectively yields document labels that can be used to train \nmachine learning-based natural language processing (NLP) models to recapitulate these \nmanual annotations. We previously detailed the PRISSMM annotation directives for \nascertaining cancer outcomes and demonstrated the feasibility of using PRISSMM labels \nto train NLP models that can identify cancer outcomes within imaging reports [3, 4] and \nmedical oncologist notes [4, 5].\nWhile applying NLP to clinical documents can dramatically accelerate outcome ascer -\ntainment, training these models from randomly initialized weights remains resource-\nintensive, requiring thousands of manually annotated documents. Modern advances in \nNLP could reduce this data labeling burden. Semi-supervised learning techniques based \non language modeling, or using components of a sentence or document to predict the \nremainder of the text, have become cornerstones of NLP [6]. The Universal Language \nModel Fine-Tuning technique demonstrated that by first training a language model on a \nlarge general text corpus and then further pre-training it on in-domain text, it is possible \nto fine-tune useful text classifiers using far fewer labeled examples than might otherwise \nbe required [7]. Simultaneously, the Transformer architecture [8] and many of its deriva-\ntives, such as Bidirectional Encoder Representations from Transformers (BERT) [9] \nand BERT versions fine-tuned on clinical text [10] have facilitated training of high-per -\nforming, large language models. These architectures have often been designed around \nprocessing relatively short segments of text, but methods for applying them to longer \ndocuments, including the Transformer-XL [11], Reformer [12], and Longformer [13] \narchitectures, have also been developed. Transformer-based models have been applied \nPage 3 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nto radiology reports and found to outperform simpler methods at certain general medi -\ncal annotation tasks [14–17]. These models also facilitate the emerging paradigm of \nzero-shot Learning, in which a scaled-up, pretrained language model is primed for NLP \ntasks via conditional language generation. It has been successful in general domain NLP \n[18] for different tasks. Specifically for question-answering tasks, the Text-to-Text Trans-\nfer Transformer [19] with instruction finetuning [20] has yielded impressive results for \nreasonable model sizes. To our knowledge, preceding investigations have not yielded \ncompetitive performance on biomedical NLP tasks while utilizing general LLMs [21]. \nThe practical utility of these architectures for ascertaining cancer outcomes in clinical \nresearch settings using limited quantities of labeled EHR text is unknown.\nIn this study, we evaluated the performance of various NLP architectures at capturing \ncancer response and progression from imaging reports for a cohort of patients with lung \ncancer. Candidate architectures included simple ‘bag of words’ linear models and convo-\nlutional neural networks [22], as well as Transformer architectures with language model \npretraining. We varied the size of the training dataset for each architecture to evaluate \nthe association of architecture with the quantity of labeled data required to train high-\nperforming models.\nResults\nWe first examined the impact of model architecture, number of parameters, language \nmodel domain adaptation, and classification head structure on the performance of \nBERT-based models for ascertaining cancer response/improvement or progression/\nworsening from imaging reports. With no domain adaptation, a frozen language model, \nand a CNN classification head, the BERT-base model yielded AUROC of 0.93 for cap -\nturing response/improvement and 0.92 for cancer progression/worsening, outperform -\ning BERT-tiny, BERT-mini, BERT-med, and a Longformer architecture (Fig.  1, Table 1). \nThe metrics of AUPRC, accuracy, precision, recall, MCC, and F1 scores are provided in \nTable 1 and shown in Additional file  1: Figs. 1–2. Additional model characteristics are \nprovided in Table 2. \nAfter domain adaptation on imaging reports from our institution (N = 662,579 reports \nfrom 27,483 patients with multiple types of cancer) was performed beginning with a \nBERT-base model, the resulting language model (DFCI-ImagingBERT) was frozen and \nfine-tuned with a CNN classification head, yielding better performance than BERT-base, \nwith AUROCs of 0.94 for ascertaining response/improvement and 0.95 for progression/\nworsening. Models based on ClinicalBERT yielded slightly lower performance compared \nto DFCI-ImagingBERT, with an AUROC of 0.93for both response and progression out -\ncomes (Fig. 2, Table 1). AUPRC, accuracy, precision, recall, F1, and MCC scores for these \nmodels are shown in Additional file 1: Figs. 3–4.\nWe also evaluated the impact of varying classification head architectures for DFCI-\nImagingBERT, including a CNN; a linear layer based on the classification token vector; \nand an RNN (bidirectional gated recurrent unit) [23] architecture on model performance \nas a function of training set size (Fig.  3, Additional file  1: Fig.  5). When the language \nmodel layers were frozen, the CNN head was associated with the best performance at \nthe largest training set size, yielding the performance metrics described above. The RNN \nhead yielded AUROCs of 0.78 for response/improvement and 0.85 for progression/\nPage 4 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nworsening; the linear head yielded AUROCs of 0.72 for response/improvement and \n0.84 for progression/worsening. With the language model unfrozen, given the full train -\ning set, the linear head yielded AUROCs of 0.92 and 0.94, and the CNN was best with \nAUROCs of 0.93 and 0.95 for response and progression outcomes respectively. The lin -\near head, however, yielded better performance at smaller training set sizes (Fig. 3).\nWe therefore selected DFCI-ImagingBERT as the BERT-based model to compare \nagainst the simple term frequency-inverse document frequency (TF-IDF) and CNN \nneural network architectures. The performance of each of these architectures as a func -\ntion of training set size is depicted in Fig.  4. For the response/improvement outcome, \nDFCI-ImagingBERT with a frozen language model and CNN classification head yielded \nthe best performance, with an AUROC of 0.94 after training on the full training data -\nset; DFCI-ImagingBERT with an unfrozen language model and linear head yielded an \nAUROC of 0.93; the simple CNN model yielded an AUROC of 0.94; and the TF-IDF \nmodel yielded an AUROC of 0.93. For the progression/worsening outcome, DFCI-\nImagingBERT (either with a frozen language model and CNN head or an unfrozen lan -\nguage model and linear head) yielded the best performance, with an AUROC of 0.95; \nfollowed by the CNN, with an AUROC of 0.93; and the TF-IDF model, with an AUROC \nof 0.92. The largest gains in model performance were achieved when the training set size \nFig. 1 Impact of BERT model architecture on performance. Performance of Transformer-based architectures \n(with the language model frozen) for the document classification tasks of identifying cancer progression/\nworsening and response/improvement. In this figure, all architectures were fine-tuned directly on the \nclassification tasks, using a convolutional neural network head, without language model pre-training. For \nboxplots in the right column, the middle line represents the median, the lower and upper hinges correspond \nto the 1st and 3rd quartiles, and the whisker corresponds to the minimum or maximum values no further \nthan 1.5 times the inter-quartile range from the hinge. Data beyond the whiskers are outlying points, plotted \nindividually in the scatter plots\nPage 5 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nTable 1 Performance of Transformer-based architectures compared to baseline models\nProgression\nAccuracy Precision AUROC\n[95% CI]\nF1 AUPRC Recall MCC\nBERT-Base 0.88\n[0.87, 0.90]\n0.71\n[0.66, 0.76]\n0.92\n[0.91, 0.94]\n0.72\n[0.68, 0.76]\n0.76\n[0.72, 0.81]\n0.74\n[0.69, 0.79]\n0.65\n[0.60, 0.70]\nBERT-Med 0.88\n[0.86, 0.89]\n0.69\n[0.64, 0.73]\n0.92\n[0.91, 0.94]\n0.73\n[0.69, 0.76]\n0.75\n[0.69, 0.80]\n0.77\n[0.72, 0.81]\n0.65\n[0.60, 0.69]\nBERT-Mini 0.85\n[0.83, 0.87]\n0.61\n[0.56, 0.66]\n0.89\n[0.88, 0.91]\n0.68\n[0.64, 0.72]\n0.68\n[0.62, 0.73]\n0.77\n[0.72, 0.81]\n0.59\n[0.54, 0.63]\nBERT-Tiny 0.80\n[0.78, 0.82]\n0.51\n[0.46, 0.56]\n0.84\n[0.82, 0.86]\n0.56\n[0.52, 0.61]\n0.56\n[0.50, 0.63]\n0.63\n[0.58, 0.68]\n0.43\n[0.38, 0.49]\nLongformer 0.86\n[0.84, 0.87]\n0.70\n[0.63, 0.75]\n0.89\n[0.87, 0.91]\n0.62\n[0.57, 0.67]\n0.70\n[0.65, 0.75]\n0.55\n[0.50, 0.61]\n0.54\n[0.48, 0.59]\nClinical BERT 0.88\n[0.86, 0.89]\n0.69\n[0.64, 0.74]\n0.93\n[0.91, 0.94]\n0.72\n[0.68, 0.75]\n0.77\n[0.72, 0.82]\n0.75\n[0.70, 0.80]\n0.64\n[0.59, 0.69]\nDFCI-Imag-\ningBERT (BERT \nfrozen, CNN \nhead)\n0.90\n[0.89, 0.92]\n0.75\n[0.70, 0.79]\n0.95\n[0.94, 0.96]\n0.78\n[0.74, 0.81]\n0.84\n[0.80, 0.87]\n0.81\n[0.77, 0.85]\n0.72\n[0.68, 0.76]\nDFCI-Imag-\ningBERT (BERT \nunfrozen, \nlinear head)\n0.90\n[0.89, 0.92]\n0.74\n[0.69, 0.79]\n0.95\n[0.94, 0.96]\n0.78\n[0.74, 0.81]\n0.85\n[0.81, 0.89]\n0.81\n[0.77, 0.85]\n0.71\n[0.67, 0.76]\nCNN 0.89\n[0.87, 0.90]\n0.72\n[0.66, 0.76]\n0.93\n[0.92, 0.95]\n0.74\n[0.70, 0.78]\n0.81\n[0.77, 0.85]\n0.77\n[0.72, 0.82]\n0.67\n[0.62, 0.72]\nTF-IDF 0.88\n[0.86, 0.89]\n0.72\n[0.67, 0.77]\n0.92\n[0.90, 0.93]\n0.69\n[0.64, 0.73]\n0.75\n[0.71, 0.80]\n0.66\n[0.61, 0.71]\n0.61\n[0.56, 0.66]\nFlan-T5-XXL \n(zero-shot)\n0.89\n[0.87, 0.90]\n0.77\n[0.72, 0.82]\n0.92\n[0.91, 0.94]\n0.71\n[0.66, 0.75]\n0.77\n[0.72, 0.81]\n0.65\n[0.60, 0.71]\n0.64\n[0.59, 0.69]\nResponse\nAccuracy Precision AUROC\n[95% CI]\nF1 AUPRC Recall MCC\nBERT-Base 0.93\n[0.92, 0.95]\n0.80\n[0.74, 0.85]\n0.93\n[0.90, 0.95]\n0.73\n[0.68, 0.78]\n0.78\n[0.73, 0.83]\n0.67\n[0.61, 0.74]\n0.70\n[0.64, 0.75]\nBERT-Med 0.93\n[0.92, 0.94]\n0.75\n[0.69, 0.81]\n0.92\n[0.90, 0.95]\n0.71\n[0.66, 0.76]\n0.78\n[0.72, 0.83]\n0.68\n[0.62, 0.74]\n0.67\n[0.62, 0.73]\nBERT-Mini 0.92\n[0.91, 0.94]\n0.72\n[0.65, 0.78]\n0.90\n[0.88, 0.93]\n0.71\n[0.66, 0.76]\n0.74\n[0.67, 0.79]\n0.71\n[0.65, 0.77]\n0.67\n[0.61, 0.72]\nBERT-Tiny 0.89\n[0.88, 0.91]\n0.59\n[0.53, 0.66]\n0.86\n[0.83, 0.89]\n0.61\n[0.55, 0.67]\n0.63\n[0.57, 0.70]\n0.63\n[0.57, 0.70]\n0.55\n[0.49, 0.61]\nLongformer 0.92\n[0.90, 0.93]\n0.80\n[0.72, 0.87]\n0.89\n[0.86, 0.91]\n0.61\n[0.54, 0.67]\n0.71\n[0.64, 0.77]\n0.49\n[0.42, 0.56]\n0.59\n[0.52, 0.65]\nClinical BERT 0.93\n[0.92, 0.94]\n0.77\n[0.70, 0.83]\n0.93\n[0.90, 0.95]\n0.72\n[0.66, 0.77]\n0.77\n[0.70, 0.83]\n0.67\n[0.61, 0.74]\n0.68\n[0.62, 0.73]\nDFCI-\nImagingBERT \n(BERT frozen, \nCNN head)\n0.94\n[0.93, 0.95]\n0.83\n[0.77, 0.89]\n0.94\n[0.93, 0.96]\n0.76\n[0.71, 0.80]\n0.81\n[0.76, 0.86]\n0.69\n[0.63, 0.76]\n0.73\n[0.67, 0.78]\nDFCI-\nImagingBERT \n(BERT unfro-\nzen, linear \nhead)\n0.94\n[0.93, 0.95]\n0.84\n[0.77, 0.89]\n0.93\n[0.91, 0.95]\n0.73\n[0.68, 0.78]\n0.80\n[0.75, 0.85]\n0.65\n[0.59, 0.72]\n0.71\n[0.65, 0.76]\nCNN 0.93\n[0.92, 0.94]\n0.92\n[0.86, 0.97]\n0.94\n[0.92, 0.96]\n0.67\n[0.60, 0.72]\n0.82\n[0.77, 0.87]\n0.52\n[0.45, 0.59]\n0.66\n[0.60, 0.72]\nTF-IDF 0.93\n[0.91, 0.94]\n0.81\n[0.74, 0.87]\n0.93\n[0.91, 0.95]\n0.68\n[0.63, 0.73]\n0.75\n[0.69, 0.81]\n0.59\n[0.53, 0.66]\n0.65\n[0.59, 0.71]\nFlan-T5-XXL \n(zero-shot)\n0.92\n[0.90, 0.93]\n0.69\n[0.63, 0.76]\n0.90\n[0.87, 0.93]\n0.69\n[0.64, 0.74]\n0.69\n[0.61, 0.75]\n0.68\n[0.61, 0.75]\n0.64\n[0.58, 0.70]\nPage 6 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nincreased from reports for 50 patients up to reports for 300 patients, with diminishing \nreturns as training set size increased further thereafter. For the progression outcome, \nDFCI-ImagingBERT with an unfrozen language model and linear classification head \nperformed best on samples of patients smaller than 300; for the response outcome, the \nTF-IDF model performed best on such samples (Fig. 4).\nFinally, we evaluated the performance of the Flan-T5-XXL text to text model for zero-\nshot learning for these tasks, with no weight updates and limited prompt engineering \nand hyperparameter tuning. This model achieved AUROC of 0.92 for the progression/\nworsening task and AUROC of 0.90 for the response/improvement task.\nTable 1 (continued)\nPerformance of Transformer-based architectures compared to baseline models for the document classification tasks of \nidentifying cancer progression/worsening and response/improvement. Additional model characteristics are provided in \nTable 2. Precision, Recall, and F1 measures are calculated using the model output score threshold that maximizes the F1 \nscore in the training set. The best AUROC for each outcome is in bold face, as are the AUROC’s for any model that are not \nstatistically significantly different from the best AUROC for each outcome\nFig. 2 Impact of BERT model language model tuning on performance. Association between language \nmodel pre-training and ultimate classification model performance. BERT-base represents a BERT model \nwithout language model pre-training on clinical text; clinical BERT-base represents a BERT-base model, \nfine-tuned on intensive care unit EHR data; DFCI-ImagingBERT represents a BERT-base model, with its \nlanguage model further pre-trained on in-domain imaging reports from our institution. Figure depicts results \nwith language models that were frozen for downstream classification task. For boxplots in the right column, \nthe middle line represents the median, the lower and upper hinges correspond to the 1st and 3rd quartiles, \nand the whisker corresponds to the minimum or maximum values no further than 1.5 times the inter-quartile \nrange from the hinge. Data beyond the whiskers are outlying points, plotted individually in the scatter plots\nPage 7 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nDiscussion\nNatural language processing has the potential to substantially accelerate precision oncol-\nogy research by enabling observational clinical outcomes to be linked to molecular can -\ncer data for downstream analysis, increasing the sample size of clinico-genomic datasets \nseveral times over [4]. This process allows outcomes to be ascertained on a timepoint-\nspecific basis, facilitating analysis for research questions that may focus on different por-\ntions of the disease trajectory. NLP could also inform cancer care delivery by processing \nEHR data to identify patients who have specific disease states at individual moments in \ntime, and who could therefore benefit from interventions such as clinical trial enroll -\nment or palliative care services.\nTransformer-based models have become standard for general NLP tasks given their \npotential to yield improved performance, potentially while relying upon less labeled \ndata to train supervised learning models. We found that a BERT model with domain \nadaptation on text from our institution performed better than simpler TF-IDF and \nCNN models for text classification, but the simple models still yielded AUROCs > 0.9, \nsuch that depending on end use case and computational resources, complex models \nmay not always be needed if training data are readily available. On the other hand, \nwe found that the Flan-T5-XXL architecture with a small amount of prompt engi -\nneering yielded good zero-shot performance with no domain adaptation pretraining \nFig. 3 Impact of classification head on performance (DFCI-ImagingBERT). Associations among classification \nhead, training dataset size, and model performance for progression/worsening (left) and response/\nimprovement (right) for the DFCI-ImagingBERT architecture. CNN = convolutional neural network; \nRNN = recurrent neural network. Linear = fully connected layer applied to the BERT [CLS] token for each \ndocument\nPage 8 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nor fine-tuning on labeled data, demonstrating the potential utility of large language \nmodels in this space when computational resources are readily available.\nThere are several potential explanations for the similar performance observed \nbetween a Transformer architecture and simpler models for this clinical text classi -\nfication task, particularly for the response/improvement outcome. One possibility is \nthat the outcomes are distinctly keyword-sensitive, such that a few words within long \ndocuments may define outcomes with relatively less dependence on other context \nfrom the document. Clinical imaging reports are also substantially longer than typi -\ncal sequence lengths for standard Transformer models; this may dilute the benefits \nderived from contextual token embedding by language models in other contexts. Still, \nwe also evaluated the Longformer, which is a Transformer specifically designed for \nlonger documents, and we did not observe improved performance for downstream \nclassification tasks compared with a BERT architecture.\nStrengths of this analysis include its derivation from a previously described labeled \ndataset of cancer outcomes linked to imaging reports; labels in this dataset have been \nshown to be clinically relevant and associated with overall survival [4]. Our results pro -\nvide practical guidance to researchers who may seek to gather just the necessary volume \nFig. 4 Comparing DFCI-ImagingBERT model performance to baseline models. Model performance as \na function of architecture and training dataset size for identifying progression/worsening (top row) and \nresponse/improvement (bottom row). For boxplots in the right column, the middle line represents the \nmedian, the lower and upper hinges correspond to the 1st and 3rd quartiles, and the whisker corresponds \nto the minimum or maximum values no further than 1.5 times the inter-quartile range from the hinge. \nData beyond the whiskers are outlying points, plotted individually in the scatter plots. TF-IDF: term \nfrequency-inverse document frequency. CNN: convolutional neural network\nPage 9 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nof labeled clinical data in order to train NLP models to perform cancer outcome extrac -\ntion. We found that, as expected, model performance improves with greater training \nset size, but that the marginal improvement once the training set reached 300 patients \n(~ 3000 imaging reports) was relatively small.\nLimitations include the single-institution nature of the data, as well as limited hyper -\nparameter tuning. Transformers are notoriously challenging to train, requiring special -\nized learning rate schedules and initialization strategies for optimization [24]. Given the \ncomputational complexity of domain adaptation and classification fine-tuning for Trans-\nformer models, it was not feasible to perform automated (e.g., grid search) hyperparam -\neter tuning for this analysis. It is possible that any of our models might have performed \nTable 2 Model characteristics\nTF-IDF Term Frequency-Inverse Document Frequency, CNN convolutional neural network, BERT Bidirectional Encoder \nRepresentations from Transformers [9], RoBERTa, Robustly optimized BERT approach [40], MIMIC Medical Information Mart \nfor Intensive Care, DFCI Dana-Farber Cancer Institute\nModel Architecture # of \nparameters \nTrainable/\nTotal\nPre-trained \nor contextual \ntoken \nembeddings?\nLanguage \nmodel pre-\ntrained in \ndomain?\nLanguage \nmodel \nfrozen for \nclassification \ntraining?\nFinal \nclassification \ntraining layer/\nstrategy \ntested\nTF-IDF Bag of words \nlogistic \nregression \nwith elastic \nnet regulari-\nzation\n40 K No N/A N/A N/A\nCNN One-\ndimensional \nconvolu-\ntional neural \nnetwork with \nglobal max-\npooling\n7 M No N/A N/A N/A\nBERT-base BERT 766 K/110 M Yes No Yes CNN head\nBERT-med BERT 521 K/42 M Yes No Yes CNN head\nBERT-mini BERT 275 K/11 M Yes No Yes CNN head\nBERT-tiny BERT 152 K/4.5 M Yes No Yes CNN head\nLongformer \n[13]\nRoBERTa with \nlocal context \nand global \nattention\n766 K/128 M Yes No Yes CNN head\nClinicalBERT \n[10]\nBERT 766 K/110 M Yes Partial \n(trained on \nMIMIC-III ICU \ndata) [39]\nYes CNN head\nDFCI-\nImagingBERT, \nfrozen\nBERT 766 K/110 M Yes Yes (trained \non DFCI \nimaging \nreports)\nYes CNN head\nDFCI-\nImagingBERT, \nunfrozen\nBERT 110 M Yes Yes (trained \non DFCI \nimaging \nreports)\nNo Linear head\nFlan-T5 XXL Text to Text \nTransfer \nTransformer\n11 B Yes No N/A (zero-shot \nlearning only)\n1−the \npredicted \nprobability of \nthe word “no”\nPage 10 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nbetter with additional tuning, and that Transformers might yield more improvements \non different EHR data tasks. Many variants of healthcare-relevant language models exist \n[10, 25]; we chose one trained on electronic health records text, rather than academic \npublications, as a basis for evaluation. This precludes sharing the DFCI-ImagingBERT \nmodel weights beyond our institution to evaluate external generalizability. However, our \ngoal in this analysis is to provide practical guidance for small teams seeking to extract \ncancer outcomes at academic medical centers, which may have computational resources \ncomparable to those in our study, not to train models for external application. NLP mod-\nels trained on institutional protected health information may carry at least some risk \nof exposing that information to adversarial attacks [26], and further research into best \npractices for generalizable cross-institution NLP healthcare modeling is needed.\nConclusion\nWe conducted a systematic evaluation of NLP models for extracting clinical cancer \noutcomes from EHR data. A BERT model with domain adaptation and supervised fine-\ntuning for classification yielded the best performance across tasks and metrics, though \nsimpler models demonstrated good performance given large quantities of training data. \nZero-shot learning based on modern large language models also demonstrated good \nperformance on some metrics. The reported quantitative results suggest that when \ndeveloping AI models to extract outcomes from imaging reports for clinical cancer \nresearch, if computational resources are plentiful but labeled training data are limited, \nlarge language models can be used for zero- or few-shot learning to achieve reasonable \nperformance. When computational resources are more limited but labeled training data \nare readily available, even simple machine learning architectures can achieve good per -\nformance for such tasks.\nMethods\nCohort\nThe overall cohort for this analysis consisted of patients with cancer participating in a \nsingle-institution genomic profiling study [27], and relevant data consisted of imaging \nreports for each patient. Each report was treated as its own unit of analysis, and reports \nwere divided, at the patient level, into training (80%), validation (10%), and test (10%) \ndatasets.\nFor language model pre-training on data from our institution, reports for all patients \nin the training set were included. This dataset included 662,579 reports from 27,483 \npatients with multiple types of cancer whose tumors were sequenced through our insti -\ntutional precision medicine study [27].\nFor classification model training, the imaging reports for a subset of patients with \nlung cancer were manually annotated to ascertain the presence of cancer response or \nprogression in each report using the PRISSMM framework, as previously described \n[3]. Briefly, during manual annotation, human reviewers recorded whether each imag -\ning report indicated any cancer, and if so, whether it was responding/improving, pro -\ngressing/worsening, stable (neither improving nor worsening), mixed (with some areas \nimproving and some worsening), or indeterminate (if assigning a category was not pos -\nsible due to radiologist uncertainty or other factors). For NLP model training, response/\nPage 11 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nimprovement and progression/worsening were each treated as binary outcomes, such \nthat an imaging report indicating no cancer, or indicating stable, mixed, or indetermi -\nnate cancer status, was coded as neither improving nor worsening. This process, and \ninterrater reliability statistics for manual annotation, have been described previously [3]. \nThe classification dataset consisted of 14,218 labeled imaging reports for 1112 patients. \nAmong the reports, 1635 (11.5%) indicated cancer response/improvement, and 3522 \n(24.8%) indicated cancer progression/worsening.\nModels\nOur baseline architecture was a simple logistic regression model in which the text of \neach imaging report was vectorized using term frequency-inverse document fre -\nquency (TF-IDF) vectorization [28]. This model used elastic net regularization with \nalpha = 0.0001, L1 ratio of 0.15, and was trained with stochastic gradient descent. Other \narchitectures included one-dimensional convolutional neural networks (CNNs) [22] \nand Transformer-based [8] networks. For the CNNs, text was tokenized and numerical -\nized using the Tensorflow Keras tokenizer, with a vocabulary size of 20,000. For Trans -\nformer networks, the Huggingface tokenizer [29] corresponding to each Transformer \narchitecture was applied. We first evaluated a convolutional neural network architecture \n(CNN), trained only using our labeled data, as previously described [3], except with only \none output per model. Next, we evaluated classification heads based on BERT models \ntrained on general domain text only, using progressively larger numbers of parameters \n(BERT-tiny, BERT-mini, BERT-med, or BERT-base); as well as a Longformer model. We \nnext evaluated BERT models adapted on general medical text (ClinicalBERT) [10], or \nfirst adapted from general domain BERT-base on in-domain imaging reports from our \ninstitution (DFCI-ImagingBERT). For BERT models, text was truncated and padded \nto the maximum sequence length of the model (512 tokens) beginning from the end of \nthe document. For the CNN model, text was truncated and padded to a length of 1000 \ntokens beginning from the end of the document. For the Longformer, text was truncated \nand padded to a length of 1024 tokens from the end of the document. Training and eval-\nuation were performed using Pytorch [30] and Tensorflow [31].\nSubsequently, we conducted an evaluation of zero-shot learning using the T5 encoder-\ndecoder model [19] based on the Transformer architecture, and the Flan-T5 model [20], \nwhich is an instruction-finetuned variant of T5 that has demonstrated good perfor -\nmance across various natural language processing tasks. We additionally evaluated OPT \n[32] models up to 30B size, T0 [33] models, and some models pretrained on clinical/\nmedical domain corpora, namely ClinicalT5 [34], Clinical-T5 [35], and SciFive [36]. All \nof these models yielded only very modest results on our validation cohort, compared to \nFlan-T5 in XXL size, which we chose for our further analysis. We employed T5ForCon -\nditionalGeneration and corresponding tokenizer from the Huggingface transformers \nlibrary [29] with the following input text template: \"question: {question} context: {imag -\ning report}” . For the response/improvement task, the corresponding question text was \"Is \nthere improvement/response/shrinking of cancer (yes/no)?\" Similarly, for the progres -\nsion/worsening task, the question text was \"Is there worsening of cancer (yes/no)?.\" The \nselection of questions for each task involved a limited amount of manual prompt engi -\nneering identifying the questions that exhibited the best performance on the validation \nPage 12 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nset. The T5 model’s utilization of relative positional embeddings enabled us to use full \ninput texts without truncation. To determine the classification output, we adopted a spe-\ncific approach where we allowed the model to generate an output text and extracted the \nfirst token logits of the generated text. The probability of the \"yes\" class was computed \nusing softmax(1 − logit of the \"no\" token). This method demonstrated superior perfor -\nmance on the validation set compared with using the logit of the \"yes\" class directly.\nModel training\nFor BERT model domain adaptation on imaging reports from our institution (DFCI-\nImagingBERT), the base model was BERT-base, pre-trained on general domain text and \naccessed using the Huggingface library. Pre-training was performed over 10 epochs, \nwhich took 10.5  days on a single machine equipped with an NVIDIA Tesla T4 GPU \n(16 GB GDDR6).\nFor classification models, separate binary prediction models were trained to identify \nresponse/improvement and progression/worsening, since these constitute distinct out -\ncomes that would be used differently for downstream analyses (e.g., calculating response \nrate in a given time period, versus progression-free survival). A binary cross-entropy loss \nfunction and the Adam optimizer were applied for training for the TF-IDF and CNN \nmodels, and the AdamW (Adam with weight decay) optimizer [37] was applied for BERT \nand Longformer-based models.\nWe trained each model using fixed samples of reports from the training set, corre -\nsponding to the reports from 10, 30, 50, 70, 100, 200, 300, 500, 700, or 884 patients, to \nevaluate the rate at which performance of each architecture improved using progres -\nsively more training examples. For BERT-based models, experiments were also con -\nducted to examine the effect of various classification head architectures, including linear, \nconvolutional, and recurrent neural network architectures, on model performance; and \nto evaluate the impact of freezing the weights of the underlying language model when \nfine-tuning for classification. The full text of radiology reports—that is, the findings \nconcatenated to the impression—was used for each model. BERT-based models have a \nsequence length limitation of 512 tokens, so for these models, the final 512 tokens of \neach report were used. For the Longformer model, the final 1024 tokens were used. For \nthe simple CNN model, a sequence length of 1000 tokens was used. Reports shorter \nthan the maximum length were padded to the maximum length. The time needed to \ntrain a DFCI-ImagingBERT model for a classification task on the full training set was \n2.8 h on a machine equipped with a single NVIDIA T4 GPU. The time needed to train a \nLongformer model for a classification task on the full training set was 5.7 h on the same \nmachine. To reflect a real-world scenario in which a small team of academic researchers \nseeks to extract cancer outcomes from EHR data with relatively limited computational \nresources, limited hyperparameter tuning was performed. For the TF-IDF model, differ -\nent regularization approaches (L1, L2, and elastic net regularization) were tried. Hyper -\nparameters for each model were tuned based on evaluation in the validation set. Model \ntraining code is provided at https:// github. com/ marak eby/ clini calNL P2.\nPage 13 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \nModel evaluation\nClassification performance for outcomes (1) cancer response and (2) cancer progression \nwas evaluated using the area under the receiver operating characteristic curve (AUROC) \nand the area under the precision-recall curve (AUPRC). Additional metrics including \naccuracy, precision, recall, Matthew correlation coefficient (MCC) [38] and F1 score are \npresented; for all fine-tuned models, the threshold model output for a positive predic -\ntion was defined as the best F1 threshold in the training set; for Flan-T5 zero-shot, the \nthreshold probability was set to 0.5. After training was complete, models were evaluated \nusing data for held-out test set patients, after which no further training was performed. \nFigures were then generated to illustrate the performance of each architecture given \nspecific training set sizes. Interquartile ranges for model performance metrics were cal -\nculated using a bootstrapping approach (evaluation on repeated random subsets of the \ntest set). The AUROC of each model was compared to the AUROC of the best model \nstatistically using two-sided alpha of 0.05 based on the bootstrapping; no adjustment for \nmultiplicity was performed.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 023- 05439-1.\nAdditional file 1: Supplementary Figures.\nAcknowledgements\nNot applicable.\nAuthor contributions\nConceptualization: HAE, KLK, and EMV; Methodology: HAE, KLK, and EMV; Software: HAE, PST, and VMA; Investigation: \nHAE, PST, VMA, IBR, DS, EMV, and KLK; Writing, original draft: KLK; Writing, review and editing: HAE, PST, VMA, IBR, DS, EMV, \nand KLK; Visualization: HAE, and IBR; Supervision: KLK and EMV; Funding acquisition: HAE, KLK and EMV.\nFunding\nNCI R00CA245899, Doris Duke Charitable Foundation 2020080, DOD Grant (W81XWH-21-PCRP-DSA), DOD CDMRP \naward (HT9425-23-1-0023), Mark Foundation Emerging Leader Award, PCF-Movember Challenge Award.\nAvailability of data and materials\nThe underlying EHR text reports used to train and evaluate NLP models for these analyses constitute protected health \ninformation for DFCI patients and therefore cannot be made publicly available. Researchers with DFCI appointments and \nInstitutional Review Board (IRB) approval can access the data on request. For external researchers, access would require \ncollaboration with the authors and eligibility for a DFCI appointment per DFCI policies. Scripts used to implement, train, \nand evaluate the models are deposited in the public repository https:// github. com/ marak eby/ clini calNL P2.\nDeclarations\nEthics approval and consent to participate\nThe data for this analysis were derived from the EHRs of patients with lung cancer who had genomic profiling performed \nthrough the Dana-Farber Cancer Institute (DFCI) PROFILE [27] precision medicine effort or as a standard of care clini-\ncal test from June 26, 2013, to July 2, 2018. PROFILE participants consented to medical records review and genomic \nprofiling of their tumor tissue. PROFILE was approved by the Dana-Farber/Harvard Cancer Center Institutional Review \nBoard (protocol #11-104 and #17-000); this supplemental retrospective analysis was declared exempt from review, and \ninformed consent was waived for the standard of care genotyping patients given the minimal risk of data analysis, also \nby the Dana-Farber/Harvard Cancer Center Institutional Review Board (protocol #16-360). All methods were performed \nin accordance with the Declaration of Helsinki and approved by the Institutional Review Board at Dana Farber Cancer \nInstitute.\nConsent for publication\nNot applicable.\nCompeting interests\nDr. Kehl reports serving as a consultant/advisor to Aetion, receiving funding from the American Association for Cancer \nResearch related to this work, and receiving honoraria from Roche and IBM. Dr. Schrag reports compensation from JAMA \nfor serving as an Associate Editor and from Pfizer for giving a talk at a symposium. She has received research funding \nfrom the American Association for Cancer Research related to this work and research funding from GRAIL for serving \nPage 14 of 15Elmarakeby et al. BMC Bioinformatics          (2023) 24:328 \nas the site-PI of a clinical trial. Unrelated to this work, Dr. Van Allen reports serving in advisory/consulting roles to Tango \nTherapeutics, Genome Medical, Invitae, Enara Bio, Janssen, Manifold Bio, and Monte Rosa; receiving research support \nfrom Novartis and BMS; holding equity in Tango Therapeutics, Genome Medical, Syapse, Enara Bio, Manifold Bio, Micro-\nsoft, and Monte Rosa; and receiving travel reimbursement from Roche/Genentech. Pavel Trukhanov reports ownership \ninterest in NoRD Bio. The remaining authors declare no competing interests.\nReceived: 2 February 2023   Accepted: 7 August 2023\nReferences\n 1. Garraway LA, Verweij J, Ballman KV. Precision oncology: an overview. J Clin Oncol Off J Am Soc Clin Oncol. \n2013;31(15):1803–5.\n 2. AACR Project GENIE Consortium. AACR Project GENIE: powering precision medicine through an international con-\nsortium. Cancer Discov. 2017;7(8):818–31.\n 3. Kehl KL, Elmarakeby H, Nishino M, Van Allen EM, Lepisto EM, Hassett MJ, et al. Assessment of deep natural language \nprocessing in ascertaining oncologic outcomes from radiology reports. JAMA Oncol. 2019;5(10):1421–9.\n 4. Kehl KL, Xu W, Gusev A, Bakouny Z, Choueiri TK, Riaz IB, et al. Artificial intelligence-aided clinical annotation of a large \nmulti-cancer genomic dataset. Nat Commun. 2021;12(1):7304.\n 5. Kehl KL, Xu W, Lepisto E, Elmarakeby H, Hassett MJ, Van Allen EM, et al. Natural language processing to ascertain \ncancer outcomes from medical oncologist notes. JCO Clin Cancer Inform. 2020;4:680–90.\n 6. Dai AM, Le QV. Semi-supervised sequence learning. arXiv; 2015 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1511. 01432\n 7. Howard J, Ruder S. Universal language model fine-tuning for text classification. arXiv; 2018 [cited 2022 Sep 6]. http:// \narxiv. org/ abs/ 1801. 06146\n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN et al. Attention is all you need. arXiv; 2017 [cited \n2022 Sep 6]. http:// arxiv. org/ abs/ 1706. 03762\n 9. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language under-\nstanding. arXiv; 2019 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1810. 04805\n 10. Huang K, Altosaar J, Ranganath R. ClinicalBERT: modeling clinical notes and predicting hospital readmission. arXiv; \n2020 Nov [cited 2022 May 31]. Report No. http:// arxiv. org/ abs/ 1904. 05342\n 11. Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R. Transformer-XL: Attentive language models beyond a \nfixed-length context. arXiv; 2019 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1901. 02860\n 12. Kitaev N, Kaiser Ł, Levskaya A. Reformer: the efficient transformer. arXiv; 2020 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ \n2001. 04451\n 13. Beltagy I, Peters ME, Cohan A. Longformer: the long-document transformer. arXiv; 2020 [cited 2022 Sep 6]. http:// \narxiv. org/ abs/ 2004. 05150\n 14. Olthof AW, Shouche P , Fennema EM, IJpma FFA, Koolstra RHC, Stirler VMA, et al. Machine learning based natu-\nral language processing of radiology reports in orthopaedic trauma. Comput Methods Programs Biomed. \n2021;208:106304.\n 15. Chaudhari GR, Liu T, Chen TL, Joseph GB, Vella M, Lee YJ, et al. Application of a domain-specific BERT for detection of \nspeech recognition errors in radiology reports. Radiol Artif Intell. 2022;4(4): e210185.\n 16. Nakamura Y, Hanaoka S, Nomura Y, Nakao T, Miki S, Watadani T, et al. Automatic detection of actionable radiology \nreports using bidirectional encoder representations from transformers. BMC Med Inform Decis Mak. 2021;21(1):262.\n 17. Olthof AW, van Ooijen PMA, Cornelissen LJ. Deep learning-based natural language processing in radiology: the \nimpact of report complexity, disease prevalence, dataset size, and algorithm type on model performance. J Med \nSyst. 2021;45(10):91.\n 18. Wei J, Bosma M, Zhao VY, Guu K, Yu AW, Lester B et al. Finetuned language models are zero-shot learners. arXiv; 2022 \n[cited 2023 May 26]. http:// arxiv. org/ abs/ 2109. 01652\n 19. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learning with a uni-\nfied text-to-text transformer. arXiv; 2020 [cited 2023 May 22]. http:// arxiv. org/ abs/ 1910. 10683\n 20. Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling instruction-finetuned language models. arXiv; \n2022 [cited 2023 May 22]. http:// arxiv. org/ abs/ 2210. 11416\n 21. Gutiérrez BJ, McNeal N, Washington C, Chen Y, Li L, Sun H, et al. Thinking about GPT-3 in-context learning for bio-\nmedical IE? Think again. arXiv; 2022 [cited 2023 May 26]. http:// arxiv. org/ abs/ 2203. 08410\n 22. Kim Y. Convolutional neural networks for sentence classification. arXiv; 2014 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ \n1408. 5882\n 23. Cho K, van Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, et al. Learning phrase representations \nusing RNN encoder–decoder for statistical machine translation. arXiv; 2014 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ \n1406. 1078\n 24. Huang XS, Perez F, Ba J, Volkovs M. Improving transformer optimization through better initialization. In: Proceedings \nof the 37th international conference on machine learning. PMLR; 2020 [cited 2022 Sep 6]. p. 4475–83. https:// proce \nedings. mlr. press/ v119/ huang 20f. html\n 25. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model \nfor biomedical text mining. Bioinformatics. 2019;btz682.\n 26. Lehman E, Jain S, Pichotta K, Goldberg Y, Wallace BC. Does BERT pretrained on clinical notes reveal sensitive data? \narXiv; 2021 Apr [cited 2022 Jun 2]. Report No. http:// arxiv. org/ abs/ 2104. 07762\n 27. Sholl LM, Do K, Shivdasani P , Cerami E, Dubuc AM, Kuo FC, et al. Institutional implementation of clinical tumor profil-\ning on an unselected cancer population. JCI Insight. 2016;1(19): e87062.\n 28. Salton G, Buckley C. Term-weighting approaches in automatic text retrieval. Inf Process Manag. 1988;24(5):513–23.\nPage 15 of 15\nElmarakeby et al. BMC Bioinformatics          (2023) 24:328 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 29. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. HuggingFace’s transformers: state-of-the-art natural \nlanguage processing. arXiv; 2020 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1910. 03771\n 30. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: an imperative style, high-performance deep \nlearning library. arXiv; 2019 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1912. 01703\n 31. Abadi M, Agarwal A, Barham P , Brevdo E, Chen Z, Citro C, et al. TensorFlow: large-scale machine learning on hetero-\ngeneous distributed systems. arXiv; 2016 [cited 2022 Sep 6]. http:// arxiv. org/ abs/ 1603. 04467\n 32. Zhang S, Roller S, Goyal N, Artetxe M, Chen M, Chen S, et al. OPT: open pre-trained transformer language models. \narXiv; 2022 [cited 2023 May 30]. http:// arxiv. org/ abs/ 2205. 01068\n 33. Sanh V, Webson A, Raffel C, Bach SH, Sutawika L, Alyafeai Z, et al. Multitask prompted training enables zero-shot task \ngeneralization. arXiv; 2022 [cited 2023 May 30]. http:// arxiv. org/ abs/ 2110. 08207\n 34. Lu Q, Dou D, Nguyen T. ClinicalT5: a generative language model for clinical text. In: Findings of the association for \ncomputational linguistics: EMNLP 2022. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics; \n2022 [cited 2023 May 30]. p. 5436–43. https:// aclan tholo gy. org/ 2022. findi ngs- emnlp. 398\n 35. Lehman E, Hernandez E, Mahajan D, Wulff J, Smith MJ, Ziegler Z, et al. Do we still need clinical language models? \narXiv; 2023 [cited 2023 May 30]. http:// arxiv. org/ abs/ 2302. 08091\n 36. Phan LN, Anibal JT, Tran H, Chanana S, Bahadroglu E, Peltekian A, et al. SciFive: a text-to-text transformer model for \nbiomedical literature. arXiv; 2021 [cited 2023 May 30]. http:// arxiv. org/ abs/ 2106. 03598\n 37. Loshchilov I, Hutter F. Decoupled weight decay regularization. 2017 Nov 14 [cited 2022 Sep 6]; https:// arxiv. org/ abs/ \n1711. 05101 v3\n 38. Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in \nbinary classification evaluation. BMC Genomics. 2020;21(1):6.\n 39. Johnson AEW, Pollard TJ, Shen L, Lehman L, Wei H, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical \ncare database. Sci Data. 2016;3(1):160035.\n 40. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: a robustly optimized BERT pretraining approach. arXiv; \n2019 [cited 2023 Jun 5]. http:// arxiv. org/ abs/ 1907. 11692\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5843318700790405
    },
    {
      "name": "DNA microarray",
      "score": 0.5731255412101746
    },
    {
      "name": "Natural language processing",
      "score": 0.5005338191986084
    },
    {
      "name": "Cancer",
      "score": 0.4357200264930725
    },
    {
      "name": "Computational biology",
      "score": 0.3830033540725708
    },
    {
      "name": "Data science",
      "score": 0.3811030685901642
    },
    {
      "name": "Information retrieval",
      "score": 0.3492524325847626
    },
    {
      "name": "Medicine",
      "score": 0.28430598974227905
    },
    {
      "name": "Biology",
      "score": 0.21308454871177673
    },
    {
      "name": "Internal medicine",
      "score": 0.13494879007339478
    },
    {
      "name": "Genetics",
      "score": 0.12090402841567993
    },
    {
      "name": "Gene",
      "score": 0.09533801674842834
    },
    {
      "name": "Gene expression",
      "score": 0.09495514631271362
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210117453",
      "name": "Dana-Farber Cancer Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1334819555",
      "name": "Memorial Sloan Kettering Cancer Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}