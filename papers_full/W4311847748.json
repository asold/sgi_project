{
  "title": "A comparative study of gastric histopathology sub-size image classification: From linear regression to visual transformer",
  "url": "https://openalex.org/W4311847748",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5114549594",
      "name": "Weiming Hu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5101882098",
      "name": "Haoyuan Chen",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100614460",
      "name": "Wanli Liu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A5100459390",
      "name": "Xiaoyan Li",
      "affiliations": [
        "China Medical University",
        "Liaoning Cancer Hospital & Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5024292688",
      "name": "Hongzan Sun",
      "affiliations": [
        "China Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5044478041",
      "name": "Xinyu Huang",
      "affiliations": [
        "University of Lübeck"
      ]
    },
    {
      "id": "https://openalex.org/A5037990310",
      "name": "Marcin Grzegorzek",
      "affiliations": [
        "University of Lübeck",
        "University of Economics in Katowice"
      ]
    },
    {
      "id": "https://openalex.org/A5100369916",
      "name": "Chen Li",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3128646645",
    "https://openalex.org/W2935936692",
    "https://openalex.org/W3015636293",
    "https://openalex.org/W4223918756",
    "https://openalex.org/W3210999250",
    "https://openalex.org/W4214808100",
    "https://openalex.org/W3034124771",
    "https://openalex.org/W3163655960",
    "https://openalex.org/W3171229070",
    "https://openalex.org/W2941584439",
    "https://openalex.org/W3131151352",
    "https://openalex.org/W4220788253",
    "https://openalex.org/W4281657029",
    "https://openalex.org/W3046220160",
    "https://openalex.org/W4281962665",
    "https://openalex.org/W3009312108",
    "https://openalex.org/W4280595708",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W4231021828",
    "https://openalex.org/W2907648650",
    "https://openalex.org/W4296188765",
    "https://openalex.org/W4243399945",
    "https://openalex.org/W2136132422",
    "https://openalex.org/W2996897273",
    "https://openalex.org/W2021833436",
    "https://openalex.org/W2768149277",
    "https://openalex.org/W2056052206",
    "https://openalex.org/W3132941258",
    "https://openalex.org/W4303578732",
    "https://openalex.org/W4281853941",
    "https://openalex.org/W4296312404",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3213783748",
    "https://openalex.org/W4281669672",
    "https://openalex.org/W3013098034",
    "https://openalex.org/W4206621942",
    "https://openalex.org/W4225616443",
    "https://openalex.org/W4281720796",
    "https://openalex.org/W4226245869",
    "https://openalex.org/W3132799678",
    "https://openalex.org/W3048886990",
    "https://openalex.org/W3102640734",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4311573926",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Introduction Gastric cancer is the fifth most common cancer in the world. At the same time, it is also the fourth most deadly cancer. Early detection of cancer exists as a guide for the treatment of gastric cancer. Nowadays, computer technology has advanced rapidly to assist physicians in the diagnosis of pathological pictures of gastric cancer. Ensemble learning is a way to improve the accuracy of algorithms, and finding multiple learning models with complementarity types is the basis of ensemble learning. Therefore, this paper compares the performance of multiple algorithms in anticipation of applying ensemble learning to a practical gastric cancer classification problem. Methods The complementarity of sub-size pathology image classifiers when machine performance is insufficient is explored in this experimental platform. We choose seven classical machine learning classifiers and four deep learning classifiers for classification experiments on the GasHisSDB database. Among them, classical machine learning algorithms extract five different image virtual features to match multiple classifier algorithms. For deep learning, we choose three convolutional neural network classifiers. In addition, we also choose a novel Transformer-based classifier. Results The experimental platform, in which a large number of classical machine learning and deep learning methods are performed, demonstrates that there are differences in the performance of different classifiers on GasHisSDB. Classical machine learning models exist for classifiers that classify Abnormal categories very well, while classifiers that excel in classifying Normal categories also exist. Deep learning models also exist with multiple models that can be complementarity. Discussion Suitable classifiers are selected for ensemble learning, when machine performance is insufficient. This experimental platform demonstrates that multiple classifiers are indeed complementarity and can improve the efficiency of ensemble learning. This can better assist doctors in diagnosis, improve the detection of gastric cancer, and increase the cure rate.",
  "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/seven.tnum December /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nJun Cheng,\nShenzhen University, China\nREVIEWED BY\nGang Yin,\nSichuan Cancer Hospital, China\nHaoda Lu,\nNanyang Technological\nUniversity, Singapore\n*CORRESPONDENCE\nChen Li\nlichen@bmie.neu.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nPathology,\na section of the journal\nFrontiers in Medicine\nRECEIVED /one.tnum/seven.tnum October /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /one.tnum/eight.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /zero.tnum/seven.tnum December /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nHu W, Chen H, Liu W, Li X, Sun H,\nHuang X, Grzegorzek M and Li C\n(/two.tnum/zero.tnum/two.tnum/two.tnum) A comparative study of gastric\nhistopathology sub-size image\nclassiﬁcation: From linear regression to\nvisual transformer.\nFront. Med./nine.tnum:/one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Hu, Chen, Liu, Li, Sun, Huang,\nGrzegorzek and Li. This is an\nopen-access article distributed under\nthe terms of the\nCreative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which\ndoes not comply with these terms.\nA comparative study of gastric\nhistopathology sub-size image\nclassiﬁcation: From linear\nregression to visual transformer\nWeiming Hu/one.tnum, Haoyuan Chen /one.tnum, Wanli Liu /one.tnum, Xiaoyan Li /two.tnum,\nHongzan Sun /three.tnum, Xinyu Huang /four.tnum, Marcin Grzegorzek /four.tnum,/five.tnumand\nChen Li /one.tnum*\n/one.tnumMicroscopic Image and Medical Image Analysis Group, College of Medi cine and Biological\nInformation Engineering, Northeastern University, Shenyang, Chi na, /two.tnumDepartment of Pathology,\nLiaoning Cancer Hospital and Institute, Cancer Hospital, Chin a Medical University, Shenyang, China,\n/three.tnumDepartment of Radiology, Shengjing Hospital, China Medical Unive rsity, Shenyang, China, /four.tnumInstitute\nof Medical Informatics, University of Luebeck, Luebeck, German y, /five.tnumDepartment of Knowledge\nEngineering, University of Economics in Katowice, Katowice, Poland\nIntroduction: Gastric cancer is the ﬁfth most common cancer in the world.\nAt the same time, it is also the fourth most deadly cancer. Early det ection\nof cancer exists as a guide for the treatment of gastric cancer. Nowa days,\ncomputer technology has advanced rapidly to assist physicians in t he diagnosis\nof pathological pictures of gastric cancer. Ensemble learning is a way to\nimprove the accuracy of algorithms, and ﬁnding multiple learnin g models with\ncomplementarity types is the basis of ensemble learning. Ther efore, this paper\ncompares the performance of multiple algorithms in anticipatio n of applying\nensemble learning to a practical gastric cancer classiﬁcation prob lem.\nMethods: The complementarity of sub-size pathology image classiﬁers when\nmachine performance is insuﬃcient is explored in this experime ntal platform.\nWe choose seven classical machine learning classiﬁers and four deep learning\nclassiﬁers for classiﬁcation experiments on the GasHisSDB datab ase. Among\nthem, classical machine learning algorithms extract ﬁve diﬀeren t image virtual\nfeatures to match multiple classiﬁer algorithms. For deep lear ning, we choose\nthree convolutional neural network classiﬁers. In addition, we als o choose a\nnovel Transformer-based classiﬁer.\nResults: The experimental platform, in which a large number of classical\nmachine learning and deep learning methods are performed, dem onstrates\nthat there are diﬀerences in the performance of diﬀerent classiﬁ ers on\nGasHisSDB. Classical machine learning models exist for classiﬁe rs that classify\nAbnormal categories very well, while classiﬁers that excel in class ifying Normal\ncategories also exist. Deep learning models also exist with mu ltiple models that\ncan be complementarity.\nDiscussion: Suitable classiﬁers are selected for ensemble learning, when\nmachine performance is insuﬃcient. This experimental platform demonstrates\nthat multiple classiﬁers are indeed complementarity and can impro ve the\nFrontiers in Medicine /zero.tnum/one.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\neﬃciency of ensemble learning. This can better assist doctors in d iagnosis,\nimprove the detection of gastric cancer, and increase the cure rate .\nKEYWORDS\ngastric histopathology, sub-size image, robustness compariso n, algorithmic\ncomplementarity, image classiﬁcation\n/one.tnum. Introduction\nGastric cancer is a serious threat to human health as a\nglobal killer disease. According to the most recent Global Cancer\nStatistics Report, gastric cancer has become the ﬁfth most\ncommon cancer and the fourth leading cause of death (\n1).\nHistopathological examination of gastric cancer constitutes the\ngold standard for the detection of gastric cancer and is a\nprerequisite for its management (\n2).\nHistopathological examinations begin by staining the\nsections with Hematoxylin and Eosin (H&E), which are used\nto visualize the nuclei and cytoplasm of tissue sections,\nhighlighting the ﬁne structure of cells and tissues for physician\nobservation (\n3). The pathologist ﬁnds the diseased area by gross\nobservation of the pathological slides with the naked eye. The\npathologist then observes and diagnoses the diseased area of\nthe pathological section using the low-power microscope of\nthe microscope. Pathologists can use high-power microscopes\nfor careful observation and judgment (\n4). For the entire\npathological slice diagnosis process ( 5), the following problems\ncan be found: slice information is easy to ignore ( 6). This shows\nthat there is subjectivity throughout the process. The workload\nof pathologists is huge and the working hours are long, which is\nhighly likely to lead to misdiagnosis (\n7). Therefore, there is an\nurgent need to address the issues more intensively.\nHowever, computer-aided diagnosis technology has\nadvanced rapidly in recent years, and the emergence of medical\nimage classiﬁcation technology in computer vision technology\ncan achieve fast and eﬃcient help for doctors to examine gastric\ncancer tissue sections (\n8). Image classiﬁcation techniques have\nbrought new breakthroughs to discriminate between benign\nand malignant cancer, distinguish between stages of tumor\ndiﬀerentiation and diﬀerentiate tumor subtypes, as image\nclassiﬁcation techniques can provide valid information for\npathologists to refer to during the diagnostic process (\n9). In\naddition, the development direction of image classiﬁcation\ntechnology is mainly to enhance the accuracy of classiﬁcation\nalgorithms and improve the anti-interference ability, ensemble\nlearning becomes an eﬀective solution, and it becomes especially\nimportant to ﬁnd multiple eﬃcient classiﬁcation algorithms\nwith complementarity properties (\n10). Moreover, there is a lack\nof computer performance in practical work, and computer-\naided medical image analysis often crops full-slice images\ninto sub-size pictures (\n11). Therefore, we compare the image\nclassiﬁcation performance of a large number of algorithms\non sub-size images in order to expect to ﬁnd algorithms with\ncomplementarity properties for ensemble learning to improve\nmedical image classiﬁcation performance.\nThe database used in this study is GasHisSDB (\n12),\ncontaining 245,196 images, of which there are 97,076 abnormal\nimages and 148,120 normal images. GasHisSDB is a database\ncontaining three sub-databases, including sub-database A (160\n× 160 pixels), Sub-database B (120 × 120 pixels.), Sub-database\nC (80 × 80 pixels). GasHisSDB provides the ability to distinguish\nbetween classical machine learning classiﬁer performance and\ndeep learning classiﬁer performance (\n13). Details are given in\nSection 2.1.\nClassical machine learning methods still have excellent\nclassiﬁcation results in the ﬁeld of image classiﬁcation ( 14).\nExisting methods can extract diﬀerent features of images\nand supply diﬀerent performance of classiﬁers for image\nclassiﬁcation (\n15). Exploring diﬀerent features using appropriate\nclassiﬁers to obtain eﬃcient classiﬁcation results is the basis of\nusing ensemble learning for medical images (\n16). Therefore,\nin this study, ﬁve diﬀerent image features including two color\nfeatures and three texture features are extracted for GasHisSDB.\nAfter extracting the features seven diﬀerent classiﬁers are used\nfor classiﬁcation. Details are given in Sections 2.2 and 2.3.\nIn the ﬁeld of medical image classiﬁcation, deep learning\nalgorithms are the most eﬀective algorithms, and Convolutional\nNeural Network (CNN) is a widely used model for image\nclassiﬁcation, which can extract information from original\nmedical images and classify normal and abnormal case\nimages (\n17). Recently, Visual Transformer, which is originally\napplied to Natural Language Processing tasks, have become\npopular in computer vision, and Vision Transformer (ViT) have\neﬀective classiﬁcation results when trained on large amounts\nof data and can signiﬁcantly reduce the computer hardware\nand software resources required for training (\n18). CNN-based\ndeep learning models, this study used VGG6, Inception-V3\nand ResNet50. Visual transformer-based deep learning models\nin this study used VIT. The above four deep learning models\nuse the same parameters with the same database: GasHisSDB.\nDetails are given in Section 2.4.\nThis study makes the following contributions to the ﬁeld of\nsub-size pathology image classiﬁcation:\nFrontiers in Medicine /zero.tnum/two.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /one.tnum Dataset scale of GasHisSDB.\nSub-database name Cropping size Abnormal Normal\nSub-database A 160 × 160 pixels 13,124 20,160\nSub-database B 120 × 120 pixels 24,801 40,460\nSub-database C 80 × 80 pixels 59,151 87,500\nTotal 97,076 148,120\n• Extensive testing is done and the complementarity of\ndiﬀerent classiﬁcation methods is found.\n• According to the complementarity, it can provide a basis\nfor future ensemble learning research.\nThis paper is structured as follows: In Section 2, we detail the\ndataset used, classical classiﬁcation methods, and deep learning\nmethods. In Section 3, we show the comparative experimental\nsetup, evaluation metrics and experimental results. In Section\n4, we compare the experimental results and analyze them.\nIn Section 5, we summarize the research and suggest future\nresearch directions.\n/two.tnum. Materials and methods\n/two.tnum./one.tnum. Dataset: GasHisSDB\nThe publicly available dataset GasHisSDB is used in this\nstudy to compare the performance of various learning models,\nexpecting to discover the complementarity of various models\nin ensemble learning (\n12). The database contains three sub-\ndatasets with a total of 245,196 images, and the size and number\nare shown in\nTable 1. The database is a sub-size gastric cancer\npathology H&E staining image database, which contains two\ncategories of images: normal and abnormal. The abnormal\nimage contains more than 50% of the cancerous area, and the\nnormal image is the image of the normal pathological slice\ntissue. Some examples of the GasHisSDB database are shown in\nFigure 1.\nGasHisSDB contains images in png format acquired using\nelectron microscopy. GasHisSDB contains two categories and\nthe details of the two categories are shown below:\n• Normal: each normal image does not contain cancerous\nregions. Each cell is almost free of anisotropy. In addition,\nthe nuclei of the cells in the images have almost no\nmitosis and are arranged in a regular layer. Therefore, when\nobserved under the light microscope, if no elimination of\nany cells and tissues is observed and the characteristics\nof a normal image are met, it can be judged as a normal\nimage (\n19).\n• Abnormal: Each abnormal image contains more than 50%\nof gastric cancer images. The general morphology of gastric\ncancer is mostly ulcerative. As the disease progresses,\nthe cancer nest inﬁltrates from the mucosal layer to the\nmuscular layer and plasma layer. The texture is hard and\nthe cross-section is often grayish white. Under microscopic\nobservation, the cancer cells can be arranged in nest-\nlike, glandular vesicle-like, tubular or cord-like, and the\nboundary with the interstitium is usually clear. However,\nwhen cancer cells inﬁltrate the stroma, the borders between\nthem are not clear. Based on these facts, abnormal\npathological images can be judged when cells are observed\nto form unevenly sized, irregularly shaped, and irregularly\narranged glandular or adenoid structures (\n19).\n/two.tnum./two.tnum. Methods of feature extraction\nTo extract a variety of virtual features of GasHisSDB is a\nprerequisite for classiﬁcation using classical machine learning\nclassiﬁers. In the comparison experiments, ﬁve methods are\nused to extract visual features from the database, including\nColor histogram, Luminance histogram, Histogram of Oriented\nGradient (HOG), Local Binary Patterns (LBP), and Gray-level\nCo-occurrence Matrix (GLCM).\n/two.tnum./two.tnum./one.tnum. Color histogram\nAmong the diﬀerent methods of feature extraction, the most\ncommon method to describe the color features of an image\nis the color histogram. The color histogram clearly represents\nthe color spread in the image. The color histogram has the\ncharacteristic of being unaﬀected by image rotation and shift\nchanges and by further normalization of image scale changes.\nIt is especially applicable to describe images that are resistant\nto automatic segmentation and images that do not require\nconsideration of the spatial location of subjects. However, the\ncolor histogram does not characterize the partial spread of\ncolors in an image, the spatial location of each color, and\nspeciﬁc objects. In this experiment, the luminance histogram\nis used as the luminance feature. The luminance feature is\nexpressed as a histogram of the average of the three color\ncomponents.\n/two.tnum./two.tnum./two.tnum. Texture features\nThe texture is a visual feature that reﬂects homogeneous\nphenomena in an image (\n20). That reﬂects the structure and\narrangements of the surface structures on the surface of an object\nwith slow or periodic changes (\n21). A texture feature is not a\npixel-based feature. It requires statistical computation of regions\ncontaining multiple pixels, such as the grayscale distribution\nof pixels and their surrounding spatial neighbors, and local\ntexture information. In addition, the global texture information\nis reﬂected as the repetition degree of local texture information.\nFrontiers in Medicine /zero.tnum/three.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /one.tnum\nExample of GasHisSDB.In this experiment, three texture features are extracted, which\nare HOG, LBP , and GLCM.\nHOG is a feature descriptor commonly used in image\nprocessing for object detection. Features are constructed by\ncomputing a histogram of the gradient direction of local regions\nof an image. HOG has the property of operating on the local\nunits of the image. So it has the advantage of maintaining\nexcellent invariance in terms of geometric and optical distortion\nof the image. LBP has advantages such as gray invariance\nand rotation invariance, and the features are easy to compute.\nGLCM is deﬁned by the joint probability density of pixels at\ntwo locations and is a second-order statistical feature about\nthe variation of image brightness. It not only reﬂects the\ndistribution of luminance. It also reﬂects the distribution of\npositions between pixels with the same or similar luminance.\nThe main statistical values are: Contrast, Correlation, Energy,\nand Homogeneity.\n/two.tnum./three.tnum. Classical classiﬁcation models\nAfter the feature extraction step, complementarity\ncomparison tests for image classiﬁcation are performed\nusing seven classical machine learning methods, including\nLinear Regression, k-Nearest Neighbor ( kNN), naive\nBayesian classiﬁer, Random Forest (RF), linear Support\nVector Machine (linear SVM), non-linear Support Vector\nMachine (non-linear SVM), and Artiﬁcial Neural Network\n(ANN).\nClassical machine learning methods perform image\nclassiﬁcation by using virtual features. Linear Regression is a\nmethod to get a linear model as much as possible to accurately\npredict the real value output label. In Linear regression, the\nleast square function is used to establish the relationship\nbetween one or more independent variables (\n22). An easy\nand commonly used supervised learning method is kNN. The\nFrontiers in Medicine /zero.tnum/four.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nmain idea of kNN is to ﬁrst ﬁnd the nearest k samples based\non the distance and then vote for the prediction result ( 23).\nThe naive Bayesian classiﬁer based on Bayesian decision\ntheory in probability theory (\n24). RF is a parallel integrated\nlearning method based on a decision tree learner. RF adds\nrandom attribute selection to the training process of decision\ntrees (\n25). SVMs are divided into linear and non-linear. The\ndiﬀerence between the two is mainly that the kernel functions\nof both are diﬀerent (\n26). Linear SVM maps training examples\nto points in space to maximize the gap between the two\ncategories. Then, the new examples are mapped to the same\nspace and predicted to belong to a category based on which\nside of the gap they fall on. In addition to performing linear\nclassiﬁcation, SVM can also use a kernel function to perform\nnon-linear classiﬁcation eﬀectively, thereby implicitly mapping\nits input to a high-dimensional feature space. The ANN is a\nclassiﬁcation algorithm composed of a structure that simulates\nhuman brain neurons and is trained through a propagation\nalgorithm (\n27).\n/two.tnum./four.tnum. Deep learning models\nComplementarity comparison experiments use deep\nlearning models for the classiﬁcation of gastric cancer\npathology images (\n28). First, the model is trained using\ntraining and validation sets generated from three sub-datasets\nof GasHisSDB. The test set is used in this experiment to\nevaluate the models’ performance (\n29). Comparative analysis\nof multiple classiﬁcation results is performed using the\nobtained evaluation metrics to determine if the classiﬁers\nwould be complementarity in Ensemble learning (\n30). This\nexperiment uses four deep learning models. Three of the\nmodels are based on CNNs, including VGG16, Inception-V3,\nand ResNet50. One more model corresponds to VT, which is\nViT (\n31).\nVGG is a convolutional neural network (CNN) improved\nby AlexNet, developed by Visual Geometry Group and Google\nDeepMind in 2014, and the most commonly used one in image\nclassiﬁcation is VGG16 (\n32). In 2014, Google’s InceptionNet\nmade its debut at the ILSVRC competition. Several versions of\nInceptionNet have been developed, with Inception-V3 being one\nof the more representative versions of this large family (\n33). He\net al. proposed ResNet to address the diﬃculty of training deep\nnetworks due to gradient disappearance. The most commonly\nused in the ﬁeld of image classiﬁcation is ResNet50 (\n34). In\nrecent years, Dosovitskiy et al. have proposed the ViT model\nusing transformer. This model is not only very eﬀective in the\nﬁeld of natural language processing, but also provides good\nresults in the ﬁeld of image classiﬁcation. Eﬀectively reduces the\ndependence of computer vision on CNN (\n35).\n/three.tnum. Experiment\n/three.tnum./one.tnum. Comparative experimental setup\nThe main process of complementarity experiments is\ndivided into two parallel parts: The classiﬁcation results of\nclassical models and deep learning models are both analyzed and\nevaluated. The experimental ﬂow is shown in\nFigure 2.\nThe various settings of the experimental platform are as\nfollows:\n1. Hardware conﬁguration: The complementarity comparison\nexperiment is conducted on a local computer with the Win10\noperating system. The computer has 32 GB of running\nmemory and is equipped with an 8 GB NVIDIA Quadro RTX\n4000.\n2. Data set partitioning: In this experiment, the training set,\nvalidation set and test set are divided in the ratio of 4:4:2.\n3. Classical machine learning software conﬁguration: The\nclassical programming software use for machine learning is\nMatlab R2020a (9.8.0.132 350 2).\n4. Deep learning software conﬁguration: The Pytorch version\n1.7.1 framework in Deep Learning Python 3.6 is very mature,\nand the code for this part of the experiment is done using\nthem.\n5. Classical machine learning parameter settings: The same\nparameters are used for all classiﬁcation comparison\nexperiments. In kNN, k is set to 9. The number of trees in\nRF is set to 10. The kernel function of the non-linear SVM is\na Gaussian kernel. The ANN uses a 2-layer network with 10\nnodes in the ﬁrst layer and 3 nodes in the second layer. The\nnumber of epochs for ANN training is set to 500, the learning\nrate is set to 0.01, and the expected loss is set to 0.01.\n6. Deep learning parameter settings: This part of the experiment\nfocuses on classifying GasHisSDB using four deep learning\nmethods to observe model complementarity. A learning rate\nof 0.00002 is used for each model, and the batch size is set\nto 32. One hundred epochs of experiments are performed to\nobserve the classiﬁcation results of this database on diﬀerent\nmodels.\n/three.tnum./two.tnum. Evaluation metrics\nThe selection of evaluation indicators is important in\ncomplementarity comparison papers. In the experiments of this\nthesis, Accuracy (Acc) is the most signiﬁcant metric, but also\nPrecision (Pre), Recall (Rec), Speciﬁcity (Spe), and F1-score (F1)\nare selected. These selected metrics are very commonly used in\ncomparison papers to analyze classiﬁers and thus better identify\ntheir complementarities to enhance and improve ensemble\nlearning (\n36).\nFrontiers in Medicine /zero.tnum/five.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /two.tnum\nWorkﬂow of the complementarity comparison experiment.\nIn the case of positive-negative binary classiﬁcation,\ntrue positives (TP) correspond to the number of positive\nsamples that are accurately predicted. The number of\nnegative samples predicted to be positive is called false\npositive (FP). The number of positive samples predicted\nto be negative samples is called false negative (FN). True\nNegative (TN) is the number of negative samples predicted\naccurately (\n37).\nThe ﬁve evaluation indicators are described below and the\nformulas are shown in Table 2.\n1. Acc: Accuracy is the ratio of the number of correct\npredictions to the total number of samples.\n2. Pre: Precision is a measure of accuracy, indicating the\nproportion of examples classiﬁed as positive that are actually\npositive.\n3. Recl: Recall is a measure of coverage, a measure of the\nnumber of positive examples classiﬁed as positive examples,\nindicating the proportion of all positive examples classiﬁed as\npairs, which measures the ability of the classiﬁer to identify\npositive examples.\n4. Spe: Speciﬁcity indicates the proportion of all negative cases\nthat were scored correctly, and measures the classiﬁer’s ability\nto identify negative cases.\nTABLE /two.tnum Evaluation metrics.\nAssessment Formula\nAccuracy (Acc) (TP + TN)/(TP + TN + FP + FN)\nPrecision (Pre) TP/TP + FP\nRecall (Rec) TP/TP + FN\nSpeciﬁcity (Spe) TN/TN + FP\nF1-score (F1) 2 × (Pre × Rec)/(Pre + Rec)\n5. F1: F1-Score combines Precision and Recall. Accuracy is the\nratio of the number of correct predictions to the total number\nof samples.\n/three.tnum./three.tnum. Experimental results\nWe set up an experimental platform to conduct various\nclassiﬁcation experiments on three sub-databases of the\nGasHisSDB. A large amount of experimental data is obtained for\nour experiments in order to investigate the complementarity of\ndiﬀerent methods (\n38).\nFrontiers in Medicine /zero.tnum/six.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nThe comparative results of classical machine learning\nmethods are shown in Tables 3–5.\nTable 6 show the comparison results of the deep learning\nmethods.\n/four.tnum. Evaluation of results\n/four.tnum./one.tnum. Evaluation of classical machine\nlearning methods\n/four.tnum./one.tnum./one.tnum. On /one.tnum/six.tnum/zero.tnum× /one.tnum/six.tnum/zero.tnum pixels sub-database\nThis section focuses on the classiﬁcation results of\nclassical machine learning methods for the 160 × 160 sub-\ndatabase.\nThe color histogram has the highest number of items among\nall features. According to\nTable 3, the classical machine learning\nclassiﬁer on the color histogram, the best performer is RF with\nan accuracy of 85.99%. In addition, in color histogram, the\nclassiﬁcation accuracy of the three classiﬁers reached around\n80%, which are LR, kNN, and ANN. All SVM classiﬁers perform\npoorly on color histogram features. However, color histogram\non GasHisSDB, the naive Bayesian classiﬁer, cannot get the\nclassiﬁcation eﬀect because of the existence of a large number\nof low luminance statistics with zero values in the three color\nchannels.\nThe luminance is the average of the colors. Its histogram\ndoes not yield better classiﬁcation accuracy as a feature.\nBecause of this, luminance histogram also has the above\nproblem on the naive Bayesian classiﬁer. The classiﬁcation\nresults of the naive Bayesian classiﬁer for these two color\nfeatures are therefore not presented in the\nTable 3. RF shows\nrobustness in two features and obtains the highest accuracy\nrate of 79.13% using luminance histogram for classiﬁcation.\nHowever, the LR, kNN, and ANN classiﬁers that perform\nbetter on color histogram signiﬁcantly drop on luminance\nhistogram.\nThe classiﬁcation eﬀect of HOG on all classiﬁers is not very\neﬀective and the accuracy is very close. The diﬀerence is not\nmuch distributed between 53 and 62%.\nOn the contrary, the distribution of LBP image classiﬁcation\naccuracy is particularly scattered, with the highest Linear\nRegression classiﬁer reaching 74.29%, followed by ANN\nreaching 71.84%. The lowest linear SVM classiﬁcation eﬀect is\n<50%.\nThe classiﬁcation eﬀect of the four statistic values of GLCM\nis 71.39% only for RF, and other classiﬁers are also above 60%. It\nis worth noting that the accuracy of non-linear SVM with other\nfeatures except color histogram and GLCM has not changed at\nall, which is 60.58%. The accuracy of non-linear SVM classiﬁer\nwith color histogram is 56.09% and the accuracy of GLCM’s\nnon-linear SVM classiﬁer is 67.76%.\n/four.tnum./one.tnum./two.tnum. On /one.tnum/two.tnum/zero.tnum× /one.tnum/two.tnum/zero.tnum pixels sub-database\nHere, we focus on the comparison of the experimental\nresults of the 120 × 120 pixels sub-database. The experimental\nresults are shown in\nTable 3. In general, compared with 160 × 160\npixels sub-database classiﬁcation results, 120 × 120 pixels sub-\ndatabase classiﬁcation results except for color histogram, the rest\nof the best classiﬁers remain unchanged.\nThe four better-performing classiﬁers on color histogram\nfeature still perform better, and the accuracy rate ﬂuctuates\nslightly, resulting in the kNN classiﬁer reaching the best accuracy\nrate of 86.32%. The classiﬁcation performance of the two SVM\nclassiﬁers on the features of color histogram is still not ideal.\nNaive Bayesian classiﬁer is still not suitable for color histogram\nand luminance histogram features. The linear SVM eﬀect of\nluminance histogram classiﬁer has been greatly improved in the\nclassiﬁcation of the 120 × 120 pixels sub-database. The accuracy\nof other classiﬁers on the features of luminance histogram has\nlittle change. The HOG feature still does not perform well in\nevery classiﬁer. The highest accuracy rate is only 62.35% of ANN.\nThe classiﬁcation results of LBP and GLCM features are similar\nto the classiﬁcation eﬀect on the 160 × 160 pixels sub-database.\nThe best accuracy rate on LBP is a linear regression with a\nprecision rate of 73.34%. The best accuracy rate on GLCM is\nthat the RF reaches 71.15%. Similarly, the non-linear SVM of\n120 × 120 pixels sub-database also has the problem of constant\naccuracy of multiple features.\n/four.tnum./one.tnum./three.tnum. On /eight.tnum/zero.tnum× /eight.tnum/zero.tnum pixels sub-database\nThe classiﬁcation results of the 80 × 80 pixels sub-database\nare shown in\nTable 4. The overall best classiﬁer on each feature\nremains the same as that of the best classiﬁer for each feature\ncorresponding to the 120 × 120 pixels sub-database except for\nHOG features that have a small gap between each classiﬁer.\nCompared with the classiﬁcation results of the other two\nsub-databases, the classiﬁcation eﬀect of each classiﬁer on color\nhistogram and luminance histogram has no particularly large\nﬂuctuations. It conﬁrms the consistency of the three databases\nof GasHisSDB.\nThe classiﬁcation accuracy of color histogram is still\npolarized. The four excellent classiﬁers reach about 80%, and\nthe other two are about 60%. The RF still showed robustness\nin the luminance histogram classiﬁcation task. RF was the best\nclassiﬁer with an accuracy of 75.10%. The classiﬁcation accuracy\ndistribution of HOG features is denser than that of the other two\nsub-databases. The highest is only 59.87%. Due to the reduced\nsample size, each classiﬁer has diﬀerent degrees of accuracy\nreduction in addition to the naive Bayesian classiﬁer for LBP\nfeatures and GLCM features. The best classiﬁer for LBP feature\nis still linear regression which reaches 70.92%. The highest\naccuracy rate of LBP feature has become 68.84% of kNN. In\nthe classiﬁcation results of the 80 × 80 pixels sub-database,\nthe naive Bayesian classiﬁer of color histogram and luminance\nFrontiers in Medicine /zero.tnum/seven.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /three.tnum Classiﬁcation results of ﬁve image features using diﬀerent classiﬁers in the /one.tnum/six.tnum/zero.tnum× /one.tnum/six.tnum/zero.tnum pixels sub-database of GasHisSDB [In (%)].\nFreatures Methods Acc Abnormal Normal\nPre Rec Spe F1 Pre Rec Spe F1\nLR 83.29 81.32 80.42 85.54 80.87 84.80 85.54 80.42 85.17\nkNN 85.52 82.95 84.35 86.43 83.64 87.58 86.43 84.35 87.01\nRF 85.99 81.65 87.83 84.55 84.63 89.88 84.55 87.83 87.13\nColor histogram Linear SVM 41.12 33.92 35.96 45.16 34.91 47.4 0 45.16 35.96 46.25\nNon-linear SVM 56.09 Null 0.00 100.00 0.00 56.09 100.00 0.00 71 .87\nANN 78.89 77.78 72.68 83.75 75.14 79.67 83.75 72.68 81.66\nLR 70.97 67.95 49.92 84.67 57.56 72.21 84.67 49.92 77.95\nkNN 77.10 70.30 72.60 80.03 71.43 81.78 80.03 72.60 80.90\nRF 79.13 72.17 76.60 80.78 74.32 84.14 80.78 76.60 82.42\nLinear SVM 42.34 40.50 98.67 5.68 57.43 86.74 5.68 98.67 10.6 6\nLuminance histogram Non-linear SVM 60.58 Null 0.00 100.00 0.0 0 60.58 100.00 0.00 75.45\nANN 71.23 64.74 59.34 78.97 61.92 74.90 78.97 59.34 76.88\nHOG\nLR 60.46 48.96 7.20 95.11 12.56 61.16 95.11 7.20 74.45\nkNN 61.42 51.31 41.65 74.28 45.98 66.17 74.28 41.65 69.99\nNaive Bayesian 54.43 45.11 71.84 43.11 55.42 70.17 43.11 71. 84 53.40\nRF 60.85 50.33 53.01 65.95 51.63 68.32 65.95 53.01 67.11\nLinear SVM 53.28 44.82 80.14 35.79 57.49 73.47 35.79 80.14 48 .13\nNon-linear SVM 60.58 Null 0.00 100.00 0.00 60.58 100.00 0.00 75 .45\nANN 61.54 54.30 15.40 91.57 23.99 62.45 91.57 15.40 74.26\nLBP\nLR 74.29 69.32 62.42 82.02 65.69 77.03 82.02 62.42 79.45\nkNN 70.21 66.11 50.11 83.28 57.01 71.95 83.28 50.11 77.20\nNaive Bayesian 57.71 47.78 78.28 44.32 59.34 75.82 44.32 78. 28 55.94\nRF 70.27 62.16 62.84 75.10 62.50 75.64 75.10 62.84 75.37\nLinear SVM 48.17 36.83 44.02 50.87 40.10 58.27 50.87 44.02 54 .32\nNon-linear SVM 60.58 Null 0.00 100.00 0.00 60.58 100.00 0.00 75 .45\nANN 71.84 67.38 55.41 82.54 60.81 73.99 82.54 55.41 78.03\nGLCM\nLR 67.73 59.71 55.75 75.52 57.67 72.40 75.52 55.75 73.93\nkNN 69.26 62.30 55.79 78.03 58.87 73.06 78.03 55.79 75.46\nNaive Bayesian 61.99 51.12 82.01 48.96 62.98 80.70 48.96 82. 01 60.94\nRF 71.39 63.16 65.85 75.00 64.48 77.14 75.00 65.85 76.06\nLinear SVM 66.50 55.89 71.27 63.39 62.65 77.22 63.39 71.27 69 .63\nNon-linear SVM 67.76 58.77 61.05 72.12 59.89 73.99 72.12 61.0 5 73.05\nANN 68.69 60.64 58.65 75.22 59.63 73.65 75.22 58.65 74.43\nThe bold text in the table indicates the highest value of the classiﬁ cation result of diﬀerent classiﬁers for the same feature.\nhistogram is not applicable, and, except for the GLCM feature,\nthe problem that the accuracy of the non-linear SVM classiﬁer\ndoes not change still exists.\n/four.tnum./two.tnum. Evaluation of deep learning methods\n/four.tnum./two.tnum./one.tnum. On /one.tnum/six.tnum/zero.tnum× /one.tnum/six.tnum/zero.tnum pixels sub-database\nAccording to\nTable 5, on 160 × 160 pixels sub-database,\nall deep learning models have better classiﬁcation results than\nclassical machine learning methods. The VGG model with the\nlongest training time and the largest model size has an accuracy\nabove 95%. Inception-V3 and ResNet50 have better model size\nand training time than VGG16. However, Inception-V3 has\nlower accuracy than VGG16, and ResNet50 has the highest\naccuracy of 96.09%, which is the highest among all models. ViT\nis a Transformer-based classiﬁer with an accuracy of 86.21%.\nHowever, it is still higher than the classiﬁcation accuracy of\nall traditional machine learning methods on this sub-database.\nSigniﬁcantly, ViT achieves such accuracy with only 1/4 of the\nFrontiers in Medicine /zero.tnum/eight.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /four.tnum Classiﬁcation results of ﬁve image features using diﬀerent classiﬁers in the /one.tnum/two.tnum/zero.tnum× /one.tnum/two.tnum/zero.tnum pixels sub-database of GasHisSDB [In (%)].\nFreatures Methods Acc Abnormal Normal\nPre Rec Spe F1 Pre Rec Spe F1\nLR 83.46 80.01 75.28 88.47 77.57 85.38 88.47 75.28 86.90\nkNN 86.32 82.28 81.55 89.24 81.92 88.75 89.24 81.55 88.99\nRF 86.08 80.36 83.87 87.43 82.08 89.84 87.43 83.87 88.62\nColor histogram Linear SVM 46.28 39.48 77.62 27.06 52.34 66.3 6 27.06 77.62 38.45\nNon-linear SVM 62.00 Null 0.00 100.00 0.00 62.00 100.00 0.00 76 .54\nANN 81.20 78.14 70.14 87.98 73.93 82.78 87.98 70.14 85.30\nLR 71.28 66.89 48.39 85.32 56.15 72.95 85.32 48.39 78.65\nkNN 76.43 68.19 71.17 79.65 69.65 81.84 79.65 71.17 80.73\nRF 77.60 69.36 73.57 80.08 71.40 83.17 80.08 73.57 81.60\nLinear SVM 58.54 47.45 84.62 42.55 60.80 81.86 42.55 84.62 55 .99\nLuminance histogram Non-linear SVM 62.00 Null 0.00 100.00 0.0 0 62.00 100.00 0.00 76.54\nANN 71.18 62.97 58.65 78.86 60.73 75.68 78.86 58.65 77.23\nHOG\nLR 61.78 33.72 0.58 99.30 1.15 61.97 99.30 0.58 76.31\nkNN 62.02 50.04 39.56 75.79 44.18 67.17 75.79 39.56 71.22\nNaive Bayesian 54.83 44.29 73.06 43.66 55.15 72.56 43.66 73. 06 54.52\nRF 60.55 48.15 49.62 67.25 48.87 68.53 67.25 49.62 67.88\nLinear SVM 50.91 39.90 57.60 46.81 47.14 64.30 46.81 57.60 54 .18\nNon-linear SVM 62.00 Null 0.00 100.00 0.00 62.00 100.00 0.00 76 .54\nANN 62.35 54.37 5.77 97.03 10.43 62.69 97.03 5.77 76.17\nLBP\nLR 73.34 67.20 58.29 82.56 62.43 76.35 82.56 58.29 79.34\nkNN 70.27 64.05 49.64 82.92 55.93 72.87 82.92 49.64 77.57\nNaive Bayesian 57.39 46.41 78.43 44.49 58.31 77.09 44.49 78. 43 56.42\nRF 70.13 60.88 59.90 76.41 60.39 75.66 76.41 59.90 76.03\nLinear SVM 46.21 29.70 30.40 55.89 30.05 56.71 55.89 30.40 56 .30\nNon-linear SVM 62.00 Null 0.00 100.00 0.00 62.00 100.00 0.00 76 .54\nANN 71.19 64.72 53.19 82.23 58.39 74.13 82.23 53.19 77.97\nGLCM\nLR 67.54 58.21 51.65 77.27 54.74 72.28 77.27 51.65 74.69\nkNN 69.79 61.98 53.04 80.05 57.16 73.56 80.05 53.04 76.67\nNaive Bayesian 61.40 49.52 80.77 49.53 61.39 80.77 49.53 80. 77 61.41\nRF 71.15 61.42 64.72 75.09 63.03 77.64 75.09 64.72 76.34\nLinear SVM 66.66 55.02 67.30 66.28 60.54 76.78 66.28 67.30 71 .14\nNon-linear SVM 69.43 60.08 58.27 76.27 59.16 74.88 76.27 58.2 7 75.57\nANN 68.10 58.45 55.56 75.79 56.97 73.56 75.79 55.56 74.66\nThe bold text in the table indicates the highest value of the classiﬁ cation result of diﬀerent classiﬁers for the same feature.\ntraining time and 1/3 of the model size compared to ResNet.\nAlso, the accuracy curve is still trending upward and the loss\nfunction is still not fully converged.\n/four.tnum./two.tnum./two.tnum. On /one.tnum/two.tnum/zero.tnum× /one.tnum/two.tnum/zero.tnum pixels sub-database\nAccording to the\nTable 5, the classiﬁcation results are\nexcellent on the sub-database of 120 × 120 pixels. Due to a large\nnumber of training samples, VGG16 is the classiﬁer with the\nhighest accuracy of 96.47% on this sub-database. However, the\ntraining time is doubled compared to that on the 160 × 160 sub-\ndatabase. The accuracies of 95.83 and 95.94% are obtained for\nInception-V3 and ResNet50, respectively. Due to the increase\nin the amount of training data, ViT also gained an accuracy\nimprovement, rising to 89.44%.\n/four.tnum./two.tnum./three.tnum. On /eight.tnum/zero.tnum× /eight.tnum/zero.tnum pixels sub-database\nAccording to\nTable 5, the classiﬁcation results of the 80 × 80\nsubdatabase can be seen. It is the sub-database with the largest\nnumber of samples, and the accuracy of the four classiﬁers\nFrontiers in Medicine /zero.tnum/nine.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /five.tnum Classiﬁcation results of ﬁve image features using diﬀerent classiﬁers in the /eight.tnum/zero.tnum× /eight.tnum/zero.tnum pixels sub-database of GasHisSDB [In (%)].\nFreatures Methods Acc Abnormal Normal\nPre Rec Spe F1 Pre Rec Spe F1\nLR 82.22 78.17 77.59 85.35 77.88 84.93 85.35 77.59 85.14\nkNN 85.24 80.60 83.52 86.41 82.03 88.58 86.41 83.52 87.48\nRF 83.27 77.14 83.15 83.34 80.03 87.98 83.34 83.15 85.60\nColor histogram Linear SVM 60.81 50.86 83.58 45.41 63.24 80.3 6 45.41 83.58 58.03\nNon-linear SVM 59.67 Null 0.00 100.00 0.00 59.67 100.00 0.00 74 .74\nANN 79.28 76.60 70.03 85.54 73.17 80.85 85.54 70.03 83.13\nLR 70.16 66.79 51.77 82.60 58.33 71.70 82.60 51.77 76.76\nkNN 74.65 67.67 71.11 77.04 69.35 79.78 77.04 71.11 78.38\nRF 75.10 67.77 72.94 76.55 70.26 80.71 76.55 72.94 78.58\nLinear SVM 54.58 46.58 85.81 33.47 60.38 77.72 33.47 85.81 46 .79\nLuminance histogram Non-linear SVM 59.67 Null 0.00 100.00 0.0 0 59.67 100.00 0.00 74.74\nANN 70.17 63.19 62.38 75.43 62.78 74.79 75.43 62.38 75.11\nHOG\nLR 59.87 53.42 3.96 97.67 7.37 60.07 97.67 3.96 74.39\nkNN 59.63 49.95 42.22 71.40 45.76 64.64 71.40 42.22 67.85\nNaive Bayesian 55.91 46.97 72.35 44.79 56.96 70.56 44.79 72. 35 54.79\nRF 59.08 49.31 51.88 63.95 50.56 66.28 63.95 51.88 65.10\nLinear SVM 53.47 44.46 61.60 47.98 51.64 64.89 47.98 61.60 55 .17\nNon-linear SVM 59.70 90.91 0.08 99.99 0.17 59.68 99.99 0.08 74 .75\nANN 59.67 50.08 2.49 98.32 4.75 59.87 98.32 2.49 74.42\nLBP\nLR 70.92 65.32 59.49 78.65 62.27 74.17 78.65 59.49 76.34\nkNN 68.48 63.20 52.32 79.41 57.24 71.13 79.41 52.32 75.04\nNaive Bayesian 59.09 49.55 77.69 46.52 60.51 75.52 46.52 77. 69 57.57\nRF 68.16 60.13 62.49 71.98 61.29 73.95 71.98 62.49 72.95\nLinear SVM 43.10 27.68 25.48 55.01 26.53 52.20 55.01 25.48 53 .56\nNon-linear SVM 59.67 Null 0.00 100.00 0.00 59.67 100.00 0.00 74 .74\nANN 68.57 62.75 54.32 78.21 58.23 71.69 78.21 54.32 74.81\nGLCM\nLR 65.56 57.32 57.24 71.19 57.28 74.65 71.21 64.23 72.89\nkNN 68.84 62.32 57.53 76.49 59.83 72.71 76.49 57.53 74.55\nnaive Bayesian 62.12 51.96 80.87 49.45 63.27 79.27 49.45 80. 87 60.91\nRF 68.39 60.13 64.23 71.21 62.11 74.65 71.21 64.23 72.89\nLinear SVM 66.82 57.14 71.04 63.97 63.33 76.57 63.97 71.04 69 .71\nNon-linear SVM 68.31 61.03 59.26 74.42 60.13 72.99 74.42 59.2 6 73.70\nANN 65.52 56.70 61.40 68.30 58.96 72.36 68.30 61.40 70.27\nThe bold text in the table indicates the highest value of the classiﬁ cation result of diﬀerent classiﬁers for the same feature.\nonly changes slightly. VGG16 performs stably with an accuracy\nof 96.12%, which is the classiﬁcation model with the highest\naccuracy. The lowest accuracy is still the ViT model with the least\ntraining time, at 90.23. It is worth noting that the training time\nof ViT is 13.26% of that of the highest accurate VGG16 on this\nsub-database.\n/four.tnum./three.tnum. Additional experiment\nAs stated in Section 4.2.1, ViT did not converge completely\nwithin 100 epochs. Experiments are added in this section to\nexplore the performance of ViT, and the results are reﬂected in\nthe last row of each sub-database in\nTable 5. The same parameter\nconditions were maintained for all additional experiments. In\nthe additional experiments for the 160 × 160 sub-database,\nthe control training time was similar to that of Inception-V3\nand ResNet running 100 epochs. ViT runs 400 epochs and the\naccuracy reaches 92.23%. In the other two sub-databases with\nlarger amount of data, again when controlling for the same\ntraining time as Inception-V3 and RseNet50. At this time, the\naccuracy of ViT models for the 120 × 120 pixel sub-database and\nthe 80 × 80 pixel sub-database improves to 94.59 and 94.57%,\nrespectively. The model size of ViT has a great advantage.\nMoreover, these image classiﬁcation results reach the general\nlevel of medical image classiﬁcation.\nFrontiers in Medicine /one.tnum/zero.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nTABLE /six.tnum Classiﬁcation results of four deep learning classiﬁers on GasHisSDB [In (%)].\nSub-database size Model Quantity of epoch Model size (MB) Best eopch Training time(s) Acc Category Pre Rec Spe F1\n160 × 160 pixels\nVGG16 100 268.16 100 13,873 95.90\nAbnormal 93.8 96.0 95.9 94.9\nNormal 97.3 95.9 96.0 96.6\nInception-V3 100 89.69 92 10,296 94.57\nAbnormal 94.1 92.0 96.2 93.0\nNormal 94.9 96.2 92.0 95.5\nResNet50 100 83.12 84 10,023 96.09\nAbnormal 94.6 95.6 96.4 95.1\nNormal 97.1 96.4 95.6 96.7\nViT\n100 31.17 97 2,587 86.21\nAbnormal 83.8 80.6 89.9 82.2\nNormal 87.7 89.9 80.6 88.8\n400 31.17 399 10,014 92.23\nAbnormal 92.1 87.8 95.1 89.9\nNormal 92.3 95.1 87.8 93.7\n120 × 120 pixels\nVGG16 100 268.16 100 26,105 96.47\nAbnormal 96.7 94.0 98.0 95.3\nNormal 96.4 98.0 94.0 97.2\nInception-V3 100 89.69 98 19,719 95.83\nAbnormal 94.6 94.4 96.7 94.5\nNormal 96.6 96.7 94.4 96.6\nResNet50 100 83.12 94 19,087 95.94\nAbnormal 96.2 93.0 97.8 94.6\nNormal 95.8 97.8 93.0 96.8\nViT\n100 31.17 100 4,077 89.44\nAbnormal 87.0 84.9 92.2 85.9\nNormal 90.9 92.2 84.9 91.5\n500 31.17 496 20,410 94.59\nAbnormal 93.5 93.4 95.3 93.2\nNormal 95.4 95.9 92.5 95.6\n80 × 80 pixels\nVGG16 100 268.16 90 62,152 96.12\nAbnormal 94.2 96.3 96.0 95.2\nNormal 97.4 96.0 96.3 96.7\nInception-V3 100 89.69 99 43,926 95.41\nAbnormal 95.5 93.0 97.0 94.2\nNormal 95.3 97.0 93.0 96.1\nResNet50 100 83.12 97 41,992 96.09\nAbnormal 96.2 94.0 97.5 95.1\nNormal 96.0 97.5 94.0 96.7\nViT\n100 31.17 89 8,247 90.23\nAbnormal 86.3 90.1 90.3 88.2\nNormal 93.1 90.3 90.1 91.7\n500 31.17 496 41,135 94.57\nAbnormal 93.1 93.4 95.3 93.2\nNormal 95.6 95.3 93.4 95.4\nThe bold text in the table indicates the maximum value or the best in dex of the classiﬁcation results of diﬀerent categories.\n/four.tnum./four.tnum. t-SNE method analysis\nTo explore the possibility of ensemble learning between deep\nlearning classiﬁers, we conducted a TSNE analysis of the top\nperforming deep learning classiﬁers. the t-SNE method analysis\nwas performed using the 160 × 160 pixels sub-database as an\nexample and the results are shown in\nFigure 3.\nThis experimental platform use the t-SNE method to\ndownscale the features extracted by the four deep learning\nmethods into two-dimensional scatters displayed in the image.\nRepresentative images from the test set are selected in the\nﬁgure, where the abnormal image suﬀers from misclassiﬁcation\nin ViT, and its points after feature downscaling fall in the\nimage normal population. This image performs well in the other\nthree classiﬁers, and its feature-descended points fall in the\nimage abnormal population. However, it can be observed that\nthe selected normal image it performs well in Inception-V3,\nResNet50, ViT, with the reduced points falling in the normal\npopulation, but performs poorly in VGG16.\n/five.tnum. Discussion\nThis chapter compares the classiﬁcation results of diﬀerent\nclassiﬁers from the Linear Regression to Visual Transformer on\nthe 160 × 160, 120 × 120, and 80 × 80 pixels sub-databases of\nthe GasHisSDB. The classiﬁcation performance of each method\non GasHisSDB reﬂects complementarity.\nClassical machine learning methods have a rigorous\ntheoretical foundation. Their simpliﬁed ideas can show\ngood classiﬁcation results on some speciﬁc features and\nalgorithms (\n39).\nFrontiers in Medicine /one.tnum/one.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nFIGURE /three.tnum\nPlot of results from t-SNE analysis of four deep learning classiﬁ cation models.\nThis experimental platform shows that seven classiﬁers for\nGLCM classiﬁcation on three sub-databases with little diﬀerence\nin accuracy, where the naive Bayesian classiﬁer has signiﬁcantly\nhigher Rec than Spe for the abnormal category, and the linear\nSVM has slightly higher Rec than Spe. It shows that these\ntwo classiﬁers are better in classifying the abnormal category.\nHowever, the Spe of the other classiﬁcation models are higher\nthan the Rec, indicating that they are more eﬀective in classifying\nthe normal category. The same phenomenon occurs for every\nfeature of every sub-database. There exist classiﬁers with high\nRec values or high Spe values in the same condition. Such\na result can be a powerful indication of the existence of this\ncomplementarity of these classiﬁers.\nHowever, deep learning methods are still far ahead\nof classical machine learning methods in terms of image\nclassiﬁcation accuracy and experiment workload (\n40).\nBy analyzing the deep learning methods using the t-\nSNE method, there is a clear classiﬁcation performance for\ntheir feature extraction. In\nFigure 3 it can also be seen that\nthere is an aggregation of normal and abnormal images\nin the four classiﬁers. However, there is still inconsistency\nin the classiﬁcation results and it can be understood that\nthese methods can exist to some extent in a complementary\nmanner (\n41).\nThe evaluation metrics for deep learning models are\ngenerally high, but complementarity in the ﬁeld of machine\nlearning also occurs in the ﬁeld of deep learning (\n42). For\nexample, the Spe of Inception-V3 and ResNet50 on sub-database\nC for abnormal category classiﬁcation is high, but the high Rec\nof VGG16 can be well performed to the complementarity of the\nabove two models.\nThe selection of suitable classiﬁers is the primary problem\nof ensemble learning, and after relevant experiments in\nthe complementarity comparison experimental platform,\nit can be observed that these classiﬁers exhibit diﬀerent\nperformances (\n43). The complementarity possessed by\nthese classiﬁers can adequately meet the needs of ensemble\nlearning (\n44).\n/six.tnum. Conclusion and future work\nIn practice, machine performance often limits model\ntraining for large-size images, and ﬁnding multiple classiﬁcation\nmodels with complementarity types is the basis for ensemble\nlearning. For sub-sized images, this experiment tries a large\nnumber of classiﬁcation models to ﬁnd their complementarity\nand thus improve the eﬃciency of ensemble learning.\nFrontiers in Medicine /one.tnum/two.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\nThe experimental results show that complementarity in\nmachine learning does exist for diﬀerent classiﬁers of the\nsame feature. Diﬀerent classiﬁers for the same feature include\nclassiﬁers that classify the abnormal category well and classiﬁers\nthat classify the normal category well. This is a powerful\nindication of the complementarity among classiﬁers.\nThe evaluation metrics of the deep learning models are\nboth very excellent. There are models that are less eﬀective in\nclassifying the abnormal category than the normal category. In\nthis case, selecting the appropriate model that performs well\nfor the abnormal category can contribute to ensemble learning.\nComplementarity can also be demonstrated in this situation.\nThere are still many excellent methods that have not\nbeen added to the experimental platform. Moreover, the\nrecently popular ViT excels in the ﬁeld of image processing,\nbut ViT does not show signiﬁcant experimental results\non sub-size images. In the future, we will add more\nmodels to explore the complementarity nature of ensemble\nlearning on sub-size images to improve the eﬃciency of\nensemble learning.\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary material, further inquiries\ncan be directed to the corresponding author.\nAuthor contributions\nWH: method, experiment, and writing. HC and WL:\nexperiment. XL and HS: medical knowledge. XH and MG:\nmethod. CL: method, writing, and funding. All authors\ncontributed to the article and approved the submitted version.\nFunding\nThis work was supported by the National Natural\nScience Foundation of China (No. 82220108007) and\nBeijing Xisike Clinical Oncology Research Foundation\n(No. Y-tongshu2021/1n-0379).\nAcknowledgments\nWe would like to thank Miss. Zixian Li and Mr. Guoxian Li\nfor their important discussion in this work.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nReferences\n1. Sung H, Ferlay J, Siegel RL, Laversanne M, Soerjomataram I, Jemal A, et al.\nGlobal cancer statistics 2020: GLOBOCAN estimates of incidenc e and mortality\nworldwide for 36 cancers in 185 countries. CA Cancer J Clin . (2021) 71:209–49.\ndoi: 10.3322/caac.21660\n2. Wang FH, Shen L, Li J, Zhou ZW, Liang H, Zhang XT, et al.\nThe Chinese society of clinical oncology (CSCO): clinical guid elines for the\ndiagnosis and treatment of gastric cancer. Cancer Commun . (2019) 39:10.\ndoi: 10.1186/s40880-019-0349-9\n3. Cheng J, Han Z, Mehra R, Shao W, Cheng M, Feng Q, et al.\nComputational analysis of pathological images enables a better d iagnosis of\nTFE3 Xp11. 2 translocation renal cell carcinoma. Nat Commun . (2020) 11:1778.\ndoi: 10.1038/s41467-020-15671-5\n4. Liang J, Yang X, Huang Y, Li H, He S, Hu X, et al. Sketch guided a nd progressive\ngrowing GAN for realistic and editable ultrasound image synthes is. Med Image\nAnal. (2022) 79:102461. doi: 10.1016/j.media.2022.102461\n5. Tahiliani HT, Purohit AP , Desai SC, Jarwani PB. Retrospecti ve analysis of\nhistopathological spectrum of premalignant and malignant colorec tal lesions.\nCancer Res Stat Treat . (2021) 4:472–8. doi: 10.4103/crst.crst_87_21\n6. Zhao P , Li C, Rahaman MM, Xu H, Yang H, Sun H, et al. A comparativ e study\nof deep learning classiﬁcation methods on a small environmenta l microorganism\nimage dataset (EMDS-6): from convolutional neural networks to visual\ntransformers. Front Microbiol. (2022) 13:792166. doi: 10.3389/fmicb.2022.792166\n7. Xue D, Zhou X, Li C, Yao Y, Rahaman MM, Zhang J, et al.\nAn application of transfer learning and ensemble learning techniqu es for\ncervical histopathology image classiﬁcation. IEEE Access . (2020) 8:104603–18.\ndoi: 10.1109/ACCESS.2020.2999816\n8. Nazarian S, Glover B, Ashraﬁan H, Darzi A, Teare J. Diagnosti c accuracy\nof artiﬁcial intelligence and computer-aided diagnosis for th e detection and\ncharacterization of colorectal polyps: systematic review and m eta-analysis. J Med\nInternet Res. (2021) 23:e27370. doi: 10.2196/27370\n9. Schmarje L, Santarossa M, Schroder SM, Koch R. A survey on s emi-, self-\nand unsupervised learning for image classiﬁcation. IEEE Access. (2021) 9:82146–68.\ndoi: 10.1109/ACCESS.2021.3084358\n10. Shinde PP , Shah S. A review of machine learning and deep lear ning\napplications. In: 2018 Fourth International Conference on Computing\nCommunication Control and Automation (ICCUBEA) . Pune: IEEE (2018). p.\n1–6. doi: 10.1109/ICCUBEA.2018.8697857\n11. Li Y, Wu X, Li C, Li X, Chen H, Sun C, et al. A hierarchical cond itional\nrandom ﬁeld-based attention mechanism approach for gastric hi stopathology\nimage classiﬁcation. Appl Intell. (2022) 1–22. doi: 10.1007/s10489-021-02886-2\nFrontiers in Medicine /one.tnum/three.tnum frontiersin.org\nHu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/one.tnum/zero.tnum/nine.tnum\n12. Hu W, Li C, Li X, Rahaman MM, Ma J, Zhang Y, et al. GasHisSDB: a new\ngastric histopathology image dataset for computer aided diagn osis of gastric cancer.\nComput Biol Med . (2022) 142:105207. doi: 10.1016/j.compbiomed.2021.1052 07\n13. Fu B, Zhang M, He J, Cao Y, Guo Y, Wang R. StoHisNet: a\nhybrid multi-classiﬁcation model with CNN and transformer fo r gastric\npathology images. Comput Methods Programs Biomed . (2022) 221:106924.\ndoi: 10.1016/j.cmpb.2022.106924\n14. Wang P , Fan E, Wang P. Comparative analysis of image classiﬁc ation\nalgorithms based on traditional machine learning and deep lear ning. Pattern\nRecogn Lett. (2021) 141:61–7. doi: 10.1016/j.patrec.2020.07.042\n15. Ma P , Li C, Rahaman MM, Yao Y, Zhang J, Zou S, et al. A state-of -the-\nart survey of object detection techniques in microorganism i mage analysis: from\nclassical methods to deep learning approaches. Artif Intell Rev . (2022) 1–72.\ndoi: 10.1007/s10462-022-10209-1\n16. Sun C, Li C, Zhang J, Rahaman MM, Ai S, Chen H, et al. Gastric\nhistopathology image segmentation using a hierarchical cond itional random ﬁeld.\nBiocybern Biomed Eng . (2020) 40:1535–55. doi: 10.1016/j.bbe.2020.09.008\n17. Zheng X, Wang R, Zhang X, Sun Y, Zhang H, Zhao Z, et al. A\ndeep learning model and human-machine fusion for prediction o f EBV-\nassociated gastric cancer from histopathology. Nat Commun . (2022) 13:2970.\ndoi: 10.1038/s41467-022-30459-5\n18. Dai Y, Gao Y, Liu F. Transmed: transformers advance multi- modal medical\nimage classiﬁcation. Diagnostics. (2021) 11:1384. doi: 10.3390/diagnostics11081384\n19. Japanese Gastric Cancer Association. Japanese classiﬁcat ion of\ngastric carcinoma: 3rd English edition. Gastric Cancer . (2011) 14:101–12.\ndoi: 10.1007/s10120-011-0041-5\n20. Humeau-Heurtier A. Texture feature extraction methods : a survey. IEEE\nAccess. (2019) 7:8975–9000. doi: 10.1109/ACCESS.2018.2890743\n21. Kulwa F, Li C, Grzegorzek M, Rahaman MM, Shirahama K, Kosov S .\nSegmentation of weakly visible environmental microorganism images using pair-\nwise deep learning features. Biomed Signal Process Control . (2023) 79:104168.\ndoi: 10.1016/j.bspc.2022.104168\n22. Hope TM. Linear regression. In: Machine Learning. London: Elsevier (2020).\np. 67–81. doi: 10.1016/B978-0-12-815739-8.00004-3\n23. Guo G, Wang H, Bell D, Bi Y, Greer K. KNN model-based approach\nin classiﬁcation. In: OTM Confederated International Conferences on the\nMove to Meaningful Internet Systems .” Catania: Springer (2003). p. 986–96.\ndoi: 10.1007/978-3-540-39964-3_62\n24. Yang FJ. An implementation of naive bayes classiﬁer. In: 2018 International\nConference on Computational Science and Computational Intelligence (CSCI) . Las\nVegas, NV: IEEE (2018). p. 301–6. doi: 10.1109/CSCI46756.20 18.00065\n25. Shi T, Horvath S. Unsupervised learning with random forest predictors. J\nComput Graph Stat . (2006) 15:118–38. doi: 10.1198/106186006X94072\n26. Suthaharan S. Support vector machine. In: Machine Learning Models and\nAlgorithms for Big Data Classiﬁcation . Boston, MA: Springer (2016). p. 207–35.\ndoi: 10.1007/978-1-4899-7641-3_9\n27. Hopﬁeld JJ. Artiﬁcial neural networks. IEEE Circ Dev Mag . (1988) 4:3–10.\ndoi: 10.1109/101.8118\n28. Zhang J, Li C, Kosov S, Grzegorzek M, Shirahama K, Jiang T, e t al. LCU-Net: a\nnovel low-cost U-Net for environmental microorganism image segmentation. Patt\nRecogn. (2021) 115:107885. doi: 10.1016/j.patcog.2021.107885\n29. Zhang J, Ma P , Jiang T, Zhao X, Tan W, Zhang J, et al. SEM-RCNN :\na squeeze-and-excitation-based mask region convolutional n eural network for\nmulti-class environmental microorganism detection. Appl Sci . (2022) 12:9902.\ndoi: 10.3390/app12199902\n30. Chen H, Li C, Wang G, Li X, Rahaman MM, Sun H, et al.\nGasHis-Transformer: a multi-scale visual transformer approac h for\ngastric histopathological image detection. Patt Recogn . (2022) 130:108827.\ndoi: 10.1016/j.patcog.2022.108827\n31. Yang H, Zhao X, Jiang T, Zhang J, Zhao P , Chen A, et al. Compara tive study for\npatch-level and pixel-level segmentation of deep learning method s on transparent\nimages of environmental microorganisms: from convolutiona l neural networks to\nvisual transformers. Appl Sci. (2022) 12:9321. doi: 10.3390/app12189321\n32. Simonyan K, Zisserman A. Very deep convolutional network s for large-scale\nimage recognition. arXiv Preprint. (2014) arXiv:14091556.\n33. Szegedy C, Vanhoucke V , Ioﬀe S, Shlens J, Wojna Z. Rethinki ng the inception\narchitecture for computer vision. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . Las Vegas, NV (2016). p. 2818–26.\ndoi: 10.1109/CVPR.2016.308\n34. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitio n.\nLas Vegas, NV (2016). p. 770–8. doi: 10.1109/CVPR.2016.90\n35. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner\nT, et al. An image is worth 16x16 words: Transformers for image recognition at\nscale. arXiv Preprint. (2020) arXiv:201011929.\n36. Liu W, Li C, Rahaman MM, Jiang T, Sun H, Wu X, et al. Is the aspec t\nratio of cells important in deep learning? A robust comparison of d eep learning\nmethods for multi-scale cytopathology cell image classiﬁcation: from convolutional\nneural networks to visual transformers. Comput Biol Med . (2022) 141:105026.\ndoi: 10.1016/j.compbiomed.2021.105026\n37. Liu W, Li C, Xu N, Jiang T, Rahaman MM, Sun H, et al. CVM-\nCervix: a hybrid cervical pap-smear image classiﬁcation frame work using CNN,\nvisual transformer and multilayer perceptron. Patt Recogn . (2022) 130:108829.\ndoi: 10.1016/j.patcog.2022.108829\n38. Zhou X, Li C, Rahaman MM, Yao Y, Ai S, Sun C, et al. A comprehens ive\nreview for breast histopathology image analysis using classica l and deep neural\nnetworks. IEEE Access. (2020) 8:90931–56. doi: 10.1109/ACCESS.2020.2993788\n39. Chen A, Li C, Zou S, Rahaman MM, Yao Y, Chen H, et al. SVIA data set: a\nnew dataset of microscopic videos and images for computer-aid ed sperm analysis.\nBiocybern Biomed Eng . (2022) 42:204–14. doi: 10.1016/j.bbe.2021.12.010\n40. Shi Z, Zhu C, Zhang Y, Wang Y, Hou W, Li X, et al. Deep learning\nfor automatic diagnosis of gastric dysplasia using whole-slide histopathology\nimages in endoscopic specimens. Gastric Cancer . (2022) 25:751–60.\ndoi: 10.1007/s10120-022-01294-w\n41. Tsuneki M, Ichihara S, Kanavati F. Weakly supervised learni ng for\npoorly diﬀerentiated adenocarcinoma classiﬁcation in gastri c endoscopic\nsubmucosal dissection whole slide images. medRxiv. (2022). p. 1–15.\ndoi: 10.1101/2022.05.28.22275729\n42. Zhang J, Zhao X, Jiang T, Rahaman MM, Yao Y, Lin YH, et al. An\napplication of pixel interval down-sampling (PID) for dense tiny m icroorganism\ncounting on environmental microorganism images. Appl Sci . (2022) 12:7314.\ndoi: 10.3390/app12147314\n43. Li X, Li C, Rahaman MM, Sun H, Li X, Wu J, et al. A\ncomprehensive review of computer-aided whole-slide image analy sis: from\ndatasets to feature extraction, segmentation, classiﬁcati on and detection\napproaches. Art Intell Rev . (2022) 55:4809–78. doi: 10.1007/s10462-021-10\n121-0\n44. Rahaman MM, Li C, Yao Y, Kulwa F, Rahman MA, Wang Q, et al.\nIdentiﬁcation of COVID-19 samples from chest X-Ray images usi ng deep learning:\na comparison of transfer learning approaches. J X-ray Sci Technol . (2020)\n28:821–39. doi: 10.3233/XST-200715\nFrontiers in Medicine /one.tnum/four.tnum frontiersin.org",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.8452870845794678
    },
    {
      "name": "Machine learning",
      "score": 0.8232868909835815
    },
    {
      "name": "Computer science",
      "score": 0.6867119669914246
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6304600834846497
    },
    {
      "name": "Ensemble learning",
      "score": 0.6282579898834229
    },
    {
      "name": "Support vector machine",
      "score": 0.6199672818183899
    },
    {
      "name": "Deep learning",
      "score": 0.5914785265922546
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5439640283584595
    },
    {
      "name": "Contextual image classification",
      "score": 0.4697205126285553
    },
    {
      "name": "Linear classifier",
      "score": 0.42197489738464355
    },
    {
      "name": "Random subspace method",
      "score": 0.4109991192817688
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37487632036209106
    },
    {
      "name": "Image (mathematics)",
      "score": 0.1263929009437561
    }
  ]
}