{
  "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
  "url": "https://openalex.org/W3161457214",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4202174151",
      "name": "Hendrycks, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226215456",
      "name": "Basart, Steven",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224152834",
      "name": "Kadavath, Saurav",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202174152",
      "name": "Mazeika Mantas",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Arora, Akul",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Guo, Ethan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287489796",
      "name": "Burns, Collin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Puranik, Samir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223932249",
      "name": "He, Horace",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751273816",
      "name": "Song, Dawn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173099975",
      "name": "Steinhardt, Jacob",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2242083635",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3083835029",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W3126259453",
    "https://openalex.org/W2795038878",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W2950700477",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3094459920",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W2607964821",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2970575144",
    "https://openalex.org/W3081454995",
    "https://openalex.org/W2096979215",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2533695286",
    "https://openalex.org/W2295159694",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1655078475",
    "https://openalex.org/W3033638351",
    "https://openalex.org/W2148190602"
  ],
  "abstract": "GPT-Neo is an implementation of model &amp; data-parallel GPT-2 and GPT-3-like models, utilizing Mesh Tensorflow for distributed support. This codebase is designed for TPUs. It should also work on GPUs, though we do not recommend this hardware configuration.",
  "full_text": "Measuring Coding Challenge Competence With\nAPPS\nDan Hendrycks∗\nUC Berkeley\nSteven Basart*\nUChicago\nSaurav Kadavath\nUC Berkeley\nMantas Mazeika\nUIUC\nAkul Arora\nUC Berkeley\nEthan Guo\nUC Berkeley\nCollin Burns\nUC Berkeley\nSamir Puranik\nUC Berkeley\nHorace He\nCornell\nDawn Song\nUC Berkeley\nJacob Steinhardt\nUC Berkeley\nAbstract\nWhile programming is one of the most broadly applicable skills in modern society,\nit is unclear how well state-of-the-art machine learning models can write code. De-\nspite its importance, there has been surprisingly little work on evaluating code gen-\neration, and it can be difﬁcult to assess code generation performance in an accurate\nand rigorous manner. To meet this challenge, we introduce APPS, a benchmark for\ncode generation. Unlike prior work in more restricted settings, our benchmark mea-\nsures the ability of models to take an arbitrary natural language speciﬁcation and\ngenerate satisfactory Python code. Similar to how companies assess candidate soft-\nware developers, we evaluate models by checking their generated code on test cases.\nOur benchmark includes 10,000 problems, which range from having simple one-\nline solutions to being substantial algorithmic challenges. We ﬁne-tune large lan-\nguage models on both GitHub and our training set, and we ﬁnd that the prevalence\nof syntax errors is decreasing exponentially as models improve. Recent models such\nas GPT-Neo can pass approximately20% of the test cases of introductory problems,\nso we ﬁnd that machine learning models are now beginning to learn how to code.\nAs the social signiﬁcance of automatic code generation increases over the coming\nyears, our benchmark can provide an objective measure for tracking advancements.\n“Everybody should learn to program a computer, because it teaches you how to think.” –Steve Jobs\n1 Introduction\nComputer programming can be found in nearly all parts of society. Spanning entertainment, health-\ncare, education, and more, programming is an extraordinarily general tool with applications that are\nvast in scope. As computers are becoming more ubiquitous in modern life, rising demand for high-\nquality code draws an ever-greater number of aspiring programmers to the profession. After years of\nstudy to become proﬁcient coders, human experts are are able to convert abstract speciﬁcations of\ndiverse cognitive tasks into concrete programs.\nIn the past few years, large-scale language models have shown promise in generalizing to various\ncognitive tasks, including linguistic inference (Wang et al., 2019a), commonsense reasoning (Zellers\net al., 2019; Huang et al., 2019; Bisk et al., 2019), logical deduction (Liu et al., 2020b), mathematics\n(Polu and Sutskever, 2020; Hendrycks et al., 2021c), and general understanding of multiple domains\n∗Equal Contribution.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\narXiv:2105.09938v3  [cs.SE]  8 Nov 2021\nProblem Generated Code Test Cases\nH-Index\nGiven a list of citations counts,\nwhere each citation is a\nnonnegative integer, write a\nfunction h_index that outputs\nthe h-index. The h-index is the\nlargest number h such that h\npapers have each least h citations.\nExample:\nInput: [3,0,6,1,4]\nOutput: 3\nInput:\n[1,4,1,4,2,1,3,5,6]\nGenerated Code Output:\n4                      ✓\nInput:\n[1000,500,500,250,100,\n100,100,100,100,75,50,\n30,20,15,15,10,5,2,1]\nGenerated Code Output:\n15                     ✓\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\b\u0001\u0002\t\n\u000b\f\r\b\u000e\u000f\u0010\u0011\u0004\u0004\n\u0004\u0004\b\u0004\u0012\u0004\u0013\u0002\b\n\u000b\f\r\b\u000e\u000f\u0010\n\u0004\u0004\u0007\u0003\u0004\b\u0004\u0014\u0004\u0015\u0011\n\u0004\u0004\u0004\u0004\u000b\f\r\b\u000e\u000f\u0016\u000f\f\u0017\u000e\n\u0010\n\u0004\u0004\u0004\u0004\u000b\f\r\b\u000e\u000f\u0016\u0017\u0002\u0018\u0002\u0017\u000f\u0002\n\u0010\n\u0004\u0004\u0004\u0004\u0005\u0004\u0012\u0004\u0015\n\u0004\u0004\u0004\u0004\u0019\u0005\u0007\u0013\u0002\u0004\n\u0005\u0004\u001a\u0004\b\u0004\u001b\b\u0001\n\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u000b\f\r\b\u000e\u000f\u001c\u0005\u001d\u001e\u001f\u0014\u0012\u0005\u0010\u0011\n\u0004\u0004\u0004\u0004\u0004\u0004\u0005\u0004 \u0012\u0004\u001f\n\u0004\u0004\u0004\u0004\u0017\u0002\u000e\r\u0017\b\u0004\u0005\u0004\u0004\n\u0004\u0004\u0002\u0013\u000f\u0002\u0011\n\u0004\u0004\u0004\u0004\u0017\u0002\u000e\r\u0017\b\u0004\u0015\u0001\n\u0002\nFigure 1: An example problem from APPS (left) along with possible generated code (middle) and\ntwo example test cases we use to evaluate the generated code (right). Our evaluation framework has\ntest cases and 10,000 code generation problems of varying difﬁculty levels.\nof human knowledge (Hendrycks et al., 2021b). However, whether large-scale language models can\nreliably write code remains an open question.\nMotivated by the potential of language models and the need for thorough code generation evaluation,\nwe introduce APPS, a benchmark for code generation from natural language speciﬁcations. Unlike\nprior work on code generation with Transformer language models (Vaswani et al., 2017), which\nmostly focuses on code translation (Lachaux et al., 2020) and pseudocode-to-code (Kulal et al., 2019),\nwe evaluate models on their ability to take speciﬁcations given in natural language and write code\nthat meets these speciﬁcations. This setting mirrors how human coders are evaluated and is a more\nrealistic and informative setting in which to benchmark models.\nAPPS provides a precise and comprehensive view of code generation. APPS evaluates models not\nonly on their ability to code syntactically correct programs, but also on their ability to understand task\ndescriptions and devise algorithms to solve these tasks. It contains 10,000 programming problems at\nvarious levels of difﬁculty, covering simple introductory problems, interview-level problems, and\ncoding competition challenges. If a model were to perform well on APPS, this would indicate an\nability to ﬂexibly use data structures and programming techniques, as well as an ability to correctly\ninterpret diverse task speciﬁcations, follow instructions, and understand human intent (Hendrycks\net al., 2021a).\nFor most text generation tasks, high-quality evaluation requires human feedback, which can be\ntime-consuming or carry pecuniary costs. As a result, automatic metrics such as BLEU (Papineni\net al., 2002) are often used to compare methods, but these metrics do not necessarily track program\ncorrectness. Since the objective for code generation is to produce correct programs, we assess\nprograms not with BLEU but with test cases and error catching. Evaluating code generation on APPS\nis facilitated by a large bank of over 130,000 test cases. The test cases are speciﬁcally chosen to\nprobe correct functionality across the input space. By using test cases, we provide a gold-standard\nmetric for code generation quality.\nIn our experiments, we ﬁnd that models are now starting to exhibit nonzero accuracy and solve some\ncoding problems. Additionally, as models improve, we observe that syntax errors are exponentially\ndecreasing. We also ﬁnd further evidence that BLEU is a problematic metric for code generation,\nsometimes being anticorrelated with gold-standard accuracy. We ﬁnd that accuracy decreases with\ndifﬁculty level and improves through ﬁne-tuning and model size increases. The strongest model that\nwe evaluate on introductory problems passes almost 20% of test cases given ﬁve attempts. These\nresults position code generation as a challenging but now tractable testbed for large-scale language\nmodels.\nWriting code to meet speciﬁcations in natural language is an economically valuable task with\nwidespread social implications should it be solved, as it could eventually facilitate malicious code\ngeneration and one day result in job automation. As large-scale language models have the potential\n2\nPY150 CONCODE SPoC APPS\nProgramming Language Python Java C++ Python\nTest Cases \u0015 \u0015\nNumber of Programs N/A 104,000 18,356 232,421\nLines per Program (Avg.) 1 26.3 14.7 18.0\nNumber of Exercises 3,000 104,000 677 10,000\nText Input Python Docstrings Pseudocode Problem Descriptions\nTable 1: A comparison of the APPS dataset to existing datasets for converting between text and code.\nAPPS has over an order of magnitude more ground-truth solutions than these datasets, test cases, and\nnatural language problem descriptions.\nto make signiﬁcant progress on code generation, it is essential that we begin to track advancements\non this task. Our new benchmark facilitates measuring performance in an accurate and rigorous\nmanner. Using APPS, we ﬁnd that programming is very difﬁcult for modern language models, though\nperformance is improving. Thus, the APPS benchmark can provide foresight about the performance\nof future large-scale language models at the critical task of program synthesis from natural language.\nThe dataset is available at https://github.com/hendrycks/apps.\n2 Related Work\nProgram Synthesis. Program synthesis is the task of generating a computer program that satisﬁes\ngiven speciﬁcations. Deductive program synthesis uses formal logic speciﬁcations to deﬁne a\nsearch problem. Complex optimization techniques are used to generate programs satisfying these\nspeciﬁcations (Alur et al., 2018). Because speciﬁcations must be converted into a formal language,\nthese approaches can be rigid. Inductive synthesis from example input-output behavior can provide\nan alternative to formal speciﬁcation (Cai et al., 2017; Gulwani et al., 2017), but it is often hard to\nfull specify behavior with examples, as any machine learning practitioner is well-aware.\nAn alternative to formal or inductive speciﬁcation is to specify program behavior in natural language,\nwhich prior work has considered in constrained settings. Raza et al. (2015) and Desai et al. (2016)\ngenerate short programs using ad-hoc programming languages to solve speciﬁcations such as “Any\n2 letters followed by any combination of 6 whole numbers.” Yu et al. (2018) introduce the Spider\ndataset for converting natural language queries into short SQL database commands. In contrast, we\nconsider long natural language speciﬁcations and general-purpose programming languages.\nCode Understanding Datasets. Language modeling is a compelling tool for code generation,\nand several works have achieved success generating code with language models in limited settings.\nLachaux et al. (2020) use unsupervised machine translation techniques to translate functions across\nprogramming languages, attaining identical behavior after translation in many cases. Kulal et al.\n(2019) introduce SPoC, a method for converting pseudocode to code utilizing seq2seq machine\ntranslation with an additional search step. To train SPoC, they collect line-by-line descriptions of C++\nprograms using Amazon Mechanical Turk. Recently, Lu et al. (2021) introduce the CodeXGLUE\nbenchmark which aggregates various previous benchmarks and use CodeBLEU (Ren et al., 2020)\nand CONCODE. Iyer et al. (2018) investigate generating Java code from docstrings and evaluate\nperformance with BLEU. The docstrings are often incomplete speciﬁcations of what should be coded\nand only 14.7 words long on average, e.g. “Convert mixed case to underscores.” By comparison,\nproblem speciﬁcations in our new APPS benchmark are self-contained and have a much larger\naverage length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise,\nenabling a high-quality evaluation of code correctness. Further comparisons are in the Appendix.\nEvaluating Large-Scale Language Models. Modern large-scale language models have demon-\nstrated impressive capabilities across a variety of text-based tasks. On the SuperGLUE benchmark\n(Wang et al., 2019b), some models now exceed human performance. On many commonsense reason-\ning benchmarks, performance is rising quickly (Zellers et al., 2019; Huang et al., 2019; Bisk et al.,\n2019). Even when language models are evaluated across diverse technical areas such as law and\nmedicine, performance is surprisingly high and poised to improve as models are scaled up further\n(Hendrycks et al., 2021b). With rapid improvements across numerous datasets, ﬁnding resilient\n3\nbenchmarks on which models signiﬁcantly underperform humans is challenging. APPS represents an\nattempt to ﬁll this gap and cleanly separate model performance from that of expert humans.\n3 The APPS Dataset\nThe APPS dataset consists of problems collected from different open-access coding websites such as\nCodeforces, Kattis, and more. The APPS benchmark attempts to mirror how humans programmers are\nevaluated by posing coding problems in unrestricted natural language and using test cases to evaluate\nsolution correctness. The problems range in difﬁculty from introductory to collegiate competition\nlevel and measure coding and problem-solving ability.\nThe Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding\nproblems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions\nwritten by humans. Problems can be complicated, as the average length of a problem is 293.2 words.\nThe data are split evenly into training and test sets, with 5,000 problems each. In the test set, every\nproblem has multiple test cases, and the average number of test cases is 21.2. Each test case is\nspeciﬁcally designed for the corresponding problem, enabling us to rigorously evaluate program\nfunctionality.\nDataset Construction. To create the APPS dataset, we manually curate problems from open-access\nsites where programmers share problems with each other, including Codewars, AtCoder, Kattis, and\nCodeforces. Problems are posed as natural language speciﬁcations of what should be coded, and they\ncome in various formats. To improve quality and consistency, we wrote custom HTML parsers for\neach source of problems, which allows us to properly format LaTeX expressions, lists, and sections in\nthe question text. Where necessary, we convert equation images to LaTeX using the MathPix API, and\nwe remove problems that rely on image ﬁgures. We also perform deduplication using tf-idf features\nwith SVD dimensionality reduction and cosine similarity. Several graduate and undergraduate student\nauthors polished and reﬁned this dataset over the course of six months, ensuring a high-quality set of\nproblems.\nExecuting and evaluating arbitrary Python code is challenging. On the websites we source data from,\nhuman solutions are allowed to run arbitrary code, including import statements for common modules\nand libraries. To handle this, each website implements a custom judging system for solutions. We\ndesign a testing framework with this in mind, which merges the judging functionality of several\nwebsites. We also standardize the format of test cases. The end result is that solutions are allowed to\nexecute arbitrary Python code, and the results are compared against test cases for a given problem.\nDataset Difﬁculty. Each of our problem sources uses a separate scale for measuring difﬁculty. We\nplace problems from these different sources into three categories. For example, problems from Kattis\nwith difﬁculty less than 3 are categorized as “introductory,” problems with difﬁculty between3 and 5\nas “interview,” and problems with difﬁculty greater than5 as “competition.”\n1. Introductory Level. These are problems that most programmers with 1-2 years of expe-\nrience can answer without requiring complicated algorithms. Examples of such problems\ninclude counting the number of vowels in a string, or returning the running sum of a list of\nintegers. There are 3,639 problems classiﬁed as introductory level and 1,000 in the test set.\n2. Interview Level. These are problems that are more algorithmic and difﬁcult in nature and\nwould be at the level of questions asked in programming technical interviews. Examples\nof such problems might include those involving data structures such as trees or graphs,\nor problems that requiring nontrivial algorithms. There are 5,000 problems classiﬁed as\ninterview level and 3,000 in the test set.\n3. Competition Level. These are problems are the most challenging and are at the level of the\nmost advanced high school and collegiate programming competitions, including USACO,\nIOI, and ACM. There are 1,361 competition level problems and 1,000 in the test set.\nProblem Formats. To accommodate a broad range of problem sources, problems in APPS come\nin two formats.\n• Call-Based Format problems generally provide initial starter code, usually in the form of a\nfunction header, and ask for the solution to be provided as the function’s return value.\n4\nProblem\nYou are given a strings = s1s2 . . . sn of length n, which only contains digits 1, 2,..., 9. A substring\ns[l . . . r] of s is a string slsl+1sl+2 . . . sr. A substring s[l . . . r] of s is called even if the number\nrepresented by it is even. Find the number of even substrings of s. Note, that even if some substrings\nare equal as strings, but have different l and r, they are counted as different substrings. The ﬁrst line\ncontains an integer n (1 ≤n ≤65000) — the length of the string s. The second line contains a string\ns of length n. The string s consists only of digits 1, 2,..., 9. Print the number of even substrings of s.\nModel Output\nn = int(input())\ns = input()\nans = 0\nfor i in range(n):\nfor j in range(i, n):\nif int(s[i:j+1]) % 2 == 0:\nans += 1\nprint(ans)\nFigure 2: Model-generated code from GPT-2 1.5B. 18/18 test cases pass for the problem. Note that\nmodels were given in problems exactly as they they are formatted here, which even meant parsing\nLATEX. For brevity, we exclude formatting instructions from this depicted problem.\n• Standard Input Format problems generally lack starter code. Instead, the model is only\nprovided with the problem and must output its answers to the STDOUT stream, such as by\nusing print statements.\nFor the call-based format problems, we prompt models using the following inputs:\n\"\\nQUESTION:\\n\" + q_str + \"\\n\" + starter_code_str + \"\\n\" + \"\\nUse\nCall-Based Format\\n\\nANSWER:\\n\"\nFor the above prompt, the variable q_strrepresents the raw text of the problem statement. The\nvariable starter_code_strrepresents the starter code given in the problem deﬁnition, or the\nempty string if no starter code was provided. For the standard input format problems, we prompt\nthe model with the input string as before, but we replace “Call-Based Format” with “Standard Input\nFormat.” Note that if starter code is given, it is only part of the input. This means that to use the\nstarter code, a model must learn to copy the starter code at the beginning of its outputted answer in\norder to get the question correct. We ﬁnd that ﬁne-tuned models are able to do this without difﬁculty.\nTest Case Quality. In the APPS test split, the average number of test cases is 21.2, but some\nproblems only have two test cases. These problems mainly come from Kattis and were chosen for\nthe test split due to limited numbers of competition problems. A potential concern is that these\nproblems could result in false positives if models happen to guess both test cases correctly. This is\nvery unlikely in problems with large output spaces, but some problems have small output spaces,\nsuch as {“YES”, “NO”}. Although the model must narrow down these two options from the space of\nall possible strings, we ﬁnd that this is not difﬁcult for current models to accomplish, and this may\ncause false positives in our evaluation.\nTo quantify the impact of these problems, we took all 2- and 3-test-case problems that GPT-Neo 2.7B\npassed and manually checked for false positives. Out of 12 problems that the model got correct with\n2 test cases, 8 were false positives. Out of 6 problems with 3 test cases, only 1 was a false positive.\nSince the test cases are from coding challenge websites with many users, we can assume that false\nnegatives are absent. Thus, the false positive rate with 2 test cases is 8/(890 −4) = 0.009 < 1% and\n1/(765−5) = 0.001 < 1% with 3 test cases. The upshot is that the potential for noise is signiﬁcantly\nless than that of most naturally annotated datasets.\n5\nProblem\nYou are given two integers n and m. Calculate the number of pairs of arrays (a, b) such that: the\nlength of both arrays is equal to m; each element of each array is an integer between 1 and n\n(inclusive); ai ≤bi for any index i from 1 to m; array a is sorted in non-descending order; array b is\nsorted in non-ascending order. As the result can be very large, you should print it modulo 109 + 7.\nInput: The only line contains two integers n and m (1 ≤n ≤1000, 1 ≤m ≤10). Output: Print one\ninteger – the number of arrays a and b satisfying the conditions described above modulo 109 + 7.\nModel Output\nn, m = map(int, input().split())\nmod = 10**9+7\nfact = [1, 1]\nfactinv = [1, 1]\ninv = [0, 1]\nfor i in range(2, n+1):\nfact.append((fact[-1] * i) % mod)\ninv.append((-inv[mod % i] * (mod//i)) % mod)\nfactinv.append((factinv[-1] * inv[-1]) % mod)\ndef cmb(n, r, mod):\nif (r < 0) or (n < r):\nreturn 0\nr = min(r, n - r)\nreturn fact[n] * factinv[r] * factinv[n-r] % mod\nprint((cmb(n, m, mod) * cmb(m-1, n-1, mod)) % mod)\nFigure 3: An example from GPT-2 1.5B. Although the code generated passes 0 test cases, it looks\nplausible at ﬁrst glance.\n4 Experiments\n4.1 Experimental Setup\nModels. We use GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and GPT-Neo (Black\net al., 2021) models. The GPT architecture is especially appropriate for text generation because it is\nautoregressive. However, GPT-2 was not pretrained on code, so we pretrain it on GitHub as described\nin the next paragraph. Anecdotal evidence indicates that GPT-3 can generate code. To determine the\nextent of its code generation ability, we use the ‘davinci’ (Instruct series) model, the largest publicly\navailable model speculated to have 175 billion parameters. Finally, GPT-Neo has an architecture\nsimilar to GPT-3, and it was pretrained on the Pile (Gao et al., 2020) which includes GitHub. Unlike\nGPT-3, GPT-Neo’s weights are publicly available, hence we are able to ﬁne-tune it with APPS.\nGPT-2 Pretraining. Since GPT-2 was trained on natural language and not code, we collected\nGitHub code to further pretrain GPT-2. GitHub repositories with fewer than one star were ﬁltered\nout. While Neo’s GitHub pretraining data didnot undergo an APPS data decontamination process,\nour GPT-2 models are trained on decontaminated data. Speciﬁcally, all repositories matching certain\nkeywords that would suggest overlap with common programming exercises were removed. We\nprovide the list of keywords in the Supplementary Materials. We also discard any GitHub code that\ncontains functions with the same signatures as functions in the starter code in many of our APPS\nproblems. This leaves us with 30 GB of Python code. To improve the efﬁciency of pretraining, we\nprocess all Python code in the pretraining dataset by converting from spaces to tabs, which saves the\ncharacter conversion when running model tokenizers.\nFine-tuning. During ﬁne-tuning with APPS, the objective is to predict the entire code solution,\ngiven both the English text problem statement and the problem format (call-based format or standard\ninput format). For problems with starter code, we exclude the starter code from the training loss.\n6\nTest Case Average Strict Accuracy\nModel Introductory Interview Competitive Average Introductory Interview Competition Average\nGPT-2 0.1B 5.64 6.93 4.37 6.16 1.00 0.33 0.00 0.40\nGPT-2 1.5B 7.40 9.11 5.05 7.96 1.30 0.70 0.00 0.68\nGPT-Neo 2.7B 14.68 9.85 6.54 10.15 3.90 0.57 0.00 1.12\nGPT-3 175B 0.57 0.65 0.21 0.55 0.20 0.03 0.00 0.06\nTable 2: Average percentage of test cases passed and strict accuracy for each model and difﬁculty\nlevel. All values are percentages. Note ‘0.1B’ indicates the number of model parameters in billions.\nGPT-3 is a few-shot model and not ﬁne-tuned, unlike the other models. GPT-Neo does best and\nattains approximately 4% strict accuracy on Introductory problems, and for these problems it passes\napproximately 15% of the test cases.\nAcross pretraining and ﬁne-tuning, we use the AdamW optimizer (Loshchilov and Hutter, 2019),\na batch size of 256, and a weight decay of 0.05. We ﬁne-tune for 10 epochs. We use DeepSpeed\nand its implementation of the ZeRO optimizer to reduce memory consumption while training large\nmodels (Rasley et al., 2020; Rajbhandari et al., 2020). Unless otherwise speciﬁed, we use the default\nHuggingFace generation parameters, except that we use beam search with a beam size of 5. Models\nare ﬁne-tuned on 8 A100 GPUs.\n4.2 Metrics\nTo obtain a comprehensive evaluation of code generation ability, we use the large bank of test cases\nand ground-truth solutions provided with APPS. Test cases allow for automatic evaluation, even\nthough the the space of possible programs can be combinatorially large. Therefore, unlike many\nother text generation tasks, manual analysis is not necessary. We aggregate the generated code’s\nperformance on test cases with two metrics, “test case average” and “strict accuracy.”\nTest Case Average. We compute the average fraction of test cases passed. Concretely, let the\nnumber of problems in the test set be P. For a given problem p, let the code generated to solve\nproblem p be denoted ⟨codep⟩, and set of test cases for problem p be {(xp,c, yp,c)}Cp\nc=1. Then the\ntest case average is\n1\nP\nP∑\np=1\n1\nCp\nCp∑\nc=1\n1{eval(⟨codep⟩, xp,c) =yp,c}.\nOftentimes, solutions can successfully pass a subset of the test cases but not cover every corner\ncase. This allows for less stringent model evaluation, as strict accuracy may currently obscure model\nimprovements.\nStrict Accuracy. Eventually, generated solutions should pass all test cases including corner cases.\nTo compute the strict accuracy which requires programs pass every test case, we run the code generated\nby the model on every test case of every problem. Strict accuracy is then computed by taking the\nnumber of solutions passing every test case divided by the total number of exercises. Using the\nnotation from before, we can write the strict accuracy as 1\nP\n∑P\np=1\n∏Cp\nc=1 1{eval(⟨codep⟩, xp,c) =\nyp,c}. Future research may only use strict accuracy when models become sufﬁciently capable.\n4.3 Model Performance Analysis\nQualitative Output Analysis. Models can sometimes generate correct or superﬁcially plausible\ncode. Figure 2 shows code generated by GPT-2 1.5B that passes all test cases. When models do\nnot pass the test cases, sometimes their generated code still appears plausible at ﬁrst glance. For\nexample, in Figure 3, we see that the 1.5B parameter model generates code that is related to the\nproblem statement and makes a plausible attempt to solve it.\nTest Case Evaluation. We show the main results in Table 2. We observe that models are able to\ngenerate code that passed some test cases, implying many generated programs are free of syntax errors\nand can successfully process inputs test cases to produce correct answers. Note that for Introductory\nquestions, GPT-Neo passes approximately 15% of the test cases. We visualize Test Case Average\n7\nIntroductory Interview Competition\nProblem Difficulty\n0\n2\n4\n6\n8\n10\n12\n14\n16Test Case Average (%)\nModel Test Case Averages\nGPT-3 (few-shot)\nGPT-2 (0.1B)\nGPT-Neo (2.7B)\nFigure 4: The average percentage of test cases\npassed increases with larger ﬁne-tuned models.\nIntroductory Interview Competition\nProblem Difficulty\n102\n101\nSolutions With Syntax Errors (%)\nSyntax Error Prevalence Is\nDecreasing Exponentially\nFigure 5: Syntax errors decrease exponentially\nwith ﬁne-tuning and increased model sizes. GPT-\nNeo 2.7B has very few syntax errors.\nresults in Figure 4. This demonstrates models are showing marked improvements on code generation\nand now starting to have traction on code generation.\nTop-1 Top-5\nTest Case Average 14.7% 19 .9%\nStrict Accuracy 3.9% 5 .5%\nTable 3: GPT-Neo 2.7B performance on in-\ntroductory problems using one generated\nprogram (Top-1) and the best of ﬁve gener-\nated programs (Top-5). Full results are in\nthe Supplementary Materials.\nPerformance can be further improved by sampling mul-\ntiple solutions and selecting the best. Here, we per-\nform beam search with beam width 5 and evaluate its\n5 beams, so that each model has ﬁve attempts to get\na problem correct rather than one. With this setup,\nGPT-Neo’s strict accuracy on Introductory problem\nthen exceeds 5%, as shown in Table 3. Our results in\nthe Supplementary Materials show that the top-5 test\ncase average GPT-2 0.1B is 10.75 while the top-1 test\ncase average of GPT-2 1.5B is 7.96. This highlights\nthat simply sampling multiple candidate solutions is a\npowerful way to markedly improve performance.\nOur results also provide us with information about the\nimportance of model choice. Evidently existing few-shot GPT-3 models are not necessarily better at\ncode generation than ﬁne-tuned models that are smaller by two orders of magnitude. Additionally,\nperformance improvement from GPT-2 1.5B to GPT-Neo 2.7B is larger than that from GPT-2 0.1B\nto GPT-2 1.5B. Potential causes of GPT-Neo’s better performance are that GPT-Neo is trained on\nmore code from GitHub, it has more parameters, or its architecture hyperparameters were chosen\nbetter. Memorization explaining all performance is an implausible explanation as performance tracks\nproblem difﬁculty; were models just memorizing, we would expect uniform performance across\ndifﬁculties. Since models still have large room for improvement, solving the APPS benchmark\nwithout unreasonable amounts of computational resources may require architectural or algorithmic\nimprovements.\nSyntax Errors. We now assess the frequency of syntax errors, errors that prevent the program from\nbeing interpreted including inconsistent spacing, unbalanced brackets, missing colons, and so on.\nSyntax errors are identiﬁed in our testing framework based on the heuristic of whether pyext is able\nto load the generated code as a Python module. For our purposes, this almost exclusively occurs for\nsyntax errors. We visualize the prevalence of syntax errors in Figure 5. While approximately 59% of\nGPT-3’s generated solutions for introductory problems have syntax errors, GPT-Neo syntax error\nfrequency is approximately 3%. Note that recent work such as Yasunaga and Liang (2020) create a\nseparate model to repair source code to ﬁx compilation issues, but our results suggest that such efforts\nmay be unnecessary in the future as syntax error frequency is sharply decreasing automatically.\n8\nBLEU. We ﬁnd that assessing model performance with BLEU is a poor substitute for evaluating\nwith test cases. To evaluate BLEU, we take the generated solution and compute its BLEU with each\nhuman-written solution for a given problem; we then record the highest BLEU score. Observe in\nFigure 6 that BLEU increases as problem sources become more difﬁcult, even though models actually\nperform worse on harder problems. Moreover, worse models can have similar or higher BLEU scores.\nFor example, GPT-2 0.1B has26.8, 29.7, and 30.2 as BLEU scores for introductory, interview, and\ncompetition problems, respectively. Meanwhile GPT-Neo 2.7B has 27.1, 29.1, and 29.3 as its BLEU\nscores, respectively. Hence BLEU wrongly suggests GPT-Neo is a worse model.\nIntroductory Interview Competition\nProblem Difficulty\n26\n27\n28\n29\n30\n31\n32BLEU\nBLEU\nTest Case Average (%)\n0\n2\n4\n6\n8\n10\n12\n14\n16\nTest Case Average (%)\nBLEU Does Not Track Performance Well\nFigure 6: BLEU scores for GPT-Neo 2.7B increase\nwith difﬁculty level and are anticorrelated with a\ngold-standard accuracy metric.\nEvaluating GPT-3. We evaluate GPT-3 175B\non APPS in a few-shot setting. A separate\nprompt is used for standard input and call-based\nquestions, and each prompt includes instruction\ntext along with two example questions and solu-\ntions from the corresponding question type. We\nﬁnd that GPT-3 only solves 3 problems out of\n5,000: two introductory problems and one inter-\nview problem. The two introductory problems\nare simple interpretation tasks, such as imple-\nmenting a speciﬁed algebraic expression. The\ninterview problem requires higher-level thinking\nthat suggests nontrivial reasoning. However, it\nis possible that GPT-3 memorized the solution\nduring pretraining, or that it took a lucky guess\nbased on heuristics in the question. One poten-\ntial factor in GPT-3’s poor performance is that\nit handles syntax poorly. Namely, we observed\ncases where improper formatting of otherwise\nfunctioning code causes a syntax error. For spe-\nciﬁc examples and more details, see the Supplementary Materials.\nEvaluations on Larger Models. Since the public release of APPS, several others have trained\neven larger models on APPS than we evaluate here. OpenAI Codex is a 12B parameter Transformer\nlanguage model pre-trained on large quantities of public code and comments. Chen et al. (2021)\nevaluate Codex on APPS under various conﬁgurations and achieve top-1 and top-5 accuracy on\nintroductory problems of 4.14% and 9.65% respectively, close to double the top-5 accuracy of\nGPT-Neo 2.7B. Furthermore, by scaling up to a top-1000 evaluation they obtain 25% accuracy.\nThis demonstrates that larger models trained speciﬁcally for code generation can improve APPS\nperformance even further, but are still far from solving the task.\n5 Conclusion\nWe introduced APPS, a benchmark of 10,000 Python programming problems. Unlike prior work\nthat focused on pseudocode to code generation or translation between programming languages, our\nbenchmark measures how well language models can generate python code given natural language\nspeciﬁcations. By performing extensive quality assurance and including hundreds of thousands of\ntest cases and ground-truth solutions across different difﬁculty levels, we created a comprehensive\nand rigorous testbed for evaluating models. We assessed state-of-the-art generative models on our\nbenchmark and found that overall performance was low. However, the prevalence of syntax errors\ndecreased exponentially as models improved, and recent models such as GPT-Neo solved over5% of\nour introductory problems. As models become more competent at code generation, it is important to\nhave a proxy for tracking this capability which could one day result in automation or malicious code\ngeneration. The APPS benchmark can provide an important measure for tracking upstream program\nsynthesis advancements.\n9\nReferences\nMiltiadis Allamanis and Charles Sutton. Mining source code repositories at massive scale using\nlanguage modeling. In 2013 10th Working Conference on Mining Software Repositories (MSR),\npages 207–216. IEEE, 2013.\nRajeev Alur, Dana Fisman, Saswat Padhi, Rishabh Singh, and Abhishek Udupa. Sygus-comp 2018:\nResults and analysis. SYNT, 2018.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language, 2019.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autore-\ngressive Language Modeling with Mesh-Tensorﬂow, March 2021. URL https://doi.org/\n10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.\nT. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, G. Krüger,\nT. Henighan, R. Child, Aditya Ramesh, D. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\nHesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J. Clark, Christopher\nBerner, Sam McCandlish, A. Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. ArXiv, abs/2005.14165, 2020.\nJonathon Cai, Richard Shin, and D. Song. Making neural programming architectures generalize via\nrecursion. 2017.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,\nYura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nAditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, and\nSubhajit Roy. Program synthesis using natural language. In Proceedings of the 38th International\nConference on Software Engineering, pages 345–356, 2016.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027, 2020.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daumeé III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010,\n2018.\nSumit Gulwani, Oleksandr Polozov, and R. Singh. Program synthesis. Found. Trends Program. Lang.,\n4:1–119, 2017.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning AI with shared human values. Proceedings of the International Conference\non Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the International\nConference on Learning Representations (ICLR), 2021b.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874, 2021c.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos qa: Machine reading\ncomprehension with contextual commonsense reasoning, 2019.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in\nprogrammatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics.\nSumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy S\nLiang. Spoc: Search-based pseudocode to code. In Advances in Neural Information Processing\nSystems, volume 32, 2019.\nMarie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. Unsupervised\ntranslation of programming languages. arXiv preprint arXiv:2006.03511, 2020.\n10\nW. Ling, P. Blunsom, Edward Grefenstette, K. Hermann, Tomás Kociský, Fumin Wang, and A. Senior.\nLatent predictor networks for code generation. ArXiv, abs/1603.06744, 2016.\nHui Liu, Mingzhu Shen, Jiaqi Zhu, Nan Niu, Ge Li, and Lu Zhang. Deep learning based program\ngeneration from requirements text: Are we there yet? IEEE Transactions on Software Engineering,\n2020a.\nJ. Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. LogiQA: A challenge\ndataset for machine reading comprehension with logical reasoning. In IJCAI, 2020b.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, A. Blanco, C. Clément, Dawn\nDrain, Daxin Jiang, Duyu Tang, Ge Li, L. Zhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, N. Duan, N. Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\nCodexglue: A machine learning benchmark dataset for code understanding and generation. 2021.\nYusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and\nSatoshi Nakamura. Learning to generate pseudo-code from source code using statistical machine\ntranslation. In International Conference on Automated Software Engineering (ASE), 2015.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting of the Association\nfor Computational Linguistics, pages 311–318, 2002.\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\nArXiv, abs/2009.03393, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models, 2020.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza-\ntions enable training deep learning models with over 100 billion parameters. Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2020.\nVeselin Raychev, Pavol Bielik, and Martin T. Vechev. Probabilistic model for code with decision\ntrees. Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented\nProgramming, Systems, Languages, and Applications, 2016.\nMohammad Raza, Sumit Gulwani, and Natasa Milic-Frayling. Compositional program synthesis\nfrom natural language and examples. In IJCAI, 2015.\nShuo Ren, Daya Guo, Shuai Lu, L. Zhou, Shujie Liu, Duyu Tang, M. Zhou, A. Blanco, and S. Ma.\nCodebleu: a method for automatic evaluation of code synthesis. ArXiv, abs/2009.10297, 2020.\nL. Tang and R. Mooney. Using multiple clause constructors in inductive logic programming for\nsemantic parsing. In ECML, 2001.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nL. Kaiser, and Illia Polosukhin. Attention is all you need. ArXiv, abs/1706.03762, 2017.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS, 2019a.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS, 2019b.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. Rat-\nsql: Relation-aware schema encoding and linking for text-to-sql parsers. arXiv preprint\narXiv:1911.04942, 2019c.\nMichihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic\nfeedback. ArXiv, abs/2005.10636, 2020.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex\nand cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.\n11\nMaksym Zavershynskyi, A. Skidanov, and Illia Polosukhin. Naps: Natural program synthesis dataset.\n2nd Workshop on Neural Abstract Machines and Program Induction, 2018.\nJ. Zelle and R. Mooney. Learning to parse database queries using inductive logic programming. In\nAAAI/IAAI, Vol. 2, 1996.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally ﬁnish your sentence?, 2019.\n12\nHearthstone Django NAPS APPS\nProgramming Language Python Python UAST Python\nTest Cases \u0015 \u0015\nNumber of Programs 665 18,805 17,477 232,421\nLines per Program (Avg.) 7.7 1 21.7 18.0\nNumber of Exercises 665 18,805 2,231 10,000\nText Input Card Text Comment Pseudocode Problem Descriptions\nTable 4: Further comparisons of APPS with previous datasets.\nTop-5 Test Case Average Top-5 Strict Accuracy\nModel Introductory Interview Competitive Average Introductory Interview Competition Average\nGPT-2 0.1B 13.81 10.97 7.03 10.75 2.70 0.73 0.00 1.02\nGPT-2 1.5B 16.86 13.84 9.01 13.48 3.60 1.03 0.00 1.34\nGPT-Neo 2.7B 19.89 13.19 9.90 13.87 5.50 0.80 0.00 1.58\nTable 5: Top-5 performance of GPT-2 models and GPT-Neo. Taking the best of ﬁve candidate\nsolutions markedly improves performance.\nA Auxiliary Dataset Information\nLegal Compliance. In APPS, we scrape question text, ground-truth solutions, and test cases from\nvarious coding challenge websites. These websites are AtCoder, CodeChef, Codeforces, Codewars,\nHackerRank, Kattis, and LeetCode. In all cases, we only scrape public-facing data. For instance, we\navoid scraping data from paywalled portions of sites. In the case of Kattis, all problems we scrape\nare under the CC BY-SA 3.0 license (https://creativecommons.org/licenses/by-sa/3.0/). For other\nwebsites, some content may be copyrighted. In these cases, we abide by Fair Use §107: “the fair\nuse of a copyrighted work, including such use by ... scholarship, or research, is not an infringement\nof copyright”, where fair use is determined by “the purpose and character of the use, including\nwhether such use is of a commercial nature or is for nonproﬁt educational purposes”, “the amount\nand substantiality of the portion used in relation to the copyrighted work as a whole”, and “the effect\nof the use upon the potential market for or value of the copyrighted work.” The APPS dataset is\nnoncommercial and is likely to have no effect on the value of the original problems. Moreover, for all\nproblem sources, we only scrape a fraction of the available problems and ground-truth solutions.\nRegarding international copyright laws, the websites that we scrape from are based in the United\nStates, Japan, India, and Russia, all of which are contracting parties to the WIPO Copyright Treaty.\nIn the United States, the WIPO Copyright Treaty is implemented by the Digital Millenium Copyright\nAct (DMCA). Since APPS was made in the United States, the DMCA is the relevant legislation that\nwe must comply with. Notably, DMCA §1201 states, “No person shall circumvent a technological\nmeasure that effectively controls access to a work protected under this title.” We do not circumvent\naccess controls when creating APPS and hence abide by §1201. Fair Use extends to content protected\nby the DMCA, for which we refer readers to the previous paragraph.\nAlthough GDPR only applies in the European Union, some of the ground-truth solutions in APPS\nmay have been written by EU citizens. GDPR is chieﬂy concerned with the protection of personal\ndata gathered by entities engaging in economic activity. The only personally linked information in\nAPPS is the problem solutions written by individuals and published under aliases to public websites.\nIn some cases, these solutions contain identifying information in comments, which we remove to\npreserve privacy. We comply with GDPR, because our processed solutions remove identiﬁers, and\nwe are compliant because we collect the data for academic research purposes.\nAuthor Statement and License. We bear all responsibility in case of violation of rights. The\nAPPS data is licensed under CC BY-SA 3.0 in accordance with the Kattis problem licenses and the\nShareAlike terms. Our code is open sourced under the MIT license.\n13\nB Datasheets\nWe follow the recommendations of Gebru et al. (2018) and provide a datasheet for the ETHICS\ndataset in this section.\nB.1 Motivation\nFor what purpose was the dataset created? Was there a speciﬁc task in mind? Was there\na speciﬁc gap that needed to be ﬁlled? Please provide a description. The APPS dataset was\ncreated to track the progress of code generation models on the task of generating arbitrary Python code\nfrom complex natural language speciﬁcations, a challenging setting that had no rigorous benchmark\nbefore our work.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)? Refer to the main document.\nWho funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number. There is no associated grant.\nAny other comments? No.\nB.2 Composition\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description. The instances\nare coding challenge problems posed in natural language, each of which consists of question text,\nground-truth solutions, and test cases. Please refer to the main document for more detail.\nHow many instances are there in total (of each type, if appropriate)? APPS contains 10,000\nproblems, 232,421 ground-truth solutions, and 131,777 test cases.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? If the dataset is a sample, then what is the larger set? Is the\nsample representative of the larger set (e.g., geographic coverage)? If so, please describe how\nthis representativeness was validated/veriﬁed. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were\nwithheld or unavailable). APPS contains a subset of all possible test cases for its problems. These\ntest cases are written by problem designers to cover important functionality.\nWhat data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or fea-\ntures? In either case, please provide a description. Each instance consists of text and numerical\ndata.\nIs there a label or target associated with each instance? If so, please provide a description.\nEach instance is associated with test cases, which provide a ground-truth signal for functional\ncorrectness.\nIs any information missing from individual instances? If so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not\ninclude intentionally removed information, but might include, e.g., redacted text. No.\nAre relationships between individual instances made explicit (e.g., users’ movie ratings, social\nnetwork links)? If so, please describe how these relationships are made explicit. We remove\nduplicate or near-duplicate problems from APPS.\n14\nAre there recommended data splits (e.g., training, development/validation, testing)? If so,\nplease provide a description of these splits, explaining the rationale behind them. We pro-\nvide a training and test split. The splits were optimized for increasing the number of test cases in the\ntest split while maintaining a ﬁxed number of problems from each difﬁculty.\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a\ndescription. See Section 3 in the main paper for a discussion of test case quality.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? The dataset is self-contained.\nDoes the dataset contain data that might be considered conﬁdential (e.g., data that is protected\nby legal privilege or by doctor-patient conﬁdentiality, data that includes the content of individ-\nuals’ non-public communications)? If so, please provide a description. No.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? If so, please describe why. Unknown.\nDoes the dataset relate to people? If not, you may skip the remaining questions in this section.\nYes.\nDoes the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how\nthese subpopulations are identiﬁed and provide a description of their respective distributions\nwithin the dataset. No.\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or in-\ndirectly (i.e., in combination with other data) from the dataset? If so, please describe how\nNo.\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that re-\nveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union\nmemberships, or locations; ﬁnancial or health data; biometric or genetic data; forms of govern-\nment identiﬁcation, such as social security numbers; criminal history)? If so, please provide a\ndescription. No.\nAny other comments? No.\nB.3 Collection Process\nHow was the data associated with each instance acquired? Was the data directly observable\n(e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly in-\nferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or lan-\nguage)? If data was reported by subjects or indirectly inferred/derived from other data, was\nthe data validated/veriﬁed? If so, please describe how. All data was collected by scraping\nproblems from coding challenge websites, such as Codewars, AtCoder and Kattis.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\nsensor, manual human curation, software program, software API)? How were these mecha-\nnisms or procedures validated? We used off-the-shelf and custom-built scrapers. We manually\nchecked whether scraped data matched text on the websites.\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with speciﬁc sampling probabilities)? Some problems we scraped were left out of\nAPPS for various reasons, e.g. they required images to solve, they lacked ground-truth solutions and\ntest cases, or they were duplicate problems.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)? All data was\ncollected by undergraduate and graduate student authors on the paper.\n15\nOver what timeframe was the data collected? Does this timeframe match the creation time-\nframe of the data associated with the instances (e.g., recent crawl of old news articles)? If\nnot, please describe the timeframe in which the data associated with the instances was created.\nData was collected from late 2020 to early 2021 and reﬁned for six months.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so,\nplease provide a description of these review processes, including the outcomes, as well as a link\nor other access point to any supporting documentation No.\nDoes the dataset relate to people? If not, you may skip the remainder of the questions in this\nsection. Yes.\nDid you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)? We scraped data via websites where individuals had publicly\nposted problem solutions.\nWere the individuals in question notiﬁed about the data collection? If so, please describe\n(or show with screenshots or other information) how notice was provided, and provide a link\nor other access point to, or otherwise reproduce, the exact language of the notiﬁcation itself.\nUsers who posted on the Internet were not notiﬁed of our collection, because their examples were\nposted publicly.\nDid the individuals in question consent to the collection and use of their data? If so, please\ndescribe (or show with screenshots or other information) how consent was requested and pro-\nvided, and provide a link or other access point to, or otherwise reproduce, the exact language\nto which the individuals consented. N/A\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses? If so, please provide a description, as well as a\nlink or other access point to the mechanism (if appropriate). N/A\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\nprotection impact analysis) been conducted? If so, please provide a description of this analysis,\nincluding the outcomes, as well as a link or other access point to any supporting documentation.\nNo.\nAny other comments? No.\nB.4 Preprocessing/Cleaning/Labeling\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,\ntokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)? If so, please provide a description. If not, you may skip the remainder of\nthe questions in this section. Yes, as described in Section 3 of the main paper.\nWas the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support\nunanticipated future uses)? If so, please provide a link or other access point to the “raw” data.\nNo.\nIs the software used to preprocess/clean/label the instances available? If so, please provide a\nlink or other access point. Not at this time.\nAny other comments? No.\nB.5 Uses\nHas the dataset been used for any tasks already? If so, please provide a description. Yes, see\nthe main paper.\n16\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please\nprovide a link or other access point. No.\nWhat (other) tasks could the dataset be used for? N/A\nIs there anything about the composition of the dataset or the way it was collected and prepro-\ncessed/cleaned/labeled that might impact future uses? For example, is there anything that a\nfuture user might need to know to avoid uses that could result in unfair treatment of individ-\nuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g.,\nﬁnancial harms, legal risks) If so, please provide a description. Is there anything a future user\ncould do to mitigate these undesirable harms? We describe how our data collection is legally\ncompliant in Appendix A.\nAre there tasks for which the dataset should not be used? If so, please provide a description.\nN/A\nAny other comments? No.\nB.6 Distribution\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? If so, please provide a description.\nYes, the dataset will be publicly distributed.\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\nDoes the dataset have a digital object identiﬁer (DOI)? The dataset is available at\nhttps://github.com/hendrycks/apps.\nWhen will the dataset be distributed? The dataset is currently available.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms\nor ToU, as well as any fees associated with these restrictions. The code for our experimental\nframework is distributed under an MIT license. Where applicable,\nHave any third parties imposed IP-based or other restrictions on the data associated with the\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any relevant licensing terms, as well as any fees associated with these\nrestrictions. In cases where websites that we scrape data from have copyright policies, we abide\nby Fair Use according to §107, and we comply with GDPR even though all our problem sources with\nground-truth solutions are based in the US. See Appendix A for details.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any supporting documentation. No.\nAny other comments? No.\nB.7 Maintenance\nWho is supporting/hosting/maintaining the dataset? Refer to the main document.\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)? Refer\nto the main document.\nIs there an erratum? If so, please provide a link or other access point. Not at this time.\n17\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete in-\nstances)? If so, please describe how often, by whom, and how updates will be communicated\nto users (e.g., mailing list, GitHub)? We plan to update the dataset with an additional JSON of\ntest cases present in the question text for each problem. This will be available through GitHub.\nIf the dataset relates to people, are there applicable limits on the retention of the data associ-\nated with the instances (e.g., were individuals in question told that their data would be retained\nfor a ﬁxed period of time and then deleted)? If so, please describe these limits and explain how\nthey will be enforced No.\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users. N/A\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/ver-\niﬁed? If so, please describe how. If not, why not? Is there a process for communicating/dis-\ntributing these contributions to other users? If so, please provide a description. Our dataset\ncould be extended with additional problems that follow the formatting of existing problems.\nAny other comments? No.\nC Additional Dataset Information\nExpanded Dataset Comparisons. We compared to several datasets in the (Kulal et al., 2019;\nYu et al., 2018; Raychev et al., 2016; Iyer et al., 2018; Lu et al., 2021) main paper. We continue\nthe comparisons below. Ling et al. (2016) introduce datasets based on Hearthstone and Magic the\nGathering card games for code generation. Oda et al. (2015) provide a language-to-code dataset\nusing simple code comments. Zavershynskyi et al. (2018) introduce the NAPS dataset for converting\npseudocode to code, obtained by crowdsourcing low-level descriptions of programming exercises,\nand apply machine translation techniques to the problem. Recent anecdotal posts on social media have\ndemonstrated that modern Transformers can in some instances generate JSX code adhering to user\nrequests, but our work provides precision to the discussion through quantitative evaluation. Allamanis\nand Sutton (2013) introduce the GitHub Java Corpus used for performing language modeling on\nJava code. Liu et al. (2020a) do a smaller-scale analysis of code generation but with their limited\nlanguage-speciﬁc training data models “fail to pass even a single predeﬁned test case” on their 300\ntest problems, while with our large training set and test set, trained models can pass tens of thousands\nof test cases. Zelle and Mooney (1996) and Tang and Mooney (2001) precedes Yu et al. (2018) by\nalso facilitating the synthesis of database queries, though more recent program synthesis works such\nas Wang et al. (2019c) use Spider from Yu et al. (2018).\nTable 4 compares APPS to Hearthstone (Ling et al., 2016), Django (Oda et al., 2015), and Zaver-\nshynskyi et al. (2018). ‘Number of Programs’ refers to the number of human-written programs or\nfunctions in the dataset, and ‘Number of Exercises’ refers to the number of tasks that the network\nmust solve. These numbers can differ in datasets such as APPS with multiple human-written solutions\nper exercise.\nExcluded Keywords. In creating the GitHub pretraining dataset, we exclude the following key-\nwords to prevent overlap with coding challenge questions similar to those in APPS: ‘atcoder’,\n‘coderbyte’, ‘leetcode’, ‘codeforces’, ‘codewars’, ‘hackerrank’, ‘topcoder’, ‘codechef’, ‘checkio’,\n‘HackerEarth’, ‘Programmr’, ‘Exercism’, ‘Codier’, ‘PyBites’, ‘Tynker’, ‘CodinGame’, ‘CodeCom-\nbat’, ‘usaco’, ‘IOI’, ‘UV A’, ‘ICFP’, ‘EPIJudge’, ‘SPOJ’, ‘UVaOJ’, ‘judge’, ‘interview’, ‘solution’,\n‘coding’, ‘code’, ‘problem’, ‘exercise’, ‘challenge’, ‘algo’, ‘practice’, ‘competitive’, ‘program’.\nD Additional Results\nTop-5 Performance. Rather than allowing models to generate just one potential solution, we let\nmodels generate ﬁve and we choose the best performing solution. Full top-5 performance results are\nin Table 5.\n18\nProblem\nMahmoud and Ehab play a game called the even-odd game. Ehab chooses his favorite integer n and\nthen they take turns, starting from Mahmoud. In each player’s turn, he has to choose an integer a and\nsubtract it from n such that: 1 ≤a ≤n. If it’s Mahmoud’s turn,a has to be even, but if it’s Ehab’s\nturn, a has to be odd.\nIf the current player can’t choose any number satisfying the conditions, he loses. Can you determine\nthe winner if they both play optimally?\n—–Input—–\nThe only line contains an integer n (1 ≤n ≤109), the number at the beginning of the game.\n—–Output—–\nOutput “Mahmoud” (without quotes) if Mahmoud wins and “Ehab” (without quotes) otherwise.\nModel Output\nn = int(input())\nif n % 2 == 0:\nprint('Mahmoud')\nelse:\nprint('Ehab')\nFigure 7: The sole interview problem solved by GPT-3. Legitimately solving this problem requires\nnontrivial reasoning about the proposed game’s branching possibilities.\na, b = int(input()), int(input())\nif a == b:\nprint(''.join(map(str, [a, b])))\nelse:\nprint(''.join(map(str, [a, b])))\nFigure 8: Oftentimes, GPT-3 generates outputs with incorrect syntax, such as the above code. Even\nthough the code is reasonable, bad formatting of the if-else block causes a syntax error.\nGPT-3. We evaluate GPT-3 175B on APPS in a few-shot setting. A separate prompt is used for\nstandard input and call-based questions, and each prompt includes instruction text along with two\nexample questions and solutions from the corresponding question type. The solutions we select\nfor each prompt are the shortest solutions passing all test cases for the corresponding problems.\nIn preliminary experiments, we found that the original GPT-3 175B did not perform well, and the\ninstruct series was necessary for reasonable generations. Inference on the 5,000 problems in the\nAPPS test set is possible for approximately $500.\nWe ﬁnd that GPT-3 only solves3 problems out of 5,000: two introductory problems and one interview\nproblem. The two introductory problems are simple interpretation tasks, such as implementing a\nspeciﬁed algebraic expression. The interview problem requires higher-level thinking and is show\nin Figure 7. Legitimately solving this problem would suggest nontrivial reasoning. However, it is\npossible that GPT-3 memorized the solution during pretraining, or that it took a lucky guess based on\nheuristics in the question.\nWe also ﬁnd that GPT-3 tends to handle syntax poorly. In Figure 8, we see a generation with improper\nformatting of an if-else block, which causes a syntax error. By contrast, GPT-2 0.1B has fewer syntax\nerrors with three orders of magnitude fewer parameters, and GPT-Neo 2.7B has almost no syntax\nerrors. These two models are ﬁne-tuned on GitHub and the APPS training set.\nMore Generated Code Examples. Additional examples of generated code can be found in Fig-\nure 9, Figure 10, and Figure 11.\n19\nProblem\nGiven is a directed graph G with N vertices and M edges. The vertices are numbered 1 to N, and\nthe i-th edge is directed from Vertex A_i to Vertex B_i. It is guaranteed that the graph contains no\nself-loops or multiple edges. Determine whether there exists an induced subgraph (see Notes) of G\nsuch that the in-degree and out-degree of every vertex are both 1. If the answer is yes, show one such\nsubgraph. Here the null graph is not considered as a subgraph.\nModel Output\nimport sys\nsys.setrecursionlimit(10 ** 7)\ninput = sys.stdin.readline\nf_inf = float('inf')\nmod = 10 ** 9 + 7\ndef resolve():\nn, m = map(int, input().split())\nedge = [[] for _ in range(n)]\nfor _ in range(m):\na, b = map(int, input().split())\nedge[a - 1].append(b - 1)\nedge[b - 1].append(a - 1)\ndef dfs(u):\nstack = [u]\nseen = [False] * n\nwhile stack:\nu = stack.pop()\nif seen[u]:\ncontinue\nseen[u] = True\nfor v in edge[u]:\nif seen[v]:\ncontinue\nseen[v] = True\nstack.append(v)\nres = None\nfor i in range(n):\nif not seen[i]:\nseen[i] = True\nres = dfs(i)\nif not res:\nprint(-1)\nelse:\nprint(res)\nif __name__ == '__main__':\nresolve()\nFigure 9: An example generated by GPT-2 1.5B. The model can sometimes compose function calls.\n20\nProblem\nGiven is a tree G with N vertices. The vertices are numbered 1 through N, and the i-th edge connects\nVertex a_i and Vertex b_i. Consider painting the edges in G with some number of colors. We want to\npaint them so that, for each vertex, the colors of the edges incident to that vertex are all different.\nAmong the colorings satisfying the condition above, construct one that uses the minimum number of\ncolors.\nModel Output\nimport sys\nfrom collections import deque, defaultdict, Counter\nfrom itertools import accumulate, product, permutations, combinations\nfrom operator import itemgetter\nfrom bisect import bisect_left, bisect_right\nfrom heapq import heappop, heappush\nfrom math import ceil, floor, sqrt, gcd, inf\nfrom copy import deepcopy\nimport numpy as np\nimport scipy as sp\nINF = inf\nMOD = 1000000007\nn = int(input())\nab = [list(map(int, input().split())) for i in range(n - 1)]\ngraph = [[] for i in range(n + 1)]\nfor a, b in ab:\ngraph[a].append(b)\ngraph[b].append(a)\nvisited = [False] * (n + 1)\ncolor = [0] * (n + 1)\ncolor[0] = 1\nstack = [1]\nwhile stack:\nx = stack.pop()\nfor y in graph[x]:\nif visited[y]:\ncontinue\nvisited[y] = True\ncolor[y] = color[x] + 1\nstack.append(y)\nans = min(color)\nprint(*ans, sep=\"\\n\")\nFigure 10: An additional generated example. Here, the model output has many import statements.\n21\nProblem\nThere is a special square room with mirrors on each of the four walls. Except for the southwest corner,\nthere are receptors on each of the remaining corners, numbered 0, 1, and 2. The square room has\nwalls of length p, and a laser ray from the southwest corner ﬁrst meets the east wall at a distance\nq from the 0th receptor. Return the number of the receptor that the ray meets ﬁrst. (It is guaranteed\nthat the ray will meet a receptor eventually.)\nExample 1:\nInput: p = 2, q = 1\nOutput: 2\nExplanation: The ray meets receptor 2 the ﬁrst time it gets reﬂected back to the left wall.\nStarter Code\nclass Solution:\ndef mirrorReflection(self, p: int, q: int) -> int:\nModel Output\nclass Solution:\ndef mirrorReflection(self, p: int, q: int) -> int:\nself.p = p\nself.q = q\nself.mirrors = [(0, 1), (0, -1), (1, 0), (-1, 0)]\nreturn self.mirrors[self.p - 1][self.q - 1]\nFigure 11: An example using the call-based format with some starter code.\n22",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7914109230041504
    },
    {
      "name": "Python (programming language)",
      "score": 0.6702026128768921
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6502725481987
    },
    {
      "name": "Machine learning",
      "score": 0.5517084002494812
    },
    {
      "name": "Source lines of code",
      "score": 0.5255060195922852
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5130835771560669
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.4687402844429016
    },
    {
      "name": "Source code",
      "score": 0.45153433084487915
    },
    {
      "name": "Code generation",
      "score": 0.4227043092250824
    },
    {
      "name": "Programming language",
      "score": 0.41736841201782227
    },
    {
      "name": "Software engineering",
      "score": 0.3780982196331024
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32374122738838196
    },
    {
      "name": "Software",
      "score": 0.3176589906215668
    },
    {
      "name": "Computer security",
      "score": 0.12069138884544373
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "cited_by": 135
}