{
  "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax",
  "url": "https://openalex.org/W3203609549",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Song, Jeong-geun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3164230001",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2757910899",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3164540605",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3212756788"
  ],
  "abstract": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
  "full_text": "UFO-ViT: High Performance Linear Vision Transformer without Softmax\nJeong-geun Song\nKakao Enterprise\npo.ai@kakaoenterprise.com\nPREPRINT\nAbstract\nVision transformers have become one of the most impor-\ntant models for computer vision tasks. Although they out-\nperform prior works, they require heavy computational re-\nsources on a scale that is quadratic to N. This is a major\ndrawback of the traditional self- attention (SA) algorithm.\nHere, we propose the Unit Force Operated Vision Trans-\nformer (UFO-ViT), a novel SA mechanism that has linear\ncomplexity. The main approach of this work is to elim-\ninate nonlinearity from the original SA. We factorize the\nmatrix multiplication of the SA mechanism without compli-\ncated linear approximation. By modifying only a few lines\nof code from the original SA, the proposed models outper-\nform most transformer-based models on image classiﬁca-\ntion and dense prediction tasks on most capacity regimes.\n1. Introduction\nAs early successes in natural language processing\n(NLP), several studies based on transformers [11, 34, 40,\n29, 19, 15, 13] have shown impressive results in vision\ntasks. Recent studies have shown that transformer-based\narchitectures renew the state of the art across a wide range\nof subject areas, including image classiﬁcation[11, 34], ob-\nject detection and semantic segmentation[2, 25, 39, 47, 49],\nand generative models[22, 14, 12].\nDespite its great successes, the original self-attention\n(SA) mechanism has O(N2) time and memory complex-\nity due to the matrix multiplication of σ(QKT ) ∈RN×N\nand V. This is one of the well-known drawbacks of tradi-\ntional transformers. For vision tasks, N is proportional to\nthe input resolution. This means that SA consumes 16 times\nthe computational resources if the width and height of the\ninput image are doubled.\nHere, we propose a new model that implements an al-\nternative novel SA mechanism to avoid this drawback. It\nis called the Unit Force Operated Vision Transformer, or\nUFO-ViT. Our key method replaces softmax nonlinearity\nwith a simple L2-norm. Using the associative law of matrix\nmultiplication, our new SA algorithm requires much less\ncomputational resources than the original SA.\nFigure 1: Top-1 accuracy vs. Model capacity. Compar-\nison of ImageNet1k top-1 accuracy of various models ac-\ncording to model capacity. Our models show the best results\nat the same parameter sizes compared to the other models.\nAlthough similar approaches have been proposed by\nPerformer[19] and Efﬁcient Attention[28], El et al. have\nshown that these approaches cause performance degrada-\ntion for XCiT[13]. Unlike the previous results, UFO-ViT\nachieves higher or competitive results on the benchmark\ndatasets of vision tasks compared with state-of-the-art mod-\nels. This is detailed in Section 4, including the results on the\nImageNet1k[10] and COCO benchmarks[24]. These results\nwere obtained without any extra-large datasets, such as Im-\nageNet21k, or distilled knowledge from another model.\nThe main contributions in this paper are summarized as\nfollows:\n1\narXiv:2109.14382v2  [cs.CV]  4 Nov 2021\n1. We propose a novel constraint scheme, XNorm, that\ngenerates a unit hypersphere to extract relational features.\nAs detailed in Section 3, this scheme prevents SA from be-\ning dependent on initialization. Furthermore, it eliminates\nnon-linearity from SA by replacing the softmax function.\nOur module has O (N) complexity, and it handles high-\nresolution inputs efﬁciently.\n2. We demonstrate that UFO-ViT can be adopted for\ngeneral purposes. Our models are tested on both image\nclassiﬁcation and dense prediction tasks. While our pro-\nposed method has linear complexity, the UFO-ViT mod-\nels outperform most of the state-of-the-art models based on\ntransformers at lower capacity and FLOPS. In particular,\nour models perform well in lightweight regimes.\n3. We empirically show that UFO-ViT models have\nfaster inference speed and require less GPU memory. For\nvarious resolutions, the required computational resources\nwere not signiﬁcantly increased. Also, the weights used in\nour model are irrelevant to the resolution. This is a use-\nful characteristic for dense prediction tasks such as object\ndetection and semantic segmentation. Most of dense pre-\ndiction tasks require a higher resolution than does the pre-\ntraining stage, that is, MLP-based structures[32, 33] require\nadditional post-processing to adjust to various resolutions.\n2. Related Works\nVision transformers. Dosovitskiy et al.[11] proposed a\nvision transformer (ViT), which showed that transformer-\nbased models could be used for vision tasks. After the\nachievements of ViT, DeiT[34] introduced data-efﬁcient\ntraining strategies for vision transformers with detailed ab-\nlation studies. They solved the ViT data efﬁciency prob-\nlem successfully, and most of the current transformer-based\nmodels follow their schemes.\nIn further research, various architectures based on trans-\nformer variants have been presented. Touvron et al.[35]\nproposed two simple types of modules. One is the class\nattention module, which is the additional SA layer used to\nextract class information. These layers help the model ag-\ngregate features from the last outputs. The other is the Lay-\nerScale modules. These are learnable parameters for scal-\ning residual connections. This prevents larger models from\nbeing overﬁtted. A simple variant of the LayerScale was\npresented at ResMLP[33]. It is called Afﬁne and Layer-\nScale with a bias term. While MLP-based models are irrel-\nevant to our model, we apply Afﬁne modules to our model\nas scalers.\nSpatial features. As CNN-based studies have shown,\nhuman heuristics for extracting spatial structures work well\nfor vision tasks. Several studies have suggested their own\nmethods instead of adopting prior works. Liu et al.[25] pro-\nposed a shifting window and patch merging. This gener-\nates local attention using two types of windows: Normal\nwindows and shifted windows. At the end of each stage,\nthis method merges the patches to preserve large receptive\nﬁelds without heavy computation. Tokens-to-token ViT, in-\ntroduced by Yuan et al.[46], aims to achieve a similar objec-\ntive through different approaches. They presented a method\nof overlapping tokens to locally correlate patches. They\ndid not use additional methods to reduce the computation,\nexcept when using small channels. Zhang et al.[48] pro-\nposed a nested SA structure. They split patches into several\nblocks and applied local attention merging to the blocks\nusing convolutional pooling layers. Chen et al. used fea-\ntures from small and large patches, implementing their own\ncross-attention structure[3]. In a similar vein, an alterna-\ntive SA method was introduced by Chu et al. [8]. Their\nmodel used global attention and local attention to handle\nboth types of spatial features.\nHybrid architectures. Various methods for integrat-\ning convolutional layers[19, 39, 15, 13, 42, 16] instead of\nsearching for new spatial structures have been introduced.\nLeViT, designed by Graham et al.[15], applies multi-stage\nnetworks to transformers using SA with convolution and\npooling layers. Xiao et al.[42] found that replacing linear\npatch embedding layers with convolutions helps transform-\ners better capture low-level features. This is very similar to\nthe stemming stage of existing CNN networks. El-Nouby\net al. introduced local patch interactions in XCiT[13]. With\ntwo depthwise convolutions[6] added after XCA, XCiT\nachieved better performance.\nOur models are generally inspired by the intrinsic op-\ntimization strategies that XCiT[13] introduced, while we\npresent our own SA method.\nEfﬁcient self-attention. Instead of architectural strate-\ngies, many approaches have been proposed to solve the\nO(N2) problem of the SA mechanism. They are summa-\nrized in several categories: those that use their own spa-\ntial patterns[20, 5, 30], those that use various low-rank fac-\ntorization methods [7, 28, 38], those that use linear ap-\nproximation by sampling important tokens[23, 44], and\nthose that use cross-covariance matrices instead of Gram\nmatrices[13]. Although detailed methods are quite differ-\nent, our UFO scheme is mainly related to low-rank factor-\nization methods.\n3. Methods\nThe structure of our model is shown in Figure 2. It is\na mixture of convolutional layers, a UFO module, and a\nsimple feed-forward MLP layer. In this section, we discuss\nhow our proposed method can replace the softmax function\nand ensure linear complexity.\n2\nFigure 2: Overview of UFO-ViT module. Note that afﬁne\nlayers[33] are following each module.\nFigure 3: UFO module.\n3.1. Basic Structure\nFor an input x ∈RN×C, the traditional SA mechanism\nis formulated as follows:\nA(x) = σ(QKT /\n√\ndk)V (1)\nQ= xWQ,K = xWK,V = xWV (2)\nwhere Adenotes the attention operator.\nσ(QKT )V can be decomposed into O(N×h+ h×N)\nif the nonlinearity of softmax is eliminated. Our approach\nuses XNorm instead of softmax, which allows SA modules\nto compute KT V ﬁrst. Because applying identity causes\ndegradation, we suggest a simple constraint to prevent it.\nOur proposed method, called cross-normalization or\nFigure 4: Centroids generating harmonic potentials.\nEach patch, represented as a particle, is interfered by\nparabolic potentials generated from hclusters.\nXNorm, is deﬁned as follows:\nA(x) = XNdim=ﬁlter (Q)(XNdim=s pace(KT V)) (3)\nXN(a) := γa√∑h\ni=0 ||a||2\n(4)\nwhere γis a learnable parameter andhis the number of em-\nbedding dimensions. It is a common L2-norm, but it is ap-\nplied along two dimensions: the spatial dimension of KT V\nand the channel dimension of Q. Thus, it is called cross-\nnormalization.\nUsing the associative law, the key and value are multi-\nplied ﬁrst, and the query is multiplied afterward. This is\ndepicted in Figure 3. Both multiplication operations have a\ncomplexity of O(hNd), so this process is linear to N.\n3.2. XNorm\nReplace softmax to XNorm. In XNorm, key and value\nof self-attention multiplied directly. It generates hclusters\nthrough linear kernel method.\n[KT V]ij =\nn∑\nk=1\nKT\nikVkj (5)\nXNorm is applied to both query and output.\nA(x) =\n\n\nˆq0 ·ˆk0 ˆq0 ·ˆk1 ··· ˆq0 ·ˆkh\nˆq1 ·ˆk0 ˆq1 ·ˆk1 ··· ˆq1 ·ˆkh\n... ... ... ...\nˆqN ·ˆk0 ˆqN ·ˆk1 ··· ˆqN ·ˆkh\n\n (6)\nˆqi = XN[(Qi0,Qi1,··· ,Qih)] (7)\nˆki = XN[([KT V]0i,[KT V]1i,··· ,[KT V]hi)] (8)\n3\nModel Depth #Dim #Embed #Head GFLOPS Params (M) Res. Patch Size\nUFO-ViT-T 24 192 96 4 1.9 10.0 224 16\nUFO-ViT-S 12 384 128 8 3.7 20.7 224 16\nUFO-ViT-M 24 384 128 8 7.0 37.3 224 16\nUFO-ViT-B 24 512 128 8 11.9 63.7 224 16\nTable 1: UFO-ViT models.\nwhere x denotes the input. Finally, the projection weight\nscales and aggregates the dot product terms using a\nweighted sum.\n[WprojA(x)]ij =\nh∑\nm=1\nwmj ˆqi ·ˆkj (9)\nIn this formulation, relational features are deﬁned by the\ncosine similarity between patches and clusters. XNorm re-\nstricts the features of each pixel in the query and clusters\nto unit vectors. This prevents their values from suppress-\ning relational properties by regularizing them to a limited\nlength. If they have arbitrary values, the region of attention\nis dependent on the initialization.\nHowever, this interpretation is not sufﬁcient to explain\nwhy XNorm has to be the L2-normalization form only.\nHere, we introduce another theoretical view that considers\na simple physical simulation.\nDetails of XNorm. Considering the residual connec-\ntions, the output of an arbitrary module is formulated as\nfollows:\nxn+1 = xn + f(xn) (10)\nwhere nand xdenote the index of the current layer and the\ninput image, respectively. If we assume that x is the dis-\nplacement of a certain object and nis time, then the above\nequation can be re-deﬁned as:\nxt+1 = xt + f(xt) (11)\nf(x) = ∆x\n∆t (12)\nMost neural networks are discrete, so ∆t is constant.\n(Let ∆t = 1 for simplicity.) The residual term expresses\nvelocity, so this term represents the force term if the parti-\ncle has a unit mass and ∆t= 1.\nIn physics, Hooke’s law is deﬁned as the dot product of\nthe elasticity vector k and the displacement vector x. The\nelastic force generates harmonic potential U, a function of\nx2. Physically, the potential energy interferes with the path\nof the particle moving through it. (Imagine a ball moving\naround a track with a parabolic shape.)\nF = −k ·x (13)\nU = 1\n2 kx2 (14)\nThe above formulation is generally used to approximate\nthe potential energy of a molecule at x ≈0. For multiple\nmolecules, the linearity of elasticity can be used:\nF = −\nn∑\ni=1\nki ·x (15)\nThis equation is similar to Eq.9, except that the above\nformula is not normalized. (Note that the wmj terms are\nstatic. They are irrelevant to each batch, so they do not\nperform signiﬁcant roles in this simulation.) Assume that\na small number of k values are too large. The shape of\ntheir potentials is wide and deep. (See Eq.14.) If a certain\nparticle moves around them, it cannot easily escape. In this\ncase, only two outcomes can occur: 1. Collapsing case. The\nparticles are aligned in the direction of the largest k. 2. The\nrelational features between particles are neglected. A few\nparticles with large x survive collapsing.\nThis case is exactly the same as in the previous section.\nXNorm forces all vectors to be unit vectors to prevent this\nsituation. In other words, XNorm is not a normalization,\nbut a constraint. This is why our method is called a unit\nforce operation, or simply UFO.\nTo show this empirically, we demonstrate that other nor-\nmalization methods cannot work well. For detailed results,\nrefer to our ablation study. (See Table 3.)\nFeed-forward Networks (FFN). In the attention mod-\nule, FFN cannot be ignored. However, this is a simple in-\nterpretation. The FFN is static and is not dependent on the\nspatial dimension. Physically, this type of function repre-\nsents the driven force of the harmonic oscillator equation.\nF = −k ·x + g(t) (16)\nThis increases or reduces the amplitude physically. In\nother words, it boosts or kills features that are irrelevant to\nspatial relations.\n3.3. UFO-ViT\nTo build our UFO-ViT model, we adopted architectural\nstrategies from earlier vision transformer models[15, 42, 13,\n35]. In this section, we introduce several intrinsic structures\nthat improve performance. The overall structure is illus-\ntrated in Figure 2.\n4\nReplace linear patch embedding with convolutions.\nSeveral recent studies [15, 42] claimed that early convolu-\ntional layers help vision transformers to be well-trained. To\nadopt their strategy, we used convolutional layers instead of\nlinear patch-embedding layers.\nPositional encoding. We used positional encoding as a\nlearnable parameter. This was proposed in the original vi-\nsion transformer[11]. For smaller or larger resolutions, we\nresized the positional encoding with bicubic interpolation.\nMulti-headed attention. Following the original\ntransformer[37], our modules are multi-headed for better\nregularization. The γ parameter in Eq.3 is applied to all\nheads to scale the importance of each head.\nConvolutional layers. Designing an extra module to ex-\ntract local features is not a new idea. We chose the most\nsimplistic method by adding various types of convolutional\nlayers. We experimented with both the simple depthwise\nconvolutions and the local patch interaction (LPI) layers\nproposed in XCiT[13]. We found that the former showed\nbetter performance on the regimes overall.\nClass attention. In the ImageNet1k experiments, we\nused the class attention layers presented in CaiT[35]. This\nhelps the class token gather spatial information. Class at-\ntention is computed on class token only to reduce compu-\ntation, as in the original paper. We implemented the class\nattention layers using UFO modules, whereas CaiT used the\nSA module for class attention.\n4. Experiments\n4.1. Image Classiﬁcation\nDataset. For the image classiﬁcation task, we evaluated\nour models using the ImageNet1k[10] dataset. No extra\ndataset or distilled knowledge from a convolutional teacher\nwere used.\nImplementation details. Our setup was almost the same\nas that of DeiT[34]. However, we optimized some hyperpa-\nrameters according to the model size. For details, see Table\n2. The learning rate followed the linear scaling rule[45] and\nwas scaled per the 512 batch size. We trained our model for\n400 epochs using the AdamW optimizer[26].\nThe learning rate was linearly warmed up during the\nﬁrst ﬁve epochs and decayed with a cosine schedule there-\nafter. RandAugment[9] was used for data augmentation. As\nshown in Table 2, we applied a stronger augmentation in\nlarger models.\nAblation study. Our ablation study mainly focused on\nthe importance of XNorm and architectural optimizations,\nas explained in Section 3.3. All experiments were per-\nformed using the UFO-ViT-M model. We show that most\nof the other normalization methods failed to train. In other\nwords, they did not properly decrease the loss. This can\nserve as implicit evidence to prove that our theoretical in-\nFigure 5: Top-1 accuracy vs. FLOPS. Comparison of Im-\nageNet1k top-1 accuracy of various models according to\nFLOPS. The results of models ﬁne-tuned at higher resolu-\ntions are added. Our models show the best results at the\nsame FLOPS compared to the other models.\nterpretation is reasonable. Interestingly, the application of\na single L2-norm also shows poor performance. All the re-\nsults are presented in Table 3.\nFine-tune at higher resolution. Instead of training from\nscratch again, we ﬁne-tuned UFO-ViT-M and UFO-ViT-B\nat the higher resolution. During ﬁne-tuning stage, the heads\nof models are updated only. This method prevented the\nmodels from over-ﬁtting and made the training faster. Our\nmodels could achieve the higher performance in 0.1×train-\ning time compared to learning from scratch.\nComparison with state-of-the-art models. We exper-\nimented with four models that used the same architectural\ndesign schemes as DeiT[34]. (See Table 4.) As summarized\nin Figure 1 and 5, all our models showed better performance\nand computational efﬁciency than most of the concurrent\ntransformer-based models.\nVisualization of attention maps. We visualized the at-\ntention maps on the class token of the UFO-ViT-S model.\nBecause they could not be computed directly, a pseudo-\ninverse algorithm was used to approximate them. All sam-\nples were randomly sampled in the validation set.\nFigure 6 qualitatively shows that our model successfully\ncaptures perception information using SA. Each attention\nmap attends well to the main objects, even if the sample\nhas a complicated shape. Those results also support our\n5\nModelsHyperparameter UFO-ViT-T UFO-ViT-S UFO-ViT-M UFO-ViT-B\nlearning rate 5e-4 5e-4 4e-4\nDropPath[21] 0.05 0.1 0.15 0.25\nRandAugment[9] 2, 7 2, 7 2, 9 3, 12\nTable 2: Hyperparameters for image classiﬁcation. All the other hyperparameters are same as DeiT[34].\nMethod Top-1 Acc. (%)\nBaseline(Linear Embed+XNorm) 81.8\nXNorm →LN[1], GN[41], IN[36] Failed\nXNorm →Learnable p-Norm 81.8\nXNorm →Single L2-norm Failed\nLinear Embed[11] →Conv Embed 82.0\nHyperparameter Optimization 82.8\nTable 3: Ablation study. The results of ablation study\non ImageNet1k classiﬁcation for UFO-ViT-M model. Note\nthat single L2-Norm means applying L2-Norm to only one\nof query and key-value interaction.\nFigure 6: Visualized attentions. Visualization of attention\nmatrices using pseudo-inverse scheme. These matrices are\nextracted from class attention module of pretrained UFO-\nViT-S. The brighter colors represent the values closer to 1.\nThe thresholds are adjusted manually.\ntheoretical assumptions in section 3.1.\n4.2. Object Detection with Mask R-CNN\nImplementation details. Our models were evaluated on\nthe COCO benchmark dataset[24] for the object detection\ntask. We used UFO-ViT as the backbone and mask R-\nModel Top-1 Params FLOPs\nAcc. (M) (G)\nRegNetY-1.6G[27] 78.0 11 1.6\nDeiT-Ti[34] 72.2 5 1.3\nXCiT-T12/16[13] 77.1 26 1.2\nUFO-ViT-T 78.8 10 1.9\nResNet-50[18] 75.3 26 3.8\nRegNetY-4G[27] 80.0 21 4.0\nDeiT-S[34] 79.8 22 4.6\nSwin-T[25] 81.3 29 4.5\nXCiT-S12/16[13] 82.0 26 4.8\nUFO-ViT-S 82.0 21 3.7\nResNet-101[18] 75.3 47 7.6\nRegNetY-8G[27] 81.7 39 8.0\nSwin-S[25] 83.0 50 8.7\nXCiT-S24/16[13] 82.6 48 9.1\nUFO-ViT-M 82.8 37 7.0\nRegNetY-16G[27] 82.9 84 16.0\nDeiT-B[34] 81.8 86 17.5\nSwin-B[25] 83.5 88 15.4\nXCiT-M24[13] 82.9 84 16.2\nUFO-ViT-B 83.3 64 11.9\nEfﬁcientNet-B7[31] 84.3 66 37.0\nXCiT-S24/8[13] 83.9 48 36.0\nSwin-B/384[25] 84.5 48 47.0\nUFO-ViT-M/384 83.8 37 20.5\nUFO-ViT-B/384 84.3 64 35.1\nTable 4: Comparison with the state of the art mod-\nels. The image classiﬁcation results, model capacity, and\nFLOPS of various models on ImageNet1k dataset.\nCNN[17] as the detector. This implementation was based\non the MMDetection library.[4] The scheduling and data\naugmentation follow the setup of DETR[2]. We used 16\nNVIDIA A100 GPUs for training over 36 epochs with two\nbatch sizes per GPU using the AdamW optimizer. Unlike\nfor image classiﬁcation, we used the same hyperparameters\nfor all models. All experiments were performed on a 3x\nschedule. The input resolution was ﬁxed at 800 ×1333 for\nall the experiments.\nEvaluation on COCO dataset. We compared\nCNNs[18, 43] and transformer-based vision models on ob-\nject detection and instance segmentation tasks. To make\n6\nthe comparison fair, the experimental environment was the\nsame for all the results. All models were pre-trained on the\nImageNet1k dataset.\nAccording to Table 5, our models signiﬁcantly outper-\nform the CNN-based models. In addition, they achieve\nhigher or more competitive results than do state-of-the-art\nvision transformers.\nThe modules in XCiT[13] models have a structure sim-\nilar to that of the UFO modules except for the SA scheme.\nHence, they show a mAP curve similar to that of UFO-ViT\naccording to the model capacity. They perform slightly bet-\nter than our model in a similar design space. It is possible\nthat the XCiT models have a larger embedding space d.\nSwin transformer[25] models showed better results in\nthe overall regime. We infer that this is because their ar-\nchitectural strategy is better optimized for dense predic-\ntion tasks. Notably, UFO-ViT-B performs slightly worse\nthan UFO-ViT-M on the bounding box detection task, but\nslightly better in detecting smaller bounding boxes and\noverall instance segmentation scores.\nFigure 7: Maximum batch size on single GPU. Compar-\nison of maximum available batch size on single GPU. We\nscaled batch size by 2 until OOM causes.\n4.3. Measuring Computational Efﬁciency\nWe measured the various computational resources re-\nquired for the inference. All measurements were performed\non a single V100 GPU with 16 GB of VRAM.\nMemory efﬁciency. As shown in Figure 8, we deter-\nmined that our models consumed much less memory for\nlarger resolutions compared to DeiT models[34] and Swin\nFigure 8: Allocated memory vs. # of tokens. To check\nthe linearity of our models empirically, we measured the\nmaximum value of allocated GPU memory on different res-\nolutions. For a batch size of 64, the memory consumption\nof our models showed linearity with the number of tokens.\nMoreover, our models required signiﬁcantly less memory\nthan the other models.\nFigure 9: GPU throughput according to the input res-\nolution. Note that the scale of throughput axis is log4\nscale. ’max batch’ means throughput measured on maxi-\nmum available batch size.\nTransformers[25]. Our model can process up to a 4×batch\nsize compared with other models showing similar perfor-\n7\nBackbone Params (M) APb APb\n50 APb\n75 APm APm\n50 APm\n75\nResNet50[18] 44.2 41.0 61.7 44.9 37.1 58.4 40.1\nPVT-Small[39] 44.1 43.0 65.3 46.9 39.9 62.5 42.8\nSwin-T[25] 47.8 46.0 68.1 50.3 41.6 65.1 44.9\nXCiT-S12/16[13] 44.3 45.3 67.0 49.5 40.8 64.0 43.8\nUFO-ViT-S 39.7 44.6 66.7 48.7 40.4 63.6 42.9\nResNet101[18] 63.2 42.8 63.2 47.1 39.2 60.1 41.3\nPVT-Medium[39] 63.9 44.2 66.0 48.2 40.5 63.1 43.5\nSwin-S[25] 69.0 48.5 70.2 53.5 43.3 67.3 46.6\nXCiT-S24/16[13] 65.8 46.5 68.0 50.9 41.8 65.2 45.0\nUFO-ViT-M 56.4 46.0 68.2 50.0 41.0 64.6 43.7\nResNeXt101-64[43] 101.9 44.4 64.9 48.8 39.7 61.9 42.6\nPVT-Large[39] 81.0 44.5 66.0 48.3 40.7 63.4 43.7\nXCiT-M24/16[13] 101.1 46.7 68.2 51.1 42.0 65.6 44.9\nUFO-ViT-B 82.4 45.8 67.4 50.1 41.2 64.5 44.1\nTable 5: Object detection performance on the COCO val2017.\nmance. (See Figure 7).\nGPU throughput. Our model is faster than other models\nshowing similar performance. (See Figure 9.) In addition,\nthe GPU throughput of our model is signiﬁcantly lower than\nthat of DeiT[34] for higher input resolution.\n5. Conclusion\nIn this paper, we proposed a simple method that ensures\nlinear complexity for SA without loss of performance. By\nreplacing the softmax function, we removed the quadratic\noperation using the associative law of matrix multipli-\ncation. This type of factorization has typically caused\nperformance degradation in earlier studies. The UFO-ViT\nmodels outperformed most of the existing state-of-the-art\ntransformer-based and CNN-based models for image\nclassiﬁcation. We have shown that our models can also\nbe deployed well for general purposes. Our UFO-ViT\nmodels show performance on dense prediction tasks that is\ncompetitive with or better than earlier models. With more\noptimized structures for dense prediction, we expect our\nmodels to become more efﬁcient and to perform better.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. 6\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with trans-\nformers. In European Conference on Computer Vi-\nsion, pages 213–229. Springer, 2020. 1, 6\n[3] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda.\nCrossvit: Cross-attention multi-scale vision trans-\nformer for image classiﬁcation. arXiv preprint\narXiv:2103.14899, 2021. 2\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao,\nYu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, et al. Mmdetection: Open mm-\nlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019. 6\n[5] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n2\n[6] Franc ¸ois Chollet. Xception: Deep learning with\ndepthwise separable convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1251–1258, 2017. 2\n[7] Krzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. Rethinking attention with per-\nformers. arXiv preprint arXiv:2009.14794, 2020. 2\n[8] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang,\nHaibing Ren, Xiaolin Wei, Huaxia Xia, and Chun-\nhua Shen. Twins: Revisiting the design of spa-\ntial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 1(2):3, 2021. 2\n[9] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops , pages\n702–703, 2020. 5, 6\n8\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierarchi-\ncal image database. In 2009 IEEE conference on com-\nputer vision and pattern recognition , pages 248–255.\nIeee, 2009. 1, 5\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 5, 6\n[12] Ricard Durall, Stanislav Frolov, Andreas Dengel,\nand Janis Keuper. Combining transformer genera-\ntors with convolutional discriminators. arXiv preprint\narXiv:2105.10189, 2021. 1\n[13] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron,\nPiotr Bojanowski, Matthijs Douze, Armand Joulin,\nIvan Laptev, Natalia Neverova, Gabriel Synnaeve,\nJakob Verbeek, et al. Xcit: Cross-covariance image\ntransformers. arXiv preprint arXiv:2106.09681, 2021.\n1, 2, 4, 5, 6, 7, 8\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer.\nTaming transformers for high-resolution image syn-\nthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages\n12873–12883, 2021. 1\n[15] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herv ´e J ´egou, and\nMatthijs Douze. Levit: a vision transformer in con-\nvnet’s clothing for faster inference. arXiv preprint\narXiv:2104.01136, 2021. 1, 2, 4, 5\n[16] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu\nAbuduweili, Jiachen Li, and Humphrey Shi. Escap-\ning the big data paradigm with compact transformers.\narXiv preprint arXiv:2104.05704, 2021. 2\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross\nGirshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n2961–2969, 2017. 6\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n6, 8\n[19] Byeongho Heo, Sangdoo Yun, Dongyoon Han,\nSanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers.\narXiv preprint arXiv:2103.16302, 2021. 1, 2\n[20] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn,\nand Tim Salimans. Axial attention in multidimen-\nsional transformers. arXiv preprint arXiv:1912.12180,\n2019. 2\n[21] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision ,\npages 646–661. Springer, 2016. 6\n[22] Yifan Jiang, Shiyu Chang, and Zhangyang Wang.\nTransgan: Two transformers can make one strong gan.\narXiv preprint arXiv:2102.07074, 2021. 1\n[23] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efﬁcient transformer. arXiv preprint\narXiv:2001.04451, 2020. 2\n[24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and\nC Lawrence Zitnick. Microsoft coco: Common ob-\njects in context. In European conference on computer\nvision, pages 740–755. Springer, 2014. 1, 6\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030 ,\n2021. 1, 2, 6, 7, 8\n[26] Ilya Loshchilov and Frank Hutter. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 5\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Gir-\nshick, Kaiming He, and Piotr Doll ´ar. Designing net-\nwork design spaces. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion, pages 10428–10436, 2020. 6\n[28] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai\nYi, and Hongsheng Li. Efﬁcient attention: Atten-\ntion with linear complexities. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 3531–3539, 2021. 1, 2\n[29] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar,\nJonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 16519–\n16529, 2021. 1\n[30] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. Adaptive attention span\nin transformers. arXiv preprint arXiv:1905.07799 ,\n2019. 2\n[31] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages\n6105–6114. PMLR, 2019. 6\n9\n[32] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-\nsica Yung, Daniel Keysers, Jakob Uszkoreit, Mario\nLucic, et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021. 2\n[33] Hugo Touvron, Piotr Bojanowski, Mathilde Caron,\nMatthieu Cord, Alaaeldin El-Nouby, Edouard Grave,\nArmand Joulin, Gabriel Synnaeve, Jakob Verbeek, and\nHerv´e J´egou. Resmlp: Feedforward networks for im-\nage classiﬁcation with data-efﬁcient training. arXiv\npreprint arXiv:2105.03404, 2021. 2, 3\n[34] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efﬁcient image transformers\n& distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 1, 2, 5, 6, 7, 8\n[35] Hugo Touvron, Matthieu Cord, Alexandre Sablay-\nrolles, Gabriel Synnaeve, and Herv ´e J ´egou. Go-\ning deeper with image transformers. arXiv preprint\narXiv:2103.17239, 2021. 2, 4, 5\n[36] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempit-\nsky. Instance normalization: The missing ingredient\nfor fast stylization. arXiv preprint arXiv:1607.08022,\n2016. 6\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn Advances in neural information processing systems,\npages 5998–6008, 2017. 5\n[38] Sinong Wang, Belinda Z Li, Madian Khabsa, Han\nFang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768,\n2020. 2\n[39] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan,\nKaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile\nbackbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021. 1, 2, 8\n[40] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Intro-\nducing convolutions to vision transformers. arXiv\npreprint arXiv:2103.15808, 2021. 1\n[41] Yuxin Wu and Kaiming He. Group normalization. In\nProceedings of the European conference on computer\nvision (ECCV), pages 3–19, 2018. 6\n[42] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Dar-\nrell, Piotr Doll ´ar, and Ross Girshick. Early convo-\nlutions help transformers see better. arXiv preprint\narXiv:2106.14881, 2021. 2, 4, 5\n[43] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu,\nand Kaiming He. Aggregated residual transformations\nfor deep neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 1492–1500, 2017. 6, 8\n[44] Yunyang Xiong, Zhanpeng Zeng, Rudrasis\nChakraborty, Mingxing Tan, Glenn Fung, Yin\nLi, and Vikas Singh. Nystr \\” omformer: A nystr \\”\nom-based algorithm for approximating self-attention.\narXiv preprint arXiv:2102.03902, 2021. 2\n[45] Yang You, Igor Gitman, and Boris Ginsburg. Large\nbatch training of convolutional networks. arXiv\npreprint arXiv:1708.03888, 2017. 5\n[46] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun\nShi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision\ntransformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 2\n[47] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin\nXiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer\nfor high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021. 1\n[48] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen,\nand Tomas Pﬁster. Aggregating nested transformers.\narXiv preprint arXiv:2105.12723, 2021. 2\n[49] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian\nZhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking\nsemantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6881–6890, 2021. 1\n10",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.6957669854164124
    },
    {
      "name": "Transformer",
      "score": 0.6730350852012634
    },
    {
      "name": "Computer science",
      "score": 0.6413666605949402
    },
    {
      "name": "Quadratic equation",
      "score": 0.5879035592079163
    },
    {
      "name": "Nonlinear system",
      "score": 0.5177447199821472
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46718886494636536
    },
    {
      "name": "Algorithm",
      "score": 0.33366477489471436
    },
    {
      "name": "Mathematics",
      "score": 0.18810659646987915
    },
    {
      "name": "Engineering",
      "score": 0.14677965641021729
    },
    {
      "name": "Artificial neural network",
      "score": 0.09803906083106995
    },
    {
      "name": "Voltage",
      "score": 0.0826030969619751
    },
    {
      "name": "Electrical engineering",
      "score": 0.07954931259155273
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}