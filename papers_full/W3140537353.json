{
  "title": "ProcessTransformer: Predictive Business Process Monitoring with Transformer Network",
  "url": "https://openalex.org/W3140537353",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4289615556",
      "name": "Bukhsh, Zaharah A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174599913",
      "name": "Saeed, Aaqib",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297710785",
      "name": "Dijkman, Remco M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3089306244",
    "https://openalex.org/W2100115309",
    "https://openalex.org/W1966598933",
    "https://openalex.org/W2170505850",
    "https://openalex.org/W2964066696",
    "https://openalex.org/W2283585398",
    "https://openalex.org/W3046140942",
    "https://openalex.org/W2944716568",
    "https://openalex.org/W2987728367",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2581522324",
    "https://openalex.org/W3083423677",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3033804831",
    "https://openalex.org/W2464785945",
    "https://openalex.org/W3091710143",
    "https://openalex.org/W2966756122",
    "https://openalex.org/W2786462606",
    "https://openalex.org/W2969940319",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2610829762",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3089641966",
    "https://openalex.org/W184106230",
    "https://openalex.org/W2963674330",
    "https://openalex.org/W2029393109",
    "https://openalex.org/W2742079690",
    "https://openalex.org/W182098796",
    "https://openalex.org/W2970863184",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3022806883",
    "https://openalex.org/W3098824823"
  ],
  "abstract": "Predictive business process monitoring focuses on predicting future characteristics of a running process using event logs. The foresight into process execution promises great potentials for efficient operations, better resource management, and effective customer services. Deep learning-based approaches have been widely adopted in process mining to address the limitations of classical algorithms for solving multiple problems, especially the next event and remaining-time prediction tasks. Nevertheless, designing a deep neural architecture that performs competitively across various tasks is challenging as existing methods fail to capture long-range dependencies in the input sequences and perform poorly for lengthy process traces. In this paper, we propose ProcessTransformer, an approach for learning high-level representations from event logs with an attention-based network. Our model incorporates long-range memory and relies on a self-attention mechanism to establish dependencies between a multitude of event sequences and corresponding outputs. We evaluate the applicability of our technique on nine real event logs. We demonstrate that the transformer-based model outperforms several baselines of prior techniques by obtaining on average above 80% accuracy for the task of predicting the next activity. Our method also perform competitively, compared to baselines, for the tasks of predicting event time and remaining time of a running case",
  "full_text": "ProcessTransformer: Predictive Business\nProcess Monitoring with Transformer Network\nZaharah A. Bukhsh\n , Aaqib Saeed\n , and Remco M. Dijkman\nEindhoven University of Technology, Eindhoven, The Netherlands\n{z.bukhsh, a.saeed, r.m.dijkman}@tue.nl\nAbstract. Predictive business process monitoring focuses on predicting\nfuture characteristics of a running process using event logs. The fore-\nsight into process execution promises great potentials for eﬃcient opera-\ntions, better resource management, and eﬀective customer services. Deep\nlearning-based approaches have been widely adopted in process mining\nto address the limitations of classical algorithms for solving multiple\nproblems, especially the next event and remaining-time prediction tasks.\nNevertheless, designing a deep neural architecture that performs compet-\nitively across various tasks is challenging as existing methods fail to cap-\nture long-range dependencies in the input sequences and perform poorly\nfor lengthy process traces. In this paper, we propose ProcessTrans-\nformer, an approach for learning high-level representations from event\nlogs with an attention-based network. Our model incorporates long-range\nmemory and relies on a self-attention mechanism to establish dependen-\ncies between a multitude of event sequences and corresponding outputs.\nWe evaluate the applicability of our technique on nine real event logs. We\ndemonstrate that the transformer-based model outperforms several base-\nlines of prior techniques by obtaining on average above 80% accuracy for\nthe task of predicting the next activity. Our method also perform com-\npetitively, compared to baselines, for the tasks of predicting event time\nand remaining time of a running case.\nKeywords: Predictive process monitoring · transformer · attention ·\ndeep learning · activity prediction · remaining time prediction\n1 Introduction\nWith the trend towards digital transformation and the availability of relatively\ncheaper storage solutions, the amount of process execution data (also referred\nto as event logs) are increasing at a tremendous scale. Process mining methods\nhave been used to discover, monitor, and improve the business processes by an-\nalyzing the process logs. Instead of post-hoc analysis, organizations are actively\ninvesting in predictive analytics solutions to gain insights into their performance.\nPredictive business process monitoring (PBPM) has emerged as a crucial area of\nprocess mining, focusing on estimating future characteristics of a running busi-\nness process. PBPM has several useful business applications, including eﬀective\narXiv:2104.00721v1  [cs.LG]  1 Apr 2021\n2 Z. A. Bukhsh et al.\nresource management, improving operational eﬃciency, and avoiding deadlock\nby predicting the next possible activities, duration, and remaining time to com-\npletion.\nIn the last decade, deep neural networks are widely adopted for tasks related\nto business process monitoring. Evermann et al.,[10] introduced recurrent neural\nnetworks (RNNs) for predicting the process behavior at run-time. Following this,\nseveral prediction models based on RNNs and related variants, such as long-short\nterm memory (LSTM) networks have been proposed for the tasks of next ac-\ntivity [33,21,35], suﬃx generation [18,4], outcome prediction [36], and process’s\nremaining time prediction [27,20]. Even though deep RNNs based PBPM meth-\nods have been ﬁrmly established for event sequence modeling, they suﬀer from\nconsiderable shortcomings, speciﬁcally for the next activity prediction task.\nFirstly, several proposed methods [33,4,20,21,35] employ one-hot encoding to\nobtain the numeric representations of categorical events sequences. These integer\nrepresentations disregard the intrinsic relationship among events and introduce\nunrealistic computational requirements due to an increase of data dimension-\nality [12]. Secondly, LSTM lacks the explicit modeling of long and short-range\ndependencies in the sense that their performance degrades in proportion to the\nlength of events sequences [23]. It is speciﬁcally undesired for event logs due to\ninterconnections introduced by control ﬂows among activities. Lastly, the inher-\nent sequential nature of LSTM and RNNs precludes parallelization, resulting in\ncritically ineﬃcient learning and inference.\nThe attention mechanism is proposed to address the problem of long-range\ndependencies for sequence modeling without regard to their distance in input\nand output sequence [2]. In particular, Vaswani et al. [37] introduced Trans-\nformer neural network architecture, a deep sequence model that employs self-\nattention to maintain coherence in long-range sequences. The Transformer-based\nencoder-decoder models have rapidly become a dominant architecture for neu-\nral machine translation and natural language understanding [40]. Speciﬁcally,\nTransformer architecture is also behind the compelling language models, such as\nGPT-3 (Generative Pretrained Transformer) and BERT (Bidirectional Encoder\nRepresentations from Transformers) that have revolutionized numerous language\nunderstanding tasks. An interested reader may refer to [2] for a comprehensive\nexplanation about attention-based networks.\nDespite showing remarkable performance in multiple sequence modeling prob-\nlems, Transformers have not been explored in the realm of business process\nmanagement. Thus, the core contribution of our work is the ProcessTrans-\nformer model with an improved strategy for learning high-level generic rep-\nresentations directly from temporal sequential events with minimal preprocess-\ning of the input. In contrast to ﬁxed-size recurrent memory models, the self-\nattention mechanism enables access to any part of the previously generated\nevents in a sequence. It allows the deep neural network to capture global depen-\ndencies between inputs and outputs for powerful general-purpose representation\nlearning. Besides, ProcessTransformer can eﬀectively diﬀerentiate the most\nrelevant features that aﬀect the model prediction. We demonstrate the appli-\nPredictive Business Process Monitoring with Transformer Network 3\ncability of ProcessTransformer on nine real event logs. We show that our\nTransformer-based architecture outperforms several baselines of existing meth-\nods on predicting the next activity task with minimal data preprocessing. Sim-\nilarly, ProcessTransformer also shows performance improvements for pre-\ndicting the next event time and completion time of a running case.\nThe remainder of the paper is structured as follows. Section 2 provides the\nbackground and related work. Section 3 provides deﬁnitions of main concepts\nrelated to PBPM. Section 4 presents the proposed approach, followed by Sec-\ntion 5, which details the experimental setup and key results. Finally, Section 6\nsummarizes the ﬁndings, contributions and outlines the future work.\n2 Background and Related work\nRepresentation learning focuses on extracting discriminative features from raw\nunstructured data to eﬀectively solve the prediction problem, ideally in an end-\nto-end manner. The general-purpose representations learned from a plethora of\nraw, real-life event logs can be used to solve several business tasks of interest,\nincluding predictive process monitoring, improvement, and enhancement.\nIn this section, we introduce sequence modeling in the context of deep learn-\ning. We also provide a brief overview of literature studies related to PBPM.\n2.1 Sequence modeling using deep neural architectures\nSequence modeling involves capturing high-level semantic relationships in a series\nof interdependent input values that can be useful for various tasks, such as text\ncompletion or predicting the next word in a sentence. Compared to standard\nindependent and identically distributed datasets, elements in the sequence follow\na certain order and are not independent of each other. Typical machine learning\nalgorithms and standard feedforward networks fall short in sequence modeling\ndue to their inability to handle the order and keep the memory of past seen\nsamples [32]. Natural language processing, time-series analysis, and predictive\nprocess monitoring are the key areas for sequence modeling.\nRecurrent neural networks (RNNs) introduced an internal memory\nstate to retain the memory of past inputs. The hidden layer within a recur-\nrent network receives input from the input layer and the previous hidden layer’s\noutput at each timestep. However, RNNs cannot maintain the context informa-\ntion for long input sequences and suﬀer from gradient vanishing or exploding\nproblems due to the recursive derivation of gradients during model training.\nLong Short Term Memory networks (LSTMs) are special kinds of\nRNNs with multiple switch gates to avoid the gradient vanishing problem and\nto remember long-range input dependencies [13]. Even though LSTMs perform\nbetter than RNNs, they suﬀer from similar limitations as RNNs when an input\nsequence become excessively long. Additionally, LSTMs are computationally ex-\npensive to train due to long sequential gradient paths which makes it harder to\nparallelize them.\n4 Z. A. Bukhsh et al.\nq(1)k(1) v(1) q(j) k(j) v(j) q(6)k(6) v(6)\nq(j) q(j) q(j)k(1) k(j) k(6)\na(1) =       ,      a(j) =         ,      a(6) =       ,      \n... ...\n... ...\na(1) x      a(j) x      a(6) x      \nv(1) v(j) v(6)\nReceive order\nShip product\nSend Invoice\nw(1)       w(j)       w(6)       \nFig. 1. An example of Self-\nattention mechanism to learn at-\ntention representation vector of\nShip product event.\nTransformer is introduced by Vaswani et\nal. [37] for neural machine translations. Es-\nsentially, the transformer replaced the recur-\nsive approach of previously introduced recur-\nrent networks with theself-attention mech-\nanism. It enables the transformer to rea-\nson over long-range dependencies and draw\ngeneric representations between input and\noutput. Self-attention mechanism decides the\nimportance of all tokens in a sequence with\nrespect to the input token. In Figure 1, we in-\ntroduce the self-attention mechanism with the\nhelp of an example. A model based on self-\nattention takes a trace having six events as\ninput. Each event is represented with embed-\nding vectors called a query, key, and values.\nTo compute the attention representation vec-\ntor wj of Ship productevent, we must measure\nhow it relates to the other events in a trace.\nFor this, we take the dot product of the query\nvector of interest with the keys of all input\nvectors, resulting in a vector of weights aj for all the input tokens. The atten-\ntion vector is then computed by taking the weighted sum ofaj with value vectors.\nIn other words, every output is a weighted sum of every input. The attention\nrepresentation for each input token can be computed in parallel since their op-\nerations are independent, thus eliminating the recurrence. A single event can be\nrelated to other events in a sequence in multiple ways, such as semantically, tem-\nporally, among others. Therefore, the self-attention is projected multiple times\nto capture all of these dependencies, forming the multi-head self-attention.\nThe transformer is mainly used for language modeling; however, it is a generic\nnetwork that can be adopted for multiple sequence modeling tasks. In this paper,\nwe propose the transformer model to address the PBPM tasks. Further details\nof ProcessTransformer are given in Section 4.\n2.2 Predictive process monitoring\nPBPM has emerged as a promising area of process mining, having a wide ar-\nray of business applications. Initially, the research focus has been on examining\nthe process outcome in terms of its duration and successful completion. Multiple\nprocess analytics techniques based on hidden Markov model [22], ﬁnite state ma-\nchines [11], stochastic Petri nets [29], and annotated transition systems (using\ndiverse abstractions) [1] have been proposed. Classical machine learning classi-\nﬁcation methods, such as random forest and support vector machines, are also\nadapted to predict the successful completion of a process using the history of\nevent sequences and hand-crafted features [5,14].\nPredictive Business Process Monitoring with Transformer Network 5\nWith the automated features learning capabilities of deep neural networks,\nspeciﬁcally LSTMs, deep learning methods have been adopted to extract useful\nrepresentations from large-scale temporal sequential event logs to solve various\ntasks. Successful application of deep sequence modeling has been explored ear-\nlier by Evermann et al. [10] for predicting the next event using a shallow LSTM\nmodel and embedding technique for handling categorical variables. A similar\nLSTM model architecture with one-hot vector encoding was employed by Tax\net al. [33] to predict the next activity with its associated timestamp, remain-\ning process duration, and process suﬃx. Likewise, the same process monitoring\nproblems were addressed by Camargo et al. [4] using the composition of LSTM\nto support both categorical and numeric features. Additional temporal features\nare proposed in [20] and [21] to improve the predictive capabilities of existing\ndeep models. An extension of LSTM with attention mechanism is used in [38]\nfor the process outcome prediction task.\nAlthough LSTM has been a popular choice due to its sequence modeling\ncharacteristics, other architectural variants of deep neural networks and analyt-\nical techniques are also explored in few studies. Prominently, Khan et al., [15]\nintroduced memory augmented neural networks as a recommendation tool for\ntackling complex process analytic problems. Pasquadibisceglie et al., [24] pro-\nposed a data engineering approach to transform events temporal data to spatial\nimage-like structure in order to use the convolution neural networks (CNN).\nSimilarly, Mauro et al., [6] adapted the inception architecture of CNN for se-\nquential data to address the next activity prediction problem. Pauwels et al. [25]\npresented a Bayesian technique to predict the next event. Taymouri et al., [34]\nadapted generative adversarial nets (GANs) with Gumbel-Softmax distribution\nto use them for (categorical) suﬃx generation and remaining time prediction.\nBohmer et al. [3] proposed combining local and global techniques using sequen-\ntial prediction rules for the next event prediction. For further relevant work on\nPBPM, an interested reader may refer to [28] and [39] survey studies.\n3 Preliminaries\nIn this section, we introduce concepts that will be used to deﬁne the problem and\ndata preprocessing for predictive process modeling in the subsequent sections.\nWe follow the standard notations provided in [33] and [28].\nDeﬁnition 1 (Event) Let A be the set of activities, C the set of cases, T the\ntime domain and D1,..,D m the set of related attributes where m> 0. An event\nis a tuple e= (a,c,t,d 1,...,d m), where a∈A, c∈C, t∈T and di ∈{Di}with\ni ∈[1,m].\nDeﬁnition 2 (Trace, Events Log) Let πA, πC, and πT be functions that\nmap an event e = (a,c,t,d 1,...,d m) to an activity, as πA(e) = a, to a unique\ncase identiﬁer, as πC(e) = c and to a timestamp, as πT(e) = t. A trace is\ndeﬁned as a ﬁnite non-empty sequence of events σ = ⟨e1,e2,...,e n⟩, such that\n∀ei,ej ∈σ, it must hold that: the events within a trace σ must have same case\nid, i.e. πC(ei) = πC(ej) and time should be non-decreasing, i.e. πT(ej) ≥πT(ei)\n6 Z. A. Bukhsh et al.\nfor j >i. We say that a trace σ= ⟨e1,e2,...,e n⟩has length n, denoted |σ|. An\nevent log is collection of traces L = {σ1,σ2,...,σ l}. We say that a collection\nL= {σ1,σ2,...,σ l}has size l, denoted |L|.\nDeﬁnition 3 (Activity Prediction) Let σ be a trace ⟨e1,e2,...,e n⟩and k∈\n[1,n−1] be a scalar positive number. The event preﬁx of lengthk, hdk can be de-\nﬁned as: hdk(σ) = ⟨e1,e2,...,e k⟩. The activity preﬁx can be obtained by the ap-\nplication of mapping function πA as πA(hdk(σ)) = ⟨πA(e1),πA(e2),...,π A(ek)⟩.\nActivity prediction is the deﬁnition of a function Θa that takes event preﬁx\nhdk(σ) where k∈[1,n −1], and predicts the next activity e′, i.e.:\nΘa(hdk(σ)) = πA(e′\nk+1)\nDeﬁnition 4 (Event Time Prediction) Let σ= ⟨e1,e2,...,e n⟩be a trace of\nlength n. To extract the time-related features of the last event en of that trace\nwe deﬁne the functions:\nfvt1(σ) =\n{\n0 if |σ|= 1,\nπT(en) −πT(en−1), otherwise.\nfvt2(σ) =\n{\n0 if |σ|∈ [1,2],\nπT(en) −πT(en−2), otherwise.\nfvt3(σ) =\n{\n0 if |σ|= 1,\nπT(en) −πT(e0) otherwise .\nThe fvt1 feature represents the time diﬀerence between the previous event and\nthe current event of a trace. Thefvt2 feature contains the time diﬀerence between\ncurrent event time and time of an event before the previous event. Finally, fvt3\ndepicts the approximate time passed since the case has initiated. (Note that\nwe say the approximate time, because, due to the fact that we only have the\ncompletion time of each event, we do not know the duration of the ﬁrst event or\nthe time the case waited for the ﬁrst event to occur.) Event time prediction is\nthe deﬁnition of a function Θt that takes event preﬁx hdk(σ) where k∈[1,n−1],\nand predicts the time moment at which the next activity will occur, i.e.:\nΘt(σ′,fvt1(σ′),fvt2(σ′),fvt3(σ′)) = πT(e′\nk+1), where σ′= hdk(σ)\nDeﬁnition 5 (Remaining Time Prediction) Let σ = ⟨e1,e2,...,e n⟩. Re-\nmaining time prediction is the deﬁnition of a function Θrt that takes event preﬁx\nhdk(σ) where k∈[1,n −1], and predicts the remaining time of the case, i.e.:\nΘrt(σ′,fvt1(σ′),fvt2(σ′),fvt3(σ′)) = πT(en) −πT(ek), where σ′= hdk(σ)\nNote that fv functions are applied manually for feature extraction as a\npreprocessing step, whereas Θ functions are learned in an end-to-end manner\nwith ProcessTransformer .\nPredictive Business Process Monitoring with Transformer Network 7\n4 Process Transformer\nReal-life event logs present temporally sequential data, which is complex, vari-\nable, has extensive dependencies due to multiple control ﬂows. Recurrent neural\nnetworks, such as LSTM, struggle to reason over long-range sequences due to\nthe limited size of a context vector, as noted in [23]. This paper addresses the\nproblem of PBPM to predict the next activity, event time, and remaining time\nof a process under execution, i.e., using deep learning to learn the functions\nΘa, Θt, and Θrt as they are deﬁned in Deﬁnitions 3-5. To that end we propose\nthe ProcessTransformer . Notably, while several deep learning-based pro-\ncess monitoring methods [33,4,24] exist, which learn a predictive model based\non varying preﬁxes length of event sequences, we develop a deep neural network\nthat considers possible preﬁxes altogether for training and inference.\nFigure 2 provides a high-level overview of the model architecture. The Pro-\ncessTransformer has Nattention blocks, which take positional encoded input\nsequence and pass learned representation to a pooling operation and then to a\nfully connected layer. We use single attention block for ProcessTransformer\n. The attention block comprises multi-headed attentionlayers, followed by feed-\nforward layers having residual connections, dropout, and a normalization layer.\nIn the following, we explain the most important building blocks of our model.\nEmbedding Layer\nTrace as an Input\nPositional\nEncoding\nMulti-Head \nSelf-Attention\nDropout\nAdd & Norm\nFeed Forward\nDropout\nAdd & Norm\nx 1N\nGlobal Pooling\nDropout\nLinear\nDropout\nLinear\nPrediction\nTrace\nEmbeddings\nvector\nPositional \nencoding\nEmbeddings with \npositional encoding\nx36\nx36\nTrace embedding and positional encoding\nZoom-In!\nLinear\nScaled Dot-Product Attention\nLinear\n Linear\nConcat\nLinear\nx4h\nMulti-Head Self-Attension\nQ K V\nZoom-In!\nMatMul\nSoftmax\nScaled Dot-Product Attention\nQ K V\nZoom-In!\nScale\nMatMul\nProcess Attension Transformer\n...\n...\n...\n...\nFig. 2.Model Architecture of Process Transformer.\n8 Z. A. Bukhsh et al.\nTrace Embedding and Positional Encoding: Starting from a trace, the\nnetwork learns vector embedding for each event as shown in the respective block\nin Figure 2. It essentially maps the categorical input to the vector of continuous\nrepresentations. The network learns vector embedding and projects them into a\ntransformed space where similar events in a semantic sense are mapped closer\nto each other. The beneﬁt of using embedding in this way is that it eliminates\nthe problem of high-dimensionality encountered in a one-hot encoding scheme.\nFor example, while a one-hot encoding would require a binary vector for each\nevent depending on the size of vocabulary (unique event instances), the embed-\nding layer learns representation of semantically similar events by mapping them\nindividually to a vector space.\nIt is worth noting that the ProcessTransformer does not use recurrence\nas former approaches (e.g., in [33,4]). This property enables eﬃcient training\nbut at the cost of omitted positional information of events from traces. The de-\nfault Transformer architecture proposed to add positional encoding along with\nan input embedding to inform the model about the relative positioning of each\ntoken in a sequence. We choose the 36-dimensional vector encoding to repre-\nsent the relative positioning of an event in a trace. The input embedding and\nposition encoding have the same dimension to allow for their summation at the\nnext step. The neural model learns to attend to positional encoding along with\ninput embedding in an end-to-end manner during learning to solve a speciﬁc task.\nSelf-Attention (Scaled Dot-Product) Self-attention models learn to se-\nlectively attend to only important parts of a trace to compute the robust repre-\nsentation. It enables the Transformer to reason over long-range dependencies and\ndraw generic representations between input and output. We illustrate the self-\nattention mechanism with the help of an example trace of order management\nprocess in Figure 3. It shows that the events such as receive order and check\ncredit, obtain product and ship product are related and will have high relative\nattention scores with respect to each other. The events like update inventory,\nsend invoice, and check creditare semantically diﬀerent in the context of order\nmanagement and may have low attention scores. This self-attention enables the\nmodel to pay attention to important event(s) of a trace in order to better solve\nthe predictive monitoring tasks.\nReceive \norder\nCheck \ncredit\nObtain\nproduct\nShip\nproduct\nUpdate\ninventory\nSend\nInvoice\nhigh attension low attension\nhigh attension\nhigh attension\nlow attension\nFig. 3.Illustration of self-attention mechanism on a trace. The model learns attention\nscores of events for solving a particular predictive monitoring task.\nPredictive Business Process Monitoring with Transformer Network 9\nThe attention mechanism initially creates three vectors called query q, key k\nand value v for each input embedding i.e. x1, as shown in Figure 2. The neural\nmodel learns the representations for these vectors. Aself-attention function maps\na query q to a set of key-value pairs, denoted as k and v, to obtain a weighted\nsum of values called output, denoted by Z. Following [37], we adopt scaled dot-\nproduct attention to compute Zas follows:\nAttention(Q, K, V) = Z= softmax(QKT\n√dk\n)V\nwhere Q, K, and V are matrices of q, k, and v vectors packed into respective\nmatrices for eﬃcient computations. The attention scores are scaled by dividing\nto √dk i.e., dimension of key k, for stable gradient computations. Afterward,\nthe softmax function is applied to normalize the scores and obtain the impor-\ntance of each token. Finally, the softmax score is multiplied with the value V\nmatrix to give the model capability to focus on which words to attend to in a\nsequence and eliminate less relevant tokens.\nMulti-Head Self Attention A single event can be related to other events\nin a trace in multiple ways, such as semantically, temporally, among others. In\norder to introduce a model with diﬀerent representation subspaces at diﬀerent\npositions [37], the query, key, and values are linearly projected htimes as shown\nin Figure 2. The scaled dot-product attention is performed in parallel for each\nof the projection, forming the multi-headed attentionas follows:\nMultiHead(Q,K,V ) = Concat(head1,...,head h)WO\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nwhere the output of each headi is linearly concatenated and multiplied by weight\nmatrix WO, that is learned by the deep neural network.\nA dropout layer follows the output of the multi-head self-attention block\nto limit over-ﬁtting, and a layer normalization is applied across the layer’s fea-\ntures. Afterward, a position-wise feed-forward layer followed by a dropout and\nlayer normalization is applied. The attention blocks Nmake use of the residual\nconnection, depicted by dashed lines in Figure 2, to enable gradients to skip\nnon-linear functions, thus avoiding the vanishing gradient problem. The rest of\nthe layers after the attention block Nare mainly network design choices, which\nare covered in detail in the following section.\nNetwork Design and Implementation Given an event log, we create a word\ndictionary to encode the event names numerically. The traces that are shorter\nthan L|σn| (i.e., the maximum length of a trace in event logs) are padded with\nzero to have a ﬁxed-length input. We transform each event in a trace using a\nlearnable distributed embedding of 36 units and a positional encoding with the\nsame dimension. The embedding outputs are then fed to amulti-headed attention\nblock with h = 4 ( h being the number of heads) in order to learn general-\npurpose representations at diﬀerent input positions. We apply a global max-\npooling on the last layer of the attention block to aggregate features, followed\n10 Z. A. Bukhsh et al.\nby a dropout with a rate of 0.1. We also use dense layers with 32 and 128 hidden\nunits having ReLU activation consecutively. A fully-connected layer with hidden\nunits corresponding to the output dimension of a predictive task is used, with\neither softmax or linear activation functions (depending on the target task), to\nproduce output.\nWe employ categorical cross-entropy loss for the next activity prediction task.\nLikewise, we use the log-cosh loss function for regression-based tasks, i.e., for\nthe event time and remaining time prediction problem. Here, we concatenate\nthe output of the attention block with scaled numeric temporal attributes. The\nrest of the architecture remained the same across tasks.\n5 Evaluation\nWe evaluate the eﬃcacy of the proposed ProcessTransformer on nine real-\nlife event logs. We also provide comparisons to established benchmarks reported\nin [28]. This section introduces the experimental setup, including datasets and\nevaluation metrics, followed by input preprocessing and network design details.\nWe conclude the section by providing evaluation results of three predictive pro-\ncess monitoring tasks namely next activity, next event time and remaining time\nprediction of a running case.\n5.1 Experimental setup\nDatasets The experiments were conducted using event logs publicly available\nat the 4TU Research Data repository1. Some of these datasets have been widely\nused to evaluate process monitoring tasks (e.g. [4,18,33,10]). Table 1 provides\nthe descriptive statistics of considered event logs.\nTable 1.Descriptive statistics of event logs used for evaluations. Time-related char-\nacteristics are reported in days.\nDatasets Cases Events Activities Max case\nlength\nAvg. case\nlength\nMax case\nduration\nAvg. case\nduration\nHelpdesk[26] 4,580 21,348 14 15 4.66 60 40.69\nBPIC12[7] 13,087 262,200 24 175 20.03 13 8.01\nBPIC12w[7] 9,658 170,107 7 156 17.61 132 10.5\nBPIC12cw[7] 9,658 72,413 6 74 7.497 82 10.46\nBPIC13[31] 7,554 65,533 13 123 8.6754 768 11.948\nBPIC20d[8] 10,500 56,437 17 24 5.37 47 11.16\nBPIC20i[9] 6,449 72,151 34 27 11,187 737 84.15\nHospital[19] 100,000 451,359 18 217 4.51 1034 127.24\nTraﬃc ﬁnes[17] 150,370 561,470 11 20 3.73 4373 342.67\n1 https://data.4tu.nl/categories/_/13500?categories=13503\nPredictive Business Process Monitoring with Transformer Network 11\nEvaluation metrics For the next activity prediction task, accuracy is a com-\nmonly used metric as reported in several studies [33,15,24,10]. Accuracy is essen-\ntially computed by taking a fraction of correctly predicted samples to the total\nnumber of samples. However, in the case of an imbalanced dataset, accuracy as\nperformance metric can be misleading. Therefore, we report weighted accuracy\nand weighted F-score in the paper. The weighted aspect considers the data im-\nbalance of the target class and assigns the weights to data samples accordingly.\nThe F-score is a combination of precision (or positive predictive value) and recall\n(sensitivity) measures [30]. The precision determines the exactness of the model,\nwhereas the recall provides a measure of the model’s completeness. F-score is\ncalculated as follows:\nF-score = 2 ×precision ×recall\nprecision + recall\nFor next event time and remaining time prediction having continuous target\noutput, we compute mean absolute error (MAE)as follows:\nMAE =\n∑n\ni=0 |yi −ˆyi|\nn\nwhere yi is the predicted value from the model, ˆyi is the desired output, and n\nis the total number of samples in the test set.\n5.2 Data preprocessing and training setup\nThe event logs are ﬁrst chronologically ordered to simulate the reality in which a\nmodel uses past traces to monitor the future performance of running traces. Each\ndataset is split into 80% and 20% for training and testing sets, respectively, while\npreserving its temporal order. Additionally, 20% of the data from the training\nsplit is used for validation and hyper-parameter tuning during the learning phase.\nFor training and evaluation of ProcessTransformer network, following\nkey points are important:\n– We use the raw event logs data with minimal preprocessing. This means we\ndid not selectively ﬁlter out events with speciﬁc k-preﬁxes, i.e.hdk(σ), unlike\nnoted in [33,34]. The use of all preﬁxes enables the model to perform process\nmonitoring tasks for extremely small (e.g., single event) to very lengthy\nrunning cases.\n– The model is trained for 100 epochs with an ADAM optimizer [16] and a\nlearning rate of 10−2 for all the considered tasks. Furthermore, we explore the\nimpact of the batch size and the number of attention heads. We report the\nresults in the subsequent section with the optimal parameters conﬁguration\nfound on the validation set.\n– We evaluate the model’s performance on the test set iteratively for each\nk-preﬁxes, i.e., hdk(σ). This is to illustrate the model’s predictive capabil-\nity given the limited size of preﬁx, e.g. only single event, as an input. We\niteratively compute the performance metric score, such as accuracy, MAE,\nfor each k-preﬁxes and reports the average results across all preﬁxes in the\nfollowing section.\n12 Z. A. Bukhsh et al.\n5.3 Results\nWe report the experimental results to assess the performance ofProcessTrans-\nformer for prediction of the next activity, event time, and remaining time of a\nrunning case. Table 2 reports accuracy, F-score, and MAE for nine datasets on\nthree process monitoring tasks. Due to large event logs and time constraints, we\ndo not provide baseline comparison scores for four datasets, namely, BPIC20i,\nBPIC20d, hospital, and traﬃc ﬁne logs. For the other ﬁve datasets, we report\nperformance comparisons with other related studies. We adopt the baselines\nscores from the benchmark survey on PBPM as reported in [28].\nTable 2.Performance evaluation scores of ProcessTransformer for nine event logs\nfor three predictive monitoring tasks.\nNext\nActivity\nNext\nEvent Time\nRemaining\nTime\nAccuracy F-score MAE MAE\nHelpdesk 85.63 0.82 2.98 3.72\nBPIC12 85.20 0.83 0.25 4.60\nBPIC12w 91.51 0.91 0.37 4.87\nBPIC12cw 78.48 0.77 0.82 5.14\nBPIC13i 62.11 0.60 0.99 8.36\nBPIC20d 86.07 0.84 1.22 2.44\nBPIC20i 93.35 0.91 3.26 10.68\nHospital 85.83 0.82 9.33 44.87\nTraﬃc ﬁnes 90.00 0.87 40.28 98.24\nNext Activity Prediction Table 3 reports the (weighted) accuracy for the\nnext activity prediction task on ﬁve real-life event logs. Except for BPIC13,\nProcessTransformer consistently outperforms multiple baselines reported\nin the literature for the next activity task. Notably, we achieve 86%, 85%, 91%\nand 78% accuracy scores for Helpdesk, BPI2012, BPI2012w, and BPI2012cw\ndatasets, respectively. The low accuracy of BPIC13 can be attributed to the\nfewer but lengthy cases in the logs. We also show that on average across all the\ndatasets, ProcessTransformer outperforms other approaches.\nIt is worth noting that we achieve better generalization and performance with-\nout performing extensive preprocessing in terms of removing incomplete process\ntraces and 1-sized preﬁx having a single event only. Importantly, the model uti-\nlizes only event preﬁxes as an input without utilizing any hand-crafted features.\nThese design choices are made to report the realistic analysis and illustrate\nthe powerful learning capabilities of ProcessTransformer even with a sin-\ngle event preﬁx, duplicate activities, and excessively long process traces. Given\nthis, a direct comparison with some of the proposed methods from literature is\nPredictive Business Process Monitoring with Transformer Network 13\nTable 3.Accuracy score (in %) and averaged scores across all datasets for next ac-\ntivity prediction task (Higher is better). The baseline scores for comparison are taken\nfrom [28].\nHelpdesk BPIC12 BPIC12w BPIC12cw BPIC13 Avg.\nTax et al. [33] 75.06 85.20 84.90 67.80 67.50 76.09\nKhan et al. [15] 69.13 82.93 86.69 75.91 64.34 75.80\nCamargo et al. [4] 76.51 83.41 83.29 65.19 68.01 75.28\nEvermann et al. [10] 70.07 60.38 75.22 65.38 68.15 67.84\nMauro et al. [6] 74.77 84.56 85.11 65.01 71.09 76.11\nPasquadibisceglie et al. [24] 65.84 82.59 81.59 66.14 31.10 65.45\nProcessTransformer 85.63 85.20 91.51 78.48 62.11 80.58\nunjust due to inconsistent data preprocessing and additional input features for\nlearning, which can be missing in event logs in a real-world setting. For instance,\n[6] utilize additional event attributes, such as timestamp for event label predic-\ntion. Similarly, [33,24,15] performed excessive preprocessing on the Helpdesk\nand BPI2012 datasets in terms of eliminating process traces depending on their\nnumber of events and their duration. It results in a predictive model trained\nand evaluated on an ideal dataset, which does not reﬂect real-life complex event\nlogs. Furthermore, we do not provide a comparison against [35], as it utilizes\nadditional synthetic data for model training. However, our technique is comple-\nmentary and can be combined with GANs to improve performance further.\nEvent Time Prediction In Table 4, we present the MAE in days for the event\ntime prediction task against multiple baselines. Our approach has achieved the\nlowest MAE on average for all considered datasets compared to the previously\nproposed methods. Besides algorithmic speciﬁcations, our approach is diﬀerent\nto [33,21,15,3] in following aspects. We deal with the event time prediction as\nan independent task as opposed to the common multi-task approach. This is\nbecause the multi-task approach requires dual loss optimization, and there is no\nguarantee that it will perform better than its single-task counterpart [41]. We\nalso create additional temporal features (see Deﬁnition 4 in Section 4) to equip\nmodel with a sense of events’ duration. To summaries, ProcessTransformer\nobtains MAE of 2.98, 0.25, 0.37, 0.82 and 0.99 for Helpdesk, BPI2012, BPI2012w,\nBPI2012cw and BPIC13 datasets, respectively.\nTable 4.MAE (in days) and averaged scores across all datasets for event time predic-\ntion task (Lower is better). The baseline scores for comparison are taken from [28].\nHelpdesk BPIC12 BPIC12w BPIC12cw BPIC13 Avg.\nTax et al. [33] 5.77 0.31 0.50 1.20 0.47 1.65\nKhan et al. [15] 6.33 0.31 0.50 1.32 0.55 1.80\nProcessTransformer2.98 0.25 0.37 0.82 0.99 1.08\n14 Z. A. Bukhsh et al.\nRemaining Time Prediction We use the same model architecture and tem-\nporal features for the remaining time prediction problem as for event time pre-\ndiction. Table 5 reports the MAE scores averaged across all the preﬁxes. Our\napproach outperforms previous methods on average across considered datasets\n(see the last column of Table 5). Speciﬁcally, we obtain MAE of 3.72, 4.60, 4.87,\n5.14, and 8 .36 for Helpdesk, BPI2012 BPI2012w, and BPI2012cw and BPIC3\ndatasets, respectively. We note that, the performance diﬀerence between Tax et\nal [33] and Navarin et al [20] is due to diﬀerence in encoding techniques and\ntemporal features used, as they both use LSTM as a base model for learning the\npredictive task.\nTable 5.MAE (in days) and averaged scores across all datasets for remaining time pre-\ndiction task (Lower is better). The baseline scores for comparison are taken from [28].\nHelpdesk BPIC12 BPIC12w BPIC12cw BPIC13 Avg.\nTax et al. [33] 71.50 330.61 387.81 210.16 38.41 207.70\nCamargo et al. [4] 11.15 30.56 32.03 7.97 260.64 68.47\nNavarin et al. [20] 10.38 6.13 6.63 6.48 2.97 6.52\nProcessTransformer3.72 4.60 4.87 5.14 8.36 5.33\n6 Conclusion\nThe main contribution of our study is a ProcessTransformer approach for\nlearning high-level representations directly from sequential event logs data with\nminimal preprocessing. We evaluate our approach for the next activity, event\ntime, and remaining time prediction tasks on nine real-life event logs. We show\nthat ProcessTransformer can capture long-range dependencies without the\nexplicit need of recurrence as LSTM-based models. Our approach outperforms\nseveral existing baselines in experimental evaluations. Speciﬁcally, we achieve\nan average of above 80% accuracy on considered datasets for the next activity\nprediction task while solely using the activity preﬁx as an input for the model.\nSimilarly, our approach obtains an average MAE of 1.08 and 5.33 for predicting\nthe next event time and completion time of a running case. Notably, we use\nminimal data preprocessing and features as an input to illustrate the learning\ncapability of ProcessTransformer . This is also to emphasis that the Pro-\ncessTransformer can provide optimal predictive performance for real event\nlogs even when additional attributes, such as resources, roles, and others, are\nmissing in the dataset.\nThe future work seeks to study how the learned representations can be used\nfor other tasks of interest, including similar trace retrieval, activity recommen-\ndations, and process outcome prediction. Another future avenue is to evaluate\nproposed ProcessTransformer with event logs having not only prolonged\nbut largely unique process activity space.\nPredictive Business Process Monitoring with Transformer Network 15\nReproducibility The source code and supplementary material is publicly\navailable at https://github.com/Zaharah/processtransformer.\nReferences\n1. Van der Aalst, W.M., Schonenberg, M.H., Song, M.: Time prediction based on\nprocess mining. Information systems 36(2) (2011)\n2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n3. B¨ ohmer, K., Rinderle-Ma, S.: LoGo: Combining local and global techniques for\npredictive business process monitoring. In: Proc. of CAiSE. Springer (2020)\n4. Camargo, M., Dumas, M., Gonz´ alez-Rojas, O.: Learning accurate LSTM models\nof business processes. In: Proc. of BPM. Springer (2019)\n5. Conforti, R., De Leoni, M., La Rosa, M., Van Der Aalst, W.M.: Supporting risk-\ninformed decisions during business process execution. In: Proc. of CAiSE (2013)\n6. Di Mauro, N., Appice, A., Basile, T.M.: Activity prediction of business process\ninstances with inception CNN models. In: Proc. of AIIA. Springer (2019)\n7. van Dongen, B.: BPI Challenge 2012 (Apr 2012).\nhttps://doi.org/10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f\n8. van Dongen, B.: BPI Challenge 2020: Domestic Declarations (Mar 2020).\nhttps://doi.org/10.4121/uuid:3f422315-ed9d-4882-891f-e180b5b4feb5\n9. van Dongen, B.: BPI Challenge 2020: International Declarations (Mar 2020).\nhttps://doi.org/10.4121/uuid:2bbf8f6a-fc50-48eb-aa9e-c4ea5ef7e8c5\n10. Evermann, J., Rehse, J.R., Fettke, P.: A deep learning approach for predicting\nprocess behaviour at runtime. In: Proc. of BPM. pp. 327–338. Springer (2016)\n11. Folino, F., Guarascio, M., Pontieri, L.: Context-aware predictions on business pro-\ncesses: an ensemble-based solution. In: Proc. of NFMCP. Springer (2012)\n12. Guo, C., Berkhahn, F.: Entity embeddings of categorical variables. arXiv preprint\narXiv:1604.06737 (2016)\n13. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n9(8) (1997)\n14. Kang, B., Kim, D., Kang, S.H.: Periodic performance prediction for real-time busi-\nness process monitoring. Industrial Management & Data Systems (2012)\n15. Khan, A., Le, H., Do, K., Tran, T., Ghose, A., Dam, H., Sindhgatta, R.: Memory-\naugmented neural networks for predictive process analytics. arXiv preprint\narXiv:1802.00938 (2018)\n16. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n17. de Leoni, M.M., Mannhardt, F.: Road traﬃc ﬁne management process (Feb 2015).\nhttps://doi.org/10.4121/uuid:270fd440-1057-4fb9-89a9-b699b47990f5\n18. Lin, L., Wen, L., Wang, J.: Mm-pred: A deep predictive model for multi-attribute\nevent sequence. In: Proc. of ICDM. SIAM (2019)\n19. Mannhardt, F.: Hospital billing - event log (Aug 2017).\nhttps://doi.org/10.4121/uuid:76c46b83-c930-4798-a1c9-4be94dfeb741\n20. Navarin, N., Vincenzi, B., Polato, M., Sperduti, A.: LSTM networks for data-aware\nremaining time prediction of business process instances. In: Proc. of SSCI. IEEE\n(2017)\n21. Nguyen, A., Chatterjee, S., Weinzierl, S., Schwinn, L., Matzner, M., Eskoﬁer, B.:\nTime Matters: Time-Aware LSTMs for Predictive Business Process Monitoring.\narXiv preprint arXiv:2010.00889 (2020)\n16 Z. A. Bukhsh et al.\n22. Pandey, S., Nepal, S., Chen, S.: A test-bed for the evaluation of business process\nprediction techniques. In: 7th International Conference on Collaborative Comput-\ning: Networking, Applications and Worksharing. IEEE (2011)\n23. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.N., Bernardi, R., Pezzelle, S.,\nBaroni, M., Boleda, G., Fern´ andez, R.: The LAMBADA dataset: Word prediction\nrequiring a broad discourse context. arXiv preprint arXiv:1606.06031 (2016)\n24. Pasquadibisceglie, V., Appice, A., Castellano, G., Malerba, D.: Using convolutional\nneural networks for predictive process analytics. In: Proc. of ICPM. IEEE (2019)\n25. Pauwels, S., Calders, T.: Bayesian network based predictions of business processes.\nIn: Proc. of BPM. Springer (2020)\n26. Polato, M.: Dataset belonging to the help desk log of an italian company (Jul\n2017). https://doi.org/10.4121/uuid:0c60edf1-6f83-4e75-9367-4c63b3e9d5bb\n27. Polato, M., Sperduti, A., Burattin, A., de Leoni, M.: Time and activity sequence\nprediction of business process instances. Computing 100(9) (2018)\n28. Rama-Maneiro, E., Vidal, J.C., Lama, M.: Deep learning for predictive business\nprocess monitoring: Review and benchmark. preprint arXiv:2009.13251 (2020)\n29. Rogge-Solti, A., Weske, M.: Prediction of remaining service execution time using\nstochastic petri nets with arbitrary ﬁring delays. In: Proc. of ICSOC. Springer\n(2013)\n30. Sokolova, M., Lapalme, G.: A systematic analysis of performance measures for\nclassiﬁcation tasks. Information processing & management 45(4), 427–437 (2009)\n31. Steeman, W.: BPI Challenge 2013, incidents (Apr 2013).\nhttps://doi.org/10.4121/uuid:500573e6-accc-4b0c-9576-aa5468b10cee\n32. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\nnetworks. arXiv preprint arXiv:1409.3215 (2014)\n33. Tax, N., Verenich, I., La Rosa, M., Dumas, M.: Predictive business process moni-\ntoring with LSTM neural networks. In: Proc. of CAiSE. Springer (2017)\n34. Taymouri, F., La Rosa, M.: Encoder-decoder generative adversarial nets for suf-\nﬁx generation and remaining time predication of business process models. arXiv\npreprint arXiv:2007.16030 (2020)\n35. Taymouri, F., La Rosa, M., Erfani, S., Bozorgi, Z.D., Verenich, I.: Predictive busi-\nness process monitoring via generative adversarial nets: The case of next event\nprediction. arXiv preprint arXiv:2003.11268 (2020)\n36. Teinemaa, I., Dumas, M., Rosa, M.L., Maggi, F.M.: Outcome-oriented predictive\nprocess monitoring: Review and benchmark. TKDD 13(2) (2019)\n37. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Proc. of NeuroIPS (2017)\n38. Wang, J., Yu, D., Liu, C., Sun, X.: Outcome-oriented predictive process monitor-\ning with attention-based bidirectional LSTM neural networks. In: Proc. of ICWS.\nIEEE (2019)\n39. Weinzierl, S., Zilker, S., Brunk, J., Revoredo, K., Nguyen, A., Matzner, M., Becker,\nJ., Eskoﬁer, B.: An empirical comparison of deep-neural-network architectures for\nnext activity prediction using context-enriched process event logs. arXiv preprint\narXiv:2005.01194 (2020)\n40. Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P.,\nFuntowicz, M., Davison, J., Shleifer, S., et al.: Transformers: State-of-the-art nat-\nural language processing. In: Proc. of EMNLP (2020)\n41. Zhang, Y., Yang, Q.: A survey on multi-task learning. arXiv preprint\narXiv:1707.08114 (2017)",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.37086355686187744
    },
    {
      "name": "Business",
      "score": 0.3626074194908142
    }
  ],
  "topic": "Computer science"
}