{
  "title": "Taking a Stance on Fake News: Towards Automatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance Detection",
  "url": "https://openalex.org/W2990303507",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4289146908",
      "name": "Dulhanty, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287269680",
      "name": "Deglint, Jason L.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Daya, Ibrahim Ben",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2536545889",
      "name": "Wong, Alexander",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950437211",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963857245",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W2946364534",
    "https://openalex.org/W2794583223",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W2251214593",
    "https://openalex.org/W2798269087",
    "https://openalex.org/W2790166049",
    "https://openalex.org/W2798640866",
    "https://openalex.org/W2963058357",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W651477617",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2799228294",
    "https://openalex.org/W2963349562",
    "https://openalex.org/W2914572204",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2735017898",
    "https://openalex.org/W1532503642",
    "https://openalex.org/W2950948422",
    "https://openalex.org/W2950782805",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2791544114",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2952996097",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2741386817",
    "https://openalex.org/W2970583189"
  ],
  "abstract": "The exponential rise of social media and digital news in the past decade has had the unfortunate consequence of escalating what the United Nations has called a global topic of concern: the growing prevalence of disinformation. Given the complexity and time-consuming nature of combating disinformation through human assessment, one is motivated to explore harnessing AI solutions to automatically assess news articles for the presence of disinformation. A valuable first step towards automatic identification of disinformation is stance detection, where given a claim and a news article, the aim is to predict if the article agrees, disagrees, takes no position, or is unrelated to the claim. Existing approaches in literature have largely relied on hand-engineered features or shallow learned representations (e.g., word embeddings) to encode the claim-article pairs, which can limit the level of representational expressiveness needed to tackle the high complexity of disinformation identification. In this work, we explore the notion of harnessing large-scale deep bidirectional transformer language models for encoding claim-article pairs in an effort to construct state-of-the-art stance detection geared for identifying disinformation. Taking advantage of bidirectional cross-attention between claim-article pairs via pair encoding with self-attention, we construct a large-scale language model for stance detection by performing transfer learning on a RoBERTa deep bidirectional transformer language model, and were able to achieve state-of-the-art performance (weighted accuracy of 90.01%) on the Fake News Challenge Stage 1 (FNC-I) benchmark. These promising results serve as motivation for harnessing such large-scale language models as powerful building blocks for creating effective AI solutions to combat disinformation.",
  "full_text": "Taking a Stance on Fake News: Towards Automatic\nDisinformation Assessment via Deep Bidirectional\nTransformer Language Models for Stance Detection\nChris Dulhanty, Jason L. Deglint, Ibrahim Ben Daya and Alexander Wong\nSystems Design Engineering, University of Waterloo\n{chris.dulhanty, jdeglint, ibendaya, a28wong}@uwaterloo.ca\nAbstract\nThe exponential rise of social media and digital news in the past decade has had the\nunfortunate consequence of escalating what the United Nations has called a global\ntopic of concern: the growing prevalence of disinformation1. Given the complexity\nand time-consuming nature of combating disinformation through human assess-\nment, one is motivated to explore harnessing AI solutions to automatically assess\nnews articles for the presence of disinformation. A valuable ﬁrst step towards\nautomatic identiﬁcation of disinformation is stance detection, where given a claim\nand a news article, the aim is to predict if the article agrees, disagrees, takes no\nposition, or is unrelated to the claim. Existing approaches in literature have largely\nrelied on hand-engineered features or shallow learned representations (e.g., word\nembeddings) to encode the claim-article pairs, which can limit the level of repre-\nsentational expressiveness needed to tackle the high complexity of disinformation\nidentiﬁcation. In this work, we explore the notion of harnessing large-scale deep\nbidirectional transformer language models for encoding claim-article pairs in an\neffort to construct state-of-the-art stance detection geared for identifying disinfor-\nmation. Taking advantage of bidirectional cross-attention between claim-article\npairs via pair encoding with self-attention, we construct a large-scale language\nmodel for stance detection by performing transfer learning on a RoBERTa deep\nbidirectional transformer language model, and were able to achieve state-of-the-art\nperformance (weighted accuracy of 90.01%) on the Fake News Challenge Stage 1\n(FNC-I) benchmark. These promising results serve as motivation for harnessing\nsuch large-scale language models as powerful building blocks for creating effective\nAI solutions to combat disinformation.\n1 Introduction\nDisinformation presents a serious threat to society, as the proliferation of fake news can have a\nsigniﬁcant impact on an individual’s perception of reality. Fake news is a claim or story that is\nfabricated, with the intention to deceive, often for a secondary motive such as economic or political\ngain [1]. In the age of digital news and social media, fake news can spread rapidly, impacting large\namounts of people in a short period of time [ 2]. To mitigate the negative impact of fake news on\nsociety, various organizations now employ personnel to verify dubious claims through a manual\nfact-checking procedure, however, this process is very laborious. With a fast-paced modern news\ncycle, many journalists and fact-checkers are under increased stress to be more efﬁcient in their daily\nwork. To assist in this process, automated fact-checking has been proposed as a potential solution\n[3, 4, 5, 6, 7].\n1https://www.ohchr.org/EN/NewsEvents/Pages/DisplayNews.aspx?NewsID=21287&LangID=E\nAI for Social Good workshop at NeurIPS (2019), Vancouver, Canada.\narXiv:1911.11951v1  [cs.CL]  27 Nov 2019\nAutomated fact-checking systems aim to assess the veracity of claims through the collection and\nassessment of news articles and other relevant documents pertaining to the claim at hand. These\nsystems have the potential to augment the work of professional fact-checkers, as well as provide\na tool to the public to verify claims they come across online or in their daily lives. An automated\nfact-checking system consists of several sub-tasks that, when combined, can predict if a claim is\ntruthful [8]. Document retrieval aims to gather relevant articles regarding the claim from a variety\nof sources. Stance detection aims to determine the position of each article with respect to the\nclaim. Reputation assessment aims to determine the trustworthiness of each article by analyzing\nits linguistics and source. Claim veriﬁcation aims to combine stance and reputation information to\ndetermine the truthfulness of the claim.\nIn this paper, we focus on stance detection; given a proposed claim and article, predict if the article\nagrees, disagrees, has no stance, or is unrelated to the claim. Within the natural language processing\n(NLP) community, research in stance detection has been catalyzed by the organization of competitions\n[9, 10, 11] and the collection of benchmark datasets [ 12, 13, 14]. Prominent methods addressing\nstance detection largely differ in terms of their feature representation (e.g., n-grams, TF-IDF, word\nembeddings, etc.) and algorithms (e.g., decision trees, multi-layer perceptions, LSTM networks, etc.);\nretrospectives on recent challenges [9, 10, 15] provide a comprehensive overview of NLP methods in\nstance detection. While results have been promising, recent developments in NLP hold the potential\nfor signiﬁcant improvement. Whereas pre-trained word embeddings such as word2vec [ 16] and\nGloVe [17] encode language into shallow numerical representations for input to machine learning\nmodels, deep bidirectional transformer language models [18, 19, 20, 21, 22] train on large, unlabelled\ndatasets to learn deeper hierarchical representations of language. The result has been a signiﬁcant\nimprovement on multi-task NLP benchmarks [23], akin to an \"ImageNet moment\"2 for the ﬁeld.\nMotivated by recent advances in NLP and the potential of this technology to meaningfully impact\nsociety by addressing the United Nation’s Sustainable Development Goals of \"Quality Education\"\nand \"Peace, Justice, and Strong Institutions\"3, we explore the notion of harnessing large-scale deep\nbidirectional transform language models for achieving state-of-the-art stance detection. Our major\ncontributions are: (1) constructing a large-scale language model for stance detection by performing\ntransfer learning on a RoBERTa deep bidirectional transformer language model by taking advantage\nof bidirectional cross-attention between claim-article pairs via pair encoding with self-attention, and\n(2) state-of-the-art results on the Fake News Challenge Stage 1 (FNC-I)4 benchmark.\n2 Methodology\nThe RoBERTa (Robustly Optimized BERT Approach) model, released in July 2019 by Liuet al. [24],\nis an open-source language model that achieves state-of-the-art results on benchmark NLP multi-task\nGeneral Language Understanding Evaluation (GLUE) benchmark [23]. RoBERTa is built upon the\nBERT (Bidirectional Encoder Representations from Transformers) model, released by Devlinet al.\nin October 2018 [20]. RoBERTa and BERT achieve high performance by pretraining a transformer\nmodel, initially proposed by Vaswani et al. [18], in a bidirectional manner on a very large corpus of\nunlabelled text, and ﬁne-tuning the model on a relatively small amount task-speciﬁc labelled data.\nThese models are well-suited for use in stance detection as the pretrained model can be leveraged to\nperform transfer learning on the target task. Using deep bidirectional transformer language models,\nRoBERTa and BERT have the ability to gain a deeper understanding of language and context when\ncompared to earlier unidirectional transformer architectures [20]. In addition, RoBERTa demonstrates\ngreat results on sentence-pair classiﬁcation tasks of GLUE, such as Multi-Genre Natural Language\nInference [25] and Question Natural Language Inference [ 26, 23], tasks very similar in nature to\nthe claim-article classiﬁcation of stance detection. Following RoBERTa’s method of ﬁne-tuning on\nGLUE tasks, we include both claim and article, separated by a special token, in each example during\ntraining and inference.\n2http://ruder.io/nlp-imagenet/\n3https://sustainabledevelopment.un.org\n4http://www.fakenewschallenge.org/\n2\nTable 1: Statistics of the FNC-I Dataset\nTraining Set Test Set\n# of claim-article pairs 49,972 25,413\n% unrelated 73.13 72.20\n% discuss 17.83 17.57\n% agree 7.36 7.49\n% disagree 1.68 2.74\n3 Experiments and Analysis\n3.1 Dataset\nTo investigate the task of stance detection in the context of fake news detection, we use data released\nfor the Fake News Challenge, Stage 1 (FNC-I). The challenge was organized by Pomerleau and Rao in\n2017, with the goal of estimating the stance of an article with respect to a claim. Data is derived from\nthe Emergent dataset [12], sourced from the Emergent Project5, a real-time rumour tracker created by\nthe Tow Center for Digital Journalism at Columbia University. The stance takes one of four labels:\nAgree if the article agrees with the claim, Disagree if the article disagrees with the claim, Discuss if\nthe article is related to the claim, but the author takes no position on the subject, and Unrelated if the\ncontent of the article is unrelated to the claim. There are approximately 50k claim-article pairs in the\ntraining set and 25k pairs in the test set; Table 1 summarizes the data distribution.\n3.2 Metrics\nTo evaluate the performance of our method, we report standard accuracy as well as weighted accuracy,\nsuggested by the organizers of the Fake News Challenge, as it provides a more objective metric for\ncomparison given the class imbalance in the dataset. The weighted accuracy, Accw, is expressed as:\nAccw = 0.25 × Accr,u + 0.75 × Acca,d,d (1)\nwhere Accr,u is the binary accuracy across related {agree, disagree, discuss} and unrelated article-\nclaim pairs, and Acca,d,d is the accuracy for pairs in related classes only.\n3.3 Model\nWe construct our large-scale language model via transfer learning on a pretrained RoBERTaBASE deep\ntransformer model, consisting of 12-layers of 768-hidden units, each with 12 attention heads, totalling\n125M parameters. We leverage the Transformers library by Hugging Face for implementation [27].\nTo perform transfer learning, we train for three epochs and follow hyperparameter recommendations\nby Liu et al. [24] for ﬁne-tuning on GLUE tasks, namely, a learning rate of 2e-5 and weight decay of\n0.1. We train on one NVIDIA 1080Ti GPU with a batch size of 8.\nPrior to training, the dataset is pre-processed by initializing each example with a start token to signify\nthe beginning of a sequence, followed by the claim, two separator tokens, the article and an additional\nseparator token. The sequence is then tokenized by RoBERTa’s byte-level byte-pair-encoding and\ntrimmed or padded to ﬁt a maximum sequence length of 512. We explore the effects of claim-article\npair sequence length and maximum sequence length on classiﬁcation accuracy in the Appendix.\n3.4 Results & Discussion\nResults of our proposed method, the top three methods in the original Fake News Challenge, and the\nbest-performing methods since the challenge’s conclusion on the FNC-I test set are displayed in Table\n2. A confusion matrix for our method is presented in the Appendix. To the best of our knowledge, our\nmethod achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset.\nNotably, since the conclusion of the Fake News Challenge in 2017, the weighted-accuracy error-rate\nhas decreased by 8%, signifying improved performance of NLP models and innovations in the domain\nof stance detection, as well as a continued interest in combating the spread of disinformation.\n5http://www.emergent.info/\n3\nTable 2: Performance of various methods on the FNC-I benchmark. The ﬁrst and second groups are\nmethods introduced during and after the challenge period, respectively. Best results are in bold.\nMethod Accw Acc\nRiedel et al. [28] 81.72 88.46\nHanselowski et al. [29] 81.97 89.48\nBaird et al. [30] 82.02 89.08\nBhatt et al. [31] 83.08 89.29\nBorges et al. [32] 83.38 89.21\nZhang et al. 2018 [33] 86.66 92.00\nWang et al. [34] 86.72 82.91\nZhang et al. 2019 [35] 88.15 93.50\nProposed Method 90.01 93.71\n4 Ethical Considerations\nImplementation and potential end-users: The implementation of our stance detection model\ninto a real-world system is predicated on the development of solutions to the document retrieval,\nreputation assessment and claim veriﬁcation elements of an automated fact-checking system. While\nthis is an active ﬁeld of research, it is imperative to note that the reputation assessment sub-task\nis difﬁcult, as the trustworthiness of an individual or media source may be interpreted differently\nby different individuals due to personal bias. Provided these elements can be developed, the ﬁrst\nintended end-users of an automated fact-checking system should be journalists and fact-checkers.\nValidation of the system through the lens of experts of the fact-checking process is something that the\nsystem’s performance on benchmark datasets cannot provide. The implementation of such a system\ninto the daily workﬂow of these individuals is likely a ﬁeld of research onto itself. Ultimately, the\ndevelopment of a simple user interface for the general public, such as a browser plug-in, is the goal\nof this system, assisting individuals to stay informed citizens.\nLimitations: The model proposed in this work is limited by the fact that it was trained solely\non claims and articles in English, from western-focused media outlets. Further work is necessary\nto extend this work to other languages, where differences in writing style and cultural norms and\nnuances may lead to differences in performance. In addition, this model is not designed to deal with\nsatire, where the stance of an article with respect to a claim may appear on the surface to be one way,\nbut the underlying intention of its author is to exploit humor to demonstrate an opposing viewpoint.\nRisks and potential unintended negative outcomes:A major risk of a stance detection model or\nautomated fact-checking system is the codiﬁcation of unintended biases into the model through biased\ntraining data. In the ﬁeld of NLP, gender and racial biases have been reported in word embeddings\n[36, 37] and captioning models [38]; the extent to which such social biases are encoded in recently\ndeveloped language models is only beginning to be studied [39, 40]. A secondary risk to the roll-out\nof these systems for adversarial attacks. Early work by Hsieh et al. to investigate the robustness\nof self-attentive architectures has demonstrated that adversarial examples that could mislead neural\nlanguage models but not humans are capable of being developed for sentiment analysis, entailment\nand machine translation [ 41]. In addition, the development of such a system may be interpreted\nby some as to provide a deﬁnitive answer with respect to the truthfulness of a claim, rather than a\npredictive estimate of its veracity. A potential unintended negative outcome of this work is for people\nto take the outputs of an automated fact-checking system as the deﬁnitive truth, without using their\nown judgement, or for malicious actors to selectively promote claims that may be misclassiﬁed by\nthe model but adhere to their own agenda.\n5 Conclusions\nWe have presented a state-of-the-art large-scale language model for stance detection based upon a\nRoBERTa deep bidirectional transformer. Our promising results motivate efforts to develop additional\nsub-components of a fully automated fact-checking system such that AI can effectively be harnessed\nto combat disinformation and allow citizens and democratic institutions to thrive.\n4\nReferences\n[1] David MJ Lazer, Matthew A Baum, Yochai Benkler, Adam J Berinsky, Kelly M Greenhill,\nFilippo Menczer, Miriam J Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild,\net al. The science of fake news. Science, 359(6380):1094–1096, 2018.\n[2] Soroush V osoughi, Deb Roy, and Sinan Aral. The spread of true and false news online.Science,\n359(6380):1146–1151, 2018.\n[3] You Wu, Pankaj K Agarwal, Chengkai Li, Jun Yang, and Cong Yu. Toward computational\nfact-checking. Proceedings of the VLDB Endowment, 7(7):589–600, 2014.\n[4] Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M Rocha, Johan Bollen, Filippo Menczer,\nand Alessandro Flammini. Computational fact checking from knowledge networks. PloS one,\n10(6):e0128193, 2015.\n[5] Georgi Karadzhov, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and Ivan Koychev.\nFully automated fact checking using external sources. arXiv preprint arXiv:1710.00341, 2017.\n[6] Naeemul Hassan, Fatma Arslan, Chengkai Li, and Mark Tremayne. Toward automated fact-\nchecking: Detecting check-worthy factual claims by claimbuster. In Proceedings of the 23rd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages\n1803–1812. ACM, 2017.\n[7] Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, and James Glass. Fakta: An automatic\nend-to-end fact checking system. arXiv preprint arXiv:1906.04164, 2019.\n[8] Andreas Vlachos and Sebastian Riedel. Fact checking: Task deﬁnition and dataset construction.\nIn Proceedings of the ACL 2014 Workshop on Language Technologies and Computational\nSocial Science, pages 18–22, 2014.\n[9] Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry.\nSemeval-2016 task 6: Detecting stance in tweets. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016), pages 31–41, 2016.\n[10] Leon Derczynski, Kalina Bontcheva, Maria Liakata, Rob Procter, Geraldine Wong Sak Hoi, and\nArkaitz Zubiaga. Semeval-2017 task 8: Rumoureval: Determining rumour veracity and support\nfor rumours. arXiv preprint arXiv:1704.05972, 2017.\n[11] Dean Pomerleau and Delip Rao. Fake news challenge, 2017.\n[12] William Ferreira and Andreas Vlachos. Emergent: a novel data-set for stance classiﬁcation.\nIn Proceedings of the 2016 conference of the North American chapter of the association for\ncomputational linguistics: Human language technologies, pages 1163–1168, 2016.\n[13] Parinaz Sobhani, Diana Inkpen, and Xiaodan Zhu. A dataset for multi-target stance detec-\ntion. In Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers, pages 551–557, 2017.\n[14] Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. Seeing\nthings from a different angle: Discovering diverse perspectives about claims. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 542–557,\n2019.\n[15] Andreas Hanselowski, Avinesh PVS, Benjamin Schiller, Felix Caspelherr, Debanjan Chaudhuri,\nChristian M Meyer, and Iryna Gurevych. A retrospective analysis of the fake news challenge\nstance detection task. arXiv preprint arXiv:1806.05180, 2018.\n[16] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems, pages 3111–3119, 2013.\n[17] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543, 2014.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n5\n[19] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[21] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019.\n[22] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\n[23] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[25] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\n[26] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[27] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s\ntransformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\n[28] Benjamin Riedel, Isabelle Augenstein, Georgios P. Spithourakis, and Sebastian Riedel. A\nsimple but tough-to-beat baseline for the Fake News Challenge stance detection task. CoRR,\nabs/1707.03264, 2017.\n[29] Andreas Hanselowski, Avinesh PVS, Benjamin Schiller, and Felix Caspelherr. Description of\nthe system developed by team athene in the fnc-1, 2017.\n[30] Sean Baird, Doug Sibley, and Yuxi Pan. Talos targets disinformation with fake news challenge\nvictory, 2017.\n[31] Gaurav Bhatt, Aman Sharma, Shivam Sharma, Ankush Nagpal, Balasubramanian Raman,\nand Ankush Mittal. Combining neural, statistical and external features for fake news stance\nidentiﬁcation. In Companion Proceedings of the The Web Conference 2018, WWW ’18, pages\n1353–1357, Republic and Canton of Geneva, Switzerland, 2018. International World Wide Web\nConferences Steering Committee.\n[32] Luís Borges, Bruno Martins, and Pável Calado. Combining similarity features and deep\nrepresentation learning for stance detection in the context of checking fake news. ACM Journal\nof Data Information and Quality, 2019.\n[33] Qiang Zhang, Emine Yilmaz, and Shangsong Liang. Ranking-based method for news stance\ndetection. In Companion Proceedings of the The Web Conference 2018 , WWW ’18, pages\n41–42, Republic and Canton of Geneva, Switzerland, 2018. International World Wide Web\nConferences Steering Committee.\n[34] Xuezhi Wang, Cong Yu, Simon Baumgartner, and Flip Korn. Relevant document discovery for\nfact-checking articles. In Companion Proceedings of the The Web Conference 2018, WWW\n’18, pages 525–533, Republic and Canton of Geneva, Switzerland, 2018. International World\nWide Web Conferences Steering Committee.\n[35] Qiang Zhang, Shangsong Liang, Aldo Lipani, Zhaochun Ren, and Emine Yilmaz. From stances’\nimbalance to their hierarchical representation and detection. InThe World Wide Web Conference,\nWWW ’19, pages 2323–2332, New York, NY , USA, 2019. ACM.\n[36] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai.\nMan is to computer programmer as woman is to homemaker? debiasing word embeddings. In\nAdvances in neural information processing systems, pages 4349–4357, 2016.\n6\n[37] Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. Word embeddings quantify\n100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences,\n115(16):E3635–E3644, 2018.\n[38] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women\nalso snowboard: Overcoming bias in captioning models. In European Conference on Computer\nVision, pages 793–811. Springer, 2018.\n[39] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. Measuring Bias in\nContextualized Word Representations. arXiv e-prints, page arXiv:1906.07337, Jun 2019.\n[40] Yi Chern Tan and L. Elisa Celis. Assessing social and intersectional biases in contextualized\nword representations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 13209–13220.\nCurran Associates, Inc., 2019.\n[41] Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho-Jui Hsieh.\nOn the robustness of self-attentive models. In Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, pages 1520–1529, 2019.\n7\nA Claim-Article Pair Sequence Length\nTable 3 presents the results of the RoBERTa model on the FNC-I test set, based on the length of\nclaim-article pair. The model has a maximum sequence length of 512 tokens, so any examples longer\nthan this are trimmed. We ﬁnd that the model performs best for examples that utilize the full capacity\nof the input sequence (385 to 512 tokens). Very short sequences (<129 tokens) provide the least\namount of information to the model, and the model performs poorly. Long sequences (>512 tokens)\nhave some of their context removed from their input, and these examples also perform relatively poor.\nTable 3: Effect of claim-article pair sequence length of FNC-I test set on classiﬁcation accuracy of\nRoBERTa model, with a maximum sequence length of 512.\nNumber of Tokens in Example Acc Number of Examples\n<129 92.05 2904\n129-256 93.90 3606\n257-384 95.07 6328\n385-512 95.11 4763\n>512 92.23 7812\nAll 93.71 25413\nB Maximum Sequence Length\nTable 4 presents the results of RoBERTa models of varying maximum sequence lengths on the FNC-I\ntest set. We ﬁnd an increase in accuracy with a longer maximum sequence length, as more context\nis provided to the model. We cannot increase the length of the input sequence beyond 512 tokens\nwithout training the RoBERTa model from scratch, which is not feasible for us.\nTable 4: Effect of maximum sequence length of RoBERTa model on weighted accuracy and classiﬁ-\ncation accuracy.\nMaximum Number of Tokens Accw Acc\n128 89.52 93.46\n256 89.54 93.48\n512 90.01 93.71\nC Confusion Matrices\nFigures 1 and 2 present confusion matrices for the previous best method and our proposed method on\nthe FNC-I test set.\nFigure 1: Confusion matrix for Zhang et al. [35].\n Figure 2: Confusion matrix for proposed method.\n8",
  "topic": "Disinformation",
  "concepts": [
    {
      "name": "Disinformation",
      "score": 0.9324694871902466
    },
    {
      "name": "Computer science",
      "score": 0.7435664534568787
    },
    {
      "name": "Construct (python library)",
      "score": 0.5570157170295715
    },
    {
      "name": "Language model",
      "score": 0.520885705947876
    },
    {
      "name": "Transformer",
      "score": 0.5034560561180115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.493524432182312
    },
    {
      "name": "Social media",
      "score": 0.460782915353775
    },
    {
      "name": "Deep learning",
      "score": 0.4570072591304779
    },
    {
      "name": "Natural language processing",
      "score": 0.38684171438217163
    },
    {
      "name": "Data science",
      "score": 0.3441486954689026
    },
    {
      "name": "Machine learning",
      "score": 0.3239460587501526
    },
    {
      "name": "World Wide Web",
      "score": 0.13471472263336182
    },
    {
      "name": "Engineering",
      "score": 0.08597239851951599
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ],
  "cited_by": 23
}