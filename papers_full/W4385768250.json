{
  "title": "Black-box Prompt Tuning for Vision-Language Model as a Service",
  "url": "https://openalex.org/W4385768250",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097310566",
      "name": "Lang Yu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1977215684",
      "name": "Qin Chen",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2642144699",
      "name": "Jiaju Lin",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2105816867",
      "name": "Liang He",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2845268560",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4385574214",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W4286336838",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W24089286",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W2192203593",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2592891920",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3095415862",
    "https://openalex.org/W12634471",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2112036188",
    "https://openalex.org/W3102143361",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4385573069",
    "https://openalex.org/W2105014696"
  ],
  "abstract": "In the scenario of Model-as-a-Service (MaaS), pre-trained models are usually released as inference APIs. Users are allowed to query those models with manually crafted prompts. Without accessing the network structure and gradient information, it's tricky to perform continuous prompt tuning on MaaS, especially for vision-language models (VLMs) considering cross-modal interaction. In this paper, we propose a black-box prompt tuning framework for VLMs to learn task-relevant prompts without back-propagation. In particular, the vision and language prompts are jointly optimized in the intrinsic parameter subspace with various evolution strategies. Different prompt variants are also explored to enhance the cross-model interaction. Experimental results show that our proposed black-box prompt tuning framework outperforms both hand-crafted prompt engineering and gradient-based prompt learning methods, which serves as evidence of its capability to train task-relevant prompts in a derivative-free manner.",
  "full_text": "Black-box Prompt Tuning for Vision-Language Model as a Service\nLang Yu1,2 , Qin Chen1,2∗ , Jiaju Lin1 and Liang He1,2\n1School of Computer Science and Technology, East China Normal University\n2Shanghai Institute of AI for Education, East China Normal University\n{lyu, jiaju lin}@stu.ecnu.edu.cn, {qchen, lhe}@cs.ecnu.edu.cn\nAbstract\nIn the scenario of Model-as-a-Service (MaaS), pre-\ntrained models are usually released as inference\nAPIs. Users are allowed to query those models\nwith manually crafted prompts. Without access-\ning the network structure and gradient informa-\ntion, it’s tricky to perform continuous prompt tun-\ning on MaaS, especially for vision-language mod-\nels (VLMs) considering cross-modal interaction. In\nthis paper, we propose a black-box prompt tun-\ning framework for VLMs to learn task-relevant\nprompts without back-propagation. In particular,\nthe vision and language prompts are jointly opti-\nmized in the intrinsic parameter subspace with var-\nious evolution strategies. Different prompt variants\nare also explored to enhance the cross-model in-\nteraction. Experimental results show that our pro-\nposed black-box prompt tuning framework outper-\nforms both hand-crafted prompt engineering and\ngradient-based prompt learning methods, which\nserves as evidence of its capability to train task-\nrelevant prompts in a derivative-free manner.\n1 Introduction\nWith the promise to learn universal cross-model represen-\ntations, pre-trained vision-language models (VLMs) have\nachieved impressive performance in an extensive range of re-\nsearch fields [Du et al., 2022]. Increased attention has fo-\ncused on potential fine-tuning approaches to adapt these mod-\nels to downstream tasks (e.g. linear probe [Radford et al.,\n2021], adapter tuning [Houlsby et al., 2019] and prompt tun-\ning [Li and Liang, 2021 ]). However, making VLMs bene-\nfit everyone is still challenging. On the one hand, given a\nlarge number of tunable parameters, previous fine-tuning ap-\nproaches can be computationally expensive; Moreover, insti-\ntutes tend to keep the pre-trained model parameters closed-\nsource due to commercial considerations such as GPT-3\n[Brown et al., 2020]. Since no gradient information is avail-\nable, it’s tricky for local users to perform fine-tuning when a\nmodel is deployed as a remote service (MaaS).\n∗Corresponding Author\nDerivative-free Optimization, also referred as Black-box\nOptimization, is a promising way to optimize problems with-\nout the availability of gradient [Larson et al., 2019]. Over\nthe past decades, various methods have been explored, such\nas Bayesian Optimization [Shahriari et al., 2015] and Evo-\nlution Strategies (ESs) [Loshchilov, 2014]. Particularly, ESs\nare widely used in automated machine learning. However,\nmost ESs suffer from the “high-dimensional” problem and\ncan only deal with thousands of parameters, which poses a\nchallenge for black-box optimization over the large-scale pre-\ntrained models, especially for the VLMs that involves multi-\nple modalities with more parameters.\nRecently, [Sun et al., 2022b] provide a solution to per-\nform black-box optimization on pre-trained language mod-\nels (LMs). Inspired by the low intrinsic dimension of LMs\n[Aghajanyan et al., 2021], the authors use projected prompts\nto bridge the gap between black-box optimization and fine-\ntuning. Whereas, this work is restricted to the single linguistic\nmodality, and no cross-modal interaction is involved. More-\nover, how to effectively find the optimal intrinsic vector in a\nmulti-modal parameter subspace remains to be studied.\nIn this paper, we propose a Black-box Prompt Tuning\nframework for VLMs (BPT-VLM) to set the stage in the sce-\nnario of MaaS. Our framework regards multi-modal prompt\ntuning as black-box optimization based on empirically suc-\ncessful ESs (e.g. CMA-ES [Hansen and Ostermeier, 2001]).\nFrom the perspective of evolutionary learning, our BPT-VLM\nmainly consists of three parts (as shown in Figure 2):\n(1) Population: According to the previous finding that\npre-trained models have very low intrinsic dimensions\n[Aghajanyan et al., 2021], optimization can actually per-\nform on individuals (intrinsic vectors) from a small pa-\nrameter subspace. Individuals forming a population can\nbe further evaluated for distribution updates.\n(2) Objective Function: As shown in Figure 1(a), model as\na service is only allowed to perform forward pass, thus\nit’s reasonable to define VLM as a black-box objective\nfunction to evaluate the fitness values of individuals;\n(3) Optimization Algorithm: With the fitness values of a\npopulation of individuals, an evolution-based optimiza-\ntion algorithm updates its multivariate distribution to\nproduce a higher-quality population in the next gener-\nation (Figure 1(b)).\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1686\nIntrinsic vectors participate in VLM’s forward passes as\nprojected vision and language prompts. Depending on the\nlength of propagation path, we explore two variants of prompt\ntuning for BPT-VLM, namely Shallow Prompt and Deep\nPrompt. After generations of black-box optimization, the\nmultivariate distribution can produce solid intrinsic vectors\nwith low loss value. In other words, the task-relevant vi-\nsion and language prompts are learned in a derivative-free\nmanner. Compared to [Sun et al., 2022b], BPT-VLM con-\nsiders both the visual and linguistic modalities in black-box\ntuning, and involves cross-modal interaction by optimizing\nvision-language prompts in the shared intrinsic subspace. Ex-\nperimental results on 9 downstream tasks show that BPT-\nVLM not only surpasses the hand-crafted prompts, but also\noutperforms the prompts learned by gradient-based methods,\nnamely Linear Probe [Radford et al., 2021] and CoOp [Zhou\net al., 2022]. The main contributions of our work can be sum-\nmarized as follows1:\n• We propose a novel black-box prompt tuning framework\nfor VLMs in the scenario of MaaS, which incorporates\ncross-modal interaction by sharing the intrinsic parame-\nter subspace of both vision and language modalities and\njointly optimizing the prompts with different modalities\nin a derivative-free manner.\n• We extend traditional evolution strategies (CMA-ES,\nMM-ES, MA-ES) to a new scope of black-box prompt\ntuning on VLMs, and explore different prompt tuning\nvariants (shallow and deep prompt) to further enhance\nthe cross-modal interaction.\n• Extensive experimental results show that prompts op-\ntimized in multi-modal intrinsic subspace can success-\nfully adapt VLM to downstream tasks without accessing\nthe gradient and model structure, which is more effective\nand efficient compared with the baselines.\n2 Related Work\n2.1 Vision-Language Models\nWith the success of pre-trained models in the field of CV\nand NLP, many works attempted to pre-train large-scale mod-\nels on both vision and language modalities, called Vision-\nLanguage Models (VLMs). According to the method of\nintegrating multi-modal information, researchers categorize\nVLMs into fusion encoder-based models and dual encoder-\nbased models. Fusion encoder-based VLMs feed the text\nembedding and image features into a unified Transformer or\ndual Transformers to perform further self-attention or cross-\nattention, such that information from two modalities could\nbe integrated. Instead of relying on heavy transformer net-\nworks to model vision-language interaction, dual encoder-\nbased VLMs adopt straightforward methods such as shallow\nattention layer or dot product to project the image embedding\nand text embedding to the same semantic space.\nA representative of dual encoder-based VLMs is CLIP,\nwhich trains two single-model encoders using a contrastive\nloss to compute similarity scores for matching image-text\n1Code is available at https://github.com/BruthYU/BPT-VLM\n(a) Gradient-based and Black-box Prompt Tuning\n(b) Covariance Matrix Adaptation Evolution Strategy\nFigure 1: As shown in (a), black-box prompt tuning aims to optimize\nimage and text prompts with derivative-free algorithms. The entire\nmodel is regarded as an objective function to evaluate the fitness of\nindividuals. (b) gives an example of covariance matrix adaptation\nevolution, which searches for the minimum value of a binary func-\ntion by adjusting the distribution of individuals through generations.\npairs. After pre-training on 400 million data pairs collected\nfrom the Internet, CLIP demonstrates remarkable zero-shot\nimage recognition capability on downstream tasks.\n2.2 Prompt-based Learning\nPrompt-based Learning originates from the NLP domain,\nwhich refers to prepend language instructions to the input\ntext, and therefore reduces the gap between pre-trained LMs\nand downstream tasks. For instance, with the manually de-\nsigned prompt “English: Hello, French: [MASK]”, GPT-3\nwill output “Bonjour” as the result of a translation task. In\naddition to discrete prompts, recent works propose to treat\nprompt as continuous vectors and optimize them with gradi-\nent descent (see Figure 1(a)). Furthermore, prompt tuning can\nalso be applied successfully to other fields: VPT [Jia et al.,\n2022] extend prompt tuning to ViT for performance improve-\nment on vision tasks; CoOp [Zhou et al., 2022] introduces\ngradient-based prompt tuning to vision-language model.\nIn contrast to gradient-based learning, [Sun et al., 2022b]\nempirically show that black-box tuning is feasible on large-\nscale LMs with prompts acting as a bridge, since the intrinsic\ndimension (the minimum number of parameters needed to be\noptimized) of pre-trained models can be compressed to sev-\neral hundreds [Aghajanyan et al., 2021].\n2.3 Black-box Optimization Algorithms\nBlack-box optimization refers to optimizing the objective\nfunction without knowing the analytic expression and gra-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1687\nFigure 2: Overview of our black-box prompt tuning framework for VLMs. The recent advanced CLIP model is used as a VLM backbone.\nGiven sampled joint intrinsic vectors (individuals), the matrices WL and WV project [zL∥zV ] into text and image prompts pL and pV .\nThrough the forward pass of CLIP, these prompts are then evaluated on a downstream few-shot dataset. According to each sampled intrinsic\nvector’s fitness value (loss value), the derivative-free algorithms adjust the distribution and reproduce a new generation of individuals.\ndient information, which is similar to model fine-tuning in\nthe scenario of MaaS. Here we mainly review the studies of\nEvolution Strategies (ESs) - a class of powerful algorithms\nfor black-box optimization. CMA-ES [Hansen and Oster-\nmeier, 2001] is one of the most successful implementations\nof evolution strategy, which adapts the covariance matrix to\napproximate the shape of function landscape. The adaptation\nof CMA-ES aims to increase the probability of reproducing\nindividuals towards promising search directions.\nHowever, the sampling procedure requires the decomposi-\ntion of the covariance matrix, which leads toΘ(n2) time com-\nplexity. Researchers have worked to reduce the computation\nthe simplify the adaptation. MA-ES [Beyer and Sendhoff,\n2017] removes the evolution path the simplify the update for\nthe Cholesky factor; MM-ES [He et al., 2020] applies fast\nmixture sampling to approximate the covariance matrix C in\na non-recursive manner. Recent studies also explore limited-\nmemory evolution strategy for memory efficient optimization\n(e.g. LM-MA-ES [Loshchilov et al., 2018]).\n3 Approach\nThis section introduces the details of our proposed BPT-VLM\nframework, where the recent advanced model CLIP is used\nas the VLM backbone. In particular, given sampled intrinsic\nvectors from the shared original parameter space, the projec-\ntion matrices transform them into image and text prompts.\nThen the CLIP working as a black-box objective function\nevaluates the fitness of these intrinsic vectors for ES-based\nblack-box optimization. Different prompt variants are also\nincorporated for enhancing cross-modal interaction. The de-\ntails are introduced in the following sections.\n3.1 Intrinsic Vector based Population\nAs discussed by [Li and Liang, 2021 ] and [Jia et al., 2022],\ndozens of prompt tokens are required to be learned for single-\nmodality Transformers like RoBERTa[2019] and ViT[2020].\nConsidering the embedding dimension of CLIP (512 for text\nencoder and 756 for image encoder), the total parameters of\nthe continuous prompts P ∈ RD1+D2 need to be optimized\ncan be tens of thousands, which is challenging for black-box\noptimization. While [Aghajanyan et al., 2021] empirically\ndemonstrates that large-scale LM actually has a very low in-\ntrinsic dimension, we extend this insight into VLMs to trans-\nfer prompt optimization from the original parameter space\ninto a intrinsic subspace.\nIn particular, we define the parameter subspace of language\nas zL ∈ Rd1 and the visual counterpart as zV ∈ Rd2 .\nZ = [zL∥zV ] (1)\nwith a concatenation operation ∥, the joint intrinsic parame-\nter subspace is denoted as Z ∈ Rd1+d2 . Intrinsic vectors be-\nlonging to this subspace can be projected to vision-language\nprompt tokens pL and pV through randomly initialized matri-\nces WL ∈ Rd1×D1 and WV ∈ Rd2×D2 .\npL = zLWL, pV = zV WV (2)\nNote that the weights of WL and WV are fixed through gen-\nerations, but directly initializing them with standard uniform\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1688\ndistribution N(0, 1/d) [Sun et al., 2022a] may result in slow\nconvergence and inferior performance. Thus, to approximate\nthe output distribution of dual encoders’ entry embedding and\nconvolutional layers, we use normal distribution to initialize\nWL and WV with means and standard deviations as follows:\n\n\n\n\n\n\n\n\nµL = ˆµL\nd1 − ˆσL\n2 , σL = ˆσLp\nd1 − ˆσL\n2\nµV = 3k2 ˆµV\nd2\n, σV = ˆσV\ns\n3k2\nd2\n(3)\nwhere ˆµL and ˆσL are observed mean and deviation of the\nword embedding layer in the text encoder, and ˆµV and ˆσV\nare the counterparts of the entry convolutional layer in the\nimage encoder, which uses 3-channel kernels with the size k.\nIn this way, we’re able to perform prompt tuning in a much\nsmaller parameter subspace (d1 + d2 ≪ D1 + D2), and mul-\ntiple intrinsic vectors as individuals can form a population for\nevolutionary learning.\n3.2 Objective Function\nIn evolution strategies, an objective function is required to\nevaluate the fitness of each individual, and knowing its pre-\ncise analytic form is unnecessary. In the scenario of MaaS,\nit’s reasonable to recognize CLIP as a black-box objective\nfunction, since only forward pass is allowed and thus cannot\ncompute gradients or the Hessian matrix.\nCLIP consists of two Transformer-based encoders - one\nimage encoder and the other text encoder (shown in Figure\n2). Assume that CLIP-like models take a batch of texts xL\nand images xV as input, and we recognize the forward pass\nas a black-box function f, which outputs the similarity scores\nbetween each image-text pairs. With the output and the labels\nY , we can calculate the cross entropy loss L. Our framework\naims to learn the optimal prompts in a derivative-free manner:\n(f(pL, pV ) = CLIP [(pL, xL); (pV , xV )]\nP⋆ = arg min\npL,pV\nL[Y ; f(pL, pV )] (4)\nwhere P is the unified formulation of (pL, pV ). The black-\nbox function f can be evaluated with forward pass, but is not\navailable to the optimizer in a closed form.\nAs discussed in 3.1, optimization of vision and language\nprompts actually performs in an intrinsic parameter subspace.\nThus the prompt tuning can also be denoted as\nZ⋆ = arg min\nzL,zV\nL[Y ; f(zLWL, zV WV )] (5)\n3.3 Black-box Optimization with ES\nOptimization in the intrinsic space only requires algorithms\nto deal with hundreds of parameters. Thus Evolution Strat-\negy (ES) is a reasonable approach to tackle black-box prompt\ntuning. In the t + 1-th iteration of CMA-ES, individuals are\nsampled from the following distribution:\nZt+1 ∼ mt + σtN(0, Ct) (6)\nwhere mt is the mean of top-λ out of all individuals evalu-\nated by the objective function L (Equation 5), σt denotes the\nmutation step-size. Through generations, the distribution is\nadapted by adding a random Gaussian mutation defined by a\ncovariance matrix Ct.\nHowever, the original CMA-ES performs eigendecomposi-\ntion C = AAT with Θ(n2) time complexity to update the co-\nvariance matrix C, which precludes its application on large-\nscale optimization. While Cholesky-CMA-ES samples can-\ndidate solutions only with the iteratively updated Cholesky\nfactor Ac:\nZt+1 ∼ mt + σtAt\ncN(0, C0) (7)\nwhich simplifies a lot the implementation of the algorithm.\nMA-ES further removes the evolution path and simplifies the\nupdate for the Cholesky factor; While MM-ES applies Fast\nMixture Sampling to approximate the covariance matrix C\nin a non-recursive manner. The performance comparison of\ndifferent black-box optimization is shown in Figure 4.\n3.4 Prompt Design\nGiven the vision-language model CLIP, we introduce contin-\nuous prompts pL and pV to transfer pre-trained knowledge\nto downstream tasks. However, there are certain differences\nbetween its dual encoders (linguistic Transformer and ViT),\nand the prompt can be involved in multiple Transformer lay-\ners. This section explains different prompt designs used in\nour framework.\nPreliminaries\nFor the text encoder, each word token in the input sentence\nS is first embedded to a dL-dimensional subspace through an\nembedding layer as\neL = Embed(S) (8)\nwhere eL ∈ RmL×dL are the embedding features of the text\ninput padded to a fixed length mL.\nOn the other hand, for a plain ViT, an input image G will\nbe first divided into mV patches, each patch is then projected\nto a dV -dimensional vector with a convolutional layer\neV = Conv2D(G) (9)\nwhere eV ∈ RmV ×dV are the convolutional features of the\nimage patches.\nPrompt Location\nAs is widely used and evaluated in previous works [Li and\nLiang, 2021 ], prefix positioning is adopted for the text en-\ncoder. Text prompt pL is placed before the sentence embed-\nding eL, and the number of tokens in pL is denoted as nL:\nIL = [cL, pL, eL] (10)\nwhere cL is the [CLS] token for text inputs, IL ∈\nR(1+nL+mL)×dL represents the input features with prompt\nfor the text encoder.\nWhile for the image encoder (ViT), suffix positioning is\napplied to keep the information of pre-trained positional em-\nbedding, which has a length just matched with mV image\npatches. A set of nV visual tokens is referred as pV :\nIV = [cV , eV , pV ] (11)\nwhere cV is the [CLS] token for image inputs, and IV ∈\nR(1+mV +nV )×dV denotes the visual features with prompt fed\nto the image encoder.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1689\nPrompt Depth\nSince prompts are inserted to transformer-based image and\ntext encoders, they have a strong ability to influence the\noutput through sufficient cross-attention with input features.\nHowever, [Liu et al., 2022] point out that long propagation\npath from prompt to the final signal may lead to information\nloss. Thus we explore two variants of prompt tuning in BPT-\nVLM, namely Shallow Prompt and Deep Prompt, depend-\ning on the depth of transformer layers inserted with projected\nprompts (Figure 3 takes the text encoder for example).\nTransformer Layer\n-\n1\nCLS\n[CLASS]\n.\nTransformer Layer\n-\n2\nP\n…\n…\n…\nTransformer Layer\n-\n1\nCLS\n[CLASS]\n.\nTransformer Layer\n-\n2\n…\n…\n…\n[replace]\nP\nP\n[replace]\nP\nP\nP\nP\nP\nFigure 3: Shallow Prompt and Deep Prompt.\nShallow Prompt only inserts prompts for the input features\nof text encoder and image encoder (ViT), the propagation pro-\ncess can be formulated as:\u001a[cL, hL, eL]1 = L1(IL)\n[cL, hL, eL]i = Li([cL, hL, eL]i−1) i ≥ 2 (12)\n\u001a[cV , eV , hV ]1 = V1(IV )\n[cV , eV , hV ]i = Vi([cV , eV , hV ]i−1) i ≥ 2 (13)\nwhere Li and Vi indicate the i-th transformer layer of image\nand text encoder; hL and hV refers to the hidden representa-\ntion of text prompt pL and pV . Prompts interacts with input\nfeatures along layer-upon-layer propagation.\nTo avoid information loss caused by long propagation path,\nDeep Prompt involves prompts in each layer of transformer-\nbased text and image encoders:\n\u001a[cL, #, eL]i = Li([cL, pL, eL]i−1) i ≥ 1\n[cV , eV , #]i = Vi([cV , eV , pV ]i−1) i ≥ 1 (14)\nwhere # is the hidden states to be replaced by another set of\nprompts. Since prompts inserted to different layers belong to\ndifferent parameter spaces, multiple projection matrices are\ninitialized. For N intrinsic parameter spaces, our framework\nrecognizes the optimization as in-dependent sub-problems:\narg min{Z⋆\ni }N\ni=1 = (arg minZ⋆\n1 , ...,arg minZ⋆\nN ). (15)\nwhen one intrinsic space is under the process of black-box\ntuning, vectors belonging to other intrinsic spaces are kept\nfixed. Such iterative optimization enhances cross-modal in-\nteraction of VLM and generally outperforms shallow prompt.\n4 Experiments\n4.1 Experiment Setup\nDatasets and Metrics\nTo evaluate the effectiveness of BPT-VLM, we conduct ex-\nperiments on 9 visual image classification datasets: Ima-\ngeNet [Deng et al., 2009], Caltech101 [Fei-Fei et al., 2004],\nOxfordPets [Parkhi et al., 2012], Flowers102 [Nilsback and\nZisserman, 2008], Food101 [Bossard et al., 2014], UCF101\n[Soomro et al., 2012], SUN397 [Xiao et al., 2010], EuroSAT\n[Helber et al., 2019] and DTD [Cimpoi et al., 2014]. These\ndatasets covers a wide range of vision tasks.\nThe ImageNet and Caltech101 datasets are designed for\ngeneric image classification. While fine-grained object clas-\nsification datasets including OxfordPets, Flowers102 and\nFood101 focus on differentiating between sub-classes be-\nlonging to the same meta-class. The SUN397 dataset is built\nfor scene recognition. EuroSAT and DTD are specialized\ndatasets catered for satellite classification and texture recog-\nnition. Following the few-shot setting adopted in [Zhou et\nal., 2022], all methods use the same 16-shot split for prompt\ntuning and are evaluated on full test-sets for comparison.\nModels for Comparison\nAs presented in Table 2, we compare BPT-VLM with\ntwo kinds of prompt tuning methods: gradient-based and\nderivative-free methods.\nFor derivative-free method, we consider Manual Prompt\nas our baseline: Following the prompt setting introduced\nin [Radford et al., 2021], we use hand-crafted templates as\ntask-relevant prompts to conduct zero-shot evaluation. For\ngradient-based methods, two baseline methods are consid-\nered: (1) Linear Probe: As suggested by [Tian et al., 2020],\ntraining a linear layer as the classification head on top of CLIP\ncan achieve competitive performance compared with other\nfine-tuning methods. We followed the same training method\nused by [Radford et al., 2021] to train the linear probe model.\n(2) CoOp: Recently proposed CoOp[Zhou et al., 2022] mod-\nels text prompt’s context words as learnable vectors while the\nother parameters of CLIP are kept fixed. We reproduced the\nresults using on each tasks using 16 middle positional tokens\nand default 200 epoch training.\nWe devise four versions of BPT-VLM for compari-\nson, namely MM-ES-Shallow, MA-ES-Shallow, CMA-ES-\nShallow and CMA-ES-Deep, which adopt different evolu-\ntion strategies [2001; 2020; 2018 ] and prompt designs (dis-\ncussed in 3.4) for black-box optimization.\n4.2 Implementation Details\nHyper-parameter Default Setting\nIntrinsic Dimension 1000\nVision Prompt Length 8\nLanguage Prompt Length 5\nPopulation Size 30\nLoss Fucntion Cross Entropy\nTable 1: Default Setting of Hyper-parameters\nThree derivative-free algorithms are introduced to our black-\nbox prompt tuning frameword: CMA-ES, MM-ES and MA-\nES, all of which are implemented based on open-source li-\nbraries PyCMA2 and PyPop73. Unless otherwise stated, all\n2PyCMA: https://github.com/CMA-ES/pycma\n3PyPop7: https://github.com/Evolutionary-Intelligence/pypop\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1690\nImageNet\nCaltech101\nOxfordPets\nStanfordCars\nFood101\nSUN397\nDTD\nEuroSAT\nUCF101\nAv\nerage\nGradient-Based Linear Probe 55.87 90.63 76.42 70.08 70.17 67.15 63.97 82.76 73.72 72.31\nCoOp 62.95 91.83 87.01 73.36 74.67 69.26 63.58 83.53 75.71 75.77\nDerivative-Free\nManual Prompt 58.18 86.29 85.77 55.61 77.31 58.52 42.32 37.56 61.46 62.56\nMM-ES-Shallow – 93.67 90.49 62.49 81.62 – 48.40 86.25 70.76 –\nMA-ES-Shallow – 93.59 90.57 65.03 81.54 – 59.63 86.93 76.34 –\nCMA-ES-Shallow 65.08 94.16 90.43 64.72 81.31 68.01 60.52 86.11 74.62 76.11\nCMA-ES-Deep 64.84 93.39 90.62 67.84 81.38 69.83 64.13 89.37 76.66 77.56\n∆(%) +2.13 +2.33 +3.61 -5.52 +4.31 +0.57 +0.16 +5.84 +0.95 +1.79\nTable 2: Comparison results on various visual understanding tasks with the pre-trained vision-language model CLIP in 16-shot setting. MM-\nES-Shallow, MA-ES-Shallow, CMA-ES-Shallow and CMA-ES-Deep are four versions of our BPT-VLM with different evolution strategies\nand prompt designs. The underlined values indicate the highest accuracy for gradient-based methods on each dataset, and the bold numbers\nare the counterparts for our BPT-VLM models. The last line ∆ values indicate our maximum improvements over the best baselines.\nmodels are built with the open-source CLIP with ViT-B/32\nas the visual encoder’s backbone. Table 1 demonstrates the\ndefault configuration of hyper-parameters used in our exper-\niments. Note that only the intrinsic vector is required to be\nupdated and no back-propagation is performed.\nSince the optimization process of gradient descent algo-\nrithm and derivative-free algorithm are quite different, here\nwe redefine the meaning of a training epoch. For the shal-\nlow variant of black-box prompt tuning, every 12 generations\nof evolution for the intrinsic vector are regarded as 1 training\nepoch. As for the deep prompt variant, 1 epoch of training in-\ndicates that each intrinsic vector (belonging to 12 transformer\nlayers) iteratively performs 1 generation of evolution.\n4.3 Main Results\nOverall Performance. Table 2 shows the comparison re-\nsults of baseline model and four versions of BPT-VLM frame-\nwork (introduced in 4.1). We observe that the CMA-ES-Deep\nmodel using CMA-ES optimization and Deep Prompt outper-\nforms both the derivative-free and gradient-based baselines\non 8 out of 9 datasets, indicating the effectiveness of our\nblack-box prompt tuning framework for large-scale VLMs.\nIn particular, we achieve an average improvement of 1.79%\nover recent advanced baseline CoOp that uses gradient-based\nprompt tuning, which further demonstrates the advantage of\nour framework in the scenario of MaaS.\nPerformance on Various Datasets. It is worth noting that\nwe achieve 2.13% improvement over the best gradient-based\nbaseline on ImageNet, which is a challenging task that con-\ntains 1000 classes. The performance improvements are also\nsignificant on fine-grained classification datasets such as Ox-\nfordPets and Food101, as well as scene and action recog-\nnition datasets (i.e. SUN397 and UCF-101). The perfor-\nmance of our method on StanfordCars is not so appealing,\nthus we further analyze the failure cases. It is observed that\nour method under-performs the gradient-based methods when\nthe text annotations only have subtle differences like “BMW\nX3 SUV” and “BMW X5 SUV”, which increase the difficulty\nfor the derivative-free methods without strong fitting mecha-\nnisms like gradient descent.\nPerformance of Various Optimization Algorithms and\nPrompt Designs. We plot the accuracy curves with differ-\nent optimization algorithms and prompting methods during\ntraining in Figure 4. Since the results on each dataset is sim-\nilar, the curves on four datasets are presented due to the lim-\nited space. We observe that the MM-ES algorithm performs\nslightly worse while converges faster, and there are no signif-\nicant differences between other optimization algorithms. In\naddition, by comparing the two models as CMA-ES-Shallow\nand CMA-ES-Deep that use different prompting methods,\nwe find that the shallow prompt achieves faster convergence\nspeed, while slightly under-performs the deep variant on fi-\nnal accuracy. This coincides with our intuition since the deep\nprompt is iteratively optimized in different intrinsic subspaces\ninvolving each layer of the pre-trained VLMs, while the shal-\nlow prompt only involves the input layer that includes less\nparameters to be optimized as described in Section 3.4.\n4.4 Further Analyses\nWe conduct further analyses on various hyper-parameters to\nexplore their effect. Each hyper-parameter is investigated\nwhile keeping the other hyper-parameters as default as listed\nin Table 1. For the limits of space, we show the experimental\nresults on Caltech101 with the same 16-shot split in Figure 5.\nSimilar results can be observed on other datasets.\nEffect of Intrinsic Dimension. The parameter subspace of\nthe intrinsic vectors is the space where optimization actually\nperforms. As shown in Figure 5(a), the model with lower in-\ntrinsic dimension converges faster, but yields higher losses.\nWhen the intrinsic dimension increases over 1000, there are\nno significant differences in losses. Thus, the intrinsic dimen-\nsion is recommended to be set to 1000 in our experiments.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1691\n(a) Food-101\n (b) OxfordPets\n (c) StanfordCars\n (d) UCF-101\nFigure 4: Accuracy curves on various datasets during training. Since one epoch of training in gradient-based and derivative-free methods\nrepresent different meanings as demonstrated in Section 4.2, the final performance of the recent advanced gradient-based method CoOp is\npresented with horizontal purple dash lines. The derivative-free baseline as manual prompt evaluated in zero-shot setting can be deemed as\nthe initial points of our method, represented with horizontal red dash lines.\n(a) Intrinsic Dimension\n (b) Prompt Length\n (c) Population Size\n (d) Parallel Evaluation\nFigure 5: Results of ablation experiments on hyper-parameters. (a) and (b) illustrate the loss curves of different intrinsic dimension and prompt\nlength settings; (c) shows the accuracy curves of black-box prompt tuning with various population sizes; (d) demonstrates comparison of time\nefficiency between serial and parallel evaluation.\nEffect of Prompt Length. We utilize text prompt and im-\nage prompt for better inducing the knowledge contained in\nthe pre-trained VLMs, which determine the original param-\neter space in text and image encoders. To investigate the ef-\nfect of prompt length, we evaluate BPT-VLM with various\nlengths. As shown in Figure 5(b), the converged loss de-\ncreases when the prompt length grows first, but then it tends\nto be stable when the length becomes larger. Given that the\nmodel converges slowly with the increasing prompt length,\nwe use a reliable setting as (V=5, L=8) for the text and prompt\nlength for performance and efficiency balance.\nEffect of Population Size. In one generation of the evolu-\ntion strategy, individuals (intrinsic vectors) are sampled from\na multivariate normal distribution. Then the algorithm eval-\nuates their fitness to adjust the distribution for the next gen-\neration. Here we explore the effect of population size within\n3600 times of individual evaluations. As suggested in Fig-\nure 5(c), a smaller population size as 2 can reach the con-\nvergence faster, but a noticeable performance degradation oc-\ncurs. In addition, the larger population size does not always\nyield better results but converges more slowly. On the whole,\nthe population size between 10 and 60 is recommended to be\na reliable setting in our experiments.\nEfficiency of Parallel Training. Open-source black-box\noptimization algorithms usually evaluate individuals in a\ntime-consuming serial manner. To improve efficiency, our\nmethod can be easily adapted to support parallel prompt tun-\ning, which can evaluate all individuals simultaneously in a\nsingle concatenated batch. Figure 5(d) shows the result of the\nefficiency analysis. It is worth noting that parallel tuning only\ntakes about 1/4 time compared with traditional serial tuning\nwhen the population size varies in {5, 10, 15, 20}, and this\nadvantage in training efficiency will be more prominent when\nthe population size grows.\n5 Conclusions and Future Work\nIn this paper, we propose a black-box prompt tuning frame-\nwork for vision-language models in the scenario of MaaS. We\nextend derivative-free algorithms to the new scope of prompt\ntuning for pre-trained VLMs, and conduct expansive experi-\nments to verify that large-scale VLMs also have very low in-\ntrinsic dimensions, which is as effective for fine-tuning as the\nfull parameter spaces. Moreover, we incorporate cross-modal\ninteraction in our framework by sharing the intrinsic param-\neter subspace of both vision and language modalities. Dif-\nferent prompt designs are also explored to enhance prompts’\ninfluence during propagation.\nIt could be interesting to consider black-box prompt tuning\nin different downstream tasks based on different pre-trained\nmodels, and the black-box algorithms could be exchanged,\nsuch as Particle Swarm and Bayesian Optimization. Our fu-\nture work will also concern a deeper analysis of cross-modal\ninteraction from the perspective of derivative-free optimiza-\ntion in shared intrinsic parameter spaces.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1692\nAcknowledgements\nThis research is funded by the National Key Research and\nDevelopment Program of China (No. 2021ZD0114002), and\nthe Science and Technology Commission of Shanghai Mu-\nnicipality Grant (No. 22511105901, No. 21511100402).\nReferences\n[Aghajanyan et al., 2021] Armen Aghajanyan, Sonal Gupta,\nand Luke Zettlemoyer. Intrinsic dimensionality explains\nthe effectiveness of language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 7319–7328, 2021.\n[Beyer and Sendhoff, 2017] Hans-Georg Beyer and Bern-\nhard Sendhoff. Simplify your covariance matrix adapta-\ntion evolution strategy. IEEE Transactions on Evolution-\nary Computation, 21(5):746–759, 2017.\n[Bossard et al., 2014] Lukas Bossard, Matthieu Guillaumin,\nand Luc Van Gool. Food-101–mining discriminative com-\nponents with random forests. In European conference on\ncomputer vision, pages 446–461. Springer, 2014.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901, 2020.\n[Cimpoi et al., 2014] Mircea Cimpoi, Subhransu Maji, Ia-\nsonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.\nDescribing textures in the wild. In Proceedings of the\nIEEE conference on computer vision and pattern recog-\nnition, pages 3606–3613, 2014.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on\ncomputer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[Du et al., 2022] Yifan Du, Zikang Liu, Junyi Li, and\nWayne Xin Zhao. A survey of vision-language pre-trained\nmodels. arXiv preprint arXiv:2202.10936, 2022.\n[Fei-Fei et al., 2004] Li Fei-Fei, Rob Fergus, and Pietro Per-\nona. Learning generative visual models from few train-\ning examples: An incremental bayesian approach tested\non 101 object categories. In 2004 conference on computer\nvision and pattern recognition workshop, pages 178–178.\nIEEE, 2004.\n[Hansen and Ostermeier, 2001] Nikolaus Hansen and An-\ndreas Ostermeier. Completely derandomized self-\nadaptation in evolution strategies. Evolutionary compu-\ntation, 9(2):159–195, 2001.\n[He et al., 2020] Xiaoyu He, Zibin Zheng, and Yuren Zhou.\nMmes: Mixture model-based evolution strategy for large-\nscale optimization. IEEE Transactions on Evolutionary\nComputation, 25(2):320–333, 2020.\n[Helber et al., 2019] Patrick Helber, Benjamin Bischke, An-\ndreas Dengel, and Damian Borth. Eurosat: A novel\ndataset and deep learning benchmark for land use and\nland cover classification. IEEE Journal of Selected Top-\nics in Applied Earth Observations and Remote Sensing,\n12(7):2217–2226, 2019.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis-\nlaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning, pages 2790–\n2799. PMLR, 2019.\n[Jia et al., 2022] Menglin Jia, Luming Tang, Bor-Chun\nChen, Claire Cardie, Serge Belongie, Bharath Hariharan,\nand Ser-Nam Lim. Visual prompt tuning. In European\nConference on Computer Vision (ECCV), 2022.\n[Larson et al., 2019] Jeffrey Larson, Matt Menickelly, and\nStefan M Wild. Derivative-free optimization methods.\nActa Numerica, 28:287–404, 2019.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-\ntuning: Optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 4582–4597, Online, August\n2021. Association for Computational Linguistics.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach. ArXiv,\nabs/1907.11692, 2019.\n[Liu et al., 2022] Xiangyang Liu, Tianxiang Sun, Xuanjing\nHuang, and Xipeng Qiu. Late prompt tuning: A late\nprompt could be better than many prompts. arXiv preprint\narXiv:2210.11292, 2022.\n[Loshchilov et al., 2018] Ilya Loshchilov, Tobias Glasmach-\ners, and Hans-Georg Beyer. Large scale black-box op-\ntimization by limited-memory matrix adaptation. IEEE\nTransactions on Evolutionary Computation, 23(2):353–\n358, 2018.\n[Loshchilov, 2014] Ilya Loshchilov. A computationally effi-\ncient limited memory cma-es for large scale optimization.\nIn Proceedings of the 2014 Annual Conference on Genetic\nand Evolutionary Computation, pages 397–404, 2014.\n[Nilsback and Zisserman, 2008] Maria-Elena Nilsback and\nAndrew Zisserman. Automated flower classification over\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1693\na large number of classes. In 2008 Sixth Indian Confer-\nence on Computer Vision, Graphics & Image Processing,\npages 722–729. IEEE, 2008.\n[Parkhi et al., 2012] Omkar M Parkhi, Andrea Vedaldi, An-\ndrew Zisserman, and CV Jawahar. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3498–3505. IEEE, 2012.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris\nHallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack\nClark, et al. Learning transferable visual models from nat-\nural language supervision. In International Conference on\nMachine Learning, pages 8748–8763. PMLR, 2021.\n[Shahriari et al., 2015] Bobak Shahriari, Kevin Swersky,\nZiyu Wang, Ryan P Adams, and Nando De Freitas. Taking\nthe human out of the loop: A review of bayesian optimiza-\ntion. Proceedings of the IEEE, 104(1):148–175, 2015.\n[Soomro et al., 2012] Khurram Soomro, Amir Roshan Za-\nmir, and Mubarak Shah. Ucf101: A dataset of 101 human\nactions classes from videos in the wild. arXiv preprint\narXiv:1212.0402, 2012.\n[Sun et al., 2022a] Tianxiang Sun, Zhengfu He, Hong Qian,\nYunhua Zhou, Xuanjing Huang, and Xipeng Qiu. Bbtv2:\nTowards a gradient-free future with large language models.\nIn Proceedings of EMNLP, 2022.\n[Sun et al., 2022b] Tianxiang Sun, Yunfan Shao, Hong Qian,\nXuanjing Huang, and Xipeng Qiu. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of ICML,\n2022.\n[Tian et al., 2020] Yonglong Tian, Yue Wang, Dilip Krish-\nnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking\nfew-shot image classification: a good embedding is all you\nneed? In European Conference on Computer Vision, pages\n266–282. Springer, 2020.\n[Xiao et al., 2010] Jianxiong Xiao, James Hays, Krista A\nEhinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In 2010\nIEEE computer society conference on computer vision and\npattern recognition, pages 3485–3492. IEEE, 2010.\n[Zhou et al., 2022] Kaiyang Zhou, Jingkang Yang,\nChen Change Loy, and Ziwei Liu. Learning to prompt\nfor vision-language models. International Journal of\nComputer Vision, 130(9):2337–2348, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1694",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8469913005828857
    },
    {
      "name": "Black box",
      "score": 0.7311134934425354
    },
    {
      "name": "Task (project management)",
      "score": 0.6799001693725586
    },
    {
      "name": "Subspace topology",
      "score": 0.5779204368591309
    },
    {
      "name": "Language model",
      "score": 0.546628475189209
    },
    {
      "name": "Inference",
      "score": 0.5355787873268127
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49413445591926575
    },
    {
      "name": "Machine learning",
      "score": 0.47730955481529236
    },
    {
      "name": "Service (business)",
      "score": 0.47439077496528625
    },
    {
      "name": "Modal",
      "score": 0.4318445920944214
    },
    {
      "name": "Human–computer interaction",
      "score": 0.36267775297164917
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 9
}