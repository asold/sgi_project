{
  "title": "Learning the Language of NMR: Structure Elucidation from NMR spectra using Transformer Models",
  "url": "https://openalex.org/W4385808309",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4321921357",
      "name": "Marvin Alberts",
      "affiliations": [
        "IBM Research - Zurich",
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1847311050",
      "name": "Federico Zipoli",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1999768387",
      "name": "Alain C. Vaucher",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3037490954",
    "https://openalex.org/W4367357905",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3174167596",
    "https://openalex.org/W4318486595",
    "https://openalex.org/W2944148756",
    "https://openalex.org/W2021395631",
    "https://openalex.org/W2784918212",
    "https://openalex.org/W4311100495",
    "https://openalex.org/W3120896887",
    "https://openalex.org/W3212396168",
    "https://openalex.org/W4282569448",
    "https://openalex.org/W4318192371",
    "https://openalex.org/W3041540407",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3210246785",
    "https://openalex.org/W4247059492",
    "https://openalex.org/W2971304966",
    "https://openalex.org/W4376612719",
    "https://openalex.org/W2616399381",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W4233511257",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2151697120",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The application of machine learning models in chemistry has made remarkable strides in recent years. Even though there is considerable interest in automating common proce- dure in analytical chemistry using machine learning, very few models have been adopted into everyday use. Among the analytical instruments available to chemists, Nuclear Mag- netic Resonance (NMR) spectroscopy is one of the most important, offering insights into molecular structure unobtainable with other methods. However, most processing and analysis of NMR spectra is still performed manually, making the task tedious and time consuming especially for larger quantities of spectra. We present a transformer-based machine learning model capable of predicting the molecular structure directly from the NMR spectrum. Our model is pretrained on synthetic NMR spectra, achieving a top–1 accuracy of 67.0% when predicting the structure from both the 1H and 13C spectrum. Additionally, we train a model which, given a spectrum and a set of likely compounds, selects the one corresponding to the spectrum. This model achieves a top–1 accuracy of 96.0% when trained on 1H spectra.",
  "full_text": "Learning the Language of NMR:\nStructure Elucidation from NMR\nspectra using Transformer Models\nMarvin Alberts1,2, Federico Zipoli1,3, and Alain C. Vaucher1,3\n1IBM Research Europe, S¨ aumerstrasse 4, 8803 R¨ uschlikon, Switzerland\n2University of Zurich, Department of Chemistry, Winterthurerstrasse 190, 8057 Zurich,\nSwitzerland\n3National Center for Competence in Research-Catalysis (NCCR-Catalysis), Zurich,\nSwitzerland\nAbstract\nThe application of machine learning models in chemistry has made remarkable strides in\nrecent years. Even though there is considerable interest in automating common proce-\ndure in analytical chemistry using machine learning, very few models have been adopted\ninto everyday use. Among the analytical instruments available to chemists, Nuclear Mag-\nnetic Resonance (NMR) spectroscopy is one of the most important, offering insights into\nmolecular structure unobtainable with other methods. However, most processing and\nanalysis of NMR spectra is still performed manually, making the task tedious and time\nconsuming especially for larger quantities of spectra. We present a transformer-based\nmachine learning model capable of predicting the molecular structure directly from the\nNMR spectrum. Our model is pretrained on synthetic NMR spectra, achieving a top–1\naccuracy of 67.0% when predicting the structure from both the 1H and 13C spectrum.\nAdditionally, we train a model which, given a spectrum and a set of likely compounds,\nselects the one corresponding to the spectrum. This model achieves a top–1 accuracy of\n96.0% when trained on 1H spectra.\n1. Main\nNuclear magnetic resonance (NMR) spectroscopy is widely considered the most crucial\ntool in determining the structure of molecules [1]. Unlike other techniques such as infrared\n1\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n(IR) spectroscopy or mass spectroscopy (MS), NMR provides comprehensive and human\ninterpretable information about the molecule. It reveals details such as the number of\nNMR-active nuclei, the functional group to which a peak belongs, and, for some nuclei,\ninformation about its surrounding environment [2]. Typically the spectra of multiple\nNMR-active nuclei are used to definitely assign the structure. Most commonly, an 1H\nNMR and a 13C NMR are used for this purpose. In the literature, the combination of\nthese two spectra has become thede factoproof that a compound has been synthesised [3].\nConsequently, NMR spectroscopy has risen to prominence as the preferred analytical\ninstrument in standard chemical laboratories.\nNevertheless, analyzing NMR spectra is not straightforward. Although there are various\nsoftware tools available to assist chemists in this process, the majority of spectra are\nstill processed manually. As a result, the analysis of NMR spectra, particularly in large\nquantities, becomes a time-consuming and tedious undertaking [4].\nThe increasing availability of computational power has ushered in a new era of statistical\nmethods: machine learning and deep learning. These approaches have revolutionized fields\nsuch as image classification and language modeling by addressing previously unsolvable\nproblems [5, 6]. In the realm of chemistry, machine learning, and particularly language\nmodeling, has emerged as a highly promising tool. Such models have diverse applications,\nspanning from predicting retrosynthetic routes over designing new drug candidates to\nassisting in the automation of experiments [7–9].\nIn addition to changes brought about by machine learning, chemistry is experiencing a\nparadigm shift due to the growing prominence of robotics and automation in laboratories\n[10,11]. Advances in both fields have carried over into chemistry, enabling fully automated\nhigh-throughput experimental campaigns that generate vast volumes of data previously\ninaccessible. By operating at nanomolar scales, these techniques can conduct hundreds\nto thousands of reactions per day [12–15]. However, one crucial step remains a limitation:\nthe analysis of the reaction products.\nCurrent high-throughput approaches are predominantly restricted to a limited number\nof reagents and reactants, largely due to their heavy reliance on high-performance liquid\nchromatography (HPLC) systems. Each reactant and product necessitates a separate\ncalibration curve, imposing limitations on the chemical space that can be explored [16,17].\nDespite the automation of most physical handling steps, the analysis of the resulting data\nstill predominantly relies on manual labor, demanding weeks to months of tedious work.\nAmong these tasks, the analysis of NMR data obtained from high-throughput experiments\ncan be particularly burdensome.\nEven though the analysis of NMR spectra obtained from high-throughput experiments\nremains time consuming, advances have been make to alleviate the burden to some ex-\ntent. Commercial NMR software offers options to automate peak picking, integration\nand multiplet assignement of the spectra [18, 19]. However, automatically determining a\n2\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nstructure from the spectra without strong prior knowledge is currently not feasible. There\nhave been advances in this task using machine learning but these approaches are so far\nlimited in the sense that they either limit the number of elements, the heavy atom count\n(all atoms other than hydrogen) or solely rely on one type of spectrum (e.g. 13C) [20–24].\nTo close the loop between automated high throughput experiments and NMR spec-\ntroscopy, an automated NMR structure elucidation workflow is required. Here we propose\nto utilise language models trained on NMR spectra to directly predict the structure. We\nachieve a top–1 accuracy in predicting the correct molecular structure from simulated\n1H and 13C NMR spectra of 67.0%. If the language model is provided with additional\ninformation such as the reagents and products of a reaction, the model is able to identify\nthe correct structure in 96.0% of cases from the 1H NMR spectrum.\n2. Results and Discussion\nWe focus on two primary tasks. The first one involves predicting the molecular structure\ndirectly from the 1H spectrum, 13C spectrum, or the combination of both spectra. The\nsecond one focuses on exploring the effect of adding additional context to the NMR\nspectrum. This second task corresponds to a typical high-throughput scenario, where\nchemists are aware of the reaction that was conducted and, consequently, the potential\nmolecules present in the spectrum. We task the model to match the correct molecule to\na given spectrum.\n2.1. Data\nAs the number of publicly available experimental NMR spectra is limited, we simulate\na large training set using MestreNova [18]. We sample reactions from the Pistachio\ndataset [25] and simulate NMR spectra for both the reactants and products. In con-\ntrast to previous work, we do not exclude stereoisomers or restrict the heavy atom count\ndrastically, opting for a range of 5 to 35, with an average heavy atom count of 22.7.\nWe limit the elements to the ones most commonly found in organic chemistry, excluding\nmolecules with elements other than carbon, hydrogen, oxygen, nitrogen, sulfur, phospho-\nrous and the halogens. In total we generate 1.94 million 1H and 19F decoupled 13C NMR\nspectra as well as 1.10 million 1H NMR spectra. Further details on the molecules can be\nfound in methods section 4.1.\nInstead of utilizing the raw 1H NMR spectrum, as demonstrated previously by Huang\net al. [20], we opt for a processed version of the spectrum. There are two main reasons\nbehind this choice. Firstly, if starting from the raw vector, the model would need to learn\nconcepts such as peak picking, peak integration, and multiplet assignment. Our approach\nreduces the learning demand on the model by preprocessing the spectra using MestreNova.\n3\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nSecondly, the wide availability of such processed experimental NMR spectra in papers\nand patents presents a potential avenue for validating our models on experimental data.\nFurther information on the exact simulation details can be found in Methods section 4.1.\n2.2. Model\nIn this study, we adopt a sequence-to-sequence encoder-decoder transformer architecture,\nbuilding upon the formulation utilized in our previous investigation of IR spectra [26].\nAs discussed above, we employ the processed NMR representation of a spectrum instead\nof a vector. For the 1H NMR this takes the form of a string containing the position of the\npeak in ppm, the multiplet type (‘s’, ‘d’, ‘t’, etc.), and the integration of the peak (i.e.\nthe number of hydrogen atoms). All 1H values are rounded to the nearest second decimal.\nOn the other hand, 13C NMR spectra are presented to the model as a simple list of peaks.\nAll values in ppm are rounded to the nearest first decimal. Examples are illustrated in\nFigure 1. A detailed account of how NMR spectra are processed can be found in Methods\nsection 4.3.\nAll molecules are presented to the model as presented to the model as Simplified\nmolecular-input line-entry system (SMILES) [27].\n1H-NMR Spectrum\nTokenization 1HNMR 1.15 1.36 t 3H | 2.38 \n2.56 t 1H | 3.54 3.91 m 2H\nRange of the peak in ppm \nrounded to two decimal points\nMultiplet Type e.g.\n ‘t’, ‘d’, etc.\nNumber of Hydrogens\nPeak separating Token\n13CNMR 17.6 57.6\nPosition of the peak in ppm \nrounded to one decimal points\nTokenization\n13C-NMR Spectrum\nFigure 1: Summary of the tokenization process for NMR spectra. Top: Tokenization of an\n1H NMR spectrum following the Range representation. Bottom: Tokenization\nof an 13C NMR spectrum.\n2.3. Structure Prediction from NMR spectra\nIn the following we focus on predicting the molecular structure directly from the NMR\nspectrum. We assess three different scenarios: Predicting the structure solely from the\n4\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n1H NMR spectrum, solely from the 13C NMR spectrum, and from the combined 1H and\n13C NMR spectra.\n2.3.1. Model optimisation\nTo explore the consequences of various data preparation methods, we examine the effects\nof supplementing the model with the chemical formula alongside the spectra, altering the\nformatting of 1H NMR peaks, and investigate the effect of a shared or separate token\nspace between the 1H and 13C NMR peaks. In total, we train 13 models to assess the\nimpact of these changes. We evaluate the performance of the trained models based on\nthe top–1, top–5, and top–10 accuracy metrics. These metrics indicate the percentage\nof cases where the predicted structure matches the target structure within the first, first\nfive, and first ten predictions, respectively. Molecules are defined as matching if their\ncanonical SMILES are identical. The results of these experiments can be found in Table\n1. In the following, we delve deeper into the different data preparation methods and their\nrespective effects.\nTable 1: Summary of experiments on simulated data and associated metrics.\nFormula Format∗ Tokens† Top–1% Top–5% Top–10%\n1H NMR\n✗ Center N/A 38.29% 54.67% 58.43%\n✓ Center N/A 53.34% 71.71% 75.09%\n✓ Adaptive N/A 53.39% 71.84% 75.23%\n✓ Range N/A 55.32% 73.59% 76.74%\n1H NMR (Augmented) ✓ Range N/A 51.58% 70.52% 73.94%\n1H NMR (Ensemble) ✓ Range N/A 57.99% 76.65% 80.04%\n13C NMR ✗ N/A N/A 37.21% 53.98% 57.45%\n✓ N/A N/A 51.37% 70.74% 74.32%\n13C NMR (Augmented) ✓ N/A N/A 49.02% 69.05% 72.90%\n13C NMR (Ensemble) ✓ N/A N/A 53.91% 73.45% 77.72%\n1H+13C NMR\n✗ Range Separate 56.88% 73.91% 76.89%\n✓ Range Separate 64.78% 81.74% 84.43%\n✓ Range Shared 65.05% 82.07% 84.70%\n1H+13C NMR (Augmented) ✓ Range Shared 62.35% 80.15% 82.93%\n1H+13C NMR (Ensemble) ✓ Range Shared 66.99% 84.09% 86.59%\n* The format used to represent the position of the1H NMR peaks\nCenter: Center of the peak\nRange: Minimum and maximum ppm of the peak\nAdaptive: If the range is larger than 0.15 ppm use the range format otherwise center format\n†\nWhether the token space of the1H and13C NMR spectrum is shared or separate\nWe trained a model for all the three scenarios (solely 1H or 13C and combined 1H and\n13C) with and without the chemical formula. We observe an increase in performance\nof ∼8–14% in performance for all three models when including the formula. Adding the\n5\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nchemical formula constrains the chemical space that the model has to explore. This trans-\nforms the task for the model from predicting the structure solely based on the spectrum\nto generating a set of isomers from the chemical formula and matching the best one to\nthe spectrum. Consequently, we include the formula in all subsequent experiments.\nAnother point of interest is the format in which 1H NMR peaks are presented to the\nmodel. In the literature, two formats are commonly used to describe 1H NMR peaks.\nFor smaller, narrower peaks, the center of the peak is typically used. Conversely, for\nlarger, broader peaks, the peak is described as a range by indicating the minimum and\nmaximum values at which the peak begins and ends. Here, we investigate three cases: (1)\nproviding the model only with the center of the peak, (2) using a range by specifying the\nstart and end values of each peak, and (3) employing an adaptive approach inspired by\nthe format found in the literature with thinner peaks using the center and broader peaks\nthe range representation. We define broad peaks as those with a width greater than 0.15\nppm. The results are presented in Table 1 within the 1H NMR section. We find that the\nrange representation yields the best performance, likely due to the additional information\non the width of the peak. Therefore, for all subsequent experiments involving 1H NMR\nspectra, we utilize the range representation.\nNext, we shift our focus to the combination of1H and 13C spectra. To assign a structure\nfrom NMR spectra, it is common practice to rely on both the 1H and 13C spectra, as\nopposed to analysing a single spectrum on its own. In these experiments, we investigate\nthe impact of providing the model with both the 1H and 13C NMR spectra. Following our\nearlier experiments we reuse the best representations for 1H spectra and concatenate it\nwith the 13C spectrum. More detailed information regarding the data format utilized to\nfeed the model can be found in Methods section 4.3. Additionally, we examine whether\nthe model performs better when the tokens representing the position of the peaks fall into\na shared space or a separate one. This is achieved by dividing the position of the 13C\nNMR peaks by 10 causing a significant overlap in tokens describing the position of peaks\nbetween the two modalities. The advantage of sharing tokens is a decreased vocabulary\nsize. However, when the tokens are shared the model has to learn to differentiate between\n1H and 13C NMR tokens. The results, presented in Table 1 under the 1H+13C NMR\nsection, demonstrate that the shared tokenization scheme outperforms the separate one\nby ∼0.25%.\nTo enhance the models’ performance and promote generalization, we augment the train-\ning data. Specifically, we utilize jitter augmentation with a range of 0.5 ppm, as outlined\nin the Methods section 4.4. This augmentation approach generates two augmented spec-\ntra for each original spectrum. When training the models on the combined augmented\nand original spectra, we observe a noticeable decline in performance across all scenarios\n(1H, 13C, and the combined 1H and 13C). This is likely caused by the simulated data\nexhibiting high homogeneity, consistency in peak position and width, and lack of noise.\n6\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nIntroducing noise through augmentation disrupts the models learning process and results\nin decreased performance on the simulated test set. However, it is important to note that\nif the models were evaluated on experimental spectra, which naturally contain noise, the\naugmented models would likely perform better.\nEnsembling was used to further increase the performance of the models. We used an\nensemble of the five best performing checkpoints for each model trained on non-augmented\ndata. Across the three scenarios this increases performance on average by ∼2.4%. Results\nof the best performing models can be seen in Table 1. Ultimately, our final top–1 accuracy\nfor 1H NMR reaches 58.0%, for 13C NMR it achieves 53.9%, and for the combined 1H and\n13C NMR spectra, it reaches 67.0%.\n2.3.2. Model Analysis\nIn the following we analyse the performance of the model across the three tasks. We use\nthe best ensembled model from above and evaluate how the performance of the model\nchanges with respect to the heavy atom count and in relation to the presence of specific\nfunctional groups. In addition, we also demonstrate that even if the model makes mistakes,\nmost predicted molecules are relatively similar to the ground truth by evaluating the\nTanimoto similarity of all predicted molecules [28].\n5 10 15 20 25 30 35\nHeavy Atom Count\n0\n20\n40\n60\n80\n100Accuracy %\n1H, Top 1%\n13C, Top 1%\n1H+13C, Top 1%\n1H, Top 10%\n13C, Top 10%\n1H+13C, Top 10%\nFigure 2: Heavy atom count vs accuracy. Results for 1H spectra are shown in blue, for\n13C in orange and in green for the combination of both.\n7\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nHeavy Atom count\nIn order to assess the model’s performance, we evaluate its accuracy in relation to the\nheavy atom count. Figure 2 shows a negative correlation between the heavy atom count\nand the model’s accuracy. The model trained on both 1H and 13C spectra outperforms the\nmodels trained on a sole spectrum, highlighting the complementary information that can\nbe extracted from both types of spectra. As expected, the 1H model demonstrates better\nperformance compared to the 13C model, albeit by a relatively small margin of ∼5%. It is\nworth noting that there is a relatively high variability in performance for molecules with\na heavy atom count ranging from 5 to 10. This can be attributed to the limited training\ndata available in this particular range, comprising only around 2.5% of the total training\ndataset.\nThe negative correlation of the model’s performance with the heavy atom count can\nbe attributed to two factors. Firstly, as the heavy atom count increases, molecules tend\nto become more complex, resulting in longer SMILES strings. Since the model generates\npredictions autoregressively, even a single incorrect token prediction can lead to a sig-\nnificantly different structure. This sensitivity to errors becomes more pronounced with\nan increase in the complexity of the molecules. Secondly, as the heavy atom count rises,\nthe chemical space expands exponentially, giving rise to a greater number of potential\nisomers that the model must differentiate, making the prediction more challenging. How-\never, both of these factors can be mitigated by extending the model’s training data. By\nincorporating a larger and more diverse dataset, the model can learn to better distinguish\nbetween various isomers, and improve overall performance. Additionally, more training\ndata could allow for a larger model architectures, further increasing the performance of\nthe model.\nFunctional Group to Structure\nWe analyse the model’s ability to generate the correct structure depending on the pres-\nence of certain functional groups by calculating the top n metrics for subsets containing\na specific functional group in the test set. The scores are shown for each of the scenarios\nin Figure 3. As with the heavy atom count, the model trained on the combined spectra\noutperforms both models trained on a sole spectrum, demonstrating the synergy that can\nbe obtained by using both.\nAcross all three tasks, we observe relatively low performance for phosphoric acids. This\ncan be attributed to the limited training volume available for this functional group, with\nonly around 0.12% of the molecules in the dataset containing either a phosphoric acid\ngroup. Surprisingly, the model also encounters challenges in predicting the structure\nof alkenes, despite the training volume for alkenes accounting for approximately 11%\nof all molecules. This difficulty may be due to the relatively wide range of chemical\nshifts of alkenes in both 1H and 13C NMR, as well as their similarity to aromatic signals.\n8\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\na)\nb)\nc)\nFigure 3: The models ability to correctly predict the molecular structure plotted against\nthe presence of certain functional groups: a) 1H NMR, b) 13C NMR, c) 1H+13C\nNMR.\n9\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAdditionally, E and Z isomerism may contribute to the lower performance, as correctly\nassigning these isomers can be challenging.\nWe observe high performance for halogens when predicting structures from 1H spectra.\nHowever, when evaluating their performance on 13C spectra, the halogens show only\nmediocre performance. This divergence can be attributed to the fundamental differences\nbetween the two modalities. While 13C-NMR offers some insight into the presence of\nhalogens, 1H-NMR spectra provide substantially more information, enabling conclusions\nto be drawn regarding the presence and even quantity of halogens on adjacent atoms.\nConversely, we find that the1H NMR model performs relatively poorly when predicting\nmolecules containing alkynes, ranking fourth lowest out of the 21 functional groups. In\ncontrast, the 13C NMR models performs well, ranking alkynes within the top six functional\ngroups. This can be attributed to two factors. Firstly, carbon NMR alkyne peaks are\nrelatively distinctive and easily identifiable. Secondly, in many cases, there are simply no\nhydrogen atoms directly attached to the alkynes. As a result, alkynes become a potential\nblind spot for 1H NMR.\nWhen both 1H and 13C NMR spectra are provided to the model, we observe an im-\nprovement for all functional groups. This is especially apparent for both halogens and\nalkynes compared to the individual models. In fact, these functional groups now perform\nabove average in the combined model. This highlights the the model’s capacity to ef-\nfectively utilize and integrate information from both modalities, thereby harnessing the\ncomplementary strengths of the two types of spectra enhancing its predictive capabilities.\nSimilarity\nWe compute the Tanimoto similarity [28] to the ground truth for all predicted molecules\nusing Morgan fingerprints with a radius of 2 and a bit vector size of 1024 [29]. The\naverage Tanimoto similarity is 0.534, 0.537, and 0.553 when the prediction relies on 1H\nNMR, 13C NMR, and combined spectra, respectively. Examples of molecules predicted\nby the combined model are shown in Figure 4, while Figure 5 illustrates the similarity\ndistribution of the prediction of this model. The similarity distribution for all three\nmodels can be found in Figure 7 in the appendix. This highlights that even when the\nmodel makes incorrect predictions, most of them still exhibit a high degree of similarity to\nthe ground truth. We assess the number of dissimilar molecules generated by the model\nby calculating the fraction of molecules with a Tanimoto similarity less than 0.4. We find\nthat it amounts to ∼33% for both 1H and 13C NMR. In contrast, when combining both\nspectra, the fraction decreases to ∼30%. This indicates that the model can extract a\ngreater amount of chemical information when provided with both spectra.\n10\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTarget Molecule Prediction No. 1\nSimilarity: 1.0\nPrediction No. 3\nSimilarity: 0.724\nPrediction No. 7 \nSimilarity: 0.152\nPrediction No. 4\nSimilarity: 0.289\nFigure 4: Four predictions of the model trained on the combined data. Illustrated are the\ntarget molecule on the left and the four predictions on the right including their\nrank and similarity to the target molecule.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90\n1\n2\n3\n4\n5\n6Percent\nFigure 5: The Tanimoto similarity distribution of all predictions of the model (10 ranked\npredictions per spectrum) trained on 1H and 13C spectra. The invalid SMILES\nstrings and the correctly-predicted molecules were excluded.\n2.4. Molecule Differentiation\nIn this task, our objective is to evaluate the model’s ability to accurately match the correct\nstructure to an NMR spectrum based on a set of potential molecules and the spectrum.\nSimulated spectra were generated for both the reactants and products of a given reaction.\nIn practical terms, this task resembles a situation in which, after a reaction has been\ncompleted and NMR spectra have been obtained for each fraction, these fractions must\nbe assigned to a potential molecule. For this task, we train three new models: on the 1H,\n13C, and combined spectrum, respectively. We compare these models to a baseline which\nrandomly picks a molecule from the set.\nWe provide the model with the complete set of reactants and products from a reaction,\nalong with an NMR spectrum of one of the molecules in the reaction. The input of\nthe model consists of the SMILES of the potential molecules separated by “.” and the\nspectrum in the same format as discussed above. With this input, the model predicts\nwhich molecule the spectrum corresponds to. For all models, we employ the optimal data\n11\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nformat as developed above. The performance of the models is evaluated based on the\ntop–1, top–2, and top–5 accuracy metrics. Results of the experiments can be found in\nTable 2.\nTable 2: Accuracies of the models in choosing the correct structure based on a set of\nmolecules and an NMR spectrum.\nTraining Set Top–1% Top–2% Top–5%\nRandom Baseline 31.16% 58.35% 85.99%\n1H NMR 96.03% 99.03% 99.43%\n13C NMR 90.45% 97.58% 98.11%\n1H+13C NMR 95.17% 98.62% 99.08%\nTable 2 shows that the random baseline achieves an accuracy of 31.16%, which is\nconsistent with the average of three potential molecules that can be chosen per reaction.\nThe accuracy increases when considering the top–2 or top–5 predictions.\nWhen considering the performance with spectra, 13C NMR performs the worst, as it\ncontains comparatively little information than an 1H NMR spectrum. However, it still\ncorrectly predicts the molecule in 90.45% of cases as the first suggestion. Surprisingly, the\nmodel provided with only the 1H NMR spectrum outperforms the model provided with\nboth spectra. The reason for this unexpected trend could be that the additional infor-\nmation provided by the 13C NMR spectrum, introduces more complexity and potential\nambiguity for the model, leading to a slight decrease in performance. This trend goes\nagainst the synergistic effects observed for structure elucidation.\nOverall, our findings demonstrate that a transformer model can accurately assign a\nmolecule to an NMR spectrum when provided with a set of reactants and products from\na reaction, achieving a high level of accuracy.\n2.5. Limitations\nOne of the key limitations of our methodological approach lies in the availability of large\nNMR datasets. While these datasets exist, licenses for their use are often expensive\nand restrict machine learning applications, limiting their use. Consequently, we opt to\nsimulate NMR spectra using MestreNova. While this approach is not inherently limiting,\nit is important to note that the resulting spectra are highly coherent and consistent.\nExperimental spectra likely exhibit greater variability and inconsistencies.\n3. Conclusions\nNMR spectroscopy is a very powerful tool routinely used by bench chemists. The analysis\nof spectra, or rather their use for structure elucidation, remains a primarily manual task.\n12\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTaking in consideration the number of spectra analyzed every day in the world, it is\nsurprising that few data-driven approaches to help in this process have been adopted so\nfar. In this work, we explored ways to change that.\nTo this end, we presented a transformer model capable of predicting the molecular\nstructure directly from NMR spectra. We trained and optimised the transformer model\nto predict the molecular structure from the 1H, 13C, and combined 1H/13C NMR spec-\ntra. We report a top–1 accuracy of 58.0%, 53.9% and 67.0% for the tasks on simulated\nspectra, respectively. In different experiments, we observe that weaknesses present in\nmodels trained on a single modality can be eliminated by combining the two modalities.\nErroneous model predictions are very similar to the target molecules, with an average\nTanimoto similarity of 0.55 for the model trained 1H and 13C spectra. This demonstrates\nthat the model predictions, even when incorrect, provide chemists with structure guesses\nthat are close to the correct compound.\nIn another task, we train models to select, among potential candidates, the molecule\ncorresponding to an NMR spectrum. We find that for all three modalities the model is\nable to accomplish this task with a top–1 accuracy above 90%, compared to a random\nbaseline of 31%.\nThe models trained on simulated data in this work will provide a basis for fine-tuning in\nsettings in which datasets of experimental spectra are available — learning the variability\nof experimental data while leveraging fundamentals learned from simulated data.\nThese advancements hold the potential to transform the analysis of NMR spectra, en-\nabling faster and more accurate identification and characterization of compounds. As a\nresult, the integration of automated NMR analysis into the workflow of high-throughput\nexperiments promises to enhance efficiency and accelerate discoveries in the field of chem-\nistry.\nCode availability\nThe code for generating the data and training the models is available athttps://github.\ncom/rxn4chemistry/nmr-to-structure.\nData availability\nThe reactions and molecules for which the NMR spectra were generated from NextMove\nSoftware in the Pistachio dataset [25]. The simulated NMR spectra are are available from\nthe authors upon request.\n13\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAcknowledgments\nThis publication was created as part of NCCR Catalysis (grant number 180544), a Na-\ntional Centre of Competence in Research funded by the Swiss National Science Founda-\ntion.\n4. Methods\n4.1. Synthetic Data\nBefore generating spectra, 1,029,381 reactions were sampled from the Pistachio patent\ndataset [25]. A set of molecules was assembled from the precursors and products of these\nreactions. Molecules were filtered out if they contain atoms other than carbon, hydrogen,\noxygen, nitrogen, sulfur, phosphorous and the halogens. In addition, all molecules with\na heavy atom count outside the range of 5–35, charged molecules or containing isotope\ninformation were filtered out.\nFrom this set, 1,120,390 1H and 1,943,950 13C NMR spectra were generated using\nMestreNova. Standard simulation settings were used for 1H NMRs. For 13C NMRs, 1H\nand 19F decoupled spectra were generated. For 13C NMR the position of all peaks was\nrecorded. On the other hand 1H NMR were further processed. First peak-picking was ap-\nplied, followed by the autointegration and automultiplet assignment. All three processing\nsteps were carried out using built-in MestreNova functions with standard settings. For\neach peak in an 1H NMR, the range of the peak, its centroid, the number of hydrogen\natoms and the multiplet was recorded. See the associated GitHub repository to replicate\nthe simulations (see “Code Availability”).\n4.2. Model\nWe base our model architecture on the Molecular Transformer [7]. The model takes the\nformatted NMR spectrum with the chemical formula as input and outputs a molecular\nstructure encoded as SMILES. This can be formulated as a translation task from the\nspectrum to the molecular structure. The model is implemented using the standard\ntransformer of OpenNMT-py library [30,31] with the following hyperparameters deviating:\nword_vec_size: 512\nhidden_size: 512\nlayers: 4\nbatch_size: 4096\nAll models are trained for 350k steps amounting to approximately 35h on a A100 GPU.\n14\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n4.3. Tokenization\nTo tokenize 1H NMR peaks, we proceed as follows. The position of the peak is rounded\nto the second decimal point, the type of multiplet (singlet, doublet, triplet, etc.) and\nthe number of hydrogens are appended as second and third token respectively. All peaks\nare separated with a separating token (“ |”). As an example a singlet at 1.239 ppm with\nan integral of 3 would become “ 1.24 s 3H |”, with tokens separated by whitespaces. A\nstring of the 1H NMR spectrum is built accordingly by concatenating the peaks starting\nwith the lowest ppm and ending at the highest one. In addition, a prefix token is used\nto differentiate 1H from 13C NMR spectra. As an example an 1H NMR with two peaks\nwould be formatted as follows: “ 1HNMR 1.24 t 3H | 1.89 q 3H |”.\n13C NMR are formatted according to a simpler scheme. As the multiplet type and\nintegration is not relevant for this type of spectrum the position of the peaks are rounded to\none decimal point and tokenized accordingly. To illustrate this, a typical NMR spectrum\nis tokenized as follows: “ 13CNMR 12.1 27.8 63.5”.\nIn addition to the spectra, the model is provided the chemical formula in addition to\nthe NMR spectrum. The formula is calculated using RDKit [32] and prepended to the\nspectrum.\nWhen both 1H and 13C NMR are used, the tokenized string consists first of the chemical\nformula, followed by the 1H NMR spectrum and finally the 13C NMR. To have the1H and\n13C NMR share the same token space, the ppm values of the 13C NMR peaks are divided\nby 10.\n4.4. Data augmentation\nThe spectra are augmented using jitter augmentation as used previously by Jonas et.\nal. [21]. This involves adding a a random distortion sampled from a range of 0.5 ppm for\n1H NMR and 5 ppm for 13C NMR. The random noise is added to each of the peaks in\nthe spectra. In total, two augmented spectra are produced for each original one.\nReferences\n[1] Qingxin Li and CongBao Kang. A Practical Perspective on the Roles of Solution\nNMR Spectroscopy in Drug Discovery. Molecules, 25(13):2974, 2020.\n[2] David R. Klein. Organic Chemistry. Wiley, 2013.\n[3] Gregory M. Banik, Grace Baysinger, Prashant V. Kamat, and Norbert Pienta. The\nACS Guide to Scholarly Communication. American Chemical Society, 2020.\n15\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n[4] Gayathri Dev Ammini, Jordan P. Hooker, Joren Van Herck, Anil Kumar, and Tanja\nJunkers. Comprehensive high-throughput screening of photopolymerization under\nlight intensity variation using inline NMR monitoring. Polym. Chem., 14(22):2708–\n2716, 2023.\n[5] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with\nDeep Convolutional Neural Networks. In Advances in Neural Information Processing\nSystems, volume 25, 2012.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need, 2017.\narXiv:1706.03762.\n[7] Philippe Schwaller, Teodoro Laino, Th´ eophile Gaudin, Peter Bolgar, Christopher A.\nHunter, Costas Bekas, and Alpha A. Lee. Molecular Transformer: A Model for\nUncertainty-Calibrated Chemical Reaction Prediction. ACS Cent. Sci., 5(9):1572–\n1583, 2019.\n[8] Zhichao Liu, Ruth A. Roberts, Madhu Lal-Nag, Xi Chen, Ruili Huang, and Weida\nTong. AI-based language models powering drug discovery and development. Drug\nDiscovery Today, 26(11):2593–2607, 2021.\n[9] Andres M. Bran, Sam Cox, Andrew D. White, and Philippe Schwaller. ChemCrow:\nAugmenting large-language models with chemistry tools, 2023. arXiv:2304.05376.\n[10] Melodie Christensen, Lars P. E. Yunker, Parisa Shiri, Tara Zepel, Paloma L. Prieto,\nShad Grunert, Finn Bork, and Jason E. Hein. Automation isn’t automatic. Chemical\nScience, 12(47):15473–15490, 2021.\n[11] Milad Abolhasani and Eugenia Kumacheva. The rise of self-driving labs in chemical\nand materials sciences. Nat. Synth, 2(6):483–492, 2023.\n[12] Steven M. Mennen, Carolina Alhambra, C. Liana Allen, Mario Barberis, Simon\nBerritt, Thomas A. Brandt, Andrew D. Campbell, Jes´ us Casta˜ n´ on, Alan H. Cherney,\nMelodie Christensen, David B. Damon, J. Eugenio de Diego, Susana Garc´ ıa-Cerrada,\nPablo Garc´ ıa-Losada, Rub´ en Haro, Jacob Janey, David C. Leitch, Ling Li, Fangfang\nLiu, Paul C. Lobben, David W. C. MacMillan, Javier Magano, Emma McInturff,\nSebastien Monfette, Ronald J. Post, Danielle Schultz, Barbara J. Sitter, Jason M.\nStevens, Iulia I. Strambeanu, Jack Twilton, Ke Wang, and Matthew A. Zajac. The\nEvolution of High-Throughput Experimentation in Pharmaceutical Development and\nPerspectives on the Future. Org. Process Res. Dev., 23(6):1213–1242, 2019.\n16\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n[13] Alexander Buitrago Santanilla, Erik L. Regalado, Tony Pereira, Michael Shevlin,\nKevin Bateman, Louis-Charles Campeau, Jonathan Schneeweis, Simon Berritt, Zhi-\nCai Shi, Philippe Nantermet, Yong Liu, Roy Helmy, Christopher J. Welch, Petr\nVachal, Ian W. Davies, Tim Cernak, and Spencer D. Dreher. Nanomole-scale high-\nthroughput chemistry for the synthesis of complex molecules. Science, 347(6217):49–\n53, 2015.\n[14] Damith Perera, Joseph W. Tucker, Shalini Brahmbhatt, Christopher J. Helal, Ash-\nley Chong, William Farrell, Paul Richardson, and Neal W. Sach. A platform for\nautomated nanomole-scale reaction screening and micromole-scale synthesis in flow.\nScience, 359(6374):429–434, 2018.\n[15] Michael Shevlin. Practical High-Throughput Experimentation for Chemists. ACS\nMed. Chem. Lett., 8(6):601–607, 2017.\n[16] Babak Mahjour, Rui Zhang, Yuning Shen, Andrew McGrath, Ruheng Zhao,\nOsama G. Mohamed, Yingfu Lin, Zirong Zhang, James L. Douthwaite, Ashootosh\nTripathi, and Tim Cernak. Rapid planning and analysis of high-throughput experi-\nment arrays for reaction discovery. Nat Commun, 14(1):3924, 2023.\n[17] Adam Cook, Roxanne Cl´ ement, and Stephen G. Newman. Reaction screening in\nmultiwell plates: high-throughput optimization of a Buchwald–Hartwig amination.\nNat Protoc, 16(2):1152–1169, 2021.\n[18] MestreLab, MNova. https://mestrelab.com/software/mnova/ (Accessed July 24,\n2023).\n[19] ACD Labs, NMR Workbook Suite. https://www.acdlabs.com/products/\nspectrus-platform/nmr-workbook-suite/ (Accessed July 24, 2023).\n[20] Zhaorui Huang, Michael S. Chen, Cristian P. Woroch, Thomas E. Markland, and\nMatthew W. Kanan. A framework for automated structure elucidation from routine\nNMR spectra. Chem. Sci., 12(46):15329–15338, 2021.\n[21] Eric Jonas. Deep imitation learning for molecular inverse problems. In Advances in\nNeural Information Processing Systems, volume 32, 2019.\n[22] Weiwei Wei, Yuxuan Liao, Yufei Wang, Shaoqi Wang, Wen Du, Hongmei Lu,\nBo Kong, Huawu Yang, and Zhimin Zhang. Deep Learning-Based Method for Com-\npound Identification in NMR Spectra of Mixtures. Molecules, 27(12):3653, 2022.\n[23] Iv´ an Cort´ es, Cristina Cuadrado, Antonio Hern´ andez Daranas, and Ariel M. Sarotti.\nMachine learning in computational NMR-aided structural elucidation. Frontiers in\nNatural Products, 2, 2023.\n17\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\n[24] Jinzhe Zhang, Kei Terayama, Masato Sumita, Kazuki Yoshizoe, Kengo Ito, Jun\nKikuchi, and Koji Tsuda. NMR-TS: de novo molecule identification from NMR\nspectra. Science and Technology of Advanced Materials, 21(1):552–561, 2020.\n[25] NextMove Software, Pistachio. https://www.nextmovesoftware.com/pistachio.\nhtml (Accessed July 24, 2023).\n[26] Marvin Alberts, Teodoro Laino, and Alain C. Vaucher. Leveraging Infrared Spec-\ntroscopy for Automated Structure Elucidation, 2023. DOI: 10.26434/chemrxiv-2023-\n5v27f.\n[27] David Weininger. SMILES, a chemical language and information system. 1. Introduc-\ntion to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31–36,\n1988.\n[28] D´ avid Bajusz, Anita R´ acz, and K´ aroly H´ eberger. Why is Tanimoto index an appro-\npriate choice for fingerprint-based similarity calculations? Journal of Cheminformat-\nics, 7(1):20, 2015.\n[29] David Rogers and Mathew Hahn. Extended-Connectivity Fingerprints. J. Chem.\nInf. Model., 50(5):742–754, 2010.\n[30] OpenNMT-py: Open-Source Neural Machine Translation, 2017. https://github.\ncom/OpenNMT/OpenNMT-py (Accessed July 24, 2023).\n[31] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M.\nRush. OpenNMT: Open-Source Toolkit for Neural Machine Translation, 2017.\narXiv:1701.02810.\n[32] RDKit. https://www.rdkit.org/ (Accessed July 24, 2023).\n[33] Daylight: SMARTS Examples. https://www.daylight.com/dayhtml_tutorials/\nlanguages/smarts/smarts_examples.html (Accessed April 20, 2023).\n18\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nAppendix\nA. Simulated NMR spectra\nIn this section, we provide a description of the molecular dataset used to generate the\nNMR spectra. We calculate the heavy atom count for all molecules in the dataset. We\nuse the heavy atom count as an easily understandable proxy metric for the complexity of\nmolecules. As can be seen in Figure 6, our dataset shows a relatively flat distribution in\nthe range of 11 to 28. In addition, we calculate the Bertz complexity for all molecules in\nthe set. The average complexity for this dataset evaluates to 744.\n5 10 15 20 25 30 35\nHeavy Atom Count\n0\n1\n2\n3\n4Percent\nFigure 6: Heavy Atom count distribution of the simulated dataset.\nB. Functional group definitions\nFunctional groups are defined in SMARTS as shown in Table 3. Using these SMARTS\nand RDKit the presence of a certain function group is determined by invoking <RDKit\nmolecule>.GetSubstrucMatches(<RDKit molecule from SMARTS pattern>)\n19\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTable 3: Functional group definitions used.\nDefinition\nAlcohol [OX2H][CX4;!$(C([OX2H])[O,S,#7,#15])]\nCarboxylic Acid [CX3](=O)[OX2H1]\nEster [#6][CX3](=O)[OX2H0][#6]\nEther [OD2]([#6])[#6]\nAldehyde [CX3H1](=O)[#6]\nKetone [#6][CX3](=O)[#6]\nAlkene [CX3]=[CX3]\nAlkyne [$([CX2]#C)]\nBenzene c1ccccc1\nPrimary Amine [NX3;H2;!$(NC=[!#6]);!$(NC#[!#6])][#6]\nSecondary Amine [NH1,nH1])\nTertiary Amine [NH0,nH0])\nAmide [NX3][CX3](=[OX1])[#6]\nCyano [NX1]#[CX2]\nFluorine [#6][F]\nChlorine [#6][Cl]\nIodine [#6][I]\nBromine [#6][Br]\nSulfonamide [#16X4]([NX3])(=[OX1])(=[OX1])[#6]\nSulfone [#16X4](=[OX1])(=[OX1])([#6])[#6]\nSulfide [#16X2H0]\nPhosphoric Acid†\n[$(P(=[OX1])([$([OX2H]),$([OX1-]),$([OX2]P)])([$([OX2H]),\n$([OX1-]),$([OX2]P)])[$([OX2H]),$([OX1-]),$([OX2]P)]),\n$([P+]([OX1-])([$([OX2H]),$([OX1-]),$([OX2]P)])([$([OX2H]),\n$([OX1-]),$([OX2]P)])[$([OX2H]),$([OX1-]),$([OX2]P)])]\n†\nAdapted from [33]\nC. Functional group definition\nIn Tables 4, 5, and 6, the accuracy of the model solely trained on 1H , 13C , and combined\n1H /13C NMR data, respectively, is shown depending on the presence of specific functional\ngroups in the target molecule. “Count” represents the number of molecules with this\nfunctional group in the test set. Additionally, the average heavy atom count (“Avg.\nHAC” in the table) is calculated to rule out bias.\n20\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTable 4: The model trained on 1H NMR spectra’s ability to predict the correct molecular\nstructure based on if a specific functional group is present in the target molecule.\nCount Avg. HAC Top–1% Top–5% Top–10%\nPhosphoric Acid 76 27.09 31.58 47.37 48.68\nAlkene 12727 22.55 46.94 66.87 70.46\nCyano 7691 23.58 53.54 71.92 75.83\nAlkyne 2071 23.39 54.23 71.61 74.89\nAlcohol 17214 22.86 54.23 74.83 78.49\nSulfide 15214 23.85 55.06 73.41 77.06\nPrimary Amine 12504 21.30 55.42 75.99 79.57\nAmide 31834 26.10 56.13 74.26 77.77\nChlorine 23685 23.59 56.42 75.31 78.95\nTertiary Amine 83118 24.01 56.74 74.85 78.30\nCarboxylic Acid 13838 23.26 56.79 77.03 80.60\nKetone 8100 22.35 56.91 73.10 76.35\nSecondary Amine 56201 24.50 56.96 75.16 78.65\nFluorine 30166 25.16 57.70 75.59 78.97\nEther 34926 24.98 58.75 76.93 80.16\nSulfone 2428 26.03 58.86 75.41 78.46\nBenzene 86972 24.08 58.86 76.92 80.18\nSulfonamide 5758 26.44 59.48 76.55 79.63\nEster 16344 23.20 59.50 79.08 82.13\nAldehyde 2208 19.09 60.19 79.71 83.02\nBromine 9687 20.11 60.48 80.21 83.47\nIodine 1728 19.93 62.21 82.52 85.30\n21\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTable 5: The model trained on 13C NMR spectra’s ability to predict the correct molecular\nstructure based on if a specific functional group is present in the target molecule.\nCount Avg. HAC Top–1% Top–5% Top–10%\nPhosphoric Acid 142 26.54 36.62 55.63 59.86\nAlkene 21149 23.09 40.65 60.35 64.87\nAlcohol 21781 23.11 48.25 69.14 74.11\nSulfide 26917 23.90 50.54 69.80 74.08\nPrimary Amine 22672 21.39 51.14 72.07 76.61\nAmide 51806 26.33 51.29 69.90 74.21\nCyano 13327 23.34 51.30 70.18 74.57\nChlorine 40757 23.39 51.43 71.69 76.30\nSecondary Amine 85969 24.94 51.97 71.35 75.62\nTertiary Amine 146144 24.10 52.21 71.32 75.66\nFluorine 49707 25.00 52.33 72.09 76.49\nCarboxylic Acid 18879 23.21 54.47 75.34 79.46\nIodine 3193 19.40 54.56 76.20 80.74\nSulfone 4928 25.85 54.61 71.25 75.59\nBenzene 149174 24.31 54.79 73.83 77.95\nAlkyne 3700 23.31 54.89 73.14 76.89\nBromine 17680 19.99 55.71 76.84 81.46\nSulfonamide 9319 26.70 55.77 73.00 76.97\nKetone 14910 22.41 56.32 73.66 77.94\nEther 65246 25.10 56.60 75.00 78.97\nAldehyde 4452 19.25 57.46 78.23 82.88\nEster 33632 23.47 58.11 78.01 81.80\n22\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nTable 6: The model trained on both 1H and 13C NMR spectra’s ability to predict the\ncorrect molecular structure based on if a specific functional group is present in\nthe target molecule.\nCount Avg. HAC Top–1% Top–5% Top–10%\nPhosphoric Acid 71 25.82 38.03 50.70 54.93\nAlkene 12799 22.36 54.68 74.48 77.40\nAlcohol 16967 22.77 62.32 82.14 85.04\nPrimary Amine 12378 21.36 63.94 83.16 85.85\nSulfide 15219 24.17 64.02 80.92 83.68\nAmide 32013 26.12 64.90 82.31 85.11\nChlorine 23849 23.58 65.57 82.79 85.53\nSecondary Amine 56290 24.50 65.67 82.79 85.55\nCyano 7767 23.61 65.91 82.17 84.97\nFluorine 30724 25.09 66.00 82.98 85.66\nSulfone 2537 26.08 66.30 81.75 84.15\nTertiary Amine 83173 24.01 66.31 82.98 85.59\nCarboxylic Acid 13719 23.30 66.80 85.41 87.82\nAlkyne 2070 23.49 66.86 83.24 85.89\nKetone 8241 22.29 67.10 82.55 85.01\nEster 16499 23.25 67.66 85.24 87.50\nBenzene 87374 24.05 67.85 84.40 86.86\nEther 34823 24.86 67.87 84.35 86.75\nSulfonamide 5663 26.58 68.20 83.68 86.39\nIodine 1705 19.88 68.68 85.34 87.21\nBromine 9838 20.19 69.66 86.91 89.04\nAldehyde 2152 19.11 70.77 87.04 89.22\n23\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0\nD. Tanimoto Similarity Distribution\nIn Figure 7 the Tanimoto similarity distribution for all three models is illustrated. The\ndistribution shows a peak around 0.55 for all three models.\na)\nb)\nc)\nFigure 7: The Tanimoto distribution of three models: a) 1H NMR, b) 13C NMR, c)\n1H+13C NMR. All correct molecules were excluded.\n24\nhttps://doi.org/10.26434/chemrxiv-2023-8wxcz ORCID: https://orcid.org/0009-0003-9198-7866 Content not peer-reviewed by ChemRxiv. License: CC BY-NC-ND 4.0",
  "topic": "NMR spectra database",
  "concepts": [
    {
      "name": "NMR spectra database",
      "score": 0.6722438335418701
    },
    {
      "name": "Spectral line",
      "score": 0.632199227809906
    },
    {
      "name": "Nuclear magnetic resonance spectroscopy",
      "score": 0.6052083373069763
    },
    {
      "name": "Transformer",
      "score": 0.5094373822212219
    },
    {
      "name": "Computer science",
      "score": 0.4716431498527527
    },
    {
      "name": "Spectrum (functional analysis)",
      "score": 0.44990074634552
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4090900719165802
    },
    {
      "name": "Chemistry",
      "score": 0.3801952004432678
    },
    {
      "name": "Machine learning",
      "score": 0.3513938784599304
    },
    {
      "name": "Biological system",
      "score": 0.32086578011512756
    },
    {
      "name": "Physics",
      "score": 0.1365366280078888
    },
    {
      "name": "Stereochemistry",
      "score": 0.08377259969711304
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}