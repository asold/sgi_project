{
  "title": "CoTexT: Multi-task Learning with Code-Text Transformer",
  "url": "https://openalex.org/W3183962691",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2003050958",
      "name": "Long Phan",
      "affiliations": [
        "Case Western Reserve University"
      ]
    },
    {
      "id": "https://openalex.org/A2120452325",
      "name": "Hieu Tran",
      "affiliations": [
        "Vietnam National University Ho Chi Minh City",
        "Ho Chi Minh City University of Science"
      ]
    },
    {
      "id": "https://openalex.org/A2124458966",
      "name": "Daniel Le",
      "affiliations": [
        "Case Western Reserve University"
      ]
    },
    {
      "id": "https://openalex.org/A2123210919",
      "name": "Hieu Nguyen",
      "affiliations": [
        "Case Western Reserve University"
      ]
    },
    {
      "id": "https://openalex.org/A3186465998",
      "name": "James Annibal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3161520421",
      "name": "Alec Peltekian",
      "affiliations": [
        "Case Western Reserve University"
      ]
    },
    {
      "id": "https://openalex.org/A2107909056",
      "name": "Yanfang Ye",
      "affiliations": [
        "Case Western Reserve University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3033638351",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3098044990",
    "https://openalex.org/W3025624935",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2972135640",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2970862273",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3170092793"
  ],
  "abstract": "We present CoTexT, a pre-trained, transformer-based encoder-decoder model that learns the representative context between natural language (NL) and programming language (PL). Using self-supervision, CoTexT is pre-trained on large programming language corpora to learn a general understanding of language and code. CoTexT supports downstream NL-PL tasks such as code summarizing/documentation, code generation, defect detection, and code debugging. We train CoTexT on different combinations of available PL corpus including both “bimodal” and “unimodal” data. Here, bimodal data is the combination of text and corresponding code snippets, whereas unimodal data is merely code snippets. We first evaluate CoTexT with multi-task learning: we perform Code Summarization on 6 different programming languages and Code Refinement on both small and medium size featured in the CodeXGLUE dataset. We further conduct extensive experiments to investigate CoTexT on other tasks within the CodeXGlue dataset, including Code Generation and Defect Detection. We consistently achieve SOTA results in these tasks, demonstrating the versatility of our models.",
  "full_text": "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), pages 40–47\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n40\nCoTexT: Multi-task Learning with Code-Text Transformer\nLong Phan1, Hieu Tran2, Daniel Le1, Hieu Nguyen1, James Anibal1, Alec Peltekian1, and Yanfang Ye1\n1Case Western Reserve University, Ohio, USA\n2University of Science, VNU-HCM, Vietnam\n{lnp26, yxy1032}@case.edu\nAbstract\nWe present CoTexT, a pre-trained, transformer-\nbased encoder-decoder model that learns\nthe representative context between natural\nlanguage (NL) and programming language\n(PL). Using self-supervision, CoTexT is pre-\ntrained on large programming language cor-\npora to learn a general understanding of lan-\nguage and code. CoTexT supports down-\nstream NL-PL tasks such as code summariz-\ning/documentation, code generation, defect de-\ntection, and code debugging. We train CoTexT\non different combinations of available PL cor-\npus including both ”bimodal” and ”unimodal”\ndata. Here, bimodal data is the combination of\ntext and corresponding code snippets, whereas\nunimodal data is merely code snippets. We\nﬁrst evaluate CoTexT with multi-task learning:\nwe perform Code Summarization on 6 differ-\nent programming languages and Code Reﬁne-\nment on both small and medium size featured\nin the CodeXGLUE dataset. We further con-\nduct extensive experiments to investigate Co-\nTexT on other tasks within the CodeXGlue\ndataset, including Code Generation and Defect\nDetection. We consistently achieve SOTA re-\nsults in these tasks, demonstrating the versatil-\nity of our models.\n1 Introduction\nIn recent years, pre-trained language models (LM)\nhave played a crucial role in the development of\nmany natural language processing (NLP) systems.\nBefore the emergence of large LMs, traditional\nword embedding gives each word/token a global\nrepresentation. Large pre-trained models such as\nELMo (Peters et al., 2018), GPT (Brown et al.,\n2020), BERT (Devlin et al., 2018), and XLNet\n(Yang et al., 2020) can derive contextualized word\nvector representations from large corpora. These\nmethods can learn generalized representations of\nlanguage and have signiﬁcantly improved a broad\nrange of downstream NLP tasks. These LMs\nmake use of learning objectives such as Masked\nLanguage Modeling (MLM) (Devlin et al., 2018)\nwhere random tokens in a sequence are masked\nand the model predicts the original tokens to learn\nthe context. The success of pre-trained models in\nNLP has created a path for domain-speciﬁc pre-\ntrained LMs, such as BioBERT (Lee et al., 2019a)\non biomedical text, or TaBERT (Yin et al., 2020)\non NL text and tabular data.\nWe introduce CoTexT (Code and Text Trans-\nfer Transformer), a pre-trained model for both nat-\nural language (NL) and programming language\n(PL) such as Java, Python, Javascript, PHP, etc.\nCoTexT follows the encoder-decoder architecture\nproposed by (Vaswani et al., 2017) with attention\nmechanisms. We then adapt the model to match T5\nframework proposed by (Raffel et al., 2019). We\ntest CoTexT by performing exhaustive experiments\non multi-task learning of multiple programming\nlanguages and other related tasks.\nWe train CoTexT using large programming lan-\nguage corpora containing multiple programming\nlanguages (including Java, Python, JavaScript,\nRuby, etc.). Here, we test different combinations\nof unimodal and bimodal data to produce the best\nresult for each downstream task. We then ﬁne-\ntune CoTexT on four CodeXGLUE tasks (Lu et al.,\n2021) including CodeSummarization, CodeGenera-\ntion, Defect Detection and Code Reﬁnement (small\nand medium dataset). Results show that we achieve\nstate-of-the-art values for each of the four tasks.\nWe found that CoTexT outperforms current SOTA\nmodels such as CodeBERT (Feng et al., 2020) and\nPLBART (Ahmad et al., 2021a).\nIn this paper we offer the following contribution:\n• Three different versions of CoTexT that\nachieve state-of-the-art on the CodeXGLUE’s\nCodeSummarization, CodeGeneration, Defect\n41\nDetection and Code Reﬁnement (small and\nmedium dataset) tasks. We publicize our\nCoTexT pre-trained checkpoints and related\nsource code available for future studies and\nimprovements.\n2 Related Work\nRecent work on domain adaptation of BERT show\nimprovements compared to the general BERT\nmodel. BioBERT (Lee et al., 2019b) is further\ntrained from BERT BASE on biomedical articles\nsuch as PubMed abstracts and PMC articles. Simi-\nlarly, SciBERT (Beltagy et al., 2019) is trained on\nthe full text of biomedical and computer science\npapers. The experimental results of these models\non domain-speciﬁc datasets show the enhanced per-\nformance compared to BERTBASE.\nRelating specﬁcally to our work, CodeBERT is\n(Feng et al., 2020) trained on bimodal data of NL-\nPL pairs. This strategy allows CodeBERT to learn\ngeneral-purpose representations of both natural lan-\nguage and programming language. GraphCode-\nBERT (Guo et al., 2021) is an extension of Code-\nBERT that moves beyond syntactic-level structure\nand uses data ﬂow in the pre-training stage to cap-\nture the semantic-level structure of code. More\nrecently, PLBART (Ahmad et al., 2021b) is a pre-\ntrained sequence-to-sequence model for NL and\nPL. Through denoising autoencoding, this model\ncan perform well on NL-PL understanding and gen-\neration tasks.\n3 CoTexT\n3.1 Vocabulary\nFollowing the example of T5 (Raffel et al., 2019),\nwe use the Sentence Piece Unsupervised Text Tok-\nenizer proposed by (Kudo and Richardson, 2018).\nThe Sentence Piece model extracts the sub-words\nthat contain the semantic context of a sequence. We\nemploy Sentence Piece as a vocabulary model for\nall of our contributed CoTexT models. However,\nthe special tokens used in code (such as ”[”, ”{”,\n”$”, etc) are out-of-vocab for the SentencePiece\nmodel 1. These tokens have a crucial representative\ncontext in programming languages. Therefore, to\nenhance the robustness of the model, we encode\nall of these missing tokens into a natural language\nrepresentation during both self-supervised and su-\npervised training.\n1https://github.com/google/sentencepiece\n)def add ( a , b : return a + b\ndef <X> a , b <Y> : return <Z>\n<X> add ( <Y> ) <Z> a + b\nT ransformer \nFigure 1: An illustration about Fill-in-the-blank objec-\ntive\n3.2 Pre-training CoTexT\nWe train CoTexT on both bimodal and unimodal\ndata. Bimodal data contains both code snippets and\nthe corresponding natural text in each sequence,\nwhile unimodal data contains only the sequence\nof code. We use two main datasets during self-\nsupervised training: CodeSearchNet Corpus Col-\nlection (Husain et al., 2020) and GitHub Reposi-\ntories2 data. The combinations of corpus used to\ntrain CoTexT are listed in Table 1. To save both\ntime and computing resources, we initialized the\ncheckpoints from the original T5 that was trained\non the C4 corpus. (Raffel et al., 2019).\n3.2.1 CodeSearchNet Corpus Collection\nCodeSearchNet Corpus (Husain et al., 2020) con-\ntains coded functions from open-source non-forked\nGithub repositories. This dataset spans 6 coding\nlanguages (Python, Java, Javascript, PHP, Ruby,\nGo), which facilitates multi-task learning. Code-\nSearchNet also contains a natural language descrip-\ntion for each function. For bimodal data, we simply\nconcatenate the natural language snippet with the\ncorresponding code snippet to create one input se-\nquence. These data are then processed as described\nin 3.1.\n3.2.2 GitHub repositories\nWe download a large collection of Java and Python\nfunctions from the GitHub repositories dataset\navailable on Google BigQuery. These Java and\nPython functions are then extracted and the natural\nlanguage descriptions are obtained using the pre-\nprocessing pipeline from (Lachaux et al., 2020).\nThese datapoints also run through a pipeline to\nreplace special tokens (as described in 3.1).\n3.3 Input/Output Representations\nCoTexT converts all NLP problems into a text-\nto-text format. This means that during both self-\n2https://console.cloud.google.com/marketplace/details/github/github-\nrepos\n42\nTable 1: Pre-training CoTexT on different combinations of natural language and programming language copora\nModel N-modal Corpus combination\nT5 NL C4\nCoTexT (1-CC) PL C4 + CodeSearchNet\nCoTexT (2-CC) NL-PL C4 + CodeSearchNet\nCoTexT (1-CCG) PL C4 + CodeSearchNet + Github Repos\nsupervised pre-training and supervised training, we\nuse an input sequence and a target sequence. For\nthe bimodal model, we concatenate a sequence\nof natural language text and the corresponding se-\nquence of programming language text as an in-\nput. For the unimodal model, we simply use each\ncoded function as an input sequence. During self-\nsupervised training, spans of the input sequence\nare randomly masked and the target sequence (Raf-\nfel et al., 2019) is formed as the concatenation\nof the same sentinel tokens and the real masked\nspans/tokens.\n3.4 Model Architecture\nCoTexT follows the sequence-to-sequence encoder-\ndecoder architecture proposed by (Vaswani et al.,\n2017). We initialize the Base T5 model released\nby (Raffel et al., 2019) which has 220 million pa-\nrameters. We train the model with a 0.001 learning\nrate and an input/target length of 1024. With the\nprovided TPU v2-8 on Google Colab, we train with\nthe recommended setting of model parallelism 2\nand batch size 128.\n3.5 Multi-task Learning\nThe model is trained with maximum likelihood ob-\njective (that is using ”teacher forcing” (Williams\nand Zipser, 1989)) regardless of the text-code or\ncode-text tasks. Therefore, for CoTexT, we lever-\nage the potential for Multi-Task learning (Raffel\net al., 2019) to complete both text-code and code-\ntext generation on CodeSummarization and Code\nReﬁnement tasks. To specify the task our model\nshould perform, we simply add a task-speciﬁc pre-\nﬁx to the input sequence. For example, when ﬁne-\ntuning of the CodeSummarization task for each pro-\ngramming language, we simply prepend a preﬁx\nfor each PL name (i.e., Java) to the input sequence.\n4 Experiments\nIn this section, we will ﬁrst describe the benchmark\ndataset for code intelligence CodeXGLUE, then we\nCoT exT \njavascript: console.log(\"Hello\");\nruby: puts \"Hello\"\ngo: fmt.Println(\"Hello\")\npython: print(\"Hello\")\njava: System.out.println(\"Hello\");\nPHP: echo \"Hello\";\nTo display Hello on the screen\nFigure 2: An illustration about Multi-task learning\nwill explain the experimental setup on the tasks we\nperform and discuss the results of each task. The\nevaluation datasets are summarized in Table 3.\n4.1 CodeXGLUE\nGeneral Language Understanding Evaluation\nbenchmark for CODE (CodeXGLUE) (Lu et al.,\n2021) is a benchmark dataset to facilitate machine\nlearning studies on code understanding and code\ngeneration problems. This dataset includes a collec-\ntion of code intelligence tasks (both classiﬁcation\nand generation), a platform for model evaluation,\nand a leaderboard for comparison. CodeXGLUE\nhas 10 code intelligence tasks including code-text,\ntext-code, code-code, and text-text scenarios. For\nCoTexT, we focus on Code Summarization, Code\nGeneration, Code Reﬁnement, and Defect Detec-\ntion tasks.\n4.2 Evaluation Tasks\nWe evaluate our programming language and natural\nlanguage generation tasks on TPU v2-8 with the\nsettings from the original T5 model (Raffel et al.,\n2019). The input length and target length for each\ntask are described in Table 2.\n4.2.1 Code Summarization\nFor Code Summarization, the objective is to gener-\nate a natural language description for a given code\nsnippet. The task includes a CodeSearchNet dataset\n(Husain et al., 2019) with 6 different programming\nlanguages: Python, Java, Javascript, PHP, Ruby,\nGo. The data comes from public open-source non-\nfork GitHub repositories and the annotations are ex-\n43\nTable 2: The input and target sequence length settings for each self-supervised learning, code summarization, code\ngeneration, code reﬁnement, and defect detection task\nTask Dataset Task Type Input Length Target Length\nSelf-supervised Learning CodSearchNet Corpus 1024 1024\nGitHub Repositories 1024 1024\nCode Summarization CodeSearchNet Multi-Task 512 512\nCode Generation CONCODE Single-Task 256 256\nCode Reﬁnement Bugs2Fix small Multi-Task 512 512Bugs2Fixmedium\nDefect Detection Devign Single-Task 1024 5\ntracted from function documentation as described\nin (Husain et al., 2019).\n4.2.2 Code Generation\nText-to-Code Generation aims to generate a coded\nfunction given a natural language description. This\ntask is completed using the CONCODE dataset\n(Iyer et al., 2018), a well-known dataset for Java\nlanguage generation. Within the dataset, there are\ntuples which contain a natural language description,\ncode environments, ad code snippets. The goal is to\ngenerate the correct Java function from the natural\nlanguage description in the form of Javadoc-style\nmethod comments.\n4.2.3 Code Reﬁnement\nCode Reﬁnement, or Code Repair, aims to au-\ntomatically correct bugs in Java code. We used\nthe Bug2Fix corpus released by CodeXGLUE (Lu\net al., 2021), which divides the task into 2 subsets:\nSMALL and MEDIUM The small dataset includes\nonly Java code functions with fewer than 50 to-\nkens. The medium dataset includes functions with\n50-100 tokens.\n4.2.4 Defect Detection\nFor Defect Detection tasks, we attempt to clas-\nsify whether a PL snippet contains vulnerabilities\nthat could lead to damaging outcomes such as re-\nsource leaks or DoS attacks. The task uses the De-\nvign dataset (Zhou et al., 2019), which contains C\nprogramming language from open-source projects.\nThis dataset is labeled based on security-related\ncommits. For details on the annotation process,\nrefer to (Zhou et al., 2019).\n4.3 Experimental Setup\n4.3.1 Baselines\nWe compare our model with some well-known pre-\ntrained models:\n• CodeGPT, CodeGPT-adapted are based on the\narchitecture and training objective of GPT-2\n(Budzianowski and Vulic, 2019). CodeGPT\nis pre-trained from scratch on CodeSearch-\nNet dataset (Lu et al., 2021) while CodeGPT-\nadapted learns this dataset starting from the\nGPT-2 checkpoint.\n• CodeBERT (Feng et al., 2020) employs the\nsame architecture as RoBERTa (Liu et al.,\n2020) but aims to minimize the combined loss\nfrom masked language modeling and replaced\ntoken detection.\n• PLBART (Ahmad et al., 2021b) is a\nTransformer-based model. BART (Lewis\net al., 2019) is trained on PL corpora using\nthree learning strategies: token masking, to-\nken deletion, and token inﬁlling.\n4.3.2 Performance Metrics\n• BLEU (Papineni et al., 2002) is an algo-\nrithm which performs automatic evaluation\nof machine-translated text. This method cal-\nculates the n-gram similarity of a candidate\ntranslation compared to a set of reference texts.\nSimilar to (Feng et al., 2020) and (Ahmad\net al., 2021b), we use smooth BLEU-4 score\n(?) for Code Summarization and corpus-level\nBLEU score for all remaining tasks.\n• CodeBLEU (Ren et al., 2020) is designed to\nconsider syntactic and semantic features of\n44\nTable 3: Data statistics about Code Intelligence datasets\nCategory Task Dataset Size LanguageTrain Val Test\nCode-Text Code Summarization\n(Lu et al., 2021) CodeSearchNet\n164K 5.1K 10.9K Java\n58K 3.8K 3.2K Javascript\n251K 13.9K 14.9K Python\n241K 12.9K 14K PHP\n167K 7.3K 8.1K Go\n24K 1.4K 1.2K Ruby\nCode-Code\nDefect Detection Devign 21K 2.7K 2.7K C(Zhou et al., 2019)\nCode Reﬁnement\n(Lu et al., 2021)\nBugs2Fixsmall 46K 5.8K 5.8K JavaBugs2Fixmedium 52K 6.5K 6.5K\nText-Code Code Generation CONCODE 100K 2K 2K Java(Iyer et al., 2018)\ncodes based on the abstract syntax tree and\nthe data ﬂow structure.\n• Accuracy is the ratio of the number of gener-\nated sequences that harmonise the reference\nto the total number of observations.\n5 Results\n5.1 Multi-Task Learning\nWe ﬁrst report the result of CoTexT in Multi-Task\nLearning tasks including Code Summarization and\nCode Reﬁnement.\n5.1.1 Code Summarization\nFor the Code Summarization task, we perform\nMulti-Task Learning by using the T5 framework\n(Raffel et al., 2019) to ﬁnetune CoTexT on 6 difer-\nent programming language (Ruby, Javascript, Go,\nPython, Java, and PHP). The results of the Code\nSummarization task are shown in Table 5.\nFirst, we observe that the base T5, which is\npre-trained only on the general domain corpus\n(C4), is effective on this task. In fact, base T5\nachieves higher overall results on the BLEU-4 met-\nric compared to all other related models on the\nCodeXGLUE leaderboard. This shows the impor-\ntance of domain-speciﬁc T5 models, which we ex-\npect to achieve superior results compared to base\nT5.\nWe further observe that CoTexT achieves state-\nof-the-art (SOTA) on the overall score, the Python-\nspeciﬁc score, the Java-speciﬁc score, and the Go-\nspeciﬁc score. While CoTexT does not signiﬁcantly\noutperform other pre-trained models, we observe\nthat CoTexT achieves SOTA on two very common\nprogramming languages (Python and Java) while\nstill obtaining competitive results on other program-\nming languages. We attribute this result to the\nlarge amount of training data for Python and Java\ncompared to the other languages (training size de-\nscribed in Table 3). Based on this result, CoTeXT\nhas the potential to further surpass competitor mod-\nels as more training data becomes availible.\n5.1.2 Code Reﬁnement\nWe also tested CoTexT by performing multi-task\nlearning for Code Reﬁnement. In this case, both the\nsmall and medium test sets have a task registry with\nrespective preﬁx prepending to the input sequence.\nThe Code Reﬁnement results of each model are\nshown in Table 6. For this task, the base T5, which\nis pre-trained only on natural language text, does\nnot perform well compared to other transformer-\nbased models. Yet, after the training on a large\nprogramming language corpus, the result from Co-\nTexT improves signiﬁcantly on all metrics for both\nsmall and medium test sets. CoTexT achieves\nSOTA for all metrics on the small test set and on\nthe accuracy metric for the medium test set.\n5.2 Single-Task Learning\nIn addition to multi-task learning, we also evalu-\nate CoTexT performance single-task learning with\n45\nTable 4: Test result on Code Generation task\nModel Text2Code Generation\nEM BLEU CodeBLEU\nPLBART 18.75 36.69 38.52\nCodeGPT-adapted 20.10 32.79 35.98\nCodeGPT 18.25 28.69 32.71\nT5 18.65 32.74 35.95\nCoText (1-CCG) 19.45 35.40 38.47\nCoText (2-CC) 20.10 36.51 39.49\nCoText (1-CC) 20.10 37.40 40.14\nNotes: The best scores are in bold and second best scores are underlined. The baseline scores were obtained from the\nCodeXGLUE’s Leaderboard (https://microsoft.github.io/CodeXGLUE/)\nTable 5: Test result on Code Summarization task\nModel All Ruby Javascript Go Python Java PHP\nRoBERTa 16.57 11.17 11.90 17.72 18.14 16.47 24.02\nCodeBERT 17.83 12.16 14.90 18.07 19.06 17.65 25.16\nPLBART 18.32 14.11 15.56 18.91 19.3 18.45 23.58\nT5 18.35 14.18 14.57 19.17 19.26 18.35 24.59\nCoTexT (1-CCG) 18.00 13.23 14.75 18.95 19.35 18.75 22.97\nCoTexT (2-CC) 18.38 13.07 14.77 19.37 19.52 19.1 24.47\nCoTexT (1-CC) 18.55 14.02 14.96 18.86 19.73 19.06 24.58\nNotes: The best scores are in bold and second best scores are underlined. The baseline scores were obtained from the\nCodeXGLUE’s Leaderboard (https://microsoft.github.io/CodeXGLUE/)\nTable 6: Test result on Code Reﬁnement task\nSmall test set Medium test set\nModel BLEU Acc(%) CodeBLEU BLEU Acc(%) CodeBLEU\nTransformer 77.21 14.70 73.31 89.25 3.70 81.72\nCodeBERT 77.42 16.40 75.58 91.07 5.16 87.52\nPLBART 77.02 19.21 / 88.5 8.98 /\nT5 74.94 15.3 75.85 88.28 4.11 85.61\nCoTexT (1-CCG) 76.87 20.39 77.34 88.58 12.88 86.05\nCoTexT (2-CC) 77.28 21.58 77.38 88.68 13.03 84.41\nCoTexT (1-CC) 77.79 21.03 76.15 88.4 13.11 85.83\nNotes: The best scores are in bold and second best scores are underlined. The baseline scores were obtained from the\nCodeXGLUE’s Leaderboard (https://microsoft.github.io/CodeXGLUE/)\n46\nTable 7: Test result on Defect Detection task\nModel Accuracy\nRoBERTa 61.05\nCodeBERT 62.08\nPLBART 63.18\nT5 61.93\nCoTexT (1-CCG) 66.62\nCoTexT (2-CC) 64.49\nCoTexT (1-CC) 65.99\nNotes: The best scores are in bold and second\nbest scores are underlined. The baseline scores\nwere obtained from the CodeXGLUE’s Leaderboard\n(https://microsoft.github.io/CodeXGLUE/)\na Code Generation Task and a classiﬁcation task\nrelating to Defect Detection.\n5.2.1 Code Generation\nIn Table 4, we reported our results for the Code\nGeneration task wherein natural language is trans-\nlated into Java code. The result shows that our\nproposed model achieves SOTA results based on\n3 metrics: Exact Match (EM), BLEU, and Code-\nBLEU. For each individual metric, CoTexT has\nonly slightly outperformed other models (e.g both\nCoTexT and CodeGPT-adapted achieve 20.10 for\nEM). However, our model is consistently superior\nacross the 3 metrics. Prior to CoTexT, CodeGPT-\nadapted was SOTA for the EM metric and PLBART\nwas SOTA for the BLUE/CodeBLUE metrics.\nFrom this result, we infer that CoTexT has the best\noverall performance on this task and has great po-\ntential in the area of code generation.\n5.2.2 Defect Detection\nThe Defect Detection results are shown in Table\n7. Speciﬁcally, CoText outperforms the previ-\nous SOTA model (PLBART) by 3.44%. For this\ntask, extra training on a large programming cor-\npus allows CoTexT to outperform all other models\nand achieve SOTA results. The Defect Detection\ndataset consists of code written in the C program-\nming language, which is not contained in our train-\ning data. Our model has a strong understanding of\nsimilar languages, and is thus able to perform De-\nfect Detection in C with improved results compared\nto competitor models.\n6 Conclusion\nIn this manuscript, we introduced CoTexT, a pre-\ntrained language representation for both program-\nming language and natural language. CoTexT fo-\ncused on text-code and code-text understanding and\ngenerating. Leveraging the T5 framework (Raffel\net al., 2019), we showed that pre-training on a large\nprogramming language corpus is effective for a di-\nverse array of tasks within the natural language and\nprogramming language domain. CoTexT achieves\nstate-of-the-art results on 4 CodeXGLUE code in-\ntelligence tasks: Code Summarization, Code Gen-\neration, Code Reﬁnement, and Code Detection. For\nfuture work, we plan to test CoTexT on a broader\nrange of programming language and natural lan-\nguage generation tasks, such as autocompletion or\ncode translation.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021a. Uniﬁed pre-\ntraining for program understanding and generation.\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021b. Uniﬁed pre-\ntraining for program understanding and generation.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. CoRR, abs/1903.10676.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nPawel Budzianowski and Ivan Vulic. 2019. Hello, it’s\nGPT-2 - how can I help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. CoRR, abs/1907.05774.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\n47\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. CoRR, abs/2002.08155.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,\nDuyu Tang, Shujie LIU, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tufano,\nShao Kun Deng, Colin Clement, Dawn Drain, Neel\nSundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.\n2021. Graphcode{bert}: Pre-training code represen-\ntations with data ﬂow. In International Conference\non Learning Representations.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of seman-\ntic code search. CoRR, abs/1909.09436.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2020. Code-\nsearchnet challenge: Evaluating the state of seman-\ntic code search.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. CoRR, abs/1808.09588.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nCoRR, abs/1808.06226.\nMarie-Anne Lachaux, Baptiste Roziere, Lowik\nChanussot, and Guillaume Lample. 2020. Unsuper-\nvised translation of programming languages.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019a. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. CoRR, abs/1901.08746.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019b. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. CoRR, abs/1910.13461.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie\nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. CoRR, abs/1802.05365.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie\nLiu, Duyu Tang, Neel Sundaresan, Ming Zhou, Am-\nbrosio Blanco, and Shuai Ma. 2020. Codebleu: a\nmethod for automatic evaluation of code synthesis.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nRonald J. Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural Comput., 1(2):270–280.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2020.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding.\nPengcheng Yin, Graham Neubig, Wen tau Yih, and Se-\nbastian Riedel. 2020. Tabert: Pretraining for joint\nunderstanding of textual and tabular data.\nYaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning\nDu, and Yang Liu. 2019. Devign: Effective vulner-\nability identiﬁcation by learning comprehensive pro-\ngram semantics via graph neural networks.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8772889375686646
    },
    {
      "name": "Automatic summarization",
      "score": 0.6514824032783508
    },
    {
      "name": "Natural language processing",
      "score": 0.629450798034668
    },
    {
      "name": "Transformer",
      "score": 0.619864821434021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5697692036628723
    },
    {
      "name": "Code (set theory)",
      "score": 0.5514107346534729
    },
    {
      "name": "Programming language",
      "score": 0.5473849177360535
    },
    {
      "name": "Encoder",
      "score": 0.5324550271034241
    },
    {
      "name": "Debugging",
      "score": 0.5071547031402588
    },
    {
      "name": "Source code",
      "score": 0.46849995851516724
    },
    {
      "name": "Task (project management)",
      "score": 0.45844268798828125
    },
    {
      "name": "Natural language",
      "score": 0.42809563875198364
    },
    {
      "name": "Code generation",
      "score": 0.42794176936149597
    },
    {
      "name": "Operating system",
      "score": 0.07687142491340637
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    }
  ]
}