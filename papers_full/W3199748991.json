{
  "title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
  "url": "https://openalex.org/W3199748991",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2547953503",
      "name": "Mostafa Abdou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2759999012",
      "name": "Artur Kulmizev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A218792259",
      "name": "Daniel Hershcovich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2658674003",
      "name": "Stella Frank",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013784948",
      "name": "Ellie Pavlick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100615786",
      "name": "Anders Søgaard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2407887752",
    "https://openalex.org/W3132711698",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W202147582",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2896822765",
    "https://openalex.org/W1984394909",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W2137735870",
    "https://openalex.org/W3163313089",
    "https://openalex.org/W1845402413",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2801844694",
    "https://openalex.org/W2314623394",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W3156509887",
    "https://openalex.org/W3090805418",
    "https://openalex.org/W2078894097",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W3094503863",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2971157635",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3193068792",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3014056998",
    "https://openalex.org/W2108783283",
    "https://openalex.org/W3092733346",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2014788144",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W1497201888",
    "https://openalex.org/W2068829449",
    "https://openalex.org/W2008989859",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2754844311",
    "https://openalex.org/W4287646439",
    "https://openalex.org/W2149671658",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2096319502",
    "https://openalex.org/W1974689608",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2977769450",
    "https://openalex.org/W2574640638",
    "https://openalex.org/W4307533339",
    "https://openalex.org/W2964895772",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2882987577",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W1982954510",
    "https://openalex.org/W4255640412",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W2799295289",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3161818437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3023745006",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2619269940",
    "https://openalex.org/W3034912286",
    "https://openalex.org/W2320666599",
    "https://openalex.org/W2039229727",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3136558904",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W4238404719"
  ],
  "abstract": "Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases — (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.",
  "full_text": "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 109–132\nNovember 10–11, 2021. ©2021 Association for Computational Linguistics\n109\nCan Language Models Encode Perceptual Structure Without Grounding?\nA Case Study in Color\nMostafa Abdou∗\nUniversity of Copenhagen\nArtur Kulmizev\nUppsala University\nDaniel Hershcovich\nUniversity of Copenhagen\nStella Frank\nUniversity of Trento\nEllie Pavlick\nBrown University\nAnders Søgaard\nUniversity of Copenhagen\nAbstract\nPretrained language models have been shown\nto encode relational information, such as\nthe relations between entities or concepts in\nknowledge-bases — (Paris, Capital, France).\nHowever, simple relations of this type can of-\nten be recovered heuristically and the extent\nto which models implicitly reﬂect topological\nstructure that is grounded in world, such as per-\nceptual structure, is unknown. To explore this\nquestion, we conduct a thorough case study\non color. Namely, we employ a dataset of\nmonolexemic color terms and color chips rep-\nresented in CIELAB, a color space with a per-\nceptually meaningful distance metric.\nUsing two methods of evaluating the structural\nalignment of colors in this space with text-\nderived color term representations, we ﬁnd sig-\nniﬁcant correspondence. Analyzing the differ-\nences in alignment across the color spectrum,\nwe ﬁnd that warmer colors are, on average, bet-\nter aligned to the perceptual color space than\ncooler ones, suggesting an intriguing connec-\ntion to ﬁndings from recent work on efﬁcient\ncommunication in color naming. Further anal-\nysis suggests that differences in alignment are,\nin part, mediated by collocationality and dif-\nferences in syntactic usage, posing questions\nas to the relationship between color perception\nand usage and context.\n1 Introduction\nWithout grounding or interaction with the world,\nlanguage models (LMs) learn representations that\nencode various aspects of formal linguistic struc-\nture (e.g., morphosyntax (Tenney et al., 2019))\nand semantic information (e.g., lexical similarity\n(Reif et al., 2019a)). Beyond this, it has been sug-\ngested that text-only training data is enough for\nLMs to also acquire factual and relational informa-\ntion about the world (Davison et al., 2019; Petroni\net al., 2019). This includes, for instance, some\n∗For correspondence: {abdou,soegaard}@di.ku.dk\nFigure 1: Right: Color orientation in 3d CIELAB space.\nLeft: linear mapping from BERT (CC, see §2) color\nterm embeddings to the CIELAB space.\nfeatures of concrete and abstract concepts, such as\nobjects’ attributes and affordances (Forbes et al.,\n2019b; Weir et al., 2020). Furthermore, the rep-\nresentational geometry of LMs has been found to\nnaturally reﬂect human lexical similarity and re-\nlatedness judgements, as well as analogy relation-\nships (Chronis and Erk, 2020). However, the extent\nto which these models reﬂect the structures that\nexist in humans’ perceptual world—such as the\ntopology of visual perception (Chen, 1982), the\nstructure of the color spectrum (Ennis and Zaidi,\n2019; Provenzi, 2020), or of odour spaces (Rossiter,\n1996; Chastrette, 1997)—is not well-understood.\nIf LMs are indeed able to capture such\ntopologies—in some domains, at least—it would\nmean that these structures are a) somehow reﬂected\nin language and, thereby, encoded in the textual\ntraining data on which models are trained, and b)\nlearnable using models’ current training objectives\nand architectural inductive biases. To the extent\nthey are not, the question becomes whether the in-\nformation is not there in the data, or whether model\nand training objective limitations are to blame. Cer-\ntainly, this latter point relates to an ongoing de-\nbate regarding what exactly language models can\nbe expected to learn from ungrounded form alone\n(Bender and Koller, 2020; Bisk et al., 2020; Merrill\net al., 2021). While there have been many inter-\n110\nesting theoretical debates around this topic, few\nstudies have tried to address this question empiri-\ncally.\nIn this paper, we conduct a case study on color.\nIndeed, color perception in humans and its rela-\ntion to speakers’ use of color terms has long been\nthe subject of studies in cognitive science (Kay\nand McDaniel, 1978; Berlin and Kay, 1991; Regier\net al., 2007; Kay et al., 2009). To this end, spaces\nhave been deﬁned in which Euclidean distances\nbetween related colors are correlated with reported\nperceptual differences.1 In addition, the semantics\nof color terms have long been understood to hold\nparticular linguistic signiﬁcance, as they are the-\norised to be subject to universal constraints that\narise directly from the neurophysiological mecha-\nnisms and properties underlying visual perception\nand cognition (Kay and McDaniel, 1978; Berlin\nand Kay, 1991; Kay et al., 1991). 2 Due to these\nfactors, color offers a useful test-bed for investigat-\ning whether or not structural information about the\ntopology of the perceptual world might be encoded\nin linguistic representations.\nTo explore this in detail, we employ a dataset\nof English color terms and their corresponding\ncolor chips3, the latter of which are represented\nin CIELAB — a perceptually uniform color space.\nIn addition to the color chip CIELAB coordinates,\nwe extract linguistic representations for the corre-\nsponding color terms. With these two representa-\ntions in mind (see Figure 1 for a demonstrative plot\nfrom our experiments), we employ two methods of\nmeasuring structural correspondence, with which\nwe evaluate the alignment between the two spaces.\nFigure 2 shows an illustration of the experimental\nsetup. We ﬁnd that the structures of various lan-\nguage model representations show alignment with\nthe structure of the CIELAB space, demonstrating\nthat some approximation of perceptual color space\ntopology can indeed be learned from text alone.\n1The differences between color stimuli which are per-\nceived by human observers.\n2These theories have been contested by work arguing for\nlinguistic relativism (cf. the Sapir–Whorf Hypothesis), which\nemphasizes the arbitrariness of language and the relativity\nof semantic structures and minimizes the role of universals.\nSuch critiques have, however, been accommodated for in the\nBerlin & Kay paradigm (Berlin and Kay, 1991), the basic\nassumptions of which, such as the existence of at least some\nperceptually-determined universal constraints on color nam-\ning, remain widely accepted.\n3Each chip is a unique color sample from the Munsell\nchart, which is made up of 330 such samples which cover the\nspace of colors perceived by humans. See §2.\nWe also show that part of this distributional signal\nis learnable by simple models — e.g. models based\non pointwise mutual information (PMI) statistics\n— although large-scale language model pretraining\n(e.g., BERT) encodes the topology markedly better.\nAnalysis shows that larger language models\nalign better than smaller ones and that much of\nthe variance in CIELAB space can be explained by\nlow-dimensional subspaces of LM-induced color\nterm representations. To better understand the re-\nsults, we also analyse the differences in alignment\nacross the color spectrum, observing that warm\ncolors are generally better aligned than cool ones.\nFurther investigation reveals a connection to ﬁnd-\nings reported in work on communication efﬁciency\nin color naming, which posits that warmer colors\nare communicated more efﬁciently. Finally, we\ninvestigate various corpus statistics which could in-\nﬂuence alignment, ﬁnding that a measure of color\nterm collocationality based on PMI statistics corre-\nsponds to lower alignment, while the entropy of a\ncolor term’s dependency relation distribution (i.e.\nterms occurring as adjectival modiﬁers, nominal\nsubjects, etc.) and how often it occurs as an adjec-\ntival modiﬁer correspond to a stronger one.\n2 Methodology\nColor data We employ the Color Lexicon of\nAmerican English, which provides extensive data\non color naming. The lexicon consists of 51\nmonolexemic color name judgements for each\nof the 330 Munsell Chart color chips 4 (Lindsey\nand Brown, 2014). The color terms are solicited\nthrough a free-naming task, resulting in 122 terms.\nPerceptual color space Following previous\nwork (Regier et al., 2007; Zaslavsky et al., 2018;\nChaabouni et al., 2021), we map colors to their cor-\nresponding points in the 3D CIELAB space, where\nthe ﬁrst dimension Lexpresses lightness, the sec-\nond Aexpresses position between red and green,\nand the third Bexpresses the position between blue\nand yellow. Distances between colors in the space\ncorrespond to their perceptual difference.\nLanguage models Our analysis is conducted on\nthree widely used language models (LMs): BERT\n(Devlin et al., 2019) and RoBERTa (Liu et al.,\n2019), both of which employ a masked language\nmodelling objective, and ELECTRA (Clark et al.,\n4http://www1.icsi.berkeley.edu/wcs/\nimages/jrus-20100531/wcs-chart-4x.png\n111\nFigure 2: Our experimental setup. In the center is a Munsell color chart. Each chip in the chart is represented in the\nCIELAB space (right) and has 51 color term annotations. Color term embeddings are extracted through various\nmethods. In the Representation Similarity Analysis experiments, a corresponding color chip centroid is computed\nin the CIELAB space. In the Linear Mapping experiments, a color term embedding centroid is computed per chip.\n2020), which is trained instead with a discrimina-\ntive token replacement detection objective.5\nBaselines In addition to the aforementioned lan-\nguage models, we consider two different baselines:\n• PMI statistics, which are computed 6 for the\ncolor terms in common crawl, using window\nsizes of 1 (pmi-1), 2 (pmi-2), and 3 (pmi-3).\nThe result is a vocabulary length vector quan-\ntifying the likelihood of co-occurrence of the\ncolor term with every other vocabulary item\nin within that window.\n• Word-type FastText embeddings trained on\nCommon Crawl (Bojanowski et al., 2017).\nRepresentation Extraction We follow Bom-\nmasani et al. (2020) and Vuli´c et al. (2020) in deﬁn-\ning conﬁgurations for the extraction of word-type\nrepresentations from LM hidden states. In the ﬁrst\nconﬁguration (NC), a color term is encoded with-\nout context, with the appropriate delimiter tokens\nattached (e.g. [CLS] red [SEP] for BERT).\nIn the second, Ssentential contexts that include the\ncolor term are encoded and the hidden states rep-\nresenting these contexts are mean pooled. These\nScontexts are either randomly sampled from com-\nmon crawl (RC), or deterministically generated to\nallow for control over contextual variation (CC). If\na color term is split by an LM’s tokenizer into more\nthan one token, subword token encodings are aver-\naged over. For each color term and conﬁguration,\n5bert-large-uncased; roberta-large;\nelectra-large-discriminator\n6Using Hyperwords: https://bitbucket.org/\nomerlevy/hyperwords\nan embedding vector of hidden state dimension\ndLM is extracted per layer, per model.\nControlled context To control for the effect of\nvariation in the sentence contexts used to construct\ncolor term representations, we employ a templative\napproach to generate a set of identical contexts\nfor all color terms. When generating controlled\ncontexts, we create three frames in which the terms\ncan appear:\n• COPULA : the <obj> is <col>\n• POSSESSION : i have a <col> <obj>\n• SPATIAL : the <col> <obj> is there\nWe use these frames in order to limit the contex-\ntual variation across colors (<col>) and to isolate\ntheir representations amidst as little semantic inter-\nference as possible, all while retaining a natural-\nistic quality to the input. We also aggregate over\nnumerous object nouns (<obj>), which the color\nterms are used to describe. We select objects from\nthe McRae et al. (2005) data which are labelled\nin the latter as plausibly occurring in many colors\nand which are stratiﬁed across 13 category sets,\ne.g. fan ∈APPLIANCES, skirt ∈CLOTHING, etc.\nCollapsing over categories, we generate sentences\ncombinatorially across frames, objects and color\nterms, resulting in 3 ×122 ×18 = 6588 sentences,\n366 per term.\n3 Evaluation\nWe employ two complimentary evaluation meth-\nods to gauge the correspondence of the color term\ntext-derived representations to the perceptual color\n112\nspace. The ﬁrst, Representation Similarity Anal-\nysis (RSA), is non-parametric and uses pairwise\ncomparisons of stimuli to provide a measure of the\nglobal topological alignment between two spaces.\nThe second employs a learned linear mapping,\nevaluating the extent to which two spaces can be\naligned via transformation (rotation, scaling, etc.).\nRSA (Kriegeskorte et al., 2008) is a method of re-\nlating different representational modalities, which\nwas ﬁrst employed in neuroscientiﬁc studies. RSA\nabstracts away from activity patterns themselves\n(e.g. neuron values in representational vectors) and\ninstead computes representational (dis)-similarity\nmatrices (RSMs), which characterize the infor-\nmation carried by a given representation method\nthrough global (dis)-similarity structure. Kendall’s\nrank correlation coefﬁcient ( τ) is computed be-\ntween RSMs derived from the two spaces, pro-\nviding a summary statistic indicative of the overall\nrepresentational alignment between them. RSA is\nnon-parametric and therefore circumvents many of\nthe various methodological weaknesses associated\nwith the probing paradigm (Belinkov, 2021).\nFor each color term, we compute a centroid\nin the CIELAB space following the approach de-\nscribed in Lindsey and Brown (2014). Each cen-\ntroid is deﬁned as the average CIELAB coordi-\nnate of the samples (i.e. color chips) that were\nnamed with the corresponding term (across the 51\nsubjects). This results in N parallel points in the\ncolor term embedding and perceptual color spaces,\nwhere N is the number of color terms considered.\nFor our analysis, we exclude color terms used less\nfrequently than a cutoff f = 100 in the color\nlexicon, leaving us with the 18 most commonly\nused color terms.7 We then separately construct an\nN ×N RSM for each of the LM spaces and for\nCIELAB . Each cell in the RSM corresponds to the\nsimilarity between the activity patterns associated\nwith pairs of experimental conditions ni,nj ∈N.\nFor the color term embedding space, we em-\nploy Pearson’s correlation coefﬁcient (r) as a sim-\nilarity measure between each pair of embeddings\nni,nj ∈N. For the CIELAB space, we elect to use\nthe following method, per Regier et al.’s (2007) sug-\ngestion: sim(ni,nj) = exp(−c×[dist(ni,nj)]2),\nwhere cis a scaling factor (set to 0.001 in all ex-\n7This includes all color terms which are considered \"basic\"\n(red, blue, etc.), and commonly used \"derived\" terms ( pink,\ngray, turquoise, maroon, etc.), but excludes the rest which\nare only infrequently used as color terms (forest, puke, dew,\nseafoam, etc.). See appendix A for full list of colors included.\nperiments reported here) and dist(ni,nj) is the\nCIELAB distance (∆ E_CMC∗)8 between chips ni\nand nj. This similarity measure is derived from\nthe psychological literature on categorization and\nis meant to model the assumption that beyond a\ncertain distance colors appear entirely different, so\nthat increasing the distance has no further effect on\ndissimilarity. Finally, we report the mean Kendall’s\nτ between the color term embedding and color\nspace RSMs. We also report τ per color term (i.e.\nper row in the RSM), which corresponds to how\nwell-aligned each individual color term is.\nLinear mapping We train regularised linear re-\ngression models to map from color term embedding\nspace X ∈Rn×dLM to CIELAB space Y ∈Rn×3,\nminimising L(W; α) = ∥XW −Y∥2\n2 + α∥W∥1,\nwhere W ∈R3×dLM is a linear map and αis the\nlasso regularization hyper parameter. We vary α\nacross a wide range of settings to examine the ef-\nfect of probe complexity, which we measure using\nthe nuclear norm of the linear projection matrix\nW ∈Rφ×ι; ||W||∗ = ∑min(φ,ι)\ni=1 σi(W), where\nσi(W) is the ith singular value of W (Pimentel\net al., 2020). The ﬁtness of the regressors, eval-\nuated using n-fold cross-validation (n = 6) indi-\ncates the alignability of the two spaces, given a\nlinear transformation. Centroids corresponding to\neach Munsell color chip are computed in the color\nterm embedding space via the weighted mean of\nthe embeddings of the 51 terms used to label it.\nAs in the RSA experiments, terms occurring less\nfrequently than the cutoff (f = 100) are excluded.\nFor evaluation, we compute the average (across\nsplits and datapoints) proportion of explained vari-\nance as well as the ranking of a predicted color\nterm embedding according to the Pearson distance\n(1 −r) to gold.\nControl task As proposed by Hewitt and Liang\n(2019), we construct a random control task for the\nlinear mapping experiments, wherein we randomly\nswap each color chip’s CIELAB code for another.\nThis is meant to break the mapping between the\ncolor chips and their corresponding terms. Control\ntask results are reported as the mean of 10 differ-\nent random re-mappings. We report probe selec-\ntivity, which is deﬁned as the difference between\nproportion of explained variance in the standard\nexperimental condition and in the control task (He-\n8We use the colormath Python package, setting illumi-\nnant to C, and assuming 2 degree standard observer.\n113\nNC RC CC\nModel RSA lin. map RSA lin. map RSA lin. map\nmax mean max mean max mean max mean max mean max mean\nBERT 0.16∗ 0.01±0.09 0.75 0.73 ±0.01 0.26† 0.20±0.03 0.74 0.73 ±0.08 0.24† 0.19±0.03 0.76 0.75 ±0.05\nRoBERTa 0.33§ 0.02±0.11 0.75 0.73 ±0.01 0.20∗ 0.14±0.04 0.74 0.73 ±0.01 0.19∗ 0.14±0.04 0.77 0.76 ±0.09\nELECTRA 0.13 0.01 ±0.08 0.75 0.64 ±0.13 0.25† 0.19±0.05 0.75 0.73 ±0.01 0.23† 0.16±0.04 0.78 0.76 ±0.01\nTable 1: Results for the RSA experiments show max and mean (across layers) Kendall’s τ; correlations that are\nsigniﬁcantly non-zero are marked with *, †and §for p <0.05, <0.01 and <0.001 respectively. Results for the\nlinear mapping experiments show max and mean selectivity.\nwitt and Liang, 2019). We run similar control for\nthe RSA experiments, where the CIELAB space\ncentroids are randomly shufﬂed.\n4 Results\nTable 1 shows the max, mean, and standard devi-\nation (across layers) of alignment scores for each\nof the LMs, per alignment method and setting. For\nRSA, we observe signiﬁcant correlations across\nall conﬁgurations: most LM layers show a topo-\nlogical alignment with color space. Notably, this\nis also true for the static embeddings and for one\nof the PMI baselines (Table 2). Although some\nvariance is observed,9 the presence of signiﬁcant\ncorrelations is telling, given the small sample size\n(18). Furthermore, randomly permuting the color\nspace centroids leads to RSA correlations that are\nnon-signiﬁcant for all setups ( p >0.05), which\nlends further credence to models’ alignment with\nCIELAB structure.\nFigure 3 shows the breakdown of correlations\nper color term for the three LMs under CC, as\nwell as for fastText. We ﬁnd that this ranking\nof color terms is largely stable across models and\nlayer. Full RSMs for all models and CIELAB are\nin appendix C. The RSMs show evidence of the\nhigher correlations for colors like violet, orange,\nand purple, being driven by general clusterings of\nsimilarity/dissimilarity. For instance, for both the\nCIELAB and CC BERT RSMs, violet’s topnear-\nest neighbors include purple, lavender, pink, and\norange, and its furthest neighbors include aqua,\nolive, black, and gray. Correlations do not, how-\never, appear to be driven by consistently aligned\npartial orderings within the clusters. In addition,\nwe compute RSA correlations between the different\n9In particular, results for NC show large variances across\nlayers. The mean correlation across layers in this setup is near\nzero, even though max correlations for BERT and RoBERTa\nare signiﬁcant; this is unsurprising, however, as the LM has\nlikely never encountered single color term tokens in isolation\n(cf. Bommasani et al. (2020))\nModel RSA lin. map\npmi-1 0.14 0.72\npmi-2 0.11 0.70\npmi-3 0.17∗ 0.71\nfastText 0.23∗ 0.72\nTable 2: Baseline results. RSA results show Kendall’s\nτ; results with * are signiﬁcantly non-zero ( p <0.05).\nLinear mapping results show selectivity.\nmodels. Results show that NC embeddings have\nlow alignment to all others (details in appendix B).\nFor the linear mapping experiments, we observe\nthe highest selectivity scores for CC (Table 1, right)\ncompared to NC and RC (Table 1, left, middle) and\nbaselines (Table 2). This validates our intuition that\ncontrolling for variation in sentence context would\nreveal increased alignment to color space.\nFurthermore, we observe that, over the full range\nof probe complexities for the experimental condi-\ntion and the control task (described as in §3), all\nmodels demonstrate high selectivity (see G for full\nresults). It is, therefore, safe to attribute the ﬁtness\nof the probes to information encoded in the color\nterm representations, rather than to memorization.\nIn terms of individual colors, Figure 4a depicts the\nranking of predicted CIELAB codes per Munsell\ncolor chip for BERT (CC). We ﬁnd that these re-\nsults are largely stable across models and layers\n(see appendix F for full set of results and for ref-\nerence chart). Also, we observe that clusterings of\nchips with certain modal color terms (green, blue)\nshow worse rankings than the rest.\n5 Analysis and Discussion\nHaving demonstrated the existence of models’\nalignment to CIELAB across various conﬁgura-\ntions, we now present an analysis and discussion\nof these results.\nDimensionality of color subspace Previous\nwork has shown that linguistic information such as\n114\nFigure 3: RSA results (Kendal’s τ) broken down by color term for each of the LMs under the CC conﬁguration\nand for the fastText baseline.\npart-of-speech category, dependency relation type,\nand word sense, is expressed in low-dimensional\nsubspaces of language model representations (Reif\net al., 2019b; Durrani et al., 2020; Hernandez and\nAndreas, 2021). We investigate the dimensionality\nof the subspace required to predict the CIELAB\nchip codes from the term embeddings, following\nthe methodology of Durrani et al. (2020). Averag-\ning over the three predicted CIELAB dimensions,\nwe rank the linear mapping coefﬁcients (from the\nexperiments described in §2), sorting the weights\nby their absolute values in descending order. Re-\nsults (appendix H) show that across models and\nlayers, ∼0.4 of the variance in the CIELAB chip\ncodes can be explained by assigning 95% of the\nweights to ∼10 dimensions. 30–40 dimensions are\nsufﬁcient to explain ∼0.7 of the variance, nearly\nthe proportion of variance explained by the full\nrepresentations (Table 1).\nModel RSA max RSA mean lin. map.. max lin. map. mean\nBERT-mini 0.077 0.043 ±0.340 0.729 0.582 ±0.291BERT-small 0.106 0.070 ±0.191 0.734 0.598 ±0.294BERT-medium 0.097 0.057±0.035 0.739 0.654 ±0.221BERT-base 0.162∗ 0.092±0.058 0.740 0.677 ±0.182\nTable 3: Results for the four smaller BERT models.\nRSA results (left) show max and mean (across lay-\ners) Kendall’s correlation coefﬁcient (τ). Correlations\nthat are signiﬁcantly non-zero are indicated with: * :\np< 0.05. Results for the Linear Mapping experiments\n(right) show max and mean selectivity. Standard devia-\ntion across layers is included with the mean results.\nEffect of model size We also evaluate the ef-\nfect of model size on alignment by testing four\nsmaller BERT (CC) models10 using the same setup\ndescribed above. The results (table 3) show that\nalignment as measured by both RSA and linear\nmapping progressively increases with model size,\n10for details see appendix I\nmeaning that that with growing complexity, model\nrepresentational geometry of color terms moves\ntowards isomorphism to CIELAB.\nColor temperature In Figures 3 & 4a we ob-\nserve that on average, warmer colors ( yellow, or-\nange, red, etc.) show a closer alignment than cooler\nones (blue, green, etc.). In recent work, Gibson\net al. (2017) reported that the former are on aver-\nage communicated more efﬁciently (see next para-\ngraph) than the latter, across languages. This is\nattributed to warmer colors being more prevalent\nas colors of behaviorally relevant items in the envi-\nronment — salient objects — compared to cooler\nones, which occur more often as background col-\nors. To verify this observation, we partition the\nspace of chips into two (see appendix D for de-\ntails) and compute the average explained variance\nacross warm and cool colors. The results (see ap-\npendix D for plots) show that, term embeddings\nof warm colors are better aligned to CIELAB than\nthose of cool ones, across models and conﬁgura-\ntions. This is consistent with the bias described in\nGibson et al. (2017), which we conjecture might\nbe ﬁltering through into the distributional statistics\nof (color terms in) textual corpora, inﬂuencing the\nrepresentations learned by various methods which\nleverage these statistics.\nConnection to listener surprisal Gibson et al.\n(2017)’s ﬁndings are based on the application of\nan information theoretic analysis to color nam-\ning, framing it as a communication game where\na speaker has a particular color chip cin mind and\nuses a word w to indicate it then a listener has\nto correctly guess c, given w. Communication ef-\nﬁciency is measured through surprisal, S, which\nin this setting corresponds to the average number\nof guesses an optimal listener takes to arrive at\nthe correct color chip. We calculate S(c) for each\n115\n(a) Each circle on the chart represents the ranking of the pre-\ndicted color chip when ranked according to Pearson distance\nfrom gold (larger circle ∼= higher/better ranking).\n(b) Each circle on the chart represents a color chip’s suprisal\nscore (larger circle ∼= higher score).\nFigure 4: (a) shows linear mapping results for BERT,\nunder the CC conﬁguration, broken down by Munsell\ncolor chip; (b) shows suprisal per chip. Circle colors\nreﬂect the modal color term assigned to the chips.\nchip in the color lexicon. Surprisal is deﬁned as\nS(c) = ∑\nwP(w|c) ·log\n(\n1\nP(c|w)\n)\n, where P(w|c)\nis the probability that a color c gets labeled as w\nand P(c|w) is computed using Bayes Theorem.\nHere, P(w) represents how often a particular word\ngets used across the color space (and participants),\nand P(c) is a uniform prior. Figure 4b shows sur-\nprisal per chip. High surprisal chips correspond to\na lower color naming consensus among speakers,\nmeaning that a more variable range of terms is used\nfor these (color) contexts. We hypothesize that this\ncould be reﬂected in the representations of color\nterms corresponding to high surprisal chips. To\ntest this, we compute Spearman’s correlation (ρ)\nbetween a chip’s regression score (predicted color\nchip code ranking) and its surprisal. We ﬁnd signif-\nicant Spearman’s rank correlation between lower\nranking and higher surprisal for all LMs under all\nconﬁgurations (0.12 ≤ρ≤0.17, p< 0.05).\nWhat factors predict color space alignment?\nGiven that LMs are trained exclusively on text\ncorpora, we hypothesize that alignment between\ntheir embeddings and CIELAB is inﬂuenced by\ncorpus usage statistics. To determine which fac-\ntors could predict alignment score, we extract color\nterm log frequency, part-of-speech tag (POS), de-\npendency relation (DREL), and dependency tree\nhead (HEAD) statistics for all color terms from a\ndependency-parsed (Straka et al., 2016) common\ncrawl corpus. In addition to this, we compute, per\ncolor term, the entropy of its normalised PMI dis-\ntribution (pmi-col, see §2) as a measure of collo-\ncation.11 We then ﬁt a Linear Mixed Effects Model\n(Gałecki and Burzykowski, 2013) to the features\nlisted above, with RSA score (Table 1) as the re-\nsponse variable, and model type as a random effect.\nWe follow a multi-level step-wise model build-\ning sequence, where a baseline model is ﬁrst\nﬁt with color term log frequency as a single\nﬁxed effect. A model which includes pmi-col\nas an additional ﬁxed effect is then ﬁt, and\nthese two terms are included as control predic-\ntors in all later models. Following this, we\ncompute POS, DREL, and HEAD lemma dis-\ntribution entropies per color term ( pos-ent,\ndeprel-ent, head-ent). Higher entropies\nindicate that the term is employed in more diverse\ncontexts with respect to those categories. Follow-\ning entropy computation, we separately ﬁt models\nincluding each three entropy statistic features. Fi-\nnally, we calculate the proportion of: POS tags that\nare adjectives, adj-prop; DRELs that are adjec-\ntival modiﬁers, amod-prop; and those that are\ncopulas, cop-prop. The ﬁrst two evaluate the\neffect of a color term occurring more or less often\nas an adjectival modiﬁer, while the latter tests the\nhypothesis that assertions such as The banana is\nyellow could provide indirect grounding (Merrill\net al., 2021), thereby leading to higher alignment.\nIncluding the entropy term which led to the best\nﬁt (deprel-ent) in the previous level, models\nare ﬁt including terms for each of the proportion\nstatistics. Model comparison is carried out by com-\nputing the log likelihood ratio between models that\ndiffer in a single term. See appendix J for model\ndetails.\n11Low entropy reﬂects frequent co-occurrence with a small\nsubset of the vocabulary and high entropy the converse.\n116\nResults show that:\n• pmi-col signiﬁcantly improves ﬁt above\nlog frequency and has a negative coefﬁcient,\nmeaning that terms that occur in more ﬁxed\ncollocations are less aligned to the percep-\ntual space. Intuitively, this makes sense as\nthe color terms in many collocations such as\ne.g. Red Armyor Black Deathare employed in\ncontexts which are largely metaphorical rather\nthan attributive or descriptive.\n• deprel-ent and head-ent (but not\npos-ent) lead to a signiﬁcantly improved\nﬁt compared to the control predictors; we ob-\nserve positive coefﬁcients for both, indicating\nRSA score is higher for terms that occur in\nmore varied syntactic dependency relations\nand modify a more diverse set of syntactic\nheads. This suggests that occurring in a more\ndiverse set of contexts might be beneﬁcial for\nrobust representation learning, in correspon-\ndence with the idea of sample diversity in the\nactive learning literature (Brinker, 2003; Yang\net al., 2015). pos-ent’s lack of signiﬁcance,\non the other hand, indicates that the degree of\nspeciﬁcation offered by the POS tagset might\nbe too coarse to meaningfully differentiate be-\ntween color terms, e.g. nouns can occur in\na variety of DRELs such as subjects, objects,\noblique modiﬁers (per the Universal Depende-\ncies (Nivre et al., 2020)).\n• out of the proportion statistics, only the\namod-prop term improves ﬁt; it has a pos-\nitive coefﬁcient, thus color terms occurring\nmore frequently as adjectival modiﬁers show\nhigher scores. adj-prop is not signiﬁ-\ncant, providing further evidence for the POS\ntagset’s level of granularity being too coarse.\nFinally, as cop-prop is not signiﬁcant, it\nappears that occurring more frequently in\nassertion-like copula constructions does not\nconfer an advantage in terms of alignment to\nperceptual structure.\nVision-and-Language models In a preliminary\nset of experiments, we evaluated multi-modal\nVision-and-Language models (VisualBERT (Li\net al., 2019) and VideoBERT (Sun et al., 2019)),\nﬁnding no major differences in results from the\ntext-only models presented in this study.\n6 Related Work\nDistributional word representations have long been\ntheorized to capture various types of information\nabout the world (Schütze, 1992). Early work in\nthis regard employed semantic similarity and re-\nlatedness datasets to measure alignment to human\njudgements (Agirre et al., 2009; Bruni et al., 2012;\nHill et al., 2015). Rubinstein et al. (2015), however,\nquestion whether the distributional hypothesis is\nequally applicable to all types of semantic infor-\nmation, ﬁnding that taxonomic properties (such as\nanimacy) are better modelled than attributive ones\n(color, size, etc.). To a similar end, Lucy and Gau-\nthier (2017) analyze how well distributional rep-\nresentations encode various aspects of grounded\nmeaning. They investigate whether language mod-\nels would “be worse off for not having physically\nbumped into walls before they hold discussions on\nwall-collisions?”, ﬁnding that perceptual features\nare poorly modelled compared to encyclopedic and\ntaxonomic ones.\nMore recently, several studies have asked related\nquestions in the context of language models. For\nexample, Davison et al. (2019) and Petroni et al.\n(2019) mine LMs for factual and commonsense\nknowledge by converting knowledge base triplets\ninto cloze statements that are used to query the\nmodels. In a similar vein, Forbes et al. (2019a)\ninvestigate LM representations’ encoding of ob-\nject properties (e.g., oranges are round), and af-\nfordances (e.g. oranges can be eaten), as well as\nthe interplay between the two. Weir et al. (2020)\ndemonstrate that LMs can capture stereotypic tacit\nassumptions about generic concepts, showing that\nthey are adept at retrieving concepts given their\nassociated properties (e.g., bear given A ___ has\nfur, is big, and has claws.). Similar to other work,\nthey ﬁnd that LMs better model encyclopedic and\nfunctional properties than they do perceptual ones.\nIn an investigation of whether or not LMs are\nable to overcome reporting bias, Shwartz and Choi\n(2020) extract all sentences in Wikipedia where\none of 11 color terms modiﬁes a noun and test how\nwell predicted the color term is when it is masked.\nThey ﬁnd that LMs are able to model this relation-\nship between concepts and associated colors to a\ncertain extent, but are prone to over-generalization.\nFinally, Ilharco et al. (2020) train a probe to map\nLM representations of textual captions to paired\nvisual representations of image patches, in order to\nevaluate how useful the former are for discerning\n117\nbetween different visual representations. They ﬁnd\nthat many recent LMs yield representations that are\neffective at retrieving semantically-aligned image\npatches, but still far under-perform humans.\n7 Outlook\nIt is commonly held that the learning of phenom-\nena which rely on sensory perception is only pos-\nsible through direct experience. Indeed, the view\nthat people born blind could not be expected to\nacquire coherent knowledge about colors has been\nprevalent since at least the empiricist philosophers\n(Locke, 1847; Hume, 1938) and still holds cur-\nrency (Jackson, 1982). Nevertheless, recent re-\nsearch highlighting the contribution of language\nand of semantic associations between concepts to-\nwards learning has demonstrated that the congeni-\ntally blind do in fact show a striking understanding\nof both color similarity (Saysani et al., 2018) and\nobject colors (Kim et al., 2020).\nThis paper investigated whether representations\nof color terms that are derived from text only ex-\npress a degree of isomorphism to the structure of\nhumans’ perceptual color space.12 Results from\nour experiments evidenced that such a topological\ncorrespondence exists. Notably, color term repre-\nsentations based on simple co-occurance statistics\nalready demonstrated correspondence; those ex-\ntracted from language models aligned more closely.\nWe observed that warm colors, on average, show\nmore alignment than cooler ones, linking to recent\nﬁndings on communication efﬁciency in color nam-\ning (Gibson et al., 2017).\nFurther analysis based on surprisal — an infor-\nmation theoretic measure, used to evaluate how efﬁ-\nciently a color is communicated between a speaker\nand a listener — revealed a correlation between\nlower topological alignment and higher color chip\nsurprisal, suggesting that the kind of contexts a\ncolor occurs in play a role in determining align-\nment. Exploring this, we tested a set of color term\ncorpus-derived statistics for how well they predict\nalignment, ﬁnding that a measure of a color term’s\ncollocationality corresponds to lower alignment,\nwhile the entropy of its dependency relation dis-\ntribution and it occurring more frequently as and\nadjectival modiﬁer correspond to closer alignment.\n12Clearly, complete isomorphism is rather unlikely: lan-\nguage in general, and color terms by extension, are far from\nbeing simply denotational, and language interacts with and is\ninﬂuenced by a myriad of factors besides perception.\nOur results and analyses present empirical evi-\ndence of topological alignment between text-based\ncolor term representations and perceptual color\nspaces. With respect to the debate started by Ben-\nder and Koller (2020), we hope that this work offers\na modest step towards furthering our understand-\ning of the kinds of “meaning” we expect language\nmodels to acquire, with and without grounded or\nembodied learning approaches, and that it will pro-\nvide motivation for further work in this direction.\nAcknowledgements\nWe would like to thank Vinit Ravishankar and\nMitja Nikolaus for their feedback and comments.\nMostafa Abdou and Anders Søgaard are supported\nby a Google Focused Research Award and a Face-\nbook Research Award.\nReferences\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana\nKravalova, Marius Pasca, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distribu-\ntional and wordnet-based approaches.\nYonatan Belinkov. 2021. Probing classiﬁers: Promises,\nshortcomings, and alternatives. arXiv preprint\narXiv:2102.12452.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nBrent Berlin and Paul Kay. 1991. Basic color terms:\nTheir universality and evolution. Univ of California\nPress.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, et al. 2020. Experience grounds lan-\nguage. arXiv preprint arXiv:2004.10151.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting Pretrained Contextualized Repre-\nsentations via Reductions to Static Embeddings. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4758–\n4781, Online. Association for Computational Lin-\nguistics.\n118\nKlaus Brinker. 2003. Incorporating diversity in active\nlearning with support vector machines. In Proceed-\nings of the 20th international conference on machine\nlearning (ICML-03), pages 59–66.\nElia Bruni, Gemma Boleda, Marco Baroni, and Nam-\nKhanh Tran. 2012. Distributional semantics in tech-\nnicolor. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 136–145.\nRahma Chaabouni, Eugene Kharitonov, Emmanuel\nDupoux, and Marco Baroni. 2021. Communi-\ncating artiﬁcial neural networks develop efﬁcient\ncolor-naming systems. Proceedings of the National\nAcademy of Sciences, 118(12).\nM Chastrette. 1997. Trends in structure-odor relation-\nship. SAR and QSAR in Environmental Research,\n6(3-4):215–254.\nLin Chen. 1982. Topological structure in visual percep-\ntion. Science, 218(4573):699–700.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? when it’s like a rabbi! multi-\nprototype BERT embeddings for estimating seman-\ntic relationships. In Proceedings of the 24th Con-\nference on Computational Natural Language Learn-\ning, pages 227–244, Online. Association for Compu-\ntational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4865–4880, Online. Association for Computational\nLinguistics.\nRobert J Ennis and Qasim Zaidi. 2019. Geometrical\nstructure of perceptual color space: mental represen-\ntations and adaptation invariance. Journal of vision,\n19(12):1–1.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019a.\nDo neural language representations learn physical\ncommonsense? arXiv preprint arXiv:1908.02899.\nMaxwell Forbes, Christine Kaeser-Chen, Piyush\nSharma, and Serge Belongie. 2019b. Neural natural-\nist: Generating ﬁne-grained image comparisons. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 708–\n717, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAndrzej Gałecki and Tomasz Burzykowski. 2013. Lin-\near mixed-effects model. In Linear Mixed-Effects\nModels Using R, pages 245–273. Springer.\nEdward Gibson, Richard Futrell, Julian Jara-Ettinger,\nKyle Mahowald, Leon Bergen, Sivalogeswaran Rat-\nnasingam, Mitchell Gibson, Steven T Piantadosi,\nand Bevil R Conway. 2017. Color naming across\nlanguages reﬂects color use. Proceedings of the Na-\ntional Academy of Sciences, 114(40):10785–10790.\nEvan Hernandez and Jacob Andreas. 2021. The low-\ndimensional linear geometry of contextualized word\nrepresentations. arXiv preprint arXiv:2105.07109.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. arXiv preprint\narXiv:1909.03368.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nDavid Hume. 1938. An Abstract of a Treatise of Hu-\nman Nature, 1740. CUP Archive.\nGabriel Ilharco, Rowan Zellers, Ali Farhadi, and Han-\nnaneh Hajishirzi. 2020. Probing text models for\ncommon ground with visual representations. arXiv\npreprint arXiv:2005.00619.\nFrank Jackson. 1982. Epiphenomenal qualia. The\nPhilosophical Quarterly (1950-), 32(127):127–136.\nPaul Kay, Brent Berlin, Luisa Mafﬁ, William R Mer-\nriﬁeld, and Richard Cook. 2009. The world color\nsurvey. CSLI Publications Stanford, CA.\nPaul Kay, Brent Berlin, and William Merriﬁeld. 1991.\nBiocultural implications of systems of color naming.\nJournal of Linguistic Anthropology, 1(1):12–25.\nPaul Kay and Chad K McDaniel. 1978. The linguis-\ntic signiﬁcance of the meanings of basic color terms.\nLanguage, pages 610–646.\n119\nJudy Sein Kim, Brianna Aheimer, Verónica Montané\nManrara, and Marina Bedny. 2020. Shared un-\nderstanding of color among congenitally blind and\nsighted adults.\nNikolaus Kriegeskorte, Marieke Mur, and Peter A Ban-\ndettini. 2008. Representational similarity analysis-\nconnecting the branches of systems neuroscience.\nFrontiers in systems neuroscience, 2:4.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nDelwin T Lindsey and Angela M Brown. 2014. The\ncolor lexicon of american english. Journal of vision,\n14(2):17–17.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJohn Locke. 1847. An essay concerning human under-\nstanding. Kay & Troutman.\nLi Lucy and Jon Gauthier. 2017. Are distributional\nrepresentations ready for the real world? evaluat-\ning word vectors for grounded perceptual meaning.\narXiv preprint arXiv:1705.11168.\nKen McRae, George S Cree, Mark S Seidenberg, and\nChris McNorgan. 2005. Semantic feature produc-\ntion norms for a large set of living and nonliving\nthings. Behavior research methods, 37(4):547–559.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form: What will fu-\nture language models understand? arXiv preprint\narXiv:2104.10809.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection.\nIn Proceedings of the 12th Language Resources\nand Evaluation Conference, pages 4034–4043, Mar-\nseille, France. European Language Resources Asso-\nciation.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTiago Pimentel, Naomi Saphra, Adina Williams, and\nRyan Cotterell. 2020. Pareto probing: Trad-\ning off accuracy for complexity. arXiv preprint\narXiv:2010.02180.\nEdoardo Provenzi. 2020. Geometry of color percep-\ntion. part 1: structures and metrics of a homoge-\nneous color space. The Journal of Mathematical\nNeuroscience, 10(1):1–19.\nTerry Regier, Paul Kay, and Naveen Khetarpal. 2007.\nColor naming reﬂects optimal partitions of color\nspace. Proceedings of the National Academy of Sci-\nences, 104(4):1436–1441.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019a. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019b. Visualizing and measuring the geometry of\nbert. Advances in Neural Information Processing\nSystems, 32:8594–8603.\nKaren J Rossiter. 1996. Structure- odor relationships.\nChemical reviews, 96(8):3201–3240.\nDana Rubinstein, Efﬁ Levi, Roy Schwartz, and Ari\nRappoport. 2015. How well do distributional mod-\nels capture different types of semantic knowledge?\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers), pages\n726–730.\nArmin Saysani, Michael C Corballis, and Paul M\nCorballis. 2018. Colour envisioned: Concepts of\ncolour in the blind and sighted. Visual Cognition,\n26(5):382–392.\nHinrich Schütze. 1992. Dimensions of meaning. In SC,\npages 787–796.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6863–6870, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nMilan Straka, Jan Hajic, and Jana Straková. 2016.\nUdpipe: trainable pipeline for processing conll-u\nﬁles performing tokenization, morphological anal-\nysis, pos tagging and parsing. In Proceedings of\nthe Tenth International Conference on Language Re-\nsources and Evaluation (LREC’16), pages 4290–\n4297.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7464–7473.\n120\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Prob-\ning pretrained language models for lexical semantics.\narXiv preprint arXiv:2010.05731.\nNathaniel Weir, Adam Poliak, and Benjamin Van\nDurme. 2020. Probing neural language models for\nhuman tacit assumptions.\nYi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang,\nand Alexander G Hauptmann. 2015. Multi-class ac-\ntive learning by uncertainty sampling with diversity\nmaximization. International Journal of Computer\nVision, 113(2):113–127.\nNoga Zaslavsky, Charles Kemp, Terry Regier, and Naf-\ntali Tishby. 2018. Efﬁcient compression in color\nnaming and its evolution. Proceedings of the Na-\ntional Academy of Sciences, 115(31):7937–7942.\n121\nA List of included color terms\nRed, green, maroon, brown, black, blue, purple,\norange, pink, yellow, peach, white, gray, olive,\nturquoise, violet, lavender, and aqua.\nB RSA between models\nFigure 5 shows a the result of representation simi-\nlarity analysis between the representations derived\nfrom all models (and conﬁgurations) as well as\nCIELAB, showing Kendall’s correlation coefﬁcient\nbetween ﬂattened RSMs.\nC Representation Similarity Matrices\nFigures 6 to 9 show the representation similarity\nmatrices employed for the RSA analyses, for the\nlayer with the highest RSA score from each of the\ncontrolled-context (CC) models.\nD Warm vs. Cool colors\nFigures 10 and 11 show Linear Mapping and RSA\nresults broken down by color temperature. The\ncolor space is split according to temperature mea-\nsured according to the Hue dimension in the Hue-\nValue-Saturation space13.\nE Corpus statistics\nFigures 12 and 13 show log frequency and entropy\nof distributions over part-of-speech categories, de-\npendency relations, and lemmas of dependency tree\nheads of color terms in common crawl.\nF Linear mapping results by munsell\ncolor chip\nFigure 14 shows linear mapping results broken\ndown by Munsell chip for all models and conﬁgu-\nrations.\nG Linear mapping control task and\nprobe complexity\nFigure 15 shows the full results over a range of\nprobe complexities for the standard experimental\ncondition as well the random control task.\nH Dimensionality of color subspace\nFigure 16 shows the proportion of explained vari-\nance with respect to the number of dimensions\nwhich are assigned 95% of the linear regression\ncoefﬁcient weights.\n13https://psychology.wikia.org/wiki/\nHSV_color_space\nI Effect of model size\nOur model size experiments are run using four\nBERT models of different sizes: BERT-mini (4\nlayers, hidden size: 256), BERT-small (4 layers,\nhidden size: 512), BERT-medium (8 layers, hidden\nsize: 512), and BERT-base (12 layers, hidden size:\n768). Further model speciﬁcation and training de-\ntails for the ﬁrst three can be found in Turc et al.\n(2019) and for last in Devlin et al. (2019).\nJ Linear Mixed Effects Model\nTo ﬁt Linear Mixed Effects Models, we use\nthe LME4 package. With model type (BERT-\nCC, RoBERTa-NC, etc.) as a random ef-\nfect, we follow a step-wise model construc-\ntion sequence which proceeds along four lev-\nels of nesting: (i) in the ﬁrst level color log-\nfrequency is the only ﬁxed effect, (ii) in the\nsecond pmi-colloc is added to that, (iii) in\nthe third, each of pos-ent, deprel-ent,\nhead-ent is added separately to the a model\nwith log frequency and pmi-colloc, (iv) the\nterm that leads to the best ﬁt from the previ-\nous level deprel-ent is included, then each of\nthe proportion terms adj-prop, amod-prop,\ncop-prop is added. The reported regression co-\nefﬁcients are extracted from the minimal model\ncontaining each term.\n122\nFigure 5: Result of representation similarity analysis between all models (and conﬁgurations), showing Kendall’s\ncorrelation coefﬁcient between ﬂattened RSMs. Results are shown for layers which are maximally correlated\nwith CIELAB, per model. -rc indicates random-context, -cc indicates controlled-context, and -nc indicates\nnon-context.\n123\nFigure 6: CIELAB RSM\n124\nFigure 7: BERT(CC) RSM\n125\nFigure 8: RoBERTa(CC) RSM\n126\nFigure 9: ELECTRA(CC) RSM\n127\nFigure 10: Linear mapping results (proportion of explained variance) broken down by color chip temperature for\neach of the baselines and the LMs.\n128\nFigure 11: RSA results (Kendall’sτ ) broken down by color temperature for each for each of the baselines and the\nLMs.\nFigure 12: Log frequency of color terms in common crawl.\n129\nFigure 13: Entropy of distributions over part-of-speech categories, dependency relations, and lemmas of depen-\ndency tree heads of color terms in common crawl.\n130\nFigure 14: Linear mapping results for each of the baselines and language models, under all extraction conﬁgura-\ntions, broken down by Munsell color chip. Each circle on the chart represents the ranking of the predicted color\nchip when ranked according to Pearson distance ( 1−Pearson’sr) from gold – the larger the circle, the higher\n(better) the ranking. Circle colors reﬂect the modal color term assigned to the chips in the lexicon. Reference plot\nshowing modal color of all chips also included.\n131\nFigure 15: Explained variance for the linear probes trained on the normal experimental condition (blue) and the\ncontrol task (red) where color terms are randomly permuted. The means are indicated by the lines and standard\ndeviation across layers is indicated by the bands.\n132\nFigure 16: The y-axis shows explained variance for the linear probes. The means are indicated by the lines and\nstandard deviation across layers is indicated by the bands. The x-axis shows the number of regression matrix\ncoefﬁcients assigned 95% of the weight.",
  "topic": "ENCODE",
  "concepts": [
    {
      "name": "ENCODE",
      "score": 0.7510999441146851
    },
    {
      "name": "Perception",
      "score": 0.6832762956619263
    },
    {
      "name": "Computer science",
      "score": 0.6327886581420898
    },
    {
      "name": "Metric (unit)",
      "score": 0.5652488470077515
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5566791892051697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5443030595779419
    },
    {
      "name": "Color space",
      "score": 0.5418179631233215
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5218438506126404
    },
    {
      "name": "Natural language processing",
      "score": 0.4879750609397888
    },
    {
      "name": "Color difference",
      "score": 0.48340097069740295
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3317989706993103
    },
    {
      "name": "Psychology",
      "score": 0.13871410489082336
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0666050910949707
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I123387679",
      "name": "Uppsala University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    }
  ],
  "cited_by": 53
}