{
  "title": "TACFN: Transformer-Based Adaptive Cross-Modal Fusion Network for Multimodal Emotion Recognition",
  "url": "https://openalex.org/W4387977958",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5085891781",
      "name": "Feng Liu",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5016048145",
      "name": "Ziwang Fu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100398969",
      "name": "Yunlong Wang",
      "affiliations": [
        "Institute of Acoustics",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5057906567",
      "name": "Qijian Zheng",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3194672963",
    "https://openalex.org/W3021173516",
    "https://openalex.org/W2777446440",
    "https://openalex.org/W2883430806",
    "https://openalex.org/W2103104224",
    "https://openalex.org/W3035333188",
    "https://openalex.org/W2122563357",
    "https://openalex.org/W2964260444",
    "https://openalex.org/W3039263585",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W3093051361",
    "https://openalex.org/W4205727320",
    "https://openalex.org/W3169801598",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W1533025524",
    "https://openalex.org/W2803193013",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2612041314",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3035570025",
    "https://openalex.org/W3015267357",
    "https://openalex.org/W1534131679",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2395639500",
    "https://openalex.org/W2095176743",
    "https://openalex.org/W2963383024",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2904518532",
    "https://openalex.org/W2963192057",
    "https://openalex.org/W3045615140",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W3209710747",
    "https://openalex.org/W3098042509"
  ],
  "abstract": "The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art performance. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.",
  "full_text": " \nTACFN: Transformer-Based Adaptive Cross-Modal Fusion\nNetwork for Multimodal Emotion Recognition\nFeng Liu1 ✉, Ziwang Fu2, Yunlong Wang3, and Qijian Zheng1\n \nABSTRACT\nThe fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods\nhave demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and\ndoes not capture complementary features well. We find that it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To\nthis  end,  we  design  an  innovative  Transformer-based  Adaptive  Cross-modal  Fusion  Network  (TACFN).  Specifically,  for  the\nredundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another modality. To better capture the complementary information\nbetween the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement\nof the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a\nsignificant performance improvement compared to other methods and reaches the state-of-the-art performance. All code and\nmodels could be accessed from https://github.com/shuzihuaiyu/TACFN.\nKEYWORDS\nmultimodal emotion recognition; multimodal fusion; adaptive cross-modal blocks; Transformer; computational perception\n \nIn recent years, the proliferation of multimedia data, including\nshort  videos  and  movies,  has  propelled  the  rise  of  multimodal\nemotion  recognition  as  a  burgeoning  field  of  study,  eliciting\nsignificant  interest  among  researchers  and  practitioners  alike[1, 2].\nThe  objective  of  this  task  is  to  classify  human  emotions  from  a\nvideo  clip  using  three  modalities:  visual,  audio,  and  text.\nMultimodality  offers  a  wealth  of  information  that  surpasses\nunimodality  and  better  aligns  with  human  performance\nbehavior[3, 4]. Humans tend to perceive the world by processing and\nfusing  high-dimensional  inputs  from  multiple  modalities\nsimultaneously[5]. However, how to efficiently extract and integrate\ninformation  from  all  input  modalities  so  that  the  machine  can\ncorrectly recognize emotions remains a challenge for this task. On\nthe  one  hand,  the  modality  input  representations  are  different,\nand  efficient  intra-modal  encoding  integrates  high-level  semantic\nfeatures  of  the  current  modality.  On  the  other  hand,  the  learning\nbetween modalities is dynamically changing[6], where some modal\nstreams  contain  more  task  information  than  others.  The  goal  of\nthis  paper  is  to  propose  an  innovative  multimodal  fusion\napproach.\nIn  the  existing  research,  multimodal  fusion  approaches  can  be\nclassified  as  early  fusion[7, 8],  late  fusion[9, 10] and  model  fusion[11–13].\nEarly  fusion  strategies  involve  fusing  the  shallow  inter-modal\nfeatures  and  focusing  on  mixed-modal  feature  processing  while\nlate  fusion  strategies  involve  finding  the  confidence  level  of  each\nmodality  and  then  coordinating  them  to  make  joint  decisions.\nWith  the  development  of  deep  learning,  model  fusion  has\nsignificantly  improved  the  performance  of  multimodal  tasks\ncompared  to  the  previous  two  approaches.  Model  fusion  enables\nflexible choice of fusion locations, taking into account the intrinsic\ncorrelation  between  sequence  elements  from  different\nmodalities[14].  Recently,  due  to  the  popularity  of  Transformer[15] in\nmultimodal  machine  learning  tasks,  model  fusion  is  often  done\nusing  a  Transformer-based  approach  for  different  modal\ninteractions.  Typically,  both  SPT[16],  and  PMR[17] use  cross-modal\nattention  to  perform  fusion  of  multimodal  sequences.  Their\napproaches  achieve  reinforcement  of  the  target  modality  by\nlearning  directed  pairwise  attention  between  cross-modal\nelements.  Moreover,  according  to  recent  findings[18],  the  cross-\nmodal  attention  performance  in  fusion  of  high-level  semantics  is\nstill able to excel in different tasks.\nCross-modal  attention,  however,  is  challenged  by  the  presence\nof  feature  redundancy.  Our  analysis  reveals  that  the  audio  and\nvisual  inputs  are  densely  packed  with  fine-grained  information,\nmuch  of  which  is  redundant.  During  the  fusion  of  audio  and\nvisual modalities, the paired cross-modal attention is quadratically\ncomplex  in  relation  to  the  length  of  the  multimodal  sequence,\nmaking  this  operation  inefficient.  Moreover,  the  cross-modal\nattention  does  not  capture  complementary  features  well.  For\nexample,  the  audio  modality  is  not  entirely  helpful  for  the  visual\nmodality,  and  the  audio  features  that  often  reinforce  the  visual\nfeatures  may  contain  only  a  fraction  of  them.  Additionally,  the\ninformation density of modalities for the emotion recognition task\nvaries,  with  some  modal  flows  carrying  more  task  information\nthan  others.  For  instance,  visual  modalities  perform  better  in\nclassifying “happy” emotions,  while  audio  modalities  are  more \n1 School of Computer Science and Technology, East China Normal University, Shanghai 200062, China\n2 MTlab, Meitu (China) Limited, Beijing 100876, China\n3 Institute of Acoustics, University of Chinese Academy of Sciences, Beijing 100084, China\nAddress correspondence to Feng Liu, lsttoy@163.com\n© The author(s) 2023. The articles published in this open access journal are distributed under the terms of the Creative Commons Attribution\n4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nhttps://doi.org/10.26599/AIR.2023.9150019 CAAI Artificial Intelligence Research\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023 1\neffective in classifying “fearful” emotions.\nTo  this  end,  we  propose  an  innovative  Transformer-based\nAdaptive Cross-modal Fusion Network (TACFN) for multimodal\nemotion recognition. Specifically, we divide the network into two\nsteps: unimodal representation and multimodal fusion. Firstly, for\nunimodal  representation,  we  perform  representation  learning  on\nthe spatio-temporal structure features[19] of video frame sequences\nand  MFCC  features[20] of  audio  sequences,  respectively.  Then,  for\nmultimodal  fusion,  the  two  core  issues  of  reducing  redundant\nfeatures  and  enhancing  complementary  features  are  mainly\nconsidered.  For  reducing  redundant  features,  a  self-attention\nmechanism  enables  a  modality  to  perform  intra-modal  feature\nselection,  and  the  selected  features  will  be  able  to  interact  with\nanother  modality  adaptively  and  efficiently.  To  enhance\ncomplementarity,  we  fuse  the  selected  modality  with  another\nmodality  by  splicing  and  generating  a  weight  vector.  The  weight\nvector is then multiplied with another modality to achieve feature\nenhancement.  Finally,  we  obtain  the  prediction  of  emotional\ncategories by splicing the fused representation. For simplicity, this\npaper  first  focuses  on  the  fusion  of  two  modalities,  visual  and\naudio.  Further,  we  extend  the  designed  fusion  block  to  cross-\nmodal fusion of three modalities, visual, audio and text. We apply\nTACFN  to  the  RAVDESS[21] and  IEMOCAP[22] datasets,  and  the\nexperimental  results  show  that  our  proposed  fusion  method  is\nmore effective. Compared with other methods, our method brings\na  significant  performance  improvement  in  emotion  recognition\nby the fusion strategy based on the same unimodal representation\nlearning. Our method achieves the state-of-the-art performance.\nThe  main  contributions  of  this  paper  can  be  summarized  as\nfollows:\n● We propose an innovative TACFN for multimodal emotion\nrecognition.\n● We  divide  TACFN  into  two  parts:  unimodal  representation\nand multimodal fusion. We perform intra-modal feature selection\nthrough  a  self-attention  mechanism  and  achieve  modal  feature\nreinforcement  by  introducing  a  fused  weight  vector.  This\noperation  reduces  redundant  features  and  enhances  inter-modal\ncomplementary information.\n● We apply TACFN to the RAVDESS and IEMOCAP datasets.\nThe  experimental  results  show  that  the  adaptive  cross-modal\nfusion block brings a significant performance improvement to the\nstate-of-the-art  based  on  the  same  unimodal  representation\nlearning. \n1    Related Work\nMultimodal  emotion  recognition  understands  various  human\nemotions by collecting and processing information from different\nmodalities[4, 23].  This  task  requires  fusion  of  cross-modal\ninformation  from  different  temporal  sequence  signals.  We  can\nclassify  multimodal  fusion  according  to  the  way  it  is  performed:\nearly fusion[7, 8], late fusion[9, 10] and model fusion[11–13]. Previous work\nhas  focused  on  early  fusion  and  late  fusion  strategies.  Early\nfusion[7, 8] is  to  extract  and  construct  multiple  modal  data  into\ncorresponding modal features before stitching them together into\na  feature  set  that  integrates  individual  modal  features.  On  the\ncontrary,  late  fusion[7, 8] is  to  find  out  the  credibility  of  individual\nmodels  and  then  to  make  coordinated,  joint  decisions.  Although\nthese  methods  obtain  better  performance  than  learning  from  a\nsingle  modality,  they  do  not  explicitly  consider  the  intrinsic\ncorrelations between sequence elements from different modalities,\nwhich are crucial for effective multimodal fusion. For multimodal\nfusion,  a  good  fusion  scheme  should  extract  and  integrate  valid\ninformation  from  multimodal  sequences  while  maintaining  the\nmutual independence between modalities[14].\nWith  the  popularity  of  Transformer[15],  model  fusion  is\ngradually using Transformer to achieve multimodal fusion. Model\nfusion[11–13] allows  flexible  choice  of  fusion  locations  compared  to\nthe  previous  two  approaches,  taking  into  account  the  intrinsic\ncorrelation  between  sequence  elements  from  different  modalities.\nTypically, MulT[24], PMR[17] and SPT[16] all use cross-modal attention\nto  model  multimodal  sequences.  The  cross-modal  attention\noperation uses information from the source modality to reinforce\nthe  target  modality  by  learning  directed  pairwise  attention\nbetween  the  source  modality  and  the  target  modality.  Moreover,\nMMTM[25] module  allows  slow  fusion  of  information  between\nmodalities  by  adding  to  different  feature  layers,  which  allows  the\nfusion  of  features  in  convolutional  layers  of  different  spatial\ndimensions. MSAF[26] module splits each channel into equal blocks\nof  features  in  the  channel  direction  and  creates  a  joint\nrepresentation that is used to generate soft notes for each channel\nacross the feature blocks. MBT[18] restricts the flow of cross-modal\ninformation between latent units through tight fusion bottlenecks,\nthat  force  the  model  to  collect  and  condense  the  most  relevant\ninputs in each modality.\nThis work proposes a new attention-based adaptive multimodal\nfusion  network  that  performs  adaptive  intra-modal  selection\nthrough  a  self-attention  mechanism  to  reduce  its  own  redundant\nfeatures  and  improve  the  ability  to  capture  complementary\nfeatures. \n2    Methodology\nIn  this  paper,  we  divide  TACFN  into  two  steps,  unimodal\nrepresentation  and  multimodal  fusion.  Our  goal  is  to  perform\nefficient  cross-modal  fusion  from  multimodal  sequences,\naggregating  intra- and  inter-modal  features  to  achieve  correct\nemotion classification. Figure 1 shows the overall framework. \n2.1    Modality encoding \n2.1.1    Audio encoder\nXa\nFor  the  audio  modality,  recent  work[20, 27] has  demonstrated  the\neffectiveness  of  deep  learning  methods  based  on  Mel  Frequency\nCepstrum  Coefficient  (MFCC)  features.  We  design  a  simple  and\nefficient  1D  Convolutional  Neural  Network  (CNN)  to  perform\nMFCC  feature  extraction.  Specifically,  we  use  the  feature\npreprocessed  audio  modal  features  as  input,  denoted  as .  We\nfirst  pass  the  features  through  a  2-layer  convolution  operation  to\nextract the local features of adjacent audio elements. After that, we\nuse  the  max-pooling  to  downsample,  compress  the  features,  and\nremove  redundant  information.  The  specific  equation  is  as\nfollows:\nˆX\n′\na = BN(ReLU(Conv1D(Xa; ka)))\nˆX\n′′\na = ReLU(Conv1D( ˆX\n′\na; ka)) (1)\n \nˆX\n′′′\na = Dropout(BN(MaxPool( ˆX\n′′\na ))) (2)\n \nBN\nka\nˆXa\nwhere  stands  for  batch  normalization,  is  the  size  of  the\nconvolution kernel of modality audio, and  denotes the learned\nhigh-level  semantic  features.  Finally,  we  flatten  the  obtained\nfeatures as\nˆXa = Flatten(BN(ReLU(Conv1D( ˆX\n′′′\na ; ka)))) (3)\n  \nCAAI Artificial Intelligence Research\n \n2 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023\n2.1.2    Visual encoder\nXv\nVideo  data  are  dependent  in  both  spatial  and  temporal\ndimensions,  thus  a  network  with  3D  convolutional  kernels  is\nneeded  to  learn  facial  expressions  and  actions.  We  consider  both\nthe  performance  and  training  efficiency  of  the  network  and\nchoose  the  3D  ResNeXt[19] network  to  obtain  the  spatio-temporal\nstructural features of visual modalities. ResNeXt proposes a group\nconvolution  strategy  between  the  deep  segmentation  convolution\nof ordinary convolutional kernels, and achieves a balance between\nthe  two  strategies  by  controlling  the  number  of  groups  with  a\nsimple  structure  but  powerful  performance.  We  use  feature\npreprocessed  visual  modal  features  as  input,  denoted  as .  We\nobtain the high-level semantic features of visual modalities by this\nnetwork:\nˆXv = ResNeXt50(Xv) 2 RC\u0002S\u0002H\u0002W (4)\n \nˆXv\nC\nS\nH\nW\nwhere  denotes the learned semantic features. , , , and \nare  the  number  of  channels,  sequence  length,  height,  and  width,\nrespectively. \n2.2    Fusion via cross-modal attention\nX\nY\nX\nY\nMCA(X; Y) =Attention(WQX; WKY; WVY)\nThe  cross-modal  attention  operation  uses  information  from  the\nsource  modality  to  reinforce  the  target  modality  by  learning\ndirected  pairwise  attention  between  the  source  modality  and  the\ntarget modality[17, 24]. Cross-modal attention is a modification of self-\nattention,  with Q as  one  modality  and K and V as  another\nmodality  to  obtain  reinforcement  of  the  modality.  We  define  the\ncross-modal attention of two tensors  and , where  forms the\nquery and  forms the keys and values used to reweight the query\nas .\nˆXm\nhm\nm 2 fv; ag\nhm\nL\nWe take the obtained unimodal high-level representations and\nperform  cross-modal  attention  interactions  between  audio  and\nvisual  modalities  to  obtain  reinforcing  features  of  each  other.  We\nfirst  encode  the  input  of  the  multimodal  sequence  into  the\nlength  representations  as , .  After  that,  we  feed \ninto  the  cross-modal  Transformer  encoder.  An  encoder  consists\nof a sequence of  cross-modal Transformer layers, and each cross-\nmodal  Transformer  layer  consists  of  Multi-head  Cross-modal\nAttention  (MCA),  Layer  Normalization  (LN),  and  Multilayer\nPerceptron  (MLP)  blocks  applying  residual  connections.  To  this\nend, we define a cross-Transformer layer as\nyl = MCA(LN(hl\nfv;ag); LN(hl\nfa;vg)) +hl\nfv;ag (5)\n \nhl+1\nm = MLP(LN(yl)) +yl m 2 fv; ag (6)\n \nThe target modality is reinforced by encouraging the model to\nattend  to  crossmodal  interaction  between  elements.  The  formula\nis as follows:\nhl+1\na = Cross-Transformer(hl\na; hl\nv;θa) (7)\n \nhl+1\nv = Cross-Transformer(hl\nv; hl\na;θv) (8)\n \nˆha\nˆhv\nI = [ˆha; ˆhv]\n  is  used  to  denote  the  reinforced  audio  modality  and  is\nused  to  denote  the  reinforced  visual  modality.  Finally,  we  splice\nthe reinforced modalities to obtain the fused data . \n2.3    Fusion via adaptive cross-modal blocks\nThe  existing  experiments  show  that  model  fusion  further\nconsiders  the  internal  relationship  between  modalities  and  has\nbetter effects and performance. However, there are some problems\nin  the  cross-modal  fusion  scheme  adopted  by  the  current  model\nfusion, namely cross-attention:\n(1) Feature redundancy exists in cross-modal attention.\n(2)  Cross-modal  attention  does  not  capture  complementary\nfeatures well.\n(3)  Since  the  modes  change  dynamically,  some  of  them  have\nmore  representation  information  for  the  task  than  others.  For\nexample,  the  visual  modality  classifies “happy” better  than  the\naudio modality, and the audio modality classifies “fearful” better.\nTherefore, on the basis of cross-modal, we introduce the design\nof adaptive cross-modal blocks. Figure 2 illustrates the architecture\nof the cross-modal attention and the adaptive cross-modal blocks.\nAfter  obtaining  the  unimodal  representations,  we  feed  them  into\nthe  adaptive  cross-modal  block  to  obtain  the  reinforcement\nfeatures  of  both  modalities.  Here,  we  show  the  process  of  using\naudio modality to reinforce visual modality. The process of using\nvisual  modality  to  reinforce  audio  modality  is  the  same.\nSpecifically,  we  first  make  the  audio  modal  pass  through  the\nTransformer  encoder  to  perform  intra-modal  feature  selection.\nThe  Transformer  encoder  is  the  same  as  the  Cross-Transformer\nencoder, the difference lies in Eq. (5).\nyl = MSA(LN(hl\nm)) +hl\nm; m 2 fv; ag (9)\n \nhm\nMSA(hm) =Attention(WQhm; WKhm; WVhm)\nHere,  the  MSA  operation  calculates  the  dot  product  attention,\nwhere the queries, keys, and values are all linear projections of the\nsame  tensor , .\n \nVisual\nAudio\nModality type\n Modality encoder\nResNeXt\nAdaptive\ncross-modal \nblocks\nCNN\nPrediction\nFusion Audio features\nVisual features\nAdd\ntanh\nsoftmax\nTransformer\nencoder\nLinear\nXA\nTα×df\nLinear\nAdaptive cross-modal blocks\nXV\nC×S×H×W\nXa\ndf\nXq\nk×S×H×W\nXns\nS×H×W\nXo\nC×S×H×W\n\n\n\n\n\n\n \nFig. 1    Overall  architecture  of  TACFN.  Left:  flow  structure  of  the  whole  model,  divided  into  two  parts,  i.e.,  unimodal  representation  and  multimodal  fusion.\nRight: the adaptive cross-modal fusion block.\nTACFN: Transformer-Based Adaptive Cross-Modal Fusion Network for Multimodal Emotion Recognition\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023 3\nThis  operation  enables  the  higher-order  features  of  the  audio\nmodality to perform feature selection, making it more focused on\nfeatures that have a greater impact on the outcome.\nˆXA 2 Rdf\nˆXV 2 RC\u0002S\u0002H\u0002W\nˆXo 2 RC\u0002S\u0002H\u0002W\nThen,  we  make  the  automatically  selected  features  and  the\nvideo  modality  perform  efficient  inter-modal  interactions.  The\nmodule accepts input for two modalities, which is called \nand .  We  obtain  the  mapping  representations  of\nthe  features  for  the  two  modalities  by  a  linear  projection.  And\nthen  we  process  the  two  representations  by  add  and  tanh\nactivation  function.  Finally,  the  fused  representation\n  is obtained through softmax. The specific formula\nis as follows:\nˆXq = tanh((Wv ˆXV + bv) +Wa ˆXA) 2 Rk\u0002S\u0002H\u0002W (10)\n \nˆXo = (softmax( ˆXq) \n ˆXV) \b ˆXV 2 RC\u0002S\u0002H\u0002W (11)\n \nWv 2 Rk\u0002C\nWa 2 Rk\u0002df\nbv 2 Rk\nk\n\b\nin  which  and  are  linear  transformation\nweights,  and  is  the  bias,  where  is  a  pre-defined  hyper-\nparameter, and  represents the broadcast addition operation of a\ntensor  and  a  vector.  We  can  see  that  this  operation  is  mainly  to\nfuse  the  audio  modality  and  the  visual  modality  after  feature\nselection by splicing to obtain the weight vector, and multiply the\nweight  vector  with  the  visual  modality  to  achieve  feature\nreinforcement.  In  this  process,  to  ensure  that  the  information  of\nthe  visual  modality  is  not  lost,  we  ensure  the  integrity  of  the\noriginal  structural  features  of  the  visual  modality  through  the\nresidual structure.\nˆXa!v\no\nˆXv!a\no\nI = [ˆXv!a\no ; ˆXa!v\no ]\nWe  use  to  denote  the  features  that  use  audio  data  to\nenhance  the  visual  modality  and  to  denote  the  features  that\nuse  visual  data  to  enhance  the  audio  modality.  Finally,  we  splice\nthe reinforced modalities to obtain the fused data . \n2.4    Classification\nFinally,  we  use  the  fused  data  for  emotion  category  prediction.\nThe  cross-entropy  (CE)  loss  is  used  to  optimize  the  model.  The\nspecific equation is shown as follows:\nprediction = W1I + b1 2 Rdout (12)\n \nL = \u0000å\ni\nyilog(ˆyi) (13)\n \ndout\nW1 2 Rdout\nb1\ny = fy1; y2; :::;yngT\nˆy = fˆy1; ˆy2; :::;ˆyngT\nn\nwhere  is  the  output  dimensions  of  emotional  categories,\n  is  the  weight  vectors,  is  the  bias, \nis  the  one-hot  vector  of  the  emotion  label,  is\nthe  predicted  probability  distribution,  and  is  the  number  of\nemotion categories. \n3    Experiment \n3.1    Datasets\nIn  this  paper,  we  use  two  mainstream  datasets:  RAVDESS  and\nIEMOCAP. For simplicity, this study first focuses on the fusion of\ntwo modalities, visual and audio. Further, we extend the designed\nfusion  block  to  cross-modal  fusion  of  three  modalities,  visual,\naudio, and text. Specifically, for the RAVDESS dataset, we use two\nmodalities:  visual  and  audio.  For  the  IEMOCAP  dataset,  we  use\nthree  modalities:  visual,  audio,  and  text.  The  code  is  available  at\nhttps://github.com/shuzihuaiyu/TACFN. \n3.1.1    RAVDESS\nRyerson  Audio-Visual  Database  of  Emotional  Speech  and  Song\n(RAVDESS)[21] is  a  multimodal  emotion  recognition  dataset\ncontaining  24  actors  (12  male,  12  female)  of  1440  video  clips  of\nshort speeches. The dataset is performed when the actors are told\nthe  emotion  to  be  expressed,  with  high  quality  in  terms  of  both\nvideo  and  audio  recordings.  Eight  emotions  are  included  in  the\ndataset:  neutral,  calm,  happy,  sad,  angry,  fearful,  disgust,  and\nsurprised.  We  perform  5-fold  cross-validation  on  the  RAVDESS\ndataset  to  provide  more  robust  results.  We  divide  the  24  actors\ninto a training and a test set in a 5:1 ratio. Since the actors’ gender\nis represented by an even or odd number of actor IDs, we enable\ngender to be evenly distributed by rotating 4 consecutive actor IDs\nas the test set for each fold of cross-validation. The final accuracy\n \nAdd\ntanh\nsoftmax\nTransformer\nencoder\nLinear\nModality βModality α\nXA\nTα×df\nLinear\nXα\nTα×dα Xβ\nTβ×dβ\nVβ\nTβ×dv\nVβ\nQαKβT\ndk\nTα×dv\nKβ\nTβ×dkQα  \n\n\n\n\n\n\n\n\n\n\nTα×dq\nXV\nC×S×H×W\nXa\nWVβWKβ\nCross-modal attention\nSoftmax (      )√\nAdaptive cross-modal blocks\nWQα\ndf\nXq\nk×S×H×W\nXns\nS×H×W\nXo\nC×S×H×W\n \nFig. 2    Architectural elements of a cross-modal attention and adaptive cross-modal blocks.\nCAAI Artificial Intelligence Research\n \n4 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023\nreported is the average accuracy over the 5 folds. \n3.1.2    IEMOCAP\nIEMOCAP[22] is  a  multimodal  emotion  recognition  dataset  that\ncontains  151  videos  along  with  corresponding  transcripts  and\naudios.  In  each  video,  two  professional  actors  conduct  dyadic\nconversations  in  English.  Its  intended  data  segmentation  consists\nof  2717  training  samples,  798  validation  samples,  and  938  test\nsamples.  The  audio  and  visual  features  are  extracted  at  the\nsampling frequencies of 12.5 Hz and 15 Hz, respectively. Although\nthe human annotation has nine emotion categories, following the\nprior  work[28],  we  take  four  categories:  neutral,  happy,  sad,  and\nangry.  Moreover,  this  is  a  multi-label  task  (e.g.,  a  person  can  feel\nsad  and  angry  at  the  same  time).  We  report  the  binary\nclassification  accuracy  and  F1-scores  for  each  emotion  category\naccording to Ref. [17]. \n3.2    Implementation details \n3.2.1    Feature extraction of the RAVDESS dataset\nFor  the  visual  modality,  we  extract  30  consecutive  images  from\neach  video.  We  crop  the  face  region  using  the  2D  face  markers\nprovided  for  each  image  and  then  resize  to  (224,  224).  Data\naugmentation is performed using random cropping, level flipping\nand normalization methods. For the audio modality, since the first\n0.5  seconds  usually  do  not  contain  sound,  we  trim  the  first  0.5\nseconds  and  keep  it  consistent  for  the  next  2.45  seconds.\nFollowing  the  suggestion  of  Ref.  [29],  we  extract  the  first  13\nMFCC features for each cropped audio clip. \n3.2.2    Feature extraction of the IEMOCAP dataset\nFor  feature  extraction  of  the  text  modality,  we  convert  video\ntranscripts  into  pre-trained  Glove[30] model  to  obtain  300-\ndimensional  word  embeddings.  For  feature  extraction  of  visual\nmodality, we use Facet[31] to represent 35 facial action units, which\nrecord  facial  muscle  movements  for  representing  basic  and  high-\nlevel  emotions  in  each  frame.  For  the  audio  modality,  we  use\nCOVAREP[32] for  extracting  acoustic  signals  to  obtain  74-\ndimensional vectors.\nThe  model  is  trained  using  the  Adam  optimizer[33] with  a\nlearning rate of 0.001, and the entire training of the model is done\non a single NVIDIA RTX 8000 GPU. \n3.3    Baselines\nFor  the  audio-visual  emotion  recognition  task,  we  implement\nmultiple  recent  multimodal  fusion  algorithms  as  our  baselines.\nWe categorize them into the following:\n● Simple  feature  concatenations  followed  by  fully  connected\nlayers based on Ref. [34] and MCBP[35] are two typical early fusion\nmethods.\n● Averaging and multiplication are the two standard late fusion\nmethods that are adopted as the baselines.\n● Multiplicative  layer[36] is  a  late  fusion  method  that  adds  a\ndown-weighting factor to CE loss to suppress weaker modalities.\n● MMTM[25] module allows slow fusion of information between\nmodalities  by  adding  to  different  feature  layers,  which  allows  the\nfusion  of  features  in  convolutional  layers  of  different  spatial\ndimensions.\n● MSAF[26] module  splits  each  channel  into  equal  blocks  of\nfeatures in the channel direction and creates a joint representation\nthat  is  used  to  generate  soft  notes  for  each  channel  across  the\nfeature blocks.\n● MCA[17] stands for cross-modal attention, and the module is a\nmodification of self-attention, with Q as one modality and K and\nV as  another  modality  to  obtain  the  reinforcement  of  the\nmodality, which is the current mainstream fusion method.\nFor  multimodal  emotion  recognition  tasks,  we  compare  the\nproposed approach with the existing state-of-the-art methods:\n● Early  Fusion  Long  Short-Term  Memory  (EF-LSTM)  and\nLate  Fusion  LSTM  (LF-LSTM)  simply  concatenate  features  at\ninput  and  output  level,  which  apply  LSTM[37] to  extract  features\nand infer prediction.\n● Recurrent    Attended     Variation     Embedding     Network\n(RAVEN)[38] and  Multimodal  Cyclic  Translation  Network\n(MCTN)[39] are  joint  representation  fusion  methods  based  on\ntemporal modeling.\n● Multimodal Transformer (MulT)[24], Low Rank Fusion based\nTransformers  (LMF-MulT)[11],  and  Progressive  Modality\nReinforcement  (PMR)[17] are  all  inter-modal  complementary\nfusion methods based on cross-modal attention, with PMR being\nthe state-of-the-art model. \n3.4    Comparison to state-of-the-art methods\nTable  1 shows  the  accuracy  comparison  of  the  proposed  method\nwith  baselines  on  RAVDESS  dataset.  From Table  1,  we  can  see\nthat our model achieves an accuracy of 76.76% reaching the state-\nof-the-art.\n(1)  The  unimodal  performance  of  TACFN  is  62.99%  and\n56.53%  for  visual  and  audio,  respectively,  and  the  accuracy  is\n76.76%  after  the  adaptive  cross-modal  blocks,  which  is  an\nimprovement  of  more  than  13.77%.  As  can  be  seen,  the  adaptive\ncross-modal block learns the complementary information of both.\nIt  learns  information  that  is  present  in  the  audio  but  not  in  the\nvisual,  thus  giving  the  visual  representation  more  semantic\ninformation of the audio modality.\n(2)  Compared  with  early  fusion  methods,  our  method  has\nmore  than  5%  accuracy  improvement,  which  shows  that  finding\nthe  association  between  visual  and  audio  modalities  is  a  difficult\ntask  in  the  early  stage.  Compared  to  the  late  fusion  methods,  we\nfind  that  the  unimodal  feature  splicing  obtained  in  the  late  stage\nwill  get  73.50%  performance.  What  is  more,  our  proposed\nTACFN  has  3.64%  improvement  over  MMTM  and  1.9%\nimprovement over the best-performing MSFA.\n(3)  We  obtain  the  reinforcement  features  fusing  the  other\nmodality  using  two  cross-modal  attentions  separately.  The\nexperimental  results  show  an  accuracy  of  74.58%,  while  our\n \nTable 1    Comparison  between  multimodal  fusion  baselines  and  ours  for\nemotion recognition on RAVDESS.\nModel Fusion Accuracy (%) Improvement (%)\n3D RexNeXt50 (visual) - 62.99 -\n1D CNN (audio) - 56.53 -\nAveraging Late 68.82 + 5.83\nβ\nMultiplicative (  = 0.3) Late 70.35 + 7.36\nMultiplication Late 70.56 + 7.57\nConcat + FC Late 73.50 + 10.51\nConcat + FC[34] Early 71.04 + 8.05\nMCBP[35] Early 71.32 + 8.33\nMMTM[25] Model 73.12 + 10.13\nMSAF[26] Model 74.86 + 11.87\nMCA[17] Model 74.58 + 11.59\nTACFN (Ours) Model 76.76 + 13.77\nTACFN: Transformer-Based Adaptive Cross-Modal Fusion Network for Multimodal Emotion Recognition\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023 5\naccuracy  is  76.76%.  TACFN  is  more  effective  under  the  same\nexperimental  setup.  The  reason  that  cross-modal  attention  does\nnot  capture  complementary  features  well  is  considered  to  be  due\nto  the  fact  that  audio  modality  is  not  entirely  helpful  for  visual\nmodality,  while  features  that  often  reinforce  visual  modality  may\nonly  contain  a  portion  of  it.  Based  on  this,  we  have  designed  the\ncross-modal  block  to  focus  more  effectively  and  adaptively  on\ninformation  that  is  more  useful  for  the  current  modality,  i.e.,\neffective complementary information.\nWe  also  apply  the  model  to  the  IEMOCAP  dataset. Table  2\nshows the results. We use adaptive cross-modal blocks to achieve\ncomplementary  learning,  i.e.,  we  use  audio  and  text  fusion\nmodalities  to  obtain  weight  information  to  reinforce  text\nmodalities, and visual and text fusion modalities to obtain weight\ninformation to reinforce text modalities.\n(1)  MulT  utilizes  cross-modal  attention  to  achieve\ncomplementary  learning.  Our  model  outperforms  MulT  in  all\nmetrics.\n(2)  We  also  compare  with  the  state-of-the-art  PMR,  and  the\nresults  achieved  comparable  levels.  Surprisingly,  TACFN  has  the\nlowest number of parameters. \n3.5    Ablation study \n3.5.1    Effectiveness of adaptive cross-modal blocks\nTable 3 shows the ablation experiments on the RAVDESS dataset.\nTo verify the effectiveness of the adaptive cross-modal blocks, we\nobtain  the  final  sentiment  by  simply  splicing  the  high-level\nsemantic  features  of  the  two  modalities.  The  experimental  results\nshow  that  our  cross-modal  block  leads  to  a  performance\nimprovement of more than 3% with only a 0.4 MB increase in the\nstorage  of  parameters,  which  indicates  that  efficient\ncomplementary  information  from  both  modalities  can  have  a\nlarge impact on the final decision.\nWe  further  explore  the  validity  of  the  internal  structure  of  the\nadaptive  cross-modal  blocks.  In  the  adaptive  cross-modal  blocks,\nthe  self-attention  mechanism  and  the  residual  structure  play  an\nimportant  role  in  the  performance  of  the  model.  We  detach  the\nself-attention mechanism and the residual structure separately and\nit  can  be  seen  that  self-attention  brings  more  than  3%  impact  on\nthe final result. This indicates that the audio semantic features we\nobtained  contain  redundant  information  and  can  be  selected  by\nthe  self-attention  mechanism  for  feature  selection  to  make  it\nefficient and adaptive for inter-modal interaction. In addition, we\nalso  see  that  the  residual  structure  has  less  impact  on  the  final\nresults, suggesting that the inclusion of the residual structure helps\nto  ensure  that  the  loss  of  visual  features  is  minimized  during  the\ninteraction.\nV ! A\nA ! V\nMoreover,  we  integrate  the  audio  modality  into  the  visual\nmodality as the final fusion result in our model design, denoted as\n .  We  have  compared  the  results  of  integrating  visual\nmodality  into  audio  modality,  denoted  as .  We  find  that\nthere is a 1.6% difference between them. We splice them together\nto bring a 1% improvement. \n3.5.2    Validity of each category\nWe  report  the  accuracy  of  unimodal  and  TACFN  for  each  class\non  the  RAVDESS  dataset  separately.  The  results  are  shown  in\nTable 4.\n(1) For the visual modality, the expressions “Happy”, “Angry”,\nand “Surprised” are easier to distinguish than the audio modality.\n(2)  For  the  audio  modality, “Sad” and “Fearful” have  higher\nperformance.\n(3) It can be seen that after the adaptive cross-modal blocks, the\naccuracy  of  each  class  has  improved  compared  to  the  unimodal\nones. “Neutral” has the lowest accuracy while “Sad” and “Fearful”\nhave the most significant improvement of about 10%. We believe\nthat  the  visual  modality  has  gained  complementary  information\nfrom the audio modality, resulting in a higher performance. \n4    Conclusion\nIn  this  paper,  we  propose  an  innovative  Transformer-based\nadaptive multimodal fusion network. We divide this network into\ntwo  steps:  unimodal  representation  and  multimodal  fusion.  The\ntwo  core  issues  of  reducing  redundant  features  and  enhancing\ncomplementary  features  are  mainly  considered  in  multimodal\nfusion.  For  reducing  redundant  features,  we  use  a  self-attention\nmechanism  to  enable  one  modality  to  perform  intra-modal\nfeature  selection,  and  the  selected  features  can  be  adaptively\ninteracted  with  another  modality  in  an  efficient  inter-modal\nmanner.  For  boosting  complementary  features,  we  fuse  the\nselected  modality  with  another  modality  by  splicing  to  obtain  a\nweight  vector,  and  multiply  the  weight  vector  with  another\nmodality to achieve feature reinforcement. We apply the model to\nRAVDESS  and  IEMOCAP  datasets,  and  the  experimental  results\nshow  that  our  proposed  fusion  method  is  more  effective.\nCompared with other models, our approach delivers a significant\n \nTable 2    Comparison  on  the  IEMOCAP  dataset  under  both  word-aligned  setting  and  unaligned  setting.  The  performance  is  evaluated  by  the  binary\nclassification  accuracy  (Acc)  and  the  F1-score  (F1)  for  each  emotion  class.  TACFN  achieves  comparable  and  superior  performance  with  only  0.34  MB\nparameters.\nMethod Fusion Storage of parameters (MB)\nHappy Sad Angry Neutral\nAcc (%) F1 (%) Acc (%) F1 (%) Acc (%) F1 (%) Acc (%) F1 (%)\nEF-LSTM Early 0.10 76.2 75.7 70.2 70.5 72.7 67.1 58.1 57.4\nLF-LSTM Late 0.17 72.5 71.8 72.9 70.4 68.6 67.9 59.6 56.2\nRAVEN[38] Model 1.20 77.0 76.8 67.6 65.6 65.0 64.1 62.0 59.5\nMCTN[39] Model 0.97 80.5 77.5 72.0 71.7 64.9 65.6 49.4 49.3\nMulT[24] Model 1.07 84.8 81.9 77.7 74.1 73.9 70.2 62.5 59.7\nLMF-MulT[11] Model 0.86 85.6 79.0 79.4 70.3 75.8 65.4 59.2 44.0\nPMR[17] Model 2.15 86.4 83.3 78.5 75.3 75.0 71.3 63.7 60.9\nTACFN (ours) Model 0.34 85.7 82.5 79.4 75.6 76.0 71.7 63.6 60.5\nCAAI Artificial Intelligence Research\n \n6 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023\nperformance  improvement  in  emotion  recognition  from  the\nfusion  strategy  based  on  the  same  unimodal  representation\nlearning. \nAcknowledgment\nThis  study  was  supported  by  Beijing  Key  Laboratory  of  Behavior\nand Mental Health, Peking University.\nArticle History\nReceived: 7 July 2023; Accepted: 31 August 2023\nReferences \n S.  Zhao,  G.  Jia,  J.  Yang,  G.  Ding,  and  K.  Keutzer,  Emotion\nrecognition  from  multiple  modalities:  Fundamentals  and\nmethodologies,  IEEE  Signal  Process.  Mag.,  vol.  38,  no.  6,  pp.\n59–73, 2021.\n[1]\n S. Poria, D. Hazarika, N. Majumder, and R. Mihalcea, Beneath the\ntip  of  the  iceberg:  Current  challenges  and  new  directions  in\nsentiment analysis research, IEEE  Trans.  Affect.  Comput., vol. 14,\nno. 1, pp. 108–132, 2023.\n[2]\n Q. Gan, S. Wang, L. Hao, and Q. Ji, A multimodal deep regression\nBayesian network for affective video content analyses, in Proc. 2017\nIEEE  Int.  Conf.  Computer  Vision (ICCV), Venice, Italy, 2017, pp.\n5123–5132.\n[3]\n D. Nguyen, K. Nguyen, S. Sridharan, D. Dean, and C. Fookes, Deep\nspatio-temporal  feature  fusion  with  compact  bilinear  pooling  for\nmultimodal emotion recognition, Comput. Vis. Image Underst., vol.\n174, pp. 33–42, 2018.\n[4]\n L. Smith and M. Gasser, The development of embodied cognition:\nSix lessons from babies, Artif. Life, vol. 11, nos. 1–2, pp. 29, 2005.\n[5]\n W. Wang, D. Tran, and M. Feiszli, What makes training multi-modal\nclassification  networks  hard?  in  Proc.  2020  IEEE/CVF  Conf.\nComputer  Vision  and  Pattern  Recognition  (CVPR),  Seattle,  WA,\nUSA, 2020, pp. 12692–12702.\n[6]\n L. P. Morency, R. Mihalcea, and P. Doshi, Towards multimodal\nsentiment analysis: Harvesting opinions from the web, in Proc. 13th\nInt.  Conf.  multimodal  interfaces,  Alicante,  Spain,  2011,  pp.\n169–176.\n[7]\n V. Pérez-Rosas, R. Mihalcea, and L. P. Morency, Utterance-level\nmultimodal  sentiment  analysis,  in  Proc.  51st  Annual  Meeting\nAssociation  for  Computational  Linguistics, Sofia, Bulgaria, 2013,\npp. 973–982.\n[8]\n A.  Zadeh,  R.  Zellers,  E.  Pincus,  and  L.  P.  Morency,  MOSI:\nMultimodal corpus of sentiment intensity and subjectivity analysis in\nonline opinion videos, arXiv preprint arXiv: 1606.06259, 2016.\n[9]\n H. Wang, A. Meghawat, L. -P. Morency, and E. P. Xing, Select-\nadditive learning: Improving generalization in multimodal sentiment\nanalysis,  in  Proc.  2017  IEEE  Int.  Conf.  Multimedia  and  Expo\n(ICME), Hong Kong, China, 2017, pp. 949–954.\n[10]\n S. Sahay, E. Okur, S. H. Kumar, and L. Nachman, Low rank fusion\nbased Transformers for multimodal sequences, arXiv preprint arXiv:\n2007.02038, 2020.\n[11]\n W. Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao, L. P.\nMorency, and E. Hoque, Integrating multimodal information in large\npretrained Transformers, in Proc. 58th Annual Meeting Association\nfor Computational Linguistics, virtual, 2020, pp. 2359–2369.\n[12]\n W.  Yu,  H.  Xu,  Z.  Yuan,  and  J.  Wu,  Learning  modality-specific\nrepresentations  with  self-supervised  multi-task  learning  for\nmultimodal sentiment analysis, Proc. AAAI Conf. Artif. Intell., vol.\n35, no. 12, pp. 10790–10797, 2021.\n[13]\n D.  Hazarika,  R.  Zimmermann,  and  S.  Poria,  MISA:  Modality-\ninvariant  and-specific  representations  for  multimodal  sentiment\nanalysis, in Proc.  28th  ACM  Int.  Conf.  Multimedia, Seattle, WA,\nUSA, 2020, pp. 1122–1131.\n[14]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez,  L.  Kaiser,  and  I.  Polosukhin,  Attention  is  all  you  need,\narXiv preprint arXiv: 1706.03762, 2017.\n[15]\n J.  Cheng,  I.  Fostiropoulos,  B.  Boehm,  and  M.  Soleymani,\nMultimodal  phased  Transformer  for  sentiment  analysis,  in  Proc.\n2021  Conf.  Empirical  Methods  in  Natural  Language  Processing,\nPunta Cana, Dominican Republic, 2021, pp. 2447–2458.\n[16]\n F.  Lv,  X.  Chen,  Y.  Huang,  L.  Duan,  and  G.  Lin,  Progressive\nmodality reinforcement for human multimodal emotion recognition\nfrom  unaligned  multimodal  sequences,  in  Proc.  2021  IEEE/CVF\nConf. Computer Vision and Pattern Recognition (CVPR), Nashville,\nTN, USA, 2021, pp. 2554–2562.\n[17]\n A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun,\nAttention bottlenecksfor multimodal fusion, in Proc.  2021  Annual\nConf.  Nerual  Information  Processing  Systems,  virtual,  2021,  pp.\n14200–14213.\n[18]\n S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, Aggregated residual\ntransformations for deep neural networks, in Proc. 2017 IEEE Conf.\nComputer  Vision  and  Pattern  Recognition (CVPR), Honolulu, HI,\nUSA, 2017, pp. 5987–5995.\n[19]\n N.  Neverova,  C.  Wolf,  G.  Taylor,  and  F.  Nebout,  ModDrop:\nAdaptive  multi-modal  gesture  recognition,  IEEE  Trans.  Pattern\nAnal. Mach. Intell., vol. 38, no. 8, pp. 1692–1706, 2016.\n[20]\n S.  R.  Livingstone  and  F.  A.  Russo,  The  Ryerson  audio-visual\ndatabase of emotional speech and song (RAVDESS): A dynamic,\nmultimodal set of facial and vocal expressions in North American\nEnglish, .PLoS One, vol. 13, no. 5, p. e0196391, 2018.\n[21]\n C. Busso, M. Bulut, C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, IEMOCAP: Interactive\nemotional dyadic motion capture database, Lang. Resour. Eval., vol.\n42, no. 4, pp. 335–359, 2008.\n[22]\n D. Nguyen, K. Nguyen, S. Sridharan, A. Ghasemi, D. Dean, and C.\nFookes,  Deep  spatio-temporal  features  for  multimodal  emotion\nrecognition,  in  Proc.  2017  IEEE  Winter  Conf.  Applications  of\nComputer  Vision  (WACV),  Santa  Rosa,  CA,  USA,  2017,  pp.\n1215–1223.\n[23]\n Y. H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L. P. Morency, and\nR.  Salakhutdinov,  Multimodal  Transformer  for  unaligned\n[24]\n \nTable 3    Ablation study on the RAVDESS dataset.\nModel Accuracy\n(%)\nStorage of parameters\n(MB)\nTACFN 76.76 26.30\nWithout adaptive cross-modal\nblocks 73.50 25.92\nWithout self-attention 73.86 26.05\nWithout residual 76.33 26.30\n!\nV  A adaptive cross-modal 75.15 25.67\n!\nA  V adaptive cross-modal 75.76 26.30\n \nTable 4    Accuracy of each class in the RAVDESS dataset.\nLabel\nAccuracy (%)\nAudio Visual TACFN\nNeutral 54.2 58.6 65.6\nCalm 63.4 66.5 71.2\nHappy 57.3 68.1 70.2\nSad 72.0 64.9 81.4\nAngry 68.3 77.2 87.0\nFearful 73.4 68.8 86.3\nDisgust 61.5 69.5 73.2\nSurprised 74.1 76.3 81.7\nTACFN: Transformer-Based Adaptive Cross-Modal Fusion Network for Multimodal Emotion Recognition\n \nCAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023 7\nmultimodal  language  sequences,  in  Proc.  57th  Annual  Meeting\nAssociation for Computational Linguistics, Florence, Italy, 2019, pp.\n6558–6569.\n H. R. Vaezi Joze, A. Shaban, M. L. Iuzzolino, and K. Koishida,\nMMTM: Multimodal transfer module for CNN fusion, in Proc. 2020\nIEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR),\nSeattle, WA, USA, 2020, pp. 13286–13296.\n[25]\n L. Su, C. Hu, G. Li, and D. Cao, MSAF: Multimodal split attention\nfusion, arXiv preprint arXiv: 2012.07175, 2020.\n[26]\n J. Wang, M. Xue, R. Culhane, E. Diao, J. Ding, and V. Tarokh,\nSpeech emotion recognition with dual-sequence LSTM architecture,\nin  Proc.  2020  IEEE  Int.  Conf.  Acoustics,  Speech  and  Signal\nProcessing (ICASSP), Barcelona, Spain, 2020, pp. 6474–6478.\n[27]\n W. Dai, Z. Liu, T. Yu, and P. Fung, Modality-transferable emotion\nembeddings  for  low-resource  multimodal  emotion  recognition,\narXiv preprint arXiv: 2009.09629, 2020.\n[28]\n Q. Jin, C. Li, S. Chen, and H. Wu, Speech emotion recognition with\nacoustic  and  lexical  features,  in  Proc.  2015  IEEE  Int.  Conf.\nAcoustics, Speech and Signal Processing (ICASSP), South Brisbane,\nAustralia, 2015, pp. 4749–4753.\n[29]\n J. Pennington, R. Socher, and C. Manning, Glove: Global vectors for\nword  representation,  in  Proc.  2014  Conf.  Empirical  Methods  in\nNatural  Language  Processing  (EMNLP),  Doha,  Qatar,  2014,  pp.\n1532–1543.\n[30]\n T. Baltrušaitis, P. Robinson, and L. P. Morency, OpenFace: An open\nsource facial behavior analysis toolkit, in Proc.  2016  IEEE  Winter\nConf.  Applications  of  Computer  Vision (WACV), Lake Placid, NY,\nUSA, 2016, pp. 1–10.\n[31]\n G.  Degottex,  J.  Kane,  T.  Drugman,  T.  Raitio,  and  S.  Scherer,\nCOVAREP—A collaborative voice analysis repository for speech\ntechnologies, in Proc. 2014 IEEE Int. Conf. Acoustics, Speech and\nSignal Processing (ICASSP), Florence, Italy, 2014, pp. 960–964.\n[32]\n D.  P.  Kingma  and  J.  Ba,  Adam:  A  method  for  stochastic\noptimization, arXiv preprint arXiv: 1412.6980, 2014.\n[33]\n J.  D.  S.  Ortega,  M.  Senoussaoui,  E.  Granger,  M.  Pedersoli,  P.\nCardinal, and A. L. Koerich, Multimodal fusion with deep neural\nnetworks for audio-video emotion recognition, arXiv preprint arXiv:\n1907.03196, 2019.\n[34]\n A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M.\nRohrbach, Multimodal compact bilinear pooling for visual question\nanswering and visual grounding, arXiv preprint arXiv: 1606.01847,\n2016.\n[35]\n K. Liu, Y. Li, N. Xu, and P. Natarajan, Learn to combine modalities\nin  multimodal  deep  learning,  arXiv  preprint  arXiv:  1805.11730,\n2018.\n[36]\n S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[37]\n Y.  Wang,  Y.  Shen,  Z.  Liu,  P.  P.  Liang,  A.  Zadeh,  and  L.  -P.\nMorency,  Words  can  shift:  Dynamically  adjusting  word\nrepresentations using nonverbal behaviors, Proc.  AAAI  Conf.  Artif.\nIntell., vol. 33, no. 1, pp. 7216–7223, 2019.\n[38]\n H. Pham, P. P. Liang, T. Manzini, L. -P. Morency, and B. Póczos,\nFound in translation: Learning robust joint representations by cyclic\ntranslations between modalities, Proc. AAAI Conf. Artif. Intell., vol.\n33, no. 1, pp. 6892–6899, 2019.\n[39]\nCAAI Artificial Intelligence Research\n \n8 CAAI Artificial Intelligence Research | VOL. 2 Article No. 9150019 | 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7480461597442627
    },
    {
      "name": "Modal",
      "score": 0.7284969091415405
    },
    {
      "name": "Modalities",
      "score": 0.6041188836097717
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.5532165765762329
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5196194052696228
    },
    {
      "name": "Transformer",
      "score": 0.5195193886756897
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.45134681463241577
    },
    {
      "name": "Machine learning",
      "score": 0.38684919476509094
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3716440498828888
    },
    {
      "name": "Engineering",
      "score": 0.11899077892303467
    },
    {
      "name": "Voltage",
      "score": 0.08556133508682251
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ]
}