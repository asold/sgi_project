{
  "title": "READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization",
  "url": "https://openalex.org/W4367666333",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126089117",
      "name": "Yun-Seok Choi",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A3158674380",
      "name": "CheolWon Na",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2129308784",
      "name": "Hyojun Kim",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2639657712",
      "name": "Jee-Hyong Lee",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2516621648",
    "https://openalex.org/W6692071231",
    "https://openalex.org/W2964268484",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W6767047860",
    "https://openalex.org/W3008733198",
    "https://openalex.org/W3034689979",
    "https://openalex.org/W6753851271",
    "https://openalex.org/W6756103864",
    "https://openalex.org/W4312354494",
    "https://openalex.org/W2979271470",
    "https://openalex.org/W6766245736",
    "https://openalex.org/W2955426500",
    "https://openalex.org/W3086449553",
    "https://openalex.org/W6789656390",
    "https://openalex.org/W3196992070",
    "https://openalex.org/W3176913510",
    "https://openalex.org/W3138429261",
    "https://openalex.org/W3177492177",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W6754601402",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W3173436362",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2999553660",
    "https://openalex.org/W6783998945",
    "https://openalex.org/W4206238733",
    "https://openalex.org/W3212549866",
    "https://openalex.org/W2999118008",
    "https://openalex.org/W3162962341",
    "https://openalex.org/W4285282363",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W2905342727",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W6783227185",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4284667247",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6678262379",
    "https://openalex.org/W2160204597",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3099636232"
  ],
  "abstract": "Code summarization is the process of automatically generating brief and informative summaries of source code to aid in software comprehension and maintenance. In this paper, we propose a novel model called READSUM, REtrieval-augmented ADaptive transformer for source code SUMmarization, that combines both abstractive and extractive approaches. Our proposed model generates code summaries in an abstractive manner, taking into account both the structural and sequential information of the input code, while also utilizing an extractive approach that leverages a retrieved summary of similar code to increase the frequency of important keywords. To effectively blend the original code and the retrieved similar code at the embedding layer stage, we obtain the augmented representation of the original code and the retrieved code through multi-head self-attention. In addition, we develop a self-attention network that adaptively learns the structural and sequential information for the representations in the encoder stage. Furthermore, we design a fusion network to capture the relation between the original code and the retrieved summary at the decoder stage. The fusion network effectively guides summary generation based on the retrieved summary. Finally, READSUM extracts important keywords using an extractive approach and generates high-quality summaries using an abstractive approach that considers both the structural and sequential information of the source code. We demonstrate the superiority of READSUM through various experiments and an ablation study. Additionally, we perform a human evaluation to assess the quality of the generated summary.",
  "full_text": "Received 23 February 2023, accepted 24 April 2023, date of publication 1 May 2023, date of current version 1 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3271992\nREADSUM: Retrieval-Augmented Adaptive\nTransformer for Source Code Summarization\nYUNSEOK CHOI\n 1, CHEOLWON NA\n 2, HYOJUN KIM\n 2, AND JEE-HYONG LEE\n2\n1Department of Platform Software, Sungkyunkwan University, Suwon 16419, South Korea\n2Department of Artificial Intelligence, Sungkyunkwan University, Suwon 16419, South Korea\nCorresponding author: Jee-Hyong Lee (john@skku.edu)\nThis work was supported in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by\nthe Korean Government [Ministry of Science and ICT (MSIT)] through the Artificial Intelligence Graduate School Program\n(Sungkyunkwan University) under Grant 2019-0-00421, in part by the Information and Communication Technology (ICT) Creative\nConsilience Program under Grant IITP-2021-2020-0-01821, and in part by the Self-Directed Multi-Modal Intelligence for Solving\nUnknown Open Domain Problems under Grant 2022-0-01045.\nABSTRACT Code summarization is the process of automatically generating brief and informative sum-\nmaries of source code to aid in software comprehension and maintenance. In this paper, we propose a novel\nmodel called READSUM, REtrieval-augmented ADaptive transformer for source code SUMmarization, that\ncombines both abstractive and extractive approaches. Our proposed model generates code summaries in an\nabstractive manner, taking into account both the structural and sequential information of the input code,\nwhile also utilizing an extractive approach that leverages a retrieved summary of similar code to increase the\nfrequency of important keywords. To effectively blend the original code and the retrieved similar code at the\nembedding layer stage, we obtain the augmented representation of the original code and the retrieved code\nthrough multi-head self-attention. In addition, we develop a self-attention network that adaptively learns the\nstructural and sequential information for the representations in the encoder stage. Furthermore, we design a\nfusion network to capture the relation between the original code and the retrieved summary at the decoder\nstage. The fusion network effectively guides summary generation based on the retrieved summary. Finally,\nREADSUM extracts important keywords using an extractive approach and generates high-quality summaries\nusing an abstractive approach that considers both the structural and sequential information of the source\ncode. We demonstrate the superiority of READSUM through various experiments and an ablation study.\nAdditionally, we perform a human evaluation to assess the quality of the generated summary.\nINDEX TERMS Abstract syntax tree, adaptive transformer, source code summarization, fusion network,\nshortest path.\nI. INTRODUCTION\nCode summarization is the process of automatically gener-\nating a concise and human-readable description of a code\nsnippet’s functionality. It aims to provide developers with an\nunderstanding of the code without having to read the entire\nimplementation. Well-written summaries make the code easy\nfor developers to understand as shown in Fig. 1. In order to\ngenerate a good summary about a source code, it is necessary\nto understand the structural and sequential meanings in the\ncode (abstractive approaches), and write the summary in an\neasy-to-understand form (extractive approaches).\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Jolanta Mizera-Pietraszko\n.\nPrevious approaches on automatic source code summa-\nrization can be categorized into sequence-based, structure-\nbased, and hybrid approaches. Sequence-based approaches\ngenerated summaries by capturing the sequential informa-\ntion of source code [1], [2], [3], [4], [5], [6], [7]. They\ntokenized source code into a sequence of code tokens, and\nencoded them using seq2seq models. Meanwhile, structure-\nbased approaches used Abstract Syntax Tree (AST) to cap-\nture the structural information of code [8], [9], [10], [11],\n[12], [13], [14], [15], [16], [17], [18], [19]. They parsed the\nsource code into the AST and utilized graph models such\nas Graph Neural Networks (GNNs). Some works flattened\nthe AST into the pre-order traversal sequence [18], [20],\n[21], [22], [23]. Hybrid approaches utilized both the token\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 51155\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nFIGURE 1. An example of code summarization. To write a good code\nsummary, it is important to focus on the key aspects of the code.\nsequences and the ASTs of codes [24], [25], [26]. They paral-\nlelly processed token sequences and ASTs with independent\nencoders, and tried to merge them in the decoder.\nA. MOTIVATION\nIn order to generate good summaries, we need to consider\nboth structural and sequential information of code. However,\nmost existing methods modeled code from either a sequential\npoint of view or a graph (AST) point of view. Some have\nattempted to consider both sequential and structural informa-\ntion together, but they independently processed both types of\ninformation and simply combined them.\nThe AST, structural information, are usually processed\nby GNNs, and code tokens by transformers. Since GNNs\nand transformers are attention-based networks, we can unify\nthem into a model. The graph attention network learns the\nstructural information using neighboring nodes in the AST\nthrough message passing to a specific node as shown in\nFig. 2(a). It locally gives attentions to neighboring nodes as\nshown in Fig. 2(d), and has difficulty in learning the global\ncontext information of code. Fig. 2(e) shows the self-attention\nin the vanilla transformer model that calculates attention\nvalues between all tokens. It can well learn the global con-\ntextual information, but it is difficult to learn the dependency\nbetween structurally related nodes because it equivalently\ngives attentions to all tokens in the source code as shown in\nFig. 2(e).\nWe may say that the local scope of attention is related\nto structural information processing and the global scope is\nrelated to sequential information processing. By controlling\nthe scope of attention or adopting the grey scope of attention\nas shown in Fig. 2(c) and Fig. 2(f), we can effectively\nmerge the structural and sequential information processing.\nWe propose a method of adaptive attention which combines\nthe advantages of two attention networks: graph attention\nnetwork reflecting structural information and self-attention\nnetwork reflecting sequential information. If we set the atten-\ntion scope narrow, the model may learn with high attention\nvalues to near nodes and low attention values to distant nodes.\nIf we set the attention scope wide, all the nodes will have\nequivalent attentions. We control the scope of attentions by\nadding biases which are learnable parameters. Since the scope\nis adjusted during training, each layer of the transformer\nencoder can adjust it roles: extracting structural information\nor sequential information. In our model, the lower layers\nof the transformer model have narrow scopes to learn the\nrelationships on nodes that are structurally close, as if they\nhave the role of graph attention network. The middle layers\nhave mid-size scopes to learn both of structural and sequential\ninformation. They may learn about blocks in a code and the\nsequential information of them. Finally, the higher layers\nlearn global contextual information about the entire code with\nlarge scopes of attention.\nAlso, some recent works have attempted to combine a\ngenerative model with a retrieval method, which searched\nfor similar codes and summaries in the databases [15], [26],\n[27]. These approaches aimed to improve the quality of the\nsummary by using extractive information which is additional\ncodes and summaries with high similarity to the original code.\nHowever, they separately processed and simply concatenated\nthem. They did not consider the relation between the origi-\nnal code and the similar code, or the relation between the orig-\ninal code and the summary of the similar code. When learning\nthe code representation from the code encoder, it is necessary\nto capture the features of the original code as well as those of\nthe similar code. Also, when generating the summary in the\ndecoder, it is necessary to have an organic fusion between the\ntwo representations, not just using the code and the summary\nof the similar code, and to properly reflect the keywords of\nthe similar code’s summary. We design an attention-based\naugmentation to reflect the relevance between the original\ncode and the retrieved code, and a fusion network to extract\nthe important keywords of similar summary from the original\ncode. Finally, we improve the performance of summarization\nby using dual copy mechanism from the original code tokens\nand the retrieved similar summary tokens.\nB. CONTRIBUTION\nOur focuses are not only ˆlocal and global attention ˆbut also\nˆsequential and structural processingîn a unified manner in a\nsingle Transformer. We consider both at the same time, not\nseparately.\nFirst, we modify AST for effectively capturing the struc-\nture of code. It is hard to learn the AST structure effectively\nbecause AST is a very sparse graph, a tree only connected\nbetween parents and children. So we propose a dense AST\nwith the shortest path distance as edge weights between all\nnodes. Then, we flatten the dense AST keeping edge infor-\nmation. The flattened AST is a sequence with edges. In a\nsequential viewpoint, the flattened AST has more sequential\ninformation than code, because not only tokens in code but\nalso abstract grammar nodes in AST are also presented. In a\nstructural viewpoint, the flattened AST effectively represent\nthe structure of code because the edge weights between two\nnodes are proportional to the distance between them in AST.\nSince the flattened AST is a sequence with structural\ninformation, we modify self-attention to process sequences\nconsidering structural information in a single Transformer.\n51156 VOLUME 11, 2023\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nFIGURE 2. Comparison of graph attention network, self-attention network and adaptive attention network. (a), (b) and (c) are the\nprocess of message passing for one node in an AST by graph attention network, self-attention network and adaptive attention\nnetwork (our proposed method), respectively. (d) graph attention network learns local and structural information about\nneighboring tokens, (e) self-attention network learns global and sequential information about all tokens, but (f) adaptive\nattention network learns adaptively both structural and sequential information using distance information between tokens.\nWe propose the adaptive Transformer, controlling the range\nof self-attention by the distance between two nodes. If two\nnodes are close in AST, each needs to pay more attention to\nthe other. If far, less attention. Since more attention will be\npaid to neighboring nodes, the adaptive attention will help\nTransformer easily capture structural information.\nBased on this idea, we calculate the self-attention based\non the distance of two nodes and a bias, a trainable param-\neter. The bias in the adaptive attention controls the scope\nof neighborhood of each layer in Transformer. If the bias\nis small, the range of neighborhood will be broadened, and\ntokens will give similar attention to all the other nodes. Thus,\na layer with a small bias tends to capture global structure.\nConversely, if the bias is large, the range of neighborhood\nwill be narrowed, and a layer tends to capture local structure.\nOur Contributions of this paper are as below.\n• We effectively unify the sequential and structural infor-\nmation into a structural sequence by flattening dense\nAST with shortest path distance.\n• We effectively process sequential and structural infor-\nmation in a unified manner by a single Transformer.\n• By adaptive self-attention, each layer can adaptively\ncapture local and global structural information.\n• To the best of our knowledge, we are the first to process\nthe sequential and structural information of code in a\nunified way.\nII. RELATED WORK\nA. CODE SUMMARIZATION\nMany works on source code summarization were mod-\neled as sequence-based approaches. Iyer et al. [1] proposed\nCode-NN, a Long Short Term Memory (LSTM) networks\nwith attention for code summarization and code retrieval\ntask. Allamanis et al. [2] used a neural convolutional atten-\ntional model to consider highly-structured source code text.\nHu et al. [4] proposed TL-CodeSum to summarize the source\ncode with the API Knowledge. Also, Chen et al. [28] pro-\nposed a novel multi-task approach to use API knowledge\nfor generating summaries from code. Wan et al. [24] used\na deep reinforcement learning framework to consider an\nAST structure and code snippets. Liu et al. [29] proposed a\nencoder-decoder model for automatically generating descrip-\ntions for pull requests. Wei et al. [5] used a dual training\nframework by training code summarization and code gener-\nation tasks. Also, Ye et al. [6] considered the probabilistic\ncorrelation between the two tasks. Ahmad et al. [7] pro-\nposed a Transformer model using a relative position. These\napproaches focused on the sequential information of the\nsource code, so the structural information was little consid-\nered about the relation between code tokens.\nAlso, some works tried to capture the structural infor-\nmation in the AST of code for source code summarization.\nHu et al. [8] proposed an RNN-based model using structural-\nbased traversal sequence as input, and Liang et al. [3] applied\na tree-based recursive neural network for representing the\nsyntax tree of code. Shi et al. [11] adopted Tree-LSTM,\nwhich is designed to capture both the syntactic and seman-\ntic structure of the source code. Harer et al. [12] proposed\nTree-Transformer to handle Tree-structured data. Bui et al.\n[30] adopted self-supervised learning mechanism to build\nsource code model by predicting subtrees from the con-\ntext of the ASTs. Alon et al. [21] leveraged the unique\nsyntactic structure of programming languages by sampling\npaths in the AST of a code snippet. Leclair et al. [14]\nVOLUME 11, 2023 51157\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nFIGURE 3. Overview of our proposed model. READSUM consists of four phases: embedding layer for code augmentation, code &\nsummary transformer encoders, summary transformer decoder and dual copy mechanism.\nproposed encoded AST using graph neural networks and\ntrained LSTM. Choi et al. [23] proposed a model that com-\nbines a GNN model and a Transformer model using modified\nAST. Shi et al. [16] tried to hierarchically split and reconstruct\nASTs using Recursive Neural Network for learning the rep-\nresentation of the complete AST. Wu et al. [19] proposed a\nTransformer model which incorporated multi-view structure\ninto attention mechanism. Guo et al. [31] proposed a Trans-\nformer model with two encoder architectures that considered\nthe structural embedding of the AST using both types of\nsource code and AST.\nThe retrieval-based method was used in Neural Machine\nTranslation (NMT) Zhang et al. [32] Xia et al. [33], but recent\nworks relied on both retrieved-based and generation-based\napproaches for the source code summarization. Zhang et al.\n[26] proposed retrieval-based approach using syntactic and\nsemantic similarity for source code summarization, and\nLiu et al. [15] proposed a hybrid GNN using a retrieval aug-\nmented graph method. Li et al. [27] leveraged the retrieve-\nand-edit framework to improve the performance for code\nsummarization.\nB. LARGE LANGUAGE MODEL FOR PROGRAM LANGUAGE\nThe success of pre-trained models based on the Trans-\nformer architecture in natural language generation has led\nto the development of methods for extending these tech-\nniques to programming language tasks. CodeBERT, a pre-\ntrained language model based on BERT [34], was proposed\nby [35], and incorporates both programming language and\nnatural language representations in the pre-training stage.\nTo incorporate code structure into CodeBERT, guo et al [36]\nproposed GraphCodeBERT, but these models have limited\neffectiveness in programming language tasks since they rely\nsolely on the transformer encoder for PL-NL representa-\ntion. In response, PLBART, a unified encoder-decoder model\nbased on BART [37], was proposed by [38] to support both\ncode understanding and generation tasks.\nIII. READSUM\nWe propose a novel model, READSUM, REtrieval-\naugmented ADaptive transformer for source code\nSUMmarization. Fig. 3 shows the overall architecture of\nour proposed model, READSUM. Our model consists of\nfour phases that are embedding layer for code augmen-\ntation, code & summary transformer encoders, summary\ntransformer decoder and dual copy mechanism.\nTo learn the relation between the original code and retrieval\ncode, we obtain augmented code representation using multi-\nhead self-attention between the original code’s AST sequence\nand the retrieved similar code’s AST sequence at the embed-\nding stage. Then the augmented representation is adaptively\ntrained on structural and sequential information through\nadaptive attention network in code transformer encoder, and\nthe retrieved similar summary representation is learned in\nsummary transformer encoder, respectively. To capture the\nrelevance between the augmented code representation and the\nsimilar summary representation at the transformer decoder,\nthey are trained by combining two representations for fusion.\nFinally, the dual copy mechanism is applied to directly reflect\nthe word of the original code and the retrieved similar sum-\nmary in the process of generating the summary. The following\nsubsections describe the problem definition of code summa-\nrization and each phase of the proposed method in detail.\nA. PROBLEM DEFINITION\nSuppose that we have a dataset of code and summary pairs.\nWe retrieve the most similar code C′ and summary S′ pair for\neach input code C based on the Levenshtein distance. Then,\nwe parse the code C as AST N to learn structural informa-\ntion as well as sequential information of the source code.\nIn order to adaptively learn both structural and sequential\ninformation at the embedding stage, we obtain the shortest\npath distance dij = φ(ni, nj) between all nodes in the AST\nN = {n1, n2, . . .}. Finally, we flatten the AST into a pre-order\nfirst sequence to train the transformer model.\n51158 VOLUME 11, 2023\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nB. EMBEDDING LAYER FOR CODE AUGMENTATION\nWe embed information on the similar code representation to\nthe original code token representation. To obtain the rele-\nvance of the similar code to the original code, we use multi-\nhead self-attention.\nLet the original code token representations and the\nretrieved similar code token representations obtained from\nthe word embedding layer be En and En′ , respectively.\nArel = MultiAtt(En, En′ , En′ ) (1)\nEnaug = En + zArel (2)\nwhere MultiAttn is multi-head self-attention and z is the\nsimilarity score between the original code and the retrieved\nsimilar code. Arel is the representation that means the rele-\nvance between the original code tokens and the similar code\ntokens. The augmented code representation Enaug is obtained\nby Equation 2 that combines the original code token represen-\ntation with the similar code token representation and the sim-\nilarity score. The obtained augmented code representations\nare used as the input on the code transformer encoder.\nC. CODE & SUMMARY TRANSFORMER ENCODERS\nREADSUM has two transformer encoders: code transformer\nencoder and summary transformer encoder. The structural\nand sequential information are learned to represent the rela-\ntion between the original and similar code in the code trans-\nformer encoder, and the sequential information of the similar\nsummary is learned in summary transformer encoder.\nOur code transformer encoder consists of 6 modified\ntransformer encoder layers. Each layer of the code trans-\nformer encoder is composed of two layers: adaptive attention\nnetwork and feed-forward network. We design a modified\nmulti-head self-attention, adaptive attention network, which\nadds a learnable bias term to self-attention to adaptively\ncapture structural and sequential information in the code. The\nadaptive attention network is calculated as follows:\nAdapAtt(Q, K, V , d) = softmax(QK⊺ + g(d))V (3)\ng(x) = 1\n√\n2πa\nexp(− x2\n2b2 ) + c (4)\nwhere AdapAtt is adaptive attention network, and d is the\nshortest path matrix between the query Q and key K, the\nbias term g(x) as a modified form of gaussian function, a,b\nand c are learnable embedding scalars. Here, the query and\nkey is the same as the original code’s AST nodes in the code\ntransformer encoder.\nOur summary transformer encoder also consists of 6 trans-\nformer encoder layers [39]. In the summary transformer\nencoder, the retrieved similar summary representations are\nlearned reflecting the similar summary context to be used in\nthe transformer decoder.\nD. SUMMARY TRANSFORMER DECODER\nThe transformer decoder aims to predict the target sum-\nmary of the original code by fusion of the augmented\ncode representation (code transformer encoder) and the\nretrieved similar summary representation (summary trans-\nformer encoder). The summary transformer decoder consists\nof 6 modified transformer decoder layers [39 ]. Each layer\nof the code transformer decoder is composed of three lay-\ners: masked multi-head self-attention, fusion network and\nfeed-forward network. We modify multi-head self-attention,\nfusion network, to learn the context by adding the sim-\nilarity score between the original code and the retrieved\ncode. First, we obtain each representation: the represen-\ntation between code transformer encoder and summary\ntransformer decoder, and the representation between sum-\nmary transformer encoder and summary transformer decoder,\nrespectively. Then, the two representations are fused via the\nfollowing equation:\nα1 = softmax(QK⊺\n1 ) (5)\nα2 = softmax(QK⊺\n2 ) (6)\nAtt1(Q, K1, V2) = α1V1 (7)\nAtt2(Q, K2, V2) = α2V2 (8)\nFusAtt = Att1 + zAtt2\n1 + z (9)\nwhere α1 is the attention score between the code transformer\nencoder and the summary transformer decoder, α2 is the\nattention score between the summary transformer encoder\nand the summary transformer decoder, FusAtt is fusion net-\nwork, and z is the similarity score. The fusion network of each\ntransformer decoder layer learns that the code encoder repre-\nsentations Att1 and summary encoder representation Att2 are\nfused by reflecting the similarity between the original code\nand the retrieved similar code.\nE. DUAL COPY MECHANISM\nFinally, the summary transformer decoder predicts the\nt-th word using the extractive information as well as the\nabstract information about the similar summary information.\nWe apply the modified copy mechanism [40], dual copy\nmechanism, to directly generate the word both from the\noriginal code tokens and from retrieved similar summary\ntokens for the extractive information. The probability of the\nword to be predicted at the t-th step is the sum of three\nscores: the attention score of the code transformer encoder\nα1, the attention score of the summary transformer encoder\nα2, and the t-th word prediction probability pt from the final\nrepresentation of the summary transformer decoder ht . The\ndual copy mechanism is as follows:\nPdual (w) = λ1α1 + λ2α2 + λ3pt (10)\nλ1 = σ(W1ht ) (11)\nλ2 = σ(W2ht ) (12)\nλ3 = 1 − (λ1 + λ2) (13)\nwhere σ is a sigmoid function, W1, W2 are linear layer\nweight matrices for the code and summary copy probabilities.\nREADSUM predicts the words of target summary by using\nVOLUME 11, 2023 51159\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\ncontext representations of the original code and its similar\ncode (abstractive information), and selecting directly original\ncode tokens and its retrieved similar summary tokens (extrac-\ntive information).\nIV. EXPERIMENT RESULTS\nA. SETUP\nWe evaluate using the benchmarks of the Java dataset [4]\nand the Python dataset [24]. However, as mentioned in [41],\nthere are duplication issues in the Java dataset, and the base-\nlines used different BLEU variants as the evaluation met-\nric. To ensure a fair comparison, we re-implement all the\nbaselines and conduct experiments on the same benchmark\ndatasets (including the deduplicated Java dataset). We also\nevaluate the baselines with the same evaluation metrics\n(including BLEU).\n1) DATASET\nWe evaluate our proposed model on three datasets: the dupli-\ncated Java dataset, the deduplicated Java dataset, and the\nPython dataset. The duplicated Java dataset is split into\n69,708/8,714/8,714 for train/valid/test. Since there are dupli-\ncate codes in the test set of the Java dataset, we removed them\nto create the deduplicated Java dataset split into 6,449 for test\nset. The Python dataset is split into 55,538/18,505/18,502 for\nthe train/valid/test.\nFor obtaining ASTs of the Java and Python dataset, we use\nthe javalang1 and ast2 library, respectively. Also, we tokenize\nthe source code and the AST to subtokens as the form Camel-\nCase and snake-case.\nFor a more detailed description of the datasets, please refer\nto Table 1.\n2) HYPERPARAMETER\nWe limit the maximum AST and summary length to 200 and\n50, and we set the training epoch to 100. The vocabulary sizes\nof code and summary are 50,000 and 30,000, respectively. For\ntraining the model, we use Adam optimizer [42].\nWe set the environment for training READSUM as\nfollows: 4 NVIDIA 2080 Ti GPUs, Ubuntu 16.04, Python 3.9\nand CUDA 10.2 version. The average training and inference\ntime for READSUM takes about 40 and 0.5 hours, respec-\ntively. READSUM has about 87 million parameters.\nThe description of the dataset and hyper-parameter for the\nexperiment is shown in Table 2.\n3) EVALUATION METRICS\nWe use 4 evaluation metrics, BLEU [43], METEOR [44],\nROUGE-L [45], and CIDEr [46] to measure the quality of\nthe generated summaries. We conduct all experiments with\nBLEU with smoothing 4 based on the version of NLTK\n(3.6.7) The details regarding the evaluation metrics is as\nfollows.\n1https://github.com/c2nes/javalang\n2https://github.com/python/cpython/blob/master/Lib/ast.py\nTABLE 1. Statistics of Java dataset [4] and Python dataset [24]. For\nobtaining their corresponding ASTs, we use thejavalang and ast library,\nrespectively.\nTABLE 2. Hyper-parameter of READSUM.\nBLEU [43] is Bilingual Evaluation Understudy to evaluate\nthe quality of generated code summaries. The formula of\ncomputing BLEU is as follows:\nBLEU − N = BP · exp\nN∑\nn=1\nωn log pn\nwhere pn is the geometric average of the modified n-gram\nprecisions, ωn is uniform weights 1/N and BP is the brevity\npenalty.\nMETEOR [44] is used to measure the correlation between\nthe metric scores and human judgments of translation quality.\nSo unigram precision (P) and unigram recall (R) are com-\nputed and combined via a harmonic-mean. The METEOR\nscore is computed as follows:\nMETEOR = (1 − γ · fragβ) · P · R\nα · P + (1 − α) · P\nwhere frag is the fragmentation fraction. α, β, and γ are three\npenalty parameters whose default values are 0.9, 3.0, and 0.5,\nrespectively.\nROUGE-L [45] is used to apply Longest Common\nSubsequence in sumarization evaluation. ROUGE-L used\nLCS-based F-measure to estimate the similarity between two\nsummaries X of length m and Y of length n, assuming X is\na reference summary sentence and Y is a candidate summary\nsentence, as follows:\nRlcs = LCS(X, Y )\nm , Plcs = LCS(X, Y )\nn\n51160 VOLUME 11, 2023\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nTABLE 3. Comparison of READSUM with the baseline models on the duplicated Java datasets [4]. The improvements of READSUM over all baselines are\nstatistically significant withp < 0.05.\nFlcs = (1 + β2)RlcsPlcs\nRlcs + β2Plcs\nwhere β = Plcs/Rlcs and Flcs is the value of ROUGE-L.\nCIDEr [46] is used to consider the frequency of n-grams\nfor automatically computing consensus. CIDErn score for\nn-grams of length n is computed using the average cosine\nsimilarity between the candidate sentence and the reference\nsentences, which accounts for both precision and recall:\nCIDErn(ci, Si) = 1\nm\n∑\nj\ngn(ci) · gn(sij)\n||gn(ci)||||gn(sij)||\nCIDEr(ci, Si) =\nN∑\nn=1\nωnCIDErn(ci, Si)\nwhere gn(ci) is a vector formed by gk (ci) corresponding to all\nn-grams of length n, ||gn(ci)|| is the magnitude of the vector\ngn(ci), and ωn is uniform weights 1/N .\n4) BASELINES\nWe experiment with reproducible code summarization mod-\nels as our baselines:\n• Code-NN [1] is a code summarization model that uses\nLSTM networks with an attention mechanism to gener-\nate summaries from code.\n• RL+Hybrid2Seq [24] is a reinforcement learning-based\ncode summarization model that combines an LSTM\nencoder with an AST-based LSTM to improve code\nsummarization.\n• DeepCom [8] is a neural code generation model that uses\na structure-based traversal method to obtain a sequence\nof tokens from an AST.\n• TL-CodeSUM [4] is a transfer learning-based code sum-\nmarization model that introduces API information dur-\ning comment generation, even without prior knowledge\nof the API.\n• Astattgru [22] is an attention-based neural model for\ncode summarization that utilizes an AST structure to\ngenerate summaries.\n• NCS (code) [7] is the first model to use the Transformer\narchitecture and includes a copying mechanism for gen-\nerating words from a vocabulary.\n• NCS (AST) is an alternative version of the NCS model\nthat utilizes abstract syntax trees as input instead of code.\n• Rencos [26] is a neural model based on a retrieval\nmethod that uses a sequence-to-sequence approach with\na combination of syntax and semantic embeddings.\n• CAST [16] is a neural model for code generation that\nuses a graph-based attention mechanism to generate\ncode from natural language descriptions.\n• CodeBERT [35] is a pre-trained language model for\ncode that uses a transformer-based neural network\narchitecture similar to BERT [34], optimized for pro-\ngramming tasks.\n• PLBART [38] is a pre-trained language model for pro-\ngramming tasks that combines BART [37] with an\nencoder-decoder architecture and is optimized for both\ncode understanding and generation.\nWe tried to re-implement and report on all available base-\nline models for code summarization, but we were unable\nto do so due to some unfairness issues and the absence\nof open-source codes for some works. For example, [16]\nand [24] removed code samples that were not parsed with\nAntlr parser, so they cannot be evaluated with the same\ndatasets as other baselines. Some works performed prepro-\ncessing specialized to a specific program language, and con-\nducted experiments on either the Java or the Python dataset.\nSo, the baselines for the Java and the Python are different.\nWe evaluate all reproducible baselines with the same evalua-\ntion metrics on the same datasets to ensure fair comparisons\nacross all baselines as much as possible.\nB. QUANTITATIVE RESULT\n1) OVERALL RESULT\nTable 3 shows the overall performance of the models on\nboth the duplicated Java benchmark dataset. The READSUM\nmodel achieved state-of-the-art performance on all evalua-\ntion metrics for the Java dataset. Compared to other models\nsuch as Code-NN, DeepCom, TL-CodeSUM, and ASTattgru,\nthe NCS model based on the Transformer architecture\nVOLUME 11, 2023 51161\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nTABLE 4. Comparison of READSUM with the baseline models on the deduplicated Java datasets. The improvements of READSUM over all baselines are\nstatistically significant withp < 0.05.\nTABLE 5. Comparison of READSUM with the baseline models on Python dataset [24]. The improvements of READSUM over all baselines are statistically\nsignificant with p < 0.05 except BLEU-4 of CodeBERT.\ndemonstrated much better performance, indicating the advan-\ntage of using a transformer-based model to capture sequence\ninformation for code summarization. Furthermore, the\nRencos model, which utilizes a retrieval method, showed\nbetter performance than other baseline models. These results\nhighlight the effectiveness of transformer-based models and\nretrieval methods in improving code summarization. Com-\npared to CAST [16], which used two types of code and\nAST as input data, READSUM shows good performance\nusing AST tokens and retrieved summary tokens. Further-\nmore, READSUM is compared with Rencos [26], which is\none of the retrieval methods. Lastly, our approach is com-\npared with CodeBERT and PLBART, strong pre-trained pro-\ngram language models. READSUM performs better than the\npre-trained models trained on large code data.\nTable 4 shows the overall performance of the models\non the deduplicated Java dataset. The READSUM model\nachieved state-of-the-art performance on all evaluation met-\nrics for the Java dataset. Similar to the previous result,\nREADSUM achieved state-of-the-art performance on the\ndeduplicated Java dataset. Evaluating a test dataset that has\nnot been seen in the training dataset is a challenging task,\nbut READSUM demonstrated superior performance com-\npared to other sequence-to-sequence models. Furthermore,\nREADSUM outperformed large language models such as\nCodeBERT and PLBART. The improvements of READSUM\nover all baselines are statistically significant with p < 0.05.\nREADSUM demonstrates its ability to effectively capture\nTABLE 6. Ablation study on each component of READSUM.\nboth the structural and sequence information of code to gen-\nerate high-quality summaries.\nTable 5 shows the performance of the models on the Python\ndataset. READSUM outperforms all the baselines except for\nCodeBERT’s BLEU-4. We believe that CodeBERT has good\nperformance of BLEU score because it resembles the data\nused for fine-tuning after pre-training. However, READSUM\nhad better overall performance than simply having a high\nBLEU-4 score. Our READSUM showed that focusing on\nthe retrieved similar summary rather than the similar code\nas retrieval information helps to predict more important and\nsimilar words in generating the summary.\n51162 VOLUME 11, 2023\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nFIGURE 4. Analysis of adaptive attention network. (a), (b) and (c) show the learned bias terms in 8 multi-heads for each of 2nd layer, 4th\nlayer, and 6th layer in code transformer encoder. The results show that the adaptive attention network is adaptively trained on the tokens\nto focus within each multi-head self-attention.\nTABLE 7. An qualitative example on the Java and Python dataset.\n2) ABLATION STUDY\nWe perform an ablation study by validating the effectiveness\nof each component of READSUM. The w/o AdapAttis our\nmethod that transformer encoder consists of vanilla multi-\nhead self-attention, the w/o FusAttis our method that the sim-\nilarity score z is 1 in fusion network, and the w/o DualCopyis\nour method with a copy mechanism that does only use AST\ncode tokens.\nTable 6 shows the experiment results of the ablation study\nfor each of the three cases. Compared with the w/o Ada-\npAtt, READSUM slightly increases BLEU, METEOR, and\nROUGE-L scores for Java and Python datasets, respectively.\nSince the general multi-head self-attention method calcu-\nlates the same attention score between all tokens, it can\nbe seen that the performance is low due to insufficient\nlearning about the AST structure. We confirm that the\nAST representation is learned both sequential and struc-\ntural information by adaptive attention. Next, in the w/o\nFusAtt, the scores of BLEU/METEOR/ROUGE-L on Java\nare slightly increased, but the BLEU/METEOR/ROUGE-L\nscores on Python dataset are significantly increased by\n0.35/0.62/0.40, respectively. This is because the code rep-\nresentation learned from the code transformer encoder and\nthe retrieved similar summary representation learned from\nthe summary transformer encoder are fused in the decoder\nphase, and the similarity between the original code and\nretrieved similar summary is important to reflect on the\ndecoder representation. Finally, in the w/o DualCopy, the\nscores of BLEU/METEOR/ROUGE-L are increased by\n0.39/0.43/0.25 in the Java dataset and 0.51/0.7/0.18 in\nthe Python dataset. The hidden state of transformer sum-\nmary decoder is obtained by fusion of augmented code\nVOLUME 11, 2023 51163\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\nrepresentation from code transformer encoder and similar\nsummary representation from summary transformer encoder.\nSo the dual copy mechanism generates a summary with high\nquality by considering both the abstractive method in which\nthe relation between the augmented code representations\nand the similar summary representations are fused, and the\nextractive method in which original code tokens and retrieved\nsimilar summary tokens are directly extracted.\nC. QUALITATIVE RESULT\n1) ANALYSIS ON ADAPTIVE ATTENTION\nWe analyze how adaptive attention learns structural and\nsequential information in our proposed model. Fig. IV-B1\nshows the learned Gaussian function in 8 multi-heads in 2nd\nlayer, 4th layer and 6th layer of code transformer encoder.\nIn the 2nd layer of the transformer encoder, the Gaussian\nfunction value becomes larger for tokens with relatively close\nshortest paths, so that the attention value becomes larger\nas shown in Fig. IV-B1(a). In 4th layer, the attention val-\nues in all multi-heads are calculated equally for all shortest\npaths, so adaptive attention network learns to focus on the\nsequential information of the AST as shown in Fig. IV-B1(b).\nIn Fig. IV-B1(c), some bias terms in multi-heads (Red and\nOrange) learn information about itself by increasing attention\nto its own token whose shortest path is 0. In other multi-heads\n(Other colors), it learns to have the same attention value for\nall shortest paths. Therefore, we show that adaptive attention\nnetwork is adaptively trained to focus on which information\nwithin each multi-head self-attention.\n2) COMPARISON WITH THE BASELINES\nWe show one example generated from READSUM, NCS,\nRencos, CodeBERT and PLBART on the Java dataset and\nPython dataset as shown in Table 7. In an example of the\nJava dataset, NCS and Rencos generate sentences of the\nform ‘‘create project within’’. Furthermore, CodeBERT and\nPLBART generate summaries that do not necessarily follow\nthe format of the Retrieved Summary, instead producing very\ngeneral sentence forms. These models, having learned many\ndifferent types of sentences during pre-training, can generate\nnatural summaries, but may not reflect important keywords or\nother specific information found in the source code. However,\nREADSUM generates the sentence similar to the ground\ntruth, because it copies the word ‘‘cluster’’in original code\ntokens and the words ‘‘in’’ and ‘‘project’’ in the retrieved\nsimilar summary tokens through the dual copy mechanism.\nV. CONCLUSION\nWe proposed a novel model for code summarization,\nREADSUM, REtrieval-augmented ADaptive transformer\nfor source code SUMmarization. READSUM effectively\ncombined an abstractive approach learning both the struc-\ntural and sequential information of the source code, and an\nextractive approach for increasing the frequency of important\nkeywords. For adaptively learning structural and sequential\ninformation, we modified the self-attention network in the\ntransformer encoder by adding a bias term as a learnable\nGaussian function with the distance between tokens. Also,\nWe demonstrated through analysis of the adaptive attention\nnetwork that our self-attention is learned adaptively for each\nlayer. Finally, we improved the performance of summariza-\ntion by using dual copy mechanism from the original code\ntokens and the retrieved similar summary words. We showed\nthe superiority of READSUM for source code summarization\nthrough various experiments and human evaluation.\nREFERENCES\n[1] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‘‘Summarizing source\ncode using a neural attention model,’’ in Proc. Annu. Meeting Assoc.\nComput. Linguistics, vol. 1, Berlin, Germany, 2016, pp. 2073–2083.\n[2] M. Allamanis, H. Peng, and C. Sutton, ‘‘A convolutional attention network\nfor extreme summarization of source code,’’ in Proc. 33rd Int. Conf. Mach.\nLearn. B. M. Florina and K. Q. Weinberger, Eds. New York, NY , USA,\nJun. 2016, pp. 2091–2100.\n[3] Y . Liang and K. Q. Zhu, ‘‘Automatic generation of text descriptive com-\nments for code blocks,’’ in Proc. 32nd AAAI Conf. Artif. Intell. (AAAI),\n30th Innov. Appl. Artif. Intell. (IAAI), 8th AAAI Symp. Educ. Adv. Artif.\nIntell., A. S. McIlraith and Q. K. Weinberger, Eds. New Orleans, LA, USA,\nFeb. 2018, pp. 5229–5236.\n[4] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, ‘‘Summarizing source\ncode with transferred API knowledge,’’ in Proc. 27th Int. Joint Conf. Artif.\nIntell., Stockholm, Sweden, Jul. 2018, pp. 2269–2275.\n[5] B. Wei, G. Li, X. Xia, Z. Fu, and Z. Jin, ‘‘Code generation as a dual\ntask of code summarization,’’ in Proc. Adv. Neural Inf. Process. Syst.,\nH. M. Wallach, H. Larochelle, A. Beygelzimer, F. A. Buc, E. B. Fox, and\nR. Garnett, Eds. Vancouver, BC, Canada, 2019, pp. 6559–6569.\n[6] W. Ye, R. Xie, J. Zhang, T. Hu, X. Wang, and S. Zhang, ‘‘Leveraging code\ngeneration to improve code retrieval and summarization via dual learning,’’\nin Proc. Web Conf., Y . Huang, I. King, T. Liu, and M. van Steen. Taipei,\nTaiwan, Apr. 2020, pp. 2309–2319.\n[7] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‘‘A transformer-\nbased approach for source code summarization,’’ in Proc. 58th Annu.\nMeeting Assoc. Comput. Linguistics, 2020, pp. 4998–5007.\n[8] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‘‘Deep code comment generation,’’\nin Proc. 26th Conf. Program Comprehension, May 2018, pp. 200–210.\n[9] P. Fernandes, M. Allamanis, and M. Brockschmidt, ‘‘Structured neural\nsummarization,’’ in Proc. 7th Int. Conf. Learn. Represent., New Orleans,\nLA, USA, May 2019, pp. 1–18.\n[10] K. Zhang, W. Wang, H. Zhang, G. Li, and Z. Jin, ‘‘Learning to rep-\nresent programs with heterogeneous graphs,’’ in Proc. 30th IEEE/ACM\nInt. Conf. Program Comprehension, Vancouver, BC, Canada, May 2022,\npp. 378–389.\n[11] Y . Shido, Y . Kobayashi, A. Yamamoto, A. Miyamoto, and T. Matsumura,\n‘‘Automatic source code summarization with extended tree-LSTM,’’ 2019,\narXiv:1906.08094.\n[12] J. Harer, C. Reale, and P. Chin, ‘‘Tree-transformer: A transformer-based\nmethod for correction of tree-structured data,’’ 2019, arXiv:1908.00449.\n[13] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, ‘‘A novel\nneural source code representation based on abstract syntax tree,’’ in Proc.\nIEEE/ACM 41st Int. Conf. Softw. Eng. (ICSE), May 2019, pp. 783–794.\n[14] A. LeClair, S. Haque, L. Wu, and C. McMillan, ‘‘Improved code summa-\nrization via a graph neural network,’’ 2020, arXiv:2004.02843.\n[15] S. Liu, Y . Chen, X. Xie, J. K. Siow, and Y . Liu, ‘‘Retrieval-augmented\ngeneration for code summarization via hybrid GNN,’’ in Proc. 9th Int.\nConf. Learn. Represent., 2021, pp. 1–16.\n[16] E. Shi, Y . Wang, L. Du, H. Zhang, S. Han, D. Zhang, and H. Sun, ‘‘CAST:\nEnhancing code summarization with hierarchical splitting and reconstruc-\ntion of abstract syntax trees,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., Punta Cana, Dominican Republic, 2021, pp. 4053–4062.\n[17] C. Lin, Z. Ouyang, J. Zhuang, J. Chen, H. Li, and R. Wu, ‘‘Improving code\nsummarization with block-wise abstract syntax tree splitting,’’ in Proc.\nIEEE/ACM 29th Int. Conf. Program Comprehension (ICPC), May 2021,\npp. 184–195.\n[18] Y . Wang and H. Li, ‘‘Code completion by modeling flattened abstract\nsyntax trees as graphs,’’ in Proc. AAAI, 2021, pp. 14015–14023.\n[19] H. Wu, H. Zhao, and M. Zhang, ‘‘Code summarization with structure-\ninduced transformer,’’ in Proc. Findings Assoc. Comput. Linguistics, 2021,\npp. 1078–1090.\n51164 VOLUME 11, 2023\nY. Choi et al.: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization\n[20] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‘‘code2vec: Learning\ndistributed representations of code,’’ 2018, arXiv:1803.09473.\n[21] U. Alon, S. Brody, O. Levy, and E. Yahav, ‘‘code2seq: Generating\nsequences from structured representations of code,’’ in Proc. 7th Int. Conf.\nLearn. Represent., New Orleans, LA, USA, May 2019, pp. 1–22.\n[22] A. LeClair, S. Jiang, and C. Mcmillan, ‘‘A neural model for generating\nnatural language summaries of program subroutines,’’ in Proc. IEEE/ACM\n41st Int. Conf. Softw. Eng. (ICSE), May 2019, pp. 795–806.\n[23] Y . Choi, J. Bak, C. Na, and J.-H. Lee, ‘‘Learning sequential and structural\ninformation for source code summarization,’’ in Proc. Findings Assoc.\nComput. Linguistics, 2021, pp. 2842–2851.\n[24] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, ‘‘Improving\nautomatic source code summarization via deep reinforcement learning,’’\nin Proc. 33rd ACM/IEEE Int. Conf. Automated Softw. Eng., Sep. 2018,\npp. 397–407.\n[25] B. Wei, Y . Li, G. Li, X. Xia, and Z. Jin, ‘‘Retrieve and refine: Exemplar-\nbased neural comment generation,’’ in Proc. 34th IEEE/ACM Int. Conf.\nAutomated Softw. Eng. (ASE), Nov. 2019, pp. 349–360.\n[26] J. Zhang, X. Wang, H. Zhang, H. Sun, and X. Liu, ‘‘Retrieval-based neural\nsource code summarization,’’ in Proc. ACM/IEEE 42nd Int. Conf. Softw.\nEng., Jun. 2020, pp. 1385–1397.\n[27] J. Li, Y . Li, G. Li, X. Hu, X. Xia, and Z. Jin, ‘‘EditSum: A retrieve-and-edit\nframework for source code summarization,’’ in Proc. 36th IEEE/ACM Int.\nConf. Automated Softw. Eng. (ASE), Nov. 2021, pp. 155–166.\n[28] F. Chen, M. Kim, and J. Choo, ‘‘Novel natural language summarization\nof program code via leveraging multiple input representations,’’ in Proc.\nFindings Assoc. Comput. Linguistics, Punta Cana, Dominican Republic,\n2021, pp. 2510–2520.\n[29] Z. Liu, X. Xia, C. Treude, D. Lo, and S. Li, ‘‘Automatic generation of pull\nrequest descriptions,’’ inProc. 34th IEEE/ACM Int. Conf. Automated Softw.\nEng. (ASE), Nov. 2019, pp. 176–188.\n[30] N. D. Q. Bui, Y . Yu, and L. Jiang, ‘‘InferCode: Self-supervised learning\nof code representations by predicting subtrees,’’ in Proc. IEEE/ACM 43rd\nInt. Conf. Softw. Eng. (ICSE), May 2021, pp. 1186–1197.\n[31] J. Guo, J. Liu, Y . Wan, L. Li, and P. Zhou, ‘‘Modeling hierarchical syntax\nstructure with triplet position for source code summarization,’’ in Proc.\n60th Annu. Meeting Assoc. Comput. Linguistics, Dublin, Ireland, 2022,\npp. 486–500.\n[32] J. Zhang, M. Utiyama, E. Sumita, G. Neubig, and S. Nakamura, ‘‘Guiding\nneural machine translation with retrieved translation pieces,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, New Orleans,\nLouisiana, 2018, pp. 1325–1335.\n[33] M. Xia, G. Huang, L. Liu, and S. Shi, ‘‘Graph based translation memory for\nneural machine translation,’’ in Proc. 33rd AAAI Conf. Artif. Intell. (AAAI),\n31st Innov. Appl. Artif. Intell. Conf. (IAAI), 9th AAAI Symp. Educ. Adv.\nArtif. Intell., Honolulu, HI, USA, 2019, pp. 7297–7304.\n[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol., vol. 1, Jun. 2019, pp. 4171–4186.\n[35] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,\nT. Liu, D. Jiang, and M. Zhou, ‘‘CodeBERT: A pre-trained model for\nprogramming and natural languages,’’ in Proc. Findings Assoc. Comput.\nLinguistics, 2020, pp. 1536–1547.\n[36] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,\nA. Svyatkovskiy, S. Fu, M. Tufano, S. K. Deng, C. Clement, D. Drain,\nN. Sundaresan, J. Yin, D. Jiang, and M. Zhou, ‘‘GraphCodeBERT: Pre-\ntraining code representations with data flow,’’ 2020, arXiv:2009.08366.\n[37] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV . Stoyanov, and L. Zettlemoyer, ‘‘BART: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,’’ inProc. 58th Annu. Meeting Assoc. Comput. Linguistics,\nJul. 2020, pp. 7871–7880.\n[38] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‘‘Unified pre-\ntraining for program understanding and generation,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Human Lang. Technol., 2021,\npp. 2655–2668.\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, N. Aidan\nGomez, L. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in\nProc. Annu. Conf. Neural Inf. Process. Syst., I. Guyon, U. von Luxburg,\nS. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, R. Garnett,\nEds. Long Beach, CA, USA, 2017, pp. 5998–6008.\n[40] A. See, P. J. Liu, and C. D. Manning, ‘‘Get to the point: Summarization with\npointer-generator networks,’’ in Proc. 55th Annu. Meeting Assoc. Comput.\nLinguistics, vol. 1, Vancouver, BC, Canada, Jul. 2017, pp. 1073–1083.\n[41] E. Shi, Y . Wang, L. Du, J. Chen, S. Han, H. Zhang, D. Zhang, and H. Sun,\n‘‘On the evaluation of neural code summarization,’’ in Proc. 44th Int. Conf.\nSoftw. Eng., New York, NY , USA, May 2022, pp. 1597–1608.\n[42] P. D. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’ in\nProc. 3rd Int. Conf. Learn. Represent., Y . Bengio and Y . LeCun, Eds. San\nDiego, CA, USA, May 2015, pp. 1–15.\n[43] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‘‘BLEU: A method for\nautomatic evaluation of machine translation,’’ in Proc. 40th Annu. Meeting\nAssoc. Comput. Linguistics, Philadelphia, PA, USA, 2001, pp. 311–318.\n[44] S. Banerjee and A. Lavie, ‘‘METEOR: An automatic metric for MT\nevaluation with improved correlation with human judgments,’’ in Proc.\nACL Workshop Intrinsic Extrinsic Eval. Measures Mach. Transl. Summa-\nrization, Ann Arbor, Michigan, 2005, pp. 65–72.\n[45] C.-Y . Lin and E. Hovy, ‘‘Manual and automatic evaluation of sum-\nmaries,’’ in Proc. Workshop Autom. Summarization, Barcelona, Spain,\n2004, pp. 74–81.\n[46] R. Vedantam, C. L. Zitnick, and D. Parikh, ‘‘CIDEr: Consensus-based\nimage description evaluation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 4566–4575.\nYUNSEOK CHOI received the B.S. degree in\ncomputer engineering from Sungkyunkwan Uni-\nversity, Suwon, South Korea, in 2015, where he\nis currently pursuing the Ph.D. degree supervised\nby Prof. Jee-Hyong Lee. His research interests\ninclude natural language processing and code\nintelligence.\nCHEOLWON NAreceived the B.S. degree in com-\nputer engineering from Kunsan National Univer-\nsity, Kunsan, South Korea, in 2020. He is currently\npursuing the Ph.D. degree with Sungkyunkwan\nUniversity, Suwon, South Korea, supervised by\nProf. Jee-Hyong Lee. His research interests\ninclude natural language processing and code\nintelligence.\nHYOJUN KIM received the B.S. degree in com-\nputer engineering from Kyunghee University,\nSuwon, South Korea, in 2020. He is currently\npursuing the master’s degree with Sungkyunkwan\nUniversity, Suwon, supervised by Prof. Jee-Hyong\nLee. His research interests include natural lan-\nguage processing and code intelligence.\nJEE-HYONG LEE received the B.S., M.S., and\nPh.D. degrees in computer science from the Korea\nAdvanced Institute of Science and Technology,\nDaejeon, South Korea, in 1993, 1995, and 1999,\nrespectively. From 2000 to 2002, he was an Inter-\nnational Fellow with SRI International, USA.\nIn 2002, he joined Sungkyunkwan University,\nSuwon, South Korea, as a Faculty Member. His\nresearch interests include fuzzy theory and appli-\ncations, intelligent systems, and machine learning.\nVOLUME 11, 2023 51165",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8333901166915894
    },
    {
      "name": "Computer science",
      "score": 0.7417638897895813
    },
    {
      "name": "Transformer",
      "score": 0.42732158303260803
    },
    {
      "name": "Information retrieval",
      "score": 0.3837107717990875
    },
    {
      "name": "Electrical engineering",
      "score": 0.10172641277313232
    },
    {
      "name": "Voltage",
      "score": 0.0716264545917511
    },
    {
      "name": "Engineering",
      "score": 0.058691054582595825
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I848706",
      "name": "Sungkyunkwan University",
      "country": "KR"
    }
  ],
  "cited_by": 9
}