{
  "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
  "url": "https://openalex.org/W4389520033",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5059657111",
      "name": "Qingru Zhang",
      "affiliations": [
        "Amazon (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5103133511",
      "name": "Dhananjay Ram",
      "affiliations": [
        "Amazon (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5070161118",
      "name": "Cole Hawkins",
      "affiliations": [
        "Amazon (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5025057192",
      "name": "Sheng Zha",
      "affiliations": [
        "Amazon (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5109592386",
      "name": "Tuo Zhao",
      "affiliations": [
        "Amazon (United States)",
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281758439",
    "https://openalex.org/W3064840847",
    "https://openalex.org/W4311991547",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4296932804",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3209824846",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4290802752",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W4320737106",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221145950",
    "https://openalex.org/W3209374680",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W2963926728"
  ],
  "abstract": "Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost ‚Äì quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with mixed attention spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on natural language modeling and generation tasks show that a decoder-only MASFormer model of 1.3B parameters can achieve competitive performance to vanilla transformers with full attention while significantly reducing computational cost (up to 75%). Additionally, we investigate the effectiveness of continual training with long sequence data and how sequence length impacts downstream generation performance, which may be of independent interest.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2775‚Äì2786\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nEfficient Long-Range Transformers: You Need to Attend More,\nbut Not Necessarily at Every Layer\nQingru Zhang‚Ä†‚àó, Dhananjay Ram ‚ãÑ, Cole Hawkins ‚ãÑ, Sheng Zha ‚ãÑ, Tuo Zhao ‚Ä†\n‚Ä†Georgia Institute of Technology ‚ãÑAmazon Web Service\n{qingru.zhang,tourzhao}@gatech.edu\n{radhna,colehawk,zhasheng}@amazon.com\nAbstract\nPretrained transformer models have demon-\nstrated remarkable performance across vari-\nous natural language processing tasks. These\nmodels leverage the attention mechanism to\ncapture long- and short-range dependencies in\nthe sequence. However, the (full) attention\nmechanism incurs high computational cost ‚Äì\nquadratic in the sequence length, which is not\naffordable in tasks with long sequences, e.g.,\ninputs with 8k tokens. Although sparse at-\ntention can be used to improve computational\nefficiency, as suggested in existing work, it\nhas limited modeling capacity and often fails\nto capture complicated dependencies in long\nsequences. To tackle this challenge, we pro-\npose MASFormer, an easy-to-implement trans-\nformer variant with Mixed Attention Spans.\nSpecifically, MASFormer is equipped with full\nattention to capture long-range dependencies,\nbut only at a small number of layers. For the\nremaining layers, MASformer only employs\nsparse attention to capture short-range depen-\ndencies. Our experiments on natural language\nmodeling and generation tasks show that a\ndecoder-only MASFormer model of 1.3B pa-\nrameters can achieve competitive performance\nto vanilla transformers with full attention while\nsignificantly reducing computational cost (up\nto 75%). Additionally, we investigate the ef-\nfectiveness of continual training with long se-\nquence data and how sequence length impacts\ndownstream generation performance, which\nmay be of independent interest.\n1 Introduction\nPre-trained transformer models have manifested\nsuperior performance in various natural language\nprocessing tasks such as natural language modeling\n(NLM) (Dai et al., 2019; Radford et al., 2019), natu-\nral language generation (NLG) (Brown et al., 2020)\nand natural language understanding (NLU) (De-\nvlin et al., 2019; Liu et al., 2019; He et al., 2021b).\n‚àó Work was done during Qingru Zhang‚Äôs internship at\nAmazon Web Service.\nThese models leverage the attention mechanism\n(Vaswani et al., 2017) to compute the dependency\nscore for each pair of tokens in an input sequence.\nSome practical tasks require these transformer\nmodels to handle long-sequence inputs like 8k to-\nkens. For example, chatbot systems gather long-\nterm contexts of user interactions to generate infor-\nmative texts (Roller et al., 2021). Summarization\nfor news, government reports, and academic papers\nrequest models to take inputs of long sequences to\ngenerate comprehensive summaries (Shaham et al.,\n2022), otherwise models often miss important in-\nformation. Note that typical transformer models\napply full attention to capture token dependencies\npair-wise. It leads to a quadratic time and space\ncomplexity w.r.t. input length. However, such a\ncomplexity is prohibitive for long sequences. In\nparticular, it incurs massive memory consumption\nduring the back propagation. For example, a trans-\nformer model with 250M parameters consumes\nover 80G GPU memory when sequence length is\n8k (Zuo et al., 2022).\nTo address this scalability issue, various ap-\nproaches have been proposed to reduce the com-\nplexity. One approach is sparse attention, which\nrestricts each token to attend a subset of tokens\nbased on predefined sparsity patterns (Beltagy et al.,\n2020; Zaheer et al., 2020; Ainslie et al., 2020). For\ninstance, block sparse attention(Kitaev et al., 2020;\nMa et al., 2023) divides the input sequence into sev-\neral blocks, and only intra-block attention is per-\nformed. Besides, sliding-window attention(Belt-\nagy et al., 2020; Zaheer et al., 2020; Ainslie et al.,\n2020) allows each token to attend to its neighboring\ntokens within a sliding window. These methods,\nthough reducing the complexity of full attention,\ncannot sufficiently capture long-range dependen-\ncies. Other variants, such as kernel approximation\n(Peng et al., 2021) and low-rank approximation\n(Wang et al., 2020; Chen et al., 2021) methods,\nshare the similar spirit and drawbacks. To com-\n2775\npensate for the lack of long-range dependencies,\nLongT5 (Guo et al., 2021) introduces global tokens\nthat are obtained by average pooling on every block\nof tokens (Ainslie et al., 2020). However, the block\npooling operations can weaken the signal of crucial\ntokens and prevent the long-range dependencies\nfrom being detected.\nIn addition to these methods, state space mod-\nels (SSMs) prespecify global dependency patterns\nto capture the long-range dependencies only (Gu\net al., 2020, 2021; Li et al., 2022; Zuo et al., 2022;\nMa et al., 2023; Smith et al., 2023). These models\ncan be regarded as linear recurrent neural networks\nwith specifically designed fixed weights. As tai-\nlored for global dependencies, SSMs fail to effec-\ntively capture local dependencies. In order to com-\nbine both local and global dependencies, SPADE\n(Zuo et al., 2022) and MEGA (Ma et al., 2023) aug-\nment SSM layers into transformer layers equipped\nwith local attention. However, state space meth-\nods require sophisticated implementation, and of-\nten encounter computational instability during the\nback propagation, especially when scaling up to\nlarge model size (Gupta et al., 2022). SPADE and\nMEGA hence inherit these drawbacks.\nNote that the aforementioned methods apply\nsame attention mechanism for every layer. We chal-\nlenge this conventional wisdom and propose a trans-\nformer variant ‚Äì MASFormer (Mixed Attention\nSpan transFormer). MASFormer utilizes full at-\ntention only at a subset of layers whereas employs\nsparse attention at the remaining layers. Our design\nis motivated by the phenomenon ‚Äì that most con-\ntexts in NLP data display a great deal of locality of\nreference (Zaheer et al., 2020; Beltagy et al., 2020).\nThat is, most of information about a token can be\nderived from its neighboring tokens. In contrast,\nlong-range dependencies among tokens are sparse\nand infrequent. Consider an academic paper as an\nexample. Within a paragraph, there exist numerous\nshort-term dependencies. Neighboring tokens are\nclosely connected to convey meaningful semantics.\nAcross paragraphs, there can be a small number of\nlong-range dependencies. For example, tokens as-\nsociated to the primary theme of the paper exhibit\nrare and weak dependencies across a long span.\nSince long-range dependencies occur much less\nfrequently, a few layers of full attention are ade-\nquate to capture them. In stark contrast, short-term\ndependencies are more frequent, necessitating local\nattention in the majority of layers to fully extract\nthese signals.\nTo demonstrate the effectiveness of MASFormer,\nWe conduct experiments on natural language mod-\neling (ArXiv and PubMed Cohan et al. (2018))\nand natural language generation (ArXiv, Cohan\net al. (2018) and SCROLLS, Shaham et al. (2022))\ntasks. Specifically, we compare the performance of\nMASFormer to other attention methods using a pre-\ntrained GPT-2 model (Radford et al., 2019) of 1.3\nbillion parameters. Our empirical results demon-\nstrate that MASFormer consistently outperforms\nbaseline methods across different attention cost\n(i.e. the total number of computed attention scores).\nIn particular, MASFormer can achieve comparable\nperformance to full attention while significantly re-\nducing the computational cost. For example, with\n27% of its attention cost, MASFormer achieves a\nclose R2 score as full attention on QMSUM dataset.\nWe also make additional discoveries with MAS-\nFormer, which are of independent interest. Firstly,\nwe investigate the effectiveness of continual train-\ning for long sequence modeling. Many publicly\navailable models are pre-trained with sequences\nshorter than 2048, and often fail to perform well on\nlonger sequences (e.g. 8k/16k tokens). To bridge\nthe gap, we explore the option of continual training\nto adapt these models to long sequences, thereby\navoiding pre-training from the scratch. We discuss\nits effectiveness with MASFormer in Section 4.3.\nSecondly, we showcase that increasing sequence\nlength can yield more performance gains on down-\nstream tasks than NLM tasks evaluated by per-\nplexity. We are aware of the recent findings by\nSun et al. (2021) that increasing context length\nexhibits limited impact on NLM perplexity. Nev-\nertheless, when applying MASFormer to down-\nstream tasks like long-context summarization, we\nfind that model performance benefits significantly\nfrom extending context length. Such a difference\narises from the fact that predicting the next tokens\nin NLM primarily relies on locality of reference.\nCapturing infrequent long-range tokens can im-\nprove perplexity but not significantly. Therefore,\nwe emphasize the necessity to evaluate model per-\nformance on downstream tasks that require long-\nrange dependencies. Furthermore, our empirical\nevidence suggests that increasing the length can\nimprove the performance only if models possess\nsufficient capability to handle additional long-range\ninformation. Local attention, as a counterexample,\noften fails to capture long-range signals and hence\n2776\nbenefits much less from long sequences.\n2 Background\n2.1 Pretrained Language Models\nPre-trained transformer models (Devlin et al., 2019;\nLiu et al., 2019; Brown et al., 2020; Dosovitskiy\net al., 2020; He et al., 2021b,a) have manifested\nsuperior performance in various NLP tasks. These\nmodels are often pre-trained on enormous amounts\nof unlabeled data in a unsupervised/self-supervised\nmanner such that they can learn rich semantic\nknowledge. By further fine-tuning these pre-trained\nmodels, we can effectively transfer such knowledge\nto benefit downstream tasks (Zhang et al., 2023).\nExisting research on long-range transformers\ncommonly requires pre-training the proposed mod-\nels from scratch to accommodate new architectures\nand long inputs (Guo et al., 2021; Zuo et al., 2022).\nHowever, the significant training overheads raise a\nbarrier for the widespread utilization of these meth-\nods across different language models. Motivated\nby this, we explore the possibility of leveraging\nexisting pre-trained models and adapting them to\nlong sequences though continual training.\n2.2 Attention Mechanism\nSuppose the input to the layer is X ‚ààRn√ód, where\nn is the input sequence length and d is embedding\ndimension, then self-attention mechanism outputs\nAttn(X) = softmax\n(QK‚ä§\n‚àö\nd\n)\nV (1)\nwhere Q = XWq, K = XWk, V = V Wv\nand Wq, Wk, Wv are learnable projection weights.\nSuch full attention can simultaneously evaluate the\nalignment between any pair of tokens in the se-\nquence. Specifically, denote the attention score\nA = softmax(QK‚ä§/\n‚àö\nd), then Aij captures the\nalignment between tokens i and j. A typical trans-\nformer model applies the full attention at every\nlayer. Denote the number of layers as L. Then its\nattention cost is Ln2.\nSparse attention variants are introduced to miti-\ngate the computational cost of full attention. Fig-\nures 1a and 1b illustrates the attention patterns of\nblock sparse attention and sliding-window atten-\ntion. For instance, block sparse attention divides\ntokens into blocks of size b and performs intra-\nblock attention only, resulting in an attention cost\nof bn. Sliding-window attention allows each token\nto attend its left/right neighboring tokens within a\nlocal window of size w. In most of cases, block\nsparse attention exhibits similar performance as\nsliding-window attention (Zuo et al., 2022).\n3 Our Approach\nWe present our method ‚Äì MASFormer, a long-\nrange transformer variant that mixes different at-\ntention spans across layers.\n3.1 MASFormer: Mixed Attention Span\nMASFormer leverages full attention exclusively at\na subset of transformer layers, whereas it employs\nblock sparse attention at the remaining layers. The\nstructure of MASFormer is illustrated in Figure 1c.\nWe choose full attention to encode long-range in-\nformation due to the following reasons: (i) full at-\ntention exhibits superior capability to capture long-\nrange dependencies compared to sparse attention;\n(ii) full attention does not require sophisticated im-\nplementation and hence is computationally stable\ncompared to SSMs (Zuo et al., 2022; Gupta et al.,\n2022); (iii) full attention is compatible with exist-\ning pre-trained transformer models, enabling us to\nconduct continual training which we elaborate in\nSection 3.2. To mitigate the computational cost, we\nrestrict the number of layers using full attention.\nMASFormer is motivated by empirical investiga-\ntions on performance comparison between models\nthat apply the same attention span at every layer.\nFigure 2 presents the performance of block sparse\nattention and full attention on language modeling\nand summarization tasks. We find that, given long-\nsequence inputs, sparse attention is often insuffi-\ncient to capture long-range dependencies beyond\nits attention span. As a result, it shows unsatis-\nfactory performance. To remedy it, one can either\nincrease attention span or switch to full attention\nto improve model capability of capturing sophis-\nticated dependencies. Though improving model\nperformance, it incurs high computational cost.\nConfronting such a trade-off between computa-\ntional cost and model performance, we challenge\nthe common practice ‚Äì that applies the same atten-\ntion span at every layer.MASFormer provides an\nalternative solution. Instead of increasing attention\nspan evenly, MASFormer allocates a large portion\nof attention computations to a subset of l layers\nby equipping them with full attention. Specifically,\nequipping bottom layers with full attention can\nyield the best performance as suggested by our\nempirical analysis in Section 4.31. At the remain-\n1Please see Section 4.3 for detailed explanations\n2777\n(a) Block Attention\n (b) Window Attention\nInput TokensLayer 1Full Layer 2Full Layer 3Block Layer 4Block Layer ùêøBlock\nOutputs (c) MASFormer\nFigure 1: Illustration of attention patterns of (a) block sparse attention with block size b = 3; (b) sliding-window\nattention with window size w = 1(on each side); (c) MASFormer that integrates full and sparse attention.\n1024 2048 8192\nBlock size b\n8\n10\n12\n14Perplexity\n13.19\n10.75\n8.63\n(a) NLM for ArXiv\n1024 2048 8192\nBlock size b\n8\n10\n12Perplexity\n12.19\n10.13\n7.63 (b) NLM for PubMed\n2048 4096 8192\nBlock size b (n=8192)\n10\n15\n20R2\n9.61\n14.49\n19.32 (c) Sum. for ArXiv\n2048 4096 8192\nBlock size b (n=8192)\n10\n15\n20\n25\n30R2\n12.31\n23.64\n28.83 (d) Sum. for GovReport\nFigure 2: (a,b): We evaluate the perplexity of a pre-trained GPT-2 model with block attention of differnet block size\nafter continual training. (c,d): We fine-tune a GPT-2 model with block attention and compare the summarization\nperformance on ArXiv and GovReport under different block size. Here the input length n is 8192.\ning layers, MASFormer utilizes block attention of\nsmall size m, resulting in a controlled attention\ncost of (L ‚àíl)mn + ln2. As mentioned in Sec-\ntion 1, such a design is inspired by the phenomenon\nthat most of contexts in NLP data exhibit a great\ndeal of locality of reference. Long-range depen-\ndencies, in contrast, are less frequent. Therefore, it\nis not necessary to enhance attention span at every\nlayer. Instead, a few layers of full attention are\nsufficient to capture infrequent long-range signals.\nThe majority of layers can maintain small attention\nspans to adequately extract local dependencies and\ncontrol the attention cost.\nOur empirical results demonstrate that, with\nthe same attention cost, MASFormer significantly\noutperforms sparse attention. Remarkably, MAS-\nFormer can achieve comparable performance to\nfull attention while substantially reducing compu-\ntational cost. Therefore, by mixing different atten-\ntion spans, MASFormer strikes a better balance be-\ntween computational cost and model performance.\nMoreover, MASFormer offers additional imple-\nmentation advantages. As using the same attention\nfunction, MASFormer is easy to implement and\ncompatible with existing pre-trained models. We\ncan build MASFormer upon pre-trained transform-\ners by changing their attention patterns, which does\nnot involve modification on model architectures\nand pre-trained weights. Meanwhile, acceleration\npackages, such as FlashAttention (Dao et al., 2022)\nand xFormers (Lefaudeux et al., 2022), are applica-\nble to further accelerate the computation of block\nattention and full attention in MASFormer.\n3.2 Continual Training with Long Sequences\nAs mentioned, MASFormer can be implemented\nupon majority of pre-trained transformers by mod-\nifying their attention patterns. However, most of\npublicly available models are pre-trained with se-\nquences shorter than 2048, and often exhibit subpar\nperformance on longer sequences such as 8k/16k.\nTo bridge this gap, we propose the continual train-\ning to adapt the revised model on long sequences\nand new attention pattern. As such, we can preserve\nexisting pre-trained knowledge and circumvent the\nintensive overheads of pre-training from scratch.\nIn particular, we first modify the attention pattern\nof the target model as proposed by MASFormer. If\nthe pre-trained model uses absolute position em-\nbeddings, we duplicate them to accommodate long\nsequences. Subsequently, we provide the revised\nmodel with long sequences (e.g., 8k) from pre-\ntraining corpus like PILE (Gao et al., 2020). Then\nwe conduct continual pre-training using casual lan-\nguage modeling (CLM) objective. We discuss the\neffectiveness of continual training in Section 4.3.\n2778\n4 Experiments\nWe evaluate the effectiveness and efficiency of\nMASFormer on natural language modeling (ArXiv\nand PubMed, Cohan et al. (2018)), natural language\ngeneration (ArXiv Cohan et al. (2018), QMSUM\nand GovReport Shaham et al. (2022)). We choose\nthe GPT-3 XL model architecture (Brown et al.,\n2020) as our base model, which consists of 1.3\nbillion parameters and 24 layers and is pre-trained\non PILE (Gao et al., 2020) for 300 billion tokens.\nGPT is a general purpose model that can be ap-\nplied to many tasks instead of tailoring them for\nspecific tasks. As such, it makes easy to control\nexperiments and showcase the difference among\nvarious methods.\nImplementation Details. Our base model uses\nabsolute positional embeddings with maximum\nlength 1024. To accommodate longer inputs, we du-\nplicate its positional embeddings to have the maxi-\nmum length as 8192 such that the model can handle\nsequences containing up to 8192 tokens. Then, we\nimplement different attention methods by modify-\ning the attention pattern of the base model. We\nimplement all the models with PyTorch (Paszke\net al., 2019). All the experiments are conducted on\nNVIDIA A100 GPUs.\nContinual Training Details. After changing the\nattention pattern, we conduct the continual training\nfor MASFormer and baseline methods on PILE cor-\npus (Gao et al., 2020) to adapt the revised models\nto new attention patterns and long-sequence inputs.\nWe leverage the casual language modeling (CLM)\nobjective to train the model for 50,000 steps with\na warmup of 2000 steps. We set the input length\nas 8192 and use a batch size of 128 such that the\nmodels are optimized with 1M tokens per step. We\nuse the constant learning 0.0001 for all methods.\nBaseline. We compare MASFormer with the fol-\nlowing methods:\n‚Ä¢All full attentionis to apply full attention at every\nlayer. It has been adopted by most of existing\ntransformer models as default. Although incurring\nthe maximum attention cost, it achieves the best\nperformance for most of our tasks. Hence, it acts\nas an upper bound for other methods.\n‚Ä¢All block sparse attentionis to apply block atten-\ntion at every layer, which is an effective method to\nreduce computational cost when modeling long se-\nquences. Block attention sets the attention span of\neach layer identical such that it evenly distributes\nthe budget of attention computation across layers.\n‚Ä¢All sliding-window attentionis to apply sliding-\nwindow attention at every layer, which is another\nvariant of sparse attention. It shares the similar spir-\nits and often performs similarly as block attention.\nIn the following experiments, we compare MAS-\nFormer and the baseline methods across different\nattention cost C. That is, for all block sparse atten-\ntion, we set the block size as b = C/(Ln). For all\nsliding-window attention, we choose the window\nsize as w = C/(2Ln). For MASFormer, we apply\na small block size m = 1024 for its block atten-\ntion and set l as (C‚àí Lmn)/(n2 ‚àímn). Then\nwe observe how their performance evolves when\nenhancing the attention cost Cor input length n.\nExperiment Overview. We briefly summarize the\nexperimental contents as follows:\n‚Ä¢Section 4.1 presents the perplexity evaluation of\nall the models on ArXiv and PubMed after contin-\nual training.\n‚Ä¢Section 4.2 compares the summarization perfor-\nmance of the models on ArXiv, QMSUM, and Gov-\nReport after fine-tuning. Besides, we also discuss\nthe difference between perplexity and downstream\nevaluation in reflecting model capacity to capture\nlong-range dependencies.\n‚Ä¢Section 4.3 provides three crucial analyses: (i)\nwe evaluate the benefits of increasing input length\nand discuss the requirements to attain these gains;\n(ii) we analyze the effectiveness of continual train-\ning for long-sequence modeling; (iii) we conduct\nan ablation study to demonstrate that equipping\nbottom layerswith full attention yields the most\nsignificant performance gains than other options.\nWe further provide the explanations.\n4.1 Natural Language Modeling\n4.1.1 Datasets and Evaluation Details\nDatasets. We evaluate the perplexity of the up-\ndated GPT-2 for each attention method after con-\ntinual training. The evaluation is conducted on test\nsets of ArXiv and PubMed (Cohan et al., 2018).\nTable 5 presents the statistics of these two datasets.\nPubmed consists of scientific documents, with a\ndocument‚Äôs content used as input and its corre-\nsponding abstract as the target summary. ArXiv is\nsimilar to PubMed, with documents from arXiv.\nEvaluation Details. We conduct the perplexity\nevaluation under two settings. (i) We calculate\nthe perplexity (ppl.) with all documents from test\nsets. Table 1 presents the overall perplexity of dif-\nferent models on two datasets. (ii) To showcase\n2779\n2000 4000 6000 8000\nExample Length\n10\n12\n14\n16\n18Perplexity\nFull Attention\nBlock(b=1024)\nBlock(b=2048)\nMASFormer(l=2)\nMASFormer(l=4)\nMASFormer(l=8)\n(a) ArXiv ppl.\n2000 4000 6000 8000\nExample Length\n8\n10\n12\n14Perplexity\nFull Attention\nBlock(b=1024)\nBlock(b=2048)\nMASFormer(l=2)\nMASFormer(l=4)\nMASFormer(l=8) (b) PubMed ppl.\nFigure 3: Perplexity evaluation on ArXiv and PubMed with examples of different length. Here x-axis is the\nmaximum document length of each subset, i.e., k √ó1024 (k = 1, 2, 3, . . .).\nthe varying behaviors of models on documents of\ndifferent length, we divide all documents into sev-\neral subsets according to their length. Each sub-\nset consists of examples, whose length is within\n((k‚àí1)√ó1024, k√ó1024] (k = 1, 2, 3, . . .). Then,\nwe evaluate the perplexity on each subset. Fig-\nure 3 presents the perplexity of models on different\nsubsets of examples.\n4.1.2 Results\nTable 1 compares the overall perplexity on test sets\nof ArXiv and PubMed. The results suggest that,\nwith l = 4 layers of full attention, MASFormer\nachieves comparable performance to all full atten-\ntion, while reducing 72% of its attention cost. With\nthe similar attention cost C, MASFormer outper-\nforms all block attention that evenly distributes\nthe budget of attention computation. For exam-\nple, MASFormer with l = 2achieves 8.75 ppl. on\nPubMed, which is 1.37 lower than that of block\nattention of b = 2048.\nMethods C ArXiv PubMed\nFull attention 1,610M 8.63 7.63\nBlock (b=1024) 201M 13.19 12.19\nBlock (b=2048) 402M 10.75 10.13\nMASFormer (l=2) 318M 10.25 8.75\nMASFormer (l=4) 436M 9.31 8.25\nMASFormer (l=8) 671M 9.63 8.25\nTable 1: Perplexity evaluation on ArXiv and PubMed.\nFigure 3 illustrates the perplexity variation of\neach method given examples of different length.\nWe can tell that MASFormer and full attention\nshow better performance on longer documents, sug-\ngesting increasing context length can improve their\nprediction performance. Full attention, though in-\ncurring the highest attention cost, always achieves\nthe best performance due to its outstanding capabil-\nity to handle sophisticated dependencies. Notably,\nwith 27% of its attention cost, MASFormer exhibits\na curve of ppl. v.s. length that closely resembles to\nthat of full attention. This demonstrates the effec-\ntiveness and efficiency of MASFormer to capture\nlong-range dependencies. In contrast, block sparse\nattention benefits much less from long contexts and\nunderperforms both of them because of its incapa-\nbility to encode long-range signals. For example,\nwhen b = 1024, block attention achieves similar\nperplexity on PubMed examples of different length.\n4.2 Natural Language Generation\n4.2.1 Datasets and Training Details\nDatasets. We evaluate the downstream perfor-\nmance of models on several abstractive summariza-\ntion tasks to compare their capability of handling\nlong sequences in practice. Specifically, we fine-\ntune models on ArXiv (Cohan et al., 2018), QM-\nSUM and GovReport (from SCROLLS benchmark,\nShaham et al. (2022)). Their statistics are summa-\nrized in Table 5. We mainly use ROUGE-2 (R2)\nscore (Lin, 2004) as the evaluation metric, which\nis more important and sensitive than R1 and RL.\nTraining Details. After continual training, we fine-\ntune each model and report R2 scores on validation\nsets. Specifically, we fine-tune models for 3000\nsteps on QMSUM, 8000 steps on GovReport, and\n12000 steps on ArXiv. We set the batch size as\n64 for ArXiv and 32 for QMSUM and GovReport.\nWe pick the learning rates from {1 √ó10‚àí5, 5 √ó\n10‚àí5, 1√ó10‚àí4, 5√ó10‚àí4}, and choose the optimal\nones to report the performance of each method.\nMoreover, the input length is fixed as 8192. We\napply the greedy decoding for generation. Please\nsee Appendix B for more details.\n4.2.2 Results\nIn Table 22 and Figure 4, we present the fine-tuning\nresults on QMSUM, ArXiv and GovReport across\n2Please see Table 6 in Appendix B for all ROUGE scores.\n2780\n0.50 0.75 1.00 1.25 1.50\nAttention Cost √ó109\n10\n12\n14\n16\n18R2\nBlock Attnention\nFull Attnention\nMASFormer\n(a) ArXiv\n0.50 0.75 1.00 1.25 1.50\nAttention Cost √ó109\n5\n6\n7\n8\n9R2 (b) QMSUM\n0.50 0.75 1.00 1.25 1.50\nAttention Cost √ó109\n15\n20\n25R2 (c) GovReport\nFigure 4: Given input length as 8192, we compare summarization performance between MASFormer and block/full\nattention when increasing the attention cost.\nMethods C QMSUM ArXiv GovReport\nFull attention 1610M 8.00 19.32 28.83\nWindow(w=1024) 402M 4.32 13.51 17.03\nBlock(b=2048) 402M 5.03 9.61 12.31\nMASFormer(l=4) 436M 6.59 14.91 18.82\nWindow(w=2048) 805M 5.05 15.21 22.79\nBlock(b=4096) 805M 5.15 14.50 23.64\nMASFormer(l=6) 553M 7.15 15.72 21.20\nMASFormer(l=8) 671M 7.46 17.00 24.42\nMASFormer(l=12) 906M 8.70 18.58 26.26\nTable 2: Summarization performance of models with\ndifferent attention methods. The best results are shown\nin bold.\ndifferent attention cost. The results demonstrate\nthat, with the similar attention cost, MASFormer\nsignificantly outperforms sparse attention variants.\nFurthermore, when enhancing attention cost, MAS-\nFormer achieves greater performance gains than\nsparse attention methods. This is evident from\nthe steeper slope of its R2 curve versus attention\ncost, in contrast to the baseline method. For exam-\nple, when increasing Cform 553M to 671M, the\nR2 score of MASFormer on QMSUM exhibits a\nsubstantial improvement, reaching 8.70 from 7.46.\nRemarkably, this score surpasses even that of full\nattention. Therefore, MASFormer addresses the\ntrade-off between computational cost and perfor-\nmance gains in a more efficient and effective way.\nNotice that, in order to achieve comparable sum-\nmarization performance to full attention, MAS-\nFormer needs at leaset l = 8layers of full attention,\nand providing more can lead to more gains. This\nobservation is different from the findings in NLM\n(Figure 3) that increasing l beyond 4 provides lim-\nited improvement in perplexity. Their different\ncapacity requirements arise from the fact that pre-\ndicting next tokens in NLM primarily relies on lo-\ncal dependencies. Capturing infrequent long-range\ntokens does not significantly improve perplexity.\nThus, this discrepancy emphasizes the necessity to\nevaluate long-range models on downstream tasks.\n4.3 Analysis\n4.3.1 Benefits of Increasing Sequence Length\nIn this section, we investigate the benefits of\nincreasing input length for downstream perfor-\nmance. Specifically, we select the input length\nfrom {2048, 4096, 6144, 8192} and present the\nfine-tuning performance of full attention in Fig-\nure 5. The results consistently demonstrate that as\nthe input length increases, the model‚Äôs performance\nimproves. That is, downstream performance ben-\nefits significantly from long-sequence inputs. In\ncontrast, increasing example length beyond 6k re-\nsults in marginal improvements in perplexity (See\nFigure 3), highlighting again the importance of\ndownstream evaluation.\nIn addition, when comparing the behaviors of\nblock attention in Figure 2c and 2d, we find that\nsparse attention often insufficiently capitalize on\nthe benefits offered by longer inputs. For instance,\ngiven block size as 4096, its performance on ArXiv\nremains nearly unchanged when increasing input\nlength from 4096 ( R2 = 15.52 in Figure 5a) to\n8192 (R2 = 14.49 in Figure 2c). This finding sug-\ngests that enhancing input length can only improve\nmodel performance if the model possesses the suffi-\ncient capability to handle long-range dependencies.\n4.3.2 Effectiveness of Continual Training\nWe analyze the effectiveness of continual training\nby comparing fine-tuning performance of MAS-\nFormer (l = 8) under the following settings: (i)\nMASFormer without continual training (w.o. C.T.);\n(ii) MASFormer continually trained with short in-\nputs (C.T. (n=2048)); (iii) MASFormer continually\ntrained with long inputs (C.T. (n=8192)). Table 3\n2781\n2048 4096 6144 8192\nInput length\n12\n14\n16\n18\n20R2\n13.09\n15.52\n16.94\n19.32\n(a) ArXiv\n2048 4096 6144 8192\nInput length\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nR2\n17.94\n23.54\n27.65\n28.83 (b) GovReport\n2048 4096 6144 8192\nInput length\n7.0\n7.2\n7.4\n7.6\n7.8\n8.0\nR2\n7.29\n7.69 7.65\n8 (c) QMSUM\nFigure 5: Fine-tuning performance of full attention under different input length.\npresents fine-tuning performance of these models.\nWe can tell that continual training with long inputs\nindeed facilitates the revised models to adapt to\nnew structures and long-sequence inputs.\nl = 8 QMSUM GovReport\nw.o. C.T. 29.33/6.43/25.71 53.28/23.61/51.74\nC.T. (n=2048) 29.87/7.16/26.15 52.28/23.01/49.83\nC.T. (n=8192) 30.91/7.45/27.02 54.37/24.42/51.87\nTable 3: We report R1/R2/RL for the above results.\n4.3.3 Where to use full attention\nTo answer where to apply full attention, we com-\npare fine-tuning performance of MASFormers that\napply full attention at (i) bottom layers; (ii) mid-\ndle layers; (iii) top layers; (iv) every L/l layers.\nThe results in Table 4 demonstrate that equipping\nbottom layers with full attention yields the best\nperformance. This is because that long-range de-\npendencies can be continually captured and rein-\nforced by bottom layers before propagated to upper\nlayers. As such, these long-range signals can be\neffectively incorporated into the upper layers with\nlocal attention, facilitating their encoding of local\ninformation. In contrast, when equipping local at-\ntention at bottom layers, long-range tokens are first\naggregated with neighboring tokens by local atten-\ntion, thereby weakening their long-range signals.\nMoreover, if alternating full and local attention ev-\nery L/l layers, the long-range signals cannot be\ncontinually reinforced nor efficiently captured.\n5 Discussion\nGPT-Neo (Black et al., 2021) introduces an atten-\ntion pattern that alternates full and window atten-\ntion. However, this models is not tailored for long\nsequences. It sets the local window size as 256\nand has the maximum input length as 2048, unable\nto handle long sequences. Instead, this attention\npattern is applied heuristically in an attempt to re-\nPosition QMSUM GovReport\nEvery 3 28.26/6.94/25.03 26.16/12.37/24.82\nTop 8 20.89/4.52/18.37 -/-/-\nMiddle 8 27.27/5.99/24.06 20.80/9.01/19.52\nBottom 8 30.91/7.45/27.02 54.37/24.42/51.87\nEvery 2 31.27/8.19/27.41 35.34/16.04/33.68\nBottom 12 32.53/8.70/28.75 56.98/26.26/54.46\nTable 4: Performance comparison of MASFormers that\napply full attention at different layers (# layers L=24).\nduce computational cost. However, as discussed\nin Section 4.3.3, this approach is neither effective\nnor efficient as MASFormer when handling long\nsequences. As shown in Table 4, applying full atten-\ntion at every 2 or 3 layers underperforms applying it\nat bottom 12 or 8 layers. Therefore, alternating be-\ntween full and block attention results in additional\ncomputational cost and performance degradation.\nIn contrast, MASFormer presents an effective\nsolution for efficient long-sequence modeling. It\nprovides guidance on adapting existing pre-trained\ntransformers to long inputs. Meanwhile, it pro-\nvides insights for designing large long-range mod-\nels, especially for deeper models. By equipping\nonly a subset of bottom layer with full attention,\nwe can substantially mitigate computational cost.\nAdditionaly, the computation of MASFormer can\nbe further optimized by leveraging system-level\nacceleration techniques (e.g., FlashAttention and\nxFormer) that support both block and full attention.\n6 Conclusion\nWe propose an efficient long-range transformer ‚Äì\nMASFormer that utilizes full attention at a few of\nbottom layers and employs sparse attention at the\nremaining layers. Our empirical results on natural\nlanguage modeling and generation tasks demon-\nstrate that MASFormer can address the trade-off\nbetween computational cost and performance gains\nin a more efficient and effective way.\n2782\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured inputs in\ntransformers. arXiv preprint arXiv:2004.08483.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri\nRudra, and Christopher R√©. 2021. Scatterbrain: Uni-\nfying sparse and low-rank attention approximation.\narXiv preprint arXiv:2110.15343.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 615‚Äì621, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher R√©. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and\nChristopher R√©. 2020. Hippo: Recurrent mem-\nory with optimal polynomial projections. Advances\nin neural information processing systems, 33:1474‚Äì\n1487.\nAlbert Gu, Karan Goel, and Christopher R√©. 2021. Effi-\nciently modeling long sequences with structured state\nspaces. arXiv preprint arXiv:2111.00396.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2021. Longt5: Efficient text-to-text transformer for\nlong sequences. arXiv preprint arXiv:2112.07916.\nAnkit Gupta, Albert Gu, and Jonathan Berant. 2022.\nDiagonal state spaces are as effective as structured\nstate spaces. In Advances in Neural Information\nProcessing Systems.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In Inter-\nnational Conference on Learning Representations.\nBenjamin Lefaudeux, Francisco Massa, Diana\nLiskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tin-\ntore, Susan Zhang, Patrick Labatut, and Daniel\nHaziza. 2022. xformers: A modular and hack-\nable transformer modelling library. https:\n//github.com/facebookresearch/xformers.\nYuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and\nDebadeepta Dey. 2022. What makes convolutional\nmodels great on long sequence modeling? arXiv\npreprint arXiv:2210.09298.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\n2783\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,\nLiangke Gui, Graham Neubig, Jonathan May, and\nLuke Zettlemoyer. 2023. Mega: Moving average\nequipped gated attention. In The Eleventh Interna-\ntional Conference on Learning Representations.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K√∂pf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024‚Äì8035.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300‚Äì325,\nOnline. Association for Computational Linguistics.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\nGeva, Jonathan Berant, et al. 2022. Scrolls: Stan-\ndardized comparison over long language sequences.\narXiv preprint arXiv:2201.03533.\nJimmy T.H. Smith, Andrew Warrington, and Scott Lin-\nderman. 2023. Simplified state space layers for se-\nquence modeling. In The Eleventh International Con-\nference on Learning Representations.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\nguage models actually use long-range context? arXiv\npreprint arXiv:2109.09115.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, 33:17283‚Äì17297.\nQingru Zhang, Minshuo Chen, Alexander Bukharin,\nPengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. 2023. Adaptive budget allocation for\nparameter-efficient fine-tuning. In The Eleventh In-\nternational Conference on Learning Representations.\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles,\nEren Manavoglu, Tuo Zhao, and Jianfeng Gao.\n2022. Efficient long sequence modeling via state\nspace augmented transformer. arXiv preprint\narXiv:2212.08136.\n2784\nA Dataset Statistics\nIn the following table, we provide the detailed statistics of datasets in our experiments, including example\nsplits and length statistics.\nDataset Example Count Input Length\nTrain Valid Test Average Median 90th percentile\nArXiv 203,037 6,436 6,440 10,720 8,519 20,170\nPubMed 119,924 6,633 6,658 4,748 3,883 8,883\nQMSUM 1,257 272 281 9,497 14,197 27,761\nGovReport 17,457 972 973 7,886 8,841 18,835\nTable 5: Statistics of datasets. Input length measured in tokens using a SentencePiece Model.\nB Natural Language Generation\nB.1 The Results of All ROUGE Scores\nMethods C QMSUM ArXiv GovReport\nFull attention 1610M 31.50 / 8.00 / 27.81 46.13 / 19.32 / 41.89 60.53 / 28.83 / 57.88\nWindow (w=1024) 402M 23.31 / 4.32 / 20.62 35.90 / 13.51 / 32.19 49.82 / 17.03 / 47.42\nWindow (w=2048) 805M 26.73 / 5.05 / 23.40 38.74 / 15.21 / 34.87 56.14 / 22.79 / 53.50\nBlock (b=2048) 402M 26.24 / 5.03 / 23.13 21.85 / 9.61 / 19.86 26.37 / 12.31 / 25.18\nBlock (b=4096) 805M 26.96 / 5.15 / 23.85 35.95 / 14.50 / 32.37 49.83 / 23.64 / 47.50\nMASFormer (l=4) 436M 29.86 / 6.59 / 25.87 38.85 / 14.91 / 34.98 46.67 / 18.82 / 44.39\nMASFormer (l=6) 553M 30.83 / 7.15 / 27.12 36.29 / 15.72 / 32.96 49.26 / 21.20 / 46.89\nMASFormer (l=8) 671M 30.91 / 8.00 / 27.81 43.31 / 17.00 / 39.12 54.37 / 24.42 / 51.87\nMASFormer (l=12) 906M 32.53 / 8.70 / 28.75 45.19 / 18.58 / 40.72 56.98 / 26.26 / 54.46\nTable 6: Finetuning performance of different attention methods.\nB.2 Training Details\nMethods QMSUM ArXiv GovReport\nFull attention 1 √ó10‚àí5 1 √ó10‚àí4 1 √ó10‚àí4\nWindow attention (w=1024) 5 √ó10‚àí4 5 √ó10‚àí5 5 √ó10‚àí4\nWindow attention (w=2048) 5 √ó10‚àí5 5 √ó10‚àí5 1 √ó10‚àí4\nBlock attention (w=2048) 1 √ó10‚àí4 1 √ó10‚àí5 1 √ó10‚àí5\nBlock attention (w=4096) 5 √ó10‚àí5 5 √ó10‚àí4 1 √ó10‚àí5\nMASFormer (l=4) 5 √ó10‚àí5 1 √ó10‚àí3 5 √ó10‚àí4\nMASFormer (l=6) 5 √ó10‚àí5 5 √ó10‚àí5 5 √ó10‚àí4\nMASFormer (l=8) 5 √ó10‚àí5 5 √ó10‚àí4 5 √ó10‚àí4\nMASFormer (l=12) 1 √ó10‚àí5 1 √ó10‚àí4 1 √ó10‚àí4\nTable 7: The fine-tuning learning rate of each method on each dataset.\nWe conduct continual training for all attention methods with training date form PILE and input length\nas 8192. After continual training, we obtain the continually trained models for each method and fine-tune\nthem on QMSUM, ArXiv and GovReport to compare their summarization performance. During the\n2785\nfine-tuning, we set the input length as 8192 for all datasets and all models. We apply the greedy decoding\nfor generation and set the maximum output length as 256 for QMSUM, 1024 for GovReport, and 512\nfor ArXiv. Table 8 lists the details of these hyperparameters. Besides, we apply the linear learning rate\nschedule to fine-tune the models and the base learning rates are summarized in Table 7.\nHyperparameter QMSUM ArXiv GovReport\nTraining steps 3000 12000 8000\nBatch size 32 32 64\nInput length 8192 8192 8192\nMaximum generation length 256 512 1024\nWeight decay 0.001 0.001 0.001\nTable 8: The other fine-tuning parameters for each dataset, which remain the same for every method.\n2786",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.782923698425293
    },
    {
      "name": "Transformer",
      "score": 0.7147221565246582
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6237527132034302
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3580980896949768
    },
    {
      "name": "Voltage",
      "score": 0.10145920515060425
    },
    {
      "name": "Engineering",
      "score": 0.08763724565505981
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}