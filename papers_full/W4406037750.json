{
  "title": "When AI-Based Agents Are Proactive: Implications for Competence and System Satisfaction in Human–AI Collaboration",
  "url": "https://openalex.org/W4406037750",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Diebel, Christopher",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A4284312296",
      "name": "Goutier, Marc",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2132986097",
      "name": "Adam Martin",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A2751077986",
      "name": "Benlian Alexander",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4310813225",
    "https://openalex.org/W4391999255",
    "https://openalex.org/W2162061662",
    "https://openalex.org/W1977793234",
    "https://openalex.org/W4249043090",
    "https://openalex.org/W4313890705",
    "https://openalex.org/W4403637448",
    "https://openalex.org/W3044483028",
    "https://openalex.org/W1977368841",
    "https://openalex.org/W115183765",
    "https://openalex.org/W2945908415",
    "https://openalex.org/W3142805939",
    "https://openalex.org/W2499471102",
    "https://openalex.org/W2790549410",
    "https://openalex.org/W4323048183",
    "https://openalex.org/W2942966180",
    "https://openalex.org/W2110506823",
    "https://openalex.org/W2063047416",
    "https://openalex.org/W4362465589",
    "https://openalex.org/W2078765786",
    "https://openalex.org/W4235474878",
    "https://openalex.org/W4387035730",
    "https://openalex.org/W3171435442",
    "https://openalex.org/W2926976566",
    "https://openalex.org/W3135424498",
    "https://openalex.org/W2052729098",
    "https://openalex.org/W2122517769",
    "https://openalex.org/W1972950717",
    "https://openalex.org/W4232985273",
    "https://openalex.org/W2102449133",
    "https://openalex.org/W4381052613",
    "https://openalex.org/W3128956710",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W2978102797",
    "https://openalex.org/W578092267",
    "https://openalex.org/W2072500831",
    "https://openalex.org/W1987258130",
    "https://openalex.org/W4200195738",
    "https://openalex.org/W3201195645",
    "https://openalex.org/W4289297051",
    "https://openalex.org/W4281654814",
    "https://openalex.org/W3210383951",
    "https://openalex.org/W3204472273",
    "https://openalex.org/W4200385660",
    "https://openalex.org/W2969587639",
    "https://openalex.org/W3082837871",
    "https://openalex.org/W2889387953",
    "https://openalex.org/W3144406692",
    "https://openalex.org/W3191131351",
    "https://openalex.org/W4308751002",
    "https://openalex.org/W3132476284",
    "https://openalex.org/W2890416438",
    "https://openalex.org/W3029022390",
    "https://openalex.org/W2950948599",
    "https://openalex.org/W4284966794",
    "https://openalex.org/W4241244205",
    "https://openalex.org/W2794729814",
    "https://openalex.org/W3113988938",
    "https://openalex.org/W3109381885",
    "https://openalex.org/W2592021215",
    "https://openalex.org/W2107984537",
    "https://openalex.org/W3214177658",
    "https://openalex.org/W2020985",
    "https://openalex.org/W2779206865",
    "https://openalex.org/W1980279180",
    "https://openalex.org/W2900107451",
    "https://openalex.org/W2585292421",
    "https://openalex.org/W6605572670",
    "https://openalex.org/W4392752316",
    "https://openalex.org/W4366549077",
    "https://openalex.org/W1985354288",
    "https://openalex.org/W2051292336",
    "https://openalex.org/W2043014755",
    "https://openalex.org/W4242883546",
    "https://openalex.org/W4360991389",
    "https://openalex.org/W2165752741",
    "https://openalex.org/W4280563381",
    "https://openalex.org/W2104057213",
    "https://openalex.org/W2251854823",
    "https://openalex.org/W2140316233",
    "https://openalex.org/W4402432718",
    "https://openalex.org/W1965645084",
    "https://openalex.org/W4381192755",
    "https://openalex.org/W4285004957",
    "https://openalex.org/W2075670547",
    "https://openalex.org/W3162051685",
    "https://openalex.org/W1849190772"
  ],
  "abstract": "Abstract As the capabilities of artificial intelligence (AI) technologies continue to improve, collaboration with AI-based agents enables users to be more efficient and productive. Not only has the quality of AI-based agents’ outcomes increased, but they can now help proactively, and even take over entire work tasks. However, users need to be satisfied with the system to remain motivated to collaborate and engage with AI-based agents. Drawing on self-determination theory, a vignette-based online experiment was conducted that revealed that proactive (vs. reactive) help from AI-based agents leads to a higher loss of users’ competence-based self-esteem and thus reduces users’ system satisfaction. This effect is moderated by the users’ knowledge of AI. Higher (vs. lower) levels of AI knowledge cause a greater loss of competence-based self-esteem through proactive (vs. reactive) help. The findings contribute to a better understanding of help from AI-based agents and provide important implications for managers and designers who seek to enhance human–AI collaboration.",
  "full_text": "RESEARCH PAPER\nWhen AI-Based Agents Are Proactive: Implications\nfor Competence and System Satisfaction in\nHuman–AI Collaboration\nChristopher Diebel • Marc Goutier • Martin Adam • Alexander Benlian\nReceived: 29 February 2024 / Accepted: 5 November 2024\n/C211The Author(s) 2025\nAbstract As the capabilities of artiﬁcial intelligence (AI)\ntechnologies continue to improve, collaboration with AI-\nbased agents enables users to be more efﬁcient and pro-\nductive. Not only has the quality of AI-based agents’ out-\ncomes increased, but they can now help proactively, and\neven take over entire work tasks. However, users need to be\nsatisﬁed with the system to remain motivated to collaborate\nand engage with AI-based agents. Drawing on self-deter-\nmination theory, a vignette-based online experiment was\nconducted that revealed that proactive (vs. reactive) help\nfrom AI-based agents leads to a higher loss of users’ com-\npetence-based self-esteem and thus reduces users’ system\nsatisfaction. This effect is moderated by the users’ knowl-\nedge of AI. Higher (vs. lower) levels of AI knowledge cause\na greater loss of competence-based self-esteem through\nproactive (vs. reactive) help. The ﬁndings contribute to a\nbetter understanding of help from AI-based agents and\nprovide important implications for managers and designers\nwho seek to enhance human–AI collaboration.\nKeywords Proactive /C1AI-based agents /C1System\nsatisfaction /C1AI knowledge /C1Self-determination theory\n1 Introduction\nEnabled by technological advances in artiﬁcial intelligence\n(AI), the capabilities of AI-based agents have evolved\nsigniﬁcantly and are becoming increasingly relevant for\npractice (Dwivedi et al. 2023). They have already sur-\npassed humans in terms of accuracy and performance,\ndemonstrate higher competencies in speciﬁc tasks, such as\nimage classiﬁcation (Fuegener et al. 2022), can act auton-\nomously (Bailey et al. 2019; Baird and Maruping 2021),\nand are able to handle entire work processes (Strich et al.\n2021). Furthermore, the increasing capabilities of AI-based\nagents have led to the emergence of proactive AI-based\nagents characterized by their ability to initiate interactions\nwithout any requests from users (Baird and Maruping\n2021). They are capable of offering help proactively by\nanticipating users’ needs (i.e., proactive help), as opposed\nto reactive AI-based agents, which help as a reaction to\nuser requests (i.e., reactive help) (Morana et al. 2017; Baird\nand Maruping 2021; Wenninger et al. 2022). For instance,\nTabnine, an AI-based agent specializing in programming,\ncan anticipate its users’ programming intentions based on\ntheir current input in their editor, such as a function title or\na comment, and proactively suggest programming code\nthat realizes these intentions (Tabnine 2023). Proactive AI-\nbased agents can potentially increase users’ efﬁciency in\nhuman–AI collaboration by approaching and supporting\nthem. Humans often fail to recognize the need for help,\neven if seeking help can increase their performance\n(Yzerbyt et al. 1998; Fuegener et al. 2022). Proactive AI-\nbased agents can overcome this obstruction in human–AI\nAccepted after two revisions by Alexander Richter.\nC. Diebel ( &) /C1M. Goutier /C1A. Benlian\nDepartment of Law and Economics, Institute of Information\nSystems and Electronic Services, Technical University of\nDarmstadt, Hochschulstrasse 1, 64289 Darmstadt, Germany\ne-mail: diebel@ise.tu-darmstadt.de\nM. Goutier\ne-mail: goutier@ise.tu-darmstadt.de\nA. Benlian\ne-mail: benlian@ise.tu-darmstadt.de\nM. Adam\nDepartment of Business and Economics, Chair of Information\nSystems and Smart Services, University of Goettingen, Platz Der\nGoettinger Sieben 5, 37037 Goettingen, Germany\ne-mail: martin.adam@uni-goettingen.de\n123\nBus Inf Syst Eng\nhttps://doi.org/10.1007/s12599-024-00918-y\ncollaboration by anticipating their human counterparts’\nneeds and potential difﬁculties for that they offer help\nwithout the need for explicit requests (Kraus et al. 2021;\nBaird and Maruping 2021). In line with this development,\nAI-based agents are increasingly perceived not only as\npassive tools but rather as partners capable of meaningful\ncollaboration (Anthony et al. 2023; Chen and Chan 2023;\nDennis et al. 2023). However, collaboration (i.e., working\ntogether toward a shared goal) usually takes place in a\nsocial setting in which the success of interactions often\ndepends on user perceptions, such as satisfaction (Marks\net al. 2001; Dennis et al. 2023).\nRecognizing the potential competitive advantages of\ncollaborating with AI-based agents that arise from their\nincreasing capabilities, nearly every ﬁfth German organi-\nzation is already planning to use AI-based agents such as\nChatGPT\n1 in the future (Bitkom 2023). However, a rep-\nresentative survey from Continental ( 2023) revealed that\nmore than 50 percent of German citizens are skeptical or\ndismissive of using systems based on AI (Continental\n2023). While collaborating with AI-based agents has the\npotential to enhance work processes, their potential in\norganizational contexts is limited by the reluctance of their\npotential users. Their success depends on users’ willing-\nness to work with AI-based agents, which is strongly\nrelated to users’ satisfaction with these AI-based agents\n(i.e., users’ system satisfaction) (Briggs et al. 2008; Hsiao\nand Chen 2022). If users are not satisﬁed with an AI-based\nagent, they are less likely to engage with it consistently in\nthe future (Bhattacherjee 2001; Ashfaq et al. 2020; Hsiao\nand Chen 2022). Therefore, maintaining users’ system\nsatisfaction is crucial in order to sustain their motivation to\ncollaborate with AI-based agents (Hsiao 2022; Briggs et al.\n2008). However, while the increasing capabilities of AI-\nbased agents can promote user productivity, they can also\nelicit negative reactions from users (Bar-Or and Meyer\n2019). For example, receiving help from competent AI-\nbased agents can lead to a decrease in help recipients’ self-\nesteem and negatively affect their perception of their own\ncompetence (Lee et al. 2019; Adam et al. 2023a). This is\nparticularly the case when this help is provided in a\nproactive rather than reactive manner, whereby the recip-\nient may feel that they are being questioned about their\ncompetencies (Lee et al. 2019; Harari et al. 2022; Adam\net al. 2023a).\nConsidering the growing prevalence and capabilities of\nproactive AI-based agents at work, there is a critical need\nto gain a deeper understanding of how collaborating with\nthem affects users in human–AI collaboration to utilize\ntheir advantages effectively. Receiving proactive rather\nthan reactive help from AI-based agents has the potential to\nincrease users’ performance by overcoming their reluc-\ntance to seek help by anticipating their needs and offering\nhelp effectively when needed (Bar-Or and Meyer 2019;\nFuegener et al. 2022). However, previous research on\nhuman–AI collaboration has typically not distinguish\nbetween different types of initiation when receiving help\nfrom AI-based agents (e.g., Fuegener et al. 2021; Gnewuch\net al. 2022; Boyac ı et al. 2023), although previous studies\nhave demonstrated that receiving proactive rather than\nreactive help can negatively affect the perceptions of help\nrecipients (Lee et al. 2019; Harari et al. 2022). Further-\nmore, those studies primarily focused on how collaborating\nwith AI-based agents improves accuracy and performance\nwithout delving deeper into users’ perceptions in terms of\nsystem satisfaction (e.g., Fuegener et al. 2021; Gnewuch\net al. 2022; Boyac ı et al. 2023), while users’ system sat-\nisfaction is an important factor in keeping users motivated\nto interact with AI-based agents (Bhattacherjee 2001;\nAshfaq et al. 2020; Hsiao and Chen 2022). Therefore, it is\ncrucial to investigate how and why the type of help initi-\nation—whether proactive or reactive—affects users’ sys-\ntem satisfaction while collaborating with AI-based agents.\nAdditionally, previous studies on human–AI collaboration\nhave demonstrated that certain user characteristics are a\ncritical factor when collaborating with AI-based agents\n(e.g., Meurisch et al. 2020; Pinski et al. 2024). In partic-\nular, these studies have highlighted the importance of\nusers’ understanding of the capabilities and applications of\nsystems based on AI (i.e., AI knowledge) when receiving\nhelp from AI-based agents (Chiu 2021; Pinski et al. 2023a).\nUnderstanding the concepts and functionalities of AI can\nlead to better decision-making (Long and Magerko 2020),\naffect users’ perception of their own competencies (Yang\nand Aurisicchio 2021), and increase users’ performance\nwhen collaborating with AI-based agents (Pinski et al.\n2023a). However, recent studies have shown that AI\nknowledge can also hinder human–AI collaboration by\nreducing users’ intention to use AI-based agents (e.g.,\nTully et al. 2023). Still, despite the growing need to\nunderstand how different types of help initiation affect\nusers’ system satisfaction and the recognition of the rele-\nvance of AI knowledge in human-AI collaboration, we\nknow little about how receiving proactive rather than\nreactive help from AI-based agents affects users’ system\nsatisfaction and what role AI knowledge plays in this\nprocess. Therefore, it is important to address these research\ngaps by examining the following research questions:\n1 ChatGPT is an AI-based agent developed by OpenAI and is\ndesigned to interact with the user in a conversational manner. It is\ncapable of answering questions, writing code, and providing sugges-\ntions based on the context of the conversation. Moreover, ChatGPT is\nable to remember what the user said during the conversation and\nrespond to follow-up questions (OpenAI 2022).\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\n1) How does proactive (vs. reactive) help from AI-\nbased agents affect users’ system satisfaction?\n2) How does users’ AI knowledge inﬂuence this effect?\nTo answer these research questions, we drew on self-\ndetermination theory and conducted a vignette-based\nonline experiment with 92 participants proﬁcient in pro-\ngramming who experienced an interaction with an AI-\nbased agent. We show that proactive (vs. reactive) help\nfrom AI-based agents leads to a higher loss of users’\ncompetence-based self-esteem (i.e., a decline in an indi-\nvidual’s conﬁdence in their competencies at work (Craig\net al. 2019)) and thus reduces users’ system satisfaction.\nWe also ﬁnd that users’ AI knowledge moderates this\neffect, in that users with higher (vs. lower) levels of AI\nknowledge experience a more signiﬁcant loss of compe-\ntence-based self-esteem.\nOur study contributes to information systems (IS)\nresearch on human–AI collaboration by providing valuable\ninsights in several important ways. We advance previous\nresearch, which has mainly focused on how collaborating\nwith AI-based agents improves accuracy and performance\nwithout considering different types of help initiation (e.g.,\nFuegener et al. 2021, 2022; Boyac ı et al. 2023), by\ninvestigating how users’ system satisfaction is affected\nwhen receiving help from AI-based agents proactively\nrather than reactively. We identiﬁed that proactive (vs.\nreactive) help from AI-based agents can lead to lower user\nsystem satisfaction. Furthermore, we ﬁnd that a loss of\ncompetence-based self-esteem is a determining mediator\nfor this effect. Building on these ﬁndings, we highlight the\nsigniﬁcance of exploring interventions to reduce the loss of\ncompetence-based self-esteem and its adverse reactions\ncaused by receiving help from AI-based agents in human–\nAI collaboration. Furthermore, as part of our study, we\ndistinguish the degree of users’ AI knowledge as an\nessential characteristic inﬂuencing users’ system satisfac-\ntion when receiving proactive (vs. reactive) help from AI-\nbased agents. Finally, we contribute to self-determination\ntheory by introducing the type of help initiation from AI-\nbased agents as a new important design principle that can\naffect users’ psychological need satisfaction when inter-\nacting with technology. Thus, our study uncovers valuable\ninsights for organizations that plan to implement AI-based\nagents and IS researcher, as it highlights the potential\nnegative effect on users’ system satisfaction caused by\nproactive (vs. reactive) help from AI-based agents in\nhuman–AI collaboration.\n2 Theoretical Background\n2.1 AI-Based Agents, Type of Help, and AI\nKnowledge\nAn agent in the context of a human–computer interaction is\na kind of software system that has the capability to adapt to\ndynamic environments and to its users (Schleiffer 2005). In\nthe domains of sales, recommendation, or automation (Li\net al. 2015; Benlian et al. 2020; Adam et al. 2023b), agents\nhave taken on a vital role in private households and in\nindustry (Maedche et al. 2019; Mirbabaie et al. 2021;\nDwivedi et al. 2023). This development appears to be\ndriven by AI, leading to a shift from simple-reﬂex agents to\nlearning agents (Russell and Norvig 2016; Stelmaszak et al.\n2024). While simple-reﬂex agents sense inputs and act in a\npre-deﬁned manner based on their built-in knowledge,\nlearning agents have an extended backend that enables\nthem to learn from experience and update the knowledge\nbase on which they rely to make decisions. Learning agents\noften use built-in machine models to learn and update their\nknowledge. However, they can also use pre-deﬁned rules or\nformulas to facilitate the process (Ku ¨ hl et al. 2022). In\nrecent years, most agents have become more autonomous\nand have shown increasing learning capabilities (Schuetz\net al. 2020; Berente et al. 2021), which is why we focus\nonly on agents with the ability to learn and to act autono-\nmously (i.e., AI-based agents). Moreover, recent develop-\nments in large language models have further changed the\ndynamic relationship between users and AI-based agents\n(Chen and Chan 2023; Dwivedi et al. 2023). AI-based\nagents that rely on large language models are now able to\nuse natural language indistinguishable from that of a\nhuman to assist in creative tasks, write computer code, and\nbe utilized for brainstorming (Chen and Chan\n2023;\nMemmert and Tavanapour 2023; Peng et al. 2023). Over-\nall, these AI-based agents are increasingly perceived not\nonly as passive tools, but rather as partners capable of\nmeaningful collaboration (Anthony et al. 2023; Chen and\nChan 2023; Dennis et al. 2023Hao et al. 2024). This\ndevelopment has led to a growing interest in understanding\nthe collaboration between users and AI-based agents and\nits consequences for user perception (Ariani 2024; Adam\net al. 2024a).\nOne of the key beneﬁts of AI-based agents is their\nability to provide help to users based on different types of\ninitiation, including reactive and proactive help. Reactive\nhelp from AI-based agents is assistance that is provided by\nan agent when the user explicitly requests it (Li et al. 2015;\nBaird and Maruping 2021). Reactive help can appear in the\nform of answering questions, providing recommendations,\nor suggesting solutions to problems (Kraus et al. 2021). In\ncontrast, proactive help from AI-based agents is assistance\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nthat is provided by an agent without the user explicitly\nrequesting help. This type of help is based on the ability of\nAI-based agents to anticipate users’ needs or preferences\nand to provide assistance accordingly (Hukal et al. 2019;\nBaird and Maruping 2021; Adam et al. 2024b). Proactive\nhelp can provide recommendations, remind users of\nimportant events or tasks, or notify users of potential\nproblems (Kraus et al. 2021). While the concept of and the\ndifferentiation between reactive and proactive help in\nhuman–human collaboration is already well established in\nthe management literature (e.g., Spitzmuller and Van Dyne\n2013; Lee et al. 2019; Parker et al. 2019; Harari et al.\n2022), most of the previous IS literature has made no real\ndistinction between these different types of help initiation\n(e.g., Fuegener et al. 2021, 2022; Gnewuch et al. 2022),\neven though reactive and proactive help has differences in\nterms of design, implementation, and the provided out-\ncomes (Parker et al. 2010). In IS research, reactive help is\npredominantly welcomed and favorably evaluated by users\n(e.g., Komiak and Benbasat 2006; Qiu and Benbasat 2009).\nHowever, previous research has demonstrated that reactive\nhelp can have disadvantages compared to proactive help.\nFor example, Lee et al. ( 2019) showed that receiving\nproactive rather than reactive help at work can undermine\nhelp recipients’ competence and self-esteem. Furthermore,\nthey demonstrated that help recipients showed less grati-\ntude to the helper when they received proactive rather than\nreactive help (Lee et al. 2019). Additionally, Harari et al.\n(2022) demonstrated that receiving proactive help was less\nlikely to be accepted than receiving reactive help, as\nproactive help can challenge the competencies of the help\nrecipient, especially when the help provider has a higher\nstatus at work.\nNevertheless, the receipt of proactive rather than reac-\ntive help may confer certain advantages. For instance,\nManiktala et al. ( 2023) showed that users who received\nproactive help from an AI-based agent had signiﬁcantly\nhigher accuracy in solving problems than users who only\nhad reactive help available to them. Furthermore, Bar-Or\nand Meyer ( 2019) demonstrated that users who received\nproactive rather than reactive help when interacting with\ntechnology could increase their task performance, espe-\ncially when the task was difﬁcult. However, they also\nshowed that users were less interested in receiving proac-\ntive help compared to reactive help in the future, although\nproactive help increased their task performance (Bar-Or\nand Meyer 2019). Moreover, recent IS research has\ndemonstrated that humans tend not to ask for help from AI-\nbased agents, even if asking for help increases overall task\nperformance (Fuegener et al. 2022). According to Fuegener\net al. ( 2022), a lack of meta-knowledge can explain this\nphenomenon. Users incorrectly assess their competence\ncompared to the competence of the agent who is providing\nthe help. Consequently, the number of times users request\nhelp falls short of the optimal frequency (Yzerbyt et al.\n1998; Fuegener et al. 2022). Implementing proactive help\ncan contribute to overcoming this effect by increasing the\nfrequency of users accepting help from AI-based agents\n(Kraus et al. 2021) and subsequently improving their per-\nformance (Fuegener et al. 2021, 2022). However, to take\nfull advantage of the potential performance gains from\nproactive AI-based agents, it is important to ensure that\nusers are willing to interact with them. Therefore, it is\nessential that users are satisﬁed with the AI-based agent\nbecause if users are not satisﬁed with an AI-based agent,\nthey will be less motivated to interact with it (Briggs et al.\n2008; Ashfaq et al. 2020; Hsiao and Chen 2022).\nRecent literature demonstrates that the effectiveness of\nreceiving help from an AI-based agent can depend on the\nuser’s level of AI knowledge (Pinski et al. 2023a). AI\nknowledge describes the skill level or level of under-\nstanding a human has about the capabilities and applica-\ntions of AI-based systems (Chiu 2021; Pinski et al. 2023a).\nAs AI-based agents have become more integrated into\nvarious aspects of daily life and industries, it has become\nessential for users to have a foundational understanding of\nthe capabilities and applications of AI-based agents.\nInsufﬁcient knowledge about the capabilities of AI-based\nagents can lead to user misunderstandings, while higher\nlevels of AI knowledge can help users make better deci-\nsions when receiving help from AI-based agents (Long and\nMagerko 2020). For instance, recent research has shown\nthat a higher level of AI knowledge improves users’ ability\nto make decisions when deciding whether to delegate tasks\nto AI-based agents or perform them on their own (Pinski\net al. 2023a). Despite its potential to increase users’ deci-\nsion-making in human–AI collaboration, knowledge about\nthe capabilities of AI-based agents can also be an obstacle.\nPrevious research has shown that knowledge about the\ncapabilities of a collaborator can negatively affect the\nperception of one’s own competence, especially when the\ncollaborators’ capabilities are perceived to be high (e.g.,\nAlicke et al. 1997; Boissicat et al. 2012). As a result, a\nlower perceived competence can have a negative impact on\nones’ motivation and well-being (Deci and Ryan 2008).\nWhen collaborating with AI-based agents a high level of\nAI knowledge can hinder the effective utilization of them.\nIn particular, AI knowledge can lead to a lower intention to\nactually collaborate with AI-based agents (Pinski et al.\n2023a; Tully et al. 2023). However, the impact of AI\nknowledge linked to the different types of help—proactive\nand reactive—from AI-based agents is still poorly\nunderstood.\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\n2.2 Self-Determination Theory in Human–AI\nCollaboration\nWhile receiving proactive help from AI-based agents can\nbe beneﬁcial in human–AI collaboration (Bar-Or and\nMeyer 2019; Fuegener et al. 2022), it can also lead to\nadverse reactions compared to reactive help (Adam et al.\n2023a). When receiving help from proactive AI-based\nagents, help recipients can feel dissatisﬁed with the system\n(Wenninger et al. 2022). Individuals can feel challenged in\ntheir competencies when receiving proactive rather than\nreactive help, especially for work-related tasks (Lee et al.\n2019; Harari et al. 2022). According to Calvo et al. ( 2020),\nself-determination theory can explain the undermining\neffect on users’ competence when collaborating with AI-\nbased agents. Self-determination theory proposes that\nindividuals have three innate psychological needs—au-\ntonomy, competence, and relatedness—that affect their\nwell-being and experience (Deci and Ryan 2000). These\nneeds are ‘‘essential for ongoing psychological growth,\nintegrity, and well-being’’ (Deci and Ryan 2000, p. 229).\nThereby, autonomy denotes a sense of being in control of\none’s own behavior (DeCharms 1968; Deci and Ryan\n1985). Competence refers to the feeling of being capable\nand effective in one’s engagement with the environment,\nwhile relatedness pertains to an individual’s sense of being\nconnected to others and having a sense of belonging (White\n1959; Deci and Ryan 1985; Ryan and Deci 2000; Deci\net al. 2008). When these needs are satisﬁed, individuals\ntend to be more engaged and experience a greater sense of\nwell-being (Deci and Ryan 2000; Ryan and Deci 2000).\nConversely, when these needs are unmet, individuals may\nexperience disengagement or ill-being (Ryan and Deci\n2000). Among other things, external events can contribute\nto the dissatisfaction of these psychological needs by\nundermining an individual’s sense of autonomy or com-\npetence (Deci and Ryan 2012; Ryan and Deci 2000). For\ninstance, when individuals encounter an event that trans-\nmits a feeling of being controlled, their need for autonomy\ncan be thwarted. Similarly, events that give individuals\nfeedback can affect their need satisfaction. While positive\nfeedback will increase individuals’ perceived competence,\nnegative feedback will transmit the feeling of being\nincompetent, dissatisfying their psychological need for\ncompetence (Deci and Ryan 2012).\nIn the context of IS, self-determination theory has\nalready been established as a relevant theory for under-\nstanding human behavior and perception when using\ntechnology (e.g., Karahanna et al. 2018; Lu et al. 2021;\nCroitor et al. 2022). Croitor et al. ( 2022) drew on self-\ndetermination theory to explain how the perception of input\ncontrol elements on online platforms can affect user sat-\nisfaction. Lu et al. ( 2021) showed that gamiﬁed\nmechanisms on online forums can motivate users to par-\nticipate, and Karahanna et al. ( 2018) demonstrated that the\nsatisfaction of psychological needs can affect users’\nbehavior on social media. Although all three psychological\nneeds can be affected when interacting with AI-based\nagents (De Vreede et al. 2021; Yang and Aurisicchio\n2021), in this study, we focus primarily on the psycho-\nlogical need for competence, as it is particularly relevant in\ntask-oriented contexts (Ryan and Deci 2000; Deci et al.\n2008). Furthermore, IS research demonstrates that users’\npsychological need for competence can be undermined\nwhen receiving help from AI-based agents and can be\naffected by users’ AI knowledge (e.g., Calvo et al. 2020;\nYang and Aurisicchio 2021). In particular, recent literature\nshows that collaborating with AI-based agents can affect\nusers’ psychological need for competence and thus users’\nengagement and well-being in human–AI collaboration in\nvarious ways (e.g., De Vreede et al. 2021; Yang and\nAurisicchio 2021). De Vreede et al. ( 2021) found that\nusers’ psychological need for competence can impact their\nsatisfaction when collaborating with an AI-based agent.\nYang and Aurisicchio ( 2021) demonstrated that users’\nknowledge of the capabilities of AI-based agents while\ninteracting with them can affect their need for competence,\nand Nguyen et al. ( 2022) showed that the satisfaction of\nusers’ competence can increase users’ system satisfaction\nwhen interacting with an AI-based agent. However, while\nprevious research based on self-determination theory has\nprovided valuable insights into how collaboration with AI-\nbased agents can affect individuals, we have little under-\nstanding of how and why proactive help (vs. reactive help)\nfrom AI-based agents may affect users’ feeling of being\ncompetent and thus decrease users’ system satisfaction in\nhuman–AI collaboration.\n3 Research Model and Hypothesis Development\nDrawing on self-determination theory as our theoretical\nlens, we derive our research model that sheds light on\nproactive (vs. reactive) help from AI-based agents and its\neffect on users’ system satisfaction in human–AI collabo-\nration. As depicted in Fig. 1, our research model proposes\nthat proactive (vs. reactive) help from AI-based agents\naffects users’ system satisfaction (H1) and that a loss of\ncompetence-based self-esteem mediates this effect (H2). In\naddition, we theorize that users’ AI knowledge inﬂuences\nthe indirect effect of proactive (vs. reactive) help from AI-\nbased agents on users’ system satisfaction via a loss of\ncompetence-based self-esteem (H3).\nPrior studies on human–AI collaboration have demon-\nstrated that receiving help from AI-based agents can\nenhance efﬁciency and increase users’ productivity (e.g.,\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nPeng et al. 2023). However, previous research has also\nshown that help from AI-based agents can lead to adverse\nreactions, especially when help is offered proactively rather\nthan reactively. For instance, Wenninger et al. ( 2022)\ndemonstrated that receiving help from AI-based agents can\nreduce user satisfaction, even if the help received was\nbeneﬁcial. Furthermore, Fuegener et al. ( 2022) found that\nwhen users receive proactive help from AI-based agents in\nhuman–AI collaboration, some users tend to adjust their\nbehavior to avoid receiving such help. This could result\nfrom the fact that users of AI-based agents must be satisﬁed\nwith them to maintain their motivation to interact with the\nAI-based agent (Briggs et al. 2008; Ashfaq et al. 2020;\nHsiao and Chen 2022). This potential negative effect of\nreceiving help from AI-based agents on user satisfaction is\nparticularly apparent when AI-based agents act autono-\nmously by proactively engaging with their users (Wen-\nninger et al. 2022), as humans tend to prefer non-\nautonomously-acting AI-based agents (Latikka et al. 2021).\nIn particular, receiving help proactively rather than reac-\ntively can negatively affect the perceptions of the help\nrecipients (Lee et al. 2019; Harari et al. 2022; Adam et al.\n2023a). All in all, these studies suggest that receiving help\nproactively rather than reactively may affect users’ system\nsatisfaction. Accordingly, we derive our ﬁrst hypothesis:\nH1 Proactive (vs. reactive) help from AI-based agents\nleads to lower users’ system satisfaction.\nTo further investigate the potential effect of proactive\n(vs. reactive) help from AI-based agents on users’ system\nsatisfaction and to explore its underlying mechanisms, we\ndraw on self-determination theory. Self-determination\ntheory proposes that individuals have a psychological need\nfor competence, referring to the feeling of being capable\nand effective in their engagement with the environment\n(Deci and Ryan 2000). When the psychological need for\ncompetence is not satisﬁed, individuals may experience\nlower self-esteem and satisfaction (Myers and Diener 1995;\nReis et al. 2000; De Vreede et al. 2021). Dissatisfaction\nwith the psychological need for competence can arise\nthrough interactions with technology (Croitor et al. 2022;\nGagne´ et al. 2022). Self-determination theory states that\nexternal events can contribute to the dissatisfaction of the\npsychological need for competence by conveying a sense\nof incompetence, undermining individuals’ feeling of being\ncapable and effective in their engagement with the envi-\nronment (Ryan and Deci 2000; Deci and Ryan 2012). In the\ncontext of receiving help, especially proactive help can\nlower the self-esteem and competence of help recipients by\ntransmitting a feeling that their competencies are being\nquestioned, thus decreasing their well-being (Lee et al.\n2019; Harari et al. 2022). In particular, receiving help from\ncolleagues at work can negatively affect the competence-\nbased self-esteem of the help recipient (Deelstra et al.\n2003).\nTherefore, we argue that proactive help from AI-based\nagents poses a more signiﬁcant level of dissatisfaction with\nusers’ psychological need for competence than reactive\nhelp. By acting proactively, AI-based agents transmit the\nsense that users’ competencies are being questioned,\nleading to a loss of competence-based self-esteem. This\nunfulﬁllment of users’ psychological need for competence\nwill result in negative feelings, especially lower satisfac-\ntion, toward the system that provides help to the user.\nH2 Proactive (vs. reactive) help from AI-based agents\nleads to lower users’ system satisfaction through a higher\nloss of users’ competence-based self-esteem.\nIndividuals’ psychological need for competence can be\nnegatively affected by external events, giving them nega-\ntive feedback relating to their competencies when engaging\nwith their environment (Ryan and Deci 2000\n; Deci and\nRyan 2012). However, the undermining effect of proactive\nhelp from AI-based agents on individuals’ need for com-\npetence depends on how much the external event is per-\nceived as meaningful negative feedback (Deci and Ryan\n2012). Thus, users’ evaluation of help when seeking or\nreceiving help from another individual is related to their\nperceived competencies (Bamberger 2009; Harari et al.\n2022). Furthermore, Harari et al. ( 2022) showed that\nFig. 1 Research model\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nproactive help challenges individuals’ competence, espe-\ncially when the help provider has a higher status than the\nrecipient (Harari et al. 2022). The capabilities of AI-based\nagents continue to improve and have become more\nadvanced in various organizational domains (Dwivedi et al.\n2023). They have already advanced to the point that they\ncan take over entire work processes, including central tasks\nemployees perform in their professions (Strich et al. 2021).\nMoreover, AI-based agents surpass the performance of\nhuman beings in tasks such as image classiﬁcation (Fue-\ngener et al. 2022). Users with higher levels of AI knowl-\nedge are especially aware of this development and the\ncurrent capabilities of AI-based agents. While AI knowl-\nedge has many advantages in collaboration with AI-based\nagents, such as improved decision-making, higher AI\nknowledge levels can reduce the intention to use (Pinski\net al. 2023a) and the receptivity of AI-based agents (Tully\net al. 2023).\nTherefore, we argue that users with greater AI\nknowledge are more likely t o be aware of its technical\ncapabilities and potential to p erform better than humans.\nConsequently, they may perc eive the competence-chal-\nlenging characteristics of proactive (vs. reactive) help\nfrom AI-based agents as a more signiﬁcant impediment\nto their psychological need for competence. Thus, we\nhypothesize that the more knowledgeable users are about\nAI, the more they will perceive proactive help from AI-\nbased agents as meaningful negative feedback on their\ncompetencies in engageme nt with the environment.\nTherefore, and in line with H1 and H2, we derive our\nthird hypothesis:\nH3 The effect of proactive (vs. reactive) help from AI-\nbased agents on users’ competence-based self-esteem is\nmoderated by the users’ AI knowledge, in that a higher\nlevel of AI knowledge leads to a greater loss of users’\ncompetence-based self-esteem.\n4 Research Methodology\n4.1 Experimental Design and Procedure\nFor our study, we decided to focus on software develop-\nment as our context, especially programming, for several\nreasons. Recent technological advances in AI have enabled\ninformation systems to support humans in writing com-\nputer code more efﬁciently (Kalliamvakou 2022). Along\nwith this development, AI-based agents are gaining popu-\nlarity among software developers to support them in writ-\ning and improving the quality of their code (Baird and\nMaruping 2021; Kalliamvakou 2022). For instance, Tab-\nnine and GitHub Copilot\n2 can be implemented in the most\ncommon programming editors to write code for the user\nbased on prompts, stated code lines, or the project’s context\n(GitHub Copilot 2023; Tabnine 2023). Another example of\nan AI-based agent that can write code for software devel-\nopers is ChatGPT, developed by OpenAI. ChatGPT, with\nover 100 million active users (Similarweb 2023), is\ndesigned to interact conversationally and to answer or\nfollow user prompts (OpenAI 2022). AI-based agents, such\nas ChatGPT and GitHub Copilot, do not just write or rec-\nommend code; they can also write code documentation,\ndescribe the logic in functions, or ﬁnd errors in existing\ncode (GitHub Copilot 2023; OpenAI 2022). Those\nadvanced capabilities of AI-based agents specialized in\nprogramming have the potential to revolutionize the way\nsoftware developers work in the future. Therefore, software\ndevelopment is a well-ﬁtting and practically relevant con-\ntext for exploring users’ system satisfaction in human–AI\ncollaboration.\nTo compare the effects of users’ system satisfaction\nwhen receiving help from AI-based agents in human–AI\ncollaboration, we conducted a vignette-based online\nexperiment with a 2 9 1 design (type of help: reactive vs.\nproactive) and between-subject treatments using SoSci\nSurvey.\n3 The vignette study methodology is a well-estab-\nlished method in IS research for capturing user perceptions\nwhen interacting with agents (e.g., Benlian et al. 2020).\nWhile online experiments are associated with high internal\nvalidity, they often face difﬁculties in providing external\nvalidity (Aguinis and Bradley 2014). Vignette studies (i.e.,\nan experimental survey method based on a description of a\nreal-world scenario or person in which participants are\nasked to immerse themselves) address these difﬁculties by\nproviding the opportunity to simulate a realistic scenario\nwithin a controlled online experiment (Atzmu ¨ ller and\nSteiner 2010; Eiﬂer and Petzold 2019). The potential high\nlevel of realism gives vignette studies higher external\nvalidity (i.e., the generalizability of ﬁndings to real-world\nproblems) than other experimental methods conducted in a\ncontrolled environment while still maintaining high inter-\nnal validity (Atzmu ¨ ller and Steiner 2010; Aguinis and\nBradley 2014; Eiﬂer and Petzold 2019). However, the\ndesign of a vignette study still has the potential to lack\nexternal validity, especially compared to non-experimental\nsettings or ﬁeld experiments, which are typically charac-\nterized by their high external but low internal validity\n(Eiﬂer and Petzold 2019). We chose the vignette study\n2 GitHub Copilot, developed by GitHub Inc., is an AI-based pair\nprogrammer based on the OpenAI language model Codex (GitHub\nCopilot 2023).\n3 SoSci Survey is an advanced tool for creating online questionnaires\nand experiments (Leiner 2024).\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nmethod for our experimental design due to its high internal\nvalidity, resulting from a high level of control in the\nexperimental setting while ensuring external validity\nwithin its limits (Aguinis and Bradley 2014; Eiﬂer and\nPetzold 2019). In particular, recent research shows that\nvignette studies are an effective way to compare the effects\nof receiving reactive versus proactive help (e.g., Harari\net al. 2022). The associated high level of control provides\nthe opportunity to control external factors that may inﬂu-\nence the reliability of an experiment that compares the\neffects of reactive and proactive help, such as the timing\nwhen help is asked for or offered. Especially, a reactive\nhelp condition requires a high level of control to ensure that\nthe help recipient requests help. In other experimental\nsettings with higher external but lower internal validity,\nthis may not be guaranteed because participants could opt\nnot to ask for help during the experiment. In addition, the\nsurvey-based nature of a vignette study is particularly\nuseful for capturing subjective data, such as participants’\nsatisfaction with the system when working with an AI-\nbased agent, whereas other methods in a non-experimental\nsetting are more suitable for measuring objective data, such\nas performance.\nAccording to established standards for vignette-based\nonline experiments (Atzmu ¨ ller and Steiner 2010; Aguinis\nand Bradley 2014), we instructed participants to report\ntheir emotions after assuming the role of a ﬁctional person\nand engaging in a simulated collaboration with an AI-based\nagent. The experimental procedure of our vignette-based\nonline experiment was as follows. First, we welcomed our\nparticipants to the experiment and told them they would be\nparticipating in an online survey about technology use at\nwork. We then randomly assigned our participants to one\nof our two experimental conditions (proactive and reactive\nhelp) by means of the SoSci Survey randomization func-\ntion to ensure randomization with roughly equal distribu-\ntion across both groups independent from other variables.\nNext, we asked all participants to step into the shoes of our\nﬁctional person, Alex, a software developer at an IT\ncompany whose main activity is writing code. In addition,\nthey read that Alex identiﬁes as a code writer. Then, they\nread that the manager was asking Alex to write code for an\nimportant new feature. In the next step, all participants read\nabout the recent introduction of CoCreator\nTM by the\nmanager. CoCreator TM was presented to the participants as\nan AI-based agent possessing the same level of coding\nproﬁciency as Alex and other human team members so as\nto avoid unintended effects due to skill differences and to\nfocus the study on the effect of receiving proactive (vs.\nreactive) help from AI-based agents rather than on per-\nformance differentials between help recipients and help\nproviders. After introducing Alex and CoCreator\nTM, the\nparticipants experienced one of our two manipulations of\nType of Help (reactive vs. proactive) according to their\nassigned condition. For our manipulation, the participants\nread about and saw a screenshot of the collaboration with\nCoCreatorTM. For the sake of realism, we decided to\nintegrate our manipulation into a mock-up of the integrated\ndevelopment environment (IDE) Visual Studio Code.\nAccording to a survey by Stack Overﬂow, Visual Studio\nCode is the most common IDE among developers (Stack\nOverﬂow 2023). See Appendix A (available online via\nhttp://link.springer.com) for the exact wording and pre-\nsentation of the text-based version of our manipulation for\nboth conditions.\nTo improve the perception of the collaboration, the\nparticipants also experienced the manipulation in a 30-s\nvideo. In both conditions, Alex begins to type the code for\nthe code-writing task for the new feature and realizes it\ntakes longer than usual to write the code. To avoid any\nunintended effects of the typed code for the code-writing\ntask, we chose to use a dummy code that appeared realistic\nbut had no speciﬁc functionality, as presented in Figure A.1\nin the Appendix. Subsequently, CoCreator\nTM offers help\neither proactively or reactively. In the reactive help con-\ndition, Alex moves the mouse cursor to a button and clicks\nit to start the collaboration with CoCreator TM, in which\nAlex actively asks for help. In the proactive help condition,\nCoCreatorTM offers help proactively, without any request\nfrom Alex. See Figs. 2 and 3 for a detailed presentation of\nthe visualization of our reactive and proactive help\nmanipulation. Both help manipulations began after Alex\nﬁnished a line of code to reduce the perceptions of dis-\nruption. In the last step, all participants answered a post-\nexperimental questionnaire that captured the manipulation\nchecks (i.e., perceived proactivity and reactivity of\nCoCreator\nTM’s help offer), our constructs, socio-demo-\ngraphics, and several control variables.\n4.2 Measured Variables and Manipulation Checks\nWe measured our dependent variable, System Satisfaction ,\nwith ﬁve items adapted from Brown et al. ( 2008) (e.g., ‘‘I\nthink I could become an enthusiastic user of CoCrea-\ntorTM’’). We measured our mediator Loss of Competence-\nBased Self-Esteem with three items adapted from Craig\net al. ( 2019) (e.g., ‘‘CoCreatorTM makes me feel less con-\nﬁdent that I understand things well enough to get work\ndone’’). For our moderator variable, AI Knowledge ,w e\nused four items from Chiu et al. ( 2021) (e.g., ‘‘I know\npretty much about AI’’). In addition to our constructs for\nour mediator, moderator, and dependent variables, we\nmeasured several control variables to account for potential\nfurther explanatory factors in our experiment. In addition to\ndemographics—that is, age and gender ( male, female, and\nothers)—we measured Personal Innovativeness (e.g., ‘‘If I\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nheard about a new technology, I would look for ways to\nexperiment with it’’) (Agarwal and Prasad 1998) to control\nfor users’ openness to new technologies, as we assume that\nusers with higher personal innovativeness are less likely to\nbe affected by proactive behavior from AI-based agents, as\nthey are more likely to welcome the initiated interaction.\nWe measured Product Knowledge (‘‘Are you familiar with\nAI pair programmer software, such as GitHub’s Copilot?’’)\n(Qiu and Benbasat 2010) to control for prior experience in\ninteractions with AI-based agents, as we assume that users\nwho are not familiar with interacting with AI-based agents\nare more likely to be taken by surprise, affecting their\nsatisfaction when receiving proactive help from an AI-\nbased agent. We measured Programming Frequencies\n(‘‘How often have you written code in the last three\nmonths?’’) (Harari et al. 2022) to address users’ experience\nin programming, as we assume that users’ competence in\nprogramming could affect users’ competence-based self-\nesteem when receiving help for a programming task from a\nproactive AI-based agent. Finally, we measured Negative\nAffectivity (e.g., ‘‘My feelings are hurt rather easily’’)\n(Ayyagari et al. 2011) as it reﬂects users’ ‘‘tendency to\nexperience negative emotional states and low self-esteem’’\n(Ayyagari et al. 2011, p. 842), and we assume that users\nwho tend to experience low self-esteem will experience a\nhigher loss of competence-based self-esteem when they\nreceive proactive rather than reactive help from an AI-\nbased agent.\nTo ensure the reliability and validity of our variables, we\nmeasured Cronbach’s alpha, composite reliability, and\naverage variance extracted (AVE). For all of our con-\nstructs, the values for Cronbach’s alpha and composite\nreliability exceeded the threshold value of 0.70, displaying\nhigh internal consistency (Nunnally 1978). In addition, the\nAVEs were above the threshold of 0.50, indicating good\nconvergent validity, suggesting that our measures are\nreliable and accurately reﬂect the underlying construct\n(Hair et al. 2021). Moreover, the square roots of the AVEs\nexceeded the inter-construct correlations, providing evi-\ndence for the discriminant validity of our constructs (For-\nnell and Larcker 1981). See Table A.1 and Table A.2 in the\nAppendix for further information about the measured\nitems.\nTo check for the effectiveness of our manipulation Type\nof Help (proactive vs. reactive), we measured three items to\nevaluate the reactive help manipulation (e.g.,\nFig. 2 Screenshot of the\nreactive help manipulation\nFig. 3 Screenshot of the\nproactive help manipulation\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nTable 1 Ordinary Least Squares Regression analyses on loss of competence-based self-esteem and system satisfaction\nLoss of competence-based self-esteem System satisfaction\nModel (1) Model (2) Model (3) Model (4)\nIntercept 2.02\n(1.27)\n2.20\n(1.27)\n4.65***\n(1.18)\n5.22***\n(1.14)\nManipulation\nType of help 0.86\n*\n(0.42)\n0.82*\n(0.41)\n-1.33***\n(0.38)\n-1.08**\n(0.38)\nMediation\nLoss of competence-based self-esteem – – – -0.28\n**\n(0.10)\nModeration\nAI knowledge – -0.23\n(0.23)\n––\nType of help x\nAI knowledge\n– 0.70\n*\n(0.29)\n––\nControls\nAge 0.00\n(0.02)\n-0.01\n(0.02)\n-0.03\n(0.02)\n-0.03\n(0.02)\nGender -0.63\n(0.44)\n-0.68\n(0.43)\n0.29\n(0.40)\n0.11\n(0.39)\nPersonal innovativeness -0.10\n(0.15)\n-0.03\n(0.17)\n0.27\n(0.14)\n0.27*(0.13)\nProduct knowledge 0.73\n(0.47)\n0.77\n(0.47)\n-0.75\n(0.43)\n-0.54\n(0.42)\nProgramming frequencies 0.09\n(0.12)\n0.07\n(0.12)\n-0.19\n(0.11)\n-0.16\n(0.11)\nNegative affectivity 0.27*\n(0.12)\n0.30\n(0.12)\n0.07\n(0.11)\n0.15\n(0.11)\nR2 0.14 0.20 0.18 0.26\nTable 2 Direct and indirect effects of (moderated) mediation analyses\nAI\nKnowledge*\nEffect SE CI Signiﬁcance\nH1: direct Type of Help— [ System Satisfaction – -1.08 0.38 [ -1.83, -0.33] Yes\nH2: indirect Type of Help— [ LOCBSE— [\nSystem Satisfaction\n– -0.24 0.17 [ -0.65, -0.01] Yes\nH3: Index of moderated mediation Type of Help: AI Knowledge— [\nLOCBSE— [ System Satisfaction\n– -0.20 0.12 [ -0.47, -0.01] Yes\nH3: Conditional effect Type of Help— [ LOCBSE 3.33\n4.67\n6.00\n-0.12\n0.81\n1.75\n0.58\n0.41\n0.55\n[-1.27, 1.03]\n[ 0.01, 1.62]\n[ 0.65, 2.85]\nNo\nYes\nYes\nH3: Conditional indirect effect Type of Help— [ LOCBSE— [\nSystem Satisfaction\n3.33\n4.67\n6.00\n0.03\n-0.23\n-0.49\n0.17\n0.16\n0.27\n[-0.34, 0.36]\n[-0.63, -0.00]\n[-1.14, -0.07]\nNo\nYes\nYes\nLOCBSE = Loss of competence-based self-esteem; CI = 95% conﬁdence interval; Signiﬁcance is determined by whether the CI includes zero;\n*16th, 50th, and 84th percentiles of the distribution, Johnson–Neyman signiﬁcance region [ 4.66\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\n‘‘CoCreatorTM agreed to do things for me when I asked’’)\nand three items to evaluate the proactive help manipulation\n(e.g., ‘‘CoCreatorTM demonstrated initiative in helping me\nin advance of being asked’’) adapted from Harari et al.\n(2022). The results of several ANOVAs indicate that our\nmanipulation achieved its desired effect. The participants\nin the reactive help condition perceived the help offered by\nthe AI-based agent as more reactive compared to the\nproactive help condition ( F = 130.10, p \\ 0.001). Addi-\ntionally, the participants in the proactive help condition\nperceived the help offered by the AI-based agent as more\nproactive compared to the reactive help condition\n(F = 49.88, p \\ 0.001). For more details about the\nmanipulation checks, see Table A.3 in the Appendix.\n4.3 Sample Description and Data Collection\nWe recruited 100 participants with programming experi-\nence from the crowdsourcing platform Proliﬁc.co. Pro-\nliﬁc.co is an established platform for acquiring subjects for\nacademic studies (Proliﬁc 2023) and is specially designed\nfor scientiﬁc purposes (Palan and Schitter 2018). Accord-\ning to multiple studies, Proliﬁc.co respondents’ data have\ndemonstrated high reliability and quality (e.g., Peer et al.\n2017; Palan and Schitter 2018). To ensure the quality of\nour data and to capture the population of interest, we\nrestricted access to our study to participants with a mini-\nmum age of 18 who were ﬂuent in English and located in\nthe United States. We decided to recruit participants from\nthe United States because participants from the United\nStates tend to provide high data quality on crowdsourcing\nplatforms (Smith et al. 2016). Furthermore, a coherent\nsample limits potential cultural inﬂuences due to different\ncultural and regulatory contexts. Additionally, we applied\nﬁlters to include only participants who reported using\ntechnology at work at least four times a week and who had\nexperience in computer programming. Finally, we used a\nﬁlter provided by Proliﬁc.co to obtain participants roughly\nevenly distributed between the sexes. For our study, we\nremoved eight participants from our dataset due to them\nfailing our attention check (see Table A.1 in the Appendix)\n(5), for completing the experiment more than twice as fast\nas the average (2), or for exceeding the maximum allowed\ncompletion time by taking eight more hours to ﬁnish the\nsurvey (1). In total, this led to a ﬁnal dataset of 92 par-\nticipants for our statistical analysis. See Table A.4 in the\nAppendix for details on the descriptive statistics of our\nﬁnal dataset.\nTo ensure the random assignment of participants to our\nexperimental conditions, we performed several one-way\nANOVAs. The results show that there was no statistically\nsigniﬁcant difference between AI Knowledge (F = 0.92,\np [ 0.05) and our control variables Age (F = 3.26,\np [ 0.05), Gender (F = 0.04, p [ 0.05), Product Innova-\ntiveness (F = 0.22, p [ 0.05), Product Knowledge\n(F = 2.92, p [ 0.05), Programming Frequencies\n(F = 0.36, p [\n0.05), and Negative Affectivity (F = 0.03,\np [ 0.05) between our two experimental groups. There-\nfore, we can assume that the assignment of participants to\nour experimental conditions was successfully randomized.\nIn addition, we performed a post hoc power analysis using\nG* Power 3.1 (Faul et al. 2009) to verify that our ﬁnal\nsample size was adequate for our statistical analysis. The\npower analysis showed that our sample size of N = 92 was\nsufﬁcient, with a power level of 0.81, thus exceeding the\nrecommended threshold of 0.80 (Ellis 2010).\n5 Results\nTo test our hypotheses, we conducted four distinct ordinary\nleast squares (OLS) regression analyses, as depicted in\nTable 1 (model (1)–(4)). We executed linear regressions on\nour mediator Loss of Competence-Based Self-Esteem and\nour dependent variable System Satisfaction . Age, Gender\n(with female coded as one and other as zero), Negative\nAffectivity, Personal Innovativeness , Programming Fre-\nquencies, and Product Knowledge (with ‘‘Yes’’ coded as\none and ‘‘No’’ as zero) were included as control variables\nfor all regressions. Our independent variable Type of Help\nwas binary-coded with proactive help as one and reactive\nhelp as zero. Model (1) in Table 1 presents the results of an\nOLS regression, testing whether Type of Help has a sig-\nniﬁcant effect on Loss of Competence-Based Self-Esteem .\nModel (2) displays the results of an additional OLS\nregression analysis in which we extended the regression\nmodel used in model (1) to include AI Knowledge as an\nadditional variable to test whether AI Knowledge has a\nmoderating effect with Type of Help on Loss of Compe-\ntence-Based Self-Esteem . Model (3) presents the results of\nan OLS regression testing whether Type of Help has a\ndirect effect on System Satisfaction . For the results in\nmodel (4), we extended the regression model used for the\nresults in model (3) to include Loss of Competence-Based\nSelf-Esteem as an additional variable, in order to test\nwhether Loss of Competence-Based Self-Esteem has a\ndirect effect on System Satisfaction . The results of the OLS\nregression in model (1), (2), and (4) indicate whether Loss\nof Competence-Based Self-Esteem mediates the effect of\nType of Help on System Satisfaction (Shrout and Bolger\n2002).\nOur results from the third analysis (model (3)) show a\nsigniﬁcant effect of\nType of Help (proactive vs. reactive) on\nSystem Satisfaction (M = 3.14 vs. M = 4.23, b = - 1.33,\np \\ 0.001), indicating support for H1. In addition, our\nresults from the ﬁrst and fourth analyses show a signiﬁcant\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\neffect of Type of Help on Loss of Competence-Based Self-\nEsteem (M = 4.21 vs. M = 3.45, b = 0.86, p \\ 0.05, model\n(1)) and a signiﬁcant effect of Loss of Competence-Based\nSelf-Esteem on System Satisfaction (b = - 0.28, p \\ 0.01,\nmodel (4)). These results provide initial support for H2,\nindicating a signiﬁcant mediating effect of Loss of Com-\npetence-Based Self-Esteem on System Satisfaction\n(b = - 0.28, p \\ 0.01). To enhance the robustness of our\nanalysis, we conducted a bootstrap analysis with 5,000\nbootstrap samples and 95% conﬁdence intervals (CIs)\nusing PROCESS 4 model 4 (see Figure B.1 in Appendix B)\n(Hayes 2022). A bootstrap analysis can increase the\nrobustness of mediation analysis with a small- to medium-\nsized sample by using a resampling technique to determine\nwhether the sampling distribution of the mediated effect\ndiffers from zero (Shrout and Bolger 2002). As displayed\nin Table 2 and in support of H1, the direct effect of Type of\nHelp (proactive vs. reactive) on System Satisfaction is\nsigniﬁcant (direct effect = - 1.08, standard error = 0.38,\n95% CI = [ - 1.83, - 0.33]). Speciﬁcally, participants in\nthe proactive help condition were, on average, 1.08 points\n(on a 7-point Likert scale) less satisﬁed with the system\nthan participants in the reactive help condition. Thus,\nproactive (vs. reactive) help from AI-based agents leads to\nlower user system satisfaction. In support of H2, the results\nprovide evidence that the mediating effect of Loss of\nCompetence-Based Self-Esteem on System Satisfaction is\nsigniﬁcant (indirect effect = - 0.24, standard error =\n0.17, 95% CI = [ - 0.65, - 0.01]). Speciﬁcally, partici-\npants were, on average, 0.24 points less satisﬁed with the\nsystem as a result of the effect of a loss of competence-\nbased self-esteem when receiving proactive rather than\nreactive help from AI-based agents. Thus, proactive (vs.\nreactive) help from AI-based agents leads to lower user\nsystem satisfaction via an increased loss of competence-\nbased self-esteem.\nNext, we extended our model with AI Knowledge as a\nmoderator to test our third hypothesis (H3), suggesting that\nthe effect of proactive (vs. reactive) help from AI-based\nagents on Loss of Competence-Based Self-Esteem and thus\non System Satisfaction is moderated by AI Knowledge . The\nresults of our second analysis (model (2)) indicate a sig-\nniﬁcant interaction between Type of Help and AI Knowl-\nedge, with respect to Loss of Competence-Based Self-\nEsteem (b = 0.70, p \\ 0.05), providing initial support for\nH3. Consistent with our prior analysis, we conducted a\nbootstrap analysis with 5,000 bootstrap samples and 95%\nCIs using PROCESS model 7 (see Figure B.2 in Appendix\nB) (Hayes 2022) to provide a more robust moderated\nmediation analysis. As displayed in Table 2 and in support\nof H3, the results provide evidence that the moderated\nmediating effect of Type of Help and AI Knowledge on\nSystem Satisfaction via Loss of Competence-Based Self-\nEsteem is signiﬁcant (index of moderated media-\ntion = - 0.20, standard error = 0.12, 95% CI = [ -\n0.47,\n- 0.01]). The negative indirect effect of proactive (vs.\nreactive) help on System Satisfaction via a loss of Com-\npetence-Based Self-Esteem (H2) increases by 0.20 points\n(on a 7-point Likert scale) per-point increase in AI\nKnowledge. Thus, the effect of proactive (vs. reactive) help\nfrom AI-based agents on users’ system satisfaction via a\nloss of competence-based self-esteem is larger when users\nhave a higher level of AI knowledge. Furthermore, the\nresults in Table 2 show that this effect is only signiﬁcant\nwhen users’ AI knowledge is higher than 4.66 on a 7-point\nLikert scale. Participants with AI Knowledge of 4.66 (or\n6.00) were, on average, 0.23 (or 0.49) points (on a 7-point\nLikert scale) less satisﬁed with the system as a result of the\neffect of a loss of competence-based self-esteem when\nreceiving proactive rather than reactive help from AI-based\nagents. In addition, the effect sizes displayed in Table 2\nshow that AI Knowledge had such a large effect on the\ndirect effect of Type of Help on Loss of Competence-Based\nSelf-Esteem that participants with AI Knowledge equal to or\ngreater than 6.00 (on a 7-point Likert scale) showed a\ndirect effect more than twice as large as the average (1.72\ncompared to 0.82 on a 7-point Likert scale). Therefore, our\nresults show that users’ AI knowledge is an important\nfactor that can multiply users’ loss of competence-based\nself-esteem and, thus, system satisfaction when receiving\nproactive rather than reactive help from AI-based agents.\nFigure 4 demonstrates the slopes of the moderation of\nAI Knowledge at different levels (two, one, and zero\nstandard deviations (SDs) below and above the mean)\n(McCabe et al. 2018), giving an overview of the effects of\nType of Help on Loss of Competence-Based Self-Esteem at\ndifferent levels of AI Knowledge .\nTo further increase the robustness of our study, we ran\nseveral additional analyses as robustness checks (see\nAppendix B for more information). Speciﬁcally, to validate\nthe derivation of our research model, we tested whether our\nmoderator AI Knowledge had any additional moderating or\ndirect effects that were not included in our research model.\nAs displayed in Appendix C, we did not ﬁnd any additional\ndirect or moderating effects of AI Knowledge on our\nmediator, Loss of Competence-Based Self-Esteem, or on\nour dependent variable, System Satisfaction, a fact that\nsupports the adequacy of our research model. Furthermore,\nwe conducted an additional mediation analysis grounded in\nour research model but with Intention to Use as an alter-\nnative dependent variable, which is highly correlated with\nSystem Satisfaction (Sun et al. 2014; Hsiao and Chen\n2022). We collected Intention to Use as an additional\n4 PROCESS is an established tool for OLS regression-based path\nanalysis (Hayes 2023).\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nmeasure during the vignette study. As shown in Appendix\nB, this analysis yielded similar patterns of results to our\nmain analysis. This consistency across different but related\nconstructs further strengthens the reliability of our ﬁndings.\n6 Discussion\nCollaboration with AI-based agents at work is becoming\nincreasingly relevant for various tasks. This development\nhas evolved due to the increasing capabilities of AI-based\nagents (Dwivedi et al. 2023). The quality of their outcomes\nhas improved; they can now help proactively, and even\ntake over entire work tasks (Baird and Maruping 2021;\nStrich et al. 2021). These emerging capabilities of AI-based\nagents have the ability to increase users’ performance. In\nparticular, receiving proactive rather than reactive help\nfrom AI-based agents can increase users’ performance by\nanticipating users’ needs and offering help without being\nasked for it (Bar-Or and Meyer 2019; Fuegener et al.\n2022). However, recent IS studies show a tendency for\nusers to be deterred when receiving proactive help from\nAI-based agents (Bar-Or and Meyer 2019; Fuegener et al.\n2022). As such, a deeper understanding of how users will\nexperience proactive compared to reactive help from AI-\nbased agents in human–AI collaboration has become more\ncrucial. Therefore, this study aimed to shed light on users’\nsystem satisfaction when an AI-based agent offers proac-\ntive (vs. reactive) help at work.\nDrawing on self-determination theory, we ﬁnd that users\nshow, on average, about 1.08 points (on a 7-point Likert\nscale) lower system satisfaction when receiving proactive\n(vs. reactive) help from AI-based agents for tasks at work\nin human–AI collaboration (supporting H1). This signiﬁ-\ncant shift in system satisfaction is particularly important\nbecause introducing a proactive rather than a reactive AI-\nbased agent to support users at work may be more bene-\nﬁcial in terms of productivity (Bar-Or and Meyer 2019;\nFuegener 2022). However, the associated lower likelihood\nof system satisfaction may result in the beneﬁts of proac-\ntive agents not being fully realized, as users must be sat-\nisﬁed with the AI-based agent to be motivated to interact\nwith it (Ashfaq et al. 2020; Hsiao and Chen 2022).\nMoreover, and in support of H2, we ﬁnd that this effect is\nmediated by a higher loss of competence-based self-es-\nteem. In particular, and in support of H3, we ﬁnd that users\nwith high levels of AI knowledge show, on average, up to\nmore than twice as much (about 0.49 vs. 0.24 points on a\n7-point Likert scale) lower system satisfaction through a\nloss of competence-based self-esteem when receiving help\nfrom AI-based agents proactively rather than reactively.\nHowever, users with low levels of AI knowledge do not\nexperience a signiﬁcantly higher loss of competence-based\nself-esteem and thus do not experience lower system sat-\nisfaction due to this effect when receiving proactive rather\nthan reactive help from AI-based agents. These ﬁndings\nhave important practical implications as they suggest that\ndesigning AI-based agents to be proactive rather than\nreactive would be less convenient for users to work with,\nFig. 4 Illustration of the moderation of AI knowledge on the effect of type of help on loss of competence-based self-esteem; SD = Standard\nDeviation, PCTL = Percentile, b = Unstandardized Coefﬁcient of the Slope, CI = Conﬁdence Interval\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nespecially in use cases where users typically have high AI\nknowledge. In summary, our research ﬁndings provide a\ntheoretical foundation on which organizations can build\nwhen planning to implement proactive AI-based agents to\nfully realize the potential of such agents. Therefore, they\nhave signiﬁcant implications for the nascent ﬁeld of\nhuman–AI collaboration, both in its theoretical and prac-\ntical aspects, which we will discuss further in the following\nsections.\n6.1 Theoretical Contributions\nOur study contributes to IS research on human–AI col-\nlaboration in several ways. First, we enhance our under-\nstanding of how users’ system satisfaction is affected when\nreceiving proactive (vs. reactive) help from AI-based\nagents. Previous IS research has primarily focused on\nusers’ interactions with AI-based agents and how collab-\noration with them can improve efﬁciency and performance,\nignoring user satisfaction as an important prerequisite in\nhuman–AI collaboration (e.g., Fuegener et al. 2021; Boyac ı\net al. 2023), yet users’ system satisfaction is an important\nfactor in keeping users motivated to interact with AI-based\nagents (Briggs et al. 2008; Ashfaq et al. 2020; Hsiao and\nChen 2022). Moreover, recent studies on human–AI col-\nlaboration have increasingly employed proactive AI-based\nagents as the basis of their experimental design (e.g., Bauer\net al. 2023; Fuegener et al. 2021; Gnewuch et al. 2022;\nSchemmer et al. 2023). However, these studies have paid\nlittle attention to how the collaboration between human and\nAI-based agents was initiated, and thus they ignored the\neffects of different types of initiation when receiving help\nfrom AI-based agents. For instance, while Fuegener et al.\n(2021) demonstrate that receiving help from AI-based\nagents can increase users’ performance, they do not con-\nsider that the type of help initiation – in this case, proactive\nhelp – could have affected their results. Receiving proac-\ntive help from AI-based agents can improve users’ per-\nformance by anticipating users’ needs so that their results\ncould differ in a setting in which help from AI-based agents\nis provided reactively rather than proactively. Overall, we\nextend our knowledge of the collaboration between\nhumans and AI by improving our understanding of how\nusers’ system satisfaction is affected when they receive\nhelp from AI-based agents proactively rather than reac-\ntively. Consistent with previous research on human–human\ncollaboration (e.g., Lee et al. 2019; Harari et al. 2022), our\nﬁndings show that the type of help initiation inﬂuences\nusers’ perceptions when collaborating with an AI-based\nagent. We especially found that receiving proactive (vs.\nreactive) help from AI-based agents leads to lower system\nsatisfaction. This is a signiﬁcant contribution to the ﬁeld of\nIS research, which seeks to differentiate between\nproactively- and reactively-acting IS (e.g., Morana et al.\n2017; Baird and Maruping 2021). Furthermore, we give\nnew insights into users’ perceptions of human–AI collab-\noration and demonstrate that the differentiation between\ndifferent types of help cannot be ignored. Therefore, we\nsuggest that IS researchers in the ﬁeld of human–AI col-\nlaboration should differentiate between the different types\nof help from AI-based agents, such as reactive and proac-\ntive help, to consider their potential different effects on\nusers’ system satisfaction.\nSecond, and relatedly, we increase our understanding of\nhow collaboration with AI-based agents affects users’\nsystem satisfaction by identifying competence-based self-\nesteem as the driving force for this effect. While some\nstudies have already indicated that receiving help proac-\ntively from AI-based agents can lead to adverse reactions\nfrom help recipients (e.g., Strich et al. 2021; Fuegener et al.\n2021; Wenninger et al. 2022), they do not delve deeper to\nget to the bottom of these effects. For instance, Wenninger\net al ( 2022) demonstrate that receiving proactive help from\nautonomously acting AI-based agents can negatively affect\nusers’ system satisfaction. However, while they provide\nsome theoretical insights into the possible reasons for this\neffect, they do not empirically investigate potential\nunderlying psychological mechanisms that might explain\nwhy users can react negatively to proactive help from AI-\nbased agents. Moreover, many IS studies continue to pri-\noritize the positive implications, such as increased efﬁ-\nciency, of collaboration with AI-based agents (e.g., Boyac ı\net al. 2023; Fuegener et al. 2022). However, recent calls\nwithin the IS research ﬁeld (e.g., Adam et al. 2024a; Zhang\net al. 2024) have emphasized the necessity for a more\ncomprehensive and nuanced understanding of the potential\nchallenges associated with human–AI collaboration. We\nextend IS research on human–AI collaboration by taking a\ncloser look at potential negative effects depending on the\ntype of help initiation when collaborating with AI-based\nagents. Furthermore, we explore the reasoning behind the\nquestion of why users experience a loss of system satis-\nfaction when receiving help from AI-based agents proac-\ntively rather than reactively by falling back on self-\ndetermination theory, which posits that feelings of com-\npetence are crucial for engagement and well-being (Ryan\nand Deci 2000). Our ﬁndings show that a loss of compe-\ntence-based self-esteem is a determining factor for lower\nsystem satisfaction caused by proactive (vs. reactive) help\nfrom AI-based agents. Our reasoning is that receiving\nproactive help from AI-based agents transmits the feeling\nthat users’ competencies are being questioned. This ﬁnding\nis in line with management literature suggesting that\nreceiving proactive rather than reactive help at work can\ndecrease the self-esteem of help recipients and negatively\naffect their perception of their competence in human–\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nhuman collaboration (Lee et al. 2019; Harari et al. 2022).\nThus, our study provides valuable insights by demonstrat-\ning that future research should consider competence-based\nself-esteem as an underlying mechanism when evaluating\nusers’ reactions to different types of help from AI-based\nagents in human–AI collaboration.\nThird, we identiﬁed the degree of users’ AI knowledge\nas an essential characteristic that affects users’ system\nsatisfaction by amplifying the loss of competence-based\nself-esteem caused by proactive (vs. reactive) help from\nAI-based agents. While most previous research has pri-\nmarily seen high levels of AI knowledge as a positive\ncharacteristic in human–AI collaboration, improving the\nexperience and acceptance of AI-based agents (e.g.,\nSchoeffer et al. 2022; Pinski and Benlian 2024), recent\nstudies have demonstrated that AI knowledge can also lead\nto adverse effects toward the utilization of AI-based agents\nby reducing the intention to use them (e.g., Pinski et al.\n2023a; Tully et al. 2023). Our study contributes to this\nrecently obtained insight by revealing that high levels of AI\nknowledge can negatively inﬂuence users’ perceptions\nduring human–AI collaboration, depending on the type of\nhelp initiation when collaborating with AI-based agents.\nWe especially found that users have an even higher loss of\ncompetence-based self-esteem and, thus, lower system\nsatisfaction while collaborating with AI-based agents when\nreceiving help proactively rather than reactively. There-\nfore, we demonstrate that it is crucial also to highlight the\nnegative aspects of increased AI knowledge. Moreover, we\nsuggest that IS researchers should consider users’ AI\nknowledge when evaluating users’ perceptions in human–\nAI collaboration, especially when help is provided via\ndifferent types of initiation. Thus, this ﬁnding is an\nimportant and counterintuitive insight, revealing a situation\nin which a higher level of knowledge can have a negative\nimpact on users’ perceptions.\nFinally, we contribute to self-determination theory by\nintroducing the type of help initiation from AI-based agents\nas a new important design principle that can affect users’\npsychological need satisfaction when interacting with\ntechnology such as AI-based agents in human–AI collab-\noration. While self-determination theory has already been\nestablished as an important theory in IS research to derive\ndesign principles and to understand users’ perception in\nhuman–AI collaboration (e.g., De Vreede et al. 2021; Yang\nand Aurisicchio 2021), previous studies, as far as we know,\nhave not introduced the type of help initiation as an\nimportant design principle, affecting users’ psychological\nneed satisfaction when interacting with technology (e.g.,\nPinski et al. 2023b; Yang and Aurisicchio 2021). There-\nfore, we extend our understanding of what speciﬁc design\nprinciples can affect psychological needs demonstrating\nthat the type of help initiation from AI-based agents,\nespecially, receiving proactive rather than reactive help\nfrom AI-based agents, can affect users’ competence-based\nself-esteem leading to lower system satisfaction. This is an\nimportant insight researchers should consider when deriv-\ning IS artifacts based on self-determination theory.\n6.2 Implications for Practice\nOur research ﬁndings have practical implications for\norganizations that use or plan to use AI-based agents to\nsupport their employees in their work activities. Our study\nsuggests that receiving help from proactive rather than\nreactive AI-based agents at work can challenge employees’\ncompetencies, leading to lower employee system satisfac-\ntion. However, collaborating with proactive AI-based\nagents can be more efﬁcient than collaborating with reac-\ntive AI-based agents, increasing employees’ productivity\n(Bar-Or and Meyer 2019; Fuegener et al. 2022). To\neffectively handle this trade-off between employees’ sys-\ntem satisfaction and productivity when implementing AI-\nbased agents at work, we recommend several actions for\ndesigners of AI-based agents and managers.\nWe recommend that managers should deploy reactive\nrather than proactive AI-based agents for tasks that are\nstrongly related to the employees’ competencies or that do\nnot beneﬁt as much from collaboration with AI-based\nagents, such as low-time-consuming tasks. This ensures a\nhigher level of system satisfaction for employees when\ncollaborating with AI-based agents, while the unrealized\nincrease in productivity remains within limits. For other\ntasks, we recommend implementing proactive AI-based\nagents and introducing measures to counteract the adverse\neffects of proactive AI-based agents on employees’ system\nsatisfaction. As one of these measures, we suggest that\nmanagers should gather regular feedback after employees\nstart working with proactive AI-based agents to get infor-\nmation on how satisﬁed they are when receiving help from\nthose agents. This would give managers the opportunity to\ncounteract potential dissatisfaction quickly. Organizations\nneed to ensure that employees feel valued and respected in\ntheir roles and provide opportunities for employees to\ndevelop their skills and competencies without receiving\nhelp from AI-based agents. Competence-promoting mea-\nsures can be introduced to satisfy employees’ need for\ncompetence. Regular positive feedback, competence-en-\nhancing workshops, and trainings in the area of work in\nwhich AI-based agents are introduced can help to increase\nemployees’ satisfaction of their psychological need for\ncompetence (Deci and Ryan 2012). Furthermore, while\nreceiving proactive rather than reactive help from AI-based\nagents in human –AI collaboration can negatively impact\nemployees’ system satisfaction, a deep sense of collabo-\nration could help to mitigate this effect by satisfying their\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\npsychological need for relatedness (Ryan and Deci 2000;\nDeci et al. 2008). Managers should therefore emphasize the\ncollaborative characteristics of proactive AI-based agents\nwhen introducing them. Finally, designers should imple-\nment the option to (de-)activate the proactivity of AI-based\nagents, thus allowing users to have the option to decide by\nthemselves if they want to receive proactive help.\nIn addition, our results suggest that employees’ AI\nknowledge further ampliﬁes the negative effect on\nemployees’ system satisfaction when receiving proactive\nrather than reactive help from AI-based agents. When\nemployees have a predominantly low level of AI knowl-\nedge, managers can expect a lower loss in system satis-\nfaction from their employees when introducing proactive\nrather than reactive AI-based agents. In contrast, when\nemployees show predominantly high levels of AI knowl-\nedge, managers must expect a higher loss in system satis-\nfaction. Therefore, employees’ AI knowledge should be\nconsidered as an important basis for deciding about\nimplementing proactive AI-based agents or introducing\nmeasures to counteract the adverse effects of proactive AI-\nbased agents on employees’ system satisfaction. In indus-\ntries or activities where employees are likely to have higher\nAI knowledge, such as in software development, managers\nshould rely less on proactive AI-based agents or at least\nprovide a high level of measures, such as competence-\nenhancing workshops, to maintain employee system satis-\nfaction when implementing proactive rather than reactive\nAI-based agents. Finally, training and educating employees\nabout AI to help them understand its capabilities and lim-\nitations can be beneﬁcial and is recommended in organi-\nzational contexts (Pinski and Benlian 2024). However, we\nsuggest that this training and education should also include\ninformation about human capabilities and advantages\ncompared to AI to enhance employees’ competence-based\nself-esteem, especially when the implementation of\nproactive AI-based agents is conceivable in the future.\n6.3 Limitations and Directions for Future Research\nDespite our theoretical and practical contributions, our\nstudy has several limitations that may offer interesting\ndirections for future research. First, our experiment is\nbased on a speciﬁc form of assistance from and area of\napplication of AI-based agents—taking over a task at work\nin the area of software development. In addition to that\nspeciﬁc context, AI-based agents are also used in many\nother contexts, such as medical healthcare and ﬁnancial\nservices (e.g., Jussupow et al. 2021; Strich et al. 2021). We\nbelieve that the context we have selected is relevant and\ntimely, so that our results provide important insights.\nHowever, future research could extend our research and\nstrengthen the robustness of our ﬁndings by investigating\nhow proactive (vs. reactive) help from AI-based agents can\naffect users’ system satisfaction in other application areas\nof AI-based agents in human–AI collaboration at work.\nIn addition, we drew on self-determination theory, par-\nticularly on the psychological need for competence, to\nexplain how proactive (vs. reactive) help from AI-based\nagents affects users’ system satisfaction. However, com-\npetence is only one of three psychological needs that may\naffect users when interacting with AI-based agents (De\nVreede et al. 2021; Nguyen et al. 2022). In particular, the\npsychological need for autonomy, along with the psycho-\nlogical need for competence, is particularly important\nwhen receiving help from AI-based agents (Calvo et al.\n2020; Nguyen et al. 2022). To extend our study and further\ninvestigate the reasons for receiving proactive rather than\nreactive help from AI-based agents, we recommend that\nfuture research should extend our study by considering\nusers’ perceptions of autonomy and relatedness as an\ninﬂuential variable when receiving proactive (vs. reactive)\nhelp from AI-based agents.\nNext, our study took place in an experimental setting by\nconducting a vignette-based online experiment that was\nlimited to a one-time interaction. The methodology of a\nvignette-based online experiment has high internal validity\nand is already an established method in the ﬁeld of IS\nresearch for capturing user perceptions when interacting\nwith agents (Benlian et al. 2020). We believe this\nmethodology is an appropriate way to begin exploring the\nnuanced effects of receiving proactive rather than reactive\nhelp from AI-based agents. However, a key limitation of\nthe vignette methodology is the lack of real-world inter-\naction, which may limit external validity. Therefore, our\nstudy may not fully capture the complexity of human–AI\ncollaboration in a real-world work environment, where\nusers could behave differently when receiving help from\nAI-based agents, which would affect the generalizability of\nour ﬁndings. Additional studies could address this limita-\ntion and verify the results of our study by building on a\nmethod with higher external validity that is grounded on a\nreal work environment instead of a simulation-based\nvignette study. Furthermore, it is conceivable that as users\ncollaborate with AI-based agents over time, they become\naccustomed to receiving help from AI-based agents,\nthereby changing the effects of receiving help from AI-\nbased agents on users’ system satisfaction. Therefore, and\nto deepen our understanding of how users’ system satis-\nfaction is affected when receiving help from AI-based\nagents in human–AI collaboration, we recommend that\nfuture research should test our hypothesis in an actual work\nenvironment over a more extended period with multiple\ninteractions. In doing so, future research could also account\nfor additional potential moderators, such as the impact of\nthe actual quality of the provided help, users’ self-esteem,\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nand users’ competence in solving the related task without\nreceiving help from AI-based agents.\nFinally, for our online experiment, we recruited partic-\nipants with programming experience located in the United\nStates on the crowdsourcing platform Proliﬁc.co. While\nProliﬁc.co is a well-established platform for recruiting\nparticipants in the ﬁeld of IS research, it is possible that the\nparticipants recruited through crowdsourcing platforms,\nsuch as Proliﬁc.co, may not fully represent the general\npopulation of users who regularly interact with AI-based\nagents in work settings. Furthermore, users with pro-\ngramming experience and regular interactions with tech-\nnology may behave differently when receiving help from\nAI-based agents than other users. Further research could\nenhance the generalizability and robustness of our ﬁndings\nby replicating our study with diverse samples and cultural\ncontexts beyond the United States and the context of\nsoftware development. In particular, a cross-cultural study\ncould offer valuable insights into how cultural differences\nmight inﬂuence users’ system satisfaction when receiving\nproactive rather than reactive help from AI-based agents.\n7 Conclusion\nIn conclusion, our study explores the evolving landscape of\nAI-based agents and their impact on users’ system satis-\nfaction in human–AI collaboration. Enabled by techno-\nlogical advances in AI, AI-based agents can increasingly\nact autonomously, are increasingly able to handle entire\nwork processes, and already outperform humans in speciﬁc\ntasks. This development has led to the emergence of\nproactive AI-based agents, which are reshaping the way in\nwhich humans interact with AI-based agents. Our ﬁndings\nprovide essential insights into how help from AI-based\nagents, offered proactively rather than reactively, affects\nusers’ system satisfaction and how users’ AI knowledge\nmoderates this effect. However, despite their increasing\nrelevance in practice and research, we believe there is more\nto be understood about proactive AI-based agents. We have\njust begun to uncover the vast array of chances and risks\nthat arise when humans collaborate with proactive AI-\nbased agents. More research is necessary to fully compre-\nhend and harness their potential in human–AI collabora-\ntion. To this end, our paper provides essential insights that\ncan be built upon in practice and in future research.\nSupplementary Information The online version contains\nsupplementary material available at https://doi.org/10.1007/s12599-\n024-00918-y.\nAcknowledgements The authors gratefully acknowledge the funding\nsupport by the German Research Foundation (DFG) (project\nnumber 522190413).\nFunding Open Access funding enabled and organized by Projekt\nDEAL.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\nAdam M, Roethke K, Benlian A (2023b) Human versus automated\nsales agents: how and why customer responses shift across sales\nstages. Inf Syst Res 34(3):1148–1168. https://doi.org/10.1287/\nisre.2022.1171\nAdam M, Diebel C, Goutier M, Benlian A (2024b) Navigating\nautonomy and control in human-AI delegation: user responses to\ntechnology- versus user-invoked task allocation. Decis Support\nSyst. https://doi.org/10.1016/j.dss.2024.114193\nAdam M, Diebel C, Goutier M (2023a) The threatening effect of\ninvoked help from highly competent intelligent agents. In: 44th\nInternational Conference on Information Systems, Hyderabad.\nhttps://aisel.aisnet.org/icis2023/hti/hti/9\nAdam M, Bauer K, Jussupow E, Benlian A, Stein M-K (2024)\nGenerating tomorrow’s me: How collaborating with generative\nAI changes humans. https://bise-journal.com/?p=2174. Accessed\n25 Oct 2024\nAgarwal R, Prasad J (1998) A conceptual and operational deﬁnition\nof personal innovativeness in the domain of information\ntechnology. Inf Syst Res 9(2):204–215. https://doi.org/10.1287/\nisre.9.2.204\nAguinis H, Bradley KJ (2014) Best practice recommendations for\ndesigning and implementing experimental vignette methodology\nstudies. Organ Res Meth 17(4):351–371. https://doi.org/10.1177/\n1094428114547952\nAlicke MD, LoSchiavo FM, Zerbst J, Zhang S (1997) The person who\noutperforms me is a genius: maintaining perceived competence\nin upward social comparison. J Person Soc Psychol\n73(4):781–789\nAnthony C, Bechky BA, Fayard A-L (2023) ‘ ‘Collaborating’’ with AI:\ntaking a system view to explore the future of work. Organ Sci\n34(5):1672–1694\nAriani G (2024) Embracing the ubiquity of machines. Nat Hum\nBehav 8(10):1823–1824. https://doi.org/10.1038/s41562-024-\n02049-6\nAshfaq M, Yun J, Yu S, Loureiro SMC (2020) I, chatbot: modeling\nthe determinants of users’ satisfaction and continuance intention\nof AI-powered service agents. Telematics Inform 54:101473.\nhttps://doi.org/10.1016/j.tele.2020.101473\nAtzmu¨ ller C, Steiner PM (2010) Experimental vignette studies in\nsurvey research. Methodol 6(3):128–138. https://doi.org/10.\n1027/1614-2241/a000014\nAyyagari R, Grover V, Purvis R (2011) Technostress: technological\nantecedents and implications. MIS Q 35(4):831. https://doi.org/\n10.2307/41409963\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nBailey D, Faraj S, Hinds P, von Krogh G, Leonardi P (2019) Special\nissue of organization science: emerging technologies and\norganizing. Organ Sci 30(3):642–646\nBaird A, Maruping LM (2021) The next generation of research on IS\nuse: a theoretical framework of delegation to and from agentic IS\nartifacts. MISQ. 45(1):315–341\nBamberger P (2009) Employee help-seeking: Antecedents, conse-\nquences and new insights for future research. In: Martocchio J,\nLiao H (eds) Research in personnel and human resources\nmanagement. Emerald, pp 49–98.\nBar-Or S, Meyer J (2019) What is good help? Responses to solicited\nand unsolicited assistance. Int J Hum-Comput Interact\n35(2):131–139. https://doi.org/10.1080/10447318.2018.1437866\nBauer K, von Zahn M, Hinz O (2023) Expl(AI)ned: The impact of\nexplainable artiﬁcial intelligence on users’ information process-\ning. Inf Syst Res 34(4):1582–1602. https://doi.org/10.1287/isre.\n2023.1199\nBenlian A, Klumpe J, Hinz O (2020) Mitigating the intrusive effects\nof smart home assistants by using anthropomorphic design\nfeatures: a multimethod investigation. Inf Syst J\n30(6):1010–1042. https://doi.org/10.1111/isj.12243\nBerente N, Bin Gu, Recker J, Santhanam R (2021) Managing artiﬁcial\nintelligence. MISQ. 45(3):1433–1450\nBhattacherjee A (2001) Understanding information systems contin-\nuance: an expectation-conﬁrmation model. MIS Q 25(3):351.\nhttps://doi.org/10.2307/3250921\nBitkom (2023) ChatGPT & Co.: Jedes sechste Unternehmen plant KI-\nEinsatz zur Textgenerierung. https://www.bitkom.org/Presse/\nPresseinformation/ChatGPT-Jedes-sechste-Unternehmen-plant-\nKI-Einsatz-Textgenerierung. Accessed 26 Apr 2023\nBoissicat N, Pansu P, Bouffard T, Cottin F (2012) Relation between\nperceived scholastic competence and social comparison mech-\nanisms among elementary school children. Soc Psychol Educ\n15(4):603–614. https://doi.org/10.1007/s11218-012-9189-z\nBoyacı T, Canyakmaz C, de Ve ´ricourt F (2023) Human and machine:\nthe impact of machine input on decision making under cognitive\nlimitations. Manag Sci 70(2):1258–1275. https://doi.org/10.\n1287/mnsc.2023.4744\nBriggs R, Reinig B, Vreede G-J (2008) The yield shift theory of\nsatisfaction and its application to the IS/IT domain. J Assoc Inf\nSyst 9(5):267–293\nBrown SA, Venkatesh V, Kuruzovich J, Massey AP (2008) Expec-\ntation conﬁrmation: an examination of three competing models.\nOrgan Behav Hum Decis Proc 105(1):52–66. https://doi.org/10.\n1016/j.obhdp.2006.09.008\nCalvo R, Peters D, Vold K, Ryan R (2020) Supporting human\nautonomy in AI Systems: A framework for ethical enquiry. In:\nBurr C, Floridi L (eds) Ethics of digital well-being. Springer,\nCham, pp 31–54\nChen Z, Chan J (2023) Large language model in creative work: the\nrole of collaboration modality and user expertise. SSRN Electron\nJ. https://doi.org/10.2139/ssrn.4575598\nChiu Y-T (2021) In the hearts and minds of employees: A model of\npre-adoptive appraisal toward artiﬁcial intelligence in organiza-\ntions. Int J Inf Manag. https://doi.org/10.1016/j.ijinfomgt.2021.\n102379\nContinental (2023) Umfrage: Deutsche fu ¨ rchten Verlust von Arbeit-\nspla¨ tzen durch Einsatz von Ku ¨ nstlicher Intelligenz. In: Cont.\nAG. https://www.continental.com/de/presse/ pressemitteilungen/\numfrage-kuenstliche-intelligenz/. Accessed 26 Apr 2023\nCraig K, Thatcher JB, Grover V (2019) The IT identity threat: a\nconceptual deﬁnition and operational measure. J Manag Inf Syst\n36(1):259–288. https://doi.org/10.1080/07421222.2018.1550561\nCroitor E, Werner D, Adam M, Benlian A (2022) Opposing effects of\ninput control and clan control for sellers on e-marketplace\nplatforms. Electron Mark 32(1):201–216. https://doi.org/10.\n1007/s12525-021-00465-4\nDeCharms R (1968) Personal causation. Academic Press, New York,\nThe internal affective determinants of behavior\nDeci EL, Ryan RM (1985) Intrinsic motivation and self-determination\nin human behavior. Springer, Boston\nDeci EL, Ryan RM (2000) The ‘‘what’’ and ‘‘why’’ of goal pursuits:\nhuman needs and the self-determination of behavior. Psychol Inq\n11(4):227–268. https://doi.org/10.1207/S15327965PLI1104_01\nDeci EL, Ryan RM (2008) Self-determination theory: a macrotheory\nof human motivation, development, and health. Can Psychol /\nPsychol Can 49(3):182–185. https://doi.org/10.1037/a0012801\nDeci EL, Ryan RM (2012) Self-determination theory. Handbook of\ntheories of social psychology, vol 1. Sage, Thousand Oaks,\npp 416–436\nDeci EL, Ryan RM, Vansteenkiste M (2008) Self-determination\ntheory and the explanatory role of psychological needs in human\nwell-being. In: Comim F (ed) Bruni L. Capabilities and\nhappiness, Oxford University Press, pp 187–223\nDeelstra JT, Peeters MCW, Schaufeli WB, Stroebe W, Zijlstra FRH,\nvan Doornen LP (2003) Receiving instrumental support at work:\nwhen help is not welcome. J Appl Psychol 88(2):324–331.\nhttps://doi.org/10.1037/0021-9010.88.2.324\nDennis AR, Lakhiwal A, Sachdeva A (2023) AI agents as team\nmembers: effects on satisfaction, conﬂict, trustworthiness, and\nwillingness to work with. J Manag Inf Syst 40(2):307–337.\nhttps://doi.org/10.1080/07421222.2023.2196773\nDe Vreede T, Raghavan M, De Vreede G-J (2021) Design founda-\ntions for AI assisted decision making: A self determination\ntheory approach. http://hdl.handle.net/10125/70630\nDwivedi YK et al (2023) ‘‘So what if ChatGPT wrote it?’’\nMultidisciplinary perspectives on opportunities, challenges and\nimplications of generative conversational AI for research,\npractice and policy. Int J Inf Manag 71:102642. https://doi.org/\n10.1016/j.ijinfomgt.2023.102642\nEiﬂer S, Petzold K (2019) Validity aspects of vignette experiments:\nExpected ‘‘what-if’’ differences between reports of behavioral\nintentions and actual behavior. In: Experimental methods in\nsurvey research. Wiley, pp 393–416\nEllis PD (2010) The essential guide to effect sizes: statistical power,\nmeta-analysis, and the interpretation of research results. Cam-\nbridge University Press, Cambridge, New York\nFaul F, Erdfelder E, Buchner A, Lang A-G (2009) Statistical power\nanalyses using G*Power 31: tests for correlation and regression\nanalyses. Behav Res Meth 41(4):1149–1160\nFornell C, Larcker DF (1981) Evaluating structural equation models\nwith unobservable variables and measurement error. J Mark Res\n18(1):39–50. https://doi.org/10.2307/3151312\nFuegener A, Grahl J, Gupta A, Ketter W (2022) Cognitive challenges\nin human–artiﬁcial intelligence collaboration: Investigating the\npath toward productive delegation. Inf Syst Res 33(2):678–696.\nhttps://doi.org/10.1287/isre.2021.1079\nFuegener A, Grahl J, Gupta A, Ketter W (2021) Will humans-in-the-\nloop become borgs? Merits and pitfalls of working with AI. MIS\nQ 45(3). https://ssrn.com/abstract=3879937\nGagne´ M, Parent-Rocheleau X, Bujold A, Gaudet M-C, Lirio P\n(2022) How algorithmic management inﬂuences worker moti-\nvation: a self-determination theory perspective. Can Psychol /\nPsychol Can 63(2):247–260. https://doi.org/10.1037/cap0000324\nGitHub (2023b) GitHub Copilot. In: Introd. GitHub Copilot X. https://\ngithub.com/features/preview/copilot-x. Accessed 25 Apr 2023\nGitHub (2023a) GitHub Copilot. In: Your AI pair program. https://\ngithub.com/features/copilot. Accessed 25 Apr 2023\nGnewuch U, Morana S, Adam MTP, Maedche A (2022) Opposing\neffects of response time in human–chatbot interaction: The\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nmoderating role of prior experience. Bus Inf Syst Eng\n64(6):773–791. https://doi.org/10.1007/s12599-022-00755-x\nHair JF, Hult GTM, Ringle CM, Sarstedt M, Danks NP, Ray S (2021)\nPartial least squares structural equation modeling (PLS-SEM)\nUsing R: A workbook. Springer\nHarari D, Parke MR, Marr JC (2022) When helping hurts helpers:\nanticipatory versus reactive helping, helper’s relative status, and\nrecipient self-threat. Acad Manag J 65(6):1954–1983. https://doi.\norg/10.5465/amj.2019.0049\nHayes AF (2022) Introduction to mediation, moderation, and\nconditional process analysis: A regression-based approach, 3rd\nedn. Guilford, New York\nHayes AF (2023) PROCESS macro for SPSS, SAS, and R. In:\nPROCESS Macro SPSS SAS R. http://processmacro.org/.\nAccessed 25 Oct 2024\nHsiao K-L, Chen C-C (2022) What drives continuance intention to\nuse a food-ordering chatbot? An examination of trust and\nsatisfaction. Library Hi Tech 40(4):929–946. https://doi.org/10.\n1108/LHT-08-2021-0274\nHukal P, Berente N, Germonprez M, Schecter A (2019) Bots\ncoordinating work in open source software projects. Comput\n52(9):52–60. https://doi.org/10.1109/MC.2018.2885970\nJussupow E, Spohrer K, Heinzl A, Gawlitza J (2021) Augmenting\nmedical diagnosis decisions? An investigation into physicians’\ndecision-making process with artiﬁcial intelligence. Inf Syst Res\n32(3):713–735. https://doi.org/10.1287/isre.2020.0980\nKalliamvakou E (2022) Research: quantifying GitHub Copilot’s\nimpact on developer productivity and happiness. In: GitHub\nBlog. https://github.blog/2022-09-07-research-quantifying-\ngithub-copilots-impact-on-developer-productivity-and-happi\nness/. Accessed 28 Apr 2023\nKarahanna E, Xu SX, Xu Y, Zhang N (2018) The needs–affordances–\nfeatures perspective for the use of social media. MISQ.\n42(3):737–756\nKomiak S, Benbasat I (2006) The effects of personalization and\nfamiliarity on trust and adoption of recommendation agents. MIS\nQ 30(4):941–960\nKraus M, Wagner N, Callejas Z, Minker W (2021) The role of trust in\nproactive conversational assistants. IEEE Access\n9:112821–112836. https://doi.org/10.1109/ACCESS.2021.\n3103893\nKu¨ hl N, Schemmer M, Goutier M, Satzger G (2022) Artiﬁcial\nintelligence and machine learning. Electron Mark\n32(4):2235–2244. https://doi.org/10.1007/s12525-022-00598-0\nLatikka R, Savela N, Koivula A, Oksanen A (2021) Attitudes toward\nrobots as equipment and coworkers and the impact of robot\nautonomy level. Int J Soc Robotics 13(7):1747–1759. https://doi.\norg/10.1007/s12369-020-00743-9\nLee HW, Bradburn J, Johnson RE, Lin S-H, Chang C-H (2019) The\nbeneﬁts of receiving gratitude for helpers: a daily investigation\nof proactive and reactive helping at work. J Appl Psychol\n104(2):197–213. https://doi.org/10.1037/apl0000346\nLeiner D (2024) Information about SoSci Survey. https://www.\nsoscisurvey.de/de/about. Accessed 25 Oct 2024\nLi S, Karahanna E (2015) Online recommendation systems in a B2C\ne-commerce context: a review and future directions. J Assoc Inf\nSyst 16(2):72–107\nLong D, Magerko B (2020) What is AI literacy? Competencies and\ndesign considerations. In: Proceedings of the 2020 CHI Confer-\nence on Human Factors in Computing Systems. ACM, Honolulu.\nhttps://doi.org/10.1145/3313831.3376727\nLu Y, Mao X, Zhou M, Zhang Y, Li Z, Wang T, Wang H (2021)\nMotivation under gamiﬁcation: an empirical study of developers’\nmotivations and contributions in stack overﬂow. IEEE Trans\nSoftw Eng 48(12):4947–4963\nMaedche A, Legner C, Benlian A, Berger B, Gimpel H, Hess T, Hinz\nO, Morana S, So ¨ llner M (2019) AI-based digital assistants:\nopportunities, threats, and research perspectives. Bus Inf Syst\nEng 61(4):535–544. https://doi.org/10.1007/s12599-019-00600-\n8\nManiktala M, Chi M, Barnes T (2023) Enhancing a student\nproductivity model for adaptive problem-solving assistance.\nUser Model User-Adapt Interact 33(1):159–188. https://doi.org/\n10.1007/s11257-022-09338-7\nMarks MA, Mathieu JE, Zaccaro SJ (2001) A temporally based\nframework and taxonomy of team processes. Acad Manag Rev\n26(3):356–376. https://doi.org/10.2307/259182\nMcCabe CJ, Kim DS, King KM (2018) Improving present practices in\nthe visual display of interactions. Adv Meth Pract Psychol Sci\n1(2):147–165. https://doi.org/10.1177/2515245917746792\nMemmert L, Tavanapour N (2023) Towards human-AI-collaboration\nin brainstorming: Empirical insights into the perception of\nworking with a generative AI. In: European Conference on\nInformation Systems, Kristiansand. https://aisel.aisnet.org/\necis2023_rp/429\nMeurisch C, Mihale-Wilson CA, Hawlitschek A, Giger F, Mu ¨ ller F,\nHinz O, Mu ¨ hlha\n¨ user M (2020) Exploring user expectations of\nproactive AI systems. Proc ACM Interact Mob Wearable\nUbiquitous Technol. 4(4):146:1-146:22\nMirbabaie M, Stieglitz S, Bru ¨ nker F, Hofeditz L, Ross B, Frick NRJ\n(2021) Understanding collaboration with virtual assistants – The\nrole of social identity and the extended self. Bus Inf Syst Eng\n63(1):21–37. https://doi.org/10.1007/s12599-020-00672-x\nMorana S, Schacht S, Scherp A, Maedche A (2017) A review of the\nnature and effects of guidance design features. Decis Support\nSyst 97:31–42. https://doi.org/10.1016/j.dss.2017.03.003\nMyers DG, Diener E (1995) Who is happy? Psychol Sci 6:10–19.\nhttps://doi.org/10.1111/j.1467-9280.1995.tb00298.x\nNguyen QN, Sidorova A, Torres R (2022) User interactions with\nchatbot interfaces versus menu based interfaces an empirical\nstudy. Comput Hum Behav 128:107093\nNunnally JC (1978) An overview of psychological measurement. In:\nWolman BB (ed) Clinical diagnosis of mental disorders.\nSpringer, Boston, pp 97–146\nOpenAI (2022) Introducing ChatGPT. https://openai.com/blog/\nchatgpt. Accessed 25 Apr 2023\nStack Overﬂow (2023) Stack overﬂow developer survey 2022. In:\nStack Overﬂow. https://survey.stackoverﬂow.co/2022/?utm_\nsource=social-share&utm_medium=social&utm_campaign=dev-\nsurvey-2022. Accessed 27 Apr 2023\nPalan S, Schitter C (2018) Proliﬁc.ac – A subject pool for online\nexperiments. J Behav Exp Fin 17:22–27. https://doi.org/10.1016/\nj.jbef.2017.12.004\nParker SK, Bindl UK, Strauss K (2010) Making things happen: a\nmodel of proactive motivation. J Manag 36(4):827–856. https://\ndoi.org/10.1177/0149206310363732\nParker SK, Wang Y, Liao J (2019) When is proactivity wise? a review\nof factors that inﬂuence the individual outcomes of proactive\nbehavior. Ann Rev Organ Psychol Organ Behav 6(1):221–248.\nhttps://doi.org/10.1146/annurev-orgpsych-012218-015302\nPeer E, Brandimarte L, Samat S, Acquisti A (2017) Beyond the Turk:\nalternative platforms for crowdsourcing behavioral research.\nJ Exp Soc Psychol 70:153–163. https://doi.org/10.1016/j.jesp.\n2017.01.006\nPeng S, Kalliamvakou E, Cihon P, Demirer M (2023) The impact of\nAI on developer productivity: Evidence from GitHub Copilot.\narXiv:2302.06590\nPinski M, Benlian A (2024) AI literacy for users – a comprehensive\nreview and future research directions of learning methods,\ncomponents, and effects. Comput Hum Behav Artif Hum\n2(1):100062. https://doi.org/10.1016/j.chbah.2024.100062\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng\nPinski M, Adam M, Benlian A (2023a) AI knowledge: Improving AI\ndelegation through human enablement. In: Proceedings of the\n2023 CHI Conference on Human Factors in Computing Systems.\nACM, Hamburg. https://doi.org/10.1145/3544548.3580794\nPinski M, Haas M, Franz A (2023b) AiLingo – A design science\napproach to advancing non-expert adults’ AI literacy. 44th\nInternational Conference on Information Systems, Hyderabad.\nhttps://aisel.aisnet.org/icis2023/learnandiscurricula/learnandis\ncurricula/10\nProliﬁc (2023) What is Proliﬁc? In: Proliﬁc. https://researcher-help.\nproliﬁc.co/hc/en-gb/articles/360009092254-What-is-Proliﬁc -.\nAccessed 29 Apr 2023\nQiu L, Benbasat I (2009) Evaluating anthropomorphic product\nrecommendation agents: a social relationship perspective to\ndesigning information systems. J Manag Inf Syst 25(4):145–182.\nhttps://doi.org/10.2753/MIS0742-1222250405\nQiu L, Benbasat I (2010) A study of demographic embodiments of\nproduct recommendation agents in electronic commerce. Int J\nHum-Comput Stud 68(10):669–688. https://doi.org/10.1016/j.\nijhcs.2010.05.005\nReis HT, Sheldon KM, Gable SL, Roscoe J, Ryan RM (2000) Daily\nwell-being: the role of autonomy, competence, and relatedness.\nPerson Soc Psychol Bull 26(4):419–435. https://doi.org/10.1177/\n0146167200266002\nRussell SJ, Norvig P (2016) Artiﬁcial intelligence: A modern\napproach. Pearson\nRyan RM, Deci EL (2000) Self-determination theory and the\nfacilitation of intrinsic motivation, social development, and\nwell-being. Am Psychol 55(1):68\nSchemmer M, Ku ¨ hl N, Benz C, Bartos A, Satzger G (2023)\nAppropriate reliance on AI advice: Conceptualization and the\neffect of explanations. In: Proceedings of the 28th International\nConference on Intelligent User Interfaces, pp 410–422. https://\ndoi.org/10.1145/3581641.3584066\nSchleiffer R (2005) An intelligent agent model. Eur J Oper Res\n166(3):666–693. https://doi.org/10.1016/j.ejor.2004.03.039\nSchoeffer J, Kuehl N, Machowski Y (2022) ‘‘There is not enough\ninformation’’: On the effects of explanations on perceptions of\ninformational fairness and trustworthiness in automated deci-\nsion-making. In: 2022 ACM Conference on Fairness, Account-\nability, and Transparency. https://doi.org/10.1145/3531146.\n3533218\nSchuetz S, Venkatesh V (2020) The rise of human machines: How\ncognitive computing systems challenge assumptions of user-\nsystem interaction. J Assoc Inf Syst 21(2):460–482\nShrout P, Bolger N (2002) Mediation in experimental and nonexper-\nimental studies: new procedures and recommendations. Psychol\nMeth 7:422–445. https://doi.org/10.1037/1082-989X.7.4.422\nSimilarweb (2023) chat.openai.com market share, revenue and trafﬁc\nanalytics. In: Similarweb. https://www.similarweb.com/website/\nchat.openai.com/. Accessed 28 Apr 2023\nSmith SM, Roster CA, Golden LL, Albaum GS (2016) A multi-group\nanalysis of online survey respondent data quality: comparing a\nregular USA consumer panel to MTurk samples. J Bus Res\n69(8):3139–3148. https://doi.org/10.1016/j.jbusres.2015.12.002\nSpitzmuller M, Van Dyne L (2013) Proactive and reactive helping:\ncontrasting the positive consequences of different forms of\nhelping: proactive and reactive helping. J Organ Behav\n34(4):560–580. https://doi.org/10.1002/job.1848\nStelmaszak M, Mo ¨ hlmann M, Sørensen C (2024) When algorithms\ndelegate to humans: Exploring human-algorithm interaction at\nUber. MIS Q (forthcoming)\nStrich F, Mayer A-S, Fiedler M (2021) What do I do in a world of\nartiﬁcial intelligence? Investigating the impact of substitutive\ndecision-making AI systems on employees’ professional role\nidentity. J Assoc Inf Syst 22(2):304–324\nSun Y, Liu L, Peng X, Dong Y, Barnes SJ (2014) Understanding\nChinese users’ continuance intention toward online social\nnetworks: an integrative theoretical model. Electron Mark\n24(1):57–66. https://doi.org/10.1007/s12525-013-0131-9\nTabnine (2023) Tabnine. In: Tabnine is an AI assistant that speeds up\ndelivery and keeps your code safe. https://www.tabnine.com/.\nAccessed 29 Oct 2023\nTully S, Longoni C, Appel G. 2023. Knowledge of artiﬁcial\nintelligence predicts lower AI receptivity. PsyArXiv. https://\ndoi.org/10.31234/osf.io/t9u8g\nWenninger A, Rau D, Ro ¨ glinger M (2022) Improving customer\nsatisfaction in proactive service design: a Kano model approach.\nElectron Mark 32(3):1399–1418. https://doi.org/10.1007/\ns12525-022-00565-9\nWhite RW (1959) Motivation reconsidered: the concept of compe-\ntence. Psychol Rev 66:297–333. https://doi.org/10.1037/\nh0040934\nYang X, Aurisicchio M (2021) Designing conversational agents: A\nself-determination theory approach. In: Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems.\nACM, Yokohama. https://doi.org/10.1145/3411764.3445445\nYzerbyt V, Lories G, Dardenne B (eds) (1998) Metacognition:\ncognitive and social dimensions. Sage, London\nZhang D, Sanyal P, Nah F, Mukkamala R (2024) Generative AI:\ntransforming human, business, and organizational decision\nmaking. https://www.sciencedirect.com/journal/decision-sup\nport-systems/about/call-for-papers. Accessed 14 Jul 2024\n123\nC. Diebel et al.: When AI-Based Agents Are Proactive..., Bus Inf Syst Eng",
  "topic": "Competence (human resources)",
  "concepts": [
    {
      "name": "Competence (human resources)",
      "score": 0.6219190359115601
    },
    {
      "name": "Knowledge management",
      "score": 0.5468797087669373
    },
    {
      "name": "User satisfaction",
      "score": 0.44856828451156616
    },
    {
      "name": "Computer science",
      "score": 0.41937020421028137
    },
    {
      "name": "Psychology",
      "score": 0.4055432379245758
    },
    {
      "name": "Engineering",
      "score": 0.36589646339416504
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3596087694168091
    },
    {
      "name": "Social psychology",
      "score": 0.18115735054016113
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I74656192",
      "name": "University of Göttingen",
      "country": "DE"
    }
  ]
}