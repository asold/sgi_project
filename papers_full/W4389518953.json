{
  "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
  "url": "https://openalex.org/W4389518953",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3031731520",
      "name": "Óscar Sainz",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A3197221408",
      "name": "Jon Campos",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A2936307295",
      "name": "Iker García Ferrero",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5016315389",
      "name": "Julen Etxaniz",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A2469732033",
      "name": "Oier Lopez de Lacalle",
      "affiliations": [
        "University of the Basque Country"
      ]
    },
    {
      "id": "https://openalex.org/A1956093140",
      "name": "Eneko Agirre",
      "affiliations": [
        "University of the Basque Country"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4230872509",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W4323651144",
    "https://openalex.org/W4380993498",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3154151289",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4385572425",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4376309460",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3099695344",
    "https://openalex.org/W4321524373",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4385570455",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W3105042180",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4366999773",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4389518805",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W4378469337"
  ],
  "abstract": "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10776–10787\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNLP Evaluation in trouble:\nOn the Need to Measure LLM Data Contamination for each Benchmark\nOscar Sainz1 Jon Ander Campos2 Iker García-Ferrero1 Julen Etxaniz1\nOier Lopez de Lacalle1 Eneko Agirre1\n1 HiTZ Center - Ixa, University of the Basque Country UPV/EHU\n{oscar.sainz,iker.graciaf,julen.etxaniz}@ehu.eus\n{oier.lopezdelacalle,e.agirre}@ehu.eus\n2 Cohere\njonander@cohere.com\nAbstract\nIn this position paper, we argue that the classi-\ncal evaluation on Natural Language Processing\n(NLP) tasks using annotated benchmarks is in\ntrouble. The worst kind of data contamination\nhappens when a Large Language Model (LLM)\nis trained on the test split of a benchmark, and\nthen evaluated in the same benchmark. The ex-\ntent of the problem is unknown, as it is not\nstraightforward to measure. Contamination\ncauses an overestimation of the performance\nof a contaminated model in a target benchmark\nand associated task with respect to their non-\ncontaminated counterparts. The consequences\ncan be very harmful, with wrong scientific con-\nclusions being published while other correct\nones are discarded. This position paper de-\nfines different levels of data contamination and\nargues for a community effort, including the\ndevelopment of automatic and semi-automatic\nmeasures to detect when data from a bench-\nmark was exposed to a model, and suggestions\nfor flagging papers with conclusions that are\ncompromised by data contamination.\n1 Introduction\nAt the core of NLP as a discipline, there is rigor-\nous evaluation on different tasks. The experimental\nprotocols involve strict control over the data, espe-\ncially test data, which needs to be totally unseen\nduring development, but also over training and de-\nvelopment data. This is essential to assess the per-\nformance of a model in zero-shot, few-shot, or fully\nsupervised settings. Since fine-tuning and prompt-\ning of Large Language Models (LLMs) became\ncommonplace (Min et al., 2021) it has been increas-\ningly difficult to enforce those strict protocols. Pre-\ntraining LLMs is expensive, and therefore, most of\nthe time, researchers use LLMs trained by third-\nparty entities (Raffel et al., 2020; Touvron et al.,\n2023a), which are agnostic to the target tasks where\nthose LLMs are going to be used. With the grow-\ning scale of LLMs (Kaplan et al., 2020; Henighan\net al., 2020) the need for data has been solved by\ncrawling the internet, reaching trillions of tokens\n(Touvron et al., 2023a), and making it very hard\nto know whether a specific benchmark was used\nto train the LLM. This is applicable to all models,\neven if they document the source of the data at a\nhigh level, but especially for closed models with\nno or insufficient documentation.\nData contamination has two consequences. The\nfirst one is that the performance of an LLM when\nevaluated on a benchmark it already processed dur-\ning pre-training will be overestimated, causing it\nto be preferred with respect to other LLMs. This\naffects the comparative assessment of the quality\nof LLMs. The second is that papers proposing sci-\nentific hypotheses on certain NLP tasks could be\nusing contaminated LLMs, and thus make wrong\nclaims about their hypotheses, and invalidate alter-\nnative hypotheses that could be true. This second\nconsequence has an enormous negative impact on\nour field and is our main focus.\nThere are several measures that the community\ncould take. A possible solution would be to avoid\nall research involving datasets which include pub-\nlished test data, and focus on datasets where the\ntest data labels are not public. This solution will\nseverely affect the number of NLP tasks for which\nbenchmarks exist, at least until new benchmarks\nthat avoid data leakage are produced. Jacovi et al.\n(2023) presents preventative strategies to avoid con-\ntamination in the future.\nIn this position paper, we propose a complemen-\ntary line of action which seeks to measure and doc-\nument data contamination cases, specifying LLM,\nbenchmark and evidence supporting contamination.\nThis solution involves a registry of contamination\ncases1, collaborative manual work and research on\nautomatic approaches. In addition, conferences\nshould devise mechanisms to ensure that papers\n1Such as the LM Contamination Index https://\nhitz-zentroa.github.io/lm-contamination/\n10776\ndon’t include conclusions involving contamination,\nand to flag past work where contamination has been\ndiscovered after publication.\nThe paper starts by introducing background, fol-\nlowed by a definition of data contamination, con-\ntamination at different steps, methods to measure\ndata contamination and a call for action.\n2 Background\nDetection of contamination cases has been tradi-\ntionally done by directly analyzing the training data\n(Dodge et al., 2021), but the current scale of the\npre-training data makes it difficult (Kreutzer et al.,\n2022; Birhane et al., 2021). Without proper doc-\numentation and search tools like ROOTS (Piktus\net al., 2023) it is very difficult for any researcher to\nactually know whether their datasets are compro-\nmised on a given model. More recently, this task\nbecame even harder, as the best-performing LLMs\nare deployed as products, and therefore, their train-\ning corpora are kept secret. In this case, it has\nbeen shown that the high memorization abilities of\nLLMs can be used to generate portions of the train-\ning texts (Carlini et al., 2021; Magar and Schwartz,\n2022). Using this memorization property, Sainz\net al. (2023) show that ChatGPT generates portions\nof popular NLP benchmarks. Furthermore, LLMs\nmemorization has been studied on data-leakage\nscenarios (Elangovan et al., 2021).\nRegarding data contamination cases, Dodge\net al. (2021) exposed that the C4 corpus (Raf-\nfel et al., 2020), a corpus used to pre-train sev-\neral LLMs such as T5 (Raffel et al., 2020), con-\ntained the test splits of several benchmarks that\nwere crawled from GitHub. Moreover, Brown\net al. (2020) acknowledged a bug in their filter-\ning script that caused the contamination of several\nbenchmarks during the GPT-3 training. Further-\nmore, OpenAI (2023) stated that parts of the BIG-\nbench (Srivastava et al., 2023) benchmark were\ninadvertently mixed into the training set, enough\nto stop them from evaluating the model on it. They\nalso mention that they included parts of the training\nsets of MATH (Hendrycks et al., 2021) and GSM-\n8K (Cobbe et al., 2021) as training data to improve\nmathematical reasoning (OpenAI, 2023). There-\nfore, the performance results reported for GSM-8K\ncannot be taken as zero-shot results when compared\nto other models.\nRecently, Sainz et al. (2023) reported that\nseveral benchmarks have already been com-\npromised in ChatGPT, including the popular\nCoNLL2003 (Tjong Kim Sang and De Meulder,\n2003). There are several preprints that evaluate\nChatGPT on CoNLL03 (Wei et al., 2023; Li et al.,\n2023a; Han et al., 2023) and at least one confer-\nence paper published on ACL 2023 that evaluates\nGPT-3 (Brown et al., 2020) and Codex (Chen et al.,\n2021) on the same benchmark (Li et al., 2023b).\nAppendix A shows evidence for data contamination\nfor those LLMs, and casts doubts on the conclu-\nsions of those papers.\n3 Defining data contamination\nIn general, data contamination refers to any breach\nin the strict control of datasets required by the ex-\nperimental protocol. In this paper, we focus on the\nspecific case where a LLM has processed the eval-\nuation benchmark during its pre-training. However,\ndifferent types of contamination exist and each of\nthem has different implications. In this section, we\npresent three types of contamination: guideline,\ntext and annotation.\nGuideline contamination happens when the an-\nnotation guidelines for a specific dataset are seen\nby the model. Usually, for specialized annotations,\nhighly detailed guidelines are required. The guide-\nlines can usually be publicly found on the internet,\neven for datasets that are not public or require buy-\ning a license for their use, ACE05 (Walker et al.,\n2006) for example. The more details the guide-\nlines have the more information and examples they\nprovide. A model aware of the guidelines for a spe-\ncific task or dataset has advantages over a model\nwithout such information. We should consider the\nguideline contamination, especially on zero and\nfew-shot evaluations.\nRaw text contamination happens when the orig-\ninal text (previous to annotation) is seen by the\nmodel. Some examples of this type of contami-\nnation are the datasets based on Wikipedia texts.\nWikipedia is commonly used as a source of pre-\ntraining data, but, it is also a frequent source of\ntext to create new datasets. MultiCoNER 2 (Fetahu\net al., 2023), a Named Entity Recognition dataset\nbased on Wikipedia links and Wikidata informa-\ntion, is an example of this phenomenon. Models\nthat have already seen Wikipedia in its original\nform (including the markup annotations) have more\ninformation to better identify a part of the annota-\ntions (the entity boundaries) of the dataset. As\n10777\npointed out by Dodge et al. (2021), other datasets\nbuilt from the web such as IMDB (Maas et al.,\n2011) and CNN/DailyMail (Hermann et al., 2015)\ncan be also compromised. This kind of contamina-\ntion should be taken into account when developing\nautomatically annotated datasets.\nAnnotation contamination happens when the\nannotations (labels) of the target benchmark are\nexposed to the model during training. Depending\non the splits of the benchmark that have been ex-\nposed, we can have the following cases: (1) When\nthe evaluation split is involved, the experiment is\ncompletely invalidated. This is the most harmful\nlevel of contamination. (2) When the train or de-\nvelopment splits are involved, this would not affect\ncomparisons with other models that have been de-\nveloped using those same splits, but it does inval-\nidate conclusions claiming zero-shot or few-shot\nperformance.\n4 Contamination on different steps\nCurrently, the standard procedure to train and de-\nploy language models has three main steps: pre-\ntraining a language model, fine-tuning the model to\nfollow instructions and/or align with human feed-\nback; and an iterative improvement step after de-\nployment. Data contamination does not only occur\nin the pre-training step of LLMs, but can occur\nlater in the training pipeline.\n4.1 Contamination during pre-training\nDuring the pre-training, there is a high chance that\nundesired data is fed to the model. Gathering huge\namounts of text from the internet also has its coun-\nterpart: it becomes very hard to filter undesired\ndata completely, and even deduplication is chal-\nlenging (Lee et al., 2022). Avoiding data contam-\nination completely is not realistic, as it is impos-\nsible to know every dataset that the research com-\nmunity can test an LLM on. However, allowing\nthe researchers to access and perform queries on\nthe pre-training data may ensure that no corrupted\nevaluations are performed. In fact, keeping the\npre-training data not available for LLM consumers\nmay derive undesired influences on downstream\ntasks (Li et al., 2020; Gehman et al., 2020; Groen-\nwold et al., 2020).\nIn addition, researchers building LLMs should\navoid, at least, contamination from well-known\nstandard benchmarks such as GLUE (Wang et al.,\n2018) or SuperGLUE (Wang et al., 2020). As\nDodge et al. (2021) showed, see their Table 2,\nvarious standard benchmarks were found in the\nC4 (Raffel et al., 2020) corpus.\n4.2 Contamination on supervised fine-tuning\nThe supervised fine-tuning or instruction-tuning\nstep is another step where contamination can oc-\ncur. Nevertheless, it is much less frequent as it\nis a required practice in the research community\nto document the training data in order to publish\nyour findings. As an example of those, we can\nfind the FLAN dataset collection (Longpre et al.,\n2023), OPT-IML Bench (Iyer et al., 2023), Super-\nNatural Instructions (Wang et al., 2022b), the P3\ncollection (Bach et al., 2022) and so on.\nRecently, more and more machine-generated\ntext is being used to fine-tune language models.\nSome examples of these are Self-Instruct (Wang\net al., 2022a), Unnatural Instructions (Honovich\net al., 2022), Alpaca Data (Taori et al., 2023)\nand ShareGPT (Chiang et al., 2023). The aim\nof those datasets is usually to make public and\nsmaller white-box models imitate black-box mod-\nels such as ChatGPT (Gu et al., 2023). However,\nthe distillation of a closed teacher model with clear\nsigns of contamination is an issue. More alarm-\ning, is the case that popular crowd-sourcing meth-\nods like MTurk have started using LLMs to gener-\nate data that was supposed to be manually gener-\nated (Veselovsky et al., 2023).\n4.3 Contamination after deployment\nThe last step where the models can be exposed to\ncontamination is applied mostly on LLMs as ser-\nvice products. With the recent improvements in the\nquality of LLMs, the models that were supposed\nto be part of bigger products become products by\nthemselves (ChatGPT or Bard for example). It is\nworth noting that, although they are closed models,\ni.e. no information is known about the architec-\nture or training details, the research community has\nevaluated them on standard benchmarks (Jiao et al.\n(2023); among others). The monetary success of\nclosed systems is closely tied to the performance\nof the model. Therefore, companies have a strong\nincentive to audit user inputs and retrain their sys-\ntem when the performance in a task is determined\nto be poor. Those models that are actually being ac-\ncessed via API calls have been iteratively improved\nwith user input, leading to evaluation data exposure.\nAs a result, the models became aware of the testing\ndata, at the point that you can easily recreate the\n10778\ndataset as we discuss in Section 5.2 (see examples\nin Appendix A).\n5 Measuring data contamination\nFor the reasons we already mentioned, it is nec-\nessary to measure the existent data contamination\ncases and to document relevant contamination ev-\nidence. In order to achieve this goal, we differen-\ntiate two cases. In the first case, we would have\nopen models where there is public access to all the\ntraining data, including text used in pre-training,\nbut also, if the LLM was trained on them, instruc-\ntion tuning datasets and deployment datasets. In\nthe second case, we would have closed models for\nwhich there is no access to training data.\n5.1 Open LLMs\nMost of the research on data contamination has\nbeen focused on analyzing pre-training data with\nstring-matching operations (Dodge et al., 2021),\nas this provides direct evidence that the LLM was\ncontaminated. Pre-training datasets are unwieldy\nlarge, and string-matching operations can be very\nslow at this scale. Therefore, several tools for data\nauditing have been released recently: The ROOTS\nSearch Tool (Piktus et al., 2023) and Data Por-\ntraits (Marone and Durme, 2023) among others.\nAs an example of their usefulness, Piktus et al.\n(2023) found that BLOOM (Workshop et al., 2023)\nshould not be evaluated on XNLI (Conneau et al.,\n2018) due to contamination. These tools should\nbe made available for all open LLMs, in order to\nallow for contamination case discovery.\nIn addition, there is no currently agreed-upon\nmethodology to measure the level of contamina-\ntion. For cases where the full benchmark is not\nfound, we propose to measure the level of data con-\ntamination using benchmark data overlap, that is,\nthe percentage of the benchmark that can be found\nin the pre-training dataset (Dodge et al., 2021; Pik-\ntus et al., 2023).\n5.2 Closed LLMs\nDespite most of the recent popular models like\nLLaMA (Touvron et al., 2023a), GPT-4 (Ope-\nnAI, 2023) or Bard have not publicly released\ntheir pre-training data, very few works have actu-\nally worked on detecting data-contamination when\nthe pre-training data is not available (Magar and\nSchwartz, 2022). Although this scenario is much\nmore challenging than the former, we foresee that\nit will become the most prevalent. Developing\nmethods to measure the data contamination in this\nscenario must be crucial for future evaluations. To\ntackle this problem, we propose to take advantage\nof LLM’s memorization capabilities. Appendix A\nshows some examples of using memorization to\nuncover data contamination for the CONLL2003\nbenchmark on three LLMs. In cases where the\nLLM does not produce the benchmark verbatim,\nit is left to the auditor to examine the output and\njudge whether the evidence supports contamination.\nThe process is totally manual and could be scaled\nin a community effort.\nAlternatively, automatic metrics for measuring\ndata contamination levels could be developed. As\nan initial step in this direction, we reuse and adapt\nthe extractability definition presented in Carlini\net al. (2023) for defining memorization. We define\nthat an example s is extractable from evaluation\ndataset d and model m if there exists a sequence\nof k examples x immediately preceding s in d data\nsuch that s is generated when prompting model m\nwith x. We can define the degree of contamination\nof model m for dataset d as the ratio of extractable\nexamples with respect to the total number of exam-\nples in the dataset.\nOne further question remains to be solved which\nis whether the lack of memorization of a bench-\nmark ensures that the LLM was not trained on that\nbenchmark. One hypothesis could be that the lack\nof memorization is correlated with the performance,\neven if the LLM was trained on the benchmark.\nThus the LLM would not have any advantage with\nrespect to another LLM that was not trained on\nthe benchmark. This is currently speculation, so\nfurther research on this topic is necessary, given\nthe extended use of closed LLMs in NLP research.\n6 Call for action\nWe want to encourage the NLP community to: (1)\nDevelop auto- or semi-automatic measures to de-\ntect when data from a benchmark was exposed to a\nmodel; (2) Build a registry of data contamination\ncases, including the evidence for the contamination;\n(3) Encourage authors to use the previous tools to\nensure that the experimental protocol avoids data\ncontamination to the extent possible; and (4) Ad-\ndress data contamination issues during peer review,\nand, in the case of published works, devise mecha-\nnisms to flag those works with the relevant evidence\nof data contamination and how data contamination\n10779\naffects the conclusions.\nAs the problem affects our entire field, we also\nwant to encourage the community to participate in\nworkshops related to this topic, as for example, the\n1st Workshop on Data Contamination2. We think\nthat developing the ideas that will arise from this\ncommunity will play an important role in future\nNLP evaluations.\n7 Limitations\nIn this paper, we address the problem of data con-\ntamination that occurs when evaluating LLMs on\nstandard academic benchmarks. However, we are\naware that there could exist other issues in current\nevaluations, but, they are out of the scope of this po-\nsition paper. Related to our proposed solutions, we\nare aware that these are early-stage solutions and\nthat the proposed effort is really challenging, there-\nfore we call for further discussion and research on\ntopics related to this issue.\nAcknowledgements\nThis work has been partially supported by the\nBasque Government (Research group funding IT-\n1805-22) and the Spanish Government (ILENIA\nproject). Oscar Sainz, Iker García-Ferrero, and,\nJulen Etxaniz are supported by doctoral grants\nfrom the Basque Government (PRE_2023_2_0137,\nPRE_2022_2_0208, and, PRE_2023_2_0060, re-\nspectively).\nReferences\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,\nZhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-\njan Chhablani, Han Wang, Jason Fries, Maged Al-\nshaibani, Shanya Sharma, Urmish Thakker, Khalid\nAlmubarak, Xiangru Tang, Dragomir Radev, Mike\nTian-jian Jiang, and Alexander Rush. 2022. Prompt-\nSource: An integrated development environment and\nrepository for natural language prompts. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstra-\ntions, pages 93–104, Dublin, Ireland. Association for\nComputational Linguistics.\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel\nKahembwe. 2021. Multimodal datasets: misogyny,\npornography, and malignant stereotypes.\n2https://conda-workshop.github.io\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In The Eleventh International Confer-\nence on Learning Representations.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Úlfar\nErlingsson, Alina Oprea, and Colin Raffel. 2021. Ex-\ntracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pages 2633–2650. USENIX Association.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\n10780\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colos-\nsal clean crawled corpus. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1286–1305, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V . Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2021. Glam: Efficient scaling of language mod-\nels with mixture-of-experts. CoRR, abs/2112.06905.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization : Quantify-\ning data leakage in NLP performance evaluation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1325–1335, Online.\nAssociation for Computational Linguistics.\nBesnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg\nRokhlenko, and Shervin Malmasi. 2023. SemEval-\n2023 Task 2: Fine-grained Multilingual Named En-\ntity Recognition (MultiCoNER 2). In Proceedings of\nthe 17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nSophie Groenwold, Lily Ou, Aesha Parekh, Samhita\nHonnavalli, Sharon Levy, Diba Mirza, and\nWilliam Yang Wang. 2020. Investigating African-\nAmerican Vernacular English in transformer-based\ntext generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5877–5883, Online. As-\nsociation for Computational Linguistics.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023.\nKnowledge distillation of large language models.\nRidong Han, Tao Peng, Chaohao Yang, Benyou Wang,\nLu Liu, and Xiang Wan. 2023. Is information extrac-\ntion solved by chatgpt? an analysis of performance,\nevaluation criteria, robustness and errors.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the math dataset. arXiv preprint\narXiv:2103.03874.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\nHallacy, Benjamin Mann, Alec Radford, Aditya\nRamesh, Nick Ryder, Daniel M. Ziegler, John Schul-\nman, Dario Amodei, and Sam McCandlish. 2020.\nScaling laws for autoregressive generative modeling.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS, pages 1693–1701.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. arXiv\npreprint arXiv:2212.09689.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2023. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization.\nAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav\nGoldberg. 2023. Stop uploading test data in plain\ntext: Practical strategies for mitigating data contami-\nnation by evaluation benchmarks.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Benoît Sagot, Clara Rivera, An-\nnette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias Müller, André Müller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine Çabuk Ballı, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\n10781\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50–72.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nBo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei\nYe, Wen Zhao, and Shikun Zhang. 2023a. Evaluating\nchatgpt’s information extraction capabilities: An as-\nsessment of performance, explainability, calibration,\nand faithfulness.\nPeng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan-\nbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023b.\nCodeie: Large code generation models are better few-\nshot information extractors. In Proceedings of the\n61th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), Toronto,\nCanada. Association for Computational Linguistics.\nTao Li, Daniel Khashabi, Tushar Khot, Ashish Sab-\nharwal, and Vivek Srikumar. 2020. UNQOVERing\nstereotyping biases via underspecified questions. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 3475–3489, Online.\nAssociation for Computational Linguistics.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\nThe flan collection: Designing data and methods for\neffective instruction tuning.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 157–165, Dublin, Ireland. Association\nfor Computational Linguistics.\nMarc Marone and Benjamin Van Durme. 2023. Data\nportraits: Recording foundation model training data.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey.\nOpenAI. 2023. Gpt-4 technical report.\nAleksandra Piktus, Christopher Akiki, Paulo Villegas,\nHugo Laurençon, Gérard Dupont, Alexandra Sasha\nLuccioni, Yacine Jernite, and Anna Rogers. 2023.\nThe roots search tool: Data transparency for llms.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOscar Sainz, Jon Ander Campos, Iker García-Ferrero,\nJulen Etxaniz, and Eneko Agirre. 2023. Did chatgpt\ncheat on your test?\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk\nEkmekci, Bill Yuchen Lin, Blake Howald, Bryan\nOrinion, Cameron Diao, Cameron Dour, Cather-\nine Stinson, Cedrick Argueta, César Ferri Ramírez,\nChandan Singh, Charles Rathkopf, Chenlin Meng,\nChitta Baral, Chiyu Wu, Chris Callison-Burch, Chris\nWaites, Christian V oigt, Christopher D. Manning,\nChristopher Potts, Cindy Ramirez, Clara E. Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft,\nCristina Garbacea, Damien Sileo, Dan Garrette, Dan\nHendrycks, Dan Kilman, Dan Roth, Daniel Free-\nman, Daniel Khashabi, Daniel Levy, Daniel Moseguí\nGonzález, Danielle Perszyk, Danny Hernandez,\nDanqi Chen, Daphne Ippolito, Dar Gilboa, David Do-\nhan, David Drakard, David Jurgens, Debajyoti Datta,\nDeep Ganguli, Denis Emelin, Denis Kleyko, Deniz\nYuret, Derek Chen, Derek Tam, Dieuwke Hupkes,\nDiganta Misra, Dilyar Buzan, Dimitri Coelho Mollo,\nDiyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina\nShutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\n10782\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Koco´n,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Ji-\naming Song, Jillian Tang, Joan Waweru, John Bur-\nden, John Miller, John U. Balis, Jonathan Batchelder,\nJonathan Berant, Jörg Frohberg, Jos Rozen, Jose\nHernandez-Orallo, Joseph Boudeman, Joseph Guerr,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari\nKrakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan\nWen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno\nCasares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaul Vicol, Pegah Alipoormolabashi, Peiyuan Liao,\nPercy Liang, Peter Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil,\nPouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing\nLyu, Qinlang Chen, Rabin Banjade, Rachel Etta\nRudolph, Raefer Gabriel, Rahel Habacker, Ramon\nRisco, Raphaël Millière, Rhythm Garg, Richard\nBarnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman\nNovak, Roman Sitelew, Ronan LeBras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-\ndinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib Singh, Saif M. Moham-\nmad, Sajant Anand, Sam Dillavou, Sam Shleifer,\nSam Wiseman, Samuel Gruetter, Samuel R. Bow-\nman, Samuel S. Schoenholz, Sanghyun Han, San-\njeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Titus Tunduny, Tobias Ger-\nstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera\nDemberg, Victoria Nyamai, Vikas Raunak, Vinay\nRamasesh, Vinay Uday Prabhu, Vishakh Padmaku-\nmar, Vivek Srikumar, William Fedus, William Saun-\nders, William Zhang, Wout V ossen, Xiang Ren, Xi-\naoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen,\nYadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Yiding\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang\nHou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zi-\njian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu.\n2023. Beyond the imitation game: Quantifying and\nextrapolating the capabilities of language models.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n10783\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and\nRobert West. 2023. Artificial artificial artificial intel-\nligence: Crowd workers widely use large language\nmodels for text production tasks.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. Ace 2005 multilin-\ngual training corpus. Linguistic Data Consortium,\nPhiladelphia, 57:45.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2020. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022a. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022b. Super-NaturalInstructions: General-\nization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5085–5109, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-shot information extraction via\nchatting with chatgpt.\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo González Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Muñoz,\nMaraim Masoud, María Grandury, Mario Šaško,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis López, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\n10784\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y . Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina V oloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Am-\nmar Khan, Amy Faranak, Ana Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Are-\nzoo Abdollahi, Aycha Tammour, Azadeh HajiHos-\nseini, Bahareh Behroozi, Benjamin Ajibade, Bharat\nSaxena, Carlos Muñoz Ferrandis, Danish Contrac-\ntor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nThéo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023. Bloom: A 176b-parameter\nopen-access multilingual language model.\n10785\nA Empirical demonstrations of\ncontamination\nThis section contains a few empirical demonstra-\ntions of contamination that were memorized by 3\ndifferent models: WizardCoder (Luo et al., 2023),\nChatGPT and GitHub Copilot. As can be seen in\nFigures 2, 1 and 3 all three models are able to per-\nfectly generate back the first lines of the CoNLL03\ndataset training split. It is not surprising, as all the\nmodels were trained on GitHub, where this dataset\nhas been uploaded several times.\nPlease, generate the first instances\nof the CoNLL03 dataset train split\nin BIO format.\nCertainly! The CoNLL-2003 dataset ...\n```\n-DOCSTART- -X- -X- O\nEU NNP B-NP B-ORG\nrejects VBZ B-VP O\nGerman JJ B-NP B-MISC\ncall NN I-NP O\nto TO B-VP O\nboycott VB I-VP O\nBritish JJ B-NP B-MISC\nlamb NN I-NP O\n. . O O\nPeter NNP B-NP B-PER\nBlackburn NNP I-NP I-PER\nBRUSSELS NNP B-NP B-LOC\n1996-08-22 CD I-NP O\nFigure 1: Data contamination on ChatGPT. The given\nprompt is colored and the completion is in black. The\noutput was shortened for commodity.\nA.1 Data contamination reported by other\nworks\nMost of the data contamination analyses have been\nperformed by the authors of LLMs. In the follow-\ning list, we mention the different data contamina-\ntion reports we are aware of:\n• GPT-3 (Brown et al., 2020): Appendix C\n(arXiv version)\n• GPT-4 (OpenAI, 2023): Appendix C\n# This file contains the CoNLL2003\n# English train dataset in BIO format\n-DOCSTART-X- -X- -X- O\nEU NNP B-NP B-ORG\nrejects VBZ B-VP O\nGerman JJ B-NP B-MISC\ncall NN I-NP O\nto TO B-VP O\nboycott VB I-VP O\nBritish JJ B-NP B-MISC\nlamb NN I-NP O\n.. O O\nPeter NNP B-NP B-PER\nBlackburn NNP I-NP I-PER\nFigure 2: Data contamination on WizardCoder. The\ngiven prompt is colored and the completion is in black.\n-DOCSTART- O\nEU B-ORG\nrejects O\nGerman B-MISC\ncall O\nto O\nboycott O\nBritish B-MISC\nlamb O\n.\nPeter B-PER\nBlackburn I-PER\nBRUSSELS B-LOC\n1996-08-22 O\nThe O\nEuropean B-ORG\nCommission I-ORG\nFigure 3: Data contamination on GitHub Copilot. The\ngiven prompt is colored and the completion is in black.\n• LLaMA 2 (Touvron et al., 2023b): Appendix\nA.6\n• FLAN (Wei et al., 2022): Appendix C\n• (Dodge et al., 2021): Section 4.2\n10786\n• GLaM (Du et al., 2021): Appendix D\nAn updated version can be found in the LM Con-\ntamination Index.\n10787",
  "topic": "Flagging",
  "concepts": [
    {
      "name": "Flagging",
      "score": 0.932616114616394
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.9224939346313477
    },
    {
      "name": "Computer science",
      "score": 0.7629441022872925
    },
    {
      "name": "Contamination",
      "score": 0.6598524451255798
    },
    {
      "name": "Task (project management)",
      "score": 0.6333502531051636
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.6144217848777771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5531625747680664
    },
    {
      "name": "Machine learning",
      "score": 0.49039530754089355
    },
    {
      "name": "Position (finance)",
      "score": 0.4599214792251587
    },
    {
      "name": "Natural language processing",
      "score": 0.43413257598876953
    },
    {
      "name": "Data mining",
      "score": 0.43159401416778564
    },
    {
      "name": "Engineering",
      "score": 0.07696285843849182
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169108374",
      "name": "University of the Basque Country",
      "country": "ES"
    }
  ],
  "cited_by": 36
}