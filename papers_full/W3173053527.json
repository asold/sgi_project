{
  "title": "On Improving Adversarial Transferability of Vision Transformers",
  "url": "https://openalex.org/W3173053527",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5083105774",
      "name": "Muzammal Naseer",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A5022250650",
      "name": "Kanchana Ranasinghe",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5000300751",
      "name": "Salman Khan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100760570",
      "name": "Fahad Shahbaz Khan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038550641",
      "name": "Fatih Porikli",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3037414870",
    "https://openalex.org/W3103557498",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2976752987",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2950906520",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2963612069",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W3108341030",
    "https://openalex.org/W2995514567",
    "https://openalex.org/W3142085127",
    "https://openalex.org/W3135159465",
    "https://openalex.org/W2963062382",
    "https://openalex.org/W3144054256",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W2746600820",
    "https://openalex.org/W2962847335",
    "https://openalex.org/W3153906112",
    "https://openalex.org/W3011924763",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2990289029",
    "https://openalex.org/W2963923490",
    "https://openalex.org/W2401231614",
    "https://openalex.org/W2982109374",
    "https://openalex.org/W3162316477",
    "https://openalex.org/W2984699060",
    "https://openalex.org/W3151081946",
    "https://openalex.org/W2964152294",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3009542902",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3006076803",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2180612164",
    "https://openalex.org/W3101298150",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963855547",
    "https://openalex.org/W2896078964",
    "https://openalex.org/W3035345420",
    "https://openalex.org/W3022021750",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2895097814",
    "https://openalex.org/W3099873944"
  ],
  "abstract": "Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \\emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs. Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models. (i) Self-Ensemble: We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. (ii) Token Refinement: We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT. Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens.",
  "full_text": "ON IMPROVING ADVERSARIAL TRANSFERABILITY OF\nVISION TRANSFORMERS\nMuzammal Naseer§† Kanchana Ranasinghe◦ Salman Khan§\nFahad Shahbaz Khan§‡ Fatih Porikli∇\n†Australian National University, ◦Stony Brook University, ‡Linköping University\n§Mohamed bin Zayed University of AI, ∇Qualcomm USA\nmuzammal.naseer@anu.edu.au\nABSTRACT\nVision transformers (ViTs) process input images as sequences of patches via self-\nattention; a radically different architecture than convolutional neural networks\n(CNNs). This makes it interesting to study the adversarial feature space of ViT\nmodels and their transferability. In particular, we observe that adversarial patterns\nfound via conventional adversarial attacks show very low black-box transferability\neven for large ViT models. We show that this phenomenon is only due to the\nsub-optimal attack procedures that do not leverage the true representation potential\nof ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture\ncomprising of self-attention and feed-forward layers, where each block is capable\nof independently producing a class token. Formulating an attack using only the last\nclass token (conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial transferability\nof ViTs. Using the compositional nature of ViT models, we enhance transferability\nof existing attacks by introducing two novel strategies speciﬁc to the architecture\nof ViT models. (i) Self-Ensemble: We propose a method to ﬁnd multiple discrimi-\nnative pathways by dissecting a single ViT model into an ensemble of networks.\nThis allows explicitly utilizing class-speciﬁc information at each ViT block. (ii)\nToken Reﬁnement: We then propose to reﬁne the tokens to further enhance the\ndiscriminative capacity at each block of ViT. Our token reﬁnement systematically\ncombines the class tokens with structural information preserved within the patch\ntokens. An adversarial attack when applied to such reﬁned tokens within the en-\nsemble of classiﬁers found in a single vision transformer has signiﬁcantly higher\ntransferability and thereby brings out the true generalization potential of the ViT’s\nadversarial space. Code: https://t.ly/hBbW.\n1 I NTRODUCTION\nTransformers compose a family of neural network architectures based on the self-attention mecha-\nnism, originally applied in natural language processing tasks achieving state-of-the-art performance\n(Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). The transformer design has been\nsubsequently adopted for vision tasks (Dosovitskiy et al., 2020), giving rise to a number of successful\nvision transformer (ViT) models (Touvron et al., 2020; Yuan et al., 2021; Khan et al., 2021). Due to\nthe lack of explicit inductive biases in their design, ViTs are inherently different from convolutional\nneural networks (CNNs) that encode biases e.g., spatial connectivity and translation equivariance.\nViTs process an image as a sequence of patches which are reﬁned through a series of self-attention\nmechanisms (transformer blocks), allowing the network to learn relationships between any individual\nparts of the input image. Such processing allows wide receptive ﬁelds which can model global context\nas opposed to the limited receptive ﬁelds of CNNs. These signiﬁcant differences between ViTs and\nCNNs give rise to a range of intriguing characteristics unique to ViTs (Caron et al., 2021; Tuli et al.,\n2021; Mao et al., 2021; Paul & Chen, 2021; Naseer et al., 2021b).\nAdversarial attacks pose a major hindrance to the successful deployment of deep neural networks\nin real-world applications. Recent success of ViTs means that adversarial properties of ViT models\n1\narXiv:2106.04169v3  [cs.CV]  3 Mar 2022\nFigure 1: Left: Conventional adversarial attacks view ViT as a single classiﬁer and maximize the prediction\nloss (e.g., cross entropy) to fool the model based on the last classiﬁcation token only. This leads to sub-optimal\nresults as class tokens in previous ViT blocks only indirectly inﬂuence adversarial perturbations. In contrast, our\napproach (right) effectively utilizes the underlying ViT architecture to create a self-ensemble using class tokens\nproduced by all blocks within ViT to design the adversarial attack. Our self-ensemble enables to use hierarchical\ndiscriminative information learned by all class tokens. Consequently, an attack based on our self-ensemble\ngenerates transferable adversaries that generalize well across different model types and vision tasks.\nalso become an important research topic. A few recent works explore adversarial robustness of\nViTs (Shao et al., 2021; Mahmood et al., 2021; Bhojanapalli et al., 2021) in different attack settings.\nSurprisingly, these works show that large ViT models exhibit lower transferability in black-box\nattack setting, despite their higher parameter capacity, stronger performance on clean images, and\nbetter generalization (Shao et al., 2021; Mahmood et al., 2021). This ﬁnding seems to indicate\nthat as ViT performance improves, its adversarial feature space gets weaker. In this work, we\ninvestigate whether the weak transferability of adversarial patterns from high-performing ViT models,\nas reported in recent works (Shao et al., 2021; Mahmood et al., 2021; Bhojanapalli et al., 2021), is\na result of weak features or a weak attack. To this end, we introduce a highly transferable attack\napproach that augments the current adversarial attacks and increase their transferability from ViTs\nto the unknown models. Our proposed transferable attack leverages two key concepts, multiple\ndiscriminative pathways and token reﬁnement, which exploit unique characteristics of ViT models.\nOur approach is motivated by the modular nature of ViTs (Touvron et al., 2020; Yuan et al., 2021; Mao\net al., 2021): they process a sequence of input image patches repeatedly using multiple multi-headed\nself-attention layers (transformer blocks) (Vaswani et al., 2017). We refer to the representation of\npatches at each transformer block as patch tokens. An additional randomly initialized vector (class\ntoken1) is also appended to the set of patch tokens along the network depth to distill discriminative\ninformation across patches. The collective set of tokens is passed through the multiple transformer\nblocks followed by passing of the class token through a linear classiﬁer (head) which is used to\nmake the ﬁnal prediction. The class token interacts with the patch tokens within each block and is\ntrained gradually across the blocks until it is ﬁnally utilized by the linear classiﬁer head to obtain\nclass-speciﬁc logit values. The class token can be viewed as extracting information useful for the\nﬁnal prediction from the set of patch tokens at each block. Given the role of the class token in ViT\nmodels, we observe that class tokens can be extracted from the output of each block and each such\ntoken can be used to obtain a class-speciﬁc logit output using the ﬁnal classiﬁer of a pretrained model.\nThis leads us to the proposed self-ensemble of models within a single transformer (Fig. 1). We show\nthat attacking such a self-ensemble (Sec. 3) containing multiple discriminative pathways signiﬁcantly\nimproves adversarial transferability from ViT models, and in particular from the large ViTs.\nGoing one step further, we study if the class information extracted from different intermediate ViT\nblocks (of the self-ensemble) can be enhanced to improve adversarial transferability. To this end,\nwe introduce a novel token reﬁnement module directed at enhancing these multiple discriminative\npathways. The token reﬁnement module strives to reﬁne the information contained in the output\nof each transformer block (within a single ViT model) and aligns the class tokens produced by\nthe intermediate blocks with the ﬁnal classiﬁer in order to maximize the discriminative power of\nintermediate blocks. Our token reﬁnement exploits the structural information stored in the patch\ntokens and fuses it with the class token to maximize the discriminative performance of each block.\nBoth the reﬁned tokens and self-ensemble ideas are combined to design an adversarial attack that is\nshown to signiﬁcantly boost the transferability of adversarial examples, thereby bringing out the true\n1Average of patch tokens can serve as a class token in our approach for ViT designs that do not use an explicit\nclass token such as Swin transformer (Liu et al., 2021) or MLP-Mixer (Tolstikhin et al., 2021)\n2\ngeneralization of ViTs’ adversarial space. Through our extensive experimentation, we empirically\ndemonstrate favorable transfer rates across different model families (convolutional and transformer)\nas well as different vision tasks (classiﬁcation, detection and segmentation).\n2 B ACKGROUND AND RELATED WORK\nAdversarial Attack Modeling: Adversarial attack methods can be broadly categorized into two\ncategories, white-box attacks and black-box attacks. While the white-box attack setting provides the\nattacker full access to the parameters of the target model, the black-box setting prevents the attacker\nfrom accessing the target model and is therefore a harder setting to study adversarial transferability.\nWhite-box Attack: Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) and Projected\nGradient Descent (PGD) (Madry et al., 2018) are two initially proposed white-box attack methods.\nFGSM corrupts the clean image sample by taking a single step within a small distance (perturbation\nbudget ϵ) along the objective function’s gradient direction. PGD corrupts the clean sample for\nmultiple steps with a smaller step size, projecting the generated adversarial example onto theϵ-sphere\naround the clean sample after each step. Other state-of-the-art white-box attack methods include\nJacobian-based saliency map attack (Papernot et al., 2016), Sparse attack (Modas et al., 2019), One-\npixel attack (Su et al., 2019), Carlini and Wagner optimization (Carlini & Wagner, 2017), Elastic-net\n(Chen et al., 2018), Diversiﬁed sampling (Tashiro et al., 2020), and more recently Auto-attack (Croce\n& Hein, 2020b). We apply white-box attacks on surrogate models to ﬁnd perturbations that are then\ntransferred to black-box target models.\nBlack-box Attack and Transferability: Black-box attacks generally involve attacking a source\nmodel to craft adversarial signals which are then applied on the target models. While gradient\nestimation methods that estimate the gradients of the target model using black-box optimization\nmethods such as Finite Differences (FD) (Chen et al., 2017; Bhagoji et al., 2018) or Natural Evolution\nStrategies (NES) (Ilyas et al., 2018; Jiang et al., 2019) exist, these methods are dependent on\nmultiple queries to the target model which is not practical in most real-world scenarios. In the\ncase of adversarial signal generation using source models, it is possible to directly adopt white-box\nmethods. In our work, we adopt FGSM and PGD in such a manner. Methods like (Dong et al., 2018)\nincorporate a momentum term into the gradient to boost the transferability of existing white-box\nattacks, building attacks named MIM. In similar spirit, different directions are explored in literature\nto boost transferability of adversarial examples; a) Enhanced Momentum: Lin et al. (Lin et al., 2019)\nand Wang et al. (Wang & He, 2021) improve momentum by using Nesterov momentum and variance\ntuning respectively during attack iterations, b) Augmentations: Xie et al. (Xie et al., 2019) showed\nthat applying differentiable stochastic transformations can bring diversity to the gradients and improve\ntransferability of the existing attacks,c) Exploiting Features: Multiple suggestions are proposed in the\nliterature to leverage the feature space for adversarial attack as well. For example, Zhou et al. (Zhou\net al., 2018) incorporate the feature distortion loss during optimization. Similarly, (Inkawhich et al.,\n2020b;a; Huang et al., 2019) also exploit intermediate layers to enhance transferability. However,\ncombining the intermediate feature response with ﬁnal classiﬁcation loss is non-trivial as it might\nrequire optimization to ﬁnd the best performing layers (Inkawhich et al., 2020b;a), and d) Generative\nApproach: Orthogonal to iterative attacks, generative methods (Poursaeed et al., 2018; Naseer et al.,\n2019; 2021a) train an autoencoder against the white-box model. In particular, Naseer et al. show that\ntransferability of an adversarial generator can be increased with relativistic cross-entropy (Naseer\net al., 2019) and augmentations (Naseer et al., 2021a). Ours is the ﬁrst work to address limited\ntransferability of ViT models.\nThe Role of Network Architecture: Recent works exploit architectural characteristics of networks\nto improve the transferability of attacks. While Wu et al. (2020) exploit skip connections of models\nlike ResNets and DenseNets to improve black-box attacks, Guo et al. (2020) build on similar ideas\nfocused on the linearity of models.\nOur work similarly focuses on unique architectural characteristics of ViT models to generate more\ntransferable adversarial perturbations with the existing white-box attacks.\nRobustness of ViTs: Adversarial attacks on ViT models are relatively unexplored. Shao et al.\n(2021) and Bhojanapalli et al. (2021) investigate adversarial attacks and robustness of ViT models\nstudying various white-box and black-box attack techniques. The transferability of perturbations\nfrom ViT models is thoroughly explored in (Mahmood et al., 2021) and they conclude that ViT\n3\nFigure 2: Adversarial examples for ViTs have only\nmoderate transferability. In fact transferabililty (%) of\nMIM (Dong et al., 2018) perturbations to target mod-\nels goes down as the source model size increases such\nas from DeiT-T (Touvron et al., 2020) (5M parameters)\nto DeiT-B (Touvron et al., 2020) (86M parameters).\nHowever, the performance of the attack improves sig-\nniﬁcantly when applied on our proposed ensemble of\nclassiﬁers found within a ViT (MIME & MIMRE).\nmodels do not transfer well to CNNs, whereas we propose a methodology to solve this shortcoming.\nMoreover, Mahmood et al. (2021) explores the idea of an ensemble of CNN and ViT models to\nimprove the transferability of attacks. Our proposed ensemble approach explores a different direction\nby converting a single ViT model into a collection of models (self-ensemble) to improve attack\ntransferability. In essence, our proposed method can be integrated with existing attack approaches to\ntake full advantage of the ViTs’ learned features and generate transferable adversaries.\n3 E NHANCING ADVERSARIAL TRANSFERABILITY OF VITS\nPreliminaries: Given a clean input image sample x with a label y, a source ViT model Fand a\ntarget model Mwhich is under-attack, the goal of an adversarial attack is generating an adversarial\nsignal, x′, using the information encoded within F, which can potentially change the target network’s\nprediction (M(x′)argmax ̸= y). A set of boundary conditions are also imposed on the adversarial\nsignal to control the level of distortion in relation to the original sample, i.e., ∥x −x′∥p <ϵ, for a\nsmall perturbation budget ϵand a p-norm, often set to inﬁnity norm (ℓ∞).\nMotivation: The recent ﬁndings (Shao et al., 2021; Mahmood et al., 2021) demonstrate low black-box\ntransferability of ViTs despite their higher parametric complexity and better feature generalization.\nMotivated by this behaviour, we set-up a simple experiment of our own to study the adversarial\ntransferability of ViTs (see Fig. 2). We note that transferability of adversarial examples found via\nmomentum iterative fast gradient sign method (Dong et al., 2018) (MIM) at ℓ∞ ≤16 on DeiT\n(Touvron et al., 2020) does not increase with model capacity. In fact, adversarial transferability\nfrom DeiT base model (DeiT-B) on ResNet152 and large vision transformer (ViT-L (Dosovitskiy\net al., 2020)) is lower than DeiT tiny model (DeiT-T). This is besides the fact that DeiT-B has richer\nrepresentations and around 17 ×more parameters than DeiT-T. We investigate if this behavior is\ninherent to ViTs or merely due to a sub-optimal attack mechanism. To this end, we exploit unique\narchitectural characteristics of ViTs to ﬁrst ﬁnd an ensemble of networks within a single pretrained\nViT model (self-ensemble, right Fig. 1). The class token produced by each self-attention block is\nprocessed by the ﬁnal local norm and classiﬁcation MLP-head to reﬁne class-speciﬁc information\n(Fig. 2). In other words, our MIME and MIMRE variants attack class information stored in the class\ntokens produced by all the self-attention blocks within the model and optimize for the adversarial\nexample (Sec. 3.1 and 3.2). Exploring the adversarial space of such multiple discriminative pathways\nin a self-ensemble generates highly transferable adversarial examples, as we show next.\n3.1 S ELF -ENSEMBLE : D ISCRIMINATIVE PATHWAYS OF VISION TRANSFORMER\nA ViT model (Dosovitskiy et al., 2020; Touvron et al., 2020), F, with ntransformer blocks can\nbe deﬁned as F= (f1 ◦f2 ◦f3 ◦...f n) ◦g, where fi represents a single ViT block comprising\nof multi-head self-attention and feed-forward layers and gis the ﬁnal classiﬁcation head. To avoid\nnotation clutter, we assume that gconsists of the ﬁnal local norm and MLP-head (Touvron et al.,\n2020; Dosovitskiy et al., 2020). Self-attention layer within the vision transformer model takes a\nsequence of m image patches as input and outputs the processed patches. We will refer to the\nrepresentations associated with the sequence of image patches as patch tokens, Pt ∈Rm×d (where d\nis the dimensionality of each patch representation). Attention in ViT layers is driven by minimizing\nthe empirical risk during training. In the case of classiﬁcation, patch tokens are further appended with\nthe class token (Qt ∈R1×d). These patch and class tokens are reﬁned across multiple blocks (fi) and\nattention in these layers is guided such that the most discriminative information from patch tokens is\npreserved within the class token. The ﬁnal class token is then projected to the number of classes by\nthe classiﬁer, g. Due to the availability of class token at each transformer block, we can create an\n4\nFigure 3: Distribution of discriminative infor-\nmation across blocks of DeiT models. Note\nhow multiple intermediate blocks contain fea-\ntures with considerable discriminative infor-\nmation as measured by top-1 accuracy on the\nImageNet val. set. These are standard mod-\nels pretrained on ImageNet with no further\ntraining. Each block (x-axis) corresponds to\na classiﬁer Fk as deﬁned in Equation 1.\nensemble of classiﬁers by learning a shared classiﬁcation head at each block along the ViT hierarchy.\nThis provides us an ensemble of nclassiﬁers from a single ViT, termed as the self-ensemble:\nFk =\n( k∏\ni=1\nfi\n)\n◦g, where k= 1,2,...,n. (1)\nWe note that the multiple classiﬁers thus formed hold signiﬁcant discriminative information. This\nis validated by studying the classiﬁcation performance of each classiﬁer (Eq. 1) in terms of top-1\n(%) accuracy on ImageNet validation set, as demonstrated in Fig. 3. Note that multiple intermediate\nlayers perform well on the task, especially towards the end of the ViT processing hierarchy.\nFor an input image x with label y, an adversarial attack can now be optimized for the ViT’s self-\nensemble by maximizing the loss at each ViT block. However, we observe that initial blocks (1-6) for\nall considered DeiT models do not contain useful discriminative information as their classiﬁcation\naccuracy is almost zero (Fig. 3). During the training of ViT models (Touvron et al., 2020; Yuan\net al., 2021; Mao et al., 2021), parameters are updated based on the last class token only, which\nmeans that the intermediate tokens are not directly aligned with the ﬁnal classiﬁcation head, gin our\nself-ensemble approach (Fig. 3) leading to a moderate classiﬁcation performance. To resolve this, we\nintroduce a token reﬁnement strategy to align the class tokens with the ﬁnal classiﬁer, g, and boost\ntheir discriminative ability, which in turn helps improve attack transferability.\n3.2 T OKEN REFINEMENT\nAs mentioned above, the multiple discriminative pathways within a ViT give rise to an ensemble of\nclassiﬁers (Eq. 1). However, the class token produced by each attention layer is being processed by\nthe ﬁnal classiﬁer, g. This puts an upper bound on classiﬁcation accuracy for each token which is\nlower than or equal to the accuracy of the ﬁnal class token. Our objective is to push the accuracy of\nthe class tokens in intermediate blocks towards the upper bound as deﬁned by the last token. For this\npurpose, we introduced a token reﬁnement module to ﬁne-tune the class tokens.\nOur proposed token reﬁnement module is illustrated in Fig. 4. It acts as an intermediate layer\ninserted between the outputs of each block (after the shared norm layer) and the shared classiﬁer\nhead. Revisiting our baseline ensemble method (Fig. 1), we note that the shared classiﬁer head\ncontains weights directly trained only on the outputs of the last transformer block. While the class\ntokens of previous layers may be indirectly optimized to align with the ﬁnal classiﬁer, there exists a\npotential for misalignment of these features with the classiﬁer: the pretrained classiﬁer (containing\nweights compatible with the last layer class token) may not extract all the useful information from the\nprevious layers. Our proposed module aims to solve this misalignment by reﬁning the class tokens in\na way such that the shared (pretrained) classiﬁer head is able to extract all discriminative information\nFigure 4: Recent ViTs process 196 image\npatches, leading to 196 patch tokens. We\nrearranged these to create a 14x14 feature\ngrid which is processed by a convolutional\nblock to extract structural information, fol-\nlowed by average pooling to create a single\npatch token. Class token is reﬁned via a\nMLP layer before feeding to the classiﬁer.\nBoth tokens are subsequently merged.\n5\nFigure 5: Self-Ensemble for DeiT (Touvron et al.,\n2020): We measure the top-1 accuracy on ImageNet\nusing the class-token of each block and compare to\nour reﬁned tokens. These results show that ﬁne-tuning\nhelps align tokens from intermediate blocks with the ﬁ-\nnal classiﬁer enhancing their classiﬁcation performance.\nThus token reﬁnement leads to strengthened discrimina-\ntive pathways allowing more transferable adversaries.\ncontained within the class tokens of each block. Moreover, intermediate patch tokens may contain\nadditional information that is not at all utilized by the class tokens of those blocks, which would also\nbe addressed by our proposed block. Therefore, we extract both patch tokens and the class token\nfrom each block and process them for reﬁnement, as explained next.\n−Patch Token Reﬁnement:One of the inputs to the token reﬁnement module is the set of patch tokens\noutput from each block. We ﬁrst rearrange these patch tokens to regain their spatial relationships.\nThe aim of this component within the reﬁnement module is to extract information relevant to spatial\nstructure contained within the intermediate patch tokens. We believe that signiﬁcant discriminative\ninformation is contained within these patches. The obtained rearranged patch tokens are passed\nthrough a convolution block (standard ResNet block containing a skip connection) to obtain a spatially\naware feature map, which is then average pooled to obtain a single feature vector (of same dimension\nas the class token). This feature vector is expected to extract all spatial information from patch tokens.\n−Class Token Reﬁnement: By reﬁning the class tokens of each block, we aim to remove any\nmisalignment between the existing class tokens and the shared (pretrained) classiﬁer head. Also,\ngiven how the class token does not contain a spatial structure, we simply use a linear layer to reﬁne it.\nWe hypothesize that reﬁned class token at each block would be much more aligned with the shared\nclassiﬁer head allowing it to extract all discriminative information contained within those tokens.\n−Merging Patch and Class Token: We obtain the reﬁned class token and the patch feature vector\n(reﬁned output of patch tokens) and sum them together to obtain a merged token. While we tested\nmultiple approaches for merging, simply summing them proved sufﬁcient.\n−Training: Given a ViT model containing ktransformer blocks, we plugin kinstances of our token\nreﬁnement module to the output of each block as illustrated in Figure 4. We obtain the pretrained\nmodel, freeze all existing weights, and train only the ktoken reﬁnement modules for only a single\nepoch on ImageNet training set. We used SGD optimizer with learning rate set to 0.001. Training\nﬁnishes in less than one day on a single GPU-V100 even for a large ViT model such as DeiT-B.\nAs expected, the trained token reﬁnement module leads to increased discriminability of the class\ntokens, which we illustrate in Figure 5. Note how this leads to signiﬁcant boosting of discriminative\npower especially in the earlier blocks, solving the misalignment problem. We build on this enhanced\ndiscriminability of the ensemble members towards better transferability, as explained next.\n3.3 A DVERSARIAL TRANSFER\nOur modiﬁcations to ViT models with respect to multiple discriminative pathways and token reﬁne-\nment are exploited in relation to adversarial transfer. We consider black-box attack perturbations that\nare generated using a source (surrogate) ViT model. The source model is only pretrained on ImageNet,\nmodiﬁed according to our proposed approach and is subsequently ﬁne-tuned to update only the token\nreﬁnement module for a single epoch. We experiment with multiple white-box attacks, generating\nthe adversarial examples using a joint loss over the outputs of each block. The transferability of\nadversarial examples is tested on a range of CNN and ViT models. Given input sample x and its\nlabel y, the adversarial object for our self-ensemble (Eq. 1) for the untargeted attack is deﬁned as,\n6\nFast Gradient Sign Method (FGSM) (Goodfellow et al., 2014)\nConvolutional TransformersSource (↓) Attack\nBiT50 Res152 WRN DN201 ViT-L T2T-24 TnT ViT-S T2T-7\nVGG19bn FGSM 23.34 28.56 33.92 33.22 13.18 10.78 12.96 25.08 29.90\nMNAS FGSM 23.16 39.82 40.10 44.34 16.60 22.56 25.82 34.10 48.96\nDeit-T\nFGSM 29.74 37.10 38.86 42.40 44.38 35.42 50.58 73.32 57.62\nFGSME 30.34 39.60 41.42 45.58 48.34 35.08 51.00 80.74 62.82\nFGSMRE 30.18(+0.44)39.82(+2.7) 41.26(+2.4) 46.06(+3.7) 46.76(+2.4) 32.68(-2.7) 48.00(-2.6) 80.10(+6.8) 63.90(+6.3)\nDeit-S\nFGSM 25.44 31.04 33.58 36.28 36.40 33.41 41.00 58.78 43.48\nFGSME 30.82 38.38 41.06 46.00 47.20 39.00 51.44 78.90 56.70\nFGSMRE 34.84(+9.4) 43.86(+12.8)46.26(+12.7)51.88(+15.6)47.92(+11.5)39.86(+6.5) 55.7(+14.7) 82.00(+23.2)66.20(+22.7)\nDeit-B\nFGSM 22.54 31.58 33.86 34.96 30.50 27.84 33.08 50.24 40.50\nFGSME 31.12 41.46 43.02 47.12 42.28 35.40 46.22 73.04 57.32\nFGSMRE 35.12(+12.6)45.74(+14.2)48.46(+14.6)52.64(+17.7)41.68(+11.2)36.60(+8.8) 49.60(+16.5)74.40(+24.2)65.92(+25.4)\nProjected Gradient Decent (PGD) (Madry et al., 2018)\nVGG19bn PGD 19.80 28.56 33.92 33.22 5.94 10.78 12.96 13.08 29.90\nMNAS PGD 19.44 36.28 36.22 40.20 8.04 18.04 21.16 19.60 41.70\nDeit-T\nPGD 14.22 23.98 24.16 26.76 35.70 21.54 44.24 86.86 53.74\nPGDE 14.42 24.58 25.46 28.38 39.84 21.86 45.08 88.44 53.80\nPGDRE 22.46(+8.24)34.64(+10.7)37.62(+13.5)40.56(+13.8)58.60(+22.9)26.58(+5.0) 55.52(+11.3)96.34(+9.5) 66.68(+12.9)\nDeit-S\nPGD 18.78 24.96 26.38 30.38 37.84 33.46 60.62 84.38 47.14\nPGDE 18.98 27.72 29.54 32.90 44.30 35.40 64.76 89.82 52.76\nPGDRE 28.96(+10.2)38.92(+14.0)42.84(+16.5)46.82(+16.4)60.86(+23.0)40.30(+6.8) 76.10(+15.5)97.32(+12.9)71.54(+24.4)\nDeit-B\nPGD 18.68 25.56 27.90 30.24 34.08 31.98 52.76 69.82 39.80\nPGDE 23.64 32.84 35.40 38.66 43.56 37.82 64.20 82.32 51.68\nPGDRE 37.92(+19.2)49.10(+23.5)53.38(+25.5)56.96(+26.7)56.90(+22.8)45.70(+13.7)79.56(+26.8)94.10(+24.3)74.78(+35.0)\nTable 1: Fool rate (%) on 5k ImageNet val. adversarial samples at ϵ≤16. Perturbations generated from our\nproposed self-ensemble with reﬁned tokens from a vision transformer have signiﬁcantly higher success rate.\nmax\nx′\nk∑\ni=1\n[ [Fk(x′)argmax ̸= y] ], s.t. ∥x −x′∥p ≤ϵ, k ∈{1,2,...,n } (2)\nwhere [ [·] ]is an indicator function. In the case of target attack, the attacker optimizes the above\nobjective towards a speciﬁc target class instead of an arbitrary misclassiﬁcation.\n4 E XPERIMENTS\nWe conduct thorough experimentation on a range of standard attack methods to establish the per-\nformance boosts obtained through our proposed transferability approach. We create ℓ∞adversarial\nattacks with ϵ≤16 and observe their transferability by using the following protocols:\nSource (white-box) models: We mainly study three vision transformers from DeiT (Touvron et al.,\n2020) family due to their data efﬁciency. Speciﬁcally, the source models are Deit-T, DeiT-S, and\nDeiT-B (with 5, 22, and 86 million parameters, respectively). They are trained without CNN\ndistillation. Adversarial examples are created on these models using an existing white-box attack\n(e.g., FGSM (Goodfellow et al., 2014), PGD (Madry et al., 2018) and MIM (Dong et al., 2018)) and\nthen transferred to the black-box target models.\nTarget (black-box) models: We test the black-box transferability across several vision tasks in-\ncluding classiﬁcation, detection and segmentation. We consider convolutional networks including\nBiT-ResNet50 (BiT50) (Beyer et al., 2021), ResNet152 (Res152) (He et al., 2016), Wide-ResNet-50-2\n(WRN) (Zagoruyko & Komodakis, 2016), DenseNet201 (DN201) (Huang et al., 2017) and other ViT\nmodels including Token-to-Token transformer (T2T) (Yuan et al., 2021), Transformer in Transformer\n(TnT) (Mao et al., 2021), DINO (Caron et al., 2021), and Detection Transformer (DETR) (Carion\net al., 2020) as the black-box target models.\nDatasets: We use ImageNet training set to ﬁne tune our proposed token reﬁnement modules. For\nevaluating robustness, we selected 5k samples from ImageNet validation set such that 5 random\nsamples from each class that are correctly classiﬁed by ResNet50 and ViT small (ViT-S) (Dosovitskiy\net al., 2020) are present. In addition, we conduct experiments on COCO (Lin et al., 2014) (5k images)\nand PASCAL-VOC12 (Everingham et al., 2012) (around 1.2k images) validation set.\n7\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nConvolutional TransformersSource (↓) Attack\nBiT50 Res152 WRN DN201 ViT-L T2T-24 TnT ViT-S T2T-7\nVGG19bn MIM 36.18 46.98 54.04 57.32 12.80 21.84 25.72 28.44 47.74\nMNAS MIM 34.78 54.34 55.40 64.06 18.88 34.54 38.70 40.58 60.02\nDeit-T\nMIM 36.22 45.56 47.86 53.26 63.84 48.44 72.52 96.44 77.66\nMIME 34.92 45.58 47.98 54.50 67.16 46.38 71.02 97.74 78.02\nMIMRE 42.04(+5.8) 54.02(+8.5) 58.48(+10.6) 63.00(+9.7) 79.12(+15.3)49.86(+1.4) 77.80(+5.3) 99.14(+2.7) 85.50(+7.8)\nDeit-S\nMIM 38.32 45.06 47.90 52.66 63.38 58.86 79.56 94.22 68.00\nMIME 40.66 49.52 52.98 58.40 71.78 61.06 84.42 98.12 74.58\nMIMRE 53.70(+15.4)61.72(+16.7) 65.10(+17.2) 71.74(+19.1) 84.30(+20.9)66.32(+7.5) 92.02(+12.5)99.42(+5.2) 89.08(+21.1)\nDeit-B\nMIM 36.98 44.66 47.98 52.14 57.48 54.40 70.84 84.74 59.34\nMIME 45.30 54.30 58.34 63.32 70.42 61.84 82.80 94.46 73.66\nMIMRE 61.58(+24.6)70.18(+25.5) 74.08(+26.1) 79.12(+27.0) 81.28(+23.8)69.6(+15.2) 92.20(+21.4)94.10(+9.4) 89.72(+30.4)\nMIM with Input Diversity (DIM) (Xie et al., 2019)\nVGG19bn DIM 46.90 62.08 68.30 73.48 16.86 30.16 34.70 35.42 58.62\nMNAS DIM 43.74 62.08 68.30 73.48 25.06 42.92 47.24 52.74 71.98\nDeit-T\nDIM 57.56 68.30 70.06 77.18 62.00 70.16 82.68 89.16 86.18\nDIME 60.14 70.06 69.84 78.00 66.38 72.30 85.98 93.72 90.78\nDIMRE 62.10(+4.5) 70.78(+2.5) 70.78(+0.7) 78.40(+1.2) 67.58(+5.6) 68.56(-1.6) 84.18(+1.5) 93.36(+4.2) 91.52(+5.3)\nDeit-S\nDIM 59.00 62.12 63.42 67.30 62.62 73.84 79.50 82.32 74.20\nDIME 68.82 74.44 75.34 80.14 76.22 84.10 91.92 94.92 88.42\nDIMRE 76.14(+17.1)81.30(+19.18)82.64(+19.22)86.98(+19.68)78.88(+16.3)85.26(+11.4)93.22(+13.7)96.56(+14.2)93.60(+19.4)\nDeit-B\nDIM 56.24 59.14 60.64 64.44 61.38 69.54 73.96 76.32 64.44\nDIME 73.04 78.36 80.28 83.70 79.06 85.10 91.84 94.38 86.96\nDIMRE 80.10(+23.9)84.92(+25.8) 86.36(+25.7) 89.24(+24.8) 78.90(+17.5)84.00(+14.5)92.28(+18.3)95.26(+18.9)93.42(+28.9)\nTable 2: Fool rate (%) on 5k ImageNet val. adversarial samples at ϵ≤16. Perturbations generated from our\nproposed self-ensemble with reﬁned tokens from a vision transformer have signiﬁcantly higher success rate.\nEvaluation Metrics: We report fooling rate (percentage of samples for which the predicted label\nis ﬂipped after adding adversarial perturbations) to evaluate classiﬁcation. In the case of object\ndetection, we report the decrease in mean average precision (mAP) and for automatic segmentation,\nwe use the popular Jaccard Index. Given the pixel masks for the prediction and the ground-truth, it\ncalculates the ratio between the pixels belonging to intersection and the union of both masks.\nBaseline Attacks: We show consistent improvements for single step fast gradient sign method\n(FGSM) (Goodfellow et al., 2014) as well as iterative attacks including PGD (Madry et al., 2018),\nMIM (Dong et al., 2018) and input diversity (transformation to the inputs) (DIM) (Xie et al., 2019)\nattacks. Iterative attacks ran for 10 iterations and we set transformation probability for DIM to default\n0.7 (Xie et al., 2019). Our approach is not limited to speciﬁc attack settings, but existing attacks\ncan simply be adopted to our self-ensemble ViTs with reﬁned tokens. Refer to appendices A-J for\nextensive analysis with more ViT designs, attacks, datasets (CIFAR10 & Flowers), computational\ncost comparison, and latent space visualization of our reﬁned token.\n4.1 C LASSIFICATION\nIn this section, we discuss the experimental results on adversarial transferability across black-box\nclassiﬁcation models. For a given attack method ‘Attack’, we refer ‘AttackE’ and ‘AttackRE’ as\nself-ensemble and self-ensemble with reﬁned tokens, respectively, which are the two variants of our\napproach. We observe that adversarial transferability from ViT models to CNNs is only moderate\nfor conventional attacks (Tables 1 & 2). For example, perturbations found via iterative attacks from\nDeiT-B to Res152 has even lower transfer than VGG19bn. However, the same attacks when applied\nusing our proposed ensemble strategy (Eq. 1) with reﬁned tokens consistently showed improved\ntransferability to other convolutional as well as transformer based models. We observe that models\nwithout inductive biases that share architecture similarities show higher transfer rate of adversarial\nperturbations among them (e.g., from DeiT to ViT (Dosovitskiy et al., 2020)). We further observe\nthat models trained with the same mechanism but lower parameters are more vulnerable to black-box\nattacks. For example, ViT-S and T2T-T are more vulnerable than their larger counterparts, ViT-L\nand T2T-24. Also models trained with better strategies that lead to higher generalizability are less\nvulnerable to black-box attacks e.g., BiT50 is more robust than ResNet152 (Tables 1 and 2).\n8\nFigure 6: Ablative Study :\nFooling rate of intermediate lay-\ners under MIM (white-box) at-\ntack using our self-ensemble ap-\nproach. We obtain favorable im-\nprovements for our method.\nSource (→) DeiT-T DeiT-S DeiT-B\nNo AttackMIM MIMRE MIM MIMRE MIM MIMRE\n38.5\n24.0 19.7 23.7 19.0 22.9 16.9\nDIM DIMRE DIM DIMRE DIM DIMRE\n20.5 13.7 20.3 12.0 19.9 11.1\nTable 3: Cross-Task Transferability (classiﬁ-\ncation→detection) Object Detector DETR (Car-\nion et al., 2020) is fooled. mAP at [0.5:0.95] IOU\non COCO val. set. Our self-ensemble approach\nwith reﬁned token (RE) signiﬁcantly improves\ncross-task transferability. (lower the better)\nSource (→) DeiT-T DeiT-S DeiT-B\nNo AttackMIM MIMRE MIM MIMRE MIM MIMRE\n42.7\n32.5 31.6 32.5 31.0 32.6 30.6\nDIM DIMRE DIM DIMRE DIM DIMRE\n31.9 31.4 31.7 31.3 32.0 31.0\nTable 4: Cross-Task Transferability (classi-\nﬁcation→segmentation) DINO (Caron et al.,\n2021) is fooled. Jaccard index metric is used\nto evaluate segmentation performance. Best ad-\nversarial transfer results are achieved using our\nmethod. (lower the better)\nClean Image\n Adv Image\n Clean Image\n Adv Image\n Clean Image\n Adv Image\n Figure 7: Visualization of\nDETR failure cases for our\nproposed DIMRE attack gener-\nated from DeiT-S source model.\n(best viewed in zoom)\nThe strength of our method is also evident by blockwise fooling rate in white-box setting (Fig. 6). It\nis noteworthy how MIM fails to fool the initial blocks of ViT, while our approach allows the attack to\nbe as effective in the intermediate blocks as for the last class token. This ultimately allows us to fully\nexploit ViT’s adversarial space leading to high transfer rates for adversarial perturbations.\n4.2 C ROSS -TASK TRANSFERABILITY\nSelf-attention is the core component of transformer architecture regardless of the task; classiﬁcation\n(Dosovitskiy et al., 2020; Touvron et al., 2020; Yuan et al., 2021; Mao et al., 2021), object detection\n(Carion et al., 2020), or unsupervised segmentation (Caron et al., 2021). We explore the effectiveness\nof our proposed method on two additional tasks: object detection (DETR) (Carion et al., 2020)\nand segmentation (DINO) (Caron et al., 2021). We select these methods considering the use of\ntransformer modules employing the self-attention mechanism within their architectures. While the\ntask of object detection contains multiple labels per image and involves bounding box regression,\nthe unsupervised model DINO is trained in a self-supervised manner with no traditional image-level\nlabels. Moreover, DINO uses attention maps of a ViT model to generate pixel-level segmentations,\nwhich means adversaries must disrupt the entire attention mechanism to degrade its performance.\nWe generate adversarial signals on source models with a classiﬁcation objective using their initial\npredictions as the label. In evaluating attacks on detection and segmentation tasks at their optimal\nsetting, the source ViT need to process images of different sizes (e.g., over 896×896 pix for DETR).\nTo cater for this, we process images in parts (refer appendix G) which allows generation of stronger\nadversaries. The performance degradation of DETR and DINO on generated adversaries are sum-\nmarised in Tables 3 & 4. For DETR, we obtain clear improvements. In the more robust DINO model,\nour transferability increases well with the source model capacity as compared to the baseline.\n5 C ONCLUSION\nWe identify a key shortcoming in the current state-of-the-art approaches for adversarial transferability\nof vision transformers (ViTs) and show the potential for much strong attack mechanisms that would\nexploit the architectural characteristics of ViTs. Our proposed novel approach involving multiple\ndiscriminative pathways and token reﬁnement is able to ﬁll in these gaps, achieving signiﬁcant\nperformance boosts when applied over a range of state-of-the-art attack methods.\n9\nReproducibility Statement: Our method simply augments the existing attack approaches and we\nused open source implementations. We highlight the steps to reproduce all of the results presented\nin our paper, a) Attacks: We used open source implementation of patchwise attack (Gao\net al., 2020) and Auto-Attack (Croce & Hein, 2020b) (refer Appendix B) with default setting.\nWherever necessary, we clearly mention attack parameters, e.g., iterations for PGD (Madry et al.,\n2018), MIM (Dong et al., 2018) and DIM (Xie et al., 2019) in section 4 (Experiments: Baseline\nAttack). Similarly, transformation probability for DIM is set to the default value provided by the\ncorresponding authors that is 0.7, b) Refined Tokens: We ﬁne tuned class tokens for the\npretrained source models (used to create perturbations) using open source code base ( https:\n//github.com/pytorch/examples/tree/master/imagenet). We provided training\ndetails for ﬁne tuning in section 3.2. Further, we will publicly release all the models with reﬁned\ntokens, c) Cross-Task Attack Implementation: We provided details in section 4.2 and\npseudo code in appendix G for cross-task transferability (from classiﬁcation to segmentation and\ndetection), and d) Dataset: We describe the procedure of selecting subset (5k) samples from\nImageNet val. set in section 4. We will also release indices of these samples to reproduce the results.\nEthics Statement: Since our work focuses on improving adversarial attacks on models, in the short\nrun our work can assist various parties with malicious intents of disrupting real-world deployed deep\nlearning systems dependent on ViTs. However, irrespective of our work, the possibility of such threats\nemerging exists. We believe that in the long run, works such as ours will support further research on\nbuilding more robust deep learning models that can withstand the kind of attacks we propose, thus\nnegating the short term risks. Furthermore, a majority of the models used are pre-trained on ImageNet\n(ILSVRC’12). We also conduct our evaluations using this dataset. The version of ImageNet used\ncontains multiple biases that portray unreasonable social stereotypes. The data contained is mostly\nlimited to the Western world, and encodes multiple gender / ethnicity stereotypes Yang et al. (2020)\nwhile also posing privacy risks due to unblurred human faces. In future, we hope to use the more\nrecent version of ImageNet Yang et al. (2021) which could address some of these issues.\nREFERENCES\nMaksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-\nefﬁcient black-box adversarial attack via random search. In European Conference on Computer Vision, pp.\n484–501. Springer, 2020. 14, 15\nLucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge\ndistillation: A good teacher is patient and consistent. arXiv preprint arXiv:2106.05237, 2021. 7\nArjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks on deep neural networks\nusing efﬁcient query mechanisms. In ECCV, 2018. 3\nSrinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit.\nUnderstanding robustness of transformers for image classiﬁcation. ArXiv, abs/2103.14586, 2021. 2, 3\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020. 1\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In European Conference on Computer Vision, pp. 213–229.\nSpringer, 2020. 7, 9\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. InS&P, 2017. 3\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. 1, 7, 9\nPin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based\nblack-box attacks to deep neural networks without training substitute models. In AISec, 2017. 3\nPin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural\nnetworks via adversarial examples. In AAAI, 2018. 3\nFrancesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary\nattack. In International Conference on Machine Learning, pp. 2196–2205. PMLR, 2020a. 14, 15\n10\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse\nparameter-free attacks. In ICML, 2020b. 3, 10, 14, 15, 16\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1\nYinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting\nadversarial attacks with momentum. In CVPR, 2018. 3, 4, 7, 8, 10, 14, 17, 18, 19, 23\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 4, 7, 8, 9\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes\nChallenge 2012 (VOC2012) Results, 2012. 7\nLianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, and Hengtao Shen. Patch-wise attack for fooling\ndeep neural network. In European Conference on Computer Vision, 2020. 10, 14, 15\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In\nICLR, 2014. 3, 7, 8, 23\nYiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of adversarial\nexamples. In Advances in Neural Information Processing Systems, 2020. 3\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 7, 20, 24\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708,\n2017. 7\nQian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing adversarial\nexample transferability with an intermediate level attack. InProceedings of the IEEE International Conference\non Computer Vision, pp. 4733–4742, 2019. 3\nAndrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited\nqueries and information. In ICML, 2018. 3\nNathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen. Transferable perturbations of deep\nfeature distributions. In International Conference on Learning Representations , 2020a. URL https:\n//openreview.net/forum?id=rJxAo2VYwr. 3\nNathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, and Yiran Chen.\nPerturbing across the feature hierarchy to improve standard and strict blackbox attack transferability. arXiv\npreprint arXiv:2004.14861, 2020b. 3\nLinxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, and Yu-Gang Jiang. Black-box adversarial attacks on\nvideo recognition models. In ACM MM, 2019. 3\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021. 1\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 14\nJiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. Nesterov accelerated gradient and\nscale invariance for adversarial attacks. arXiv preprint arXiv:1908.06281, 2019. 3\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer\nvision, pp. 740–755. Springer, 2014. 7\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n2, 14, 16\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep\nlearning models resistant to adversarial attacks. In ICLR, 2018. 3, 7, 8, 10, 14, 16, 17, 23\n11\nKaleel Mahmood, Rigel Mahmood, and Marten van Dijk. On the robustness of vision transformers to adversarial\nexamples. ArXiv, abs/2104.02610, 2021. 2, 3, 4\nYuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan, and\nNick Barnes. Transformer transforms salient object detection and camouﬂaged object detection. arXiv\npreprint arXiv:2104.10127, 2021. 1, 2, 5, 7, 9\nApostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Sparsefool: a few pixels make a big\ndifference. In CVPR, 2019. 3\nMuzammal Naseer, Salman H Khan, Harris Khan, Fahad Shahbaz Khan, and Fatih Porikli. Cross-domain\ntransferability of adversarial perturbations. Advances in Neural Information Processing Systems, 2019. 3\nMuzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. On generating\ntransferable targeted perturbations. arXiv preprint arXiv:2103.14641, 2021a. 3, 14\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-\nHsuan Yang. Intriguing properties of vision transformers, 2021b. 1\nMaria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large number of classes.\nIn 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722–729. IEEE,\n2008. 14\nNicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.\nThe limitations of deep learning in adversarial settings. In EuroS&P, 2016. 3\nSayak Paul and Pin-Yu Chen. Vision transformers are robust learners, 2021. 1\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\nMachine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. 21\nOmid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturbations. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4422–4431, 2018. 3\nRulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness of visual\ntransformers. ArXiv, abs/2103.15670, 2021. 2, 3, 4\nJiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural networks.\nIn IEEE Transactions on Evolutionary Computation. IEEE, 2019. 3\nYusuke Tashiro, Yang Song, and Stefano Ermon. Output diversiﬁed initialization for adversarial attacks. arXiv\npreprint arXiv:2003.06878, 2020. 3\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica\nYung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for vision.\narXiv preprint arXiv:2105.01601, 2021. 2, 14, 20, 22\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.\nTraining data-efﬁcient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877,\n2020. 1, 2, 4, 5, 6, 7, 9, 14, 15\nShikhar Tuli, Ishita Dasgupta, Erin Grant, and Thomas L. Grifﬁths. Are convolutional neural networks or\ntransformers more like human vision?, 2021. 1\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008. 21\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 1, 2\nXiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. arXiv\npreprint arXiv:2103.15571, 2021. 3\nDongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter: On the\ntransferability of adversarial examples generated with resnets. In ICLR, 2020. 3\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. 14, 20, 22\n12\nCihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving\ntransferability of adversarial examples with input diversity. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 2730–2739, 2019. 3, 8, 10, 14, 17, 18, 19, 23\nKaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering and\nbalancing the distribution of the people subtree in the imagenet hierarchy. In faacct, pp. 547–558, 2020. 10\nKaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in\nimagenet. arXiv preprint arXiv:2103.06191, 2021. 10\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet.arXiv preprint arXiv:2101.11986,\n2021. 1, 2, 5, 7, 9, 15\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 7\nWen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and Yong Yang. Transferable\nadversarial perturbations. In Proceedings of the European Conference on Computer Vision (ECCV) , pp.\n452–467, 2018. 3\n13\nAppendix\nAdversarial perturbations found with ensemble of models are shown to be more transferable (Dong\net al., 2018; Naseer et al., 2021a). In appendix A, we demonstrate the effectiveness of our self-\nensemble to boost adversarial transferability within ensemble of different models. Our approach\naugments and enhances the transferability of the existing attack. We further demonstrate this with\nrecent attacks, Patchwise (Gao et al., 2020) and Auto-Attack (Croce & Hein, 2020b) in appendix\nB. Auto-Attack is a strong white-box attack which is a combination of PGD with novel losses and\nother attacks such as boundary-based, FAB (Croce & Hein, 2020a) and, query based Square attack\n(Andriushchenko et al., 2020). This highlights that our method can be used as plug-and-play with\nexisting attack methods. We then evaluate adversarial transferability of Auto-Attack and PGD (100\niterations) at ϵ4, 8, and 16 in appendix C. Our self-ensemble with reﬁned tokens approach consistently\nperforms better with these attack settings as well. In appendix D, we extended our approach to other\ndataset including CIFAR10 (Krizhevsky et al., 2009) and Flowers (Nilsback & Zisserman, 2008)\ndatasets. We highlight vulnerability of Swin transformer (Liu et al., 2021) against our approach in\nappendix E. We showed the results on full ImageNet validation set (50k samples) in appendix F. This\ndemonstrates the effectiveness of our method regardless of dataset or task. We discuss the generation\nof adversarial samples for images of sizes that are different to the source ViT models’ input size\n(e.g., greater than 224) in appendix G. We provide computational cost analysis in Appendix H and\nvisualize the latent space of reﬁned tokens in Appendix H.1. Finally, we extended our approach to\ndiverse ViT designs (Wu et al., 2021; Tolstikhin et al., 2021) and CNN in Appendices I and J.\nA S ELF -ENSEMBLE WITHIN ENSEMBLE\nWe created an ensemble of pre-trained Deit models (Touvron et al., 2020) including Deit-T, Deit-\nS and DeiT-B. These models have 12 blocks (layers) and differ in patch embedding size. These\nmodels are trained in a similar fashion without distillation from CNN. As expected, adversarial\ntransferability improves with an ensemble of models (Table 5). We also note that unlike such\nmultiple-model ensemble approaches, our self-ensemble attack achieves performance improvements\nwith minimal increase in computational complexity of the attack that is from a single ViT. Our\nself-ensemble extended three classiﬁers ensemble to an ensemble of 36 classiﬁers. Such multi-model\nensemble combined with our proposed self-ensemble with reﬁned tokens approach leads to clear\nattack improvements in terms of transferability (refer Table 5).\nProjected Gradient Decent (PGD) (Madry et al., 2018)\nConvolutional TransformersSource (↓) Attack\nRes152 WRN DN201 T2T-24 T2T-7 TnT ViT-S\nDeit-[T+S+B]\nPGD 46.1 48.5 51.0 62.4 64.0 81.5 85.7\nPGDE 49.4 50.7 56.7 62.8 68.9 83.4 87.4\nPGDRE 64.1(+18) 69.0(+20.5) 75.1(+24.1) 73.7(+11.3) 87.0(+23) 93.5(+12) 95.3(+9.6)\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nDeit-[T+S+B]\nMIM 62.6 66.6 70.2 77.2 76.2 86.4 87.4\nMIME 67.3 68.0 73.5 76.2 78.7 87.9 87.9\nMIMRE 76.7(+14.1) 80.6(+14) 84.0(+13.8) 84.0(+6.8) 89.3(+13.1) 93.1(+6.7) 93.6(+6.2)\nMIM with Input Diversity (Xie et al., 2019)\nDeit-[T+S+B]\nDIM 77.2 77.9 79.7 80.7 81.4 85.0 85.7\nDIME 77.8 78.0 80.7 84.6 81.5 86.1 86.4\nDIMRE 83.1(+5.9) 84.8(+6.9) 86.0(+6.3) 84.9(+4.2) 87.2(+5.8) 87.3(+2.3) 89.0(+3.3)\nTable 5: Self-ensemble within Ensemble: Fool rate (%) on 5k ImageNet val. adversarial samples at ϵ≤16.\nOur proposed self-ensemble with reﬁned tokens have signiﬁcantly higher success rate of adversarial perturbations\nfound within an ensemble of models (Deit-T, Deit-S and Deit-B).\n14\nB S ELF -ENSEMBLE FOR MORE ATTACKS : PATCHWISE AND AUTO -ATTACK\nWe conducted additional experiments with recent methods such as Patch-wise attack Gao et al. (2020)\nand Auto-Attack Croce & Hein (2020b) to show case that our approach does not need special setup\nand can be used as plug-and-play with the existing attacks. Patch-wise Gao et al. (2020) is a recent\nblack-box attack while Auto-Attack is a combination of PGD with novel losses and other attacks such\nas boundary-based, FAB (Croce & Hein, 2020a) and, query based Square attack (Andriushchenko\net al., 2020). In each case, attack transferability is increased signiﬁcantly using our approach of\nself-ensemble with reﬁned tokens (refer Table 6).\nPatchwise (PI) attack (Gao et al., 2020)\nConvolutional TransformersSource (↓) Attack\nRes152 WRN DN201 T2T-24 T2T-7 TnT ViT-S\nDeit-T\nPI 54.1 58.5 62.6 41.1 65.3 55.6 87.0\nPIE 56.0 61.3 65.3 42.8 68.8 58.0 89.9\nPIRE 60.4(+6.3) 67.3(+8.8) 71.4(+8.8) 44.1(+3) 74.8(+9.5) 64.0(+8.4) 95.6(+8.6)\nDeit-S\nPI 53.1 57.6 63.9 49.6 59.7 64.1 84.2\nPIE 57.0 62.6 66.8 51.8 65.0 71.6 89.5\nPIRE 63.6(+10.5) 70.4(+12.8) 74.8(+10.9) 53.1(+3.5) 76.3(+16.6) 78.4(+14.3) 96.2(+12)\nDeit-B\nPI 53.3 56.6 60.2 47.8 54.0 59.8 74.7\nPIE 60.7 64.4 70.0 53.0 63.3 71.4 86.6\nPIRE 70.0(+16.6) 73.8(+17.2) 77.8(+17.6) 57.7(+9.9) 77.3(+23.3) 78.4(+18.6) 94.0(+19.3)\nAuto-Attack (AA) (Croce & Hein, 2020b)\nDeit-T\nAA 15.9 19.6 19.0 11.0 34.3 19.9 52.0\nAAE 14.0 15.8 17.1 10.3 27.9 18.0 49.3\nAARE 20.0(+4.1) 24.2(+4.6) 27.7(+8.7) 13.4(+2.4) 40.0(+5.7) 28.1(+8.2) 58.8(+6.8)\nDeit-S\nAA 23.5 22.9 24.8 21.0 40.4 36.2 61.3\nAAE 21.9 23.3 25.8 22.2 38.6 37.8 60.3\nAARE 29.3(+5.8) 32.9(+10) 36.7(+11.9) 30.2(+9.2) 51.9(+11.5) 49.7(+13.5) 68.5(+7.2)\nDeit-B\nAA 25.6 28.5 28.4 23.4 39.0 39.9 55.7\nAAE 26.6 29.2 31.5 26.1 40.7 40.2 55.8\nAARE 37.4(+11.8) 40.4(+11.9) 42.6(+14.2) 36.3(+12.9) 56.7(+17.7) 55.6(+15.7) 69.5(+13.8)\nTable 6: Fool rate (%) on 5k ImageNet val. adversarial samples at ϵ ≤16. Auto-attack and patchwise\nattacks when applied to our proposed self-ensemble with reﬁned tokens have signiﬁcantly higher transfer rate to\nunknown convolutional and other vision transformers.\nC S ELF -ENSEMBLE FOR DIFFERENT ϵ: 4, 8, 16\nWe evaluate if adversarial transferability boost provided by our method also hold for large number of\nattack iterations such PGD with 100 iterations and Auto-Attack which is based on multiple random\nrestarts, thousands of queries, etc. We further evaluate adversarial transfer at various perturbation\nbudgets, ϵsuch as 4, 8, and 16. Our self-ensemble with reﬁned tokens approach consistently improves\nblack-box adversarial transfer under such attack setting, thus making them not only powerful in\nwhite-box but also in black-box setting. Results are presented in Table 7.\nD S ELF -ENSEMBLE FOR MORE DATASETS : CIFAR10 AND FLOWERS\nWe extended our approach to CIFAR10 and Flowers datasets. Since ViT training from scratch on\nsmall datasets is unstable Touvron et al. (2020); Yuan et al. (2021), we ﬁne-tuned ViTs trained on\nImageNet on CIFAR10 and Flowers. Transferability of adversarial attacks increases with notable\ngains by using our approach based on self-ensemble and reﬁned tokens (refer Tables 8, 9, 10 and 11).\n15\nAuto-Attack (Croce & Hein, 2020b)\nConvolutional TransformersSource (↓) Attack\nRes152 WRN DN201 T2T-24 T2T-7 TnT ViT-S\nDeit-T\nAA 4.2/9.0/15.9 4.3/8.9/19.6 5.4/9.3/19.03.3/4.8/11.0 10.6/18.8/34.3 5.1/10.3/19.9 17.6/33.7/52.0\nAAE 4.4/8.9/14.0 4.4/8.6/15.8 5.0/9.6/17.13.7/4.8/11.3 10.8/17.6/27.9 5.7/10.0/18.0 18.5/34.4/49.3\nAARE 6.3/12.1/20.0 7.3/12.9/24.2 7.3/13.9/27.74.7/9.8/13.4 15.9/28.3/40.0 9.2/15.5/28.1 28.7/47.6/58.8\nDeit-S\nAA 8.4/12.9/23.5 7.9/13.2/22.9 8.0/14.7/24.87.3/11.3/21 15.9/25.0/40.4 12.4/20.6/36.2 20.7/42.0/61.3\nAAE 8.8/12.5/21.9 7.6/13.6/23.3 9.0/15.1/25.89.1/14.3/22.2 16.0/25.2/38.6 18.3/27.2/37.8 28.9/44.9/60.3\nAARE 9.5/18.5/29.3 10.5/19.7/32.9 12.0/22.9/36.711.6/18.1/30.2 24.6/37.2/51.9 23.3/35.2/49.7 37.6/58.4/68.5\nDeit-B\nAA 8.2/14.4/25.6 8.4/13.6/28.5 9.8/16.5/28.47.7/12.4/23.4 15.4/22.4/39.0 13.2/21.6/39.9 16.6/32.8/55.7\nAAE 8.8/15.3/26.6 9.1/17.4/29.2 11.1/19.3/31.59.5/15.7/26.1 17.3/26.3/40.7 16.2/27.0/40.2 21.3/37.0/55.8\nAARE 12.1/21.5/37.4 13.3/24.5/40.4 14.2/26.7/42.612.6/21.5/36.3 25.0/39.5/56.7 26.9/40.6/55.6 29.6/53.8/69.5\nProjected Gradient Decent (PGD) (Madry et al., 2018)\nDeit-T\nPGD 6.3/15.2/27.5 8.1/17.9/30.1 8.7/20.3/33.87.0/9.6/24.2 18.6/30.8/56.1 13.7/33.6/45.3 33.6/63.7/87.8\nPGDE 8.5/18.6/30.5 9.0/18.2/30.7 10.7/19.2/34.57.9/10.0/26.3 23.1/39.8/58.9 14.9/35.8/47.1 52.3/75.0/90.3\nPGDRE12.3/28.9/40.6 18.8/23.5/44.0 16.5/29.1/48.815.0/26.8/33.1 30.9/53.9/73.1 29.2/52.3/69.7 68.2/80.1/98.9\nDeit-S\nPGD 9.3/16.8/30.1 14.9/20.5/32.7 9.8/23.5/35.316.3/20.3/31.5 26.9/36.3/51.8 26.8/40.6/56.1 40.9/62.5/84.5\nPGDE 12.3/17.0/35.2 16.7/24.1/36.5 11.2/26.3/38.917.0/22.2/34.8 32.5/41.2/53.0 28.0/44.8/62.1 51.2/77.7/90.7\nPGDRE17.8/30.1/44.7 18.7/33.6/50.6 20.7/35.6/54.923.1/34.1/45.8 48.5/67.8/80.6 40.1/65.9/85.0 67.8/88.8/98.4\nDeit-B\nPGD 13.0/20.4/33.2 11.5/18.6/33.8 14.7/28.9/46.114.7/19.4/30.0 15.3/24.8/37.9 20.2/35.2/50.7 20.6/44.8/65.2\nPGDE 13.5/21.5/40.1 17.1/28.4/44.3 17.8/30.7/47.816.7/35.7/45.0 26.7/42.2/57.6 36.8/60.1/73.3 44.8/67.5/85.6\nPGDRE19.1/38.9/60.8 23.3/40.8/64.7 30.2/56.8/70.819.6/40.3/56.1 49.6/66.6/80.2 45.9/69.9/88.7 59.6/82.6/98.6\nTable 7: Fool rate (%) on 5k ImageNet val. at various perturbation budgets. Auto-attack and PGD (100\niterations) are evaluated at ϵ≤4/8/16. Our method consistently performs better.\nProjected Gradient Descent (PGD) (Madry et al., 2018)\nTransformers ConvolutionalSource (↓) Attack\nT2T-24 T2T-7 Res50\nDeit-T\nPGD 26.8 / 41.9 46.4 / 61.0 25.4 / 39.2\nPGDE 30.6 / 42.3 49.6 / 60.9 35.2 / 42.2\nPGDRE 37.7 / 57.3 65.4 / 80.7 49.5 / 57.2\nDeit-S\nPGD 35.6 / 47.6 42.9 / 54.9 27.2 / 36.1\nPGDE 35.4 / 47.8 46.9 / 59.6 31.2 / 40.6\nPGDRE 46.6 / 65.5 68.5 / 82.1 51.2 / 61.9\nDeit-B\nPGD 29.1 / 40.4 33.5 / 43.7 26.2 / 34.6\nPGDE 30.1 / 43.0 40.7 / 54.8 33.4 / 41.0\nPGDRE 40.3 / 44.2 61.4 / 78.0 52.3 / 65.7\nTable 8: Self-ensemble with reﬁned tokens for CIFAR10: PGD fool rate (%) on CIFAR10 test set (10k\nsamples). In each table cell, performances are shown for two perturbation budgets i.e., ϵ≤8/16.\nE V ULNERABILITY OF SWIN TRANSFORMER\nWe show how the black-box vulnerability of ViT architecture without explicit class token (Swin\ntransformer (Liu et al., 2021)) increases against our approach (refer Table 12).\nF F ULL IMAGE NET VALIDATION SET\nIn line with most existing attack methods, we use subset of ImageNet (5k samples, 5 samples from\neach class) for all our experiments. Our subset from ImageNet is balanced as we sample from each\nclass. We will release the image-indices of ours subset publicly to reproduce the results.\nAdditionally, we re-run our best performing attacks (MIM and DIM) on the whole ImageNet val.\nset (50k samples) to validate the merits of our approach ( refer Table 13). Our approach shows\nconsiderable improvements in attack transferability regardless the dataset size.\n16\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nTransformers ConvolutionalSource (↓) Attack\nT2T-24 T2T-7 Res50\nDeit-T\nMIM 45.8 / 70.2 63.9 / 81.2 50.6 / 72.2\nMIME 48.8 / 70.1 65.7 / 81.0 57.3 / 74.0\nMIMRE 57.8 / 83.0 78.8 / 90.7 57.8 / 77.3\nDeit-S\nMIM 62.3 / 79.1 62.1 / 79.0 48.4 / 69.6\nMIME 62.1 / 78.0 65.2 / 82.3 52.3 / 71.4\nMIM 70.3 / 88.8 82.0 / 91.4 60.2 / 79.3\nDeit-B\nMIM 54.3 / 71.1 51.6 / 72.7 50.6 / 68.7\nMIME 55.4 / 73.8 60.8 / 79.6 55.6 / 72.4\nMIMRE 62.3 / 74.8 75.8 / 89.7 63.6 / 81.3\nMIM with Input Diversity (DIM) (Xie et al., 2019)\nDeit-T\nDIM 56.4 / 90.1 67.1 / 89.1 67.4 / 85.4\nDIME 62.6 / 90.2 77.8 / 90.3 68.4 / 85.6\nDIMRE 70.2 / 92.2 80.4 / 93.6 72.5 / 89.0\nDeit-S\nDIM 66.9 / 90.5 70.8 / 86.5 67.7 / 83.8\nDIME 73.7 / 90.8 71.2 / 89.2 69.7 / 87.2\nDIMRE 75.9 / 94.5 82.6 / 93.7 75.3 / 92.6\nDeit-B\nDIM 71.7 / 86.9 63.6 / 82.4 60.3 / 83.3\nDIME 73.0 / 93.6 72.4 / 90.3 68.7 / 89.8\nDIMRE 75.7 / 95.0 77.5 / 93.0 72.4 / 92.8\nTable 9: Self-ensemble with reﬁned tokens for CIFAR10: MIM and DIM fool rate (%) on CIFAR10 test set\n(10k samples). In each table cell, performances are shown for two perturbation budgets i.e., ϵ≤8/16.\nProjected Gradient Descent (PGD) (Madry et al., 2018)\nTransformers ConvolutionalSource (↓) Attack\nT2T-24 T2T-7 Res50\nDeit-T\nPGD 22.4 / 28.8 31.3 / 41.3 16.3 / 26.8\nPGDE 25.6 / 35.5 35.6 / 48.7 25.4 / 33.8\nPGDRE 26.0 / 37.6 39.5 / 55.9 38.4 / 47.2\nDeit-S\nPGD 20.7 / 26.9 30.8 / 41.3 17.4 / 26.2\nPGDE 23.2 / 31.8 34.4 / 47.3 25.0 / 32.6\nPGDRE 25.3 / 35.4 40.1 / 56.1 43.2 / 52.0\nDeit-B\nPGD 20.6 / 26.9 32.0 / 41.3 18.4 / 27.2\nPGDE 23.5 / 32.0 34.3 / 48.0 29.3 / 36.3\nPGDRE 27.0 / 39.0 39.4 / 56.9 49.8 / 57.0\nTable 10: Self-ensemble with reﬁned tokens for Flowers: PGD fool rate (%) on Flowers test set. In each table\ncell, performances are shown for two perturbation budgets i.e., ϵ≤8/16.\nG C ROSS -TASK TRANSFERABILITY WITH LARGER IMAGES (>224)\nWe generate adversarial signals on source models with a classiﬁcation objective using their initial\npredictions as the label (Algorithm 1). In the case of ViT models trained on 224x224 images, we\nrescale all larger images to a multiple of 224x224, split them into smaller 224x224 image portions,\nobtain the prediction of the source (surrogate) model for each smaller image portion, and apply the\nattack separately for each image portion using the prediction as the label for the adversarial objective\nfunction (Algorithm 2). For example, in the case of DETR, we use an image size of 896 x 896, split\n17\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nTransformers ConvolutionalSource (↓) Attack\nT2T-24 T2T-7 Res50\nDeit-T\nMIM 26.3 / 45.2 38.1 / 64.6 35.8 / 54.6\nMIME 27.5 / 47.9 39.4 / 65.9 37.8 / 56.9\nMIMRE 28.0 / 48.6 42.2 / 69.5 47.2 / 64.9\nDeit-S\nMIM 24.1 / 43.5 37.0 / 63.3 35.3 / 53.2\nMIME 25.4 / 45.6 38.3 / 65.5 38.6 / 53.8\nMIMRE 26.7 / 47.1 43.0 / 69.6 51.8 / 69.5\nDeit-B\nMIM 23.7 / 43.4 37.8 / 63.4 39.2 / 56.3\nMIME 25.6 / 45.2 39.0 / 65.0 40.3 / 60.0\nMIMRE 28.7 / 49.2 43.7 / 69.3 56.3 / 71.2\nMIM with Input Diversity (DIM) (Xie et al., 2019)\nDeit-T\nDIM 24.2 / 44.4 36.5 / 62.6 45.8 / 62.5\nDIME 26.0 / 44.7 37.9 / 62.9 47.2 / 62.8\nDIMRE 27.3 / 46.9 40.1 / 67.1 54.7 / 70.0\nDeit-S\nDIM 23.1 / 42.0 35.7 / 61.5 47.2 / 64.1\nDIME 24.0 / 42.8 36.1 / 62.2 48.1 / 65.3\nDIMRE 27.1 / 47.0 41.5 / 68.7 56.3 / 78.6\nDeit-B\nDIM 23.4 / 42.9 36.3 / 61.5 46.3 / 64.3\nDIME 24.4 / 44.5 37.1 / 63.4 46.4 / 66.0\nDIMRE 30.0 / 50.2 44.3 / 69.4 58.4 / 78.8\nTable 11: Self-ensemble with reﬁned tokens for Flowers: MIM and DIM fool rate (%) on Flowers test set. In\neach table cell, performances are shown for two perturbation budgets i.e., ϵ≤8/16.\nSource (↓) PGD PGDE PGDRE MIM MIME MIMRE DIM DIME DIMRE\nDeit-T 17.1 18.3 23.8 36.0 37.9 40.1 52.0 54.4 56.7\nDeit-S 26.7 28.2 36.2 47.1 49.9 56.0 61.5 69.6 71.8\nDeit-B 29.3 35.7 46.8 49.4 57.7 66.7 60.3 74.9 77.5\nTable 12: Swin Transformer (patch-4, window-7): Fool rate (%) on 5k ImageNet val. at perturbation budget,\nϵ≤16. Our method increases the black-box strength of adversarial attacks against Swin Transformer.\nit into 16 portions of size 224x224, and use these to individually generate adversarial signals which\nwe later combine to build a joint 896x896 sized adversary.\nAlgorithm 1 Cross-Task Attack\n0 for samples, _ in (dataloader):\n1 orig_shape = samples.shape\n2\n3 # run attack\n4 with torch.no_grad():\n5 clean_out = src_model(samples)\n6 label = clean_out.argmax(dim=-1).detach()\n7\n8 adv = generate_adversary(src_model, samples, label)\n9 # done running attack\n10\n11 samples = adv\n12 # continue model evaluation\nWe run a generic attack on cross-\ntasks where a single image level\nlabel is not available. The source\nmodel is used to generate a label\n(its original prediction) which is\nused as the target for the white-\nbox attack. The generated adver-\nsarial signal is then applied onto\nthe target task.\n18\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nConvolutional TransformersSource (↓) Attack\nRes152 WRN DN201 T2T-24 T2T-7 TnT ViT-S\nDeit-T\nMIM 48.2 52.2 55.7 53.3 75.3 73.0 93.1\nMIME 49.9 53.6 58.5 52.8 76.6 73.5 96.5\nMIMRE 57.8(+9.8) 62.0(+9.8) 65.2(+9.5) 55.4(+2.1) 84.7(+) 78.8(+9.4) 99.5(+6.4)\nDeit-S\nMIM 48.3 46.3 56.4 60.5 70.3 80.2 92.5\nMIME 53.1 50.7 60.2 63.3 78.0 85.6 97.1\nMIMRE 66.8(+18.5) 69.0(+22.7) 75.0(+18.6) 69.1(+8.6) 92.3(+22) 96.1(+15.9) 99.5(+7)\nDeit-B\nMIM 41.6 45.0 55.9 56.5 60.0 68.4 70.2\nMIME 58.3 60.3 65.8 64.9 76.3 85.6 83.0\nMIMRE 75.9(+34.3) 80.2(+35.2) 84.8(+28.9) 72.3(+15.8) 90.1(+30.1) 97.1(+28.7) 96.5(+26.3)\nMIM with Input Diversity (DIM) (Xie et al., 2019)\nDeit-T\nDIM 66.4 67.0 72.0 70.1 82.8 80.5 90.2\nDIME 70.2 70.9 77.0 70.2 87.2 83.5 95.3\nDIMRE 72.8(+6.4) 74.8(+7.8) 79.7(+7.7) 73.2(+3.1) 89.0(+6.2) 85.4(+4.9) 97.8(+7.6)\nDeit-S\nDIM 60.3 60.2 65.0 74.2 67.0 81.5 80.1\nDIME 77.6 76.8 81.3 88.3 89.1 92.6 95.6\nDIMRE 82.0(+21.7) 83.1(+22.9) 87.2(+22.3) 89.9(+15.7) 89.1(+) 96.4(+22.1) 98.3(+18.2)\nDeit-B\nDIM 58.4 59.2 65.3 70.5 65.3 75.0 77.2\nDIME 79.6 82.7 84.8 87.3 88.3 90.4 95.7\nDIMRE 87.0(+28.6) 85.6(+26.4) 91.2(+25.9) 87.0(+16.5) 91.7(+26.4) 93.0(+18) 96.1(+18.9)\nTable 13: Fool rate (%) on the whole ImageNet val. (50k) adversarial samples at ϵ≤16. Our method remains\neffective and signiﬁcantly increase adversarial transferability on large dataset as well.\nAlgorithm 2 Attack for different input sizes\n0 from itertools import product\n1\n2 for samples, targets in (dataloader):\n3 orig_shape = samples.shape\n4\n5 # run attack\n6 product_list = product([0, 1, 2, 3], [0, 1, 2, 3])\n7 temp = torch.cat([\n8 samples[:,:,224*x:224*(1+x),224*y:224*(1+y)]\n9 for x, y in product_list], dim=0)\n10\n11 with torch.no_grad():\n12 clean_out = src_model(temp)\n13 label = clean_out.argmax(dim=-1).detach()\n14\n15 adv = generate_adversary(src_model, temp, label)\n16 temp = torch.zeros_like(samples)\n17 for idx, (x, y) in enumerate(product_list):\n18 temp[:,:,224*x:224*(1+x),224*y:224*(1+y)] =\n19 adv[orig_shape[0]*idx:orig_shape[0]*(idx+1)]\n20 adv = temp\n21 assert orig_shape == adv.shape\n22 # done running attack\n23 samples = adv\n24 # continue model evaluation\nWe run a modiﬁed version of\nthe attack for cross-task exper-\niments requiring speciﬁc input\nsizes not compatible with the\nViT models’ input size. We split\nlarger images into smaller im-\nage portions, generate adversar-\nial signals separately for each\nportion, and combine these to\nobtain a joint adversarial signal\nrelevant to the entire image. For\neach image portion, we use the\nsource model to generate a la-\nbel (its prediction for that image\nportion), and use it as the target\nwhen generating the adversarial\nsignal.\nH C OMPUTATION COST AND INFERENCE SPEED ANALYSIS\nIn this section, we analyze the parameter complexity and computational cost of our proposed approach.\nThe basic version of our self-ensemble (AttackE) only uses an off the shelf pretrained ViT without\nadding any additional parameters. However, we introduce additional learnable parameters with token\nreﬁnement module to solve misalignment problem between the tokens produced by an intermediate\n19\nFigure 8: Our proposed reﬁnement module (Sec. 3.2)\nprocesses patch tokens using a convolutional block\n(He et al., 2016) while the class token is processed by\na linear layer. Class and patch tokens are the outputs of\nan intermediate ViT block. The convolutional layers\nhave a ﬁlter size of 3x3xd. We rearrange the number\nof patch tokens into 14x14 grid before feeding them\nto convolutional block. The embedding dimension\n(d) of the class token and each patch token dictates\nthe number of parameters and inference compute cost\nwithin convolutional and MLP layers of the reﬁnement\nmodule.\nAttack Inference Speed Analysis\nModel Self-ensemble Reﬁned Tokens Attacks\nFGSM PGD MIM DIM\nDeit-T \u0017 \u0017 0.19 1.48 1.49 1.51\n\u0013 \u0017 0.19 1.59 1.69 1.6\n\u0013 \u0013 0.23 2.12 2.13 2.15\nDeit-S \u0017 \u0017 0.34 3.22 3.21 3.23\n\u0013 \u0017 0.36 3.34 3.32 3.35\n\u0013 \u0013 0.54 5.19 5.22 5.24\nDeit-B \u0017 \u0017 0.97 9.45 9.32 9.27\n\u0013 \u0017 1.0 9.66 9.43 9.44\n\u0013 \u0013 1.64 16.15 15.88 15.9\nTable 14: We compare inference\nspeed (in minutes) of attacks on\nthe conventional model against\nthe attack on our proposed self-\nensemble (with and without reﬁne-\nment module). We used 5k se-\nlected samples (Sec. 4) from Im-\nageNet validation set for this ex-\nperiment and all iterative attacks\n(PGD, MIM, DIM) ran for 10 it-\nerations. Inference speed is com-\nputed using Nvidia Quadro RTX\n6000 with Pytorch library.\nblock and the ﬁnal classiﬁer of a vision transformer (Sec. 3.2). Our reﬁnement module processes\nclass and patch tokens using MLP (linear layer) and Convolutional block (Fig. 8) and its parameter\ncomplexity is dependent on the embedding dimension of these pretrained tokens. For example,\nDeiT-S produces tokens with embedding dimension of size R384 and our reﬁnement module adds\n1.47 million parameters to its sub-model within the self-ensemble trained with reﬁned tokens.\nFine tuning with additional parameters provides notable gains in recognition accuracy. Our approach\nincreases top-1 (%) accuracy (averaged across self-ensemble) by 12.43, 15.21, and 16.70 for Deit-T,\nDeit-S, and DeiT-B, respectively. Similar trend holds for the convolutional vision transformer (Wu\net al., 2021) and MLP-Mixer (Tolstikhin et al., 2021) as well (Fig. 10).\nFurther, we analyze inference speed for different attacks in Table 14. The computational cost\ndifference between the attack on a conventional model and the attack on our self-ensemble (without\nreﬁnement module) is only marginal. As expected, attacking self-ensemble with reﬁnement module\nis slightly more expensive, however, the notable difference is only with very large models such as\nDeiT-B. In fact, the increase in computational cost is a function of the original complexity of the\npre-trained ViT model e.g., DeiT-B with an original parametric complexity of 86 million generates\nhigh-dimensional tokens R784, which leads to higher compute cost in our reﬁnement module.\nH.1 L ATENT SPACE OF REFINED TOKENS\nWe visualize the latent space of our reﬁned tokens in Fig. 9. We randomly selected 10 classes from\nImageNet validation set distributed across the entire dataset. We extracted class tokens with and\nwithout reﬁnement from the intermediate blocks (5,6,7,8) of Deit-T, Deit-S, and Deit-B. Our reﬁned\ntokens have lower intra-class variations i.e., feature representations of samples from the same class\nare clustered together. Furthermore, the reﬁned tokens have better inter-class separation than the\noriginal tokens. This indicates that reﬁnement minimizes the misalignment between the ﬁnal classiﬁer\nand intermediate class tokens, which leads to more disentangled representations. Attacking such\ndisentangled representations across the self-ensemble allows us to ﬁnd better adversarial direction\nthat leads to more powerful attacks.\n20\nClass Tokens from intermediate blocks of DeiT-T are projected fromR192 to R2.\nClass Tokens from intermediate blocks of DeiT-S are projected fromR384 to R2.\nClass Tokens from intermediate blocks of DeiT-B are projected fromR784 to R2.\nFigure 9: Class Tokens t-SNE visualization: We extracted class tokens from the intermediate blocks (used for\ncreating individual models within self-ensemble) and visualize these in 2D space via t-SNE (Van der Maaten &\nHinton, 2008). Our reﬁned tokens have lower intraclass variations i.e., feature representations of the same class\nsamples are clustered together. Further reﬁned tokens have better interclass separation than original tokens. We\nused sklearn (Pedregosa et al., 2011) and perplexity is set to 30 for all the experiments. (best viewed in zoom)\n21\nFigure 10: Self-Ensemble for CvT (Wu et al., 2021): We measure the top-1 (%) accuracy on ImageNet\nvalidation set using the class-token and average of patch tokens of each block for CvT and MLP-Mixer and\ncompare to our reﬁned tokens. These results show that ﬁne-tuning helps align tokens from intermediate blocks\nwith the ﬁnal classiﬁer enhancing their classiﬁcation performance. An interesting observation is that pretrained\nMLP-Mixer has lower ﬁnal accuracy than CvT models, however, its early blocks show more discriminability\nthan CvT. This allows a powerful self-ensemble which in turn boost adversarial transferability (Table 15).\nI S ELF -ENSEMBLE FOR HYBRID VIT AND MLP-M IXER\nIn this section, we apply our proposed approach to a diverse set of ViT models including a hybrid\nvision transformer (convolutional vision transformer (CvT) (Wu et al., 2021)) as well as a MLP-Mixer\n(Tolstikhin et al., 2021). CvT design incorporates convolutional properties into the ViT architecture.\nWe apply our approach on two CvT models, CvT-Tiny (CvT-T) and CvT-Small (CvT-S). We create\n10 and 16 models within self-ensemble of CvT-T and CvT-S, respectively. On the other hand,\nMLP-Mixer thrives on a simplistic architecture design. It does not have self-attention layers or class\ntoken. We use an average of patch tokens as class token in this case and create 12 models within\nself-ensemble of MLP-Mixer-Base (Mixer-B). We train reﬁnement module for each of these models\nusing the same setup as described in Sec. 3.2. The reﬁned class tokens show clear improvements\nin top-1 (%) accuracy on ImageNet validation set (Fig. 10). Similarly attacking self-ensemble with\nreﬁned tokens has non-trivial boost in attack transferability (Table 15). The successful application of\nour approach to such diverse ViT designs highlights the generality of our method.\n22\nFast Gradient Sign Method (FGSM) (Goodfellow et al., 2014)\nConvolutional TransformersSource (↓) Attack\nBiT50 Res152 WRN DN201 ViT-L T2T-24 TnT ViT-S T2T-7\nCvT-T\nFGSM 23.82 28.34 29.66 30.84 16.74 26.06 28.96 30.28 41.74\nFGSME 27.66 34.42 35.58 38.66 18.26 30.98 35.38 38.42 51.46\nFGSMRE 30.24(+6.4) 37.74(+9.4) 39.44(+9.8) 42.52(+11.7) 18.30(+1.6) 28.52(+2.5) 35.00(+6.0) 39.30(+9.0) 59.50(+17.8)\nCvT-S\nFGSM 19.50 24.04 26.56 27.42 14.6 21.48 22.58 25.80 36.82\nFGSME 30.0 37.30 39.08 42.04 19.48 32.38 34.56 39.24 52.92\nFGSMRE 33.10(+13.6) 40.28(+16.2) 41.84(+15.3) 45.66(+18.2) 19.44(+4.8) 32.12(+10.6) 35.98(+13.4) 41.26(+15.5) 59.38(+22.6)\nMixer-B\nFGSM 25.00 31.02 33.00 34.54 29.28 29.24 35.08 52.44 40.86\nFGSME 33.08 39.06 41.62 46.30 36.76 32.64 45.66 73.48 58.14\nFGSMRE 35.21(+10.2) 45.26(+14.2) 45.12(+12.1) 48.65(+14.11)38.54(+9.3) 36.70(+7.3) 47.22(+12.1) 80.44(+28.0) 63.58(+22.7)\nProjected Gradient Decent (PGD) (Madry et al., 2018)\nCvT-T\nPGD 20.40 23.82 25.98 25.76 8.16 27.34 31.30 20.74 46.16\nPGDE 21.36 26.06 28.86 28.18 8.78 28.66 33.38 23.40 50.14\nPGDRE 23.20(+2.8) 28.44(+4.6) 29.56(+3.8) 31.28(+5.5) 8.80(+0.6) 26.92(-0.4) 31.32(+0.02) 22.14(+1.4) 57.70(+11.5)\nCvT-S\nPGD 19.80 22.98 25.14 24.28 7.92 24.92 25.24 18.40 40.72\nPGDE 25.58 30.28 32.76 34.32 9.18 32.34 34.52 24.08 55.90\nPGDRE 27.12(+7.3) 30.56(+7.6) 32.28(+7.1) 33.78(+9.5) 8.66(+0.7) 30.22(+5.3) 33.76(+8.5) 23.00(+4.6) 58.68(+17.9)\nMixer-B\nPGD 12.92 16.70 17.96 19.20 14.68 16.98 23.90 42.32 29.46\nPGDE 24.74 32.36 35.48 37.78 28.22 31.42 49.50 76.54 58.00\nPGDRE 26.50(+13.6) 34.28(+17.6) 39.50(+21.5) 38.60(+19.4) 32.56(+17.9) 33.30(+16.32)54.68(+30.8) 82.90(+40.6) 62.86(+33.4)\nMomemtum Iterative Fast Gradient Sign Method (MIM) (Dong et al., 2018)\nCvT-T\nMIM 39.30 42.68 45.94 48.88 20.38 48.74 53.50 45.46 65.94\nMIME 42.24 45.66 49.88 54.02 21.46 50.28 56.26 49.46 71.60\nMIMRE 48.92(+9.6) 52.00(+9.3) 55.18(+9.24) 60.22(+11.3) 20.24(-0.1) 50.54(+1.8) 58.74(+5.2) 50.94(+5.5) 81.36(+15.4)\nCvT-S\nMIM 36.60 39.94 42.64 44.8 18.48 45.06 44.92 39.12 58.64\nMIME 47.04 50.72 55.02 59.56 23.06 56.06 57.84 51.20 76.54\nMIMRE 53.26(+16.7) 51.04(+11.6) 55.68(+13.0) 60.22(+15.4) 22.24(+3.76) 55.58(+10.5) 56.80(+12.0) 52.60(+13.5) 79.26(+20.6)\nMixer-B\nMIM 26.96 32.66 35.28 38.42 33.96 34.68 45.08 68.16 46.88\nMIME 42.62 49.82 52.98 59.32 51.72 49.90 68.86 92.58 75.48\nMIMRE 46.50(+19.5) 55.30(+22.6) 56.20(+21.5) 62.56(+24.1) 53.44(+19.5) 52.00(+17.32)72.26(+27.2) 94.66(+26.5) 80.12(+33.2)\nMIM with Input Diversity (DIM) (Xie et al., 2019)\nCvT-T\nDIM 61.94 61.60 64.22 67.72 36.94 69.80 76.20 65.70 73.68\nDIME 72.22 72.94 75.88 80.80 41.54 79.24 86.12 76.02 85.62\nDIMRE 78.02(+16.1) 77.20(+15.6) 80.90(+16.7) 85.74(+18.0) 40.06(+3.12) 78.62(+8.82) 89.34(+13.1) 77.88(+12.2) 92.42(+18.7)\nCvT-S\nDIM 51.64 55.28 54.16 57.22 30.98 60.1 62.36 53.30 62.46\nDIME 74.96 75.88 79.56 83.5 44.06 80.94 86.40 76.86 88.06\nDIMRE 80.45(+28.8) 78.94(+23.7) 81.64(+27.5) 85.74(+28.5) 42.78(+11.8) 82.36(+22.3) 88.50(+26.1) 77.66(+24.3) 92.10(+29.6)\nMixer-B\nDIM 48.64 49.08 52.14 57.44 39.28 57.24 64.00 71.88 63.02\nDIME 69.32 72.46 74.98 80.38 55.96 74.66 84.66 91.86 88.78\nDIMRE 74.88(+26.2) 76.72(+27.6) 80.10(+28.0) 84.66(+27.2) 60.43(+21.2) 82.17(+24.9) 88.62(+24.6) 95.22(+23.3) 92.92(+29.9)\nTable 15: Fool rate (%) on 5k ImageNet val. adversarial samples at ϵ≤16. Perturbations generated from our\nproposed self-ensemble with reﬁned tokens from a vision transformer have signiﬁcantly higher success rate.\nJ C AN SELF -ENSEMBLE IMPROVE TRANSFERABILITY FROM CNN?\nOur proposed idea of the self-ensemble can be applied to a CNN model (e.g., ResNet50) with\nan average pooling operation over the intermediate layer outputs. However, due to the varying\nchannel dimension of feature maps across a pretrained CNN , building an ensemble with a shared\nclassiﬁer is not as straight-forward as in ViT (where equidimensional class token is available at\neach level). For example, ResNet50 has four intermediate blocks that produce feature maps of\nsizes R256×56×56, R512×28×28, R1024×14×14, and R2048×7×7, respectively. Then, the ﬁnal classiﬁer\nprocesses the average pooled features from the last block. Since there is a mismatch between feature\ndimensions of the intermediate layers and the ﬁnal classiﬁer, therefore applying the basic version of\nour self-ensemble approach (AttackE) is not possible for the pretrained ResNet.\nAn intermediate layer is essential to project intermediate feature vectors to the same dimension\nas the ﬁnal feature vector in the case of CNNs. Therefore, for the reﬁned embedding variant of\n23\nFigure 11: Self-Ensemble for ResNet50 (He et al., 2016): We report relative improvement after adding\nreﬁnement module for Resnet50 and Deit-S mdoels. Note that the basic version of self-ensemble can not applied\nto ResNet50 due to varying channel dimension across different layers. Feature reﬁnement improves adversarial\ntransfer from ResNet50, however relative gains are signiﬁcant for vision transformer, Deit-S.\nour approach, we ﬁnetune ResNet50 features to create self-ensemble with the procedure described\nin Sec. 3.2. We report the relative improvements of self-ensemble with reﬁned features w.r.t the\noriginal (single) model and compare ResNet50 with Deit-S (Fig. 11). Both these off-the-shelf models,\nResNet50 (25 million parameters) and Deit-S (22 million parameters), are comparable in terms of\ntheir computational complexity. We observe that feature reﬁnement also helps to boost adversarial\ntransferability from the ResNet50 model. However, adversarial transferability improvement of our\nself-ensemble with token reﬁnement on ViTs is considerably better than for the case of ResNet50.\nThese results suggest that the proposed approach is well-suited for improving adversarial transferabil-\nity of ViT models.\n24",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.791364312171936
    },
    {
      "name": "Discriminative model",
      "score": 0.734373927116394
    },
    {
      "name": "Transferability",
      "score": 0.6953812837600708
    },
    {
      "name": "Security token",
      "score": 0.6532129049301147
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.608352780342102
    },
    {
      "name": "Adversarial system",
      "score": 0.5741387605667114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5690997838973999
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5157468914985657
    },
    {
      "name": "Machine learning",
      "score": 0.44353121519088745
    },
    {
      "name": "Deep learning",
      "score": 0.4379939138889313
    },
    {
      "name": "Benchmarking",
      "score": 0.4331067204475403
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.43191784620285034
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3507644534111023
    },
    {
      "name": "Computer security",
      "score": 0.11855050921440125
    },
    {
      "name": "Mathematics",
      "score": 0.07907074689865112
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ]
}