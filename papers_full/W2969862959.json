{
    "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
    "url": "https://openalex.org/W2969862959",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2148392122",
            "name": "Tan Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202139477",
            "name": "Bansal, Mohit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2979949198",
        "https://openalex.org/W2931316642",
        "https://openalex.org/W2968543045",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2463565445",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2884093133",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2962749469",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2968388725",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2747623286",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W2950761309",
        "https://openalex.org/W2950178297",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2963921132",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3038035611",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W2963521239",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1847088711",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2963224792",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2938082352",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W2966683369",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2965628639",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2613718673"
    ],
    "abstract": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
    "full_text": "LXMERT: Learning Cross-Modality Encoder Representations\nfrom Transformers\nHao Tan Mohit Bansal\nUNC Chapel Hill\n{haotan, mbansal}@cs.unc.edu\nAbstract\nVision-and-language reasoning requires an un-\nderstanding of visual concepts, language se-\nmantics, and, most importantly, the align-\nment and relationships between these two\nmodalities. We thus propose the LXMERT\n(Learning Cross-Modality Encoder Represen-\ntations from Transformers) framework to learn\nthese vision-and-language connections. In\nLXMERT, we build a large-scale Transformer\nmodel that consists of three encoders: an ob-\nject relationship encoder, a language encoder,\nand a cross-modality encoder. Next, to en-\ndow our model with the capability of con-\nnecting vision and language semantics, we\npre-train the model with large amounts of\nimage-and-sentence pairs, via ﬁve diverse rep-\nresentative pre-training tasks: masked lan-\nguage modeling, masked object prediction\n(feature regression and label classiﬁcation),\ncross-modality matching, and image ques-\ntion answering. These tasks help in learn-\ning both intra-modality and cross-modality re-\nlationships. After ﬁne-tuning from our pre-\ntrained parameters, our model achieves the\nstate-of-the-art results on two visual ques-\ntion answering datasets (i.e., VQA and GQA).\nWe also show the generalizability of our pre-\ntrained cross-modality model by adapting it to\na challenging visual-reasoning task, NLVR 2,\nand improve the previous best result by 22%\nabsolute ( 54% to 76%). Lastly, we demon-\nstrate detailed ablation studies to prove that\nboth our novel model components and pre-\ntraining strategies signiﬁcantly contribute to\nour strong results; and also present several\nattention visualizations for the different en-\ncoders.1\n1 Introduction\nVision-and-language reasoning requires the un-\nderstanding of visual contents, language seman-\n1Published at EMNLP 2019. Code and pre-trained mod-\nels publicly available at: https://github.com/airsplay/lxmert\ntics, and cross-modal alignments and relation-\nships. There has been substantial past works in\nseparately developing backbone models with bet-\nter representations for the single modalities of vi-\nsion and of language. For visual-content under-\nstanding, people have developed several backbone\nmodels (Simonyan and Zisserman, 2014; Szegedy\net al., 2015; He et al., 2016) and shown their ef-\nfectiveness on large vision datasets (Deng et al.,\n2009; Lin et al., 2014; Krishna et al., 2017). Pi-\noneering works (Girshick et al., 2014; Xu et al.,\n2015) also show the generalizability of these pre-\ntrained (especially on ImageNet) backbone mod-\nels by ﬁne-tuning them on different tasks. In terms\nof language understanding, last year, we witnessed\nstrong progress towards building a universal back-\nbone model with large-scale contextualized lan-\nguage model pre-training (Peters et al., 2018; Rad-\nford et al., 2018; Devlin et al., 2019), which\nhas improved performances on various tasks (Ra-\njpurkar et al., 2016; Wang et al., 2018) to sig-\nniﬁcant levels. Despite these inﬂuential single-\nmodality works, large-scale pretraining and ﬁne-\ntuning studies for the modality-pair of vision and\nlanguage are still under-developed.\nTherefore, we present one of the ﬁrst works in\nbuilding a pre-trained vision-and-language cross-\nmodality framework and show its strong perfor-\nmance on several datasets. We name this frame-\nwork “LXMERT: Learning Cross-Modality En-\ncoder Representations from Transformers” (pro-\nnounced: ‘leksmert’). This framework is mod-\neled after recent BERT-style innovations while\nfurther adapted to useful cross-modality scenar-\nios. Our new cross-modality model focuses on\nlearning vision-and-language interactions, espe-\ncially for representations of a single image and its\ndescriptive sentence. It consists of three Trans-\nformer (Vaswani et al., 2017) encoders: an object\nrelationship encoder, a language encoder, and a\narXiv:1908.07490v3  [cs.CL]  3 Dec 2019\ncross-modality encoder. In order to better learn\nthe cross-modal alignments between vision and\nlanguage, we next pre-train our model with ﬁve\ndiverse representative tasks: (1) masked cross-\nmodality language modeling, (2) masked object\nprediction via RoI-feature regression, (3) masked\nobject prediction via detected-label classiﬁcation,\n(4) cross-modality matching, and (5) image ques-\ntion answering. Different from single-modality\npre-training (e.g., masked LM in BERT), this\nmulti-modality pre-training allows our model to\ninfer masked features either from the visible ele-\nments in the same modality, or from aligned com-\nponents in the other modality. In this way, it helps\nbuild both intra-modality and cross-modality rela-\ntionships.\nEmpirically, we ﬁrst evaluate LXMERT on\ntwo popular visual question-answering datasets,\nVQA (Antol et al., 2015) and GQA (Hudson and\nManning, 2019). Our model outperforms previ-\nous works in all question categories (e.g., Binary,\nNumber, Open) and achieves state-of-the-art re-\nsults in terms of overall accuracy. Further, to show\nthe generalizability of our pre-trained model, we\nﬁne-tune LXMERT on a challenging visual rea-\nsoning task, Natural Language for Visual Reason-\ning for Real (NLVR 2) (Suhr et al., 2019), where\nwe do not use the natural images in their dataset\nfor our pre-training, but ﬁne-tune and evaluate\non these challenging, real-world images. In this\nsetup, we achieve a large improvement of22% ab-\nsolute in accuracy ( 54% to 76%, i.e., 48% rela-\ntive error reduction) and 30% absolute in consis-\ntency ( 12% to 42%, i.e., 34% relative error re-\nduction). Lastly, we conduct several analysis and\nablation studies to prove the effectiveness of our\nmodel components and diverse pre-training tasks\nby removing them or comparing them with their\nalternative options. Especially, we use several\nways to take the existing BERT model and its vari-\nants, and show their ineffectiveness in vision-and-\nlanguage tasks, which overall proves the need of\nour new cross-modality pre-training framework.\nWe also present several attention visualizations\nfor the different language, object-relationship, and\ncross-modality encoders.\n2 Model Architecture\nWe build our cross-modality model with self-\nattention and cross-attention layers following the\nrecent progress in designing natural language pro-\ncessing models (e.g., transformers (Vaswani et al.,\n2017)). As shown in Fig. 1, our model takes two\ninputs: an image and its related sentence (e.g., a\ncaption or a question). Each image is represented\nas a sequence of objects, and each sentence is rep-\nresented as a sequence of words. Via careful de-\nsign and combination of these self-attention and\ncross-attention layers, our model is able to gen-\nerate language representations, image representa-\ntions, and cross-modality representations from the\ninputs. Next, we describe the components of this\nmodel in detail.\n2.1 Input Embeddings\nThe input embedding layers in LXMERT con-\nvert the inputs (i.e., an image and a sentence)\ninto two sequences of features: word-level sen-\ntence embeddings and object-level image embed-\ndings. These embedding features will be further\nprocessed by the latter encoding layers.\nWord-Level Sentence Embeddings A sentence\nis ﬁrst split into words {w1,...,w n}with length\nof nby the same WordPiece tokenizer (Wu et al.,\n2016) in Devlin et al. (2019). Next, as shown in\nFig. 1, the word wi and its index i(wi’s absolute\nposition in the sentence) are projected to vectors\nby embedding sub-layers, and then added to the\nindex-aware word embeddings:\nˆwi = WordEmbed (wi)\nˆui = IdxEmbed (i)\nhi = LayerNorm ( ˆwi + ˆui)\nObject-Level Image Embeddings Instead of\nusing the feature map output by a convolutional\nneural network, we follow Anderson et al. (2018)\nin taking the features of detected objects as the em-\nbeddings of images. Speciﬁcally, the object detec-\ntor detects m objects {o1,...,o m}from the im-\nage (denoted by bounding boxes on the image in\nFig. 1). Each object oj is represented by its po-\nsition feature (i.e., bounding box coordinates) pj\nand its 2048-dimensional region-of-interest (RoI)\nfeature fj. Instead of directly using the RoI feature\nfj without considering its position pj in Anderson\net al. (2018), we learn a position-aware embedding\nvj by adding outputs of 2 fully-connected layers:\nˆfj = LayerNorm (WFfj + bF)\nˆpj = LayerNorm (WPpj + bP)\nvj =\n(\nˆfj + ˆpj\n)\n/2 (1)\n+\nRoI Feat \nPos Feat \nA woman \nriding a bike \nwith a dog in a \nbasket. \n+\nWord Emb \nIdx Emb \nSelf FF + + FF +\nSelf FF + + FF +\nSelf +\nSelf +\nCross +\nCross +\nVision \nOutput \nLanguage \nOutput \nCross- \nModality \nOutput \nCross-Modality Encoder Language Encoder \nObject-Relationship Encoder \nN R ⇥\nN L ⇥\nN X ⇥\nFigure 1: The LXMERT model for learning vision-and-language cross-modality representations. ‘Self’ and\n‘Cross’ are abbreviations for self-attention sub-layers and cross-attention sub-layers, respectively. ‘FF’ denotes\na feed-forward sub-layer.\nIn addition to providing spatial information in vi-\nsual reasoning, the inclusion of positional infor-\nmation is necessary for our masked object pre-\ndiction pre-training task (described in Sec. 3.1.2).\nSince the image embedding layer and the follow-\ning attention layers are agnostic to the absolute in-\ndices of their inputs, the order of the object is not\nspeciﬁed. Lastly, in Equation 1, the layer normal-\nization is applied to the projected features before\nsummation so as to balance the energy of the two\ndifferent types of features.\n2.2 Encoders\nWe build our encoders, i.e., the language encoder,\nthe object-relationship encoder, and the cross-\nmodality encoder, mostly on the basis of two kinds\nof attention layers: self-attention layers and cross-\nattention layers. We ﬁrst review the deﬁnition and\nnotations of attention layers and then discuss how\nthey form our encoders.\nBackground: Attention Layers Attention lay-\ners (Bahdanau et al., 2014; Xu et al., 2015) aim to\nretrieve information from a set of context vectors\n{yj}related to aquery vector x. An attention layer\nﬁrst calculates the matching score aj between the\nquery vector xand each context vector yj. Scores\nare then normalized by softmax:\naj = score(x,yj)\nαj = exp(aj)/\n∑\nk\nexp(ak)\nThe output of an attention layer is the weighted\nsum of the context vectors w.r.t. the softmax-\nnormalized score: AttX→Y (x,{yj}) = ∑\nj αjyj.\nAn attention layer is calledself-attention when the\nquery vector xis in the set ofcontext vectors {yj}.\nSpeciﬁcally, we use the multi-head attention fol-\nlowing Transformer (Vaswani et al., 2017).\nSingle-Modality Encoders After the embed-\nding layers, we ﬁrst apply two transformer en-\ncoders (Vaswani et al., 2017), i.e., a language en-\ncoder and an object-relationship encoder , and\neach of them only focuses on a single modal-\nity (i.e., language or vision). Different from\nBERT (Devlin et al., 2019), which applies the\ntransformer encoder only to language inputs, we\napply it to vision inputs as well (and to cross-\nmodality inputs as described later below). Each\nlayer (left dashed blocks in Fig. 1) in a single-\nmodality encoder contains a self-attention (‘Self’)\nsub-layer and a feed-forward (‘FF’) sub-layer,\nwhere the feed-forward sub-layer is further com-\nposed of two fully-connected sub-layers. We take\nNL and NR layers in the language encoder and the\nobject-relationship encoder, respectively. We add\na residual connection and layer normalization (an-\nnotated by the ‘+’ sign in Fig. 1) after each sub-\nlayer as in Vaswani et al. (2017).\nCross-Modality Encoder Each cross-modality\nlayer (the right dashed block in Fig. 1) in the cross-\nmodality encoder consists of two self-attention\nsub-layers, one bi-directional cross-attention sub-\nlayer, and two feed-forward sub-layers. We stack\n(i.e., using the output of k-th layer as the input\nof (k+1)-th layer) NX these cross-modality lay-\ners in our encoder implementation. Inside the k-th\nlayer, the bi-directional cross-attention sub-layer\n(‘Cross’) is ﬁrst applied, which contains two uni-\ndirectional cross-attention sub-layers: one from\nlanguage to vision and one from vision to lan-\nguage. The query and context vectors are the out-\nputs of the (k-1)-th layer (i.e., language features\n+\nRoI Feat RoI-Feature \nRegression \nPos Feat \n \nWho is eating \nthe carrot? +\nWord Emb \nIdx Emb \nMask \nFeat \n [CLS] who \n[MASK] eat -ing \nthe [MASK] ? \n[EOS] \nDetected-Label \nClassification \nMasked Cross- \nModality LM \nAnswer? {RABBIT}\t\n   Match?   {YES}\t\n{DOG}\t\n…\t\nObjectRel \nEncoder \nLanguage \nEncoder \nCross- \nModality \nEncoder \n [CLS] who \nis eat -ing  \nthe carrot ? \n[EOS] \nCross-Modality \nMatching & QA \nFigure 2: Pre-training in LXMERT. The object RoI features and word tokens are masked. Our ﬁve pre-training\ntasks learn the feature representations based on these masked inputs. Special tokens are in brackets and classiﬁca-\ntion labels are in braces.\n{hk−1\ni }and vision features {vk−1\nj }):\nˆhk\ni = CrossAttL→R\n(\nhk−1\ni ,{vk−1\n1 ,...,v k−1\nm }\n)\nˆvk\nj = CrossAttR→L\n(\nvk−1\nj ,{hk−1\n1 ,...,h k−1\nn }\n)\nThe cross-attention sub-layer is used to exchange\nthe information and align the entities between\nthe two modalities in order to learn joint cross-\nmodality representations. For further building in-\nternal connections, the self-attention sub-layers\n(‘Self’) are then applied to the output of the cross-\nattention sub-layer:\n˜hk\ni = SelfAttL→L\n(\nˆhk\ni ,{ˆhk\n1,..., ˆhk\nn}\n)\n˜vk\nj = SelfAttR→R\n(\nˆvk\nj ,{ˆvk\n1 ,..., ˆvk\nm}\n)\nLastly, the k-th layer output {hk\ni }and {vk\nj }are\nproduced by feed-forward sub-layers (‘FF’) on top\nof {ˆhk\ni }and {ˆvk\nj }. We also add a residual connec-\ntion and layer normalization after each sub-layer,\nsimilar to the single-modality encoders.\n2.3 Output Representations\nAs shown in the right-most part of Fig. 1, our\nLXMERT cross-modality model has three outputs\nfor language, vision, and cross-modality, respec-\ntively. The language and vision outputs are the\nfeature sequences generated by the cross-modality\nencoder. For the cross-modality output, follow-\ning the practice in Devlin et al. (2019), we ap-\npend a special token [CLS] (denoted as the top\nyellow block in the bottom branch of Fig. 1) before\nthe sentence words, and the corresponding feature\nvector of this special token in language feature se-\nquences is used as the cross-modality output.\n3 Pre-Training Strategies\nIn order to learn a better initialization which un-\nderstands connections between vision and lan-\nguage, we pre-train our model with different\nmodality pre-training tasks on a large aggregated\ndataset.\n3.1 Pre-Training Tasks\n3.1.1 Language Task: Masked\nCross-Modality LM\nOn the language side, we take the masked cross-\nmodality language model (LM) task. As shown\nin the bottom branch of Fig. 2, the task setup\nis almost same to BERT (Devlin et al., 2019):\nwords are randomly masked with a probabil-\nity of 0.15 and the model is asked to predict\nthese masked words. In addition to BERT where\nmasked words are predicted from the non-masked\nwords in the language modality, LXMERT, with\nits cross-modality model architecture, could pre-\ndict masked words from the vision modality as\nwell, so as to resolve ambiguity. For example, as\nshown in Fig. 2, it is hard to determine the masked\nword ‘carrot’ from its language context but the\nword choice is clear if the visual information is\nconsidered. Hence, it helps building connections\nfrom the vision modality to the language modality,\nand we refer to this task as masked cross-modality\nLM to emphasize this difference. We also show\nthat loading BERT parameters into LXMERT will\ndo harm to the pre-training procedure in Sec. 5.1\nsince BERT can perform relatively well in the\nlanguage modality without learning these cross-\nmodality connections.\n3.1.2 Vision Task: Masked Object Prediction\nAs shown in the top branch of Fig. 2, we pre-\ntrain the vision side by randomly masking ob-\nImage Split Images Sentences (or Questions)\nCOCO-Cap VG-Cap VQA GQA VG-QA All\nMS COCO - VG 72K 361K - 387K - - 0.75M\nMS COCO ∩VG 51K 256K 2.54M 271K 515K 724K 4.30M\nVG - MS COCO 57K - 2.85M - 556K 718K 4.13M\nAll 180K 617K 5.39M 658K 1.07M 1.44M 9.18M\nTable 1: Amount of data for pre-training. Each image has multiple sentences/questions. ‘Cap’ is caption. ‘VG’ is\nVisual Genome. Since MS COCO and VG share 51K images, we list it separately to ensure disjoint image splits.\njects (i.e., masking RoI features with zeros) with\na probability of 0.15 and asking the model to pre-\ndict proprieties of these masked objects. Similar\nto the language task (i.e., masked cross-modality\nLM), the model can infer the masked objects ei-\nther from visible objects or from the language\nmodality. Inferring the objects from the vision\nside helps learn the object relationships, and infer-\nring from the language side helps learn the cross-\nmodality alignments. Therefore, we perform two\nsub-tasks: RoI-Feature Regression regresses the\nobject RoI feature fj with L2 loss, and Detected-\nLabel Classiﬁcation learns the labels of masked\nobjects with cross-entropy loss. In the ‘Detected-\nLabel Classiﬁcation’ sub-task, although most of\nour pre-training images have object-level anno-\ntations, the ground truth labels of the annotated\nobjects are inconsistent in different datasets (e.g.,\ndifferent number of label classes). For these rea-\nsons, we take detected labels output by Faster R-\nCNN (Ren et al., 2015). Although detected labels\nare noisy, experimental results show that these la-\nbels contribute to pre-training in Sec. 5.3.\n3.1.3 Cross-Modality Tasks\nAs shown in the middle-rightmost part of Fig. 2,\nto learn a strong cross-modality representation, we\npre-train the LXMERT model with 2 tasks that ex-\nplicitly need both language and vision modalities.\nCross-Modality Matching For each sentence,\nwith a probability of 0.5, we replace it with a mis-\nmatched2 sentence. Then, we train a classiﬁer to\npredict whether an image and a sentence match\neach other. This task is similar to ‘Next Sentence\nPrediction’ in BERT (Devlin et al., 2019).\nImage Question Answering (QA) In order to\nenlarge the pre-training dataset (see details in\n2 We take a sentence from another image as the mis-\nmatched sentence. Although the sentence and the image still\nhave chance to match each other, this probability is very low.\nSec. 3.2), around 1/3 sentences in the pre-training\ndata are questions about the images. We ask\nthe model to predict the answer to these image-\nrelated questions when the image and the ques-\ntion are matched (i.e., not randomly replaced in\nthe cross-modality matching task). We show that\npre-training with this image QA leads to a better\ncross-modality representation in Sec. 5.2.\n3.2 Pre-Training Data\nAs shown in Table. 1, we aggregate pre-training\ndata from ﬁve vision-and-language datasets whose\nimages come from MS COCO (Lin et al., 2014)\nor Visual Genome (Krishna et al., 2017). Be-\nsides the two original captioning datasets, we also\naggregate three large image question answering\n(image QA) datasets: VQA v2.0 (Antol et al.,\n2015), GQA balanced version (Hudson and Man-\nning, 2019), and VG-QA (Zhu et al., 2016). We\nonly collect train and dev splits in each dataset to\navoid seeing any test data in pre-training. We con-\nduct minimal pre-processing on the ﬁve datasets to\ncreate aligned image-and-sentence pairs. For each\nimage question answering dataset, we take ques-\ntions as sentences from the image-and-sentence\ndata pairs and take answers as labels in the im-\nage QA pre-training task (described in Sec. 3.1.3).\nThis provides us with a large aligned vision-and-\nlanguage dataset of 9.18M image-and-sentence\npairs on 180K distinct images. In terms of tokens,\nthe pre-training data contain around 100M words\nand 6.5M image objects.\n3.3 Pre-Training Procedure\nWe pre-train our LXMERT model on the large ag-\ngregated dataset (discussed in Sec. 3.2) via the pre-\ntraining tasks (Sec. 3.1). The details about the data\nsplits are in the Appendix. The input sentences are\nsplit by the WordPiece tokenizer (Wu et al., 2016)\nprovided in BERT (Devlin et al., 2019). The ob-\njects are detected by Faster R-CNN (Ren et al.,\nMethod VQA GQA NLVR 2\nBinary Number Other Accu Binary Open Accu Cons Accu\nHuman - - - - 91.2 87.4 89.3 - 96.3\nImage Only - - - - 36.1 1.74 17.8 7.40 51.9\nLanguage Only 66.8 31.8 27.6 44.3 61.9 22.7 41.1 4.20 51.1\nState-of-the-Art 85.8 53.7 60.7 70.4 76.0 40.4 57.1 12.0 53.5\nLXMERT 88.2 54.2 63.1 72.5 77.8 45.0 60.3 42.1 76.2\nTable 2: Test-set results. VQA/GQA results are reported on the ‘test-standard’ splits and NLVR 2 results are\nreported on the unreleased test set (‘Test-U’). The highest method results are in bold. Our LXMERT framework\noutperforms previous (comparable) state-of-the-art methods on all three datasets w.r.t. all metrics.\n2015) which is pre-trained on Visual Genome\n(provided by Anderson et al. (2018)). We do not\nﬁne-tune the Faster R-CNN detector and freeze\nit as a feature extractor. Different from detect-\ning variable numbers of objects in Anderson et al.\n(2018), we consistently keep 36 objects for each\nimage to maximize the pre-training compute uti-\nlization by avoiding padding. For the model archi-\ntecture, we set the numbers of layers NL, NX, and\nNR to 9, 5, and 5 respectively.3 More layers are\nused in the language encoder to balance the visual\nfeatures extracted from 101-layer Faster R-CNN.\nThe hidden size768 is the same as BERTBASE. We\npre-train all parameters in encoders and embed-\nding layers from scratch (i.e., model parameters\nare randomly initialized or set to zero). We also\nshow results of loading pre-trained BERT parame-\nters in Sec. 5.1. LXMERT is pre-trained with mul-\ntiple pre-training tasks and hence multiple losses\nare involved. We add these losses with equal\nweights. For the image QA pre-training tasks, we\ncreate a joint answer table with 9500 answer can-\ndidates which roughly cover 90% questions in all\nthree image QA datasets.\nWe take Adam (Kingma and Ba, 2014) as\nthe optimizer with a linear-decayed learning-rate\nschedule (Devlin et al., 2019) and a peak learn-\ning rate at 1e −4. We train the model for 20\nepochs (i.e., roughly 670K4 optimization steps)\nwith a batch size of 256. We only pre-train with\nimage QA task (see Sec. 3.1.3) for the last 10\nepochs, because this task converges faster and em-\npirically needs a smaller learning rate. The whole\n3If we count a single modality layer as one half cross-\nmodality layer, the equivalent number of cross-modality lay-\ners is (9 + 5)/2 + 5 = 12, which is same as the number of\nlayers in BERTBASE.\n4For comparison, ResNet on ImageNet classiﬁcation\ntakes 600K steps and BERT takes 1000K steps.\npre-training process takes 10 days on 4 Titan Xp.\nFine-tuning Fine-tuning is fast and robust. We\nonly perform necessary modiﬁcation to our model\nwith respect to different tasks (details in Sec. 4.2).\nWe use a learning rate of1e−5 or 5e−5, a batch\nsize of 32, and ﬁne-tune the model from our pre-\ntrained parameters for 4 epochs.\n4 Experimental Setup and Results\nIn this section, we ﬁrst introduce the datasets that\nare used to evaluate our LXMERT framework and\nempirically compare our single-model results with\nprevious best results.\n4.1 Evaluated Datasets\nWe use three datasets for evaluating our LXMERT\nframework: VQA v2.0 dataset (Goyal et al.,\n2017), GQA (Hudson and Manning, 2019), and\nNLVR2. See details in Appendix.\n4.2 Implementation Details\nOn VQA and GQA, we ﬁne-tune our model from\nthe pre-trained snapshot without data augmenta-\ntion (analysis in Sec. 5.2). When training GQA,\nwe only take raw questions and raw images as in-\nputs and do not use other supervisions (e.g., func-\ntional programs and scene graphs). Since each da-\ntum in NLVR2 has two natural imagesimg0,img1\nand one language statement s, we use LXMERT\nto encode the two image-statement pairs(img0,s)\nand (img1,s), then train a classiﬁer based on the\nconcatenation of the two cross-modality outputs.\nMore details in Appendix.\n4.3 Empirical Comparison Results\nWe compare our single-model results with pre-\nvious best published results on VQA/GQA test-\nstandard sets and NLVR 2 public test set. Be-\nsides previous state-of-the-art (SotA) methods, we\nalso show the human performance and image-\nonly/language-only results when available.\nVQA The SotA result is BAN+Counter in Kim\net al. (2018), which achieves the best accuracy\namong other recent works: MFH (Yu et al.,\n2018), Pythia (Jiang et al., 2018), DFAF (Gao\net al., 2019a), and Cycle-Consistency (Shah et al.,\n2019).5 LXMERT improves the SotA over-\nall accuracy (‘Accu’ in Table 2) by 2.1% and\nhas 2.4% improvement on the ‘Binary’/‘Other’\nquestion sub-categories. Although LXMERT\ndoes not explicitly take a counting module as in\nBAN+Counter, our result on the counting-related\nquestions (‘Number’) is still equal or better.6\nGQA The GQA (Hudson and Manning, 2019)\nSotA result is taken from BAN (Kim et al., 2018)\non the public leaderbaord. Our 3.2% accuracy\ngain over the SotA GQA method is higher than\nVQA, possibly because GQA requires more vi-\nsual reasoning. Thus our framework, with novel\nencoders and cross-modality pre-training, is suit-\nable and achieves a 4.6% improvement on open-\ndomain questions (‘Open’ in Table 2).7\nNLVR2 NLVR2 (Suhr et al., 2019) is a chal-\nlenging visual reasoning dataset where some ex-\nisting approaches (Hu et al., 2017; Perez et al.,\n2018) fail, and the SotA method is ‘MaxEnt’ in\nSuhr et al. (2019). The failure of existing meth-\nods (and our model w/o pre-training in Sec. 5.1)\nindicates that the connection between vision and\nlanguage may not be end-to-end learned in a\ncomplex vision-and-language task without large-\nscale pre-training. However, with our novel pre-\ntraining strategies in building the cross-modality\nconnections, we signiﬁcantly improve the accu-\nracy (‘Accu’ of 76.2% on unreleased test set ‘Test-\nU’, in Table 2) by 22%. Another evaluation met-\nric consistency measures the proportion of unique\nsentences for which all related image pairs 8 are\n5 These are state-of-the-art methods at the time of our\nEMNLP May 21, 2019 submission deadline. Since then,\nthere have been some recently updated papers such as\nMCAN (Yu et al., 2019b), MUAN (Yu et al., 2019a), and\nMLI (Gao et al., 2019b). MCAN (VQA challenge ver-\nsion) uses stronger mixture of detection features and achieves\n72.8% on VQA 2.0 test-standard. MUAN achieves 71.1%\n(compared to our 72.5%).\n6Our result on VQA v2.0 ‘test-dev’ is 72.4%.\n7Our result on GQA ‘test-dev’ is 60.0%.\n8Each statement in NLVR 2 is related to multiple image\npairs in order to balance the dataset answer distribution.\nMethod VQA GQA NLVR 2\nLSTM + BUTD 63.1 50.0 52.6\nBERT + BUTD 62.8 52.1 51.9\nBERT + 1 CrossAtt 64.6 55.5 52.4\nBERT + 2 CrossAtt 65.8 56.1 50.9\nBERT + 3 CrossAtt 66.4 56.6 50.9\nBERT + 4 CrossAtt 66.4 56.0 50.9\nBERT + 5 CrossAtt 66.5 56.3 50.9\nTrain + BERT 65.5 56.2 50.9\nTrain + scratch 65.1 50.0 50.9\nPre-train + BERT 68.8 58.3 70.1\nPre-train + scratch 69.9 60.0 74.9\nTable 3: Dev-set accuracy of using BERT.\ncorrectly predicted. Our LXMERT model im-\nproves consistency (‘Cons’) to 42.1% (i.e., by3.5\ntimes).9\n5 Analysis\nIn this section, we analyze our LXMERT\nframework by comparing it with some alter-\nnative choices or by excluding certain model\ncomponents/pre-training strategies.\n5.1 BERT versus LXMERT\nBERT (Devlin et al., 2019) is a pre-trained lan-\nguage encoder which improves several language\ntasks. As shown in Table 3, we discuss sev-\neral ways to incorporate a BERT BASE pre-trained\nmodel for vision-language tasks and empirically\ncompare it with our LXMERT approach. Al-\nthough our full model achieves accuracy of74.9%\non NLVR 2, all results without LXMERT pre-\ntraining is around 22% absolute lower.\nBERT+BUTD Bottom-Up and Top-Down\n(BUTD) attention (Anderson et al., 2018) method\nencodes questions with GRU (Chung et al.,\n2015), then attends to object RoI features {fj}to\npredict the answer. We apply BERT to BUTD by\nreplacing its GRU language encoder with BERT.\nAs shown in the ﬁrst block of Table. 3, results of\nBERT encoder is comparable to LSTM encoder.\nBERT+CrossAtt Since BUTD only takes the\nraw RoI features {fj}without considering the ob-\nject positions {pj}and object relationships, we\n9These are the unreleased test set (‘Test-U’) results. On\nthe public test set (‘Test-P’), LXMERT achieves 74.5% Accu\nand 39.7% Cons.\nMethod VQA GQA NLVR 2\n1. P20 + DA 68.0 58.1 -\n2. P20 + FT 68.9 58.2 72.4\n3. P10+QA10 + DA 69.1 59.2 -\n4. P10+QA10 + FT 69.9 60.0 74.9\nTable 4: Dev-set accuracy showing the importance\nof the image-QA pre-training task. P10 means pre-\ntraining without the image-QA loss for10 epochs while\nQA10 means pre-training with the image-QA loss. DA\nand FT mean ﬁne-tuning with and without Data Aug-\nmentation, resp.\nenhance BERT+BUTD with our novel position-\naware object embedding (in Sec. 2.1) and cross-\nmodality layers (in Sec. 2.2). As shown in the\nsecond block of Table 3, the result of 1 cross-\nmodality layer is better than BUTD, while stack-\ning more cross-modality layers further improves\nit. However, without our cross-modality pre-\ntraining (BERT is language-only pre-trained), re-\nsults become stationary after adding 3 cross-\nattention layers and have a 3.4% gap to our full\nLXMERT framework (the last bold row in Ta-\nble 3).\nBERT+LXMERT We also try loading BERT\nparameters10 into LXMERT, and use it in model\ntraining (i.e., without LXMERT pre-training) or\nin pre-training. We show results in the last block\nof Table. 3. Compared to the ‘from scratch’ (i.e.,\nmodel parameters are randomly initialized) ap-\nproach, BERT improves the ﬁne-tuning results but\nit shows weaker results than our full model. Em-\npirically, pre-training LXMERT initialized with\nBERT parameters has lower (i.e., better) pre-\ntraining loss for the ﬁrst 3 pre-training epochs\nbut was then caught up by our ‘from scratch’ ap-\nproach. A possible reason is that BERT is already\npre-trained with single-modality masked language\nmodel, and thus could do well based only on the\nlanguage modality without considering the con-\nnection to the vision modality (as discussed in\nSec. 3.1.1).\n5.2 Effect of the Image QA Pre-training Task\nWe show the importance of image QA pre-training\ntask (introduced in Sec. 3.1.3) by excluding it or\n10 Since our language encoder is same as BERT BASE, ex-\ncept the number of layers (i.e., LXMERT has 9 layers and\nBERT has 12 layers), we load the top 9 BERT-layer parame-\nters into the LXMERT language encoder.\nMethod VQA GQA NLVR 2\n1. No Vision Tasks 66.3 57.1 50.9\n2. Feat 69.2 59.5 72.9\n3. Label 69.5 59.3 73.5\n4. Feat + Label 69.9 60.0 74.9\nTable 5: Dev-set accuracy of different vision pre-\ntraining tasks. ‘Feat’ is RoI-feature regression; ‘Label’\nis detected-label classiﬁcation.\ncomparing it with its alternative: data augmenta-\ntion.\nPre-training w/ or w/o Image QA To fairly\ncompare with our original pre-training procedure\n(10 epochs w/o QA + 10 epochs w/ QA, details in\nSec. 3.3) , we pre-train LXMERT model without\nimage QA task for 20 epochs. As shown in Ta-\nble 4 rows 2 and 4, pre-training with QA loss im-\nproves the result on all three datasets. The 2.1%\nimprovement on NLVR2 shows the stronger rep-\nresentations learned with image-QA pre-training,\nsince all data (images and statements) in NLVR 2\nare not used in pre-training.\nPre-training versus Data Augmentation Data\naugmentation (DA) is a technique which is used\nin several VQA implementations (Anderson et al.,\n2018; Kim et al., 2018; Jiang et al., 2018). It\nincreases the amount of training data by adding\nquestions from other image QA datasets. Our\nLXMERT framework instead uses multiple QA\ndatasets in pre-training and is ﬁne-tuned only on\none speciﬁc dataset. Since the overall amounts of\ndata used in pre-training and DA are similar, we\nthus can fairly compare these two strategies, and\nresults show that our QA pre-training approach\noutperforms DA. We ﬁrst exclude the QA task in\nour pre-training and show the results of DA ﬁne-\ntuning. As shown in Table. 4 row 1, DA ﬁne-\ntuning decreases the results compared to non-DA\nﬁne-tuning in row 2. Next, we use DA after QA-\npre-training (row 3) and DA also drops the results.\n5.3 Effect of Vision Pre-training tasks\nWe analyze the effect of different vision pre-\ntraining tasks in Table 5. Without any vision tasks\nin pre-training (i.e., only using the language and\ncross-modality pre-training tasks), the results (row\n1 of Table 5) are similar to BERT+3 CrossAtt in\nTable 3. The two visual pre-training tasks (i.e.,\nRoI-feature regression and detected-label classiﬁ-\ncation) could get reasonable results (row2 and row\n3) on their own, and jointly pre-training with these\ntwo tasks achieves the highest results (row 4).\n5.4 Visualizing LXMERT Behavior\nIn the appendix, we show the behavior of\nLXMERT by visualizing its attention graphs in\nthe language encoder, object-relationship encoder,\nand cross-modality encoder, respectively.\n6 Related Work\nModel Architecture Our model is closely re-\nlated to three ideas: bi-directional attention,\nTransformer, and BUTD. Lu et al. (2016) applies\nbi-directional attention to the vision-and-language\ntasks while its concurrent work BiDAF (Seo et al.,\n2017) adds modeling layers in solving reading\ncomprehension. Transformer (Vaswani et al.,\n2017) is ﬁrst used in machine translation, we\nutilize it as our single-modality encoders and\ndesign our cross-modality encoder based on it.\nBUTD (Anderson et al., 2018) embeds images\nwith the object RoI features, we extend it with ob-\nject positional embeddings and object relationship\nencoders.\nPre-training After ELMo (Peters et al., 2018),\nGPT (Radford et al., 2018), and BERT (Devlin\net al., 2019) show improvements in language un-\nderstanding tasks with large-scale pre-trained lan-\nguage model, progress has been made towards the\ncross-modality pre-training. XLM (Lample and\nConneau, 2019) learns the joint cross-lingual rep-\nresentations by leveraging the monolingual data\nand parallel data. VideoBert (Sun et al., 2019)\ntakes masked LM on the concatenation of lan-\nguage words and visual tokens, where the visual\ntokens are converted from video frames by vec-\ntor quantization. However, these methods are still\nbased on a single transformer encoder and BERT-\nstype token-based pre-training, thus we develop\na new model architecture and novel pre-training\ntasks to satisfy the need of cross-modality tasks.\nRecent works since our EMNLP submission\nThis version of our paper (and all current results)\nwas submitted to EMNLP 11 and was used to par-\nticipate in the VQA and GQA challenges in May\n2019. Since our EMNLP submission, a few other\nuseful preprints have recently been released (in\n11EMNLP deadline was on May 21, 2019, and the standard\nACL/EMNLP arxiv ban rule was in place till the notiﬁcation\ndate of August 12, 2019.\nAugust) on similar cross-modality pre-training di-\nrections: ViLBERT (Lu et al., 2019) and Visual-\nBERT (Li et al., 2019). Our LXMERT methods\ndiffers from them in multiple ways: we use a more\ndetailed, multi-component design for the cross-\nmodality model (i.e., with an object-relationship\nencoder and cross-modality layers) and we em-\nploy additional, useful pre-training tasks (i.e., RoI-\nfeature regression and image question answering).\nThese differences result in the current best perfor-\nmance (on overlapping reported tasks): a margin\nof 1.5% accuracy on VQA 2.0 and a margin of\n9% accuracy on NLVR2 (and 15% in consistency).\nLXMERT is also the only method which ranks in\nthe top-3 on both the VQA and GQA challenges\namong more than 90 teams. We provide a detailed\nanalysis to show how these additional pre-training\ntasks contribute to the ﬁne-tuning performance in\nSec. 5.2 and Sec. 5.3.\n7 Conclusion\nWe presented a cross-modality framework,\nLXMERT, for learning the connections between\nvision and language. We build the model based\non Transfermer encoders and our novel cross-\nmodality encoder. This model is then pre-trained\nwith diverse pre-training tasks on a large-scale\ndataset of image-and-sentence pairs. Empirically,\nwe show state-of-the-art results on two image\nQA datasets (i.e., VQA and GQA) and show the\nmodel generalizability with a 22% improvement\non the challenging visual reasoning dataset of\nNLVR2. We also show the effectiveness of several\nmodel components and training methods via\ndetailed analysis and ablation studies.\nAcknowledgments\nWe thank the reviewers for their helpful com-\nments. This work was supported by ARO-YIP\nAward #W911NF-18-1-0336, and awards from\nGoogle, Facebook, Salesforce, and Adobe. The\nviews, opinions, and/or ﬁndings contained in this\narticle are those of the authors and should not be\ninterpreted as representing the ofﬁcial views or\npolicies, either expressed or implied, of the fund-\ning agency. We also thank Alane Suhr for evalua-\ntion on NLVR2.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho,\nand Yoshua Bengio. 2015. Gated feedback recur-\nrent neural networks. In International Conference\non Machine Learning, pages 2067–2075.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference\non Computer Vision and Pattern Recognition, pages\n248–255. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT.\nPeng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\nSteven C. H. Hoi, Xiaogang Wang, and Hongsheng\nLi. 2019a. Dynamic fusion with intra- and inter-\nmodality attention ﬂow for visual question answer-\ning. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nPeng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang\nWang, and Hongsheng Li. 2019b. Multi-modality\nlatent interaction network for visual question an-\nswering. arXiv preprint arXiv:1908.04289.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Ji-\ntendra Malik. 2014. Rich feature hierarchies for ac-\ncurate object detection and semantic segmentation.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 580–587.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nv in vqa matter: Elevating the role of image under-\nstanding in visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904–6913.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nDan Hendrycks and Kevin Gimpel. 2016.\nBridging nonlinearities and stochastic reg-\nularizers with gaussian error linear units.\nhttps://openreview.net/forum?id=Bk0MRI5lg.\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2019. exbert: A visual analysis tool\nto explore learned representations in transformers\nmodels. arXiv preprint arXiv:1910.05276.\nRonghang Hu, Jacob Andreas, Marcus Rohrbach,\nTrevor Darrell, and Kate Saenko. 2017. Learning\nto reason: End-to-end module networks for visual\nquestion answering. In Proceedings of the IEEE In-\nternational Conference on Computer Vision , pages\n804–813.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: a new dataset for compositional question an-\nswering over real-world images. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition.\nYu Jiang, Vivek Natarajan, Xinlei Chen, Marcus\nRohrbach, Dhruv Batra, and Devi Parikh. 2018.\nPythia v0. 1: the winning entry to the vqa challenge\n2018. arXiv preprint arXiv:1807.09956.\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.\n2018. Bilinear attention networks. In Advances\nin Neural Information Processing Systems , pages\n1564–1574.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma,\net al. 2017. Visual genome: Connecting language\nand vision using crowdsourced dense image anno-\ntations. International Journal of Computer Vision ,\n123(1):32–73.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. arXiv preprint arXiv:1908.02265.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi\nParikh. 2016. Hierarchical question-image co-\nattention for visual question answering. InAdvances\nIn Neural Information Processing Systems , pages\n289–297.\nEthan Perez, Florian Strub, Harm De Vries, Vincent\nDumoulin, and Aaron Courville. 2018. Film: Vi-\nsual reasoning with a general conditioning layer. In\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. In\nAdvances in neural information processing systems,\npages 91–99.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nﬂow for machine comprehension. In International\nConference on Learning Representations.\nMeet Shah, Xinlei Chen, Marcus Rohrbach, and Devi\nParikh. 2019. Cycle-consistency for robust visual\nquestion answering. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nAlane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai,\nand Yoav Artzi. 2019. A corpus for reasoning about\nnatural language grounded in photographs. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. arXiv preprint arXiv:1904.01766.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre\nSermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Ra-\nbinovich. 2015. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 1–9.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. EMNLP 2018,\npage 353.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In International conference on machine learn-\ning, pages 2048–2057.\nZhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, and\nQi Tian. 2019a. Multimodal uniﬁed attention net-\nworks for vision-and-language interactions. arXiv\npreprint arXiv:1908.04107.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and\nQi Tian. 2019b. Deep modular co-attention net-\nworks for visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6281–6290.\nZhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and\nDacheng Tao. 2018. Beyond bilinear: Generalized\nmultimodal factorized high-order pooling for visual\nquestion answering. IEEE Transactions on Neu-\nral Networks and Learning Systems , 29(12):5947–\n5959.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7w: Grounded question answering\nin images. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n4995–5004.\nAppendix\nA Evaluated Datasets Description\nWe use three datasets for evaluating our LXMERT\nframework.\nVQA The goal of visual question answering\n(VQA) (Antol et al., 2015) is to answer a natu-\nral language question related to an image. We take\nVQA v2.0 dataset (Goyal et al., 2017) which re-\nduces the answer bias compared to VQA v1.0. The\ndataset contains an average of 5.4 questions per\nimage and the total amount of questions is 1.1M.\nGQA The task of GQA (Hudson and Manning,\n2019) is same as VQA (i.e., answer single-image\nrelated questions), but GQA requires more reason-\ning skills (e.g., spatial understanding and multi-\nstep inference). 22M questions in the dataset are\ngenerated from ground truth image scene graph to\nexplicitly control the question quality.\nNLVR2 Since the previous two datasets are used\nin pre-training for increasing the amount of pre-\ntraining data to a certain scale, we evaluate our\nLXMERT framework on another challenging vi-\nsual reasoning dataset NLVR 2 where all the sen-\ntences and images are not covered in pre-training.\nEach datum in NLVR 2 contains two related nat-\nural images and one natural language statement.\nThe task is to predict whether the statement cor-\nrectly describes these two images or not. NLVR 2\nhas 86K, 7K, 7K data in training, development,\nand test sets, respectively.\nB Details of NLVR 2 Fine-tuning\nEach datum in NLVR 2 consists of a two-image\npair (img0, img1), one statement s, and a ground\ntruth label y∗indicating whether the statement cor-\nrectly describe the two images. The task is to pre-\ndict the labelygiven the images and the statement.\nTo use our LXMERT model on NLVR 2, we\nconcatenate the cross-modality representations of\nthe two images and then build the classiﬁer with\nGeLU activation(Hendrycks and Gimpel, 2016).\nSuppose that LXMERT(img,sent) is the single-\nvector cross-modality representation, the pre-\ndicted probability is:\nx0 = LXMERT(img0,s)\nx1 = LXMERT(img1,s)\nz0 = W0[x0; x1] +b0\nz1 = LayerNorm\n(\nGeLU(z0)\n)\nprob = σ(W1z1 + b1)\nwhere σ is sigmoid function. The model is op-\ntimized by maximizing the log-likelihood, which\nis equivalent to minimize the binary cross entropy\nloss:\nL= -y∗log prob −(1 −y∗) log(1−prob)\nC Training, Validation, and Testing\nSplits\nWe carefully split each dataset to ensure that\nall testing images are not involved in any pre-\ntraining or ﬁne-tuning steps. Our data splits for\neach dataset and reproducible code are available\nat https://github.com/airsplay/lxmert.\nLXMERT Pre-Traininig Since MS COCO has\na relative large validation set, we sample a set\nof 5k images from the MS COCO validation set\nas the mini-validation (minival) set. The rest of\nthe images in training and validation sets (i.e.,\nCOCO training images, COCO validation images\nbesides minival, and all the other images in Visual\nGenome) are used in pre-training. Although the\ncaptions and questions of the MS COCO test sets\nare available, we exclude all of them to make sure\nthat testing images are not seen in pre-training.\nFine-tuning For training and validating VQA\nv2.0, we take the same split convention as in our\nLXMERT pre-training. The data related to im-\nages in LXMERT mini-validation set is used to\nvalidate model performance and the rest of the\ndata in train+val are used in ﬁne-tuning. We test\nour model on the VQA v2.0 ‘test-dev’ and ‘test-\nstandard’ splits. For GQA ﬁne-tuning, we follow\nthe suggestions in ofﬁcial GQA guidelines 12 to\ntake testdev as our validation set and ﬁne-tune our\nmodel on the joint train + validation sets. We test\nour GQA model on GQA ‘test-standard’ split. The\nimages in NLVR2 are not from either MS COCO\nor Visual Genome, we thus keep using the original\nsplit: ﬁne-tune on train split, validate the model\nchoice on val split, and test on the public (‘Test-\nP’) and unreleased (‘Test-U’) test splits.\nD Training Details of ‘BERT versus\nLXMERT’\nWhen training with BERT only, we train each\nexperiments for 20 epochs with a batch size\n64/128 since it was not pre-trained on these cross-\nmodality datasets. The learning rate is set to1e−4\ninstead of 5e−5.\n12 https://cs.stanford.edu/people/dorarad/gqa/evaluate.html\n(a)  LXMERT 2nd Lang-layer (b) BERT 3rd Layer \n(c) LXMERT 4th Lang-layer  (d) BERT 4th Layer \nFigure 3: Attention graphs reveal similar behavior in\nthe LXMERT language encoder (a, c) and in the origi-\nnal BERT encoder (b, d). Fig. a & b show the attention\npointing to next words while Fig. c & d show the atten-\ntion pointing to previous words.\n(a)  LXMERT 1st Visn-layer (b) Recovered graphs \nFigure 4: The attention graph (a) and its recovered\nscene graph (b) in the ﬁrst layer of LXMERT’s object-\nrelationship encoder.\nE Visualizing LXMERT Behavior\nIn this section, we show the behavior of LXMERT\nby visualizing its attention graphs in the language\nencoder, object-relationship encoder, and cross-\nmodality encoder, respectively.\nE.1 Language Encoder\nIn Fig. 3, we reveal that the LXMERT language\nencoder has similar behaviour as the original\nBERT encoder, by using the same sentence “Is it\nwarm enough for him to be wearing shorts?” as\nthe input to both models. LXMERT’s attention\ngraphs (in Fig. 3(a, c)) are extracted from the pre-\ntrained LXMERT without ﬁne-tuning on a spe-\nciﬁc task. BERT’s attention graphs (in Fig. 3(b,\nIs it warm enough for him to be wearing shorts ? \nWhat colors are the pole the horse is jumping over? \nFigure 5: Attention graphs in LXMERT’s cross-\nmodality encoder showing that the attention focus on\npronouns (marked in pink), nouns (marked in blue),\nand articles (marked in red).\nd)) come from Hoover et al. (2019). 13 We ﬁnd\nthat both the second LXMERT layer (Fig. 3(a))\nand third BERT layer (Fig. 3(b)) point to the\nnext words while both the fourth LXMERT layer\n(Fig. 3(c)) and fourth BERT layer (Fig. 3(d)) point\nto the previous words, thus showing the similar be-\nhaviour of the two encoders.\nE.2 Object-Relationship Encoder\nIn Fig. 4, we visualize the attention graph of the\nﬁrst layer in LXMERT’s object-relationship en-\ncoder. We only highlight the objects with the\nhighest attention scores while the other objects\nare mostly not attended to. We manually build\nthe connections between objects (marked as yel-\nlow lines in Fig. 4(b)) according to the attention\ngraph. These connections faithfully draw a scene\ngraph of the ﬁgure, which indicates that the object-\nrelationship encoder might be learning a reason-\nably good network of the relationships between\nobjects.\nE.3 Cross-Modality Encoder\nIn Fig. 5, we visualize the attention in LXMERT’s\ncross-modality encoder to reveal the connections\nbetween objects and words. We ﬁnd that the atten-\ntion focuses on nouns and pronouns as shown in\nthe top ﬁgure of Fig. 5 because they are the most\n13exBERT demo (Hoover et al., 2019) is available at\nhttp://exbert.net/\ninformative words in current vision-and-language\ntasks. However, for non-plural nouns (as shown in\nthe bottom example in Fig. 5), the attention will\nfocus on the articles. Although we do not specif-\nically design for this behavior, we think that arti-\ncles are possibly serving as special tokens (e.g.,\n[CLS], [SEP] in BERT), thus providing uniﬁed\ntarget entries for the attention layers. Next, we\nare also looking at how to utilize pre-training tasks\nwhich directly capture pairwise noun-noun and\nnoun-verb relationships between the images and\ntext sentences."
}