{
  "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games",
  "url": "https://openalex.org/W2747206248",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2554710403",
      "name": "Prithvijit Chattopadhyay",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2604110358",
      "name": "Deshraj Yadav",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2604605517",
      "name": "Viraj Prabhu",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2232895817",
      "name": "Arjun Chandrasekaran",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2131535338",
      "name": "Abhishek Das",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2131064080",
      "name": "Stefan Lee",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098683697",
      "name": "Dhruv Batra",
      "affiliations": [
        "Meta (United States)",
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2223275083",
      "name": "Devi Parikh",
      "affiliations": [
        "Georgia Institute of Technology",
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098683697",
      "name": "Dhruv Batra",
      "affiliations": [
        "Georgia Institute of Technology",
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2223275083",
      "name": "Devi Parikh",
      "affiliations": [
        "Georgia Institute of Technology",
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1561665100",
    "https://openalex.org/W753012316",
    "https://openalex.org/W6745764446",
    "https://openalex.org/W2558809543",
    "https://openalex.org/W6683221892",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W6636202728",
    "https://openalex.org/W6714262827",
    "https://openalex.org/W6714618025",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W6609531583",
    "https://openalex.org/W6646321410",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W6639657675",
    "https://openalex.org/W2035683813",
    "https://openalex.org/W1974474704",
    "https://openalex.org/W1651525653",
    "https://openalex.org/W2951357606",
    "https://openalex.org/W4237615223",
    "https://openalex.org/W2953119472",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W2605369869",
    "https://openalex.org/W1983405110",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2603266952",
    "https://openalex.org/W1991313121",
    "https://openalex.org/W2410983263",
    "https://openalex.org/W1600300810",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W256394748",
    "https://openalex.org/W2158069883",
    "https://openalex.org/W2080942732",
    "https://openalex.org/W2768661419",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2067329295",
    "https://openalex.org/W2141282920",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2599940792",
    "https://openalex.org/W2407683213"
  ],
  "abstract": "As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams. In this work, we design a cooperative game — GuessWhich — to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, is provided an image which is unseen by the human. Following a brief description of the image, the human questions ALICE about this secret image to identify it from a fixed pool of images. We measure performance of the human-ALICE team by the number of guesses it takes the human to correctly identify the secret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for two versions of ALICE. Our human studies suggest a counterintuitive trend – that while AI literature shows that one version outperforms the other when paired with an AI questioner bot, we find that this improvement in AI-AI performance does not translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation and in the context of human-AI teams.",
  "full_text": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games\nPrithvijit Chattopadhyay,1,2,4 Deshraj Yadav,1,2,4 Viraj Prabhu,2,4 Arjun Chandrasekaran,2\nAbhishek Das,2 Stefan Lee,2,4 Dhruv Batra,3,2 Devi Parikh,3,2\n2Georgia Institute of Technology\n3Facebook AI Research\n{prithvijit3, deshraj, parikh}@gatech.edu\nvisualdialog.org\nAbstract\nAs AI continues to advance, human-AI teams are inevitable.\nHowever, progress in AI is routinely measured in isolation,\nwithout a human in the loop. It is crucial to benchmark\nprogress in AI, not just in isolation, but also in terms of how\nit translates to helping humans perform certain tasks,i.e., the\nperformance of human-AI teams.\nIn this work, we design a cooperative game – GuessWhich –\nto measure human-AI team performance in the speciﬁc con-\ntext of the AI being a visual conversational agent. Guess-\nWhich involves live interaction between the human and the\nAI. The AI, which we call A\nLICE , is provided an image which\nis unseen by the human. Following a brief description of the\nimage, the human questions ALICE about this secret image to\nidentify it from a ﬁxed pool of images.\nWe measure performance of the human-ALICE team by the\nnumber of guesses it takes the human to correctly identify the\nsecret image after a ﬁxed number of dialog rounds with A\nL-\nICE . We compare performance of the human-ALICE teams for\ntwo versions of ALICE . Our human studies suggest a counter-\nintuitive trend – that while AI literature shows that one ver-\nsion outperforms the other when paired with an AI questioner\nbot, we ﬁnd that this improvement in AI-AI performance does\nnot translate to improved human-AI performance. This sug-\ngests a mismatch between benchmarking of AI in isolation\nand in the context of human-AI teams.\n1 Introduction\nAs Artiﬁcial Intelligence (AI) systems become increasingly\naccurate and interactive (e.g. Alexa, Siri, Cortana, Google\nAssistant), human-AI teams are inevitably going to become\nmore commonplace. To be an effective teammate, an AI\nmust overcome the challenges involved with adapting to hu-\nmans; however, progress in AI is routinely measured in iso-\nlation, without a human in the loop. In this work, we focus\nspeciﬁcally on the evaluation of visual conversational agents\nand develop a human computation game to benchmark their\nperformance as members of human-AI teams.\nVisual conversational agents (Das et al. 2017a; 2017b;\nde Vries et al. 2017; Strub et al. 2017) are AI agents trained\nCopyright © 2017, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1The ﬁrst two authors (PC, DY) contributed equally.\n4Work done at Virginia Tech.\nFigure 1: A human and an AI (a visual conversation agent\ncalled ALICE ) play the proposed GuessWhich game. At the\nstart of the game (top), ALICE is provided an image (shown\nabove ALICE ) which is unknown to the human. Both ALICE\nand the human are then provided a brief description of the\nimage. The human then attempts to identify the secret im-\nage. In each subsequent round of dialog, the human asks a\nquestion about the unknown image, receives an answer from\nA\nLICE , and makes a best guess of the secret image from a\nﬁxed pool of images. After 9 rounds of dialog, the human\nmakes consecutive guesses until the secret image is iden-\ntiﬁed. The fewer guesses the human needs to identify the\nsecret image, the better the human-AI team performance.\nto understand and communicate about the contents of a\nscene in natural language. For example, in Fig. 1, the visual\nconversational agent (shown on the right) replies to answers\nquestions about a scene while inferring context from the di-\nalog history – Human: ”What is he doing?” Agent: ”Playing\nProceedings of the Fifth Conference on  \nHuman Computation and Crowdsourcing \n(HCOMP 2017)\n2\nfrisbee”. These agents are typically trained to mimic large\ncorpora of human-human dialogs and are evaluated auto-\nmatically on how well they retrieve actual human responses\n(ground truth) in novel dialogs.\nRecent work has evaluated these models more pragmati-\ncally by evaluating how well pairs of visual conversational\nagents perform on goal-based conversational tasks rather\nthan response retrieval from ﬁxed dialogs. Speciﬁcally, (Das\net al. 2017b) train two visual conversational agents – a\nquestioning bot Q\nBOT , and an answering bot ABOT – for\nan image-guessing task. Starting from a description of the\nscene, Q\nBOT and ABOT converse over multiple rounds of\nquestions (QBOT ) and answers (ABOT ) in order to improve\nQBOT ’s understanding of a secret image known only to\nABOT . After a ﬁxed number of rounds, QBOT must guess\nthe secret image from a large pool and both QBOT and ABOT\nare evaluated based on this guess.\n(Das et al. 2017b) compare supervised baseline mod-\nels with QBOT -ABOT teams trained through reinforcement\nlearning based self-talk on this image-guessing task. They\nﬁnd that the AI-AI teams improve signiﬁcantly at guessing\nthe correct image after self-talk updates compared to the su-\npervised pretraining. While these results indicate that the\nself-talk ﬁne-tuned agents are better visual conversational\nagents, crucially, it remains unclear if these agents are in-\ndeed better at this task wheninteracting with humans.\nGuessWhich. In this work, we propose to evaluate if\nand how this progress in AI-AI evaluation translates to the\nperformance of human-AI teams. Inspired by the popular\nGuessWhat or 20-Questions game, we design a human com-\nputation game – GuessWhich – which requires collaboration\nbetween human and visual conversational AI agents. Mir-\nroring the setting of (Das et al. 2017b), GuessWhich is an\nimage-guessing game that consists of 2 participants –ques-\ntioner and answerer. At the start of the game, the answerer\nis provided an image that is unknown to the questioner and\nboth questioner and answerer are given a brief description\nof the image content. The questioner interacts with the an-\nswerer for a ﬁxed number of rounds of question-answer (dia-\nlog) to identify the secret image from a ﬁxed pool of images\n(see Fig. 1).\nWe evaluate human-AI team performance in Guess-\nWhich, for the setting where the questioner is a human and\nthe answerer is an AI (that we denote A\nLICE ). Speciﬁcally,\nwe evaluate two versions of ALICE for GuessWhich:\n1. A LICE SL which is trained in a supervised manner on the\nVisual Dialog dataset (Das et al. 2017a) to mimic the an-\nswers given by humans when engaged in a conversation\nwith other humans about an image, and\n2. A\nLICE RL which is pre-trained with supervised learning\nand ﬁne-tuned via reinforcement learning for an image-\nguessing task as in (Das et al. 2017b).\nIt is important to appreciate the difﬁculty and sensitivity of\nthe GuessWhich game as an evaluation tool – agents have\nto understand human questions and respond with accurate,\nconsistent, ﬂuent and informative answers for the human-AI\nteam to do well. Furthermore, they have to be robust to their\nown mistakes,i.e., if an agent makes an error at a particular\nround, that error is now part of its conversation history, and\nit must be able to correct itself rather than be consistently\ninaccurate. Similarly, human players must also learn to adapt\nto A\nLICE ’s sometime noisy and inaccurate responses.\nAt its core, GuessWhich is a game-with-a-purpose\n(GW AP) that leverages human computation to evaluate vi-\nsual conversational agents. Traditionally, GW AP (V on Ahn\nand Dabbish 2008) have focused onhuman-human collabo-\nration, i.e. collecting data by making humans play games to\nlabel images (V on Ahn and Dabbish 2004), music (Law et\nal. 2007) and movies (Michelucci 2013). We extend this to\nhuman-AI teams and to the best of our knowledge, our work\nis the ﬁrst to evaluate visual conversational agents in an in-\nteractive setting where humans are continuously engaging\nwith agents to succeed at a cooperative game.\nContributions. More concretely, we make the following\ncontributions in this work:\n• We design an interactive image-guessing game (Guess-\nWhich) for evaluating human-AI team performance in the\nspeciﬁc context of the AIs being visual conversational\nagents. GuessWhich pairs humans with A\nLICE ,a nA Ic a -\npable of answering a sequence of questions about images.\nALICE is assigned a secret image and answers questions\nasked about that image from a human for 9 rounds to help\nthem identify the secret image (Sec. 4).\n• We evaluate human-AI team performance on this game\nfor both supervised learning (SL) and reinforcement\nlearning (RL) versions of A\nLICE . Our main experimen-\ntal ﬁnding is that despite signiﬁcant differences between\nSL and RL agents reported in previous work (Das et al.\n2017b), we ﬁnd no signiﬁcant differencein performance\nbetween A\nLICE SL or ALICE RL when paired with human\npartners (Sec. 6.1). This suggests that while self-talk and\nRL are interesting directions to pursue for building better\nvisual conversational agents, there appears to be a discon-\nnect between AI-AI and human-AI evaluations – progress\non former does not seem predictive of progress on latter.\nThis is an important ﬁnding to guide future research.\n2 Related Work\nGiven that our goal is to evaluate visual conversational\nagents through a human computation game, we draw con-\nnections to relevant work on visual conversational agents,\nhuman computation games, and dialog evaluation below.\nVisual Conversational Agents. Our AI agents are vi-\nsual conversational models, which have recently emerged\nas a popular research area in visually-grounded language\nmodeling (Das et al. 2017a; 2017b; de Vries et al. 2017;\nStrub et al. 2017). (Das et al. 2017a) introduced the task\nof Visual Dialog and collected the VisDial dataset by pair-\ning subjects on Amazon Mechanical Turk (AMT) to chat\nabout an image (with assigned roles of questioner and an-\nswerer). (Das et al. 2017b) pre-trained questioner and an-\nswerer agents on this VisDial dataset via supervised learn-\ning and ﬁne-tuned them via self-talk (reinforcement learn-\ning), observing that RL-ﬁne-tuned Q\nBOT -ABOT are better at\nimage-guessing after interacting with each other. However,\n3\nas described in Section 1, they do not evaluate if this change\nin QBOT -ABOT performance translates to human-AI teams.\nHuman Computation Games.Human computation games\nhave been shown to be time- and cost-efﬁcient, reliable, in-\ntrinsically engaging for participants (Jain and Parkes 2013;\nKrause and Smeddinck 2011), and hence an effective\nmethod to collect data annotations. There is a long line of\nwork on designing such Games with a Purpose (GW AP)\n(V on Ahn and Dabbish 2008) for data labeling purposes\nacross various domains including images (V on Ahn and\nDabbish 2004; V on Ahn, Liu, and Blum 2006; Law and\nV on Ahn 2009; Kazemzadeh et al. 2014), audio (Diakopou-\nlos, Luther, and Essa 2008; Law et al. 2007), language (Aras\net al. 2010; Chamberlain, Poesio, and Kruschwitz 2008),\nmovies (Michelucci 2013)etc. While such games have tra-\nditionally focused on human-human collaboration, we ex-\ntend these ideas to human-AI teams. Rather than collecting\nlabeled data, our game is designed to measure the effective-\nness of the AI in the context of human-AI teams.\nEvaluating Conversational Agents. Goal-driven (non-\nvisual) conversational models have typically been evaluated\non task-completion rate or time-to-task-completion (Paek\n2001), so shorter conversations are better. At the other end\nof the spectrum, free-form conversation models are often\nevaluated by metrics that rely on n-gram overlaps, such as\nBLEU, METEOR, ROUGE, but these have been shown to\ncorrelate poorly with human judgment (Liu et al. 2016). Hu-\nman evaluation of conversations is typically in the format\nwhere humans rate the quality of machine utterances given\ncontext, without actually taking part in the conversation, as\nin (Das et al. 2017b) and (Li et al. 2016). To the best of our\nknowledge, we are the ﬁrst to evaluate conversational mod-\nels via team performance where humans are continuously\ninteracting with agents to succeed at a downstream task.\nTuring Test. Finally, our GuessWhich game is in line with\nideas in (Grosz 2012), re-imagining the traditional Turing\nTest for state-of-the-art AI systems, taking the pragmatic\nview that an effective AI teammate need not appear human-\nlike, act or be mistaken for one, provided its behavior does\nnot feel jarring or bafﬂe teammates, leaving them wondering\nnot about what it is thinking but whether it is.\nNext, we formally deﬁne the AI agent A\nLICE (Sec. 3),\ndescribe the GuessWhich game setup (Sec. 4 and 5), and\npresent results and analysis from human studies (Sec. 6).\n3 The AI: A LICE\nRecall from Section 1 that our goal is to evaluate how\nprogress in AI measured through automatic evaluation trans-\nlates to performance of human-AI teams in the context of vi-\nsual conversational agents. Speciﬁcally, we are considering\nthe question-answering agent A\nBOT from (Das et al. 2017b)\nas ABOT is the agent more likely to be deployed with a hu-\nman partner in real applications (e.g. to answer questions\nabout visual content to aid a visually impaired user). For\ncompleteness, we will review this work in this section.\n(Das et al. 2017b) formulate a self-supervised image-\nguessing task between a questioner bot (QBOT ) and an an-\nswerer bot (ABOT ) which plays out over multiple rounds of\ndialog. At the start of the task, QBOT and ABOT are shown\na one sentence description (i.e. a caption) of an image (un-\nknown to QBOT ). The pair can then engage in question and\nanswer based dialog for a ﬁxed number of iterations after\nwhich Q\nBOT must try to select the secret image from a\npool. The goal of the QBOT -ABOT team is two-fold, QBOT\nshould: 1) build a mental model of the unseen image purely\nfrom the dialog and 2) be able to retrieve that image from a\nline-up of images.\nBoth Q\nBOT and ABOT are modeled as Hierarchical Re-\ncurrent Encoder-Decoder neural networks (Das et al. 2017a;\nSerban et al. 2016) which encode each round of dialog in-\ndependently via a recurrent neural network (RNN) before\naccumulating this information through time with an addi-\ntional RNN (resulting in hierarchical encoding). This rep-\nresentation (and a convolutional neural network based im-\nage encoding in A\nBOT ’s case) are used as input to a de-\ncoder RNN which produces an agent’s utterance (question\nfor Q\nBOT and answer for ABOT ) based on the dialog (and\nimage for ABOT ). In addition, QBOT includes an image fea-\nture regression network that predicts a representation of the\nsecret image based on dialog history. We refer to (Das et al.\n2017b) for complete model details.\nThese agents are pre-trained with supervised dialog data\nfrom the VisDial dataset (Das et al. 2017a) with a Max-\nimum Likelihood Estimation objective. This pre-training\nensures that agents can generally recognize objects/scenes\nand utter English. Following this, the models are ﬁne-tuned\nby ‘smoothly’ transitioning to a deep reinforcement learn-\ning framework to directly improve image-guessing perfor-\nmance. This annealed transition avoids abrupt divergence of\nthe dialog in face of an incorrect question-answer pair in\nthe Q\nBOT -ABOT exchange. During RL based self-talk, the\nagents’ parameters are updated by gradients corresponding\nto rewards depending on individual good or bad exchanges.\nWe refer to the baseline supervised learning based A\nBOT\nas ALICE SL and the RL ﬁne-tuned bot as ALICE RL. (Das\net al. 2017b) found that the AI-AI pair succeeds in retriev-\ning the correct image more often after being ﬁne-tuned with\nRL. In the following section, we outline our GuessWhich\ngame designed to evaluate whether this improvement be-\ntween A\nLICE SL and ALICE RL in automatic metrics translates\nto human-AI collaborations.\n4 Our GuessWhich Game\nWe begin by describing our game setting; outlin-\ning the players and gameplay mechanics. A video\nof an example game being played can be found at\nhttps://vimeo.com/229488160.\nPlayers. We replace QBOT in the AI-AI dialog with hu-\nmans to perform a collaborative task of identifying a secret\nimage from a pool. In the following, we will refer to A\nBOT\nas ALICE and the human player as H. We evaluate two ver-\nsions of ALICE –A LICE SL and ALICE RL, where SL and RL\ncorrespond to agentstrained in a supervised settingand ﬁne-\ntuned with reinforcement learningrespectively.\nGameplay. In our game setting, ALICE is assigned a se-\ncret image Ic (unknown to H) from a pool of imagesI =\n{I1,I 2, ..., In} taken from the COCO dataset (Lin et al.\n2014). Prior to beginning the dialog, both ALICE and H are\n4\nFigure 2: GuessWhich Interface: A user asks a question to ALICE in each round and ALICE responds with an answer. The user\nthen selects an appropriate image which they think is the secret image after each round of conversation. At the end of the dialog,\nuser successively clicks on their best guesses until they correctly identify the secret image.\nprovided a brief description (i.e. a caption) ofI\nc generated\nby Neuraltalk2 (Karpathy 2016), an open-source implemen-\ntation of (Vinyals et al. 2015). H then makes a guess about\nthe secret image by selecting one from the poolI based only\non the caption,i.e. before the dialog begins.\nIn each of the following rounds, H asks A\nLICE a question\nqt about the secret imageIc in order to better identify it from\nthe pool and ALICE responds with an answerat. After each\nround, H must select an imageIt that they feel is most likely\nthe secret imageIc from poolI based on the dialog so far. At\nthe end ofk =9 rounds of dialog, H is asked to successively\nclick on their best guess. At each click, the interface gives H\nfeedback on whether their guess is correct or not and this\ncontinues until H guesses the true secret image. In this way,\nH induces a partial ranking of the pool up to the secret image\nbased on their mental model ofI\nc from the dialog.\n4.1 Pool Selection\nWhen creating a pool of images, our aim is to ensure that the\ngame is challenging and engaging, and not too easy or too\nhard. Thus, we construct each pool of imagesI in two steps –\nﬁrst, we choose the secret imageI\nc, and then sample similar\nimages as distractors forIc. Fig. 2 shows a screenshot of our\ngame interface including a sample image pool and chat.\nSecret Image Selection.VisDial v0.5 is constructed on68k\nCOCO images which contain complex everyday scenes with\n80 object categories. A\nBOT is trained and validated on Vis-\nDial v0.5train and val splits respectively. As the images for\nboth these splits come from COCO-train, we sample secret\nimages and pools from COCO-validation to avoid overlap.\nTo select representative secret images and diverse image\npools, we do the following. For each image in the COCO\nvalidation set, we extract the penultimate layer (‘fc7’) ac-\ntivations of a standard deep convolutational neural network\n(VGG-19 from (Simonyan and Zisserman 2015)). For each\nof the 80 categories, we average the embedding vector of all\nimages containing that category. We then pick those images\nclosest to the mean embeddings, yielding 80 candidates.\nGenerating Distractor Images. The distractor images are\ndesigned to be semantically similar to the secret imageI\nc.\nFor each candidate secret image, we created 3 concentric\nhyper-spheres as euclidean balls (of radii increasing in arith-\nmetic progression) centered on the candidate secret image in\nfc7 embedding space, and sampled images from each sphere\nin a ﬁxed proportion to generate a pool corresponding to the\nsecret image. The radius of the largest sphere was varied\nand manually validated to ensure pool difﬁculty. The sam-\npling proportion can be varied to generate pools of varying\ndifﬁculty. Of the 80 candidate pools, we picked 10 that were\nof medium difﬁculty based on manual inspection.\n4.2 Data Collection and Player Reward Structure\nWe use AMT to solicit human players for our game. Each\nHuman Intelligence Task (HIT) consists of 10 games (each\ngame corresponds to one pool) and we ﬁnd that overall\n76.7% of users who started a HIT completed iti.e. played\nall 10 games. We note that incomplete game data was dis-\n5\ncarded and does not contribute to the analysis presented in\nsubsequent sections.\nWe published HITs until 28 games with both ALICE SL\nand ALICE RL were completed. This results in a total of 560\ngames split between the agents, with each game consisting\nof 9 rounds of dialog and 10 rounds of guessing. Workers\nare paid a base pay of $5 per HIT (∼$10/hour).\nTo incentivize workers to try their best at guessing the se-\ncret image, workers are paid a two-part bonus – (1) based on\nthe number of times their best guess matched the true secret\nimage after each round (up to $1 per HIT), and (2) based on\nthe rank of the true secret image in their ﬁnal sorting at the\nend of dialog (up to $2 per HIT).\nThis ﬁnal ranking explicitly captures the workers’ mental\nmodel of the secret image (unlike the per-round, best-guess\nestimates), and is closer to the overall purpose of the game\n(identifying the secret image at the end of the dialog). As\nsuch, this ﬁnal sorting is given a higher potential bonus.\n4.3 Evaluation\nSince the game is structured as a retrieval task, we evaluate\nthe human-AI collaborative performance using standard re-\ntrieval metrics. Note that the successive selection of images\nby H at the end of the dialog tells us the rank of the true\nsecret image in a sorting of the image pool based on H’s\nmental model. For example, if H makes 4 guesses before\ncorrectly selecting the secret image, then H’s mental model\nranked the secret image 5th within the pool.\nTo evaluate human-AI collaboration, we use the following\nmetrics: (1) Mean Rank (MR), which is the mean rank of the\nsecret image (i.e. number of guesses it takes to identify the\nsecret image). Lower values indicate better performance. (2)\nMean Reciprocal Rank (MRR), which is the mean of the\nreciprocal of the rank of the secret image. MRR penalizes\ndifferences in lower ranks (e.g., between 1 and 2) greater\nthan those in higher ranks (e.g., between 19 and 20). Higher\nvalues indicate better performance.\nAt the end of each round, H makes their best guess of\nthe secret image. To get a coarse estimate of the rank of the\nsecret image in each round, we sort the image pool based on\ndistance in fc7 embedding space from H’s best guess. This\ncan be used to assess accuracy of H’s mental model of the\nsecret image after each round of dialog (e.g., Fig. 4b).\n5 Infrastructure\nWe brieﬂy outline the backend architecture of GuessWhich\nin this section. Unlike most human-labeling tasks that are\none-way and static in nature (i.e., only involving a human\nlabeling static data), evaluating AI agents via our game re-\nquires live interaction between the AI agent and the human.\nWe develop a robust workﬂow that can maintain a queue of\nworkers and pair them up in real-time with an AI agent.\nWe deploy A\nLICE SL and ALICE RL on an AWS EC2 (AWS\n2017) GPU instance. We use Django (a Model-View-\nController web framework written in Python) which helps\nin monitoring HITs in real-time. We use (RabbitMQ 2017),\nan open source message broker, to queue inference jobs that\ngenerate dialog responses from the model. Our backend is\nFigure 3: We outline the backend architecture of our im-\nplementation of GuessWhich. Since GuessWhich requires\na live interaction between the human and the AI, we design\na workﬂow that can handle multiple queues and can quickly\npair a human with an AI agent.\nasynchronously connected to the client browser via web-\nsockets such that whenever an inference job is completed,\na websocket polls the AI response and delivers it to the hu-\nman in real-time. We store and fetch data efﬁciently to and\nfrom a PostgreSQL database. Fig. 3 shows a schematic dia-\ngram of the backend architecture. Our complete backend in-\nfrastructure and code is publicly available on\ngithub.com/VT-\nvision-lab/GuessWhich for others to easily make use of our\nhuman-AI game interface.\n6 Results\n6.1 A LICE SL vs. ALICE RL\nWe compare the performance of the two agents ALICE SL and\nALICE RL in the GuessWhich game. These bots are state-of-\nthe-art visual dialog agents with respect to emulating human\nresponses and generating visually discriminative responses\nin AI-AI dialog. (Das et al. 2017b) evaluate these agents\nagainst strong baselines and report AI-AI team results that\nare signiﬁcantly better than chance on a pool of∼10k im-\nages (rank∼1000 for SL, rank∼500 for RL). In addition to\nevaluating them in the context of human-AI teams we also\nreport Q\nBOT -ALICE team performances for reference.\nIn Table 1, we compare the performances of human-\nALICE SL and human-A LICE RL teams according to Mean\nRank (MR) and Mean Reciprocal Rank (MRR) of the se-\ncret image based on the guesses H makes at the end of di-\nalog. We observe that at the end of each game (9 rounds of\ndialog), human subjects correctly guessed the secret image\non their 6.86th attempt (Mean Rank) when A\nLICE SL was\ntheir teammate. With ALICE RL as their teammate, the aver-\nage number of guesses required was 7.19. We also observe\nthat A\nLICE RL outperforms ALICE SL on the MRR metric. On\n6\n/g33\n/g35\n/g37\n/g39\n/g32/g31\n/g32/g33\n/g3/g26/g32/g1/g3/g26/g33/g1/g3/g26/g34/g1/g3/g26/g35/g1/g3/g26/g36/g1/g3/g26/g37/g1/g3/g26/g38/g1/g3/g26/g39/g1/g3/g26/g40/g1/g3/g26/g32/g31/g1\n/g5/g13/g9/g19/g1/g7/g9/g19/g16\n/g3/g9/g18/g13/g1/g6/g23/g18/g10/g13/g21\n/g5/g13/g9/g19/g1/g7/g9/g19/g16/g1/g24/g22/g25/g1/g6/g23/g18/g10/g13/g21/g1/g20/g14/g1/g3/g9/g18/g13/g22\n/g2/g17/g15/g11/g13/g1/g27/g7/g4/g28/g2/g17/g15/g11/g13/g1/g27/g8/g4/g28/g7/g9/g19/g12/g20/g18\n(a) ALICE SL and ALICE RL perform about the same for most\ngames and outperform a baseline model that makes a string of\nrandom guesses at the end of each game.\n/g33/g23/g31\n/g33/g23/g36\n/g34/g23/g31\n/g34/g23/g36\n/g35/g23/g31\n/g35/g23/g36\n/g6/g24/g30/g1/g6/g24/g31/g1/g6/g24/g32/g1/g6/g24/g33/g1/g6/g24/g34/g1/g6/g24/g35/g1/g6/g24/g36/g1/g6/g24/g37/g1/g6/g24/g38/g1/g6/g24/g30/g29/g1\n/g5/g11/g8/g18/g1/g6/g8/g18/g15\n/g6/g19/g21/g18/g10/g1/g19/g12/g1/g3/g14/g8/g16/g19/g13\n/g12/g9/g36/g1/g5/g11/g8/g18/g1/g6/g8/g18/g15/g1/g22/g20/g23/g1/g6/g19/g21/g18/g10/g1/g19/g12/g1/g3/g14/g8/g16/g19/g13\n/g2/g16/g14/g9/g11/g1/g25/g6/g4/g26/g1/g12/g9/g36/g2/g16/g14/g9/g11/g1/g25/g7/g4/g26/g1/g12/g9/g36/g6/g8/g18/g10/g19/g17\n(b) ALICE SL and ALICE RL perform about the same, and clearly\noutperform a baseline model that randomly chooses an image.\nAs described in Sec. 4.3, this is only a coarse estimate of the\nrank of the secret image after each round of dialog.\nFigure 4: Mean rank (MR) of secret image across (a) number of games and (b) rounds of dialog. Lower is better. Error bars are\n95% conﬁdence intervals from 1000 bootstrap samples.\nTeam MR MRR\nHuman-ALICE SL 6.86 ± 0.53 0.27 ± 0.03\nHuman-ALICE RL 7.19 ± 0.55 0.25 ± 0.03\nTable 1: Performance of Human-A LICE teams with A L-\nICE SL and ALICE RL measured by MR (lower is better) and\nMRR (higher is better). Error bars are 95% CIs from 1000\nbootstrap samples. Unlike (Das et al., 2017b), we ﬁnd no\nsigniﬁcant difference between A\nLICE SL and ALICE RL.\nboth metrics, however, the differences are within the stan-\ndard error margins (reported in the table) and not statisti-\ncally signiﬁcant. As we collected additional data, the error\nmargins became smaller but the means also became closer.\nThis interesting ﬁnding stands in stark contrast to the results\nreported by (Das et al. 2017b), where A\nLICE RL was found to\nbe signiﬁcantly more accurate than ALICE SL when evaluated\nin an AI-AI team. Our results suggest that the improvements\nof RL over SL (in AI-AI teams) do not seem to translate to\nwhen the agents are paired with a human in a similar setting.\nMR with varying number of games.In Fig. 4a, we plot\nthe mean rank (MR) of the secret image across different\ngames. We see that the human-A\nLICE team performs about\nthe same for both ALICE SL and ALICE RL except Game 5,\nwhere ALICE SL seems to marginally outperform ALICE RL.\nWe compare the performance of these teams against a base-\nline model that makes a string of random guesses at the end\nof the game. The human-A\nLICE teams outperforms this ran-\ndom baseline with a relative improvement of about 25%.\nAI-ALICE teams versus human-ALICE teams. In Table 2,\nwe compare team performances by pairing three kinds of\nquestioners – human, Q\nBOT (SL) and QBOT (RL) with AL-\nICE SL and ALICE RL (6 teams in total) to gain insights about\nhow the questioner and ALICE inﬂuence team performances.\nInterestingly, we observe that AI-ALICE teams outperform\nhuman-ALICE teams. On average, a QBOT (SL)-ALICE SL\nTeam A LICE SL ALICE RL\nHuman 6.9 7.2\nQBOT (SL) 5.6 5.3\nQBOT (RL) 4.7 4.7\nTable 2: Performance of Human-ALICE and QBOT -ALICE\nteams measured by MR (lower is better). Error bars are 95%\nconﬁdence intervals from 1000 bootstrap samples. We ob-\nserve that AI-AI teams outperform human-AI teams.\nteam takes about 5.6 guesses to arrive at the correct secret\nimage (as opposed to 6.86 guesses for a human-A\nLICE SL\nteam). Similarly, a Q BOT (RL)-ALICE RL team takes 4.7\nguesses as opposed to a human-ALICE RL team which takes\n7.19 guesses. When we compare AI-AI teams (see Row 2\nand 3) under different settings, we observe that teams hav-\ning Q\nBOT (RL) as the questioner outperform those with\nQBOT (SL). Qualitatively, we found that QBOT (SL) tends to\nask repeating questions in a dialog and that questions from\nQBOT (RL) tend to be more visually grounded compared to\nQBOT (SL). Also, note that among the four teams ALICE\ndoes not seem to affect performance across SL and RL.\nSince we observe that QBOT (RL) tends to be a better\nquestioner on average compared to QBOT (SL), as future\nwork, it will be interesting to explore a setting where we\nevaluate Q\nBOT via a similar game with the human playing\nthe role of answerer in a QBOT -human team.\nMR with varying rounds of dialog.Fig. 4b shows a coarse\nestimate of the mean rank of the secret image across rounds\nof a dialog, averaged across games and workers. As ex-\nplained in Sec. 4.3, image ranks are computed via distance\nin embedding space from the guessed image (and hence, are\nonly an estimate). We see that the human-A\nLICE team per-\nforms about the same for both ALICE SL and ALICE RL across\nrounds of dialog in a game. When compared with a baseline\n7\nFigure 5: Worker ratings for ALICE SL and ALICE RL on 6\nmetrics. Higher is better. Error bars are 95% conﬁdence\nintervals from 1000 bootstrap samples. Humans perceive\nno signiﬁcant differences between A\nLICE SL and ALICE RL\nacross the 6 feedback metrics.\nagent that makes random guesses after every round of dia-\nlog, the human-ALICE team clearly performs better.\nStatistical tests. Observe that on both the metrics (MR and\nMRR), the differences between performances of ALICE SL\nand ALICE RL are within error margins. Since both standard\nerror and bootstrap based95% conﬁdence intervals overlap\nsigniﬁcantly, we ran further statistical tests. We ﬁnd no sig-\nniﬁcant difference between the mean ranks of A\nLICE SL and\nALICE RL under a Mann-Whitney U test (p =0 .44).\n6.2 Human perception of AI teammate\nAt the end of each HIT, we asked workers for feedback on\nALICE . Speciﬁcally, we asked workers to rate ALICE o na5 -\npoint scale (where 1=Strongly disagree, 5=Strongly agree),\nalong 6 dimensions. As shown in Fig. 5, A\nLICE was rated\non – how accurate they thought it was (accuracy), how con-\nsistent its answers were with its previous answers (consis-\ntency), how well it understood the secret image (image un-\nderstanding), how detailed its answers were (detail), how\nwell it seemed to understand their questions (question un-\nderstanding) and how ﬂuent its answers were (ﬂuency).\nWe see in Fig. 5 that humans perceive both A\nLICE SL and\nALICE RL as comparable in terms of all metrics. The small\ndifferences in perception are not statistically signiﬁcant.\n6.3 Questioning Strategies\nFig. 6 shows the distribution of questions that human sub-\njects ask A\nLICE in GuessWhich. Akin to the format of\nthe human-human GuessWhat game, we observe that bi-\nnary (yes/no) questions are overwhelmingly the most com-\nmon question type, for instance, “Is there/the/he ...?” (re-\ngion shaded yellow in the ﬁgure), “Are there ...?” (region\nshaded red), etc. The next most frequent question is “What\ncolor ...?”. These questions may be those that help the hu-\nman discriminate the secret image the best. It could also be\nthat humans are attempting to play to the perceived strengths\nof A\nLICE . As people play multiple games with ALICE ,i t\nis possible that they discover ALICE ’s strengths and learn\nto ask questions that play to its strengths. Another com-\nmon question type is counting questions, such as “How\nmany ...?”. Interestingly, some workers adopt the strategy\nof querying A\nLICE with a single word (e.g., nouns such as\nFigure 6: Distribution of ﬁrst n-grams for questions asked\nto A\nLICE . Word ordering starts from the center and radiates\noutwards. Arc length is proportional to the number of ques-\ntions containing the word. The most common question-types\nare binary – followed by ‘What color..’ questions.\n“people”, “pictures”, etc.) or a phrase (e.g., “no people”,\n“any cars”, etc.). This strategy, while minimizing human ef-\nfort, does not appear to change A\nLICE ’s performance. Fig. 7\nshows a game played by two different subjects.\n7 Challenges\nThere exist several challenges that are unique to human com-\nputation in the context of evaluating human-AI teams, for in-\nstance, making our games engaging while still ensuring fair\nand accurate evaluation. In this section, we brieﬂy discuss\nsome of the challenges we faced and our solutions to them.\nKnowledge Leak. It has been shown that work division in\ncrowdsourcing tasks follows a Pareto principle (Little 2009),\nas a small fraction of workers usually complete a majority\nof the work. In the context of evaluating an AI based on\nperformance of a human-AI team, this poses a challenge.\nRecently, (Chandrasekaran et al. 2017) showed that hu-\nman subjects can predict the responses of an AI more accu-\nrately with higher familiarity with the AI. That is, a human’s\nknowledge gained from familiarity with their AI teammate,\ncan bias the performance of the human-AI team – knowl-\nedge from previous tasks might leak to later tasks. To pre-\nvent a biased evaluation of team performance due to human\nsubjects who have differing familiarity with A\nLICE , every\nperson only plays a ﬁxed number of games (10) with ALICE .\nThus, a human subject can only accept one task on AMT,\nwhich involves playing 10 games. The downside to this is\nthat our ability to conduct a fair evaluation of an AI in an\ninteractive, game-like setting is constrained by the number\nof unique workers who accept our tasks.\n8\nA man sitting on a couch with a \nlaptop\nHow many people are in the picture? \n1 person\nWhat color is the man's shirt? it is \nblack\nWhat color is the mans pants? He has \nblack pants\nWhat color is the laptop? It is black\nWhat color are the walls? They are \nbeige\nHow big is the room? It looks pretty \nlarge\nWhat color is the ﬂoor? It is brown\nIs there a tv in the room? No I don’t \nsee TV\nIs there a coffee table in the room? I \ncan’t tell\nA man sitting on a couch with a \nlaptop\nWhat color is the couch? Brown\nIs there a television? Yes\nDo you see any lamps? Yes\nAre there any windows? Yes\nWhat color is the ﬂoor? Brown\nWhat color are the walls? Beige\nAre there any doors? No\nDo you see any pictures on the walls? \nNo\nAre there curtains at the window? Yes\nPool Dialog 2 (Human-Alice RL)Dialog 1 (Human-AliceSL)\nRank: 3 Rank: 3\nFigure 7: We contrast two games played by different workers with ALICE SL and ALICE RL on the same pool (secret image\noutlined in green). In both cases, the workers are able to ﬁnd the secret image within three guesses. It is also interesting to note\nhow the answers provided by A\nLICE are different in the two cases.\nEngagement vs. Fairness. In order to improve user-\nengagement while playing our games, we offer subjects\nperformance-based incentives that are tied to the success of\nthe human-AI team. There is one potential issue with this\nhowever. Owing to the inherent complexity of the visual di-\nalog task, A\nLICE tends to be inaccurate at times. This in-\ncreases both the difﬁculty and unpredictability of the game,\nas it tends to be more accurate for certain types of ques-\ntions compared to others. We observe that this often leads\nto unsuccessful game-plays, sometimes due to errors accu-\nmulating from successive incorrect responses from A\nLICE\nto questions from the human. In a few other cases, the hu-\nman is misled by A\nLICE by a single wrong answer or by the\nseed caption that tends to be inaccurate at times. While we\nwould like to keep subjects engaged in the game to the best\nextent possible by providing performance-based incentives,\nissuing a performance bonus that depends on both the hu-\nman and A\nLICE (who is imperfect), can be dissatisfying. To\nbe fair to the subjects performing the task while still reward-\ning good performance, we split our overall budget for each\nHIT into a suitable fraction between the base pay (majority),\nand the performance bonus.\n8 Conclusion\nIn contrast to the common practice of measuring AI progress\nin isolation, our work proposes benchmarking AI agents\nvia interactive downstream tasks (cooperative games) per-\nformed by human-AI teams. In particular, we evaluate visual\nconversational agents in the context of human-AI teams. We\ndesign a cooperative game – GuessWhich – that involves a\nhuman engaging in a dialog with an answerer-bot (A\nLICE )\nto identify a secret image known to ALICE but unknown to\nthe human from a pool of images. At the end of the dialog,\nthe human is asked to pick out the secret image from the\nimage pool by making successive guesses. We ﬁnd that A\nL-\nICE RL (ﬁne-tuned with reinforcement learning) that has been\nfound to be more accurate in AI literature than it’s super-\nvised learning counterpart when evaluated via a questioner\nbot (Q\nBOT )-ALICE team, is not more accurate when evalu-\nated via a human-ALICE team. This suggests that there is a\ndisconnect between between benchmarking of AI in isola-\ntion versus in the context of human-AI interaction. An inter-\nesting direction of future work could be to evaluate Q\nBOT\nvia QBOT -human teams.\nWe describe the game structure and the backend archi-\ntecture and discuss the unique computation and infrastruc-\nture challenges that arise when designing such live inter-\nactive settings on AMT relative to static human-labeling\ntasks. Our code and infrastructure is publicly available on\ngithub.com/VT-vision-lab/GuessWhich.\nAcknowledgements\nWe would like to acknowledge the effort provided by work-\ners on Amazon Mechanical Turk. We are grateful to the\ndevelopers of Torch (Collobert, Kavukcuoglu, and Farabet\n2011) for building an excellent framework. This work was\nfunded in part by NSF CAREER awards to DB and DP , ONR\nYIP awards to DP and DB, ONR Grant N00014-14-1-0679\nto DB, ONR Grant N00014-16-1-2713 to DP , a Sloan Fel-\nlowship to DP , an Allen Distinguished Investigator award to\nDP from the Paul G. Allen Family Foundation, Google Fac-\nulty Research Awards to DP and DB, Amazon Academic\n9\nResearch Awards to DP and DB, AWS in Education Re-\nsearch grant to DB, and NVIDIA GPU donations to DB. SL\nwas partially supported by the Bradley Postdoctoral Fellow-\nship. The views and conclusions contained herein are those\nof the authors and should not be interpreted as necessarily\nrepresenting the ofﬁcial policies or endorsements, either ex-\npressed or implied, of the U.S. Government, or any sponsor.\nReferences\nAras, H.; Krause, M.; Haller, A.; and Malaka, R. 2010.\nWebpardy: harvesting qa by hc. InProceedings of the ACM\nSIGKDD Workshop on Human Computation, 49–52. ACM.\nAWS. 2017. Amazon. https://aws.amazon.com/ec2/. [On-\nline; accessed 04-May-2017].\nChamberlain, J.; Poesio, M.; and Kruschwitz, U. 2008.\nPhrase detectives: A web-based collaborative annotation\ngame. In Proceedings of the International Conference on\nSemantic Systems (I-Semantics’ 08), 42–49.\nChandrasekaran, A.; Y adav, D.; Chattopadhyay, P .; Prabhu,\nV .; and Parikh, D. 2017. It Takes Two to Tango: Towards\nTheory of AI’s Mind.arXiv preprint arXiv:1704.00717.\nCollobert, R.; Kavukcuoglu, K.; and Farabet, C. 2011.\nTorch7: A matlab-like environment for machine learning. In\nBigLearn, NIPS Workshop.\nDas, A.; Kottur, S.; Gupta, K.; Singh, A.; Y adav, D.; Moura,\nJ. M.; Parikh, D.; and Batra, D. 2017a. Visual Dialog. In\nCVPR.\nDas, A.; Kottur, S.; Moura, J. M.; Lee, S.; and Batra, D.\n2017b. Learning cooperative visual dialog agents with deep\nreinforcement learning. InICCV.\nde Vries, H.; Strub, F.; Chandar, S.; Pietquin, O.; Larochelle,\nH.; and Courville, A. 2017. GuessWhat?! visual object dis-\ncovery through multi-modal dialogue. InCVPR.\nDiakopoulos, N.; Luther, K.; and Essa, I. 2008. Audio puz-\nzler: piecing together time-stamped speech transcripts with a\npuzzle game. InProceedings of the 16th ACM international\nconference on Multimedia, 865–868. ACM.\nGrosz, B. 2012. What question would turing pose today?AI\nMagazine 33(4):73.\nJain, S., and Parkes, D. C. 2013. A game-theoretic analysis\nof the esp game.ACM Trans. Econ. Comput.1(1):3:1–3:35.\nKarpathy, A. 2016. Neuraltalk2.\nhttps://github.com/karpathy/neuraltalk2. [Online; accessed\n04-May-2017].\nKazemzadeh, S.; Ordonez, V .; Matten, M.; and Berg, T. L.\n2014. ReferItGame: Referring to Objects in Photographs of\nNatural Scenes. InEMNLP.\nKrause, M., and Smeddinck, J. 2011. Human computation\ngames: A survey. In Signal Processing Conference, 2011\n19th European, 754–758. IEEE.\nLaw, E., and V on Ahn, L. 2009. Input-agreement: a new\nmechanism for collecting data using human computation\ngames. In Proceedings of the SIGCHI Conference on Hu-\nman Factors in Computing Systems, 1197–1206. ACM.\nLaw, E. L.; V on Ahn, L.; Dannenberg, R. B.; and Crawford,\nM. 2007. Tagatune: A game for music and sound annotation.\nIn ISMIR, volume 3, 2.\nLi, J.; Monroe, W.; Ritter, A.; Galley, M.; Gao, J.; and Juraf-\nsky, D. 2016. Deep Reinforcement Learning for Dialogue\nGeneration. In EMNLP.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P .; Ra-\nmanan, D.; Dollr, P .; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. InECCV.\nLittle, G. 2009. How many turkers are there.(dec 2009).\nLiu, C.-W.; Lowe, R.; Serban, I. V .; Noseworthy, M.; Char-\nlin, L.; and Pineau, J. 2016. How NOT To Evaluate\nY our Dialogue System: An Empirical Study of Unsuper-\nvised Evaluation Metrics for Dialogue Response Genera-\ntion. In EMNLP.\nMichelucci, P . 2013. Handbook of human computation. In\nSpringer.\nPaek, T. 2001. Empirical methods for evaluating dialog\nsystems. In Proceedings of the workshop on Evaluation for\nLanguage and Dialogue Systems-V olume 9.\nRabbitMQ. 2017. RabbitMQ. https://www.rabbitmq.com/.\n[Online; accessed 04-May-2017].\nSerban, I. V .; Sordoni, A.; Bengio, Y .; Courville, A.; and\nPineau, J. 2016. Building End-To-End Dialogue Systems\nUsing Generative Hierarchical Neural Network Models. In\nAAAI.\nSimonyan, K., and Zisserman, A. 2015. V ery deep convolu-\ntional networks for large-scale image recognition. InICLR.\nStrub, F.; de Vries, H.; Mary, J.; Piot, B.; Courville, A. C.;\nand Pietquin, O. 2017. End-to-end optimization of goal-\ndriven and visually grounded dialogue systems. arXiv\npreprint arXiv:1703.05423.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. InCVPR.\nV on Ahn, L., and Dabbish, L. 2004. Labeling images with\na computer game. InCHI.\nV on Ahn, L., and Dabbish, L. 2008. Designing games with\na purpose. Communications of the ACM51(8):58–67.\nV on Ahn, L.; Liu, R.; and Blum, M. 2006. Peekaboom: a\ngame for locating objects in images. InProceedings of the\nSIGCHI conference on Human Factors in computing sys-\ntems, 55–64. ACM.\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7455264925956726
    },
    {
      "name": "Alice (programming language)",
      "score": 0.708592414855957
    },
    {
      "name": "Context (archaeology)",
      "score": 0.641358494758606
    },
    {
      "name": "Benchmarking",
      "score": 0.6263415813446045
    },
    {
      "name": "Isolation (microbiology)",
      "score": 0.5755341649055481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5592197775840759
    },
    {
      "name": "Human-in-the-loop",
      "score": 0.5354143977165222
    },
    {
      "name": "Dialog box",
      "score": 0.5012655258178711
    },
    {
      "name": "Conversation",
      "score": 0.48966673016548157
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4851137399673462
    },
    {
      "name": "Counterintuitive",
      "score": 0.4741517901420593
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3938188850879669
    },
    {
      "name": "Machine learning",
      "score": 0.3330499529838562
    },
    {
      "name": "Psychology",
      "score": 0.15458279848098755
    },
    {
      "name": "Communication",
      "score": 0.1275562047958374
    },
    {
      "name": "World Wide Web",
      "score": 0.07579013705253601
    },
    {
      "name": "Microbiology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}