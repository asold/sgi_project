{
    "title": "Sign language recognition from digital videos using feature pyramid network with detection transformer",
    "url": "https://openalex.org/W4322769066",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100345725",
            "name": "Yu Liu",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5110382633",
            "name": "Parma Nand",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5064276817",
            "name": "Md Akbar Hossain",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5042399868",
            "name": "Minh Nguyen",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5025273836",
            "name": "Wei Yan",
            "affiliations": [
                "Auckland University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1785465496",
        "https://openalex.org/W2139359116",
        "https://openalex.org/W1793695472",
        "https://openalex.org/W2905935816",
        "https://openalex.org/W3034765865",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2981618422",
        "https://openalex.org/W4231293494",
        "https://openalex.org/W2954798773",
        "https://openalex.org/W2463640844",
        "https://openalex.org/W2064851185",
        "https://openalex.org/W2341234201",
        "https://openalex.org/W4206547549",
        "https://openalex.org/W2963689837",
        "https://openalex.org/W2471695703",
        "https://openalex.org/W3127711005",
        "https://openalex.org/W2426359742",
        "https://openalex.org/W2740825418",
        "https://openalex.org/W3175585044",
        "https://openalex.org/W639708223",
        "https://openalex.org/W4235517609",
        "https://openalex.org/W2066601700",
        "https://openalex.org/W4234001826",
        "https://openalex.org/W3214774672",
        "https://openalex.org/W2520613337",
        "https://openalex.org/W3114337930",
        "https://openalex.org/W6607999579",
        "https://openalex.org/W2962914678",
        "https://openalex.org/W4207015831"
    ],
    "abstract": "Abstract Sign language recognition is one of the fundamental ways to assist deaf people to communicate with others. An accurate vision-based sign language recognition system using deep learning is a fundamental goal for many researchers. Deep convolutional neural networks have been extensively considered in the last few years, and a slew of architectures have been proposed. Recently, Vision Transformer and other Transformers have shown apparent advantages in object recognition compared to traditional computer vision models such as Faster R-CNN, YOLO, SSD, and other deep learning models. In this paper, we propose a Vision Transformer-based sign language recognition method called DETR (Detection Transformer), aiming to improve the current state-of-the-art sign language recognition accuracy. The DETR method proposed in this paper is able to recognize sign language from digital videos with a high accuracy using a new deep learning model ResNet152 + FPN (i.e., Feature Pyramid Network), which is based on Detection Transformer. Our experiments show that the method has excellent potential for improving sign language recognition accuracy. For instance, our newly proposed net ResNet152 + FPN is able to enhance the detection accuracy up to 1.70% on the test dataset of sign language compared to the standard Detection Transformer models. Besides, an overall accuracy 96.45% was attained by using the proposed method.",
    "full_text": "Sign language recognition from digital videos using\nfeature pyramid network with detection transformer\nYu Liu 1 & Parma Nand 1 & Md Akbar Hossain 1 & Minh Nguyen 1 & Wei Qi Yan 1\nAbstract\nSign language recognition is one of the fundamental ways to assist deaf people to\ncommunicate with others. An accurate vision-based sign language recognition system\nusing deep learning is a fundamental goal for many researchers. Deep convolutional\nneural networks have been extensively considered in the last few years, and a slew of\narchitectures have been proposed. Recently, Vision Transformer and other Transformers\nhave shown apparent advantages in object recognition compared to traditional computer\nvision models such as Faster R-CNN, YOLO, SSD, and other deep learning models. In\nthis paper, we propose a Vision Transformer-based sign language recognition method\ncalled DETR (Detection Transformer), aiming to improve the current state-of-the-art sign\nlanguage recognition accuracy. The DETR method proposed in this paper is able to\nrecognize sign language from digital videos with a high accuracy using a new deep\nlearning model ResNet152 + FPN (i.e., Feature Pyramid Network), which is based on\nDetection Transformer. Our experiments show that the method has excellent potential for\nimproving sign language recognition accuracy. For instance, our newly proposed net\nResNet152 + FPN is able to enhance the detection accuracy up to 1.70% on the test\ndataset of sign language compared to the standard Detection Transformer models.\nBesides, an overall accuracy 96.45% was attained by using the proposed method.\nKeywords Sign language recognition. ResNet152. Detection transformer. Feature pyramid\nnetwork\n1 Introduction\nSign language recognition is significant for deaf or hearing-impaired people [ 2]. Sign language\ncomprises a series of gestures that can be recognized and translated into semantical symbols in\ntexts. The history of sign languages does not correspond to that of spoken languages. For\nhttps://doi.org/10.1007/s11042-023-14646-0\n* W e iQ iY a n\nwyan@aut.ac.nz\n1 Auckland University of Technology, Auckland 1010, New Zealand\nMultimedia Tools and Applications (2023) 82:21673–21685\nReceived: 11 March 2022 / Revised: 13 June 2022 / Accepted: 4 February 2023 /\n# The Author(s) 2023\nPublished online: 28 February 2023\nexample, though the uses of the same spoken language (with minor differences), NZSL, BSL\nand American Sign Language (ASL) are unrelated languages and are not mutually intelligible.\nIt is now universally accepted in the linguistic community that sign languages such as NZSL\n(i.e., New Zealand Sign Language), ASL (i.e., American Sign Language), CSL (i.e., Chinese\nSign Language), etc., are natural languages with comparable power to that of spoken lan-\nguages. Indeed, it is true that sign language is one of the great linguistic discoveries. It acts as a\nfundamental mode of communications for the hearing and speech impaired people, without it\nthe communications between them and others might be difficult.\nSign language recognition from digital images and videos is regarded as a type of behavior\nidentification. Usually, it is implemented by using machine learning approaches. Nowadays,\ndeep learning is employed [ 12] for sign language recognition. Deep learning models have\npeformed extremely well in image processing as well as natural language processing, however\na fundamental requirement of this class of models is a large dataset. In the case of sign\nlanguage recognition with deep learning, we would need a large dataset with sign language\ngestures spanning the range of the sign language vocabulary. In general, sign language\nrecognition has three steps: Detecting, tracking, and recognizing gestures. The difficulty of\nthis recognition is that we need to extract task-related data or features in an efficient way. We\napply Detection Transformer (DETR) as a basic structure to solve this problem of the RNN\nmodels based on sequential computations, hence large matrix multiplications could not be\nparallelized for computational efficiency. As a solution to this, the encoder-decoder framework\nwas proposed which takes use of attention mechanism [ 27], hence it can be easily parallelized\nfor machine understanding tasks.\nThe attention mechanism is employed to form an encoder-decoder framework for machine\nunderstanding. In 2020, Transformer was applied to Vision Transformer for image classifica-\ntion. In the work, the image is cut into blocks as serialized data for an encoder, and an attention\nmechanism is applied to match the image and classification labels. The novelty of this\nproposed method is the use of an attention mechanism to increase the speed of model training.\nIt is a deep learning model entirely based on self-attention mechanism because it is suitable for\nparallel computing. In this paper, the main contributions are:\n& We create a new model that makes use of a novel backbone network ResNet152 and\nFeature Pyramid Network (FPN) as the neck. The structure is able to increase input\nfeatures, which boosts the quality of the final output.\n& As one part of this research work, we create our own dataset. This dataset is now publicly\navailable for model training and testing at github.com.\n& Regarding the purpose of evaluations, we also compare our proposed method with other\nDETR-based models, the results surpass ResNet34, ResNet50 and ResNet101 in terms of\nAP, AP\n50,A P 75, and F 1 scores.\nThe paper is organized as follows. In Section 2, we highlight previous work related to this\nresearch project. The methodology is explained in Section 3. In addition, our experimental\nresults are detailed in Section 4.F i n a l l y ,i nS e c t i o n5, we conclude this work by highlighting\nthe findings and future work.\n21674 Multimedia Tools and Applications (2023) 82:21673–21685\n2 Related work\nComputational sign language recognition has been a hot topic over the past decades [ 3, 19,\n23–25]. In recent years, using Transformers to detect visual objects has become a mainstream\nmethodology. One of them is Vision Transformer (ViT) [ 7]. Firstly, ViT segments an image\ninto a grid of squares and flattens each square into a single vector by concatenating all pixel\nchannels in a square. Transformer is independent on the structure of the input images, so\npositional embeddings are added to each square, which enables the model to be trained with\nthe input images. The feature maps were identical in the top layers of the deep ViT model; a re-\nattention method was proposed to enhance the features. As a result, the Top-1 accuracy was\nimproved up to 1.6% by using the datasets ImageNet [ 33].\nThough Vision Transformer has a good performance, it still needs to be pre-trained based\non massive data (e.g., JFT-300 M, 300 million images) and fine-tuned based on the ImageNet\ndataset to achieve comparable performance to the CNN method, which requires enormous\ncomputational resources that limits the applications of the ViT method.\nThe computational complexity is related to the square of the token. The token is a non-\noverlapping patch sequence from cutting images. If the input feature map is a 56 × 56 image,\nit will have matrix operations around 3000+, which requires a large number of computational\ncalculations. At the same time, the number of tokens in the original Transformer and the\nhidden size remain unchanged. Regarding the ResNet structure and the pyramid structure, the\nhigher the number of layers, the less the number of tokens. To this end, we make use of local\nwindow self-attention by considering a part of the feature map for self-attention, and find a\nway to interact with this local information. Convolutions are made in order to replace the fully\nconnected layer, which reduces the parameters and hence the computational cost.\nThe main advantage of data-efficient image Transformers (DeiT) [ 26]i st h a ti td o e sn o t\nrequire a massive amount of pre-training data, as it only relies on ImageNet data to generate\nthe results.\nOne of the reasons that Transformer requires enormous computing power is that the model\nitself cannot encode the position of an object. The Transformer is different from CNN, which\nrequires positional embedding to encode the position information of tokens. That is, disrupting\nthe order of tokens in sequence will not significantly change the outcome. If the location\ninformation of a patch is not provided to the model, the model needs to be trained through the\nsemantics of patches, which increases the training costs. In order to solve this problem, the\nfixed-position coding has been harnessed in DETR. Positional encoding is a 2D method\nproposed in 2020 [ 6]. The positional encoding is added to the self-attention of the encoder\nand multi-head attention of the decoder; object queries are also plugged into the decoder ’s\nattention modules. Multi-head attention makes use of multiple queries to compute multiple\ninputs in parallel.\nRegarding sign language recognition, gesture representation can be of various types, such\nas class-related attributes [ 13], class labels [ 16], and handcrafted features [ 20, 30, 34]. In\naddition, typical methods were applied to 2D CNN to extract feature maps from input images\nand then recognize sign language gestures from temporal information [ 11, 28]. In the same\nway, 3D CNN [ 9, 14, 17] is an updated version of 2D CNN, which extracts visual features by\napplying 3D convolutional layers. 3D CNN shows excellent performance in extracting feature\nmaps [22]. In this paper, our proposed method is to adopt CNN to extract feature maps from\ninput images and add FPN to enhance the features from each layer of CNN.\n21675Multimedia Tools and Applications (2023) 82:21673–21685\nA spate of computational methods have been proposed to translate sign languages from\ndigital videos to natural languages, textual sentences [ 8, 10, 18, 31]. Yin et al. [ 32] proposed\nSTMC-Transformer to improve the state-of-the-art ways by using 7 BLEU based on the video-\nto-text translation of the 2014 T dataset. Camgoz et al. [ 5] put forward the method by using\nconnectionist temporal classification loss based on the Transformer to have an end-to-end\ntranslation. The performance was evaluated based on PHOENIX-Weather-2014 dataset; the\nperformance was improved from 9.58 to 21.80 in BLEU-4 scores. Rastgoo et al. proffered a\nmethod called zero-shot sign language recognition (ZS-SLR). In the research work, a Trans-\nformer was employed for hand detection with AutoEncoder (AE) based on Long Short-Term\nMemory (LSTM). As a result, the proposed method showed better performance than other\nmethods based on four datasets: RKS-PERSIANSIGN, First-Person, ASLVID, and isoGD\n[21].\nBesides, the languages based on multimedia technology have attained great progress.\nBastanfard et al. proposed a speech therapy system for hearing impaired children [ 1].\nMinoofam et al. proffered an adaptive reinforcement learning framework called RALF through\nCellular Learning Automata (CLA) to produce semantic meanings [ 15]. Additionally, an\nalgorithm called spatial-spectral HSI classification has been put forward for extracting more\neffective features [ 4].\nPertaining to Detection Transformer, as shown in Fig. 1, this structure consists of the\nResNet152, which replaces ResNet50 as the backbone. DETR adopts a regular CNN backbone\nto get 2D representations of input images. The model flattens it and passes it to the Trans-\nformer encoder for positional encoding. The Transformer decoder then takes a small number\nof positional embeddings as input, the target query additionally participates in the output of the\nencoder. Each output embedding of the decoder is passed to predict bounding boxes and detect\nobject classes or shared feedforward network (FFN) without target class.\nAs shown in Fig. 2, the contribution of this paper is to propose a new model that consists of\nResNet152 + FPN. Deep residual nets make use of residual blocks to improve the accuracy of\nthe existing models. The concept of “skip connections” lies in the core of the residual blocks. It\ntakes use of output from one layer of the network and quickly feed it into the next layer or even\ndeeper into the neural network. We are use of jump connections to construct the ResNet, which\ncan resolve the computational burden problem of a deep neural network. This is the strength of\nthe new type of artificial neural networks.\nResNet152 is a ResNet model, which has 150 convolutional layers along with one max-\npooling layer and one average-pooling layer. The FPN naturally exploits the pyramidal form of\nCNN features and generates feature pyramids with strong semantic information on all scales.\nFig. 1 The structure of DETR\n21676 Multimedia Tools and Applications (2023) 82:21673–21685\nTherefore, the structure of FPN is designed with a top-down structure and horizontal connec-\ntions to fuse the shallow layer with high resolution and the deep layer with rich semantic\ninformation. Therefore, constructing a feature pyramid with strong semantic information on all\nscales from a single input image on a single scale is possible without incurring high costs.\nThus, FPN can enhance the feature extraction of ResNet152 at multiple scales. Besides, an\nRTX 3060 GPU accelerates the training process to achieve computational efficiency.\n3 Methodology\nIn order to improve the accuracy and speed of the proposed methodology for sign language\nrecognition, in this paper, we make use of ResNet152 to replace the ResNet50 as a backbone.\nIts aim is to increase convolutional layers and improve the feature map. As shown in Fig. 2,t h e\nbackbone makes use of the improved ResNet152 network. The function of FPN structure is to\nenhance the feature maps for each scale of the network as the neck part before data processing.\nResNet152 has two basic blocks, called Conv Block and Identity Block. The functionality\nof Conv Block is to change the dimension of the network, the input dimension and output\ndimension of Identity Block. The dimensions are the same that can be connected to deepen the\nnet. As shown in Fig. 3, ResNet152 is based on ResNet50; the difference between ResNet152\nand ResNet50 is that ResNet152 has 36 blocks, ResNet50 has 6 blocks. Thus, ResNet152 can\nget better results. Equation ( 1) is used to calculate the size of the feature map,\nw\n0\n¼ w þ 2p−k\ns þ 1 ð1Þ\nwhere w is the size of convolution input matrix, k is the convolution kernel size, s is the length\nof convolution steps, and p is the padding. The size of input images is 224 × 224 pixels. After\ndownsampling convolutions, multiple 1 × 1 convolutions and 3 × 3 convolutions, the scales\nof output feature maps are 7 × 7, 14 × 14, 28 × 28, 56 × 56 which are calculated as\nS\ni; jðÞ ¼ X /C2 VðÞ ∑\nM\n∑\nN\nxi þ m; j þ nðÞ vm ; nðÞ ð 2Þ\nFig. 2 The structure of ResNet152 + FPN\n21677Multimedia Tools and Applications (2023) 82:21673–21685\nwhere x is the variable of the input image, v is the convolution kernel, M × N is the size of the\ninput image [ 29]. Compared with ResNet50, ResNet152 has more convolution blocks and\nconvolution kernels. The semantic information and location information of multi-scale features\nare output to the neck and enhance object detection accuracy.\nIn FPN nets, the convolution kernel calculates the feature map, the feature map usually\nbecomes smaller than the last few layers. However, multiple feature layers whose output is as\nsame as the original size, are called the same network stage. For the feature pyramid in this\npaper, a pyramid level is defined for every stage, and the output of every stage at the last layer\nis selected as the reference of feature maps. The choice is usually natural. The reason is that\nthere should be the most robust features in the deepest layer for each stage. Specifically,\npertaining to ResNets, we take use of the output of residual structure of each stage, denote\nthese residual module outputs as C2, C3, C4, C5 corresponding to the outputs of conv2,\nFig. 3 The structure of ResNet50 and ResNet152\n21678 Multimedia Tools and Applications (2023) 82:21673–21685\nconv3, conv4, and conv5. We know that they have steps 4, 8, 16, 32 related to the input image.\nConv1 is not included in this pyramid framework by considering the memory footprint.\nRegarding the loss function, the output of Transformer is N predictions of visual object\nclasses, where N is larger than the number of visual objects. The annotation of the dataset\nconsists of two parts: One is ci representing the class of the visual object, the other is bi which\nshows the bounding box of the object. The prediction probability is bpbσ iðÞ ci\n/C0/C1\n.I nF i g . 4, N =5\ni ss e ta sa ne x a m p l e .T h i ss a t i s f i e se q .(3), and the loss function of this optimization is\ncalculated by using eq. ( 4).\nbσ ¼ argminσ∑N\ni Lmatch yi; byσ iðÞ\n/C16/C17\nð3Þ\nLHungarian y; by\n/C16/C17\n¼ ∑\nN\ni¼1\n−logbp\nbσ iðÞ\nciðÞ þ 1 ci≠∅fg Lbox bi; bb\nbσ\niðÞ\n/C18/C19/C20/C21\nð4Þ\n4 Experimental results\nIn this article, we utilize our own dataset for model training and testing to get the stellar\nperformance of experimental results. There are 8600 frames in total, 6450 frames were selected\nfor the training section, 2150 frames were picked up for the testing. Another dataset contains\n12 video fragments of nine classes with the labels: “Love”, ‘Good”, “You”, “Meet”, “Yes”,\n“No”, “Please”, “Name”, “My”, all these images of sign language gestures were created and\ncollected by ourselves. The total number of images is 7192 in this dataset, 5000 frames were\nemployed for the model training, 2192 frames were picked for the testing. Figure 5 shows the\ngesture samples for the nine classes. Fig. 6.\nApropos the evaluations, the metrics for evaluating our model are AP (Average Precision)\nand FPS (Frames Per Second). Regarding multiclass object detection, an introduction is\nemployed for calculating the evaluation parameters: True Positive (TP), True Negative (TN),\nFalse Positive (FP), False Negative (FN). As shown in Table 1, all the experimental indexes\nwill be calculated separately for AP, recall, and precision. Precision is the proportion of true\nexamples that should be predicted as positive, calculated by using eq. ( 5). As shown in eq. ( 6),\nTP + FN is the number of all positive samples, recall (recall rate) is the proportion of all\nConv5_x\nConv4_x\nConv3_x\nConv2_x\nBackbone\n112×112\n224×224\nPosi/g415onal encoding\nSet of image \nfeatures\nTransformer\nEncoder-Decoder\nFFN class,\nbox\nno \nobjectFFN\nFFN\nFFN\nFFN\nclass,\nbox\nno \nobject\nclass,\nbox\n.\n.\n.\n.\n.\n.\nNeck\nFig. 4 The structure of ResNet152 + FPN + DETR\n21679Multimedia Tools and Applications (2023) 82:21673–21685\npositive samples that are correctly predicted. In our experiments, the average precision is\ncalculated by using eq. ( 7).\nPrecision ¼ TP\nTP þ FP ð5Þ\nRecall ¼ TP\nTP þ FN ð6Þ\nAP ¼ TP þ TN\nTP þ TN þ FP ð7Þ\nFig. 5 The samples of our own sign language dataset\nFig. 6 The results of sign language recognition\n21680 Multimedia Tools and Applications (2023) 82:21673–21685\nIn Table 1, TP, TN, FP, and FN are mainly employed to count two types of classification\nproblems, and multiple classes were counted separately. The samples are split into positive\nsamples and negative samples. The first letter in TP, TN, FP, and FN indicates whether the\nrecognition result of the classifier is correct.\nThe focus of this paper is mainly on the proposed deep learning methods based on DETR\nand its impact on the result. We mainly emphasized on four state-of-the-art backbones to fulfill\nthe sign language recognition, which are ResNet34, ResNet50, ResNet101, and ResNet152 +\nFPN. In Fig. 7, we demonstrate the result of sign language recognition from the video frames.\nThroughout our experiments, we made use of multiple deep learning methods to compare\nour experimental results. The deep learning models with the feature pyramid networks are\nmuch more stable and robust in sign language recognition. In Table 1, we compare our deep\nlearning models for sign language recognition using our dataset.\nTable 1 The deep learning Models for sign languagr recognition\nModels APs AP 50 AP75 Param F1 FPS\nResNet18+DETR 30.6 49.6 29.7 40 M 62.2 20\nResNet34+DETR 31.8 50.2 30.2 41 M 63.0 21\nResNet50+DETR 32.5 51.0 29.3 50 M 64.2 27\nResNet101+DETR 33.3 51.8 29.8 62 M 66.8 20\nYOLOv3 31.3 52.5 30.6 66 M 67.7 22\nYOLOv4 31.7 52.8 31.8 65 M 68.5 23\nYOLOv5+Attention 32.4 53.9 31.5 68 M 70.8 24\nYOLOX + ViT 34.6 54.3 32.6 70 M 71.6 26\nResNet152+FPN+DETR (proposed) 35 54.8 33.9 73 M 72.2 28\nFig. 7 The examples of our implemented methods\n21681Multimedia Tools and Applications (2023) 82:21673–21685\nAs shown in Table 1, compared with ResNet34, ResNet50, ResNet101 and YOLO series,\nour method ResNet152 + FPN reaches the highest performance on Average Precision ( AP)\nrating at 31.50%. Comparative experiments show that the new method improves the detection\naccuracy around 1.70% compared to DETR based on our dataset. The detection accuracy is\nhigher than the standard DETR model in AP, AP 50,A P 75.\nIn Table 2, our proposed method shows excellent results for sign language recognition. We\nare able to obtain 96.45% accuracy which has a 5.99% growth of the total accuracy compared\nwith the ResNet101 + DETR. YOLOX + Vision Transformer for sign language recognition\nattains 93.72% accuracy.\nFrom DETR, we see the structure as shown in Fig. 3, the convolution blocks and\nconvolution kernels are increased step by step from ResNet18 to ResNet152. With the increase\nof convolution kernel, the feature map increases accordingly, the accuracy thus has been\nimproved.\nFigure 8 shows the accuracy and validation losses. The black bar represents the proposed\nmethod. All the methods get the maxima, the proposed method reaches the highest accuracy of\n96%. The proposed method also attains the best performance for the validation process than\nother methods.\nFrom Table 2 and Fig. 8, we see that the proposed method has a better recognition\nrate that can reach 28 FPS compared to existing methods due to its jump connection\nstructure to avoid gradient vanishing pro blem. ResNet152 as the feature extraction\nnetwork, contains more feature information and more semantic information in the upper\nlayer of the feature map. Combined with the FPN structure to fuse high-level and low-\nlevel information, ResNet152 is able to improve the average accuracy of 96.45%. For our\nproposed method as well as the compared methods, we employed an RTX 3060 GPU\nand AMD Ryzen 55600H CPU to accelerate the training and detecting process to achieve\nbetter computational efficiency.\nTable 2 The comparison of deep learning Models\nModels/\nClasses\n“Love”“ Good”“ You”“ Meet”“ Yes”“ No”“ Please”“ Name”“ My” AP\nResNet18+\nDETR\n85.23% 83.82% 84.28% 84.11% 83.37% 84.45% 85.68% 84.79% 84.25% 84.44%\nResNet34+\nDETR\n84.77% 85.27% 86.89% 85.83% 84.12% 85.35% 86.63% 85.76% 85.47% 85.56%\nResNet50+\nDETR\n87.35% 88.26% 89.71% 88.12% 87.07% 88.34% 89.33% 88.23% 88.26% 88.29%\nResNet101+\nDETR\n89.37% 90.49% 91.55% 89.35% 90.23% 91.30% 90.24% 89.66% 91.95% 90.46%\nYOLOv3 82.63% 83.66% 84.36% 83.11% 82.19% 83.59% 84.87% 83.41% 84.92% 83.63%\nYOLOv4 84.89% 83.02% 84.93% 85.96% 84.53% 84.75% 83.83% 84.29% 85.81% 84.66%\nYOLOv5+\nAttention\n91.28% 92.79% 90.33% 91.28% 92.72% 91.85% 91.97% 90.38% 91.45% 91.56%\nYOLOX +\nViT\n93.76% 92.96% 93.94% 94.22% 93.18% 93.27% 94.39% 93.78% 92.96% 93.61%\nResNet152+\nFPN\n+ DETR\n(pro-\nposed)\n95.64% 96.73% 97.15% 96.27% 96.55% 97.40% 96.16% 95.52% 96.69% 96.45%\n21682 Multimedia Tools and Applications (2023) 82:21673–21685\n5 Conclusion and future work\nIn this article, we employed ResNet152 + FPN + DETR model to achieve a superior\nperformance for sign language recognition. The experimental results show that the new model\nhas better results compared to the existing methods, which attained 1.7% growth of accuracy\nby adding the FPN nets.\nThe results show that Transformer still has excellent potential for improving sign language\nrecognition by adding the convolutional layers and increasing the feature maps to improve the\nmodel’s accuracy. Although the computational complexity and parameters have increased a lot\ncompared to the previous method, this problem can still be continuously improved in the\nfuture [2]. Besides, applying the FPN nets to the DETR-based models shows great betterment\nin sign language recognition.\nThe limitation of this work is that the data corpus and the size of the dataset are limited\nbecause we created our own dataset. The complexity of this model is higher than the\npreviously proposed recurrent neural network (RNN), however this can be easily compensated\nusing more GPU power.\nIn our future work, we will combine YOLO model and Transformer to obtain better results,\nwhich in turn will uplift the overall performance of sign language recognition. In addition, we\nintend to expand the dataset for a much wider range of vocabulary to increase the validation of\nour experiments so far.\nFunding Open Access funding enabled and organized by CAUL and its Member Institutions.\nDeclarations This work was supported by our school ’s summer research grant, it has not any conflicts of\ninterests or competing interests.\nFig. 8 The accuracy rates ( a)a n dl o s s e s(b) of our implemented methods\n21683Multimedia Tools and Applications (2023) 82:21673–21685\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and\nindicate if changes were made. The images or other third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included\nin the article's Creative Commons licence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy\nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n1. Bastanfard A, Rezaei NA, Mottaghizadeh M, Fazel M (2010) A novel multimedia educational speech\ntherapy system for hearing impaired children. Springer, pp. 705 –715\n2. Bauer B, Hienz H, Kraiss KF (2000) Video-based continuous sign language recognition using statistical\nmethods. In: International Conference on Pattern Recognition (ICPR), pp. 463 –466\n3. Bauer, B., Hienz, H., Kraiss, K. (2000) Video-based continuous sign language recognition using statistical\nmethods. In: International Conference on Pattern Recognition (ICPR)\n4. Bhatti UA, Huang M, Wu D, Zhang Y, Mehmood A, Han H (2019) Recommendation system using feature\nextraction and pattern recognition in clinical care systems. Enterprise Inform Syst 13(3):329 –351\n5. Camgoz NC, Koller O, Hadfield S, Bowden R (2020) Sign language Transformers: Joint end-to-end sign\nlanguage recognition and translation. arXiv: 2003.13830\n6. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020) End-to-end object detection\nwith Transformers. arXiv: 2005.12872\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani M,\nMinderer M, Heigold G, Gelly S, et al. (2020) An image is worth 16 × 16 words: Transformers for image\nrecognition at scale. arXiv:2010.11929\n8. Duarte A (2019) Cross-modal neural sign language translation. In: IEEE International Conference on\nMultimedia and Expo\n9. Huang J, Zhou W, Li H, Li W (2015) Sign language recognition using 3D convolutional neural networks.\nIn: IEEE International Conference on Multimedia and Expo\n10. Ko SK, Kim CJ, Jung H, Cho C (2019) Neural sign language translation based on human keypoint\nestimation. Appl Sci 9(13):2683\n11. Koller O, Ney H, Bowden R (2016) Deep hand: How to train a CNN on 1 million hand images when your\ndata is continuous and weakly labelled. In: IEEE Conference on Computer Vision and Pattern Recognition,\npp. 3793–3802\n12. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural\nnetworks. In: Advances in Neural Information Processing Systems, pp. 1097 –1105\n13. Liu J, Kuipers B, Savarese S. (2011) Recognizing human actions by attributes, In: IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 3337 –3344\n14. Liu Z, Zhang C, Tian Y (2016) 3D-based deep convolutional neural network for action recognition with\ndepth sequences. Image Vis Comput 55:93 –100\n15. Minoofam SAH, Bastanfard A, Keyvanpour MR (2022) RALF: an adaptive reinforcement learning\nframework for teaching dyslexic students. Multimed Tools Appl 81:6389 –6412\n16. Mishra A, Kumar V, Shiva M, Reddy K, Arulkumar S, Rai P, Mittal A (2018) A generative approach to\nzero-shot and few-shot action recognition. In: IEEE Winter Conference on Applications of Computer\nVision. pp. 372 –380\n17. Molchanov P, Yang X, Gupta S, Kim K, Tyree S, Kautz J (2016) Online detection and classification of\ndynamic hand gestures with recurrent 3D convolutional neural network. In: IEEE Conference on Computer\nVision and Pattern Recognition, pp. 4207 –4215\n18. Orbay A, Akarun L (2020) Neural sign language translation by learning tokenization. arXiv:2002.00479\n19. Özdemir O, Camgöz NC, Akarun L (2016) Isolated sign language recognition using improved dense\ntrajectories. In: Sig Proc Commun Appl Conf (SIU)\n20. Qin J, Liu L, Shao L, Shen F, Ni B, Chen J, Wang Y (2017) Zero-shot action recognition with error-\ncorrecting output codes, In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 2833 –2842\n21. Rastgoo R, Kiani K, Escalera S, Sabokrou M (2021) Multi-modal zero-shot sign language recognition.\narXiv: 2109.00796\n22. Ren S, He K, Girshick R, Sun J (2016) Faster R-CNN: towards real-time object detection with region\nproposal networks. IEEE Trans Pattern Anal Mach Intell 39(6):1137 –1149\n21684 Multimedia Tools and Applications (2023) 82:21673–21685\n23. Starner T, Pentland A (1997) Real-time American sign language recognition from video using hidden\nMarkov models. In: Shah M, Jain R (eds) Motion-based recognition. Computational Imaging and Vision,\nvol 9, pp 227 –243\n24. Süzgün M et al (2015) Hospisign: an interactive sign language platform for hearing impaired. J Naval Sci\nEng 11(3):75–92\n25. Tamura S, Kawasaki S (1988) Recognition of sign language motion images. Pattern Recogn 21(4):343 –353\n26. Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Jégou H (2021) Training data-efficient image\ntransformers & distillation through attention. In: International Conference on Machine Learning, pp. 10347–\n10357\n27. Vaswani A, Shazeer N, Parmar N, Yang L, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017)\nAttention is all you need. arXiv: 1706.03762\n28. Wu J, Ishwar P, Konrad J (2016) Two-stream CNNs for gesture-based verification and identification:\nLearning user style. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 42 –\n50\n29. Xiang N, Pan C, Li X (2021) An object algorithm combining FPN structure with DETR. In: ACM ICCCV,\npp. 57–63\n30. Xu T, Hospedales M, Gong S (2016) Multi-task zero-shot action recognition with prioritized data augmen-\ntation, In: European Conference on Computer Vision, pp. 343 –359\n31. Yin, K. (2020) Sign Language translation with Transformers. arXiv:2004.00588\n32. Yin K, Read J (2020) Better sign language Translation with STMC-Transformer. In: International\nConference on Computational Linguistics, pp. 5975 –5989\n33. Zhou D, Kang B, Jin X, Yang L, Lian X, Jiang Z, Hou Q, Jiashi FJ (2021) DeepViT: Towards deeper\nVision Transformer. arXiv: 2103.11886\n34. Zhu Y, Long Y, Guan Y, Newsam S, Shao L(2018) Towards universal representation for unseen action\nrecognition, In: IEEE Conference on Computer Vision and Pattern Recognition\nPublisher’sn o t e Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional affiliations.\n21685Multimedia Tools and Applications (2023) 82:21673–21685"
}