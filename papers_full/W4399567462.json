{
    "title": "Planning to Guide LLM for Code Coverage Prediction",
    "url": "https://openalex.org/W4399567462",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5099100763",
            "name": "Hridya Dhulipala",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A5013721981",
            "name": "Aashish Yadavally",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A5089000736",
            "name": "Tien N. Nguyen",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3101845936",
        "https://openalex.org/W4284706927",
        "https://openalex.org/W2160468841",
        "https://openalex.org/W3049735680",
        "https://openalex.org/W2888491619",
        "https://openalex.org/W2036700291",
        "https://openalex.org/W2063068372",
        "https://openalex.org/W2966980041",
        "https://openalex.org/W2964241064",
        "https://openalex.org/W4384302804",
        "https://openalex.org/W3195442242",
        "https://openalex.org/W3033180469",
        "https://openalex.org/W4246724267",
        "https://openalex.org/W2963846926",
        "https://openalex.org/W4389519896",
        "https://openalex.org/W2155891645",
        "https://openalex.org/W4378513026",
        "https://openalex.org/W1970803519",
        "https://openalex.org/W2472622445"
    ],
    "abstract": "Code coverage serves as a crucial metric to assess testing effectiveness, measuring the degree to which a test suite exercises different facets of the code, such as statements, branches, or paths. Despite its significance, coverage profilers necessitate access to the entire codebase, constraining their usefulness in situations where the code is incomplete or execution is not feasible, and even cost-prohibitive. In this paper, we present CodePilot, a plan-based prompting approach grounded in program semantics, which collaborates with a Large Language Model (LLM) to enhance code coverage prediction. To address the intricacies of predicting code coverage, CodePilot employs planning by discerning various types of statements in an execution flow. Planning empowers GPT to autonomously generate plans based on guided examples, and then CodePilot prompts the GPT model to predict code coverage (Action) based on the plan it generated (Reasoning). Our experiments evaluating CodePilot demonstrate high accuracy, achieving up to 55% in exact-match and 89% in statement-match. It performs relatively better than the baselines, achieving up to 33% and 19% relatively higher in those metrics. We also showed that due to highly accurate plans (90%), GPT model predicts better code coverage. Moreover, we show CodePilot's utility in correctly predicting the least covered statements.",
    "full_text": null
}