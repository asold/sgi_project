{
  "title": "Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature Corpus",
  "url": "https://openalex.org/W3137011367",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057920247",
      "name": "Sang Joon Park",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014648454",
      "name": "Gwanghyun Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069516527",
      "name": "Yujin Oh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5006695326",
      "name": "Joon Beom Seo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100342098",
      "name": "Sang Min Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008393937",
      "name": "Jin Hwan Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103033828",
      "name": "Sung-Jun Moon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5029864503",
      "name": "Jae‐Kwang Lim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012644755",
      "name": "Jong Chul Ye",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3006882119",
    "https://openalex.org/W3105081694",
    "https://openalex.org/W3028070348",
    "https://openalex.org/W3007273493",
    "https://openalex.org/W3013019084",
    "https://openalex.org/W3013601031",
    "https://openalex.org/W3033546701",
    "https://openalex.org/W3023402713",
    "https://openalex.org/W3033586327",
    "https://openalex.org/W3037810051",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W3012556938",
    "https://openalex.org/W3162351260",
    "https://openalex.org/W3034734289",
    "https://openalex.org/W3033814865",
    "https://openalex.org/W3112516115",
    "https://openalex.org/W3011414569",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3013130152",
    "https://openalex.org/W2811374795",
    "https://openalex.org/W3016610966",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3030810099",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3013515352",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W3026951029",
    "https://openalex.org/W3034978746"
  ],
  "abstract": "Under the global COVID-19 crisis, developing robust diagnosis algorithm for COVID-19 using CXR is hampered by the lack of the well-curated COVID-19 data set, although CXR data with other disease are abundant. This situation is suitable for vision transformer architecture that can exploit the abundant unlabeled data using pre-training. However, the direct use of existing vision transformer that uses the corpus generated by the ResNet is not optimal for correct feature embedding. To mitigate this problem, we propose a novel vision Transformer by using the low-level CXR feature corpus that are obtained to extract the abnormal CXR features. Specifically, the backbone network is trained using large public datasets to obtain the abnormal features in routine diagnosis such as consolidation, glass-grass opacity (GGO), etc. Then, the embedded features from the backbone network are used as corpus for vision transformer training. We examine our model on various external test datasets acquired from totally different institutions to assess the generalization ability. Our experiments demonstrate that our method achieved the state-of-art performance and has better generalization capability, which are crucial for a widespread deployment.",
  "full_text": "Vision Transformer for COVID-19 CXR\nDiagnosis using Chest X-ray Feature Corpus\nSangjoon Park1[0000−0002−9223−3172], Gwanghyun Kim1, Yujin Oh1, Joon Beom\nSeo2, Sang Min Lee 2, Jin Hwan Kim3, Sungjun Moon 4, Jae-Kwang Lim5, and\nJong Chul Ye1[0000−0001−9763−9609]\n1 Korea Advanced Institute of Science and Technology, Daejeon, South Korea\n{depecher, gwang.kim, yujin.oh, jong.ye}@kaist.ac.kr\n2 Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea\n3 College of Medicine, Chungnam National Univerity, Daejeon, South Korea\n4 College of Medicine, Yeungnam University, Daegu, South Korea\n5 School of Medicine, Kyungpook National University, Daegu, South Korea\nAbstract. Under the global COVID-19 crisis, developing robust diag-\nnosis algorithm for COVID-19 using CXR is hampered by the lack of\nthe well-curated COVID-19 data set, although CXR data with other\ndisease are abundant. This situation is suitable for vision transformer\narchitecture that can exploit the abundant unlabeled data using pre-\ntraining. However, the direct use of existing vision transformer that uses\nthe corpus generated by the ResNet is not optimal for correct feature\nembedding. To mitigate this problem, we propose a novel vision Trans-\nformer by using the low-level CXR feature corpus that are obtained to\nextract the abnormal CXR features. Speciﬁcally, the backbone network\nis trained using large public datasets to obtain the abnormal features in\nroutine diagnosis such as consolidation, glass-grass opacity (GGO), etc.\nThen, the embedded features from the backbone network are used as\ncorpus for vision transformer training. We examine our model on vari-\nous external test datasets acquired from totally diﬀerent institutions to\nassess the generalization ability. Our experiments demonstrate that our\nmethod achieved the state-of-art performance and has better generaliza-\ntion capability, which are crucial for a widespread deployment.\nKeywords: COVID-19 · Chest X-ray · Vision Transformer · Low-level\nfeatures · Transfer learning · Limited dataset.\n1 Introduction\nThe novel coronavirus disease 2019 (COVID-19), caused by severe acute respi-\nratory syndrome coronavirus-2, is an ongoing pandemic resulting in 113,695,296\npeople infected with 2,526,007 death worldwide as of 1 March 2021. In the face\nof the unprecedent pandemic by COVID-19, public health care systems have\nconfronted challenges in many aspects including critical shortage of medical re-\nsources, while many health care provider have themselves been infected [15].\nBecause of its highly transmissible and pathologic natures of COVID-19, the\narXiv:2103.07055v1  [eess.IV]  12 Mar 2021\n2 S.Park et al.\nearly screening of COVID-19 is becoming increasingly important to prevent fur-\nther spread of disease and lessen the burden of health care systems.\nCurrently, real-time polymerase chain reaction (RT-PCR) is the gold stan-\ndard in COVID-19 conﬁrmation due to high sensitivity and speciﬁcity [20], but it\ntakes several hours to get the result. As many patients with conﬁrmed COVID-\n19 present radiological ﬁndings of pneumonia, radiologic examinations may be\nuseful for fast diagnosis [18]. Though chest computed tomography (CT) has su-\nperior sensitivity and speciﬁcity for diagnosis of COVID-19 [2], the routine use of\nCT places a huge burden on health care system due to its high cost and relatively\nlonger scan time than chest radiograph (CXR). Therefore, there exist practical\nadvantages to use CXR as primary screening tool under global pandemic. The\ncommon CXR ﬁndings of COVID-19 include bilateral involvement, peripheral\nand lower zone dominance of ground glass opacities and patchy consolidations [7].\nEven though it has been reported that the sensitivity and speciﬁcity of COVID-\n19 diagnosis with CXR alone is lower than with CT or RT-PCR [25], CXR still\nhas its potential for the fast screening of COVID-19 during the patient triage,\ndetermining the priority of patient’s care to help saturated health care system\nin pandemic situation.\nAccordingly, many approaches have been proposed using deep learning to\ndiagnose COVID-19 with CXR [11, 14, 16, 22], but they suﬀered from common\nproblems of limited number of labelled COVID-19 data, resulting poor general-\nization ability [12,27]. The reliable generalization performance on unseen, totally\ndiﬀerent data set is crucial for real world adoption of the system.\nIn general, the most common approach to solve this problem is to build ad-\nversarially robust model with millions of training data [4]. However, construct-\ning well-curated dataset containing large number of labelled COVID-19 cases is\ndiﬃcult due to saturation of health care system in many countries. Though the\nprevious studies have tried to mitigate the problem either by using transfer learn-\ning from other large dataset like ImageNet [1], or by utilizing weakly-supervised\nlearning method [24, 29] and anomaly detection [28], their performances are of-\nten suboptimal and do not guarantee the ability to generalize. In addition, as\nCOVID-19 usually involves both lung ﬁelds with lower zone dominance, the\nmodel should extract features based on the global manifestation of the diseases.\nTransformer, which was ﬁrst introduced in the ﬁeld of natural language pro-\ncessing (NLP), is a deep neural network based on self-attention mechanism that\nresults in signiﬁcantly large receptive ﬁelds [21]. After achieving astounding re-\nsults in NLP, it has inspired the vision community to study its applications\nin computer vision since it enables modeling long-range dependency within im-\nages. Vision Transformer (ViT) has ﬁrst showed how Transformer can totally\nreplace standard convolution operations in deep neural network achieving state-\nof-the-art (SOTA) performance [10]. However, training Vision Transformer from\nscratch requires huge amount of data, so that the authors also suggested hybrid\nmodel by conjugating conventional convolutional neural network (e.g. ResNet)\nbackbone that produces initial feature embedding. As such, Transformer, being\ntrained using the feature corpus generated by the ResNet backbone, can mainly\nViT for COVID-19 Diagnosis using Low-level CXR Feature Corpus 3\nfocus on learning the global attention. Empirical results shows that the hybrid\nmodel present better performance in small-sized data set.\nAlthough this preliminary results is promising, there are still remaining con-\ncerns that the corpus generated by the ResNet may not be an optimal input\nfeature embedding for diagnosis with CXRs. Fortunately, there are several pub-\nlicly available large-scale datasets for CXR classiﬁcation which was built before\nthe COVID-19 outbreak. Among them CheXpert [13] dataset consist of labeled\nabnormal observations including low-level CXR features (e.g. opacity, consolida-\ntion, edema, etc.) useful for diagnosis of infectious diseases. Furthermore, there\nexists many advanced CNN architectures including the model proposed by [26],\nwhich utilizes probabilistic class activation map (PCAM) pooling to explicitly\nleverage the beneﬁt of class activation map to improve both classiﬁcation and\nlocalization ability for these low-level features. Therefore, we propose a novel\nvision Transformer which utilizes this existing network as a backbone for CXR\nlow-level feature embedding, upon which vision Transformer is trained using the\ngenerated corpus from the backbone network.\nIt is remarkable that our network fundamentally resembles the text classiﬁca-\ntion task using Transformer, which classiﬁes the entire sentence by aggregating\nthe meaning, location and relationship of words in a sentence, using the low-\nlevel word embedding. Furthermore, our network emulates the clinical experts\nwho make a CXR image-level diagnosis (e.g. normal, tuberculosis, COVID-19\npneumonia, etc.) by collating the information of low-level features in terms of\ntheir pattern, multiplicity, distribution and locations.\nContribution. In this paper, we proposed a Vision Transformer model tailored\nfor COVID-19 CXR diagnosis, leveraging the low-level CXR feature corpus at-\ntained from prebuilt large-scale public dataset for CXR features such as opacity,\nconsolidation, edema, etc. We show that our method is superiority to both base-\nline Vision Transformer and SOTA models especially in terms of the generaliza-\ntion performance on unseen datasets. We also provided clinically interpretable\nvisualization results of model, which is of great help for COVID-19 diagnosis\nand localization.\n2 Method\nThe merit of our model is that Transformer can exploit the low-level CXR feature\ncorpus, which was obtained from the backbone network trained to extract abnor-\nmal CXR features from publicly available large and well-curated CXR dataset.\nThe overall framework of our method is illustrated in Fig. 1.\nPre-training Backbone Network for Low-level Feature Corpus. As a\nbackbone network to extract low-level CXR feature corpus from an image, we\nadopted a modiﬁed version of model proposed by [26], which utilizes probabilistic\nclass activation map (PCAM) pooling to explicitly leverage the beneﬁt of class\n4 S.Park et al.\nFig. 1.Overall framework of our method. (A) We ﬁrst trained the backbone network\nutilizing probabilistic-CAM (PCAM) pooling for each of ten low-level CXR ﬁndings,\nand then trained (B) the overall network, which consists of backbone and Transformer.\nactivation map to improve both classiﬁcation and localization ability (see Fig. 1\n(A)). The backbone network was trained beforehand with prebuilt public CXR\ndataset to classify 10 labelled observations including no ﬁnding, cardiomegaly,\nlung opacity, edema, consolidation, pneumonia, atelectasis, pneumothorax, pleu-\nral eﬀusion and support devices.\nAs shown in Fig. 1 (A), there are several layers one could extract the fea-\nture embedding, and we found that the intermediate level embedding before the\nPCAM operation contains the most useful information. However, care should\nbe taken since the PCAM unit trained with speciﬁc low-level CXR features\n(e.g.cardiomegaly, lung opacity, edema, consolidation) was essential to improve\nthe accuracy of the intermediate level feature embedding by guiding the feature\naligned to provide the optimal PCAM maps. Speciﬁcally, with the pre-trained\nbackbone network G, an input image x ∈RH×W×C is encoded into intermedi-\nate feature map f ∈RH′×W′×C′\n. We used the C′ dimension feature vectors f\nof each H′×W′pixels as encoded representations for low-level features at each\npixel locations, and constructed the low-level CXR feature corpus.\nf = G(x), x ∈RH×W×C, f ∈RH′×W′×C′\n(1)\nf = [f1; f2; f3; ...; fH′×W′\n], f ∈RC′\n(2)\nViT for COVID-19 Diagnosis using Low-level CXR Feature Corpus 5\nVision Transformer. Similar to BERT [9], Vision Transformer model is encoder-\nonly architecture (see Fig. 1 (B)). As the Transformer encoder uses constant\nlatent vector of dimension D, we ﬁrst projected encoded features f of dimension\nC′ into fp of dimension D using 1 ×1 convolution kernel. Similar to [class]\ntoken of BERT, we prepended additional learnable embedding vector fcls to\nprojected features fp, to make the last L layer output of this [class] token z0\nL\nrepresent the diagnosis of a whole CXR image (= y) by attaching the classiﬁca-\ntion heads to z0\nL. In addition, we added a positional embedding Epos to encode\na notion of the sequential order to the projected features fp. The Transformer\nencoder layers used in our model are the same as standard Transformer encoder\nconsisting of alternating layers of multihead self-attention (MSA), multilayer\nperceptron (MLP), layer normalization (LN), and residual connections in each\nblock, which is described as follows:\n[f1\np ; f2\np ; f3\np ; ...; fH′×W′\np ] = conv([f1; f2; f3; ...; fH′×W′\n]), f p ∈RD (3)\n[z0\n0; z1\n0; z2\n0; ...; zH′×W′\n0 ] = [fcls; f1\np ; f2\np ; f3\np ; ...; fH′×W′\np ] + Epos (4)\nz0 = [z0\n0; z1\n0; z2\n0; ...; zH′×W′\n0 ] (5)\nz′\nl = MSA(LN(zl−1)) + zl−1, l = 1...L (6)\nzl = MLP(LN(z′\nl)) + z′\nl, l = 1...L (7)\ny = HEAD(z0\nL) (8)\nModel Interpretability. For model interpretability, we adopted a saliency\nmap visualization method tailored for Vision Transformer. Diﬀerent from other\nmethods relying on attention maps or heuristic propagation of attention, the\nmethod proposed by [3] assigns local relevance with deep Taylor decomposition,\nand propagate the local relevance throughout the layers. By the relevance prop-\nagation, this method overcomes the challenges of attentions layers and the skip\nconnections.\n3 Experimental Results\nDatasets and Partitioning. For the pre-training of the feature encoder net-\nwork, we used CheXpert [13] dataset containing 10 labelled observations includ-\ning no ﬁnding, cardiomegaly, lung opacity, edema, consolidation, pneumonia,\natelectasis, pneumothorax, pleural eﬀusion and support devices. After excluding\nlateral view images, 190,847 CXRs were available. For training and evaluation\nof Transformer model for COVID-19 diagnosis using CXR, we aggregated ret-\nrospectively collected CXR data from four independent local institutions (Asan\nMedical Center [AMC], Seoul, Korea; Chonnam National Univerity Hospital\n[CNUH], Daejeon, Korea; Yeungnam University Hospital [YNU], Daegu, Korea;\nKyungpook National University Hospital [KNUH], Daegu, Korea), which were\nlabelled by board-certiﬁed radiologists and publicly available CXR datasets con-\ntaining labels of infectious disease (Brixia [19], BIMCV [8], NIH [23] datasets).\n6 S.Park et al.\nTable 1.Summary of dataset resources and disease classes.\nResource Total External test data Training and Validation data\nCNUH YNU KNUH AMC NIH Brixia BIMCV\nNormal 13,649 320 300 400 8,861 3,768 - -\nOther infection1,468 39 144 308 977 - - -\nCOVID-19 2,431 6 8 80 - - 1,929 408\nTotal images 17,548 365 452 788 9,838 3,768 1,929 408\nTotal dataset was curated into three classes; normal, other infections (which\nincludes bacterial pneumonia and tuberculosis), and COVID-19. The numbers\nof images for each class are summarized in Table 1. To evaluate the generaliza-\ntion ability of models in various institutional settings, we set aside 3 institutional\ndatasets (CNUH, KNUH, YNU) collected from totally diﬀerent institutions with\ndiﬀerent devices and settings as external test datasets as they contain the cases\nof all three label classes.\nImplementation Details and Evaluation Metrics.Our pre-processing method\nincludes histogram equalization, Gaussian blurring with 3 ×3 kernel, normal-\nization and resizing to 512 ×512. As the backbone network, we took up on the\nnetwork architecture that scored the ﬁrst place in CheXpert challenge 2019 [26],\nwhich consists of DenseNet-121 backbone followed by PCAM operations for each\nclass. In particular, we used intermediate feature map of size 16 ×16 ×1024 be-\nfore PCAM operation as input for Transformer, since this feature map contains a\ncommon representation of all the abnormal ﬁndings. As this feature map already\nencodes the representations for important CXR ﬁndings, we adopted relatively\nsimple transformer architecture with four encoder layers with eight attention\nheads. For training of backbone network, we used Adam optimizer with learn-\ning rate of 0.0001. The backbone network was trained for 160,000 optimization\nsteps with step decay scheduler. The batch size was set to 8. For training of\nmodel for COVID-19 diagnosis, We used SGD optimizer (momentum = 0.9)\nwith max grad norm set to 1 with learning rate of 0.001. The model was trained\nfor 10,000 optimization steps with cosine warm-up scheduler (warm-up steps =\n500), with batch size set to 8. These optimal hyperparameters are determined\nexperimentally. We adopted area under the ROC curve (AUC) as our evaluation\nmetric, but also calculated sensitivity, speciﬁcity and accuracy after adjusting\nthe threshold to meet the sensitivity ≥80% if possible. Pre-processing, devel-\nopment and evaluation of the algorithm was performed with Python version 3.7\nand PyTorch 1.6 on Nvidia Tesla V100.\nDiagnostic Performances on Three Diﬀerent External Test Datasets.\nThe diagnostic performances of our model on three diﬀerent external test datasets\nare provided in Table 2. Our model shows average AUCs of 0.941, 0.909, 0.915,\naverage sensitivities of 87.0%, 85.1%, 85.9%, average speiciﬁcities of 91.2%.\n84.7%, 84.8%, and average accuracy of 86.4%, 85.9%, 85.2% in the three dif-\nferent institutional datasets obtained with diﬀerent decvices and settings. These\nViT for COVID-19 Diagnosis using Low-level CXR Feature Corpus 7\nTable 2.Diagnostic performance the proposed model in various external test datasets\nfrom three diﬀerent institutions and Comparison wiht baseline and SOTA methods.\nMetrics External set 1 (CNUH)External set 2 (YNU)External set 3 (KNUH)\nAvg.NormalOthersCOVIDAvg.NormalOthersCOVIDAvg.NormalOthersCOVID\nAUC 0.941 0.927 0.948 0.947 0.909 0.938 0.955 0.833 0.915 0.938 0.938 0.868\nSensitivity87.0 88.4 87.2 85.5 85.1 90.7 89.6 75.0 85.9 90.3 89.0 78.5\nSpeciﬁcity 91.2 88.9 84.7 100.0 84.7 86.2 87.7 80.2 84.8 85.3 89.2 80.0\nAccuracy 86.4 88.5 84.9 85.8 85.9 89.2 88.3 80.1 85.2 87.8 89.1 78.7\nMethods AUC for External set 1AUC for External set 2AUC for External set 3\nAvg.NormalOthersCOVIDAvg.NormalOthersCOVIDAvg.NormalOthersCOVID\nResNet-500.856 0.859 0.910 0.799 0.856 0.826 0.943 0.800 0.876 0.869 0.849 0.910\nViT 0.857 0.856 0.865 0.851 0.852 0.833 0.887 0.8370.811 0.843 0.686 0.904\nViT(hybrid)0.890 0.893 0.903 0.875 0.890 0.913 0.942 0.815 0.863 0.927 0.799 0.864\nOurs 0.9410.9270.9480.9470.9090.9380.9550.8330.9150.9380.9380.868\nresults demonstrate that our model retains stable performance (AUC ≥0.900)\nirrespective of external settings, which conﬁrms fair excellent generalization abil-\nity of our model.\nComparison with baseline and SOTA models. To compare the diagnostic\nperformance of our model with the baseline and SOTA models, ResNet-50 was\nused as baseline and ViT (ViT-B/16), hybrid ViT model (R50-ViT-B/16) were\nused as SOTA models. All models except ours were trained using the pre-trained\nImageNet weights since it signiﬁcantly improves their performance, and were\nsubjected to same training setting with our model for fair comparison. As shown\nin Table 2, our model outperformed the SOTA models as well as the baseline\nin all of the external test datasets, demonstrating the superior performance and\nstability in real-world application.\nModel Interpretability Results. Fig. 2 illustrates the examples of visualiza-\ntion of saliency map for each disease classes. As shown in Fig. 2 (A), the our\nmodel well-localized a focal cavitary lesion caused by bacterial infection, while\nit was also able to delineate the multi-focal areas of involvement by virus which\nis common for COVID-19 infection as in Fig. 2 (B).\nFig. 2.Examples of visualization of saliency map for each disease classes. (A) Bacterial\ninfection and (B) COVID-19 infection.\n8 S.Park et al.\nTable 3. Performance with or without self-supervised contrastive pre-training and\nAblation study with ﬁxed and trainable backbone weights.\nMethods External 1 (CNUH)External 2 (YNU)External 3 (KNUH)\nAvg. AUC of 3 classesAvg. AUC of 3 classesAvg. AUC of 3 classes\nViT(hybrid) w/o pretrain 0.890 0.890 0.863\nViT(hybrid) w pretrain 0.881 0.911 0.870\nOurs w/o pretrain 0.941 0.909 0.915\nOurs w pretrain 0.935 0.876 0.903\nNot trainable backbone 0.908 0.895 0.899\nTrainable backbone 0.941 0.909 0.915\nSelf-supervised Contrastive Pre-training. Since it has been suggested that\nthe Transformer-based model may beneﬁt from pre-training to learn the se-\nquence structure by self-supervised manner before the ﬁne-tuning for down-\nstream tasks [5, 9, 17], we evaluated the beneﬁt of self-supervised pre-training\nin Transformer-based SOTA (hybrid ViT) model and our model. We used Sim-\nCLR [6], which is a self-supervised contrastive learning method with data aug-\nmentation framework, as our self-supervised pre-training method. As provided\nin Table 3, the experimental results reveal that the self-supervised pre-trainig\nwas superﬂuous or even detrimental to our model since it already equipped with\nwell-trained backbone network, though it slightly improved the performance of\nTransformer-based SOTA model. However, our model still outperformed SOTA\nmodel, which alludes that the corpus generated by the ResNet may not be a\noptimal input feature embedding for CXR classiﬁcation.\nAblation. We conducted an ablation study to determine whether to have back-\nbone network ﬁxed or trainable after training with CheXpert dataset. The results\nof Table 3 suggests that having weights of trainable is better than ﬁxing them\nfor in all of the 3 external test datasets, which is thought to be mitigated though\nimproved capacity by trainable parameter of backbone network, dispelling the\nconcern about overﬁtting.\n4 Conclusion\nIn this work, we proposed a novel Vision Transformer model for COVID-19\nCXR diagnosis, using the low-level CXR feature corpus. The novelty of this\nstudy lies in leveraging a backbone network trained to ﬁnd low-level abnormal\nCXR ﬁndings in prebuilt large-scale dataset to embed feature corpus suitable for\nhigh-level disease classiﬁcation with Transformer model. The experimental re-\nsults on various external test datasets conﬁrm that our model not only achieves\nSOTA performance in the diagnosis of COVID-19 and other infectious disease\nbut also retains stable performance irrespecitve of the external settings, which is\na sine-qua-non for widespread application of system. In addition, we provided in-\nterpretable results with improved visualization method beyond attention, which\nis expected to be of great help to clinicians.\nViT for COVID-19 Diagnosis using Low-level CXR Feature Corpus 9\nReferences\n1. Apostolopoulos, I.D., Mpesiana, T.A.: Covid-19: automatic detection from x-ray\nimages utilizing transfer learning with convolutional neural networks. Physical and\nEngineering Sciences in Medicine 43(2), 635–640 (2020)\n2. Bernheim, A., Mei, X., Huang, M., Yang, Y., Fayad, Z.A., Zhang, N., Diao, K., Lin,\nB., Zhu, X., Li, K., et al.: Chest ct ﬁndings in coronavirus disease-19 (covid-19):\nrelationship to duration of infection. Radiology p. 200463 (2020)\n3. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visu-\nalization. arXiv preprint arXiv:2012.09838 (2020)\n4. Chen, L., Min, Y., Zhang, M., Karbasi, A.: More data can expand the general-\nization gap between adversarially robust and standard models. In: International\nConference on Machine Learning. pp. 1670–1680. PMLR (2020)\n5. Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., Sutskever, I.: Gener-\native pretraining from pixels. In: International Conference on Machine Learning.\npp. 1691–1703. PMLR (2020)\n6. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: International conference on machine\nlearning. pp. 1597–1607. PMLR (2020)\n7. Cozzi, D., Albanesi, M., Cavigli, E., Moroni, C., Bindi, A., Luvar` a, S., Lucarini, S.,\nBusoni, S., Mazzoni, L.N., Miele, V.: Chest x-ray in new coronavirus disease 2019\n(covid-19) infection: ﬁndings and correlation with clinical outcome. La radiologia\nmedica 125, 730–737 (2020)\n8. De La Iglesia Vay´ a, M., Saborit, J.M., Montell, J.A., Pertusa, A., Bustos, A.,\nCazorla, M., Galant, J., Barber, X., Orozco-Beltr´ an, D., Garc´ ıa-Garc´ ıa, F., et al.:\nBimcv covid-19+: a large annotated dataset of rx and ct images from covid-19\npatients. arXiv preprint arXiv:2006.01174 (2020)\n9. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n10. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n11. Hemdan, E.E.D., Shouman, M.A., Karar, M.E.: Covidx-net: A framework of\ndeep learning classiﬁers to diagnose covid-19 in x-ray images. arXiv preprint\narXiv:2003.11055 (2020)\n12. Hu, Y., Jacob, J., Parker, G.J., Hawkes, D.J., Hurst, J.R., Stoyanov, D.: The\nchallenges of deploying artiﬁcial intelligence models in a rapidly evolving pandemic.\nNature Machine Intelligence 2(6), 298–300 (2020)\n13. Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H.,\nHaghgoo, B., Ball, R., Shpanskaya, K., et al.: Chexpert: A large chest radiograph\ndataset with uncertainty labels and expert comparison. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence. vol. 33, pp. 590–597 (2019)\n14. Narin, A., Kaya, C., Pamuk, Z.: Automatic detection of coronavirus disease (covid-\n19) using x-ray images and deep convolutional neural networks. arXiv preprint\narXiv:2003.10849 (2020)\n15. Ng, K., Poon, B.H., Kiat Puar, T.H., Shan Quah, J.L., Loh, W.J., Wong, Y.J.,\nTan, T.Y., Raghuram, J.: Covid-19 and the risk to health care workers: a case\nreport. Annals of internal medicine 172(11), 766–767 (2020)\n10 S.Park et al.\n16. Oh, Y., Park, S., Ye, J.C.: Deep learning covid-19 features on cxr using limited\ntraining data sets. IEEE Transactions on Medical Imaging39(8), 2688–2700 (2020)\n17. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language un-\nderstanding by generative pre-training (2018)\n18. Shi, H., Han, X., Jiang, N., Cao, Y., Alwalid, O., Gu, J., Fan, Y., Zheng, C.:\nRadiological ﬁndings from 81 patients with covid-19 pneumonia in wuhan, china:\na descriptive study. The Lancet infectious diseases 20(4), 425–434 (2020)\n19. Signoroni, A., Savardi, M., Benini, S., Adami, N., Leonardi, R., Gibellini, P.,\nVaccher, F., Ravanelli, M., Borghesi, A., Maroldi, R., et al.: End-to-end learn-\ning for semiquantitative rating of covid-19 severity on chest x-rays. arXiv preprint\narXiv:2006.04603 (2020)\n20. Tahamtan, A., Ardebili, A.: Real-time rt-pcr in covid-19 detection: issues aﬀecting\nthe results. Expert review of molecular diagnostics 20(5), 453–454 (2020)\n21. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. arXiv preprint arXiv:1706.03762 (2017)\n22. Wang, L., Lin, Z.Q., Wong, A.: Covid-net: A tailored deep convolutional neural\nnetwork design for detection of covid-19 cases from chest x-ray images. Scientiﬁc\nReports 10(1), 1–12 (2020)\n23. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8:\nHospital-scale chest x-ray database and benchmarks on weakly-supervised classi-\nﬁcation and localization of common thorax diseases. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp. 2097–2106 (2017)\n24. Wang, X., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma, H., Liu, W., Zheng, C.:\nA weakly-supervised framework for covid-19 classiﬁcation and lesion localization\nfrom chest ct. IEEE transactions on medical imaging 39(8), 2615–2625 (2020)\n25. Wong, H.Y.F., Lam, H.Y.S., Fong, A.H.T., Leung, S.T., Chin, T.W.Y., Lo, C.S.Y.,\nLui, M.M.S., Lee, J.C.Y., Chiu, K.W.H., Chung, T.W.H., et al.: Frequency and dis-\ntribution of chest radiographic ﬁndings in patients positive for covid-19. Radiology\n296(2), E72–E78 (2020)\n26. Ye, W., Yao, J., Xue, H., Li, Y.: Weakly supervised lesion localization with\nprobabilistic-cam pooling. arXiv preprint arXiv:2005.14480 (2020)\n27. Zech, J.R., Badgeley, M.A., Liu, M., Costa, A.B., Titano, J.J., Oermann, E.K.:\nVariable generalization performance of a deep learning model to detect pneumo-\nnia in chest radiographs: a cross-sectional study. PLoS medicine 15(11), e1002683\n(2018)\n28. Zhang, J., Xie, Y., Li, Y., Shen, C., Xia, Y.: Covid-19 screening on chest x-ray im-\nages using deep learning based anomaly detection. arXiv preprint arXiv:2003.12338\n(2020)\n29. Zheng, C., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma, H., Liu, W., Wang, X.: Deep\nlearning-based detection for covid-19 from chest ct using weak label. MedRxiv\n(2020)",
  "concepts": [
    {
      "name": "Exploit",
      "score": 0.8094397783279419
    },
    {
      "name": "Transformer",
      "score": 0.7084078192710876
    },
    {
      "name": "Computer science",
      "score": 0.6199126839637756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5844041109085083
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.5715293288230896
    },
    {
      "name": "Embedding",
      "score": 0.5570055842399597
    },
    {
      "name": "Software deployment",
      "score": 0.47431087493896484
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43855154514312744
    },
    {
      "name": "Architecture",
      "score": 0.4279329180717468
    },
    {
      "name": "Training set",
      "score": 0.41713792085647583
    },
    {
      "name": "Transfer of learning",
      "score": 0.4149243235588074
    },
    {
      "name": "Machine learning",
      "score": 0.40616804361343384
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3703007996082306
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.17778927087783813
    },
    {
      "name": "Engineering",
      "score": 0.1578056812286377
    },
    {
      "name": "Computer security",
      "score": 0.13544946908950806
    },
    {
      "name": "Geography",
      "score": 0.11261391639709473
    },
    {
      "name": "Medicine",
      "score": 0.08936905860900879
    },
    {
      "name": "Pathology",
      "score": 0.08027961850166321
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "topic": "Exploit",
  "institutions": [],
  "cited_by": 26
}