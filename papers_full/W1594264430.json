{
  "title": "Statistical Language Models of Lithuanian Based on Word Clustering and Morphological Decomposition",
  "url": "https://openalex.org/W1594264430",
  "year": 2004,
  "authors": [
    {
      "id": "https://openalex.org/A2408646837",
      "name": "Airenas Vaičiūnas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2236040600",
      "name": "Vytautas Kaminskas",
      "affiliations": [
        "Vytautas Magnus University"
      ]
    },
    {
      "id": "https://openalex.org/A4208044182",
      "name": "Gailius Raškinis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2097927681",
    "https://openalex.org/W1549285799",
    "https://openalex.org/W139293362",
    "https://openalex.org/W7071444332",
    "https://openalex.org/W6607467106",
    "https://openalex.org/W6604042305",
    "https://openalex.org/W132379711",
    "https://openalex.org/W1815039817",
    "https://openalex.org/W249480477",
    "https://openalex.org/W2968047920",
    "https://openalex.org/W1579609743",
    "https://openalex.org/W1538155009",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W194103849",
    "https://openalex.org/W1495314497",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W182831726",
    "https://openalex.org/W98731357",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W88864901",
    "https://openalex.org/W4390121167"
  ],
  "abstract": "This paper describes our research on statistical language modeling of Lithuanian.The idea of improving sparse n-gram models of highly inflected Lithuanian language by interpolating them with complex n-gram models based on word clustering and morphological word decomposition was investigated.Words, word base forms and part-of-speech tags were clustered into 50 to 5000 automatically generated classes.Multiple 3-gram and 4-gram class-based language models were built and evaluated on Lithuanian text corpus, which contained 85 million words.Class-based models linearly interpolated with the 3-gram model led up to a 13% reduction in the perplexity compared with the baseline 3-gram model.Morphological models decreased out-of-vocabulary word rate from 1.5% to 1.02%.",
  "full_text": "INFORMATICA, 2004, V ol. 15, No. 4, 565–580 565\n 2004 Institute of Mathematics and Informatics, Vilnius\nStatistical Language Models of Lithuanian Based\non Word Clustering and Morphological\nDecomposition\nAirenas V AIˇCI ¯UNAS, Vytautas KAMINSKAS\nDepartment of Applied Informatics, Vytautas Magnus University\nVileikos 8, LT-3035 Kaunas, Lithuania\ne-mail: airenas@freemail.lt, V .Kaminskas@if.vdu.lt\nGailius RAŠKINIS\nCenter of Computational Linguistics, Vytautas Magnus University\nDonelaiˇcio 52, LT-3000 Kaunas, Lithuania\ne-mail: idgara@vdu.lt\nReceived: March 2004\nAbstract.This paper describes our research on statistical language modeling of Lithuanian. The\nidea of improving sparsen-gram models of highly inﬂected Lithuanian language by interpolating\nthem with complexn-gram models based on word clustering and morphological word decompo-\nsition was investigated. Words, word base forms and part-of-speech tags were clustered into 50 to\n5000 automatically generated classes. Multiple 3-gram and 4-gram class-based language models\nwere built and evaluated on Lithuanian text corpus, which contained 85 million words. Class-based\nmodels linearly interpolated with the 3-gram model led up to a 13% reduction in the perplexity\ncompared with the baseline 3-gram model. Morphological models decreased out-of-vocabulary\nword rate from 1.5% to 1.02%.\nKey words:language models,n-grams, class-based models, morphology, inﬂections, interpolation,\nperplexity reduction, out-of-vocabulary words.\n1. Introduction\nStatistical language modeling attempts to capture and exploit regularities in natural lan-\nguage. Statistical language models (LM) have become key components for large vocabu-\nlary continuous speech recognition (LVCSR) systems. These models provide prior prob-\nabilities that are used torate hypothesized sentences and to disambiguate their acoustical\nsimilarities.\nDuring the last few decades, much experimental work has been done in the ﬁeld of sta-\ntistical language modeling covering widespread world languages such as English, French,\nand German. Unfortunately, statistical language modeling of Lithuanian is still at its in-\nfancy stage due to two primary reasons:\n566 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\n1. There was no demand for Lithuanian LM as there were no attempts to build Lithua-\nnian LVCSR systems. Lithuanian speech recognition research was basically limited\nto solving isolated-word ASR tasks of small to medium vocabulary (Lipeikaet al.,\n2002; Filipoviˇc, 2003; Laurinˇciukait˙e, 2003; Raškinis and Raškinien˙e, 2003).\n2. Until very recently, there were no Lithuanian text corpora large enough to serve as\na basis for building statistical LM.\nLithuanian language is highly inﬂected, i.e., new words are easily formed by inﬂec-\ntional afﬁxation. This property of a language results in difﬁculties of statistical modeling\nknown as huge vocabulary size, model sparseness, high perplexity, and a high out-of-\nvocabulary (OOV) word rate. An attempt to overcome the abovementioned difﬁculties\nby applying word parsing into stems and endings was described in our previous pa-\nper (Vaiˇci¯unas and Raškinis, 2003). In this paper, we investigate alternative approaches\nof modeling highly inﬂected languages, such as conventional class-based modeling and\nmorphological modeling based on word decomposition into word base form and part-of-\nspeech tag.\n1.1.n-gram Language Models\nThe aim of the statistical LM is to return the a priori probability for every word sequence\nw\n1,...,w n. The deﬁnition of joint probability states that:\nP(w1 ...w n)= P(w1)P(w2|w1)P(w3|w1w2) ...P (wn|w1 ...w n−1)\n=\nn∏\ni=1\nP(wi|w1 ...w i−1). (1)\nThough there are no effective methods for calculating the probability (1) accurately,\nas it would require too much data, it can be approximated by a series of probabilities\nbased on a limited number of previous words. LMs built using this approach are called\nn-gram LMs. The most frequently usedn-gram LM is 3-gram LM which is based on the\nconditional probability of seeing one word given the two preceding words:\nP(w1 ...w N ) ≈\nN∏\ni=1\nP(wi|wi−2,wi−1). (2)\nConditional probabilities of a 3-gram LM can be estimated by formula\nˆPW3 (wi|wi−2,wi−1)= C(wi−2,wi−1,wi)\nC(wi−2,wi−1) , (3)\nwhere C(· ) denotes the count function in training corpus.\nBecause any particular training corpus is ﬁnite, then-gram LMs have a very large\nnumber of zero probabilityn-grams that should really have some non-zero probability.\nThis is known as model sparseness problem. Good-Turing, Witten-Bell (Jelinek, 2001)\nStatistical Language Models of Lithuanian 567\nand Knesser-Ney (Chen and Goodman, 1999) smoothing techniques are often applied\nfor re-evaluating the zero-probabilityn-grams. These techniques operate by subtracting\nprobability mass from the observedn-grams and redistributing it to the zero-probability\nn-grams. Backing-off (Katz, 1987) is also used and refers to using a probability estimate\nfrom a more generaln′-gram distribution when the estimate from the speciﬁcn-gram\ndistribution (n′<n ) is non-existent or unreliable.\n1.2.Class-Basedn-gram Language Models\nClass-based modeling is another approach for dealing with model sparseness problem.\nClass-based LM requires all words of a given language to be clustered into equivalence\nclasses. Class-basedn-gram LM can be thought of as a generalization of word-basedn-\ngram model. It estimates the conditional probability of seeing one word given the two\npreceding words as follows:\nˆP\nC(W3)(wi|wi−2,wi−1)= ˆPWC (W)(wi|ci) · ˆPC3(W)(ci|ci−2,c i−1),w i ∈ci. (4)\nHere, ˆPC3(W)(ci|ci−2,c i−1) is the estimate of the conditional probability of seeing class\nci given the two previous classesci−2,c i−1,a n dˆPWC (W)(wi|ci) is obtained from the\n1-gram distribution of words assigned to the classci. The formula (4) describes the case\nwhen every wordwi is assigned just to one classci. Conditional probability can be also\ndeﬁned for the case when words are assigned to multiple classes (Huanget al., 2001).\n1.3.Combining Multiplen-gram Language Models\nStandalone class-based models of type (4) usually perform poorly and must be combined\nwith some other LMs. Linear interpolation is the simplest way of combining statistical\nLMs. For instance, standalone class-based 3-gram modelP\nC(W3)(wi|wi−2,wi−1) and\na word-based 3-gram modelPW3 (wi|wi−2,wi−1) can be interpolated in the following\nway:\nˆPW3+C(W3)(wi|wi−2,wi−1)\n= λ ˆPW3 (wi|wi−2,wi−1)+( 1 − λ) ˆPC(W3)(wi|wi−2,wi−1), (5)\nhere0 ⩽ λ ⩽ 1 is the interpolation parameter optimized on the validation corpus.\n1.4.Evaluating and Comparing Language Models\nStatistical language models are evaluated onthe homogenous corpus partition (called test\ncorpus) that was excluded from model training and optimization. Models are evaluated\nby measuring their perplexity and out-of-vocabulary (OOV) word rate.\n568 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nPerplexityPP refers to how many different equally probable words a statistical LM\nexpects to appear in average for a particular type of a context:\nPP =2 LP , where LP = 1\nn\nn∑\ni=1\nlog P(wi|wi−2,wi−1), (6)\nand n is a count of words in the test corpus.\nOOV rate refers to the percentage of words that are not covered by the language model\nand found within test corpus.\n2. Related Work\nClass-based modeling is among the most popular techniques for reducing huge-\nvocabulary-related sparseness of statistical language models. Class-based LM requires\nall words of a given vocabulary to be clustered into equivalence classes. Both manual and\nautomatic word clustering techniques are used. Automatic word clustering is reported to\noutperform manual (Malteseet al., 2001) but is computationally very expensive. Auto-\nmatic word clustering is often based on iterative hill climbing (Whittaker, 2000) or sim-\nulated annealing (Malteseet al., 2001) search techniques. Standalone class-based mod-\nels usually perform poorly\n1 and are combined with some other LMs. Many researchers\ndemonstrated that linear interpolation (5) of a standalone class-based LM and a stan-\ndard word 3-gram model reduced model perplexity. Perplexity reduction ranging from\n4.48% to 49.6% was reported for English, French, Greek, Italian and Spanish (Maltese\net al., 2001), up to 19% reduction was reported for Russian (Whittaker, 2000). There ex-\nist more sophisticated techniques for combining two statistical LMs, such as log-linear\ninterpolation (Klakow, 1998) and interpolation by dynamically updating model weights\nλ (Kobayashiet al., 1999). The maximum entropy approach (Jelinek, 2001) for combin-\ning LMs is also attractive as it combines features from different and sometimes disparate\nmodels into one model instead of combining models themselves.\nWord decomposition into a sequence of particles is another popular technique for\nreducing vocabulary size, extending coverage of a language model and reducing out-of-\nvocabulary word rate. Words are decomposedeither by means of some string matching\nstrategy, such as longest sufﬁx match principle (Sepesy Maucecet al., 2001; Vaiˇci¯unas\nand Raškinis, 2003), or by means of a morphological analyzer (Siivolaet al., 2001; Ircing\net al., 2001; Martins et al, 1999). Iterative search techniques are often used in a hope to\ndiscover an optimal word decomposition (Whittaker, 2000). Particles themselves may\nrepresent either sub-strings of the original word: morphemes, stems, inﬂectional sufﬁxes\n(Martinset al., 1999, Ircinget al., 2001) or some derivative information about the word,\nsuch as word base form\n2 and its part of speech (Siivolaet al., 2001). Sometimes a single\n1Russian standalone class-based model outperforming standard word 3-gram (Whittaker, 2000) is an ex-\nception to this rule.\n2For instance, the inﬁnitive is the base form of the verb.\nStatistical Language Models of Lithuanian 569\nparticle-based model is built out of the sequence of word particles (Martinset al., 1999).\nAs an alternative, two separate models for word stems and for inﬂectional sufﬁxes can be\nbuilt and then combined into one model (Siivolaet al., 2001; Ircinget al., 2001).\nThe abovementioned word decomposition techniques were reported to reduce the vo-\ncabulary by 29% for Portuguese LM (Martinset al., 1999). The vocabulary of Czech LM\nwas reduced by 58% and OOV rate dropped from 8.56% to 4.62% (Ircinget al., 2001).\nHowever, the perplexity of particle-based models is greater in comparison to word-based\nmodels. Particle-based methods are often paired with complementary techniques such\nas automatic topic detection and topic adaptation. Topic adaptation can be achieved by\ncombining multiple topic-speciﬁc models as reported for Slovenian (Sepesy Maucecet\nal., 2001) and Finnish (Siivolaet al., 2001).\nThough it was shown that in many cases perplexity of a statistical LM embedded\nwithin a LVCSR system correlates with the word error rate of the LVCSR system (Rosen-\nfeld, 2000) it is not always the case for highly inﬂected languages. For instance, changes\nin WER dues to LM of Czech LVCSR system (Ircinget al., 2001) and that of Portuguese\n(Martinset al., 1999) were insigniﬁcant.\n3. Investigation of Statistical Language Models of Lithuanian\n3.1.Modeling Data and Tools\nOur experiments were based on a 84,202,576 word Lithuanian text corpus\n3 which had\nvocabulary ofVW = 1422746 distinct words4. This corpus represented a great variety of\ngenres and topics of the present day written Lithuanian. It included texts from local and\nnational newspapers and journals, law and administrative documents, novels, and books\non such speciﬁc subjects as history and philosophy.\nThe whole corpus was manually divided into three parts: 98% were used for training,\n1% for validation (optimization), and 1% for testing of our models. While subdividing the\ncorpus we tried to keep the same proportions of text genres within training, validation and\ntesting parts. We used some text clearing. All punctuation was removed and all numbers\nwhere replaced by the same tag <num>.\nMajority of our investigations were carried out using CMU-Cambridge Statistical\nLanguage Modeling Toolkit (Clarkson and Rosenfeld, 1997). We have extended this\ntoolkit to handle vocabulariesV\nW > 65k words and developed our own word clustering\ntools. Morphological analysis was performed by morphological lemmatizer of Lithua-\nnian (Zinkeviˇcius, 2000). We have extended its functionality by adding support of the\nmost frequent family names of Lithuanian and foreign origin.\n3The corpus was compiled by the Center of Computational Linguistics at Vytautas Magnus University,\nKaunas, Lithuania (Marcinkeviˇcien˙e, 2000). At the time we were ﬁnishing this article, the size of this corpus\nreached 100 million words.\n4Words correspond to character strings and include misspellings, names, non-Lithuanian words.\n570 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\n3.2.Baseline Wordn-gram Language Model\nOur baseline language model was deﬁned to be the conventional word-based 3-gram\nˆPW3 (wi|wi−2,wi−1) including all singletonn-grams and smoothed using Katz back off\nand Good-Turing discounting techniques (Jelinek, 2001).\nIt can be seen from the Tables 1 and 2 that word 3-gram language models of inﬂected\nfree word order languages (Lithuanian, Russian) have much greater perplexities and OOV\nrates in comparison to less inﬂected ﬁxed word order languages (English). Though LM\nbased on 65k word vocabulary is sufﬁcient for English LVCSR applications (1% OOV\nrate) it cannot be applied to Lithuanian LVCSR (11% OOV rate). Models having lower\nperplexity and OOV rates must be developed for using them within Lithuanian LVCSR\nsystems.\n3.3.Class-basedn-gram Language Models\nPrior to building class-based modelˆPC(W3)(wi|wi−2,wi−1)(4), we clustered all words\ninto equivalence classes. LetV be the vocabulary size,K be the desired number of classes\nand IT be the number of iterations. An iterative hill climbing clustering technique de-\nscribed in (Whittaker, 2000) was used:\nTable 1\nPerplexities and OOV rates of the baseline Lithuanian LM for various vocabulary sizesVW\nV ocabulary,VW Perplexity,PPW3 OOV , %\n65k 414.30 10.92\n100k 449.76 8.25\n200k 512.75 5.05\n400k 570.32 3.07\n500k 588.03 2.58\n800k 621.69 1.82\n1000k 631.09 1.62\nTable 2\nPerplexity and OOV rate of baseline Lithuanian LM forVW =6 5k in comparison with corresponding English\nand Russian (Whitaker, 2000) perplexities and OOV rates\nLanguage Perplexity, PPW3 OOV , %\nLithuanian 414.30 10.92\nRussian 387.40 7.60\nEnglish 208.40 1.10\n\nStatistical Language Models of Lithuanian 571\nCluster (V, K, IT)\nsortwords wi, i=1,... ,V in order of decreasing frequency\nmoveword wito class cii=1,... , K-1\nmoveall remaining words wi, i=K-1,... , V to the class cK\nfora fixed number of iterations IT\nforeach word wi, i=1,... ,V\nforeach class cjj=1,... ,K\nmoveword wito class cj\ncalculate mutual information criteria I\nmoveword wito the class cmingiving minimum I\nHere average mutual information criterionI is deﬁned:\nI =\nK∑\ni=1\nK∑\nj=1\nP(ci,c j)l o gP(cj|ci)\nP(cj) . (7)\nIt can be rewritten using 2-gram estimates by\n⌢\nI = 1\nn\nK∑\ni=1\nK∑\nj=1\nC(ci,c j)l o gC(ci,c j)n\nC(ci)C(cj) , (8)\nwhere C(· ) denotes count function, and n is the total number of words in training corpus.\nThe experiments were conducted using two ﬁxed vocabulary sizesVW =6 5k, 1000k,\nmultiple setsKW = 102, 202, 502, 1002, 2002, 3002, 4002, 50025 of classes and run for\nIT = 2 iterations. Class 3-gramˆPC3(W)(ci|ci−2,c i−1) estimates were smoothed using\nKatz back off and Witten-Bell method (Jurafsky, 2000). Word probability given its class\nwas estimated by\nˆP\nWC (W)(w|c)= C(w)\nC(c) . (9)\nLinear interpolation (5) was used to join class-based and word-based 3-gram models\ntogether. The optimum valueλ was obtained by minimizing perplexity on validation data.\nClustering results are illustrated and the performance of class-basedn-gram models is\ns h o w ni nT a b l e s3t o5 .\nWe run the same clustering algorithm forIT =1 5 iterations and found that perplexity\nstill improves with every iteration but the improvement becomes insigniﬁcant forIT > 2.\nThus IT =2 iterations can be considered enough to reach the local minimum of the\nclustering algorithm (see Fig. 1).\n5Two classes are reserved for unknown words and numbers.\n572 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nTable 3\n10 words of the ﬁrst 4 classes in alphabetical order (VW =6 5k,KW =5 0 0 2,IT =2 )\nClass 10 ﬁrst words of the class in alphabetical order\n1. aiškiu, amžinu, auksiniu, aukštu, aukštuoju, aukšˇciausiuoju, aštriu, baisiu, begaliniu, blogu, . . .\n2. adat ↪a, akmen↪i, aplank↪a, apsiaust↪a, automat↪a, bandel↪e, bat↪a, bokal↪a, bulv↪e, buteliuk↪a , ...\n3. akmeniniai, anoniminiai, antriniai, apatiniai,apsauginiai, augaliniai, biologiniai, branduoliniai,\nbuitiniai, cheminiai, . . .\n4. adom ↪a, albertui, albert↪a, albinui, albin↪a, aldonai, aldon↪a, aleksandrui, aleksandr↪a, alfonsui, . . .\nTable 4\nPerplexities of class-based and interpolated LMs forVW =6 5k\nNumber of\nclasses,KW\nClass model\nperplexity,\nPPC(W3)\nInterpolated\nperplexity,\nPPW3+C(W3)\nImprovement6\nover the baseline\n(631.09), %\nλ∗\n102 2509.28 405.13 2.21 0.19\n202 1957.58 400.73 3.28 0.23\n502 1406.09 393.66 4.98 0.27\n1002 1061.08 388.55 6.22 0.30\n2002 807.93 383.87 7.34 0.30\n3002 696.43 382.20 7.75 0.30\n4002 627.36 381.27 7.97 0.31\n5002 585.09 380.90 8.06 0.31\n6Improvement is calculated:improvement = baseline − perplexity\nbaseline · 100%.\n∗See formula (5) for details.\nTable 5\nPerplexities of class-based and interpolated LMs forVW = 1000k\nNumber of\nclasses,KW\nClass model\nperplexity,\nPPC(W3)\nInterpolated\nperplexity,\nPPW3+C(W3)\nImprovement\nover the baseline\n(414.30), %\nλ\n102 4659.78 607.57 3.73 0.23\n202 3702.17 596.86 5.42 0.26\n502 2637.05 579.23 8.22 0.31\n1002 1987.87 567.06 10.14 0.33\n2002 1499.88 555.53 11.97 0.33\n3002 1276.30 551.26 12.65 0.33\n4002 1144.45 549.10 12.99 0.33\n5002 1058.86 548.81 13.04 0.33\n\nStatistical Language Models of Lithuanian 573\nFig. 1. Impact of number of iterations onthe perplexity of interpolated model (VW =6 5k,KW = 3002).\n3.4.Language Models Based on Morphological Word Decomposition\n3.4.1.Word Decomposition\nMorphological analyzer was at the basis of our word decomposition procedure. Ideal\nmorphological analyzer can be thought of as a procedureM which outputs a pair {s, g}\nfor each input wordw,w h e r es and g are the base form and the part-of-speech (POS) tag\nof the wordw respectively.\nM: w → M(w)= <s ,g>. (10)\nUnfortunately, many Lithuanian words are morphologically ambiguous, and morpho-\nlogical analyzer outputs a set of possible base form/POS tag decompositions{s′\ni,g ′\nij},\ni =1 ,...,m ,j =1 ,...,n i.\nM′: w → M′(w)\n=\n{\n<s ′\n1,g ′\n11 > ,...,<s ′\n1\n,g ′\n1n1 >, < s′\n2,g ′\n21 > ,...,<s ′\nm\n,g ′\nmnm >\n}\n. (11)\nAs the ambiguity could not be solved without some context-aware morphological\ndisambiguator we had to make some arbitrary choices. Base forms was assigned the ﬁrst\nbase form out of the list of base forms returned by morphological analyzer, i.e.,s = s′\n1.\nPOS tag g(henceforth called generalized POS tag or GPOS tag) was assigned the set of\nall POS tags returned by morphological analyzer, i.e.,g = {g′\nij},i =1 ,...,m , j =\n1,...,n i. In addition, certain non-inﬂectedprepositions and conjunctions hadg = s as\nwe thought such words may inﬂuence the inﬂections of the following words (see line\n5 in Table 6). The words rejected by the morphological analyzer hads = w and g =\nunrecognized (see line 6 in Table 6).\n3.4.2.Pure Morphological Language Models\nWe have investigated two language modelsˆPS3×G3S and ˆPS3×(GS+G3) b a s e do nm o r -\nphological word decomposition. ModelˆPS3×(GS+G3) can be thought of as the simpliﬁ-\ncation ofˆPS3×G3S , because ofˆPG3S is replaced by the linear interpolation of two simpler\n574 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nTable 6\nWord decomposition examples\nWord w Base forms POS tags Decomposition M\nvaikas (child) s1 =vaikas g11 =<noun masc. sg. nom.> < s1, {g11} >\nvaikai (children,\ndissipate [you])\ns1 =vaikas\ns2 =vaikyti\ng11 =<noun masc. pl. nom.>\ng12 =<noun masc. pl. voc.>\ng21 =<verb ind. pres. t. sg. 2 pers.>\n<s1, {g11,g 12,g 21} >\nb˙egau (ran [I]) s1 =b˙egti g11 =<verb ind. past t. sg. 1 pers.> <s1, {g11} >\nb˙ego (ran [he/they])s1 =b˙egti g11 =<verb ind. past t. sg. 3 pers.> <s1, {g11,g 12} >\ng12 =<verb ind. past t. pl. 3 pers.>\npo (after) s1 =po g11 =<conj.> <s 1, {s1} >\nAminor ¯ugštys\n(amino acids)\n<w, {unrecognized}>\nmodels:\nˆPS3×G3S(wi|wi−2 ...w i−1)\n= ˆPS3×G3S(sigi|si−2gi−2si−1gi−1)\n≈ ˆPS3 (si|si−2si−1) ˆPG3S(gi|gi−2gi−1si), (12)\nˆPS3×(GS+G3)(wi|wi−2 ...w i−1)\n≈ ˆPS3 (si|si−2si−1)\n(\nλ1 ˆPGS(gi|si)+( 1 − λ1) ˆPG3 (gi|gi−2gi−1)\n)\n, (13)\nwhere\nˆPS3 (si|si−2si−1) is the probability estimate of seeing word base formsi given the\ntwo preceding word base formssi−1 and si−2,\nPG3 (gi|gi−2gi−1) is the probability estimate of seeing GPOS taggi given the two\npreceding GPOS tagsgi−1 and gi−2,\nˆPG3S(gi|gi−2gi−1si) is the probability estimate of seeing GPOS taggi given the word\nbase formsi and the two preceding GPOS tagsgi−1 and gi−2,\nˆPGS(gi|si) is the probability estimate of seeing GPOS taggi given the word base\nform si.\nModels ˆPS3 , ˆPG3S, ˆPGS and ˆPG3 were smoothed using Witten-Bell smoothing tech-\nnique.\nMorphological word decomposition resulted inVS = 371251 distinct base forms and\nVG = 3164 distinct GPOS tags (Table 7).\nBoth morphological models decreased OOV rate from 1.5% to 1.02% at the\nexpense of increased perplexity. ModelˆPS3×G3S achieved lower perplexity than\nmodel ˆPS3×(GS+G3) . WithinˆPS3×(GS+G3) model, the 3-gramˆPG3 was practically ig-\nnored (λ1 =0 .99). This means that the relative frequency of GPOS tag given word’s\nbase form is of much greater importance.\nStatistical Language Models of Lithuanian 575\nTable 7\nPerplexities and OOV rates of pure morphological language models\nVocabulary size\nModel Perplexity, PP OOV %\nVS VG\nˆPS3×G3S 371251 3164 1336.77 1.02\nˆPS3×(GS+G3) 371251 3164 1363.16 1.02\nˆPW3 (baseline) VW 7 = 1157911 644.46 1.50\n7 Baseline LM hadVW = 1157911 instead ofVW = 1422746 words. V ocabulary reduction\nresulted from corpus clearing performed beforemorphology-based modeling: misspelled words,\nforeign words and words that were both rejectedby morphological analyzer and found just once in\nthe training corpus were removed from this new vocabulary.\n3.4.3.Class-based Morphological Language Models\nThe model ˆPS3×G3S(12) was selected for further investigations as it gave lower per-\nplexity thanˆPS3×(GS+G3) . We investigated 5 ways of introducing class-based modeling\nwithin modelˆPS3×G3S . We clustered word base forms and GPOS tags into classes and\nreplaced componentsˆPS3, ˆPG3S of ˆPS3×G3S by class-based models:\nˆP(S3+C3(S))×G3S(wi|wi−2 ...w i−1) ≈\n[\nλ2 ˆPS3 (si|si−2si−1)\n+( 1−λ2) ˆPSC(S)(si|csi) ˆPC(S3)(csi|csi−2csi−1)\n]\n· ˆPG3S(gi|gi−2gi−1si),(14)\nˆPS3×C3(G)S(wi|wi−2 ...w i−1) ≈ ˆPS3 (si|si−2si−1) ˆPGC(G)(gi|cgi)\n× ˆPC(G3)S(cgi|cgi−2cgi−1si), (15)\nˆPS3×(G3S+C3(G)S)(wi|wi−2 ...w i−1) ≈ ˆPS3 (si|si−2si−1)\n×\n[\nλ3 ˆPG3S(gi|gi−2gi−1si)+( 1 − λ3) ˆPGC(G)(gi|cgi)\n× ˆPC(G3)S(cgi|cgi−2cgi−1si)\n]\n, (16)\nˆPS3×(G3S+G3C(S))(wi|wi−2 ...w i−1) ≈ ˆPS3 (si|si−2si−1)\n×\n[\nλ4 ˆPG3S(gi|gi−2gi−1si)+( 1 − λ4) ˆPG3C(S)(gi|gi−2gi−1csi)\n]\n, (17)\nˆP(S3+C3(S))×C3(G)S(wi|wi−2 ...w i−1) ≈\n[\nλ5 ˆPS3 (si|si−2si−1)\n+( 1− λ5) ˆPSC(S)(si|csi) ˆPC(S3)(csi|csi−2csi−1)\n]\n· ˆPGC(G)(gi|cgi)\n× ˆPC(G3)S(cgi|cgi−2cgi−1si), (18)\nwhere\ncsi is the class to which the base formsi of the wordwi is assigned,\ncgi is the class of a GPOS taggi,\nˆPSC(S)(si|csi) is a relative frequency of the base formsi given its classcsi,\nˆPGC(G)(gi|cgi) is a relative frequency of the GPOS taggi given its classcgi,\nˆPC(S3)(csi|csi−2csi−1) is the probability estimate of seeing word base form classcsi\ngiven the two preceding word base form classescsi−1 and csi−2,\n576 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nˆPC(G3)S(cgi|cgi−2cgi−1si) is the probability estimate of seeing class of GPOS tags\ngi given the word base formsi and the two preceding classes of GPOS tagsgi−1 and\ngi−2,\nˆPG3C(S)(gi|gi−2gi−1csi) is the probability estimate of seeing GPOS taggi given the\nword base form classcsi and the two preceding GPOS tagsgi−1 and gi−2,\nWe clusteredVS = 371251 base forms intoKS = 102, 502, 1002, 3002, 5002 classes\nand VG = 3164 GPOS tags intoKG =5 2, 102, 202, 502, 1002, 2002 classes using the\nmethod described in Subsection 3.3. The results were as follows.\nModels ˆP(S3+C3(S))×G3S(14) and ˆP(S3+C3(S))×C3(G)S(18) gave the improvement\nin perplexity as shown in Fig. 2.\nThe greatest improvement in perplexityPP(S3+C3(S))×G3S = 1236 .73 (or 7.48%\nwith respect toPPS3×G3S = 1336.77) was obtained withKS = 3002 classes of word\nbaseforms. Complex modelˆP(S3+C3(S))×C3(G)S was slightly worse thanˆP(S3+C3(S))×G3S.\nThe lowest perplexity obtained with this model wasPP(S3+C3(S))×C3(G)S = 1239.88\nforKG = 2002 and KS = 3002.\nModels ˆPS3×C3(G)S(15) and ˆPS3×(G3S+C3(G)S)(16) based on the use of GPOS tag\nclasses did not give the improvement in perplexity as shown in Fig. 3.\nThe perplexity of modelˆPS3×C3(G)S reached the perplexity of a non-class-based\nmodel ˆPS3×G3S forKG = 2002 GPOS tag classes.\nModel ˆPS3×(G3S+G3C(S))(17) resulted in no signiﬁcant improvement in perplexity\n(PPS3×(G3S+G3C(S)) >P P S3×G3S). The componentˆPG3C(S) of ˆPS3×(G3S+G3C(S))\nwas practically ignored during the interpolation.\nFig. 2. Impact of number of classes of word base forms on the perplexity of interpolated model (14).\nFig. 3. Impact of number of classes of morphological information units on perplexity for model (15).\nStatistical Language Models of Lithuanian 577\n4. Discussion\nOur work described in this paper afﬁrmed the difﬁculties of modeling highly inﬂected\nlanguages, such as huge vocabulary size necessary to achieve moderate OOV word rates,\nmodel sparseness, and high model perplexity. We were not able to achieve 1% OOV rate\neven withVW = 1000k Lithuanian vocabulary.\nOur investigations conﬁrmed that class-based language modeling is helpful in cop-\ning with data sparseness problem. Class-based models improved perplexity estimate by\n13.04% and 8.06% for 1000k and 65k vocabularies respectively. We found that the op-\ntimum number of Lithuanian word classes is somewhere about 3000. Word 3-gram built\nover 1000k word vocabulary is sparser and has worse probability estimates than 3-gram\nbuilt over 65k vocabulary. This explains why class based modeling gives more signiﬁcant\nimprovement in perplexity estimate for bigger vocabularies.\nThe time complexity of one clustering iteration of the algorithm described in 3.3 grew\nas fast asO(V ·K\n2),w h e r eV was the number of distinct words andK was the number of\nclasses. For instance, the average time required for one clustering iteration withK = 102,\nK = 4002 and K = 5002 classes was 0.75h, 150h and 220h respectively (V = 1000k;\nPentium III 935Mhz operating Windows 2000). Fortunately, it appeared that clustering\nalgorithm reached the local maximum and converged almost after 2 iterations. More so-\nphisticated initialization methods or some less greedy clustering techniques may lead to\nperplexity improvements surpassing 13% given we can ﬁnd a way of overcoming the\nlimitations of the computational resources (parallel computing).\nLanguage models based on word decomposition into its base form and its POS tag\ndecreased vocabulary by 67,7% (from 1157911 to 371251+3164) at the expense of in-\ncreased perplexity. We see three possible ways of improving perplexity estimates:\nThe ﬁrst possibility is to decrease the vocabulary of base forms by cleaning text corpus\n(removing garbage, ﬁxing typesetting errors, marking-up sentence boundaries, further\nextending the possibilities of themorphological analyzer).\nThe second possibility is to improve the morphological languagemodel by solving\nthe problem of morphological ambiguity on the basis of contextual information. It is\nclear that our present set of 3164 generalized POS tags is redundant and morphological\ndisambiguation can reduce this set.\nThe third possibility is to use adaptation to text corpus, i.e., to complement language\nmodels described in this paper by adaptive language models such as cache-based models\ntopic-speciﬁc language models, etc.\n5. Conclusions\nIn this paper, we presented our ongoing research on statistical language modeling of\nLithuanian. We investigated the idea of improving sparsen-gram models of highly in-\nﬂected Lithuanian language by interpolating them with complexn-gram models based\non word clustering and morphological word decomposition. Words, word base forms\n578 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nand part-of-speech tags were clustered into 50 to 5000 automatically generated classes.\nComplex 3-gram and 4-gram class-based language models were built and evaluated on\nLithuanian text corpus, which contained 85 million words. Our investigations conﬁrmed\nthat class-based language modeling is helpful in coping with data sparseness problem.\nClass-based models improved perplexity estimate by 13.04% and 8.06% in comparison\nwith the baseline 3-gram model for 1000k and 65k vocabularies respectively. We found\nthat the optimum number of Lithuanian word classes is somewhere about 3000. Language\nmodels based on word decomposition into its base form and its POS tag decreased vo-\ncabulary by 67,7% and out-of-vocabulary word rate from 1,5% to 1.02% at the expense\nof increased perplexity.\nReferences\nChen, S., and J.T. Goodman (1999). An empirical study of smoothing techniques for language modeling.Com-\nputer Speech and Language,13, 359–394.\nClarkson, P., and R. Rosenfeld (1997). Statistical language modeling using the CMU – Cambridge Toolkit. In\nProceedings of 5th European Conference on Speech Communication and Technology,Eurospeech’97.\nFilipoviˇc, M. (2003). Isolated word recognition using neural networks. InProceedings of the Conference “In-\nformacin˙es technologijos 2003”, IX, KTU. pp. 10–20 (in Lithuanian).\nHuang, X., A. Acero and H.W. Hon (2001).Spoken Language Processing. Prentice–Hall, New Jersey.\nIrcing, P., P. Krbec, J. Hajic, J. Psutka, S. Khudanpur, F. Jelinek and W. Byrne (2001). On large vocabulary\ncontinuous speech recognition of highly inﬂectional language – Czech. InProceedings of 7th European\nConference on Speech Communication and Technology,Eurospeech’2001, B14. pp. 487–491.\nJelinek, F. (2001).Statistical Methods for Speech Recognition. Massachusetts Institute of Technology, Cam-\nbridge.\nJurafsky, D., and J.H. Martin (2000).Speech and Language Processing. Prentice–Hall, New Jersey.\nKlakow, D. (1998). Log-linear interpolation of language models. InProceedings of the International Conference\non Spoken Language Processing, Sydney, Australia.\nKobayashi, N., and T. Kobayashi (1999). Class-combined wordN-gram for robust language modeling. InPro-\nceedings of 6th European Conference on Speech Communication and Technology,Eurospeech’99. pp. 1599–\n1602.\nLaurinˇciukait˙e, S. (2003). Isolated Lithuanian word recognition based on hidden Markov models. InProceed-\nings of the Conference “Informacin˙es technologijos 2003”, IX, KTU. pp. 21–24 (in Lithuanian).\nLipeika, A., J. Lipeikien˙e and L. Telksnys (2002). Development of isolated word speech recognition system.\nInformatica,13(1), 37–46.\nMaltese, G., P. Bravetti, H. Crépy, B.J. Grainger,M. Herzog and F. Palou (2001). Combining word- and\nclass-based language models: a comparative study inseveral languages using automatic and manual word-\nclustering techniques. InProceedings of 7th European Conference on Speech Communication and Technol-\nogy,Eurospeech’2001, A32. pp. 21–24.\nMarcinkeviˇcien˙e, R. (2000). Corpus linguistics in theory and practice.Darbai ir Dienos, VDU, 24, 7–64 (in\nLithuanian).http://donelaitis.vdu.lt/\nMartins, C., J.P. Neto and L.B. Almeida (1999). Usingpartial morphological analysis in language modeling\nestimation for large vocabulary portuguese speech recognition. InProceedings of 6th European Conference\non Speech Communication and Technology,Eurospeech’99. pp. 1603–1606.\nRaškinis, G., and D. Raškinien˙e (2003). Building medium-vocabulary isolated-word Lithuanian HMM speech\nrecognition system.Informatica,14(1), 75–84.\nRosenfeld, R. (2000). Two decades of statisticallanguage modeling: where do we go from here. InProceedings\nof the IEEE,88(8).\nSepesy Maucec, M., and Z. Kacic (2001). Topic detection for language model adaptation of highly-inﬂected\nlanguages by using a fuzzy comparison function. InProceedings of 7th European Conference on Speech\nCommunication and Technology,Eurospeech’2001, A42. pp. 243–247.\nStatistical Language Models of Lithuanian 579\nSiivola, V ., M. Kurimo and K. Lagus (2001). Large vocabulary statistical language modeling for continuous\nspeech recognition in Finnish. InProceedings of 7th European Conference on Speech Communication and\nTechnology,Eurospeech’2001, B25. pp. 737–741.\nVaiˇci¯unas, A., and G. Raškinis (2003). Statistical modeling of Lithuanian language. InProceedings of the\nConference “Informacin˙es technologijos 2003”, KTU, IX. pp. 35–40 (in Lithuanian).\nWhittaker, E.W.D. (2000).Statistical Language Modelling for Automatic Speech Recognition of Russian and\nEnglish. PhD thesis. Cambridge University, Cambridge.\nZinkeviˇcius, V . (2000). Lemuoklis – tool for morphological analysis.Darbai ir Dienos, VDU, 24, 245–274 (in\nLithuanian).\nA. Vaiˇci¯unas (born in 1976) received his MSc degree in computer science from the Vy-\ntautas Magnus University in Kaunas in 2000. Presently, he is a PhD student at the same\nuniversity. His research interests are natural language modelling and speech recognition.\nG. Raškinis(born in 1972) received his MSc degree in artiﬁcial intelligence and pattern\nrecognition from the University of Pierre et Marie Curie in Paris in 1995. He received doc-\ntor’s degree in the ﬁeld of informatics (physical sciences) in 2000. Presently, he works at\nthe Center of Computational Linguistics andteaches at the Department of Applied Infor-\nmatics of VMU. His research interests include application of machine learning techniques\nto human language processing.\nV . Kaminskas(born in 1946) graduated from Kaunas Polytechnic Institute in 1968. He\nreceived doctor’s (1972) and habilitated doctor’s (1983) degrees in the ﬁeld of control\nsystems and theory of information. He is an Expert-Member (1991) and a Corresponding\nMember (1998) of the Lithuanian Academy of Sciences. Prof. V . Kaminskas has pub-\nlished 4 books and over 200 scientiﬁc papers. Presently, he is a rector of Vytautas Mag-\nnus University. His research interests include computer aided simulation, identiﬁcation,\ncontrol and diagnostic systems.\n580 A. Vaiˇci¯unas, G. Raškinis, V . Kaminskas\nStatistiniai lietuvi↪u kalbos modeliai, pagr↪isti žodži↪u klasterizacija ir\nžodži↪u morfologiniu išskaidymu\nAirenas V AIˇCI ¯UNAS, Gailius RAŠKINIS, Vytautas KAMINSKAS\nŠiame straipsnyje pateikti lietuvi↪u kalbos statistinio modeliavimo tyrimai. Darbe išnagrin˙eti\ndu b¯udai, kuriais↪imanoma pagerinti smarkiai kaitomos Lietuvi↪u kalbosn-gramos tipo statistinius\nmodelius: kalbos žodži↪u grupavimas↪i klasterius ir morfologinis žodži↪u skaidymas↪i sudedamasias\ndalis. Tyrimo metu žodžiai, žodži↪u pagrindin˙es formos, ir žodži↪u kalbos dalies žym˙es buvo au-\ntomatiškai grupuojamios↪i 50–5000 klasteri↪u. Panaudojant 85 mln. žodži↪u apimties lietuvi↪u kalbos\ntekstyn↪a, buvo sukurti ir↪ivertinti keletas skirting↪u 3-gramos ir 4-gramos tipo statistini↪u mode-\nli↪u, panaudojanˇci↪u informacij↪a apie žodži↪u klasterius. Modeliai, panaudojantys žodži↪u klasterius\ntiesiškai interpoliuoti su↪iprastu 3-gramos tipo modeliu sumažino lietuvi↪u kalbos modelio maišat↪i\n13%. Morfologiniai modeliai sumažino neapr˙epto žodyno dyd↪i nuo 1,5% iki 1,02%.",
  "topic": "Lithuanian",
  "concepts": [
    {
      "name": "Lithuanian",
      "score": 0.889513373374939
    },
    {
      "name": "Computer science",
      "score": 0.7078399658203125
    },
    {
      "name": "Decomposition",
      "score": 0.6877108812332153
    },
    {
      "name": "Natural language processing",
      "score": 0.656714141368866
    },
    {
      "name": "Cluster analysis",
      "score": 0.6100066304206848
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5914087295532227
    },
    {
      "name": "Word (group theory)",
      "score": 0.5512078404426575
    },
    {
      "name": "Linguistics",
      "score": 0.4071439504623413
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188730046",
      "name": "Vytautas Magnus University",
      "country": "LT"
    }
  ],
  "cited_by": 11
}