{
  "title": "Automatic Spelling Correction with Transformer for CTC-based End-to-End Speech Recognition",
  "url": "https://openalex.org/W2941602825",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101777591",
      "name": "Shiliang Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058896884",
      "name": "Ming Lei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061850214",
      "name": "Zhijie Yan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963212250",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1586532344",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2208299922",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2745596852",
    "https://openalex.org/W1571874193",
    "https://openalex.org/W21942408",
    "https://openalex.org/W2345190899",
    "https://openalex.org/W2949174760",
    "https://openalex.org/W2041670430",
    "https://openalex.org/W2972686346",
    "https://openalex.org/W2791396024",
    "https://openalex.org/W2291513470",
    "https://openalex.org/W1633171621",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2797303992",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W1736701665"
  ],
  "abstract": "Connectionist Temporal Classification (CTC) based end-to-end speech recognition system usually need to incorporate an external language model by using WFST-based decoding in order to achieve promising results. This is more essential to Mandarin speech recognition since it owns a special phenomenon, namely homophone, which causes a lot of substitution errors. The linguistic information introduced by language model will help to distinguish these substitution errors. In this work, we propose a transformer based spelling correction model to automatically correct errors especially the substitution errors made by CTC-based Mandarin speech recognition system. Specifically, we investigate using the recognition results generated by CTC-based systems as input and the ground-truth transcriptions as output to train a transformer with encoder-decoder architecture, which is much similar to machine translation. Results in a 20,000 hours Mandarin speech recognition task show that the proposed spelling correction model can achieve a CER of 3.41%, which results in 22.9% and 53.2% relative improvement compared to the baseline CTC-based systems decoded with and without language model respectively.",
  "full_text": "Automatic Spelling Correction with Transformer for CTC-based End-to-End\nSpeech Recognition\nShiliang Zhang, Ming Lei, Zhijie Yan\nMachine Intelligence Technology, Alibaba Group\n{sly.zsl, lm86501, zhijie.yzj}@alibaba-inc.com\nAbstract\nConnectionist Temporal Classiﬁcation (CTC) based end-to-end\nspeech recognition system usually need to incorporate an exter-\nnal language model by using WFST-based decoding in order to\nachieve promising results. This is more essential to Mandarin\nspeech recognition since it owns a special phenomenon, namely\nhomophone, which causes a lot of substitution errors. The lin-\nguistic information introduced by language model will help to\ndistinguish these substitution errors. In this work, we propose\na transformer based spelling correction model to automatically\ncorrect errors especially the substitution errors made by CTC-\nbased Mandarin speech recognition system. Speciﬁcally, we in-\nvestigate using the recognition results generated by CTC-based\nsystems as input and the ground-truth transcriptions as output\nto train a transformer with encoder-decoder architecture, which\nis much similar to machine translation. Results in a 20,000\nhours Mandarin speech recognition task show that the proposed\nspelling correction model can achieve a CER of 3.41%, which\nresults in 22.9% and 53.2% relative improvement compared to\nthe baseline CTC-based systems decoded with and without lan-\nguage model respectively.\nIndex Terms: speech recognition, spelling correction, CTC,\nEnd-to-End, Transformer\n1. Introduction\nConventional hybrid DNN-HMM based speech recognition sys-\ntem usually consists of acoustic, pronunciation and language\nmodels. These components are trained separately, each with a\ndifferent objective, and then combined together during model\ninference. Recent works in this area attempt to rectify this\ndisjoint training problem and simplify the training process by\nbuilding speech recognition system in the so-called end-to-end\nframework [1, 2, 3, 4, 5, 6, 7, 8, 9]. Two popular approaches for\nthis are the Connectionist Temporal Classiﬁcation (CTC) [10]\nand attention-based encoder-decoder models [11]. Both meth-\nods regard speech recognition as a sequence-to-sequence map-\nping problem and address the problem of variable-length input\nand output sequences. CTC uses intermediate label represen-\ntation allowing repetitions of labels and occurrences of blank\nlabel to identify less informative frames, which enables CTC-\nbased acoustic models to automatically learn the alignments be-\ntween speech frames and target labels. On the other hand, atten-\ntion based models use an attention mechanism to perform align-\nment between acoustic frames and recognized symbols. Both\nmethods do not require frame-level training targets, which sim-\npliﬁes the training process of speech recognition system.\nCTC assumes that the label outputs are conditionally in-\ndependent of each other, which can be seen as an acoustic-\nonly model. Although CTC-based models can directly gener-\nate the recognition results by using the greedy search decoding\n[10], it’s better to incorporate an external language model at\nthe character or word level by using the WFST-based decod-\ning [12, 13]. On the other hand, attention-based models with\nencoder-decoder framework can jointly learn acoustic, pronun-\nciation and language models. As a result, it is widely ob-\nserved that attention-based models will achieve better perfor-\nmance than CTC-based models when decoded without external\nlanguage model [7]. However, the language model component\nin attention-based models is only trained on transcribed audio-\ntext pairs. Further improvements can achieve by incorporating\nan external language model at inference time [14].\nMandarin is a tonal language with a special phenomenon,\nnamely homophone, which many Chinese characters share the\nsame pronunciation. As a result, the substitution errors are the\ndominant error made by Mandarin speech recognition system\n[15]. These substitution errors require linguistic information to\ndistinguish effectively. Thereby, the language model is essen-\ntial to CTC-based models for Mandarin speech recognition. As\nshown in [13], the performance gap between CTC-based mod-\nels decoded with and without external language model is huge.\nHowever, the language model accompanied with CTC-based\nacoustic models is usually the n-gram language model, which\nhas limited history context information. Further improvement\ncan be obtained by using N-best rescoring with an RNN-LM\n[16, 17].\nIn this work, we propose a transformer [18] based spelling\ncorrection model to automatically correct some errors made by\nthe CTC-based speech recognition system. Speciﬁcally, we in-\nvestigate using the recognition results generated by CTC-based\nsystems as input and the ground-truth transcriptions as output\nto train a transformer with encoder-decoder architecture, which\nis similar to machine translation. During inference, the spelling\ncorrection model takes the preliminary recognition results as in-\nput and generates the ﬁnal results with greedy search. We have\ninvestigated various CTC-based systems as front-end: differ-\nent acoustic modeling units (syllable, character-2k, character-\n4k, character-6k), different optimization criteria (CTC, CTC-\nsMBR) and decoding methods (greedy search, WFST search).\nMoreover, we have proposed to extend the diversity of training\ndata by using N-best lists and used the SGDR [19] optimization,\nwhich will signiﬁcantly improve the performance.\nWe have evaluated our proposed approach on a 20,000\nhours Mandarin speech recognition task that consists of about\n20 millions paired sentences. Our results show that the pro-\nposed spelling correction models can improve the performance\nof CTC system with greedy search from 7.28% to 4.89% in\ncharacter error rate (CER). And it can further improve to 4.21%\nby extending N-best lists as training data. As a compari-\nson, the performance of a well-trained CTC-sMBR system us-\ning WFST-based decoding with external word-level language\nmodel is 4.42%. Moreover, by jointly using character based\nacoustic modeling units, DFSMN-CTC-sMBR acoustic model,\nWFST-based decoding and N-best data expansion, the proposed\narXiv:1904.10045v1  [eess.AS]  27 Mar 2019\nDFSMN-CTC-sMBR\n(syllable, \ncharacter)\nTransformer\nGreedy search\nWFST search\nN-best \nSequenceAcoustic \nfeature\nListener Decoder Speller\nN-gramText\nFigure 1: Overall system of the proposed approach.\nspelling correction models can achieve a CER of 3.41%, which\nresults in a 22.9% relative improvement. Our analysis show\nthat the transformer based spelling correction model can signif-\nicantly reduce substitution errors in recognition results, due to\nit can utilize the sentence-level linguistic information.\n2. Related Works\nAutomatic correction of recognition errors is crucial not only to\nimprove the performance of ASR system but also to avoid the\npropagation of errors to the post process (e.g. machine trans-\nlation, natural language processing). [20] presents an overview\nof previous work on error correction for ASR. However, most\nof researches were limited to the detection [21, 22, 23] and just\nfew researches addressed the correction process of ASR errors.\nIn [24], it built an ASR errors detector and corrector using co-\noccurrence analysis. [25] proposed a post-editing ASR errors\ncorrection method based on Microsoft N-gram dataset. More\nrecently, work in [26, 27] proposed to use the attention based\nsequence-to-sequence model to automatically correct the ASR\nerrors, which is much similar to our work.\n3. Our Approach\nFigure 1 demonstrates the overall system of the proposed ap-\nproach, which consists of three components: listener, decoder\nand speller. For listener, we use the DFSMN-CTC-sMBR [15]\nbased acoustic model. As to decoder, we compare the greedy\nsearch [10] and WFST search [12] based decoding strategies to\ngenerate preliminary recognition results given static sequence\nof probabilities generated by the listener. Moreover, we also in-\nvestigate how to extend the diversity of preliminary recognition\nresults with candidate N-best. Finally, the outputs generated by\nthe decoder are used to train a transformer based speller.\n3.1. Listener\n3.1.1. DFSMN-CTC-sMBR\nDFSMN [28] is an improved FSMN [29] structure that enables\nto build extreme deep architecture by introducing skip connec-\ntions. As shown in Figure 2, it is a DFSMN with 10 DFSMN-\ncomponents followed by 2 fully-connected ReLU layers, a lin-\near layer and a softmax output layer. For CTC based acoustic\nmodel, the softmax output corresponds to the acoustic model-\ning units and a blank unit. The key element in DFSMN is the\nlearnable FIR-like memory blocks, which are used to encode\nlong context information into ﬁxed-size representation. As a\nresult, DFSMN is able to model the long-term dependency in\nsequential signals while without using recurrent feedback. The\noperation in ℓ-th memory block takes the following form:\nmℓ\nt = mℓ−1\nt +pℓ\nt+\nNℓ\n1∑\ni=0\naℓ\ni⊙pℓ\nt−s1∗i+\nNℓ\n2∑\nj=1\ncℓ\nj⊙pℓ\nt+s2∗j (1)\nLinear Layer: 512\nReLU Layer: 2048\nInput Layer: 5x80\nReLU Layer: 2048\nLinear Layer: 512 Memory block 1: 512\nReLU Layer: 2048\nLinear Layer: 512 Memory block 2: 512\nReLU Layer: 2048\nLinear Layer: 512 Memory block 10: 512\nReLU Layer: 2048\n⋮\n⋮\n- ⋯\nDFSMN\nComponent\nAcoustic \nModeling UnitsBlank\n10\nDFSMN\nComponents\nFigure 2: Illustration of DFSMN-CTC-sMBR acoustic model.\nHere, pℓ\nt denote the outputs of the linear projection layer and\nmℓ\nt denotes the output of the memory block. Nℓ\n1 and Nℓ\n2 de-\nnotes the look-back order and lookahead order of the memory\nblock, respectively. s1 is the stride factor of look-back ﬁlter and\ns2 is the stride of lookahead ﬁlter.\nConnectionist temporal classiﬁcation (CTC) [10] is a loss\nfunction for sequence labeling problems that converts the se-\nquence of labels with timing information into the shorter se-\nquence of labels by removing timing and alignment informa-\ntion. The main idea is to introduce the additional CTC blank\n(–) label during training, and then remove the blank labels and\nmerging repeating labels to obtain the unique corresponding se-\nquence during decoding. For a set of target labels, Ω, and its\nextended CTC target set is deﬁned as ¯Ω = Ω∪{–}. Given an\ninput sequence x and its corresponding output label sequence\ny. The CTC path, π, is deﬁned as a sequence over ¯Ω, π∈ ¯ΩT,\nwhere T is the length of the input sequence x. The label se-\nquence y can be represented by a set of all possible CTC paths,\nΦ(y), that are mapped to y with a sequence to sequence map-\nping function F, y = F(Φ(y)). Thereby, the log-likelihood of\nreference label sequence y given the input x can be calculated\nas an aggregation of the probabilities of all possible CTC paths:\np(y|x) =\n∑\nπ∈Φ(y)\np(π|x) (2)\nModel training can then be carried out by minimizing the\nnegative log-likelihood. Furthermore, CTC trained acoustic\nmodel can be further optimized with sequence-level discrimi-\nnative training criteria such as state-level minimum Bayes risk\n(sMBR) criterion [30, 28].\n3.1.2. Acoustic modeling units\nIn [15], it have investigated the performance of DFSMN-CTC-\nsMBR acoustic models with CI-IF, CD-IF, syllable and hy-\nbrid Character-Syllable as modeling units for Mandarin speech\nrecognition. Experimental results suggest that the hybrid\nCharacter-Syllable modeling units, which mixed the high fre-\nquency Chinese characters and syllables, is the best choice for\nMandarin speech recognition. For hybrid Character-Syllable,\nthe low frequency characters are mapped into the syllables to\ndeal with the OOV problem. In this work, instead of mapping\nTable 1: Model architecture of the Transformers used in this\nwork.\nTransformer N dmodel dff h dk dv\nsmall 3 512 2048 4 512 512\nbig 6 512 2048 8 512 512\nthe low frequency characters into syllables, we propose to map\nthem into high frequency characters with the same pronunci-\nation. As a result, we come up with a pure Chinese charac-\nters based modeling units without OOV for Mandarin speech\nrecognition. Speciﬁcally, we keep the top 2000, 4000 and 6000\nChinese characters as acoustic modeling units, denoted aschar-\n2k, char-4k and char-6k respectively. The coverages is 95.58%,\n99.54% and 99.86% in our text dataset, respectively.\n3.2. Decoder\n3.2.1. Greedy search\nFor greedy search [10] based decoding, the most likely symbol\nat each time step is chose as the output. The best CTC-path can\nbe generated as followings:\nπ∗= arg max\nπ\nN∏\nt=1\nPAM(πt|x) (3)\nFurthermore, the CTC-path can mapped to the token sequence\nby using the mapping function F.\ny = F(π∗) (4)\nFor Chinese character units based CTC acoustic model, the to-\nken sequence is the ﬁnal recognition results. For syllable or\nother modeling units, it still need another mapping function.\n3.2.2. WFST search\nThe WFST-based decoding is proposed in [12] that enables ef-\nﬁcient incorporation of lexicons and language models into CTC\ndecoding. The search graph is built by composing the language\nmodel WFST G, lexicon WFST L and token WFST T. The to-\nken WFST T maps a sequence of frame-level CTC labels to a\nsingle lexicon unit. The overall oder of the FST operations is:\nS = T ◦min(det(L◦G)) (5)\nwhere ◦, det and min denote composition, determinization and\nminimization respectively. The search graph S encodes the\nmapping from a sequence of CTC labels emitted on speech\nframes to the ﬁnal transcription. The best decoding path can\nthen be exported from the search graphSusing the beam search.\n3.2.3. N-best data expansion\nFor both greedy search and WFST search based decoding, we\nusually take the best path as recognition result. In our work,\nwe ﬁnd the diversity of training data is important to the perfor-\nmance of spelling correction model. Thereby, we investigate\nto extend the diversity of training data using the N-best lists.\nFor WFST-based decoding, we can easily get the top N paths\nfrom the decoding lattice. As to greedy search based decoding,\nwe propose a threshold-based path retention method. At each\ntime-step, in addition to retaining the token with the highest\nposterior probability ( p1), we judge whether to retain the sec-\nond token (posterior probability, p2) based on two thresholds\nTable 2: CER (%) of CTC based ASR systems with various\nacoustic modeling units and decoding methods (Greedy search\nVs. WFST search).\nExp Modeling units Criterion Greedy WFST\nSearch Search\n1 syllable CTC - 5.55\n2 char-2k CTC 11.93 5.21\n3 char-4k CTC 7.28 5.20\n4 +SMBR 8.48 4.42\n5 char-6k CTC 7.33 5.25\nTable 3: Performance of greedy search CTC with speller. (1M:\n1 million sentences.)\nExp Modeling units Transformer Data CER%\n1 syllable small 1M 8.17\n20M 6.40\n2 char-2k small 20M 6.25\n3 char-4k small 20M 5.70\n4 char-4k big 20M 5.55\n5 char-6k big 20M 5.65\n(upper th,lower th). if lower th < p1 < upperth and\nlower th<p 2, we will keep both tokens. Otherwise, we only\nkeep the top one token. Based on different thresholds, we will\ngenerate different CTC paths that can further mapped to token\nsequence with Eq.4.\nIn our work, we also investigate to extend the training data\nusing data augmentation with the text-only data. We try to add\ninsertion, deletion and substitution errors to the original text\nbased on a probability distribution. Unfortunately, this method\ndoesn’t work well. We suspect that constructed errors can’t re-\nally simulate the types of error produced by the acoustic model.\n3.3. Speller\nFor spelling correction model (speller), we use the Transformer\nwith encode-decoder architecture, which is much similar to ma-\nchine translation model in [18]. The preliminary recognition\nresults generated by the front-end CTC-based acoustic models\nwith different decoding and data expansion methods are used\nas input and the ground-truth transcriptions are used as output\nto train the speller. We use the OpenNMT toolkit [31] to train\nthe Transformer based speller with default setting.Specially, we\nhave compared two Transformer architectures, denoted assmall\nand big. The detailed conﬁgurations are as shown in Table 1.\n4. Experiments\n4.1. Experimental setup\nWe conduct our experiments on a large Mandarin speech recog-\nnition task that consists of about 20,000 hours training data\nwith about 20 million sentences. A test set contains about 10\nhours data is used to evaluated the performance of all models.\nAcoustic feature used for all experiments are 80-dimensional\nlog-mel ﬁlter-bank (FBK) energies computed on 25ms window\nwith 10ms shift. We stack the consecutive frames within a\ncontext window of 5 (2+1+2) to produce the 400-dimensional\nfeatures and then down-sample the inputs frame rate to 30ms.\nFor WFST-based decoding, a pruned trigram language model\ntrained with the text data is used. Evaluations are performed in\nTable 4: Performance of speller trained with data generated by\ngreedy search CTC and threshold-based data expansion.\nTraining data Steps/Pass Pass1 Pass2 Pass3 Pass4\nD1 100000 5.55 5.18 5.02 4.89\nD1-3 200000 4.73 4.68 4.46 4.36\nD1-6 400000 4.62 4.38 4.28 4.21\nTable 5: Performance of spellers trained with data generated by\nWFST search CTC and N-best data expansion.\nTraining data Steps/Pass Pass1 Pass2 Pass3 Pass4\nN-best(1) 100000 4.14 4.01 3.98 3.91\nN-best(5) 250000 3.79 3.69 3.60 3.54\nN-best(10) 350000 3.72 3.61 3.50 3.41\nterm of character error rate (CER). For all experiments, we use\nthe same DFSMN architecture as in [32]. CTC-based acoustic\nmodel is trained in a distributed manner using 16 GPUs and the\nTransformer based speller is trained using 2 GPUs.\n4.2. CTC baseline system\nThe performance of various baseline CTC-based ASR systems\nwith different acoustic modeling units and decoding methods\nare shown in Table 2. CTC-based models decoded with WFST\nsearch perform much better than greedy search, which indi-\ncates the importance of linguistic information. Chinese char-\nacter modeling units based CTC models with different numbers\nof characters (2k, 4k, 6k) can achieve similar performance when\nusing WFST-based decoding, and all can outperform the sylla-\nble based CTC model. However, when decoded with greedy\nsearch, char-4k and char-6k based CTC models signiﬁcantly\noutperform the char-2k based CTC model. This experimental\nresult also indicates the importance of linguistic information.\nAs to sMBR training, we ﬁnd that it improves the performance\nwhen using WFST based decoding while hurt the performance\nwhen using greedy search based decoding. This is due to the\nmismatch between training and decoding, since sMBR based\ntraining uses the WFST-based decoding to generate the training\nlattices.\n4.3. Greedy search CTC with speller\nFirst, we evaluate the performance of spellers trained with the\noutput of baseline CTC models in Table 2 using greedy search\nbased decoding. Comparison of the experimental results ofexp2\nand exp3 in Table 3 and Table 2 shows that good preliminary\nrecognition result will lead to better ﬁnal result. Moreover, the\namount of training data is essential to the performance. As\nshown in Table 3, increasing the training data from 1 million\n(1M) sentences to 20M sentences will result in more than 20%\nrelative improvement. Thereby, we also investigate how to ex-\ntend the training data in this work. Since the big Transformer\nbased speller performs better than thesmall one, we will use the\nbig one in our following experiments.\nIn Sec.3.2.3, we have introduced the threshold-based data\nexpansion method for CTC-based acoustic model with greedy\nsearch. In this experiment, we use the baseline models in Ta-\nble 3 with different thresholds to generate various training data,\ndenoted as D(AM upper thlower th). Specially, we use the\nCTC model ( exp3) and CTC-sMBR model ( exp4) to generate\nsix datasets, denoted as D1 (CTC 1.0 1.0), D2(CTC 0.5 0.1),\nD3(CTC 0.6 0.1), D4(CTC-sMBR 1.0 1.0), D5(CTC-sMBR 0.5\n1056\n629 586 550\n886 740 857 655\n5599\n3205 2920\n2329\n0\n1000\n2000\n3000\n4000\n5000\n6000\nCTC+Geeedy Search CTC-sMBR+WFST Search CTC+Geeedy\nSearch+Speller(D1-6)\nCTC-sMBR+WFST\nSearch+Speller(N-best10)\nins del sub\nFigure 3: Error analysis of various systems.\nDFSMN-CTC-sMBR + WFST-based Decoder + Speller\n1) 我 们 一 直 是 按 双 轨 制 在 推 进 一\n个 鬼 叫 公 共 鬼 另 外 一 个 轨 叫 商 业\n鬼\n(We have been pushing on a two-track system one\nghost is called the public ghost and the other is\ncalled the commercial ghost)\n2) 自 古 就 有 李 舍 之 说 李 和 射 分 别\n是 我 们 中 国 古 代 六 艺 之 一 包 括 礼\n乐 射 御 书 数\n3) 丰 富 的 多 酚 物 质 对 于 促 使 黑 色\n素 生 成 的 酪 胺 酸 酶 具 有 意 志 的 作\n用 因 此 可 以 在 一 定 程 度 上 预 防 皮\n肤 变 黑\n1) 我 们 一 直 是 按 双 轨 制 在 推 进 一\n个 轨 叫 公 共 轨 另 外 一 个 轨 叫 商 业\n轨\n(We have been pushing on a two-track system one\ntrack is called the public track and the other is called\nthe commercial track)\n2) 自 古 就 有 礼 射 之 说 礼 和 射 分 别\n是 我 们 中 国 古 代 六 艺 之 一 包 括 礼\n乐 射 御 书 数\n3) 丰 富 的 多 酚 物 质 对 于 促 使 黑 色\n素 生 成 的 酪 氨 酸 酶 具 有 抑 制 的 作\n用 因 此 可 以 在一 定 程 度 上 预 防 皮\n肤 变 黑\nFigure 4: Examples of recognition result with and without\nspeller.\n0.3), D6(CTC-sMBR 0.6 0.3). Different data conﬁgurations and\nexperimental results are as shown in Table 4. Inspired by the\nSGDR [19], we propose to train the speller with 4 passes and\nreset the learning rate after each pass. The training steps of\neach pass is determined by the amount of training data. For\nmodel inference, we use the results of CTC-based model with\ngreedy search as input and generate the ﬁnal recognition result\nby also using greedy search. Results show that this optimization\nmethod will achieve a better convergence performance. More\nimportantly, data expansion will signiﬁcantly improve the per-\nformance of speller even using the training data generated by\nCTC-sMBR model. As a result, we can achieve a character er-\nror rate (CER) of 4.21%, which is 42.17% relative improvement\ncompared to the baseline greedy search decoded CTC model\n(exp3 in Table 4.2). It can also outperform the CTC-sMBR\nmodel decoded with external language model using the WFST-\nbased decoding.\n4.4. WFST search CTC with speller\nIn this experiment, we use the DFSMN-CTC-sMBR listener\n(exp4 in Table 4.2) with WFST-based decoder to generate the\npreliminary recognition results, which are then paired with the\nground-truth transcriptions to train the speller. We also extend\nthe training data with N-best lists in decoding lattice. Specially,\nwe keep the top 1, 5 and 10 paths that result in three training\nsets, denoted as N-best(1), N-best(5) and N-best(10). These\nthree training sets contain of about 20 million (20M), 55M and\n84M sentences respectively 1. We list the experimental con-\nﬁgurations and results in Table 5. During inference, the best\npaths from the WFST-based decoder are fed into the speller to\ngenerate the ﬁnal recognition result with greedy search. The\nexperimental conclusions are the same to greedy search CTC\n1There are less than 10 paths in the decoding lattice for many sen-\ntences in the training set.\n3.49\n8.22\n18.42\n16.15\n19.7\n13.65\n22.95\n15.94\n10.95\n28.2\n25.61 24.94\n21.05\n0\n5\n10\n15\n20\n25\n30\ntest1 test2 test3 test4 test5 test6 test7 test8 test9 test10 test11 test12 test13\nbaseline + Speller Relative Gain%\nFigure 5: Performance of systems with and without speller in various test sets.\nbased experiments in Sec.4.3 that N-best based data expansion\nmethod and SGDR based multi-pass training can both signif-\nicantly improve the performance. Finally, the speller trained\nwith N-best(10) training set achieves a CER of 3.41% while\nthe performance of baseline DFSMN-CTC-sMBR system with\nWFST-based decoding is 4.42%(in Table 2).\n4.5. Analysis\nIn order to understand the role of speller, as shown in Figure 3,\nwe plot the number of insertion, deletion and substitution errors\nin the recognition results of systems with and without speller.\nResults demonstrate that the speller can automatic correct many\nsubstitution errors made by the front-end listener no matter de-\ncoded with or without external language model. Figure 4 shows\nsome representative examples in the test set. Results demon-\nstrate that the speller is able to utilize the sentence-level linguis-\ntic information and learn knowledge base from the training set,\nwhich is helpful to distinguish homophone in Mandarin. More-\nover, we also make an extensive evaluation of the speller. Figure\n5 shows the performance of baseline DFSMN-CTC-sMBR sys-\ntem with and without speller in 13 test sets. We sort the test sets\nwith the CER(%) of baseline system from high to low. Results\nshow that usually the better the baseline, the more performance\ngains you can achieve by using speller. This is because speller\nneeds to utilize the context information. If there exists many\nerrors in the original recognition results, it will increase the dif-\nﬁculty of error correction.\n5. Conclusions\nIn this work, we propose a transformer based spelling correc-\ntion model with encode-decoder architecture to automatically\ncorrect errors made by CTC-based speech recognition system.\nExperimental results show that the speller is able to utilize the\nsentence-level linguistic information, which will help to sig-\nniﬁcantly reduce the substitution errors in the recognition re-\nsults. Moreover, we propose to extend the diversity of training\ndata using the N-best based data expansion method that results\nin more than 10% relative improvement. Results in a 20,000\nhours Mandarin speech recognition task show that the proposed\nspelling correction model can achieve a CER of 3.41%, which\nresults in 22.9% and 53.2% relative improvement compared to\nthe baseline CTC-based systems decoded with and without lan-\nguage model respectively. As to future work, we will investi-\ngate to utilize the text-only data that will signiﬁcantly extend\nthe diversity of training data.\n6. References\n[1] A. Graves and N. Jaitly, “Towards end-to-end speech recognition\nwith recurrent neural networks,” in International Conference on\nMachine Learning, 2014, pp. 1764–1772.\n[2] J. Chorowski, D. Bahdanau, K. Cho, and Y . Bengio, “End-to-\nend continuous speech recognition using attention-based recurrent\nNN: First results,” arXiv preprint arXiv:1412.1602, 2014.\n[3] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al.,\n“Deep speech: Scaling up end-to-end speech recognition,” arXiv\npreprint arXiv:1412.5567, 2014.\n[4] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben-\ngio, “Attention-based models for speech recognition,” in Ad-\nvances in neural information processing systems, 2015, pp. 577–\n585.\n[5] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,” in 2016 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2016,\npp. 4960–4964.\n[6] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Bat-\ntenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen\net al., “Deep speech 2: End-to-end speech recognition in english\nand mandarin,” in International conference on machine learning,\n2016, pp. 173–182.\n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Ben-\ngio, “End-to-end attention-based large vocabulary speech recog-\nnition,” in 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2016, pp. 4945–\n4949.\n[8] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based\nend-to-end speech recognition using multi-task learning,” in2017\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2017, pp. 4835–4839.\n[9] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Goninaet al., “State-\nof-the-art speech recognition with sequence-to-sequence models,”\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 4774–4778.\n[10] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Con-\nnectionist temporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks,” in Proceedings of\nthe 23rd international conference on Machine learning. ACM,\n2006, pp. 369–376.\n[11] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[12] Y . Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-end\nspeech recognition using deep RNN models and WFST-based de-\ncoding,” in 2015 IEEE Workshop on Automatic Speech Recogni-\ntion and Understanding (ASRU). IEEE, 2015, pp. 167–174.\n[13] J. Li, H. Zhang, X. Cai, and B. Xu, “Towards end-to-end speech\nrecognition for chinese mandarin using long short-term memory\nrecurrent neural networks,” in Sixteenth annual conference of the\ninternational speech communication association, 2015.\n[14] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, “An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,” in 2018 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2018, pp. 1–5828.\n[15] S. Zhang, M. Lei, Y . Liu, and W. Li, “Investigation of model-\ning units for mandarin speech recognition using DFSMN-CTC-\nSMBR,” in 2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2019.\n[16] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. ˇCernock`y, and S. Khu-\ndanpur, “Recurrent neural network based language model,” in\nEleventh annual conference of the international speech commu-\nnication association, 2010.\n[17] X. Liu, X. Chen, Y . Wang, M. J. Gales, and P. C. Woodland, “Two\nefﬁcient lattice rescoring methods using recurrent neural network\nlanguage models,” IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 24, no. 8, pp. 1438–1449, 2016.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n5998–6008.\n[19] I. Loshchilov and F. Hutter, “SGDR: Stochastic gradient descent\nwith warm restarts,”arXiv preprint arXiv:1608.03983, 2016.\n[20] R. Errattahi, A. El Hannani, and H. Ouahmane, “Automatic\nspeech recognition errors detection and correction: A review,”\nProcedia Computer Science, vol. 128, pp. 32–37, 2018.\n[21] L. Zhou, Y . Shi, J. Feng, and A. Sears, “Data mining for detect-\ning errors in dictation speech recognition,” IEEE transactions on\nspeech and audio processing, vol. 13, no. 5, pp. 681–688, 2005.\n[22] A. Allauzen, “Error detection in confusion network,” in Eighth\nAnnual Conference of the International Speech Communication\nAssociation, 2007.\n[23] T. Pellegrini and I. Trancoso, “Error detection in broadcast news\nasr using markov chains,” in Language and Technology Confer-\nence. Springer, 2009, pp. 59–69.\n[24] A. Sarma and D. D. Palmer, “Context-based speech recognition\nerror detection and correction,” in Proceedings of HLT-NAACL\n2004: Short Papers. Association for Computational Linguistics,\n2004, pp. 85–88.\n[25] Y . Bassil and P. Semaan, “ASR context-sensitive error cor-\nrection based on microsoft n-gram dataset,” arXiv preprint\narXiv:1203.5262, 2012.\n[26] T. Zenkel, R. Sanabria, F. Metze, J. Niehues, M. Sperber,\nS. St¨uker, and A. Waibel, “Comparison of decoding strategies for\nctc acoustic models,” arXiv preprint arXiv:1708.04469, 2017.\n[27] J. Guo, T. N. Sainath, and R. J. Weiss, “A spelling correc-\ntion model for end-to-end speech recognition,” arXiv preprint\narXiv:1902.07178, 2019.\n[28] S. Zhang, M. Lei, Z. Yan, and L. Dai, “Deep-FSMN for large\nvocabulary continuous speech recognition,” in 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 5869–5873.\n[29] S. Zhang, C. Liu, H. Jiang, S. Wei, L. Dai, and Y . Hu, “Feed-\nforward sequential memory networks: A new structure to learn\nlong-term dependency,”arXiv preprint arXiv:1512.08301, 2015.\n[30] H. Sak, F. de Chaumont Quitry, T. Sainath, K. Rao et al.,\n“Acoustic modelling with cd-ctc-smbr lstm rnns,” in 2015 IEEE\nWorkshop on Automatic Speech Recognition and Understanding\n(ASRU). IEEE, 2015, pp. 604–609.\n[31] G. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush, “OpenNMT: Open-source toolkit for neural machine\ntranslation,” in Proc. ACL, 2017. [Online]. Available: https:\n//doi.org/10.18653/v1/P17-4012\n[32] S. Zhang and M. Lei, “Acoustic modeling with DFSMN-CTC and\njoint CTC-CE learning,” Proc. Interspeech 2018, pp. 771–775,\n2018.",
  "topic": "Spelling",
  "concepts": [
    {
      "name": "Spelling",
      "score": 0.7980260848999023
    },
    {
      "name": "End-to-end principle",
      "score": 0.6504713892936707
    },
    {
      "name": "Transformer",
      "score": 0.6063966751098633
    },
    {
      "name": "Speech recognition",
      "score": 0.5745581984519958
    },
    {
      "name": "Computer science",
      "score": 0.5381582379341125
    },
    {
      "name": "Natural language processing",
      "score": 0.3643531799316406
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2848249673843384
    },
    {
      "name": "Linguistics",
      "score": 0.19709500670433044
    },
    {
      "name": "Engineering",
      "score": 0.16185876727104187
    },
    {
      "name": "Electrical engineering",
      "score": 0.07975071668624878
    },
    {
      "name": "Philosophy",
      "score": 0.04886004328727722
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}