{
  "title": "The Importance of Generation Order in Language Modeling",
  "url": "https://openalex.org/W2888693386",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2255215409",
      "name": "Nicolas Ford",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2878790949",
      "name": "Daniel Duckworth",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1966219104",
      "name": "Mohammad Norouzi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2151732507",
      "name": "George Dahl",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953318193",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2949237929",
    "https://openalex.org/W2057147815",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2311132329",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2557321428",
    "https://openalex.org/W2405897494",
    "https://openalex.org/W3198494294",
    "https://openalex.org/W2626778328"
  ],
  "abstract": "Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence \"templates\" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of generation order for neural language models.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2942–2946\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n2942\nThe Importance of Generation Order in Language Modeling\nNicolas Ford∗ Daniel Duckworth Mohammad Norouzi George E. Dahl\nGoogle Brain\n{nicf,duckworthd,mnorouzi,gdahl}@google.com\nAbstract\nNeural language models are a critical compo-\nnent of state-of-the-art systems for machine\ntranslation, summarization, audio transcrip-\ntion, and other tasks. These language models\nare almost universally autoregressive in nature,\ngenerating sentences one token at a time from\nleft to right. This paper studies the inﬂuence of\ntoken generation order on model quality via a\nnovel two-pass language model that produces\npartially-ﬁlled sentence “templates” and then\nﬁlls in missing tokens. We compare various\nstrategies for structuring these two passes and\nobserve a surprisingly large variation in model\nquality. We ﬁnd the most effective strategy\ngenerates function words in the ﬁrst pass fol-\nlowed by content words in the second. We be-\nlieve these experimental results justify a more\nextensive investigation of generation order for\nneural language models.\n1 Introduction\nNeural networks have been extremely successful\nstatistical models of text in language modeling and\nmachine translation. Despite differences in model\narchitectures, state of the art neural nets gener-\nate sequences from left to right (Vaswani et al.,\n2017; Jozefowicz et al., 2016; Wu et al., 2016).\nAlthough in some sense humans produce and con-\nsume language from left to right as well, there are\nmany other intuitively appealing ways to gener-\nate text. For instance, language is slow enough\non a neurological time scale for multiple passes\nof generation that incorporate feedback to occur.\nLinguistic intuition might suggest that we should\nﬁrst generate some abstract representation of what\nwe want to say and then serialize it, a process that\nseems more universally appropriate given the ex-\nistence of languages with freer word order such as\nCzech and Polish.\n∗Work done as a member of the Google AI Residency\nprogram (g.co/airesidency)\nThere has been interest in moving beyond the\nleft-to-right generation order by developing alter-\nnative multi-stage strategies such as syntax-aware\nneural language models (Bowman et al., 2016)\nand latent variable models of text (Wood et al.,\n2011). Before embarking on a long-term research\nprogram to ﬁnd better generation strategies that\nimprove modern neural networks, one needs ev-\nidence that the generation strategy can make a\nlarge difference. This paper presents one way of\nisolating the generation strategy from the general\nneural network design problem. Our key techni-\ncal contribution involves developing a ﬂexible and\ntractable architecture that incorporates different\ngeneration orders, while enabling exact computa-\ntion of the log-probabilities of a sentence. Our ex-\nperiments demonstrate that even when using a few\nsimple two-pass generation orders, the differences\nbetween good and bad orderings are substantial.\nWe consider ways of reordering the tokens\nwithin a sequence based on their identities. The\nbest ordering we tried generates function words\nﬁrst and content words last, which cuts against the\nidea of committing to the general topic of a sen-\ntence ﬁrst and only then deciding exactly how to\nphrase it. We offer some possible explanations in\nSection 3, and we conclude that our experimen-\ntal results justify a more extensive investigation of\nthe generation order for language and translation\nmodels.\n2 Two-pass Language Models\nWe develop a family of two-pass language mod-\nels that depend on a partitioning of the vocabu-\nlary into a set of ﬁrst-pass and second-pass tokens\nto generate sentences. We perform a preprocess-\ning step on each sequence y, creating two new se-\nquences y(1) and y(2). The sequence y(1), which\nwe call the template, has the same length as y,\nand consists of the ﬁrst-pass tokens from y to-\ngether with a special placeholder token wherever\n2943\nsentence common ﬁrst rare ﬁrst function ﬁrst content ﬁrst odd ﬁrst\n” all you need to do\nif you want the na-\ntion ’s press camped\non your doorstep is to\nsay you once had a\n[UNK] in 1947 , ”\nhe noted memorably in\nhis diary . [EOS]\n” all you to if you\nthe ’s on\nis to you had a\n[UNK] in , ” he\nin his . [EOS]\nneed do\nwant nation\npress camped your\ndoorstep say\nonce 1947\nnoted memorably\ndiary [EOS]\n” all you to if you\nthe ’s on your\nis to you a\nin , ” he in his\n. [EOS]\nneed do\nwant nation press\ncamped doorstep\nsay once had\n[UNK] 1947\nnoted memorably\ndiary [EOS]\n” all you need\nyou the nation ’s\npress camped on your\ndoorstep say you\nonce had\n” noted his .\n[EOS]\nthe team announced\nthursday that the 6-\nfoot-1 , [UNK] starter\nwill remain in detroit\nthrough the 2013 sea-\nson . [EOS]\nthe that the ,\n[UNK] will in\nthe . [EOS]\nteam announced\nthursday 6-foot-1\nstarter remain\ndetroit through\n2013 season [EOS]\nthe that the\n, will in\nthrough the .\n[EOS]\nteam announced\nthursday 6-foot-1\n[UNK] starter\nremain detroit\n2013 season [EOS]\nthe team announced\nthe 6-foot-1\nwill remain\nthrough the 2013 .\n[EOS]\nscotland ’s next game\nis a friendly against\nthe czech republic at\nhampden on 3 march .\n[EOS]\n’s is a the\nat on . [EOS]\nscotland next game\nfriendly against\nczech republic ham-\npden 3 march\n[EOS]\n’s is a against\nthe at on .\n[EOS]\nscotland next game\nfriendly\nczech republic ham-\npden 3 march\n[EOS]\n’s next game\nthe czech republic at\nhampden on 3 march .\n[EOS]\nof course , millions of\nadditional homeown-\ners did make a big mis-\ntake : they took ad-\nvantage of ” liar loans\n” and other [UNK]\ndeals to buy homes\nthey couldn ’t afford .\n[EOS]\nof , of\na : they of\n” ” and [UNK]\nto they ’t .\n[EOS]\ncourse millions\nadditional homeown-\ners did make big\nmistake took ad-\nvantage liar loans\nother deals\nbuy homes couldn\nafford [EOS]\nof , of a\n: they of ”\n” and to\nthey . [EOS]\ncourse millions\nadditional home-\nowners did make\nbig mistake\ntook advantage\nliar loans other\n[UNK] deals buy\nhomes couldn ’t\nafford [EOS]\nof of additional\nbig\nthey advantage of\n” liar ” and other\ndeals buy homes\nthey couldn afford .\n[EOS]\nTable 1: Some example sentences from the dataset and their corresponding templates. The placeholder token is\nindicated by “ ”.\ny had a second-pass token. The sequence y(2) has\nlength equal to the number of these placeholders,\nand consists of the second-pass tokens from y in\norder.\nWe use a neural language model p1 to generate\ny(1), and then a conditional translation model p2\nto generate y(2) given y(1). Note that, since the\ndivision of the vocabulary into ﬁrst- and second-\npass tokens is decided in advance, there is a one-\nto-one correspondence between sequences y and\npairs (y(1),y(2)). The total probability ofy is then\np(y) = p1(y(1)) p2(y(2) | y(1)) . (1)\nTwo-pass language models present a unique op-\nportunity to study the importance of generation or-\nder because, since the template is a deterministic\nfunction of y, the probability of y can be com-\nputed exactly. This is in contrast to a language\nmodel using a latent generation order, which re-\nquires a prohibitive marginalization over permu-\ntations to compute the exact probabilities. Given\nthe tractable nature of the model, exact learning\nbased on log-likelihood is possible, and we can\ncompare different vocabulary partitioning strate-\ngies both against each other and against a single-\npass language model.\nOur implementation consists of two copies of\nthe Transformer model from Vaswani et al. (2017).\nThe ﬁrst copy just generates the template, so it has\nno encoder. The second copy is a sequence-to-\nsequence model that translates the template into\nthe complete sentence. There are three places in\nthis model where word embeddings appear — the\nﬁrst-phase decoder, the second-phase encoder, and\nthe second-phase decoder — and all three sets\nof parameters are shared. The output layer also\nshares the embedding parameters.1\nFor the second pass, we include the entire target\nsentence, not just the second-pass tokens, on the\noutput side. In this way, when generating a token,\nthe decoder is allowed to examine all tokens to the\nleft of its position. However, only the second-pass\ntokens count toward the loss, since in the other po-\nsitions the correct token is already known. Our\nloss function is then the sum of all of these num-\nbers (from both copies) divided by the length of\nthe original sentence, which is the log-perplexity\nthat our model assigns to the sentence.\nWe tried ﬁve different ways of splitting the vo-\ncabulary:\nCommon First and Rare First : The vocabu-\nlary was sorted by frequency and then a cutoff was\nchosen, splitting the vocabulary into “common”\nand “rare” tokens. The location of the cutoff2 was\nchosen so that the number of common tokens and\nthe number of rare tokens in the average sentence\nwere approximately the same. In “common ﬁrst”\n1This behavior is enabled in the publicly available im-\nplementation of Transformer using the hyperparameter called\nshared embedding and softmax weights.\n2In our experiments on LM1B, this is at index 78.\n2944\nwe place the common tokens in the ﬁrst pass, and\nin “rare ﬁrst” we start with the rare tokens.\nFunction First and Content First : We parsed\nabout 1% of LM1B’s training set using Parsey Mc-\nParseface (Andor et al., 2016) and assigned each\ntoken in the vocabulary to the grammatical role it\nwas assigned most frequently by the parser. We\nused this data to divide the vocabulary into “func-\ntion” words and “content” words; punctuation,\nadpositions, conjunctions, determiners, pronouns,\nparticles, modal verbs, “wh-adverbs” (Penn part-\nof-speech tag WRB), and conjugations of “be” were\nchosen to be function words. In “function ﬁrst” we\nplace the function words in the ﬁrst phase and in\n“content ﬁrst” we start with the content words.\nOdd First: As a control, we also used a linguis-\ntically meaningless split where tokens at an odd\nindex in the frequency-sorted vocabulary list were\nassigned to the ﬁrst pass and tokens with an even\nindex were assigned to the second pass.\nA few sentences from the dataset are shown in\nTable 1 together with their templates. Note that the\ncommon and function tokens are very similar; the\nmain differences are the “unknown” token, conju-\ngations of “have,” and some prepositions.\n3 Experimental Results and Discussion\nWe ran experiments with several different ways of\nsplitting the vocabulary into ﬁrst-pass and second-\npass tokens. We trained all of these models on the\nOne Billion Word Language Modeling benchmark\n(LM1B) dataset (Chelba et al., 2013). One sixth\nof the training data was used as a validation set.\nWe used a vocabulary of size 65,536 consisting of\nwhole words (rather than word pieces) converted\nto lower-case.\nWe compared the two-pass generation strategies\nto a baseline version of Transformer without an\nencoder, which was trained to unconditionally pre-\ndict the target sentences in the ordinary way. Be-\ncause the two-pass models contain slightly more\ntrainable parameters than this baseline, we also\ncompare to an “enhanced baseline” in which the\nsize of Transformer’s hidden space was increased\nto make the number of parameters match the two-\npass models.\nBoth the two-pass models and the baselines\nused the hyperparameters referred to as base in\nthe publicly available implementation of Trans-\nformer,3 which has a hidden size of 512, a ﬁlter\n3github.com/tensorﬂow/tensor2tensor\nsize of 2048, and 8 attention heads, except that the\nenhanced baseline used a hidden size of 704. We\nused a batch size of 4096. All models were trained\nusing ADAM (Kingma and Ba, 2014), with β1 =\n0.85, β2 = 0.997, and ϵ= 10−6. The learning rate\nwas tuned by hand separately for each experiment\nand the experiments that produced the best results\non the validation set are reported. Dropout was\ndisabled after some initial experimentation found\nit to be detrimental to the ﬁnal validation loss.\nTable 2 shows the results for all the two-pass\ngeneration strategies we tried as well as the base-\nlines, sorted from worst to best on the validation\nset. Strikingly, the linguistically meaningless odd\nﬁrst generation strategy that splits words arbitrar-\nily between the two phases is far worse than the\nbaseline, showing that the two-pass setup on its\nown provides no inherent advantage over a single\nphase. The common ﬁrst and closely related func-\ntion ﬁrst strategies perform the best of all the two-\npass strategies, whereas the rare ﬁrst and closely\nrelated content ﬁrst strategies are much worse.\nSince the control, rare ﬁrst, and content ﬁrst order-\nings are all worse than the baseline, the gains seen\nby the other two orderings cannot be explained by\nthe increase in the number of trainable parameters\nalone.\nThe enhanced version of the baseline achieved\nslightly better perplexity than the best of the two-\npass models we trained. Given that state-of-the-\nart results with Transformer require models larger\nthan the ones we trained, we should expect grow-\ning the embedding and hidden size to produce\nlarge beneﬁts. However, the two-pass model we\nproposed in this work is primarily a tool to under-\nstand the importance of sequence generation or-\nder and was not designed to be parameter efﬁcient.\nThus, as these results indicate, increasing the em-\nbedding size in Transformer is a more effective use\nof trainable parameters than having extra copies\nof the other model parameters for the second pass\n(recall that the embeddings are shared across both\npasses).\nOne potential explanation for why the func-\ntion ﬁrst split performed the best is that, in or-\nder to generate a sentence, it is easier to ﬁrst de-\ncide something about its syntactic structure. If\nthis is the primary explanation for the observed\nresults, then common ﬁrst’s success can be at-\ntributed to how many function words are also com-\nmon. However, an alternative explanation might\n2945\nModel Train Validation Test\nodd ﬁrst 39.925 45.377 45.196\nrare ﬁrst 38.283 43.293 43.077\ncontent ﬁrst 38.321 42.564 42.394\ncommon ﬁrst 36.525 41.018 40.895\nfunction ﬁrst 36.126 40.246 40.085\nbaseline 38.668 41.888 41.721\nenhanced baseline 35.945 39.845 39.726\nTable 2: The perplexities achieved by the best version of each of our models.\nsimply be that it is preferable to delay committing\nto a rare token for as long as possible as all subse-\nquent decisions will then be conditioning on a low-\nprobability event. This is particularly problematic\nin language modeling where datasets are too small\nto cover the space of all utterances. We lack sufﬁ-\ncient evidence to decide between these hypotheses\nand believe further investigation is necessary.\nUltimately, our results show that content-\ndependent generation orders can have a surpris-\ningly large effect on model quality. Moreover, the\ngaps between different generation strategies can\nbe quite large.\n4 Related Work\nFor tasks conditioning on sequences and sets, it is\nwell known that order signiﬁcantly affects model\nquality in applications such as machine transla-\ntion (Sutskever et al., 2014), program synthesis\n(Vinyals et al., 2016), and text classiﬁcation (Yo-\ngatama et al., 2016). Experimentally, Khandelwal\net al. (2018) show that recurrent neural networks\nhave a memory that degrades with time. Tech-\nniques such as attention (Bahdanau et al., 2014)\ncan be seen as augmenting that memory.\nText generation via neural networks, as in lan-\nguage models and machine translation, proceeds\nalmost universally left-to-right (Jozefowicz et al.,\n2016; Sutskever et al., 2014). This is in stark con-\ntrast to phrase-based machine translation systems\n(Charniak et al., 2003) which traditionally split\ntoken translation and “editing” (typically via re-\nordering) into separate stages. This line of work is\ncarried forward in Post-Editing Models (Junczys-\nDowmunt and Grundkiewicz, 2016), Deliberation\nNetworks (Xia et al., 2017), and Review Network\n(Yang et al., 2016) which produce a “draft” de-\ncoding that is further edited. As any valid se-\nquence may be used in a draft, calculating perplex-\nity in these models is unfortunately intractable,\nand model quality can only be evaluated via ex-\nternal tasks.\nIn addition to surface-form intermediate rep-\nresentation, syntax-based representations have a\nrich history in text modeling. Chelba and Je-\nlinek (1998); Yamada and Knight (2001); Graham\nand Genabith (2010); Shen et al. (2018) integrate\nparse structures, explicitly designed or automati-\ncally learned, into the decoding process.\nSimilar to the second phase of this work’s pro-\nposed model, (Fedus et al., 2018) directly tackles\nthe problem of ﬁlling in the blank, akin to the sec-\nond stage of our proposed model. The Multi-Scale\nversion of PixelRNN in (Van Oord et al., 2016)\nwas also an inspiration for the two-pass setup we\nused here.\n5 Conclusion and Future Work\nTo investigate the question of generation order\nin language modeling, we proposed a model that\ngenerates a sentence in two passes, ﬁrst generat-\ning tokens from left to right while skipping over\nsome positions and then ﬁlling in the positions that\nit skipped. We found that the decision of which to-\nkens to place in the ﬁrst pass had a strong effect.\nGiven the success of our function word ﬁrst\ngeneration procedure, we could imagine taking\nthis idea beyond splitting the vocabulary. One\ncould run a parser on each sentence and use the\nresulting tree to decide on the generation order.\nSuch a scheme might shed light on which aspect\nof this split was most helpful. Finally, ﬁlling in a\ntemplate with missing words is a task that might be\ninteresting in its own right. One might want to pro-\nvide partial information about the target sentence\nas part of scripting ﬂexible responses for a dia-\nlogue agent, question answering system, or other\nsystem that mixes a hand-designed grammar with\nlearned responses.\n2946\nReferences\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei\nSeveryn, Alessandro Presta, Kuzman Ganchev, Slav\nPetrov, and Michael Collins. 2016. Globally nor-\nmalized transition-based neural networks. CoRR,\nabs/1603.06042.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSamuel R Bowman, Jon Gauthier, Abhinav Ras-\ntogi, Raghav Gupta, Christopher D Manning, and\nChristopher Potts. 2016. A fast uniﬁed model for\nparsing and sentence understanding. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 1466–1477.\nEugene Charniak, Kevin Knight, and Kenji Yamada.\n2003. Syntax-based language models for statistical\nmachine translation. In Proceedings of MT Summit\nIX, pages 40–46.\nCiprian Chelba and Frederick Jelinek. 1998. Ex-\nploiting syntactic structure for language model-\ning. In Proceedings of the 36th Annual Meet-\ning of the Association for Computational Linguis-\ntics and 17th International Conference on Compu-\ntational Linguistics-Volume 1, pages 225–231. As-\nsociation for Computational Linguistics.\nCiprian Chelba, Tom´aˇs Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2013. One bil-\nlion word benchmark for measuring progress in sta-\ntistical language modeling. CoRR, abs/1312.3005.\nWilliam Fedus, Ian Goodfellow, and Andrew M. Dai.\n2018. MaskGAN: Better text generation via ﬁlling\nin the _______. In International Conference on\nLearning Representations (ICLR).\nYvette Graham and Josef Genabith. 2010. Deep syntax\nlanguage models and statistical machine translation.\nIn Proceedings of the 4th Workshop on Syntax and\nStructure in Statistical Translation, pages 118–126.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Log-linear combinations of monolingual\nand bilingual neural machine translation mod-\nels for automatic post-editing. arXiv preprint\narXiv:1605.04800.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away: How\nneural language models use context. arXiv preprint\narXiv:1805.04623.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nYikang Shen, Zhouhan Lin, Chin-wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In Interna-\ntional Conference on Learning Representations.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Process-\ning Systems (NIPS), pages 3104–3112.\nAaron Van Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. 2016. Pixel recurrent neural net-\nworks. In International Conference on Machine\nLearning, pages 1747–1756.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nOriol Vinyals, Samy Bengio, and Manjunath Kudlur.\n2016. Order matters: Sequence to sequence for sets.\nIn International Conference on Learning Represen-\ntations (ICLR).\nFrank Wood, Jan Gasthaus, C ´edric Archambeau,\nLancelot James, and Yee Whye Teh. 2011. The se-\nquence memoizer. Communications of the ACM ,\n54(2):91–98.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1782–1792.\nKenji Yamada and Kevin Knight. 2001. A syntax-\nbased statistical translation model. In Proceedings\nof the 39th Annual Meeting on Association for Com-\nputational Linguistics, pages 523–530. Association\nfor Computational Linguistics.\nZhilin Yang, Ye Yuan, Yuexin Wu, William W Cohen,\nand Ruslan R Salakhutdinov. 2016. Review net-\nworks for caption generation. In Advances in Neural\nInformation Processing Systems, pages 2361–2369.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2016. Learning to\ncompose words into sentences with reinforcement\nlearning. arXiv preprint arXiv:1611.09100.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8110724687576294
    },
    {
      "name": "Computer science",
      "score": 0.7831510305404663
    },
    {
      "name": "Security token",
      "score": 0.7777714729309082
    },
    {
      "name": "Machine translation",
      "score": 0.728378415107727
    },
    {
      "name": "Language model",
      "score": 0.6032902002334595
    },
    {
      "name": "Sentence",
      "score": 0.5490572452545166
    },
    {
      "name": "Autoregressive model",
      "score": 0.5448749661445618
    },
    {
      "name": "Natural language processing",
      "score": 0.5393765568733215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5260529518127441
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4561937749385834
    },
    {
      "name": "Natural language generation",
      "score": 0.43411970138549805
    },
    {
      "name": "Template",
      "score": 0.4173825681209564
    },
    {
      "name": "Structuring",
      "score": 0.4102481007575989
    },
    {
      "name": "Speech recognition",
      "score": 0.35582125186920166
    },
    {
      "name": "Natural language",
      "score": 0.16417831182479858
    },
    {
      "name": "Mathematics",
      "score": 0.09854400157928467
    },
    {
      "name": "Programming language",
      "score": 0.08066722750663757
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 4
}