{
  "title": "An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model",
  "url": "https://openalex.org/W2775766866",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5023098355",
      "name": "Anjuli Kannan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5010253402",
      "name": "Yonghui Wu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5111455517",
      "name": "Patrick Nguyen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5070513394",
      "name": "Tara N. Sainath",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100715424",
      "name": "Zhifeng Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5032640894",
      "name": "Rohit Prabhavalkar",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6743477263",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6714264011",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2296545762",
    "https://openalex.org/W6638749077",
    "https://openalex.org/W6692563993",
    "https://openalex.org/W6743440867",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W6640059789",
    "https://openalex.org/W6676562027",
    "https://openalex.org/W6639156005",
    "https://openalex.org/W1566256432",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2515439472",
    "https://openalex.org/W6629052376",
    "https://openalex.org/W6747398299",
    "https://openalex.org/W2750499125",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2410539690",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963920996",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2962988733",
    "https://openalex.org/W2748679025"
  ],
  "abstract": "Attention-based sequence-to-sequence models for automatic speech recognition jointly train an acoustic model, language model, and alignment mechanism. Thus, the language model component is only trained on transcribed audio-text pairs. This leads to the use of shallow fusion with an external language model at inference time. Shallow fusion refers to log-linear interpolation with a separately trained language model at each step of the beam search. In this work, we investigate the behavior of shallow fusion across a range of conditions: different types of language models, different decoding units, and different tasks. On Google Voice Search, we demonstrate that the use of shallow fusion with a neural LM with wordpieces yields a 9.1% relative word error rate reduction (WERR) over our competitive attention-based sequence-to-sequence model, obviating the need for second-pass rescoring.",
  "full_text": "AN ANALYSIS OF INCORPORATING AN EXTERNAL LANGUAGE MODEL INTO A\nSEQUENCE-TO-SEQUENCE MODEL\nAnjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N. Sainath,\nZhifeng Chen, Rohit Prabhavalkar\nGoogle, Inc., USA\n{anjuli,yonghui,drpng,tsainath,zhifengc,prabhavalkar}@google.com\nABSTRACT\nAttention-based sequence-to-sequence models for automatic\nspeech recognition jointly train an acoustic model, language model,\nand alignment mechanism. Thus, the language model component is\nonly trained on transcribed audio-text pairs. This leads to the use of\nshallow fusion with an external language model at inference time.\nShallow fusion refers to log-linear interpolation with a separately\ntrained language model at each step of the beam search. In this\nwork, we investigate the behavior of shallow fusion across a range of\nconditions: different types of language models, different decoding\nunits, and different tasks. On Google V oice Search, we demonstrate\nthat the use of shallow fusion with an neural LM with wordpieces\nyields a 9.1% relative word error rate reduction (WERR) over our\ncompetitive attention-based sequence-to-sequence model, obviating\nthe need for second-pass rescoring.\n1. INTRODUCTION\nSequence-to-sequence models have started to gain popularity for\nautomatic speech recognition (ASR) tasks, particularly for their ben-\neﬁt of folding various parts of the speech recognition pipeline (i.e.,\nacoustic, prononcuation and language modeling) into one neural net-\nwork [1, 2, 3, 4]. For example, the Listen, Attend, and Spell (LAS)\nmodel jointly learns an encoder, which serves as an acoustic model, a\ndecoder, which serves as a language model (LM), and an attention\nmechanism, which learns alignments. Recently, a comparison of\nthese different methods showed that performance still lagged behind\na state-of-the-art ASR system with separate acoustic, pronunciation\nand language models [ 5]. The focus of this paper is to explore a\nmeans of making LAS competitive to a conventional ASR model.\nWe propose that one reason for the performance degradation\ncould be that the LAS decoder, which replaces the LM component in\na traditional ASR system, is trained only on transcribed audio-text\npairs, which is about 15 million utterances for the Google V oice\nSearch task [5]. In comparison, state-of-the-art LMs are typically\ntrained on a billion words or more [ 6]. This raises the question of\nwhether the LAS decoder can learn a strong enough LM from the\ntraining transcripts. In particular, we posit that in a task like Google\nV oice Search, which has a very long tail of queries, the training\ntranscripts may not sufﬁciently expose the LAS decoder to rare words\nand phrases.\nHowever, these words may appear in auxiliary sources of text-\nonly data such as web documents or news articles, which comprise\nbillions of words. This work investigates the impact of training a\nseparate LM on auxiliary text-only data, and incorporating this model\nas an additional cost term when decoding a LAS model.\nSeveral recent works have also investigated the use of LMs with\nattention-based models. [1] demonstrated signiﬁcant improvement\nby rescoring the n-best hypotheses produced by LAS with a 5-gram\nLM. [2] extended this idea by performing log-linear interpolation\nbetween LAS and an n-gram LM at each step of the beam search, a\nmethod we will henceforth refer to as shallow fusion, following the\nterminology of [7]. Shallow fusion was further studied in [8], which\nextended it with use of a coverage penalty. Both of these works were\nlimited to Wall Street Journal (WSJ), which, given its scarcity of data,\nstands to gain more from an external LM than a large-scale task such\nas Google V oice Search. All of these works only investigatedn-gram\nLMs, and all focused on bidirectional models that output graphemes.\nThe use of an external LM has also been investigated in the\ncontext of training, such that the LAS model could learn when and\nhow to use the LM [7, 9, 10]. These works applied Recurrent Neural\nNetwork (RNN) LMs, but this was largely cited as a means to make\nthe integration simpler. None provided a direct comparison of RNN\nLMs to n-gram LMs. Further, they were all limited to grapheme\nsystems, with [7] focused on machine translation.\nThis work has two goals. First, we extend the work of [ 8] by\nexploring the behavior of shallow fusion across different sub-word\nunits and different types of LMs on a small corpus task. We ﬁnd that\nRNN LMs are more effective at reducing error than n-gram LMs,\nwith the magnitude of this reduction consistent across sub-word units.\nThe second goal of our work is to explore the behavior of shallow\nfusion on a large-scale, large-vocabulary English V oice Search task.\nV oice Search has much more training data than WSJ so it is not clear\nthat the beneﬁts observed on WSJ should necssarily translate; given\nsufﬁcient training data, the LAS decoder may be strong enough to\neliminate the effect of any external LM. Additionally, V oice Search\nrequires a unidirectional model, which has not previously been stud-\nied with shallow fusion. Ultimately, we ﬁnd that shallow fusion\nwith a worpiece-level RNN LM yields a 9.1% relative WERR on a\ncompetitive unidirectional baseline.\nThe next two sections will provide more details about the method\nwe use for integrating the LM and the variants that we compare.\nSection 4 describes the setup for our experiments on two different\ntasks, and Section 5 provides the results of these experiments. Finally,\nin Section 6 we conclude this study.\n2. SHALLOW FUSION WITH LAS MODELS\n2.1. Listen, attend, and spell\nAs shown inside the dotted line box in Figure 1, the LAS model\nconsists of an encoder (“listen”), an attention mechanism (“attend”),\narXiv:1712.01996v1  [eess.AS]  6 Dec 2017\nand a decoder (“spell”).\nThe encoder, which is akin to an acoustic model, consists of\na stack of long short-term memory layers (LSTMs) [ 11]. These\ntake as input a sequence of d-dimensional feature vectors, x =\n(x1,x2,··· ,xT ), where xt ∈ Rd, and produces a higher-order\nfeature representation, denoted henc\n1 ,··· ,henc\nT .\nThe output of the encoder is passed to an attention mechanism,\nwhich determines which part of the encoder features to attend to\nin order to predict each output symbol, effectively performing a\ndynamic time warping. The output of the attention mechanism is a\nsingle context vector that encodes this information.\nFinally, the decoder is another stack of LSTMs which is con-\nditioned on the context vector. Given the context vector and the\nprevious prediction yu−1 at timestep u, the decoder network gener-\nates logits hdecu. These are passed through a softmax to compute a\nprobability distribution P(yu|hdecu).\nThe decoder can be thought of as a neural LM conditioned on the\nacoustic model output; however, since the LAS model is structured\nsuch that the encoder feeds the decoder, this internal LM can only be\ntrained on audio-text pairs. In the next section, we will discuss the\nincorporation of an external LM.\nFig. 1: The dotted line box shows the basic LAS model, including an\nencoder, attention, and decoder. In shallow fusion, an external LM is\nincorporated via log-linear interpolation.\n2.2. Integrating a language model\nShallow fusion, shown in Figure 1, is a method for incorporating an\nexternal LM during inference only. As the ﬁgure shows, only the\ncontents of the dotted line box are used to train of the LAS model. At\ninference time, however, we perform log-linear interpolation with an\nLM at each step of the beam search. In other words, while the objec-\ntive criterion for decoding a sequence-to-sequence model typically\nwould be:\ny∗ = arg max\ny\nlog p(y|x) (1)\nwe instead use the following criterion:\ny∗ = arg max\ny\nlog p(y|x) +λlog pLM (y) +γc(x,y) (2)\nwhere pLM is provided by an LM, and λand γare tuned on a dev\nset. c(x,y) is referred to as a coverage penalty and is designed to\npenalize incomplete transcripts. It measures the extent to which the\ninput frames are “covered” by the attention weights, computed as:\nc(x,y) =\n∑\nj\nlog(min(\n∑\ni\nai,j,0.5)) (3)\nwhere ai,j is attention probability of the jth output label yj on the\nith input feature vector xi. By promoting transcripts which require\nattention to more of the audio frames, the coverage penalty addresses\nthe common sequence-to-sequence failure mode of assigning high\nprobability to a truncated output sequence [12]; like [8, 13], however,\nwe apply this only at decoding time. The effect of promoting longer\ntranscripts is similar to that of a length normalization or word insertion\nreward; unlike these atlernatives, however, it is less prone to produce\n“babbling”, since simply inserting more tokens while attending to the\nsame frames will not reduce the coverage penalty.\nAn alternative method of incorporating an LM would be to sim-\nply rescore the nbest transcripts produced by the beam search, as\nin [1]. Our initial experiments on the WSJ corpus showed this method\nprovided some reduction in error, but not as much as shallow fusion.\nThis is because the correct preﬁx may get pruned by the beam search\nearly on, and not make it into the n-best list.\n3. EXPLORING SHALLOW FUSION ACROSS TASKS,\nDECODING UNITS, AND TYPES OF LANGUAGE MODELS\n3.1. Tasks: WSJ vs. Google Voice Search\nThis work investigates the impact of shallow fusion on two different\ntasks. This is because we hypothesize that there are several task-\nspeciﬁc properties that can affect the relative gain afforded by an\nexternal LM:\n• Size of training corpus, because on a large training corpus the\nLAS decoder will itself be a very strong LM.\n• Size of vocabulary, as some of the beneﬁt of an external LM\nmay simply be exposure to unseen words and phrases.\n• Availability of LM training data, since the LM training data\nmust come from the same domain as the task\nOur ﬁrst set of experiments focuses on the WSJ corpus for several\nreasons. First, we have a large amount of text-only data also from\nWSJ, which reduces the possibility of domain mismatch between the\nLM and the LAS model. Second, given the relatively small size of\nthe WSJ corpus, we see that indeed many errors in a vanilla LAS\nmodel result from a poor LM. Third, we can use the standard setup\nfor the training data and vocabulary of the LM, making comparison\nto previous works more direct. Thus WSJ serves as a useful testbed\nfor measuring the contribution of an external LM.\nThe small training corpus, however, means that the gains seen\non the WSJ task may not necessarily transfer to a task with a much\nlarger training set. For this reason our second set of experiments is\ndone on the Google V oice Search task. Two notable properties of\nV oice Search are that it has a large vocabulary and it has a very long\ntail of queries.\n3.2. Decoding Units: Wordpieces vs. Graphemes\nWhile previous works have only investigated shallow fusion for\ngraphemes, we extend our study to wordpieces. Wordpieces [ 14]\nare sub-word units that can be as small as a single grapheme or as\nlarge as a complete word. First, a ﬁxed wordpiece vocabulary is\ndetermined based on frequencies of words in a training corpus. Once\nthe set of valid wordpieces is learned, a transcript can be tokenized\nby choosing the longest possible component wordpieces in a greedy\nfashion.\nLike graphemes, wordpieces have the advantage that there are no\nout-of-vocabulary terms because any word can be decomposed into\nwordpieces. (All graphemes are included in the wordpiece vocabu-\nlary.) But wordpieces have the additional beneﬁt that they effectively\ncapture more context per decoding step than graphemes. This reduces\nthe length of dependencies that must be learned by an LM.\nFor example, the phrase “the company announced today” consists\nof 27 graphemes, which means that a grapheme-level LM (LM-G)\nwould require 27 decoding steps to output the full phrase; but a\nwordpiece-level LM (LM-WP) might compose this phrase as, for ex-\nample the _com pany _announc ed _today which would\nrequire only 5 steps to output.\nAs a result, we expect that LM-WP can achieve lower (word-\nlevel) perplexity than LM-G, which could make it more effective in\nshallow fusion.\n3.3. Language Models: RNNs vs.n-gram\nThis work further compares shallow fusion across various types of\nLMs. Previous works have focused on n-gram LMs when applying\nshallow fusion [8, 2] or RNN LMs for deep or cold fusion [ 7, 15].\nHere we consider both n-gram LMs and RNN LMs [16] for shallow\nfusion.\nThere are several reasons that n-gram LMs have been preferred\nin past work. First, they can incorporate word-level constraints. Since\nwe incorporate the LM at each step of the beam search, the LM must\nprovide a probability distribution at the level of the LAS model’s\ndecoding unit (either grapheme or wordpiece). In the case of an RNN\nLM, this means that we train at the grapheme or wordpiece level. In\nthe case of an n-gram LM, however, there are two possible setups.\nThe most obvious is to train the LM at the level of the decoding\nunit (grapheme or wordpiece). However, in order to have a strong\ngrapheme-level LM, it is necessary to train at a very high order,\nsuch as 20-gram, to capture at least a few words worth of context.\nFollowing [2], an alternative is to train the LM at the word level,\nand then, using the Weighted Finite State Transducer framework\n[17, 18], compose it with a “speller” which breaks each word into its\ncomponent units (graphemes or wordpieces). In this way, we can still\nget a probability distribution at the unit level, while incorporating the\nknowledge of a word-level LM.\nFurthermore, this latter setup implicitly introduces a dictionary.\nIn a task like WSJ, the baseline model has a relatively weak decoder,\nso it will frequently output sequences of graphemes which do not\ncomprise English words. The dictionary constraints imposed by the\nn-gram LM can be helpful to prune these out.\nFinally, in a task like Google V oice Search, there are many\nsources of data that can potentially be useful in an external LM.\nWe can use Bayesian interpolation to combine n-gram LMs trained\nindividually on each of these domains, optimizing the interpolation\nweights against WER on a dev set [19]. Currently this sort of tech-\nnique only exists for n-gram LMs.\nDespite all these advantages of n-gram LMs, recent literature\nhas shown that state-of-the-art RNN LMs have a signiﬁcantly lower\nperplexity than n-gram LMs on the 1 billion word benchmark, par-\nticularly on rare words [ 6]. Thus we hypothesize that they should\nalso provide a greater reduction in error when used in shallow fusion.\nFurthermore, given enough training data, as we have in the Google\nV oice Search task, we suggest that the introduction of the dictionary\nmay not be necessary; in fact, it may be limiting to the model since\nthe LAS model can actually “sound out” words that it has never seen\nbefore but which are spelled phonetically. Though the techniques\nof Bayesian interpolation and incorporating dictionary constraints\ncurrently apply only the n-gram models, we posit that analogous\nmethods should be possible for RNN LMs, and identify these as areas\nfor future work.\n4. EXPERIMENTAL DETAILS\n4.1. Wall Street Journal\nOur experiments are conducted on two tasks. The ﬁrst is the WSJ\ndataset. Following the setup in [ 8], we train on si284, validate on\ndev93 and evaluate on eval92.\nFor grapheme experiments, our baseline model is a LAS model\nwith 3 convolutional layers and a convolutional LSTM layer, followed\nby 3 bidirectional [20] LSTM layers. The output vocabulary is 72\ngraphemes. Temporal label smoothing is applied as described in\n[8]. For wordpiece experiments, our baseline model has the same\narchitecture as the grapheme model, except that the output vocabulary\nhas 1,024 wordpieces and no label smoothing is applied because label\nsmoothing resulted in a weaker model. Instead, L2 regularization is\nused.\nThe external LMs are trained using the WSJ text corpus and\nextended vocabulary provided in the Kaldi WSJ s5 recipe [21]. The\nRNN LMs consist of two LSTM layers of 512 hidden units. The word-\n, grapheme- , and wordpiece-level n-gram LMs are all trained with\nKatz smoothing and pruned to between 15M and 20M n-grams. The\nword-level LM is composed with a speller to decode at the grapheme\nor wordpiece level.\n4.2. Google Voice Search\nThe second task is a ∼12,500 hour training set consisting of 15M\nEnglish utterances. The training utterances are anonymized and hand-\ntranscribed, and are representative of Google’s V oice Search trafﬁc.\nThis data set is created by artiﬁcially corrupting clean utterances using\na room simulator, adding varying degrees of noise and reverberation\nsuch that the overall SNR is between 0dB and 30dB, with an average\nSNR of 12dB. The noise sources are from YouTube and daily life\nnoisy environmental recordings. We report results on two sets of\n∼14,800 anonymized, hand-transcribed V oice Search utterances each,\nextracted from Google trafﬁc.\nThe baseline model for V oice Search experiments has an encoder\nconsisting of 5 unidirectional LSTM layers of 1,400 units each, a\ndecoder consisting of 2 LSTM layers with 1,024 hidden units each,\nand a multi-headed attention mechanism [22]. We use a unidirectional\nencoder because the V oice Search task requires a streaming model.\nAll experiments use 80-dimensional log-mel features, computed\nwith a 25-ms window and shifted every 10ms. Similar to [23, 24], at\nthe current frame, t, these features are stacked with 3 frames to the left\nand downsampled to a 30ms frame rate. The models are trained with\nthe cross-entropy criterion, using asynchronous stochastic gradient\ndescent optimization in TensorFlow [25].\nOur text dataset consists of billions of sentences from several\nsources: untranscribed anonymized V oice Search queries, untran-\nscribed anonymized voice dictation queries, anonymized typed\nqueries from Google Search, as well as the transcribed training utter-\nances mentioned above. The production LMs denoted as PRODLM 1\nand PRODLM 1 are both 5-gram LMs with a vocabulary of 4M.\nPRODLM 1 is constructed as a Bayesian-interpolated mixture of LMs\ntrained on the individual data sources [19], while PRODLM 2 is trained\non all data. Following [26], the RNN LM is trained on about half a\nbillion sentences sampled from the full pool data. It consists of two\nLSTM layers of 2,048 units each.\n5. RESULTS\n5.1. Comparing LMs for shallow fusion\nWe begin by comparing three types of LMs in the context of shallow\nfusion with the LAS grapheme model LAS -G on the WSJ task: (1) an\nRNN LM trained on graphemes (RNN -G), (2) a 20-gram LM trained\non graphemes (20- GRAM -G), and (3) a 3-gram LM trained on words\nand composed with a speller (3-GRAM -W).\nComparing these, we see that 3-GRAM -W barely outperforms 20-\nGRAM -G. This shows that, given the same amount of context, having\nword constraints and an implicit dictionary has only a slight beneﬁt.\nRNN -G, however, outperforms both of then-gram LMs, suggesting\nwhile the word constraints may help, they are insufﬁcient to make up\nthe gap between RNN LMs and n-gram LMs. One opportunity for\nfuture work would be incorporating word constraints into RNN -G.\nSystem Dev Test\nLAS -G 13.0 10.3\nLAS -G + 20- GRAM -G 10.3 7.7\nLAS -G + 3-GRAM -W 10.0 7.6\nLAS -G + RNN -G 9.3 6.9\nTable 1: WER of LAS -G fused with various LMs. While word\nconstraints do help the n-gram LM, RNN -G performs even better.\n5.2. Extending shallow fusion to wordpiece models\nNext, we perform a comparison for LAS -WP. Since we have shown\nthat word constraints are helpful for sub-word-level n-gram LMs, we\nlimit our comparison to just two LMs: (1) an RNN LM trained on\nwordpieces (RNN -WP), and (2) a 3-gram LM trained on words and\ncomposed with a speller (3-GRAM -W).\nAs Table 2 shows, we see the same trend on LAS -WP, with\nRNN -WP signiﬁcantly better than 3-GRAM -W. However, it should be\nnoted that the baseline LAS -WP is worse than LAS -G. This is likely\ndue to the small amount of data being insufﬁcient to train the large\nnumber of additional parameters: we found that the larger we made\nthe wordpiece vocabulary, the worse the model became. As a result of\nthis difference, the LM results forLAS -WP are not directly comparable\nto the LM results for LAS -G. The main observation we make is that\nthe RNN performs best in both cases, with the relative improvement\nbeing roughly consistent for both graphemes and wordpieces.\nSystem Dev Test\nLAS -WP 15.7 12.3\nLAS -WP + 3-GRAM -W 12.9 9.3\nLAS -WP + RNN -WP 11.5 8.2\nTable 2: WER of LAS -WP combined with various LMs on WSJ.\nRNN -WP again performs best.\n5.3. Scaling up to Voice Search\nWe now turn to the V oice Search task. First, since we have an abun-\ndance of training data, we see in the ﬁrst two lines of Table 3 that the\nwordpiece model (LAS -WP) is now comparable with the grapheme\nmodel (LAS -G). Thus our analysis here is limited to LAS -WP.\nIn the traditional HMM/CTC-based system, the decoding\nproceeds in two passes: the ﬁrst pass uses a small n-gram LM\n(PRODLM 1), which ﬁts in memory and minimizes the search space\nto meet real-time requirements. The ﬁrst pass generates an N-best\nlist which we rescore with a much larger n-gram LM (PRODLM 2)\n[19]. In the third and fourth lines of Table 3 we see the results of\napplying the production LMs to the LAS model with shallow fusion:\nthe LM inherent in LAS is quite competitive, but there is a small\ngain from the highly-pruned PRODLM 1. The much larger PRODLM 2,\ndespite being 40x larger, provides only slightly more improvement.\nIn addition, PRODLM 2 is 80GB and must be run on multiple servers.\nThis is operationally unwieldy and cannot be efﬁciently integrated\nwith low latency during the ﬁrst pass.\nOn the other hand, while computationally expensive, RNN LMs\nare known to be more compact than theirn-gram counterparts. In line\n5 of Table 3, LAS -WP + RNN -WP, we show that the shallow fusion of\nLAS with RNN -WP provides an even greater beneﬁt than PRODLM 2.\nIts much lower memory footprint (1.1 GB) allows it to ﬁt in the ﬁrst\npass. We then rescore the system with PRODLM 2 (as LAS -WP + RNN -\nWP + PRODLM 2). This yields no further gain, showing that we have\nobviated the need for a second-pass rescoring at all.\nThus, as with WSJ, we see thatRNN -WP more effectively encodes\nthe LM information compared to the n-gram model. In addition,\nRNN -WP is 1.5% the size of PRODLM 2, and also enjoys the additional\nbeneﬁt of not having out-of-vocabulary words since it is trained on\nwordpieces. Note that both PRODLM 1 and PRODLM 2 are interpolated\nacross several data-source-speciﬁc LMs, while RNN -WP uses ad hoc\nmixing weights for the various data sources. Investigating a more\nprincipled method of mixing the data sources for RNN -WP is an\nopportunity for future work.\nSystem Dev Test LM size\nLAS -G 9.5 7.7 0GB\nLAS -WP 9.2 7.7 0GB\nLAS -WP + PRODLM 1 8.8 7.4 2GB\nLAS -WP + PRODLM 2 8.7 7.2 80GB\nLAS -WP + RNN -WP 8.4 7.0 1.1GB\nLAS -WP + RNN -WP + PRODLM 2 8.4 7.0 81.1GB\nTable 3: WER of shallow fusion of LAS with production n-gram\nLMs and an RNN LM. The RNN LM captures all the beneﬁts of\nPRODLM 2 in a compact form.\n6. CONCLUSIONS\nIn this work we investigated the technique of shallow fusion, in\nwhich an external LM is used to augment a LAS model at inference\ntime. We demonstrated that on the small WSJ task, an RNN LM\nyielded greater improvement than an n-gram LM, and the gains were\nconsistent across graphemes and wordpieces. On the much larger\nV oice Search task, we showed that the decoder LM inherent in LAS\nis already very competitive, yielding little beneﬁt from shallow fusion\nwith the ﬁrst-pass production LM. However, we found that shallow\nfusion with an RNN LM provided greater beneﬁt. In fact, with 9.1%\nrelative WERR on a competitive unidirectional system, it eliminated\nthe need for a second pass rescoring, despite being 70 times smaller\nthan the second pass LM.\n7. ACKNOWLEDGEMENTS\nThe authors would like to thank Jan Chorowski, Navdeep Jaitly,\nShankar Kumar, Kanishka Rao, Brian Roark, and David Rybach, for\nhelpful discussions.\n8. REFERENCES\n[1] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, “Listen, attend\nand spell,” CoRR, vol. abs/1508.01211, 2015.\n[2] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Ben-\ngio, “End-to-End Attention-based Large V ocabulary Speech\nRecognition,” in Proc. ICASSP, 2016.\n[3] A. Graves, “Sequence transduction with recurrent neural net-\nworks,” CoRR, vol. abs/1211.3711, 2012.\n[4] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, “Con-\nnectionist Temporal Classiﬁcation: Labeling Unsegmented\nSeuqnece Data with Recurrent Neural Networks,” in Proc.\nICML, 2006.\n[5] R. Prabhavalkar, K. Rao, B. Li, L. Johnson, and N. Jaitly,\n“A Comparison of Sequence-to-sequence Models for Speech\nRecognition,” in Proc. Interspeech, 2017.\n[6] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and\nY . Wu, “Exploring the limits of language modeling,” CoRR,\nvol. abs/1602.02410, 2016.\n[7] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y .Bengio, “On using mono-\nlingual corpora in neural machine translation,” CoRR, vol.\nabs/1503.03535, 2015.\n[8] J. K. Chorowski and N. Jaitly, “Towards Better Decoding and\nLanguage Model Integration in Sequence to Sequence Models,”\nin Proc. Interspeech, 2017.\n[9] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, “Advances in\njoint ctc-attention based end-to-end speech recognition with a\ndeep cnn encoder and rnn-lm,” 2017.\n[10] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold fu-\nsion: Training seq2seq models together with language models,”\nCoRR, vol. abs/1708.06426, 2017.\n[11] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,”\nNeural Computation, vol. 9, no. 8, pp. 1735–1780, Nov 1997.\n[12] Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, “Modeling coverage for\nneural machine translation,” CoRR, vol. abs/1601.04811, 2016.\n[13] Y . Wu, M. Schuster, and et. al., “Google’s neural machine trans-\nlation system: Bridging the gap between human and machine\ntranslation,” CoRR, vol. abs/1609.08144, 2016.\n[14] M. Schuster and K. Nakajima, “Japanese and Korean voice\nsearch,” 2012 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, 2012.\n[15] Improving neural machine translation models with monolingual\ndata, “R. sennrich and b. haddow and a. birch,” Proceedings of\nthe 54th Annual Meeting of the Association for Computational\nLinguistics, 2015.\n[16] T.Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and S. Khudan-\nbur, “Recurrent neural network based language model,” Proc.\nInterspeech, 2010.\n[17] M. Mohri, F. Pereira, and M. Riley, “Weighted ﬁnite state\ntransducers in speech recognition,” vol. 16, pp. 69–88, 2002.\n[18] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri,\n“Openfst: A general and efﬁcient weighted ﬁnite-state transducer\nlibrary,” pp. 11–23, 2007.\n[19] C. Allauzen and M. Riley, “Bayesian language model interpola-\ntion for mobile speech input,” Proc. Interspeech, 2011.\n[20] M. Schuster and K. K. Paliwal, “Bidirectional LSTM Networks\nfor Improved Phoneme Classiﬁcation and Recognition,” Artiﬁ-\ncial Neural Networks: Formal Models and Their Applications-\nICANN, pp. 799–804, 2005.\n[21] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, J. Silovsky\nP. Schwarz, G. Stemmer, , and K. Vesely, “The kaldi speech\nrecognition toolkit,” 2011.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention Is All You\nNeed,” CoRR, vol. abs/1706.03762, 2017.\n[23] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and Accu-\nrate Recurrent Neural Network Acoustic Models for Speech\nRecognition,” in Proc. Interspeech, 2015.\n[24] G. Pundak and T. N. Sainath, “Lower Frame Rate Neural\nNetwork Acoustic Models,” in Proc. Interspeech, 2016.\n[25] M. Abadi et al., “TensorFlow: Large-Scale Machine Learn-\ning on Heterogeneous Distributed Systems,” Available on-\nline: http://download.tensorﬂow.org/paper/whitepaper2015.pdf,\n2015.\n[26] K. Rao, R. Prabhavalkar, and H. Sak, “Exploring Architectures,\nData and Units for Streaming End-to-End Speech Recognition\nwith RNN-Transducer,” in Proc. ASRU, 2017.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.7530738711357117
    },
    {
      "name": "Computer science",
      "score": 0.7318333387374878
    },
    {
      "name": "Sequence (biology)",
      "score": 0.7240543365478516
    },
    {
      "name": "Speech recognition",
      "score": 0.5686619281768799
    },
    {
      "name": "Inference",
      "score": 0.5374683737754822
    },
    {
      "name": "Word error rate",
      "score": 0.536735475063324
    },
    {
      "name": "Beam search",
      "score": 0.5326209664344788
    },
    {
      "name": "Decoding methods",
      "score": 0.522860050201416
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.5195032358169556
    },
    {
      "name": "Fusion",
      "score": 0.5132744312286377
    },
    {
      "name": "Word (group theory)",
      "score": 0.5032088160514832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4496316909790039
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4436846971511841
    },
    {
      "name": "Natural language processing",
      "score": 0.4430438280105591
    },
    {
      "name": "Algorithm",
      "score": 0.22123825550079346
    },
    {
      "name": "Search algorithm",
      "score": 0.13932177424430847
    },
    {
      "name": "Linguistics",
      "score": 0.1341344118118286
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Motion (physics)",
      "score": 0.0
    }
  ]
}