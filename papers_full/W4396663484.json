{
    "title": "Private-preserving language model inference based on secure multi-party computation",
    "url": "https://openalex.org/W4396663484",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2061253032",
            "name": "Chen Song",
            "affiliations": [
                "Guangxi University",
                "Guangxi Science and Technology Department"
            ]
        },
        {
            "id": "https://openalex.org/A2336075677",
            "name": "Ruwei Huang",
            "affiliations": [
                "Guangxi Science and Technology Department",
                "Guangxi University"
            ]
        },
        {
            "id": "https://openalex.org/A2121283381",
            "name": "Sai Hu",
            "affiliations": [
                "Guangxi University",
                "Guangxi Science and Technology Department"
            ]
        },
        {
            "id": "https://openalex.org/A2061253032",
            "name": "Chen Song",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2336075677",
            "name": "Ruwei Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121283381",
            "name": "Sai Hu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6606067566",
        "https://openalex.org/W6778434676",
        "https://openalex.org/W6745940941",
        "https://openalex.org/W2039605106",
        "https://openalex.org/W2141420453",
        "https://openalex.org/W2435473771",
        "https://openalex.org/W2765200655",
        "https://openalex.org/W3173128495",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6767212563",
        "https://openalex.org/W3032769455",
        "https://openalex.org/W3128341947",
        "https://openalex.org/W6870621415",
        "https://openalex.org/W6795063325",
        "https://openalex.org/W6800286734",
        "https://openalex.org/W1635361314",
        "https://openalex.org/W6631325483",
        "https://openalex.org/W4308242475",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W150223756",
        "https://openalex.org/W4386764217",
        "https://openalex.org/W3102554603",
        "https://openalex.org/W2970612075",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2952357537",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4281806276",
        "https://openalex.org/W2768347741",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4400382079",
        "https://openalex.org/W4385573876",
        "https://openalex.org/W2923890923",
        "https://openalex.org/W3028867652",
        "https://openalex.org/W4400020588",
        "https://openalex.org/W2542518443",
        "https://openalex.org/W4286989576",
        "https://openalex.org/W3121694563"
    ],
    "abstract": "With the exponential expansion of Internet information, technology that combines big data and artificial intelligence has gradually developed. Pre-trained large-scale language models with the transformer architecture as the core have begun to be used in daily life, resulting in the huge market of MLaaS. leading to the significant market of Machine Learning as a Service (MLaaS). Although MLaaS brings huge benefits to users, it requires receiving users' data for processing, which includes many sensitive data. While MLaaS offers considerable benefits, it necessitates processing users' data, which includes much sensitive data.Therefore, the problem of privacy data leakage has also been exposed. In this article paper, we propose a novel language model secure inference scheme based on secure multi-party computation (MPC) technology. This solution involves three non-colluding parties: the data provider, the model provider, and the computing power provider. Compared with direct inference on pre-trained large models, the proposed security inference framework improves the inference speed by 1.55-6.25 times. Our findings demonstrate that, when compared to conventional inference methods on pre-trained large-scale models, our approach significantly enhances inference efficiency, achieving speed improvements ranging from 1.55 to 6.25 times.",
    "full_text": null
}